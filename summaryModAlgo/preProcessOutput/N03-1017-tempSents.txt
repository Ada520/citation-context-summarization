36	koehn et al demonstrated that choosing the appropriate heuristic for extracting phrases is very important. 
170	recently, various works have improved the quality of statistical machine translation systems by using phrase translation . 
159	koehn et al mention german „Äàes gibt, there is„Äâ as an example of a good phrase pair which is not a syntactic phrase pair, and report that favoring syntactic phrases does not improve accuracy. 
57	modifiers within german clauses were translated using the phrase-based model of koehn et al . 
114	based on their analysis of the relationship between translation quality and phrase length, koehn et al suggest limiting phrase length to three words or less. 
162	the current state of the art is represented by the so-called phrase-based translation approach . 
131	it has been shown that phrasal machine translation systems are not affected by the quality of the input word alignments . 
97	138 2 rule generation we start with phrase translations on the parallel training data using the techniques and implementation described in . 
78	accordingly, in this section we describe a set of experiments which extends the work of by evaluating the marker-based ebmt system of against a phrase-based smt system built using the following components: ‚Ä¢ giza++, to extract the word-level correspondences; ‚Ä¢ the giza++ word alignments are then refined and used to extract phrasal alignments ; or for a more recent implementation); ‚Ä¢ probabilities of the extracted phrases are calculated from relative frequencies; ‚Ä¢ the resulting phrase translation table is passed to the pharaoh phrase-based smt decoder which along with sri language modelling toolkit5 performs translation. 
142	we used the heuristic combination described in and extracted phrasal translation pairs from this combined alignment as described in . 
43	phrase-based smt systems have been shown to outperform word-based approaches . 
19	word alignment and phrase extraction we used the giza++ word alignment software 3 to produce initial word alignments for our miniature bilingual corpus consisting of the source french file and the english reference file, and the refined word alignment strategy of to obtain improved word and phrase alignments. 
102	word alignment‚Äîdetection of corresponding words between two sentences that are translations of each other‚Äîis usually an intermediate step of statistical machine translation , but also has been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval. 
118	 show that exploiting all contiguous word blocks in phrase-based alignment is better than focusing on syntactic constituents only. 
4	training begins with phrase pairs, obtained as by och, koehn, and others: giza++ is used to obtain one-to-many word alignments in both directions, which are combined into a single set of refined alignments using the final-andù method of koehn et al ; then those pairs of substrings that are exclusively aligned to each other are extracted as phrase pairs. 
152	we compared a baseline system, the state-of-the-art phrase-based system pharaoh , against our system. 
29	the heuristics in koehn et al decide whether to extract a given phrase pair based on the underlying word alignments , which we call constellations. 
51	we use the model of koehn et al as a baseline for our experiments. 
147	phrase tables were learned from the training corpus using the diag-andù method , and using ibm model 2 to produce initial word alignments . 
83	a similar argument applies to phrase-based translation methods ). 
127	for an initial alignment, we used giza++ in both directions or spanish ), and also two different combined alignments: intersection of e-to-f and f-to-e; and ra using a heuristic combination approach called grow-diag-final . 
100	for comparison purposes, three additional heuristically-induced alignments are generated for each system: intersection of both directions ); union of both directions ); and the previously bestknown heuristic combination approach called growdiag-final ). 
106	the features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in ; phrase translation model probabilities; and trigram language model probabilities logp , using kneser-ney smoothing as implemented in the srilm toolkit . 
9	the phrase-based decoder extracts phrases from the word alignments produced by giza++, and computes translation probabilities based on the frequency of one phrase being aligned with another . 
150	the normalization is visualized as a translation problem where messages in the sms language are to be translated to normal english using a similar phrase-based statistical mt method . 
123	pharaoh performed decoding using a set of default parameters for weighting the relative influence of the language, translation and distortion models . 
54	each list contained the n-best translations produced by the phrase-based system of koehn et al . 
110	koehn et al compare a number of different approaches to phrase-based statistical machine 255 length num uniq average # translations avg trans length 1 .88 
135	one is a phrase-based translation in which a phrasal unit is employed for translation . 
65	results using the method show an improvement from 25.2% bleu score to 26.8% bleu score , using a phrase-based system which has been shown in the past to be a highly competitive smt system. 
93	baseline pharaoh with phrases extracted from ibm model 4 training with maximum phrase length 7 and extraction method diag-growthfinal lex phrase-decoder simulation: using only the initial lexical rules from the phrase table, all with lhs x, the glue rule, and a binary reordering rule with its own reordering-feature ‚Ä¢ xcat all nonterminals merged into a single x nonterminal: simulation of the system hiero . 
70	under a phrase based translation model , this distinction is important and will be discussed in more detail. 
91	for the future, the joint model would benefit from lexical weighting like that used in the standard model . 
166	the inclusion of phrases longer than three words in translation resources has been avoided, as it has been shown not to have a strong impact on translation performance . 
76	we collected the pp parameters bys imply reading the alignment matrices resulting from the word alignment, in a way similar to the one described in . 
41	in an experimental evaluation on the test-set that was used in koehn et al we show that for examples that are in coverage of the grammar-based system, we can achieve stateof-the-art quality on n-gram based evaluation measures. 
38	final results are reported on the test set of 1,755 sentences of length 5-15 that was used in koehn et al . 
105	791 and score the alignment template model phrases . 
56	an evaluation of the method on translation from german to english shows similar performance to the phrase-based model of koehn et al . 
167	whereas language generation has benefited from syntax , the performance of statistical phrase-based machine translation when relying solely on syntactic phrases has been reported to be poor . 
137	 the phrase extraction algorithm is based on those presented by koehn et al . 
89	most phrase-based translation models rely on a pre-existing set of word-based alignments from which they induce their parameters. 
132	this dependency graph is partitioned into treelets; like , we assume a uniform probability distribution over all partitions. 
11	the lexical weight a27 a14a12a91 a29 a92a93a21 of the block a9 a72 a14a12a91 a19a86a92a93a21 is computed similarly to , details are given in section 3.4. 
7	in , various aspects of phrase-based systems are compared.
133	for each differently tokenized corpus, we computed word alignments by a hmm translation model and by a word alignment refinement heuristic of ‚Äúgrow-diagfinal‚Äù . 
112	table 4 gives statistics about phrases which occur more than once in the english section of the europarl corpus which was used in the koehn et al experiments. 
33	at the end we ran our models once on test to get final numbers.2 4 models our experiments used phrase-based models , which require a translation table and language model for decoding and feature computation. 
129	for our experiments, we chose giza++ and the ra approach ‚Äî the best known alignment combination technique‚Äî as our initial aligners.1 4.2 tbl templates our templates consider consecutive words in both languages. 
113	the framework that we used to calculate the translation probabilities was similar to that detailed in koehn et al . 
52	evaluation in terms of both bleu scores and human judgments shows that our system performs similarly to the phrase-based model of koehn et al . 
8	although the best performing systems are ‚Äúphrasebased‚Äù or koehn et al ), possible phrase translations must first be extracted from word-aligned bilingual text segments. 
2	we generated for each phrase pair in the translation table 5 features: phrase translation probability , lexical weighting and phrase penalty . 
49	during the last decade, statistical machine translation systems have evolved from the original word-based approach into phrase-based translation systems . 
103	limitations of basic word-based models prompted researchers to exploit morphological and/or syntactic/phrasal structure , lee, , yamada and knight , marcu and wong , och and ney ,koehn et al , among others.) 
48	we carry out experiments using a phrase-based statistical machine translation system . 
61	we therefore adopted a modified weighting scheme following , whichincorporates null alignments. 
134	our phrase-based model uses a standard pharaoh feature functions listed as follows : ‚Ä¢ relative-count based phrase translation probabilities in both directions. 
25	the translation model used in is the product of translation probability a34a35a4 a29 a0 a33 a6 a29 a2 a33 a8 and distortion probability a36a37a4a39a38 a33a41a40a43a42a44a33a46a45 a32 a8 , a3a5a4a35a29 a0 a30 a32 a6 a29 a2 a30 a32 a8 a10 a30 a47 a33a49a48 a32 a34a35a4 a29 a0a22a33 a6 a29 a2 a33a50a8 a36a51a4a39a38 a33 a40a52a42 a33a53a45 a32 a8 where a38 a33 denotes the start position of the source phrase translated into the a54 -th target phrase, and a42 a33a53a45 a32 denotes the end position of the source phrase translated into the a4a53a54 a40a56a55 a8 -th target phrase. 
94	recent work in machine translation has evolved from the traditional word and phrase based models to include hierarchical phrase models and bilingual synchronous grammars . 
115	our system is a re-implementation of the phrase-based system described in koehn , and uses publicly available components for word alignment 1, decoding 2, language modeling 3 and finite-state processing 4. 
86	the focus of the task was to build a probabilistic phrase translation table, since most of the other resources were provided ‚Äî for more on phrase-based statistical machine translation, refer to koehn et al . 
160	it is important because a word-aligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based machine translation , , 
163	phrase-based models are good at learning local translations that are pairs of sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. 
107	traditionally, maximum-likelihood estimation from relative frequencies is used to obtain conditional probabilities , eg, p = c /summationtextÀús c and p are symmetrical, we will usually refer only to p for brevity). 
20	similarly, propose a relative distortion model to be used with a phrase decoder. 
31	to facilitate comparison with previous work, we created the translation tables using the same techniques as koehn et al .3 
169	a word link extension algorithm similar to the one presented in this paper is given in . 
90	the impact of constraining the joint model trained on 10,000 sentences of the german-english europarl corpora and tested with the europarl test set used in koehn et al than 10 million times that of the phrase pair with the highest count, are pruned from the phrase table. 
116	we used the word alignment produced by giza out of an ibm model 2. we did try to use the alignment produced with ibm model 4, but did not notice significant differences over our experiments; an observation consistent with the findings of koehn et al . 
109	this is the traditional approach for glass-box smoothing . 
140	for details, please refer to koehn et al . 
126	most current smt systems use a generative model for word alignment such as the freely available giza++ , an implementation of the ibm alignment models . 
172	word alignment is an important component of a complete statistical machine translation pipeline . 
69	our baseline is the phrase-based mt system of . 
98	to evaluate neuralign, we used giza++ in both directions or spanish ) as input and a refined alignment approach that uses a heuristic combination method called grow-diagfinal for comparison. 
45	today's statistical machine translation systems rely on high quality phrase translation pairs to acquire state-of-the-art performance, see . 
168	during the last four years, various implementations and extentions to phrase-based statistical models have led to significant increases in machine translation accuracy. 
50	in recent years, phrase-based systems for statistical machine translation have delivered state-of-the-art performance on standard translation tasks. 
68	in this paper we use the phrase-based system of as our underlying model. 
24	here, ppickerù shows the accuracy when phrases are extracted by using the n-best phrase alignment method described in section 4.1, while growdiag-finalù shows the accuracy when phrases are extracted using the standard phrase extraction algorithm described in . 
3	the lexical-weighting features are estimated using a method similar to that of koehn et al . 
171	using giza++ model 4 alignments and pharaoh , we achieved a bleu score of 0.3035. 
67	in experiments with the system of we have found that in practice a large number of complete translations are completely monotonic , suggesting that the system has difficulty learning exactly what points in the translation should allow reordering. 
154	the baseline system we used for comparison was pharaoh , as publicly distributed. 
143	systems must therefore impose some limits on phrasal reordering, often hard limits based on distance as in koehn et al or some linguistically motivated constraint, such as itg . 
82	we generate widl-expressions from chinese strings by exploiting a phrase-based translation table . 
66	recent research on statistical machine translation has lead to the development of phrase-based systems . 
119	the pharaoh decoder use an alternative 66 figure 2: flow chart associated to the expansion of a hypothesis when using a multi-stack algorithm. 
39	under the nist measure, we achieve results in the range of the state-of-the-art phrase-based system of koehn et al for in-coverage examples of the lfgbased system. 
149	however, found that it is actually harmful to restrict phrases to constituents in parse trees, because the restriction would cause the system to miss many reliable translations, such as the correspondence between ‚Äúthere is‚Äù in english and ‚Äúes gibt‚Äù in german. 
145	details of this model are described by koehn et al . 
34	we also compared our model with pharaoh . 
37	the statistical components of our system are modeled on the phrase-based system of koehn et al , and component weights are adjusted by minimum error rate training . 
79	this translation model differs from the well known phrase-based translation approach in two basic issues: rst, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. 
63	the advantage of using phrase-based translation in a statistical framework has been shown in many studies such as . 
157	to do this, we first identify initial phrase pairs using the same criterion as previous systems : definition 1. 
26	during the last few years, smt systems have evolved from the original word-based approach to phrase-based translation systems . 
111	1while the improvements to translation quality reported in koehn et al are minor, their evaluation metric may not have been especially sensitive to adding longer phrases. 
1	a common approach to phrase-based translation is to extract an inventory of phrase pairs from bitext , for example, in the phraseextract algorithm , a word alignment ÀÜam1 is generated over the bitext, and all word subsequences ei2i1 and fj2j1 are found that satisfy : ÀÜam1 : ÀÜaj ‚àà iff j ‚àà . 
151	for our experiments we used the following features, analogous to pharaoh's default feature set: ‚Ä¢ p and p , the latter of which is not found in the noisy-channel model, but has been previously found to be a helpful feature ; the lexical weights and , which estimate how well the words in Œ± translate the words in a phrase penalty exp , which allows the model to learn a preference for longer or shorter derivations, analogous to koehn's phrase penalty . 
95	we present results that compare our system against the baseline pharaoh implementation and mer training scripts provided for this workshop. 
138	in a phrase-based statistical translation , a bilingual text is decomposed as k phrase translation pairs , ,...: 
17	statistical phrase-based translation : here ‚Äúphrase-based‚Äù means ‚Äúsubsequence-based‚Äù, as there is no guarantee that the phrases learned by the model will have any relation to what we would think of as syntactic phrases. 
30	the proposed system is phrase-based, as in koehn et al , but uses an online perceptron training scheme to learn model parameters. 
130	in englishto-german, this result produces results very comparable to a phrasal smt system trained on the same data. 
84	see , , , , and . 
125	we view this as a particularly promising aspect of our work, given that phrase-based systems such as pharaoh perform better with higher recall alignments. 
73	in recent years, various phrase translation approaches have been shown to outperform word-to-word translation models . 
124	while the model and training regimen for œÜem differ from the model from marcu and wong , we achieved results similar to koehn et al : œÜem slightly underperformed œÜh. 
21	 used the following distortion model, which simply penalizes non-monotonic phrase alignments based on the word distance of successively translated source phrases with an appropriate value for the parameter.
42	the statistical components of our system are modeled on the statistical components of the phrase-based system pharaoh, described in koehn et al and koehn . 
117	for acquiring a pbm, we followed the approach described by koehn et al . 
47	currently, the most successful such systems employ so-called phrase-based methods that translate input text by translating sequences of words at a time phrase-based machine translation systems make use of a language model trained for the target language and a translation model trained from a parallel corpus. 
60	for maximum phrase length, we used 3 and 7 . 
148	a comparison of the two approaches can be found in koehn, och, and marcu . 
128	the standard method to overcome this problem to use the model in both directions and applying heuristic-based combination techniques to produce a refined alignment ‚Äîhenceforth referred to as ‚Äúra.‚Äù 
10	the lexical weight a27 a14a12a91 a29 a92a93a21 of the block a9 a72 a14a12a91 a19a86a92a93a21 is computed similarly to , but the lexical translation probability a27 a14a12a94 a29 a97a100a21 is derived from the block set itself rather than from a word alignment, resulting in a simplified training. 
108	to derive the joint counts c from which p and p are estimated, we use the phrase induction algorithm described in , with symmetrized word alignments generated using ibm model 2 . 
153	we obtain the word alignments using the method of koehn et al , which is based on that of och and ney . 
71	the first system is the pharaoh decoder provided by for the shared data task. 
96	the hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder . 
161	the block set is generated using a phrase-pair selection algorithm similar to , which includes some heuristic filtering to mal statement here. 
164	phrase-based translation models , which go beyond the original ibm translation models 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations. 
121	koehn et al showed that translation quality is very sensitive to how this table is extracted from the training data. 
53	in this paper, we implement the translation of modifiers with the phrase-based system of koehn et al . 
77	this includes the standard notion of phrase, popular with phrasedbased smt aswellassequencesofwordsthatcontaingaps . 
165	the baseline system we used for comparison was pharaoh , a freely available decoder for phrase-based translation models.
155	but koehn et al find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. 
158	above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content , or not at all . 
156	when we run a phrase-based system, pharaoh , on this sentence , we get the following phrases with translations: 1 [dipl. 
27	the process of phrase extraction is difficult to optimize in a non-discriminative setting: many heuristics have been proposed , but it is not obvious which one should be chosen for a given language pair. 
101	during decoding, the number of english phrases per fl phrase was limited to 100 and the distortion of phrases was limited by 4. based on the observations in , we also limited the phrase length to 3 for computational reasons. 
146	defining scms the work presented here was done in the context of phrase-based mt . 
59	is relevant to finite-state phrase-based models that use no parse trees , tree-tostring models that rely on one parse tree , and tree-to-tree models that rely on two parse trees . 
12	this section describes a phrase-based model for smt similar to the models presented in . 
5	 )charac
104	we proceeded with the following sequence of experiments: baseline: as a baseline system, we used a pure word-based approach and used pharaoh training tool , to train on the 22,500 sentences, and decoded using pharaoh to obtain translations for a test set of 50 sentences. 
122	the system follows the structure proposed in the documentation for the pharaoh decoder and uses many publicly available components . 
74	the huge increase in computational and storage cost of including longer phrases does not provide a significant improvement in quality as the probability of reappearance of larger phrases decreases. 
13	two block sets are derived for each of the training sets using a phrase-pair selection algorithm similar to . 
120	on the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in , alignment templates are used in , and the alignment template approach is re-framed into the so-called phrase based translation in . 
87	in this project we use the model described by koehn et al which extracts its phrase alignments from a corpus that has been word aligned. 
88	on smaller data sets the joint model shows performance comparable to the standard model, however the joint model does not reach the level of performance of the stan156 en-es es-en joint 3-gram, dl4 20.51 26.64 5-gram, dl6 26.34 27.17 + lex. 
92	we use the following features for our rules: ‚Ä¢ sourceand target-conditioned neg-log lexical weights as described in ‚Ä¢ neg-log relative frequencies: left-handside-conditioned, target-phrase-conditioned, source-phrase-conditioned ‚Ä¢ counters: n.o. 
81	the second one is heuristic and tries to use a word-aligned corpus . 
32	we used sentences of length 5-15 to facilitate comparisons with koehn et al and to enable rapid experimentation with various feature sets. 
15	our method for identifying paraphrases is an extension of recent work in phrase-based statistical machine translation . 
72	for further information on these parameter settings, confer . 
141	as an additional baseline, we compare against a phrasal smt decoder, pharaoh . 
136	second, phrase translation pairs are extracted from the word aligned corpus . 
22	figure 4: n-best phrase alignments phrase pairs are extracted using the standard phrase extraction method described in . 
55	in this paper, we use the phrase-based system of koehn et al to generate n-best translations for each of the modifiers, and we then use a discriminative reranking algorithm to choose between these modifiers. 
80	phrase-pairs are then extracted from the word alignments . 
64	more recently, phrase-based models have been proposed as a highly successful alternative to the ibm models. 
14	by introducing the hidden word alignment variable a, the following approximate optimization criterion can be applied for that purpose: e‚àó = argmaxe pr = argmaxe summationdisplay a pr ‚âà argmaxe,a pr exploiting the maximum entropy framework, the conditional distribution pr can be determined through suitable real valued functions hr ,r = 1...r, and takes the parametric form: pŒª ‚àù exp{ rsummationdisplay r=1 Œªrhr } the itc-irst system is based on a log-linear model which extends the original ibm model 4 to phrases . 
44	a phrase-based translation model can be estimated in two stages: first a parallel corpus is aligned at the word-level and then phrase pairs are extracted . 
62	different approaches have been suggested as using relative frequencies , calculate probabilities based on a statistical word-to-word dictionary or use a linear interpolation of these scores . 
35	the phrase-based model of koehn et al is an instance of this framework. 
99	2 the problem of coverage in smt statistical machine translation made considerable advances in translation quality with the introduction of phrase-based translation . 
18	recent work in statistical machine translation has sought to overcome the limitations of phrase-based models by making use of syntactic information. 
144	the last several years have seen phrasal statistical machine translation systems outperform word-based approaches by a wide margin . 
46	we collect bidirectional refined word alignment by growing the intersection of chinese-to-english alignments and english-to-chinese alignments with the neighboring unaligned word pairs which appear in the union similar to the final-andù approaches . 
75	it has the advantage of naturally capturing local reorderings and is shown to outperform word-based machine translation . 
85	nowadays, most of the state-of-the-art smt systems are based on bilingual phrases . 
16	one is distortion model which penalizes translations according to their jump distance instead of their content. 
139	the decoding process is very similar to those described in : it starts from an initial empty hypothesis. 
6	the basic model uses the following features, analogous to pharaoh‚Äôs default feature set: ‚Ä¢ p and p ‚Ä¢ the lexical weights pw and pw ;1 ‚Ä¢ a phrase penalty exp ; ‚Ä¢ a word penalty exp , where l is the number of terminals in Œ±. 
40	recent approaches to statistical machine translation piggyback on the central concepts of phrasebased smt and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process. 
58	for example, koehn et al reported that requiring constituents to be syntactically motivated does not lead to better constituent pairs, but only fewer constituent pairs, with loss of a good amount of valuable knowledge.
28	in the future, we plan to explore our discriminative framework on a full distortion model or even a hierarchical model . 
23	standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases . 
