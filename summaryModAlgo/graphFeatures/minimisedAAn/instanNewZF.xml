<paper>
<cited id="ZF0">
<title id=" W09-2507.xml">building an annotated textual inference corpus for motion and space </title>
<section> properties of an inference corpus.  </section>
<citcontext>
<prevsection>
<prevsent>this greatly benefits the annotation process: passages or sentences without verb or nominal ization that fits into the motion class can immediately be discarded.
</prevsent>
<prevsent>levins verb classes are easily accessible via verbnet (kipper et al, 1998), which provides additional syntactic and semantic information as well as mappings into wordnet (fell baum, 1998).
</prevsent>
</prevsection>
<citsent citstr=" C08-2024 ">
(muller, 1998) proposes qualitative theory of motion based on spatio-temporal primitives, while (pustejovsky and moszkowicz, 2008) <papid> C08-2024 </papid>shows an annotation structure for motion.</citsent>
<aftsection>
<nextsent>furthermore, representing motion requires the complete representation of spatial information, as motion is simply continuous function that transforms space.
</nextsent>
<nextsent>(hobbsand narayanan, 2002) discuss many of the properties for spatial representation, including dimensionality, frame of reference, regions, relative location, orientation, shape, and motion.
</nextsent>
<nextsent>it is therefore desirable for motion corpus to require inference over many different aspects of space as well as motion.
</nextsent>
<nextsent>table 2 shows the properties of motion incorporated in the inference system.in practice, these properties are far from uniformly distributed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1">
<title id=" W09-2412.xml">semeval2010 task 2 cross lingual lexical substitution </title>
<section> background: the english lexical.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, the system could be used to assist language learners, by providing them with the interpretation of the unknown words in text written in the language they are learning.
</prevsent>
<prevsent>last but not least, the output of cross-lingual lexical substitution system could be used as input to existing systems for cross-language information retrieval or automatic machine translation.
</prevsent>
</prevsection>
<citsent citstr=" W02-0816 ">
substitution taskthe english lexical substitution task (hereafter referred to as lexsub) was run at semeval-2007 following earlier ideas on method of testing wsd systems without predetermining the inventory (mccarthy, 2002).<papid> W02-0816 </papid></citsent>
<aftsection>
<nextsent>the issue of which inventory is appropriate for the task has been longstanding issue for debate, and while there is hope that coarse grained inventories will allow for increased system performance (ide and wilks, 2006) we do not yet know if these will make the distinctions that will most benefit practical systems (stokoe, 2005) <papid> H05-1051 </papid>or reflect cognitive processes (kilgarriff, 2006).</nextsent>
<nextsent>lexsubwas proposed as task which, while requiring contextual disambiguation, did not presuppose specific sense inventory.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF2">
<title id=" W09-2412.xml">semeval2010 task 2 cross lingual lexical substitution </title>
<section> background: the english lexical.  </section>
<citcontext>
<prevsection>
<prevsent>last but not least, the output of cross-lingual lexical substitution system could be used as input to existing systems for cross-language information retrieval or automatic machine translation.
</prevsent>
<prevsent>substitution taskthe english lexical substitution task (hereafter referred to as lexsub) was run at semeval-2007 following earlier ideas on method of testing wsd systems without predetermining the inventory (mccarthy, 2002).<papid> W02-0816 </papid></prevsent>
</prevsection>
<citsent citstr=" H05-1051 ">
the issue of which inventory is appropriate for the task has been longstanding issue for debate, and while there is hope that coarse grained inventories will allow for increased system performance (ide and wilks, 2006) we do not yet know if these will make the distinctions that will most benefit practical systems (stokoe, 2005) <papid> H05-1051 </papid>or reflect cognitive processes (kilgarriff, 2006).</citsent>
<aftsection>
<nextsent>lexsubwas proposed as task which, while requiring contextual disambiguation, did not presuppose specific sense inventory.
</nextsent>
<nextsent>in fact, it is quite possible to use alternative representations of meaning (schutze, 1998; pantel and lin, 2002).
</nextsent>
<nextsent>the motivation for substitution task was that it would reflect capabilities that might be useful for natural language processing tasks such as paraphrasing and textual entailment, while only focusing onone aspect of the problem and therefore not requiring complete system that might mask system capabilities at lexical level and at the same time make 76 participation in the task difficult for small research teams.
</nextsent>
<nextsent>the task required systems to produce substitute word for word in context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF3">
<title id=" W09-2412.xml">semeval2010 task 2 cross lingual lexical substitution </title>
<section> background: the english lexical.  </section>
<citcontext>
<prevsection>
<prevsent>the scoring was conducted using recall and precision measures using: ? the frequency distribution of responses from the annotators and ? the mode of the annotators (the most frequent response).
</prevsent>
<prevsent>the systems were scored using their best guess as well as an out-of-ten score which allowed up to 10 attempts.
</prevsent>
</prevsection>
<citsent citstr=" W07-2009 ">
1 the results are reported in mccarthy and navigli (2007) <papid> W07-2009 </papid>and in more detail in mccarthy and navigli (in press).</citsent>
<aftsection>
<nextsent>1the details are available at http://nlp.cs.swarthmore.edu/semeval/tasks/ task10/task10documentation.pdf.
</nextsent>
<nextsent>while there has been lot of discussion on the relevant sense distinctions for monolingual wsd systems, for machine translation applications there is consensus that the relevant sense distinctions are those that reflect different translations.
</nextsent>
<nextsent>one early and notable work was the senseval-2 japanese translation task (kurohashi, 2001) that obtained alternative translation records of typical usages of atest word, also referred to as translation memory.
</nextsent>
<nextsent>systems could either select the most appropriate translation memory record for each instance andwere scored against gold-standard set of annotations, or they could provide translation that was scored by translation experts after the results were submitted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF4">
<title id=" W09-2412.xml">semeval2010 task 2 cross lingual lexical substitution </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>systems could either select the most appropriate translation memory record for each instance andwere scored against gold-standard set of annotations, or they could provide translation that was scored by translation experts after the results were submitted.
</prevsent>
<prevsent>in contrast to this work, we propose to provide actual translations for target instances in advance, rather than predetermine translations using lexicographers or relyon post-hoc evaluation, which does not permit evaluation of new systems after the competition.
</prevsent>
</prevsection>
<citsent citstr=" W07-2010 ">
previous standalone wsd tasks based on parallel data have obtained distinct translations for senses as listed in dictionary (ng and chan, 2007).<papid> W07-2010 </papid></citsent>
<aftsection>
<nextsent>in this way fine-grained senses with the same translations can be lumped together, however this does not fully allow for the fact that some senses for the same words may have some translations in common but also others that are not.
</nextsent>
<nextsent>an example from resnik and yarowsky (2000) (table 4 in that paper) is the first two senses from wordnet for the noun interest: wordnet sense spanish translation monetary e.g. on loan interes, redito stake/share interes,participacion for wsd tasks, decision can be made to lump senses with such overlap, or split them using the distinctive translation and then use the distinctive translations as sense inventory.
</nextsent>
<nextsent>this sense inventory is then used to collect training from parallel data (ngand chan, 2007).<papid> W07-2010 </papid></nextsent>
<nextsent>we propose that it would be interesting to collect dataset where the overlap in translations for an instance can remain and that thi swill depend on the token instance rather than mapping to pre-defined sense inventory.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF7">
<title id=" W09-2412.xml">semeval2010 task 2 cross lingual lexical substitution </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>we intend to allow annotators to supply as many translations as they feel are equally valid.
</prevsent>
<prevsent>this will allow us to examine more subtle relationships between usages and to allow partial credit to systems which get close approximation to the annotatorstranslations.
</prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
unlike full blown machine translation task (carpuat and wu, 2007), <papid> D07-1007 </papid>annotators and systems will not be required to translate the whole context but just the target word.</citsent>
<aftsection>
<nextsent>task here we discuss our proposal for cross-linguallexical substitution task.
</nextsent>
<nextsent>the task will follow lex sub except that the annotations will be translations rather than paraphrases.given target word in context, the task is to provide several correct translations for that word in given language.
</nextsent>
<nextsent>we will use english as the source language and spanish as the target language.
</nextsent>
<nextsent>mul tiwords are part and parcel?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF8">
<title id=" W09-2606.xml">parenthetical constructions  an argument against modularity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>expressions, and individual grammatical levels are distributed in independent modules.organizing the grammar this way severely limits the flexibility of nlg systems.
</prevsent>
<prevsent>it has long been recognized in the literature that text fluency can be improved by modeling interactions between grammar modules.
</prevsent>
</prevsection>
<citsent citstr=" P03-1034 ">
the most commonly mentioned interactions are those among discourse/rhetorical relations and syntax (scott and souza, 1990;hovy, 1993; callaway, 2003), <papid> P03-1034 </papid>rhetorical relations, syntax and referring expressions (kibbleand power, 2004); <papid> J04-4001 </papid>and layout and referring expressions (n. bouayad-agha, 2001).</citsent>
<aftsection>
<nextsent>it is clear that in order to generate high quality, coherent discourse, generator needs access to grammar which is able to model the interdependent,context-sensitive behaviour of these separate linguistic phenomena.in this paper we draw parallel between grammar design and the design of natural language generation systems.
</nextsent>
<nextsent>we argue that in order to generate complex linguistic constructions, current nlg systems tend to have overly complicated architectures.
</nextsent>
<nextsent>to illustrate this point we show how surface realizer can take on tasks from other components when linguistic information from different grammar modules (and hence, system modules) is integrated.
</nextsent>
<nextsent>this simplifies system architecture by reducing the need for interaction between modules and enables the generator to produce more complex and coherent text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF9">
<title id=" W09-2606.xml">parenthetical constructions  an argument against modularity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>expressions, and individual grammatical levels are distributed in independent modules.organizing the grammar this way severely limits the flexibility of nlg systems.
</prevsent>
<prevsent>it has long been recognized in the literature that text fluency can be improved by modeling interactions between grammar modules.
</prevsent>
</prevsection>
<citsent citstr=" J04-4001 ">
the most commonly mentioned interactions are those among discourse/rhetorical relations and syntax (scott and souza, 1990;hovy, 1993; callaway, 2003), <papid> P03-1034 </papid>rhetorical relations, syntax and referring expressions (kibbleand power, 2004); <papid> J04-4001 </papid>and layout and referring expressions (n. bouayad-agha, 2001).</citsent>
<aftsection>
<nextsent>it is clear that in order to generate high quality, coherent discourse, generator needs access to grammar which is able to model the interdependent,context-sensitive behaviour of these separate linguistic phenomena.in this paper we draw parallel between grammar design and the design of natural language generation systems.
</nextsent>
<nextsent>we argue that in order to generate complex linguistic constructions, current nlg systems tend to have overly complicated architectures.
</nextsent>
<nextsent>to illustrate this point we show how surface realizer can take on tasks from other components when linguistic information from different grammar modules (and hence, system modules) is integrated.
</nextsent>
<nextsent>this simplifies system architecture by reducing the need for interaction between modules and enables the generator to produce more complex and coherent text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF10">
<title id=" W09-2606.xml">parenthetical constructions  an argument against modularity </title>
<section> the problem of generating.  </section>
<citcontext>
<prevsection>
<prevsent>we show that by incorporating the above kinds of linguistic information into the grammar of surface realizer we can improve the flexibility of the system (i.e., generate more paraphrases for thesame input) and improve the quality of the generated text without adding more modules to the system.
</prevsent>
<prevsent>2.1 syntactic constraints on.
</prevsent>
</prevsection>
<citsent citstr=" L08-1459 ">
pronominalizationto design grammar for parenthetical constructions, we have carried out corpus study on embedded rhetorical relations in the rst treebank (banik and lee, 2008).<papid> L08-1459 </papid></citsent>
<aftsection>
<nextsent>the corpus study has shown that the most numerous class of embedded subordinate clauses that occur in sentence medial position contain subject pronoun (as in 4a).
</nextsent>
<nextsent>this embedded subject pronoun in all cases referred back to the subject of the matrix clause,which always immediately preceded the subordinate clause.
</nextsent>
<nextsent>the pronoun can be either explicit (as in 4a) or implicit (as in the examples in 1).
</nextsent>
<nextsent>of the 119 sentence-medial subordinate clauses that we looked at in the study, 35 were of this type (what we call pseudo-relatives).1 this suggests that insentence-medial subordinate clauses (or sentence final ones immediately following the main clause object) the type of referring expression is solely determined by syntax, much like wh-pronoun in relative clauses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF11">
<title id=" W09-2606.xml">parenthetical constructions  an argument against modularity </title>
<section> an integrated discourse-syntax.  </section>
<citcontext>
<prevsection>
<prevsent>in modular systems this is typically achieved by two modules: text planning module which constructs text plan and surface realizer that converts the text plan into sentences.
</prevsent>
<prevsent>however, text planning and linguistic realization are not two independent processes and many linguistic decisions are in fact made by the text planner.
</prevsent>
</prevsection>
<citsent citstr=" P88-1022 ">
the interactions between text planning and linguistic realization in modular systems have been handled in several ways, including backtracking (appelt,1985), inter leaving the two components (mcdon ald, 1983) and restrictive planning (hovy, 1988).<papid> P88-1022 </papid>these approaches however make the system inflexible because all possible interactions between modules have to be anticipated by the system de signer.</citsent>
<aftsection>
<nextsent>another, more recent approach to tackle this problem is to use lexicalization not only for sentences but also for texts.
</nextsent>
<nextsent>the theoretical back ground for lexicalization on the discourse level has been laid down for tree adjoining grammar (joshi and schabes, 1997) by several researchers, including webber (2004), and danlos (2000).
</nextsent>
<nextsent>in particular, danlos (2000) shows that extending lexicalization to the discourse level makes it possible to completely integrate text planning and surface realization.
</nextsent>
<nextsent>48 we have designed tree adjoining grammar for parenthetical constructions following this latter approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF12">
<title id=" W09-2606.xml">parenthetical constructions  an argument against modularity </title>
<section> comparison.  </section>
<citcontext>
<prevsection>
<prevsent>when the discourse connective is combined with the embedded clause, these features are percolated to the referring expression in subject position, requiring it to be realized by pronoun.
</prevsent>
<prevsent>figure 4 illustrates the elementary trees and the derived tree for the embedded clause in (7).
</prevsent>
</prevsection>
<citsent citstr=" J03-2003 ">
as an experiment, we have implemented grammar fragment in the geni surface realizer (kow, 2007) and regenerated an example from the iconoclast generator (power et al, 2003).<papid> J03-2003 </papid>the example we used is represented by the following input semantics: h1: elixir(e) h2: fda(f) h3: elixir plus(p) h4: gestodene(g) h5: contain(e g) h6: ban(f e) h7: approve(f p) h8: concession(h6 h7) h9: cause(h5 h6) h10: contain(p o) h11: oestradiol(o) h12: cause(h10 h7) 50 h6:ban(f e) c    h s    h np rx [idx:e] vp    h v banned by np rx [idx:f] punct . h2:drug(e) np    h np?</citsent>
<aftsection>
<nextsent>[pron:no] text phrase   h , np   h det drug , (a) elementary trees for (6) c     h h       h h h np     h h np rx [ idx:e pron:no ] text phrase   h , np   h det drug , vp   h banned by rx [idx:f] punct .
</nextsent>
<nextsent>(b) derived tree for (6) figure 3: pronouns not allowed before an appositive h2:contain(e a) s[s pron:x]      h h np rx [ pron:x idx:e ] vp    h v contain np rx [idx:f] h2:cause(h7 h0) np     h h np?
</nextsent>
<nextsent>pred:h7 pron:no idx :e ? ?
</nextsent>
<nextsent>text clause    h conn since s?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF13">
<title id=" W09-2606.xml">parenthetical constructions  an argument against modularity </title>
<section> comparison.  </section>
<citcontext>
<prevsection>
<prevsent>pred:h7 pron:no idx :e ? ?
</prevsent>
<prevsent>text clause     h h conn since s[s pron:yes]     h h np rx [ pron:yes idx :e ] vp   h contain np rx [idx:f] (b) derived tree for (7) figure 4: obligatory pronouns in parenthetical subordinate clauses iconoclast is constraint-based system which integrates text planning, document planning and pronominal ization to generate all possible paraphrases forgiven input.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
it uses version of centering theory (grosz et al, 1995) <papid> J95-2003 </papid>adapted to natural language generation to decide when to pronominalize noun phrases in the generated text.iconoclast has an over generate and test approach, where all possible paraphrases are generated and the solutions are ranked according to setof soft constraints.</citsent>
<aftsection>
<nextsent>the system generated 172 solutions for the above input, of which (8) illustrates the top three: (8) since elixir contains gestodene it is banned by the fda.
</nextsent>
<nextsent>however, the fda approves elixir plus since elixir plus contains oestradiol.
</nextsent>
<nextsent>b elixir contains gestodene so it is banned by the fda.
</nextsent>
<nextsent>however, the fda approves elixir plus since elixir plus contains oestradiol.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF14">
<title id=" W09-2606.xml">parenthetical constructions  an argument against modularity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a common idea behind all these approaches is to build an integrated text understanding or generation system in which the same mechanisms are used for the sentence and discourse levels.
</prevsent>
<prevsent>dltag (webber, 2004) is an extension of ltag in which discourse syntax is projected by different types of discourse connectives.
</prevsent>
</prevsection>
<citsent citstr=" J03-4002 ">
in this approach discourse-level syntax is considered to be separate layer on top of sentence-level syntax and there are two kinds of discourse connectives: anaphoric and structural (webber et al, 2003).<papid> J03-4002 </papid></citsent>
<aftsection>
<nextsent>this analysis is not suitable for natural language generation systems which need to have an explicit representation for the arguments of discourse con nectives.g-tag (danlos, 2000) is another discourse level extension of tag where underspecified derivation trees?
</nextsent>
<nextsent>are created for conceptual input and grouped into lexical databases.
</nextsent>
<nextsent>a g-derivation tree specifies set of surface variants, one of which is produced by linearization of the g-derived tree.
</nextsent>
<nextsent>the other surface variants are created by post-processing module.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF15">
<title id=" W10-0105.xml">parallel active learning eliminating wait time with minimal staleness </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>empirically, the parallel al algorithm effectively has batch size ofone and large candidate set size but eliminates the time an annotator would have to wait for similarly parameterized batch scheme to select instances.
</prevsent>
<prevsent>the exact performance of our method on other tasks will depend on the relative ratios of time spent annotating, training,and scoring, but in general we expect our parameter less method to perform favorably compared to batch when accounting for wait time.
</prevsent>
</prevsection>
<citsent citstr=" W09-1903 ">
recent emphasis has been placed on evaluating the effectiveness of active learning (al) based on realistic cost estimates (haertel et al, 2008; settles et al, 2008; arora et al, 2009).<papid> W09-1903 </papid></citsent>
<aftsection>
<nextsent>however, to our knowledge, no previous work has included in thecost measure the amount of time that an expert annotator must wait for the active learner to provide instances.
</nextsent>
<nextsent>in fact, according to the standard approach to cost measurement, there is no reason not to use the theoretically optimal (w.r.t. model, training procedure, and utility function) (but intractable) approach (see haertel et al, 2008).
</nextsent>
<nextsent>in order to more fairly compare complex andtime-consuming (but presumably superior) selection algorithms with simpler (but presumably inferior) algorithms, we describe best-case?
</nextsent>
<nextsent>(minimum, from the standpoint of the payer) and worst case?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF16">
<title id=" W10-0105.xml">parallel active learning eliminating wait time with minimal staleness </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in sense, waiting time serves as natural penalty for expensive selectionalgorithms.
</prevsent>
<prevsent>therefore, conclusions about the usefulness of al selection algorithms should take both best-case and worst-case costs into consideration.
</prevsent>
</prevsection>
<citsent citstr=" D07-1051 ">
although it is current practice to measure only best-case costs, tomanek et al (2007) <papid> D07-1051 </papid>mention as desideratum for practical al algorithms the need forwhat they call fast selection time cycles, i.e., algorithms that minimize the amount of time annotators wait for instances.</citsent>
<aftsection>
<nextsent>they address this by employing the batch selection technique of engle son and dagan (1996).
</nextsent>
<nextsent>in fact, most al practitioners andre searchers implicitly acknowledge the importance of wait time by employing batch selection.
</nextsent>
<nextsent>however, batch selection is not perfect solution.
</nextsent>
<nextsent>first, using the trad tional implementation, good?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF17">
<title id=" W09-2808.xml">syntax driven sentence revision for broadcast news summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we address the problem of revising the lead sentence in broadcast news text to increase the amount of background information in the lead.
</prevsent>
<prevsent>this is one of the draft and revision approaches to summarization, which has received keen attention in the research community.
</prevsent>
</prevsection>
<citsent citstr=" I08-1016 ">
unlike many other methods that directly utilize noun phrase (np) coreference (nenkova 2008; <papid> I08-1016 </papid>mani et al 1999), <papid> P99-1072 </papid>we propose method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences.</citsent>
<aftsection>
<nextsent>we also show its effectiveness in revision experiment.
</nextsent>
<nextsent>as is well known, the extractive summary that has been extensively studied from the early days of summarization history (luhn, 1958) suffers from various drawbacks.
</nextsent>
<nextsent>these include the problems of break in cohesion in the summary text such as dangling anaphora and sudden shift in topic.
</nextsent>
<nextsent>to ameliorate these problems, the idea of revising the extracted sentences was proposed in single document summarization study.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF18">
<title id=" W09-2808.xml">syntax driven sentence revision for broadcast news summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we address the problem of revising the lead sentence in broadcast news text to increase the amount of background information in the lead.
</prevsent>
<prevsent>this is one of the draft and revision approaches to summarization, which has received keen attention in the research community.
</prevsent>
</prevsection>
<citsent citstr=" P99-1072 ">
unlike many other methods that directly utilize noun phrase (np) coreference (nenkova 2008; <papid> I08-1016 </papid>mani et al 1999), <papid> P99-1072 </papid>we propose method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences.</citsent>
<aftsection>
<nextsent>we also show its effectiveness in revision experiment.
</nextsent>
<nextsent>as is well known, the extractive summary that has been extensively studied from the early days of summarization history (luhn, 1958) suffers from various drawbacks.
</nextsent>
<nextsent>these include the problems of break in cohesion in the summary text such as dangling anaphora and sudden shift in topic.
</nextsent>
<nextsent>to ameliorate these problems, the idea of revising the extracted sentences was proposed in single document summarization study.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF19">
<title id=" W09-2808.xml">syntax driven sentence revision for broadcast news summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these include the problems of break in cohesion in the summary text such as dangling anaphora and sudden shift in topic.
</prevsent>
<prevsent>to ameliorate these problems, the idea of revising the extracted sentences was proposed in single document summarization study.
</prevsent>
</prevsection>
<citsent citstr=" A00-2024 ">
jing and mckeown (1999), jing and mckeown (2000) <papid> A00-2024 </papid>found that human summarization can be traced back to six cut-and paste operations of text and proposed revision method consisting of sentence reduction and combination modules with sentence extraction part.</citsent>
<aftsection>
<nextsent>mani and colleagues (1999) proposed summarization system based on draft and revision?
</nextsent>
<nextsent>together with sentence extraction.
</nextsent>
<nextsent>the revision part is achieved with the sentence aggregation and smoothing modules.
</nextsent>
<nextsent>the cohesion break problem becomes particularly conspicuous in multi-document summarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF23">
<title id=" W09-2808.xml">syntax driven sentence revision for broadcast news summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nenkova (2008) <papid> I08-1016 </papid>proposed revision idea that utilizes noun coreference with linguistic quality improvements in mind.</prevsent>
<prevsent>other than the break in cohesion, multi document summarization faces the problem of information overlap particularly when the document set consists of similar sentences.</prevsent>
</prevsection>
<citsent citstr=" J05-3002 ">
barzilay and mckeown (2005) <papid> J05-3002 </papid>proposed an idea called sentence fusion that integrates information in overlapping sentences to produce nonoverlapping summary sentence.</citsent>
<aftsection>
<nextsent>their algorithm firstly analyzes the sentences to obtain the dependency trees and sets basis tree by finding the centro id of the dependency trees.
</nextsent>
<nextsent>it next augments the basis tree with the sub-trees in other sentences and finally prunes the predefined constituents.
</nextsent>
<nextsent>their algorithm was further modified and applied to the german biographies by filippova and strube (2008).<papid> D08-1019 </papid></nextsent>
<nextsent>like the work of jing and mckeown (2000) <papid> A00-2024 </papid>and mani et al (1999), <papid> P99-1072 </papid>our work was inspired by the summarization method used by human abstractors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF24">
<title id=" W09-2808.xml">syntax driven sentence revision for broadcast news summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>their algorithm firstly analyzes the sentences to obtain the dependency trees and sets basis tree by finding the centro id of the dependency trees.
</prevsent>
<prevsent>it next augments the basis tree with the sub-trees in other sentences and finally prunes the predefined constituents.
</prevsent>
</prevsection>
<citsent citstr=" D08-1019 ">
their algorithm was further modified and applied to the german biographies by filippova and strube (2008).<papid> D08-1019 </papid></citsent>
<aftsection>
<nextsent>like the work of jing and mckeown (2000) <papid> A00-2024 </papid>and mani et al (1999), <papid> P99-1072 </papid>our work was inspired by the summarization method used by human abstractors.</nextsent>
<nextsent>actually, our abs tractors first extract important sentences, which is called lead identification, and then revise them, which is referred to as phrase elaboration or specification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF33">
<title id=" W09-2808.xml">syntax driven sentence revision for broadcast news summarization </title>
<section> revision experiments.  </section>
<citcontext>
<prevsection>
<prevsent>with these purposes in mind, we designed our experiment as follows.
</prevsent>
<prevsent>data total of 257 articles from news programs broadcast on 20 jan., 20 apr., and 20 july in 2004 were tagged with lead, body, and supplement tags by native japanese evaluator.
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
the articles were morphologically analyzed by me cab (kudo et al, 2003) and syntactically parsed by cabocha (kudo and matsumoto, 2002).<papid> W02-2016 </papid></citsent>
<aftsection>
<nextsent>evaluator and evaluation detail we prepared an evaluation interface that presents lead with one revision point (insertion or sub stitution) that was obtained using the body and supplemental sentences to an evaluator.
</nextsent>
<nextsent>a japanese native speaker evaluated the results one by one with the above interface.
</nextsent>
<nextsent>we planned linguistic evaluation like duc2005 (hoa trang, 2005).
</nextsent>
<nextsent>since their five-type evaluation is intended for multi-document summarization, whereas our task is single-document summarization, and we are interested in evaluating our questions mentioned above, we carried out the evaluation as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF34">
<title id=" W09-2103.xml">inferring tutorial dialogue structure with hidden markov modeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>encouraging results have emerged from using general model of the task structure to inform automatic dialogue act tagging as well as subtask segmentation.
</prevsent>
<prevsent>our current work examines modeling technique that does not require priori knowledge of the task structure: specifically, we propose to use hidden markov models (hmms) (rabiner 1989) to capture the structure of tutorial dialogue implicit within sequences of tagged dialogue acts.
</prevsent>
</prevsection>
<citsent citstr=" N04-1015 ">
such probablistic inference of discourse structure has been used in recent work with hmms for topic identification (barzilay &amp; lee 2004) <papid> N04-1015 </papid>and related graphical models for segmenting multi-party spoken discourse (purver et al 2006).<papid> P06-1003 </papid></citsent>
<aftsection>
<nextsent>analogously, our current work focuses on identifying dia logic structures that emerge during tutorial dialogue.
</nextsent>
<nextsent>our approach is based on the premise that at any given point in the tutorial dialogue, the collaborative interaction is in?
</nextsent>
<nextsent>a dialogue mode (cade et al 2008) that characterizes the nature of the exchanges between tutor and student; these modes correspond to the hidden states in the hmm.
</nextsent>
<nextsent>results to date suggest that meaningful descriptive models of tutorial dialogue can be generated by this simple stochastic modeling technique.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF35">
<title id=" W09-2103.xml">inferring tutorial dialogue structure with hidden markov modeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>encouraging results have emerged from using general model of the task structure to inform automatic dialogue act tagging as well as subtask segmentation.
</prevsent>
<prevsent>our current work examines modeling technique that does not require priori knowledge of the task structure: specifically, we propose to use hidden markov models (hmms) (rabiner 1989) to capture the structure of tutorial dialogue implicit within sequences of tagged dialogue acts.
</prevsent>
</prevsection>
<citsent citstr=" P06-1003 ">
such probablistic inference of discourse structure has been used in recent work with hmms for topic identification (barzilay &amp; lee 2004) <papid> N04-1015 </papid>and related graphical models for segmenting multi-party spoken discourse (purver et al 2006).<papid> P06-1003 </papid></citsent>
<aftsection>
<nextsent>analogously, our current work focuses on identifying dia logic structures that emerge during tutorial dialogue.
</nextsent>
<nextsent>our approach is based on the premise that at any given point in the tutorial dialogue, the collaborative interaction is in?
</nextsent>
<nextsent>a dialogue mode (cade et al 2008) that characterizes the nature of the exchanges between tutor and student; these modes correspond to the hidden states in the hmm.
</nextsent>
<nextsent>results to date suggest that meaningful descriptive models of tutorial dialogue can be generated by this simple stochastic modeling technique.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF36">
<title id=" W10-0722.xml">non expert evaluation of summarization systems is risky </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>assuming single judge evaluates all summaries for topic (more redundancy would be better), we get rough time estimate: 17.5 hours to evaluate two systems.
</prevsent>
<prevsent>thus it is of great interest to find ways of speeding up evaluation while minimizing subjectivity.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
amazons mechanical turk (mturk) system has been used for variety of labeling and annotation tasks (snow et al, 2008), <papid> D08-1027 </papid>but such crowd-sourcing has not been tested for summarization.</citsent>
<aftsection>
<nextsent>we describe an experiment to test whether mturk is able to reproduce system-level rankings that match expert opinion.
</nextsent>
<nextsent>unlike the results of other crowd-sourcing annotations for natural language tasks, we find that non-expert judges are unable to provide expert-like scores and tend to disagree significantly with each other.this paper is organized as follows: section 2 introduces the particular summarization task and data we use in our experiments; section 3 describes the design of our human intelligence task (hit).
</nextsent>
<nextsent>section 4 shows experimental results and gives some analysis.
</nextsent>
<nextsent>section 5 reviews our main findings and provides suggestions for researchers wishing to conduct their own crowd-sourcing evaluations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF37">
<title id=" W10-0722.xml">non expert evaluation of summarization systems is risky </title>
<section> tac summarization task.  </section>
<citcontext>
<prevsection>
<prevsent>one way to dramatically speed up evaluation is touse the experts?
</prevsent>
<prevsent>reference summaries as gold standard, leaving the source documents out entirely.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
this is the idea behind automatic evaluation with rouge (lin, 2004), <papid> W04-1013 </papid>which measures ngram overlap with the references, and assisted evaluation with pyramid (nenkova and passonneau, 2004), <papid> N04-1019 </papid>which measures overlap of facts or semantic content units?</citsent>
<aftsection>
<nextsent>with the references.
</nextsent>
<nextsent>the same idea has also been employed in various manual evaluations, for example by haghighi and vanderwende (2009), <papid> N09-1041 </papid>to directly compare the summaries of two different systems.</nextsent>
<nextsent>the potential bias introduced by such abbreviated evaluation has not been explored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF38">
<title id=" W10-0722.xml">non expert evaluation of summarization systems is risky </title>
<section> tac summarization task.  </section>
<citcontext>
<prevsection>
<prevsent>one way to dramatically speed up evaluation is touse the experts?
</prevsent>
<prevsent>reference summaries as gold standard, leaving the source documents out entirely.
</prevsent>
</prevsection>
<citsent citstr=" N04-1019 ">
this is the idea behind automatic evaluation with rouge (lin, 2004), <papid> W04-1013 </papid>which measures ngram overlap with the references, and assisted evaluation with pyramid (nenkova and passonneau, 2004), <papid> N04-1019 </papid>which measures overlap of facts or semantic content units?</citsent>
<aftsection>
<nextsent>with the references.
</nextsent>
<nextsent>the same idea has also been employed in various manual evaluations, for example by haghighi and vanderwende (2009), <papid> N09-1041 </papid>to directly compare the summaries of two different systems.</nextsent>
<nextsent>the potential bias introduced by such abbreviated evaluation has not been explored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF39">
<title id=" W10-0722.xml">non expert evaluation of summarization systems is risky </title>
<section> tac summarization task.  </section>
<citcontext>
<prevsection>
<prevsent>this is the idea behind automatic evaluation with rouge (lin, 2004), <papid> W04-1013 </papid>which measures ngram overlap with the references, and assisted evaluation with pyramid (nenkova and passonneau, 2004), <papid> N04-1019 </papid>which measures overlap of facts or semantic content units?</prevsent>
<prevsent>with the references.</prevsent>
</prevsection>
<citsent citstr=" N09-1041 ">
the same idea has also been employed in various manual evaluations, for example by haghighi and vanderwende (2009), <papid> N09-1041 </papid>to directly compare the summaries of two different systems.</citsent>
<aftsection>
<nextsent>the potential bias introduced by such abbreviated evaluation has not been explored.
</nextsent>
<nextsent>the overall structure of the hit we designed for summary evaluation is as follows: the worker is asked to read the topic and description, and then two reference summaries (there is no mention of the source documents).
</nextsent>
<nextsent>the candidate summary appears next, followed by instructions to provide scores between 1 (very poor) and 10 (very good) in each cat egory1.
</nextsent>
<nextsent>mouse-over on the category names provides 1besides overall quality and linguistic quality, we include information content, to encourage judges to distinguish be extra details, copied with slight modifications from dang (2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF40">
<title id=" W09-2603.xml">mining of parsed data to derive deverbal argument structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>large corpora of text annotated with part of speech information arealso available (such as the british national cor pus).
</prevsent>
<prevsent>however, it is much harder to find widely available, large corpora annotated for syntactic or semantic structure.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>has until recently been the only such corpus, covering 4.5m words in single genre of financial reporting.</citsent>
<aftsection>
<nextsent>at the same time, the accuracy and speed of syntactic parsers has been improving greatly, so that in recent years it has become possible to automatically create parsed corpora of reasonable quality, using much larger amounts of text with greater genre variation.
</nextsent>
<nextsent>for many nlp tasks, having more training data greatly improves the quality of the resulting models (banko and brill, 2001), <papid> P01-1005 </papid>even if the training data are not perfect.</nextsent>
<nextsent>we have access to the entire english-language text of wikipedia (about 2m pages) that was parsed using the xle parser (riezler et al, 2002),<papid> P02-1035 </papid>as well as an architecture for distributed data mining within this corpus, called oceanography (waterman, 2009).<papid> W09-1510 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF41">
<title id=" W09-2603.xml">mining of parsed data to derive deverbal argument structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>has until recently been the only such corpus, covering 4.5m words in single genre of financial reporting.</prevsent>
<prevsent>at the same time, the accuracy and speed of syntactic parsers has been improving greatly, so that in recent years it has become possible to automatically create parsed corpora of reasonable quality, using much larger amounts of text with greater genre variation.</prevsent>
</prevsection>
<citsent citstr=" P01-1005 ">
for many nlp tasks, having more training data greatly improves the quality of the resulting models (banko and brill, 2001), <papid> P01-1005 </papid>even if the training data are not perfect.</citsent>
<aftsection>
<nextsent>we have access to the entire english-language text of wikipedia (about 2m pages) that was parsed using the xle parser (riezler et al, 2002),<papid> P02-1035 </papid>as well as an architecture for distributed data mining within this corpus, called oceanography (waterman, 2009).<papid> W09-1510 </papid></nextsent>
<nextsent>using the parsed corpus, we extract large volume of dependency relations and derive lexical models that significantly improvea rule-based system for determining the underlying argument structure of deverbal noun construc tions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF42">
<title id=" W09-2603.xml">mining of parsed data to derive deverbal argument structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at the same time, the accuracy and speed of syntactic parsers has been improving greatly, so that in recent years it has become possible to automatically create parsed corpora of reasonable quality, using much larger amounts of text with greater genre variation.
</prevsent>
<prevsent>for many nlp tasks, having more training data greatly improves the quality of the resulting models (banko and brill, 2001), <papid> P01-1005 </papid>even if the training data are not perfect.</prevsent>
</prevsection>
<citsent citstr=" P02-1035 ">
we have access to the entire english-language text of wikipedia (about 2m pages) that was parsed using the xle parser (riezler et al, 2002),<papid> P02-1035 </papid>as well as an architecture for distributed data mining within this corpus, called oceanography (waterman, 2009).<papid> W09-1510 </papid></citsent>
<aftsection>
<nextsent>using the parsed corpus, we extract large volume of dependency relations and derive lexical models that significantly improvea rule-based system for determining the underlying argument structure of deverbal noun constructions.
</nextsent>
<nextsent>deverbal nouns, or nominalizations, are nouns that designate some aspect of the event referred to by the verb from which they are morphologically derived (quirk et al, 1985).
</nextsent>
<nextsent>for example, the noun destruction refers to the action described by the verb destroy, and destroyer may refer to the agent of that event.
</nextsent>
<nextsent>deverbal nouns are very common in english texts: by one count, about half of all sentences in written text contain at least one deverbal noun (gurevich et al, 2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF43">
<title id=" W09-2603.xml">mining of parsed data to derive deverbal argument structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at the same time, the accuracy and speed of syntactic parsers has been improving greatly, so that in recent years it has become possible to automatically create parsed corpora of reasonable quality, using much larger amounts of text with greater genre variation.
</prevsent>
<prevsent>for many nlp tasks, having more training data greatly improves the quality of the resulting models (banko and brill, 2001), <papid> P01-1005 </papid>even if the training data are not perfect.</prevsent>
</prevsection>
<citsent citstr=" W09-1510 ">
we have access to the entire english-language text of wikipedia (about 2m pages) that was parsed using the xle parser (riezler et al, 2002),<papid> P02-1035 </papid>as well as an architecture for distributed data mining within this corpus, called oceanography (waterman, 2009).<papid> W09-1510 </papid></citsent>
<aftsection>
<nextsent>using the parsed corpus, we extract large volume of dependency relations and derive lexical models that significantly improvea rule-based system for determining the underlying argument structure of deverbal noun constructions.
</nextsent>
<nextsent>deverbal nouns, or nominalizations, are nouns that designate some aspect of the event referred to by the verb from which they are morphologically derived (quirk et al, 1985).
</nextsent>
<nextsent>for example, the noun destruction refers to the action described by the verb destroy, and destroyer may refer to the agent of that event.
</nextsent>
<nextsent>deverbal nouns are very common in english texts: by one count, about half of all sentences in written text contain at least one deverbal noun (gurevich et al, 2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF45">
<title id=" W09-2603.xml">mining of parsed data to derive deverbal argument structure </title>
<section> deverbal argument mapping.  </section>
<citcontext>
<prevsection>
<prevsent>the lexical preferences are derived by comparing argument preferences of verbs with those of relateddeverbal nouns, derived from large parsed corpus using oceanography.
</prevsent>
<prevsent>2.1 current deverbal mapping system.
</prevsent>
</prevsection>
<citsent citstr=" W04-2705 ">
we have list of approximately 4000 deverbalnoun / verb pairs, constructed from combination of wordnets derivational links (fellbaum,1998), nomlex (macleod et al, 1998), nomlexplus (meyers et al, 2004<papid> W04-2705 </papid>b) and some independent curation.</citsent>
<aftsection>
<nextsent>in the current system implementation, we attempt to map deverbal nouns onto corresponding verbs using small set of heuristics described in (gurevich et al, 2008).
</nextsent>
<nextsent>we distinguish between event nouns like destruction, agentive nouns like destroyer, and patient-like nouns like employee.
</nextsent>
<nextsent>if deverbal noun maps onto transitive verb and has only one argument, the heuristics are asfollows.
</nextsent>
<nextsent>arguments of agentive nouns become objects while the nouns themselves become subjects,so the ships destroyer maps to subj(destroy, destroyer); obj(destroy, ship).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF50">
<title id=" W09-2603.xml">mining of parsed data to derive deverbal argument structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it is unclear to what extent better information about animacy would have helped.
</prevsent>
<prevsent>one of the earliest computational attempts to derive argument structures for deverbal nouns is(hull and gomez, 1996), with hand-crafted mapping rules for small set of individual nouns, exemplifying highly precise but not easily scalable method.
</prevsent>
</prevsection>
<citsent citstr=" N04-4036 ">
in recent years, nombank (meyers et al,2004<papid> W04-2705 </papid>a) has provided set of about 200,000 manually annotated instances of nominalizations with arguments, giving rise to supervised machine learned approaches such as (pradhan et al, 2004) <papid> N04-4036 </papid>and (liu and ng, 2007), <papid> P07-1027 </papid>which perform fairly wellin the overall task of classifying deverbal arguments.</citsent>
<aftsection>
<nextsent>however, no evaluation results are provided for specific, problematic classes of nominal arguments such as possessives; it is likely that the amount of annotations in nombank is insufficient to reliably map such cases onto verbal arguments.
</nextsent>
<nextsent>(pado?
</nextsent>
<nextsent>et al, 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in slr tasks (cf.
</nextsent>
<nextsent>(gildea and jurafsky, 2000)) <papid> P00-1065 </papid>rather than syntactic roles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF51">
<title id=" W09-2603.xml">mining of parsed data to derive deverbal argument structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it is unclear to what extent better information about animacy would have helped.
</prevsent>
<prevsent>one of the earliest computational attempts to derive argument structures for deverbal nouns is(hull and gomez, 1996), with hand-crafted mapping rules for small set of individual nouns, exemplifying highly precise but not easily scalable method.
</prevsent>
</prevsection>
<citsent citstr=" P07-1027 ">
in recent years, nombank (meyers et al,2004<papid> W04-2705 </papid>a) has provided set of about 200,000 manually annotated instances of nominalizations with arguments, giving rise to supervised machine learned approaches such as (pradhan et al, 2004) <papid> N04-4036 </papid>and (liu and ng, 2007), <papid> P07-1027 </papid>which perform fairly wellin the overall task of classifying deverbal arguments.</citsent>
<aftsection>
<nextsent>however, no evaluation results are provided for specific, problematic classes of nominal arguments such as possessives; it is likely that the amount of annotations in nombank is insufficient to reliably map such cases onto verbal arguments.
</nextsent>
<nextsent>(pado?
</nextsent>
<nextsent>et al, 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in slr tasks (cf.
</nextsent>
<nextsent>(gildea and jurafsky, 2000)) <papid> P00-1065 </papid>rather than syntactic roles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF52">
<title id=" W09-2603.xml">mining of parsed data to derive deverbal argument structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(pado?
</prevsent>
<prevsent>et al, 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in slr tasks (cf.
</prevsent>
</prevsection>
<citsent citstr=" P00-1065 ">
(gildea and jurafsky, 2000)) <papid> P00-1065 </papid>rather than syntactic roles.</citsent>
<aftsection>
<nextsent>a combination of our much larger training set and the sophisticated probabilistic methods used by pado?
</nextsent>
<nextsent>et al would most likely improve performance for both syntactic and semantic roles labelling tasks.
</nextsent>
<nextsent>we have demonstrated that large amounts of lexical data derived from an unsupervised parsed corpus improve role assignment for deverbal nouns.the improvements are significant even with relatively small training set, relying on parses that have not been hand-corrected, using very simple prediction model.
</nextsent>
<nextsent>larger amounts of extracted data improve performance even more.there is clearly still headroom for improvement in this method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF53">
<title id=" W09-2603.xml">mining of parsed data to derive deverbal argument structure </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>class would cover between 15% and 20% of all the data.
</prevsent>
<prevsent>this third possibility becomes even more important for other deverbal arguments.
</prevsent>
</prevsection>
<citsent citstr=" J02-3004 ">
for example, if the deverbalnoun has pre nominal modifier (as in city destruc tion), in third of the cases the underlying relation is neither the subject nor the object (lapata, 2002).<papid> J02-3004 </papid></citsent>
<aftsection>
<nextsent>and, of course, the methodology of extracting lexical preferences based on large parsed corpora can be applied to many other nl tasks not related to deverbal nouns.
</nextsent>
<nextsent>acknowledgments we gratefully acknowledge the helpful advice and comments of our colleagues tracy holloway king and dick crouch, as well as the three anonymous reviewers.
</nextsent>
<nextsent>26
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF54">
<title id=" W09-2605.xml">construction of a german hpsg grammar from a detailed treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1
</prevsent>
<prevsent>previous studies have shown that treebanks can be helpful when constructing grammars.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
themost well-known example is pcfg-based statistical parsing (charniak and johnson, 2005), <papid> P05-1022 </papid>where pcfg is induced from, for instance, the penn treebank.</citsent>
<aftsection>
<nextsent>the underlying statistical technique shave been refined in the last decade, and previous work indicates that the labelled f-score of this method converges to around 91%.
</nextsent>
<nextsent>an alternative to pcfgs, with more linguistic relevance, is formed by deeper formalisms, suchas tag (joshi and schabes, 1997), ccg (steed man, 1996), lfg (kaplan and bresnan, 1995) and hpsg (pollard and sag, 1994).
</nextsent>
<nextsent>for lfg (butt et al , 2002) <papid> W02-1503 </papid>and hpsg (flickinger, 2000; muller, 2002), large hand-written grammars have been developed.</nextsent>
<nextsent>in the case of hpsg, the grammar writers found the small number of principles too restrictive, and created more rules (approxi mately 50 to 300) to accommodate for phenomena 1the research reported in this paper has been carried outwith financial support from the deutsche forschungsgemein schaft and the german excellence cluster of multimodal computing &amp; interaction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF55">
<title id=" W09-2605.xml">construction of a german hpsg grammar from a detailed treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the underlying statistical technique shave been refined in the last decade, and previous work indicates that the labelled f-score of this method converges to around 91%.
</prevsent>
<prevsent>an alternative to pcfgs, with more linguistic relevance, is formed by deeper formalisms, suchas tag (joshi and schabes, 1997), ccg (steed man, 1996), lfg (kaplan and bresnan, 1995) and hpsg (pollard and sag, 1994).
</prevsent>
</prevsection>
<citsent citstr=" W02-1503 ">
for lfg (butt et al , 2002) <papid> W02-1503 </papid>and hpsg (flickinger, 2000; muller, 2002), large hand-written grammars have been developed.</citsent>
<aftsection>
<nextsent>in the case of hpsg, the grammar writers found the small number of principles too restrictive, and created more rules (approxi mately 50 to 300) to accommodate for phenomena 1the research reported in this paper has been carried outwith financial support from the deutsche forschungsgemein schaft and the german excellence cluster of multimodal computing &amp; interaction.
</nextsent>
<nextsent>that vanilla hpsg cannot describe correctly.
</nextsent>
<nextsent>the increased linguistic precise ness comes at cost,though: such grammars have lower out-of-the box coverage, i.e. they will not give an analysis on certain portion of the corpus.
</nextsent>
<nextsent>experiments have been conducted, where lexicalised grammar is learnt from treebanks, methodology for which we coin the name deep grammar extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF56">
<title id=" W09-2605.xml">construction of a german hpsg grammar from a detailed treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(2004) tailor their core grammar for optimal use with the penn treebank and the english language, for example by adding new schema for relative clauses.
</prevsent>
<prevsent>hockenmaier and steedman (2002), miyao et al .
</prevsent>
</prevsection>
<citsent citstr=" P04-1041 ">
(2004) and cahill et al  (2004) <papid> P04-1041 </papid>show fairly good results on the penn treebank (for ccg, hpsg andlfg, respectively): these parsers achieve accuracies on predicate-argument relations between 80%and 87%, which show the feasibility and scala bility of this approach.</citsent>
<aftsection>
<nextsent>however, while this is simple method for highly configurational language like english, it is more difficult to extend to languages with more complex morphology or with word orders that display more freedom.
</nextsent>
<nextsent>hockenmaier (2006) <papid> P06-1064 </papid>is the only study known to the authors that applies this method to german, language that displays these properties.</nextsent>
<nextsent>this article reports on experiments where the advantages of hand-written and derived grammars 37are combined.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF57">
<title id=" W09-2605.xml">construction of a german hpsg grammar from a detailed treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(2004) and cahill et al  (2004) <papid> P04-1041 </papid>show fairly good results on the penn treebank (for ccg, hpsg andlfg, respectively): these parsers achieve accuracies on predicate-argument relations between 80%and 87%, which show the feasibility and scala bility of this approach.</prevsent>
<prevsent>however, while this is simple method for highly configurational language like english, it is more difficult to extend to languages with more complex morphology or with word orders that display more freedom.</prevsent>
</prevsection>
<citsent citstr=" P06-1064 ">
hockenmaier (2006) <papid> P06-1064 </papid>is the only study known to the authors that applies this method to german, language that displays these properties.</citsent>
<aftsection>
<nextsent>this article reports on experiments where the advantages of hand-written and derived grammars 37are combined.
</nextsent>
<nextsent>compared to previous deep grammar extraction approaches, more sophisticated core grammar (in the framework of hpsg) is created.
</nextsent>
<nextsent>also, more detailed syntactic features are learnt from the resource treebank, which leads toa more precise lexicon.
</nextsent>
<nextsent>parsing results are compared with gg (german grammar), previously hand-written german hpsg grammar (muller, 2002; crysmann, 2003; crysmann, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF59">
<title id=" W09-2605.xml">construction of a german hpsg grammar from a detailed treebank </title>
<section> core grammar.  </section>
<citcontext>
<prevsection>
<prevsent>can be infixed as well: er versucht auszuschlafen?
</prevsent>
<prevsent>(he tries to sleep in?).these characteristics make german comparatively complex language to parse with cfgs:more variants of the same lemma have to be mem orised, and the expansion of production rules willbe more diverse, with less peaked statistical distribution.
</prevsent>
</prevsection>
<citsent citstr=" P03-1013 ">
efforts have been made to adapt existing cfg models to german (dubey and keller, 2003),<papid> P03-1013 </papid>but the results still dont compare to state-of-the art parsing of english.</citsent>
<aftsection>
<nextsent>2.3 structure of the core grammar.
</nextsent>
<nextsent>the grammar uses the main tenets from head driven phrase structure grammar (pollard and sag, 1994).
</nextsent>
<nextsent>however, different from earlier deep grammar extraction studies, more sophisticated structures are added.
</nextsent>
<nextsent>muller (2002) proposes new schema (head-cluster) to account for verb clusters in the right bracket, which includes the possibility to merge subcategorisation frames of e.g. object-control verbs and its dependent verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF67">
<title id=" W09-2422.xml">error analysis of the tempe val temporal relation identification task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we provide an analysis of the tempeval07 competition results,identifying aspects of the tasks which presented the systems with particular challenges and those that were accomplished with relative ease.
</prevsent>
<prevsent>the automatic temporal interpretation of text haslong been an important area computational linguistics research (bennett and partee, 1972; kamp and reyle, 1993).
</prevsent>
</prevsection>
<citsent citstr=" P06-1095 ">
in recent years, with the advent of the timeml markup language (pustejovsky et al, 2003) and the creation of the time bank resource (pustejovsky et al, 2003) interest has focussed on the application of variety of automatic techniques to this task (boguraev and ando, 2005; mani et al, 2006; <papid> P06-1095 </papid>bramsen et al, 2006; <papid> W06-1623 </papid>chambers et al, 2007; <papid> P07-2044 </papid>lee and katz, 2008).</citsent>
<aftsection>
<nextsent>the task of identifying the events and times described in text and classifying the relations that hold among them has proven to be difficult, however, with reported results for relation classification tasks ranging in f-score from 0.52 to 0.60.
</nextsent>
<nextsent>variation in the specifics has made comparison among research methods difficult, however.
</nextsent>
<nextsent>a first attempt to standardize this task was the 2007 tem peval competition(verhagen et al, 2007).<papid> W07-2014 </papid></nextsent>
<nextsent>this competition provided standardized training an devaluation scheme for automatic temporal interpretation systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF68">
<title id=" W09-2422.xml">error analysis of the tempe val temporal relation identification task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we provide an analysis of the tempeval07 competition results,identifying aspects of the tasks which presented the systems with particular challenges and those that were accomplished with relative ease.
</prevsent>
<prevsent>the automatic temporal interpretation of text haslong been an important area computational linguistics research (bennett and partee, 1972; kamp and reyle, 1993).
</prevsent>
</prevsection>
<citsent citstr=" W06-1623 ">
in recent years, with the advent of the timeml markup language (pustejovsky et al, 2003) and the creation of the time bank resource (pustejovsky et al, 2003) interest has focussed on the application of variety of automatic techniques to this task (boguraev and ando, 2005; mani et al, 2006; <papid> P06-1095 </papid>bramsen et al, 2006; <papid> W06-1623 </papid>chambers et al, 2007; <papid> P07-2044 </papid>lee and katz, 2008).</citsent>
<aftsection>
<nextsent>the task of identifying the events and times described in text and classifying the relations that hold among them has proven to be difficult, however, with reported results for relation classification tasks ranging in f-score from 0.52 to 0.60.
</nextsent>
<nextsent>variation in the specifics has made comparison among research methods difficult, however.
</nextsent>
<nextsent>a first attempt to standardize this task was the 2007 tem peval competition(verhagen et al, 2007).<papid> W07-2014 </papid></nextsent>
<nextsent>this competition provided standardized training an devaluation scheme for automatic temporal interpretation systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF69">
<title id=" W09-2422.xml">error analysis of the tempe val temporal relation identification task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we provide an analysis of the tempeval07 competition results,identifying aspects of the tasks which presented the systems with particular challenges and those that were accomplished with relative ease.
</prevsent>
<prevsent>the automatic temporal interpretation of text haslong been an important area computational linguistics research (bennett and partee, 1972; kamp and reyle, 1993).
</prevsent>
</prevsection>
<citsent citstr=" P07-2044 ">
in recent years, with the advent of the timeml markup language (pustejovsky et al, 2003) and the creation of the time bank resource (pustejovsky et al, 2003) interest has focussed on the application of variety of automatic techniques to this task (boguraev and ando, 2005; mani et al, 2006; <papid> P06-1095 </papid>bramsen et al, 2006; <papid> W06-1623 </papid>chambers et al, 2007; <papid> P07-2044 </papid>lee and katz, 2008).</citsent>
<aftsection>
<nextsent>the task of identifying the events and times described in text and classifying the relations that hold among them has proven to be difficult, however, with reported results for relation classification tasks ranging in f-score from 0.52 to 0.60.
</nextsent>
<nextsent>variation in the specifics has made comparison among research methods difficult, however.
</nextsent>
<nextsent>a first attempt to standardize this task was the 2007 tem peval competition(verhagen et al, 2007).<papid> W07-2014 </papid></nextsent>
<nextsent>this competition provided standardized training an devaluation scheme for automatic temporal interpretation systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF70">
<title id=" W09-2422.xml">error analysis of the tempe val temporal relation identification task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of identifying the events and times described in text and classifying the relations that hold among them has proven to be difficult, however, with reported results for relation classification tasks ranging in f-score from 0.52 to 0.60.
</prevsent>
<prevsent>variation in the specifics has made comparison among research methods difficult, however.
</prevsent>
</prevsection>
<citsent citstr=" W07-2014 ">
a first attempt to standardize this task was the 2007 tem peval competition(verhagen et al, 2007).<papid> W07-2014 </papid></citsent>
<aftsection>
<nextsent>this competition provided standardized training an devaluation scheme for automatic temporal interpretation systems.
</nextsent>
<nextsent>systems were pitted against one an other on three simple relation-identification tasks.
</nextsent>
<nextsent>the competing systems made use of variety of techniques but their results were comparable, but poor, with average system performance on the tasks ranging in f-score from 0.74 on the easiest task to 0.51 on the most difficult.
</nextsent>
<nextsent>in this paper we provide an analysis of the tempe val 07 competition, identifying aspects of the tasks which presented the systems with particular challenges and those that were accomplished with relative ease.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF71">
<title id=" W10-0307.xml">computational creativity tools for songwriters </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typical lyric writing tools consist of rhyme or phrase dictionaries which are search able but otherwise static, passive resources.
</prevsent>
<prevsent>by contrast, we wish to develop advanced software which uses learned linguistic knowledge to actively help stimulate creative thought.to formalize the task of developing computational creativity tools, let us first define creativity as the ability to extrapolate beyond existing ideas, rules, patterns, interpretations, etc., and to generate meaningful new ones.?
</prevsent>
</prevsection>
<citsent citstr=" W09-2012 ">
by this working definition, which is similar to zhu et al  (2009), <papid> W09-2012 </papid>tools that assist humans in creative endeavors should: 1.</citsent>
<aftsection>
<nextsent>suggest instances unlike the majority that ex-.
</nextsent>
<nextsent>ist.
</nextsent>
<nextsent>if one were to model instances statistically, system proposals should be outliers.?
</nextsent>
<nextsent>random proposals might be outliers, but they are not likely to be interesting or useful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF72">
<title id=" W10-0307.xml">computational creativity tools for songwriters </title>
<section> suggest instances that are meaningful. purely.  </section>
<citcontext>
<prevsection>
<prevsent>previous approaches to linguistic lyric-modeling have generally not focused on creativity, but rather on quantifying hit potential?
</prevsent>
<prevsent>(yang et al , 2007),which is arguably the opposite, or classifying musical genre (li and ogihara, 2004; neu mayer and 49rauber, 2007).
</prevsent>
</prevsection>
<citsent citstr=" W09-2006 ">
there has been some work on automatically generating percussive lyrics to accompany given piece of musical input (oliveira et al , 2007; ramakrishnan et al , 2009), <papid> W09-2006 </papid>and there exists rich body of related work on natural language generation for fiction (montfort, 2006; solis et al , 2009), <papid> W09-2009 </papid>poetry (gervas, 2001; manurung, 2004; netzer et al , 2009), <papid> W09-2005 </papid>and even jokes (binstead, 1996).</citsent>
<aftsection>
<nextsent>however, the goal of these systems is to be an artificial artist which can create complete works of language autonomously, rather than interactive tools for assisting humans in their creative process.
</nextsent>
<nextsent>a few computational lyric-writing tools have been developed outside of academia, such as ver basizer, which was famously co-created by rock star david bowie to help him brainstorm ideas (thomp son, 2007).
</nextsent>
<nextsent>these types of systems take small amount of seed text as input, such as newspaper article, and generate novel phrases by ite rating through random word permutations.
</nextsent>
<nextsent>however, these approaches fail the second criterion for creativity tools, since the majority of output is not meaningful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF73">
<title id=" W10-0307.xml">computational creativity tools for songwriters </title>
<section> suggest instances that are meaningful. purely.  </section>
<citcontext>
<prevsection>
<prevsent>previous approaches to linguistic lyric-modeling have generally not focused on creativity, but rather on quantifying hit potential?
</prevsent>
<prevsent>(yang et al , 2007),which is arguably the opposite, or classifying musical genre (li and ogihara, 2004; neu mayer and 49rauber, 2007).
</prevsent>
</prevsection>
<citsent citstr=" W09-2009 ">
there has been some work on automatically generating percussive lyrics to accompany given piece of musical input (oliveira et al , 2007; ramakrishnan et al , 2009), <papid> W09-2006 </papid>and there exists rich body of related work on natural language generation for fiction (montfort, 2006; solis et al , 2009), <papid> W09-2009 </papid>poetry (gervas, 2001; manurung, 2004; netzer et al , 2009), <papid> W09-2005 </papid>and even jokes (binstead, 1996).</citsent>
<aftsection>
<nextsent>however, the goal of these systems is to be an artificial artist which can create complete works of language autonomously, rather than interactive tools for assisting humans in their creative process.
</nextsent>
<nextsent>a few computational lyric-writing tools have been developed outside of academia, such as ver basizer, which was famously co-created by rock star david bowie to help him brainstorm ideas (thomp son, 2007).
</nextsent>
<nextsent>these types of systems take small amount of seed text as input, such as newspaper article, and generate novel phrases by ite rating through random word permutations.
</nextsent>
<nextsent>however, these approaches fail the second criterion for creativity tools, since the majority of output is not meaningful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF74">
<title id=" W10-0307.xml">computational creativity tools for songwriters </title>
<section> suggest instances that are meaningful. purely.  </section>
<citcontext>
<prevsection>
<prevsent>previous approaches to linguistic lyric-modeling have generally not focused on creativity, but rather on quantifying hit potential?
</prevsent>
<prevsent>(yang et al , 2007),which is arguably the opposite, or classifying musical genre (li and ogihara, 2004; neu mayer and 49rauber, 2007).
</prevsent>
</prevsection>
<citsent citstr=" W09-2005 ">
there has been some work on automatically generating percussive lyrics to accompany given piece of musical input (oliveira et al , 2007; ramakrishnan et al , 2009), <papid> W09-2006 </papid>and there exists rich body of related work on natural language generation for fiction (montfort, 2006; solis et al , 2009), <papid> W09-2009 </papid>poetry (gervas, 2001; manurung, 2004; netzer et al , 2009), <papid> W09-2005 </papid>and even jokes (binstead, 1996).</citsent>
<aftsection>
<nextsent>however, the goal of these systems is to be an artificial artist which can create complete works of language autonomously, rather than interactive tools for assisting humans in their creative process.
</nextsent>
<nextsent>a few computational lyric-writing tools have been developed outside of academia, such as ver basizer, which was famously co-created by rock star david bowie to help him brainstorm ideas (thomp son, 2007).
</nextsent>
<nextsent>these types of systems take small amount of seed text as input, such as newspaper article, and generate novel phrases by ite rating through random word permutations.
</nextsent>
<nextsent>however, these approaches fail the second criterion for creativity tools, since the majority of output is not meaningful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF75">
<title id=" W10-0307.xml">computational creativity tools for songwriters </title>
<section> suggest instances that are meaningful. purely.  </section>
<citcontext>
<prevsection>
<prevsent>for potential seed word s, we can compute asimilarity score with every other word in the corpus vocabulary using the following measure: sim(s, w) = ( 1 + log c(s, w) ) ? log u(w) ,where c(s, w) is the number of times and co occur in the same line of lyric in the corpus, u(w)is the number of unique words with which occurs in any line of the corpus, and is the size ofthe overall vocabulary.
</prevsent>
<prevsent>this is essentially the well known log-tempered tfidf measure from the information retrieval literature, if we treat each seed as document?
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
by concatenating all the lines of text in which appears.i also experimented with the co-occurence frequency c(s, w) and point-wise mutual information (church and hanks, 1989) <papid> P89-1010 </papid>as similarity functions.</citsent>
<aftsection>
<nextsent>52 the former still used overly common words (e.g.,love,?
</nextsent>
<nextsent>heart,?
</nextsent>
<nextsent>baby?), which fails the first criterion for creativity tools, and the latter yielded overly seed-specific results (and often typos not filtered by the pre-processing step), which can fail the second criterion.
</nextsent>
<nextsent>the log-tempered tfidf metric provided reasonable balance between the two extremes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF76">
<title id=" W10-0307.xml">computational creativity tools for songwriters </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>this section discusses some results and observations about titular and lyricloud.
</prevsent>
<prevsent>3.1 empirical evaluation.
</prevsent>
</prevsection>
<citsent citstr=" P09-1113 ">
i conducted an empirical study of these systems using amazon mechanical turk3, which is being used increasingly to evaluate several systems on open ended tasks for which gold-standard evaluation data does not exist (mintz et al , 2009; <papid> P09-1113 </papid>carlson et al , 3http://www.mturk.com 53 template bigram real title 1 2 3 4 5 inspiration (titular) tfidf topic random 1 2 3 4 5 inspiration (lyricloud) tfidf topic random 1 2 3 4 5 relatedness (lyricloud) ea ev alu at or ra tin figure 3: box plots illustrating the distribution of inspiration?</citsent>
<aftsection>
<nextsent>and relatedness?
</nextsent>
<nextsent>ratings assigned by mechanical turk evaluators to titular and lyric loud output.
</nextsent>
<nextsent>boxes represent the middle 50% of ratings, with the medians indicated by thick black lines.
</nextsent>
<nextsent>whiskers on either side span the first and last quart iles of each distribution; circles indicate possible outliers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF77">
<title id=" W09-2011.xml">morphological productivity rankings of complex adjectives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>morphological productivity falls within the domain of how the broad notion of creativity is realized in language, by providing mechanisms for generating new words that are unintentional, unlimited and regular (evert and ldeling 2001).
</prevsent>
<prevsent>previous work has typically considered complex adjectives only in the context of tokenization and generally low-level text processing, without specific focus on the class perse.
</prevsent>
</prevsection>
<citsent citstr=" P08-2028 ">
moilanen and pulman (2008) <papid> P08-2028 </papid>identify polarity markers in certain complex adjectives for the purpose for assigning sentiment polarities to unknown words (well-built is positive, rat-infested is negative).</citsent>
<aftsection>
<nextsent>highly productive classes like complex adjectives are problematic for computational lexicons in that they bring about large number of unknown words.
</nextsent>
<nextsent>a typology of complex adjectives is important to support computational lexicons and the nlp applications based on them, for example by listing out the patterns (as defined in section 3) that complex adjectives are based on.
</nextsent>
<nextsent>the analysis presented here is based on two large text collections.
</nextsent>
<nextsent>the first is portion of wikipedia, consisting of about 250 million tokens.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF78">
<title id=" W09-2403.xml">refining the most frequent sense baseline </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>rather than proposing new algorithm which will tackle all words, we focus on improving upon the mfs baseline system when an alternative system proposes high confidence answer.
</prevsent>
<prevsent>an mfs refining system can therefore benefit from answers suggested by very low recall (but high precision) wsd system.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
we propose number of novel approaches to wsd, but also demonstrate the importance of ahighly accurate lemmatizer and part of speech tagger to the english all words task of s???????-3.1 we present our enriched most frequent sense 1unless specified otherwise, we use wordnet 1.7.1 (milleret al, 1990) and the associated sense annotated semcor corpus (miller et al, 1993) (<papid> H93-1061 </papid>translated to wordnet 1.7.1 by rada mihalcea).</citsent>
<aftsection>
<nextsent>10baseline in section 2, which motivates the lemmatizer and part of speech tagger refinements presentedin section 3.
</nextsent>
<nextsent>our novel high precision wsd algorithms include reranking algorithm (section 4),and wikipedia-based similarity measure (sec tion 5).
</nextsent>
<nextsent>the individual systems are combined in section 6, and we close with our conclusions in section 7.
</nextsent>
<nextsent>the most frequent sense (mfs) baseline assumesa sense annotated corpus from which the frequencies of individual senses are learnt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF79">
<title id=" W09-2403.xml">refining the most frequent sense baseline </title>
<section> most frequent sense baseline.  </section>
<citcontext>
<prevsection>
<prevsent>for each target word, part of speech tagger is used to determine the words part of speech, and the mfs for that part of speech is selected.
</prevsent>
<prevsent>although this is afairly naive baseline, it has been shown to be difficult to beat, with only 5 systems of the 26 submitted to the s???????-3 english all words task outperforming the reported 62.5% mfs baseline.
</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
the success of the mfs baseline is mainly due to the frequency distribution of senses, with the shape of the sense rank versus frequency graph being zipfian curve (i.e., the top-ranked sense being much more likely than any other sense).however, two different mfs baseline performance results are reported in snyder and palmer(2004), <papid> W04-0811 </papid>with further implementations being different still.</citsent>
<aftsection>
<nextsent>the differences in performance of the mfs baseline can be attributed to number of factors: the english all words task is run on natural text and therefore performance greatly depends on the accuracy of the lemmatizer and the part of speech tagger employed.2 if the lemmatizer incorrectly identifies the stem of the word, the mfs will be looked up for the wrong word and the resulting sense assignment will be incorrect.
</nextsent>
<nextsent>the performance of the mfs given the correct lemma and part of speech information is 66%, while the performance of themfs with port stemmer without any pos information is 32%.
</nextsent>
<nextsent>with tree tagger (schmidt, 1994),and sophisticated lemma back-off strategy, the performance increases to 56%.
</nextsent>
<nextsent>it is this difference in 2other possible factors include: 1) the sense distribution in the corpus which the mfs baseline is drawn from, 2) if semcor is used as the underlying sense annotated corpus, the accuracy of the mapping from wordnet 1.6 (with which semcor was initially annotated) to wordnet 1.7.1 could also have an effect on the performance).performance which motivates refining the most frequent sense baseline, and our work on improving the underlying lemmatizer and part of speech tagger presented in section 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF80">
<title id=" W09-2403.xml">refining the most frequent sense baseline </title>
<section> most frequent sense baseline.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 predominant sense.
</prevsent>
<prevsent>mccarthy et al (2007) demonstrate that it is possible to acquire the predominant sense for word in corpus without having access to annotated data.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
they employ an automatically created thesaurus (lin, 1998), <papid> P98-2127 </papid>and sense word similarity metric to assign to each sense si of word a score corresponding to ? jnw dss(w, j) ? sss(si, j)?</citsent>
<aftsection>
<nextsent>sisenses(w) sss(si , j)where dss(w, j) reflects the distributional similarity of word to j, ws thesaural neighbour, andsss(si, j) = maxsxsenses(n j) sss?(si, sx) is the maximum similarity3 between ws sense si and sense sx of ws thesaural neighbour j. the authors show that although this method does not always outperform the mfs baseline based on semcor, it does outperform it when the words semcor frequency is below 5.
</nextsent>
<nextsent>we therefore switch our mfs baseline to this value for such words.
</nextsent>
<nextsent>this result is represented as mccarthy?
</nextsent>
<nextsent>in table 1, which contains the results of the techniques presented in this section evaluated on the s???????-3 english all words task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF81">
<title id=" W09-2403.xml">refining the most frequent sense baseline </title>
<section> most frequent sense baseline.  </section>
<citcontext>
<prevsection>
<prevsent>in table 1, which contains the results of the techniques presented in this section evaluated on the s???????-3 english all words task.
</prevsent>
<prevsent>2.2 verb predominant sense.
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
mccarthy et al (2007) observe that their predominant sense method is not performing as well for 3we use the lesk (overlap) similarity as implemented by the wordnet::similarity package (pedersen et al, 2004).<papid> N04-3012 </papid></citsent>
<aftsection>
<nextsent>11 system precision recall f-measure mfs 58.4% 58.4% 58.4% mccarthy 58.5% 58.5% 58.5% verbs 58.5% 58.5% 58.5% all 58.6% 58.6% 58.6% table 1: refining the mfs baseline with predominant sense verbs as it does for nouns and adjectives.
</nextsent>
<nextsent>we hypothesize that this is due to the thesaural neighbours obtained from lins thesaurus, and we group verbs according to the subcategorization frame (scf) distributions they present in the ?????
</nextsent>
<nextsent>(korhonen et al, 2006) lexicon.
</nextsent>
<nextsent>a word w1 is grouped with word w2 if the bhattacharyya coefficient bc(w1, w2) = ? xx p(x)q(x)where p(x) and q(x) represent the probability values for subcategorization class x, is above certain threshold.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF82">
<title id=" W09-2403.xml">refining the most frequent sense baseline </title>
<section> most frequent sense baseline.  </section>
<citcontext>
<prevsection>
<prevsent>the bc coefficient then replaces thedss value in the original formula and the predominant senses are obtained.
</prevsent>
<prevsent>again, this system is only used for words with frequency lower than 5 in semcor.
</prevsent>
</prevsection>
<citsent citstr=" W02-2014 ">
the great advantage of the bhattacharyya coefficient over various entropy based similarity measures which are usually used to compare scf distributions (korhonen and krymolowski, 2002), <papid> W02-2014 </papid>is that it is guaranteed to lie between 0 and 1, unlike the entropy based measures which are not easily comparable between different word pairs.</citsent>
<aftsection>
<nextsent>this result is represented by verbs?
</nextsent>
<nextsent>in table 1.
</nextsent>
<nextsent>table 1 displays the results for the mfs, the mfs combined with the two approaches described above,and the mfs combining mfs with verbs and mccarthy.
</nextsent>
<nextsent>tagging we made use of several lemmatizers and part-ofspeech taggers, in order to give the other wsd components the best starting point possible.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF83">
<title id=" W09-2403.xml">refining the most frequent sense baseline </title>
<section> lemmatization and part of speech.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 lemmatization.
</prevsent>
<prevsent>lemmatization, the process of obtaining the canonical form of word, was the first step for us to ultimately identify the correct wordnet sense of given word in the english all words task.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
we found that without any lemmatizing of the test input, the maximum -score possible was in the mid-50s.conversely, we found that basic most-frequent sense system that had perfectly-lemmatized input achieved an -score in the mid-60s. this large difference in the ceiling of non-lemmatized system and the floor of perfectly-lemmatized system motivated us to focus on this task.we looked at three different lemmatizers: the lemmatizing backend of the xtag project (xtag research group, 2001)4, celex (baayen et al, 1995), and the lemmatizing component of an enhancedtbl tagger (brill, 1992)<papid> A92-1021 </papid>5 we then employed voting system on these three components, taking the lemma from the most individual lemmatizers.</citsent>
<aftsection>
<nextsent>if all three differ, we take the lemma from the most accurate individual system, namely the tbl tagger.
</nextsent>
<nextsent>3.1.1 lemmatizer evaluation we evaluated the lemmatizers against the lem mas found in the s???????-3 gold standard.6 even the lowest performing system improved accuracy by 31.74% over the baseline, which baseline simply equates the given token with the lemma.
</nextsent>
<nextsent>table 2 shows the results of evaluating the lemmatizers against the eaw key.while the simple voting system performed better than any of the individual lemmatizers, hyphenated words proved problematic for all of the systems.
</nextsent>
<nextsent>some hyphenated words in the test set remained hyphenated in the gold standard, and some others were separated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF85">
<title id=" W09-2403.xml">refining the most frequent sense baseline </title>
<section> lemmatization and part of speech.  </section>
<citcontext>
<prevsection>
<prevsent>however, evaluation results show that splitting hyphenated words increases lem mati zing accuracy by 0.9% . 3.2 part of speech tagging.
</prevsent>
<prevsent>we also investigated the contribution of part of speech taggers to the task of word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" A94-1009 ">
we considered three taggers: the elworthy bigram tagger (elworthy, 1994) <papid> A94-1009 </papid>within the rasp parser (briscoe et al, 2006), <papid> P06-4020 </papid>an enhanced 4http://www.cis.upenn.edu/xtag 5http://gposttl.sourceforge.net 6we removed those lines from both the test input and the gold standard which were marked (= unknown, 34 lines), and we removed the 40 lines from the test input that were missing from the gold standard.</citsent>
<aftsection>
<nextsent>this gave us 2007 words in both the test set and the gold standard.
</nextsent>
<nextsent>12 lemmatizer accuracy baseline 57.50% xtag 89.24% celex 91.58% tbl 92.38% voting {xtag,celex,tbl} 93.77% voting, no hyphen {xtag,celex,tbl} 94.67% table 2: accuracy of several lemmatizers on  head  words of eaw task.
</nextsent>
<nextsent>tbl tagger (brill, 1992)<papid> A92-1021 </papid>7, and tnt-style trigram tagger (halacsy et al, 2007).8 the baseline was aunigram tagger which selects the most frequently occurring tag of singletons when dealing with unseen words.all three of the main taggers performed comparably, although only the elworthy tagger provides probabilities associated with tags, rather than getting single tag as output.</nextsent>
<nextsent>this additional information can be useful, since we can employ different strategies for word with one single tag with probability of 1, versus word with multiple tags,the most probable of which might only have probability of 0.3 for example.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF86">
<title id=" W09-2403.xml">refining the most frequent sense baseline </title>
<section> lemmatization and part of speech.  </section>
<citcontext>
<prevsection>
<prevsent>however, evaluation results show that splitting hyphenated words increases lem mati zing accuracy by 0.9% . 3.2 part of speech tagging.
</prevsent>
<prevsent>we also investigated the contribution of part of speech taggers to the task of word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
we considered three taggers: the elworthy bigram tagger (elworthy, 1994) <papid> A94-1009 </papid>within the rasp parser (briscoe et al, 2006), <papid> P06-4020 </papid>an enhanced 4http://www.cis.upenn.edu/xtag 5http://gposttl.sourceforge.net 6we removed those lines from both the test input and the gold standard which were marked (= unknown, 34 lines), and we removed the 40 lines from the test input that were missing from the gold standard.</citsent>
<aftsection>
<nextsent>this gave us 2007 words in both the test set and the gold standard.
</nextsent>
<nextsent>12 lemmatizer accuracy baseline 57.50% xtag 89.24% celex 91.58% tbl 92.38% voting {xtag,celex,tbl} 93.77% voting, no hyphen {xtag,celex,tbl} 94.67% table 2: accuracy of several lemmatizers on  head  words of eaw task.
</nextsent>
<nextsent>tbl tagger (brill, 1992)<papid> A92-1021 </papid>7, and tnt-style trigram tagger (halacsy et al, 2007).8 the baseline was aunigram tagger which selects the most frequently occurring tag of singletons when dealing with unseen words.all three of the main taggers performed comparably, although only the elworthy tagger provides probabilities associated with tags, rather than getting single tag as output.</nextsent>
<nextsent>this additional information can be useful, since we can employ different strategies for word with one single tag with probability of 1, versus word with multiple tags,the most probable of which might only have probability of 0.3 for example.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF88">
<title id=" W09-2403.xml">refining the most frequent sense baseline </title>
<section> wikipedia for word sense.  </section>
<citcontext>
<prevsection>
<prevsent>signals whether the system backs off to the most frequent sense baseline.
</prevsent>
<prevsent>livier and senellart, 2007) based on each sense or context of interest.advantages of this method over alternative methods that attempt to incorporate wikipedia into wsdis that our system is unsupervised and that no manual mapping needs to take place between wordnet and wikipedia.
</prevsent>
</prevsection>
<citsent citstr=" N07-1025 ">
mihalcea (2007) <papid> N07-1025 </papid>demonstrates that manual mappings can be created for small number of words with relative ease, but for very large number of words the effort involved in mapping would approach presented involves no be considerable.</citsent>
<aftsection>
<nextsent>the approach presented here involves no mapping between wordnet and wikipedia but human effort in mapping between wordnet and wikipedia,but instead initializes the green method with vector based only on the article names (as described in section 5.2).
</nextsent>
<nextsent>5.1 green method.
</nextsent>
<nextsent>the green method (ollivier and senellart, 2007) is used to determine the importance of one node in directed graph with respect to other nodes.15 in the context of wikipedia the method finds the articles which are most likely to be frequented if random walk were used to traverse the articles, starting with specific article and returning to that article if the random walk either strays too far off topic or to an article which is generally popular even without the context of the initial article.
</nextsent>
<nextsent>one of the features ofthe green method is that it does not simply reproduce the global page rank (brin and page, 1998), instead determining the related pages nearby due to relevance to the initial node.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF89">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>inference over predicates, however, often requires change in argument positions, whichis not specified in wordnet.
</prevsent>
<prevsent>we propose novel framework for augmenting wordnet-based inferences over predicates with corresponding argument mappings.we further present concrete implementation of this framework, which yields substantial improvement to wordnet-based inference.
</prevsent>
</prevsection>
<citsent citstr=" W07-1401 ">
wordnet (miller, 1995), manually constructed lexical database, is probably the mostly used resource for lexical inference in nlp tasks, suchas question answering (qa), information extraction (ie), information retrieval and textual entailment (rte) (moldovan and mihalcea, 2000; pasca and harabagiu, 2001; bar-haim et al , 2006; giampiccolo et al , 2007).<papid> W07-1401 </papid>inference using wordnet typically involves lexical substitutions for words in text based on wordnet relations, process known as lexical chains (barzilay and elhadad, 1997; <papid> W97-0703 </papid>moldovan and novischi, 2002).<papid> C02-1167 </papid></citsent>
<aftsection>
<nextsent>for example, the answer to from which country was louisiana ac quired??
</nextsent>
<nextsent>can be inferred from the united states bought up louisiana from france?
</nextsent>
<nextsent>using the chains france ? european country ? country?
</nextsent>
<nextsent>and buy up?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF90">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>inference over predicates, however, often requires change in argument positions, whichis not specified in wordnet.
</prevsent>
<prevsent>we propose novel framework for augmenting wordnet-based inferences over predicates with corresponding argument mappings.we further present concrete implementation of this framework, which yields substantial improvement to wordnet-based inference.
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
wordnet (miller, 1995), manually constructed lexical database, is probably the mostly used resource for lexical inference in nlp tasks, suchas question answering (qa), information extraction (ie), information retrieval and textual entailment (rte) (moldovan and mihalcea, 2000; pasca and harabagiu, 2001; bar-haim et al , 2006; giampiccolo et al , 2007).<papid> W07-1401 </papid>inference using wordnet typically involves lexical substitutions for words in text based on wordnet relations, process known as lexical chains (barzilay and elhadad, 1997; <papid> W97-0703 </papid>moldovan and novischi, 2002).<papid> C02-1167 </papid></citsent>
<aftsection>
<nextsent>for example, the answer to from which country was louisiana ac quired??
</nextsent>
<nextsent>can be inferred from the united states bought up louisiana from france?
</nextsent>
<nextsent>using the chains france ? european country ? country?
</nextsent>
<nextsent>and buy up?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF91">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>inference over predicates, however, often requires change in argument positions, whichis not specified in wordnet.
</prevsent>
<prevsent>we propose novel framework for augmenting wordnet-based inferences over predicates with corresponding argument mappings.we further present concrete implementation of this framework, which yields substantial improvement to wordnet-based inference.
</prevsent>
</prevsection>
<citsent citstr=" C02-1167 ">
wordnet (miller, 1995), manually constructed lexical database, is probably the mostly used resource for lexical inference in nlp tasks, suchas question answering (qa), information extraction (ie), information retrieval and textual entailment (rte) (moldovan and mihalcea, 2000; pasca and harabagiu, 2001; bar-haim et al , 2006; giampiccolo et al , 2007).<papid> W07-1401 </papid>inference using wordnet typically involves lexical substitutions for words in text based on wordnet relations, process known as lexical chains (barzilay and elhadad, 1997; <papid> W97-0703 </papid>moldovan and novischi, 2002).<papid> C02-1167 </papid></citsent>
<aftsection>
<nextsent>for example, the answer to from which country was louisiana ac quired??
</nextsent>
<nextsent>can be inferred from the united states bought up louisiana from france?
</nextsent>
<nextsent>using the chains france ? european country ? country?
</nextsent>
<nextsent>and buy up?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF92">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> argument-mapping entailment rules.  </section>
<citcontext>
<prevsection>
<prevsent>the results show that amwn substantially improves wordnet-based inference in terms of both recall and precision 1 .
</prevsent>
<prevsent>in our framework we represent argument mappings for inferential relations between predicates through an extension of entailment rules over syntactic representations.
</prevsent>
</prevsection>
<citsent citstr=" C08-1107 ">
as defined in earlier works,an entailment rule specifies an inference relation between an entailing template and an entailed template, where templates are parse subtrees with argument variables (szpektor and da gan, 2008).<papid> C08-1107 </papid></citsent>
<aftsection>
<nextsent>for example, subj ???
</nextsent>
<nextsent>buy obj ???
</nextsent>
<nextsent>y ? subj ???
</nextsent>
<nextsent>pay prepfor ???????
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF99">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> results and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>this occurs typically when the match is against another sense of the predicate, or when an argument is not of the requested type (e.g. the enron sentence?
</prevsent>
<prevsent>vs. one month sentence?).
</prevsent>
</prevsection>
<citsent citstr=" P08-1078 ">
in future work, we plan to address this problem by utilizing context-sensitive application of rules in the spirit of (szpektor et al , 2008).<papid> P08-1078 </papid></citsent>
<aftsection>
<nextsent>table 5 presents the false negatives analysis.
</nextsent>
<nextsent>most missed extractions are due to rules that werenot learned (67.7%).
</nextsent>
<nextsent>these mainly involve complex templates (file lawsuit ? sue?)
</nextsent>
<nextsent>and inference rules that are not synonyms/hypernyms (execute ? sentence?), which are not widely annotated inwordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF100">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>was generated only via the shared nominal founding?.
</prevsent>
<prevsent>in future work, we plan to apply amwn to coarse-grained set of wordnet synsets (palmer et al , 2007) as possible solution to sense drifting.
</prevsent>
</prevsection>
<citsent citstr=" P01-1052 ">
several works attempt to extend wordnet with additional lexical semantic information (moldovan andrus, 2001; <papid> P01-1052 </papid>snow et al , 2006; <papid> P06-1101 </papid>suchanek et al ,2007; clark et al , 2008).</citsent>
<aftsection>
<nextsent>however, the only previous work we are aware of that enriches wordnet with argument mappings is (novischi and moldovan, 2006).<papid> P06-1113 </papid></nextsent>
<nextsent>this work utilizes verb nets subcategorization frames to identify possible verb arguments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF101">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>was generated only via the shared nominal founding?.
</prevsent>
<prevsent>in future work, we plan to apply amwn to coarse-grained set of wordnet synsets (palmer et al , 2007) as possible solution to sense drifting.
</prevsent>
</prevsection>
<citsent citstr=" P06-1101 ">
several works attempt to extend wordnet with additional lexical semantic information (moldovan andrus, 2001; <papid> P01-1052 </papid>snow et al , 2006; <papid> P06-1101 </papid>suchanek et al ,2007; clark et al , 2008).</citsent>
<aftsection>
<nextsent>however, the only previous work we are aware of that enriches wordnet with argument mappings is (novischi and moldovan, 2006).<papid> P06-1113 </papid></nextsent>
<nextsent>this work utilizes verb nets subcategorization frames to identify possible verb arguments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF102">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in future work, we plan to apply amwn to coarse-grained set of wordnet synsets (palmer et al , 2007) as possible solution to sense drifting.
</prevsent>
<prevsent>several works attempt to extend wordnet with additional lexical semantic information (moldovan andrus, 2001; <papid> P01-1052 </papid>snow et al , 2006; <papid> P06-1101 </papid>suchanek et al ,2007; clark et al , 2008).</prevsent>
</prevsection>
<citsent citstr=" P06-1113 ">
however, the only previous work we are aware of that enriches wordnet with argument mappings is (novischi and moldovan, 2006).<papid> P06-1113 </papid></citsent>
<aftsection>
<nextsent>this work utilizes verb nets subcategorization frames to identify possible verb arguments.
</nextsent>
<nextsent>argument mapping is provided only between verbs, ignoring relations between verb sand nouns.
</nextsent>
<nextsent>arguments are mapped based on thematic role names shared between frames of different verbs.
</nextsent>
<nextsent>however, the semantic interpretation of thematic roles is generally inconsistent across verbs (lowe et al , 1997; <papid> W97-0204 </papid>kaisser and webber, 2007).<papid> W07-1206 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF103">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>argument mapping is provided only between verbs, ignoring relations between verb sand nouns.
</prevsent>
<prevsent>arguments are mapped based on thematic role names shared between frames of different verbs.
</prevsent>
</prevsection>
<citsent citstr=" W97-0204 ">
however, the semantic interpretation of thematic roles is generally inconsistent across verbs (lowe et al , 1997; <papid> W97-0204 </papid>kaisser and webber, 2007).<papid> W07-1206 </papid></citsent>
<aftsection>
<nextsent>instead, we discover these mappings from corpus statistics, offering an accurate approach (as analyzed in section 5.2).
</nextsent>
<nextsent>a frame semantics approach for argument mapping between predicates is proposed by the framenet project (baker et al , 1998).<papid> P98-1013 </papid></nextsent>
<nextsent>currently, framenet is the only resource for frame-semanticargument mappings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF104">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>argument mapping is provided only between verbs, ignoring relations between verb sand nouns.
</prevsent>
<prevsent>arguments are mapped based on thematic role names shared between frames of different verbs.
</prevsent>
</prevsection>
<citsent citstr=" W07-1206 ">
however, the semantic interpretation of thematic roles is generally inconsistent across verbs (lowe et al , 1997; <papid> W97-0204 </papid>kaisser and webber, 2007).<papid> W07-1206 </papid></citsent>
<aftsection>
<nextsent>instead, we discover these mappings from corpus statistics, offering an accurate approach (as analyzed in section 5.2).
</nextsent>
<nextsent>a frame semantics approach for argument mapping between predicates is proposed by the framenet project (baker et al , 1998).<papid> P98-1013 </papid></nextsent>
<nextsent>currently, framenet is the only resource for frame-semanticargument mappings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF105">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, the semantic interpretation of thematic roles is generally inconsistent across verbs (lowe et al , 1997; <papid> W97-0204 </papid>kaisser and webber, 2007).<papid> W07-1206 </papid></prevsent>
<prevsent>instead, we discover these mappings from corpus statistics, offering an accurate approach (as analyzed in section 5.2).</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
a frame semantics approach for argument mapping between predicates is proposed by the framenet project (baker et al , 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>currently, framenet is the only resource for frame-semanticargument mappings.
</nextsent>
<nextsent>however, it is manually constructed and currently covers much less predicates and relations than wordnet.
</nextsent>
<nextsent>furthermore,frame-semantic parsers are less robust than syntactic parsers, presently hindering the utilization ofthis approach in applications (burchardt and pennacchiotti, 2008).<papid> L08-1038 </papid></nextsent>
<nextsent>nomlex argument mapping patterns similar to ours were derived for ie in (meyers et al , 1998), but they were not integrated with any additional information, such as wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF106">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>currently, framenet is the only resource for frame-semanticargument mappings.
</prevsent>
<prevsent>however, it is manually constructed and currently covers much less predicates and relations than wordnet.
</prevsent>
</prevsection>
<citsent citstr=" L08-1038 ">
furthermore,frame-semantic parsers are less robust than syntactic parsers, presently hindering the utilization ofthis approach in applications (burchardt and pennacchiotti, 2008).<papid> L08-1038 </papid></citsent>
<aftsection>
<nextsent>nomlex argument mapping patterns similar to ours were derived for ie in (meyers et al , 1998), but they were not integrated with any additional information, such as wordnet.
</nextsent>
<nextsent>we presented argument-mapped wordnet(amwn), novel framework for augmenting wordnet with argument mappings at the syntactic representation level.
</nextsent>
<nextsent>with amwn, non-substitutable wordnet relations can also be utilized correctly, increasing the coverage ofwordnet-based inference.
</nextsent>
<nextsent>the standard entailment rule representation is augmented in our work with functional roles and subcategorization frames, shown to be feasible extension needed for correct rule application in general.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF107">
<title id=" W09-2504.xml">augmenting wordnet based inference with argument mapping </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>our experiments show that amwn substantially improves standard wordnet-based inference.in future work we plan to add mappings between verbs and adjectives and between different frames of verb.
</prevsent>
<prevsent>we also want to incorporate resources for additional subcategorization frames, such as verbnet.
</prevsent>
</prevsection>
<citsent citstr=" E03-1073 ">
finally, we plan to enhance ourtext annotation based on noun-compound disambiguation (lapata and lascarides, 2003).<papid> E03-1073 </papid></citsent>
<aftsection>
<nextsent>acknowledgements this work was partially supported by the negev project (www.negev-initiative.org), the pascal 2 network of excellence of the european commu-.
</nextsent>
<nextsent>nity fp7-ict-2007-1-216886, the fbk-irst/barilan university collaboration and the israel science foundation grant 1112/08.
</nextsent>
<nextsent>34
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF108">
<title id=" W09-2112.xml">generrate generating errors for use in grammatical error detection </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>by grammatical error detection, wemean either the task of distinguishing the grammatical from the ungrammatical at the sentence level or more local targeted error detection, involving the identification, and possibly also correction, of particular types of errors.
</prevsent>
<prevsent>distinguishing grammatical utterances from ungrammatical ones involves theuse of binary classifier or grammaticality scor 1http://www.cambridge.org/elt/corpus/ learner_corpus2.htm 82 ing model.
</prevsent>
</prevsection>
<citsent citstr=" P07-1010 ">
examples are andersen (2006), examples are andersen (2007), okanohara and tsujii (2007), <papid> P07-1010 </papid>sun et al (2007) <papid> P07-1011 </papid>and wagner et al (2007).<papid> D07-1012 </papid></citsent>
<aftsection>
<nextsent>in targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (de pending on the application).
</nextsent>
<nextsent>for esl applications,this includes the detection of errors involving articles (han et al, 2006; de felice and pulman, 2008; gamon et al, 2008), <papid> I08-1059 </papid>prepositions (de felice and pulman, 2008; gamon et al, 2008; <papid> I08-1059 </papid>tetreault and chodorow, 2008), <papid> C08-1109 </papid>verb forms (lee and seneff, 2008<papid> P08-1021 </papid>b), mass/count noun confusions (brockett et al, 2006) <papid> P06-1032 </papid>and word order (metcalf and meurers, 2006).the presence of pattern in corpus of well formed language is positive evidence that the pattern is well-formed.</nextsent>
<nextsent>the presence of pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF109">
<title id=" W09-2112.xml">generrate generating errors for use in grammatical error detection </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>by grammatical error detection, wemean either the task of distinguishing the grammatical from the ungrammatical at the sentence level or more local targeted error detection, involving the identification, and possibly also correction, of particular types of errors.
</prevsent>
<prevsent>distinguishing grammatical utterances from ungrammatical ones involves theuse of binary classifier or grammaticality scor 1http://www.cambridge.org/elt/corpus/ learner_corpus2.htm 82 ing model.
</prevsent>
</prevsection>
<citsent citstr=" P07-1011 ">
examples are andersen (2006), examples are andersen (2007), okanohara and tsujii (2007), <papid> P07-1010 </papid>sun et al (2007) <papid> P07-1011 </papid>and wagner et al (2007).<papid> D07-1012 </papid></citsent>
<aftsection>
<nextsent>in targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (de pending on the application).
</nextsent>
<nextsent>for esl applications,this includes the detection of errors involving articles (han et al, 2006; de felice and pulman, 2008; gamon et al, 2008), <papid> I08-1059 </papid>prepositions (de felice and pulman, 2008; gamon et al, 2008; <papid> I08-1059 </papid>tetreault and chodorow, 2008), <papid> C08-1109 </papid>verb forms (lee and seneff, 2008<papid> P08-1021 </papid>b), mass/count noun confusions (brockett et al, 2006) <papid> P06-1032 </papid>and word order (metcalf and meurers, 2006).the presence of pattern in corpus of well formed language is positive evidence that the pattern is well-formed.</nextsent>
<nextsent>the presence of pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF110">
<title id=" W09-2112.xml">generrate generating errors for use in grammatical error detection </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>by grammatical error detection, wemean either the task of distinguishing the grammatical from the ungrammatical at the sentence level or more local targeted error detection, involving the identification, and possibly also correction, of particular types of errors.
</prevsent>
<prevsent>distinguishing grammatical utterances from ungrammatical ones involves theuse of binary classifier or grammaticality scor 1http://www.cambridge.org/elt/corpus/ learner_corpus2.htm 82 ing model.
</prevsent>
</prevsection>
<citsent citstr=" D07-1012 ">
examples are andersen (2006), examples are andersen (2007), okanohara and tsujii (2007), <papid> P07-1010 </papid>sun et al (2007) <papid> P07-1011 </papid>and wagner et al (2007).<papid> D07-1012 </papid></citsent>
<aftsection>
<nextsent>in targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (de pending on the application).
</nextsent>
<nextsent>for esl applications,this includes the detection of errors involving articles (han et al, 2006; de felice and pulman, 2008; gamon et al, 2008), <papid> I08-1059 </papid>prepositions (de felice and pulman, 2008; gamon et al, 2008; <papid> I08-1059 </papid>tetreault and chodorow, 2008), <papid> C08-1109 </papid>verb forms (lee and seneff, 2008<papid> P08-1021 </papid>b), mass/count noun confusions (brockett et al, 2006) <papid> P06-1032 </papid>and word order (metcalf and meurers, 2006).the presence of pattern in corpus of well formed language is positive evidence that the pattern is well-formed.</nextsent>
<nextsent>the presence of pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF113">
<title id=" W09-2112.xml">generrate generating errors for use in grammatical error detection </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>examples are andersen (2006), examples are andersen (2007), okanohara and tsujii (2007), <papid> P07-1010 </papid>sun et al (2007) <papid> P07-1011 </papid>and wagner et al (2007).<papid> D07-1012 </papid></prevsent>
<prevsent>in targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (de pending on the application).</prevsent>
</prevsection>
<citsent citstr=" I08-1059 ">
for esl applications,this includes the detection of errors involving articles (han et al, 2006; de felice and pulman, 2008; gamon et al, 2008), <papid> I08-1059 </papid>prepositions (de felice and pulman, 2008; gamon et al, 2008; <papid> I08-1059 </papid>tetreault and chodorow, 2008), <papid> C08-1109 </papid>verb forms (lee and seneff, 2008<papid> P08-1021 </papid>b), mass/count noun confusions (brockett et al, 2006) <papid> P06-1032 </papid>and word order (metcalf and meurers, 2006).the presence of pattern in corpus of well formed language is positive evidence that the pattern is well-formed.</citsent>
<aftsection>
<nextsent>the presence of pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous.
</nextsent>
<nextsent>discriminative techniques usually lead to more accurate systems than those based on one class alone.
</nextsent>
<nextsent>the use of the two types of evidence can be seen at work in the system described by lee and seneff (2008<papid> P08-1021 </papid>b): verb phrases are parsed and their parse trees are examined.</nextsent>
<nextsent>if the parse trees resemble the disturbed?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF115">
<title id=" W09-2112.xml">generrate generating errors for use in grammatical error detection </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>examples are andersen (2006), examples are andersen (2007), okanohara and tsujii (2007), <papid> P07-1010 </papid>sun et al (2007) <papid> P07-1011 </papid>and wagner et al (2007).<papid> D07-1012 </papid></prevsent>
<prevsent>in targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (de pending on the application).</prevsent>
</prevsection>
<citsent citstr=" C08-1109 ">
for esl applications,this includes the detection of errors involving articles (han et al, 2006; de felice and pulman, 2008; gamon et al, 2008), <papid> I08-1059 </papid>prepositions (de felice and pulman, 2008; gamon et al, 2008; <papid> I08-1059 </papid>tetreault and chodorow, 2008), <papid> C08-1109 </papid>verb forms (lee and seneff, 2008<papid> P08-1021 </papid>b), mass/count noun confusions (brockett et al, 2006) <papid> P06-1032 </papid>and word order (metcalf and meurers, 2006).the presence of pattern in corpus of well formed language is positive evidence that the pattern is well-formed.</citsent>
<aftsection>
<nextsent>the presence of pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous.
</nextsent>
<nextsent>discriminative techniques usually lead to more accurate systems than those based on one class alone.
</nextsent>
<nextsent>the use of the two types of evidence can be seen at work in the system described by lee and seneff (2008<papid> P08-1021 </papid>b): verb phrases are parsed and their parse trees are examined.</nextsent>
<nextsent>if the parse trees resemble the disturbed?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF116">
<title id=" W09-2112.xml">generrate generating errors for use in grammatical error detection </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>examples are andersen (2006), examples are andersen (2007), okanohara and tsujii (2007), <papid> P07-1010 </papid>sun et al (2007) <papid> P07-1011 </papid>and wagner et al (2007).<papid> D07-1012 </papid></prevsent>
<prevsent>in targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (de pending on the application).</prevsent>
</prevsection>
<citsent citstr=" P08-1021 ">
for esl applications,this includes the detection of errors involving articles (han et al, 2006; de felice and pulman, 2008; gamon et al, 2008), <papid> I08-1059 </papid>prepositions (de felice and pulman, 2008; gamon et al, 2008; <papid> I08-1059 </papid>tetreault and chodorow, 2008), <papid> C08-1109 </papid>verb forms (lee and seneff, 2008<papid> P08-1021 </papid>b), mass/count noun confusions (brockett et al, 2006) <papid> P06-1032 </papid>and word order (metcalf and meurers, 2006).the presence of pattern in corpus of well formed language is positive evidence that the pattern is well-formed.</citsent>
<aftsection>
<nextsent>the presence of pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous.
</nextsent>
<nextsent>discriminative techniques usually lead to more accurate systems than those based on one class alone.
</nextsent>
<nextsent>the use of the two types of evidence can be seen at work in the system described by lee and seneff (2008<papid> P08-1021 </papid>b): verb phrases are parsed and their parse trees are examined.</nextsent>
<nextsent>if the parse trees resemble the disturbed?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF117">
<title id=" W09-2112.xml">generrate generating errors for use in grammatical error detection </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>examples are andersen (2006), examples are andersen (2007), okanohara and tsujii (2007), <papid> P07-1010 </papid>sun et al (2007) <papid> P07-1011 </papid>and wagner et al (2007).<papid> D07-1012 </papid></prevsent>
<prevsent>in targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (de pending on the application).</prevsent>
</prevsection>
<citsent citstr=" P06-1032 ">
for esl applications,this includes the detection of errors involving articles (han et al, 2006; de felice and pulman, 2008; gamon et al, 2008), <papid> I08-1059 </papid>prepositions (de felice and pulman, 2008; gamon et al, 2008; <papid> I08-1059 </papid>tetreault and chodorow, 2008), <papid> C08-1109 </papid>verb forms (lee and seneff, 2008<papid> P08-1021 </papid>b), mass/count noun confusions (brockett et al, 2006) <papid> P06-1032 </papid>and word order (metcalf and meurers, 2006).the presence of pattern in corpus of well formed language is positive evidence that the pattern is well-formed.</citsent>
<aftsection>
<nextsent>the presence of pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous.
</nextsent>
<nextsent>discriminative techniques usually lead to more accurate systems than those based on one class alone.
</nextsent>
<nextsent>the use of the two types of evidence can be seen at work in the system described by lee and seneff (2008<papid> P08-1021 </papid>b): verb phrases are parsed and their parse trees are examined.</nextsent>
<nextsent>if the parse trees resemble the disturbed?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF134">
<title id=" W09-2112.xml">generrate generating errors for use in grammatical error detection </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, foster (2007) investigates the effect of common english grammatical errors on two widely-used statistical parsers using distorted treebank trees as references.
</prevsent>
<prevsent>the procedure used by wagner et al (2007), <papid> D07-1012 </papid>wagner et al (2009) is used to introduce errors into the treebank sentences.finally, negative evidence in the form of automatically distorted sentences has been used in unsupervised learning.</prevsent>
</prevsection>
<citsent citstr=" P05-1044 ">
smith and eisner (2005<papid> P05-1044 </papid>a), smith and eisner (2005<papid> P05-1044 </papid>b)generate negative evidence for their contrastive estimation method by moving or removing word in sentence.</citsent>
<aftsection>
<nextsent>since the aim of this work is not to detect grammatical errors, there is no requirement to generate the kind of negative evidence that might actually be produced by either native or non-native speakers of language.
</nextsent>
<nextsent>the negative examples are used to guide the unsupervised learning of part-of-speech tagger and dependency grammar.
</nextsent>
<nextsent>we can conclude from this survey that synthetic error data is useful in variety of nlp applications,including error detection and evaluation of error detectors.
</nextsent>
<nextsent>in section 3, we describe an automatic error generation tool, which has modular design and is flexible enough to accommodate the generation of the various types of synthetic data described above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF154">
<title id=" W09-2112.xml">generrate generating errors for use in grammatical error detection </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>we have presented generrate, tool for automatically introducing syntactic errors into sentences and shown how it can be useful for creating synthetic training data to be used in grammatical error detection research.
</prevsent>
<prevsent>although we have focussed on the binary classification task, we also intend to test gen errate in targeted error detection.
</prevsent>
</prevsection>
<citsent citstr=" P06-4001 ">
another avenue for future work is to explore whether generratecould be of use in the automatic generation of language test items (chen et al, 2006, <papid> P06-4001 </papid>for example).</citsent>
<aftsection>
<nextsent>our immediate aim is to produce new version ofgenerrate which tackles some of the coverage issues highlighted by our experiments.
</nextsent>
<nextsent>acknowledgments this paper reports on research supported by the university of cambridge esol examinations.
</nextsent>
<nextsent>we arevery grateful to cambridge university press for giving us access to the cambridge learner corpus andto james hunter from gonzaga college for supplying us with the spoken language learner corpus.
</nextsent>
<nextsent>we thank ted briscoe, josef van genabith, joachim wagner and the reviewers for their very helpful suggestions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF155">
<title id=" W09-2807.xml">a classification algorithm for predicting the structure of summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure 2: professional abstract with inserted predicates from internet &amp; personal computing abstracts they inform or alert the reader about the content of the abstracted document by explicitly marking what the author says or mentions, presents or introduces, concludes, or includes, in her paper.montesi and owen (2007) observe that the revision of abstracts is carried out to improve com prehensibility and style and to make the abstract objective.
</prevsent>
<prevsent>we investigate how to create the discourse structure of the abstracts: more specifically we are interested in predicting the inserted predicates or phrases and at which positions in the abstract they should be prepended.
</prevsent>
</prevsection>
<citsent citstr=" C08-1018 ">
abs tractive techniques in text summarization include sentence compression (cohn and lapata, 2008), <papid> C08-1018 </papid>headline generation (soricut and marcu, 2007), and canned-based generation (oakes and paice, 2001).</citsent>
<aftsection>
<nextsent>close to the problem studied here is jing and mckeowns (jing and mckeown,2000) <papid> A00-2024 </papid>cut-and-paste method founded on endres niggemeyers observations.</nextsent>
<nextsent>the cut-and-paste 31method includes such operations as sentence trun cation, aggregation, specialization/generalization, reference adjustment and rewording.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF156">
<title id=" W09-2807.xml">a classification algorithm for predicting the structure of summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we investigate how to create the discourse structure of the abstracts: more specifically we are interested in predicting the inserted predicates or phrases and at which positions in the abstract they should be prepended.
</prevsent>
<prevsent>abs tractive techniques in text summarization include sentence compression (cohn and lapata, 2008), <papid> C08-1018 </papid>headline generation (soricut and marcu, 2007), and canned-based generation (oakes and paice, 2001).</prevsent>
</prevsection>
<citsent citstr=" A00-2024 ">
close to the problem studied here is jing and mckeowns (jing and mckeown,2000) <papid> A00-2024 </papid>cut-and-paste method founded on endres niggemeyers observations.</citsent>
<aftsection>
<nextsent>the cut-and-paste 31method includes such operations as sentence trun cation, aggregation, specialization/generalization, reference adjustment and rewording.
</nextsent>
<nextsent>none of these operations account for the transformations observed in the abstracts of figures 1 and 2.
</nextsent>
<nextsent>the formulaic expressions or predicates inserted in the abstract glue?
</nextsent>
<nextsent>together the extracted fragments, thus creating the abstracts discourse structure.to the best of our knowledge, and with the exception of saggion and lapalme (2002) <papid> J02-4005 </papid>indicative generation approach which included operations toadd extra linguistic material to generate an indicative abstract, the work presented here is the first to investigate this relevant operation in the field oftext abstracting and to propose robust computational method for its simulation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF157">
<title id=" W09-2807.xml">a classification algorithm for predicting the structure of summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>none of these operations account for the transformations observed in the abstracts of figures 1 and 2.
</prevsent>
<prevsent>the formulaic expressions or predicates inserted in the abstract glue?
</prevsent>
</prevsection>
<citsent citstr=" J02-4005 ">
together the extracted fragments, thus creating the abstracts discourse structure.to the best of our knowledge, and with the exception of saggion and lapalme (2002) <papid> J02-4005 </papid>indicative generation approach which included operations toadd extra linguistic material to generate an indicative abstract, the work presented here is the first to investigate this relevant operation in the field oftext abstracting and to propose robust computational method for its simulation.</citsent>
<aftsection>
<nextsent>in this paper we are interested in the process of generating the structure of the abstract by automatic means.
</nextsent>
<nextsent>in order to study this problem, wehave collected corpus of abstracts written by abstractors; we have designed an algorithm for predicting the structure; implemented the algorithm;and evaluated the structure predicted by the automatic system against the true structure.
</nextsent>
<nextsent>and annotation the abstracts we study in this research follow the pattern: abstract?
</nextsent>
<nextsent>n i=1 pred ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF159">
<title id=" W09-2807.xml">a classification algorithm for predicting the structure of summaries </title>
<section> problem specification, data collection,.  </section>
<citcontext>
<prevsection>
<prevsent>electronic copies of the abstracted documents can also be accessed through our institution following link,thus allowing us to check abstracts against abstracted document (additional information on the abstracts is given in the appendix).
</prevsent>
<prevsent>2.1 document processing and annotation.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
each electronic version of the abstract was processed using the freely available gate text analysis software (cunningham et al, 2002).<papid> P02-1022 </papid></citsent>
<aftsection>
<nextsent>first each abstract was analyzed by text structure analysis program to identify meta-data such as title, author, source document, the text of the abstract, etc. each sentence in the abstract was stripped from the predicate or phrase inserted by the abs tractor (e.g., mentions that?, concludes with?)
</nextsent>
<nextsent>and normalised version of the expression was used to annotate the sentence, in way similar to the abstracts in figures 1 and 2.
</nextsent>
<nextsent>after this each abstract and document title was token ised, sentence split ted, part-of-speech tagged, and morphologically analyzed.
</nextsent>
<nextsent>a rule-based system was used to carryout partial, robust syntactic and semantic analysis of the abstracts (gaizauskas et al, 2005) producing predicate-argument representations where predicates which are used to represent entities are created from the morphological roots of nouns or verbs in the text (unary predicates) and predicates with are used to represent binary relations are aclosed set of names representing grammatical relations such as the verb logical object, or the verb logical subject or prepositional attachment, etc. this predicate-argument structure representation was further analysed in order to extract semantic?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF160">
<title id=" W09-2807.xml">a classification algorithm for predicting the structure of summaries </title>
<section> proposed solution.  </section>
<citcontext>
<prevsection>
<prevsent>i ? ?.?; context?
</prevsent>
<prevsent>extractcontext(abstract); end for return abstract end at each iteration the algorithm selects the best available phrase or predicate to prepend to the current fragment from finite vocabulary (induced from the analysed corpus) based on local and contextual information.
</prevsent>
</prevsection>
<citsent citstr=" P00-1041 ">
one could relyon existing trainable sentence selection (kupiec et al, 1995) or even phrase selection (banko et al, 2000) <papid> P00-1041 </papid>strategies to pick up appropriate ? is from the document to be abstracted and relyon recent information ordering techniques to sort the ? fragments(lapata, 2003).<papid> P03-1069 </papid></citsent>
<aftsection>
<nextsent>this is the reason why we only address here the discourse structure generation problem.
</nextsent>
<nextsent>3.1 predicting discourse structure as.
</nextsent>
<nextsent>classification there are various possible ways of predicting what expression to insert at each point in the generation process (i.e., the predict predicate function in algorithm 1).
</nextsent>
<nextsent>in the experiments reported here we use classification algorithm based on lexical, syntactic, and discursive features, which decides which of the possible available phrases is mostsuitable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF161">
<title id=" W09-2807.xml">a classification algorithm for predicting the structure of summaries </title>
<section> proposed solution.  </section>
<citcontext>
<prevsection>
<prevsent>i ? ?.?; context?
</prevsent>
<prevsent>extractcontext(abstract); end for return abstract end at each iteration the algorithm selects the best available phrase or predicate to prepend to the current fragment from finite vocabulary (induced from the analysed corpus) based on local and contextual information.
</prevsent>
</prevsection>
<citsent citstr=" P03-1069 ">
one could relyon existing trainable sentence selection (kupiec et al, 1995) or even phrase selection (banko et al, 2000) <papid> P00-1041 </papid>strategies to pick up appropriate ? is from the document to be abstracted and relyon recent information ordering techniques to sort the ? fragments(lapata, 2003).<papid> P03-1069 </papid></citsent>
<aftsection>
<nextsent>this is the reason why we only address here the discourse structure generation problem.
</nextsent>
<nextsent>3.1 predicting discourse structure as.
</nextsent>
<nextsent>classification there are various possible ways of predicting what expression to insert at each point in the generation process (i.e., the predict predicate function in algorithm 1).
</nextsent>
<nextsent>in the experiments reported here we use classification algorithm based on lexical, syntactic, and discursive features, which decides which of the possible available phrases is mostsuitable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF163">
<title id=" W09-2807.xml">a classification algorithm for predicting the structure of summaries </title>
<section> proposed solution.  </section>
<citcontext>
<prevsection>
<prevsent>where the classification algorithm is concerned, we have decided to use support vector machines which have recently been used in different tasks in natural language processing, they have been shown particularly suitable for text categorization (joachims, 1998).
</prevsent>
<prevsent>we have tried other machine learning algorithms such as decision trees, naive bayes classification, and nearest neighbor from the weka toolkit (witten and frank, 1999), but the support vector machines gave us the best classification accuracy (a comparison with naive bayes will be presented in section 4).
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
the features used for the experiments reported here are inspired by previous work in text summarization on content selection (kupiec et al, 1995), rhetorical classification (teufel and moens, 2002), <papid> J02-4002 </papid>and information ordering (lapata, 2003).<papid> P03-1069 </papid></citsent>
<aftsection>
<nextsent>the features are extracted from the analyzed abstracts with specialized programs.
</nextsent>
<nextsent>in particular we use positional features (position of the predicate to be generated in the structure), length features (num ber of words in the sentence), title features (e.g.,presence of title words in sentence), content features computed as the syntactic head of noun and verb phrases, semantic features computed as the 33 to add; to conclude; to contain; to describe; todiscuss; to explain; to feature; to include; to indi cate; to mention; to note; to point out; to present; to provide; to report; to say table 2: predicates in the reduced corpus arguments of semantic?
</nextsent>
<nextsent>triples (section 2.1) extracted from the parsed abstracts.
</nextsent>
<nextsent>features occur ring less than 4 times in the corpus were removed for the experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF166">
<title id=" W09-2807.xml">a classification algorithm for predicting the structure of summaries </title>
<section> proposed solution.  </section>
<citcontext>
<prevsection>
<prevsent>cohesion information has been used in rhetorical-based parsing for summarization (marcu, 1997) in order to decide between list?
</prevsent>
<prevsent>or elaboration?
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
relations andalso in content selection for summarization (barzi lay and elhadad, 1997).<papid> W97-0703 </papid></citsent>
<aftsection>
<nextsent>for some experiments we also use word-level information (lemmas) and part-of-speech tags.
</nextsent>
<nextsent>for some of the experiments reported here the variable context at iteration in algorithm 1 is instantiated with the predicates predicted at iterations i1 and i2.
</nextsent>
<nextsent>the experiments reported here correspond to the use of different features as input for the classifier.
</nextsent>
<nextsent>in these experiments we have used subset of the collected abstracts, they contain predicates which appeared at least 5 times in the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF167">
<title id=" W09-2807.xml">a classification algorithm for predicting the structure of summaries </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>predicates such as to present?
</prevsent>
<prevsent>andto include?
</prevsent>
</prevsection>
<citsent citstr=" A97-1042 ">
have the tendency of appearing towards the very beginning or the very end of the abstract been therefore predicted by position-based features (edmundson, 1969; lin and hovy, 1997).<papid> A97-1042 </papid></citsent>
<aftsection>
<nextsent>note that in this work we have decided to evaluate the predicted structure against the true structure (a hard evaluation measure), in future work we will assess the abstracts with set of quality questions similar to those put forward by the document understanding conference evaluations (also in way similar to (kan and mckeown, 2002) who evaluated their abstracts in retrieval environment).
</nextsent>
<nextsent>we expect to obtain reasonable evaluation result given that it appears that some of the predicates or phrases are interchangeable?
</nextsent>
<nextsent>(e.g., to contain?
</nextsent>
<nextsent>and to include?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF171">
<title id=" W09-2807.xml">a classification algorithm for predicting the structure of summaries </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous studies on text-to-text abstracting (banko et al., 2000; <papid> P00-1041 </papid>knight and marcu, 2000) have studied problems such as sentence compression and sentence combination but not the pasting?</prevsent>
<prevsent>procedure presented here.</prevsent>
</prevsection>
<citsent citstr=" N04-1015 ">
the insertion in the abstract of linguistic material not present in the input document has been addressed in paraphrase generation(barzilay and lee, 2004) <papid> N04-1015 </papid>and canned-based summarization (oakes and paice, 2001) in limited do mains.</citsent>
<aftsection>
<nextsent>saggion and lapalme (2002) <papid> J02-4005 </papid>have studied and implemented rule-based verb selection?</nextsent>
<nextsent>operation in their sumum system which has been applied to introduce document topics during indicative summary generation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF174">
<title id=" W09-2201.xml">coupling semi supervised learning of categories and relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>examples, use those seed examples totrain an initial model, then use this model to label some of the unlabeled data.
</prevsent>
<prevsent>the model is then retrained, using the original seed examples plus the self-labeled examples.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
this process ite rates, gradually expanding the amount of labeled data.such approaches have shown promise in applications such as web page classification (blum and mitchell, 1998), named entity classification (collins and singer, 1999), <papid> W99-0613 </papid>parsing (mcclosky et al, 2006), <papid> N06-1020 </papid>and machine translation (ueffing, 2006).bootstrapping approaches to information extraction can yield impressive results with little initial human effort (brin, 1998; agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>pasca et al,2006).</citsent>
<aftsection>
<nextsent>however, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept drifts?
</nextsent>
<nextsent>from what was intended (curran et al, 2007).
</nextsent>
<nextsent>coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (riloff and jones, 1999; yangarber, 2003).<papid> P03-1044 </papid></nextsent>
<nextsent>additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (pasca et al, 2006; rosenfeld and feldman, 2007).<papid> P07-1076 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF175">
<title id=" W09-2201.xml">coupling semi supervised learning of categories and relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>examples, use those seed examples totrain an initial model, then use this model to label some of the unlabeled data.
</prevsent>
<prevsent>the model is then retrained, using the original seed examples plus the self-labeled examples.
</prevsent>
</prevsection>
<citsent citstr=" N06-1020 ">
this process ite rates, gradually expanding the amount of labeled data.such approaches have shown promise in applications such as web page classification (blum and mitchell, 1998), named entity classification (collins and singer, 1999), <papid> W99-0613 </papid>parsing (mcclosky et al, 2006), <papid> N06-1020 </papid>and machine translation (ueffing, 2006).bootstrapping approaches to information extraction can yield impressive results with little initial human effort (brin, 1998; agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>pasca et al,2006).</citsent>
<aftsection>
<nextsent>however, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept drifts?
</nextsent>
<nextsent>from what was intended (curran et al, 2007).
</nextsent>
<nextsent>coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (riloff and jones, 1999; yangarber, 2003).<papid> P03-1044 </papid></nextsent>
<nextsent>additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (pasca et al, 2006; rosenfeld and feldman, 2007).<papid> P07-1076 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF176">
<title id=" W09-2201.xml">coupling semi supervised learning of categories and relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>examples, use those seed examples totrain an initial model, then use this model to label some of the unlabeled data.
</prevsent>
<prevsent>the model is then retrained, using the original seed examples plus the self-labeled examples.
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
this process ite rates, gradually expanding the amount of labeled data.such approaches have shown promise in applications such as web page classification (blum and mitchell, 1998), named entity classification (collins and singer, 1999), <papid> W99-0613 </papid>parsing (mcclosky et al, 2006), <papid> N06-1020 </papid>and machine translation (ueffing, 2006).bootstrapping approaches to information extraction can yield impressive results with little initial human effort (brin, 1998; agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>pasca et al,2006).</citsent>
<aftsection>
<nextsent>however, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept drifts?
</nextsent>
<nextsent>from what was intended (curran et al, 2007).
</nextsent>
<nextsent>coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (riloff and jones, 1999; yangarber, 2003).<papid> P03-1044 </papid></nextsent>
<nextsent>additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (pasca et al, 2006; rosenfeld and feldman, 2007).<papid> P07-1076 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF177">
<title id=" W09-2201.xml">coupling semi supervised learning of categories and relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept drifts?
</prevsent>
<prevsent>from what was intended (curran et al, 2007).
</prevsent>
</prevsection>
<citsent citstr=" P03-1044 ">
coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (riloff and jones, 1999; yangarber, 2003).<papid> P03-1044 </papid></citsent>
<aftsection>
<nextsent>additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (pasca et al, 2006; rosenfeld and feldman, 2007).<papid> P07-1076 </papid></nextsent>
<nextsent>our work builds on these ideas to couple the simultaneous boot strapped training of multiple categories and multiple relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF178">
<title id=" W09-2201.xml">coupling semi supervised learning of categories and relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>from what was intended (curran et al, 2007).
</prevsent>
<prevsent>coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (riloff and jones, 1999; yangarber, 2003).<papid> P03-1044 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1076 ">
additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (pasca et al, 2006; rosenfeld and feldman, 2007).<papid> P07-1076 </papid></citsent>
<aftsection>
<nextsent>our work builds on these ideas to couple the simultaneous boot strapped training of multiple categories and multiple relations.
</nextsent>
<nextsent>our approach to information extraction is based on using high precision contextual patterns (e.g., is mayor of arg1?
</nextsent>
<nextsent>suggests that arg1 is city).
</nextsent>
<nextsent>an earlypattern-based approach to information extraction acquired is a?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF179">
<title id=" W09-2201.xml">coupling semi supervised learning of categories and relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>suggests that arg1 is city).
</prevsent>
<prevsent>an earlypattern-based approach to information extraction acquired is a?
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
relations from text using generic contextual patterns (hearst, 1992).<papid> C92-2082 </papid></citsent>
<aftsection>
<nextsent>this approach was later scaled up to the web by etzioni et al (2005).
</nextsent>
<nextsent>2other research explores the task of open information extraction?, where the predicates to be learned are not specified in advance (shinyama and sekine, 2006; <papid> N06-1039 </papid>banko et al, 2007), but emerge instead from analysis of the data.</nextsent>
<nextsent>in contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF181">
<title id=" W09-2201.xml">coupling semi supervised learning of categories and relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>relations from text using generic contextual patterns (hearst, 1992).<papid> C92-2082 </papid></prevsent>
<prevsent>this approach was later scaled up to the web by etzioni et al (2005).</prevsent>
</prevsection>
<citsent citstr=" N06-1039 ">
2other research explores the task of open information extraction?, where the predicates to be learned are not specified in advance (shinyama and sekine, 2006; <papid> N06-1039 </papid>banko et al, 2007), but emerge instead from analysis of the data.</citsent>
<aftsection>
<nextsent>in contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy.
</nextsent>
<nextsent>chang et al (2007) <papid> P07-1036 </papid>present framework for learning that optimizes the data likelihood plus constraint-based penalty terms than capture prior knowledge, and demonstrate it with semi-supervised learning of segmentation models.</nextsent>
<nextsent>constraints that capture domain knowledge guide bootstrap learning of structured model by penalizing or disallowing violations of those constraints.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF182">
<title id=" W09-2201.xml">coupling semi supervised learning of categories and relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2other research explores the task of open information extraction?, where the predicates to be learned are not specified in advance (shinyama and sekine, 2006; <papid> N06-1039 </papid>banko et al, 2007), but emerge instead from analysis of the data.</prevsent>
<prevsent>in contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy.</prevsent>
</prevsection>
<citsent citstr=" P07-1036 ">
chang et al (2007) <papid> P07-1036 </papid>present framework for learning that optimizes the data likelihood plus constraint-based penalty terms than capture prior knowledge, and demonstrate it with semi-supervised learning of segmentation models.</citsent>
<aftsection>
<nextsent>constraints that capture domain knowledge guide bootstrap learning of structured model by penalizing or disallowing violations of those constraints.
</nextsent>
<nextsent>while similar in spirit, our work differs in that we consider learning many models, rather than one structured model, and that we are consider much larger scale application in different domain.
</nextsent>
<nextsent>4.1 coupling of predicates.
</nextsent>
<nextsent>as mentioned above, our approach hinges on the notion of coupling the learning of multiple functions in order to constrain the semi-supervised learning problem we face.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF185">
<title id=" W09-2604.xml">auto segmental representations in an hpsg of hausa </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W02-1502 ">
in this paper shall present treatment of lexical and grammatical tone and vowel length in hausa, as implemented in an emerging bidirectional hpsg of the language based on the lingo grammar matrix (bender et al, 2002).<papid> W02-1502 </papid></citsent>
<aftsection>
<nextsent>i shall argue in particular that systematic treatment of supra segmental phonology is indispensible in an implemented grammar of the language, both for theoretical and practicalreasons.
</nextsent>
<nextsent>i shall propose an lkb representation that is strongly inspired by linguistic and computational work on auto segmental phonology.
</nextsent>
<nextsent>finally, shall show that the specific implementation presented here is flexible enough to accommodate different levels of supra segmental information in the input.
</nextsent>
<nextsent>hausa is tone language spoken by over 30 million speakers in northern nigeria and bordering areas of niger.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF186">
<title id=" W09-2604.xml">auto segmental representations in an hpsg of hausa </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>used elsewhere (see jaggar (2006) for discussion of the use of the preterite in narratives).
</prevsent>
<prevsent>furthermore, hausa usesverb-final vowel length to signal presence of following in-situ direct object (hayes, 1990; crysmann, 2005).
</prevsent>
</prevsection>
<citsent citstr=" J94-3010 ">
despite the fact that the sophisticated models of supra segmental phonology developed more than quarter of century ago within auto segmental 28theory (goldsmith, 1976; leben, 1973) have already been rigorously formalised in the nineties in the context of feature-structure-based computational phonology (bird, 1995; scobbie, 1991; birdand klein, 1994; <papid> J94-3010 </papid>walther, 1999), the representation of tone and length has received little or no attention in the area of grammar engineering.</citsent>
<aftsection>
<nextsent>this may be partly due to the fact that the languages for which substantial grammars have been developed are not tone languages.
</nextsent>
<nextsent>existing grammar implementations of tone languages like chinese (fangand king, 2007) do not appear to make use of au to segmental models either, possibly because the assignment of tone in an isolating language is notas intimately connected to inflectional and deriva tional processes, as it is in morphologically rich language like hausa.
</nextsent>
<nextsent>in this paper, shall argue that the issue of supra segmental phonology is an integral part of any implemented grammar of hausa, not only from the point of view of linguistic adequacy, but also under grammar-engineering and application-oriented perspectives.
</nextsent>
<nextsent>i shall propose treatment of toneand length in an lkb-grammar of hausa that systematically builds on separate representations of segments, tone and length and discuss how various salient aspects of hausa syntax and morphology can be addressed using representation inspired by auto segmental theory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF187">
<title id=" W09-2604.xml">auto segmental representations in an hpsg of hausa </title>
<section> representing auto segmental phonology.  </section>
<citcontext>
<prevsection>
<prevsent>the very same goes for length.
</prevsent>
<prevsent>in order to enable the grammar to flexibly infer the meaning of these underspecified annotations, we introduce the following type hierarchy of tonal marking.
</prevsent>
</prevsection>
<citsent citstr=" L08-1024 ">
the only assumption made here is thatthe marking strategy being adopted is used consistently across the entire input sentence.6 lexical and grammatical tones will be one ofhigh, low, or fall.7 in addition to these three linguistic tones, the type hierarchy features tonal types that correspond to tonal annotations found in theinput: utone is the type associated with tonally unmarked syllables, tone_ is the type associated with5in the near future, we plan to supplant this two-step solution with direct conversion of using dia critical information into feature structure annotations, using the advanced token mapping developed by adolphs et al (2008).<papid> L08-1024 </papid></citsent>
<aftsection>
<nextsent>at present, however, this token-mapping has only been integrated into the pet run-time system (callmeier, 2000), but not yet into the lkb.
</nextsent>
<nextsent>6in principle, even this assumption can be relaxed, at the peril of having reduced cross-sentence disambiguation.
</nextsent>
<nextsent>7i do not decompose falling tone into hl sequences, thereby simplifying the alignment between tone specifications, length specifications and segments.a high-marking strategy, _tone corresponds to low marking, and _tone_ to full tonal marking (overt high and low).
</nextsent>
<nextsent>(15) tone _tone _utone _uhigh _tone_ _low_ _high_ _fall_ _low _high _fall utone utone_ ulow_ ufall tone_ low_ high_ fall_ low high fall depending on which annotations are present inthe input, the meaning of underspecified annotations can be determined on the basis of type inference.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF188">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>manual correction of such informative links can then be applied to create labeled dataset used by semi-supervised word alignment model.
</prevsent>
<prevsent>our experiments show that using active learning leads to maximal reduction of alignment error rates with reduced human effort.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the success of statistical approaches to machine translation (mt) can be attributed to the ibm models (brown et al, 1993) <papid> J93-2003 </papid>that characterize word level alignments in parallel corpora.</citsent>
<aftsection>
<nextsent>parameters of these alignment models are learnt in an unsupervised manner using the em algorithm over sentence-levelaligned parallel corpora.
</nextsent>
<nextsent>while the ease of automatically aligning sentences at the word-level with tools like giza++ (och and ney, 2003) <papid> J03-1002 </papid>has enabled fast development of statistical machine translation(smt) systems for various language pairs, the quality of alignment is typically quite low for language pairs that diverge from the independence assumptions made by the generative models.</nextsent>
<nextsent>also, an immense amount of parallel data enables better estimation of the model parameters, but large number of language pairs still lack parallel data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF190">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the success of statistical approaches to machine translation (mt) can be attributed to the ibm models (brown et al, 1993) <papid> J93-2003 </papid>that characterize word level alignments in parallel corpora.</prevsent>
<prevsent>parameters of these alignment models are learnt in an unsupervised manner using the em algorithm over sentence-levelaligned parallel corpora.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
while the ease of automatically aligning sentences at the word-level with tools like giza++ (och and ney, 2003) <papid> J03-1002 </papid>has enabled fast development of statistical machine translation(smt) systems for various language pairs, the quality of alignment is typically quite low for language pairs that diverge from the independence assumptions made by the generative models.</citsent>
<aftsection>
<nextsent>also, an immense amount of parallel data enables better estimation of the model parameters, but large number of language pairs still lack parallel data.
</nextsent>
<nextsent>two directions of research have been pursued for improving generative word alignment.
</nextsent>
<nextsent>the first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (cherry and lin, 2006).<papid> P06-2014 </papid></nextsent>
<nextsent>the second is to use extra annotation, typically word-levelhuman alignment for some sentence pairs, in conjunction with the parallel data to learn alignment in semi-supervised manner.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF192">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also, an immense amount of parallel data enables better estimation of the model parameters, but large number of language pairs still lack parallel data.
</prevsent>
<prevsent>two directions of research have been pursued for improving generative word alignment.
</prevsent>
</prevsection>
<citsent citstr=" P06-2014 ">
the first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (cherry and lin, 2006).<papid> P06-2014 </papid></citsent>
<aftsection>
<nextsent>the second is to use extra annotation, typically word-levelhuman alignment for some sentence pairs, in conjunction with the parallel data to learn alignment in semi-supervised manner.
</nextsent>
<nextsent>our research is in the direction of the latter, and aims to reduce the effort involved in hand-generation of word alignments by using active learning strategies for careful selection of word pairs to seek alignment.
</nextsent>
<nextsent>active learning for mt has not yet been explored to its full potential.
</nextsent>
<nextsent>much of the literature has explored one task ? selecting sentences to translate and add to the training corpus (haffari et al, 2009).<papid> N09-1047 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF193">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our research is in the direction of the latter, and aims to reduce the effort involved in hand-generation of word alignments by using active learning strategies for careful selection of word pairs to seek alignment.
</prevsent>
<prevsent>active learning for mt has not yet been explored to its full potential.
</prevsent>
</prevsection>
<citsent citstr=" N09-1047 ">
much of the literature has explored one task ? selecting sentences to translate and add to the training corpus (haffari et al, 2009).<papid> N09-1047 </papid></citsent>
<aftsection>
<nextsent>in this paper we explore active learning for word alignment, where the input to the active learner is sentence pair (sj1 , i1), present in two different languages = {s?} and = {t?}, and the annotation elicited from human is set of links {(j, i) : = 0 ? ?
</nextsent>
<nextsent>j ; = 0 ? ?
</nextsent>
<nextsent>i}.
</nextsent>
<nextsent>unlike previous approaches,our work does not require elicitation of full alignment for the sentence pair, which could be effort intensive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF194">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>many semi-supervised learning algorithms use co-trainingframework, which assumes that the dataset has multiple views, and training different classifiers on non-overlapping subset of these features provides additional labeled data (zhu, 2005).
</prevsent>
<prevsent>active query selection for training semi-supervised learning algorithm is an interesting method that has been applied to clustering problems.
</prevsent>
</prevsection>
<citsent citstr=" P09-1117 ">
tomanek and hahn (2009) <papid> P09-1117 </papid>applied active semi supervised learning tothe sequence-labeling problem.</citsent>
<aftsection>
<nextsent>tur et al (2005) describe active and semi-supervised learning methods for reducing labeling effort for spoken language understanding.
</nextsent>
<nextsent>they train supervised classification algorithms for the task of call classification and applyit to large unlabeled dataset to select the least confident instances for human labeling.researchers have begun to explore semi supervised word alignment models that use both labeled and unlabeled data.
</nextsent>
<nextsent>fraser and marcu (2006) <papid> P06-1097 </papid>pose the problem of alignment as search problem in log-linear space with features coming from the ibm alignment models.</nextsent>
<nextsent>the log-linear model is trained on the available labeled datato improve performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF195">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>tur et al (2005) describe active and semi-supervised learning methods for reducing labeling effort for spoken language understanding.
</prevsent>
<prevsent>they train supervised classification algorithms for the task of call classification and applyit to large unlabeled dataset to select the least confident instances for human labeling.researchers have begun to explore semi supervised word alignment models that use both labeled and unlabeled data.
</prevsent>
</prevsection>
<citsent citstr=" P06-1097 ">
fraser and marcu (2006) <papid> P06-1097 </papid>pose the problem of alignment as search problem in log-linear space with features coming from the ibm alignment models.</citsent>
<aftsection>
<nextsent>the log-linear model is trained on the available labeled datato improve performance.
</nextsent>
<nextsent>they propose semi supervised training algorithm which alternates between discriminative error training on the labeled data to learn the weighting parameters and maximum-likelihood em training on unlabeled data to estimate the parameters.
</nextsent>
<nextsent>callison-burch et al.
</nextsent>
<nextsent>(2004) also improve alignment by interpol ating human alignments with automatic alignments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF196">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(2004) also improve alignment by interpol ating human alignments with automatic alignments.
</prevsent>
<prevsent>they observe that while working with such datasets, alignments of higher quality should be given much higher weight than the lower-quality alignments.
</prevsent>
</prevsection>
<citsent citstr=" P06-2117 ">
wu et al (2006) <papid> P06-2117 </papid>learn separate models from labeled and unlabeled data using the standard em algo rithm.</citsent>
<aftsection>
<nextsent>the two models are then interpolated as learner in the semi-supervised ada boost algorithm to improve word alignment.
</nextsent>
<nextsent>active learning has been applied to various fields of natural language processing like statistical parsing, entity recognition among others (hwa, 2004; <papid> J04-3001 </papid>tang et al, 2001; shen et al, 2004).<papid> P04-1075 </papid></nextsent>
<nextsent>in case of mt, the potential of active learning has remained largely unexplored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF197">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>wu et al (2006) <papid> P06-2117 </papid>learn separate models from labeled and unlabeled data using the standard em algo rithm.</prevsent>
<prevsent>the two models are then interpolated as learner in the semi-supervised ada boost algorithm to improve word alignment.</prevsent>
</prevsection>
<citsent citstr=" J04-3001 ">
active learning has been applied to various fields of natural language processing like statistical parsing, entity recognition among others (hwa, 2004; <papid> J04-3001 </papid>tang et al, 2001; shen et al, 2004).<papid> P04-1075 </papid></citsent>
<aftsection>
<nextsent>in case of mt, the potential of active learning has remained largely unexplored.
</nextsent>
<nextsent>for statistical machine translation, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce costof data acquisition.
</nextsent>
<nextsent>recent work in this area discussed multiple query selection strategies forsta tistical phrase based translation system (haffari et al., 2009).<papid> N09-1047 </papid></nextsent>
<nextsent>their framework requires source text to be translated by the system and the translated data is used in self-training setting to train mt models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF198">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>wu et al (2006) <papid> P06-2117 </papid>learn separate models from labeled and unlabeled data using the standard em algo rithm.</prevsent>
<prevsent>the two models are then interpolated as learner in the semi-supervised ada boost algorithm to improve word alignment.</prevsent>
</prevsection>
<citsent citstr=" P04-1075 ">
active learning has been applied to various fields of natural language processing like statistical parsing, entity recognition among others (hwa, 2004; <papid> J04-3001 </papid>tang et al, 2001; shen et al, 2004).<papid> P04-1075 </papid></citsent>
<aftsection>
<nextsent>in case of mt, the potential of active learning has remained largely unexplored.
</nextsent>
<nextsent>for statistical machine translation, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce costof data acquisition.
</nextsent>
<nextsent>recent work in this area discussed multiple query selection strategies forsta tistical phrase based translation system (haffari et al., 2009).<papid> N09-1047 </papid></nextsent>
<nextsent>their framework requires source text to be translated by the system and the translated data is used in self-training setting to train mt models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF204">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> word alignment.  </section>
<citcontext>
<prevsection>
<prevsent>p (sj/ti) = ? count(ti, sj ; a?)?
</prevsent>
<prevsent>s count(ti) (8) (ti/sj) = ? count(ti, sj ; a?)?
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
s count(sj) (9) we perform all our experiments on symmetrizedalignment that combines the bidirectional alignments using heuristics as discussed in (koehn et al, 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>we represent this alignment as = {aij : = 0 ? ?
</nextsent>
<nextsent>j ? sj1 ; = 0 ? ?
</nextsent>
<nextsent>i ? i 1}.
</nextsent>
<nextsent>3.2 semi-supervised word alignment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF205">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> word alignment.  </section>
<citcontext>
<prevsection>
<prevsent>i ? i 1}.
</prevsent>
<prevsent>3.2 semi-supervised word alignment.
</prevsent>
</prevsection>
<citsent citstr=" W08-0509 ">
we use an extended version of mgiza++ (gaoand vogel, 2008) <papid> W08-0509 </papid>to perform the constrained semi supervised word alignment.</citsent>
<aftsection>
<nextsent>to get full benefit from the manual alignments, mgiza++ modifies all alignment models used in the standard training procedure, i.e. the ibm1, hmm, ibm3 and ibm4 models.
</nextsent>
<nextsent>manual alignments are incorporated in the em training phase of these models as constraints that restrict the summation over all possible alignmentpaths.
</nextsent>
<nextsent>typically in the em procedure for ibm models, the training procedure requires for each source sentence position, the summation over all position sin the target sentence.
</nextsent>
<nextsent>the manual alignments allow for one-to-many alignments and many-to-many alignments in both directions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF206">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> active learning for word alignment.  </section>
<citcontext>
<prevsection>
<prevsent>the word-level aligned labeled dataset is then provided to our semi-supervised word alignment algorithm, which uses it to produces the alignment model t+1 for . algorithm 1 al for word alignment 1: unlabeled data set: = {(sk, tk)} 2: manual alignment set : a0 = {akij ,si ? sk, tj ? tk} 3: train semi-supervised word alignment using (u , a0)?
</prevsent>
<prevsent>0 4: : batch size 5: for = 0 to do 6: lt = linkselection(u ,at,t,n ) 7: request human alignment for lt 8: at+1 = at + lt9: re-train semi-supervised word alignment on (u,at+1)?
</prevsent>
</prevsection>
<citsent citstr=" J07-3002 ">
t+1 10: end forwe can iteratively perform the algorithm forde fined number of iterations or until certain desired performance is reached, which is measured by alignment error rate (aer) (fraser and marcu, 2007) <papid> J07-3002 </papid>inthe case of word alignment.</citsent>
<aftsection>
<nextsent>in more typical scenario, since reducing human effort or cost of elicitation is the objective, we iterate until the available budget is exhausted.
</nextsent>
<nextsent>we propose multiple query selection strategies for our active learning setup.
</nextsent>
<nextsent>the scoring criteria is designed to select alignment links across sentence pairs that are highly uncertain under current automatic translation models.
</nextsent>
<nextsent>these links are difficult to align correctly by automatic alignment and will cause incorrect phrase pairs to be extracted inthe translation model, in turn hurting the translation quality of the smt system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF207">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> query strategies for link selection.  </section>
<citcontext>
<prevsection>
<prevsent>these lexicons capture the conditional distributions of source-giventarget (s/t) and target-given-source (t/s) probabilities at the word level where si ? and tj ? . we define certainty of link as the harmonic meanof the bidirectional probabilities.
</prevsent>
<prevsent>the selection strategy selects the least scoring links according to the formula below which corresponds to links with maximum uncertainty: score(aij/si1, j 1 ) = 2 ? (tj/si) ? (si/tj) (tj/si) + (si/tj) (10) 5.2 confidence based: posterior alignment.
</prevsent>
</prevsection>
<citsent citstr=" J07-1003 ">
probabilities confidence estimation for mt output is an interesting area with meaningful initial exploration (blatz 13et al, 2004; ueffing and ney, 2007).<papid> J07-1003 </papid></citsent>
<aftsection>
<nextsent>given sentence pair (si1, j1 ) and its word alignment, we compute two confidence metrics at alignment link level ? based on the posterior link probability and simple ibm model 1 as seen in equation 13.
</nextsent>
<nextsent>we select the alignment links that the initial word aligner is least confident according to our metric and seek manual correction of the links.
</nextsent>
<nextsent>we use t2s to denote computation using higher order (ibm4) target-given source models and s2t to denote source-given-target models.
</nextsent>
<nextsent>targeting some of the uncertain parts of word alignment has already been shown to improve translation quality in smt (huang, 2009).<papid> P09-1105 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF208">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> query strategies for link selection.  </section>
<citcontext>
<prevsection>
<prevsent>we select the alignment links that the initial word aligner is least confident according to our metric and seek manual correction of the links.
</prevsent>
<prevsent>we use t2s to denote computation using higher order (ibm4) target-given source models and s2t to denote source-given-target models.
</prevsent>
</prevsection>
<citsent citstr=" P09-1105 ">
targeting some of the uncertain parts of word alignment has already been shown to improve translation quality in smt (huang, 2009).<papid> P09-1105 </papid></citsent>
<aftsection>
<nextsent>in our current work, we use confidence metrics as an active learning sampling strategy to obtain most informative links.
</nextsent>
<nextsent>we also experiment with other confidence metrics as discussed in (ueffing and ney, 2007), <papid> J07-1003 </papid>especially the ibm 1 model score metric which showed some improvement as well.</nextsent>
<nextsent>pt2s(aij , tj1 /s 1) = pt2s(tj/si, aij ? a) i pt2s(tj/si) (11) ps2t(aij , si1/t 1 ) = ps2t(si/tj , aij ? a) i pt2s(tj/si) (12) conf(aij/s, ) = 2 ? pt2s ? ps2t pt2s + ps2t (13) 5.3 agreement based: query by committee.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF213">
<title id=" W10-0102.xml">active semi supervised learning for improving word alignment </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we now take our best link selection criteria, which is the confidence based method and re-train the mt system after eliciting manual information for only 20% of the alignment links.
</prevsent>
<prevsent>we observe that at this point we have reduced the aer from 37.09 to 26.57.
</prevsent>
</prevsection>
<citsent citstr=" W07-0734 ">
the translation accuracy reported in table 3, as measured by bleu (papineni et al, 2002) and meteor (lavieand agarwal, 2007), <papid> W07-0734 </papid>also shows significant improvement and approaches the quality achieved using gold standard data.</citsent>
<aftsection>
<nextsent>we did not perform mt experiments with arabic-english dataset due to the incompatibility of tokenization schemes between the manually aligned parallel corpora and publicly available evaluation sets.
</nextsent>
<nextsent>word-alignment is particularly challenging problem and has been addressed incompletely unsupervised manner thus far (brown et al, 1993).<papid> J93-2003 </papid></nextsent>
<nextsent>while generative alignment models have been successful,lack of sufficient data, model assumptions and local optimum during training are well known problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF216">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in machine translation, the evaluation tool most commonly used for evaluating out put, the bleu score (papineni et al, 2001), rates the goodness?
</prevsent>
<prevsent>of output based on n-gram overlap with human-generated text.
</prevsent>
</prevsection>
<citsent citstr=" W07-0736 ">
however this metric has been criticized for not accurately measuring the fluency of text and there is active research into other metrics (callison-burch et al, 2006; ye et al, 2007).<papid> W07-0736 </papid>previous studies suggest that text simplified forma chine and human comprehension are categorically different (chae and nenkova, 2009).</citsent>
<aftsection>
<nextsent>our research considers text simplified for human readers, but the findings can be used to identify features that discriminate simple text for both applications.the process of ts can be divided into three aspects: removing extraneous or superfluous text, substituting more complex lexical and syntactic forms,and inserting information to offer further clarification where needed (alusio et al, 2008).
</nextsent>
<nextsent>in this regard, ts is related to several different natural language processing tasks such as text summarization,compression, machine translation, and paraphrasing.
</nextsent>
<nextsent>while none of these tasks alone directly provide solution to text simplification, techniques can be drawn from each.
</nextsent>
<nextsent>summarization techniques can be used to identify the crucial, most informative parts of text and compression can be used to remove superfluous words and phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF217">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> wikipedia as corpus.  </section>
<citcontext>
<prevsection>
<prevsent>this corpus was designed specifically for xml retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. yawn (schenkel et al, 2007), awikipedia xml corpus with semantic tags, is an other example of exploiting wikipedias structural information.
</prevsent>
<prevsent>wikipedia provides xml site dumps every few weeks in all languages as well as static html dumps.
</prevsent>
</prevsection>
<citsent citstr=" N07-1025 ">
a diverse array of nlp research in the past few years has used wikipedia, such as for word sense disambiguation (mihalcea, 2007), <papid> N07-1025 </papid>classification (gantner and schmidt-thieme, 2009), machine translation (smith et al, 2010), coreference resolution (versley et al, 2008; <papid> L08-1328 </papid>yang and su, 2007), sentence extraction for summarization (biadsy et al, 2008), <papid> P08-1092 </papid>information retrieval (muller and gurevych, 2008), and semantic role labeling (ponzetto and strube, 2006), <papid> N06-1025 </papid>to name few.</citsent>
<aftsection>
<nextsent>however, except for very recent work by yatskar et al (2010), to our knowledge there has not been comparable research in using wikipedia for text simplification.
</nextsent>
<nextsent>43 ordinary wikipedia hawking was the luc asian professor of mathematics at the university of cambridge for thirty years, taking up the post in 1979 and retiring on 1 october 2009.
</nextsent>
<nextsent>simple wikipedia hawking was professor of mathematics at the university of cambridge (a position that isaac newton once had).
</nextsent>
<nextsent>he retired on october 1st 2009.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF218">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> wikipedia as corpus.  </section>
<citcontext>
<prevsection>
<prevsent>this corpus was designed specifically for xml retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. yawn (schenkel et al, 2007), awikipedia xml corpus with semantic tags, is an other example of exploiting wikipedias structural information.
</prevsent>
<prevsent>wikipedia provides xml site dumps every few weeks in all languages as well as static html dumps.
</prevsent>
</prevsection>
<citsent citstr=" L08-1328 ">
a diverse array of nlp research in the past few years has used wikipedia, such as for word sense disambiguation (mihalcea, 2007), <papid> N07-1025 </papid>classification (gantner and schmidt-thieme, 2009), machine translation (smith et al, 2010), coreference resolution (versley et al, 2008; <papid> L08-1328 </papid>yang and su, 2007), sentence extraction for summarization (biadsy et al, 2008), <papid> P08-1092 </papid>information retrieval (muller and gurevych, 2008), and semantic role labeling (ponzetto and strube, 2006), <papid> N06-1025 </papid>to name few.</citsent>
<aftsection>
<nextsent>however, except for very recent work by yatskar et al (2010), to our knowledge there has not been comparable research in using wikipedia for text simplification.
</nextsent>
<nextsent>43 ordinary wikipedia hawking was the luc asian professor of mathematics at the university of cambridge for thirty years, taking up the post in 1979 and retiring on 1 october 2009.
</nextsent>
<nextsent>simple wikipedia hawking was professor of mathematics at the university of cambridge (a position that isaac newton once had).
</nextsent>
<nextsent>he retired on october 1st 2009.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF219">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> wikipedia as corpus.  </section>
<citcontext>
<prevsection>
<prevsent>this corpus was designed specifically for xml retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. yawn (schenkel et al, 2007), awikipedia xml corpus with semantic tags, is an other example of exploiting wikipedias structural information.
</prevsent>
<prevsent>wikipedia provides xml site dumps every few weeks in all languages as well as static html dumps.
</prevsent>
</prevsection>
<citsent citstr=" P08-1092 ">
a diverse array of nlp research in the past few years has used wikipedia, such as for word sense disambiguation (mihalcea, 2007), <papid> N07-1025 </papid>classification (gantner and schmidt-thieme, 2009), machine translation (smith et al, 2010), coreference resolution (versley et al, 2008; <papid> L08-1328 </papid>yang and su, 2007), sentence extraction for summarization (biadsy et al, 2008), <papid> P08-1092 </papid>information retrieval (muller and gurevych, 2008), and semantic role labeling (ponzetto and strube, 2006), <papid> N06-1025 </papid>to name few.</citsent>
<aftsection>
<nextsent>however, except for very recent work by yatskar et al (2010), to our knowledge there has not been comparable research in using wikipedia for text simplification.
</nextsent>
<nextsent>43 ordinary wikipedia hawking was the luc asian professor of mathematics at the university of cambridge for thirty years, taking up the post in 1979 and retiring on 1 october 2009.
</nextsent>
<nextsent>simple wikipedia hawking was professor of mathematics at the university of cambridge (a position that isaac newton once had).
</nextsent>
<nextsent>he retired on october 1st 2009.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF220">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> wikipedia as corpus.  </section>
<citcontext>
<prevsection>
<prevsent>this corpus was designed specifically for xml retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. yawn (schenkel et al, 2007), awikipedia xml corpus with semantic tags, is an other example of exploiting wikipedias structural information.
</prevsent>
<prevsent>wikipedia provides xml site dumps every few weeks in all languages as well as static html dumps.
</prevsent>
</prevsection>
<citsent citstr=" N06-1025 ">
a diverse array of nlp research in the past few years has used wikipedia, such as for word sense disambiguation (mihalcea, 2007), <papid> N07-1025 </papid>classification (gantner and schmidt-thieme, 2009), machine translation (smith et al, 2010), coreference resolution (versley et al, 2008; <papid> L08-1328 </papid>yang and su, 2007), sentence extraction for summarization (biadsy et al, 2008), <papid> P08-1092 </papid>information retrieval (muller and gurevych, 2008), and semantic role labeling (ponzetto and strube, 2006), <papid> N06-1025 </papid>to name few.</citsent>
<aftsection>
<nextsent>however, except for very recent work by yatskar et al (2010), to our knowledge there has not been comparable research in using wikipedia for text simplification.
</nextsent>
<nextsent>43 ordinary wikipedia hawking was the luc asian professor of mathematics at the university of cambridge for thirty years, taking up the post in 1979 and retiring on 1 october 2009.
</nextsent>
<nextsent>simple wikipedia hawking was professor of mathematics at the university of cambridge (a position that isaac newton once had).
</nextsent>
<nextsent>he retired on october 1st 2009.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF221">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> wikipedia as corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the experiments that follow randomly extract documents and sentences from this collection.before extracting features, we ran series of natural language processing tools to pre process the collection.
</prevsent>
<prevsent>first, all of the xml and wiki markup was removed.
</prevsent>
</prevsection>
<citsent citstr=" J06-4003 ">
each document was split into sentences using the punkt sentence tokenizer (kiss and strunk, 2006) <papid> J06-4003 </papid>in nltk (bird and loper, 2004).<papid> P04-3031 </papid></citsent>
<aftsection>
<nextsent>we then parsed each sentence using the pcfg parser of huang and harper (2009), modified version of the berkeley parser (petrov et al, 2006; <papid> P06-1055 </papid>petrov and klein, 2007), <papid> N07-1051 </papid>for the tree structure and part-of speech tags.</nextsent>
<nextsent>to evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF222">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> wikipedia as corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the experiments that follow randomly extract documents and sentences from this collection.before extracting features, we ran series of natural language processing tools to pre process the collection.
</prevsent>
<prevsent>first, all of the xml and wiki markup was removed.
</prevsent>
</prevsection>
<citsent citstr=" P04-3031 ">
each document was split into sentences using the punkt sentence tokenizer (kiss and strunk, 2006) <papid> J06-4003 </papid>in nltk (bird and loper, 2004).<papid> P04-3031 </papid></citsent>
<aftsection>
<nextsent>we then parsed each sentence using the pcfg parser of huang and harper (2009), modified version of the berkeley parser (petrov et al, 2006; <papid> P06-1055 </papid>petrov and klein, 2007), <papid> N07-1051 </papid>for the tree structure and part-of speech tags.</nextsent>
<nextsent>to evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF223">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> wikipedia as corpus.  </section>
<citcontext>
<prevsection>
<prevsent>first, all of the xml and wiki markup was removed.
</prevsent>
<prevsent>each document was split into sentences using the punkt sentence tokenizer (kiss and strunk, 2006) <papid> J06-4003 </papid>in nltk (bird and loper, 2004).<papid> P04-3031 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
we then parsed each sentence using the pcfg parser of huang and harper (2009), modified version of the berkeley parser (petrov et al, 2006; <papid> P06-1055 </papid>petrov and klein, 2007), <papid> N07-1051 </papid>for the tree structure and part-of speech tags.</citsent>
<aftsection>
<nextsent>to evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes.
</nextsent>
<nextsent>using the two document collections, we constructed simple binary classification task: label piece of text as either simple or ordinary.
</nextsent>
<nextsent>the text was labeled according to its source: simple or ordinary wikipedia.from each piece of text, we extracted set of features designed to capture differences between the texts, using cognitively motivated features based on documents lexical, syntactic, and surface features.we first describe our features and then our experimental setup.
</nextsent>
<nextsent>44
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF224">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> wikipedia as corpus.  </section>
<citcontext>
<prevsection>
<prevsent>first, all of the xml and wiki markup was removed.
</prevsent>
<prevsent>each document was split into sentences using the punkt sentence tokenizer (kiss and strunk, 2006) <papid> J06-4003 </papid>in nltk (bird and loper, 2004).<papid> P04-3031 </papid></prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
we then parsed each sentence using the pcfg parser of huang and harper (2009), modified version of the berkeley parser (petrov et al, 2006; <papid> P06-1055 </papid>petrov and klein, 2007), <papid> N07-1051 </papid>for the tree structure and part-of speech tags.</citsent>
<aftsection>
<nextsent>to evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes.
</nextsent>
<nextsent>using the two document collections, we constructed simple binary classification task: label piece of text as either simple or ordinary.
</nextsent>
<nextsent>the text was labeled according to its source: simple or ordinary wikipedia.from each piece of text, we extracted set of features designed to capture differences between the texts, using cognitively motivated features based on documents lexical, syntactic, and surface features.we first describe our features and then our experimental setup.
</nextsent>
<nextsent>44
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF225">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>idioms should be avoid edas well as compounds and the passive voice as opposed to single simple verb.
</prevsent>
<prevsent>to capture these properties in the text, we created four classes of features: lexical, part-of-speech, surface, and parse.
</prevsent>
</prevsection>
<citsent citstr=" E09-1027 ">
several of our features have previously been used for measuring text fluency (alusio et al, 2008; chae and nenkova, 2009; feng et al, 2009; <papid> E09-1027 </papid>petersen and ostendorf, 2007).lexical.</citsent>
<aftsection>
<nextsent>previous work by feng et al (2009) <papid> E09-1027 </papid>suggests that the document vocabulary is good predictor of document readability.</nextsent>
<nextsent>simple texts are more likely to use basic words more often as opposed to more complicated, domain-specific words used in ordinary texts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF227">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>we also considered relative ratios, such as the ratio of noun to verb phrases, prepositional to noun phrases, and relative clauses to noun phrases.
</prevsent>
<prevsent>we used the length of the longest noun phrase as signal of complexity, and we also sought features that measured how typical the sentences were of english text.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
we included some of the features from the parser reranking work of charniak and johnson (2005): <papid> P05-1022 </papid>the height of the parse tree and the number of right branches from the root of the tree to the furthest right leaf that is not punctuation.</citsent>
<aftsection>
<nextsent>using the feature sets described above, we evaluated simple/ordinary text classifier in several settings on each category.
</nextsent>
<nextsent>first, we considered the taskof document classification, where classifier determines whether full wikipedia article was from ordinary english wikipedia or simple wikipedia.for each category of articles, we measured accuracy on this binary classification task using 10-foldcross-validation.
</nextsent>
<nextsent>in the second setting, we consid category documents sentences everyday life 15,124 7,392 geography 10,470 5,852 history 5,174 1,644 literature 992 438 media 502 429 people 4,326 1,562 religion 1,863 1,581 science 25,787 21,054 all 64,238 39,952table 5: the number of examples available in each category.
</nextsent>
<nextsent>to compare experiments in each category we used at most 2000 instances in each experiment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF228">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we report the accuracy on each category in these transfer experiments.for learning we require binary classifier training algorithm.
</prevsent>
<prevsent>we evaluated several learning algorithms for classification and report results for eachone: a) miraa large margin online learning algorithm (crammer et al, 2006).
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
online learning algorithms observe examples sequentially and up 46 date the current hypothesis after each observation; b)confidence weighted (cw) learn inga probabilistic large margin online learning algorithm (dredze etal., 2008); c) maximum entropya log-linear discriminative classifier (berger et al, 1996); <papid> J96-1002 </papid>and d) support vector machines (svm)a large margin discriminator (joachims, 1998).</citsent>
<aftsection>
<nextsent>for each experiment, we used default settings of the parameters and 10 online iterations for the online methods (mira, cw).
</nextsent>
<nextsent>to create fair comparison for each category, we limited the number of examples to maximum of 2000.
</nextsent>
<nextsent>for the first task of document classification, we sawat least 90% mean accuracy with each of the classifiers.
</nextsent>
<nextsent>using all features, svm and maximum entropy performed almost perfectly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF229">
<title id=" W10-0406.xml">learning simple wikipedia a cogitation in ascertaining abecedarian language </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, alusio et al (2008) claim that sentences that are easier to read are also easier to parse, so the entropy of the parser or confidence in the output may be indicative of texts difficulty.
</prevsent>
<prevsent>additionally, language models trained on large corpora can assign probability scores to texts, which may indicate text difficulty.
</prevsent>
</prevsection>
<citsent citstr=" D09-1116 ">
of particular interest are syntactic language models that incorporate some of the syntactic observations in this paper (filimonov and harper, 2009).<papid> D09-1116 </papid></citsent>
<aftsection>
<nextsent>our next goal will be to look at parallel sentence sto learn rules for simplifying text.
</nextsent>
<nextsent>one of the advantages of the wikipedia collection is the parallel articles in ordinary english wikipedia and simplewikipedia.
</nextsent>
<nextsent>while the content of the articles can differ, these are excellent examples of comparable texts that can be useful for learning simplification rules.
</nextsent>
<nextsent>such learning can draw from machine translation, which learns rules that translate between languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF230">
<title id=" W09-1906.xml">a web survey on the use of active learning to support annotation of text data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the active learning process takes as input set of labeled examples, as well as larger set of unlabeled examples, and produces classifier and relatively small set of newly labeled data.
</prevsent>
<prevsent>the overall goal is to create as good classifier as possible, without having to mark-up and supply the learner with more data than necessary.
</prevsent>
</prevsection>
<citsent citstr=" W07-1516 ">
al aimsat keeping the human annotation effort to minimum, only asking the oracle for advice where the training utility of the result of such query is high.settles (2009) gives detailed overview of the literature on al.it has been experimentally shown that alcan indeed be successfully applied to range of nlp tasks including, e.g., text categorization (lewis and gale, 1994), part-of-speech tagging (dagan and engelson,1995; ringger et al, 2007), <papid> W07-1516 </papid>parsing (becker and osborne, 2005), and named entity recognition (shen etal., 2004; <papid> P04-1075 </papid>tomanek et al, 2007).<papid> D07-1051 </papid></citsent>
<aftsection>
<nextsent>despite that somewhat impressive results in terms of reduced annotation effort have been achieved by such studies, itseems that al is rarely applied in real-life annotation endeavors.
</nextsent>
<nextsent>this paper presents the results from web survey we arranged to analyze the extent to which al has been used to support the annotation of textual data in the context of nlp, as well as addressing the reasons to why or why not al has been found applicable to aspecific task.
</nextsent>
<nextsent>section 2 describes the survey in general, section 3 introduces the questions and presents the answers received.
</nextsent>
<nextsent>finally, the answers received are discussed in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF231">
<title id=" W09-1906.xml">a web survey on the use of active learning to support annotation of text data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the active learning process takes as input set of labeled examples, as well as larger set of unlabeled examples, and produces classifier and relatively small set of newly labeled data.
</prevsent>
<prevsent>the overall goal is to create as good classifier as possible, without having to mark-up and supply the learner with more data than necessary.
</prevsent>
</prevsection>
<citsent citstr=" P04-1075 ">
al aimsat keeping the human annotation effort to minimum, only asking the oracle for advice where the training utility of the result of such query is high.settles (2009) gives detailed overview of the literature on al.it has been experimentally shown that alcan indeed be successfully applied to range of nlp tasks including, e.g., text categorization (lewis and gale, 1994), part-of-speech tagging (dagan and engelson,1995; ringger et al, 2007), <papid> W07-1516 </papid>parsing (becker and osborne, 2005), and named entity recognition (shen etal., 2004; <papid> P04-1075 </papid>tomanek et al, 2007).<papid> D07-1051 </papid></citsent>
<aftsection>
<nextsent>despite that somewhat impressive results in terms of reduced annotation effort have been achieved by such studies, itseems that al is rarely applied in real-life annotation endeavors.
</nextsent>
<nextsent>this paper presents the results from web survey we arranged to analyze the extent to which al has been used to support the annotation of textual data in the context of nlp, as well as addressing the reasons to why or why not al has been found applicable to aspecific task.
</nextsent>
<nextsent>section 2 describes the survey in general, section 3 introduces the questions and presents the answers received.
</nextsent>
<nextsent>finally, the answers received are discussed in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF232">
<title id=" W09-1906.xml">a web survey on the use of active learning to support annotation of text data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the active learning process takes as input set of labeled examples, as well as larger set of unlabeled examples, and produces classifier and relatively small set of newly labeled data.
</prevsent>
<prevsent>the overall goal is to create as good classifier as possible, without having to mark-up and supply the learner with more data than necessary.
</prevsent>
</prevsection>
<citsent citstr=" D07-1051 ">
al aimsat keeping the human annotation effort to minimum, only asking the oracle for advice where the training utility of the result of such query is high.settles (2009) gives detailed overview of the literature on al.it has been experimentally shown that alcan indeed be successfully applied to range of nlp tasks including, e.g., text categorization (lewis and gale, 1994), part-of-speech tagging (dagan and engelson,1995; ringger et al, 2007), <papid> W07-1516 </papid>parsing (becker and osborne, 2005), and named entity recognition (shen etal., 2004; <papid> P04-1075 </papid>tomanek et al, 2007).<papid> D07-1051 </papid></citsent>
<aftsection>
<nextsent>despite that somewhat impressive results in terms of reduced annotation effort have been achieved by such studies, itseems that al is rarely applied in real-life annotation endeavors.
</nextsent>
<nextsent>this paper presents the results from web survey we arranged to analyze the extent to which al has been used to support the annotation of textual data in the context of nlp, as well as addressing the reasons to why or why not al has been found applicable to aspecific task.
</nextsent>
<nextsent>section 2 describes the survey in general, section 3 introduces the questions and presents the answers received.
</nextsent>
<nextsent>finally, the answers received are discussed in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF233">
<title id=" W10-0301.xml">automatic conjugation and identification of regular and irregular verb neologisms in spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the operation of these processes mean that each spanish verb can comprise 135 different forms, including compound verb forms.
</prevsent>
<prevsent>1onoma can be accessed at http://conjugador.onoma.es several researchers have developed tools and methods related to spanish verbs.
</prevsent>
</prevsection>
<citsent citstr=" C90-3049 ">
these include morphological processors (tzoukermann and liberman, 1990), (<papid> C90-3049 </papid>santana et al, 1997), (santana et al, 2002), semantic verb classification (esteve ferrer, 2004) <papid> P04-2007 </papid>or verb sense disambiguation (lapata and brew, 2004).<papid> J04-1003 </papid></citsent>
<aftsection>
<nextsent>nevertheless, to our knowledge, ours is the first attempt to automatically identify, classify and conjugate new spanish verbs.
</nextsent>
<nextsent>our method identifies new and existing spanish verbs and categorises them into seven classes: one class for regular verbs and six classes of irregular verbs depending on the type of the irregularity rule whose operation produced it.
</nextsent>
<nextsent>this algorithm is implemented by means of six modules or transducers which process each new infinitive form and classify the neologism.
</nextsent>
<nextsent>once the new infinitive is classified, it is conjugated by the system using set of high accuracy conjugation rules according to its class.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF234">
<title id=" W10-0301.xml">automatic conjugation and identification of regular and irregular verb neologisms in spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the operation of these processes mean that each spanish verb can comprise 135 different forms, including compound verb forms.
</prevsent>
<prevsent>1onoma can be accessed at http://conjugador.onoma.es several researchers have developed tools and methods related to spanish verbs.
</prevsent>
</prevsection>
<citsent citstr=" P04-2007 ">
these include morphological processors (tzoukermann and liberman, 1990), (<papid> C90-3049 </papid>santana et al, 1997), (santana et al, 2002), semantic verb classification (esteve ferrer, 2004) <papid> P04-2007 </papid>or verb sense disambiguation (lapata and brew, 2004).<papid> J04-1003 </papid></citsent>
<aftsection>
<nextsent>nevertheless, to our knowledge, ours is the first attempt to automatically identify, classify and conjugate new spanish verbs.
</nextsent>
<nextsent>our method identifies new and existing spanish verbs and categorises them into seven classes: one class for regular verbs and six classes of irregular verbs depending on the type of the irregularity rule whose operation produced it.
</nextsent>
<nextsent>this algorithm is implemented by means of six modules or transducers which process each new infinitive form and classify the neologism.
</nextsent>
<nextsent>once the new infinitive is classified, it is conjugated by the system using set of high accuracy conjugation rules according to its class.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF235">
<title id=" W10-0301.xml">automatic conjugation and identification of regular and irregular verb neologisms in spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the operation of these processes mean that each spanish verb can comprise 135 different forms, including compound verb forms.
</prevsent>
<prevsent>1onoma can be accessed at http://conjugador.onoma.es several researchers have developed tools and methods related to spanish verbs.
</prevsent>
</prevsection>
<citsent citstr=" J04-1003 ">
these include morphological processors (tzoukermann and liberman, 1990), (<papid> C90-3049 </papid>santana et al, 1997), (santana et al, 2002), semantic verb classification (esteve ferrer, 2004) <papid> P04-2007 </papid>or verb sense disambiguation (lapata and brew, 2004).<papid> J04-1003 </papid></citsent>
<aftsection>
<nextsent>nevertheless, to our knowledge, ours is the first attempt to automatically identify, classify and conjugate new spanish verbs.
</nextsent>
<nextsent>our method identifies new and existing spanish verbs and categorises them into seven classes: one class for regular verbs and six classes of irregular verbs depending on the type of the irregularity rule whose operation produced it.
</nextsent>
<nextsent>this algorithm is implemented by means of six modules or transducers which process each new infinitive form and classify the neologism.
</nextsent>
<nextsent>once the new infinitive is classified, it is conjugated by the system using set of high accuracy conjugation rules according to its class.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF236">
<title id=" W09-2408.xml">meeting tempeval2 shallow approach for temporal tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a complex approach achieved an f1 measure value of 0.817 for exact match and 0.896 for detecting sloopy?
</prevsent>
<prevsent>spans.
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
another known implementation for time bank is an adaptation of (mani and wilson, 2000) <papid> P00-1010 </papid>from timex2 to timex3 with no reported performance level.</citsent>
<aftsection>
<nextsent>machine learning recognition systems: successful machine learning timex recognition systems are described in (ahn et al, 2005; hacioglu et al, 2005; poveda et al, 2007).
</nextsent>
<nextsent>proposed approaches made use of token-by-token classification for temporal expressions represented by b-i-o encoding with set of lexical and syntactic features, e.g., token itself, part-of-speech tag, label in the chunk phrase and the same features for each token in the context window.
</nextsent>
<nextsent>the performance levels are presented in table 1.
</nextsent>
<nextsent>all the results were obtained on the ace tern dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF237">
<title id=" W09-2408.xml">meeting tempeval2 shallow approach for temporal tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>all the results were obtained on the ace tern dataset.
</prevsent>
<prevsent>approach f1 (detection) f1 (exact match) ahn et al, 2005 0.914 0.798 hacioglu et al, 2005 0.935 0.878 poveda et al, 2007 0.986 0.757 table 1.
</prevsent>
</prevsection>
<citsent citstr=" N07-1053 ">
performance of machine learning approaches with b-i-o encoding constituent-based classification approach for temporal expression recognition was presented in (ahn et al, 2007).<papid> N07-1053 </papid></citsent>
<aftsection>
<nextsent>by comparing to the previous work (ahn et al, 2005) on the same ace tern dataset, the method demonstrates slight decrease in detection with f1-measure of 0.844 and nearly equivalent f1-measure value for exact match of 0.787.
</nextsent>
<nextsent>54 the major characteristic of machine learning approaches was simple system design with minimal human effort.
</nextsent>
<nextsent>machine-learning based recognition systems have proven to have comparable recognition performance level to state of-the-art rule-based detectors.
</nextsent>
<nextsent>the approach we describe in this section employs machine-learning technique and more specifically binary constituent based classification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF239">
<title id=" W10-0719.xml">crowdsourcing and language studies the new generation of linguistic data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for these, we report number of successful methods for evaluating data quality in the absence of correct?
</prevsent>
<prevsent>response for any given data point.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
crowdsourcings greatest contribution to language studies might be the ability to generate new kinds of data, especially within experimental paradigms.the speed and cost benefits for annotation are certainly impressive (snow et al, 2008; <papid> D08-1027 </papid>callison burch, 2009; hsueh et al, 2009) <papid> W09-1904 </papid>but we hope to show that some of the greatest gains are in the very nature of the phenomena that we can now study.</citsent>
<aftsection>
<nextsent>for psycho linguistic experiments in particular, weare not so much utilizing artificial artificial?
</nextsent>
<nextsent>intelligence as the plain intelligence and linguistic intuitions of each crowd sourced worker ? the voices in the crowd?, so to speak.
</nextsent>
<nextsent>in many experiments we are studying gradient phenomena where there are no right answers.
</nextsent>
<nextsent>even when there is binary response we are often interested in the distribution of responses over many speakers rather than specific data points.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF240">
<title id=" W10-0719.xml">crowdsourcing and language studies the new generation of linguistic data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for these, we report number of successful methods for evaluating data quality in the absence of correct?
</prevsent>
<prevsent>response for any given data point.
</prevsent>
</prevsection>
<citsent citstr=" W09-1904 ">
crowdsourcings greatest contribution to language studies might be the ability to generate new kinds of data, especially within experimental paradigms.the speed and cost benefits for annotation are certainly impressive (snow et al, 2008; <papid> D08-1027 </papid>callison burch, 2009; hsueh et al, 2009) <papid> W09-1904 </papid>but we hope to show that some of the greatest gains are in the very nature of the phenomena that we can now study.</citsent>
<aftsection>
<nextsent>for psycho linguistic experiments in particular, weare not so much utilizing artificial artificial?
</nextsent>
<nextsent>intelligence as the plain intelligence and linguistic intuitions of each crowd sourced worker ? the voices in the crowd?, so to speak.
</nextsent>
<nextsent>in many experiments we are studying gradient phenomena where there are no right answers.
</nextsent>
<nextsent>even when there is binary response we are often interested in the distribution of responses over many speakers rather than specific data points.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF241">
<title id=" W10-0206.xml">a text driven rule based system for emotion cause detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the current study should lay the ground for future research on the inferences of implicit information and the discovery of new information based on cause-event relation.
</prevsent>
<prevsent>text-based emotion processing has attracted plenty of attention in nlp.
</prevsent>
</prevsection>
<citsent citstr=" C08-1111 ">
most research has focused on the emotion detection and classification by identifying the emotion types, for instances happiness and sadness, forgiven sentence or document (alm 2005, mihalcea and liu 2006, tokuhisa et al  2008).<papid> C08-1111 </papid></citsent>
<aftsection>
<nextsent>however, on top of this surface level information, deeper level information regarding emotions, such as the experiencer, cause, and result of an emotion, needs to be extracted and analyzed for real world applications (alm 2009).
</nextsent>
<nextsent>in this paper, we aim at mining one of the crucial deep level types of information, i.e. emotion cause, which provides useful information for applications ranging from economic forecasting, public opinion mining, to product design.
</nextsent>
<nextsent>emotion cause detection is new research area in emotion processing.
</nextsent>
<nextsent>in emotion processing, the cause event and emotion correlation is fertile ground for extraction and entailment of new information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF242">
<title id=" W10-0206.xml">a text driven rule based system for emotion cause detection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 emotion processing in text.
</prevsent>
<prevsent>textual emotion processing is still in its early stages in nlp.
</prevsent>
</prevsection>
<citsent citstr=" W07-2072 ">
most of the previous works focus on emotion classification given known emotion context such as sentence or document using either rule-based (masum et al  2007, chaumartin 2007) or statistical approaches (mihalcea and liu 2005, kozareva et al  2007).<papid> W07-2072 </papid></citsent>
<aftsection>
<nextsent>however, the performance is far from satisfactory.
</nextsent>
<nextsent>what is more, many basic issues remain unresolved, for instances, the relationships among emotions, emotion type selection, etc. tokuhisa et al  (2008) <papid> C08-1111 </papid>was the first to explore both the issues of emotion detection and classifica tion.</nextsent>
<nextsent>it created japanese emotion-provoking event corpus for an emotion classification task using an unsupervised approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF244">
<title id=" W10-0206.xml">a text driven rule based system for emotion cause detection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>it created japanese emotion-provoking event corpus for an emotion classification task using an unsupervised approach.
</prevsent>
<prevsent>however, only 49.4% of cases were correctly labeled.
</prevsent>
</prevsection>
<citsent citstr=" W09-3001 ">
chen et al  (2009) <papid> W09-3001 </papid>developed two cognitive-based chinese emotion corpora using semi-unsupervised approach, i.e. an emotion-sentence (sentences containing emotions) corpus and neutral-sentence (sentences containing no emotion) corpus.</citsent>
<aftsection>
<nextsent>they showed that studies based on the emotion-sentence corpus (~70%) outperform previous corpora.
</nextsent>
<nextsent>little research, if not none, has been done to examine the interactions between emotions and the corresponding cause events, which may make great step towards an effective emotion classification model.
</nextsent>
<nextsent>the lack of research on cause events restricted current emotion analysis to simple classi fica tory work without exploring the potentials of the rich applications of putting emotion in context?.
</nextsent>
<nextsent>in fact, emotions can be invoked by perceptions of external events and in turn trigger reactions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF247">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the specific details of this deep?
</prevsent>
<prevsent>analysis have varied quite bit, perhaps more than surface syntax.in the 1970s and 1980s, lexical function grammars (lfg) way of dividing c-structure (surface) and f-structure (deep) led to parsers such as (hobbsand grishman, 1976) which produced these two levels, typically in two stages.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
however, enthusiasm for these two-stage parsers was eclipsed by the advent of one stage parsers with much higher accuracy (about 90% vs about 60%), the now-popular treebank-based parsers including (charniak, 2001; <papid> P01-1017 </papid>collins, 1999) and many others.</citsent>
<aftsection>
<nextsent>currently, many different deeper?
</nextsent>
<nextsent>levels are being manually annotated and automatically transduced, typically using surface parsing and other processors as input.
</nextsent>
<nextsent>oneof the most popular, semantic role labels (annotation and transducers based on the annotation) characterize relations anchored by select predicate types like verbs (palmer et al, 2005), <papid> J05-1004 </papid>nouns (meyers et al., 2004<papid> W04-2705 </papid>a), discourse connectives (miltsakaki et al, 2004) <papid> W04-2703 </papid>or those predicates that are part of particular semantic frames (baker et al, 1998).<papid> P98-1013 </papid></nextsent>
<nextsent>the conll tasks for 2008 and 2009 (surdeanu et al, 2008; <papid> W08-2121 </papid>hajic?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF248">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>currently, many different deeper?
</prevsent>
<prevsent>levels are being manually annotated and automatically transduced, typically using surface parsing and other processors as input.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
oneof the most popular, semantic role labels (annotation and transducers based on the annotation) characterize relations anchored by select predicate types like verbs (palmer et al, 2005), <papid> J05-1004 </papid>nouns (meyers et al., 2004<papid> W04-2705 </papid>a), discourse connectives (miltsakaki et al, 2004) <papid> W04-2703 </papid>or those predicates that are part of particular semantic frames (baker et al, 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>the conll tasks for 2008 and 2009 (surdeanu et al, 2008; <papid> W08-2121 </papid>hajic?</nextsent>
<nextsent>et al, 2009) has focused on unifying many of these individual efforts to produce logical structure for multiple parts of speech and multiple languages.like the conll shared task, we link surface levels to logical levels for multiple languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF249">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>currently, many different deeper?
</prevsent>
<prevsent>levels are being manually annotated and automatically transduced, typically using surface parsing and other processors as input.
</prevsent>
</prevsection>
<citsent citstr=" W04-2705 ">
oneof the most popular, semantic role labels (annotation and transducers based on the annotation) characterize relations anchored by select predicate types like verbs (palmer et al, 2005), <papid> J05-1004 </papid>nouns (meyers et al., 2004<papid> W04-2705 </papid>a), discourse connectives (miltsakaki et al, 2004) <papid> W04-2703 </papid>or those predicates that are part of particular semantic frames (baker et al, 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>the conll tasks for 2008 and 2009 (surdeanu et al, 2008; <papid> W08-2121 </papid>hajic?</nextsent>
<nextsent>et al, 2009) has focused on unifying many of these individual efforts to produce logical structure for multiple parts of speech and multiple languages.like the conll shared task, we link surface levels to logical levels for multiple languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF250">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>currently, many different deeper?
</prevsent>
<prevsent>levels are being manually annotated and automatically transduced, typically using surface parsing and other processors as input.
</prevsent>
</prevsection>
<citsent citstr=" W04-2703 ">
oneof the most popular, semantic role labels (annotation and transducers based on the annotation) characterize relations anchored by select predicate types like verbs (palmer et al, 2005), <papid> J05-1004 </papid>nouns (meyers et al., 2004<papid> W04-2705 </papid>a), discourse connectives (miltsakaki et al, 2004) <papid> W04-2703 </papid>or those predicates that are part of particular semantic frames (baker et al, 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>the conll tasks for 2008 and 2009 (surdeanu et al, 2008; <papid> W08-2121 </papid>hajic?</nextsent>
<nextsent>et al, 2009) has focused on unifying many of these individual efforts to produce logical structure for multiple parts of speech and multiple languages.like the conll shared task, we link surface levels to logical levels for multiple languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF251">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>currently, many different deeper?
</prevsent>
<prevsent>levels are being manually annotated and automatically transduced, typically using surface parsing and other processors as input.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
oneof the most popular, semantic role labels (annotation and transducers based on the annotation) characterize relations anchored by select predicate types like verbs (palmer et al, 2005), <papid> J05-1004 </papid>nouns (meyers et al., 2004<papid> W04-2705 </papid>a), discourse connectives (miltsakaki et al, 2004) <papid> W04-2703 </papid>or those predicates that are part of particular semantic frames (baker et al, 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>the conll tasks for 2008 and 2009 (surdeanu et al, 2008; <papid> W08-2121 </papid>hajic?</nextsent>
<nextsent>et al, 2009) has focused on unifying many of these individual efforts to produce logical structure for multiple parts of speech and multiple languages.like the conll shared task, we link surface levels to logical levels for multiple languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF252">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>levels are being manually annotated and automatically transduced, typically using surface parsing and other processors as input.
</prevsent>
<prevsent>oneof the most popular, semantic role labels (annotation and transducers based on the annotation) characterize relations anchored by select predicate types like verbs (palmer et al, 2005), <papid> J05-1004 </papid>nouns (meyers et al., 2004<papid> W04-2705 </papid>a), discourse connectives (miltsakaki et al, 2004) <papid> W04-2703 </papid>or those predicates that are part of particular semantic frames (baker et al, 1998).<papid> P98-1013 </papid></prevsent>
</prevsection>
<citsent citstr=" W08-2121 ">
the conll tasks for 2008 and 2009 (surdeanu et al, 2008; <papid> W08-2121 </papid>hajic?</citsent>
<aftsection>
<nextsent>et al, 2009) has focused on unifying many of these individual efforts to produce logical structure for multiple parts of speech and multiple languages.like the conll shared task, we link surface levels to logical levels for multiple languages.
</nextsent>
<nextsent>how ever, there are several differences: (1) the logical structures produced automatically by our system canbe expected to be more accurate than the comparable conll systems because our task involves predicting semantic roles with less fine-grained distinctions.
</nextsent>
<nextsent>our english and japanese results were higher than the conll 2009 srl systems.
</nextsent>
<nextsent>our english scores range from 76.3% (spoken) to 89.9% (news): 146 the best conll 2009 english scores were 73.31% (brown) and 85.63% (wsj).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF253">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> the glarf framework.  </section>
<citcontext>
<prevsection>
<prevsent>as our main goal is to use our system to regularize data, we freely incorporate any analysis that fits this goal.
</prevsent>
<prevsent>over time, we have found ways of incorporating named entities, propbank, nombank and the penn discourse treebank.
</prevsent>
</prevsection>
<citsent citstr=" W05-0302 ">
our agenda also includes incorporating the results of other research efforts (pustejovsky et al, 2005).<papid> W05-0302 </papid></citsent>
<aftsection>
<nextsent>for each sentence, we generate feature structure (fs) representing our most complete analysis.
</nextsent>
<nextsent>we distill subset of this information into dependency structure governed by theoretical assumptions, e.g., about identifying functors of phrases.
</nextsent>
<nextsent>each glarf dependency is between functor and an argument,where the functor is the head of phrase, conjunction, complementizer, or other function word.
</nextsent>
<nextsent>we have built applications that use each of these two representations, e.g., the dependency representation is used in (shinyama, 2007) and the fs representation is used in (k. parton and k. r. mckeown and r. coyne and m. diab and r. grishman and d. hakkani-tur and m. harper and h. ji and w. y. ma and a. meyers and s. stolbach and a. sun and g. tur and w. xu and s. yarman, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF256">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> english glarf.  </section>
<citcontext>
<prevsection>
<prevsent>parser described in (charniak, 2001), <papid> P01-1017 </papid>which label precision and recall scores in the 85% range.</prevsent>
<prevsent>the updated version of the parser seemsto perform closer to 90% on news data and perform lower on other genres.</prevsent>
</prevsection>
<citsent citstr=" L08-1115 ">
that performance would reflect reports on other versions of the charniak parser for which statistics are available (foster and van genabith, 2008).<papid> L08-1115 </papid></citsent>
<aftsection>
<nextsent>2.
</nextsent>
<nextsent>named entity (ne) tags from the jet ne sys-.
</nextsent>
<nextsent>tem (ji and grishman, 2006), <papid> P06-2055 </papid>which achieves f-scores ranging 86%-91% on newswire for both english and chinese (depending on epoch).</nextsent>
<nextsent>the jet system identifies seven classes of nes: person, gpe, location, organization, facility, weapon and vehicle.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF257">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> english glarf.  </section>
<citcontext>
<prevsection>
<prevsent>2.
</prevsent>
<prevsent>named entity (ne) tags from the jet ne sys-.
</prevsent>
</prevsection>
<citsent citstr=" P06-2055 ">
tem (ji and grishman, 2006), <papid> P06-2055 </papid>which achieves f-scores ranging 86%-91% on newswire for both english and chinese (depending on epoch).</citsent>
<aftsection>
<nextsent>the jet system identifies seven classes of nes: person, gpe, location, organization, facility, weapon and vehicle.
</nextsent>
<nextsent>3.
</nextsent>
<nextsent>machine readable dictionaries: comlex.
</nextsent>
<nextsent>(macleod et al, 1998), nombank dictionaries (from http://nlp.cs.nyu.edu/ meyers/nombank/) and others.omitted) such that: (1) the first set of rules convert the penn treebank into feature structure representation; and (2) each rule after the first rule is applied to an entire feature structure that is the output of rule ? 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF258">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> english glarf.  </section>
<citcontext>
<prevsection>
<prevsent>uncc.edu/cncc.php).
</prevsent>
<prevsent>in both cases, we assumed perfect sentence splitting (based on penn treebank annotation).
</prevsent>
</prevsection>
<citsent citstr=" W07-1529 ">
the icic, switchboard and charlotte texts that we used are part of the open american national corpus (oanc), in particular, the sigann shared sub corpus of the oanc (http://nlp.cs.nyu.edu/wiki/ corpuswg/ula-oanc-1) (meyers et al, 2007).<papid> W07-1529 </papid>comparable work for english includes: (1) (gab bard et al, 2006), <papid> N06-1024 </papid>system which reproduces the function tags of the penn treebank with 89% accuracy and empty categories (and their antecedents) with varying accuracies ranging from 82.2% to 96.3%, excluding null complementizers, as these are theory-internal and have no value for filling gaps.</citsent>
<aftsection>
<nextsent>(2) current systems that generate lfg f-structure 150 such as (wagner et al, 2007) which achieve an score of 91.1 on the f-structure pred relations, which are similar to our logic1 relations.
</nextsent>
<nextsent>the chinese glarf program takes chinese treebank-style syntactic parse and the output of achinese prop banker (xue, 2008) <papid> J08-2004 </papid>as input, and attempts to determine the relations between the head and its dependents within each constituent.</nextsent>
<nextsent>it does this by first exploiting the structural information and detecting six broad categories of syntactic relations that hold between the head and its dependents.these are predication, modification, complementation, coordination, auxiliary, and flat.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF259">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> english glarf.  </section>
<citcontext>
<prevsection>
<prevsent>uncc.edu/cncc.php).
</prevsent>
<prevsent>in both cases, we assumed perfect sentence splitting (based on penn treebank annotation).
</prevsent>
</prevsection>
<citsent citstr=" N06-1024 ">
the icic, switchboard and charlotte texts that we used are part of the open american national corpus (oanc), in particular, the sigann shared sub corpus of the oanc (http://nlp.cs.nyu.edu/wiki/ corpuswg/ula-oanc-1) (meyers et al, 2007).<papid> W07-1529 </papid>comparable work for english includes: (1) (gab bard et al, 2006), <papid> N06-1024 </papid>system which reproduces the function tags of the penn treebank with 89% accuracy and empty categories (and their antecedents) with varying accuracies ranging from 82.2% to 96.3%, excluding null complementizers, as these are theory-internal and have no value for filling gaps.</citsent>
<aftsection>
<nextsent>(2) current systems that generate lfg f-structure 150 such as (wagner et al, 2007) which achieve an score of 91.1 on the f-structure pred relations, which are similar to our logic1 relations.
</nextsent>
<nextsent>the chinese glarf program takes chinese treebank-style syntactic parse and the output of achinese prop banker (xue, 2008) <papid> J08-2004 </papid>as input, and attempts to determine the relations between the head and its dependents within each constituent.</nextsent>
<nextsent>it does this by first exploiting the structural information and detecting six broad categories of syntactic relations that hold between the head and its dependents.these are predication, modification, complementation, coordination, auxiliary, and flat.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF260">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> chinese glarf.  </section>
<citcontext>
<prevsection>
<prevsent>the icic, switchboard and charlotte texts that we used are part of the open american national corpus (oanc), in particular, the sigann shared sub corpus of the oanc (http://nlp.cs.nyu.edu/wiki/ corpuswg/ula-oanc-1) (meyers et al, 2007).<papid> W07-1529 </papid>comparable work for english includes: (1) (gab bard et al, 2006), <papid> N06-1024 </papid>system which reproduces the function tags of the penn treebank with 89% accuracy and empty categories (and their antecedents) with varying accuracies ranging from 82.2% to 96.3%, excluding null complementizers, as these are theory-internal and have no value for filling gaps.</prevsent>
<prevsent>(2) current systems that generate lfg f-structure 150 such as (wagner et al, 2007) which achieve an score of 91.1 on the f-structure pred relations, which are similar to our logic1 relations.</prevsent>
</prevsection>
<citsent citstr=" J08-2004 ">
the chinese glarf program takes chinese treebank-style syntactic parse and the output of achinese prop banker (xue, 2008) <papid> J08-2004 </papid>as input, and attempts to determine the relations between the head and its dependents within each constituent.</citsent>
<aftsection>
<nextsent>it does this by first exploiting the structural information and detecting six broad categories of syntactic relations that hold between the head and its dependents.these are predication, modification, complementation, coordination, auxiliary, and flat.
</nextsent>
<nextsent>predication holds at the clause level between the subject and the predicate, where the predicate is considered to bethe head and the subject is considered to the dependent.
</nextsent>
<nextsent>modification can also hold mainly within nps and vps, where the dependents are modifiers of the np head or adjuncts to the head verb.
</nextsent>
<nextsent>coordination holds almost for all phrasal categories where eachnon-punctuation child within this constituent is either conjunction or conjunct.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF261">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> japanese glarf.  </section>
<citcontext>
<prevsection>
<prevsent>for japanese, we process text with the knp parser (kurohashi and nagao, 1998) and convert the output into the glarf framework.
</prevsent>
<prevsent>the knp/kyoto corpus framework is japanese-specific dependency framework, very different from the penn treebank framework used for the other systems.
</prevsent>
</prevsection>
<citsent citstr=" I05-4002 ">
processing in japanese proceeds as follows: (1) we process the japanese with the juman segmenter (kuro 151 type prec rec no function tags version aggr 8431374 = 61.4% 843 1352 = 62.4% 61.8% aver 62.3% 63.5% 63.6% function tags version aggr 10311415 = 72.9% 1031 1352 = 76.3% 74.5% aver 73.0% 75.3% 74.9% table 3: 53 chinese newswire sentences: aggregate and average sentence scores hashi et al, 1994) and knp parser 2.0 (kurohashi and nagao, 1998), which has reported accuracy of91.32% score for dependency accuracy, as reported in (noro et al, 2005).<papid> I05-4002 </papid></citsent>
<aftsection>
<nextsent>as is standard in japanese linguistics, the knp/kyoto corpus (k) framework uses dependency analysis that has some features of phrase structure analysis.
</nextsent>
<nextsent>in particular, the dependency relations are between bunsetsu, small constituents which include head word and some number of modifiers which are typically function words (particles, auxiliaries, etc.), but can also be pre nominal noun modifiers.
</nextsent>
<nextsent>bunsetsu can also include multiple words in the case of names.the framework differentiates types of dependencies into: the normal head-argument variety, coordination (or parallel) and apposition.
</nextsent>
<nextsent>we convert the head-argument variety of dependency straightforwardly into phrase consisting of the head andall the arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF262">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> concluding remarks and future work.  </section>
<citcontext>
<prevsection>
<prevsent>one obstacle is that we do not currently use subcategorization dictionaries for either language, while we have several for english.
</prevsent>
<prevsent>in particular, these would be helpful in predicting and filling relative clause and others gaps.
</prevsent>
</prevsection>
<citsent citstr=" C02-1122 ">
we are considering automatically acquiring simple dictionaries by recording frequently occurring argument types of verbs over larger corpus, e.g., along the lines of (kawahara and kurohashi, 2002).<papid> C02-1122 </papid></citsent>
<aftsection>
<nextsent>in addition, existing japanese dictionaries such as the ipal (monolingual) dictionary (technology promotion agency, 1987) or previously acquired case information reported in (kawa hara and kurohashi, 2002).<papid> C02-1122 </papid></nextsent>
<nextsent>finally, we are investigating several avenues for using this system output for machine translation (mt) including: (1) aiding word alignment for other mt system (wang et al, 2007); <papid> D07-1077 </papid>and (2) aiding the creation various mt models involving analyzed text, e.g., (gildea, 2004; <papid> W04-3228 </papid>shen et al, 2008).<papid> P08-1066 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF264">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> concluding remarks and future work.  </section>
<citcontext>
<prevsection>
<prevsent>we are considering automatically acquiring simple dictionaries by recording frequently occurring argument types of verbs over larger corpus, e.g., along the lines of (kawahara and kurohashi, 2002).<papid> C02-1122 </papid></prevsent>
<prevsent>in addition, existing japanese dictionaries such as the ipal (monolingual) dictionary (technology promotion agency, 1987) or previously acquired case information reported in (kawa hara and kurohashi, 2002).<papid> C02-1122 </papid></prevsent>
</prevsection>
<citsent citstr=" D07-1077 ">
finally, we are investigating several avenues for using this system output for machine translation (mt) including: (1) aiding word alignment for other mt system (wang et al, 2007); <papid> D07-1077 </papid>and (2) aiding the creation various mt models involving analyzed text, e.g., (gildea, 2004; <papid> W04-3228 </papid>shen et al, 2008).<papid> P08-1066 </papid></citsent>
<aftsection>
<nextsent>acknowledgments this work was supported by nsf grant iis 0534700 structure alignment-based mt.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF265">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> concluding remarks and future work.  </section>
<citcontext>
<prevsection>
<prevsent>we are considering automatically acquiring simple dictionaries by recording frequently occurring argument types of verbs over larger corpus, e.g., along the lines of (kawahara and kurohashi, 2002).<papid> C02-1122 </papid></prevsent>
<prevsent>in addition, existing japanese dictionaries such as the ipal (monolingual) dictionary (technology promotion agency, 1987) or previously acquired case information reported in (kawa hara and kurohashi, 2002).<papid> C02-1122 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3228 ">
finally, we are investigating several avenues for using this system output for machine translation (mt) including: (1) aiding word alignment for other mt system (wang et al, 2007); <papid> D07-1077 </papid>and (2) aiding the creation various mt models involving analyzed text, e.g., (gildea, 2004; <papid> W04-3228 </papid>shen et al, 2008).<papid> P08-1066 </papid></citsent>
<aftsection>
<nextsent>acknowledgments this work was supported by nsf grant iis 0534700 structure alignment-based mt.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF266">
<title id=" W09-2423.xml">automatic recognition of logical relations for english chinese and japanese in the glarf framework </title>
<section> concluding remarks and future work.  </section>
<citcontext>
<prevsection>
<prevsent>we are considering automatically acquiring simple dictionaries by recording frequently occurring argument types of verbs over larger corpus, e.g., along the lines of (kawahara and kurohashi, 2002).<papid> C02-1122 </papid></prevsent>
<prevsent>in addition, existing japanese dictionaries such as the ipal (monolingual) dictionary (technology promotion agency, 1987) or previously acquired case information reported in (kawa hara and kurohashi, 2002).<papid> C02-1122 </papid></prevsent>
</prevsection>
<citsent citstr=" P08-1066 ">
finally, we are investigating several avenues for using this system output for machine translation (mt) including: (1) aiding word alignment for other mt system (wang et al, 2007); <papid> D07-1077 </papid>and (2) aiding the creation various mt models involving analyzed text, e.g., (gildea, 2004; <papid> W04-3228 </papid>shen et al, 2008).<papid> P08-1066 </papid></citsent>
<aftsection>
<nextsent>acknowledgments this work was supported by nsf grant iis 0534700 structure alignment-based mt.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF267">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent cases of public figures making headlines for the wrong reasons have shown how companies take into account public opinion to distance themselves from figures which can damage their public image.the web has become an important source for finding information, in the field of business intelligence, business analysts are turning their eyes to the web in order to monitor public perception on products,services, policies, and managers.
</prevsent>
<prevsent>the field of sentiment analysis has recently emerged (pang and lee, 2008) as an important area of research in natural language processing (nlp) which can provide viable solutions for monitoring public perception on number of issues; with evaluation programs suchas the text retrieval conference track on blog mining 1, the text analysis conference 2 track on opinion summarisation, and the defi fouille de textes program (grouin et al, 2009) advances in the state of the art have been produced.
</prevsent>
</prevsection>
<citsent citstr=" P05-1015 ">
although sentiment analysis involves various different problems such as identifying subjective sentences or identifying positive and negative opinions in text, here we concentrate on the opinion classification task; and more specifically on rating-inference, the task of identifying the authors evaluation of an entity with respect to an ordinal-scale based on the authors textual evaluation of the entity (pang and lee, 2005).<papid> P05-1015 </papid></citsent>
<aftsection>
<nextsent>the specific problem we study in this paper is that of associating fine-grained rating (1=worst,...5=best)to review.
</nextsent>
<nextsent>this is in general considered difficult problem because of the fuzziness inherent ofmid-range ratings (mukras et al, 2007).
</nextsent>
<nextsent>a considerable body of research has recently been produced to tackle this problem (chakraborti et al, 2007; ferrari et al, 2009) and reported figures showing accuracies ranging from 30% to 50% for such complextask; most approaches derive features for the classification task from the full document.
</nextsent>
<nextsent>in this research we ask whether extracting features from document summaries could help classification system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF268">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this research we ask whether extracting features from document summaries could help classification system.
</prevsent>
<prevsent>since text summaries are meant to contain the essential content of document (mani, 2001), we investigate whether filtering noise through text summarisationis of any help in the rating-inference task.
</prevsent>
</prevsection>
<citsent citstr=" P98-1012 ">
in re 1http:trec.nist.gov/ 2http://www.nist.gov/tac/ 107cent years, text summarisation has been used to support both manual and automatic tasks; in the sum mac evaluation (mani et al, 1998), text summaries were tested in document classification and question answering tasks where summaries were considered suitable surrogates for full documents; bagga and baldwin (1998) <papid> P98-1012 </papid>studied summarisation in the context of cross-document coreference task and found that summaries improved the performance ofa clustering-based coreference mechanism; more recently latif and mcgee (2009) have proposed text summarisation as preprocessing step for student essay assessment finding that summaries could beused instead of full essays to group similar?</citsent>
<aftsection>
<nextsent>quality essays.
</nextsent>
<nextsent>summarisation has been studied in the field of sentiment analysis with the objective of producing opinion summaries, however, to the best of our knowlegde there has been little research on the study of document summarisation as text processing step for opinion classification.
</nextsent>
<nextsent>this paper presents framework and extensive experiments on text summarisation for opinion classification, and in particular, for the rating-inference problem.
</nextsent>
<nextsent>we will present results indicating that some types of summaries could be as effective or better than the full documents in this task.the remainder of the paper is organised as fol lows: section 2 will compile the existing work with respect to the inference-rating problem; section 3 and section 4 will describe the corpus and the nlp tools used for all the experimental set-up.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF269">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>next, the text summarisation approaches will be described in section 5, and then section 6 will show the experiments conducted and the results obtained together with discussion.
</prevsent>
<prevsent>finally, we will draw some conclusions and address further work in section 7.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
most of the literature regarding sentiment analysis addresses the problem either by detecting and classifying opinions at sentence level (wilson et al,2005; <papid> H05-1044 </papid>du and tan, 2009), <papid> N09-1055 </papid>or by attempting to capture the overall sentiment of document (mcdonald et al, 2007; <papid> P07-1055 </papid>hu et al, 2008).</citsent>
<aftsection>
<nextsent>traditional approaches tackle the task as binary classification, where text units (e.g. words, sentences, fragments) are classified into positive vs. negative, or subjective vs. objective, according to their polarity and subjectivity degree, respectively.
</nextsent>
<nextsent>however, sentiment classification taking into account finer granularity has been less considered.
</nextsent>
<nextsent>rating-inference is particular task within sentiment analysis, which aims at inferring the authors numerical rating for review.
</nextsent>
<nextsent>for instance, given review and 5-star-rating scale (rang ing from 1 -the worst- to 5 -the best), this task should correctly predict the reviews rating, based on the language and sentiment expressed in its content.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF270">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>next, the text summarisation approaches will be described in section 5, and then section 6 will show the experiments conducted and the results obtained together with discussion.
</prevsent>
<prevsent>finally, we will draw some conclusions and address further work in section 7.
</prevsent>
</prevsection>
<citsent citstr=" N09-1055 ">
most of the literature regarding sentiment analysis addresses the problem either by detecting and classifying opinions at sentence level (wilson et al,2005; <papid> H05-1044 </papid>du and tan, 2009), <papid> N09-1055 </papid>or by attempting to capture the overall sentiment of document (mcdonald et al, 2007; <papid> P07-1055 </papid>hu et al, 2008).</citsent>
<aftsection>
<nextsent>traditional approaches tackle the task as binary classification, where text units (e.g. words, sentences, fragments) are classified into positive vs. negative, or subjective vs. objective, according to their polarity and subjectivity degree, respectively.
</nextsent>
<nextsent>however, sentiment classification taking into account finer granularity has been less considered.
</nextsent>
<nextsent>rating-inference is particular task within sentiment analysis, which aims at inferring the authors numerical rating for review.
</nextsent>
<nextsent>for instance, given review and 5-star-rating scale (rang ing from 1 -the worst- to 5 -the best), this task should correctly predict the reviews rating, based on the language and sentiment expressed in its content.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF271">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>next, the text summarisation approaches will be described in section 5, and then section 6 will show the experiments conducted and the results obtained together with discussion.
</prevsent>
<prevsent>finally, we will draw some conclusions and address further work in section 7.
</prevsent>
</prevsection>
<citsent citstr=" P07-1055 ">
most of the literature regarding sentiment analysis addresses the problem either by detecting and classifying opinions at sentence level (wilson et al,2005; <papid> H05-1044 </papid>du and tan, 2009), <papid> N09-1055 </papid>or by attempting to capture the overall sentiment of document (mcdonald et al, 2007; <papid> P07-1055 </papid>hu et al, 2008).</citsent>
<aftsection>
<nextsent>traditional approaches tackle the task as binary classification, where text units (e.g. words, sentences, fragments) are classified into positive vs. negative, or subjective vs. objective, according to their polarity and subjectivity degree, respectively.
</nextsent>
<nextsent>however, sentiment classification taking into account finer granularity has been less considered.
</nextsent>
<nextsent>rating-inference is particular task within sentiment analysis, which aims at inferring the authors numerical rating for review.
</nextsent>
<nextsent>for instance, given review and 5-star-rating scale (rang ing from 1 -the worst- to 5 -the best), this task should correctly predict the reviews rating, based on the language and sentiment expressed in its content.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF273">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of only one, corresponding to the overall sentiment of the review.
</prevsent>
<prevsent>also, in (baccianella et al, 2009) the rating of different features regarding hotel reviews (cleanli ness, location, staff, etc.) is addressed by analysing several aspects involved in the generation of product reviews representations, such as part-of-speech and lexicons.
</prevsent>
</prevsection>
<citsent citstr=" P07-1124 ">
other approaches (devitt and ahmad,2007), (<papid> P07-1124 </papid>turney, 2002) <papid> P02-1053 </papid>face this problem by grouping documents with closer stars under the same category, i.e. positive or negative, simplifying the task into binary classification problem.recently, due to the vast amount of on-line information and the subjectivity appearing in documents,the combination of sentiment analysis and summari 108 sation task in tandem can result in great benefits for stand-alone applications of sentiment analysis,as well as for the potential uses of sentiment analysis as part of other nlp applications (stoyanov andcardie, 2006).<papid> W06-0302 </papid></citsent>
<aftsection>
<nextsent>whilst there is much literature combining sentiment analysis and text summarisation focusing on generating opinion-oriented summaries for the new textual genres, such as blogs (lloret et al, 2009), <papid> N09-3013 </papid>or reviews (zhuang et al, 2006), the use of summaries as substitutes of full documents intasks such as rating-inference has been not yet explored to the best of our knowledge.</nextsent>
<nextsent>in contrast tothe existing literature, this paper uses summaries instead of full reviews to tackle the rating-inferencetask in the financial domain, and we carry out preliminary analysis concerning the potential benefits of text summaries for this task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF274">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of only one, corresponding to the overall sentiment of the review.
</prevsent>
<prevsent>also, in (baccianella et al, 2009) the rating of different features regarding hotel reviews (cleanli ness, location, staff, etc.) is addressed by analysing several aspects involved in the generation of product reviews representations, such as part-of-speech and lexicons.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
other approaches (devitt and ahmad,2007), (<papid> P07-1124 </papid>turney, 2002) <papid> P02-1053 </papid>face this problem by grouping documents with closer stars under the same category, i.e. positive or negative, simplifying the task into binary classification problem.recently, due to the vast amount of on-line information and the subjectivity appearing in documents,the combination of sentiment analysis and summari 108 sation task in tandem can result in great benefits for stand-alone applications of sentiment analysis,as well as for the potential uses of sentiment analysis as part of other nlp applications (stoyanov andcardie, 2006).<papid> W06-0302 </papid></citsent>
<aftsection>
<nextsent>whilst there is much literature combining sentiment analysis and text summarisation focusing on generating opinion-oriented summaries for the new textual genres, such as blogs (lloret et al, 2009), <papid> N09-3013 </papid>or reviews (zhuang et al, 2006), the use of summaries as substitutes of full documents intasks such as rating-inference has been not yet explored to the best of our knowledge.</nextsent>
<nextsent>in contrast tothe existing literature, this paper uses summaries instead of full reviews to tackle the rating-inferencetask in the financial domain, and we carry out preliminary analysis concerning the potential benefits of text summaries for this task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF275">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of only one, corresponding to the overall sentiment of the review.
</prevsent>
<prevsent>also, in (baccianella et al, 2009) the rating of different features regarding hotel reviews (cleanli ness, location, staff, etc.) is addressed by analysing several aspects involved in the generation of product reviews representations, such as part-of-speech and lexicons.
</prevsent>
</prevsection>
<citsent citstr=" W06-0302 ">
other approaches (devitt and ahmad,2007), (<papid> P07-1124 </papid>turney, 2002) <papid> P02-1053 </papid>face this problem by grouping documents with closer stars under the same category, i.e. positive or negative, simplifying the task into binary classification problem.recently, due to the vast amount of on-line information and the subjectivity appearing in documents,the combination of sentiment analysis and summari 108 sation task in tandem can result in great benefits for stand-alone applications of sentiment analysis,as well as for the potential uses of sentiment analysis as part of other nlp applications (stoyanov andcardie, 2006).<papid> W06-0302 </papid></citsent>
<aftsection>
<nextsent>whilst there is much literature combining sentiment analysis and text summarisation focusing on generating opinion-oriented summaries for the new textual genres, such as blogs (lloret et al, 2009), <papid> N09-3013 </papid>or reviews (zhuang et al, 2006), the use of summaries as substitutes of full documents intasks such as rating-inference has been not yet explored to the best of our knowledge.</nextsent>
<nextsent>in contrast tothe existing literature, this paper uses summaries instead of full reviews to tackle the rating-inferencetask in the financial domain, and we carry out preliminary analysis concerning the potential benefits of text summaries for this task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF276">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>also, in (baccianella et al, 2009) the rating of different features regarding hotel reviews (cleanli ness, location, staff, etc.) is addressed by analysing several aspects involved in the generation of product reviews representations, such as part-of-speech and lexicons.
</prevsent>
<prevsent>other approaches (devitt and ahmad,2007), (<papid> P07-1124 </papid>turney, 2002) <papid> P02-1053 </papid>face this problem by grouping documents with closer stars under the same category, i.e. positive or negative, simplifying the task into binary classification problem.recently, due to the vast amount of on-line information and the subjectivity appearing in documents,the combination of sentiment analysis and summari 108 sation task in tandem can result in great benefits for stand-alone applications of sentiment analysis,as well as for the potential uses of sentiment analysis as part of other nlp applications (stoyanov andcardie, 2006).<papid> W06-0302 </papid></prevsent>
</prevsection>
<citsent citstr=" N09-3013 ">
whilst there is much literature combining sentiment analysis and text summarisation focusing on generating opinion-oriented summaries for the new textual genres, such as blogs (lloret et al, 2009), <papid> N09-3013 </papid>or reviews (zhuang et al, 2006), the use of summaries as substitutes of full documents intasks such as rating-inference has been not yet explored to the best of our knowledge.</citsent>
<aftsection>
<nextsent>in contrast tothe existing literature, this paper uses summaries instead of full reviews to tackle the rating-inferencetask in the financial domain, and we carry out preliminary analysis concerning the potential benefits of text summaries for this task.
</nextsent>
<nextsent>since there is no standard dataset for carrying outthe rating-inference task, the corpus used for our experiments was one associated to current project on business intelligence we are working on.
</nextsent>
<nextsent>these data consisted of 89 reviews of several english banks (abbey, barca lys, halifax, hsbc, lloyds tsb, and national westminster) gathered from the internet.
</nextsent>
<nextsent>in particular the documents were collected from ciao3,a website where users can write reviews about different products and services, depending on their own experience.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF277">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> natural language processing tools.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, we have only 9 reviews that have been rated as 3-star, consisting of the 10% of the corpus, which is very low number.
</prevsent>
<prevsent>star-rating # reviews % 1-star 17 19 2-star 11 12 3-star 9 10 4-star 28 32 5-star 24 27 table 2: class distribution
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
linguistic analysis of textual input is carried out using the general architecture for text engineering (gate) ? framework for the development and deployment of language processing technology in large scale (cunningham et al, 2002).<papid> P02-1022 </papid></citsent>
<aftsection>
<nextsent>we make use of typical gate components: token isation, parts of speech tagging, and morphological analysis to produce document annotations.
</nextsent>
<nextsent>from the annotation swe produce number of features for document representation.
</nextsent>
<nextsent>features produced from the annotations are: string ? the original, unmodified text of each token; root ? the lemmatised, lower-case form of the token; category ? the part-of-speech (pos) tag, symbol that represents grammatical category such as determiner, present-tense verb, past-tense verb,singular noun, etc.; orth ? code representing the tokens combination of upper- and lower-case letters.in addition to these basic features, sentiment?
</nextsent>
<nextsent>features based on lexical resource are computed as explained below.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF278">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> natural language processing tools.  </section>
<citcontext>
<prevsection>
<prevsent>we use this aggregated scores in our classification experiments.note that we do not apply any word sense disambiguation procedure here.
</prevsent>
<prevsent>4.2 machine learning tool.
</prevsent>
</prevsection>
<citsent citstr=" C02-1054 ">
for the experiments reported here, we adopt support vector machine (svm) learning paradigm notonly because it has recently been used with success in different tasks in natural language processing (isozaki and kazawa, 2002), <papid> C02-1054 </papid>but it has been shown particularly suitable for text categorization (kumar and gopal, 2009) where the feature space is huge, asit is in our case.</citsent>
<aftsection>
<nextsent>we relyon the support vector machines implementation distributed with the gate system (li et al, 2009) which hides from the userthe complexities of feature extraction and conversion from documents to the machine learning implementation.
</nextsent>
<nextsent>the tool has been applied with success to number of datasets for opinion classification and rating-inference (saggion and funk, 2009).
</nextsent>
<nextsent>in this section, three approaches for carrying out the summarisation process are explained in detail.
</nextsent>
<nextsent>first, generic approach is taken as basis, and then, it is adapted into query-focused and opinion-oriented approach, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF279">
<title id=" W10-0213.xml">experiments on summary based opinion classification </title>
<section> text summarisation approach.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, in the first stage the body of the review out of the whole web page is automatically del imitated by means of patterns, andonly this text is used as the input for the next sum mar isation stages.
</prevsent>
<prevsent>further on, sentence relevance detection process is carried out employing different combinations of various techniques.
</prevsent>
</prevsection>
<citsent citstr=" W07-1411 ">
in particular, the techniques employed are: term frequency (tf ): this technique has been widely used in different summarisation approaches, showing the the most frequent words in document contain relevant information and can be indicative of the documents topic (nenkova et al, 2006) textual entailment (te): te module (ferrandez et al, 2007) <papid> W07-1411 </papid>is used to detect redundant information in the document, by computing the entailment between two consecutive sentences and discarding the entailed ones.</citsent>
<aftsection>
<nextsent>the identification of these entailment relations helps to avoid incorporating redundant information in summaries.code quantity principle (cqp): this is linguistic principle which proves the existence of proportional relation between how important the information is, and the number of coding elements it has(givon, 1990).
</nextsent>
<nextsent>in this approach we assume that sentences containing longer noun-phrases are more relevant.
</nextsent>
<nextsent>the aforementioned techniques are combined together taking always into account the term frequency, leading to different summarisation strategies (tf, te+tf, cqp+tf, te+cqp+tf ).
</nextsent>
<nextsent>finally, the resulting summary is produced by extracting the highest scored sentences up to the desired length, according the techniques explained.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF280">
<title id=" W09-2421.xml">relation detection between named entities report of a shared task </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>if we want to go beyond the detection of entities, natural step is establishing semantic relations between these entities, and this is what this paper is about.
</prevsent>
<prevsent>there are two fairly independent communities that focus on the task of detecting relations between named entities: the work on anaphora resolution, illustrated by (mitkov, 2000; collovini et al, 2007;de souza et al, 2008) and the work on relation detection in information extraction, see e.g.
</prevsent>
</prevsection>
<citsent citstr=" P05-1052 ">
(agichteinand gravano, 2000; zhao and grishman, 2005; <papid> P05-1052 </papid>culotta and sorensen, 2004).<papid> P04-1054 </papid></citsent>
<aftsection>
<nextsent>although both communities are doing computational semantics, the two fields are largely non-overlapping, and one of the merits of our work is that we tried to merge the two.
</nextsent>
<nextsent>let us briefly describe both traditions: as (mitkov, 2000) explains, anaphora resolution is concerned with studying the linguistic phenomenon of pointing back to another expression in the text.
</nextsent>
<nextsent>the semantic relations between the referents of these expressions can be of different types, being co-reference special case when the relation is identity.
</nextsent>
<nextsent>the focus of anaphora resolution is determining the antecedent chains, although it implicitly also allows to elicit semantic relations between referents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF281">
<title id=" W09-2421.xml">relation detection between named entities report of a shared task </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>if we want to go beyond the detection of entities, natural step is establishing semantic relations between these entities, and this is what this paper is about.
</prevsent>
<prevsent>there are two fairly independent communities that focus on the task of detecting relations between named entities: the work on anaphora resolution, illustrated by (mitkov, 2000; collovini et al, 2007;de souza et al, 2008) and the work on relation detection in information extraction, see e.g.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
(agichteinand gravano, 2000; zhao and grishman, 2005; <papid> P05-1052 </papid>culotta and sorensen, 2004).<papid> P04-1054 </papid></citsent>
<aftsection>
<nextsent>although both communities are doing computational semantics, the two fields are largely non-overlapping, and one of the merits of our work is that we tried to merge the two.
</nextsent>
<nextsent>let us briefly describe both traditions: as (mitkov, 2000) explains, anaphora resolution is concerned with studying the linguistic phenomenon of pointing back to another expression in the text.
</nextsent>
<nextsent>the semantic relations between the referents of these expressions can be of different types, being co-reference special case when the relation is identity.
</nextsent>
<nextsent>the focus of anaphora resolution is determining the antecedent chains, although it implicitly also allows to elicit semantic relations between referents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF284">
<title id=" W09-2421.xml">relation detection between named entities report of a shared task </title>
<section> track description.  </section>
<citcontext>
<prevsection>
<prevsent>the tiporel field of ex1-42 contains another relation, inclui, which stands for the relation of inclusion, this time with the previously mentioned servicos administrativos (ex1-40), specific department of the university.
</prevsent>
<prevsent>in order to minimize human labour and also to let systems mark relations the way it would better suit them, we have postulated from the start that, forall purposes, it would be equivalent to annotate everything or just enough relations so that all others can be automatically computed.
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
so, the evaluation programs, in an obvious extension of what was proposed in (vilain et al, 1995) <papid> M95-1005 </papid>for identity, 1.</citsent>
<aftsection>
<nextsent>add/expand all relations with their in verses (e.g., includes b?
</nextsent>
<nextsent>entails is included in a?), and 2.
</nextsent>
<nextsent>apply set of expansion rules (see examples in table 4) to compute the closure as consequence, different systems maytag the same text in different ways, but encoding the same knowledge.
</nextsent>
<nextsent>2.4 what is relevant relation?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF285">
<title id=" W09-2007.xml">quantifying constructional productivity with unseen slot members </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as one of the defining properties of language, productivity has received much attention in debates about the nature of derivational processes, the structure of the mental lexicon and the interpretation of key terms such as compositionality, gram maticality judgments or well-formedness.
</prevsent>
<prevsent>however in computational linguistics it is probably fair to say that it can be regarded most of all as problem.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
familiar items present in training data can be listed in lexical resources, the probabilities of their different realizations can be estimated from corpus frequency distributions etc. thus using lexical information (statistically extracted or handcrafted resources) is the most successful strategy in resolving syntactic ambiguities such as pp-attachment (hindle and rooth, 1993; <papid> J93-1005 </papid>ratnaparkhi, 1998; stetina and nagao, 1997; <papid> W97-0109 </papid>pantel and lin, 2000; <papid> P00-1014 </papid>kawahara and kurohashi, 2005), <papid> I05-1017 </papid>basing decisions on previous cases with identical lexemes or additional information about those lexemes.</citsent>
<aftsection>
<nextsent>yet because of productivity, even very large training data will never cover examples for all inputs being analyzed.
</nextsent>
<nextsent>in morphological theory (and corresponding computational linguistic practice), the situation has been somewhat different: much larger part of the word formations encountered in data can be listed in lexicon, with neologisms being the exception, whereas in syntax most sentences are novel, with recurring combinations being the exception.2 the focus in morphology has therefore often been on which word formation processes are productive and to what extent, with the computational counterpart being whether or not corresponding rules should be built into morphological analyzer.
</nextsent>
<nextsent>syn tacticians, conversely, may ask which apparently regular constructions are actually lexicalized or have at least partly non-compositional properties (e.g. collocations, see choueka, 1988, evert, 2005, standing, at least for some languages, between syntax and word formation and often generating an unusually large amount of items unlisted in lexica (cf.
</nextsent>
<nextsent>bauer, 2001:36-7).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF287">
<title id=" W09-2007.xml">quantifying constructional productivity with unseen slot members </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as one of the defining properties of language, productivity has received much attention in debates about the nature of derivational processes, the structure of the mental lexicon and the interpretation of key terms such as compositionality, gram maticality judgments or well-formedness.
</prevsent>
<prevsent>however in computational linguistics it is probably fair to say that it can be regarded most of all as problem.
</prevsent>
</prevsection>
<citsent citstr=" W97-0109 ">
familiar items present in training data can be listed in lexical resources, the probabilities of their different realizations can be estimated from corpus frequency distributions etc. thus using lexical information (statistically extracted or handcrafted resources) is the most successful strategy in resolving syntactic ambiguities such as pp-attachment (hindle and rooth, 1993; <papid> J93-1005 </papid>ratnaparkhi, 1998; stetina and nagao, 1997; <papid> W97-0109 </papid>pantel and lin, 2000; <papid> P00-1014 </papid>kawahara and kurohashi, 2005), <papid> I05-1017 </papid>basing decisions on previous cases with identical lexemes or additional information about those lexemes.</citsent>
<aftsection>
<nextsent>yet because of productivity, even very large training data will never cover examples for all inputs being analyzed.
</nextsent>
<nextsent>in morphological theory (and corresponding computational linguistic practice), the situation has been somewhat different: much larger part of the word formations encountered in data can be listed in lexicon, with neologisms being the exception, whereas in syntax most sentences are novel, with recurring combinations being the exception.2 the focus in morphology has therefore often been on which word formation processes are productive and to what extent, with the computational counterpart being whether or not corresponding rules should be built into morphological analyzer.
</nextsent>
<nextsent>syn tacticians, conversely, may ask which apparently regular constructions are actually lexicalized or have at least partly non-compositional properties (e.g. collocations, see choueka, 1988, evert, 2005, standing, at least for some languages, between syntax and word formation and often generating an unusually large amount of items unlisted in lexica (cf.
</nextsent>
<nextsent>bauer, 2001:36-7).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF289">
<title id=" W09-2007.xml">quantifying constructional productivity with unseen slot members </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as one of the defining properties of language, productivity has received much attention in debates about the nature of derivational processes, the structure of the mental lexicon and the interpretation of key terms such as compositionality, gram maticality judgments or well-formedness.
</prevsent>
<prevsent>however in computational linguistics it is probably fair to say that it can be regarded most of all as problem.
</prevsent>
</prevsection>
<citsent citstr=" P00-1014 ">
familiar items present in training data can be listed in lexical resources, the probabilities of their different realizations can be estimated from corpus frequency distributions etc. thus using lexical information (statistically extracted or handcrafted resources) is the most successful strategy in resolving syntactic ambiguities such as pp-attachment (hindle and rooth, 1993; <papid> J93-1005 </papid>ratnaparkhi, 1998; stetina and nagao, 1997; <papid> W97-0109 </papid>pantel and lin, 2000; <papid> P00-1014 </papid>kawahara and kurohashi, 2005), <papid> I05-1017 </papid>basing decisions on previous cases with identical lexemes or additional information about those lexemes.</citsent>
<aftsection>
<nextsent>yet because of productivity, even very large training data will never cover examples for all inputs being analyzed.
</nextsent>
<nextsent>in morphological theory (and corresponding computational linguistic practice), the situation has been somewhat different: much larger part of the word formations encountered in data can be listed in lexicon, with neologisms being the exception, whereas in syntax most sentences are novel, with recurring combinations being the exception.2 the focus in morphology has therefore often been on which word formation processes are productive and to what extent, with the computational counterpart being whether or not corresponding rules should be built into morphological analyzer.
</nextsent>
<nextsent>syn tacticians, conversely, may ask which apparently regular constructions are actually lexicalized or have at least partly non-compositional properties (e.g. collocations, see choueka, 1988, evert, 2005, standing, at least for some languages, between syntax and word formation and often generating an unusually large amount of items unlisted in lexica (cf.
</nextsent>
<nextsent>bauer, 2001:36-7).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF290">
<title id=" W09-2007.xml">quantifying constructional productivity with unseen slot members </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as one of the defining properties of language, productivity has received much attention in debates about the nature of derivational processes, the structure of the mental lexicon and the interpretation of key terms such as compositionality, gram maticality judgments or well-formedness.
</prevsent>
<prevsent>however in computational linguistics it is probably fair to say that it can be regarded most of all as problem.
</prevsent>
</prevsection>
<citsent citstr=" I05-1017 ">
familiar items present in training data can be listed in lexical resources, the probabilities of their different realizations can be estimated from corpus frequency distributions etc. thus using lexical information (statistically extracted or handcrafted resources) is the most successful strategy in resolving syntactic ambiguities such as pp-attachment (hindle and rooth, 1993; <papid> J93-1005 </papid>ratnaparkhi, 1998; stetina and nagao, 1997; <papid> W97-0109 </papid>pantel and lin, 2000; <papid> P00-1014 </papid>kawahara and kurohashi, 2005), <papid> I05-1017 </papid>basing decisions on previous cases with identical lexemes or additional information about those lexemes.</citsent>
<aftsection>
<nextsent>yet because of productivity, even very large training data will never cover examples for all inputs being analyzed.
</nextsent>
<nextsent>in morphological theory (and corresponding computational linguistic practice), the situation has been somewhat different: much larger part of the word formations encountered in data can be listed in lexicon, with neologisms being the exception, whereas in syntax most sentences are novel, with recurring combinations being the exception.2 the focus in morphology has therefore often been on which word formation processes are productive and to what extent, with the computational counterpart being whether or not corresponding rules should be built into morphological analyzer.
</nextsent>
<nextsent>syn tacticians, conversely, may ask which apparently regular constructions are actually lexicalized or have at least partly non-compositional properties (e.g. collocations, see choueka, 1988, evert, 2005, standing, at least for some languages, between syntax and word formation and often generating an unusually large amount of items unlisted in lexica (cf.
</nextsent>
<nextsent>bauer, 2001:36-7).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF291">
<title id=" W09-2007.xml">quantifying constructional productivity with unseen slot members </title>
<section> the british national corpus (http://www.natcorp.ox.ac.uk/),.  </section>
<citcontext>
<prevsection>
<prevsent>with this established, the question arises whether cfg rule like the one above should take account of the likelihood of each slot to contain novel vs. familiar members.
</prevsent>
<prevsent>for instance, if pcfg parser correctly identifies novel comparative and the input matches the rule, should it be more skeptical of an unseen bare cc1 than an unseen bare cc2 (keeping in mind that the latter have so far been better in 88% of cases)?
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
to illustrate this, we may consider the output of pcfg parser (in this case the stanford parser, klein and manning, 2003) <papid> P03-1054 </papid>for an ambiguous example.</citsent>
<aftsection>
<nextsent>since ccs are rather rare, pcfgs will tend to prefer most other parses of sentence, if these are available.
</nextsent>
<nextsent>where no other reading is available we may get the expected two clause structure, as in the example in figure 2.11 11 the nodes conform to the penn treebank ii bracketing.
</nextsent>
<nextsent>guidelines for ccs (bies et al, 1995:178).
</nextsent>
<nextsent>the stanford parser fares quite well in cases like these, since the pronoun (it, i) can hardly be modified by the comparative (*[np the closer it] or *[np the more worried i]), and similarly for nps with articles (*[np the closer the time]).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF295">
<title id=" W09-2007.xml">quantifying constructional productivity with unseen slot members </title>
<section> pp attachment and productivity.  </section>
<citcontext>
<prevsection>
<prevsent>the problem of attaching prepositional phrases as sister nodes of vp or as adjuncts to its object nouns 51 is classic case of syntactic ambiguity that causes trouble for parsers (see hindle and rooth, 1993; <papid> J93-1005 </papid>manning and schtze, 1999:278-287; atterer and schtze, 2007), e.g. the difference between ate fish with fork and ate fish with bones12, i.e. denoting the instrument or an attribute of the fish.</prevsent>
<prevsent>there are also two further common readings of the preposition with in this context, namely attached either high or low in the vp in comitative sense: ate fish with mary and ate fish with potatoes respectively, though most approaches do not distinguish these, rather aiming at getting the attachment site right.</prevsent>
</prevsection>
<citsent citstr=" H94-1048 ">
already in early work on pp attachment (hindle and rooth, 1993) <papid> J93-1005 </papid>it was realized that the lexical identity of the verb, its object, the preposition and in later approaches also the prepositional object noun (ratnaparkhi et al, 1994) <papid> H94-1048 </papid>are useful for predicting the attachment site, casting the task as classification of tuples  v, n1, p, n2  into the classes (vp attachment) and (np attachment).</citsent>
<aftsection>
<nextsent>classifiers are commonly either supervised, with disambiguated training data, or more recently unsupervised (ratnaparkhi, 1998) using data from unambiguous cases where no n1 or appears.
</nextsent>
<nextsent>other approaches supplement this information with hand-built or automatically acquired lexical resources and collocation databases to determine the relationship between the lexemes, or, for lexemes un attested in the tuples, for semantically similar ones (stetina and nagao, 1997; <papid> W97-0109 </papid>pantel and lin, 2000).<papid> P00-1014 </papid></nextsent>
<nextsent>although the state of the art in lexically based systems actually approaches human performance, they lose their power when confronted with unfamiliar items.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF299">
<title id=" W09-2107.xml">automated suggestions for mis collocations </title>
<section> error detection and correction in nlp.  </section>
<citcontext>
<prevsection>
<prevsent>our study focuses, therefore, on vn mis collocation correction.
</prevsent>
<prevsent>error detection and correction have been two major issues in nlp research in the past decade.
</prevsent>
</prevsection>
<citsent citstr=" I08-1059 ">
projects involving learner corpora in analyzing and categorizing learner errors include nict japanese learners of english (jle), the chinese learners of english corpus (gamon et al, 2008) <papid> I08-1059 </papid>and english taiwan learner corpus (or tlc) (wible et al, 2003).</citsent>
<aftsection>
<nextsent>studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (han et al, 2004) and prepositions (chodorow et al., 2007).<papid> W07-1604 </papid></nextsent>
<nextsent>one goal of this work-in-progress is to address the less studied issue of open class lexical errors, specifically lexical collocation errors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF300">
<title id=" W09-2107.xml">automated suggestions for mis collocations </title>
<section> error detection and correction in nlp.  </section>
<citcontext>
<prevsection>
<prevsent>error detection and correction have been two major issues in nlp research in the past decade.
</prevsent>
<prevsent>projects involving learner corpora in analyzing and categorizing learner errors include nict japanese learners of english (jle), the chinese learners of english corpus (gamon et al, 2008) <papid> I08-1059 </papid>and english taiwan learner corpus (or tlc) (wible et al, 2003).</prevsent>
</prevsection>
<citsent citstr=" W07-1604 ">
studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (han et al, 2004) and prepositions (chodorow et al., 2007).<papid> W07-1604 </papid></citsent>
<aftsection>
<nextsent>one goal of this work-in-progress is to address the less studied issue of open class lexical errors, specifically lexical collocation errors.
</nextsent>
<nextsent>we focus on providing correct collocation suggestions for lexical miscollocations.
</nextsent>
<nextsent>three features are employed to identify the correct collocation substitute for miscollocation: word association measurement, semantic similarity between the correction candidate and the misused word to be replaced, and intercollocability (i.e., the concept of shared collocates in collocation clusters proposed by cowie and howarth, 1995).
</nextsent>
<nextsent>nlp research on learner errors includes work on error detection and error correction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF301">
<title id=" W09-2409.xml">using lexical patterns in the google web 1t corpus to deduce semantic relations between nouns </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>work in theoretical linguistics has suggested that noun-noun compounds may be formed by the deletion of predicate verb or preposition (levi 1978).
</prevsent>
<prevsent>however, whether the set of possible predicates numbers 5 or 50, there are likely to be some examples of noun phrases that fit into none of the categories and some that fit in multiple categories.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
the idea of searching large corpus for specific lexico syntactic phrases to indicate semantic relation of interest was first described by hearst (1992).<papid> C92-2082 </papid></citsent>
<aftsection>
<nextsent>lauer (1995) tackled the problem of semantically disambiguating noun phrases by trying to find the preposition which best describes there lation between the modifier and head noun.
</nextsent>
<nextsent>his method involves searching corpus for occurrences paraphrases of the form noun preposition modifier?.
</nextsent>
<nextsent>whichever preposition is most frequent in this context is chosen to represent the predicate 58 of the nominal, which poses the same problem of vagueness as levi approach.
</nextsent>
<nextsent>lapata and keller (2005) improved on lauer results on the same task by using the web as corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF302">
<title id=" W09-1907.xml">active dual supervision reducing the cost of annotating examples and features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this kind of annotation requires human to condense prior linguistic experience with word into sentiment label that reflects the net emotion that the word evokes.
</prevsent>
<prevsent>we refer to the general setting of learning from both labels on examples and features as dual supervision.
</prevsent>
</prevsection>
<citsent citstr=" D08-1004 ">
this setting arises more broadly in tasks where in addition to labeled documents, it is frequently possible to provide domain knowledge in the form of words, or phrases (zaidan and eisner, 2008) <papid> D08-1004 </papid>or even more sophisticated linguistic features, that associate strongly with class.</citsent>
<aftsection>
<nextsent>recent work (druck et al, 2008; sindhwani and melville, 2008) has demonstrated that the presence of word supervision can greatly reduce the number of labeled documents 49 required to build high quality text classifiers.
</nextsent>
<nextsent>in general, these two sources of supervision arenot mutually redundant, and have different annotation costs, human response quality, and degrees of utility towards learning dual supervision model.
</nextsent>
<nextsent>this leads naturally to the problem of active dual supervision, or, how to optimally query human oracle to simultaneously collect document and feature annotations, with the objective of building the highest quality model with the lowest cost.
</nextsent>
<nextsent>much of the machine learning literature on active learning has focused on one-sided example-only annotation for classification problems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF303">
<title id=" W09-2804.xml">optimization based content selection for opinion summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an entity is decomposed into hierarchy of features, and relevance score is independently calculated for each feature, based on the preferences of the user and the value of that feature for the product.
</prevsent>
<prevsent>content selection involves selecting the most relevant features for the current user.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
there is also work in sentiment analysis relying on optimization or clustering-based approaches.pang and lee (2004) <papid> P04-1035 </papid>frame the problem of detecting subjective sentences as finding the minimum cut in graph representation of the sentences.they produce compressed versions of movie reviews using just the subjective sentences, which retain the polarity information of the review.</citsent>
<aftsection>
<nextsent>ga monet al (2005) use heuristic approach to cluster sentences drawn from car reviews, grouping sentences that share common terms, especially those salient in the domain such as drive?
</nextsent>
<nextsent>orhan dling?.
</nextsent>
<nextsent>the resulting clusters are displayed by treemap visualization.our work is most similar to the content selection method of the multimedia conversation system ria (responsive information architect) (zhou and aggarwal, 2004).
</nextsent>
<nextsent>in ria, content selection involves selecting dimensions (such as price in the real estate domain) in response to query such that the desirability of the dimensions selected for the query is maximized while respect 7 ing time and space constraints.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF304">
<title id=" W09-2804.xml">optimization based content selection for opinion summarization </title>
<section> comparative evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>despite the widespread adoption of user reviews in online web sites, there is to our knowledge no publicly available corpus of customer reviews of sufficient size which is annotated with features arranged in hierarchy.
</prevsent>
<prevsent>while small scale corpora do exist for small number of products, the size of the corpora is too small to be representative of all possible distributions of evaluations and feature hierarchies of products, which limits our ability to draw any meaningful conclusion from the dataset.
</prevsent>
</prevsection>
<citsent citstr=" P94-1050 ">
2 thus, we stochastic ally 2 using constructed dataset based on real data where no resources or agreed-upon evaluation methodology yet exist shas been done in other nlp tasks such as topic boundary detection (reynar, 1994) <papid> P94-1050 </papid>and local coherence modelling (barzilay and lapata, 2005).<papid> P05-1018 </papid></citsent>
<aftsection>
<nextsent>we are encouraged, however, that subsequent to our experiment, more resources for opinion anal 11 mean std.
</nextsent>
<nextsent># features 55.3889 8.5547 # evaluated features 21.6667 5.9722 # children (depth 0) 11.3056 0.7753 # children (depth 1 fertile) 5.5495 1.7724 table 2: statistics on the 36 generated datasets.
</nextsent>
<nextsent>at depth 1, 134 of the 407 features in total across the trees were barren.
</nextsent>
<nextsent>the generated tree hierarchies were quite flat, with maximum depth of 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF305">
<title id=" W09-2804.xml">optimization based content selection for opinion summarization </title>
<section> comparative evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>despite the widespread adoption of user reviews in online web sites, there is to our knowledge no publicly available corpus of customer reviews of sufficient size which is annotated with features arranged in hierarchy.
</prevsent>
<prevsent>while small scale corpora do exist for small number of products, the size of the corpora is too small to be representative of all possible distributions of evaluations and feature hierarchies of products, which limits our ability to draw any meaningful conclusion from the dataset.
</prevsent>
</prevsection>
<citsent citstr=" P05-1018 ">
2 thus, we stochastic ally 2 using constructed dataset based on real data where no resources or agreed-upon evaluation methodology yet exist shas been done in other nlp tasks such as topic boundary detection (reynar, 1994) <papid> P94-1050 </papid>and local coherence modelling (barzilay and lapata, 2005).<papid> P05-1018 </papid></citsent>
<aftsection>
<nextsent>we are encouraged, however, that subsequent to our experiment, more resources for opinion anal 11 mean std.
</nextsent>
<nextsent># features 55.3889 8.5547 # evaluated features 21.6667 5.9722 # children (depth 0) 11.3056 0.7753 # children (depth 1 fertile) 5.5495 1.7724 table 2: statistics on the 36 generated datasets.
</nextsent>
<nextsent>at depth 1, 134 of the 407 features in total across the trees were barren.
</nextsent>
<nextsent>the generated tree hierarchies were quite flat, with maximum depth of 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF306">
<title id=" W09-2804.xml">optimization based content selection for opinion summarization </title>
<section> comparative evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>table 2 presents some statistics on the generated datasets.
</prevsent>
<prevsent>4.2 building human performance model.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
we adopt the evaluation approach that good content selection strategy should perform similarly to humans, which is the view taken by existing summarization evaluation schemes such as rouge (lin, 2004) <papid> W04-1013 </papid>and the pyramid method (nenkova et al., 2007).</citsent>
<aftsection>
<nextsent>for evaluating our content selection strategy, we conducted user study asking human participants to perform selection task to create gold standard?
</nextsent>
<nextsent>selections.
</nextsent>
<nextsent>participants viewed and selected udf features using treemap information visualization.
</nextsent>
<nextsent>see figure 2 for an example.we recruited 25 university students or graduates, who were each presented with 19 to 20 of the cases we generated as described above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF307">
<title id=" W10-0706.xml">crowd sourced accessibility elicitation of wikipedia articles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>suitable tasks are best limited to those easily accomplished in short bites?
</prevsent>
<prevsent>requiring little context switching.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
for instance, most annotation tasks in prior work (snow et al, 2008) <papid> D08-1027 </papid>required selection from an enumerated list, allowing for easy automated quality control and data collection.more recent work to collect speech transcription (novotney and callison-burch, 2010) or parallel text translations (callison-burch, 2009) demonstrated that turkers can provide useful free-form an notation.in this paper, we extend open ended collection even further by eliciting narrations of english wikipedia articles.</citsent>
<aftsection>
<nextsent>to vet prospective narrators, we use qualitative qualifications by aggregating the opinions of other turkers on narrative style, thus avoiding quantification of qualitative tasks.
</nextsent>
<nextsent>the spoken wikipedia project1 aims to increase the accessibility of wikipedia by recording articles for use by blind or illiterate users.
</nextsent>
<nextsent>since 2008, over 1600 english articles covering topics from art to.
</nextsent>
<nextsent>technology have been narrated by volunteers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF308">
<title id=" W10-0204.xml">emotions evoked by common words and phrases using mechanical turk to create an emotion lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the lexicon will also be useful for evaluating automatic methods that identify the emotions evoked by word.
</prevsent>
<prevsent>such algorithms may then be used to automatically generate emotion lexicons in languages where no such lexicons exist.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
as of now, high-quality high-coverage emotion lexicons do not exist for any language, although there are afew limited-coverage lexicons for handful of languages, for example, the wordnet affect lexicon(wal) (strapparava and valitutti, 2004) for six basic emotions and the general inquirer (gi) (stone et al., 1966), which categorizes words into number of categories, including positive and negative semantic orientation.amazon has an online service called mechanical turk that can be used to obtain large amount of human annotation in an efficient and inexpensive manner (snow et al, 2008; <papid> D08-1027 </papid>callison-burch, 2009).1however, one must define the task carefully to obtain annotations of high quality.</citsent>
<aftsection>
<nextsent>several checks mustbe placed to ensure that random and erroneous annotations are discouraged, rejected, and re-annotated.
</nextsent>
<nextsent>in this paper, we show how we compiled moderate-sized english emotion lexicon by manual 1https://www.mturk.com/mturk/welcome 26annotation through amazons mechanical turk service.
</nextsent>
<nextsent>this dataset, which we will call emolex, ismany times as large as the only other known emotion lexicon, wordnet affect lexicon.
</nextsent>
<nextsent>more importantly, the terms in this lexicon are carefully chosen to include some of the most frequent nouns, verbs, adjectives, and adverbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF309">
<title id=" W10-0204.xml">emotions evoked by common words and phrases using mechanical turk to create an emotion lexicon </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>complex emotions can be viewed as combinations of these basic emotions.
</prevsent>
<prevsent>wordnet affect lexicon (strapparava and valitutti, 2004) has few hundred words annotated with the emotions they evoke.2 it was created by manually identifying the emotions of few seed words and then marking all their wordnet synonyms as having the same emotion.
</prevsent>
</prevsection>
<citsent citstr=" H05-1073 ">
the general inquirer (stone etal., 1966) has 11,788 words labeled with 182 categories of word tags, including positive and negative semantic orientation.3 it also has certain other affect categories, such as pleasure, arousal, feeling,and pain but these have not been exploited to significant degree by the natural language processing 2http://wndomains.fbk.eu/wnaffect.html 3http://www.wjh.harvard.edu/inquirer community.work in emotion detection can be roughly classified into that which looks for specific emotion denoting words (elliott, 1992), that which determines tendency of terms to co-occur with seed words whose emotions are known (read, 2004), that which uses hand-coded rules (neviarouskaya et al, 2009), and that which uses machine learning and number of emotion features, including emotion denoting words (alm et al, 2005).<papid> H05-1073 </papid>much of this recent work focuses on six emotions studied by ekman (1992).</citsent>
<aftsection>
<nextsent>these emotions?
</nextsent>
<nextsent>joy, sadness, anger, fear, disgust, and surprise?
</nextsent>
<nextsent>are subset of the eight proposed in plutchik (1980).we focus on the plutchik emotions because the emotions can be naturally paired into oppositesjoy?
</nextsent>
<nextsent>sadness, anger fear, trust disgust, and anticipation?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF310">
<title id=" W10-0204.xml">emotions evoked by common words and phrases using mechanical turk to create an emotion lexicon </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>sadness, anger fear, trust disgust, and anticipation?
</prevsent>
<prevsent>surprise.
</prevsent>
</prevsection>
<citsent citstr=" D08-1103 ">
natural symmetry apart, we believe that prior work on automatically computing word pair antonymy (lin et al, 2003; mohammad et al, 2008;<papid> D08-1103 </papid>lobanova et al, 2010) can now be leveraged in automatic emotion detection.</citsent>
<aftsection>
<nextsent>in the subsections below we present the challenges in obtaining high-quality emotion annotation, howwe address those challenges, how we select the target terms, and the questionnaire we created for the annotators.
</nextsent>
<nextsent>3.1 key challenges.
</nextsent>
<nextsent>words used in different senses can evoke different emotions.
</nextsent>
<nextsent>for example, the word shout evokes adifferent emotion when used in the context of admonishment, than when used in give me shout if you need any help.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF311">
<title id=" W10-0204.xml">emotions evoked by common words and phrases using mechanical turk to create an emotion lexicon </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>we also identified which emotions tend to be evoked simultaneously by the same term.
</prevsent>
<prevsent>the lexicon is available for free download.4 since this pilot experiment with about 2000 target terms was successful, we will now obtain emotion annotations for tens of thousands of english terms.we will use the emotion lexicon to identify emotional tone of larger units of text, such as newspaper headlines and blog posts.
</prevsent>
</prevsection>
<citsent citstr=" D09-1063 ">
we will also use it to evaluate automatically generated lexicons, such as the polarity lexicons by turney and littman (2003) and mohammad et al (2009).<papid> D09-1063 </papid></citsent>
<aftsection>
<nextsent>we will explore the variance in emotion evoked by near-synonyms, and also how common it is for words with many meanings to evoke different emotions in different senses.
</nextsent>
<nextsent>acknowledgments this research was funded by the national research council canada (nrc).
</nextsent>
<nextsent>thanks to diana inkpen and diman ghazi for early discussions on emotion.thanks to joel martin for encouragement and support.
</nextsent>
<nextsent>thanks to norm vinson and the ethics committee at nrc for examining, guiding, and approving the survey.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF312">
<title id=" W10-0201.xml">emotion analysis using latent affective folding and embedding </title>
<section> motivation and overview.  </section>
<citcontext>
<prevsection>
<prevsent>first, the inherent lack of supervision routinely leads to latent semantic space which is not particularly representative of the underlying domain of discourse.
</prevsent>
<prevsent>and second, the construction of the affective categories still relies heavily on pre-defined lexical affinity, potentially resulting in an unwarranted bias in the taxonomy of affective states.
</prevsent>
</prevsection>
<citsent citstr=" E09-1065 ">
the first limitation impinges on the effectiveness of any lsa-based approach, which is known to vary substantially based on the size and quality of the training data (bellegarda, 2008; mohler and mihalcea, 2009).<papid> E09-1065 </papid></citsent>
<aftsection>
<nextsent>in the present case, any discrepancy between latent semantic space and domain of discourse may distort the position of certain words in the space, which could in turn lead to subsequentsub-optimal affective weight assignment.
</nextsent>
<nextsent>for instance, in the examples above, the word smell is considerably more critical to the resolution of awful as marker of disgust than the word car.
</nextsent>
<nextsent>but that fact may never be uncovered if the only pertinent documents in the training corpus happen to be about expensive fragrances and automobiles.
</nextsent>
<nextsent>thus, it is highly desirable to derive the latent semantic space using data representative of the application considered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF313">
<title id=" W10-0710.xml">can crowds build parallel corpora for machine translation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for most remaining languages there is very little interest or funding available and limited or expensive access to experts for data elicitation.
</prevsent>
<prevsent>crowd-sourcing compensates for the lack of experts with large pool of expert/non-expert crowd.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
however, crowd-sourcinghas thus far been explored in the context of eliciting annotations for supervised classification task, typically monolingual in nature (snow et al, 2008).<papid> D08-1027 </papid></citsent>
<aftsection>
<nextsent>in this shared task we test the feasibility of eliciting parallel data for machine translation (mt) using mechanical turk (mturk).
</nextsent>
<nextsent>mt poses an interesting challenge as we require turkers to have understand ing/writing skills in both the languages.
</nextsent>
<nextsent>our work is similar to some recent work on crowd-sourcing and machine translation (ambati et al, 2010; callison burch, 2009), but focuses primarily on the setup and design of translation tasks on mturk with varying granularity levels, both at sentence- and phrase-level translation.
</nextsent>
<nextsent>we first conduct pilot study by posting 25 sentences each from variety of language pairs and probing to see the reception on mturk.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF314">
<title id=" W10-0710.xml">can crowds build parallel corpora for machine translation systems </title>
<section> challenges for crowd-sourcing and.  </section>
<citcontext>
<prevsection>
<prevsent>therefore it is difficult to match translation outputs from two turkers or even with gold standard data.
</prevsent>
<prevsent>we therefore need fuzzy matching algorithm to account for lexical choices, synonymy, word ordering and morphological variations.
</prevsent>
</prevsection>
<citsent citstr=" W07-0734 ">
this problem is similar to the task of automatic translation output evaluation and so we use meteor (lavie and agarwal,2007), <papid> W07-0734 </papid>an automatic mt evaluation metric for comparing two sentences.</citsent>
<aftsection>
<nextsent>meteor has an internal aligner that matches words in the sentences given 63 and scores them separately based on whether the match was supported by synonymy, exact match orfuzzy match.
</nextsent>
<nextsent>the scores are then combined to provide global matching score.
</nextsent>
<nextsent>if the score is above threshold ?, we treat the sentences to be equivalent translations of the source sentence.
</nextsent>
<nextsent>we can set the?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF315">
<title id=" W10-0710.xml">can crowds build parallel corpora for machine translation systems </title>
<section> phrase translation.  </section>
<citcontext>
<prevsection>
<prevsent>was helpful.
</prevsent>
<prevsent>for this set of experiments we usethe spanish-english language pair, where the turkers were presented with spanish phrases to translate.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the phrases were selected from the standard phrase tables produced by statistical phrase-basedmt (koehn et al, 2007), <papid> P07-2045 </papid>that was trained on the entire 128k btec corpus for spanish-english.</citsent>
<aftsection>
<nextsent>we computed an entropy score for each entry in the phrase table under the translation probability distributions in both directions and picked the set of 50 1http://www.google.com/transliterate/ 64 type %agreement %gold match out of context 64% 32% in context 68% 33% table 3: phrase translation: spanish-english length count example 1 2 cierras 2 11 vienes aqu 3 26 hay una en 4 8 conocer su decisin 5 4 viene bien esa hora table 4: details of spanish-english phrases used most ambiguous phrases according to this metric.
</nextsent>
<nextsent>table 4 shows sample and the length distribution of the phrases selected for this task.
</nextsent>
<nextsent>5.1 in context vs. out of context.
</nextsent>
<nextsent>we performed two kinds of experiments to study phrase translation and role of context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF316">
<title id=" W09-2109.xml">kscpal a peer learning agent that encourages students to take the initiative </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>collaboration in dialogue has long been researched in computational linguistics (chu-carroll and carberry, 1998; constantino-gonzalez and suthers, 2000; jordan and di eugenio, 1997; lochbaum andsidner, 1990; soller, 2004; vizcano, 2005), however, the study of peer learning from computational perspective is still in the early stages.
</prevsent>
<prevsent>this is an important area of study because peer learning has been shown to be an effective mode of learning, potentially for all of the participants (cohen et al., 1982; brown and palincsar, 1989; birtz et al, 1989; rekrut, 1992).
</prevsent>
</prevsection>
<citsent citstr=" A97-2007 ">
additionally, while there hasbeen focus on using natural language for intelligent tutoring systems (evens et al, 1997; <papid> A97-2007 </papid>graesser et al, 2004; vanlehn et al, 2002), peer to peer interactions are notably different from those of expert novice pairings, especially with respect to the richness of the problem-solving deliberations and ne gotiations.</citsent>
<aftsection>
<nextsent>using natural language in collaborative this work is funded by nsf grants 0536968 and 0536959.
</nextsent>
<nextsent>learning could have profound impact on the way in which educational applications engage students in learning.previous research has suggested several mechanisms that explain why peer learning is effective forall participants.
</nextsent>
<nextsent>among them are: self-directed ex plaining(chi et al, 1994), other-directed explaining (ploetzner et al, 1999; roscoe and chi, 2007) and knowledge co-construction ? kcc for short (haus mann et al, 2004).
</nextsent>
<nextsent>kcc episodes are defined as portions of the dialogue in which students are jointly constructing shared meaning of concept required for problem solving.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF317">
<title id=" W09-2109.xml">kscpal a peer learning agent that encourages students to take the initiative </title>
<section> analysis of peer learning interactions.  </section>
<citcontext>
<prevsection>
<prevsent>using this definition, an outside annotator and one of the authors coded 30 dialogues (ap proximately 46% of the corpus) for kcc episodes.
</prevsent>
<prevsent>this entailed marking the beginning utterance andthe end utterance of such episodes, under the assumption that all intervening utterances do belong to the same kcc episode (otherwise the coder would mark an earlier end for the episode).
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
the resulting interco der reliability, measured with the kappa statistic(carletta, 1996), <papid> J96-2004 </papid>is considered excellent (?</citsent>
<aftsection>
<nextsent>= 0.80).
</nextsent>
<nextsent>our annotation of initiative was twofold.
</nextsent>
<nextsent>since there is disagreement in the computational linguistics community as to the precise definition of 57 initiative(chu-carroll and carberry, 1998; jordan and di eugenio, 1997), we annotated the dialogues for both dialogue initiative, which tracks who is leading the conversation and determining the current conversational focus, and task initiative, which tracks the lead in problem solving.
</nextsent>
<nextsent>for dialogue initiative annotation, we used the well-known utterance-based rules for allocation of control from (walker and whittaker, 1990).<papid> P90-1010 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF318">
<title id=" W09-2109.xml">kscpal a peer learning agent that encourages students to take the initiative </title>
<section> analysis of peer learning interactions.  </section>
<citcontext>
<prevsection>
<prevsent>our annotation of initiative was twofold.
</prevsent>
<prevsent>since there is disagreement in the computational linguistics community as to the precise definition of 57 initiative(chu-carroll and carberry, 1998; jordan and di eugenio, 1997), we annotated the dialogues for both dialogue initiative, which tracks who is leading the conversation and determining the current conversational focus, and task initiative, which tracks the lead in problem solving.
</prevsent>
</prevsection>
<citsent citstr=" P90-1010 ">
for dialogue initiative annotation, we used the well-known utterance-based rules for allocation of control from (walker and whittaker, 1990).<papid> P90-1010 </papid></citsent>
<aftsection>
<nextsent>in this scheme, each utterance is tagged with one of four dialogue acts (assertion, command, question or prompt) and control is then allocated based on setof rules.
</nextsent>
<nextsent>the dialogue act annotation was done automatically, by marking turns that end in question mark as questions, those that start with verb as commands, prompts from list of commonly used prompts (e.g. ok, yeah) and the remaining turns as assertions.
</nextsent>
<nextsent>to verify that the automatic annotation was good, we manually annotated sizable portion of the dialogues with those four dialogue acts.
</nextsent>
<nextsent>we then compared the automatic annotation against the human gold standard, and we found an excellent accuracy: it ranged from 86% for assertions and questions, to 97% for prompts, to 100% for commands.once the dialogue acts had been automatically annotated, two coders, one of the authors and an out side annotator, coded 24 dialogues (1449 utterances,approximately 45% of the corpus) for dialogue initiative, by using the four control rules from (walker and whittaker, 1990): <papid> P90-1010 </papid>1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF323">
<title id=" W10-0717.xml">using mechanical turk to annotate lexicons for less commonly used languages </title>
<section> inducing translation candidates.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluate the annotations both alone and as feedback into our lexicon induction system.
</prevsent>
<prevsent>various linguistic and corpus cues are helpful for relating word translations across pair of languages.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
a plethora of prior work has exploited orthographic, topic, and contextual similarity, to name few (rapp, 1999; <papid> P99-1067 </papid>fung and yee, 1998; <papid> P98-1069 </papid>koehn and knight, 2000; mimno et al, 2009; <papid> D09-1092 </papid>schafer and yarowsky, 2002; <papid> W02-2026 </papid>haghighi et al, 2008; <papid> P08-1088 </papid>garera etal., 2008).</citsent>
<aftsection>
<nextsent>in this work, our aim is to induce translation candidates for further mturk annotation for large number of language pairs with varying degrees of relatedness and resource availability.
</nextsent>
<nextsent>therefore, we opt for simple and language agnostic approach of using contextual information to score translation sand discover set of candidates for further annotation.
</nextsent>
<nextsent>table 1 shows our 42 languages of interest and the number of wikipedia articles with inter lin gual links to their english counterparts.
</nextsent>
<nextsent>the idea is that tokens which tend to appear in the context of given type in one language should be similar to contextual tokens of its translation in the other language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF324">
<title id=" W10-0717.xml">using mechanical turk to annotate lexicons for less commonly used languages </title>
<section> inducing translation candidates.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluate the annotations both alone and as feedback into our lexicon induction system.
</prevsent>
<prevsent>various linguistic and corpus cues are helpful for relating word translations across pair of languages.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
a plethora of prior work has exploited orthographic, topic, and contextual similarity, to name few (rapp, 1999; <papid> P99-1067 </papid>fung and yee, 1998; <papid> P98-1069 </papid>koehn and knight, 2000; mimno et al, 2009; <papid> D09-1092 </papid>schafer and yarowsky, 2002; <papid> W02-2026 </papid>haghighi et al, 2008; <papid> P08-1088 </papid>garera etal., 2008).</citsent>
<aftsection>
<nextsent>in this work, our aim is to induce translation candidates for further mturk annotation for large number of language pairs with varying degrees of relatedness and resource availability.
</nextsent>
<nextsent>therefore, we opt for simple and language agnostic approach of using contextual information to score translation sand discover set of candidates for further annotation.
</nextsent>
<nextsent>table 1 shows our 42 languages of interest and the number of wikipedia articles with inter lin gual links to their english counterparts.
</nextsent>
<nextsent>the idea is that tokens which tend to appear in the context of given type in one language should be similar to contextual tokens of its translation in the other language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF325">
<title id=" W10-0717.xml">using mechanical turk to annotate lexicons for less commonly used languages </title>
<section> inducing translation candidates.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluate the annotations both alone and as feedback into our lexicon induction system.
</prevsent>
<prevsent>various linguistic and corpus cues are helpful for relating word translations across pair of languages.
</prevsent>
</prevsection>
<citsent citstr=" D09-1092 ">
a plethora of prior work has exploited orthographic, topic, and contextual similarity, to name few (rapp, 1999; <papid> P99-1067 </papid>fung and yee, 1998; <papid> P98-1069 </papid>koehn and knight, 2000; mimno et al, 2009; <papid> D09-1092 </papid>schafer and yarowsky, 2002; <papid> W02-2026 </papid>haghighi et al, 2008; <papid> P08-1088 </papid>garera etal., 2008).</citsent>
<aftsection>
<nextsent>in this work, our aim is to induce translation candidates for further mturk annotation for large number of language pairs with varying degrees of relatedness and resource availability.
</nextsent>
<nextsent>therefore, we opt for simple and language agnostic approach of using contextual information to score translation sand discover set of candidates for further annotation.
</nextsent>
<nextsent>table 1 shows our 42 languages of interest and the number of wikipedia articles with inter lin gual links to their english counterparts.
</nextsent>
<nextsent>the idea is that tokens which tend to appear in the context of given type in one language should be similar to contextual tokens of its translation in the other language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF326">
<title id=" W10-0717.xml">using mechanical turk to annotate lexicons for less commonly used languages </title>
<section> inducing translation candidates.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluate the annotations both alone and as feedback into our lexicon induction system.
</prevsent>
<prevsent>various linguistic and corpus cues are helpful for relating word translations across pair of languages.
</prevsent>
</prevsection>
<citsent citstr=" W02-2026 ">
a plethora of prior work has exploited orthographic, topic, and contextual similarity, to name few (rapp, 1999; <papid> P99-1067 </papid>fung and yee, 1998; <papid> P98-1069 </papid>koehn and knight, 2000; mimno et al, 2009; <papid> D09-1092 </papid>schafer and yarowsky, 2002; <papid> W02-2026 </papid>haghighi et al, 2008; <papid> P08-1088 </papid>garera etal., 2008).</citsent>
<aftsection>
<nextsent>in this work, our aim is to induce translation candidates for further mturk annotation for large number of language pairs with varying degrees of relatedness and resource availability.
</nextsent>
<nextsent>therefore, we opt for simple and language agnostic approach of using contextual information to score translation sand discover set of candidates for further annotation.
</nextsent>
<nextsent>table 1 shows our 42 languages of interest and the number of wikipedia articles with inter lin gual links to their english counterparts.
</nextsent>
<nextsent>the idea is that tokens which tend to appear in the context of given type in one language should be similar to contextual tokens of its translation in the other language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF327">
<title id=" W10-0717.xml">using mechanical turk to annotate lexicons for less commonly used languages </title>
<section> inducing translation candidates.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluate the annotations both alone and as feedback into our lexicon induction system.
</prevsent>
<prevsent>various linguistic and corpus cues are helpful for relating word translations across pair of languages.
</prevsent>
</prevsection>
<citsent citstr=" P08-1088 ">
a plethora of prior work has exploited orthographic, topic, and contextual similarity, to name few (rapp, 1999; <papid> P99-1067 </papid>fung and yee, 1998; <papid> P98-1069 </papid>koehn and knight, 2000; mimno et al, 2009; <papid> D09-1092 </papid>schafer and yarowsky, 2002; <papid> W02-2026 </papid>haghighi et al, 2008; <papid> P08-1088 </papid>garera etal., 2008).</citsent>
<aftsection>
<nextsent>in this work, our aim is to induce translation candidates for further mturk annotation for large number of language pairs with varying degrees of relatedness and resource availability.
</nextsent>
<nextsent>therefore, we opt for simple and language agnostic approach of using contextual information to score translation sand discover set of candidates for further annotation.
</nextsent>
<nextsent>table 1 shows our 42 languages of interest and the number of wikipedia articles with inter lin gual links to their english counterparts.
</nextsent>
<nextsent>the idea is that tokens which tend to appear in the context of given type in one language should be similar to contextual tokens of its translation in the other language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF329">
<title id=" W10-0717.xml">using mechanical turk to annotate lexicons for less commonly used languages </title>
<section> mechanical turk task.  </section>
<citcontext>
<prevsection>
<prevsent>while longer lists will increase the chance of including correct translations and their morphological variants, they require more effort on the partof annotators.
</prevsent>
<prevsent>to strike reasonable balance, we extracted relatively short candidate lists, but allowed mturk users to type their own translations as well.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
following previous work on posting nlp tasks on mturk (snow et al, 2008; <papid> D08-1027 </papid>callison-burch, 2009), we use the service to gather annotations for proposed bilingual lexicon entries.</citsent>
<aftsection>
<nextsent>for 32 of our 42 languages of interest, we were able to induce lexical translation 1a simple string match is used for projection.
</nextsent>
<nextsent>while we expect that more sophisticated approaches (e.g. exploiting morphological analyses) are likely to help, we cannot assume that such linguistic resources are available for our languages.
</nextsent>
<nextsent>e (1) (2) (3) (k-1) (k) (i) ? ?
</nextsent>
<nextsent>f (1) (2) (3) (n-1) (n) ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF330">
<title id=" W10-0712.xml">annotating large email datasets for named entity recognition with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>additionally, for cer?
</prevsent>
<prevsent>tain applications in natural language processing (nlp), it has been noted that the particular al? gorithms or feature sets used tend to become ir?
</prevsent>
</prevsection>
<citsent citstr=" P01-1005 ">
relevant as the size of the corpus increases (banko and brill 2001).<papid> P01-1005 </papid></citsent>
<aftsection>
<nextsent>it is therefore not sur?
</nextsent>
<nextsent>prising that obtaining large annotated datasets is an issue of great practical importance for the working researcher.
</nextsent>
<nextsent>traditionally, annotated training data have been provided by experts in the field or the researchers themselves, often at great costs in terms of time and money.
</nextsent>
<nextsent>re?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF331">
<title id=" W10-0712.xml">annotating large email datasets for named entity recognition with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>traditionally, annotated training data have been provided by experts in the field or the researchers themselves, often at great costs in terms of time and money.
</prevsent>
<prevsent>re?
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
cently, however, attempts have been made to leverage non expert annotations provided by amazon mechanical turk (mturk) service to create large training corpora at fraction of the usual costs (snow et al  2008).<papid> D08-1027 </papid></citsent>
<aftsection>
<nextsent>the initial results seem promising, and new avenue for enhancing existing sources of annotated data appears to have been opened.
</nextsent>
<nextsent>named entity recognition (ner) is one of the many fields of nlp that relyon machine learn?
</nextsent>
<nextsent>ing methods, and therefore large training cor?
</nextsent>
<nextsent>pora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF332">
<title id=" W10-0712.xml">annotating large email datasets for named entity recognition with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pora.
</prevsent>
<prevsent>indeed, it is field where more is almost always better, as indicated by the traditional use of named entity gazette ers (often culled from ex?
</prevsent>
</prevsection>
<citsent citstr=" H05-1056 ">
ternal sources) to simulate data that would have been inferred from larger training set (minkov et al  2005; <papid> H05-1056 </papid>mikheev et al  1999).<papid> E99-1001 </papid></citsent>
<aftsection>
<nextsent>therefore, it appears to be field that could profit from the enormous bargain price workforce available through mturk.
</nextsent>
<nextsent>it is not immediately obvious, though, that mturk is well suited for the task of ner annota?
</nextsent>
<nextsent>tion.
</nextsent>
<nextsent>commonly, mturk has been used for the classification task (snow et al . 2008) <papid> D08-1027 </papid>or for straightforward data entry.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF333">
<title id=" W10-0712.xml">annotating large email datasets for named entity recognition with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pora.
</prevsent>
<prevsent>indeed, it is field where more is almost always better, as indicated by the traditional use of named entity gazette ers (often culled from ex?
</prevsent>
</prevsection>
<citsent citstr=" E99-1001 ">
ternal sources) to simulate data that would have been inferred from larger training set (minkov et al  2005; <papid> H05-1056 </papid>mikheev et al  1999).<papid> E99-1001 </papid></citsent>
<aftsection>
<nextsent>therefore, it appears to be field that could profit from the enormous bargain price workforce available through mturk.
</nextsent>
<nextsent>it is not immediately obvious, though, that mturk is well suited for the task of ner annota?
</nextsent>
<nextsent>tion.
</nextsent>
<nextsent>commonly, mturk has been used for the classification task (snow et al . 2008) <papid> D08-1027 </papid>or for straightforward data entry.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF335">
<title id=" W10-0712.xml">annotating large email datasets for named entity recognition with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, ner does not fit well into either of these formats.
</prevsent>
<prevsent>as poin?
</prevsent>
</prevsection>
<citsent citstr=" E06-3004 ">
ted out by kozareva (2006), <papid> E06-3004 </papid>ner can be thought of as composition of two subtasks: 1) determin?</citsent>
<aftsection>
<nextsent>ing the start and end boundaries of textual en?
</nextsent>
<nextsent>tity, and 2) determining the label of the identified span.
</nextsent>
<nextsent>the second task is the well understood classification task, but the first task presents subtler problems.
</nextsent>
<nextsent>one is that mturk form?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF338">
<title id=" W10-0712.xml">annotating large email datasets for named entity recognition with mechanical turk </title>
<section> general problem definition.  </section>
<citcontext>
<prevsection>
<prevsent>ally posed as sequence labeling task similar to the partofspeech (pos) tagging or phrase?
</prevsent>
<prevsent>chunking tasks, where each token in the input text corresponds to label in the output, and is solved with sequential classification algorithms (such as crf, svmcmm, or memm).
</prevsent>
</prevsection>
<citsent citstr=" W04-1221 ">
previous works have tackled ner within the biomedical domain (settles 2004), <papid> W04-1221 </papid>newswire do?</citsent>
<aftsection>
<nextsent>main (grishman and sundheim 1996), <papid> C96-1079 </papid>and email domain (minkov et al  2005).<papid> H05-1056 </papid></nextsent>
<nextsent>in this paper, we focus on extracting entities from email text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF339">
<title id=" W10-0712.xml">annotating large email datasets for named entity recognition with mechanical turk </title>
<section> general problem definition.  </section>
<citcontext>
<prevsection>
<prevsent>chunking tasks, where each token in the input text corresponds to label in the output, and is solved with sequential classification algorithms (such as crf, svmcmm, or memm).
</prevsent>
<prevsent>previous works have tackled ner within the biomedical domain (settles 2004), <papid> W04-1221 </papid>newswire do?</prevsent>
</prevsection>
<citsent citstr=" C96-1079 ">
main (grishman and sundheim 1996), <papid> C96-1079 </papid>and email domain (minkov et al  2005).<papid> H05-1056 </papid></citsent>
<aftsection>
<nextsent>in this paper, we focus on extracting entities from email text.
</nextsent>
<nextsent>it should be noted that email text has many distinctive features that create unique challenge when applying ner.
</nextsent>
<nextsent>for one, email text tends to be more informal than either newswire or bio?
</nextsent>
<nextsent>medical text, which reduces the usefulness of learned features that depend on patterns of capit?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF345">
<title id=" W09-1905.xml">evaluating automation strategies in language documentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the igt systems developed by documentation projects tend to be idiosyncratic: they may be linguistically well-motivated and intuitive, but they are unlikely tobe compatible or interchangeable with systems developed by other projects.
</prevsent>
<prevsent>they may lack internal consistency as well.
</prevsent>
</prevsection>
<citsent citstr=" N07-1057 ">
nonetheless, igt in readily accessible format is an important resource thatcan be used fruitfully by linguists to examine hypotheses on novel data (e.g. xia and lewis (2007), <papid> N07-1057 </papid>xia and lewis (2008), <papid> I08-1069 </papid>lewis and xia (2008)).<papid> I08-2093 </papid></citsent>
<aftsection>
<nextsent>furthermore, it can be used by educators and language activists to create curriculum material for mother language education and promote the survival of the language.
</nextsent>
<nextsent>despite the urgent need for such resources, igt annotations are time consuming to create entirely by hand, and both human and financial resources are extremely limited in this domain.
</nextsent>
<nextsent>thus, language 1key: com=completive aspect, dem=demonstrative, dir=directional 36 documentation presents an interesting test case andan ideal context for use of machine labeling and active learning.
</nextsent>
<nextsent>this paper describes series of experiments designed to assess this promise in realistic documentation context: creation of igt for the mayan language uspanteko.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF346">
<title id=" W09-1905.xml">evaluating automation strategies in language documentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the igt systems developed by documentation projects tend to be idiosyncratic: they may be linguistically well-motivated and intuitive, but they are unlikely tobe compatible or interchangeable with systems developed by other projects.
</prevsent>
<prevsent>they may lack internal consistency as well.
</prevsent>
</prevsection>
<citsent citstr=" I08-1069 ">
nonetheless, igt in readily accessible format is an important resource thatcan be used fruitfully by linguists to examine hypotheses on novel data (e.g. xia and lewis (2007), <papid> N07-1057 </papid>xia and lewis (2008), <papid> I08-1069 </papid>lewis and xia (2008)).<papid> I08-2093 </papid></citsent>
<aftsection>
<nextsent>furthermore, it can be used by educators and language activists to create curriculum material for mother language education and promote the survival of the language.
</nextsent>
<nextsent>despite the urgent need for such resources, igt annotations are time consuming to create entirely by hand, and both human and financial resources are extremely limited in this domain.
</nextsent>
<nextsent>thus, language 1key: com=completive aspect, dem=demonstrative, dir=directional 36 documentation presents an interesting test case andan ideal context for use of machine labeling and active learning.
</nextsent>
<nextsent>this paper describes series of experiments designed to assess this promise in realistic documentation context: creation of igt for the mayan language uspanteko.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF347">
<title id=" W09-1905.xml">evaluating automation strategies in language documentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the igt systems developed by documentation projects tend to be idiosyncratic: they may be linguistically well-motivated and intuitive, but they are unlikely tobe compatible or interchangeable with systems developed by other projects.
</prevsent>
<prevsent>they may lack internal consistency as well.
</prevsent>
</prevsection>
<citsent citstr=" I08-2093 ">
nonetheless, igt in readily accessible format is an important resource thatcan be used fruitfully by linguists to examine hypotheses on novel data (e.g. xia and lewis (2007), <papid> N07-1057 </papid>xia and lewis (2008), <papid> I08-1069 </papid>lewis and xia (2008)).<papid> I08-2093 </papid></citsent>
<aftsection>
<nextsent>furthermore, it can be used by educators and language activists to create curriculum material for mother language education and promote the survival of the language.
</nextsent>
<nextsent>despite the urgent need for such resources, igt annotations are time consuming to create entirely by hand, and both human and financial resources are extremely limited in this domain.
</nextsent>
<nextsent>thus, language 1key: com=completive aspect, dem=demonstrative, dir=directional 36 documentation presents an interesting test case andan ideal context for use of machine labeling and active learning.
</nextsent>
<nextsent>this paper describes series of experiments designed to assess this promise in realistic documentation context: creation of igt for the mayan language uspanteko.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF348">
<title id=" W09-1905.xml">evaluating automation strategies in language documentation </title>
<section> data: uspanteko igt.  </section>
<citcontext>
<prevsection>
<prevsent>it was thus necessary to identify complete texts for usein our experiments.
</prevsent>
<prevsent>some missing labels in nearly complete texts were filled in by the expert annotator.a challenge for representing igt in machine readable format is maintaining the links between s=sustantivo (noun), sc=category suffix, suf=suffix, tam=tense/aspect/mood, vt=transitive verb 5http://www.sil.org/computing/shoebox/ the source text morphemes in the second tier and the morpheme-by-morpheme glosses in the thirdtier.
</prevsent>
</prevsection>
<citsent citstr=" W07-1528 ">
the standard shoebox output format, forex ample, enforces these links through management of the number of spaces between items in the output.to address this, we converted the cleaned annotations into igt-xml (palmer and erk, 2007) <papid> W07-1528 </papid>with help from the shoebox/toolbox interfaces provided in the natural language toolkit (robinson et al,2007).</citsent>
<aftsection>
<nextsent>automating the transformation from shoebox format to igt-xmls hierarchical format required cleaning up tier-to-tier alignment and checking segmentation in some cases where morphemes and glosses were misaligned, as in (5) below.6 (4) non li in yolow rkil (5) non dem dem li dem demin yo pron yolow platicar vi r-kil ap suf e3s.-sr pers srel solo asi yo aprendi con el.?
</nextsent>
<nextsent>here, the number of elements in the morpheme tier(first line of (5)) does not match the number of elements in the gloss tier (second line of (5)).
</nextsent>
<nextsent>the problem is mis analysis of yolow: it should be segmented yol-ow with the gloss platicar-ap.
</nextsent>
<nextsent>automating this transformation has the advantage of identifying such inconsistencies and errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF349">
<title id=" W09-1905.xml">evaluating automation strategies in language documentation </title>
<section> integrated annotation and automation.  </section>
<citcontext>
<prevsection>
<prevsent>the processes are managed and coordinated using the opennlp igt editor.7 the annotation component of the tool, and in particular the user interface, is built on the inter linear text editor (lowe et al, 2004).
</prevsent>
<prevsent>for tagging we use strong but simple standard classifier.
</prevsent>
</prevsection>
<citsent citstr=" D08-1112 ">
there certainly are many other modeling strategies that could be used, for example conditional random field (as in settles and craven (2008)), <papid> D08-1112 </papid>or model that deals differently with pos labels and morpheme gloss labels.</citsent>
<aftsection>
<nextsent>nonetheless, documentary linguistics project would be most likely to use straightforward, off-the-shelf labeler, and our focus is on exploring different annotation approaches in realistic documentation setting rather than building an optimal classifier.
</nextsent>
<nextsent>to that end, we use standard maximum entropy classifier which predicts the label for morpheme based on the morpheme itself plusa window of two morphemes before and after.
</nextsent>
<nextsent>standard features used in part-of-speech taggers are extracted from the morpheme to help with predicting labels for previously unseen stems and morphemes.
</nextsent>
<nextsent>3.2 annotators and annotation procedures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF351">
<title id=" W09-1905.xml">evaluating automation strategies in language documentation </title>
<section> integrated annotation and automation.  </section>
<citcontext>
<prevsection>
<prevsent>not all examples take the same amount of effort to annotate.
</prevsent>
<prevsent>even so, the bulk of the literature on active learning assumes some sort of unit cost to determine the effectiveness of different sample selection strategies.
</prevsent>
</prevsection>
<citsent citstr=" W00-1306 ">
examples of unit cost measurements include the number of documents in text classification, the number of sentences in part-of-speech tagging (set tles and craven, 2008), <papid> D08-1112 </papid>or the number of constituent sin parsing (hwa, 2000).<papid> W00-1306 </papid></citsent>
<aftsection>
<nextsent>these measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (haertel et al, 2008<papid> P08-2017 </papid>a; settles et al, 2008), with ngai and yarowsky (2000) <papid> P00-1016 </papid>being an early exception to the unit-cost approach.</nextsent>
<nextsent>also, baldridge and osborne (2004) <papid> W04-3202 </papid>use discriminants in parse selection, which are annotation decisions that they later showed correlate with timing information (baldridge and osborne, 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF352">
<title id=" W09-1905.xml">evaluating automation strategies in language documentation </title>
<section> integrated annotation and automation.  </section>
<citcontext>
<prevsection>
<prevsent>even so, the bulk of the literature on active learning assumes some sort of unit cost to determine the effectiveness of different sample selection strategies.
</prevsent>
<prevsent>examples of unit cost measurements include the number of documents in text classification, the number of sentences in part-of-speech tagging (set tles and craven, 2008), <papid> D08-1112 </papid>or the number of constituent sin parsing (hwa, 2000).<papid> W00-1306 </papid></prevsent>
</prevsection>
<citsent citstr=" P08-2017 ">
these measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (haertel et al, 2008<papid> P08-2017 </papid>a; settles et al, 2008), with ngai and yarowsky (2000) <papid> P00-1016 </papid>being an early exception to the unit-cost approach.</citsent>
<aftsection>
<nextsent>also, baldridge and osborne (2004) <papid> W04-3202 </papid>use discriminants in parse selection, which are annotation decisions that they later showed correlate with timing information (baldridge and osborne, 2008).</nextsent>
<nextsent>the cost of annotation ultimately comes down to 40 money.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF354">
<title id=" W09-1905.xml">evaluating automation strategies in language documentation </title>
<section> integrated annotation and automation.  </section>
<citcontext>
<prevsection>
<prevsent>even so, the bulk of the literature on active learning assumes some sort of unit cost to determine the effectiveness of different sample selection strategies.
</prevsent>
<prevsent>examples of unit cost measurements include the number of documents in text classification, the number of sentences in part-of-speech tagging (set tles and craven, 2008), <papid> D08-1112 </papid>or the number of constituent sin parsing (hwa, 2000).<papid> W00-1306 </papid></prevsent>
</prevsection>
<citsent citstr=" P00-1016 ">
these measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (haertel et al, 2008<papid> P08-2017 </papid>a; settles et al, 2008), with ngai and yarowsky (2000) <papid> P00-1016 </papid>being an early exception to the unit-cost approach.</citsent>
<aftsection>
<nextsent>also, baldridge and osborne (2004) <papid> W04-3202 </papid>use discriminants in parse selection, which are annotation decisions that they later showed correlate with timing information (baldridge and osborne, 2008).</nextsent>
<nextsent>the cost of annotation ultimately comes down to 40 money.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF355">
<title id=" W09-1905.xml">evaluating automation strategies in language documentation </title>
<section> integrated annotation and automation.  </section>
<citcontext>
<prevsection>
<prevsent>examples of unit cost measurements include the number of documents in text classification, the number of sentences in part-of-speech tagging (set tles and craven, 2008), <papid> D08-1112 </papid>or the number of constituent sin parsing (hwa, 2000).<papid> W00-1306 </papid></prevsent>
<prevsent>these measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (haertel et al, 2008<papid> P08-2017 </papid>a; settles et al, 2008), with ngai and yarowsky (2000) <papid> P00-1016 </papid>being an early exception to the unit-cost approach.</prevsent>
</prevsection>
<citsent citstr=" W04-3202 ">
also, baldridge and osborne (2004) <papid> W04-3202 </papid>use discriminants in parse selection, which are annotation decisions that they later showed correlate with timing information (baldridge and osborne, 2008).</citsent>
<aftsection>
<nextsent>the cost of annotation ultimately comes down to 40 money.
</nextsent>
<nextsent>since annotator pay may be variable but will (under standard assumptions) be constant for given annotator, the best approximation of likely cost savings is to measure the time taken to annotate under different levels of automated support.
</nextsent>
<nextsent>this is especially important in sample selection and its interaction with automated suggestions: active learning seeks to find more informative examples, and these will most likely involve more difficult decisions, decreasing annotation quality and/or increasing annotation time (hachey et al, 2005).<papid> W05-0619 </papid></nextsent>
<nextsent>thus, we measure cost in terms of the time taken by each annotator on each example.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF356">
<title id=" W09-1905.xml">evaluating automation strategies in language documentation </title>
<section> integrated annotation and automation.  </section>
<citcontext>
<prevsection>
<prevsent>the cost of annotation ultimately comes down to 40 money.
</prevsent>
<prevsent>since annotator pay may be variable but will (under standard assumptions) be constant for given annotator, the best approximation of likely cost savings is to measure the time taken to annotate under different levels of automated support.
</prevsent>
</prevsection>
<citsent citstr=" W05-0619 ">
this is especially important in sample selection and its interaction with automated suggestions: active learning seeks to find more informative examples, and these will most likely involve more difficult decisions, decreasing annotation quality and/or increasing annotation time (hachey et al, 2005).<papid> W05-0619 </papid></citsent>
<aftsection>
<nextsent>thus, we measure cost in terms of the time taken by each annotator on each example.
</nextsent>
<nextsent>this allows us to measure the actual time taken to produce given labeled dataset, and thus compare the effectiveness of different levels of automated support plus their interaction with annotators of different levels of expertise.recent work shows that paying attention to predicted annotation cost in sample selection itself can increase the effectiveness of active learning (settles et al, 2008; haertel et al, 2008<papid> P08-2017 </papid>b).</nextsent>
<nextsent>though we havenot explored cost-sensitive selection here, the scenario described here is an appropriate test ground for it: in fact, the results of our experiments, reported in the next section, provide strong evidence for real natural language annotation task that active learning selection with cost-sensitivity is indeed sub-optimal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF360">
<title id=" W10-0215.xml">newsviz emotional visualization of news stories </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>commonsense knowledge and mental images need to be structured, related through logical rules and entered into databases before computational text interpretation is possible.
</prevsent>
<prevsent>wordnet (miller, 1995) determines semantic relations between words and is an extended dictionary specifying word relations such as similarity, part-of relations, hierarchy or manner.
</prevsent>
</prevsection>
<citsent citstr=" N03-3009 ">
story segmentation is performed by e.g. select (stokes, 2003), <papid> N03-3009 </papid>an example application based on semantic analysis to find story or sub topic changes within text.</citsent>
<aftsection>
<nextsent>groups of semantically related words called cohesive lexical chains?
</nextsent>
<nextsent>are extracted from text.they are determined through wordnets semantic relations and additionally through statistically acquired co-occurrences (e.g. diego maradonna,hand of god).
</nextsent>
<nextsent>their starting and endpoints indicate topical unit boundaries.
</nextsent>
<nextsent>sensing emotions from multimodal input has mainly been investigated with the objective of developing human-like agents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF361">
<title id=" W10-0215.xml">newsviz emotional visualization of news stories </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>the football commentary system, byrne (binsted and luke, 1999), includes commentator with emotions influenced byhis personality and intentions.
</prevsent>
<prevsent>soccer (retz schmidt, 1988) analyses football scenes visually in order to simultaneously add linguistic descriptions of the events.
</prevsent>
</prevsection>
<citsent citstr=" E06-2010 ">
soba (buitelaar et al, 2006) <papid> E06-2010 </papid>extracts information from soccer match reports, annotates relevant expressions (e.g. players, teams, goals.)</citsent>
<aftsection>
<nextsent>and generates knowledge base entities.
</nextsent>
<nextsent>the collected football knowledge can set preconditions and context to consequently evaluate current events and assign appropriate emotions.
</nextsent>
<nextsent>the moodnewswebsite (mitchell, 2005) demonstrates very simple linguistic method to distinguish positive, negative and neutral content in bbc news headlines.
</nextsent>
<nextsent>it effectively ranks them on color scale between good to bad.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF362">
<title id=" W09-2202.xml">surrogate learning  from feature independence to semi supervised classification </title>
<section> surrogate learning.  </section>
<citcontext>
<prevsection>
<prevsent>p (x1|x2) (x1|y = 1)?
</prevsent>
<prevsent>p (x1|y = 0)(3) we have succeeded in writing (y = 0|x1,x2) as function of (x1|x2) and (x1|y).
</prevsent>
</prevsection>
<citsent citstr=" P02-1046 ">
although this result was previously observed in different context by abney in (abney, 2002), <papid> P02-1046 </papid>he does not use it to derive semi-supervised learning algorithm.</citsent>
<aftsection>
<nextsent>this result can lead to significant simplification of the learning task when large amount of unlabeled data is available.
</nextsent>
<nextsent>the semi-supervised learning algorithm involves the following two steps.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>from unlabeled data learn predictor from the.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF363">
<title id=" W10-0404.xml">the design of a proofreading software service </title>
<section> spell checking.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 sorting suggestions.
</prevsent>
<prevsent>the sorting step relies on score function that accepts typo and suggestion as parameters.
</prevsent>
</prevsection>
<citsent citstr=" P00-1037 ">
the perfect score function calculates the probability of suggestion given the misspelled word (brill and moore, 2000).<papid> P00-1037 </papid></citsent>
<aftsection>
<nextsent>we approximate our scoring function using neural network.
</nextsent>
<nextsent>our neural network is multilayer perceptron network, implemented as described in chapter 4 of programming collective intelligence (segaran, 2007).
</nextsent>
<nextsent>we created training dataset for our spelling corrector by combining misspelled words from the wpcm list with random sentences from wikipedia.
</nextsent>
<nextsent>our neural network sees each typo (wordn) and suggestion pair as several features with values ranging from 0.0 to 1.0.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF364">
<title id=" W10-0404.xml">the design of a proofreading software service </title>
<section> grammar and style checking.  </section>
<citcontext>
<prevsection>
<prevsent>this prevents us from writing rules that reference the sentence subject, verb, and object directly.
</prevsent>
<prevsent>in practice this means were unable to rewrite passive voice for users and create general rules to catch many subject-verb agreement errors.
</prevsent>
</prevsection>
<citsent citstr=" I08-1059 ">
functionally, our grammar and style checker is similar to language tool (naber, 2003) with the exception that it uses the language model to filter suggestions that dont fit the context of the text they replace, similar to work from microsoft research (gamon, et al 2008).<papid> I08-1059 </papid></citsent>
<aftsection>
<nextsent>5.1 text segmentation.
</nextsent>
<nextsent>our text segmentation function uses rule-based approach similar to yona (2002) to split raw text into paragraphs, sentences, and words.
</nextsent>
<nextsent>the segmentation is good enough for most purposes.
</nextsent>
<nextsent>because our sentence segmentation is wrong at times, we do not notify user when they fail to capitalize the first word in sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF365">
<title id=" W10-0404.xml">the design of a proofreading software service </title>
<section> grammar and style checking.  </section>
<citcontext>
<prevsection>
<prevsent>a tag is hint about the grammatical category of the word.
</prevsent>
<prevsent>such tagging allows grammar and style rules to reference all nouns or all verbs rather than having to account for individual words.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
our system uses the penn tagset (marcus et al  1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>the/dt little/jj dog/nn laughed/vbd here we have tagged the sentence the little dog laughed.
</nextsent>
<nextsent>the is labeled as determiner, little is an adjective, dog is noun, and laughed is past tense verb.
</nextsent>
<nextsent>we can reference little, large, and mean laughing dogs with the pattern the .*/jj dog laughed.
</nextsent>
<nextsent>our grammar checker separates phrases and tags with forward slash character.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF366">
<title id=" W10-0404.xml">the design of a proofreading software service </title>
<section> grammar and style checking.  </section>
<citcontext>
<prevsection>
<prevsent>this alternate model uses the last three letters of the word.
</prevsent>
<prevsent>again the goal is to maximize this probability.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
we apply rules from brills tagger (brill, 1995) <papid> J95-4004 </papid>to fix some cases of known incorrect tagging.</citsent>
<aftsection>
<nextsent>table 5 compares our tagger accuracy for known and unknown words to probabilistic tagger that maximizes p(tagn|wordn) only.
</nextsent>
<nextsent>tagger known unknown probability tagger 91.9% 72.9% trigram tagger 94.0% 76.7% table 5.
</nextsent>
<nextsent>pos tagger accuracy.
</nextsent>
<nextsent>to train the tagger we created training and testing datasets by running the stanford pos tagger (toutanova and manning, 2000) <papid> W00-1308 </papid>against the wikipedia and project gutenberg corpus data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF367">
<title id=" W10-0404.xml">the design of a proofreading software service </title>
<section> grammar and style checking.  </section>
<citcontext>
<prevsection>
<prevsent>tagger known unknown probability tagger 91.9% 72.9% trigram tagger 94.0% 76.7% table 5.
</prevsent>
<prevsent>pos tagger accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W00-1308 ">
to train the tagger we created training and testing datasets by running the stanford pos tagger (toutanova and manning, 2000) <papid> W00-1308 </papid>against the wikipedia and project gutenberg corpus data.</citsent>
<aftsection>
<nextsent>5.3 rule engine.
</nextsent>
<nextsent>it helps to think of grammar checker aslan guage for describing phrases.
</nextsent>
<nextsent>phrases that match grammar rule return suggestions that are transforms of the matched phrase.
</nextsent>
<nextsent>some rules are simple string substitutions (e.g., utilized ? used).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF368">
<title id=" W10-0404.xml">the design of a proofreading software service </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>in these instances we made the choice to trade accuracy for speed.
</prevsent>
<prevsent>when implementing the smarts of our system, weve opted to use simpler algorithms and focus on acquiring more data and increasing the quality of data our system learns from.
</prevsent>
</prevsection>
<citsent citstr=" P01-1005 ">
as others have pointed out (banko and brill, 2001), <papid> P01-1005 </papid>with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods.</citsent>
<aftsection>
<nextsent>our real-word error detector is an example of simplicity over complexity.
</nextsent>
<nextsent>with our simple trigram language model, we were able to correct nearly quarter of the errors in the dyslexic writer corpus.
</nextsent>
<nextsent>we could improve the performance of our real-word error corrector simply by adding more confusion sets.
</nextsent>
<nextsent>we define do what works?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF369">
<title id=" W09-2418.xml">semeval2010 task 13 evaluating events time expressions and temporal relations tempeval2 </title>
<section> tempeval-1.  </section>
<citcontext>
<prevsection>
<prevsent>tempeval-1 consisted of three tasks: a. determine the relation between an event and timex in the same sentence; b. determine the relation between an event and the document creation time; c. determine the relation between the main events of two consecutive sentences.the datasets were based on time bank (pustejovsky et al, 2003; boguraev et al, 2007), hand built gold standard of annotated texts using the timeml markup scheme.1 the datasets included sentence boundaries, timex3 tags (including the special document creation time tag), and event tags.
</prevsent>
<prevsent>for tasks and b, restricted set of events was used, namely those events that occur more than5 times in timebank.
</prevsent>
</prevsection>
<citsent citstr=" W07-2014 ">
for all three tasks, the relation labels used were before, after, overlap, before-or-overlap, overlap-or-after and vague.2 for more elaborate description of tempeval-1, see (verhagen et al, 2007; <papid> W07-2014 </papid>verhagen et al, 2009).1see www.timeml.org for details on timeml, time bank is distributed free of charge by the linguistic data consortium (www.ldc.upenn.edu), catalog number ldc2006t08.</citsent>
<aftsection>
<nextsent>2which is different from the set of 13 labels from timeml.
</nextsent>
<nextsent>the set of labels for tempeval-1 was simplified to aid data preparation and to reduce the complexity of the task.
</nextsent>
<nextsent>112there were six systems competing in tempe val 1: university of colorado at boulder (cu-tmp); language computer corporation (lcc-te); nara institute of science and technology (naist); university of sheffield (usfd); universities of wolverhampton and allicante (wvali); and xerox research centre europe (xrce-t).
</nextsent>
<nextsent>the difference between these systems was not large, and details of system performance, along with comparisons and evaluation, are presented in (ver hagen et al, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF370">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a thorough studyof the combination of syntactical and word lattice reordering approaches is another novelty of the paper.
</prevsent>
<prevsent>many reordering algorithms have appeared over the past few years.
</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
word class-based reordering was part of ochs alignment template system (och et al., 2004); <papid> N04-1021 </papid>the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order.</citsent>
<aftsection>
<nextsent>the state-of-the-artsmt system moses implements distance-based reordering model (koehn et al, 2003) <papid> N03-1017 </papid>and distortion model, operating with rewrite patterns extracted from phrase alignment table (tillman, 2004).many smt models implement the brute force approach, introducing several constrains for there ordering search as described in kanthak et al (2005) <papid> W05-0831 </papid>and crego et al (2005).</nextsent>
<nextsent>the main criticism of such systems is that the constraints are not lexicalized.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF371">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>many reordering algorithms have appeared over the past few years.
</prevsent>
<prevsent>word class-based reordering was part of ochs alignment template system (och et al., 2004); <papid> N04-1021 </papid>the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order.</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
the state-of-the-artsmt system moses implements distance-based reordering model (koehn et al, 2003) <papid> N03-1017 </papid>and distortion model, operating with rewrite patterns extracted from phrase alignment table (tillman, 2004).many smt models implement the brute force approach, introducing several constrains for there ordering search as described in kanthak et al (2005) <papid> W05-0831 </papid>and crego et al (2005).</citsent>
<aftsection>
<nextsent>the main criticism of such systems is that the constraints are not lexicalized.
</nextsent>
<nextsent>recently there has been interest in smt exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement.
</nextsent>
<nextsent>the variety of such models includes constrained distance-based reordering (costa-juss?
</nextsent>
<nextsent>et al, 2006); and constrained version of distortion model where the reordering search problem is tackled through set of linguistically motivated rules used during decoding (crego and mario, 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF372">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>many reordering algorithms have appeared over the past few years.
</prevsent>
<prevsent>word class-based reordering was part of ochs alignment template system (och et al., 2004); <papid> N04-1021 </papid>the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order.</prevsent>
</prevsection>
<citsent citstr=" W05-0831 ">
the state-of-the-artsmt system moses implements distance-based reordering model (koehn et al, 2003) <papid> N03-1017 </papid>and distortion model, operating with rewrite patterns extracted from phrase alignment table (tillman, 2004).many smt models implement the brute force approach, introducing several constrains for there ordering search as described in kanthak et al (2005) <papid> W05-0831 </papid>and crego et al (2005).</citsent>
<aftsection>
<nextsent>the main criticism of such systems is that the constraints are not lexicalized.
</nextsent>
<nextsent>recently there has been interest in smt exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement.
</nextsent>
<nextsent>the variety of such models includes constrained distance-based reordering (costa-juss?
</nextsent>
<nextsent>et al, 2006); and constrained version of distortion model where the reordering search problem is tackled through set of linguistically motivated rules used during decoding (crego and mario, 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF373">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>78 quite popular class of reordering algorithms is monotonization of the source part of the parallel corpus prior to translation.
</prevsent>
<prevsent>the first work on this approach is described in nieen and ney (2004),where morpho-syntactic information was used to account for the reorderings needed.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
a representative set of similar systems includes: set of hand-crafted reordering patterns for german-to-english (collins et al, 2005) <papid> P05-1066 </papid>and chinese-english (wang et al,2007) <papid> D07-1077 </papid>translations, emphasizing the distinction between german/chinese and english clause structure; and statistical machine reordering (smr) technique where monotonization of the source words sequence is performed by translating them into the reordered one using well established smt mechanism (costa-juss?</citsent>
<aftsection>
<nextsent>and fonollosa, 2006).
</nextsent>
<nextsent>coupling of smr algorithm and the search space extension via generating set of weighted reordering hypotheses has demonstrated significant improvement, as shown in costa-juss?
</nextsent>
<nextsent>and fonollosa (2008).
</nextsent>
<nextsent>the technique proposed in this study is most similar to the one proposed for french-to-english translation task in xia and mccord (2004), <papid> C04-1073 </papid>where the authors present hybrid system for french english translation based on the principle of automatic rewrite patterns extraction using parse tree and phrase alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF374">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>78 quite popular class of reordering algorithms is monotonization of the source part of the parallel corpus prior to translation.
</prevsent>
<prevsent>the first work on this approach is described in nieen and ney (2004),where morpho-syntactic information was used to account for the reorderings needed.
</prevsent>
</prevsection>
<citsent citstr=" D07-1077 ">
a representative set of similar systems includes: set of hand-crafted reordering patterns for german-to-english (collins et al, 2005) <papid> P05-1066 </papid>and chinese-english (wang et al,2007) <papid> D07-1077 </papid>translations, emphasizing the distinction between german/chinese and english clause structure; and statistical machine reordering (smr) technique where monotonization of the source words sequence is performed by translating them into the reordered one using well established smt mechanism (costa-juss?</citsent>
<aftsection>
<nextsent>and fonollosa, 2006).
</nextsent>
<nextsent>coupling of smr algorithm and the search space extension via generating set of weighted reordering hypotheses has demonstrated significant improvement, as shown in costa-juss?
</nextsent>
<nextsent>and fonollosa (2008).
</nextsent>
<nextsent>the technique proposed in this study is most similar to the one proposed for french-to-english translation task in xia and mccord (2004), <papid> C04-1073 </papid>where the authors present hybrid system for french english translation based on the principle of automatic rewrite patterns extraction using parse tree and phrase alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF375">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>coupling of smr algorithm and the search space extension via generating set of weighted reordering hypotheses has demonstrated significant improvement, as shown in costa-juss?
</prevsent>
<prevsent>and fonollosa (2008).
</prevsent>
</prevsection>
<citsent citstr=" C04-1073 ">
the technique proposed in this study is most similar to the one proposed for french-to-english translation task in xia and mccord (2004), <papid> C04-1073 </papid>where the authors present hybrid system for french english translation based on the principle of automatic rewrite patterns extraction using parse tree and phrase alignments.</citsent>
<aftsection>
<nextsent>we propose using word distortion model not only to monotonize the source part of the corpus (using different approach to rewrite rule organization from xia and mccord), but also to extend the search space during decoding.
</nextsent>
<nextsent>the reference system which was used as translation mechanism is the state-of-the-art moses-based smt (koehn et al, 2007).
</nextsent>
<nextsent>the training and weights tuning procedures can be found on the moses web page1.
</nextsent>
<nextsent>classical phrase-based translation is considered as three step algorithm: (1) the source sequence of words is segmented into phrases, (2) each phra seis translated into the target language using translation table, (3) the target phrases are reordered to fit the target language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF376">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> baseline phrase-based smt systems.  </section>
<citcontext>
<prevsection>
<prevsent>classical phrase-based translation is considered as three step algorithm: (1) the source sequence of words is segmented into phrases, (2) each phra seis translated into the target language using translation table, (3) the target phrases are reordered to fit the target language.
</prevsent>
<prevsent>the probabilities of the phrases are estimated by relative frequencies of their appearance in the training corpus.
</prevsent>
</prevsection>
<citsent citstr=" N04-4026 ">
1http://www.statmt.org/moses/in baseline experiments we used phrase dependent lexicalized reordering model, as proposed in tillmann (2004).<papid> N04-4026 </papid></citsent>
<aftsection>
<nextsent>according to this model, monotonic or reordered local orientations enriched with probabilities are learned from training data.
</nextsent>
<nextsent>during decoding, translation is viewed as monotone block sequence generation process with the possibility to swap pair of neighbor blocks.
</nextsent>
<nextsent>word graph our syntax-based reordering system requires access to source and target language parse trees and word alignments intersections.
</nextsent>
<nextsent>4.1 notation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF377">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> syntax-based reordering coupled with.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 rules extraction.
</prevsent>
<prevsent>concept.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
inspired by the ideas presented in imamura et al (2005), where monolingual correspondences of syntactic nodes are used during decoding, we extract set of bilingual patterns allowing for reordering as described below: 79 (1) align the monotone bilingual corpus with giza++ (och and ney, 2003) <papid> J03-1002 </papid>and find the intersection of direct and inverse word alignments, resulting in the construction of the projection matrix (see below)); (2) parse the source and the target parts of the parallel corpus;(3) extract reordering patterns from the parallel non-isomorphic cfg-trees based on the word alignment intersection.</citsent>
<aftsection>
<nextsent>step 2 is straightforward; we explain aspects of steps 1 and 3 in more detail below.
</nextsent>
<nextsent>figures 1 and 2 show an example of the extraction of two lexicalized rules for parallel arabic-english sentence: arabic: english: h*a this hw is fndq your +k hotel we use this below in our explanations.
</nextsent>
<nextsent>figure 2: example of subtree transfer and reordering rules extraction.projection matrix.
</nextsent>
<nextsent>bilingual content can be represented in the form of words or sequences of words depending on the syntactic role of the corresponding grammatical element (constituent or pos).given two parse trees and word alignment intersection, projection matrix is defined as an n matrix such that is the number of words in the target phrase; is the number of words in the source phrase; and cell (i, j) has value based on the alignment intersection ? this value is zero if word and word do not align, and is unique non-zero link number if they do.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF378">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> syntax-based reordering coupled with.  </section>
<citcontext>
<prevsection>
<prevsent>for example, for the pair of patterns with the same lexicon (which is empty for general rule leading to recurring contradiction np@0 vp@1 ? vp@1 np@0 p1, vp@0 np@1?
</prevsent>
<prevsent>np@1 vp@0 p2 ), the less probable rule is removed.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
finally, there are three resulting parameter tables analogous to the  r-table  as stated in (yamada and knight, 2001), <papid> P01-1067 </papid>consisting of pos- and constituent based patterns allowing for reordering and monotone distortion (examples can be found in table 5).</citsent>
<aftsection>
<nextsent>4.4 source-side monotonization.
</nextsent>
<nextsent>rule application is performed as bottom-up parse tree traversal following two principles: (1) the longest possible rule is applied, i.e. among set of nested rules, the rule with longest left-side covering is selected.
</nextsent>
<nextsent>for example, in the case of the appearance of an nn jj rb sequence and presence of the two reordering rules nn@0 jj@1 ? ... and nn@0 jj@1 rb@2 ? ... the latter pattern will be applied.(2) the rule containing the maximum lexical information is applied, i.e. in case there is more than one alternative pattern from different groups, the lexicalized rules have preference over the partially lexicalized, and partially lexicalized over general ones.
</nextsent>
<nextsent>figure 4: reordered source-side parse tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF379">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> syntax-based reordering coupled with.  </section>
<citcontext>
<prevsection>
<prevsent>in order to improve reordering power of the translation system, we implemented an additional reordering as described in crego and mario (2006).multiple word segment ations is encoded in lattice, which is then passed to the input of the decoder, containing reordering alternatives consistent with the previously extracted rules.
</prevsent>
<prevsent>the decoder takes the n-best reordering of source sentence coded in the form of word lattice.
</prevsent>
</prevsection>
<citsent citstr=" P08-2020 ">
this approach is in line with recent research tendencies in smt, as described for example in (hildebrand et al, 2008; <papid> P08-2020 </papid>xu et al, 2005).</citsent>
<aftsection>
<nextsent>originally, word lattice algorithms do not involve syntax into reordering process, therefore their reordering power is limited at representing long-distance reordering.
</nextsent>
<nextsent>our approach is designed in the spirit of hybrid mt, integrating syntax transfer approach and statistical word lattice methods to achieve better mt performance on the basis of the standard state-of-the-art models.
</nextsent>
<nextsent>during training set of word permutation pattern sis automatically learned following given word-to word alignment.
</nextsent>
<nextsent>since the original and monotonized (reordered) alignments may vary, different sets of reordering patterns are generated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF380">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>both datasets have 4 reference translations per sentence.
</prevsent>
<prevsent>5.2 arabic data preprocessing.
</prevsent>
</prevsection>
<citsent citstr=" N06-2013 ">
we took similar approach to that shown in habash and sadat (2006), <papid> N06-2013 </papid>using the mada+tokan system for disambiguation and tokenization.</citsent>
<aftsection>
<nextsent>for disambiguation only dia critic unigram statistics were employed.
</nextsent>
<nextsent>for tokenization we used the d3 scheme with -tagbies option.
</nextsent>
<nextsent>the scheme splits the following set of clitics: w+, f+, b+, k+, l+, al+ and pronominal clitics.
</nextsent>
<nextsent>the -tagbies option produces bies pos tags on all tag gable tokens.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF381">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>the -tagbies option produces bies pos tags on all tag gable tokens.
</prevsent>
<prevsent>5.3 experimental setup.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we used the stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>for both languages, penn english treebank (marcus et al, 1993) <papid> J93-2004 </papid>and penn arabic tree bank set (kulick et al, 2006).</citsent>
<aftsection>
<nextsent>the english treebank is provided with 48 pos and 14 syntactic tags, the arabic treebank has 26 pos and 23 syntactic categories.
</nextsent>
<nextsent>as mentioned above, specific rules are not pruned away due to limited amount of training material we set the thresholds kpart and kgener to relatively low values, 1 and 3, respectively.
</nextsent>
<nextsent>evaluation conditions were case-insensitive andwith punctuation marks considered.
</nextsent>
<nextsent>the target side 4-gram language model was estimated using the srilm toolkit (stolcke, 2002) and modified kneser-ney discounting with interpolation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF382">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>the -tagbies option produces bies pos tags on all tag gable tokens.
</prevsent>
<prevsent>5.3 experimental setup.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
we used the stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>for both languages, penn english treebank (marcus et al, 1993) <papid> J93-2004 </papid>and penn arabic tree bank set (kulick et al, 2006).</citsent>
<aftsection>
<nextsent>the english treebank is provided with 48 pos and 14 syntactic tags, the arabic treebank has 26 pos and 23 syntactic categories.
</nextsent>
<nextsent>as mentioned above, specific rules are not pruned away due to limited amount of training material we set the thresholds kpart and kgener to relatively low values, 1 and 3, respectively.
</nextsent>
<nextsent>evaluation conditions were case-insensitive andwith punctuation marks considered.
</nextsent>
<nextsent>the target side 4-gram language model was estimated using the srilm toolkit (stolcke, 2002) and modified kneser-ney discounting with interpolation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF383">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>evaluation conditions were case-insensitive andwith punctuation marks considered.
</prevsent>
<prevsent>the target side 4-gram language model was estimated using the srilm toolkit (stolcke, 2002) and modified kneser-ney discounting with interpolation.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the highest bleu score (papineni et al, 2002) <papid> P02-1040 </papid>was chosen as the optimization criterion.</citsent>
<aftsection>
<nextsent>apart from bleu, standard automatic measure meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>was used for evaluation.</nextsent>
<nextsent>5.4 results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF384">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>the target side 4-gram language model was estimated using the srilm toolkit (stolcke, 2002) and modified kneser-ney discounting with interpolation.
</prevsent>
<prevsent>the highest bleu score (papineni et al, 2002) <papid> P02-1040 </papid>was chosen as the optimization criterion.</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
apart from bleu, standard automatic measure meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>was used for evaluation.</citsent>
<aftsection>
<nextsent>5.4 results.
</nextsent>
<nextsent>the scores considered are: bleu scores obtained for the development set as the final point of the mert procedure (dev), and bleu and meteor scores obtained on test dataset (test).we present btec results (tables 2), characterized by relatively short sentence length, and there 83 sults obtained on the nist corpus (tables 3) with much longer sentences and much need of global reordering.
</nextsent>
<nextsent>dev test bleu bleu meteor plain 48.31 45.02 65.98 bl 48.46 47.10 68.10 sbr 48.75 47.52 67.33 sbr+lattice 48.90 48.78 68.85 table 2: summary of btec experimental results.
</nextsent>
<nextsent>dev test bleu bleu meteor plain 41.83 43.80 62.03 bl 42.68 43.52 62.17 sbr 42.71 44.01 63.29 sbr+lattice 43.05 44.89 63.30 table 3: summary of nist50k experimental results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF385">
<title id=" W09-2310.xml">coupling hierarchical word reordering and decoding in phrase based statistical machine translation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>the meteor score is only slightly better for the sbr configurations in case of btec task; in the case of nist50k the meteor improvement is more evident.
</prevsent>
<prevsent>the general trend is that automatic scores evaluated onthe test set increase with the reordering model complexity.
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
application of the sbr algorithm only (without word lattice decoding) does not allow achieving statistical significance threshold for 95% confidence interval and 1000 re samples (koehn, 2004) <papid> W04-3250 </papid>for either of considered corpora.</citsent>
<aftsection>
<nextsent>however, the sbr+lattice system configuration outperforms the bl by about 1.7 bleu points (3.5%) for btec task and about 1.4 bleu point (3.1%) for nist task.
</nextsent>
<nextsent>these differences is statistically significant.figure 6 demonstrates how two reordering techniques interact within sentence with need for both global and local word permutations.
</nextsent>
<nextsent>5.5 syntax-based rewrite rules.
</nextsent>
<nextsent>as mentioned above, the sbr operates with three groups of reordering rules, which are the product of complete or partial delexicalization of the originally extracted patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF386">
<title id=" W10-0607.xml">an investigation on polysemy and lexical organization of verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>understanding how characteristics like polysemy influence acquisition is essential for the construction of more precise theories.
</prevsent>
<prevsent>therefore, the hypothesis we investigate is that more polysemous words have higher chance of earlier acquisition.
</prevsent>
</prevsection>
<citsent citstr=" W07-0610 ">
for this purpose, we compare data from children and adults from the same linguistic community, native speakers of brazilian portuguese, in an action naming task, looking at lexical evolution by using statistical and topological analysis of the data modeled as graphs (following steyvers and tenenbaum, 2005, and gorman and curran, 2007).<papid> W07-0610 </papid></citsent>
<aftsection>
<nextsent>this approach innovates in the sense that it directly simulates the influence of linguistic factor over the process of lexical evolution.
</nextsent>
<nextsent>52this paper is structured as follows.
</nextsent>
<nextsent>section 2 describes relevant work on computational modeling of language acquisition.
</nextsent>
<nextsent>section 3 presents the materials and methods employed in the exper ments of the present work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF399">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure 1 shows sample input for word aligner (knight, 1997).
</prevsent>
<prevsent>after analyzing thetext, we may conclude, for example, that sprok corresponds to dat in the first sentence pair.word alignment has several downstream consumers.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
one is machine translation, where programs extract translation rules from word-aligned corpora (och and ney, 2004; <papid> J04-4002 </papid>galley et al, 2004;chiang, 2007; quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>other down stream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual ir (schonhofen et al, 2008) or re-score candidate translation outputs (och et al, 2004).<papid> N04-1021 </papid></nextsent>
<nextsent>many methods of automatic alignment have been proposed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF400">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure 1 shows sample input for word aligner (knight, 1997).
</prevsent>
<prevsent>after analyzing thetext, we may conclude, for example, that sprok corresponds to dat in the first sentence pair.word alignment has several downstream consumers.
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
one is machine translation, where programs extract translation rules from word-aligned corpora (och and ney, 2004; <papid> J04-4002 </papid>galley et al, 2004;chiang, 2007; quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>other down stream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual ir (schonhofen et al, 2008) or re-score candidate translation outputs (och et al, 2004).<papid> N04-1021 </papid></nextsent>
<nextsent>many methods of automatic alignment have been proposed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF401">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after analyzing thetext, we may conclude, for example, that sprok corresponds to dat in the first sentence pair.word alignment has several downstream consumers.
</prevsent>
<prevsent>one is machine translation, where programs extract translation rules from word-aligned corpora (och and ney, 2004; <papid> J04-4002 </papid>galley et al, 2004;chiang, 2007; quirk et al, 2005).<papid> P05-1034 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
other down stream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual ir (schonhofen et al, 2008) or re-score candidate translation outputs (och et al, 2004).<papid> N04-1021 </papid></citsent>
<aftsection>
<nextsent>many methods of automatic alignment have been proposed.
</nextsent>
<nextsent>probabilistic generative models like ibm 1-5 (brown et al, 1993), <papid> J93-2003 </papid>hmm (vogel et al, 1996), <papid> C96-2141 </papid>itg (wu, 1997), <papid> J97-3002 </papid>and leaf (fraser and marcu, 2007) <papid> D07-1006 </papid>define formulas for p(f | e) or p(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat figure 1: word alignment exercise (knight, 1997).</nextsent>
<nextsent>28 hidden alignment variables.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF402">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other down stream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual ir (schonhofen et al, 2008) or re-score candidate translation outputs (och et al, 2004).<papid> N04-1021 </papid></prevsent>
<prevsent>many methods of automatic alignment have been proposed.</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
probabilistic generative models like ibm 1-5 (brown et al, 1993), <papid> J93-2003 </papid>hmm (vogel et al, 1996), <papid> C96-2141 </papid>itg (wu, 1997), <papid> J97-3002 </papid>and leaf (fraser and marcu, 2007) <papid> D07-1006 </papid>define formulas for p(f | e) or p(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat figure 1: word alignment exercise (knight, 1997).</citsent>
<aftsection>
<nextsent>28 hidden alignment variables.
</nextsent>
<nextsent>em algorithms estimate dictionary and other probabilities in order to maximize those quantities.
</nextsent>
<nextsent>one can then ask for viterbialignments that maximize p(alignment | e, f).
</nextsent>
<nextsent>discriminative models, e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF403">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other down stream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual ir (schonhofen et al, 2008) or re-score candidate translation outputs (och et al, 2004).<papid> N04-1021 </papid></prevsent>
<prevsent>many methods of automatic alignment have been proposed.</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
probabilistic generative models like ibm 1-5 (brown et al, 1993), <papid> J93-2003 </papid>hmm (vogel et al, 1996), <papid> C96-2141 </papid>itg (wu, 1997), <papid> J97-3002 </papid>and leaf (fraser and marcu, 2007) <papid> D07-1006 </papid>define formulas for p(f | e) or p(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat figure 1: word alignment exercise (knight, 1997).</citsent>
<aftsection>
<nextsent>28 hidden alignment variables.
</nextsent>
<nextsent>em algorithms estimate dictionary and other probabilities in order to maximize those quantities.
</nextsent>
<nextsent>one can then ask for viterbialignments that maximize p(alignment | e, f).
</nextsent>
<nextsent>discriminative models, e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF404">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other down stream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual ir (schonhofen et al, 2008) or re-score candidate translation outputs (och et al, 2004).<papid> N04-1021 </papid></prevsent>
<prevsent>many methods of automatic alignment have been proposed.</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
probabilistic generative models like ibm 1-5 (brown et al, 1993), <papid> J93-2003 </papid>hmm (vogel et al, 1996), <papid> C96-2141 </papid>itg (wu, 1997), <papid> J97-3002 </papid>and leaf (fraser and marcu, 2007) <papid> D07-1006 </papid>define formulas for p(f | e) or p(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat figure 1: word alignment exercise (knight, 1997).</citsent>
<aftsection>
<nextsent>28 hidden alignment variables.
</nextsent>
<nextsent>em algorithms estimate dictionary and other probabilities in order to maximize those quantities.
</nextsent>
<nextsent>one can then ask for viterbialignments that maximize p(alignment | e, f).
</nextsent>
<nextsent>discriminative models, e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF405">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other down stream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual ir (schonhofen et al, 2008) or re-score candidate translation outputs (och et al, 2004).<papid> N04-1021 </papid></prevsent>
<prevsent>many methods of automatic alignment have been proposed.</prevsent>
</prevsection>
<citsent citstr=" D07-1006 ">
probabilistic generative models like ibm 1-5 (brown et al, 1993), <papid> J93-2003 </papid>hmm (vogel et al, 1996), <papid> C96-2141 </papid>itg (wu, 1997), <papid> J97-3002 </papid>and leaf (fraser and marcu, 2007) <papid> D07-1006 </papid>define formulas for p(f | e) or p(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat figure 1: word alignment exercise (knight, 1997).</citsent>
<aftsection>
<nextsent>28 hidden alignment variables.
</nextsent>
<nextsent>em algorithms estimate dictionary and other probabilities in order to maximize those quantities.
</nextsent>
<nextsent>one can then ask for viterbialignments that maximize p(alignment | e, f).
</nextsent>
<nextsent>discriminative models, e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF406">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one can then ask for viterbialignments that maximize p(alignment | e, f).
</prevsent>
<prevsent>discriminative models, e.g.
</prevsent>
</prevsection>
<citsent citstr=" H05-1010 ">
(taskar et al, 2005), <papid> H05-1010 </papid>instead set parameters to maximize alignment accuracy against hand-aligned development set.</citsent>
<aftsection>
<nextsent>emd training (fraser and marcu, 2006) <papid> P06-1097 </papid>combines generative and discriminative elements.</nextsent>
<nextsent>low accuracy is weakness for all systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF407">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discriminative models, e.g.
</prevsent>
<prevsent>(taskar et al, 2005), <papid> H05-1010 </papid>instead set parameters to maximize alignment accuracy against hand-aligned development set.</prevsent>
</prevsection>
<citsent citstr=" P06-1097 ">
emd training (fraser and marcu, 2006) <papid> P06-1097 </papid>combines generative and discriminative elements.</citsent>
<aftsection>
<nextsent>low accuracy is weakness for all systems.
</nextsent>
<nextsent>most practitioners still use 1990s algorithms to align their data.
</nextsent>
<nextsent>it stands to reason that we have not yet seen the last word in alignment models.
</nextsent>
<nextsent>in this paper, we develop new objective function for alignment, inspired by watching people manually solve the alignment exercise of figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF408">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also address another weakness of currentaligners: they only align full words.
</prevsent>
<prevsent>with few exceptions, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P08-1084 ">
(zhang et al, 2003; snyder and barzilay, 2008), <papid> P08-1084 </papid>align ers do not operate at the sub-wordlevel, making them much less useful for agglutina tive languages such as turkish.</citsent>
<aftsection>
<nextsent>our present contributions are as follows: ? we offer simple new objective function that scores corpus alignment based on how many distinct bilingual word pairs it contains.
</nextsent>
<nextsent>we use an integer programming solver to carry out optimization and corpus alignment.?
</nextsent>
<nextsent>we extend the system to perform sub word alignment, which we demonstrate on turkish-english corpus.the results in this paper constitute proof of concept of these ideas, executed on small corpora.
</nextsent>
<nextsent>we conclude by listing future directions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF411">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> new objective function for alignment.  </section>
<citcontext>
<prevsection>
<prevsent>what is legal alignment?
</prevsent>
<prevsent>for now, we consider it to be one where: ? every foreign word is aligned exactly once (brown et al, 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P97-1063 ">
every english word has either 0 or 1 alignments (melamed, 1997).<papid> P97-1063 </papid></citsent>
<aftsection>
<nextsent>we formulate our integer program (ip) as follows.
</nextsent>
<nextsent>we set up two types of binary variables: ? alignment link variables.
</nextsent>
<nextsent>if link-i-j-k = 1, that means in sentence pair i, the foreign word at position aligns to the english words at position k. ? bilingual dictionary variables.
</nextsent>
<nextsent>if dict-f-e = 1,that means word pair (f, e) is in?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF416">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> sub-word alignment.  </section>
<citcontext>
<prevsection>
<prevsent>however, none of the popular machine alignersis able to do this, since they align at the whole word level.
</prevsent>
<prevsent>designers of translation systems sometimes employ language-specific word breakers before alignment, though these are hard to build and maintain, and they are usually not only language specific, but also language-pair-specific.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
good un 31 supervised monolingual morpheme segment ers are also available (goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2005), though again, these do not do joint inference of alignment and word segmentation.we extend our objective function straightforwardly to sub-word alignment.</citsent>
<aftsection>
<nextsent>to test our extension, we construct turkish-english corpus of 1616sentence pairs.
</nextsent>
<nextsent>we first manually construct regular tree grammar (rtg) (gecseg and steinby, 1984) for fragment of english.
</nextsent>
<nextsent>this grammar produces english trees; it has 86 rules, 26 states, and 53 terminals (english words).
</nextsent>
<nextsent>we then construct tree-tostring transducer (rounds, 1970) that converts english trees into turkish character strings, including space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF421">
<title id=" W09-1804.xml">a new objective function for word alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>feature for the discriminative model is the score assigned by an ibm model, which must be separately trained on the full parallel data.
</prevsent>
<prevsent>our work differs in two ways: (1) our training is unsupervised, requiring no manually aligned data, and (2) we do not bootstrap off ibm models.
</prevsent>
</prevsection>
<citsent citstr=" P08-2007 ">
(denero and klein, 2008) <papid> P08-2007 </papid>gives an integer linear programming formulation of another alignment model based on phrases.</citsent>
<aftsection>
<nextsent>there, integer programming is used only for alignment, not for learning parameter values.
</nextsent>
<nextsent>we have presented novel objective function for alignment, and we have applied it to whole-word and sub-word alignment problems.
</nextsent>
<nextsent>preliminary results look good, especially given that new objective function is simpler than those previously proposed.
</nextsent>
<nextsent>the integer programming framework makes the model easy to implement, and its optimal behavior frees us from worrying about search errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF422">
<title id=" W09-2608.xml">using largescale parser output to guide grammar development </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the large size of the corpus in question also brings scala bility concerns to the foreground.
</prevsent>
<prevsent>initial development of rule-based parsers1 is often guided by the grammar writers knowledge of the language and test suites that cover the core?
</prevsent>
</prevsection>
<citsent citstr=" C96-2120 ">
linguistic phenomena of the language (nerbonne et al., 1988; cooper et al, 1996; lehmann et al, 1996).<papid> C96-2120 </papid></citsent>
<aftsection>
<nextsent>once the basic grammar is implemented, including an appropriate lexicon, the direction of grammar development becomes less clear.
</nextsent>
<nextsent>integration of grammar in particular application and the use of particular corpus can guide grammar development: the corpus and application will require the implementation of specific constructions and lexical items, as well as the reevaluation of existing analyses.
</nextsent>
<nextsent>to streamline this sort of output-driven development, tools to examine parser output over large corpora are necessary, andas corpus size increases, the efficiency and scal ability of those tools become crucial concerns.some immediate relevant questions for the grammar writer include: 1the techniques discussed here may also be relevant to purely machine-learned parsers and are certainly applicable to hybrid parsers.
</nextsent>
<nextsent>what constructions and lexical items need to be added for the application and corpus in question?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF423">
<title id=" W09-2608.xml">using largescale parser output to guide grammar development </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an error mining technique presented by van noord (2004) (henceforth: the van noord tool)can reveal gaps in grammar coverage by comparing the frequency of arbitrary n-grams of words in unsuccessfully parsed sentences with the same n-grams in un problematic sentences, for large unannotated corpora.2 parser can be run over new text, and comparison of the in-domain and 2the suffix array error mining software is available at: http://www.let.rug.nl/vannoord/suffixarrays.tgz 63out-of-domain sentences can determine, for instance, that the grammar cannot parse adjective noun hyphenation correctly (e.g. an electrical switch cover).
</prevsent>
<prevsent>a different technique for error mining that uses discriminative tree banking is described in (baldwin et al, 2005).
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
this technique aims at determining issues with lexical coverage, grammatical (rule) coverage, ungrammati cality within the corpus (e.g. misspelled words), and extragrammaticality within the corpus (e.g. bulleted lists).a second approach involves querying gold standard treebanks such as the penn treebank (marcus et al, 1994) <papid> H94-1020 </papid>and tiger treebank (brantset al, 2004) to determine the frequency of certain phenomena.</citsent>
<aftsection>
<nextsent>for example, tiger search (lezius, 2002) can be used to list and frequency sort stacked prepositions (e.g. up to the door) or temporal noun/adverbs after prepositions (e.g. bynow).
</nextsent>
<nextsent>the search tools over these treebanks allow for complex searches involving specification of lexical items, parts of speech, and tree configurations (see (mrovsky?, 2008) for discussion ofquery requirements for searching tree and dependency banks).
</nextsent>
<nextsent>the third approach we discuss here differs from querying gold-standard treebanks in that corpora of actual parser output are queried to examine how constructions are analyzed by the grammar.
</nextsent>
<nextsent>for example, bouma and kloosterman (2002) use xquery (an xml query language) to mine parse results stored as xml data.3 it is this sort of examination of parser output that is the focus of the present paper, and specific examples of our experiences follow in section 2.2.use of such tools has proven vital to the development of large-scale grammars.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF424">
<title id=" W09-2608.xml">using largescale parser output to guide grammar development </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on our experiences with them, we began extensively using tool called oceanography (waterman, 2009)to search parser output for very large (approxi mately 125 million sentence) parse runs stored on distributed file system.
</prevsent>
<prevsent>oceanography queries the parser output and returns counts of specific constructions or properties, as well as the example sentences they were extracted from.
</prevsent>
</prevsection>
<citsent citstr=" W07-1503 ">
in the subsequent sections we discuss how this tool (inconjunction with existing ones like the van no ord tool and tiger search) has enhanced grammar development for an english-language lexical3see also (bouma and kloosterman, 2007) <papid> W07-1503 </papid>for further discussion of this technique.</citsent>
<aftsection>
<nextsent>functional grammar used for semantic search application over wikipedia.
</nextsent>
<nextsent>1.2 the grammar and its role.
</nextsent>
<nextsent>the grammar being developed is lexical functional grammar (lfg (dalrymple, 2001)) that is part of the pargram parallel grammar project (butt et al, 1999; butt et al, 2002).<papid> W02-1503 </papid></nextsent>
<nextsent>it runson the xle system (crouch et al, 2009) and produces c(onstituent)-structures which are trees and f(unctional)-structures which are attribute value matrices recording grammatical functions and other syntactic features such as tense and number, as well as debugging features such as the source of lexical items (e.g. from named entity finder, the morphology, or the guesser).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF425">
<title id=" W09-2608.xml">using largescale parser output to guide grammar development </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>functional grammar used for semantic search application over wikipedia.
</prevsent>
<prevsent>1.2 the grammar and its role.
</prevsent>
</prevsection>
<citsent citstr=" W02-1503 ">
the grammar being developed is lexical functional grammar (lfg (dalrymple, 2001)) that is part of the pargram parallel grammar project (butt et al, 1999; butt et al, 2002).<papid> W02-1503 </papid></citsent>
<aftsection>
<nextsent>it runson the xle system (crouch et al, 2009) and produces c(onstituent)-structures which are trees and f(unctional)-structures which are attribute value matrices recording grammatical functions and other syntactic features such as tense and number, as well as debugging features such as the source of lexical items (e.g. from named entity finder, the morphology, or the guesser).
</nextsent>
<nextsent>there is base grammar which covers the constructions found in standard written english, as well as three overlay grammars: one for parsing wikipedia sentences, one for parsing wikipedia headers, and one for parsing queries (sentential, phrasal, and keyword).the grammar is being used by power set (a microsoft company) in semantic consumer-search reference vertical which allows people to search wikipedia using natural language queries as well as traditional keyword queries.
</nextsent>
<nextsent>the system uses apipeline architecture which includes: text extraction, sentence breaking, named entity detection,parsing (tokenization, morphological analysis, structure, f-structure, ranking), semantic analysis,and indexing of selected semantic facts (see figure 1).
</nextsent>
<nextsent>a similar pipeline is used on the query side except that the resulting semantic analysis is turned into query execution language which is used to query the index.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF426">
<title id=" W09-2608.xml">using largescale parser output to guide grammar development </title>
<section> grammar development.  </section>
<citcontext>
<prevsection>
<prevsent>approximately 1300 of these wereverbs.
</prevsent>
<prevsent>the decision to eliminate verbs as possible guessed parts of speech was directly motivated by data extracted using oceanography.since the guesser works with regular expressions (e.g. lowercase letters + form plural nouns), it is possible to encounter forms in the corpus that neither the morphology nor the guesser recognize.
</prevsent>
</prevsection>
<citsent citstr=" P02-1035 ">
the grammar will fragment on these sentences, creating well-formed f-structurechunks but no single spanning parse, and theun recognized forms will be recorded as tokens(riezler et al, 2002).<papid> P02-1035 </papid></citsent>
<aftsection>
<nextsent>an oceanography run extracting all tokens resulted in the addition of several new patterns to the guesser as well as the addition of some of the frequent forms to the morphology.
</nextsent>
<nextsent>for example, sequences of all upper case letters followed by hyphen and then by sequence of digits were added for forms like ak-47, f-22, and v-1.
</nextsent>
<nextsent>the guesser and tokens oceanography runs look for general problems with the morphology and lexicon, and can be run for every new corpus.
</nextsent>
<nextsent>more specific jobs are run when evaluating whether to implement new analysis, or when evaluating whether current analysis is functioning properly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF427">
<title id=" W09-2410.xml">improvements to monolingual english word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>despite the advances in natural language processing (nlp), word sense disambiguation (wsd) isstill considered one of the most challenging problems in the field.
</prevsent>
<prevsent>ever since the fields inception,wsd has been perceived as one of the central problems in nlp as an enabling technology that could potentially have far reaching impact on nlp applications in general.
</prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
we are starting to see the beginnings of positive effect of wsd in nlp applications such as machine translation (carpuat andwu, 2007; <papid> D07-1007 </papid>chan et al, 2007).<papid> P07-1005 </papid></citsent>
<aftsection>
<nextsent>advances in research on wsd in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as the second author has been partially funded by darpa gale project.
</nextsent>
<nextsent>we would also like to thank the useful comments rendered by three anonymous reviewers.wordnets (fellbaum, 1998; miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized datasets over the past 10 years through the different test beds of senseval and semeval competitions,1 devising more robust computing algorithms to handle large scale datasets, and simply advancement in hardware machinery.
</nextsent>
<nextsent>in this paper, we address the problem of wsd ofall the content words in sentence.
</nextsent>
<nextsent>in this framework, the task is to associate all tokens with their contextually relevant meaning definitions from some computational lexical resource.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF428">
<title id=" W09-2410.xml">improvements to monolingual english word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>despite the advances in natural language processing (nlp), word sense disambiguation (wsd) isstill considered one of the most challenging problems in the field.
</prevsent>
<prevsent>ever since the fields inception,wsd has been perceived as one of the central problems in nlp as an enabling technology that could potentially have far reaching impact on nlp applications in general.
</prevsent>
</prevsection>
<citsent citstr=" P07-1005 ">
we are starting to see the beginnings of positive effect of wsd in nlp applications such as machine translation (carpuat andwu, 2007; <papid> D07-1007 </papid>chan et al, 2007).<papid> P07-1005 </papid></citsent>
<aftsection>
<nextsent>advances in research on wsd in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as the second author has been partially funded by darpa gale project.
</nextsent>
<nextsent>we would also like to thank the useful comments rendered by three anonymous reviewers.wordnets (fellbaum, 1998; miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized datasets over the past 10 years through the different test beds of senseval and semeval competitions,1 devising more robust computing algorithms to handle large scale datasets, and simply advancement in hardware machinery.
</nextsent>
<nextsent>in this paper, we address the problem of wsd ofall the content words in sentence.
</nextsent>
<nextsent>in this framework, the task is to associate all tokens with their contextually relevant meaning definitions from some computational lexical resource.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF429">
<title id=" W09-2410.xml">improvements to monolingual english word sense disambiguation </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>a thorough review of the current state of the art is in (navigli, 2009).
</prevsent>
<prevsent>several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning approaches.
</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
to date, the best approaches that solve the all words wsd task are supervised as illustrated in the different senseeval and semeval all words tasks (m. palmer and dang, 2001; snyder and palmer, 2004; <papid> W04-0811 </papid>pradhan et al, 2007).<papid> W07-2016 </papid>in this paper, we present an unsupervised approach to the all words wsd problem relying on wordnet similarity measures.</citsent>
<aftsection>
<nextsent>we will review only three of the most relevant related research due to space limitations.
</nextsent>
<nextsent>we acknowledge the existence of many research papers that tackled the problem using unsupervised approaches.
</nextsent>
<nextsent>firstly, in work by (pedersen and patwardhan,2005), the authors investigate different word similarity measures as means of disambiguating wordsin context.
</nextsent>
<nextsent>they compare among different similarity measures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF430">
<title id=" W09-2410.xml">improvements to monolingual english word sense disambiguation </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>a thorough review of the current state of the art is in (navigli, 2009).
</prevsent>
<prevsent>several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning approaches.
</prevsent>
</prevsection>
<citsent citstr=" W07-2016 ">
to date, the best approaches that solve the all words wsd task are supervised as illustrated in the different senseeval and semeval all words tasks (m. palmer and dang, 2001; snyder and palmer, 2004; <papid> W04-0811 </papid>pradhan et al, 2007).<papid> W07-2016 </papid>in this paper, we present an unsupervised approach to the all words wsd problem relying on wordnet similarity measures.</citsent>
<aftsection>
<nextsent>we will review only three of the most relevant related research due to space limitations.
</nextsent>
<nextsent>we acknowledge the existence of many research papers that tackled the problem using unsupervised approaches.
</nextsent>
<nextsent>firstly, in work by (pedersen and patwardhan,2005), the authors investigate different word similarity measures as means of disambiguating wordsin context.
</nextsent>
<nextsent>they compare among different similarity measures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF431">
<title id=" W09-2410.xml">improvements to monolingual english word sense disambiguation </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>the majority of the words in this set is polysemous.
</prevsent>
<prevsent>they achieve an f-measure of 41.2% on nouns, 21.2% on verbs, and 25.1% on adjectives.
</prevsent>
</prevsection>
<citsent citstr=" H05-1052 ">
the second related work to ours is the work by (mihalcea, 2005).<papid> H05-1052 </papid></citsent>
<aftsection>
<nextsent>mihalcea (2005) <papid> H05-1052 </papid>introduced graph based unsupervised technique for all word sense disambiguation.</nextsent>
<nextsent>similar to the previous study, the author relied on the similarity of the wordnet entry glosses using the lesk similarity measure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF438">
<title id=" W10-0210.xml">am textual attitude analysis model </title>
<section> introduction and related work.  </section>
<citcontext>
<prevsection>
<prevsent>with rapidly growing online sources aimed at encouraging and stimulating peoples discussions concerning personal, public or social issues (news, blogs, discussion forums, etc.), there is great need in development of computational tool for the analysis of peoples attitudes.
</prevsent>
<prevsent>according to the appraisal theory (martin and white, 2005), attitude types define the specifics of appraisal being expressed: affect (personal emotional state), judgment (social or ethical appraisal of others behav iour), and appreciation (evaluation of phenomena).
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
to analyse contextual sentiment (polarity) of phrase or sentence, rule-based approaches (nasukawa and yi, 2003; mulder et al, 2004; moilanen and pulman, 2007; subrahmanian and reforgiato, 2008), machine-learning method using not only lexical but also syntactic features (wilson et al, 2005), <papid> H05-1044 </papid>and model of integration of machine learning approach with compositional semantics (choi and cardie, 2008) <papid> D08-1083 </papid>were proposed.</citsent>
<aftsection>
<nextsent>with the aim to recognize fine-grained emotions from text on the level of distinct sentences, researchers have employed keyword spotting technique (olveres et al, 1998; chuang and wu, 2004; strapparava et al, 2007), technique calculating emotion scores using pointwise mutual information (pmi) (kozareva et al, 2007), <papid> W07-2072 </papid>an approach inspired by common-sense knowledge (liu et al, 2003), rule-based linguistic approaches (boucou valas, 2003; chaumartin, 2007), <papid> W07-2094 </papid>machine-learning methods (alm, 2008; aman and szpakowicz, 2008; <papid> I08-1041 </papid>strapparava and mihalcea, 2008), and an ensemble based multi-label classification technique (bhowmick et al, 2009).</nextsent>
<nextsent>early attempts to focus on distinct attitude types in the task of attitude analysis were made by taboada and grieve (2004), who determined potential value of adjectives for affect, judgement and appreciation by calculating the pmi with the pro noun-copular pairs was (affect)?, he was (judgement)?, and it was (appreciation)?, and whitelaw et al (2005), who used machine learning technique (svm) with fine-grained semantic distinctions in features (attitude type, orientation) in combination with bag of words?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF439">
<title id=" W10-0210.xml">am textual attitude analysis model </title>
<section> introduction and related work.  </section>
<citcontext>
<prevsection>
<prevsent>with rapidly growing online sources aimed at encouraging and stimulating peoples discussions concerning personal, public or social issues (news, blogs, discussion forums, etc.), there is great need in development of computational tool for the analysis of peoples attitudes.
</prevsent>
<prevsent>according to the appraisal theory (martin and white, 2005), attitude types define the specifics of appraisal being expressed: affect (personal emotional state), judgment (social or ethical appraisal of others behav iour), and appreciation (evaluation of phenomena).
</prevsent>
</prevsection>
<citsent citstr=" D08-1083 ">
to analyse contextual sentiment (polarity) of phrase or sentence, rule-based approaches (nasukawa and yi, 2003; mulder et al, 2004; moilanen and pulman, 2007; subrahmanian and reforgiato, 2008), machine-learning method using not only lexical but also syntactic features (wilson et al, 2005), <papid> H05-1044 </papid>and model of integration of machine learning approach with compositional semantics (choi and cardie, 2008) <papid> D08-1083 </papid>were proposed.</citsent>
<aftsection>
<nextsent>with the aim to recognize fine-grained emotions from text on the level of distinct sentences, researchers have employed keyword spotting technique (olveres et al, 1998; chuang and wu, 2004; strapparava et al, 2007), technique calculating emotion scores using pointwise mutual information (pmi) (kozareva et al, 2007), <papid> W07-2072 </papid>an approach inspired by common-sense knowledge (liu et al, 2003), rule-based linguistic approaches (boucou valas, 2003; chaumartin, 2007), <papid> W07-2094 </papid>machine-learning methods (alm, 2008; aman and szpakowicz, 2008; <papid> I08-1041 </papid>strapparava and mihalcea, 2008), and an ensemble based multi-label classification technique (bhowmick et al, 2009).</nextsent>
<nextsent>early attempts to focus on distinct attitude types in the task of attitude analysis were made by taboada and grieve (2004), who determined potential value of adjectives for affect, judgement and appreciation by calculating the pmi with the pro noun-copular pairs was (affect)?, he was (judgement)?, and it was (appreciation)?, and whitelaw et al (2005), who used machine learning technique (svm) with fine-grained semantic distinctions in features (attitude type, orientation) in combination with bag of words?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF440">
<title id=" W10-0210.xml">am textual attitude analysis model </title>
<section> introduction and related work.  </section>
<citcontext>
<prevsection>
<prevsent>according to the appraisal theory (martin and white, 2005), attitude types define the specifics of appraisal being expressed: affect (personal emotional state), judgment (social or ethical appraisal of others behav iour), and appreciation (evaluation of phenomena).
</prevsent>
<prevsent>to analyse contextual sentiment (polarity) of phrase or sentence, rule-based approaches (nasukawa and yi, 2003; mulder et al, 2004; moilanen and pulman, 2007; subrahmanian and reforgiato, 2008), machine-learning method using not only lexical but also syntactic features (wilson et al, 2005), <papid> H05-1044 </papid>and model of integration of machine learning approach with compositional semantics (choi and cardie, 2008) <papid> D08-1083 </papid>were proposed.</prevsent>
</prevsection>
<citsent citstr=" W07-2072 ">
with the aim to recognize fine-grained emotions from text on the level of distinct sentences, researchers have employed keyword spotting technique (olveres et al, 1998; chuang and wu, 2004; strapparava et al, 2007), technique calculating emotion scores using pointwise mutual information (pmi) (kozareva et al, 2007), <papid> W07-2072 </papid>an approach inspired by common-sense knowledge (liu et al, 2003), rule-based linguistic approaches (boucou valas, 2003; chaumartin, 2007), <papid> W07-2094 </papid>machine-learning methods (alm, 2008; aman and szpakowicz, 2008; <papid> I08-1041 </papid>strapparava and mihalcea, 2008), and an ensemble based multi-label classification technique (bhowmick et al, 2009).</citsent>
<aftsection>
<nextsent>early attempts to focus on distinct attitude types in the task of attitude analysis were made by taboada and grieve (2004), who determined potential value of adjectives for affect, judgement and appreciation by calculating the pmi with the pro noun-copular pairs was (affect)?, he was (judgement)?, and it was (appreciation)?, and whitelaw et al (2005), who used machine learning technique (svm) with fine-grained semantic distinctions in features (attitude type, orientation) in combination with bag of words?
</nextsent>
<nextsent>to classify movie reviews.
</nextsent>
<nextsent>however, the concentration only on adjectives, that express appraisal, and their modifiers, greatly narrows the potential of the whitelaw et al.s (2005) approach.
</nextsent>
<nextsent>in this paper we introduce our system @am (attitude analysis model), which (1) classifies 80 sentences according to the fine-grained attitude labels (nine affect categories (izard, 1971): anger?, disgust?, fear?, guilt?, interest?, joy?, sadness?, shame?, surprise?; four polarity labels for judgment and appreciation: pos jud?, neg jud?, pos app?, neg app?; and neutral?); (2) assigns the strength of the attitude; and (3) determines the level of confidence, with which the attitude is expressed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF441">
<title id=" W10-0210.xml">am textual attitude analysis model </title>
<section> introduction and related work.  </section>
<citcontext>
<prevsection>
<prevsent>according to the appraisal theory (martin and white, 2005), attitude types define the specifics of appraisal being expressed: affect (personal emotional state), judgment (social or ethical appraisal of others behav iour), and appreciation (evaluation of phenomena).
</prevsent>
<prevsent>to analyse contextual sentiment (polarity) of phrase or sentence, rule-based approaches (nasukawa and yi, 2003; mulder et al, 2004; moilanen and pulman, 2007; subrahmanian and reforgiato, 2008), machine-learning method using not only lexical but also syntactic features (wilson et al, 2005), <papid> H05-1044 </papid>and model of integration of machine learning approach with compositional semantics (choi and cardie, 2008) <papid> D08-1083 </papid>were proposed.</prevsent>
</prevsection>
<citsent citstr=" W07-2094 ">
with the aim to recognize fine-grained emotions from text on the level of distinct sentences, researchers have employed keyword spotting technique (olveres et al, 1998; chuang and wu, 2004; strapparava et al, 2007), technique calculating emotion scores using pointwise mutual information (pmi) (kozareva et al, 2007), <papid> W07-2072 </papid>an approach inspired by common-sense knowledge (liu et al, 2003), rule-based linguistic approaches (boucou valas, 2003; chaumartin, 2007), <papid> W07-2094 </papid>machine-learning methods (alm, 2008; aman and szpakowicz, 2008; <papid> I08-1041 </papid>strapparava and mihalcea, 2008), and an ensemble based multi-label classification technique (bhowmick et al, 2009).</citsent>
<aftsection>
<nextsent>early attempts to focus on distinct attitude types in the task of attitude analysis were made by taboada and grieve (2004), who determined potential value of adjectives for affect, judgement and appreciation by calculating the pmi with the pro noun-copular pairs was (affect)?, he was (judgement)?, and it was (appreciation)?, and whitelaw et al (2005), who used machine learning technique (svm) with fine-grained semantic distinctions in features (attitude type, orientation) in combination with bag of words?
</nextsent>
<nextsent>to classify movie reviews.
</nextsent>
<nextsent>however, the concentration only on adjectives, that express appraisal, and their modifiers, greatly narrows the potential of the whitelaw et al.s (2005) approach.
</nextsent>
<nextsent>in this paper we introduce our system @am (attitude analysis model), which (1) classifies 80 sentences according to the fine-grained attitude labels (nine affect categories (izard, 1971): anger?, disgust?, fear?, guilt?, interest?, joy?, sadness?, shame?, surprise?; four polarity labels for judgment and appreciation: pos jud?, neg jud?, pos app?, neg app?; and neutral?); (2) assigns the strength of the attitude; and (3) determines the level of confidence, with which the attitude is expressed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF442">
<title id=" W10-0210.xml">am textual attitude analysis model </title>
<section> introduction and related work.  </section>
<citcontext>
<prevsection>
<prevsent>according to the appraisal theory (martin and white, 2005), attitude types define the specifics of appraisal being expressed: affect (personal emotional state), judgment (social or ethical appraisal of others behav iour), and appreciation (evaluation of phenomena).
</prevsent>
<prevsent>to analyse contextual sentiment (polarity) of phrase or sentence, rule-based approaches (nasukawa and yi, 2003; mulder et al, 2004; moilanen and pulman, 2007; subrahmanian and reforgiato, 2008), machine-learning method using not only lexical but also syntactic features (wilson et al, 2005), <papid> H05-1044 </papid>and model of integration of machine learning approach with compositional semantics (choi and cardie, 2008) <papid> D08-1083 </papid>were proposed.</prevsent>
</prevsection>
<citsent citstr=" I08-1041 ">
with the aim to recognize fine-grained emotions from text on the level of distinct sentences, researchers have employed keyword spotting technique (olveres et al, 1998; chuang and wu, 2004; strapparava et al, 2007), technique calculating emotion scores using pointwise mutual information (pmi) (kozareva et al, 2007), <papid> W07-2072 </papid>an approach inspired by common-sense knowledge (liu et al, 2003), rule-based linguistic approaches (boucou valas, 2003; chaumartin, 2007), <papid> W07-2094 </papid>machine-learning methods (alm, 2008; aman and szpakowicz, 2008; <papid> I08-1041 </papid>strapparava and mihalcea, 2008), and an ensemble based multi-label classification technique (bhowmick et al, 2009).</citsent>
<aftsection>
<nextsent>early attempts to focus on distinct attitude types in the task of attitude analysis were made by taboada and grieve (2004), who determined potential value of adjectives for affect, judgement and appreciation by calculating the pmi with the pro noun-copular pairs was (affect)?, he was (judgement)?, and it was (appreciation)?, and whitelaw et al (2005), who used machine learning technique (svm) with fine-grained semantic distinctions in features (attitude type, orientation) in combination with bag of words?
</nextsent>
<nextsent>to classify movie reviews.
</nextsent>
<nextsent>however, the concentration only on adjectives, that express appraisal, and their modifiers, greatly narrows the potential of the whitelaw et al.s (2005) approach.
</nextsent>
<nextsent>in this paper we introduce our system @am (attitude analysis model), which (1) classifies 80 sentences according to the fine-grained attitude labels (nine affect categories (izard, 1971): anger?, disgust?, fear?, guilt?, interest?, joy?, sadness?, shame?, surprise?; four polarity labels for judgment and appreciation: pos jud?, neg jud?, pos app?, neg app?; and neutral?); (2) assigns the strength of the attitude; and (3) determines the level of confidence, with which the attitude is expressed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF443">
<title id=" W09-2602.xml">developing german semantics on the basis of parallel lfg grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>section 3 discusses the development strategy and the core semantic phenomena covered by the german semantics.
</prevsent>
<prevsent>in section 3.4, we will discuss the benefits and the limitations of the presented architecture forcrosslingual semantics by means of an example phenomenon, the semantics of clause embedding verbs.
</prevsent>
</prevsection>
<citsent citstr=" C04-1180 ">
the rest of this introduction will be devoted to the broader theoretical context of this work.recently, the state of the art in wide coverage parsing has made wide-coverage semantic processing come into the reach of research in computational semantics (bos etal., 2004).<papid> C04-1180 </papid></citsent>
<aftsection>
<nextsent>this shift from the theoretical conception of semantic formalisms to wide-coverage semantic analysis raises many questions about appropriate meaning representations as well as engineering problems concerning the development and evaluation strategies of semantic processing systems.
</nextsent>
<nextsent>the general aim of this work is to explore wide-coverage lfg syntax as backbone for linguistically motivated semantic processing.research in the framework of lfg has traditionally adopted cross lingual perspective on linguistic theory (bresnan, 2000).
</nextsent>
<nextsent>in the context of the pargram project, number of high quality, broad-coverage grammars for several languages have been produced over the years (butt et al , 2002; butt and king, 2007).1 the projects research methodology particularly focusses on parallelism which means that the researchers relyon common syntactic theory as well as development tools, but which also concerns parallelism on the level of syntactic analyses.
</nextsent>
<nextsent>as the lfgformalism assumes two-level syntax that di 1also see the webpage for nice project overview: http://www2.parc.com/isl/groups/nltt/pargram/ 10 vides the analysis into more language and surface dependent constituent structure anda functional structure which basically represents the surface independent grammatical relations of sentence, it constitutes particularly appropriate basis for large-scale, multilingual syntax.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF444">
<title id=" W09-2602.xml">developing german semantics on the basis of parallel lfg grammars </title>
<section> f-structure rewriting as an </section>
<citcontext>
<prevsection>
<prevsent>in this paper.
</prevsent>
<prevsent>the main idea of the system is to convert the surface-independent, syntactic relation sand features encoded in an f-structure to normalized semantic relations.
</prevsent>
</prevsection>
<citsent citstr=" W07-1403 ">
the representation simplifies many phenomena usually discussed in the formal semantic literature (see the next section), but is tailored for use in question answering (bobrow et al , 2007<papid> W07-1403 </papid>a) or textual entailment (bobrow et al , 2007<papid> W07-1403 </papid>b) applications.</citsent>
<aftsection>
<nextsent>the semantic conversion was implemented by means of the xle platform, used for grammar development in the pargramproject.
</nextsent>
<nextsent>it makes use of the built-in transfer module to convert lfg f-structures to semantic representations.
</nextsent>
<nextsent>the idea to use transfer rules to model semantic concstruc tion has also been pursued by (spreyer and frank, 2005) who use the transfer module to model rmrs semantic construction for the german treebank tiger . 2.1 the semantic representation.
</nextsent>
<nextsent>as first example, simplified f-structure analysis for the following sentence and the corresponding semantic representation are given in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF462">
<title id=" W09-2602.xml">developing german semantics on the basis of parallel lfg grammars </title>
<section> tense and aspect.  </section>
<citcontext>
<prevsection>
<prevsent>an example for these mantic representation of clause-embedding verb is given in figure 4.for many semantic applications, such embedded contexts are of particular interest since they often express propositions to 15 ctx_head(ctx(s),schlafen), ctx_index(t,schlafen), in_ctx(t,interrogative(ctx(s))), in_ctx(ctx(s),perf(schlafen)), in_ctx(ctx(s),pres(schlafen)), in_ctx(ctx(s),query_term(wo)), in_ctx(ctx(s),cardinality(tom?,sg)), in_ctx(ctx(s),proper_name(tom?,name,tom?)), in_ctx(ctx(s),role(agent?,schlafen,tom?)), in_ctx(ctx(s),role(adeg,gestern,normal)), in_ctx(ctx(s),role(adeg,wo,normal)), in_ctx(ctx(s),role(amod,schlafen,gestern)), in_ctx(ctx(s),role(amod,schlafen,wo)) ctx_head(ctx(s),sleep), ctx_index(t,sleep), in_ctx(t,interrogative(ctx(s))), in_ctx(ctx(s),past(sleep)), in_ctx(ctx(s),query_term(where)), in_ctx(ctx(s),cardinality(tom?,sg)), in_ctx(ctx(s),time_expr(yesterday,?+?)), in_ctx(ctx(s),proper_name(tom?,name,tom?)), in_ctx(ctx(s),role(agent?,sleep,tom?)), in_ctx(ctx(s),role(occurs_during,sleep,yesterday)), in_ctx(ctx(s),role(prep(where),sleep,where)) figure 3: parallel semantic analyses for the sentence pair given in example (3) whom the speaker is not committed to, i.e. which arent veridical.
</prevsent>
<prevsent>in our system, the veridicality inferences that these embed dings exhibit are computed by further knowledge representation modules that explicit ely represent the speaker commitment of context(bobrow et al , 2007<papid> W07-1403 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W06-3907 ">
concerning the complements of clause-embedding verbs, these inferences are modelled via lexical verb classification that basically distinguishes im plicatives (manage to true - dont manag eto false ) and fact ives (know that true dont know that true ) (nairn et al , 2006).<papid> W06-3907 </papid>veridicality ent ailments of sentential complements are treated as interaction of the lexical class of the subordinating verb and the polarity of the context.</citsent>
<aftsection>
<nextsent>(4) tom glaubt, dass der nachbar ihn nicht erkannt hat.tom believes that the neighbour didnt recognize him.this account of clause-embeddings - unified semantic representation and lexical entailment classification - generalizes and probably simplifies too much the various theoretical insights into the semantics of complementation.
</nextsent>
<nextsent>in the formal semantics literature, various theories opt for semantic representation that assumes several types of abstract semantic entities (e.g. events (parsons, 1990), situations (barwise and perry, 1999) or other, very fine-grained categories (asher,1993) ).
</nextsent>
<nextsent>in terms of entailment, the typologi cal literature reports crosslingually relatively stable distinctions of types of complements according to the semantic relations thema trix verbs have to their complement (givon, 1990).
</nextsent>
<nextsent>for instance, while in example (5),the infinite complement has causal, temporal and spatial relations to the matrix event, there is no such inferential relation between matrix and complement in example (4) .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF463">
<title id=" W10-0212.xml">emotional perception of fairy tales achieving agreement in emotion annotation of text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to investigate these issues, we conducted an experiment to find out the strategies people use to annotate selected folk fairytale texts for emotions.
</prevsent>
<prevsent>the participants had to choose from set of fifteen emotion categories, significantly larger set than typically used in ea, and assign them to an unrestricted range of text.
</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
to explore whether human annotators can reliably perform task, inter-annotator agreement (iaa) (artstein and poesio, 2008) <papid> J08-4004 </papid>is the relevant measure.</citsent>
<aftsection>
<nextsent>this measure can be calculated between every two individual annotations in order to find pairs or even teams of annotators whose strategies seem to be consistent and coherent enough so that they can be used further as the gold-standard annotation suited to train machine learning approach for automatic ea analysis.
</nextsent>
<nextsent>a resulting ea system, capable of simulating human emotional perception of text, would be useful for information retrieval and many other fields.there are two main aspects of the resulting annotations to be researched.
</nextsent>
<nextsent>first, how consistently can people perceive and locate the emotional aspect of fairytale texts?
</nextsent>
<nextsent>second, how do they express their perception of text by means of annotation strategies?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF464">
<title id=" W10-0212.xml">emotional perception of fairy tales achieving agreement in emotion annotation of text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this section is not meant as full overview of the related research as that scopeis too great for the length of this paper.
</prevsent>
<prevsent>to contextu alize the research presented in this paper we focus on the projects that inspired us and fostered the ideas.
</prevsent>
</prevsection>
<citsent citstr=" H05-1073 ">
the work done by alm (alm and sproat, 2005; alm et al, 2005; <papid> H05-1073 </papid>alm, 2008) is close to our project in its sprit and goals.</citsent>
<aftsection>
<nextsent>alm, (2008) aims at implementing affective text-to-speech system for storytelling scenarios.
</nextsent>
<nextsent>an ea system, detecting sentences with emotions expressed in written text is crucial element for achieving this goal.
</nextsent>
<nextsent>the annotated corpus was composed of three sets of childrens stories written by beatrix potter, h. c. andersen, and the brothers grimm.like liu et al (2003), alm (2008) uses several emotional categories, while most research in automatic ea works with pure polarities.
</nextsent>
<nextsent>the set of emotion categories used is essentially the list of basic emotions (ekman, 1993), which has justified preference for negative emotion categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF466">
<title id=" W10-0212.xml">emotional perception of fairy tales achieving agreement in emotion annotation of text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their annotation scheme is also sentence-based.
</prevsent>
<prevsent>the ea system was tested on short user-composed text emails describing emotionally colored events.
</prevsent>
</prevsection>
<citsent citstr=" J09-3003 ">
in the research on recognizing contextual polarity done by wilson et al (2009) <papid> J09-3003 </papid>rich prior-polarity lexicon and dependency parsing technique were employed to detect and analyze subjectivity on phrasal level, taking into account all the power of context, captured through such features as negation, polarity modification and polarity shifters.</citsent>
<aftsection>
<nextsent>the work presents auspicious results of high accuracy scores for classification between neutrality and polarized private states and between negative and positive subjective phrases.
</nextsent>
<nextsent>a detailed account of several ml algorithms performance tests is discussed in thought-provoking manner.
</nextsent>
<nextsent>this work encouraged us to build lexicon of subjective clues and use sentence structure information for future feature extraction and ml architecture training.
</nextsent>
<nextsent>another thought-provoking work by polanyj(2006) shows the influence of the context on subjective clues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF467">
<title id=" W10-0212.xml">emotional perception of fairy tales achieving agreement in emotion annotation of text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(2004) provide valuable information about corpus annotation for ea means and give accounts on the performance of various existing mlalgorithms.
</prevsent>
<prevsent>they provide excellent analysis of automatic extraction of opinion proposition and their holders.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
for feature extraction, the authors employ such well-known resources as wordnet (miller et al., 1990), propbank (kingsbury et al, 2002) and framenet (baker et al, 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>several types of classification tasks involve evaluation on the level of documents.
</nextsent>
<nextsent>for example, detecting subjective sentences, expressions, and other opinionated items in documents representing certain press categories (wiebe et al, 2004) <papid> J04-3002 </papid>and measuring strength of subjective clauses (wilson et al, 2004).</nextsent>
<nextsent>all these and many more helped us to decide upon our own strategies, provided many examples of corpus collection and annotation, feature extraction and ml techniques usage in ways specific for the ea task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF468">
<title id=" W10-0212.xml">emotional perception of fairy tales achieving agreement in emotion annotation of text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for feature extraction, the authors employ such well-known resources as wordnet (miller et al., 1990), propbank (kingsbury et al, 2002) and framenet (baker et al, 1998).<papid> P98-1013 </papid></prevsent>
<prevsent>several types of classification tasks involve evaluation on the level of documents.</prevsent>
</prevsection>
<citsent citstr=" J04-3002 ">
for example, detecting subjective sentences, expressions, and other opinionated items in documents representing certain press categories (wiebe et al, 2004) <papid> J04-3002 </papid>and measuring strength of subjective clauses (wilson et al, 2004).</citsent>
<aftsection>
<nextsent>all these and many more helped us to decide upon our own strategies, provided many examples of corpus collection and annotation, feature extraction and ml techniques usage in ways specific for the ea task.
</nextsent>
<nextsent>99
</nextsent>
<nextsent>having established the research context, we now turn to the questions we investigate in this paper: the use of an enriched category set and the flexible annotation units, and their influence on annotation quality.
</nextsent>
<nextsent>we describe the experiment we conducted and its main results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF470">
<title id=" W10-0205.xml">a corpus based method for extracting paraphrases of emotion terms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we present results of our evaluation and discuss our results in section 5, and finally in section 6 we present the conclusions and future work.
</prevsent>
<prevsent>35 his little boy was so happy to see him princess and she were very glad to visit him table 1: two sentence fragments (candidate contexts) from the emotion class happy, from the blog corpus.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
three main approaches for collecting paraphrases were proposed in the literature: manual collection,utilization of existing lexical resources, and corpus based extraction of expressions that occur in similar contexts (barzilay and mckeown, 2001).<papid> P01-1008 </papid></citsent>
<aftsection>
<nextsent>manually collected paraphrases were used in natural language generation (nlg) (iordanskaja et al, 1991).
</nextsent>
<nextsent>langk ilde et al (1998) used lexical resources in statistical sentence generation, summarization, and question answering.
</nextsent>
<nextsent>barzilay and mckeown (2001) <papid> P01-1008 </papid>used corpus-based method to identify paraphrases from corpus of multiple english translations of the same source text.</nextsent>
<nextsent>our method is similar to this method,but it extracts paraphrases only for particular emotion, and it needs only regular corpus, not parallel corpus of multiple translations.some research has been done in paraphrase extraction for natural language processing and generation for different applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF474">
<title id=" W10-0205.xml">a corpus based method for extracting paraphrases of emotion terms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>barzilay and mckeown (2001) <papid> P01-1008 </papid>used corpus-based method to identify paraphrases from corpus of multiple english translations of the same source text.</prevsent>
<prevsent>our method is similar to this method,but it extracts paraphrases only for particular emotion, and it needs only regular corpus, not parallel corpus of multiple translations.some research has been done in paraphrase extraction for natural language processing and generation for different applications.</prevsent>
</prevsection>
<citsent citstr=" P09-1053 ">
das and smith (2009)<papid> P09-1053 </papid>presented approach to decide whether two sentences hold paraphrase relationship.</citsent>
<aftsection>
<nextsent>they applied generative model that generates paraphrase of given sentence, then used probabilistic inference to reason about whether two sentences share the paraphrase relationship.
</nextsent>
<nextsent>in another research,wang et. al (2009) <papid> P09-2050 </papid>studied the problem of extracting technical paraphrases from parallel software corpus.</nextsent>
<nextsent>their aim was to report duplicate bugs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF475">
<title id=" W10-0205.xml">a corpus based method for extracting paraphrases of emotion terms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>das and smith (2009)<papid> P09-1053 </papid>presented approach to decide whether two sentences hold paraphrase relationship.</prevsent>
<prevsent>they applied generative model that generates paraphrase of given sentence, then used probabilistic inference to reason about whether two sentences share the paraphrase relationship.</prevsent>
</prevsection>
<citsent citstr=" P09-2050 ">
in another research,wang et. al (2009) <papid> P09-2050 </papid>studied the problem of extracting technical paraphrases from parallel software corpus.</citsent>
<aftsection>
<nextsent>their aim was to report duplicate bugs.
</nextsent>
<nextsent>in their method for paraphrase extraction, they used:sentence selection, global context-based and co occurrence-based scoring.
</nextsent>
<nextsent>also, some studies have been done in paraphrase generation in nlg (zhao et al, 2009), (<papid> P09-1094 </papid>chevelu et al, 2009).<papid> P09-2063 </papid></nextsent>
<nextsent>bootstrapping methods have been applied to various natural language applications, for example to word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>lexicon construction for information extraction (riloff and jones, 1999), and named entity classification (collins and singer,1999).<papid> W99-0613 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF476">
<title id=" W10-0205.xml">a corpus based method for extracting paraphrases of emotion terms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their aim was to report duplicate bugs.
</prevsent>
<prevsent>in their method for paraphrase extraction, they used:sentence selection, global context-based and co occurrence-based scoring.
</prevsent>
</prevsection>
<citsent citstr=" P09-1094 ">
also, some studies have been done in paraphrase generation in nlg (zhao et al, 2009), (<papid> P09-1094 </papid>chevelu et al, 2009).<papid> P09-2063 </papid></citsent>
<aftsection>
<nextsent>bootstrapping methods have been applied to various natural language applications, for example to word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>lexicon construction for information extraction (riloff and jones, 1999), and named entity classification (collins and singer,1999).<papid> W99-0613 </papid></nextsent>
<nextsent>in our research, we use the bootstrapping approach to learn paraphrases for emotions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF477">
<title id=" W10-0205.xml">a corpus based method for extracting paraphrases of emotion terms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their aim was to report duplicate bugs.
</prevsent>
<prevsent>in their method for paraphrase extraction, they used:sentence selection, global context-based and co occurrence-based scoring.
</prevsent>
</prevsection>
<citsent citstr=" P09-2063 ">
also, some studies have been done in paraphrase generation in nlg (zhao et al, 2009), (<papid> P09-1094 </papid>chevelu et al, 2009).<papid> P09-2063 </papid></citsent>
<aftsection>
<nextsent>bootstrapping methods have been applied to various natural language applications, for example to word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>lexicon construction for information extraction (riloff and jones, 1999), and named entity classification (collins and singer,1999).<papid> W99-0613 </papid></nextsent>
<nextsent>in our research, we use the bootstrapping approach to learn paraphrases for emotions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF478">
<title id=" W10-0205.xml">a corpus based method for extracting paraphrases of emotion terms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in their method for paraphrase extraction, they used:sentence selection, global context-based and co occurrence-based scoring.
</prevsent>
<prevsent>also, some studies have been done in paraphrase generation in nlg (zhao et al, 2009), (<papid> P09-1094 </papid>chevelu et al, 2009).<papid> P09-2063 </papid></prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
bootstrapping methods have been applied to various natural language applications, for example to word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>lexicon construction for information extraction (riloff and jones, 1999), and named entity classification (collins and singer,1999).<papid> W99-0613 </papid></citsent>
<aftsection>
<nextsent>in our research, we use the bootstrapping approach to learn paraphrases for emotions.
</nextsent>
<nextsent>the text data from which we will extract paraphrases is composed of four concatenated datasets.
</nextsent>
<nextsent>they contain sentences annotated with the six basic emotions.
</nextsent>
<nextsent>the number of sentences in each dataset is presented in table 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF479">
<title id=" W10-0205.xml">a corpus based method for extracting paraphrases of emotion terms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in their method for paraphrase extraction, they used:sentence selection, global context-based and co occurrence-based scoring.
</prevsent>
<prevsent>also, some studies have been done in paraphrase generation in nlg (zhao et al, 2009), (<papid> P09-1094 </papid>chevelu et al, 2009).<papid> P09-2063 </papid></prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
bootstrapping methods have been applied to various natural language applications, for example to word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>lexicon construction for information extraction (riloff and jones, 1999), and named entity classification (collins and singer,1999).<papid> W99-0613 </papid></citsent>
<aftsection>
<nextsent>in our research, we use the bootstrapping approach to learn paraphrases for emotions.
</nextsent>
<nextsent>the text data from which we will extract paraphrases is composed of four concatenated datasets.
</nextsent>
<nextsent>they contain sentences annotated with the six basic emotions.
</nextsent>
<nextsent>the number of sentences in each dataset is presented in table 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF480">
<title id=" W10-0205.xml">a corpus based method for extracting paraphrases of emotion terms </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>we selected only the texts corresponding to the six emotions that we mentioned.
</prevsent>
<prevsent>3.2 text affect dataset.
</prevsent>
</prevsection>
<citsent citstr=" W07-2013 ">
this dataset (strapparava and mihalcea, 2007) <papid> W07-2013 </papid>consists of newspaper headlines that were used in the semeval 2007-task 14.</citsent>
<aftsection>
<nextsent>it includes development dataset of 250 annotated headlines, and test dataset of 1000 news headlines.
</nextsent>
<nextsent>we use all of them.
</nextsent>
<nextsent>the annotations were made with the six basic emotions on intensity scales of [-100, 100], therefore threshold is used to choose the main emotion of each sentence.
</nextsent>
<nextsent>3.3 fairy tales dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF481">
<title id=" W10-0205.xml">a corpus based method for extracting paraphrases of emotion terms </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>the annotations were made with the six basic emotions on intensity scales of [-100, 100], therefore threshold is used to choose the main emotion of each sentence.
</prevsent>
<prevsent>3.3 fairy tales dataset.
</prevsent>
</prevsection>
<citsent citstr=" H05-1073 ">
this dataset consists in 1580 annotated sentences (alm et al, 2005), <papid> H05-1073 </papid>from tales by the grimm brothers, h.c. andersen, and b. potter.</citsent>
<aftsection>
<nextsent>the annotations used the extended set of nine basic emotions of izard (1971).
</nextsent>
<nextsent>we selected only those marked with the six emotions that we focus on.
</nextsent>
<nextsent>3.4 annotated blog dataset.
</nextsent>
<nextsent>we also used the dataset provided by aman and szpakowicz (2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF483">
<title id=" W10-0205.xml">a corpus based method for extracting paraphrases of emotion terms </title>
<section> method for paraphrase extraction.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 preprocessing.
</prevsent>
<prevsent>during preprocessing, html and xml tags are eliminated from the blogs data and other datasets, then the text is tokenized and annotated with part of speech tags.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
we use the stanford part-of-speechtagger and chunker (toutanova et al, 2003) <papid> N03-1033 </papid>to identify noun and verb phrases in the sentences.</citsent>
<aftsection>
<nextsent>in the next step, we use sliding window based on the k-window approach, to identify candidate contexts that contain the target seeds.
</nextsent>
<nextsent>4.2 the k-window algorithm.
</nextsent>
<nextsent>we use the k-window algorithm introduced by bostad (2003) in order to identify all the tokens surrounding specific term in window with size of k. here, we use this approach to extract candidate patterns for each seed, from the sentences.
</nextsent>
<nextsent>we start with one seed and truncate all contexts around the seed within window of words before and words after the seed,until all the seeds are processed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF484">
<title id=" W10-0205.xml">a corpus based method for extracting paraphrases of emotion terms </title>
<section> method for paraphrase extraction.  </section>
<citcontext>
<prevsection>
<prevsent>4.3 feature extraction.
</prevsent>
<prevsent>previous research on word sense disambiguation on contextual analysis has acknowledged several loc aland topical features as good indicators of word properties.
</prevsent>
</prevsection>
<citsent citstr=" W04-2405 ">
these include surrounding words and their part of speech tags, collocations, keywords in contexts (mihalcea, 2004).<papid> W04-2405 </papid></citsent>
<aftsection>
<nextsent>also recently, other features have been proposed: bigrams, named entities, syntactic features, and semantic relations with other words in the context.
</nextsent>
<nextsent>we transfer the candidate phrases extracted by the sliding k-window into the vector space of features.
</nextsent>
<nextsent>we consider features that include both lexical and syntactic descriptions of the paraphrases for all pairs of two candidates.
</nextsent>
<nextsent>the lexical features include the sequence of tokens for each phrase in the paraphrase pair; the syntactic feature consists of sequence of part-of-speech (pos) tags where equal words and words with the same root and pos are marked.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF499">
<title id=" W10-0211.xml">generating shifting sentiment for a conversational agent </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while our ultimate aim is generation of language that relects emotional state, in this work we investigate the automatic generation of varying sentiment?
</prevsent>
<prevsent>in output utterances; we focus on sentiment mainly due to the recent development of useful resources for this task.
</prevsent>
</prevsection>
<citsent citstr=" L08-1620 ">
(guerini et al, 2008)<papid> L08-1620 </papid>s valentino system is an approach to automatically generating candidate output utterances with different sentiment from an original; the authors suggest ecas as possible application scenario for theirtechniques.</citsent>
<aftsection>
<nextsent>we explore this suggestion, implementing lexical substitution (mccarthy and navigli,2007) <papid> W07-2009 </papid>approach to dialogue generation with sentiment, using the valentino approach and associated resources.</nextsent>
<nextsent>lexical substitution approaches raise well-known challenges, and we investigate number of techniques to address these in section 4; forex ample, using bigrams and grammatical relations to determine which substitutions are acceptable based on their context in sentence.1our techniques show improvement over naive lexical substitution; however, an evaluation with human subjects suggests that deeper problem is that even acceptable?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF501">
<title id=" W10-0211.xml">generating shifting sentiment for a conversational agent </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in output utterances; we focus on sentiment mainly due to the recent development of useful resources for this task.
</prevsent>
<prevsent>(guerini et al, 2008)<papid> L08-1620 </papid>s valentino system is an approach to automatically generating candidate output utterances with different sentiment from an original; the authors suggest ecas as possible application scenario for theirtechniques.</prevsent>
</prevsection>
<citsent citstr=" W07-2009 ">
we explore this suggestion, implementing lexical substitution (mccarthy and navigli,2007) <papid> W07-2009 </papid>approach to dialogue generation with sentiment, using the valentino approach and associated resources.</citsent>
<aftsection>
<nextsent>lexical substitution approaches raise well-known challenges, and we investigate number of techniques to address these in section 4; forex ample, using bigrams and grammatical relations to determine which substitutions are acceptable based on their context in sentence.1our techniques show improvement over naive lexical substitution; however, an evaluation with human subjects suggests that deeper problem is that even acceptable?
</nextsent>
<nextsent>candidate sentences generated by the method do not match human judgements with respect to sentiment shift: i.e., alternatives labeled as more positive (resp., negative) than the original by the system are often seen as sentiment shift in the opposite direction by human judges (section 5).
</nextsent>
<nextsent>the valentino2 system (guerini et al, 2008)<papid> L08-1620 </papid> is tool developed from wordnet and sentiwordnet 1guerini et al suggest this as an area for further work.</nextsent>
<nextsent>2valenced text inoculator 89 designed to produce more positively or negatively slanted versions of text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF520">
<title id=" W10-0211.xml">generating shifting sentiment for a conversational agent </title>
<section> evaluation: candidate filtering.  </section>
<citcontext>
<prevsection>
<prevsent>the sentence must contain at least one term.
</prevsent>
<prevsent>which is found in the ovvts (otherwise it would be pointless for evaluation purposes); the term may have any valence.12our second filtering technique requires information about the grammatical relations between terms in sentence (illustrated in figure 1).
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
for this, we used version of the bnc which was pre-processed with the rasp parser (briscoe et al, 2006).<papid> P06-4020 </papid></citsent>
<aftsection>
<nextsent>our gold standard for candidate acceptability was created using the first authors judgements.13 in or 10the size of our test dataset was capped at 25 due to the time required to create the gold standard (i.e., judging 1030 substitutions consistently).
</nextsent>
<nextsent>11these constraints reduced our sample set from the 4.6 million sentences in the bnc to approx.
</nextsent>
<nextsent>627,000 sentences.12the sentence can theoretically be valence-shifted by substituting that term, regardless of the terms valence.
</nextsent>
<nextsent>13with more time we would of course have preferred to use multiple annotators.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF524">
<title id=" W10-0211.xml">generating shifting sentiment for a conversational agent </title>
<section> evaluation: candidate filtering.  </section>
<citcontext>
<prevsection>
<prevsent>4.4.4 grammatical errors grammatical error occurs when the alternative term is acceptable semantically, yet further syntactic modification to the sentence is needed to preserve correct grammar: see table 3.
</prevsent>
<prevsent>an extension of our bigram approach could be to use larger window around replaced words to assess the suitability of substitution.
</prevsent>
</prevsection>
<citsent citstr=" W07-2100 ">
recent work has shown this technique could be used to rank potential substitutions in order of acceptability (hawker, 2007) <papid> W07-2100 </papid>and is worth considering as future work.</citsent>
<aftsection>
<nextsent>4.4.5 limitations of bigrams and corpus coverage in some cases, our bigram selection technique is ineffective when the term being changed is flanked by stop words.
</nextsent>
<nextsent>in corpus of sufficient size and coverage, the majority of terms will occur next to stop words far more often than they occur next to other, less common terms.
</nextsent>
<nextsent>hence, bigrams containing stop words were common source of false positives.
</nextsent>
<nextsent>this limitation could be addressed in future work by extending our grammatical relation technique to include ternary grs, which provide relations for noun-verb phrases such as solution to fitness?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF526">
<title id=" W10-0211.xml">generating shifting sentiment for a conversational agent </title>
<section> evaluation: sentiment shift.  </section>
<citcontext>
<prevsection>
<prevsent>as described in (hawker, 2007), <papid> W07-2100 </papid>use of an even larger window, such as 4-grams and 5-grams around replaced terms may also address this issue, however the size of the web1t corpus for larger n-grams presents serious processing challenges.22</prevsent>
<prevsent>the technqiues described above attempt to create acceptable candidates to shift sentiment.</prevsent>
</prevsection>
<citsent citstr=" W07-2091 ">
however, this22(hassan et al, 2007) <papid> W07-2091 </papid>describes successful approach to lexical substitution that combines multiple knowledge sources.</citsent>
<aftsection>
<nextsent>leaves open the question as to whether the technique has its desired effect: i.e. appropriately shifting sentiment.
</nextsent>
<nextsent>we designed an experiment which aims to measure correlation between human judgements of the sentiment shift in our generated candidates, and our systems representation of sentiment shift.
</nextsent>
<nextsent>we presented subjects with an original sentence, along with one of the generated candidates.
</nextsent>
<nextsent>our six subjects had no specialised knowledge of thetask and were all native english speakers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF529">
<title id=" W10-0214.xml">recognizing stances in ideological online debates </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>additionally, by employing both types of opinion features,we are able to perform better than unigram based system.
</prevsent>
<prevsent>in this work, we explore if and how ideological stances can be recognized using opinion analysis.
</prevsent>
</prevsection>
<citsent citstr=" P09-1026 ">
following (somasundaran and wiebe, 2009),<papid> P09-1026 </papid>stance, as used in this work, refers to an overall position held by person toward an object, idea orproposition.</citsent>
<aftsection>
<nextsent>for example, in debate do you believe in the existence of god?,?
</nextsent>
<nextsent>a person may take for-existence of god stance or an against existence of god stance.
</nextsent>
<nextsent>similarly, being pro-choice, believing in creation ism, and supporting universal healthcare are all examples of ideological stances.online web forums discussing ideological and political hot-topics are popular.1 in this work, we are 1http://www.opposingviews.com, http://wiki.idebate.org, http://www.createdebate.com and http://www.forandagainst.com are examples of such debating websites.interested in dual-sided debates (there are two possible polarizing sides that the participants can take).
</nextsent>
<nextsent>for example, in healthcare debate, participants can take for-healthcare stance or an against-healthcarestance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF530">
<title id=" W10-0214.xml">recognizing stances in ideological online debates </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this negative opinion reveals his against-healthcare stance.
</prevsent>
<prevsent>we observed that arguing, less well explored type of subjectivity, is prominently manifested in ideological debates.
</prevsent>
</prevsection>
<citsent citstr=" W05-0308 ">
as used in this work, arguing isa type of linguistic subjectivity, where person is arguing for or against something or expressing belief about what is true, should be true or should be done2as used in this work, sentiment is type of linguistic subjectivity, specifically positive and negative expressions of emotions, judgments, and evaluations (wilson and wiebe, 2005; <papid> W05-0308 </papid>wilson, 2007; somasundaran et al, 2008).<papid> C08-1101 </papid></citsent>
<aftsection>
<nextsent>116 in his or her view of the world (wilson and wiebe, 2005; <papid> W05-0308 </papid>wilson, 2007; somasundaran et al, 2008).<papid> C08-1101 </papid></nextsent>
<nextsent>for instance, let us consider the following snippet from post supporting an against-existence of god stance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF533">
<title id=" W10-0214.xml">recognizing stances in ideological online debates </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this negative opinion reveals his against-healthcare stance.
</prevsent>
<prevsent>we observed that arguing, less well explored type of subjectivity, is prominently manifested in ideological debates.
</prevsent>
</prevsection>
<citsent citstr=" C08-1101 ">
as used in this work, arguing isa type of linguistic subjectivity, where person is arguing for or against something or expressing belief about what is true, should be true or should be done2as used in this work, sentiment is type of linguistic subjectivity, specifically positive and negative expressions of emotions, judgments, and evaluations (wilson and wiebe, 2005; <papid> W05-0308 </papid>wilson, 2007; somasundaran et al, 2008).<papid> C08-1101 </papid></citsent>
<aftsection>
<nextsent>116 in his or her view of the world (wilson and wiebe, 2005; <papid> W05-0308 </papid>wilson, 2007; somasundaran et al, 2008).<papid> C08-1101 </papid></nextsent>
<nextsent>for instance, let us consider the following snippet from post supporting an against-existence of god stance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF542">
<title id=" W10-0214.xml">recognizing stances in ideological online debates </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this work, we investigate whether sentiment and arguing expressions of opinion are useful for ideological stance classification.
</prevsent>
<prevsent>for this, we explore ways to capture relevant opinion information as machine learning features into supervised stance classifier.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
while there is large body of resources for sentiment analysis (e.g., the sentiment lexicon from (wilson et al, 2005)), <papid> H05-1044 </papid>arguing analysis does not seem to have well established lexical resource.in order to remedy this, using simple automatic approach and manually annotated corpus,3 we construct an arguing lexicon.</citsent>
<aftsection>
<nextsent>we create features calledopinion-target pairs, which encode not just the opinion information, but also what the opinion is about, its target.
</nextsent>
<nextsent>systems employing sentiment-based andarguing-based features alone, or both in combination, are analyzed.
</nextsent>
<nextsent>we also take qualitative look at features used by the learners to get insights about the information captured by them.we perform experiments on four different ideological domains.
</nextsent>
<nextsent>our results show that systems using both sentiment and arguing features can perform substantially better than distribution-based baseline and marginally better than unigram-based system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF552">
<title id=" W10-0214.xml">recognizing stances in ideological online debates </title>
<section> features for stance classification.  </section>
<citcontext>
<prevsection>
<prevsent>words, this lexicon also contains subjective words that are themselves neutral (=) with respect to polarity.
</prevsent>
<prevsent>examples of neutral entries are absolutely?, amplify?, believe?, and think?.we find the sentiment polarity of the entire sentence and assign this polarity to each content word in the sentence (denoted, for example, as target+).
</prevsent>
</prevsection>
<citsent citstr=" D09-1062 ">
in order to detect the sentence polarity, we use the vote 120 and flip algorithm from choi and cardie (2009).<papid> D09-1062 </papid>this algorithm essentially counts the number of positive, negative and neutral lexicon hits in given expression and accounts for negator words.</citsent>
<aftsection>
<nextsent>the algorithm is used as is, except for the default polarity assignment (as we do not know the most prominent polarity in the corpus).
</nextsent>
<nextsent>note that the vote and flip algorithm has been developed for expressions but we employ it on sentences.
</nextsent>
<nextsent>once the polarity of sentence is determined, we create sentiment features for the sentence.
</nextsent>
<nextsent>this is done for all sentences in the post.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF553">
<title id=" W10-0214.xml">recognizing stances in ideological online debates </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our first baseline is distribution-based baseline, which has an accuracy of 50%.
</prevsent>
<prevsent>we also constructunigram, system based on unigram content information, but no explicit opinion information.
</prevsent>
</prevsection>
<citsent citstr=" W06-2915 ">
unigrams are reliable for stance classification in political domains (as seen in (lin et al, 2006;<papid> W06-2915 </papid>kim and hovy, 2007)).<papid> D07-1113 </papid></citsent>
<aftsection>
<nextsent>intuitively, evoking particular topic can be indicative of stance.
</nextsent>
<nextsent>for example, participant who chooses to speak about child?
</nextsent>
<nextsent>and life?
</nextsent>
<nextsent>in an abortion debate is more likely from an against-abortion side, while someone speaking about woman?, rape?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF555">
<title id=" W10-0214.xml">recognizing stances in ideological online debates </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our first baseline is distribution-based baseline, which has an accuracy of 50%.
</prevsent>
<prevsent>we also constructunigram, system based on unigram content information, but no explicit opinion information.
</prevsent>
</prevsection>
<citsent citstr=" D07-1113 ">
unigrams are reliable for stance classification in political domains (as seen in (lin et al, 2006;<papid> W06-2915 </papid>kim and hovy, 2007)).<papid> D07-1113 </papid></citsent>
<aftsection>
<nextsent>intuitively, evoking particular topic can be indicative of stance.
</nextsent>
<nextsent>for example, participant who chooses to speak about child?
</nextsent>
<nextsent>and life?
</nextsent>
<nextsent>in an abortion debate is more likely from an against-abortion side, while someone speaking about woman?, rape?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF558">
<title id=" W10-0214.xml">recognizing stances in ideological online debates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in general, unigram features associate the choice of topics with the stances, while the arguing feature scan capture the concerns, defenses, appeals or denials that signify each side (though we do not explicitly encode these fine-grained distinctions in thiswork).
</prevsent>
<prevsent>interestingly, we found that sentiment features in arg+sent are not as informative as the arguing features discussed above.
</prevsent>
</prevsection>
<citsent citstr=" N06-3005 ">
generally, research in identifying political viewpoints has employed information from words in the document (malouf and mullen, 2008; mullen and malouf, 2006; grefenstette et al, 2004; laver et al, 2003; martin and vanberg, 2008; lin et al, 2006;<papid> W06-2915 </papid>lin, 2006).<papid> N06-3005 </papid></citsent>
<aftsection>
<nextsent>specifically, lin et al observe that people from opposing perspectives seem to use words in differing frequencies.
</nextsent>
<nextsent>on similar lines, kim and hovy (2007) <papid> D07-1113 </papid>use unigrams, bigrams and trigrams for election prediction from forum posts.</nextsent>
<nextsent>in contrast, our work specifically employs sentiment-based andarguing-based features to perform stance classification in political debates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF560">
<title id=" W10-0214.xml">recognizing stances in ideological online debates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our experiments are focused on determining how different opinion expressions reinforce an overall political stance.
</prevsent>
<prevsent>our results indicate that while unigram information is reliable, further improvements can be achieved in certain domains using our opinion-based approach.
</prevsent>
</prevsection>
<citsent citstr=" N09-1057 ">
our work is also complementary to that by greene andres nik (2009), <papid> N09-1057 </papid>which focuses on syntactic packaging for recognizing perspectives.</citsent>
<aftsection>
<nextsent>122 for gay rights against gay rights unigram features constitution, fundamental, rights, suffrage, pursuit, discrimination, government, happiness, shame, wed, gay, heterosexual ity, chromosome, evolution, genetic, christianity, mormon ism, corinthians, procreate, adopt pervert, hormone, liberty, fidelity, naval, retarded, orientation, private, partner, kingdom, bible, sin, bigot arguing features from arg+sent ap-constitution, ap-fundamental, ap-rights, ap-hormone,ap-liberty, ap-independence, ap-suffrage, ap-pursuit, ap discrimination, an-government, ap-fidelity, ap-happiness, an-pervert, an-naval, an-retarded, an-orientation, an-shame, ap-private, ap-wed, ap-gay, an-heterosexuality, ap-partner,ap-chromosome, ap-evolution, ap-genetic, an-kingdom, an christianity, an-mormonism, an-corinthians, an-bible, an-sin, an-bigot, an-procreate, ap-adopt, an-constitution, an-fundamental, an-rights, an-hormone,an-liberty, an-independence, an-suffrage, an-pursuit, an discrimination, ap-government, an-fidelity, an-happiness, ap-pervert, ap-naval, ap-retarded, ap-orientation, ap-shame, an-private, an-wed, an-gay, ap-heterosexuality, an-partner,an-chromosome, an-evolution, an-genetic, ap-kingdom, ap christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin, ap-bigot, ap-procreate, an-adopt table 5: examples of features associated with the stances in gay rights domain discourse-level participant relation, that is, whether participants agree/disagree has been found useful for determining political side-taking (thomas et al, 2006; <papid> W06-1639 </papid>bansal et al, 2008; <papid> C08-2004 </papid>agrawal etal., 2003; malouf and mullen, 2008).</nextsent>
<nextsent>agree ment/disagreement relations are not the main focusof our work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF561">
<title id=" W10-0214.xml">recognizing stances in ideological online debates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our results indicate that while unigram information is reliable, further improvements can be achieved in certain domains using our opinion-based approach.
</prevsent>
<prevsent>our work is also complementary to that by greene andres nik (2009), <papid> N09-1057 </papid>which focuses on syntactic packaging for recognizing perspectives.</prevsent>
</prevsection>
<citsent citstr=" W06-1639 ">
122 for gay rights against gay rights unigram features constitution, fundamental, rights, suffrage, pursuit, discrimination, government, happiness, shame, wed, gay, heterosexual ity, chromosome, evolution, genetic, christianity, mormon ism, corinthians, procreate, adopt pervert, hormone, liberty, fidelity, naval, retarded, orientation, private, partner, kingdom, bible, sin, bigot arguing features from arg+sent ap-constitution, ap-fundamental, ap-rights, ap-hormone,ap-liberty, ap-independence, ap-suffrage, ap-pursuit, ap discrimination, an-government, ap-fidelity, ap-happiness, an-pervert, an-naval, an-retarded, an-orientation, an-shame, ap-private, ap-wed, ap-gay, an-heterosexuality, ap-partner,ap-chromosome, ap-evolution, ap-genetic, an-kingdom, an christianity, an-mormonism, an-corinthians, an-bible, an-sin, an-bigot, an-procreate, ap-adopt, an-constitution, an-fundamental, an-rights, an-hormone,an-liberty, an-independence, an-suffrage, an-pursuit, an discrimination, ap-government, an-fidelity, an-happiness, ap-pervert, ap-naval, ap-retarded, ap-orientation, ap-shame, an-private, an-wed, an-gay, ap-heterosexuality, an-partner,an-chromosome, an-evolution, an-genetic, ap-kingdom, ap christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin, ap-bigot, ap-procreate, an-adopt table 5: examples of features associated with the stances in gay rights domain discourse-level participant relation, that is, whether participants agree/disagree has been found useful for determining political side-taking (thomas et al, 2006; <papid> W06-1639 </papid>bansal et al, 2008; <papid> C08-2004 </papid>agrawal etal., 2003; malouf and mullen, 2008).</citsent>
<aftsection>
<nextsent>agree ment/disagreement relations are not the main focusof our work.
</nextsent>
<nextsent>other work in the area of polarizing political discourse analyze co-citations (efron, 2004) and linking patterns (adamic and glance, 2005).
</nextsent>
<nextsent>in contrast, our focus is on document content and opinion expressions.somasundaran et al (2007b) have noted the usefulness of the arguing category for opinion qa.
</nextsent>
<nextsent>our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF562">
<title id=" W10-0214.xml">recognizing stances in ideological online debates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our results indicate that while unigram information is reliable, further improvements can be achieved in certain domains using our opinion-based approach.
</prevsent>
<prevsent>our work is also complementary to that by greene andres nik (2009), <papid> N09-1057 </papid>which focuses on syntactic packaging for recognizing perspectives.</prevsent>
</prevsection>
<citsent citstr=" C08-2004 ">
122 for gay rights against gay rights unigram features constitution, fundamental, rights, suffrage, pursuit, discrimination, government, happiness, shame, wed, gay, heterosexual ity, chromosome, evolution, genetic, christianity, mormon ism, corinthians, procreate, adopt pervert, hormone, liberty, fidelity, naval, retarded, orientation, private, partner, kingdom, bible, sin, bigot arguing features from arg+sent ap-constitution, ap-fundamental, ap-rights, ap-hormone,ap-liberty, ap-independence, ap-suffrage, ap-pursuit, ap discrimination, an-government, ap-fidelity, ap-happiness, an-pervert, an-naval, an-retarded, an-orientation, an-shame, ap-private, ap-wed, ap-gay, an-heterosexuality, ap-partner,ap-chromosome, ap-evolution, ap-genetic, an-kingdom, an christianity, an-mormonism, an-corinthians, an-bible, an-sin, an-bigot, an-procreate, ap-adopt, an-constitution, an-fundamental, an-rights, an-hormone,an-liberty, an-independence, an-suffrage, an-pursuit, an discrimination, ap-government, an-fidelity, an-happiness, ap-pervert, ap-naval, ap-retarded, ap-orientation, ap-shame, an-private, an-wed, an-gay, ap-heterosexuality, an-partner,an-chromosome, an-evolution, an-genetic, ap-kingdom, ap christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin, ap-bigot, ap-procreate, an-adopt table 5: examples of features associated with the stances in gay rights domain discourse-level participant relation, that is, whether participants agree/disagree has been found useful for determining political side-taking (thomas et al, 2006; <papid> W06-1639 </papid>bansal et al, 2008; <papid> C08-2004 </papid>agrawal etal., 2003; malouf and mullen, 2008).</citsent>
<aftsection>
<nextsent>agree ment/disagreement relations are not the main focusof our work.
</nextsent>
<nextsent>other work in the area of polarizing political discourse analyze co-citations (efron, 2004) and linking patterns (adamic and glance, 2005).
</nextsent>
<nextsent>in contrast, our focus is on document content and opinion expressions.somasundaran et al (2007b) have noted the usefulness of the arguing category for opinion qa.
</nextsent>
<nextsent>our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF565">
<title id=" W09-2002.xml">topic model analysis of metaphor frequency for psycho linguistic stimuli </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 metaphor classification.
</prevsent>
<prevsent>recent years have seen rising interest in metaphor classification systems.
</prevsent>
</prevsection>
<citsent citstr=" E06-1042 ">
birke and sarkar (2006) <papid> E06-1042 </papid>tooka semi-supervised approach, collecting noisy examples of literal and non-literal sentences from both wordnet and metaphor dictionaries, and using word-based measure of sentence similarity to group sentences into literal and non-literal clusters.</citsent>
<aftsection>
<nextsent>they evaluated on hand-annotated sentences for 25 target words and reported an f-score of 0.538, substantial improvement over the 0.294 majority class baseline.
</nextsent>
<nextsent>gedigian et al  (2006) <papid> W06-3506 </papid>approached metaphor identification as supervised classification, annotating around 4000 wsj motion words as literal or metaphorical, and training maximum entropy classifier using as features based on named entities,wordnet and semantic roles.</nextsent>
<nextsent>they achieved an accuracy of 95.1%, decent improvement over the very high majority class baseline of 93.8%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF567">
<title id=" W09-2002.xml">topic model analysis of metaphor frequency for psycho linguistic stimuli </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>birke and sarkar (2006) <papid> E06-1042 </papid>tooka semi-supervised approach, collecting noisy examples of literal and non-literal sentences from both wordnet and metaphor dictionaries, and using word-based measure of sentence similarity to group sentences into literal and non-literal clusters.</prevsent>
<prevsent>they evaluated on hand-annotated sentences for 25 target words and reported an f-score of 0.538, substantial improvement over the 0.294 majority class baseline.</prevsent>
</prevsection>
<citsent citstr=" W06-3506 ">
gedigian et al  (2006) <papid> W06-3506 </papid>approached metaphor identification as supervised classification, annotating around 4000 wsj motion words as literal or metaphorical, and training maximum entropy classifier using as features based on named entities,wordnet and semantic roles.</citsent>
<aftsection>
<nextsent>they achieved an accuracy of 95.1%, decent improvement over the very high majority class baseline of 93.8%.
</nextsent>
<nextsent>krishnakumaran and zhu (2007) <papid> W07-0103 </papid>focused on three syntactically constrained sub-types of metaphors: nouns joined by be, nouns following verbs, and nouns following adjectives.</nextsent>
<nextsent>they combined wordnet hypernym information with bigram statistics and threshold, and evaluated their algorithm on the berkeley master metaphor list (lakoff, 1994), achieving an accuracy of around 46%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF569">
<title id=" W09-2002.xml">topic model analysis of metaphor frequency for psycho linguistic stimuli </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>gedigian et al  (2006) <papid> W06-3506 </papid>approached metaphor identification as supervised classification, annotating around 4000 wsj motion words as literal or metaphorical, and training maximum entropy classifier using as features based on named entities,wordnet and semantic roles.</prevsent>
<prevsent>they achieved an accuracy of 95.1%, decent improvement over the very high majority class baseline of 93.8%.</prevsent>
</prevsection>
<citsent citstr=" W07-0103 ">
krishnakumaran and zhu (2007) <papid> W07-0103 </papid>focused on three syntactically constrained sub-types of metaphors: nouns joined by be, nouns following verbs, and nouns following adjectives.</citsent>
<aftsection>
<nextsent>they combined wordnet hypernym information with bigram statistics and threshold, and evaluated their algorithm on the berkeley master metaphor list (lakoff, 1994), achieving an accuracy of around 46%.
</nextsent>
<nextsent>all of these approaches produced models which could be applied to new text to identify metaphors, but each has some drawbacks for our task.
</nextsent>
<nextsent>the wsj study of gedigian et al  (2006) <papid> W06-3506 </papid>found 94% oftheir target words to be metaphorical, vastly differ 10 target m m% attacked 32 18 36% born 45 5 10% budding 16 34 68% collapsed 10 40 80% digest 7 43 86% drifted 16 34 68% floating 25 25 50% sank 31 19 38% spoke 47 3 6% total 229 221 49% table 1: metaphorical (m) and literal (l) counts, and metaphorical percentage (m%), for the annotated verbs.</nextsent>
<nextsent>ent number from the 49% for our target words (seesection 3).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF578">
<title id=" W09-2205.xml">a comparison of structural correspondence learning and self training for discriminative parse selection </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>this paper evaluates two semi-supervisedtechniques for the adaptation of parse selection model to wikipedia domains.
</prevsent>
</prevsection>
<citsent citstr=" W06-1615 ">
the techniques examined are structural correspondence learning (scl) (blitzer et al, 2006) <papid> W06-1615 </papid>and self-training (abney, 2007; mcclosky et al., 2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>a preliminary evaluation favors theuse of scl over the simpler self-training techniques.
</nextsent>
<nextsent>parse selection constitutes an important part of many parsing systems (hara et al, 2005; <papid> I05-1018 </papid>van noord and malouf, 2005; mcclosky et al, 2006).<papid> N06-1020 </papid></nextsent>
<nextsent>yet, there is little to no work focusing on the adaptation of parse selection models to novel domains.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF582">
<title id=" W09-2205.xml">a comparison of structural correspondence learning and self training for discriminative parse selection </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>this paper evaluates two semi-supervisedtechniques for the adaptation of parse selection model to wikipedia domains.
</prevsent>
</prevsection>
<citsent citstr=" N06-1020 ">
the techniques examined are structural correspondence learning (scl) (blitzer et al, 2006) <papid> W06-1615 </papid>and self-training (abney, 2007; mcclosky et al., 2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>a preliminary evaluation favors theuse of scl over the simpler self-training techniques.
</nextsent>
<nextsent>parse selection constitutes an important part of many parsing systems (hara et al, 2005; <papid> I05-1018 </papid>van noord and malouf, 2005; mcclosky et al, 2006).<papid> N06-1020 </papid></nextsent>
<nextsent>yet, there is little to no work focusing on the adaptation of parse selection models to novel domains.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF584">
<title id=" W09-2205.xml">a comparison of structural correspondence learning and self training for discriminative parse selection </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>the techniques examined are structural correspondence learning (scl) (blitzer et al, 2006) <papid> W06-1615 </papid>and self-training (abney, 2007; mcclosky et al., 2006).<papid> N06-1020 </papid></prevsent>
<prevsent>a preliminary evaluation favors theuse of scl over the simpler self-training tech niques.</prevsent>
</prevsection>
<citsent citstr=" I05-1018 ">
parse selection constitutes an important part of many parsing systems (hara et al, 2005; <papid> I05-1018 </papid>van noord and malouf, 2005; mcclosky et al, 2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>yet, there is little to no work focusing on the adaptation of parse selection models to novel domains.
</nextsent>
<nextsent>this is most probably due to the fact that potential gains for this task are inherently bounded by the under lying grammar.
</nextsent>
<nextsent>the few studies on adapting parse disambiguation models, like hara et al (2005), <papid> I05-1018 </papid>have focused exclusively on supervised domain adaptation, i.e. one has access to comparably small, but labeled amount of target data.</nextsent>
<nextsent>in contrast, in semi supervised domain adaptation one has only unlabeled target data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF592">
<title id=" W09-2205.xml">a comparison of structural correspondence learning and self training for discriminative parse selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>for empirical evaluation (section 4) we use the alpino parsing system for dutch (van noord and malouf, 2005).
</prevsent>
<prevsent>as target domain, we exploit wikipedia as primary test and training collection.
</prevsent>
</prevsection>
<citsent citstr=" P07-1056 ">
so far, structural correspondence learning hasbeen applied successfully to pos tagging and sentiment analysis (blitzer et al, 2006; <papid> W06-1615 </papid>blitzer et al., 2007).<papid> P07-1056 </papid></citsent>
<aftsection>
<nextsent>an attempt was made in the conll2007 shared task to apply scl to non-projective dependency parsing (shimizu and nakagawa, 2007).<papid> D07-1129 </papid></nextsent>
<nextsent>however, the system just ended up at rank 7 out of 8 teams.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF593">
<title id=" W09-2205.xml">a comparison of structural correspondence learning and self training for discriminative parse selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>as target domain, we exploit wikipedia as primary test and training collection.
</prevsent>
<prevsent>so far, structural correspondence learning hasbeen applied successfully to pos tagging and sentiment analysis (blitzer et al, 2006; <papid> W06-1615 </papid>blitzer et al., 2007).<papid> P07-1056 </papid></prevsent>
</prevsection>
<citsent citstr=" D07-1129 ">
an attempt was made in the conll2007 shared task to apply scl to non-projective dependency parsing (shimizu and nakagawa, 2007).<papid> D07-1129 </papid></citsent>
<aftsection>
<nextsent>however, the system just ended up at rank 7 out of 8 teams.
</nextsent>
<nextsent>based on annotation differences in the datasets (dredze et al, 2007) <papid> D07-1112 </papid>and bug in their system (shimizu and nakagawa, 2007), <papid> D07-1129 </papid>their results are inconclusive.</nextsent>
<nextsent>a recent attempt (plank, 2009) shows promising results on applying scl to parse disam biguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF594">
<title id=" W09-2205.xml">a comparison of structural correspondence learning and self training for discriminative parse selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>an attempt was made in the conll2007 shared task to apply scl to non-projective dependency parsing (shimizu and nakagawa, 2007).<papid> D07-1129 </papid></prevsent>
<prevsent>however, the system just ended up at rank 7 out of 8 teams.</prevsent>
</prevsection>
<citsent citstr=" D07-1112 ">
based on annotation differences in the datasets (dredze et al, 2007) <papid> D07-1112 </papid>and bug in their system (shimizu and nakagawa, 2007), <papid> D07-1129 </papid>their results are inconclusive.</citsent>
<aftsection>
<nextsent>a recent attempt (plank, 2009) shows promising results on applying scl to parse disambiguation.
</nextsent>
<nextsent>in this paper, we extend that line of work and compare scl to bootstrapping approaches such as self-training.
</nextsent>
<nextsent>studies on self-training have focused mainly on generative, constituent based parsing (steedman etal., 2003; <papid> E03-1008 </papid>mcclosky et al, 2006; <papid> N06-1020 </papid>reichart and rappoport, 2007).<papid> P07-1078 </papid></nextsent>
<nextsent>steedman et al (2003) <papid> E03-1008 </papid>as well as reichart and rappoport (2007) <papid> P07-1078 </papid>examine self-trainingfor pcfg parsing in the small seed case (  1k labeled data), with different results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF596">
<title id=" W09-2205.xml">a comparison of structural correspondence learning and self training for discriminative parse selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>a recent attempt (plank, 2009) shows promising results on applying scl to parse disambiguation.
</prevsent>
<prevsent>in this paper, we extend that line of work and compare scl to bootstrapping approaches such as self-training.
</prevsent>
</prevsection>
<citsent citstr=" E03-1008 ">
studies on self-training have focused mainly on generative, constituent based parsing (steedman etal., 2003; <papid> E03-1008 </papid>mcclosky et al, 2006; <papid> N06-1020 </papid>reichart and rappoport, 2007).<papid> P07-1078 </papid></citsent>
<aftsection>
<nextsent>steedman et al (2003) <papid> E03-1008 </papid>as well as reichart and rappoport (2007) <papid> P07-1078 </papid>examine self-trainingfor pcfg parsing in the small seed case (  1k labeled data), with different results.</nextsent>
<nextsent>in contrast, mcclosky et al (2006) <papid> N06-1020 </papid>focus on large seeds and expl oita reranking-parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF598">
<title id=" W09-2205.xml">a comparison of structural correspondence learning and self training for discriminative parse selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>a recent attempt (plank, 2009) shows promising results on applying scl to parse disambiguation.
</prevsent>
<prevsent>in this paper, we extend that line of work and compare scl to bootstrapping approaches such as self-training.
</prevsent>
</prevsection>
<citsent citstr=" P07-1078 ">
studies on self-training have focused mainly on generative, constituent based parsing (steedman etal., 2003; <papid> E03-1008 </papid>mcclosky et al, 2006; <papid> N06-1020 </papid>reichart and rappoport, 2007).<papid> P07-1078 </papid></citsent>
<aftsection>
<nextsent>steedman et al (2003) <papid> E03-1008 </papid>as well as reichart and rappoport (2007) <papid> P07-1078 </papid>examine self-trainingfor pcfg parsing in the small seed case (  1k labeled data), with different results.</nextsent>
<nextsent>in contrast, mcclosky et al (2006) <papid> N06-1020 </papid>focus on large seeds and expl oita reranking-parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF604">
<title id=" W09-2205.xml">a comparison of structural correspondence learning and self training for discriminative parse selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>steedman et al (2003) <papid> E03-1008 </papid>as well as reichart and rappoport (2007) <papid> P07-1078 </papid>examine self-trainingfor pcfg parsing in the small seed case (  1k labeled data), with different results.</prevsent>
<prevsent>in contrast, mcclosky et al (2006) <papid> N06-1020 </papid>focus on large seeds and expl oita reranking-parser.</prevsent>
</prevsection>
<citsent citstr=" P08-2026 ">
improvements are obtained (mc closky et al, 2006; <papid> N06-1020 </papid>mcclosky and charniak, 2008), <papid> P08-2026 </papid>showing that reranker is necessary for successful self-training in such high-resource scenario.</citsent>
<aftsection>
<nextsent>while they self-trained generative model, we examineself-training and scl for semi-supervised adaptation of discriminative parse selection system.
</nextsent>
<nextsent>37
</nextsent>
<nextsent>3.1 structural correspondence learning.
</nextsent>
<nextsent>structural correspondence learning (blitzer et al, 2006) <papid> W06-1615 </papid>exploits unlabeled data from both source and target domain to find correspondences among features from different domains.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF620">
<title id=" W09-2205.xml">a comparison of structural correspondence learning and self training for discriminative parse selection </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>the first stage consists of hpsg-like grammar that constitutes the parse generation component.
</prevsent>
<prevsent>the second stage is maximum entropy (maxent) parse selection model.
</prevsent>
</prevsection>
<citsent citstr=" C00-1085 ">
to train the maxent model, parameters are estimated based on informative samples (os borne, 2000).<papid> C00-1085 </papid></citsent>
<aftsection>
<nextsent>a parse is added to the training data with score indicating its goodness?
</nextsent>
<nextsent>(van noorda nd malouf, 2005).
</nextsent>
<nextsent>the score is obtained by comparing it with the gold standard (if available; otherwise the score is approximated through parse proba bility).
</nextsent>
<nextsent>the source domain is the alpino treebank (van noord and malouf, 2005) (newspaper text; approx.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF629">
<title id=" W10-0723.xml">shedding a thousand points of light on biased language </title>
<section> annotation task.  </section>
<citcontext>
<prevsection>
<prevsent>one likely indicator of bias is the use of terms that are particular to one side or the other in debate (monroe et al, 2008).
</prevsent>
<prevsent>in order to identify such terms, we independently created two lists of sticky?
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
(i.e., strongly associated) bigrams in liber aland conservative sub corpora, measuring association using the log-likelihood ratio (dunning, 1993) <papid> J93-1003 </papid>and omitting bigrams containing stopwords.11 we identified bigram as liberal?</citsent>
<aftsection>
<nextsent>if it was among the top 1,000 bigrams from the liberal blogs, as measured by strength of association,and was also not among the top 1,000 bigrams on the conservative side.
</nextsent>
<nextsent>the reverse definition yielded the conser vative?
</nextsent>
<nextsent>bigrams.
</nextsent>
<nextsent>the resulting liberal list contained 495 bigrams, and the conservative list contained 539.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF630">
<title id=" W10-0723.xml">shedding a thousand points of light on biased language </title>
<section> annotation task.  </section>
<citcontext>
<prevsection>
<prevsent>a bunch of ugly facts are nailing the biggest scare story in history.
</prevsent>
<prevsent>the five most frequent matches in the corpus for each category are as follows.13 negative emotion: war attack* problem* numb* argu* positive emotion: like well good party* secur* causation: how because lead* make why anger: war attack* argu* fight* threat*kill verbs.
</prevsent>
</prevsection>
<citsent citstr=" N09-1057 ">
greene and resnik (2009) <papid> N09-1057 </papid>discuss the relevance of syntactic structure to the perception of senti ment.</citsent>
<aftsection>
<nextsent>for example, their psycho linguistic experiments would predict that when comparing millions of people starved under stalin (inchoative) with stalin starved millions of people (transitive), the latter will be perceived asmore negative toward stalin, because the transitive syntactic frame tends to be connected with semantic properties such as intended action by the subject and change of state in the object.
</nextsent>
<nextsent>kill verbs?
</nextsent>
<nextsent>provide particularly strong examples of such phenomena, because they exhibit large set of semantic properties canonically associated with the transitive frame (dowty, 1991).
</nextsent>
<nextsent>the study by greene and resnik used 11 verbs of killing and similar action to study the effect of syntactic packaging?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF632">
<title id=" W09-2005.xml">gaiku  generating haiku with word associations norms </title>
<section> wordnet vs. associations.  </section>
<citcontext>
<prevsection>
<prevsent>thus, we take two words to be associatively related if their associative distance is 1 or 2.
</prevsent>
<prevsent>similarly, we define the wordnet distance between two stemmed wordsto be the number of edges in the shortest path between any synset of one word to any synset of the other word2.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
two words are wordnet-related if their wordnet distance is less than 4 (this is consistent with works on lexical-cohesion, (morris and hirst, 1991)).<papid> J91-1002 </papid></citsent>
<aftsection>
<nextsent>we take the associativity of piece of text to bethe number of associated word pairs in the text, normalized by the number of word pairs in the text of which both words are in the wan.3 we take the wordnet-relations level of piece of text to be the number of wordnet-related word pairs in the text.2this is the inverse of the path-similarity measure of (ped ersen et al, 2004).
</nextsent>
<nextsent>3this normalization is performed to account for the limited lexical coverage of the wan.
</nextsent>
<nextsent>we dont want words that appear in text, but are not covered by the wan, to affect the associa tivity level of the text.
</nextsent>
<nextsent>source avg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF634">
<title id=" W09-2111.xml">user input and interactions on microsoft research esl assistant </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>language learner error correction techniques typically fall into either of two categories: rule-based or data-driven.
</prevsent>
<prevsent>eeg-olofsson and knutsson (2003) report on rule-based system that detects and corrects preposition errors in non-native swedish text.
</prevsent>
</prevsection>
<citsent citstr=" C94-1002 ">
rule-based approaches have also been used to predict definite ness and indefinite ness of japanese noun phrases as preprocessing step for japanese to english machine translation (murata and nagao 1993; bond et al 1994; <papid> C94-1002 </papid>heine, 1998), task that is similar to the prediction of english articles.</citsent>
<aftsection>
<nextsent>more recently, data-driven approaches have gained popularity and been applied to article prediction in english (knight and chander 1994; minnen et al 2000; <papid> W00-0708 </papid>turner and charniak 2007), <papid> N07-2045 </papid>to an array of japanese learners?</nextsent>
<nextsent>errors in english (izumi et al 2003), <papid> P03-2026 </papid>to verb errors (lee and seneff, 2008), <papid> P08-1021 </papid>and to article and preposition correction in texts written by non-native ells (han et al 2004, 2006; nagata et al 2005; nagata et al 2006; <papid> P06-1031 </papid>de felice and pul man, 2007; chodorow et al 2007; <papid> W07-1604 </papid>gamon et al 2008, 2009; tetreault and chodorow, 2008<papid> C08-1109 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF635">
<title id=" W09-2111.xml">user input and interactions on microsoft research esl assistant </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>eeg-olofsson and knutsson (2003) report on rule-based system that detects and corrects preposition errors in non-native swedish text.
</prevsent>
<prevsent>rule-based approaches have also been used to predict definite ness and indefinite ness of japanese noun phrases as preprocessing step for japanese to english machine translation (murata and nagao 1993; bond et al 1994; <papid> C94-1002 </papid>heine, 1998), task that is similar to the prediction of english articles.</prevsent>
</prevsection>
<citsent citstr=" W00-0708 ">
more recently, data-driven approaches have gained popularity and been applied to article prediction in english (knight and chander 1994; minnen et al 2000; <papid> W00-0708 </papid>turner and charniak 2007), <papid> N07-2045 </papid>to an array of japanese learners?</citsent>
<aftsection>
<nextsent>errors in english (izumi et al 2003), <papid> P03-2026 </papid>to verb errors (lee and seneff, 2008), <papid> P08-1021 </papid>and to article and preposition correction in texts written by non-native ells (han et al 2004, 2006; nagata et al 2005; nagata et al 2006; <papid> P06-1031 </papid>de felice and pul man, 2007; chodorow et al 2007; <papid> W07-1604 </papid>gamon et al 2008, 2009; tetreault and chodorow, 2008<papid> C08-1109 </papid>a).</nextsent>
<nextsent>1 http://www.eslassistant.com 73</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF636">
<title id=" W09-2111.xml">user input and interactions on microsoft research esl assistant </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>eeg-olofsson and knutsson (2003) report on rule-based system that detects and corrects preposition errors in non-native swedish text.
</prevsent>
<prevsent>rule-based approaches have also been used to predict definite ness and indefinite ness of japanese noun phrases as preprocessing step for japanese to english machine translation (murata and nagao 1993; bond et al 1994; <papid> C94-1002 </papid>heine, 1998), task that is similar to the prediction of english articles.</prevsent>
</prevsection>
<citsent citstr=" N07-2045 ">
more recently, data-driven approaches have gained popularity and been applied to article prediction in english (knight and chander 1994; minnen et al 2000; <papid> W00-0708 </papid>turner and charniak 2007), <papid> N07-2045 </papid>to an array of japanese learners?</citsent>
<aftsection>
<nextsent>errors in english (izumi et al 2003), <papid> P03-2026 </papid>to verb errors (lee and seneff, 2008), <papid> P08-1021 </papid>and to article and preposition correction in texts written by non-native ells (han et al 2004, 2006; nagata et al 2005; nagata et al 2006; <papid> P06-1031 </papid>de felice and pul man, 2007; chodorow et al 2007; <papid> W07-1604 </papid>gamon et al 2008, 2009; tetreault and chodorow, 2008<papid> C08-1109 </papid>a).</nextsent>
<nextsent>1 http://www.eslassistant.com 73</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF637">
<title id=" W09-2111.xml">user input and interactions on microsoft research esl assistant </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>rule-based approaches have also been used to predict definite ness and indefinite ness of japanese noun phrases as preprocessing step for japanese to english machine translation (murata and nagao 1993; bond et al 1994; <papid> C94-1002 </papid>heine, 1998), task that is similar to the prediction of english articles.</prevsent>
<prevsent>more recently, data-driven approaches have gained popularity and been applied to article prediction in english (knight and chander 1994; minnen et al 2000; <papid> W00-0708 </papid>turner and charniak 2007), <papid> N07-2045 </papid>to an array of japanese learners?</prevsent>
</prevsection>
<citsent citstr=" P03-2026 ">
errors in english (izumi et al 2003), <papid> P03-2026 </papid>to verb errors (lee and seneff, 2008), <papid> P08-1021 </papid>and to article and preposition correction in texts written by non-native ells (han et al 2004, 2006; nagata et al 2005; nagata et al 2006; <papid> P06-1031 </papid>de felice and pul man, 2007; chodorow et al 2007; <papid> W07-1604 </papid>gamon et al 2008, 2009; tetreault and chodorow, 2008<papid> C08-1109 </papid>a).</citsent>
<aftsection>
<nextsent>1 http://www.eslassistant.com 73
</nextsent>
<nextsent>esl assistant takes hybrid approach that combines statistical and rule-based techniques.
</nextsent>
<nextsent>machine learning is used for those error types that are difficult to identify and resolve without taking into account complex contextual interactions, like article and preposition errors.
</nextsent>
<nextsent>rule-based approaches handle those error types that are amenable to simpler solutions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF638">
<title id=" W09-2111.xml">user input and interactions on microsoft research esl assistant </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>rule-based approaches have also been used to predict definite ness and indefinite ness of japanese noun phrases as preprocessing step for japanese to english machine translation (murata and nagao 1993; bond et al 1994; <papid> C94-1002 </papid>heine, 1998), task that is similar to the prediction of english articles.</prevsent>
<prevsent>more recently, data-driven approaches have gained popularity and been applied to article prediction in english (knight and chander 1994; minnen et al 2000; <papid> W00-0708 </papid>turner and charniak 2007), <papid> N07-2045 </papid>to an array of japanese learners?</prevsent>
</prevsection>
<citsent citstr=" P08-1021 ">
errors in english (izumi et al 2003), <papid> P03-2026 </papid>to verb errors (lee and seneff, 2008), <papid> P08-1021 </papid>and to article and preposition correction in texts written by non-native ells (han et al 2004, 2006; nagata et al 2005; nagata et al 2006; <papid> P06-1031 </papid>de felice and pul man, 2007; chodorow et al 2007; <papid> W07-1604 </papid>gamon et al 2008, 2009; tetreault and chodorow, 2008<papid> C08-1109 </papid>a).</citsent>
<aftsection>
<nextsent>1 http://www.eslassistant.com 73
</nextsent>
<nextsent>esl assistant takes hybrid approach that combines statistical and rule-based techniques.
</nextsent>
<nextsent>machine learning is used for those error types that are difficult to identify and resolve without taking into account complex contextual interactions, like article and preposition errors.
</nextsent>
<nextsent>rule-based approaches handle those error types that are amenable to simpler solutions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF639">
<title id=" W09-2111.xml">user input and interactions on microsoft research esl assistant </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>rule-based approaches have also been used to predict definite ness and indefinite ness of japanese noun phrases as preprocessing step for japanese to english machine translation (murata and nagao 1993; bond et al 1994; <papid> C94-1002 </papid>heine, 1998), task that is similar to the prediction of english articles.</prevsent>
<prevsent>more recently, data-driven approaches have gained popularity and been applied to article prediction in english (knight and chander 1994; minnen et al 2000; <papid> W00-0708 </papid>turner and charniak 2007), <papid> N07-2045 </papid>to an array of japanese learners?</prevsent>
</prevsection>
<citsent citstr=" P06-1031 ">
errors in english (izumi et al 2003), <papid> P03-2026 </papid>to verb errors (lee and seneff, 2008), <papid> P08-1021 </papid>and to article and preposition correction in texts written by non-native ells (han et al 2004, 2006; nagata et al 2005; nagata et al 2006; <papid> P06-1031 </papid>de felice and pul man, 2007; chodorow et al 2007; <papid> W07-1604 </papid>gamon et al 2008, 2009; tetreault and chodorow, 2008<papid> C08-1109 </papid>a).</citsent>
<aftsection>
<nextsent>1 http://www.eslassistant.com 73
</nextsent>
<nextsent>esl assistant takes hybrid approach that combines statistical and rule-based techniques.
</nextsent>
<nextsent>machine learning is used for those error types that are difficult to identify and resolve without taking into account complex contextual interactions, like article and preposition errors.
</nextsent>
<nextsent>rule-based approaches handle those error types that are amenable to simpler solutions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF640">
<title id=" W09-2111.xml">user input and interactions on microsoft research esl assistant </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>rule-based approaches have also been used to predict definite ness and indefinite ness of japanese noun phrases as preprocessing step for japanese to english machine translation (murata and nagao 1993; bond et al 1994; <papid> C94-1002 </papid>heine, 1998), task that is similar to the prediction of english articles.</prevsent>
<prevsent>more recently, data-driven approaches have gained popularity and been applied to article prediction in english (knight and chander 1994; minnen et al 2000; <papid> W00-0708 </papid>turner and charniak 2007), <papid> N07-2045 </papid>to an array of japanese learners?</prevsent>
</prevsection>
<citsent citstr=" W07-1604 ">
errors in english (izumi et al 2003), <papid> P03-2026 </papid>to verb errors (lee and seneff, 2008), <papid> P08-1021 </papid>and to article and preposition correction in texts written by non-native ells (han et al 2004, 2006; nagata et al 2005; nagata et al 2006; <papid> P06-1031 </papid>de felice and pul man, 2007; chodorow et al 2007; <papid> W07-1604 </papid>gamon et al 2008, 2009; tetreault and chodorow, 2008<papid> C08-1109 </papid>a).</citsent>
<aftsection>
<nextsent>1 http://www.eslassistant.com 73
</nextsent>
<nextsent>esl assistant takes hybrid approach that combines statistical and rule-based techniques.
</nextsent>
<nextsent>machine learning is used for those error types that are difficult to identify and resolve without taking into account complex contextual interactions, like article and preposition errors.
</nextsent>
<nextsent>rule-based approaches handle those error types that are amenable to simpler solutions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF641">
<title id=" W09-2111.xml">user input and interactions on microsoft research esl assistant </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>rule-based approaches have also been used to predict definite ness and indefinite ness of japanese noun phrases as preprocessing step for japanese to english machine translation (murata and nagao 1993; bond et al 1994; <papid> C94-1002 </papid>heine, 1998), task that is similar to the prediction of english articles.</prevsent>
<prevsent>more recently, data-driven approaches have gained popularity and been applied to article prediction in english (knight and chander 1994; minnen et al 2000; <papid> W00-0708 </papid>turner and charniak 2007), <papid> N07-2045 </papid>to an array of japanese learners?</prevsent>
</prevsection>
<citsent citstr=" C08-1109 ">
errors in english (izumi et al 2003), <papid> P03-2026 </papid>to verb errors (lee and seneff, 2008), <papid> P08-1021 </papid>and to article and preposition correction in texts written by non-native ells (han et al 2004, 2006; nagata et al 2005; nagata et al 2006; <papid> P06-1031 </papid>de felice and pul man, 2007; chodorow et al 2007; <papid> W07-1604 </papid>gamon et al 2008, 2009; tetreault and chodorow, 2008<papid> C08-1109 </papid>a).</citsent>
<aftsection>
<nextsent>1 http://www.eslassistant.com 73
</nextsent>
<nextsent>esl assistant takes hybrid approach that combines statistical and rule-based techniques.
</nextsent>
<nextsent>machine learning is used for those error types that are difficult to identify and resolve without taking into account complex contextual interactions, like article and preposition errors.
</nextsent>
<nextsent>rule-based approaches handle those error types that are amenable to simpler solutions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF647">
<title id=" W09-2506.xml">ranking paraphrases in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an evaluation on the semeval 2007 lexical substitution task data shows promising re sults: the model significantly outperforms current state of the art model, and our treatment of context is effective.
</prevsent>
<prevsent>knowledge about paraphrases is of central importance to textual inference modeling.
</prevsent>
</prevsection>
<citsent citstr=" W04-3206 ">
systems which support automatic extraction of large repositories of paraphrase or inference rules like lin and pantel (2001) or szpektor et al (2004) <papid> W04-3206 </papid>thus form first-class candidate resources to be leveraged for nlp tasks like question answering, information extraction, or summarization, and the meta-task of recognizing textual entailment.</citsent>
<aftsection>
<nextsent>existing knowledge bases still suffer number of limitations, making their use in applications challenging.
</nextsent>
<nextsent>one of the most serious problem sis insensitivity to context.
</nextsent>
<nextsent>natural-language inference is highly context-sensitive, the applicability of inference rules depending on word sense andeven finer grained contextual distinctions in us age (szpektor et al, 2007).<papid> P07-1058 </papid></nextsent>
<nextsent>application of rule like shed ? throw y?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF648">
<title id=" W09-2506.xml">ranking paraphrases in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>existing knowledge bases still suffer number of limitations, making their use in applications challenging.
</prevsent>
<prevsent>one of the most serious problem sis insensitivity to context.
</prevsent>
</prevsection>
<citsent citstr=" P07-1058 ">
natural-language inference is highly context-sensitive, the applicability of inference rules depending on word sense andeven finer grained contextual distinctions in us age (szpektor et al, 2007).<papid> P07-1058 </papid></citsent>
<aftsection>
<nextsent>application of rule like shed ? throw y?
</nextsent>
<nextsent>is appropriate in sentence like mouse study sheds light on the mixed results,?
</nextsent>
<nextsent>but not in sentences like the economy seems to be shedding fewer jobs?
</nextsent>
<nextsent>or cats do not shed the virus to other cats.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF649">
<title id=" W09-2506.xml">ranking paraphrases in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but not in sentences like the economy seems to be shedding fewer jobs?
</prevsent>
<prevsent>or cats do not shed the virus to other cats.?
</prevsent>
</prevsection>
<citsent citstr=" N07-1071 ">
systems like the above-mentioned ones base the extraction of inference rules on distributional similarity of words rather than word senses, and apply unconditionally whenever one side of the rule matches on the word level, which may lead to considerable precision problems (geffet and dagan, 2005) .some approaches address the problem of context sensitivity by deriving inference rules whose argument slots bear selectional preference information (pantel et al, 2007; <papid> N07-1071 </papid>basili et al, 2007).</citsent>
<aftsection>
<nextsent>a different line of accounting for contextual variation has been taken by mitchell and lapata (2008), <papid> P08-1028 </papid>who propose compositional approach, contextualiz ing?</nextsent>
<nextsent>the vector-space meaning representation of predicates by combining the distributional properties of the predicate with those of its arguments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF650">
<title id=" W09-2506.xml">ranking paraphrases in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or cats do not shed the virus to other cats.?
</prevsent>
<prevsent>systems like the above-mentioned ones base the extraction of inference rules on distributional similarity of words rather than word senses, and apply unconditionally whenever one side of the rule matches on the word level, which may lead to considerable precision problems (geffet and dagan, 2005) .some approaches address the problem of context sensitivity by deriving inference rules whose argument slots bear selectional preference information (pantel et al, 2007; <papid> N07-1071 </papid>basili et al, 2007).</prevsent>
</prevsection>
<citsent citstr=" P08-1028 ">
a different line of accounting for contextual variation has been taken by mitchell and lapata (2008), <papid> P08-1028 </papid>who propose compositional approach, contextualiz ing?</citsent>
<aftsection>
<nextsent>the vector-space meaning representation of predicates by combining the distributional properties of the predicate with those of its arguments.
</nextsent>
<nextsent>a related approach has been proposed by erk and pad?
</nextsent>
<nextsent>(2008), who integrate selectional preferences into the compositional picture.
</nextsent>
<nextsent>in this paper, we propose context-sensitive vector-space approach which draws some important ideas from erk andpados paper (e&p;? in the following), but implements them in different, more effective way: an evaluation on the semeval 2007 lexical substitution task data shows that our model significantly outperforms e&p; in terms of average precision.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF651">
<title id=" W09-2506.xml">ranking paraphrases in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the vectors representing predicate meaning in our model.
</prevsent>
<prevsent>3 evaluation.
</prevsent>
</prevsection>
<citsent citstr=" W07-2009 ">
we evaluate our model on paraphrase ranking task on subset of the semeval 2007 lexical substitution task (mccarthy and navigli, 2007) <papid> W07-2009 </papid>data, and compare it to random baseline and e&ps; state of the art model.dataset.</citsent>
<aftsection>
<nextsent>the lexical substitution task dataset contains 10 instances for 44 target verbs in different sentential contexts.
</nextsent>
<nextsent>systems that participated in the task had to generate paraphrases for each of these instances, which are evaluated against gold standard containing up to 9 possible paraphrases for individual instances.
</nextsent>
<nextsent>following erk and pad?
</nextsent>
<nextsent>(2008), we use the data in different fashion: we pool paraphrases for all instances of verb in all contexts, and use the models to rank these paraphrase candidates in specific contexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF652">
<title id=" W09-2506.xml">ranking paraphrases in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on average, target verbs have 20.5 paraphrase candidates, 3.9 of which are correct in specific contexts.
</prevsent>
<prevsent>experimental setup.
</prevsent>
</prevsection>
<citsent citstr=" P93-1016 ">
we parse the bnc usingminipar (lin, 1993) <papid> P93-1016 </papid>and extract co-occurrence frequencies, considering only dependency relations for the most frequent 2000 verbs.</citsent>
<aftsection>
<nextsent>we dont use raw frequency counts directly but re weight the vectors by pointwise mutual information.to rank paraphrases in context, we compute contextually constrained vectors for the verb in the input sentence and all its paraphrase candidates by taking the corresponding predicate vectors and restricting them to the argument meanings of the argument head nouns in the input sentence.
</nextsent>
<nextsent>the restricted vectors for the paraphrase candidates are then ranked by comparing them to the restricted vector of the input verb using cosine similarity.
</nextsent>
<nextsent>in order to compare our model with state of the art, we re implement e&ps; structured vector space model.
</nextsent>
<nextsent>we filter stop words, and compute lexical vectors in syntactic?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF653">
<title id=" W09-2506.xml">ranking paraphrases in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for the variants of the e&p; models in which the basis correspond sto words indexed by their syntactic role, we obtain different results, but our model is still   4% better than these variants.
</prevsent>
<prevsent>we can also see that our treatment of context is effective, leading to   3% increase of gap.
</prevsent>
</prevsection>
<citsent citstr=" C00-2137 ">
a stratified shuffling-basedrandomization test (yeh, 2000) <papid> C00-2137 </papid>shows that the differences are statistically significant (p   0.05).</citsent>
<aftsection>
<nextsent>in terms of oot , the add?
</nextsent>
<nextsent>e&p; model performs better than our model, which might look surprising,given its low gap score.
</nextsent>
<nextsent>fig.
</nextsent>
<nextsent>1 gives more fine grained comparison between the two models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF654">
<title id=" W09-2705.xml">addressing howto questions using a spoken dialogue system a viable approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>question answering (qa) is nowadays an established technology, advancing information retrieval to the point of allowing queries to be formulated in natural language and to return actual answers (in the form of sentences/phrases).
</prevsent>
<prevsent>while the first qa systems (simmons, 1965)mainly dealt with factoid questions, i.e. questions about names, dates and all that can be reduced to fact, number of systems in the last decade have appeared with the aim of addressingnon-factoid questions (voorhees, 2003).
</prevsent>
</prevsection>
<citsent citstr=" P06-1136 ">
in particular, the problem of addressing definition questions has received great attention from the research community (chen et al, 2006; <papid> P06-1136 </papid>moschitti et al, 2007), <papid> P07-1098 </papid>while less research has been conducted sofar on other types of non-factoid qa, such as why questions (verberne et al, 2007; pechsiri et al,2008) and procedural (also called how-to) questions (yin, 2006; <papid> E06-3006 </papid>delpech and saint-dizier, 2008).</citsent>
<aftsection>
<nextsent>another recent trend in qa is interactivity, i.e. the use of dialogue interface to better support the user, e.g. by resolving anaphoric and elliptic expressions in his/her queries (webb and strzalkowski, 2006).
</nextsent>
<nextsent>indeed, the dialogue community has been addressing the problem of information seeking for decades, often with very satisfying commercial products able to interact not only in text but especially via spoken interfaces (gupta et al., 2006; traum, 1996).
</nextsent>
<nextsent>however, also in this field the information retrieval task has mainly focused on limited domain (travel planning, telecom rates) and on returning database values rather than cooperatively solving problems or providing complex information.
</nextsent>
<nextsent>in this paper, we focus on handling procedural questions, not as commonly researched as definitional qa but for which number of resources are available on the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF655">
<title id=" W09-2705.xml">addressing howto questions using a spoken dialogue system a viable approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>question answering (qa) is nowadays an established technology, advancing information retrieval to the point of allowing queries to be formulated in natural language and to return actual answers (in the form of sentences/phrases).
</prevsent>
<prevsent>while the first qa systems (simmons, 1965)mainly dealt with factoid questions, i.e. questions about names, dates and all that can be reduced to fact, number of systems in the last decade have appeared with the aim of addressingnon-factoid questions (voorhees, 2003).
</prevsent>
</prevsection>
<citsent citstr=" P07-1098 ">
in particular, the problem of addressing definition questions has received great attention from the research community (chen et al, 2006; <papid> P06-1136 </papid>moschitti et al, 2007), <papid> P07-1098 </papid>while less research has been conducted sofar on other types of non-factoid qa, such as why questions (verberne et al, 2007; pechsiri et al,2008) and procedural (also called how-to) questions (yin, 2006; <papid> E06-3006 </papid>delpech and saint-dizier, 2008).</citsent>
<aftsection>
<nextsent>another recent trend in qa is interactivity, i.e. the use of dialogue interface to better support the user, e.g. by resolving anaphoric and elliptic expressions in his/her queries (webb and strzalkowski, 2006).
</nextsent>
<nextsent>indeed, the dialogue community has been addressing the problem of information seeking for decades, often with very satisfying commercial products able to interact not only in text but especially via spoken interfaces (gupta et al., 2006; traum, 1996).
</nextsent>
<nextsent>however, also in this field the information retrieval task has mainly focused on limited domain (travel planning, telecom rates) and on returning database values rather than cooperatively solving problems or providing complex information.
</nextsent>
<nextsent>in this paper, we focus on handling procedural questions, not as commonly researched as definitional qa but for which number of resources are available on the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF656">
<title id=" W09-2705.xml">addressing howto questions using a spoken dialogue system a viable approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>question answering (qa) is nowadays an established technology, advancing information retrieval to the point of allowing queries to be formulated in natural language and to return actual answers (in the form of sentences/phrases).
</prevsent>
<prevsent>while the first qa systems (simmons, 1965)mainly dealt with factoid questions, i.e. questions about names, dates and all that can be reduced to fact, number of systems in the last decade have appeared with the aim of addressingnon-factoid questions (voorhees, 2003).
</prevsent>
</prevsection>
<citsent citstr=" E06-3006 ">
in particular, the problem of addressing definition questions has received great attention from the research community (chen et al, 2006; <papid> P06-1136 </papid>moschitti et al, 2007), <papid> P07-1098 </papid>while less research has been conducted sofar on other types of non-factoid qa, such as why questions (verberne et al, 2007; pechsiri et al,2008) and procedural (also called how-to) questions (yin, 2006; <papid> E06-3006 </papid>delpech and saint-dizier, 2008).</citsent>
<aftsection>
<nextsent>another recent trend in qa is interactivity, i.e. the use of dialogue interface to better support the user, e.g. by resolving anaphoric and elliptic expressions in his/her queries (webb and strzalkowski, 2006).
</nextsent>
<nextsent>indeed, the dialogue community has been addressing the problem of information seeking for decades, often with very satisfying commercial products able to interact not only in text but especially via spoken interfaces (gupta et al., 2006; traum, 1996).
</nextsent>
<nextsent>however, also in this field the information retrieval task has mainly focused on limited domain (travel planning, telecom rates) and on returning database values rather than cooperatively solving problems or providing complex information.
</nextsent>
<nextsent>in this paper, we focus on handling procedural questions, not as commonly researched as definitional qa but for which number of resources are available on the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF657">
<title id=" W10-0703.xml">clustering dictionary definitions using amazon mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>beyond that, automatic processing, known as word sense disambiguation (wsd) is essential.
</prevsent>
<prevsent>most approaches are supervised and need large amounts of data to train the classifier for each and every word that is to be taught and assessed.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
amazon mechanical turk (mturk) has been used for the purpose of word sense disambiguation (snow et al 2008).<papid> D08-1027 </papid></citsent>
<aftsection>
<nextsent>the results show that non experts do very well (100% accuracy) when asked to identify the correct sense of word out of finite set of labels created by an expert.
</nextsent>
<nextsent>it is therefore possible to use mturk to build training corpus for wsd.
</nextsent>
<nextsent>in order to extend the snow et al crowd sourced disambiguation to large number of words, we need an efficient way to create the set of senses of word.
</nextsent>
<nextsent>asking an expert to do this is costly in time and money.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF660">
<title id=" W10-0703.xml">clustering dictionary definitions using amazon mechanical turk </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>semeval 2007 and 2010 (siglex, 2008) both evaluate wsi systems.
</prevsent>
<prevsent>the i2r system achieved the best results in 2007 with an f-score of 81.6% (i2r by niu (2007)).
</prevsent>
</prevsection>
<citsent citstr=" D07-1107 ">
snow et al (2007) <papid> D07-1107 </papid>have good description of the inherent problem of wsi where the appropriate granularity of the clusters varies for each application.</citsent>
<aftsection>
<nextsent>they try to solve this problem by building hierarchical-like word sense structures.
</nextsent>
<nextsent>in our case, each dictionary definition for word could be considered as unique sense for that word.
</nextsent>
<nextsent>then, when using mturk as platform for wsd, we could simply ask the workers to select which of the dictionary definitions best expresses the meaning of the words in document.
</nextsent>
<nextsent>the problem here is that most dictionaries give quite several definitions for word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF663">
<title id=" W10-0703.xml">clustering dictionary definitions using amazon mechanical turk </title>
<section> evaluation of global-view vs. local-view.  </section>
<citcontext>
<prevsection>
<prevsent>the literature offers many metrics to compare two annotators?
</prevsent>
<prevsent>clustering solutions (purity and entropy (zhao and karypis, 2001), clustering f-measure (fung et al, 2003) and many others).
</prevsent>
</prevsection>
<citsent citstr=" D07-1043 ">
semeval-2 includes wsi task where v-measure (rosenberg and hirschberg, 2007) <papid> D07-1043 </papid>is used to evaluate the clustering solutions.</citsent>
<aftsection>
<nextsent>v-measure involves two metrics, homogeneity and completeness, that can be thought of as precision and recall.
</nextsent>
<nextsent>perfect homogeneity is obtained if the solutions have clusters whose data points belong to single cluster in the gs.
</nextsent>
<nextsent>perfect completeness is obtained if the clusters in the gs contain data points that belong to single cluster in the evaluated solution.
</nextsent>
<nextsent>the v-measure is (weighted) harmonic mean of the homogeneity and of the completeness metrics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF664">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" H05-1101 ">
the empirical adequacy of synchronous context-free grammars of rank two (2-scfgs)(satta and peserico, 2005), <papid> H05-1101 </papid>used in syntax based machine translation systems such aswu (1997), <papid> J97-3002 </papid>zhang et al (2006) <papid> N06-1033 </papid>and chiang (2007), <papid> J07-2003 </papid>in terms of what alignments they induce, has been discussed in wu (1997) <papid> J97-3002 </papid>and wellington et al (2006), <papid> P06-1123 </papid>but with one-sided focus on so-called inside-out alignments?.</citsent>
<aftsection>
<nextsent>other alignment configurations that cannot be induced by 2-scfgs are identified in this paper, and their frequencies across wide collection of hand-aligned parallel corpora are examined.
</nextsent>
<nextsent>empirical lower bounds on two measures of alignment error rate, i.e. the one introduced in och and ney (2000) <papid> C00-2163 </papid>and one where only complete translation units are considered, are derived for 2-scfgs and related formalisms.</nextsent>
<nextsent>syntax-based approaches to machine translation typically use synchronous grammars to recognize or produce translation equivalents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF665">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J97-3002 ">
the empirical adequacy of synchronous context-free grammars of rank two (2-scfgs)(satta and peserico, 2005), <papid> H05-1101 </papid>used in syntax based machine translation systems such aswu (1997), <papid> J97-3002 </papid>zhang et al (2006) <papid> N06-1033 </papid>and chiang (2007), <papid> J07-2003 </papid>in terms of what alignments they induce, has been discussed in wu (1997) <papid> J97-3002 </papid>and wellington et al (2006), <papid> P06-1123 </papid>but with one-sided focus on so-called inside-out alignments?.</citsent>
<aftsection>
<nextsent>other alignment configurations that cannot be induced by 2-scfgs are identified in this paper, and their frequencies across wide collection of hand-aligned parallel corpora are examined.
</nextsent>
<nextsent>empirical lower bounds on two measures of alignment error rate, i.e. the one introduced in och and ney (2000) <papid> C00-2163 </papid>and one where only complete translation units are considered, are derived for 2-scfgs and related formalisms.</nextsent>
<nextsent>syntax-based approaches to machine translation typically use synchronous grammars to recognize or produce translation equivalents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF668">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" N06-1033 ">
the empirical adequacy of synchronous context-free grammars of rank two (2-scfgs)(satta and peserico, 2005), <papid> H05-1101 </papid>used in syntax based machine translation systems such aswu (1997), <papid> J97-3002 </papid>zhang et al (2006) <papid> N06-1033 </papid>and chiang (2007), <papid> J07-2003 </papid>in terms of what alignments they induce, has been discussed in wu (1997) <papid> J97-3002 </papid>and wellington et al (2006), <papid> P06-1123 </papid>but with one-sided focus on so-called inside-out alignments?.</citsent>
<aftsection>
<nextsent>other alignment configurations that cannot be induced by 2-scfgs are identified in this paper, and their frequencies across wide collection of hand-aligned parallel corpora are examined.
</nextsent>
<nextsent>empirical lower bounds on two measures of alignment error rate, i.e. the one introduced in och and ney (2000) <papid> C00-2163 </papid>and one where only complete translation units are considered, are derived for 2-scfgs and related formalisms.</nextsent>
<nextsent>syntax-based approaches to machine translation typically use synchronous grammars to recognize or produce translation equivalents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF670">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J07-2003 ">
the empirical adequacy of synchronous context-free grammars of rank two (2-scfgs)(satta and peserico, 2005), <papid> H05-1101 </papid>used in syntax based machine translation systems such aswu (1997), <papid> J97-3002 </papid>zhang et al (2006) <papid> N06-1033 </papid>and chiang (2007), <papid> J07-2003 </papid>in terms of what alignments they induce, has been discussed in wu (1997) <papid> J97-3002 </papid>and wellington et al (2006), <papid> P06-1123 </papid>but with one-sided focus on so-called inside-out alignments?.</citsent>
<aftsection>
<nextsent>other alignment configurations that cannot be induced by 2-scfgs are identified in this paper, and their frequencies across wide collection of hand-aligned parallel corpora are examined.
</nextsent>
<nextsent>empirical lower bounds on two measures of alignment error rate, i.e. the one introduced in och and ney (2000) <papid> C00-2163 </papid>and one where only complete translation units are considered, are derived for 2-scfgs and related formalisms.</nextsent>
<nextsent>syntax-based approaches to machine translation typically use synchronous grammars to recognize or produce translation equivalents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF673">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P06-1123 ">
the empirical adequacy of synchronous context-free grammars of rank two (2-scfgs)(satta and peserico, 2005), <papid> H05-1101 </papid>used in syntax based machine translation systems such aswu (1997), <papid> J97-3002 </papid>zhang et al (2006) <papid> N06-1033 </papid>and chiang (2007), <papid> J07-2003 </papid>in terms of what alignments they induce, has been discussed in wu (1997) <papid> J97-3002 </papid>and wellington et al (2006), <papid> P06-1123 </papid>but with one-sided focus on so-called inside-out alignments?.</citsent>
<aftsection>
<nextsent>other alignment configurations that cannot be induced by 2-scfgs are identified in this paper, and their frequencies across wide collection of hand-aligned parallel corpora are examined.
</nextsent>
<nextsent>empirical lower bounds on two measures of alignment error rate, i.e. the one introduced in och and ney (2000) <papid> C00-2163 </papid>and one where only complete translation units are considered, are derived for 2-scfgs and related formalisms.</nextsent>
<nextsent>syntax-based approaches to machine translation typically use synchronous grammars to recognize or produce translation equivalents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF674">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>the empirical adequacy of synchronous context-free grammars of rank two (2-scfgs)(satta and peserico, 2005), <papid> H05-1101 </papid>used in syntax based machine translation systems such aswu (1997), <papid> J97-3002 </papid>zhang et al (2006) <papid> N06-1033 </papid>and chiang (2007), <papid> J07-2003 </papid>in terms of what alignments they induce, has been discussed in wu (1997) <papid> J97-3002 </papid>and wellington et al (2006), <papid> P06-1123 </papid>but with one-sided focus on so-called inside-out alignments?.</prevsent>
<prevsent>other alignment configurations that cannot be induced by 2-scfgs are identified in this paper, and their frequencies across wide collection of hand-aligned parallel corpora are examined.</prevsent>
</prevsection>
<citsent citstr=" C00-2163 ">
empirical lower bounds on two measures of alignment error rate, i.e. the one introduced in och and ney (2000) <papid> C00-2163 </papid>and one where only complete translation units are considered, are derived for 2-scfgs and related formalisms.</citsent>
<aftsection>
<nextsent>syntax-based approaches to machine translation typically use synchronous grammars to recognize or produce translation equivalents.
</nextsent>
<nextsent>the synchronous this work was done while the first author was senior researcher at the dpt.
</nextsent>
<nextsent>of linguistics, university of potsdam, supported by the german research foundation in the emmynoether project ptolemaios on grammar learning from parallel corpora; and while he was postdoctoral researcher at the isv computational linguistics group, copenhagen business school, supported by the danish research foundation in the project efficient syntax- and semantics-based machine translation.
</nextsent>
<nextsent>the second author is supported by the german research foundation in the emmy noether project ptolemaios on grammar learning from parallel corpora.production rules are typically learned from alignment structures (wu, 1997; <papid> J97-3002 </papid>zhang and gildea, 2004; <papid> C04-1060 </papid>chiang, 2007) <papid> J07-2003 </papid>or from alignment structures and derivation trees for the source string (yamada and knight, 2001; <papid> P01-1067 </papid>zhang and gildea, 2004).<papid> C04-1060 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF677">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the synchronous this work was done while the first author was senior researcher at the dpt.
</prevsent>
<prevsent>of linguistics, university of potsdam, supported by the german research foundation in the emmynoether project ptolemaios on grammar learning from parallel corpora; and while he was postdoctoral researcher at the isv computational linguistics group, copenhagen business school, supported by the danish research foundation in the project efficient syntax- and semantics-based machine translation.
</prevsent>
</prevsection>
<citsent citstr=" C04-1060 ">
the second author is supported by the german research foundation in the emmy noether project ptolemaios on grammar learning from parallel corpora.production rules are typically learned from alignment structures (wu, 1997; <papid> J97-3002 </papid>zhang and gildea, 2004; <papid> C04-1060 </papid>chiang, 2007) <papid> J07-2003 </papid>or from alignment structures and derivation trees for the source string (yamada and knight, 2001; <papid> P01-1067 </papid>zhang and gildea, 2004).<papid> C04-1060 </papid></citsent>
<aftsection>
<nextsent>they are also used for inducing alignments (wu, 1997; <papid> J97-3002 </papid>zhang and gildea, 2004).<papid> C04-1060 </papid>it is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (wellington et al, 2006).<papid> P06-1123 </papid></nextsent>
<nextsent>such alignments are supposed to reflect the structure of translations, typically contain fewer errors and are used to evaluate automatically induced alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF679">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the synchronous this work was done while the first author was senior researcher at the dpt.
</prevsent>
<prevsent>of linguistics, university of potsdam, supported by the german research foundation in the emmynoether project ptolemaios on grammar learning from parallel corpora; and while he was postdoctoral researcher at the isv computational linguistics group, copenhagen business school, supported by the danish research foundation in the project efficient syntax- and semantics-based machine translation.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
the second author is supported by the german research foundation in the emmy noether project ptolemaios on grammar learning from parallel corpora.production rules are typically learned from alignment structures (wu, 1997; <papid> J97-3002 </papid>zhang and gildea, 2004; <papid> C04-1060 </papid>chiang, 2007) <papid> J07-2003 </papid>or from alignment structures and derivation trees for the source string (yamada and knight, 2001; <papid> P01-1067 </papid>zhang and gildea, 2004).<papid> C04-1060 </papid></citsent>
<aftsection>
<nextsent>they are also used for inducing alignments (wu, 1997; <papid> J97-3002 </papid>zhang and gildea, 2004).<papid> C04-1060 </papid>it is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (wellington et al, 2006).<papid> P06-1123 </papid></nextsent>
<nextsent>such alignments are supposed to reflect the structure of translations, typically contain fewer errors and are used to evaluate automatically induced alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF697">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the synchronous grammars used in these systems are, formally, synchronous context-free grammars of rank two (2-scfgs), or equivalently (nor mal form) inversion transduction grammars (itgs).1the notion of rank is defined as the maximum number of constituents aligned by production rule, i.e. the maximum number of distinct indeces.
</prevsent>
<prevsent>our results will be extended to slight extensions of 2 scfgs, incl.
</prevsent>
</prevsection>
<citsent citstr=" P03-1019 ">
the extension of itgs proposed by zens and ney (2003) <papid> P03-1019 </papid>xitgs), synchronous tree substitution grammars of rank two (2-stsgs) (eisner, 2003; <papid> P03-2041 </papid>shieber, 2007), <papid> W07-0412 </papid>i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 12-scfgs allow distinct lhs nonterminals, while itgs do not; but for any 2-scfg an equivalent itg can be constructed by creating cross-product of nonterminals from two sides.</citsent>
<aftsection>
<nextsent>19 (2-stags) (shieber and schabes, 1990; <papid> C90-3045 </papid>harbusch and poller, 1996; nesson et al, 2008).</nextsent>
<nextsent>the over all frequency of alignment structures that cannot be induced by these approaches is examined across wide collection of hand-aligned parallel corpora.empirical lower bounds on the coverage of the systems are derived from our results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF700">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the synchronous grammars used in these systems are, formally, synchronous context-free grammars of rank two (2-scfgs), or equivalently (nor mal form) inversion transduction grammars (itgs).1the notion of rank is defined as the maximum number of constituents aligned by production rule, i.e. the maximum number of distinct indeces.
</prevsent>
<prevsent>our results will be extended to slight extensions of 2 scfgs, incl.
</prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
the extension of itgs proposed by zens and ney (2003) <papid> P03-1019 </papid>xitgs), synchronous tree substitution grammars of rank two (2-stsgs) (eisner, 2003; <papid> P03-2041 </papid>shieber, 2007), <papid> W07-0412 </papid>i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 12-scfgs allow distinct lhs nonterminals, while itgs do not; but for any 2-scfg an equivalent itg can be constructed by creating cross-product of nonterminals from two sides.</citsent>
<aftsection>
<nextsent>19 (2-stags) (shieber and schabes, 1990; <papid> C90-3045 </papid>harbusch and poller, 1996; nesson et al, 2008).</nextsent>
<nextsent>the over all frequency of alignment structures that cannot be induced by these approaches is examined across wide collection of hand-aligned parallel corpora.empirical lower bounds on the coverage of the systems are derived from our results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF701">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the synchronous grammars used in these systems are, formally, synchronous context-free grammars of rank two (2-scfgs), or equivalently (nor mal form) inversion transduction grammars (itgs).1the notion of rank is defined as the maximum number of constituents aligned by production rule, i.e. the maximum number of distinct indeces.
</prevsent>
<prevsent>our results will be extended to slight extensions of 2 scfgs, incl.
</prevsent>
</prevsection>
<citsent citstr=" W07-0412 ">
the extension of itgs proposed by zens and ney (2003) <papid> P03-1019 </papid>xitgs), synchronous tree substitution grammars of rank two (2-stsgs) (eisner, 2003; <papid> P03-2041 </papid>shieber, 2007), <papid> W07-0412 </papid>i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 12-scfgs allow distinct lhs nonterminals, while itgs do not; but for any 2-scfg an equivalent itg can be constructed by creating cross-product of nonterminals from two sides.</citsent>
<aftsection>
<nextsent>19 (2-stags) (shieber and schabes, 1990; <papid> C90-3045 </papid>harbusch and poller, 1996; nesson et al, 2008).</nextsent>
<nextsent>the over all frequency of alignment structures that cannot be induced by these approaches is examined across wide collection of hand-aligned parallel corpora.empirical lower bounds on the coverage of the systems are derived from our results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF702">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results will be extended to slight extensions of 2 scfgs, incl.
</prevsent>
<prevsent>the extension of itgs proposed by zens and ney (2003) <papid> P03-1019 </papid>xitgs), synchronous tree substitution grammars of rank two (2-stsgs) (eisner, 2003; <papid> P03-2041 </papid>shieber, 2007), <papid> W07-0412 </papid>i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 12-scfgs allow distinct lhs nonterminals, while itgs do not; but for any 2-scfg an equivalent itg can be constructed by creating cross-product of nonterminals from two sides.</prevsent>
</prevsection>
<citsent citstr=" C90-3045 ">
19 (2-stags) (shieber and schabes, 1990; <papid> C90-3045 </papid>harbusch and poller, 1996; nesson et al, 2008).</citsent>
<aftsection>
<nextsent>the over all frequency of alignment structures that cannot be induced by these approaches is examined across wide collection of hand-aligned parallel corpora.empirical lower bounds on the coverage of the systems are derived from our results.
</nextsent>
<nextsent>our notion of an alignment structure is standard.
</nextsent>
<nextsent>words can be aligned to multiple words.
</nextsent>
<nextsent>unaligned nodes are permitted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF709">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>theaer, in the case where all alignments are sure alignments, is aer = 1?
</prevsent>
<prevsent>2|sa ga||sa| + |ga| where ga are the gold standard alignments, and sa the alignments produced by the system.
</prevsent>
</prevsection>
<citsent citstr=" J07-3002 ">
aer has been criticized by fraser and marcu (2007).<papid> J07-3002 </papid></citsent>
<aftsection>
<nextsent>they show that aer does not penalize unequal precision and recall when distinction between sure and possible alignments is2one of the hand-aligned parallel corpora used in our experiments, the one also used in pado?
</nextsent>
<nextsent>and lapata (2006), includes incomplete alignment structures.
</nextsent>
<nextsent>made.
</nextsent>
<nextsent>since no such distinction is assumed below, the classical definition is used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF713">
<title id=" W09-2303.xml">empirical lower bounds on aligment error rates in syntax based machine translation </title>
<section> inside-out alignments.  </section>
<citcontext>
<prevsection>
<prevsent>sect.
</prevsent>
<prevsent>6 compares our results to related work, in particular zens and ney (2003).<papid> P03-1019 </papid></prevsent>
</prevsection>
<citsent citstr=" E06-1019 ">
wu (1997) <papid> J97-3002 </papid>identified so-called inside-out alignments, two alignment configurations that cannot be induced by binary synchronous context-free grammars; these alignment configurations, while infrequent in language pairs such as english french (cherry and lin, 2006; <papid> E06-1019 </papid>wellington et al, 2006),<papid> P06-1123 </papid>have been argued to be frequent in other language pairs, incl.</citsent>
<aftsection>
<nextsent>english chinese (wellington etal., 2006) <papid> P06-1123 </papid>and english spanish (lepage and denoual, 2005).</nextsent>
<nextsent>while our main focus is on configurations that involve discontinuous translation units,the frequencies of inside-out alignments in our parallel corpora are also reported.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF734">
<title id=" W09-2307.xml">discriminative reordering with chinese grammatical relations features </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>the prevalence in chinese of grammatical structures that translate into english in different word orders is an important cause of translation difficulty.
</prevsent>
<prevsent>while previous work has used phrase-structure parses to deal with such ordering problems, we introduce richer set of chinese grammatical relations that describes more semantically abstract relations betweenwords.
</prevsent>
</prevsection>
<citsent citstr=" W06-3108 ">
using these chinese grammatical relations, we improve phrase orientation classifier (introduced by zens and ney (2006)) <papid> W06-3108 </papid>that decides the ordering of two phrases when translated into english by adding path features designed over the chinese typed dependencies.</citsent>
<aftsection>
<nextsent>we then apply the log probability of the phrase orientation classifier as an extra feature in phrase-based mt system, and get significant bleu point gains on three test sets: mt02 (+0.59), mt03 (+1.00) andmt05 (+0.77).
</nextsent>
<nextsent>our chinese grammatical relations are also likely to be useful for other nlp tasks.
</nextsent>
<nextsent>structural differences between chinese and english are major factor in the difficulty of machine translation from chinese to english.
</nextsent>
<nextsent>the wide variety of such chinese-english differences include the ordering of head nouns and relative clauses, and the ordering of prepositional phrases and the heads theymodify.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF735">
<title id=" W09-2307.xml">discriminative reordering with chinese grammatical relations features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the wide variety of such chinese-english differences include the ordering of head nouns and relative clauses, and the ordering of prepositional phrases and the heads theymodify.
</prevsent>
<prevsent>previous studies have shown that using syntactic structures from the source side can help mt performance on these constructions.
</prevsent>
</prevsection>
<citsent citstr=" W06-3601 ">
most of the previous syntactic mt work has used phrase structure parses in various ways, either by doing syntax directed translation to directly translate parse trees into strings in the target language (huang et al, 2006), <papid> W06-3601 </papid>or by using source-side parses to pre process the source sentences (wang et al, 2007).<papid> D07-1077 </papid>one intuition for using syntax is to capture different chinese structures that might have the same (a) (root (ip (lcp (qp (cd ?)</citsent>
<aftsection>
<nextsent>(clp (m ?))) (lc ?)) (pu ?)
</nextsent>
<nextsent>(np (dp (dt ??)) (np (nn ??))) (vp (advp (ad ??)) (vp (vv ??)
</nextsent>
<nextsent>(np (np (adjp (jj ??)) (np (nn ??))) (np (nn ??))) (qp (cd ?????)
</nextsent>
<nextsent>(clp (m ?))))) (pu ?))) (b) (root (ip (np (dp (dt ??)) (np (nn ??))) (vp (lcp (qp (cd ?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF736">
<title id=" W09-2307.xml">discriminative reordering with chinese grammatical relations features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the wide variety of such chinese-english differences include the ordering of head nouns and relative clauses, and the ordering of prepositional phrases and the heads theymodify.
</prevsent>
<prevsent>previous studies have shown that using syntactic structures from the source side can help mt performance on these constructions.
</prevsent>
</prevsection>
<citsent citstr=" D07-1077 ">
most of the previous syntactic mt work has used phrase structure parses in various ways, either by doing syntax directed translation to directly translate parse trees into strings in the target language (huang et al, 2006), <papid> W06-3601 </papid>or by using source-side parses to pre process the source sentences (wang et al, 2007).<papid> D07-1077 </papid>one intuition for using syntax is to capture different chinese structures that might have the same (a) (root (ip (lcp (qp (cd ?)</citsent>
<aftsection>
<nextsent>(clp (m ?))) (lc ?)) (pu ?)
</nextsent>
<nextsent>(np (dp (dt ??)) (np (nn ??))) (vp (advp (ad ??)) (vp (vv ??)
</nextsent>
<nextsent>(np (np (adjp (jj ??)) (np (nn ??))) (np (nn ??))) (qp (cd ?????)
</nextsent>
<nextsent>(clp (m ?))))) (pu ?))) (b) (root (ip (np (dp (dt ??)) (np (nn ??))) (vp (lcp (qp (cd ?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF739">
<title id=" W09-2307.xml">discriminative reordering with chinese grammatical relations features </title>
<section> discriminative reordering model.  </section>
<citcontext>
<prevsection>
<prevsent>to achieve this, we train adiscriminative phrase orientation classifier following the work by zens and ney (2006), <papid> W06-3108 </papid>and we use the grammatical relations between words as extra features to build the classifier.</prevsent>
<prevsent>we then apply the phrase orientation classifier as feature in phrase based mt system to help reordering.</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>the disadvantage of these models is their insensitivity to the content of the words or phrases.
</nextsent>
<nextsent>more recent work (tillman,2004; och et al, 2004; <papid> N04-1021 </papid>koehn et al, 2007) <papid> P07-2045 </papid>has introduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases.</nextsent>
<nextsent>lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF740">
<title id=" W09-2307.xml">discriminative reordering with chinese grammatical relations features </title>
<section> discriminative reordering model.  </section>
<citcontext>
<prevsection>
<prevsent>basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
<prevsent>the disadvantage of these models is their insensitivity to the content of the words or phrases.</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
more recent work (tillman,2004; och et al, 2004; <papid> N04-1021 </papid>koehn et al, 2007) <papid> P07-2045 </papid>has introduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases.</citsent>
<aftsection>
<nextsent>lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable.
</nextsent>
<nextsent>zens and ney (2006) <papid> W06-3108 </papid>proposed discriminatively trained phrase orientation model and evaluated its performance as classifier and when plugged into phrase-based mt system.</nextsent>
<nextsent>their framework allows us to easily add in extra features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF741">
<title id=" W09-2307.xml">discriminative reordering with chinese grammatical relations features </title>
<section> discriminative reordering model.  </section>
<citcontext>
<prevsection>
<prevsent>basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
<prevsent>the disadvantage of these models is their insensitivity to the content of the words or phrases.</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
more recent work (tillman,2004; och et al, 2004; <papid> N04-1021 </papid>koehn et al, 2007) <papid> P07-2045 </papid>has introduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases.</citsent>
<aftsection>
<nextsent>lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable.
</nextsent>
<nextsent>zens and ney (2006) <papid> W06-3108 </papid>proposed discriminatively trained phrase orientation model and evaluated its performance as classifier and when plugged into phrase-based mt system.</nextsent>
<nextsent>their framework allows us to easily add in extra features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF764">
<title id=" W09-2307.xml">discriminative reordering with chinese grammatical relations features </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>the parallel data contains 1,560,071 sentence pairs from various parallel corpora.
</prevsent>
<prevsent>there are 12,259,997 words on the english side.
</prevsent>
</prevsection>
<citsent citstr=" W08-0336 ">
chinese word segmentation is done by the stanford chinese segmenter (chang et al, 2008).<papid> W08-0336 </papid></citsent>
<aftsection>
<nextsent>after segmentation, there are 11,061,792 words on the chinese side.
</nextsent>
<nextsent>the alignment is done by the berkeley word aligner (liang et al, 2006) <papid> N06-1014 </papid>and then we symmetrized the word alignment using the grow-diag heuristic.</nextsent>
<nextsent>for the phrase orientation classifier experiments, we extracted labeled examples using the parallel data and the alignment as in figure 2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF765">
<title id=" W09-2307.xml">discriminative reordering with chinese grammatical relations features </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>chinese word segmentation is done by the stanford chinese segmenter (chang et al, 2008).<papid> W08-0336 </papid></prevsent>
<prevsent>after segmentation, there are 11,061,792 words on the chinese side.</prevsent>
</prevsection>
<citsent citstr=" N06-1014 ">
the alignment is done by the berkeley word aligner (liang et al, 2006) <papid> N06-1014 </papid>and then we symmetrized the word alignment using the grow-diag heuristic.</citsent>
<aftsection>
<nextsent>for the phrase orientation classifier experiments, we extracted labeled examples using the parallel data and the alignment as in figure 2.
</nextsent>
<nextsent>we extracted 9,194,193 total valid examples: 86.09% of them are ordered and the other 13.91% are reversed.
</nextsent>
<nextsent>to evaluate the classifier performance, we split these examples into training, dev and test set (8 : 1 : 1).
</nextsent>
<nextsent>the phrase orientation classifier used in mt experiments is trained with all of the available labeled examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF768">
<title id=" W09-2307.xml">discriminative reordering with chinese grammatical relations features </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>standard eight features as well as the lexicalized reordering features.
</prevsent>
<prevsent>to have more comparable setting with (zens and ney, 2006), <papid> W06-3108 </papid>wealso have baseline experiment with only the standard eight features.</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
parameter tuning is done with minimum error rate training (mert) (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>the tuning set for mert is the nist mt06 dataset, which includes 1664 sentences.
</nextsent>
<nextsent>we evaluate the result with mt02 (878 sentences), mt03 (919 sen 1ldc2002e18, ldc2003e07, ldc2003e14, ldc2005e83, ldc2005t06, ldc2006e26, ldc2006e85, ldc2002l27 and ldc2005t34.
</nextsent>
<nextsent>tences), and mt05 (1082 sentences).
</nextsent>
<nextsent>4.2 phrase orientation classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF774">
<title id=" W09-2307.xml">discriminative reordering with chinese grammatical relations features </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>we compared to two different baselines: one is moses8features which has distance-based reordering model, the other is baseline which also includes lexicalized reordering features.
</prevsent>
<prevsent>from the table we can see that using the discriminative reordering model with path features gives significant improvement over both base 57 setting #mert features mt06(tune) mt02 mt03 mt05 moses8features 8 31.49 31.63 31.26 30.26moses8features+discrimrereordernopath 9 31.76(+0.27) 31.86(+0.23) 32.09(+0.83) 31.14(+0.88)moses8features+discrimrereorderwithpath 9 32.34(+0.85) 32.59(+0.96) 32.70(+1.44) 31.84(+1.58) baseline (moses with lexicalized reordering) 16 32.55 32.56 32.65 31.89baseline+discrimrereordernopath 17 32.73(+0.18) 32.58(+0.02) 32.99(+0.34) 31.80(0.09)baseline+discrimrereorderwithpath 17 32.97(+0.42) 33.15(+0.59) 33.65(+1.00) 32.66(+0.77) table 4: mt experiments of different settings on various nist mt evaluation datasets.
</prevsent>
</prevsection>
<citsent citstr=" W05-0908 ">
all differences marked in bold are significant at the level of 0.05 with the approximate randomization test in (riezler and maxwell, 2005).<papid> W05-0908 </papid></citsent>
<aftsection>
<nextsent>det every level product nn ? ?
</nextsent>
<nextsent>products of all level ???
</nextsent>
<nextsent>whole city this year industry total output value det nn gross industrial output value of the whole city this year figure 4: two examples for the feature path:det-nn and how the reordering occurs.
</nextsent>
<nextsent>lines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF775">
<title id=" W10-0721.xml">collecting image annotations using amazonrsquos mechanical turk </title>
<section> quality control through pre-screening.  </section>
<citcontext>
<prevsection>
<prevsent>the corpus was annotated in 284 hours8, at total cost of $812.00 (plus amazons 10% fee).
</prevsent>
<prevsent>6 related work and conclusions.
</prevsent>
</prevsection>
<citsent citstr=" W09-0619 ">
related work mturk has been used for many different nlp and vision tasks (tietze et al, 2009; <papid> W09-0619 </papid>zaidan and callison-burch, 2009; snow et al, 2008;<papid> D08-1027 </papid>sorokin and forsyth, 2008).</citsent>
<aftsection>
<nextsent>due to the noise inherent in non-expert annotations, many other attempts at quality control have been made.
</nextsent>
<nextsent>kittur et al (2008) solicit ratings about different aspects of wikipedia articles.
</nextsent>
<nextsent>at first they receive very noisy results, due to turkers?
</nextsent>
<nextsent>not paying attention when completing the task or specifically trying to cheat the requester.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF776">
<title id=" W10-0721.xml">collecting image annotations using amazonrsquos mechanical turk </title>
<section> quality control through pre-screening.  </section>
<citcontext>
<prevsection>
<prevsent>the corpus was annotated in 284 hours8, at total cost of $812.00 (plus amazons 10% fee).
</prevsent>
<prevsent>6 related work and conclusions.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
related work mturk has been used for many different nlp and vision tasks (tietze et al, 2009; <papid> W09-0619 </papid>zaidan and callison-burch, 2009; snow et al, 2008;<papid> D08-1027 </papid>sorokin and forsyth, 2008).</citsent>
<aftsection>
<nextsent>due to the noise inherent in non-expert annotations, many other attempts at quality control have been made.
</nextsent>
<nextsent>kittur et al (2008) solicit ratings about different aspects of wikipedia articles.
</nextsent>
<nextsent>at first they receive very noisy results, due to turkers?
</nextsent>
<nextsent>not paying attention when completing the task or specifically trying to cheat the requester.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF777">
<title id=" W10-0721.xml">collecting image annotations using amazonrsquos mechanical turk </title>
<section> quality control through pre-screening.  </section>
<citcontext>
<prevsection>
<prevsent>they also added question that required the turkers to comprehend the content of the wikipedia article.
</prevsent>
<prevsent>with this new set up, they find that the quality greatly increases and carelessness is reduced.
</prevsent>
</prevsection>
<citsent citstr=" L08-1307 ">
kaisser and lowe (2008)<papid> L08-1307 </papid>7our final dataset consists of 1482 pictures from action photography, 1904 from dogs, 776 from flickr-social, 916 from out door, 1257 from strangers and 1773 from wild-child.8note that the annotation process scaled pretty well, considering that annotating more than eight times the number of images took only 31 hours longer.</citsent>
<aftsection>
<nextsent>collected question and answer pairs by presenting turkers with question and telling them to copy and paste from document of text they know to contain the answer.
</nextsent>
<nextsent>they achieve good but far from perfect inter annotator agreement based on the extracted answers.
</nextsent>
<nextsent>we speculate that the quality would bemuch worse if the turkers wrote the sentences themselves.
</nextsent>
<nextsent>callison-burch (2009) asks turkers to produce translations when given reference sentences inother languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF778">
<title id=" W10-0721.xml">collecting image annotations using amazonrsquos mechanical turk </title>
<section> quality control through pre-screening.  </section>
<citcontext>
<prevsection>
<prevsent>overall, he finds find that turkers produce better translations than machine translation systems.
</prevsent>
<prevsent>to eliminate translations from turkers who simply put the reference sentence into an online translation website, he performs follow-up task, where he asks other turkers to vote on if they believe that sentences were generated using an online translation system.
</prevsent>
</prevsection>
<citsent citstr=" P09-2078 ">
mihalcea and strapparava (2009) <papid> P09-2078 </papid>ask turkers to produce 4-5 sentence opinion paragraphs about the death penalty, about abortion and describing friend.</citsent>
<aftsection>
<nextsent>they report that aside from small number of invalid responses, all of the paragraphs were of good quality and followed their instructions.
</nextsent>
<nextsent>their success is surprising to us because they do not report using qualification test, and when we didthis our responses contained large amount of in correct english spelling and grammar.
</nextsent>
<nextsent>the turkit toolkit (little et al, 2009) provides another approach to improving the quality of mturk annotations.
</nextsent>
<nextsent>their iterative framework allows the requester to set up series of tasks that first solicits text annotations from turkers and then asks otherturkers to improve the annotations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF779">
<title id=" W10-0715.xml">an enriched mt grammar for under 100 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to that end, we ask human annotators on amazon mechanical turk to compare translation candidates, andthen interpret their preferences of one candidate over another as an implicit preference for one derivation over another, and therefore asan implicit preference for one or more grammar rules.
</prevsent>
<prevsent>our framework also allows us to generalize these preferences to grammar rules corresponding to previously unseen test set, namely rules for which no candidates have been judged.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
when translating between two languages, stateof-the-art statistical machine translation systems (koehn et al, 2007; <papid> P07-2045 </papid>li et al, 2009) generate candidate translations by relying on set of relevant grammar (or phrase table) entries.</citsent>
<aftsection>
<nextsent>each of those entries, or rules, associates string in the source language with string in the target language, with these associations typically learned by examining large parallel bitext.
</nextsent>
<nextsent>by the very nature of the translation process, target side sentence can be candidate translation for source sentence only if can be constructed using small subset of the grammar, namely the subset of rules with source side sequences relevant to the word sequence of . however, even this limited set of candidates (call it e(f)) is quite large, with |e(f)| growing exponentially in the length of . the system is able to rank the translations within e(f) by assigning score s(e) to each candidate translation.
</nextsent>
<nextsent>this score is the dot product: s(e) = ~?(e) ? ~w (1) where ~?(e) is feature vector characterizing e, and ~w is system-specific weight vector characterizing the systems belief of how much the different features reflect translation quality.
</nextsent>
<nextsent>the features of candidate are computed by examining the way is constructed (or derived), and so if we let d(e) be the derivation of e, the feature vector can be denoted:1 ~?(d(e)) = 1(d(e)), . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF780">
<title id=" W10-0713.xml">annotating named entities in twitter data with crowdsourcing </title>
<section> introduction and dataset description.  </section>
<citcontext>
<prevsection>

<prevsent>information extraction researchers commonly workon popular formal domains, such as news articles.
</prevsent>
</prevsection>
<citsent citstr=" L08-1390 ">
more diverse studies have included broadcast news transcripts, blogs and emails (strassel et al, 2008).<papid> L08-1390 </papid></citsent>
<aftsection>
<nextsent>however, extremely informal domains, suchas face book, twitter, youtube or flickr are starting to receive more attention.
</nextsent>
<nextsent>any effort aimed at studying these informal genres will require at least aminimal amount of labeled data for evaluation purposes.
</nextsent>
<nextsent>this work details how to efficiently annotate large volumes of data, for information extraction tasks, atlow cost using mturk (snow et al, 2008; <papid> D08-1027 </papid>callison burch, 2009).</nextsent>
<nextsent>this paper describes case study for information extraction tasks involving short, informal messages from twitter.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF781">
<title id=" W10-0713.xml">annotating named entities in twitter data with crowdsourcing </title>
<section> introduction and dataset description.  </section>
<citcontext>
<prevsection>
<prevsent>however, extremely informal domains, suchas face book, twitter, youtube or flickr are starting to receive more attention.
</prevsent>
<prevsent>any effort aimed at studying these informal genres will require at least aminimal amount of labeled data for evaluation purposes.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
this work details how to efficiently annotate large volumes of data, for information extraction tasks, atlow cost using mturk (snow et al, 2008; <papid> D08-1027 </papid>callison burch, 2009).</citsent>
<aftsection>
<nextsent>this paper describes case study for information extraction tasks involving short, informal messages from twitter.
</nextsent>
<nextsent>twitter is large multi user site for broadcasting short informal messages.
</nextsent>
<nextsent>twitter is an extreme example of an informal genre (java et al, 2007) as users frequently abbreviate their posts to fit within the specified limit.
</nextsent>
<nextsent>twitter is good choice because it is very popular: twitter users generate tremendous number of status updates (tweets) every day1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF782">
<title id=" W10-0708.xml">using amazon mechanical turk for transcription of nonnative speech </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, substantial savings in cost can be achieved.
</prevsent>
<prevsent>due to its ability to provide multiple sources of information forgiven task in cost-effectiveway, several recent studies have combined multiple mturk outputs for nlp annotation tasks.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
for example, one study involving annotation of emotions in text used average scores from up to 10 turkers to show the minimum number of mturk annotations required to achieve performance comparable to experts (snow et al, 2008).<papid> D08-1027 </papid></citsent>
<aftsection>
<nextsent>another study used preference voting to combine up to 5 mturk rankings of machine translation quality and showed that the resulting judgments approached expert inter annotator agreement (callison-burch, 2009).
</nextsent>
<nextsent>these 53 tasks, however, are much simpler than transcription.mturk has been used extensively as transcription provider, as is apparent from the success of middleman site that act as an interface to mturk for transcription tasks.1 however, to our knowledge, only one previous study has systematically evaluated the quality of mturk transcriptions (marge et al,to appear).
</nextsent>
<nextsent>this recent study also combined multiple mturk transcriptions using the rover method (fiscus, 1997) to produce merged transcriptions that approached the accuracy of expert transcribers.
</nextsent>
<nextsent>our study is similar to that study, except that the speech data used in our study is much more difficult totranscribethe utterances used in that study were relatively predictable (providing route instructions forrobots), and contained speech from native speakers and high-proficiency non-native speakers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF783">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>further,we show the potential for improved translation results with the inclusion of syntacticgrammar.
</prevsent>
<prevsent>we also introduce new syntax prioritized technique for combining syntactic and non-syntactic phrases that reduces overall phrase table size and decoding time by 61%,with only minimal drop in automatic translation metric scores.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
the dominance of traditional phrase-based statistical machine translation (pbsmt) models (koehn etal., 2003) <papid> N03-1017 </papid>has recently been challenged by the development and improvement of number of new models that explicity take into account the syntax of the sentences being translated.</citsent>
<aftsection>
<nextsent>one simple approach is to limit the phrases learned by standard pbsmt translation model to only those contiguous sequences of words that additionally correspond to constituents in syntactic parse tree.
</nextsent>
<nextsent>however, total reliance on such syntax-based phrases has been shown to be detrimental to translation quality, as the space of phrase segmentation of parallel sentence is heavily constrained by both the source-side and target-side tree structures.
</nextsent>
<nextsent>noting that the number of phrase pairs extracted from corpus is reduced by around 80% when they are required to correspond to syntactic constituents, koehn et al (2003) <papid> N03-1017 </papid>observed that many non-constituent phrase pairs that would not be included in syntax-only model are in fact extremely important to system performance.</nextsent>
<nextsent>since then, researchers have explored effective ways for combining phrase pairs derived from syntax-aware methods with those extracted from more traditional pbsmt.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF785">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>noting that the number of phrase pairs extracted from corpus is reduced by around 80% when they are required to correspond to syntactic constituents, koehn et al (2003) <papid> N03-1017 </papid>observed that many non-constituent phrase pairs that would not be included in syntax-only model are in fact extremely important to system performance.</prevsent>
<prevsent>since then, researchers have explored effective ways for combining phrase pairs derived from syntax-aware methods with those extracted from more traditional pbsmt.</prevsent>
</prevsection>
<citsent citstr=" W06-3119 ">
briefly stated, the goal is to retain the high level of coverage provided by non-syntactic pbsmtphrases while simultaneously incorporating and exploiting specific syntactic knowledge.zollmann and venugopal (2006) <papid> W06-3119 </papid>overcome there strictiveness of the syntax-only model by starting with complete set of phrases as produced by traditional pbsmt heuristics, then annotating the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span.</citsent>
<aftsection>
<nextsent>they then introduce new constituent labels to handle the cases where the phrasal entries do not exactly correspond to the syntactic constituents.
</nextsent>
<nextsent>liu et al (2006) <papid> P06-1077 </papid>also add non-syntactic pbsmt phrases into their tree-to-string translation system.</nextsent>
<nextsent>working from the other direction, marton and resnik (2008) <papid> P08-1114 </papid>extend hierarchical pbsmt 1system with number of features to prefer or dis prefer certain types of syntactic phrases in different contexts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF786">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>briefly stated, the goal is to retain the high level of coverage provided by non-syntactic pbsmtphrases while simultaneously incorporating and exploiting specific syntactic knowledge.zollmann and venugopal (2006) <papid> W06-3119 </papid>overcome there strictiveness of the syntax-only model by starting with complete set of phrases as produced by traditional pbsmt heuristics, then annotating the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span.</prevsent>
<prevsent>they then introduce new constituent labels to handle the cases where the phrasal entries do not exactly correspond to the syntactic constituents.</prevsent>
</prevsection>
<citsent citstr=" P06-1077 ">
liu et al (2006) <papid> P06-1077 </papid>also add non-syntactic pbsmt phrases into their tree-to-string translation system.</citsent>
<aftsection>
<nextsent>working from the other direction, marton and resnik (2008) <papid> P08-1114 </papid>extend hierarchical pbsmt 1system with number of features to prefer or dis prefer certain types of syntactic phrases in different contexts.</nextsent>
<nextsent>restructuring the parse trees to ease their restrictiveness is another recent approach: in particular, wang et al (2007) <papid> D07-1078 </papid>binarize source-side parse trees in order to provide phrase pair coverage for phrases that are partially syntactic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF787">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they then introduce new constituent labels to handle the cases where the phrasal entries do not exactly correspond to the syntactic constituents.
</prevsent>
<prevsent>liu et al (2006) <papid> P06-1077 </papid>also add non-syntactic pbsmt phrases into their tree-to-string translation system.</prevsent>
</prevsection>
<citsent citstr=" P08-1114 ">
working from the other direction, marton and resnik (2008) <papid> P08-1114 </papid>extend hierarchical pbsmt 1system with number of features to prefer or dis prefer certain types of syntactic phrases in different contexts.</citsent>
<aftsection>
<nextsent>restructuring the parse trees to ease their restrictiveness is another recent approach: in particular, wang et al (2007) <papid> D07-1078 </papid>binarize source-side parse trees in order to provide phrase pair coverage for phrases that are partially syntactic.</nextsent>
<nextsent>tinsley et al (2007) showed an improvement over pbsmt baseline on four tasks in bidirectional german english and spanish english translation by incorporating syntactic phrases derived from parallel trees into the pbsmt translation model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF788">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>liu et al (2006) <papid> P06-1077 </papid>also add non-syntactic pbsmt phrases into their tree-to-string translation system.</prevsent>
<prevsent>working from the other direction, marton and resnik (2008) <papid> P08-1114 </papid>extend hierarchical pbsmt 1system with number of features to prefer or dis prefer certain types of syntactic phrases in different contexts.</prevsent>
</prevsection>
<citsent citstr=" D07-1078 ">
restructuring the parse trees to ease their restrictiveness is another recent approach: in particular, wang et al (2007) <papid> D07-1078 </papid>binarize source-side parse trees in order to provide phrase pair coverage for phrases that are partially syntactic.</citsent>
<aftsection>
<nextsent>tinsley et al (2007) showed an improvement over pbsmt baseline on four tasks in bidirectional german english and spanish english translation by incorporating syntactic phrases derived from parallel trees into the pbsmt translation model.
</nextsent>
<nextsent>they first word align and extract phrases from parallel corpus using the open-source moses pbsmt toolkit (koehn et al, 2007), <papid> P07-2045 </papid>which provides baseline smt system.</nextsent>
<nextsent>then, both sides of the parallel corpus are parsed with independent automatic parsers, subtrees from the resulting parallel treebank are aligned, andan additional set of phrases (with each phrase corresponding to syntactic constituent in the parse tree)is extracted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF789">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>restructuring the parse trees to ease their restrictiveness is another recent approach: in particular, wang et al (2007) <papid> D07-1078 </papid>binarize source-side parse trees in order to provide phrase pair coverage for phrases that are partially syntactic.</prevsent>
<prevsent>tinsley et al (2007) showed an improvement over pbsmt baseline on four tasks in bidirectional german english and spanish english translation by incorporating syntactic phrases derived from parallel trees into the pbsmt translation model.</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
they first word align and extract phrases from parallel corpus using the open-source moses pbsmt toolkit (koehn et al, 2007), <papid> P07-2045 </papid>which provides baseline smt system.</citsent>
<aftsection>
<nextsent>then, both sides of the parallel corpus are parsed with independent automatic parsers, subtrees from the resulting parallel treebank are aligned, andan additional set of phrases (with each phrase corresponding to syntactic constituent in the parse tree)is extracted.
</nextsent>
<nextsent>the authors report statistically significant improvements in translation quality, as measured by variety of automatic metrics, when thetwo types of phrases are combined in the moses decoder.
</nextsent>
<nextsent>our approach in this paper is structurally similar to that of tinsley et al (2007), but we extend or modify it in number of key ways.
</nextsent>
<nextsent>first, we extract both non-syntactic pbsmt and syntax-driven phrases from parallel corpus that is two orders of magnitude larger, making our system competitive in size to state-of-the-art smt systems elsewhere.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF790">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our approach in this paper is structurally similar to that of tinsley et al (2007), but we extend or modify it in number of key ways.
</prevsent>
<prevsent>first, we extract both non-syntactic pbsmt and syntax-driven phrases from parallel corpus that is two orders of magnitude larger, making our system competitive in size to state-of-the-art smt systems elsewhere.
</prevsent>
</prevsection>
<citsent citstr=" W08-0411 ">
second, we apply different algorithm for subtree alignment, proposed by lavie et al (2008), <papid> W08-0411 </papid>which proceeds bottom-up from existing statistical word alignments, rather than inducing them top-downfrom lexical alignment probabilities.</citsent>
<aftsection>
<nextsent>third, in addition to straightforwardly combining syntax-derivedphrases with traditional pbsmt phrases, we demonstrate new combination technique that removes pbsmt phrases whose source-language strings are already covered by syntax-derived phrase.
</nextsent>
<nextsent>this new syntax-prioritized technique results in 61% reduction in the size of the combined phrase table with only minimal decrease in automatic translation metric scores.
</nextsent>
<nextsent>finally, and crucially, we carryout the joint decoding over both syntactic and non syntactic phrase pairs in syntax-aware mt system, which allows syntactic grammar to be put inplace on top of the phrase pairs to carry out linguistically motivated reordering, hierarchical decoding, and other operations.
</nextsent>
<nextsent>after this introduction, we first describe the basemt system we used, its formalism for specifying translation rules, and the method for extracting syntax-derived phrase pairs from parallel corpus (section 2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF792">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the ambiguity factor is the ratio of the number of unique entries to the number of unique source sides, or the average number of target-language alternatives per source phrase.
</prevsent>
<prevsent>pairs.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
statistical word alignments are learned in both directions with giza++ (och and ney, 2003), <papid> J03-1002 </papid>then combined with the grow-diag-final?</citsent>
<aftsection>
<nextsent>heuristic.
</nextsent>
<nextsent>forthe extraction of syntax-based phrase pairs, we obtain english-side constituency parses using the stanford parser (klein and manning, 2003), and french side constituency parses using the xerox xip parser (at-mokhtar et al, 2001).
</nextsent>
<nextsent>in phrase extraction, we concentrate on the expanded tree-to-tree-string scenario described in section 2.2, as it results in nearly 50% increase in the number of extracted phrase pairs over the tree-to-tree method.
</nextsent>
<nextsent>forde coding, we construct suffix-array language model(zhang and vogel, 2006) from corpus of 430 million words, including the english side of our training data, the english side of the hansard corpus, and newswire data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF793">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our test set is the 2000-sentence test2007?
</prevsent>
<prevsent>dataset, also released as part of the wmt workshopseries.
</prevsent>
</prevsection>
<citsent citstr=" W07-0734 ">
we report case-insensitive scores on version 0.6 of meteor (lavie and agarwal, 2007) <papid> W07-0734 </papid>with all modules enabled, version 1.04 of ibm-style bleu (papineni et al, 2002), <papid> P02-1040 </papid>and version 5 of ter (snover et al, 2006).</citsent>
<aftsection>
<nextsent>figure 1 gives an overall summary of our results on the test2007 data.
</nextsent>
<nextsent>overall, we train and test 10different configurations of phrase pairs in the stat xfer decoder.
</nextsent>
<nextsent>we begin by testing each type of phrase separately, producing one set of baseline systems with only phrase pairs that correspond to syntactic constituents (syntactic only?)
</nextsent>
<nextsent>and one baseline system with only phrase pairs that were extracted from moses (pbsmt only?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF794">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our test set is the 2000-sentence test2007?
</prevsent>
<prevsent>dataset, also released as part of the wmt workshopseries.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
we report case-insensitive scores on version 0.6 of meteor (lavie and agarwal, 2007) <papid> W07-0734 </papid>with all modules enabled, version 1.04 of ibm-style bleu (papineni et al, 2002), <papid> P02-1040 </papid>and version 5 of ter (snover et al, 2006).</citsent>
<aftsection>
<nextsent>figure 1 gives an overall summary of our results on the test2007 data.
</nextsent>
<nextsent>overall, we train and test 10different configurations of phrase pairs in the stat xfer decoder.
</nextsent>
<nextsent>we begin by testing each type of phrase separately, producing one set of baseline systems with only phrase pairs that correspond to syntactic constituents (syntactic only?)
</nextsent>
<nextsent>and one baseline system with only phrase pairs that were extracted from moses (pbsmt only?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF795">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>and one baseline system with only phrase pairs that were extracted from moses (pbsmt only?).
</prevsent>
<prevsent>we then test our two combination techniques, and their variants, as described in section 3.
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
statistical significance is tested on the bleu metric using paired bootstrap re sampling (koehn, 2004) <papid> W04-3250 </papid>with = 1000 and = 0.05.</citsent>
<aftsection>
<nextsent>in the figure, the best baseline system andthe configurations statistically equivalent to it are indicated in bold type.
</nextsent>
<nextsent>in addition to automatic metric scores, we also list the number of unique phrase pairs extracted for each configuration.
</nextsent>
<nextsent>(because of the large number of phrase pairs, we pre-filter them to only the set whose source sides appear in the test data; these numbers are the ones reported.)
</nextsent>
<nextsent>as an additional point of comparison, we build and tune moses mt system on the same data as our stat-xfer experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF797">
<title id=" W09-2301.xml">decoding with syntactic and non syntactic phrases in a syntax based machine translation system </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>however, this is trade-off that sacrifices some amount of recall.
</prevsent>
<prevsent>experimenting with different symmetric alignment heuristics may lead to amore optimal configuration for phrase-pair extraction or combination with pbsmt phrases.
</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
we also suspect that the choice of source- and target-side parsers plays significant role in the number and nature of phrase pairs we extract; to address this,we are in the process of re-trying our line of experiments using the berkeley parser (petrov and klein, 2007) <papid> N07-1051 </papid>for english, french, or both.</citsent>
<aftsection>
<nextsent>acknowledgments this research was supported in part by nsf grant iis-0534217 (letras) and the darpa gale program.
</nextsent>
<nextsent>we thank the members of the parsing and semantics group at xerox research center europe for parsing the french data with their xip parser.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF798">
<title id=" W10-0104.xml">domain adaptation meets active learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to the source domain (section 4).
</prevsent>
<prevsent>figure 1 shows our basic setup which uses source (or unsupervised domain-adapted source)classifier v0 as an initializer for doing active learning in the target domain having some small, fixed budget for querying labels.
</prevsent>
</prevsection>
<citsent citstr=" W06-1615 ">
our framework consists of 2 phases: 1) learning the best possible classi1for instance, either by simply training supervised classifier on the labeled source data, or by using unsupervised domain adaptation techniques (blitzer et al, 2006; <papid> W06-1615 </papid>sugiyama et al,2007) that use labeled data from the source domain, and additionally unlabeled data from the source and target domains.</citsent>
<aftsection>
<nextsent>27
</nextsent>
<nextsent>ls ut us domain adaptation v0 lt active learning oracle wt figure 1: block diagram of our basic approach.
</nextsent>
<nextsent>stage-1 can use any black-box unsupervised domain adaptation approach (e.g., (blitzer et al, 2006; <papid> W06-1615 </papid>sugiyama et al, 2007)) fier v0 using source labeled (ls) and unlabeled data(us), and target unlabeled (ut ) data, and 2) querying labels for target domain examples by leveraging information from the classifier learned in phase-1.</nextsent>
<nextsent>the active learning phase of our algorithm is based on (cesa-bianchi et al, 2006), henceforth referred to as cbgz.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF802">
<title id=" W10-0104.xml">domain adaptation meets active learning </title>
<section> using domain separator hypothesis.  </section>
<citcontext>
<prevsection>
<prevsent>to be similar to the source domain examples.
</prevsent>
<prevsent>as an illustration, fig.
</prevsent>
</prevsection>
<citsent citstr=" P07-1056 ">
2 shows typical distribution separator hypothesis (blitzer et al, 2007<papid> P07-1056 </papid>a) which separates the source and target examples.</citsent>
<aftsection>
<nextsent>if the source and target domains are reasonably different, then the separator hypothesis can perfectly distinguish between the examples drawn from these two domains.
</nextsent>
<nextsent>on the other hand, if the domains are similar, one would expected that there will be some overlap and therefore some of the target domain examples willlie on the source side (cf., fig.
</nextsent>
<nextsent>2).
</nextsent>
<nextsent>acquiring labels for such examples is not really needed since the initializing hypothesis v0 (cf., fig 1) of aodawould already have taken into account such examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF807">
<title id=" W10-0104.xml">domain adaptation meets active learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as mentioned earlier, however, any other unsupervised domain adaptation technique can be used here and, in general, aoda is expected to perform better than sial.
</prevsent>
<prevsent>active learning in domain adaptation setting has received little attention so far.
</prevsent>
</prevsection>
<citsent citstr=" P07-1007 ">
one interesting setting was proposed in (chan &amp; ng, 2007) <papid> P07-1007 </papid>where they apply active learning for word sense disambiguation in domain adaptation setting.</citsent>
<aftsection>
<nextsent>their active learning setting is pool-based whereas ours is streaming (online) setting.
</nextsent>
<nextsent>furthermore, our second algorithm also uses the domain separator hypothesis torule out querying the labels of target examples similar to the source.
</nextsent>
<nextsent>a combination of transfer learning with active learning has been presented in (shi et al,2008).
</nextsent>
<nextsent>one drawback of their approach is there quirement of an initial pool of labeled target domain data used to train an in-domain classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF808">
<title id=" W10-0216.xml">sentiment classification using automatically extracted subgraph features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a constant number of these features are added to the unigram feature space, adding much of the representational benefits without the computational cost of drastic increase in feature space size.
</prevsent>
<prevsent>in the remainder of the paper, we review prior work on features commonly used for sentiment analysis.
</prevsent>
</prevsection>
<citsent citstr=" N09-3010 ">
we then describe the annotation graph representation proposed by arora and nyberg (2009).<papid> N09-3010 </papid></citsent>
<aftsection>
<nextsent>following this, we describe the frequent subgraph mining algorithm proposed in yan and han (2002), and used in this work to extract frequent subgraphs from the annotation graphs.
</nextsent>
<nextsent>we then introduce our novel feature evolution approach, and discuss our experimental setup and results.
</nextsent>
<nextsent>subgraph features combined with the feature evolution approach gives promising results, with an improvement in performance over the baseline.
</nextsent>
<nextsent>some of the recent work in sentiment analysis has shown that structured features (features that capture syntactic patterns in text), such as n-grams, dependency relations, etc., improve performance beyond 131 the bag of words approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF809">
<title id=" W10-0216.xml">sentiment classification using automatically extracted subgraph features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>subgraph features combined with the feature evolution approach gives promising results, with an improvement in performance over the baseline.
</prevsent>
<prevsent>some of the recent work in sentiment analysis has shown that structured features (features that capture syntactic patterns in text), such as n-grams, dependency relations, etc., improve performance beyond 131 the bag of words approach.
</prevsent>
</prevsection>
<citsent citstr=" N09-2010 ">
arora et al (2009) <papid> N09-2010 </papid>show that deep syntactic scope features constructed from transitive closure of dependency relations give significant improvement for identifying types of claims in product reviews.</citsent>
<aftsection>
<nextsent>gamon (2004) <papid> C04-1121 </papid>found that using deep linguistic features derived from phrase structure trees and part of speech annotations yields significant improvements on the task of predicting satisfaction ratings in customer feedback data.</nextsent>
<nextsent>wil sonet al (2004) use syntactic clues derived from dependency parse tree as features for predicting the intensity of opinion phrases1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF810">
<title id=" W10-0216.xml">sentiment classification using automatically extracted subgraph features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some of the recent work in sentiment analysis has shown that structured features (features that capture syntactic patterns in text), such as n-grams, dependency relations, etc., improve performance beyond 131 the bag of words approach.
</prevsent>
<prevsent>arora et al (2009) <papid> N09-2010 </papid>show that deep syntactic scope features constructed from transitive closure of dependency relations give significant improvement for identifying types of claims in product reviews.</prevsent>
</prevsection>
<citsent citstr=" C04-1121 ">
gamon (2004) <papid> C04-1121 </papid>found that using deep linguistic features derived from phrase structure trees and part of speech annotations yields significant improvements on the task of predicting satisfaction ratings in customer feedback data.</citsent>
<aftsection>
<nextsent>wil sonet al (2004) use syntactic clues derived from dependency parse tree as features for predicting the intensity of opinion phrases1.
</nextsent>
<nextsent>structured features that capture linguistic patterns are often handcrafted by domain experts (wilson et al, 2005) <papid> H05-1044 </papid>after careful examination of the data.</nextsent>
<nextsent>thus, they do not always generalize well across datasets and domains.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF811">
<title id=" W10-0216.xml">sentiment classification using automatically extracted subgraph features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>gamon (2004) <papid> C04-1121 </papid>found that using deep linguistic features derived from phrase structure trees and part of speech annotations yields significant improvements on the task of predicting satisfaction ratings in customer feedback data.</prevsent>
<prevsent>wil sonet al (2004) use syntactic clues derived from dependency parse tree as features for predicting the intensity of opinion phrases1.</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
structured features that capture linguistic patterns are often handcrafted by domain experts (wilson et al, 2005) <papid> H05-1044 </papid>after careful examination of the data.</citsent>
<aftsection>
<nextsent>thus, they do not always generalize well across datasets and domains.
</nextsent>
<nextsent>this also requires significant amount of time and resources.
</nextsent>
<nextsent>by automatically deriving structured features, we might be able to learn new annotations faster.
</nextsent>
<nextsent>matsumoto et al (2005) propose an approach that uses frequent sub-sequence and sub-tree mining approaches (asai et al, 2002; pei et al, 2004) to derive structured features such as word sub-sequences and dependency sub-trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF816">
<title id=" W10-0216.xml">sentiment classification using automatically extracted subgraph features </title>
<section> annotation graph representation and.  </section>
<citcontext>
<prevsection>
<prevsent>both have positive prior polarity, however, the phrase expresses negative sentiment towards the movie.
</prevsent>
<prevsent>heuristics for special handling of negation have been proposed inthe literature.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
for example, pang et al (2002) <papid> W02-1011 </papid>append every word following negation, until punctuation, with not?</citsent>
<aftsection>
<nextsent>applying similar technique to our example gives us two sentiment bearing features, one positive (interesting?)
</nextsent>
<nextsent>and one negative (not-compelling?), and the model may not be as sure about the predicted label, since there is both 132 positive and negative sentiment present.in figure 2, we show three discriminating subgraph features derived from the annotation graph in figure 1.
</nextsent>
<nextsent>these subgraph features capture the negative sentiment in our example phrase.
</nextsent>
<nextsent>the first feature in 2(a) captures the pattern using dependency relations between words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF819">
<title id=" W10-0216.xml">sentiment classification using automatically extracted subgraph features </title>
<section> feature construction using genetic.  </section>
<citcontext>
<prevsection>
<prevsent>programminga challenge to overcome when adding expressiveness to the feature space for any text classification problem is the rapid increase in the feature space size.
</prevsent>
<prevsent>among this large set of new features, most are not predictive or are very weak predictors, and only few carry novel information that improves classification performance.
</prevsent>
</prevsection>
<citsent citstr=" W06-1652 ">
because of this, adding more complex features often gives no improvement or even worsens performance as the feature spaces signal is drowned out by noise.riloff et al (2006) <papid> W06-1652 </papid>propose feature subsump tion approach to address this issue.</citsent>
<aftsection>
<nextsent>they define hierarchy for features based on the information they represent.
</nextsent>
<nextsent>a complex feature is only added if its discriminative power is delta above the discriminative power of all its simpler forms.
</nextsent>
<nextsent>in this work, we use genetic programming (koza, 1992) based approach which evaluates interactions between fea 3http://www.cs.ucsb.edu/xyan/software/ gspan.htm, http://www.kyb.mpg.de/bs/people/ nowozin/gboost/ tures and evolves complex features from them.
</nextsent>
<nextsent>the advantage of the genetic programing based approach over feature subsumption is that it allows us to evaluate feature using multiple criteria.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF820">
<title id=" W10-0216.xml">sentiment classification using automatically extracted subgraph features </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluate our approach on sentiment classification task, where the goal is to classify movie review sentence as expressing positive or negative sentiment towards the movie.
</prevsent>
<prevsent>6.1 data and experimental setup.
</prevsent>
</prevsection>
<citsent citstr=" P05-1015 ">
data: the dataset consists of snippets from rotten tomatoes (pang and lee, 2005) <papid> P05-1015 </papid>6.</citsent>
<aftsection>
<nextsent>it consists of 10662 snippets/sentences total with equal number positive and negative sentences (5331 each).
</nextsent>
<nextsent>this dataset was created and used by pang and lee (2005) <papid> P05-1015 </papid>to train classifier for identifying positive sentences in full length review.</nextsent>
<nextsent>we use the first 8000 (4000 positive, 4000 negative) sentences as training data and evaluate on remaining 2662 (1331 positive, 1331 negative) sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF822">
<title id=" W10-0216.xml">sentiment classification using automatically extracted subgraph features </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>this dataset was created and used by pang and lee (2005) <papid> P05-1015 </papid>to train classifier for identifying positive sentences in full length review.</prevsent>
<prevsent>we use the first 8000 (4000 positive, 4000 negative) sentences as training data and evaluate on remaining 2662 (1331 positive, 1331 negative) sentences.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we added part of speech and dependency triple annotations to this data using the stanford parser (klein and manning, 2003).<papid> P03-1054 </papid>annotation graph: for the annotation graph representation, we used unigrams (u), part of speech (p) and dependency relation type (d) as labels forthe nodes, and parentofgov and parentofdep as labels for the edges.</citsent>
<aftsection>
<nextsent>for dependency triple such asamod good movie?, five nodes are added to the annotation graph as shown in figure 4(a).
</nextsent>
<nextsent>parent of gov and parentofdep edges are added from the 6http://www.cs.cornell.edu/people/pabo/ movie-review-data/rt-polaritydata.tar.gz d_amod u_good p_jj p_nn u_movie parentofgov parentofgovparentofdep parentofdep (a) d_amod u_good p_nn parentofgov parentofdep (b) d_amod p_jj p_nn posq parentofgov parentofdep posq (c) figure 4: annotation graph and feature subgraph for dependency triple annotation amod good camera?.
</nextsent>
<nextsent>(c) shows an alternative representation with wild cards dependency relation node amod to the unigram nodes good and movie.
</nextsent>
<nextsent>these edges are also added for the part of speech nodes that correspond to the two unigrams in the dependency relation, as shown in figure 4(a).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF824">
<title id=" W10-0216.xml">sentiment classification using automatically extracted subgraph features </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>results for both settings are reported.
</prevsent>
<prevsent>baselines: to the best of our knowledge, there is no supervised machine learning result published onthis dataset.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
we compare our results with the following baselines: ? unigram-only baseline: in sentiment analysis,unigram-only features have been strong base line (pang et al, 2002; <papid> W02-1011 </papid>pang and lee, 2004).<papid> P04-1035 </papid></citsent>
<aftsection>
<nextsent>we only use unigrams that occur in at least two sentences of the training data same as matsumoto et al (2005).
</nextsent>
<nextsent>we also filter out stop words using small stop word list9.?
</nextsent>
<nextsent>2 baseline: for our training data, after filtering infrequent unigrams and stop words, we get 7http://svmlight.joachims.org/ 8full movie review data by pang et al (2002) <papid> W02-1011 </papid>9http://nlp.stanford.edu/ ir-book/html/htmledition/ dropping-common-terms-stop-words-1.html (with one modification: removed will?, added this?)8424 features.</nextsent>
<nextsent>adding subgraph features increases the total number of features to 44, 161,a factor of 5 increase in size.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF829">
<title id=" W09-2508.xml">using hypernymy acquisition to tackle part of textual entailment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>hypothesis: kyodo newsagency is based in japan.the entailment pairs share set of similar features: they have very high word overlap regardless of being true or false ent ailments, forex ample.
</prevsent>
<prevsent>high word overlap is one of the features for an rte system for the majority of the entailment pair types, which presumably hints at true, but this is not useful in our case.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
akhmatova anddras (2007) described two-fold probabilistic approach to recognizing entailment, that in its turn was based on the well-known noisy channel model from statistical machine translation (brown et al., 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>in the work of this paper, by contrast, we look at only identifying hypernymy-relatedtext, so the problem reduces to one of classification over the text.
</nextsent>
<nextsent>2.2 hypernymy extraction.
</nextsent>
<nextsent>the aim of work on hypernymy extraction is usually the enrichment of lexical resource such as wordnet, or creation of specific hierarchical lexical data directly for the purpose of some application, such as information extraction or question answering.
</nextsent>
<nextsent>there can be found several approaches to the task of hypernymy extraction: cooccurrence approaches, asymmetric association measures, and pattern-based methods.cooccurence approaches co-occurrence approaches first cluster words into similarity classe sand consider the elements of class to be siblings of one parent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF830">
<title id=" W09-2508.xml">using hypernymy acquisition to tackle part of textual entailment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there can be found several approaches to the task of hypernymy extraction: cooccurrence approaches, asymmetric association measures, and pattern-based methods.cooccurence approaches co-occurrence approaches first cluster words into similarity classe sand consider the elements of class to be siblings of one parent.
</prevsent>
<prevsent>therefore the search for parent for some members from the class gives parent for the other members of the class.
</prevsent>
</prevsection>
<citsent citstr=" P99-1016 ">
the first work that introduced co-occurrence methods to the field is that of caraballo (1999).<papid> P99-1016 </papid></citsent>
<aftsection>
<nextsent>first she clusters nouns into groups based on conjunctive and appositive data collected from the wall streetjournal.
</nextsent>
<nextsent>nouns are grouped according to the similarity of being seen with other nouns in conjunc tive and appositive relationships.
</nextsent>
<nextsent>in the second stage, using some knowledge about which con juncts connect hypernyms reliably, parent for agroup of nouns is searched for in the same text corpora.
</nextsent>
<nextsent>other co-occurrence methods can be found in works by pantel et al (2004) <papid> C04-1111 </papid>and pantel and ravichandran (2004).<papid> N04-1041 </papid>asymmetric association measures in asymmetric association (see dias et al (2008)) hy pernymy is derived through the measure of howmuch one word attracts?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF831">
<title id=" W09-2508.xml">using hypernymy acquisition to tackle part of textual entailment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>nouns are grouped according to the similarity of being seen with other nouns in conjunc tive and appositive relationships.
</prevsent>
<prevsent>in the second stage, using some knowledge about which con juncts connect hypernyms reliably, parent for agroup of nouns is searched for in the same text corpora.
</prevsent>
</prevsection>
<citsent citstr=" C04-1111 ">
other co-occurrence methods can be found in works by pantel et al (2004) <papid> C04-1111 </papid>and pantel and ravichandran (2004).<papid> N04-1041 </papid>asymmetric association measures in asymmetric association (see dias et al (2008)) hy pernymy is derived through the measure of howmuch one word attracts?</citsent>
<aftsection>
<nextsent>another one.
</nextsent>
<nextsent>when hearing fruit?, more common fruits will be likely to come into mind such as apple?
</nextsent>
<nextsent>or banana?.
</nextsent>
<nextsent>inthis case, there exists an oriented association between fruit?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF832">
<title id=" W09-2508.xml">using hypernymy acquisition to tackle part of textual entailment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>nouns are grouped according to the similarity of being seen with other nouns in conjunc tive and appositive relationships.
</prevsent>
<prevsent>in the second stage, using some knowledge about which con juncts connect hypernyms reliably, parent for agroup of nouns is searched for in the same text corpora.
</prevsent>
</prevsection>
<citsent citstr=" N04-1041 ">
other co-occurrence methods can be found in works by pantel et al (2004) <papid> C04-1111 </papid>and pantel and ravichandran (2004).<papid> N04-1041 </papid>asymmetric association measures in asymmetric association (see dias et al (2008)) hy pernymy is derived through the measure of howmuch one word attracts?</citsent>
<aftsection>
<nextsent>another one.
</nextsent>
<nextsent>when hearing fruit?, more common fruits will be likely to come into mind such as apple?
</nextsent>
<nextsent>or banana?.
</nextsent>
<nextsent>inthis case, there exists an oriented association between fruit?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF833">
<title id=" W09-2508.xml">using hypernymy acquisition to tackle part of textual entailment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>generally, some amount of manual work on finding the seed patterns isdone first.
</prevsent>
<prevsent>automated algorithms use these patterns for discovering more patterns and for the subsequent hypernymy extraction.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
the fundamental work for the pattern-based approaches is that of hearst (1992).<papid> C92-2082 </papid></citsent>
<aftsection>
<nextsent>more recently, snow et al (2005) and snow et al (2006) <papid> P06-1101 </papid>have described method of hypernymy extraction using machine learning of 53 patterns.</nextsent>
<nextsent>pattern-based methods are known to be successfully used for the creation of hierarchical data for other languages as well, such as dutch; for example, see tjong kim sang and hofmann (2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF835">
<title id=" W09-2508.xml">using hypernymy acquisition to tackle part of textual entailment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>automated algorithms use these patterns for discovering more patterns and for the subsequent hypernymy extraction.
</prevsent>
<prevsent>the fundamental work for the pattern-based approaches is that of hearst (1992).<papid> C92-2082 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1101 ">
more recently, snow et al (2005) and snow et al (2006) <papid> P06-1101 </papid>have described method of hypernymy extraction using machine learning of 53 patterns.</citsent>
<aftsection>
<nextsent>pattern-based methods are known to be successfully used for the creation of hierarchical data for other languages as well, such as dutch; for example, see tjong kim sang and hofmann (2007).
</nextsent>
<nextsent>for our purposes, pattern-based methods are particularly suitable, as we have as context two words and single pattern connecting them; we thus describe these approaches in more detail.
</nextsent>
<nextsent>in her early work on pattern-based hypernymy extraction hearst (1992) <papid> C92-2082 </papid>noticed that particular semantic relationship between two nouns in the sentence can be indicated by the presence of certain lexico-syntactic patterns linking those nouns.</nextsent>
<nextsent>hypernymy (is-a, is kind of relation) is one such relationship.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF849">
<title id=" W09-2406.xml">largescale semantic networks annotation and evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the nodes of the multi net graph are connected based on corpus-wide interpretation of the entities referred to in the corpus.
</prevsent>
<prevsent>these global connections are determined by the intra-sententialinterpretation but are not restricted to that interpretation.
</prevsent>
</prevsection>
<citsent citstr=" L08-1391 ">
therefore, the procedure for computing annotator agreement differs from the standard approaches to evaluating syntactic and semantic dependency treebanks (e.g., dependency link agreement, label agreement, predicate-argument structure agreement).as noted in (bos, 2008), <papid> L08-1391 </papid>even though the design of annotation schemes has been initiated for single semantic phenomena, there exists no annotation scheme (as far as know) that aims to inte 37 grate wide range of semantic phenomena all at once.</citsent>
<aftsection>
<nextsent>it would be welcome to have such resource at ones disposal, and ideally semantic annotation scheme should be multi-layered, where certain semantic phenomena can be properly analysed or left simply unanalysed.in section 1 we introduce the theoretical back ground of the frameworks on which our annotation tool is based: multi net and the tectogrammatical representation (tr) of the pdt.
</nextsent>
<nextsent>section 2 describes the annotation process in detail, including an introduction to the encyclopedic tools available to the annotators.
</nextsent>
<nextsent>in section 3 we present an evaluation metric for multinet/tr labeled data.
</nextsent>
<nextsent>we also present an evaluation of the data we have had annotated using the proposed procedure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF850">
<title id=" W09-2406.xml">largescale semantic networks annotation and evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to bridge the gap between different language swe employ the deep syntactico-semantic representation available in the functional generative description framework (sgall et al, 1986).
</prevsent>
<prevsent>1.2 prague dependency treebank.
</prevsent>
</prevsection>
<citsent citstr=" W04-2706 ">
the prague dependency treebank (pdt) presents alan guage resource containing deep manual analysis of texts(sgall et al, 2004).<papid> W04-2706 </papid></citsent>
<aftsection>
<nextsent>the pdt contains annotations on three layers: morphological rich morphological annotation is provided when such information is available in the language.
</nextsent>
<nextsent>this includes lemmatization and detailed morphological tagging.
</nextsent>
<nextsent>analytical the analytical layer is dependency analysis based purely on the syntactic inter pre tation.tectogrammatical the tectogrammatical annotation provides deep-syntactic (syntacticosemantic) analysis of the text.
</nextsent>
<nextsent>the formalism abstracts away from word-order, function words (syn-semantic words), and morphological variation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF851">
<title id=" W09-2406.xml">largescale semantic networks annotation and evaluation </title>
<section> network evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the entries of individual annotators in the wiki are logged and feed of changes can be observed using an rss reader.
</prevsent>
<prevsent>thecedit annotation tool allows users to display appropriate wiki pages of individual relation types, function types and attributes directly from the tool using their preferred web browser.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
we present an evaluation which has been carried out on an initial set of annotations of english articles from the wall street journal (covering those annotated at the syntactic level in the penn tree bank (marcus et al, 1993)).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>we use the annotation from the prague czech-english dependency treebank (curn et al, 2004), which contains large portion of the wsj treebank annotated according to the pdt annotation scheme (including all layers of the fgd formalism).
</nextsent>
<nextsent>we reserved small set of data to be used to train our annotators and have excluded these articles fromthe evaluation.
</nextsent>
<nextsent>three native english-speaking annotators were trained and then asked to annotate sentences from the corpus.
</nextsent>
<nextsent>we have sample of 67sentences (1793 words) annotated by two of the an notators; of those, 46 sentences (1236 words) were annotated by three annotators.2 agreement is measured for each individual sentences in two steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF852">
<title id=" W10-0503.xml">detecting word misuse in chinese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are 5000 commonly used characters.
</prevsent>
<prevsent>while the number of distinct pinyin (toneless) is only 412.
</prevsent>
</prevsection>
<citsent citstr=" P00-1031 ">
therefore pinyin to character conversion is highly ambigurous and is active research topic (zhou et al, 2007), (lin and zhang, 2008), (chen and lee, 2000).<papid> P00-1031 </papid></citsent>
<aftsection>
<nextsent>on the other hand, automatic pinyin generation is considered solved task, (liu and guthrie, 2009) shows that using the most frequent pinyin approach to assign pinyin to each character can achieve 98% accuracy.
</nextsent>
<nextsent>in fact, we test on the gigaword chinese (verson 2) corpus and find out that only about 15% of the characters have ambigurous pinyin.
</nextsent>
<nextsent>we divided the detection process into three steps as below: ? segmentation: given piece of chinese text,we first feed it into an automatic word seg menter (zhang et al, 2003) <papid> W03-1709 </papid>to break the text into semantic units.</nextsent>
<nextsent>because we consider only multiple-character anomaly cases, anomalies can only be contained within sequences of single characters.?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF853">
<title id=" W10-0503.xml">detecting word misuse in chinese </title>
<section> automatically detecting word misuse.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, automatic pinyin generation is considered solved task, (liu and guthrie, 2009) shows that using the most frequent pinyin approach to assign pinyin to each character can achieve 98% accuracy.
</prevsent>
<prevsent>in fact, we test on the gigaword chinese (verson 2) corpus and find out that only about 15% of the characters have ambigurous pinyin.
</prevsent>
</prevsection>
<citsent citstr=" W03-1709 ">
we divided the detection process into three steps as below: ? segmentation: given piece of chinese text,we first feed it into an automatic word seg menter (zhang et al, 2003) <papid> W03-1709 </papid>to break the text into semantic units.</citsent>
<aftsection>
<nextsent>because we consider only multiple-character anomaly cases, anomalies can only be contained within sequences of single characters.?
</nextsent>
<nextsent>character sequence extraction: after segmentation, we are interested in sequences of single characters, because anomalies will occur only within those sequences.
</nextsent>
<nextsent>once we obtain these sequences, we generate all possible sub strings for each sequence because any anomalous words can be part of character sequence.
</nextsent>
<nextsent>detection: we assume the anomaly shares many phonetic similarities with the true?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF854">
<title id=" W09-1901.xml">active learning for anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have adopted an uncertainty-based active learning approach in order to do that, and it uses our probabilistic model as the base classifier.
</prevsent>
<prevsent>the uncertainty-based approach has been applied to, for instance, named-entity recognition by shen etal.
</prevsent>
</prevsection>
<citsent citstr=" P02-1016 ">
(2004) who report at least 80% reduction in annotation costs, parsing by tang et al  (2002) <papid> P02-1016 </papid>whore ports 67% savings, and parse selection by baldridge and osborne (2003) <papid> W03-0403 </papid>who report 60% savings.</citsent>
<aftsection>
<nextsent>we are not aware of any work that has applied active learning to anaphora resolution.for calculating the uncertainty of an anaphora resolution model, we feel the need to combine the information about the confindence of the model forthe classification of each antecedent candidate associated to given anaphor.
</nextsent>
<nextsent>we have tested threeentropy-based uncertainty measures in order to select the instances to be added to the training data.our training corpus is composed of five full length scientific articles from the biomedical do main.
</nextsent>
<nextsent>we have used this corpus to simulate active learning: we have divided our training data into two parts, one for the initial training and the other for active learning (simulating un labelled data), and have compared the classifier performance when trained on sample selected by active learning to its performance when trained on the same amount of randomly selected instances.
</nextsent>
<nextsent>in the next section we describe our probabilistic model for anaphora resolution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF855">
<title id=" W09-1901.xml">active learning for anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have adopted an uncertainty-based active learning approach in order to do that, and it uses our probabilistic model as the base classifier.
</prevsent>
<prevsent>the uncertainty-based approach has been applied to, for instance, named-entity recognition by shen etal.
</prevsent>
</prevsection>
<citsent citstr=" W03-0403 ">
(2004) who report at least 80% reduction in annotation costs, parsing by tang et al  (2002) <papid> P02-1016 </papid>whore ports 67% savings, and parse selection by baldridge and osborne (2003) <papid> W03-0403 </papid>who report 60% savings.</citsent>
<aftsection>
<nextsent>we are not aware of any work that has applied active learning to anaphora resolution.for calculating the uncertainty of an anaphora resolution model, we feel the need to combine the information about the confindence of the model forthe classification of each antecedent candidate associated to given anaphor.
</nextsent>
<nextsent>we have tested threeentropy-based uncertainty measures in order to select the instances to be added to the training data.our training corpus is composed of five full length scientific articles from the biomedical do main.
</nextsent>
<nextsent>we have used this corpus to simulate active learning: we have divided our training data into two parts, one for the initial training and the other for active learning (simulating un labelled data), and have compared the classifier performance when trained on sample selected by active learning to its performance when trained on the same amount of randomly selected instances.
</nextsent>
<nextsent>in the next section we describe our probabilistic model for anaphora resolution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF856">
<title id=" W09-1901.xml">active learning for anaphora resolution </title>
<section> anaphora resolution model.  </section>
<citcontext>
<prevsection>
<prevsent>in section ??
</prevsent>
<prevsent>we describe the strategy we have adopted to select the samples to take part in the active learning, and in section 5 1 we describe our experiments.
</prevsent>
</prevsection>
<citsent citstr=" C08-1033 ">
we have inplemented probabilistic model for anaphora resolution in the biomedical domain (gasperin and briscoe, 2008).<papid> C08-1033 </papid></citsent>
<aftsection>
<nextsent>this model aims to resolve both co referent and associative (also called bridging (poesio and vieira, 1998)) <papid> J98-2001 </papid>cases of non pronominal anaphora.</nextsent>
<nextsent>table 1 shows examples of these types of anaphoric relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF858">
<title id=" W09-1901.xml">active learning for anaphora resolution </title>
<section> anaphora resolution model.  </section>
<citcontext>
<prevsection>
<prevsent>we describe the strategy we have adopted to select the samples to take part in the active learning, and in section 5 1 we describe our experiments.
</prevsent>
<prevsent>we have inplemented probabilistic model for anaphora resolution in the biomedical domain (gasperin and briscoe, 2008).<papid> C08-1033 </papid></prevsent>
</prevsection>
<citsent citstr=" J98-2001 ">
this model aims to resolve both co referent and associative (also called bridging (poesio and vieira, 1998)) <papid> J98-2001 </papid>cases of non pronominal anaphora.</citsent>
<aftsection>
<nextsent>table 1 shows examples of these types of anaphoric relations.
</nextsent>
<nextsent>co referent are the cases in which the anaphor and the antecedent refer to the same entity in the world, while associative cases are the ones in which the anaphor and antecedent refer to different but somehow related entities.
</nextsent>
<nextsent>we only take into account noun phrases referring to biomedical entities, since this was the focus of our resolution model.
</nextsent>
<nextsent>we consider two types of associative relations: biotype relations, which are anaphoric associative relations between noun phrases that share specific onto logical relations in the biomedical domain; and set-member relations, in which the noun phrases share set-membership relation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF859">
<title id=" W09-1901.xml">active learning for anaphora resolution </title>
<section> anaphora resolution model.  </section>
<citcontext>
<prevsection>
<prevsent>we consider two types of associative relations: biotype relations, which are anaphoric associative relations between noun phrases that share specific onto logical relations in the biomedical domain; and set-member relations, in which the noun phrases share set-membership relation.
</prevsent>
<prevsent>it is frequent however that some noun phrases do not have an antecedent, these are considered discourse-new cases, which we also aim to identify.the probabilistic model results from simple decomposition process applied to conditional probability equation that involves several parameters (features).
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
it is inspired by ge et al (1998) <papid> W98-1119 </papid>probabilistic model for pronoun resolution.</citsent>
<aftsection>
<nextsent>the decomposition makes use of bayes?
</nextsent>
<nextsent>theorem and independence assumptions, and aims to decrease the impact of data sparseness on the model, so that even small training corpora can be viable.
</nextsent>
<nextsent>the decomposed model can be thought of as more sophisticated version of the naive-bayes algorithm, since we consider the dependence among some of the features instead of full independence as in naive bayes.
</nextsent>
<nextsent>probabilistic models can return confidence measure (probabil ity) for each decision they make, which allow us to adopt techniques such as active learning for further processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF862">
<title id=" W09-1901.xml">active learning for anaphora resolution </title>
<section> our corpus.  </section>
<citcontext>
<prevsection>
<prevsent>we collect the statistics to train this model from corpus annotated with anaphoric relations that we have created.
</prevsent>
<prevsent>the corpus is described in the next section.
</prevsent>
</prevsection>
<citsent citstr=" W05-1306 ">
there are very few biomedical corpora annotated with anaphora information, and all of them are built from paper abstracts (cohen et al , 2005), <papid> W05-1306 </papid>instead offull papers.</citsent>
<aftsection>
<nextsent>as anaphora is phenomenon that develops through text, we believe that short abstracts are not the best source to work with and decided to concentrate on full papers.
</nextsent>
<nextsent>in order to collect the statistics to train our model, we have manually annotated anaphoric relations between biomedical entities in 5 full-text articles (approx.
</nextsent>
<nextsent>33,300 words)1, which are part of the drosophila molecular biology literature.
</nextsent>
<nextsent>the corpus and annotation process are described in (gasperin et al ., 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF863">
<title id=" W09-1901.xml">active learning for anaphora resolution </title>
<section> our corpus.  </section>
<citcontext>
<prevsection>
<prevsent>to tag all gene names in the corpus, we have applied the gene name recogniser developed by vlachos et al .
</prevsent>
<prevsent>(2006).
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
to identify all nps, their sub constituents (head, modifiers, determiner) and broader pre- and post-modification patterns, we have used the raspparser (briscoe et al , 2006).<papid> P06-4020 </papid></citsent>
<aftsection>
<nextsent>to classify the nps according to their type in biomedical terms, we have adopted the sequence ontology (so)2 (eilbeck and lewis, 2004).
</nextsent>
<nextsent>so is fine-grained ontology, which contains the names of practically all entities that participate in genomic sequences, besides the relations among these entities (e.g. is-a, part-of, derived-from relations).
</nextsent>
<nextsent>we derived from so seven bio types to be used to classify the entities in the text, namely: gene?, gene product?, part of gene?, part of product?, gene variant?, gene subtype?, and gene 2http://www.sequenceontology.org/ 3 class relations co referent 1678 biotype 274 set-member 543 discourse new 436 total 3048 none 873,731 table 3: training instances, according to anaphoric class supertype?.
</nextsent>
<nextsent>we also created the biotype other-bio?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF864">
<title id=" W09-1901.xml">active learning for anaphora resolution </title>
<section> our corpus.  </section>
<citcontext>
<prevsection>
<prevsent>for example, our biggest cluster (feature values are: fa=pn?, fa=pn?, hm=no?, hmm=no?, mm=no?, bm=yes?, gp=yes?, num=yes?, sr=none?, d=16 ?, dm=50 ?)
</prevsent>
<prevsent>with 33,998 instances is reduced to 3,399 ? still considerably more numerous than any positive sample.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
other works have used different strategy to reduce the imbalance between positive and negative samples (soon et al , 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>strube et al , 2002), <papid> W02-1040 </papid>where only samples composed by negative antecedent that is closer than the annotated one are considered.</citsent>
<aftsection>
<nextsent>our strategy is more flexible and is able to the reduce further the number of negative samples.
</nextsent>
<nextsent>the higher the number of negative samples, the higher the precision of the resolution, but the lower the recall.
</nextsent>
<nextsent>when trained using all our annotated corpus on 10 fold cross-validation setting our anaphora resolution model, presented above, reached the results shown in table 43.we would like to improve this results without having to annotate too much more data, therefore we decided to experiment with active learning.
</nextsent>
<nextsent>we defined three entropy-based measures to calculate the uncertanty of our model for each decidion is makes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF865">
<title id=" W09-1901.xml">active learning for anaphora resolution </title>
<section> our corpus.  </section>
<citcontext>
<prevsection>
<prevsent>for example, our biggest cluster (feature values are: fa=pn?, fa=pn?, hm=no?, hmm=no?, mm=no?, bm=yes?, gp=yes?, num=yes?, sr=none?, d=16 ?, dm=50 ?)
</prevsent>
<prevsent>with 33,998 instances is reduced to 3,399 ? still considerably more numerous than any positive sample.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
other works have used different strategy to reduce the imbalance between positive and negative samples (soon et al , 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>strube et al , 2002), <papid> W02-1040 </papid>where only samples composed by negative antecedent that is closer than the annotated one are considered.</citsent>
<aftsection>
<nextsent>our strategy is more flexible and is able to the reduce further the number of negative samples.
</nextsent>
<nextsent>the higher the number of negative samples, the higher the precision of the resolution, but the lower the recall.
</nextsent>
<nextsent>when trained using all our annotated corpus on 10 fold cross-validation setting our anaphora resolution model, presented above, reached the results shown in table 43.we would like to improve this results without having to annotate too much more data, therefore we decided to experiment with active learning.
</nextsent>
<nextsent>we defined three entropy-based measures to calculate the uncertanty of our model for each decidion is makes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF866">
<title id=" W09-1901.xml">active learning for anaphora resolution </title>
<section> our corpus.  </section>
<citcontext>
<prevsection>
<prevsent>for example, our biggest cluster (feature values are: fa=pn?, fa=pn?, hm=no?, hmm=no?, mm=no?, bm=yes?, gp=yes?, num=yes?, sr=none?, d=16 ?, dm=50 ?)
</prevsent>
<prevsent>with 33,998 instances is reduced to 3,399 ? still considerably more numerous than any positive sample.
</prevsent>
</prevsection>
<citsent citstr=" W02-1040 ">
other works have used different strategy to reduce the imbalance between positive and negative samples (soon et al , 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>strube et al , 2002), <papid> W02-1040 </papid>where only samples composed by negative antecedent that is closer than the annotated one are considered.</citsent>
<aftsection>
<nextsent>our strategy is more flexible and is able to the reduce further the number of negative samples.
</nextsent>
<nextsent>the higher the number of negative samples, the higher the precision of the resolution, but the lower the recall.
</nextsent>
<nextsent>when trained using all our annotated corpus on 10 fold cross-validation setting our anaphora resolution model, presented above, reached the results shown in table 43.we would like to improve this results without having to annotate too much more data, therefore we decided to experiment with active learning.
</nextsent>
<nextsent>we defined three entropy-based measures to calculate the uncertanty of our model for each decidion is makes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF867">
<title id=" W09-2105.xml">supporting the adaptation of texts for poor literacy readers a text simplification editor for brazilian portuguese </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our system, on the other hand, is autonomous, even though the user is able to undo any undesirable simplification or to choose alternative simplifications.
</prevsent>
<prevsent>these alternative simplifications may be produced in two cases: i) to compose new subject in simplifications involving relatives and appositions and ii) to choose among one of the coordinate or subordinate simplifications when there is ambiguity regarding to conjunctions.
</prevsent>
</prevsection>
<citsent citstr=" W03-1602 ">
inui et al (2003) <papid> W03-1602 </papid>proposes rule-based system for text simplification aimed at deaf people.</citsent>
<aftsection>
<nextsent>the authors create readability assessments based on questionnaires answered by teachers about the deaf.
</nextsent>
<nextsent>with approximately one thousand manually created rules, the authors generate several paraphrases for each sentence and train classifier to select the simpler ones.
</nextsent>
<nextsent>promising results are obtained, although different types of errors on the paraphrase generation are encountered, such as problems with verb conjugation and regency.
</nextsent>
<nextsent>in our work we produce alternative simplifications only in the two cases explained above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF868">
<title id=" W10-0101.xml">using variance as a stopping criterion for active learning of frame assignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>active learning is supervised machine learning method in which informative instances are chosen by the classifier for labeling.
</prevsent>
<prevsent>unlike the normal supervised set-up where data annotation and learning are completely independent, active learning is sequential process (settles, 2009; busser and morante, 2005).
</prevsent>
</prevsection>
<citsent citstr=" P09-1021 ">
this learning method is used in variety ofnlp tasks such as information extraction (thomp sonet al, 1999), semantic role labeling (busser and morante, 2005), machine translation (haffari and sarkar, 2009), <papid> P09-1021 </papid>and name entity recognition (laws and schutze, 2008).</citsent>
<aftsection>
<nextsent>in our study, we apply this method for the frame assignment task as kind of semantic analysis.
</nextsent>
<nextsent>the process of active learning is as follows: the learner takes set of labeled instances, called seed data, as an input for initial training of the classifier; and then larger set of unlabeled instances will be selected by the classifier to be labeled with the human interaction.
</nextsent>
<nextsent>even small set of well selected samples for labeling can achieve the same level of performance of large labeled data set; and the oracles effort will be reduced as result.the motivation behind active learning is selecting the most useful examples for the classifier and thereby minimizing the annotation effort while still keeping up the performance level (thompson et al, 1999).
</nextsent>
<nextsent>there are two major learning scenarios inactive learning which are very popular among researchers and frequently used in various nlp tasks:stream-based sampling (cohn et al, 1994) and pool based sampling (lewis and gale, 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF869">
<title id=" W10-0101.xml">using variance as a stopping criterion for active learning of frame assignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the samples that are selected should be hard andvery informative.
</prevsent>
<prevsent>there are different query methods for sample selection which are independent of the active learning scenarios (settles, 2009).
</prevsent>
</prevsection>
<citsent citstr=" W04-3202 ">
among them, uncertainty sampling (lewis and gale, 1994)is the most well-known and the simplest sample selection method which only needs one classifier (baldridge and osborne, 2004).<papid> W04-3202 </papid></citsent>
<aftsection>
<nextsent>in this query method, the samples that the classifier is least con 1 algorithm 1 uncertainty sampling inactive learning input: seed data s, pool of unlabeled samples use to train the classifier while the stopping criterion is met do use to annotate select the top samples from predicted by which have the lowest confidence label k, augment with thek samples, andre move from use to retrain end while fident on their labels are selected and handed out tothe oracle.
</nextsent>
<nextsent>to this aim, confidence score is required which is in fact the prediction of the classifier with the highest probability for the label of the sample (busser and morante, 2005).
</nextsent>
<nextsent>the approach taken inactive learning for our taskis based on the uncertainty of the classifier with access to the pool of data.
</nextsent>
<nextsent>the learning process is presented in algorithm 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF870">
<title id=" W10-0101.xml">using variance as a stopping criterion for active learning of frame assignment </title>
<section> related work on stopping criteria.  </section>
<citcontext>
<prevsection>
<prevsent>practically, it is not clear how much annotation is sufficient for inducing classifier with maximum effectiveness (lewis and gale, 1994).
</prevsent>
<prevsent>schohn and cohn (2000) have used support vector machines (svm) for document classification using the selective sampling method and they have proposed criterion to stop the learning process in theirtask.
</prevsent>
</prevsection>
<citsent citstr=" D07-1082 ">
based on their idea, when there is no informative instance in the pool which is closer to the separating hyper plane than any of the support vectors, the margin exhausts and the learning process stops.zhu and hovey (2007) <papid> D07-1082 </papid>have used confidence based approach for the stopping criteria by utlizing the maximum confidence and the minimum error of the classifier.</citsent>
<aftsection>
<nextsent>the maximum confidence is based on the uncertainty measurement when the entropy ofthe selected unlabeled sample is less than predefined threshold close to zero.
</nextsent>
<nextsent>the minimum error is the feedback from the oracle when active learning asks for the true label of the selected unlabeled sample and the accuracy prediction of the classifier forthe selected unlabeled sample is larger than predefined accuracy threshold.
</nextsent>
<nextsent>these criteria are considered as upper-bound and lower-bound of the stopping condition.
</nextsent>
<nextsent>zhu et al (2008) <papid> I08-1048 </papid>proposed another stopping criterion based on statistical learning approach called minimum expected error strategy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF871">
<title id=" W10-0101.xml">using variance as a stopping criterion for active learning of frame assignment </title>
<section> related work on stopping criteria.  </section>
<citcontext>
<prevsection>
<prevsent>the minimum error is the feedback from the oracle when active learning asks for the true label of the selected unlabeled sample and the accuracy prediction of the classifier forthe selected unlabeled sample is larger than predefined accuracy threshold.
</prevsent>
<prevsent>these criteria are considered as upper-bound and lower-bound of the stopping condition.
</prevsent>
</prevsection>
<citsent citstr=" I08-1048 ">
zhu et al (2008) <papid> I08-1048 </papid>proposed another stopping criterion based on statistical learning approach called minimum expected error strategy.</citsent>
<aftsection>
<nextsent>in this approach, the maximum effectiveness of the classifier is reached when the classifiers expected errors on future unlabeled data is minimum.
</nextsent>
<nextsent>vlachos (2008) has used the classifier confidence score as stopping criterion for the uncertainty sampling.
</nextsent>
<nextsent>he has applied his model to two nlp tasks: text classification and named entity recognition.
</nextsent>
<nextsent>hehas built his models with the svm and the maximum entropy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF872">
<title id=" W10-0101.xml">using variance as a stopping criterion for active learning of frame assignment </title>
<section> related work on stopping criteria.  </section>
<citcontext>
<prevsection>
<prevsent>the convergence criterion is met when more examples from the pool of unlabeled data do not contribute more information to the classifiers performance, so that the classifier has reached its maximum performance.
</prevsent>
<prevsent>laws and schutze computed the convergence as the gradient of the classifiers estimated performance or uncertainty.
</prevsent>
</prevsection>
<citsent citstr=" L08-1208 ">
tomanek and hahn (2008) <papid> L08-1208 </papid>proposed stopping criterion based on the performance of the classifier without requiring labeled gold standard for acommittee-based active learning on the name entity recognition application.</citsent>
<aftsection>
<nextsent>in their criterion, they approximated the progression of the learning curve based on the disagreement among the committeemembers.
</nextsent>
<nextsent>they have used the validation set agreement curve as an adequate approximation for the progression of the learning curve.
</nextsent>
<nextsent>this curve was based on the data in each active learning iteration that makes the agreement values comparable between different active learning iterations.
</nextsent>
<nextsent>bloodgood and vijay-shanker (2009) explained three areas of stopping active learning that should be improved: applicability (restricting the usage in certain situation), lack of aggressive stopping (find ing the stopping points which are too far, so more examples than necessary are annotated), instability (well working of method on some dataset but notthe other data set).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF873">
<title id=" W09-2420.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora.
</prevsent>
<prevsent>with this paperwe want to motivate the creation of an all words test dataset for wsd on the environment domain in several languages, and present the overall design of this semeval task.
</prevsent>
</prevsection>
<citsent citstr=" W04-0807 ">
word sense disambiguation (wsd) competitions have focused on general domain texts, as attested in the last senseval and semeval competitions (kil garriff, 2001; mihalcea et al, 2004; <papid> W04-0807 </papid>pradhan et al, 2007).<papid> W07-2016 </papid></citsent>
<aftsection>
<nextsent>specific domains pose fresh challenges to wsd systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved.
</nextsent>
<nextsent>both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information.
</nextsent>
<nextsent>domain adaptation of supervised techniques is ahot issue in natural language processing, including word sense disambiguation.
</nextsent>
<nextsent>supervised word sense disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (escudero et al, 2000; <papid> W00-1322 </papid>martnezand agirre, 2000), and domain adaptation techniques have been proposed as solution to this problem with mixed results.current research on applying wsd to specific domains has been evaluated on three available lexical sample datasets (ng and lee, 1996; <papid> P96-1006 </papid>weeber et al, 2001; koeling et al, 2005).<papid> H05-1053 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF875">
<title id=" W09-2420.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora.
</prevsent>
<prevsent>with this paperwe want to motivate the creation of an all words test dataset for wsd on the environment domain in several languages, and present the overall design of this semeval task.
</prevsent>
</prevsection>
<citsent citstr=" W07-2016 ">
word sense disambiguation (wsd) competitions have focused on general domain texts, as attested in the last senseval and semeval competitions (kil garriff, 2001; mihalcea et al, 2004; <papid> W04-0807 </papid>pradhan et al, 2007).<papid> W07-2016 </papid></citsent>
<aftsection>
<nextsent>specific domains pose fresh challenges to wsd systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved.
</nextsent>
<nextsent>both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information.
</nextsent>
<nextsent>domain adaptation of supervised techniques is ahot issue in natural language processing, including word sense disambiguation.
</nextsent>
<nextsent>supervised word sense disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (escudero et al, 2000; <papid> W00-1322 </papid>martnezand agirre, 2000), and domain adaptation techniques have been proposed as solution to this problem with mixed results.current research on applying wsd to specific domains has been evaluated on three available lexical sample datasets (ng and lee, 1996; <papid> P96-1006 </papid>weeber et al, 2001; koeling et al, 2005).<papid> H05-1053 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF876">
<title id=" W09-2420.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information.
</prevsent>
<prevsent>domain adaptation of supervised techniques is ahot issue in natural language processing, including word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" W00-1322 ">
supervised word sense disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (escudero et al, 2000; <papid> W00-1322 </papid>martnezand agirre, 2000), and domain adaptation techniques have been proposed as solution to this problem with mixed results.current research on applying wsd to specific domains has been evaluated on three available lexical sample datasets (ng and lee, 1996; <papid> P96-1006 </papid>weeber et al, 2001; koeling et al, 2005).<papid> H05-1053 </papid></citsent>
<aftsection>
<nextsent>this kind of dataset contains hand-labeled examples for handful of selected target words.
</nextsent>
<nextsent>as the systems are evaluated on few words, the actual performance of the systems over complete texts can not be measured.
</nextsent>
<nextsent>differences in behavior of wsd systems when applied tolexical-sample and all-words datasets have been observed on previous senseval and semeval competitions (kilgarriff, 2001; mihalcea et al, 2004; <papid> W04-0807 </papid>pradhan et al, 2007): <papid> W07-2016 </papid>supervised systems attain result son the high 80s and beat the most frequent base line by large margin for lexical-sample datasets, but results on the all-words datasets were much more modest, on the low 70s, and few points above the most frequent baseline.thus, the behaviour of wsd systems on domain specific texts is largely unknown.</nextsent>
<nextsent>while some words could be supposed to behave in similar ways, and thus be amenable to be properly treated by generic 123 wsd algorithm, other words have senses closely linked to the domain, and might be disambiguated using purpose-built domain adaptation strategies (cf.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF877">
<title id=" W09-2420.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information.
</prevsent>
<prevsent>domain adaptation of supervised techniques is ahot issue in natural language processing, including word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
supervised word sense disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (escudero et al, 2000; <papid> W00-1322 </papid>martnezand agirre, 2000), and domain adaptation techniques have been proposed as solution to this problem with mixed results.current research on applying wsd to specific domains has been evaluated on three available lexical sample datasets (ng and lee, 1996; <papid> P96-1006 </papid>weeber et al, 2001; koeling et al, 2005).<papid> H05-1053 </papid></citsent>
<aftsection>
<nextsent>this kind of dataset contains hand-labeled examples for handful of selected target words.
</nextsent>
<nextsent>as the systems are evaluated on few words, the actual performance of the systems over complete texts can not be measured.
</nextsent>
<nextsent>differences in behavior of wsd systems when applied tolexical-sample and all-words datasets have been observed on previous senseval and semeval competitions (kilgarriff, 2001; mihalcea et al, 2004; <papid> W04-0807 </papid>pradhan et al, 2007): <papid> W07-2016 </papid>supervised systems attain result son the high 80s and beat the most frequent base line by large margin for lexical-sample datasets, but results on the all-words datasets were much more modest, on the low 70s, and few points above the most frequent baseline.thus, the behaviour of wsd systems on domain specific texts is largely unknown.</nextsent>
<nextsent>while some words could be supposed to behave in similar ways, and thus be amenable to be properly treated by generic 123 wsd algorithm, other words have senses closely linked to the domain, and might be disambiguated using purpose-built domain adaptation strategies (cf.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF878">
<title id=" W09-2420.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information.
</prevsent>
<prevsent>domain adaptation of supervised techniques is ahot issue in natural language processing, including word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" H05-1053 ">
supervised word sense disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (escudero et al, 2000; <papid> W00-1322 </papid>martnezand agirre, 2000), and domain adaptation techniques have been proposed as solution to this problem with mixed results.current research on applying wsd to specific domains has been evaluated on three available lexical sample datasets (ng and lee, 1996; <papid> P96-1006 </papid>weeber et al, 2001; koeling et al, 2005).<papid> H05-1053 </papid></citsent>
<aftsection>
<nextsent>this kind of dataset contains hand-labeled examples for handful of selected target words.
</nextsent>
<nextsent>as the systems are evaluated on few words, the actual performance of the systems over complete texts can not be measured.
</nextsent>
<nextsent>differences in behavior of wsd systems when applied tolexical-sample and all-words datasets have been observed on previous senseval and semeval competitions (kilgarriff, 2001; mihalcea et al, 2004; <papid> W04-0807 </papid>pradhan et al, 2007): <papid> W07-2016 </papid>supervised systems attain result son the high 80s and beat the most frequent base line by large margin for lexical-sample datasets, but results on the all-words datasets were much more modest, on the low 70s, and few points above the most frequent baseline.thus, the behaviour of wsd systems on domain specific texts is largely unknown.</nextsent>
<nextsent>while some words could be supposed to behave in similar ways, and thus be amenable to be properly treated by generic 123 wsd algorithm, other words have senses closely linked to the domain, and might be disambiguated using purpose-built domain adaptation strategies (cf.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF887">
<title id=" W09-2420.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> possible settings for domain adaptation.  </section>
<citcontext>
<prevsection>
<prevsent>there is an additional setting, where generic wsd system is supplemented with untagged examples from the domain.
</prevsent>
<prevsent>good results in this setting would show that semi-supervised domain adaptation works, and that generic wsd systems can be supplemented with untagged examples from the target domain in order to improve their results.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
most of current all-words generic supervised wsd systems take semcor (miller et al, 1993) <papid> H93-1061 </papid>as their source corpus, i.e. they are trained on semcorexamples and then applied to new examples.</citsent>
<aftsection>
<nextsent>semcor is the largest publicly available annotated corpus.
</nextsent>
<nextsent>its mainly subset of the brown corpus, plus the novel the red badge of courage.
</nextsent>
<nextsent>the brown corpus is balanced, yet not from the general domain,as it comprises 500 documents drawn from different domains, each approximately 2000 words long.
</nextsent>
<nextsent>although the brown corpus is balanced, semcor is not, as the documents were not chosen at random.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF892">
<title id=" W09-2420.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> state-of-the-art in wsd for specific.  </section>
<citcontext>
<prevsection>
<prevsent>iii, 2007) shows that simple feature augmentation method for svm is able to effectively use both labeled target and source data to provide the best domain adaptation results in number of nlp tasks.
</prevsent>
<prevsent>his method improves or equals over previously explored more sophisticated methods (daume?
</prevsent>
</prevsection>
<citsent citstr=" W04-3237 ">
iii and marcu, 2006; chelba and acero, 2004).<papid> W04-3237 </papid></citsent>
<aftsection>
<nextsent>in contrast, (agirre and lopez de lacalle, 2009) reimplemented this method and showed that the improvement on wsd in the (koeling et al, 2005) <papid> H05-1053 </papid>data was marginal.better results have been obtained using purpose built adaptation methods.</nextsent>
<nextsent>chan and ng (2007) <papid> P07-1007 </papid>performed supervised domain adaptation on manually selected subset of 21 nouns from the dso cor pus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF895">
<title id=" W09-2420.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> state-of-the-art in wsd for specific.  </section>
<citcontext>
<prevsection>
<prevsent>iii and marcu, 2006; chelba and acero, 2004).<papid> W04-3237 </papid></prevsent>
<prevsent>in contrast, (agirre and lopez de lacalle, 2009) reimplemented this method and showed that the improvement on wsd in the (koeling et al, 2005) <papid> H05-1053 </papid>data was marginal.better results have been obtained using purpose built adaptation methods.</prevsent>
</prevsection>
<citsent citstr=" P07-1007 ">
chan and ng (2007) <papid> P07-1007 </papid>performed supervised domain adaptation on manually selected subset of 21 nouns from the dso cor pus.</citsent>
<aftsection>
<nextsent>they used active learning, count-merging, and predominant sense estimation in order to save target annotation effort.
</nextsent>
<nextsent>they showed that adding just 30% of the target data to the source examples the same precision as the full combination of target and source data could be achieved.
</nextsent>
<nextsent>they also showed that using the source corpus significantly improved results when only 10%-30% of the target corpus was used for training.
</nextsent>
<nextsent>in followup work (zhong et 125 projections for 2100 suggest that temperature in europe will have risen by between 2 to 6.3 above 1990 levels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF898">
<title id=" W09-2420.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> state-of-the-art in wsd for specific.  </section>
<citcontext>
<prevsection>
<prevsent>they report 22% error reduction when using both source and target data compared to classifier trained on target the target data alone, even when the full dataset is used.
</prevsent>
<prevsent>semi-supervised adaptation there are less works on semi-supervised domain adaptation in nlp tasks, and fewer in wsd task.
</prevsent>
</prevsection>
<citsent citstr=" W06-1615 ">
blitzer et al (2006) <papid> W06-1615 </papid>used structural correspondence learning and unlabeled data to adapt part-of speech tagger.</citsent>
<aftsection>
<nextsent>they carefully select so-called pivot features to learn linear predictors, perform svd on the weights learned by the predictor, and thus learn correspondences among features in both source and target domains.
</nextsent>
<nextsent>agirre and lopez de lacalle (2008) show that methods based on svd with unlabeled data and combination of distinct feature spaces produce positive semi-supervised domain adaptation results for wsd.
</nextsent>
<nextsent>unsupervised adaptation in this context, we take unsupervised to mean knowledge-based methods which do not requirehand-tagged corpora.
</nextsent>
<nextsent>the predominant sense acquisition method was succes fully applied to specific domains in (koeling et al, 2005).<papid> H05-1053 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF905">
<title id=" W09-2413.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the former use machine learning techniques to induce classifier from manually sense-tagged data, where each occurrence of polysemous word gets assigned sense label from predefined sense inventory such as wordnet (fellbaum, 1998).
</prevsent>
<prevsent>these supervised methods, however, heavily relyon large sense tagged corpora which are very time consuming and expensive to build.
</prevsent>
</prevsection>
<citsent citstr=" P92-1032 ">
this phenomenon, well known as the knowledge acquisition bottleneck (gale et al., 1992), <papid> P92-1032 </papid>explains the modest use and success of supervised wsd in real applications.</citsent>
<aftsection>
<nextsent>although wsd has long time been studied as stand-alone nlp task, there is growing feeling in the wsd community that wsd should preferably be integrated in real applications such as machine translation or multilingual information retrieval (agirre and edmonds, 2006).
</nextsent>
<nextsent>several studies have demonstrated that for instance statistical machine translation (smt) benefits from incorporating dedicated wsd module (chan et al, 2007; <papid> P07-1005 </papid>carpuat and wu, 2007).<papid> D07-1007 </papid></nextsent>
<nextsent>using translations from corpus instead of human-defined sense labels is one way of facilitating the integration ofwsd in multilingual applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF906">
<title id=" W09-2413.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this phenomenon, well known as the knowledge acquisition bottleneck (gale et al., 1992), <papid> P92-1032 </papid>explains the modest use and success of supervised wsd in real applications.</prevsent>
<prevsent>although wsd has long time been studied as stand-alone nlp task, there is growing feeling in the wsd community that wsd should preferably be integrated in real applications such as machine translation or multilingual information retrieval (agirre and edmonds, 2006).</prevsent>
</prevsection>
<citsent citstr=" P07-1005 ">
several studies have demonstrated that for instance statistical machine translation (smt) benefits from incorporating dedicated wsd module (chan et al, 2007; <papid> P07-1005 </papid>carpuat and wu, 2007).<papid> D07-1007 </papid></citsent>
<aftsection>
<nextsent>using translations from corpus instead of human-defined sense labels is one way of facilitating the integration ofwsd in multilingual applications.
</nextsent>
<nextsent>it also implic 1http://www.senseval.org/ 82 itly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the translations.
</nextsent>
<nextsent>furthermore,this type of corpus-based approach is language independent, which makes it valid alternative for languages lacking sufficient sense inventories and sense-tagged corpora, although one could argue that the lack of parallel corpora for certain language pairs might be problematic as well.
</nextsent>
<nextsent>the methodology to deduce word senses from parallel corpora starts from the hypothesis that the different sense distinctions of polysemous word are often lexicalized cross-linguistically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF907">
<title id=" W09-2413.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this phenomenon, well known as the knowledge acquisition bottleneck (gale et al., 1992), <papid> P92-1032 </papid>explains the modest use and success of supervised wsd in real applications.</prevsent>
<prevsent>although wsd has long time been studied as stand-alone nlp task, there is growing feeling in the wsd community that wsd should preferably be integrated in real applications such as machine translation or multilingual information retrieval (agirre and edmonds, 2006).</prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
several studies have demonstrated that for instance statistical machine translation (smt) benefits from incorporating dedicated wsd module (chan et al, 2007; <papid> P07-1005 </papid>carpuat and wu, 2007).<papid> D07-1007 </papid></citsent>
<aftsection>
<nextsent>using translations from corpus instead of human-defined sense labels is one way of facilitating the integration ofwsd in multilingual applications.
</nextsent>
<nextsent>it also implic 1http://www.senseval.org/ 82 itly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the translations.
</nextsent>
<nextsent>furthermore,this type of corpus-based approach is language independent, which makes it valid alternative for languages lacking sufficient sense inventories and sense-tagged corpora, although one could argue that the lack of parallel corpora for certain language pairs might be problematic as well.
</nextsent>
<nextsent>the methodology to deduce word senses from parallel corpora starts from the hypothesis that the different sense distinctions of polysemous word are often lexicalized cross-linguistically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF908">
<title id=" W09-2413.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several wsd studies are based on the idea of cross-lingual evidence.
</prevsent>
<prevsent>gale et al (1993) use bilingual parallel corpus for the automatic creation of sense-tagged dataset, where target words in the source language are tagged with their translation of the word in the target language.
</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
diab and resnik (2002) <papid> P02-1033 </papid>present an unsupervised approach to wsd that exploits translational correspondences in parallel corpora that were artificially created by applying commercial mt systems on sense-tagged english corpus.</citsent>
<aftsection>
<nextsent>ide et al (2002) <papid> W02-0808 </papid>use multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators.</nextsent>
<nextsent>moreover, some studies present multilingual wsd systems that attain state-of-the-art performance in all-words disambiguation (ng et al, 2003).<papid> P03-1058 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF909">
<title id=" W09-2413.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>gale et al (1993) use bilingual parallel corpus for the automatic creation of sense-tagged dataset, where target words in the source language are tagged with their translation of the word in the target language.
</prevsent>
<prevsent>diab and resnik (2002) <papid> P02-1033 </papid>present an unsupervised approach to wsd that exploits translational correspondences in parallel corpora that were artificially created by applying commercial mt systems on sense-tagged english corpus.</prevsent>
</prevsection>
<citsent citstr=" W02-0808 ">
ide et al (2002) <papid> W02-0808 </papid>use multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators.</citsent>
<aftsection>
<nextsent>moreover, some studies present multilingual wsd systems that attain state-of-the-art performance in all-words disambiguation (ng et al, 2003).<papid> P03-1058 </papid></nextsent>
<nextsent>the proposed cross-lingual word sense disambiguation task differs from earlier work (e.g. ide et al (2002)) <papid> W02-0808 </papid>through its independence from an externally defined sense set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF910">
<title id=" W09-2413.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>diab and resnik (2002) <papid> P02-1033 </papid>present an unsupervised approach to wsd that exploits translational correspondences in parallel corpora that were artificially created by applying commercial mt systems on sense-tagged english corpus.</prevsent>
<prevsent>ide et al (2002) <papid> W02-0808 </papid>use multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators.</prevsent>
</prevsection>
<citsent citstr=" P03-1058 ">
moreover, some studies present multilingual wsd systems that attain state-of-the-art performance in all-words disambiguation (ng et al, 2003).<papid> P03-1058 </papid></citsent>
<aftsection>
<nextsent>the proposed cross-lingual word sense disambiguation task differs from earlier work (e.g. ide et al (2002)) <papid> W02-0808 </papid>through its independence from an externally defined sense set.</nextsent>
<nextsent>the remainder of this paper is organized as follows.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF912">
<title id=" W09-2413.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> task set up.  </section>
<citcontext>
<prevsection>
<prevsent>the document collection which serves as the basis for the gold standard construction and system evaluation is the europarl parallel corpus2, which is extracted from the proceedings of the european parliament (koehn, 2005).
</prevsent>
<prevsent>we selected 6 languages from the 11 european languages represented in the corpus: english (our target language), dutch, french, german, italian and spanish.
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
all sentences are aligned using tool based on the gale and church (1991) <papid> P91-1023 </papid>algorithm.</citsent>
<aftsection>
<nextsent>we only consider the 1-1 sentence alignments between english and the five other languages (see also tufis et al (2004) for similar strategy).
</nextsent>
<nextsent>these 1-1 alignments will be made available to all task participants.
</nextsent>
<nextsent>participants are free to use other training corpora, but additional translations which are not present in europarl will not be included in the sense inventory that is used for evaluation.
</nextsent>
<nextsent>for the competition, two datasets will be developed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF913">
<title id=" W09-2413.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> task set up.  </section>
<citcontext>
<prevsection>
<prevsent>1.
</prevsent>
<prevsent>in the first annotation step, the 5 translations.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
of the english word are identified per sentence id. in order to speed up this identification,giza++ (och and ney, 2003) <papid> J03-1002 </papid>is used to generate the initial word alignments for the 5 languages.</citsent>
<aftsection>
<nextsent>all word alignments are manually verified.
</nextsent>
<nextsent>in this step, we might come across multiword translations, especially in dutch and german which tend to glue parts of compounds together in one orthographic unit.
</nextsent>
<nextsent>we decided to keep these translations as such, even if they do not correspond exactly to the english target word.
</nextsent>
<nextsent>in following sentence, the dutch translationwitboek corresponds in fact to the english compound white paper, and not to the english target word paper: english: the european commission presented its white paper dutch: de presentatie van hetwitboek door de europ ese com missie although we will not remove these compound translations from our sense inventory, we will make sure that the development and test sentences do not contain target words that are part 4http://lit.csci.unt.edu/index.php/semeval 2010 84of larger multiword unit, in order not to disadvantage systems that do not deal with decom pounding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF914">
<title id=" W09-2413.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> system evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the evaluation ofthe multilingual evaluation task is simply the average of the system scores on the five bilingual evaluation tasks.
</prevsent>
<prevsent>3.1 evaluation strategies.
</prevsent>
</prevsection>
<citsent citstr=" W07-2009 ">
for the evaluation of the participating systems we will use an evaluation scheme which is inspired by the english lexical substitution task in semeval 2007 (mccarthy and navigli, 2007).<papid> W07-2009 </papid></citsent>
<aftsection>
<nextsent>the evaluation will be performed using precision and recall (p and in the equations that follow).
</nextsent>
<nextsent>we perform both best result evaluation and more relaxed evaluation for the top five results.
</nextsent>
<nextsent>let be the set of annotators, be the set of test items and hi be the set of responses for an item ? for annotator ? . let be the set of items from where the system provides at least one answer and ai : ? be the set of guesses from the system for item i. for each i, we calculate the multi set union (hi) for all hi for all ? and for each unique type (res) in hi that has an associated frequency (freqres).
</nextsent>
<nextsent>in the formula of (mccarthy and navigli, 2007), <papid> W07-2009 </papid>the associated frequency (freqres) is equal to the number of times an item appears in hi.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF916">
<title id=" W10-0724.xml">evaluation of commonsense knowledge with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while it is expected that eventually sufficiently clean knowledge bases will be produced for inferences to be made about everyday things and events,currently the average quality of automatically acquired knowledge is not good enough to be used in traditional reasoning systems.
</prevsent>
<prevsent>an obstacle for knowledge extraction is the lack of an easy method for evaluating ? and thus improving ? the quality of results.
</prevsent>
</prevsection>
<citsent citstr=" C02-1144 ">
evaluation in acquisition systems is typically done by human judging of random samples of output, usually by the reporting authors themselves (e.g., lin and pantel, 2002; <papid> C02-1144 </papid>schubert and tong, 2003; <papid> W03-0902 </papid>banko etal., 2007).</citsent>
<aftsection>
<nextsent>this is time-consuming, and it has the potential for bias: it would be preferable to have people other than ai researchers label whether an output is commonsense knowledge or not.
</nextsent>
<nextsent>we explore the useof amazons mechanical turk service, an online labor market, as means of acquiring many non-expert judgements for little cost.
</nextsent>
<nextsent>while open mind commons (speer, 2007) asks usersto vote for or against commonsense statements contributed by others users in order to come to consensus, we seek to evaluate an automatic system.
</nextsent>
<nextsent>snow et al (2008) <papid> D08-1027 </papid>compared the quality of labels produced by non-expert turkers against those made by experts for variety of nlp tasks and found that theyre quired only four responses per item to emulate expert annotations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF917">
<title id=" W10-0724.xml">evaluation of commonsense knowledge with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while it is expected that eventually sufficiently clean knowledge bases will be produced for inferences to be made about everyday things and events,currently the average quality of automatically acquired knowledge is not good enough to be used in traditional reasoning systems.
</prevsent>
<prevsent>an obstacle for knowledge extraction is the lack of an easy method for evaluating ? and thus improving ? the quality of results.
</prevsent>
</prevsection>
<citsent citstr=" W03-0902 ">
evaluation in acquisition systems is typically done by human judging of random samples of output, usually by the reporting authors themselves (e.g., lin and pantel, 2002; <papid> C02-1144 </papid>schubert and tong, 2003; <papid> W03-0902 </papid>banko etal., 2007).</citsent>
<aftsection>
<nextsent>this is time-consuming, and it has the potential for bias: it would be preferable to have people other than ai researchers label whether an output is commonsense knowledge or not.
</nextsent>
<nextsent>we explore the useof amazons mechanical turk service, an online labor market, as means of acquiring many non-expert judgements for little cost.
</nextsent>
<nextsent>while open mind commons (speer, 2007) asks usersto vote for or against commonsense statements contributed by others users in order to come to consensus, we seek to evaluate an automatic system.
</nextsent>
<nextsent>snow et al (2008) <papid> D08-1027 </papid>compared the quality of labels produced by non-expert turkers against those made by experts for variety of nlp tasks and found that theyre quired only four responses per item to emulate expert annotations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF918">
<title id=" W10-0724.xml">evaluation of commonsense knowledge with mechanical turk </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we explore the useof amazons mechanical turk service, an online labor market, as means of acquiring many non-expert judgements for little cost.
</prevsent>
<prevsent>while open mind commons (speer, 2007) asks usersto vote for or against commonsense statements contributed by others users in order to come to consensus, we seek to evaluate an automatic system.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
snow et al (2008) <papid> D08-1027 </papid>compared the quality of labels produced by non-expert turkers against those made by experts for variety of nlp tasks and found that theyre quired only four responses per item to emulate expert annotations.</citsent>
<aftsection>
<nextsent>kittur et al (2008) describe the use and 1public release of the basic knext engine is forthcoming.
</nextsent>
<nextsent>159 the statement above is reasonably clear, entirely plausible, generic claim and seems neither too specific nor too general or vague to be useful: ? agree.
</nextsent>
<nextsent>i lean towards agreement.
</nextsent>
<nextsent>im not sure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF919">
<title id=" W10-0709.xml">exploring normalization techniques for human judgments of machine translation adequacy collected using amazon mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present novel 2-stage normalization technique shown to have the best performance on this task and further discuss the results of all techniques and the usability of the resulting adequacy scores.
</prevsent>
<prevsent>human judgments of translation quality play vital role in the development of effective machine translation (mt) systems.
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
such judgments can be usedto measure system quality in evaluations (callison burch et al, 2009) and to tune automatic metrics such as meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>which act as stand-ins for human evaluators.</citsent>
<aftsection>
<nextsent>however, collecting reliable human judgments often requires significant time commitments from expert annotators,leading to general scarcity of judgments and significant time lag when seeking judgments for new tasks or languages.amazons mechanical turk (mturk) service facilitates inexpensive collection of large amounts ofdata from users around the world.
</nextsent>
<nextsent>however, turk ers are not trained to provide reliable annotations for natural language processing (nlp) tasks, and some turkers attempt to game the system by submitting random answers.
</nextsent>
<nextsent>for these reasons, nlp tasks must be designed to be accessible to untrained users and data normalization techniques must be employed to ensure that the data collected is usable.
</nextsent>
<nextsent>this paper describes mt evaluation task for translations of english into arabic conducted using mturk and compares several data normalization techniques.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF923">
<title id=" W09-2110.xml">using first and second language models to correct preposition errors in second language authoring </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(2006) used brown noise channel translation model to record patterns of determiner error correction on small set of mass-nouns, and reducing the error spectrum in both class and semantic domain, but adding detection capabilities.
</prevsent>
<prevsent>note that although they use translation model, it processes only text that is in one language.
</prevsent>
</prevsection>
<citsent citstr=" W07-1604 ">
more specifically, the system learned to  translate  from poorly written english into correctly written english.chodorow et al (2007) <papid> W07-1604 </papid>employed maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to np/vp chunks.</citsent>
<aftsection>
<nextsent>they use lemmatization as means of generalization and trained their model over 7 million prepositional contexts, achieving results of 84% precision and 19% recall in preposition error detection in the best of the system configurations.
</nextsent>
<nextsent>gamon et al (2008) <papid> I08-1059 </papid>worked on similar approach using only tagged trigram left and right contexts: model of prepositions uses serves to identify preposition errors and the web provides examples of correct form.</nextsent>
<nextsent>they evaluate their framework on the task of preposition identification and report results ranging from 74 to 45% precision on set of 13 prepositions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF924">
<title id=" W09-2110.xml">using first and second language models to correct preposition errors in second language authoring </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>more specifically, the system learned to  translate  from poorly written english into correctly written english.chodorow et al (2007) <papid> W07-1604 </papid>employed maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to np/vp chunks.</prevsent>
<prevsent>they use lemmatization as means of generalization and trained their model over 7 million prepositional contexts, achieving results of 84% precision and 19% recall in preposition error detection in the best of the system configurations.</prevsent>
</prevsection>
<citsent citstr=" I08-1059 ">
gamon et al (2008) <papid> I08-1059 </papid>worked on similar approach using only tagged trigram left and right contexts: model of prepositions uses serves to identify preposition errors and the web provides examples of correct form.</citsent>
<aftsection>
<nextsent>they evaluate their framework on the task of preposition identification and report results ranging from 74 to 45% precision on set of 13 prepositions.
</nextsent>
<nextsent>yi et al (2008) <papid> I08-2082 </papid>use the web as corpus and send segments of sentences of varying length as bag-ofconstituents queries to retrieve occurrence con texts.</nextsent>
<nextsent>the number of the queried segments is pos condition of  check-points  sensitive to typical errors made by l2 authors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF925">
<title id=" W09-2110.xml">using first and second language models to correct preposition errors in second language authoring </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>gamon et al (2008) <papid> I08-1059 </papid>worked on similar approach using only tagged trigram left and right contexts: model of prepositions uses serves to identify preposition errors and the web provides examples of correct form.</prevsent>
<prevsent>they evaluate their framework on the task of preposition identification and report results ranging from 74 to 45% precision on set of 13 prepositions.</prevsent>
</prevsection>
<citsent citstr=" I08-2082 ">
yi et al (2008) <papid> I08-2082 </papid>use the web as corpus and send segments of sentences of varying length as bag-ofconstituents queries to retrieve occurrence con texts.</citsent>
<aftsection>
<nextsent>the number of the queried segments is pos condition of  check-points  sensitive to typical errors made by l2 authors.
</nextsent>
<nextsent>the contexts retrieved are in turn analyzed for correspondence with the original input.
</nextsent>
<nextsent>the detection and correction methods differ according to the class of the error.
</nextsent>
<nextsent>determiner errors call for distinct detection and correction procedures while collocation errors use the same procedure for both.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF927">
<title id=" W09-2110.xml">using first and second language models to correct preposition errors in second language authoring </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>determiner errors are discovered by thresholds ratios on search hits statistics, taking into account probable ambiguities, since multiple forms of determiners can be valid in single context.
</prevsent>
<prevsent>collocation errors on the other hand, are assessed only by threshold on absolute counts, that is, form different from the input automatically signals an error and provides its correction.
</prevsent>
</prevsection>
<citsent citstr=" L08-1322 ">
this suggests that detection and correction procedures coincide when the error ceases to bear on function word.similarly, hermet et al (2008) <papid> L08-1322 </papid>use web as corpus based approach to address the correction of preposition errors in french-as-a-second-lan guage (fsl) context.</citsent>
<aftsection>
<nextsent>candidate prepositions are substituted for erroneous ones following taxonomy of semantic classes, which produces set of al 66 ternate sentences for each error.
</nextsent>
<nextsent>the main interest of their study is the use of syntax-based sentence generalization method to maximize the likelihood that at least one of the alternatives will have at least one hits on the web.
</nextsent>
<nextsent>they achieve accuracy of 69% in error repair (no error detection), on small set of clauses written by fsl learners.
</nextsent>
<nextsent>very little work has been done to actually exploit knowledge of l2 author first language, in correcting errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF929">
<title id=" W09-2110.xml">using first and second language models to correct preposition errors in second language authoring </title>
<section> algorithmic framework.  </section>
<citcontext>
<prevsection>
<prevsent>note also that the uni lingual approach can only deal with preposition errors (although it would be easy enough to extend it to other kinds of function words), and cannot deal with more semantically deep l1 interference.
</prevsent>
<prevsent>to address these issues, we experimented with second strategy which we will refer to as the round trip machine translation approach (or round trip mt for short).
</prevsent>
</prevsection>
<citsent citstr=" P06-1032 ">
note that our approach is different from that of brockett et al (2006), <papid> P06-1032 </papid>as we do make use of truly multi-lingual translation model.</citsent>
<aftsection>
<nextsent>in contrast, brocketts translation model was trained on texts that were written in the same language, with the sources being ill-written text in the same language as the properly-formed target texts.
</nextsent>
<nextsent>one drawback of our approach however is category prepositions localization in front, behind, after, before, above, in, at, on, below, above...
</nextsent>
<nextsent>temporal at, in, after, before, for, during, since...
</nextsent>
<nextsent>cause for, because of goal for, at manner in, by, with, according to...
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF931">
<title id=" W09-2110.xml">using first and second language models to correct preposition errors in second language authoring </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we limited ourselves to error correction, since it could be solved through very simple round-trip translation, without requiring detailed control of the mt system, or access to lower level information generated by the system in the course of translation (for example, intermediate hypotheses with probabilities and alignment information between source and target sentences).
</prevsent>
<prevsent>in contrast, we believe that error detection with an mt approach will require this kind of finer control and access to the guts of the mt system.
</prevsent>
</prevsection>
<citsent citstr=" W07-0724 ">
we plan to investigate this using the portage mt system (ueffing et al, 2007).<papid> W07-0724 </papid></citsent>
<aftsection>
<nextsent>essentially, we plan to use the mt system internal information to assign confidence scores to various segments of the round trip translation, and label them as corrections if this confidence is above certain threshold.
</nextsent>
<nextsent>in doing this, we will be following in the footsteps of yi et al (2008) <papid> I08-2082 </papid>who use the same algorithm for error detection and error correction.</nextsent>
<nextsent>the process of detecting an error is simply one of determining whether the system topmost alternative is different from what appeared in the original sentence, and whether the system confidence in that alternative is sufficiently high to take the risk of presenting it to the user as suggested correction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF934">
<title id=" W09-1904.xml">data quality from crowdsourcing a study of annotation selection criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>crowdsourcing (howe, 2008) is an attractive solution to the problem of cheaply and quickly acquiring annotations for the purposes of constructing all kinds of predictive models.
</prevsent>
<prevsent>to sense the potential of crowdsourcing, consider an observation in von ahn et al (2004): crowd of 5,000 people playing an appropriately designed computer game 24 hours day, could be made to label all images on google (425,000,000 images in 2005) in matter of just 31 days.
</prevsent>
</prevsection>
<citsent citstr=" P08-1080 ">
several recent papers have studied the use of annotations obtained from amazon mechanical turk, marketplace for recruiting online workers (su et al, 2007; kaisser et al, 2008; <papid> P08-1080 </papid>kittur et al, 2008; sheng et al, 2008; snow et al, 2008; <papid> D08-1027 </papid>sorokin and forsyth, 2008).with efficiency and cost-effectiveness, online recruitment of anonymous annotators brings new set of issues to the table.</citsent>
<aftsection>
<nextsent>these workers are not usually specifically trained for annotation, and might not be highly invested in producing good-quality annotations.
</nextsent>
<nextsent>consequently, the obtained annotations maybe noisy by nature, and might require additional validation or scrutiny.
</nextsent>
<nextsent>several interesting questions immediately arise in how to optimally utilize annotations in this setting: how does one handle differences among workers in terms of the quality of annotations they provide?
</nextsent>
<nextsent>how useful are noisy annotations for the end task of creating model?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF935">
<title id=" W09-1904.xml">data quality from crowdsourcing a study of annotation selection criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>crowdsourcing (howe, 2008) is an attractive solution to the problem of cheaply and quickly acquiring annotations for the purposes of constructing all kinds of predictive models.
</prevsent>
<prevsent>to sense the potential of crowdsourcing, consider an observation in von ahn et al (2004): crowd of 5,000 people playing an appropriately designed computer game 24 hours day, could be made to label all images on google (425,000,000 images in 2005) in matter of just 31 days.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
several recent papers have studied the use of annotations obtained from amazon mechanical turk, marketplace for recruiting online workers (su et al, 2007; kaisser et al, 2008; <papid> P08-1080 </papid>kittur et al, 2008; sheng et al, 2008; snow et al, 2008; <papid> D08-1027 </papid>sorokin and forsyth, 2008).with efficiency and cost-effectiveness, online recruitment of anonymous annotators brings new set of issues to the table.</citsent>
<aftsection>
<nextsent>these workers are not usually specifically trained for annotation, and might not be highly invested in producing good-quality annotations.
</nextsent>
<nextsent>consequently, the obtained annotations maybe noisy by nature, and might require additional validation or scrutiny.
</nextsent>
<nextsent>several interesting questions immediately arise in how to optimally utilize annotations in this setting: how does one handle differences among workers in terms of the quality of annotations they provide?
</nextsent>
<nextsent>how useful are noisy annotations for the end task of creating model?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF936">
<title id=" W09-1904.xml">data quality from crowdsourcing a study of annotation selection criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such an analysis provides view of the opinion around subject of interest, e.g., us presidential candidates, aggregated across the blogsphere.
</prevsent>
<prevsent>recently, sentiment analy 27 sis is emerging as critical methodology for social media analytics.
</prevsent>
</prevsection>
<citsent citstr=" J04-3002 ">
previous research has focused on classifying subjective-versus-objective expressions (wiebe et al, 2004), <papid> J04-3002 </papid>and also on accurate sentiment polarity assignment (turney, 2002; <papid> P02-1053 </papid>yi et al, 2003; pang and lee, 2004; <papid> P04-1035 </papid>sindhwani and melville, 2008; melville et al, 2009).the success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around topic(yi et al, 2003), or quality annotation data for statistical training.</citsent>
<aftsection>
<nextsent>while manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>wiebe and riloff, 2005), manual intervention in the annotation process is harder to avoid.</nextsent>
<nextsent>more over, the task of annotating blog-post snippets is challenging, particularly in charged political atmosphere with complex discourse spanning many issues, use of cynicism and sarcasm, and highly domain-specific and contextual cues.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF937">
<title id=" W09-1904.xml">data quality from crowdsourcing a study of annotation selection criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such an analysis provides view of the opinion around subject of interest, e.g., us presidential candidates, aggregated across the blogsphere.
</prevsent>
<prevsent>recently, sentiment analy 27 sis is emerging as critical methodology for social media analytics.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
previous research has focused on classifying subjective-versus-objective expressions (wiebe et al, 2004), <papid> J04-3002 </papid>and also on accurate sentiment polarity assignment (turney, 2002; <papid> P02-1053 </papid>yi et al, 2003; pang and lee, 2004; <papid> P04-1035 </papid>sindhwani and melville, 2008; melville et al, 2009).the success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around topic(yi et al, 2003), or quality annotation data for statistical training.</citsent>
<aftsection>
<nextsent>while manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>wiebe and riloff, 2005), manual intervention in the annotation process is harder to avoid.</nextsent>
<nextsent>more over, the task of annotating blog-post snippets is challenging, particularly in charged political atmosphere with complex discourse spanning many issues, use of cynicism and sarcasm, and highly domain-specific and contextual cues.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF938">
<title id=" W09-1904.xml">data quality from crowdsourcing a study of annotation selection criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such an analysis provides view of the opinion around subject of interest, e.g., us presidential candidates, aggregated across the blogsphere.
</prevsent>
<prevsent>recently, sentiment analy 27 sis is emerging as critical methodology for social media analytics.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
previous research has focused on classifying subjective-versus-objective expressions (wiebe et al, 2004), <papid> J04-3002 </papid>and also on accurate sentiment polarity assignment (turney, 2002; <papid> P02-1053 </papid>yi et al, 2003; pang and lee, 2004; <papid> P04-1035 </papid>sindhwani and melville, 2008; melville et al, 2009).the success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around topic(yi et al, 2003), or quality annotation data for statistical training.</citsent>
<aftsection>
<nextsent>while manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>wiebe and riloff, 2005), manual intervention in the annotation process is harder to avoid.</nextsent>
<nextsent>more over, the task of annotating blog-post snippets is challenging, particularly in charged political atmosphere with complex discourse spanning many issues, use of cynicism and sarcasm, and highly domain-specific and contextual cues.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF939">
<title id=" W09-1904.xml">data quality from crowdsourcing a study of annotation selection criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently, sentiment analy 27 sis is emerging as critical methodology for social media analytics.
</prevsent>
<prevsent>previous research has focused on classifying subjective-versus-objective expressions (wiebe et al, 2004), <papid> J04-3002 </papid>and also on accurate sentiment polarity assignment (turney, 2002; <papid> P02-1053 </papid>yi et al, 2003; pang and lee, 2004; <papid> P04-1035 </papid>sindhwani and melville, 2008; melville et al, 2009).the success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around topic(yi et al, 2003), or quality annotation data for statistical training.</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
while manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>wiebe and riloff, 2005), manual intervention in the annotation process is harder to avoid.</citsent>
<aftsection>
<nextsent>more over, the task of annotating blog-post snippets is challenging, particularly in charged political atmosphere with complex discourse spanning many issues, use of cynicism and sarcasm, and highly domain-specific and contextual cues.
</nextsent>
<nextsent>the downside is that high-performance models are generally difficult to construct, but the upside is that annotation and data-quality issues are more clearly exposed.
</nextsent>
<nextsent>in this paper we aim to provide an empirical basis for the use of data selection criteria in the context of sentiment analysis in political blogs.
</nextsent>
<nextsent>specifically, we highlight the need for set of criteria that can be applied to screen untrustworthy annotators and select informative yet unambiguous examples for the end goal of predictive modeling.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF940">
<title id=" W09-1904.xml">data quality from crowdsourcing a study of annotation selection criteria </title>
<section> annotation quality measures.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 snippet-level sentiment ambiguity.
</prevsent>
<prevsent>we have observed that not all snippets are equally easy to annotate, with some containing more ambiguous expressions.
</prevsent>
</prevsection>
<citsent citstr=" W08-1106 ">
to incorporate this concern in the selection process, key question to be answered is whether there exist snippets whose sentiment is substantially less distinguishable than the others.we address this question by quantifying ambiguity measures with the two key properties shown as important in evaluating the controversiality of annotation snippets (carenini and cheung, 2008): (<papid> W08-1106 </papid>1) the strength of the annotators?</citsent>
<aftsection>
<nextsent>judgements and (2) the polarity of the annotations.
</nextsent>
<nextsent>the measurement needsto satisfy the constraints demonstrated in the following snippets: (1) an example that has received three positive codings are more ambiguous than that has received five, and (2) an example that has received five positive codings is more ambiguous than the onethat has received four positive and one negative coding.
</nextsent>
<nextsent>in addition, as some snippets were shown to 30 annotator noise level predictio accuracy 0.0 0.2 0.4 0.6 0.80.
</nextsent>
<nextsent>0 0.2 0.4 0.6 0.8 1.0 annotator noise 0.0 0.2 0.4 0.6 0.8 1.00.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF941">
<title id=" W09-1902.xml">on proper unit selection inactive learning co selection effects for named entity recognition </title>
<section> the missed class effect.  </section>
<citcontext>
<prevsection>
<prevsent>so, under certain situations similarity of classes can mitigate the missed class effect.the co-selection effect many nlp tasks are sequence learning problems including, e.g., pos tagging, and named entity recognition.
</prevsent>
<prevsent>sequences are consecutive text tokens constituting linguistically plausible chunks, e.g., sentences.
</prevsent>
</prevsection>
<citsent citstr=" D08-1112 ">
algorithms for sequence learning obviously work on sequence data, so respective al approaches need to select complete sequences instead of single text tokens (settles and craven, 2008).<papid> D08-1112 </papid></citsent>
<aftsection>
<nextsent>furthermore, sentence selection has been preferred over token selection in other works with the argument that the manual annotation of single, possibly isolated tokens is almost impossible or at least extremely time-consuming (ringger et al, 2007; <papid> W07-1516 </papid>tomanek et al, 2007).<papid> D07-1051 </papid></nextsent>
<nextsent>within such sequences, instances of different classes often co-occur.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF942">
<title id=" W09-1902.xml">on proper unit selection inactive learning co selection effects for named entity recognition </title>
<section> the missed class effect.  </section>
<citcontext>
<prevsection>
<prevsent>sequences are consecutive text tokens constituting linguistically plausible chunks, e.g., sentences.
</prevsent>
<prevsent>algorithms for sequence learning obviously work on sequence data, so respective al approaches need to select complete sequences instead of single text tokens (settles and craven, 2008).<papid> D08-1112 </papid></prevsent>
</prevsection>
<citsent citstr=" W07-1516 ">
furthermore, sentence selection has been preferred over token selection in other works with the argument that the manual annotation of single, possibly isolated tokens is almost impossible or at least extremely time-consuming (ringger et al, 2007; <papid> W07-1516 </papid>tomanek et al, 2007).<papid> D07-1051 </papid></citsent>
<aftsection>
<nextsent>within such sequences, instances of different classes often co-occur.
</nextsent>
<nextsent>thus, an active learner that selects uncertain examples of one class gets examples of second class as an unintended, yet positive side effect.
</nextsent>
<nextsent>we call this the co-selection effect.
</nextsent>
<nextsent>as result, al for sequence labeling is not pure?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF943">
<title id=" W09-1902.xml">on proper unit selection inactive learning co selection effects for named entity recognition </title>
<section> the missed class effect.  </section>
<citcontext>
<prevsection>
<prevsent>sequences are consecutive text tokens constituting linguistically plausible chunks, e.g., sentences.
</prevsent>
<prevsent>algorithms for sequence learning obviously work on sequence data, so respective al approaches need to select complete sequences instead of single text tokens (settles and craven, 2008).<papid> D08-1112 </papid></prevsent>
</prevsection>
<citsent citstr=" D07-1051 ">
furthermore, sentence selection has been preferred over token selection in other works with the argument that the manual annotation of single, possibly isolated tokens is almost impossible or at least extremely time-consuming (ringger et al, 2007; <papid> W07-1516 </papid>tomanek et al, 2007).<papid> D07-1051 </papid></citsent>
<aftsection>
<nextsent>within such sequences, instances of different classes often co-occur.
</nextsent>
<nextsent>thus, an active learner that selects uncertain examples of one class gets examples of second class as an unintended, yet positive side effect.
</nextsent>
<nextsent>we call this the co-selection effect.
</nextsent>
<nextsent>as result, al for sequence labeling is not pure?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF944">
<title id=" W09-1902.xml">on proper unit selection inactive learning co selection effects for named entity recognition </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the third (syn) is synthetic set constructed to have specific characteristics.
</prevsent>
<prevsent>for simplicity, we consider only scenarios with two entity classes, majority class (maj) and minority class (min).
</prevsent>
</prevsection>
<citsent citstr=" W04-3111 ">
we discarded all other entity annotations originally contained in the corpus assigning the outside class.2the first dataset (pbio) is based on the annotations of the pennbioie corpus for biomedical entity extraction (kulick et al, 2004).<papid> W04-3111 </papid></citsent>
<aftsection>
<nextsent>as pennbioie makes fine-grained and subtle distinctions between various sub types of classes irrelevant for this study, we combined several of the original classes into two entity classes: the majority class consists of the three original classes gene-protein?, gene-generic?, and gene-rna?.
</nextsent>
<nextsent>the minority class consists of the original and similar classes variation-type?
</nextsent>
<nextsent>and 2the outside class marks that token is not part of an named entity.variation-event?.
</nextsent>
<nextsent>all other entity labels were replaced by the outside class.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF945">
<title id=" W09-1902.xml">on proper unit selection inactive learning co selection effects for named entity recognition </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the success of al is usually measured as reduction of annotation effort according to some cost measure.
</prevsent>
<prevsent>traditionally, the most common cost measure considers unit cost per annotated token, which favors al systems that select individual tokens.
</prevsent>
</prevsection>
<citsent citstr=" W05-0619 ">
in real annotation setting, however, it is unnatural, and therefore hard for humans to annotate single, possibly isolated tokens, leading to bad annotation quality (hachey et al, 2005; <papid> W05-0619 </papid>ringger et al, 2007).<papid> W07-1516 </papid></citsent>
<aftsection>
<nextsent>when providing context, the question arises whether the annotator can label several tokens present in the context (e.g., an entire multi-token entity or eventhe whole sentence) at little more cost than annotating single token.
</nextsent>
<nextsent>thus, assigning linear cost of to sentence where is the sentences leng thin tokens seems to unfairly disadvantage sentence selectional setups.however, more work is needed to find more realistic cost measure.
</nextsent>
<nextsent>at present there is no other generally accepted cost measure than unit cost per token, so we report costs using the token measure.
</nextsent>
<nextsent>this section presents the results of our experiments on the missed class effect in two different alscenarios, i.e., sentence selection (s-al) and token selection (t-al).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF947">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> semantic relation classification: issues.  </section>
<citcontext>
<prevsection>
<prevsent>a wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications.
</prevsent>
<prevsent>some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have more general focus.
</prevsent>
</prevsection>
<citsent citstr=" W01-0511 ">
some schemes are specific to domain such as biomedical text.rosario and hearst (2001) <papid> W01-0511 </papid>classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier.</citsent>
<aftsection>
<nextsent>rosario et al (2002) <papid> P02-1032 </papid>classify noun compounds using the mesh hierarchy and multi-level hierarchy of semantic relations, with 15classes at the top level.</nextsent>
<nextsent>stephens et al (2001) propose 17 very specific classes targeting relations between genes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF948">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> semantic relation classification: issues.  </section>
<citcontext>
<prevsection>
<prevsent>some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have more general focus.
</prevsent>
<prevsent>some schemes are specific to domain such as biomedical text.rosario and hearst (2001) <papid> W01-0511 </papid>classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier.</prevsent>
</prevsection>
<citsent citstr=" P02-1032 ">
rosario et al (2002) <papid> P02-1032 </papid>classify noun compounds using the mesh hierarchy and multi-level hierarchy of semantic relations, with 15classes at the top level.</citsent>
<aftsection>
<nextsent>stephens et al (2001) propose 17 very specific classes targeting relations between genes.
</nextsent>
<nextsent>nastase and szpakowicz (2003) address the problem of classifying noun-modifier relations in general text.
</nextsent>
<nextsent>they propose two-level hierarchy, with 5 classes at the first level and 30 class esat the second one; other researchers (kim and baldwin, 2005; <papid> I05-1082 </papid>nakov and hearst, 2008; <papid> P08-1052 </papid>nastase et al, 2006; turney, 2005; turney and littman, 2005) have used their class scheme and data set.</nextsent>
<nextsent>moldovan et al (2004) <papid> W04-2609 </papid>propose 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (girju et al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF949">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> semantic relation classification: issues.  </section>
<citcontext>
<prevsection>
<prevsent>stephens et al (2001) propose 17 very specific classes targeting relations between genes.
</prevsent>
<prevsent>nastase and szpakowicz (2003) address the problem of classifying noun-modifier relations in general text.
</prevsent>
</prevsection>
<citsent citstr=" I05-1082 ">
they propose two-level hierarchy, with 5 classes at the first level and 30 class esat the second one; other researchers (kim and baldwin, 2005; <papid> I05-1082 </papid>nakov and hearst, 2008; <papid> P08-1052 </papid>nastase et al, 2006; turney, 2005; turney and littman, 2005) have used their class scheme and data set.</citsent>
<aftsection>
<nextsent>moldovan et al (2004) <papid> W04-2609 </papid>propose 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (girju et al, 2005).</nextsent>
<nextsent>lapata (2002) <papid> J02-3004 </papid>presents binary classification of relations in nominalizations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF950">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> semantic relation classification: issues.  </section>
<citcontext>
<prevsection>
<prevsent>stephens et al (2001) propose 17 very specific classes targeting relations between genes.
</prevsent>
<prevsent>nastase and szpakowicz (2003) address the problem of classifying noun-modifier relations in general text.
</prevsent>
</prevsection>
<citsent citstr=" P08-1052 ">
they propose two-level hierarchy, with 5 classes at the first level and 30 class esat the second one; other researchers (kim and baldwin, 2005; <papid> I05-1082 </papid>nakov and hearst, 2008; <papid> P08-1052 </papid>nastase et al, 2006; turney, 2005; turney and littman, 2005) have used their class scheme and data set.</citsent>
<aftsection>
<nextsent>moldovan et al (2004) <papid> W04-2609 </papid>propose 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (girju et al, 2005).</nextsent>
<nextsent>lapata (2002) <papid> J02-3004 </papid>presents binary classification of relations in nominalizations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF952">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> semantic relation classification: issues.  </section>
<citcontext>
<prevsection>
<prevsent>nastase and szpakowicz (2003) address the problem of classifying noun-modifier relations in general text.
</prevsent>
<prevsent>they propose two-level hierarchy, with 5 classes at the first level and 30 class esat the second one; other researchers (kim and baldwin, 2005; <papid> I05-1082 </papid>nakov and hearst, 2008; <papid> P08-1052 </papid>nastase et al, 2006; turney, 2005; turney and littman, 2005) have used their class scheme and data set.</prevsent>
</prevsection>
<citsent citstr=" W04-2609 ">
moldovan et al (2004) <papid> W04-2609 </papid>propose 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (girju et al, 2005).</citsent>
<aftsection>
<nextsent>lapata (2002) <papid> J02-3004 </papid>presents binary classification of relations in nominalizations.</nextsent>
<nextsent>pantel and pennacchiotti (2006) <papid> P06-1015 </papid>concentrate on five relations in an ie-style setting.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF953">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> semantic relation classification: issues.  </section>
<citcontext>
<prevsection>
<prevsent>they propose two-level hierarchy, with 5 classes at the first level and 30 class esat the second one; other researchers (kim and baldwin, 2005; <papid> I05-1082 </papid>nakov and hearst, 2008; <papid> P08-1052 </papid>nastase et al, 2006; turney, 2005; turney and littman, 2005) have used their class scheme and data set.</prevsent>
<prevsent>moldovan et al (2004) <papid> W04-2609 </papid>propose 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (girju et al, 2005).</prevsent>
</prevsection>
<citsent citstr=" J02-3004 ">
lapata (2002) <papid> J02-3004 </papid>presents binary classification of relations in nominalizations.</citsent>
<aftsection>
<nextsent>pantel and pennacchiotti (2006) <papid> P06-1015 </papid>concentrate on five relations in an ie-style setting.</nextsent>
<nextsent>in short, there is little agreement on relation inventories.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF954">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> semantic relation classification: issues.  </section>
<citcontext>
<prevsection>
<prevsent>moldovan et al (2004) <papid> W04-2609 </papid>propose 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (girju et al, 2005).</prevsent>
<prevsent>lapata (2002) <papid> J02-3004 </papid>presents binary classification of relations in nominalizations.</prevsent>
</prevsection>
<citsent citstr=" P06-1015 ">
pantel and pennacchiotti (2006) <papid> P06-1015 </papid>concentrate on five relations in an ie-style setting.</citsent>
<aftsection>
<nextsent>in short, there is little agreement on relation inventories.
</nextsent>
<nextsent>2.2 the role of context.
</nextsent>
<nextsent>a fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context.
</nextsent>
<nextsent>when one looks at real data, it becomes clear that context does indeed play role.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF955">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> semantic relation classification: issues.  </section>
<citcontext>
<prevsection>
<prevsent>it isknown from other fields that the availability of standard benchmark datasets can provide boost to the advancement of field.
</prevsent>
<prevsent>as first step, semeval 2007 task 4 offered many useful insights into the.
</prevsent>
</prevsection>
<citsent citstr=" P08-1027 ">
performance of different approaches to semantic relation classification; it has also motivated followup research (davidov and rappoport, 2008; <papid> P08-1027 </papid>katrenko and adriaans, 2008; <papid> P08-2047 </papid>nakov and hearst, 2008; <papid> P08-1052 </papid>o?</citsent>
<aftsection>
<nextsent>seaghdha and copestake, 2008).
</nextsent>
<nextsent>our objective is to build on the achievements ofsemeval-2007 task 4 while addressing its shortcomings.
</nextsent>
<nextsent>in particular, we consider larger set of semantic relations (9 instead of 7), we assume proper multi-class classification setting, we emulate the effect of an open?
</nextsent>
<nextsent>relation inventory by means of tenth class other, and we will release to the research community dataset with considerably 1http://www.itl.nist.gov/iad/mig/tests/ ace/ 2although it was not designed for multi-class set-up, some subsequent publications tried to use the datasets in that manner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF956">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> semantic relation classification: issues.  </section>
<citcontext>
<prevsection>
<prevsent>it isknown from other fields that the availability of standard benchmark datasets can provide boost to the advancement of field.
</prevsent>
<prevsent>as first step, semeval 2007 task 4 offered many useful insights into the.
</prevsent>
</prevsection>
<citsent citstr=" P08-2047 ">
performance of different approaches to semantic relation classification; it has also motivated followup research (davidov and rappoport, 2008; <papid> P08-1027 </papid>katrenko and adriaans, 2008; <papid> P08-2047 </papid>nakov and hearst, 2008; <papid> P08-1052 </papid>o?</citsent>
<aftsection>
<nextsent>seaghdha and copestake, 2008).
</nextsent>
<nextsent>our objective is to build on the achievements ofsemeval-2007 task 4 while addressing its shortcomings.
</nextsent>
<nextsent>in particular, we consider larger set of semantic relations (9 instead of 7), we assume proper multi-class classification setting, we emulate the effect of an open?
</nextsent>
<nextsent>relation inventory by means of tenth class other, and we will release to the research community dataset with considerably 1http://www.itl.nist.gov/iad/mig/tests/ ace/ 2although it was not designed for multi-class set-up, some subsequent publications tried to use the datasets in that manner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF959">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> designing an inventory of semantic re-.  </section>
<citcontext>
<prevsection>
<prevsent>this encourages good generalization behaviour to larger, noisier datasets commonly seen in real-world applications.
</prevsent>
<prevsent>3.1 semantic relations versus semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
there are three main differences between our task(classification of semantic relations between nomi nals) and the related task of automatic labeling of semantic roles (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>the first difference is to do with the linguistic phenomena described.
</nextsent>
<nextsent>lexical resources for theories of semantic roles such as framenet (fillmore et 96 al., 2003) and propbank (palmer et al, 2005) <papid> J05-1004 </papid>have been developed to describe the linguistic realization patterns of events and states.</nextsent>
<nextsent>thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF960">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> designing an inventory of semantic re-.  </section>
<citcontext>
<prevsection>
<prevsent>there are three main differences between our task(classification of semantic relations between nomi nals) and the related task of automatic labeling of semantic roles (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
<prevsent>the first difference is to do with the linguistic phenomena described.</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
lexical resources for theories of semantic roles such as framenet (fillmore et 96 al., 2003) and propbank (palmer et al, 2005) <papid> J05-1004 </papid>have been developed to describe the linguistic realization patterns of events and states.</citsent>
<aftsection>
<nextsent>thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns.
</nextsent>
<nextsent>in contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominal sin this task.
</nextsent>
<nextsent>also, semantic role descriptions typically relate an event to set of multiple participants and props, while semantic relations are in practice (although not necessarily) binary.the second major difference is the syntactic context.
</nextsent>
<nextsent>theories of semantic roles usually developed out of syntactic descriptions of verb valencies, and thus they focus on describing the linking patterns of verbs and their direct dependents, phenomena like raising and noninstantiations notwithstanding (fill more, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF961">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> designing an inventory of semantic re-.  </section>
<citcontext>
<prevsection>
<prevsent>propbank does use small number of semantic roles, but these are again to be interpreted at the level of individual predicates, with little cross-predicate generalization.
</prevsent>
<prevsent>in contrast, all of the semantic relation inventories discussed in section 1 contain fewer than50 types of semantic relations.
</prevsent>
</prevsection>
<citsent citstr=" W04-3205 ">
more generally, semantic relation inventories attempt to generalize relations across wide groups of verbs (chklovski andpantel, 2004) <papid> W04-3205 </papid>and include relations that are not verb centered (nastase and szpakowicz, 2003; moldova net al, 2004).<papid> W04-2609 </papid></citsent>
<aftsection>
<nextsent>using the same labels for similar semantic relations facilitates supervised learning.
</nextsent>
<nextsent>for example, model trained with examples of sell relations should be able to transfer what it has learned to give relations.
</nextsent>
<nextsent>this has the potential of adding5for example, it relates the buyer role of the commerce sell frame (verb sell ) to the recipient role of the giving frame (verb give).
</nextsent>
<nextsent>1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF963">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> annotation.  </section>
<citcontext>
<prevsection>
<prevsent>we focus on heads that are common nouns.
</prevsent>
<prevsent>this emphasis distinguishes our task from much work inie, which focuses on named entities and on considerably more fine-grained relations than we do.
</prevsent>
</prevsection>
<citsent citstr=" D07-1075 ">
for example, patwardhan and riloff (2007) <papid> D07-1075 </papid>identify categories like terrorist organization as participants interror-related semantic relations, which consists predominantly of named entities.</citsent>
<aftsection>
<nextsent>we feel that named entities are specific category of nominal expressions best dealt with using techniques which do not apply to common nouns; for example, they do not lend themselves well to semantic generalization.figure 1 shows two examples of annotated sentences.
</nextsent>
<nextsent>the xml tags  e1  and  e2  mark the target nominals.
</nextsent>
<nextsent>since all nine proper semantic relations in this task are asymmetric, the ordering of the two nominals must be taken into account.
</nextsent>
<nextsent>in example 1, cause-effect(e1, e2) does not hold,although cause-effect(e2, e1) would.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF964">
<title id=" W09-2415.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> annotation.  </section>
<citcontext>
<prevsection>
<prevsent>in order to ensure wide variety of example sentences, we will use several dozen patterns per relation.
</prevsent>
<prevsent>we will also ensure that patterns retrieve both positive and negative example sentences; the latter will help populate the other relation with realistic near-miss negative examples of the other relations.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
the patterns will be manually constructed following the approach of hearst (1992) <papid> C92-2082 </papid>and nakov and hearst (2008).<papid> P08-1052 </papid>6 the example collection for each relation will be passed to two independent annotators.</citsent>
<aftsection>
<nextsent>in order to maintain exclusivity of relations, only examples that are negative for all relations but will be included as positive and only examples that are negative for all nine relations will be included as other.
</nextsent>
<nextsent>next,the annotators will compare their decisions and assess inter-annotator agreement.
</nextsent>
<nextsent>consensus will besought; if the annotators cannot agree on an example it will not be included in the dataset, but it will be recorded for future analysis.
</nextsent>
<nextsent>finally, two other task organizers will look for overlap across all relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF966">
<title id=" W09-2101.xml">automated assessment of spoken modern standard arabic </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ordinates spoken spanish test also included automatically scored passage retellings that used an adapted form of latent semantic analysis to estimate vocabulary scores.
</prevsent>
<prevsent>more recently at ets, zechner et al (2007) describe experiments in automatic scoring of test taker responses in toefl ibt practice environment, focusing mostly on fluency features.
</prevsent>
</prevsection>
<citsent citstr=" W08-0912 ">
zechner and xi (2008) <papid> W08-0912 </papid>report work on similar algorithms to score item types with varying degrees of response predictability, including items with very restricted range of possible answers (e.g., reading aloud) as well as item types with progressively less restricted answers (e.g., describing picture ? relatively predictable, or stating an opinion ? less pre dictable).</citsent>
<aftsection>
<nextsent>the scoring mechanism in zechner and xi (2008) <papid> W08-0912 </papid>employs features such as the average number of word types or silences for fluency estimation, the asr hmm log-likelihood for pronunciation or vector-based similarity measure to assess vocabulary and content.</nextsent>
<nextsent>zechner and xi present correlations of machine scores with human scores for two tasks: r=0.50 for an opinion task and r=0.69 for picture description, which are comparable to the modest human rater agreement figures in this data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF970">
<title id=" W09-2806.xml">evaluation of automatic summaries metrics under varying data conditions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since these measures to neutralize judgment variation involve the production of multiple model summaries, as well as multiple topics, evaluation can become quite costly.
</prevsent>
<prevsent>therefore, it is interesting to examine how many models and topics are necessary to obtain relatively stable evaluation, and whether this number is different for manual and automatic metrics.
</prevsent>
</prevsection>
<citsent citstr=" W04-1003 ">
in their examination of summary evaluations, van halteren and teufel (2003) suggest that it is necessary to use at least 30 to 40 model summaries for stableevaluation; however, harman and over (2004) <papid> W04-1003 </papid>argue that stable evaluation can be conducted even with single model, as long as there is an adequate number of topics.</citsent>
<aftsection>
<nextsent>this view is supported by lin (2004<papid> W04-1013 </papid>a), who concludes that correlations tohuman judgments were increased by using multiple references but using single reference summary with enough number of samples was valid al ternative?.</nextsent>
<nextsent>interestingly, similar conclusions were also reached in the area of machine translation evaluation; in their experiments, zhang and vogel (2004) show that adding an additional reference translation compensates the effects of removing1015% of the testing data, and state that, therefore, it seems more cost effective to have more test sentences but fewer reference translations?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF971">
<title id=" W09-2806.xml">evaluation of automatic summaries metrics under varying data conditions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, it is interesting to examine how many models and topics are necessary to obtain relatively stable evaluation, and whether this number is different for manual and automatic metrics.
</prevsent>
<prevsent>in their examination of summary evaluations, van halteren and teufel (2003) suggest that it is necessary to use at least 30 to 40 model summaries for stableevaluation; however, harman and over (2004) <papid> W04-1003 </papid>argue that stable evaluation can be conducted even with single model, as long as there is an adequate number of topics.</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
this view is supported by lin (2004<papid> W04-1013 </papid>a), who concludes that correlations tohuman judgments were increased by using multiple references but using single reference summary with enough number of samples was valid al ternative?.</citsent>
<aftsection>
<nextsent>interestingly, similar conclusions were also reached in the area of machine translation evaluation; in their experiments, zhang and vogel (2004) show that adding an additional reference translation compensates the effects of removing1015% of the testing data, and state that, therefore, it seems more cost effective to have more test sentences but fewer reference translations?.
</nextsent>
<nextsent>in this paper, we look at how various metrics behave with respect to variable number of topics and models used in the evaluation.
</nextsent>
<nextsent>this lets us determine the stability of individual metrics, and helps to illuminate the trade-offs inherent in designing good evaluation.
</nextsent>
<nextsent>for our experiments, we used data from the summarization track at thetext analysis conference (tac) 2008, where participating systems were assessed on their summarization of 48 topics, and the automatic metrics rouge and be, as well as the manual pyramid evaluation method, had access to 4 human models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF972">
<title id=" W09-2806.xml">evaluation of automatic summaries metrics under varying data conditions </title>
<section> summary evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>tac 2008 was the first task of the tac/duc (document understanding conference) series inwhich the pyramid method was used on all evaluated data, making it possible to conduct full com 23parison among the manual and automatic methods.
</prevsent>
<prevsent>despite the lack of full pyramid evaluation in duc 2007, we look at the remaining metrics applied that year (rouge, be, and content responsiveness), in order to see whether they confirm the insights gained from the tac 2008 data.
</prevsent>
</prevsection>
<citsent citstr=" N04-1019 ">
the main evaluation at tac 2008 was performed manually, assessing the automatic candidate summaries with respect to overall responsiveness,overall readability, and content coverage according to the pyramid framework (nenkova and passonneau, 2004; <papid> N04-1019 </papid>passonneau et al, 2005).</citsent>
<aftsection>
<nextsent>task participants were asked to produce two summaries for each of the 48 topics; the first (initial summary) was straightforward summary of 10 documents in response to topic statement, which is request for information about subject or event; the second was an update summary, generated on the basis of another set of 10 documents, which followed the first set in temporal order and described further developments in the given topic.
</nextsent>
<nextsent>the idea behind the update summary was to avoid repeating all the information included in the first set of documents, on the assumption that the reader is familiar with that information already.
</nextsent>
<nextsent>the participating teams submitted up to three runs each; however, only the first and second runs were evaluated manually due to limited resources.
</nextsent>
<nextsent>for each summary under evaluation, assessors rated the summary from 1 (very poor) to 5 (very good) in terms of overall responsiveness, which measures how well the summary responds to the need for information expressed in the topic statement and whether its linguistic quality is adequate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF978">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>analysis of statistical machine translation (smt) out put showed that despite ignoring document structure, the one translation per discourse hypothesis is strongly supported in part because of the low variability in smt lexical choice.more interestingly, cases where the hypothesis does not hold can reveal lexical choiceerrors.
</prevsent>
<prevsent>a preliminary study showed that enforcing the one translation per discourse constraint in smt can potentially improve translation quality, and that smt systems might benefit from translating sentences within their entire document context.
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
the one sense per discourse hypothesis formulated by gale et al (1992<papid> H92-1045 </papid>b) has proved to be simple yet powerful observation and has been successfully used in word sense disambiguation (wsd) andre lated tasks (e.g., yarowsky (1995); <papid> P95-1026 </papid>agirre and rigauthe author was partially funded by gale darpa contract no.</citsent>
<aftsection>
<nextsent>hr0011-06-c-0023.
</nextsent>
<nextsent>any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the defense advanced research projects agency.
</nextsent>
<nextsent>(1996)).
</nextsent>
<nextsent>in this paper, we investigate its potential usefulness in the context of machine translation.a growing body of work suggests that translational differences represent observable sense distinctions that are useful in applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF981">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>analysis of statistical machine translation (smt) out put showed that despite ignoring document structure, the one translation per discourse hypothesis is strongly supported in part because of the low variability in smt lexical choice.more interestingly, cases where the hypothesis does not hold can reveal lexical choiceerrors.
</prevsent>
<prevsent>a preliminary study showed that enforcing the one translation per discourse constraint in smt can potentially improve translation quality, and that smt systems might benefit from translating sentences within their entire document context.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
the one sense per discourse hypothesis formulated by gale et al (1992<papid> H92-1045 </papid>b) has proved to be simple yet powerful observation and has been successfully used in word sense disambiguation (wsd) andre lated tasks (e.g., yarowsky (1995); <papid> P95-1026 </papid>agirre and rigauthe author was partially funded by gale darpa contract no.</citsent>
<aftsection>
<nextsent>hr0011-06-c-0023.
</nextsent>
<nextsent>any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the defense advanced research projects agency.
</nextsent>
<nextsent>(1996)).
</nextsent>
<nextsent>in this paper, we investigate its potential usefulness in the context of machine translation.a growing body of work suggests that translational differences represent observable sense distinctions that are useful in applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF982">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1996)).
</prevsent>
<prevsent>in this paper, we investigate its potential usefulness in the context of machine translation.a growing body of work suggests that translational differences represent observable sense distinctions that are useful in applications.
</prevsent>
</prevsection>
<citsent citstr=" P03-1058 ">
in monolingual wsd, word alignments in parallel corpora have been successfully used as learning evidence (resnik and yarowsky, 1999; diab and resnik, 2002; ng et al, 2003).<papid> P03-1058 </papid></citsent>
<aftsection>
<nextsent>in statistical machine translation(smt), recent work shows that wsd helps translation quality when the wsd system directly uses translation candidates as sense inventories (carpuat and wu, 2007; <papid> D07-1007 </papid>chan et al, 2007; <papid> P07-1005 </papid>gimenez and ma`rquez, 2007).in this paper, we revisit the one sense per discourse hypothesis using word translations in parallel text as senses.</nextsent>
<nextsent>our first goal is to empirically evaluate whether the one translation per document hypothesis holds on french-english reference corpora, thus verifying whether translations exhibit the same properties as monolingual senses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF983">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we investigate its potential usefulness in the context of machine translation.a growing body of work suggests that translational differences represent observable sense distinctions that are useful in applications.
</prevsent>
<prevsent>in monolingual wsd, word alignments in parallel corpora have been successfully used as learning evidence (resnik and yarowsky, 1999; diab and resnik, 2002; ng et al, 2003).<papid> P03-1058 </papid></prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
in statistical machine translation(smt), recent work shows that wsd helps translation quality when the wsd system directly uses translation candidates as sense inventories (carpuat and wu, 2007; <papid> D07-1007 </papid>chan et al, 2007; <papid> P07-1005 </papid>gimenez and ma`rquez, 2007).in this paper, we revisit the one sense per discourse hypothesis using word translations in parallel text as senses.</citsent>
<aftsection>
<nextsent>our first goal is to empirically evaluate whether the one translation per document hypothesis holds on french-english reference corpora, thus verifying whether translations exhibit the same properties as monolingual senses.
</nextsent>
<nextsent>our second goal consists in evaluating whether the one translation per discourse hypothesis has the potential to be as useful to statistical machine translation as theone sense per discourse hypothesis to wsd.
</nextsent>
<nextsent>current statistical machine translation (smt) systems translate one sentence at time, ignoring any document level information.
</nextsent>
<nextsent>implementing one translation per document constraint might help provide consistency in translation for sentences drawn from the same document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF985">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we investigate its potential usefulness in the context of machine translation.a growing body of work suggests that translational differences represent observable sense distinctions that are useful in applications.
</prevsent>
<prevsent>in monolingual wsd, word alignments in parallel corpora have been successfully used as learning evidence (resnik and yarowsky, 1999; diab and resnik, 2002; ng et al, 2003).<papid> P03-1058 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1005 ">
in statistical machine translation(smt), recent work shows that wsd helps translation quality when the wsd system directly uses translation candidates as sense inventories (carpuat and wu, 2007; <papid> D07-1007 </papid>chan et al, 2007; <papid> P07-1005 </papid>gimenez and ma`rquez, 2007).in this paper, we revisit the one sense per discourse hypothesis using word translations in parallel text as senses.</citsent>
<aftsection>
<nextsent>our first goal is to empirically evaluate whether the one translation per document hypothesis holds on french-english reference corpora, thus verifying whether translations exhibit the same properties as monolingual senses.
</nextsent>
<nextsent>our second goal consists in evaluating whether the one translation per discourse hypothesis has the potential to be as useful to statistical machine translation as theone sense per discourse hypothesis to wsd.
</nextsent>
<nextsent>current statistical machine translation (smt) systems translate one sentence at time, ignoring any document level information.
</nextsent>
<nextsent>implementing one translation per document constraint might help provide consistency in translation for sentences drawn from the same document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF989">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a subsequent larger scale study of polysemybased on the wordnet sense inventory in the sem cor corpus does not support the hypothesis as strongly (krovetz, 1998).
</prevsent>
<prevsent>only 77% of ambiguous words have single sense per discourse.
</prevsent>
</prevsection>
<citsent citstr=" W07-0717 ">
analysis revealed that the one sense per discourse hypothesis is only supported for homonymous senses and not for finer-grained sense distinction.in machine translation, discourse level information has only been indirectly used by adaptation of translation or language models to specific genre or topics (e.g., foster and kuhn (2007); <papid> W07-0717 </papid>koehn and schroeder (2007)).<papid> W07-0733 </papid></citsent>
<aftsection>
<nextsent>while phrase-based smt models incorporate the one sense per collocation hypothesis by attempting to translate phrases rather than single words (koehn et al, 2007), <papid> P07-2045 </papid>the one sense per discourse hypothesis has not been explicitly used in smt modeling.</nextsent>
<nextsent>even the recent generation of smt models that explicitly use wsd modeling to perform lexical choice relyon sentence context rather than wider document context and translate sentences in isolation (carpuat and wu, 2007; <papid> D07-1007 </papid>chan et al, 2007; <papid> P07-1005 </papid>gimenez and ma`rquez, 2007; stroppa et al, 2007; specia et al, 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF990">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a subsequent larger scale study of polysemybased on the wordnet sense inventory in the sem cor corpus does not support the hypothesis as strongly (krovetz, 1998).
</prevsent>
<prevsent>only 77% of ambiguous words have single sense per discourse.
</prevsent>
</prevsection>
<citsent citstr=" W07-0733 ">
analysis revealed that the one sense per discourse hypothesis is only supported for homonymous senses and not for finer-grained sense distinction.in machine translation, discourse level information has only been indirectly used by adaptation of translation or language models to specific genre or topics (e.g., foster and kuhn (2007); <papid> W07-0717 </papid>koehn and schroeder (2007)).<papid> W07-0733 </papid></citsent>
<aftsection>
<nextsent>while phrase-based smt models incorporate the one sense per collocation hypothesis by attempting to translate phrases rather than single words (koehn et al, 2007), <papid> P07-2045 </papid>the one sense per discourse hypothesis has not been explicitly used in smt modeling.</nextsent>
<nextsent>even the recent generation of smt models that explicitly use wsd modeling to perform lexical choice relyon sentence context rather than wider document context and translate sentences in isolation (carpuat and wu, 2007; <papid> D07-1007 </papid>chan et al, 2007; <papid> P07-1005 </papid>gimenez and ma`rquez, 2007; stroppa et al, 2007; specia et al, 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF991">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>only 77% of ambiguous words have single sense per discourse.
</prevsent>
<prevsent>analysis revealed that the one sense per discourse hypothesis is only supported for homonymous senses and not for finer-grained sense distinction.in machine translation, discourse level information has only been indirectly used by adaptation of translation or language models to specific genre or topics (e.g., foster and kuhn (2007); <papid> W07-0717 </papid>koehn and schroeder (2007)).<papid> W07-0733 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
while phrase-based smt models incorporate the one sense per collocation hypothesis by attempting to translate phrases rather than single words (koehn et al, 2007), <papid> P07-2045 </papid>the one sense per discourse hypothesis has not been explicitly used in smt modeling.</citsent>
<aftsection>
<nextsent>even the recent generation of smt models that explicitly use wsd modeling to perform lexical choice relyon sentence context rather than wider document context and translate sentences in isolation (carpuat and wu, 2007; <papid> D07-1007 </papid>chan et al, 2007; <papid> P07-1005 </papid>gimenez and ma`rquez, 2007; stroppa et al, 2007; specia et al, 2008).</nextsent>
<nextsent>other context-sensitive smt approaches (gimpel and smith, 2008) <papid> W08-0302 </papid>and global lexical choice models (bangalore et al, 2007) <papid> P07-1020 </papid>also translate sentences independently.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF996">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while phrase-based smt models incorporate the one sense per collocation hypothesis by attempting to translate phrases rather than single words (koehn et al, 2007), <papid> P07-2045 </papid>the one sense per discourse hypothesis has not been explicitly used in smt modeling.</prevsent>
<prevsent>even the recent generation of smt models that explicitly use wsd modeling to perform lexical choice relyon sentence context rather than wider document context and translate sentences in isolation (carpuat and wu, 2007; <papid> D07-1007 </papid>chan et al, 2007; <papid> P07-1005 </papid>gimenez and ma`rquez, 2007; stroppa et al, 2007; specia et al, 2008).</prevsent>
</prevsection>
<citsent citstr=" W08-0302 ">
other context-sensitive smt approaches (gimpel and smith, 2008) <papid> W08-0302 </papid>and global lexical choice models (bangalore et al, 2007) <papid> P07-1020 </papid>also translate sentences independently.</citsent>
<aftsection>
<nextsent>reference translations in this section we investigate whether the one sense per discourse hypothesis holds in translation.
</nextsent>
<nextsent>does one sense per discourse mean one translation per discourse?
</nextsent>
<nextsent>on the one hand, one translation per discourse might be too strict constraint to allow for variations in lexicalization of given sense.
</nextsent>
<nextsent>while wsd task produces set of predefined sense labels, single sense might be correctly translated in many different ways in full sentence translation.on the other hand, if the author of the source language text is assumed to consistently use one sense per word per document, translators might also prefer consistent translations of the same source language word throughout document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF997">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while phrase-based smt models incorporate the one sense per collocation hypothesis by attempting to translate phrases rather than single words (koehn et al, 2007), <papid> P07-2045 </papid>the one sense per discourse hypothesis has not been explicitly used in smt modeling.</prevsent>
<prevsent>even the recent generation of smt models that explicitly use wsd modeling to perform lexical choice relyon sentence context rather than wider document context and translate sentences in isolation (carpuat and wu, 2007; <papid> D07-1007 </papid>chan et al, 2007; <papid> P07-1005 </papid>gimenez and ma`rquez, 2007; stroppa et al, 2007; specia et al, 2008).</prevsent>
</prevsection>
<citsent citstr=" P07-1020 ">
other context-sensitive smt approaches (gimpel and smith, 2008) <papid> W08-0302 </papid>and global lexical choice models (bangalore et al, 2007) <papid> P07-1020 </papid>also translate sentences independently.</citsent>
<aftsection>
<nextsent>reference translations in this section we investigate whether the one sense per discourse hypothesis holds in translation.
</nextsent>
<nextsent>does one sense per discourse mean one translation per discourse?
</nextsent>
<nextsent>on the one hand, one translation per discourse might be too strict constraint to allow for variations in lexicalization of given sense.
</nextsent>
<nextsent>while wsd task produces set of predefined sense labels, single sense might be correctly translated in many different ways in full sentence translation.on the other hand, if the author of the source language text is assumed to consistently use one sense per word per document, translators might also prefer consistent translations of the same source language word throughout document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF998">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> one translation per discourse in.  </section>
<citcontext>
<prevsection>
<prevsent>it would have been interesting to perform this analysis with multiple references, but this is unfortunately not possible with 1http://www.statmt.org/wmt09/translation-task.html 20 test set language sentences tokens types singletons no. 1 french 1070 27440 5958 3727 english (ref) 1070 24544 5566 3342 english (smt) 1070 24758 5075 2932 no. 2 french 1080 27924 6150 3839 english (ref) 1080 24825 5686 3414 english (smt) 1080 25128 5240 3080 table 1: data statistics for the bilingual corpus, including the french side, the manually translated english side (ref) and the automatic english translations (smt) the french-english data currently available.
</prevsent>
<prevsent>since golden word-alignments are not available,we automatically word align the corpus using standard smt training techniques.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
using ibm-4 alignment models learned on the large wmt training corpus (see section 4.1 for more details), we aligngiza++(och and ney, 2003) <papid> J03-1002 </papid>to obtain the ibm 4 alignments in both translation directions, expand their intersection with additional links using the grow-diag-final-and heuristic (koehn et al, 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>this creates total of 51660 alignment links, and about 89% of french tokens are aligned to at lea stone english token.
</nextsent>
<nextsent>note that all links involving stop words are not considered for the rest of the study.
</nextsent>
<nextsent>3.2 one translation per discourse holds.
</nextsent>
<nextsent>for every french lemma that occurs more than once in document, we compute the number of english translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1006">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> one translation per discourse in.  </section>
<citcontext>
<prevsection>
<prevsent>do these translation differences reflect sense ambiguity in french, or are they close variations in english lexical choice?
</prevsent>
<prevsent>for 21 reference smt lemmas stems lemmas stems 1 80.82% 85.14% 83.03% 86.38% 2 17.88% 13.91% 15.43% 12.47% 3 01.12% 00.95% 01.25% 00.85% table 2: distribution of number of english translation per document using the word-aligned reference translations and the automatic smt translations given french word, how semantically similar are the various english translations?
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
we measure semantic similarity using the shortest path length in wordnet (fellbaum, 1998) as implemented in the wordnet similarity package (peder sen et al, 2004).<papid> N04-3012 </papid></citsent>
<aftsection>
<nextsent>the path length is defined as the number of wordnet nodes or synsets in path between two words: words that belong to the same synset therefore have shortest path length of 1, while words that are related via common synonym, hypernym or hyponym have shortest path length of 2.
</nextsent>
<nextsent>note that this similarity metric is only defined for.
</nextsent>
<nextsent>two wordnet vocabulary words of the same pos.
</nextsent>
<nextsent>for 57% of the french lemmas with multiple translations, those translations can be linked by wordnet path of no more than 4 nodes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1008">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> detecting smt errors.  </section>
<citcontext>
<prevsection>
<prevsent>note that the supervised strategy does not predict perfect translations, but an approximation of the golden translations: in addition to noise in word alignments due to phrasal translations, the translations selected are lemmas that might not be in the correctly inflected form for use in the full sentence translation.third, we integrate the selected translation candidates by (1) postprocessing the baseline smt out put - the translations of the french target word are simply replaced by the recommended translation, and (2) encouraging the smt system to choose the recommended translations by annotating smt in put using the xml input markup scheme - again,this approach is not optimal as it introduces additional translation candidates without probability scores and forces single word translation to compete with phrasal translation even if they are consistent.
</prevsent>
<prevsent>5.2 impact on translation quality.
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
as reported in table 3, small increases in meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>bleu (papineni et al, 2002) <papid> P02-1040 </papid>and nist scores (doddington, 2002) suggest that smt output matches the references better after postprocessing or decoding with the suggested lemma translations.</citsent>
<aftsection>
<nextsent>examples of both improved and degraded lexical choice are given in table 4.
</nextsent>
<nextsent>since we are modifying translations for limited set of single-words only, only 10% to 30% of the testset sentences are translated differently.
</nextsent>
<nextsent>we manually inspected random sample of 100 of those sentence pairs for two different systems: postprocess(bestmatch) and decode (bestmatch).
</nextsent>
<nextsent>for each sentence pair, we determined whether the one sense per discourse?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1009">
<title id=" W09-2404.xml">one translation per discourse </title>
<section> detecting smt errors.  </section>
<citcontext>
<prevsection>
<prevsent>note that the supervised strategy does not predict perfect translations, but an approximation of the golden translations: in addition to noise in word alignments due to phrasal translations, the translations selected are lemmas that might not be in the correctly inflected form for use in the full sentence translation.third, we integrate the selected translation candidates by (1) postprocessing the baseline smt out put - the translations of the french target word are simply replaced by the recommended translation, and (2) encouraging the smt system to choose the recommended translations by annotating smt in put using the xml input markup scheme - again,this approach is not optimal as it introduces additional translation candidates without probability scores and forces single word translation to compete with phrasal translation even if they are consistent.
</prevsent>
<prevsent>5.2 impact on translation quality.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
as reported in table 3, small increases in meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>bleu (papineni et al, 2002) <papid> P02-1040 </papid>and nist scores (doddington, 2002) suggest that smt output matches the references better after postprocessing or decoding with the suggested lemma translations.</citsent>
<aftsection>
<nextsent>examples of both improved and degraded lexical choice are given in table 4.
</nextsent>
<nextsent>since we are modifying translations for limited set of single-words only, only 10% to 30% of the testset sentences are translated differently.
</nextsent>
<nextsent>we manually inspected random sample of 100 of those sentence pairs for two different systems: postprocess(bestmatch) and decode (bestmatch).
</nextsent>
<nextsent>for each sentence pair, we determined whether the one sense per discourse?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1011">
<title id=" W10-0504.xml">an information retrieval approach to language modeling applications to social data </title>
<section> n-gram slm and ir-lm  </section>
<citcontext>
<prevsection>
<prevsent>in smoothed backoff slm (e.g., goodman (2001)), all the n-grams up to order are computed and smoothed and backoff probabilities are calculated.
</prevsent>
<prevsent>if new data is introduced or removed from the corpus, the whole model, the counts and weights would need to be recalculated.
</prevsent>
</prevsection>
<citsent citstr=" D09-1079 ">
levenberg and osborne (2009) <papid> D09-1079 </papid>proposed an approach for incorporating new data as it is seen in the stream.</citsent>
<aftsection>
<nextsent>language models have been used to support ir as method to extend queries (lavrenko et al 2001); in this paper we focus on using ir to carry out language modeling.
</nextsent>
<nextsent>2.1 their language model.
</nextsent>
<nextsent>the ir-lm approach consists of two steps: the first is the identification of set of matches from corpus given query sentence, and second is the estimation of likelihood-like value for the query.
</nextsent>
<nextsent>in the first step, given corpus and query sentence s, we identify the k-closest matching sentences in the corpus through an information retrieval approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1012">
<title id=" W10-0725.xml">cheap facts and counter facts </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>what would be good negative hypotheses.
</prevsent>
<prevsent>compared with the positive ones?
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
in this paper, we address these issues by using amazons mechanical turk (mturk), online non expert annotators (snow et al, 2008).<papid> D08-1027 </papid></citsent>
<aftsection>
<nextsent>instead of constructing the hypotheses targeted to ie or qa, we just ask the human annotators to come up with some facts they consider as relevant to the given text.
</nextsent>
<nextsent>for negative hypotheses, we change the instruction and ask them to write counter-factual but still relevant statements.
</nextsent>
<nextsent>in order to narrow down the content ofthe generated hypotheses, we give focused named entity (ne) for each text to guide the annotators.
</nextsent>
<nextsent>the early related research was done by cooper et al(1996), where they manually construct textbook style corpus aiming at different semantic phenomena involved in inference.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1013">
<title id=" W10-0725.xml">cheap facts and counter facts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the early related research was done by cooper et al(1996), where they manually construct textbook style corpus aiming at different semantic phenomena involved in inference.
</prevsent>
<prevsent>however, the dataset isnot large enough to train robust machine-learning based rte system.
</prevsent>
</prevsection>
<citsent citstr=" W05-1209 ">
the recent research from therte community focused on acquiring large quantities of textual entailment pairs from news headlines (burger and ferro, 2005) <papid> W05-1209 </papid>and negative examples from sequential sentences with transitional discourse connectives (hickl et al, 2006).</citsent>
<aftsection>
<nextsent>although the quality of the data collected were quite good,most of the positive examples are similar to summarization and the negative examples are more like comparison/contrast between two sentences instead of contradiction.
</nextsent>
<nextsent>those data are the real sentences used in news articles, but the way of obtaining them is not necessarily the (only) best way to 163 find entailment pairs.
</nextsent>
<nextsent>in this paper, we investigate an alternative inexpensive way of collecting entail ment/contradiction text pairs by crowdsourcing.
</nextsent>
<nextsent>in addition to the information given by the text, common knowledge is also allowed to be involved in the inference procedure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1014">
<title id=" W10-0725.xml">cheap facts and counter facts </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>we give turkers one example as guide along with the instructions.
</prevsent>
<prevsent>the texts we use in our experiments are the development set of the rte-5 challenge (bentivogli et al, 1http://www.cs.utexas.edu/pclark/ bpi-test-suite/ total average (per text) extracted nes facts 244 1.19 counter-facts 121 1.11 generated hypotheses facts 790 3.85 counter-facts 203 1.86 table 1: the statistics of the (valid) data we collect.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
the total column presents the number of extracted nes and generated hypotheses and the average column shows the average numbers per text respectively.2009), and we pre process the data using the stanford named-entity recognizer (finkel et al, 2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>in all, it contains 600 t-h pairs, and we use the texts to generate facts and counter-facts and hypotheses asreferences.
</nextsent>
<nextsent>we put our task online through crowd flower2, and on average, we pay one cent for each (counter-)fact to the turkers.
</nextsent>
<nextsent>crowd flower can help with finding trustful turkers and the data were collected within few hours.
</nextsent>
<nextsent>to get sense of the quality of the data we collect, we mainly focus on analyzing the following three aspects: 1) the statistics of the datasets themselves; 2) the comparison between the data we collect andthe original rte dataset; and 3) the comparison between the facts and the counter-facts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1015">
<title id=" W09-2405.xml">using web selectors for the disambiguation of all words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, these experiments also help to draw insights about the future direction of similar research.
</prevsent>
<prevsent>the great amount of text on the web has emerged as an unprecedented electronic source of natural language.
</prevsent>
</prevsection>
<citsent citstr=" W04-3204 ">
recently, word sense disambiguation systems have fostered the size of the web in order to supplant the issue of limited annotated data availability for supervised systems (mihalcea, 2002; agirre and martinez, 2004).<papid> W04-3204 </papid></citsent>
<aftsection>
<nextsent>some unsupervised or minimally supervised methods use the web more directly in disambiguation algorithms that do not use training set for the specific target words.one such minimally supervised method uses selectors acquired from the web for noun sense disambiguation by comparing the selectors of given sentence to target noun within the sentence (schwartz and gomez, 2008).
</nextsent>
<nextsent>although this work found strong results, many aspects of the use of selectors was left unexplored.
</nextsent>
<nextsent>for one, the method was only applied to noun sense disambiguation, focusing on the well developed noun hypernym hierarchy within wordnet (miller et al, 1993).
</nextsent>
<nextsent>additionally, the role of different types of selectors was not extensively explored, and adverb selectors were not used at all.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1016">
<title id=" W09-2405.xml">using web selectors for the disambiguation of all words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we seek to address those issues.in this paper, we extend our method of using selectors from the web for noun sense disambiguation into more robust method of disambiguatingwords of all parts of speech.
</prevsent>
<prevsent>after brief back ground on selectors and related work, we explain the acquisition and empirical application of selectors from nouns, verbs, adjectives, pronouns/proper nouns, and adverbs.
</prevsent>
</prevsection>
<citsent citstr=" W07-2006 ">
finally, results are presented from the semeval-2007 coarse grained all-wordstask (navigli et al, 2007), <papid> W07-2006 </papid>and we explore the influence of various types of selectors on the algorithm in order to draw insight for future improvement of web-based methods.</citsent>
<aftsection>
<nextsent>in this section we describe related research in selectors and solving the problem of word sense disambiguation (wsd).
</nextsent>
<nextsent>specifically, two types of wsd research are examined: works that used the web in direct manner, and works which applied similarity or relatedness measure.
</nextsent>
<nextsent>2.1 selectors.
</nextsent>
<nextsent>the term selector comes from (lin, 1997), <papid> P97-1009 </papid>and refers to word which can take the place of another given word within the same local context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1017">
<title id=" W09-2405.xml">using web selectors for the disambiguation of all words </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>specifically, two types of wsd research are examined: works that used the web in direct manner, and works which applied similarity or relatedness measure.
</prevsent>
<prevsent>2.1 selectors.
</prevsent>
</prevsection>
<citsent citstr=" P97-1009 ">
the term selector comes from (lin, 1997), <papid> P97-1009 </papid>and refers to word which can take the place of another given word within the same local context.</citsent>
<aftsection>
<nextsent>although 28 lin searched dependency relationship database in order to match local context, it is not yet possible to parse dependency relationships of the entire web.
</nextsent>
<nextsent>in turn, one must search for text as local context.
</nextsent>
<nextsent>for example, in the sentence below, the local context for strikers?
</nextsent>
<nextsent>would be composed of he addressed the?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1018">
<title id=" W09-2405.xml">using web selectors for the disambiguation of all words </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>and word sense?
</prevsent>
<prevsent>are used interchangeably throughout this paper.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
this idea of distinguishing similarity and relatedness has an extensive history (rada et al, 1989; resnik, 1999; patwardhan et al, 2003; budanitsky and hirst, 2006), <papid> J06-1003 </papid>but most algorithms only find use for one or the other.</citsent>
<aftsection>
<nextsent>2.2 related word sense disambiguation.
</nextsent>
<nextsent>a key aspect of using selectors for disambiguation is the inclusion of context in the web search queries.
</nextsent>
<nextsent>this was done in works by (martinez et al, 2006) and (yuret, 2007), <papid> W07-2044 </papid>which substituted relatives or similar words in place of the target word within agiven context.</nextsent>
<nextsent>the context, restricted with window size, helped to limit the results from the web.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1019">
<title id=" W09-2405.xml">using web selectors for the disambiguation of all words </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 related word sense disambiguation.
</prevsent>
<prevsent>a key aspect of using selectors for disambiguation is the inclusion of context in the web search queries.
</prevsent>
</prevsection>
<citsent citstr=" W07-2044 ">
this was done in works by (martinez et al, 2006) and (yuret, 2007), <papid> W07-2044 </papid>which substituted relatives or similar words in place of the target word within agiven context.</citsent>
<aftsection>
<nextsent>the context, restricted with window size, helped to limit the results from the web.
</nextsent>
<nextsent>these works followed (mihalcea and moldovan,1999; agirre et al, 2001) in that queries were constructed through the use of knowledge-base, filling the queries with pre-chosen words.
</nextsent>
<nextsent>we also use context in the web search, but we acquire words matching wild card in the search rather than incorporate knowledge-base to construct queries with pre-chosen relatives.
</nextsent>
<nextsent>consequently, the later half ofour algorithm uses knowledge-base through similarity and relatedness measures.some recent works have used similarity or relatedness measures to assist with wsd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1020">
<title id=" W09-2405.xml">using web selectors for the disambiguation of all words </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>to apply selectors in disambiguation, similarity and relatedness measures are used to compare the selectors with the target word.
</prevsent>
<prevsent>we incorporate the use of few previously defined measures over wordnet(miller et al, 1993).
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
the wordnet::similarity package provides flexible implementation of many ofthese measures (pedersen et al, 2004).<papid> N04-3012 </papid></citsent>
<aftsection>
<nextsent>we configured wordnet::similarity for wordnet version 2.1, 1http://developer.yahoo.com/search/ he addressed the * at the rally crowd:1 he addressed * at the rally student:1, supporter:2 he addressed * at the council:1, muslim:1, saturday:1, ugandan:1, analyst:2, attendee:20, audience:3, class:2, consumer:1, council:1, delegate:64, diplomat:2, employee:2, engineer:1, fan:1, farmer:1, globalization:1, graduate:5, guest:2, hundred:3, investor:1, issue:1, journalist:9, lawmaker:11, legislator:1, member:6, midshipman:1, mourner:1, official:2, parliamentarian:1, participant:17, patient:1, physician:18, reporter:8, sailor:1, secretary:1, soldier:3, staff:3, student:20, supporter:8, thousand:3, today:2, trader:1, troops:2, visitor:1, worker:1 he * the strikers at the treat:2 he * the strikers at get:1, keep:1, price:1, treat:1 table 1: lists of selectors for the target words striker?
</nextsent>
<nextsent>and address?
</nextsent>
<nextsent>returned by corresponding web queries.the same version used to annotate our chosen experimental corpus.a relatedness measure was used with context selectors, and we chose the adapted lesk algorithm(banerjee and pedersen, 2002).
</nextsent>
<nextsent>an important characteristic of this measure is that it can handle multiple parts of speech.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1022">
<title id=" W09-2405.xml">using web selectors for the disambiguation of all words </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>then, we delve into analyzing the acquired selectors and the influence of each type of context selector in order to gain insights into future related work.
</prevsent>
<prevsent>31 blrand med ws blmfs 53.43 70.21 76.02 78.89 table 2: results as f1 values of our system, ws,compared with baselines: random, blrand; most frequent sense, blmfs ; median system performance at se meval07, med.
</prevsent>
</prevsection>
<citsent citstr=" W07-2097 ">
upv-wsd nus-pt ssi 78.63 82.50 83.21 table 3: results as f1 values of top performing systems for the semeval07 task07 (upv = (buscaldi and rosso, 2007), <papid> W07-2097 </papid>nus-pt = (chan et al, 2007), <papid> W07-2054 </papid>and ssi = task organizers system (navigli and velardi, 2005)).</citsent>
<aftsection>
<nextsent>4.1 evaluating all words.
</nextsent>
<nextsent>in this section, we seek to apply the algorithm to all instances of the testing corpus in order to compare with baselines and other disambiguation algorithms.
</nextsent>
<nextsent>unless stated otherwise, all results are presented asf1 values, where f1 = 2?
</nextsent>
<nextsent>prp+r . for semeval2007,all systems performed better than the random base line of 53.43%, but only 4 of 13 systems achieved an f1 score higher than the mfs baseline of 78.89% (navigli et al, 2007).<papid> W07-2006 </papid>table 2 lists the results of applying the generalized web selector algorithm described in this paper in straight-forward manner, such that all scale(t ) are set to 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1023">
<title id=" W09-2405.xml">using web selectors for the disambiguation of all words </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>then, we delve into analyzing the acquired selectors and the influence of each type of context selector in order to gain insights into future related work.
</prevsent>
<prevsent>31 blrand med ws blmfs 53.43 70.21 76.02 78.89 table 2: results as f1 values of our system, ws,compared with baselines: random, blrand; most frequent sense, blmfs ; median system performance at se meval07, med.
</prevsent>
</prevsection>
<citsent citstr=" W07-2054 ">
upv-wsd nus-pt ssi 78.63 82.50 83.21 table 3: results as f1 values of top performing systems for the semeval07 task07 (upv = (buscaldi and rosso, 2007), <papid> W07-2097 </papid>nus-pt = (chan et al, 2007), <papid> W07-2054 </papid>and ssi = task organizers system (navigli and velardi, 2005)).</citsent>
<aftsection>
<nextsent>4.1 evaluating all words.
</nextsent>
<nextsent>in this section, we seek to apply the algorithm to all instances of the testing corpus in order to compare with baselines and other disambiguation algorithms.
</nextsent>
<nextsent>unless stated otherwise, all results are presented asf1 values, where f1 = 2?
</nextsent>
<nextsent>prp+r . for semeval2007,all systems performed better than the random base line of 53.43%, but only 4 of 13 systems achieved an f1 score higher than the mfs baseline of 78.89% (navigli et al, 2007).<papid> W07-2006 </papid>table 2 lists the results of applying the generalized web selector algorithm described in this paper in straight-forward manner, such that all scale(t ) are set to 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1028">
<title id=" W10-0402.xml">scientific authoring support a tool to navigate in typed citation graphs </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>moreover, once such measure is formalized and standardized e.g. for science evaluation, it can be exploited to tuneup statistics.
</prevsent>
<prevsent>since the first proposal of the science citation index (garfield, 1955), it has also provoked criticism.
</prevsent>
</prevsection>
<citsent citstr=" W04-3113 ">
in the bibliometrics and computational linguistics literature, many proposals are available on how citations could be further classified by careful analysis of citation sentences and context (garfield, 1965; garzone, 1996; mercer and dimarco, 2004; <papid> W04-3113 </papid>teufel et al, 2006; <papid> W06-1613 </papid>born mann and daniel, 2008).</citsent>
<aftsection>
<nextsent>the number of different classes proposed varies from 3 to 35.
</nextsent>
<nextsent>different authors try to identify dimensions and mutually exclusive classes, but the more classes schema contains, the more difficult becomes the automatic classification.
</nextsent>
<nextsent>the focus of our paper is to combine automatic classification approaches with tool that supports scientists in graphically navigating through typed citation graphs (tcg).
</nextsent>
<nextsent>such tcgs can be generated 7by augmenting simple citation graph with information synonymously called citation function (teufel et al, 2006), <papid> W06-1613 </papid>citation relation (mercer and dimarco, 2004) <papid> W04-3113 </papid>or citation sentiment, forming the labels of the graphs edges.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1029">
<title id=" W10-0402.xml">scientific authoring support a tool to navigate in typed citation graphs </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>moreover, once such measure is formalized and standardized e.g. for science evaluation, it can be exploited to tuneup statistics.
</prevsent>
<prevsent>since the first proposal of the science citation index (garfield, 1955), it has also provoked criticism.
</prevsent>
</prevsection>
<citsent citstr=" W06-1613 ">
in the bibliometrics and computational linguistics literature, many proposals are available on how citations could be further classified by careful analysis of citation sentences and context (garfield, 1965; garzone, 1996; mercer and dimarco, 2004; <papid> W04-3113 </papid>teufel et al, 2006; <papid> W06-1613 </papid>born mann and daniel, 2008).</citsent>
<aftsection>
<nextsent>the number of different classes proposed varies from 3 to 35.
</nextsent>
<nextsent>different authors try to identify dimensions and mutually exclusive classes, but the more classes schema contains, the more difficult becomes the automatic classification.
</nextsent>
<nextsent>the focus of our paper is to combine automatic classification approaches with tool that supports scientists in graphically navigating through typed citation graphs (tcg).
</nextsent>
<nextsent>such tcgs can be generated 7by augmenting simple citation graph with information synonymously called citation function (teufel et al, 2006), <papid> W06-1613 </papid>citation relation (mercer and dimarco, 2004) <papid> W04-3113 </papid>or citation sentiment, forming the labels of the graphs edges.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1033">
<title id=" W10-0402.xml">scientific authoring support a tool to navigate in typed citation graphs </title>
<section> data preparation and automatic.  </section>
<citcontext>
<prevsection>
<prevsent>we compare with related work in section 5.
</prevsent>
<prevsent>finally, we conclude and give an outlook to future work in section 6.
</prevsent>
</prevsection>
<citsent citstr=" L08-1005 ">
citation type classification our corpus is based on 6300 electronically-available papers, subset (published 2002-2008) of the acl anthology (bird et al, 2008), <papid> L08-1005 </papid>comprehensive collection of scientific conference and workshop papers in the area of computational linguistics and language technology.</citsent>
<aftsection>
<nextsent>the overall workflow of the employed tools and data is shown in fig.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>we ran the open source tool parscit (councill etal., 2008) <papid> L08-1291 </papid>to extract references lists and corresponding citation sentences from raw paper texts.</nextsent>
<nextsent>to build the citation graph, we used the levenshtein distance(levenshtein, 1966) to find and match titles and authors of identical papers yet tolerating spelling and pdf extraction errors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1034">
<title id=" W10-0402.xml">scientific authoring support a tool to navigate in typed citation graphs </title>
<section> data preparation and automatic.  </section>
<citcontext>
<prevsection>
<prevsent>the overall workflow of the employed tools and data is shown in fig.
</prevsent>
<prevsent>1.
</prevsent>
</prevsection>
<citsent citstr=" L08-1291 ">
we ran the open source tool parscit (councill etal., 2008) <papid> L08-1291 </papid>to extract references lists and corresponding citation sentences from raw paper texts.</citsent>
<aftsection>
<nextsent>to build the citation graph, we used the levenshtein distance(levenshtein, 1966) to find and match titles and authors of identical papers yet tolerating spelling and pdf extraction errors.
</nextsent>
<nextsent>to increase robustness, publication years were not considered as they would hinder matches for figure 1: workflow from acl anthology data (top) to citation graph navigation applet and citation sentence viewer (bottom) delayed journal publications.
</nextsent>
<nextsent>generation of the graph edges, i.e. matching of papers and reference strings, is performed by means of the acl id, unique identifier for each paper, available for the pdf (source nodes of references) and bibtex files (targets of references).
</nextsent>
<nextsent>we evaluated the generated graph against the onethat was corrected manually by the acl anthology network (aan) group (radev et al, 2009) <papid> W09-3607 </papid>and found that 10821 citation links were shared between both and can be considered correct1.3883 additional ones were in the aan but not recognized by us, the other way round, 1021 discovered by us were not in the aan.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1035">
<title id=" W10-0402.xml">scientific authoring support a tool to navigate in typed citation graphs </title>
<section> data preparation and automatic.  </section>
<citcontext>
<prevsection>
<prevsent>to increase robustness, publication years were not considered as they would hinder matches for figure 1: workflow from acl anthology data (top) to citation graph navigation applet and citation sentence viewer (bottom) delayed journal publications.
</prevsent>
<prevsent>generation of the graph edges, i.e. matching of papers and reference strings, is performed by means of the acl id, unique identifier for each paper, available for the pdf (source nodes of references) and bibtex files (targets of references).
</prevsent>
</prevsection>
<citsent citstr=" W09-3607 ">
we evaluated the generated graph against the onethat was corrected manually by the acl anthology network (aan) group (radev et al, 2009) <papid> W09-3607 </papid>and found that 10821 citation links were shared between both and can be considered correct1.3883 additional ones were in the aan but not recognized by us, the other way round, 1021 discovered by us were not in the aan.</citsent>
<aftsection>
<nextsent>in addition, the publication bases were not identical.
</nextsent>
<nextsent>the anthology network data ends in february 2007 but covers years before 2002, while our data covers 2002-2008 inclusively.
</nextsent>
<nextsent>given the fact that our graph is computed fully automatically, the result can be considered very good.
</nextsent>
<nextsent>in the next step, we augmented the citation graphby types for each edge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1043">
<title id=" W09-2411.xml">semeval2010 task 1 coreference resolution in multiple languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in these systems, there is need to identify the different pieces of information that refer to the same discourse entity in order to produce coherent and fluent summaries, disambiguate the references to an entity, and solve anaphoric pronouns.
</prevsent>
<prevsent>coreference is an inherently complex phenomenon.
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
some of the limitations of the traditional rule based approaches (mitkov, 1998) <papid> P98-2143 </papid>could be over come by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.</citsent>
<aftsection>
<nextsent>this task will promote the development of linguistic resources annotated corpora1?
</nextsent>
<nextsent>and ma chine-learning techniques oriented to coreference resolution.
</nextsent>
<nextsent>in particular, we aim to evaluate and compare coreference resolution systems in multilingual context, including catalan, english, and spanish languages, and by means of two different evaluation metrics.
</nextsent>
<nextsent>by setting up multilingual scenario, we can explore to what extent it is possible to implement general system that is portable to the three languages, how much language-specific tuning is necessary, and the significant differences between romance languages and english, as well as those between two closely related languages such as spanish and catalan.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1044">
<title id=" W09-2411.xml">semeval2010 task 1 coreference resolution in multiple languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by setting up multilingual scenario, we can explore to what extent it is possible to implement general system that is portable to the three languages, how much language-specific tuning is necessary, and the significant differences between romance languages and english, as well as those between two closely related languages such as spanish and catalan.
</prevsent>
<prevsent>besides, we expect to gain some useful insight into the development of multilingual nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" H05-1004 ">
as far as the evaluation is concerned, by employing b-cubed (bagga and baldwin, 1998) and ceaf (luo, 2005) <papid> H05-1004 </papid>algorithms we can consider both the advantages and drawbacks of using one or the other scoring metric.</citsent>
<aftsection>
<nextsent>for comparison purposes, the muc score will also be reported.
</nextsent>
<nextsent>among others, we are interested in the following questions: which evaluation metric provides more accurate picture of the accuracy of the system performance?
</nextsent>
<nextsent>is there strong correlation between them?
</nextsent>
<nextsent>can 1 corpora annotated with coreference are scarce, especially for.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1046">
<title id=" W09-2411.xml">semeval2010 task 1 coreference resolution in multiple languages </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>for the purpose of the current task, we have selected two of them ? b-cubed and ceaf ? as the most appropriate ones.
</prevsent>
<prevsent>in what follows we justify our choice.
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
the muc scoring algorithm (vilain et al, 1995) <papid> M95-1005 </papid>has been the most widely used for at least two rea sons.</citsent>
<aftsection>
<nextsent>firstly, the muc corpora and the muc scorer were the first available systems.
</nextsent>
<nextsent>secondly, the muc scorer is easy to understand and implement.
</nextsent>
<nextsent>however, this metric has two major weak nesses: (i) it does not give any credit to the correct identification of singleton entities (chains consisting of one single mention), and (ii) it intrinsically favors systems that produce fewer coreference chains, which may result in higher f-measures for worse systems.
</nextsent>
<nextsent>a second well-known scoring algorithm, the ace value (nist, 2003), owes its popularity to the ace evaluation campaign.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1050">
<title id=" W09-2411.xml">semeval2010 task 1 coreference resolution in multiple languages </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>the former does not look at the links produced by system as the muc algorithm does, but looks at the presence/absence of mentions for each entity in the system output.
</prevsent>
<prevsent>precision and recall numbers are computed for each mention, and the average gives the final precision and recall numbers.
</prevsent>
</prevsection>
<citsent citstr=" D08-1067 ">
ceaf (luo, 2005) <papid> H05-1004 </papid>is novel metric for evaluating coreference resolution that has already been used in some published papers (ng, 2008; <papid> D08-1067 </papid>denis and baldridge, 2008).<papid> D08-1069 </papid></citsent>
<aftsection>
<nextsent>it mainly differs from cubed in that it finds the best one-to-one entity alignment between the gold and system responses before computing precision and recall.
</nextsent>
<nextsent>the best mapping is that which maximizes the similarity over pairs of chains.
</nextsent>
<nextsent>the ceaf measure has two variants: mention-based, and an entity-based one.
</nextsent>
<nextsent>while the former scores the similarity of two chains as the absolute number of common mentions between them, the latter scores the relative number of common mentions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1051">
<title id=" W09-2411.xml">semeval2010 task 1 coreference resolution in multiple languages </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>the former does not look at the links produced by system as the muc algorithm does, but looks at the presence/absence of mentions for each entity in the system output.
</prevsent>
<prevsent>precision and recall numbers are computed for each mention, and the average gives the final precision and recall numbers.
</prevsent>
</prevsection>
<citsent citstr=" D08-1069 ">
ceaf (luo, 2005) <papid> H05-1004 </papid>is novel metric for evaluating coreference resolution that has already been used in some published papers (ng, 2008; <papid> D08-1067 </papid>denis and baldridge, 2008).<papid> D08-1069 </papid></citsent>
<aftsection>
<nextsent>it mainly differs from cubed in that it finds the best one-to-one entity alignment between the gold and system responses before computing precision and recall.
</nextsent>
<nextsent>the best mapping is that which maximizes the similarity over pairs of chains.
</nextsent>
<nextsent>the ceaf measure has two variants: mention-based, and an entity-based one.
</nextsent>
<nextsent>while the former scores the similarity of two chains as the absolute number of common mentions between them, the latter scores the relative number of common mentions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1053">
<title id=" W10-0705.xml">rating computer generated questions with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>final output, or to evaluate technologies that generate natural language.
</prevsent>
<prevsent>we discuss the question rating scheme we developed, assess the quality of the ratings that we gathered through amazon mechanical turk, and show evidence that these ratings can be used to im prove question generation.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
this paper discusses the use of amazon mechanical turk (mturk) to rate computer-generated reading comprehension questions about wikipedia arti cles.we have developed question generation system (heilman and smith, 2009; heilman and smith, 2010) that uses the overgenerate-and-rank paradigm (langkilde and knight, 1998).<papid> P98-1116 </papid></citsent>
<aftsection>
<nextsent>in thethe overgenerate-and-rank approach, many system generated outputs are ranked in order to select higher quality outputs.
</nextsent>
<nextsent>while the approach has had considerable success in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>walker et al, 2001), <papid> N01-1003 </papid>it often requires human labels on system output for the purpose of learning to rank.</nextsent>
<nextsent>we employ mturk to reduce the time and cost of acquiring these labels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1055">
<title id=" W10-0705.xml">rating computer generated questions with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper discusses the use of amazon mechanical turk (mturk) to rate computer-generated reading comprehension questions about wikipedia arti cles.we have developed question generation system (heilman and smith, 2009; heilman and smith, 2010) that uses the overgenerate-and-rank paradigm (langkilde and knight, 1998).<papid> P98-1116 </papid></prevsent>
<prevsent>in thethe overgenerate-and-rank approach, many system generated outputs are ranked in order to select higher quality outputs.</prevsent>
</prevsection>
<citsent citstr=" N01-1003 ">
while the approach has had considerable success in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>walker et al, 2001), <papid> N01-1003 </papid>it often requires human labels on system output for the purpose of learning to rank.</citsent>
<aftsection>
<nextsent>we employ mturk to reduce the time and cost of acquiring these labels.
</nextsent>
<nextsent>for many problems, large labeled datasets do notexist.
</nextsent>
<nextsent>one alternative is to build rule-based systems, but it is often difficult and time-consuming to accurately encode relevant linguistic knowledge in rules.
</nextsent>
<nextsent>another alternative, unsupervised or semi supervised learning, usually requires clever formulations of bias that guide the learning process (car roll and charniak, 1992; yarowsky, 1995); <papid> P95-1026 </papid>such intuitions are not always available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1056">
<title id=" W10-0705.xml">rating computer generated questions with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for many problems, large labeled datasets do notexist.
</prevsent>
<prevsent>one alternative is to build rule-based systems, but it is often difficult and time-consuming to accurately encode relevant linguistic knowledge in rules.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
another alternative, unsupervised or semi supervised learning, usually requires clever formulations of bias that guide the learning process (car roll and charniak, 1992; yarowsky, 1995); <papid> P95-1026 </papid>such intuitions are not always available.</citsent>
<aftsection>
<nextsent>thus, small, application-specific labeled datasets, which can be cheaply constructed using mturk, may provide considerable benefits by enabling the use of supervised learning.
</nextsent>
<nextsent>in addition to using mturk ratings to train learned ranking component, we could also usemturk ratings to evaluate the final top-ranked out put of our system.
</nextsent>
<nextsent>more generally, mturk can be auseful evaluation tool for systems that output natural language (e.g., systems for natural language generation, summarization, translation).
</nextsent>
<nextsent>for example,callison-burch (2009) used mturk to evaluate machine translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1057">
<title id=" W10-0705.xml">rating computer generated questions with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more generally, mturk can be auseful evaluation tool for systems that output natural language (e.g., systems for natural language generation, summarization, translation).
</prevsent>
<prevsent>for example,callison-burch (2009) used mturk to evaluate machine translations.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
mturk facilitates the efficient measurement and understanding of errors made by such technologies, and could be used to complement automatic evaluation metrics such as bleu (pap ineni et al, 2002) <papid> P02-1040 </papid>and rouge (lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>it is true that, for our task, mturk workers annotate computer-generated rather than human generated natural language.
</nextsent>
<nextsent>thus, the data willnot be as generally useful as other types of annotations, such as parse trees, which could be used to build general purpose syntactic parsers.
</nextsent>
<nextsent>however, for the reasons described above, we believe the use of mturk to rate computer-generated output can be useful for the training, development, and evaluation of language technologies.the remainder of the paper is organized as follows: 2 and 3 briefly describe the question generation system and corpora used in our experiments.4 provides the details of our rating scheme.
</nextsent>
<nextsent>5 discusses the quantity, cost, speed, and quality of the ratings we gathered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1058">
<title id=" W10-0705.xml">rating computer generated questions with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more generally, mturk can be auseful evaluation tool for systems that output natural language (e.g., systems for natural language generation, summarization, translation).
</prevsent>
<prevsent>for example,callison-burch (2009) used mturk to evaluate machine translations.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
mturk facilitates the efficient measurement and understanding of errors made by such technologies, and could be used to complement automatic evaluation metrics such as bleu (pap ineni et al, 2002) <papid> P02-1040 </papid>and rouge (lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>it is true that, for our task, mturk workers annotate computer-generated rather than human generated natural language.
</nextsent>
<nextsent>thus, the data willnot be as generally useful as other types of annotations, such as parse trees, which could be used to build general purpose syntactic parsers.
</nextsent>
<nextsent>however, for the reasons described above, we believe the use of mturk to rate computer-generated output can be useful for the training, development, and evaluation of language technologies.the remainder of the paper is organized as follows: 2 and 3 briefly describe the question generation system and corpora used in our experiments.4 provides the details of our rating scheme.
</nextsent>
<nextsent>5 discusses the quantity, cost, speed, and quality of the ratings we gathered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1059">
<title id=" W10-0705.xml">rating computer generated questions with mechanical turk </title>
<section> quantity, cost, speed, and quality.  </section>
<citcontext>
<prevsection>
<prevsent>the correlation between the independent judges ratings and the mturk workers was =0.74.
</prevsent>
<prevsent>these fairly strong positive correlations between the mturk ratings and the two human judges provide evidence that the rating scheme is consistent and well-defined.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
the results also agree with snow et al (2008), <papid> D08-1027 </papid>who found that aggregating labels from 3 to 7 workers often provides expert levels of agreement.</citsent>
<aftsection>
<nextsent>interestingly, the agreement between the two human raters was somewhat lower (r = 0.65), suggesting that aggregated labels from crowd of mturk workers can be more reliable than individual humans.5
</nextsent>
<nextsent>ranking in this section, we provide some preliminary results to demonstrate that mturk ratings can be used for learning to rank qg output.first, we briefly characterize the quality of unranked output.
</nextsent>
<nextsent>figure 3 shows histogram of themean mturk ratings for the 1,195 questions, showing that only relatively small fraction of the questions created by the over generating steps of our system are acceptable: 12.9% when using 3.5 as the threshold for acceptability.
</nextsent>
<nextsent>however, ranking can lead to substantially higher levels of quality in the top-ranked questions, which 5we also converted the ratings into binary values based on whether they exceeded threshold of 3.5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1060">
<title id=" W10-0704.xml">semi supervised word alignment with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the quality of word alignment is usually measured by aer, which is loosely related to bleu score (lopez and resnik, 2006).
</prevsent>
<prevsent>there has been research on utilizing manually aligned corpus to assist automatic word alignment, and obtains encouraging results on alignment error rate.
</prevsent>
</prevsection>
<citsent citstr=" P06-1009 ">
(callison-burch et al, 2004; blunsom and cohn, 2006; <papid> P06-1009 </papid>fraser and marcu, 2006; <papid> P06-1097 </papid>niehues and vogel, 2008; <papid> W08-0303 </papid>taskar et al, 2005; <papid> H05-1010 </papid>liu et al, 2005; <papid> P05-1057 </papid>moore, 2005).<papid> H05-1011 </papid></citsent>
<aftsection>
<nextsent>however, how to obtain large amount of alignments with good quality is problematic.
</nextsent>
<nextsent>labeling word-aligned parallel corpora requires significant amount of labor.
</nextsent>
<nextsent>in this paper we explore the possibility of using amazon mechanical turk (mturk) to obtain manual word alignment faster, cheaper, with high quality.
</nextsent>
<nextsent>crowdsourcing is way of getting random labor force on-line with low cost.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1061">
<title id=" W10-0704.xml">semi supervised word alignment with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the quality of word alignment is usually measured by aer, which is loosely related to bleu score (lopez and resnik, 2006).
</prevsent>
<prevsent>there has been research on utilizing manually aligned corpus to assist automatic word alignment, and obtains encouraging results on alignment error rate.
</prevsent>
</prevsection>
<citsent citstr=" P06-1097 ">
(callison-burch et al, 2004; blunsom and cohn, 2006; <papid> P06-1009 </papid>fraser and marcu, 2006; <papid> P06-1097 </papid>niehues and vogel, 2008; <papid> W08-0303 </papid>taskar et al, 2005; <papid> H05-1010 </papid>liu et al, 2005; <papid> P05-1057 </papid>moore, 2005).<papid> H05-1011 </papid></citsent>
<aftsection>
<nextsent>however, how to obtain large amount of alignments with good quality is problematic.
</nextsent>
<nextsent>labeling word-aligned parallel corpora requires significant amount of labor.
</nextsent>
<nextsent>in this paper we explore the possibility of using amazon mechanical turk (mturk) to obtain manual word alignment faster, cheaper, with high quality.
</nextsent>
<nextsent>crowdsourcing is way of getting random labor force on-line with low cost.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1062">
<title id=" W10-0704.xml">semi supervised word alignment with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the quality of word alignment is usually measured by aer, which is loosely related to bleu score (lopez and resnik, 2006).
</prevsent>
<prevsent>there has been research on utilizing manually aligned corpus to assist automatic word alignment, and obtains encouraging results on alignment error rate.
</prevsent>
</prevsection>
<citsent citstr=" W08-0303 ">
(callison-burch et al, 2004; blunsom and cohn, 2006; <papid> P06-1009 </papid>fraser and marcu, 2006; <papid> P06-1097 </papid>niehues and vogel, 2008; <papid> W08-0303 </papid>taskar et al, 2005; <papid> H05-1010 </papid>liu et al, 2005; <papid> P05-1057 </papid>moore, 2005).<papid> H05-1011 </papid></citsent>
<aftsection>
<nextsent>however, how to obtain large amount of alignments with good quality is problematic.
</nextsent>
<nextsent>labeling word-aligned parallel corpora requires significant amount of labor.
</nextsent>
<nextsent>in this paper we explore the possibility of using amazon mechanical turk (mturk) to obtain manual word alignment faster, cheaper, with high quality.
</nextsent>
<nextsent>crowdsourcing is way of getting random labor force on-line with low cost.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1063">
<title id=" W10-0704.xml">semi supervised word alignment with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the quality of word alignment is usually measured by aer, which is loosely related to bleu score (lopez and resnik, 2006).
</prevsent>
<prevsent>there has been research on utilizing manually aligned corpus to assist automatic word alignment, and obtains encouraging results on alignment error rate.
</prevsent>
</prevsection>
<citsent citstr=" H05-1010 ">
(callison-burch et al, 2004; blunsom and cohn, 2006; <papid> P06-1009 </papid>fraser and marcu, 2006; <papid> P06-1097 </papid>niehues and vogel, 2008; <papid> W08-0303 </papid>taskar et al, 2005; <papid> H05-1010 </papid>liu et al, 2005; <papid> P05-1057 </papid>moore, 2005).<papid> H05-1011 </papid></citsent>
<aftsection>
<nextsent>however, how to obtain large amount of alignments with good quality is problematic.
</nextsent>
<nextsent>labeling word-aligned parallel corpora requires significant amount of labor.
</nextsent>
<nextsent>in this paper we explore the possibility of using amazon mechanical turk (mturk) to obtain manual word alignment faster, cheaper, with high quality.
</nextsent>
<nextsent>crowdsourcing is way of getting random labor force on-line with low cost.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1064">
<title id=" W10-0704.xml">semi supervised word alignment with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the quality of word alignment is usually measured by aer, which is loosely related to bleu score (lopez and resnik, 2006).
</prevsent>
<prevsent>there has been research on utilizing manually aligned corpus to assist automatic word alignment, and obtains encouraging results on alignment error rate.
</prevsent>
</prevsection>
<citsent citstr=" P05-1057 ">
(callison-burch et al, 2004; blunsom and cohn, 2006; <papid> P06-1009 </papid>fraser and marcu, 2006; <papid> P06-1097 </papid>niehues and vogel, 2008; <papid> W08-0303 </papid>taskar et al, 2005; <papid> H05-1010 </papid>liu et al, 2005; <papid> P05-1057 </papid>moore, 2005).<papid> H05-1011 </papid></citsent>
<aftsection>
<nextsent>however, how to obtain large amount of alignments with good quality is problematic.
</nextsent>
<nextsent>labeling word-aligned parallel corpora requires significant amount of labor.
</nextsent>
<nextsent>in this paper we explore the possibility of using amazon mechanical turk (mturk) to obtain manual word alignment faster, cheaper, with high quality.
</nextsent>
<nextsent>crowdsourcing is way of getting random labor force on-line with low cost.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1065">
<title id=" W10-0704.xml">semi supervised word alignment with mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the quality of word alignment is usually measured by aer, which is loosely related to bleu score (lopez and resnik, 2006).
</prevsent>
<prevsent>there has been research on utilizing manually aligned corpus to assist automatic word alignment, and obtains encouraging results on alignment error rate.
</prevsent>
</prevsection>
<citsent citstr=" H05-1011 ">
(callison-burch et al, 2004; blunsom and cohn, 2006; <papid> P06-1009 </papid>fraser and marcu, 2006; <papid> P06-1097 </papid>niehues and vogel, 2008; <papid> W08-0303 </papid>taskar et al, 2005; <papid> H05-1010 </papid>liu et al, 2005; <papid> P05-1057 </papid>moore, 2005).<papid> H05-1011 </papid></citsent>
<aftsection>
<nextsent>however, how to obtain large amount of alignments with good quality is problematic.
</nextsent>
<nextsent>labeling word-aligned parallel corpora requires significant amount of labor.
</nextsent>
<nextsent>in this paper we explore the possibility of using amazon mechanical turk (mturk) to obtain manual word alignment faster, cheaper, with high quality.
</nextsent>
<nextsent>crowdsourcing is way of getting random labor force on-line with low cost.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1066">
<title id=" W10-0704.xml">semi supervised word alignment with mechanical turk </title>
<section> utilizing the manual alignments.  </section>
<citcontext>
<prevsection>
<prevsent>figure 2 shows the comparison of partial alignments (the bold link) and full alignments (the dashed and the bold links).
</prevsent>
<prevsent>in the example, if full alignment is given, we can assert 2005 is only aligned to 2005#, not to {or , but we cannot do that if only partial alignment is given.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
in this paper we experiment with novel method which uses the partial alignment to constraint the em algorithm in the parameter estimation of ibm models.ibm models (brown et. al., 1993) <papid> J93-2003 </papid>are series of generative models for word alignment.</citsent>
<aftsection>
<nextsent>giza++ (och and ney, 2003) <papid> J03-1002 </papid>is the most widely used implementation of ibm models and hmm (vogel et al, 1996) <papid> C96-2141 </papid>where em algorithm is employed to estimate the model parameters.</nextsent>
<nextsent>in the e-step, it is possible to obtain sufficient statistics from all possible alignments for simple models such as model 1 and model 2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1067">
<title id=" W10-0704.xml">semi supervised word alignment with mechanical turk </title>
<section> utilizing the manual alignments.  </section>
<citcontext>
<prevsection>
<prevsent>in the example, if full alignment is given, we can assert 2005 is only aligned to 2005#, not to {or , but we cannot do that if only partial alignment is given.
</prevsent>
<prevsent>in this paper we experiment with novel method which uses the partial alignment to constraint the em algorithm in the parameter estimation of ibm models.ibm models (brown et. al., 1993) <papid> J93-2003 </papid>are series of generative models for word alignment.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
giza++ (och and ney, 2003) <papid> J03-1002 </papid>is the most widely used implementation of ibm models and hmm (vogel et al, 1996) <papid> C96-2141 </papid>where em algorithm is employed to estimate the model parameters.</citsent>
<aftsection>
<nextsent>in the e-step, it is possible to obtain sufficient statistics from all possible alignments for simple models such as model 1 and model 2.
</nextsent>
<nextsent>meanwhile for fertility-based models such as model 3, 4, 5, enumerating all possible alignments is np-complete.
</nextsent>
<nextsent>in practice, we use simpler models such as hmm or model 2 to generate center alignment?
</nextsent>
<nextsent>and then try to find better alignments among the neighbors of it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1068">
<title id=" W10-0704.xml">semi supervised word alignment with mechanical turk </title>
<section> utilizing the manual alignments.  </section>
<citcontext>
<prevsection>
<prevsent>in the example, if full alignment is given, we can assert 2005 is only aligned to 2005#, not to {or , but we cannot do that if only partial alignment is given.
</prevsent>
<prevsent>in this paper we experiment with novel method which uses the partial alignment to constraint the em algorithm in the parameter estimation of ibm models.ibm models (brown et. al., 1993) <papid> J93-2003 </papid>are series of generative models for word alignment.</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
giza++ (och and ney, 2003) <papid> J03-1002 </papid>is the most widely used implementation of ibm models and hmm (vogel et al, 1996) <papid> C96-2141 </papid>where em algorithm is employed to estimate the model parameters.</citsent>
<aftsection>
<nextsent>in the e-step, it is possible to obtain sufficient statistics from all possible alignments for simple models such as model 1 and model 2.
</nextsent>
<nextsent>meanwhile for fertility-based models such as model 3, 4, 5, enumerating all possible alignments is np-complete.
</nextsent>
<nextsent>in practice, we use simpler models such as hmm or model 2 to generate center alignment?
</nextsent>
<nextsent>and then try to find better alignments among the neighbors of it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1069">
<title id=" W10-0701.xml">creating speech and language data with amazonrsquos mechanical turk </title>
<section> mechanical turk.  </section>
<citcontext>
<prevsection>
<prevsent>payments are frequently as low as $0.01.
</prevsent>
<prevsent>turkers are free to select whichever hits interest them.], and to disregard hits that they find uninteresting or which they deem pay too little.because of its focus on tasks requiring human intelligence, mechanical turk is obviously applicable to the field of natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
snow et al (2008) <papid> D08-1027 </papid>used mechanical turk to inexpensively collect labels for several nlp tasks including word sense disambiguation, word similarity, textual entailment, and temporal ordering of events.</citsent>
<aftsection>
<nextsent>snow et al. had two exciting findings.
</nextsent>
<nextsent>first, they showed that strong correlation between non-expert and expert annotators can be achieved by combining the judgments of multiple non-experts, for instance by voting on each label using 10 different turkers.
</nextsent>
<nextsent>correlation and accuracy of labeling could be further improved by weighting each turkers vote by calibrating them on small amount of gold standard data created by expert annotators.
</nextsent>
<nextsent>second, they collected staggering number of labels for very small amount of money.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1070">
<title id=" W10-0701.xml">creating speech and language data with amazonrsquos mechanical turk </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>you can do this by measuring the 5http://castingwords.com/ 6http://groups.csail.mit.edu/uid/turkit/ 4inter-annotator agreement of the turkers against experts on small amounts of gold standard data, or by stating what controls you used and what criteria youused to block bad turkers.
</prevsent>
<prevsent>finally, whenever possible you should publish the data that you generate on mechanical turk (and your analysis scripts and hit templates) alongside your paper so that other people can verify it.
</prevsent>
</prevsection>
<citsent citstr=" L08-1307 ">
in the past two years, several papers have published about applying mechanical turk to diverse set of natural language processing tasks, including: creating question-answer sentence pairs (kaisser andlowe, 2008), <papid> L08-1307 </papid>evaluating machine translation quality and crowd souring translations (callison-burch,2009), paraphrasing noun-noun compouds for semeval (butnariu et al, 2009), <papid> W09-2416 </papid>human evaluation oftopic models (chang et al, 2009), and speech transcription (mcgraw et al, 2010; marge et al, 2010a; novotney and callison-burch, 2010a).</citsent>
<aftsection>
<nextsent>others have used mturk for novel research directions like non simulated active learning for nlp tasks such as sentiment classification (hsueh et al, 2009) <papid> W09-1904 </papid>or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (zaidan and callison-burch, 2009).some projects have demonstrated the super scala bility of crowd sourced efforts.</nextsent>
<nextsent>deng et al(2009) used mturk to construct ima genet, an annotated image database containing 3.2 million that are hierarchically categorized using the wordnet ontology (fellbaum, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1071">
<title id=" W10-0701.xml">creating speech and language data with amazonrsquos mechanical turk </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>you can do this by measuring the 5http://castingwords.com/ 6http://groups.csail.mit.edu/uid/turkit/ 4inter-annotator agreement of the turkers against experts on small amounts of gold standard data, or by stating what controls you used and what criteria youused to block bad turkers.
</prevsent>
<prevsent>finally, whenever possible you should publish the data that you generate on mechanical turk (and your analysis scripts and hit templates) alongside your paper so that other people can verify it.
</prevsent>
</prevsection>
<citsent citstr=" W09-2416 ">
in the past two years, several papers have published about applying mechanical turk to diverse set of natural language processing tasks, including: creating question-answer sentence pairs (kaisser andlowe, 2008), <papid> L08-1307 </papid>evaluating machine translation quality and crowd souring translations (callison-burch,2009), paraphrasing noun-noun compouds for semeval (butnariu et al, 2009), <papid> W09-2416 </papid>human evaluation oftopic models (chang et al, 2009), and speech transcription (mcgraw et al, 2010; marge et al, 2010a; novotney and callison-burch, 2010a).</citsent>
<aftsection>
<nextsent>others have used mturk for novel research directions like non simulated active learning for nlp tasks such as sentiment classification (hsueh et al, 2009) <papid> W09-1904 </papid>or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (zaidan and callison-burch, 2009).some projects have demonstrated the super scala bility of crowd sourced efforts.</nextsent>
<nextsent>deng et al(2009) used mturk to construct ima genet, an annotated image database containing 3.2 million that are hierarchically categorized using the wordnet ontology (fellbaum, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1072">
<title id=" W10-0701.xml">creating speech and language data with amazonrsquos mechanical turk </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, whenever possible you should publish the data that you generate on mechanical turk (and your analysis scripts and hit templates) alongside your paper so that other people can verify it.
</prevsent>
<prevsent>in the past two years, several papers have published about applying mechanical turk to diverse set of natural language processing tasks, including: creating question-answer sentence pairs (kaisser andlowe, 2008), <papid> L08-1307 </papid>evaluating machine translation quality and crowd souring translations (callison-burch,2009), paraphrasing noun-noun compouds for semeval (butnariu et al, 2009), <papid> W09-2416 </papid>human evaluation oftopic models (chang et al, 2009), and speech transcription (mcgraw et al, 2010; marge et al, 2010a; novotney and callison-burch, 2010a).</prevsent>
</prevsection>
<citsent citstr=" W09-1904 ">
others have used mturk for novel research directions like non simulated active learning for nlp tasks such as sentiment classification (hsueh et al, 2009) <papid> W09-1904 </papid>or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (zaidan and callison-burch, 2009).some projects have demonstrated the super scala bility of crowd sourced efforts.</citsent>
<aftsection>
<nextsent>deng et al(2009) used mturk to construct ima genet, an annotated image database containing 3.2 million that are hierarchically categorized using the wordnet ontology (fellbaum, 1998).
</nextsent>
<nextsent>because mechanical turk allows researchers to experiment with crowdsourcing by providing small incentives to turkers, other successful crowdsourcing efforts like wikipedia or games with purpose (von ahn and dabbish, 2008) also share something in common with mturk.
</nextsent>
<nextsent>the workshop included shared task in which participants were provided with $100 to spend on mechanical turk experiments.
</nextsent>
<nextsent>participants submitted 1 page proposal in advance describing their intended use of the funds.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1073">
<title id=" W10-0701.xml">creating speech and language data with amazonrsquos mechanical turk </title>
<section> future directions.  </section>
<citcontext>
<prevsection>
<prevsent>the introduction of new large and widely accessible datasets creates whole new areas of research.
</prevsent>
<prevsent>there are many examples of such impact, the most famous of which is the penn treebank (marcus.
</prevsent>
</prevsection>
<citsent citstr=" W09-3607 ">
et al, 1994), which has 2910 citations in google scholar and is the single most cited paper on the acl anthology network (radev et al, 2009).<papid> W09-3607 </papid></citsent>
<aftsection>
<nextsent>other examples include the conll named entity corpus (sang and meulder (2003) with 348 citation son google scholar), the imdb movie reviews sentiment data (pang et al (2002) <papid> W02-1011 </papid>with 894 citations) and the amazon sentiment multi-domain data (blitzer et al.</nextsent>
<nextsent>(2007) with 109 citations) . mturk means that creating similar datasets is now much cheaper and easier than ever before.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1074">
<title id=" W10-0701.xml">creating speech and language data with amazonrsquos mechanical turk </title>
<section> future directions.  </section>
<citcontext>
<prevsection>
<prevsent>there are many examples of such impact, the most famous of which is the penn treebank (marcus.
</prevsent>
<prevsent>et al, 1994), which has 2910 citations in google scholar and is the single most cited paper on the acl anthology network (radev et al, 2009).<papid> W09-3607 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
other examples include the conll named entity corpus (sang and meulder (2003) with 348 citation son google scholar), the imdb movie reviews sentiment data (pang et al (2002) <papid> W02-1011 </papid>with 894 citations) and the amazon sentiment multi-domain data (blitzer et al.</citsent>
<aftsection>
<nextsent>(2007) with 109 citations) . mturk means that creating similar datasets is now much cheaper and easier than ever before.
</nextsent>
<nextsent>it is highly likely that new mturk produced datasets will achieve prominence and have significant impact.
</nextsent>
<nextsent>additionally, the creation of shared data means more comparison and evaluation against previous work.
</nextsent>
<nextsent>progress is made when it can be demonstrated against previous approaches on the same data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1076">
<title id=" W09-2419.xml">semeval2010 task 14 evaluation setting for word sense induction x26 disambiguation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wsi is field of significant value, because it aims to overcome the limitations originated by representing word sen sesas fixed-list of dictionary definitions.
</prevsent>
<prevsent>these limitations of hand-crafted lexicons include the use of general sense definitions, the lack of explicit semantic and topical relations between concepts (agirre etal., 2001), and the inability to reflect the exact content of the context in which target word appears (veronis, 2004).given the significance of wsi, the objective assessment and comparison of wsi methods is crucial.
</prevsent>
</prevsection>
<citsent citstr=" W07-2002 ">
the first effort to evaluate wsi methods under common framework (evaluation schemes &amp; dataset) was undertaken in the semeval-2007 wsi task (swsi) (agirre and soroa, 2007), <papid> W07-2002 </papid>where two separate evaluation schemes were employed.</citsent>
<aftsection>
<nextsent>the first one, unsupervised evaluation, treats the wsi results as clusters of target word contexts and gold standard (gs) senses as classes.
</nextsent>
<nextsent>the traditional clustering measure of f-score (zhao et al, 2005) is used to assess the performance of wsi systems.
</nextsent>
<nextsent>the second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to gs senses.
</nextsent>
<nextsent>in the next step, the testing corpus is used to measure the performance of systems in word sense disambiguation (wsd) setting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1077">
<title id=" W09-2419.xml">semeval2010 task 14 evaluation setting for word sense induction x26 disambiguation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to gs senses.
</prevsent>
<prevsent>in the next step, the testing corpus is used to measure the performance of systems in word sense disambiguation (wsd) setting.
</prevsent>
</prevsection>
<citsent citstr=" D07-1043 ">
a significant limitation of f-score is that it does not evaluate the make up of clusters beyond the majority class (rosenberg and hirschberg, 2007).<papid> D07-1043 </papid>moreover, f-score might also fail to evaluate clusters which are not matched to any gs class due to their small size.</citsent>
<aftsection>
<nextsent>these two limitations define the matching problem of f-score (rosenberg and hirschberg, 2007) <papid> D07-1043 </papid>which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality.</nextsent>
<nextsent>the supervised evaluation scheme employs method in order to map the automatically induced clusters to gs senses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1088">
<title id=" W09-2419.xml">semeval2010 task 14 evaluation setting for word sense induction x26 disambiguation systems </title>
<section> semeval-2007 wsi evaluation setting.  </section>
<citcontext>
<prevsection>
<prevsent>the propose devaluation setting will be applied in the semeval 2010 wsi task..
</prevsent>
<prevsent>the semeval-2007 wsi task (agirre and soroa, 2007) <papid> W07-2002 </papid>evaluates wsi systems on 35 nouns and 65 verbs.</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
the corpus consists of texts of the wall street journal corpus, and is hand-tagged with ontonotes senses (hovy et al, 2006).<papid> N06-2015 </papid></citsent>
<aftsection>
<nextsent>for each target word tw, the task consists of firstly identifying the senses oftw (e.g. as clusters of target word instances, cooccurring words, etc.), and secondly tagging the instances of the target word using the automatically induced clusters.
</nextsent>
<nextsent>in the next sections, we describe and review the two evaluation schemes.
</nextsent>
<nextsent>2.1 swsi unsupervised evaluation.
</nextsent>
<nextsent>let us assume that given target word tw, wsi method has produced 3 clusters which have tagged 2100 instances of tw.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1123">
<title id=" W10-0601.xml">learning semantic features for fmri data from definitional text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this technique produces generative probabilistic model of text corpora where each document (article) is viewed as bag-of-words (i.e. only which words appear, and how often, matters) with each word being drawn from finite mixture of an underlying set of topics, each of which is in turn probability distribution over vocabulary words.
</prevsent>
<prevsent>we will use topics as our semantic features, with the proportions of each topic in the article forgiven noun being the values of the features for that noun.
</prevsent>
</prevsection>
<citsent citstr=" D09-1065 ">
(murphy et al, 2009) <papid> D09-1065 </papid>does something similar in flavour to this, by decomposing the patterns of cooccurrences in text corpus between the 20000 most frequent nouns and 5000 most frequent verbs using svd.</citsent>
<aftsection>
<nextsent>this is used to identify 25 singular vectors which yield feature values across nouns.
</nextsent>
<nextsent>2.1 data.
</nextsent>
<nextsent>we use the dataset from (mitchell et al, 2008),which contains data from 9 subjects.
</nextsent>
<nextsent>for each subject there is dataset of 360 examples - average fmri volume around the peak of an experiment trial - comprising 6 replications (epochs) of each of 60 nouns as stimuli.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1124">
<title id=" W09-2203.xml">keepinrsquo it real semi supervised learning with realistic tuning </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>in this study we consider only binary classification tasks.
</prevsent>
<prevsent>note that is the number of dimensions, (y = 1) is the proportion of instances in the full dataset belonging to class = 1, and |dtest|refers to the size of the test set (the instances remaining after max(l) + max(u) = 1100 have been set aside for training trials).[macwin] is the mac versus windows text classification data from the 20-newsgroups dataset, preprocessed by the authors of (sindhwani et al, 2005).
</prevsent>
</prevsection>
<citsent citstr=" P94-1020 ">
[interest] is binary version of the word sense disambiguation data from (bruce and wiebe, 1994).<papid> P94-1020 </papid></citsent>
<aftsection>
<nextsent>the task is to distinguish the sense of interest?
</nextsent>
<nextsent>meaning money paid for the use of money?
</nextsent>
<nextsent>fromthe other five senses (e.g., readiness to give atten tion,?
</nextsent>
<nextsent>a share in company or business?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1125">
<title id=" W10-0303.xml">comparing semantic role labeling with typed dependency parsing in computational metaphor identification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results show that srl can be used to identify potential metaphors and that it overcomes some of the limitations of using typed dependencies, but also that srl introduces its own set of complications.
</prevsent>
<prevsent>the paper concludes by suggesting future directions, both for evaluating the use of srl in cmi, and for fostering critical and creative thinking about metaphors.
</prevsent>
</prevsection>
<citsent citstr=" J91-1003 ">
metaphor, the partial framing of one concept in terms of another, pervades human language and thought (lakoff and johnson, 1980; lakoff, 1993).a variety of computational approaches to metaphorical language have been developed, e.g., (martin,1990; fass, 1991; <papid> J91-1003 </papid>gedigian et al, 2006; <papid> W06-3506 </papid>krishnakumaran and zhu, 2007).<papid> W07-0103 </papid></citsent>
<aftsection>
<nextsent>however, most such methods see metaphor as an obstacle to be overcome in the task of discerning the actual, literal meaning of phrase or sentence.
</nextsent>
<nextsent>in contrast, the work presented here approaches conceptual metaphor not as an obstacle but as resource.
</nextsent>
<nextsent>metaphor is an integral part in human understanding of myriad abstract or complex concepts (lakoff and johnson, 1980), and metaphorical thinking can be powerful component in critical and creative thinking, cf.
</nextsent>
<nextsent>(gordon, 1974; oxman-michelli, 1991).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1126">
<title id=" W10-0303.xml">comparing semantic role labeling with typed dependency parsing in computational metaphor identification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results show that srl can be used to identify potential metaphors and that it overcomes some of the limitations of using typed dependencies, but also that srl introduces its own set of complications.
</prevsent>
<prevsent>the paper concludes by suggesting future directions, both for evaluating the use of srl in cmi, and for fostering critical and creative thinking about metaphors.
</prevsent>
</prevsection>
<citsent citstr=" W06-3506 ">
metaphor, the partial framing of one concept in terms of another, pervades human language and thought (lakoff and johnson, 1980; lakoff, 1993).a variety of computational approaches to metaphorical language have been developed, e.g., (martin,1990; fass, 1991; <papid> J91-1003 </papid>gedigian et al, 2006; <papid> W06-3506 </papid>krishnakumaran and zhu, 2007).<papid> W07-0103 </papid></citsent>
<aftsection>
<nextsent>however, most such methods see metaphor as an obstacle to be overcome in the task of discerning the actual, literal meaning of phrase or sentence.
</nextsent>
<nextsent>in contrast, the work presented here approaches conceptual metaphor not as an obstacle but as resource.
</nextsent>
<nextsent>metaphor is an integral part in human understanding of myriad abstract or complex concepts (lakoff and johnson, 1980), and metaphorical thinking can be powerful component in critical and creative thinking, cf.
</nextsent>
<nextsent>(gordon, 1974; oxman-michelli, 1991).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1127">
<title id=" W10-0303.xml">comparing semantic role labeling with typed dependency parsing in computational metaphor identification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results show that srl can be used to identify potential metaphors and that it overcomes some of the limitations of using typed dependencies, but also that srl introduces its own set of complications.
</prevsent>
<prevsent>the paper concludes by suggesting future directions, both for evaluating the use of srl in cmi, and for fostering critical and creative thinking about metaphors.
</prevsent>
</prevsection>
<citsent citstr=" W07-0103 ">
metaphor, the partial framing of one concept in terms of another, pervades human language and thought (lakoff and johnson, 1980; lakoff, 1993).a variety of computational approaches to metaphorical language have been developed, e.g., (martin,1990; fass, 1991; <papid> J91-1003 </papid>gedigian et al, 2006; <papid> W06-3506 </papid>krishnakumaran and zhu, 2007).<papid> W07-0103 </papid></citsent>
<aftsection>
<nextsent>however, most such methods see metaphor as an obstacle to be overcome in the task of discerning the actual, literal meaning of phrase or sentence.
</nextsent>
<nextsent>in contrast, the work presented here approaches conceptual metaphor not as an obstacle but as resource.
</nextsent>
<nextsent>metaphor is an integral part in human understanding of myriad abstract or complex concepts (lakoff and johnson, 1980), and metaphorical thinking can be powerful component in critical and creative thinking, cf.
</nextsent>
<nextsent>(gordon, 1974; oxman-michelli, 1991).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1128">
<title id=" W10-0303.xml">comparing semantic role labeling with typed dependency parsing in computational metaphor identification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cmi identifies potential metaphors by mapping selectional preferences (resnik, 1993) from source corpus to target corpus.
</prevsent>
<prevsent>previous work on cmi utilized typed dependency parses (de marneffe et al., 2006) to calculate these selectional preferences.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
this paper explores the use of semantic role labeling (srl) (gildea and jurafsky, 2002; <papid> J02-3001 </papid>johansson and nugues, 2008) <papid> D08-1008 </papid>to calculate selectional preferences.</citsent>
<aftsection>
<nextsent>typed dependencies focus on syntactic structure and grammatical relations, while semantic roles emphasize conceptual and semantic structure, so srl may 14be more effective for identifying potential conceptual metaphors.
</nextsent>
<nextsent>this paper describes how srl was incorporated into cmi and compares both the relational data and the metaphors identified with typed dependency parsing and semantic role labeling.
</nextsent>
<nextsent>the results show that semantic roles enabled effective identification of potential metaphors.
</nextsent>
<nextsent>however, neither typed dependencies nor semantic roles were necessarily superior.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1129">
<title id=" W10-0303.xml">comparing semantic role labeling with typed dependency parsing in computational metaphor identification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cmi identifies potential metaphors by mapping selectional preferences (resnik, 1993) from source corpus to target corpus.
</prevsent>
<prevsent>previous work on cmi utilized typed dependency parses (de marneffe et al., 2006) to calculate these selectional preferences.
</prevsent>
</prevsection>
<citsent citstr=" D08-1008 ">
this paper explores the use of semantic role labeling (srl) (gildea and jurafsky, 2002; <papid> J02-3001 </papid>johansson and nugues, 2008) <papid> D08-1008 </papid>to calculate selectional preferences.</citsent>
<aftsection>
<nextsent>typed dependencies focus on syntactic structure and grammatical relations, while semantic roles emphasize conceptual and semantic structure, so srl may 14be more effective for identifying potential conceptual metaphors.
</nextsent>
<nextsent>this paper describes how srl was incorporated into cmi and compares both the relational data and the metaphors identified with typed dependency parsing and semantic role labeling.
</nextsent>
<nextsent>the results show that semantic roles enabled effective identification of potential metaphors.
</nextsent>
<nextsent>however, neither typed dependencies nor semantic roles were necessarily superior.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1134">
<title id=" W10-0303.xml">comparing semantic role labeling with typed dependency parsing in computational metaphor identification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(fass, 1991, <papid> J91-1003 </papid>p. 54).</prevsent>
<prevsent>one previous metaphor system avoids making such literal/metaphorical distinctions.</prevsent>
</prevsection>
<citsent citstr=" J04-1002 ">
cormet (mason, 2004) <papid> J04-1002 </papid>is designed to extract known conventional metaphors from domain-specific textual corpora, which are derived from google queries.cormet calculates selectional preferences and associations (resnik, 1993) for each corpuss characteristic verbs, i.e., those verbs at least twice as frequent in the corpus as in general english.</citsent>
<aftsection>
<nextsent>based on these selectional associations, cormet clusters the nouns for which the characteristic verbs select.
</nextsent>
<nextsent>to identify metaphors, mappings are sought from clusters in the source corpus to clusters in the target corpus, based on the degree to which the same verbs select for members of both clusters.
</nextsent>
<nextsent>for example, cormetwas used to extract the metaphor money is liquid1 by mapping from cluster for the concept liquid in corpus for the domain laboratory to cluster for the concept money in corpus for the domain finance, based on the selectional associations of verbs such as pour,?
</nextsent>
<nextsent>flow,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1137">
<title id=" W10-0303.xml">comparing semantic role labeling with typed dependency parsing in computational metaphor identification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 semantic role labeling.
</prevsent>
<prevsent>while interpretations vary somewhat, semantic role labeling (srl) generally aims to represent some thing about the meaning of phrase at deeper level than surface syntactic structure.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
one of themost common approaches to performing srl automatically is to use statistical classifier trained on labeled corpora (gildea and jurafsky, 2002), <papid> J02-3001 </papid>with framenet (baker et al, 1998) <papid> P98-1013 </papid>and propbank (palmer et al, 2005) <papid> J05-1004 </papid>being the primary sources.</citsent>
<aftsection>
<nextsent>an important result of the gildea and jurafsky workwas identifying the significant utility of using pre segmented constituents as input to their labeler, and accordingly most srl systems perform syntactic analysis as an initial step.the principal alternative to using statistical classifier is to use rule-based labeler for operating on 1small caps are metaphors, italics are concepts, caps are domains, and quotes?
</nextsent>
<nextsent>are example phrases.
</nextsent>
<nextsent>15the syntactic parse tree.
</nextsent>
<nextsent>for example, shi and mihalcea (2004) <papid> N04-3006 </papid>extract explicit srl rules by analyzing framenet cases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1138">
<title id=" W10-0303.xml">comparing semantic role labeling with typed dependency parsing in computational metaphor identification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 semantic role labeling.
</prevsent>
<prevsent>while interpretations vary somewhat, semantic role labeling (srl) generally aims to represent some thing about the meaning of phrase at deeper level than surface syntactic structure.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
one of themost common approaches to performing srl automatically is to use statistical classifier trained on labeled corpora (gildea and jurafsky, 2002), <papid> J02-3001 </papid>with framenet (baker et al, 1998) <papid> P98-1013 </papid>and propbank (palmer et al, 2005) <papid> J05-1004 </papid>being the primary sources.</citsent>
<aftsection>
<nextsent>an important result of the gildea and jurafsky workwas identifying the significant utility of using pre segmented constituents as input to their labeler, and accordingly most srl systems perform syntactic analysis as an initial step.the principal alternative to using statistical classifier is to use rule-based labeler for operating on 1small caps are metaphors, italics are concepts, caps are domains, and quotes?
</nextsent>
<nextsent>are example phrases.
</nextsent>
<nextsent>15the syntactic parse tree.
</nextsent>
<nextsent>for example, shi and mihalcea (2004) <papid> N04-3006 </papid>extract explicit srl rules by analyzing framenet cases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1139">
<title id=" W10-0303.xml">comparing semantic role labeling with typed dependency parsing in computational metaphor identification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>are example phrases.
</prevsent>
<prevsent>15the syntactic parse tree.
</prevsent>
</prevsection>
<citsent citstr=" N04-3006 ">
for example, shi and mihalcea (2004) <papid> N04-3006 </papid>extract explicit srl rules by analyzing framenet cases.</citsent>
<aftsection>
<nextsent>another system, relex (fun del et al, 2006) also uses rules and is structured like the implementation used here (see below for details),but despite having the same name, is different system.
</nextsent>
<nextsent>statistical and rule-based methods may also beused within the same system, such as in lth (jo hansson and nugues, 2008).<papid> D08-1008 </papid>one reason for preferring rule-based srl system is that rule-based approaches may be less susceptible to the loss of accuracy that statistically trained classifiers suffer when applied to domains that are different than the corpora they are trained on (johansson and nugues, 2008).<papid> D08-1008 </papid></nextsent>
<nextsent>that problem is compounded by the limited domain coverage provided by the labeled corpora currently available for srl classifier training (gildea and jurafsky, 2002).<papid> J02-3001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1143">
<title id=" W10-0303.xml">comparing semantic role labeling with typed dependency parsing in computational metaphor identification </title>
<section> computational metaphor identification.  </section>
<citcontext>
<prevsection>
<prevsent>source corpora are composed of wikipedia articles, as they provide readily available, categorically organized, large source of content on wide variety of topics.
</prevsent>
<prevsent>a source corpus for given domain consists of all the wikipedia articles in the category for that domain, as well as all articles in its subcategories.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
all documents in the source and target corpora are parsed to extract sentence structure and typed dependencies (klein and manning, 2003; <papid> P03-1054 </papid>de marneffe et al, 2006).the crux of cmi is selectional preference learning (resnik, 1993), which quantifies the tendency of particular words to appear with certain other classes of words in specific grammatical relationships.</citsent>
<aftsection>
<nextsent>for example, words for the concept of food are often the direct object of the verb eat.?
</nextsent>
<nextsent>using the parsed documents, cmi calculates selectional preferences of the characteristic nouns in corpus, where characteristic means that the noun is highly frequent inthe corpus relative to its frequency in general english, as derived from (kilgarriff, 1996).
</nextsent>
<nextsent>selectional preference is quantified as the relative entropy of the posterior distribution conditioned on specific noun and grammatical relation with respect to the prior distribution of verbs in general english: s(c) = ? p (v|c) log (v|c) (v) (1) where is class of nouns (i.e., concept likefood) and grammatical relation (such as direct object), and ranges over all the verbs for which appears in the given relation.
</nextsent>
<nextsent>these selectional preference strengths are then divided among the verbs that appear in each grammatical relation to determine the noun classs selectional association for each verb in each relation (resnik, 1993).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1144">
<title id=" W10-0209.xml">identifying emotions intentions and attitudes in text using a game with a purpose </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then present some classification results achieved by using small-scale database created with this methodology.
</prevsent>
<prevsent>a focus of much information extraction research has been identifying surface-level semantic content (e.g., identifying who did what to whom when).
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
in recent years, research on sentiment analysis and opinion mining has recognized that more subtle information can be communicated via linguistic features in the text (see pang and lee (2008) for re view), such as whether text (e.g., movie review) is positive or negative (turney 2002, <papid> P02-1053 </papid>pang, lee,and vaithyanathan 2002, dave, lawrence, and pen nock 2003, wiebe et al 2004, <papid> J04-3002 </papid>kennedy and inkpen 2006, agarwal, biadsy, and mckeown 2009, greene and resnik 2009, <papid> N09-1057 </papid>among many others).</citsent>
<aftsection>
<nextsent>however, other subtle information available in text, such as aspe akers emotional states (e.g., anger, embarrass ment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work in detecting emotion (e.g., subasic and huettner 2001, alm, roth, and sproat 2005, nicolov et al 2006, abbasi 2007) and detecting deception (e.g., annolli, balconi, and ciceri 2002, zhou et al 2004, gupta and skillicorn 2006, zhou and sung 2008).
</nextsent>
<nextsent>this latter kind of social information is useful for identifying the tone?
</nextsent>
<nextsent>of message, i.e., for understanding the underlying intention behind messages creation, and also for predicting how this message will be interpreted by humans reading it.
</nextsent>
<nextsent>a technical barrier to extracting this kind of social information is that there are currently no large-scaletext databases that are annotated with social information from which to learn the relevant linguisticcues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1146">
<title id=" W10-0209.xml">identifying emotions intentions and attitudes in text using a game with a purpose </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then present some classification results achieved by using small-scale database created with this methodology.
</prevsent>
<prevsent>a focus of much information extraction research has been identifying surface-level semantic content (e.g., identifying who did what to whom when).
</prevsent>
</prevsection>
<citsent citstr=" J04-3002 ">
in recent years, research on sentiment analysis and opinion mining has recognized that more subtle information can be communicated via linguistic features in the text (see pang and lee (2008) for re view), such as whether text (e.g., movie review) is positive or negative (turney 2002, <papid> P02-1053 </papid>pang, lee,and vaithyanathan 2002, dave, lawrence, and pen nock 2003, wiebe et al 2004, <papid> J04-3002 </papid>kennedy and inkpen 2006, agarwal, biadsy, and mckeown 2009, greene and resnik 2009, <papid> N09-1057 </papid>among many others).</citsent>
<aftsection>
<nextsent>however, other subtle information available in text, such as aspe akers emotional states (e.g., anger, embarrass ment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work in detecting emotion (e.g., subasic and huettner 2001, alm, roth, and sproat 2005, nicolov et al 2006, abbasi 2007) and detecting deception (e.g., annolli, balconi, and ciceri 2002, zhou et al 2004, gupta and skillicorn 2006, zhou and sung 2008).
</nextsent>
<nextsent>this latter kind of social information is useful for identifying the tone?
</nextsent>
<nextsent>of message, i.e., for understanding the underlying intention behind messages creation, and also for predicting how this message will be interpreted by humans reading it.
</nextsent>
<nextsent>a technical barrier to extracting this kind of social information is that there are currently no large-scaletext databases that are annotated with social information from which to learn the relevant linguisticcues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1147">
<title id=" W10-0209.xml">identifying emotions intentions and attitudes in text using a game with a purpose </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then present some classification results achieved by using small-scale database created with this methodology.
</prevsent>
<prevsent>a focus of much information extraction research has been identifying surface-level semantic content (e.g., identifying who did what to whom when).
</prevsent>
</prevsection>
<citsent citstr=" N09-1057 ">
in recent years, research on sentiment analysis and opinion mining has recognized that more subtle information can be communicated via linguistic features in the text (see pang and lee (2008) for re view), such as whether text (e.g., movie review) is positive or negative (turney 2002, <papid> P02-1053 </papid>pang, lee,and vaithyanathan 2002, dave, lawrence, and pen nock 2003, wiebe et al 2004, <papid> J04-3002 </papid>kennedy and inkpen 2006, agarwal, biadsy, and mckeown 2009, greene and resnik 2009, <papid> N09-1057 </papid>among many others).</citsent>
<aftsection>
<nextsent>however, other subtle information available in text, such as aspe akers emotional states (e.g., anger, embarrass ment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work in detecting emotion (e.g., subasic and huettner 2001, alm, roth, and sproat 2005, nicolov et al 2006, abbasi 2007) and detecting deception (e.g., annolli, balconi, and ciceri 2002, zhou et al 2004, gupta and skillicorn 2006, zhou and sung 2008).
</nextsent>
<nextsent>this latter kind of social information is useful for identifying the tone?
</nextsent>
<nextsent>of message, i.e., for understanding the underlying intention behind messages creation, and also for predicting how this message will be interpreted by humans reading it.
</nextsent>
<nextsent>a technical barrier to extracting this kind of social information is that there are currently no large-scaletext databases that are annotated with social information from which to learn the relevant linguisticcues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1148">
<title id=" W10-0209.xml">identifying emotions intentions and attitudes in text using a game with a purpose </title>
<section> reliable databases of social information.  </section>
<citcontext>
<prevsection>
<prevsent>for example, steyvers et al (2009) has shown that such wisdom of crowds?
</prevsent>
<prevsent>phenomena occur in many knowledge domains, including human memory, problem solving, and prediction.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
in addition, snow et al (2008) <papid> D08-1027 </papid>have demonstrated that relatively small number of non-expert annotations in natural language tasks can achieve the same results as expert annotation.</citsent>
<aftsection>
<nextsent>2.2 games with purpose.
</nextsent>
<nextsent>one approach is to use game with purpose (gwap) (von ahn and dabbish 2004, von ahn2006, von ahn, kedia, and blum 2006) that is designed to encourage people to provide the information needed in the database.
</nextsent>
<nextsent>gwaps are currently being used to accumulate information about many things that humans find easy to identify (see http://www.gwap.com/gwap/ for several examples), such as objects in images (von ahn and dabbish 2004), the musical style of songs, impressions of sights and sounds in videos, and common sense relationships between concepts (von ahn, kedia, and blum 2006).
</nextsent>
<nextsent>in addition, as the collected data comes from and is vetted by large number of participants, we can gauge which messages are reliable examples of particular social information and which are confusing examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1154">
<title id=" W09-2308.xml">on the complexity of alignment problems in two synchronous grammar formalisms </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>the alignment problem for synchronous grammars in its unrestricted form, i.e. whether for grammar and string pair the grammar induces an alignment of the two strings, reduces to the universal recognition problem,but restrictions may be imposed on the alignment sought, e.g. alignments may be 1 : 1,island-free or sure-possible sorted.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
the complexities of 15 restricted alignment problems in two very different synchronous grammar formalisms of syntax-based machine translation, inversion transduction grammars (itgs) (wu, 1997) <papid> J97-3002 </papid>and restricted form of range concatenation grammars ((2), (2)-brcgs) (s? gaard, 2008), are investigated.</citsent>
<aftsection>
<nextsent>the universal recognition problems, and therefore also the unrestricted alignment problems, of both formalisms can be solved in time o(n6|g|).
</nextsent>
<nextsent>the complexities of the restricted alignment problems differ significantly, however.
</nextsent>
<nextsent>the synchronous grammar formalisms used in syntax-based machine translation typically induce alignments by aligning all words that are recognized simultaneously (wu, 1997; <papid> J97-3002 </papid>zhang and gildea, this work was done while the first author was senior researcher at the dpt.</nextsent>
<nextsent>of linguistics, university of potsdam, supported by the german research foundation in the emmynoether project ptolemaios on grammar learning from parallel corpora; and while he was postdoctoral researcher at the isv computational linguistics group, copenhagen business school, supported by the danish research foundation in the project efficient syntax- and semantics-based machine transla tion.2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1162">
<title id=" W09-2308.xml">on the complexity of alignment problems in two synchronous grammar formalisms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the universal recognition problems of both itgs and (2), (2)-brcgs can be solved in time o(n6|g|).
</prevsent>
<prevsent>this may come as surprise, as itgs restrict the alignment search space considerably, while (2), (2) brcgs do not.
</prevsent>
</prevsection>
<citsent citstr=" J99-4005 ">
in the context of the np-hardness of decoding in statistical machine translation (knight, 1999; <papid> J99-4005 </papid>udupa and maji, 2006), <papid> E06-1004 </papid>it is natural to askwhy the universal recognition problem of (2), (2)brcgs isnt np-hard?</citsent>
<aftsection>
<nextsent>how can (2), (2)-brcgs induce all possible alignments and still avoid np hardness?
</nextsent>
<nextsent>this paper bridges the gap between these results and shows that when alignments are restricted to be 1 : 1, island-free or sure-possible sorted (see below), or all combinations thereof, the alignment problem of (2), (2)-brcgs is np-hard.(2), (2)-brcgs in sense avoid np-hardness by giving up control over global properties of alignments, e.g. any pair of words may be aligned multiple times in derivation.
</nextsent>
<nextsent>60 the alignment structures induced by synchronous grammars in syntax-based machine translation havethe following property: if an alignment structure includes alignments v|v?, v|w? and w|w?, it also includes the alignment w|v?, where w,w?, v, v?
</nextsent>
<nextsent>are word instances.1 this follows from the fact that only words that are recognized simultanously, are aligned.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1164">
<title id=" W09-2308.xml">on the complexity of alignment problems in two synchronous grammar formalisms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the universal recognition problems of both itgs and (2), (2)-brcgs can be solved in time o(n6|g|).
</prevsent>
<prevsent>this may come as surprise, as itgs restrict the alignment search space considerably, while (2), (2) brcgs do not.
</prevsent>
</prevsection>
<citsent citstr=" E06-1004 ">
in the context of the np-hardness of decoding in statistical machine translation (knight, 1999; <papid> J99-4005 </papid>udupa and maji, 2006), <papid> E06-1004 </papid>it is natural to askwhy the universal recognition problem of (2), (2)brcgs isnt np-hard?</citsent>
<aftsection>
<nextsent>how can (2), (2)-brcgs induce all possible alignments and still avoid np hardness?
</nextsent>
<nextsent>this paper bridges the gap between these results and shows that when alignments are restricted to be 1 : 1, island-free or sure-possible sorted (see below), or all combinations thereof, the alignment problem of (2), (2)-brcgs is np-hard.(2), (2)-brcgs in sense avoid np-hardness by giving up control over global properties of alignments, e.g. any pair of words may be aligned multiple times in derivation.
</nextsent>
<nextsent>60 the alignment structures induced by synchronous grammars in syntax-based machine translation havethe following property: if an alignment structure includes alignments v|v?, v|w? and w|w?, it also includes the alignment w|v?, where w,w?, v, v?
</nextsent>
<nextsent>are word instances.1 this follows from the fact that only words that are recognized simultanously, are aligned.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1166">
<title id=" W09-2308.xml">on the complexity of alignment problems in two synchronous grammar formalisms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some brief motivation for the properties singled out:is production rule with the start symbol in the lhs that introduces all the words in the right order, since all production rules with nonterminal symbols in the rhs are branching and contain no terminal symbols.
</prevsent>
<prevsent>61 result 1 : 1 if(s) if(t) sp itgs (2), (2)-brcgs (1) o(n6|g|) np-complete (2) o(n6|g|) np-complete (3) o(n6|g|) np-complete (4) o(n6|g|) np-complete (5) x o(n6|g|) np-complete (6) x o(n6|g|) np-complete (7) x o(n6|g|) np-complete (8) x o(n6|g|) np-complete (9) x o(n6|g|) np-complete (10) x o(n6|g|) np-complete (11) x o(n6|g|) np-complete (12) x o(n6|g|) np-complete (13) x o(n6|g|) np-complete (14) x o(n6|g|) np-complete (15) x x o(n6|g|) np-complete figure 1: the complexity of restricted alignment problems for itgs and (2), (2)-brcgs.?
</prevsent>
</prevsection>
<citsent citstr=" J99-1003 ">
1 : 1-alignments have been argued to be adequate by melamed (1999) <papid> J99-1003 </papid>and elsewhere, and it may therefore be useful to know if grammar extracted from parallel corpus produces 1 : 1 alignments for finite set of sentence pairs.?</citsent>
<aftsection>
<nextsent>island-free alignments are interesting to the extent that unaligned nodes increase the chance of translation errors.
</nextsent>
<nextsent>an island threshold may for instance be used to rule out risky translations.
</nextsent>
<nextsent>the notion of sure-possible sorted alignments is more unusual, but can, for instance, be used to check if the use of possible alignments is consistently triggered by words that are hard to align.
</nextsent>
<nextsent>the results for all cross-classifications of the four properties ? 1 : 1, source-side island-free(if(s)), target-side island-free (if(t)) and sure possible sorted (sp) ? are presented in the table in figure 1.3 note that all (24 ? 1 = 15) combinations of the four properties lead to np-hard alignment problems for (2), (2)-brcgs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1174">
<title id=" W09-2417.xml">semeval2010 task 10 linking events and their participants in discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>specifically, the task aims at linking locally uninstantiated roles to their co referents in the wider discourse context (if suchco-referents exist).
</prevsent>
<prevsent>this task is potentially beneficial for number of nlp applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
semantic role labelling (srl) has been defined as sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of predicate (gildea and jurafsky, 2002).<papid> J02-3001 </papid>semantic roles describe the function of the participants in an event.</citsent>
<aftsection>
<nextsent>identifying the semantic roles of the predicates in text allows knowing who did what to whom when where how, etc. srl has attracted much attention in recent years,as witnessed by several shared tasks in sense val/semeval (ma`rquez et al , 2007; litkowski, 2004; <papid> W04-0803 </papid>baker et al , 2007; <papid> W07-2018 </papid>diab et al , 2007), <papid> W07-2017 </papid>and conll (carreras and ma`rquez, 2004; carreras and ma`rquez, 2005; surdeanu et al , 2008).</nextsent>
<nextsent>the state-of-the-art in semantic role labelling has now advanced somuch that number of studies have shown that automatically inferred semantic argument structure scan lead to tangible performance gains in nlp applications such as information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>question answering (shen and lapata, 2007) <papid> D07-1002 </papid>or recognising textual entailment (burchardt and frank, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1175">
<title id=" W09-2417.xml">semeval2010 task 10 linking events and their participants in discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this task is potentially beneficial for number of nlp applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction.
</prevsent>
<prevsent>semantic role labelling (srl) has been defined as sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of predicate (gildea and jurafsky, 2002).<papid> J02-3001 </papid>semantic roles describe the function of the participants in an event.</prevsent>
</prevsection>
<citsent citstr=" W04-0803 ">
identifying the semantic roles of the predicates in text allows knowing who did what to whom when where how, etc. srl has attracted much attention in recent years,as witnessed by several shared tasks in sense val/semeval (ma`rquez et al , 2007; litkowski, 2004; <papid> W04-0803 </papid>baker et al , 2007; <papid> W07-2018 </papid>diab et al , 2007), <papid> W07-2017 </papid>and conll (carreras and ma`rquez, 2004; carreras and ma`rquez, 2005; surdeanu et al , 2008).</citsent>
<aftsection>
<nextsent>the state-of-the-art in semantic role labelling has now advanced somuch that number of studies have shown that automatically inferred semantic argument structure scan lead to tangible performance gains in nlp applications such as information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>question answering (shen and lapata, 2007) <papid> D07-1002 </papid>or recognising textual entailment (burchardt and frank, 2006).</nextsent>
<nextsent>however, semantic role labelling as it is currently defined also misses lot of information that would be beneficial for nlp applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or questionanswering.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1176">
<title id=" W09-2417.xml">semeval2010 task 10 linking events and their participants in discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this task is potentially beneficial for number of nlp applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction.
</prevsent>
<prevsent>semantic role labelling (srl) has been defined as sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of predicate (gildea and jurafsky, 2002).<papid> J02-3001 </papid>semantic roles describe the function of the participants in an event.</prevsent>
</prevsection>
<citsent citstr=" W07-2018 ">
identifying the semantic roles of the predicates in text allows knowing who did what to whom when where how, etc. srl has attracted much attention in recent years,as witnessed by several shared tasks in sense val/semeval (ma`rquez et al , 2007; litkowski, 2004; <papid> W04-0803 </papid>baker et al , 2007; <papid> W07-2018 </papid>diab et al , 2007), <papid> W07-2017 </papid>and conll (carreras and ma`rquez, 2004; carreras and ma`rquez, 2005; surdeanu et al , 2008).</citsent>
<aftsection>
<nextsent>the state-of-the-art in semantic role labelling has now advanced somuch that number of studies have shown that automatically inferred semantic argument structure scan lead to tangible performance gains in nlp applications such as information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>question answering (shen and lapata, 2007) <papid> D07-1002 </papid>or recognising textual entailment (burchardt and frank, 2006).</nextsent>
<nextsent>however, semantic role labelling as it is currently defined also misses lot of information that would be beneficial for nlp applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or questionanswering.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1177">
<title id=" W09-2417.xml">semeval2010 task 10 linking events and their participants in discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this task is potentially beneficial for number of nlp applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction.
</prevsent>
<prevsent>semantic role labelling (srl) has been defined as sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of predicate (gildea and jurafsky, 2002).<papid> J02-3001 </papid>semantic roles describe the function of the participants in an event.</prevsent>
</prevsection>
<citsent citstr=" W07-2017 ">
identifying the semantic roles of the predicates in text allows knowing who did what to whom when where how, etc. srl has attracted much attention in recent years,as witnessed by several shared tasks in sense val/semeval (ma`rquez et al , 2007; litkowski, 2004; <papid> W04-0803 </papid>baker et al , 2007; <papid> W07-2018 </papid>diab et al , 2007), <papid> W07-2017 </papid>and conll (carreras and ma`rquez, 2004; carreras and ma`rquez, 2005; surdeanu et al , 2008).</citsent>
<aftsection>
<nextsent>the state-of-the-art in semantic role labelling has now advanced somuch that number of studies have shown that automatically inferred semantic argument structure scan lead to tangible performance gains in nlp applications such as information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>question answering (shen and lapata, 2007) <papid> D07-1002 </papid>or recognising textual entailment (burchardt and frank, 2006).</nextsent>
<nextsent>however, semantic role labelling as it is currently defined also misses lot of information that would be beneficial for nlp applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or questionanswering.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1178">
<title id=" W09-2417.xml">semeval2010 task 10 linking events and their participants in discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labelling (srl) has been defined as sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of predicate (gildea and jurafsky, 2002).<papid> J02-3001 </papid>semantic roles describe the function of the participants in an event.</prevsent>
<prevsent>identifying the semantic roles of the predicates in text allows knowing who did what to whom when where how, etc. srl has attracted much attention in recent years,as witnessed by several shared tasks in sense val/semeval (ma`rquez et al , 2007; litkowski, 2004; <papid> W04-0803 </papid>baker et al , 2007; <papid> W07-2018 </papid>diab et al , 2007), <papid> W07-2017 </papid>and conll (carreras and ma`rquez, 2004; carreras and ma`rquez, 2005; surdeanu et al , 2008).</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
the state-of-the-art in semantic role labelling has now advanced somuch that number of studies have shown that automatically inferred semantic argument structure scan lead to tangible performance gains in nlp applications such as information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>question answering (shen and lapata, 2007) <papid> D07-1002 </papid>or recognising textual entailment (burchardt and frank, 2006).</citsent>
<aftsection>
<nextsent>however, semantic role labelling as it is currently defined also misses lot of information that would be beneficial for nlp applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or questionanswering.
</nextsent>
<nextsent>the reason for this is that srl has traditionally been viewed as sentence-internal task.hence, relations between different local semantic argument structures are disregarded and this leads to loss of important semantic information.
</nextsent>
<nextsent>this view of srl as sentence-internal task is partly due to the fact that large-scale manual annotation projects such as framenet1 and propbank2 typically present their annotations lexico graphically by lemma rather than by source text.
</nextsent>
<nextsent>furthermore, in the case of framenet, the annotation effort didnot start out with the goal of exhaustive corpus annotation but instead focused on isolated instances of the target words sampled from very large corpus, which did not allow for view of the data as full-text annotation?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1179">
<title id=" W09-2417.xml">semeval2010 task 10 linking events and their participants in discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labelling (srl) has been defined as sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of predicate (gildea and jurafsky, 2002).<papid> J02-3001 </papid>semantic roles describe the function of the participants in an event.</prevsent>
<prevsent>identifying the semantic roles of the predicates in text allows knowing who did what to whom when where how, etc. srl has attracted much attention in recent years,as witnessed by several shared tasks in sense val/semeval (ma`rquez et al , 2007; litkowski, 2004; <papid> W04-0803 </papid>baker et al , 2007; <papid> W07-2018 </papid>diab et al , 2007), <papid> W07-2017 </papid>and conll (carreras and ma`rquez, 2004; carreras and ma`rquez, 2005; surdeanu et al , 2008).</prevsent>
</prevsection>
<citsent citstr=" D07-1002 ">
the state-of-the-art in semantic role labelling has now advanced somuch that number of studies have shown that automatically inferred semantic argument structure scan lead to tangible performance gains in nlp applications such as information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>question answering (shen and lapata, 2007) <papid> D07-1002 </papid>or recognising textual entailment (burchardt and frank, 2006).</citsent>
<aftsection>
<nextsent>however, semantic role labelling as it is currently defined also misses lot of information that would be beneficial for nlp applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or questionanswering.
</nextsent>
<nextsent>the reason for this is that srl has traditionally been viewed as sentence-internal task.hence, relations between different local semantic argument structures are disregarded and this leads to loss of important semantic information.
</nextsent>
<nextsent>this view of srl as sentence-internal task is partly due to the fact that large-scale manual annotation projects such as framenet1 and propbank2 typically present their annotations lexico graphically by lemma rather than by source text.
</nextsent>
<nextsent>furthermore, in the case of framenet, the annotation effort didnot start out with the goal of exhaustive corpus annotation but instead focused on isolated instances of the target words sampled from very large corpus, which did not allow for view of the data as full-text annotation?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1180">
<title id=" W09-2417.xml">semeval2010 task 10 linking events and their participants in discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, in the case of framenet, the annotation effort didnot start out with the goal of exhaustive corpus annotation but instead focused on isolated instances of the target words sampled from very large corpus, which did not allow for view of the data as full-text annotation?.
</prevsent>
<prevsent>it is clear that there is an interplay between local argument structure and the surrounding discourse (fillmore, 1977).
</prevsent>
</prevsection>
<citsent citstr=" P86-1004 ">
in early work, palmer et al  (1986) <papid> P86-1004 </papid>discussed filling null complements from context byusing knowledge about individual predicates and ten 1http://framenet.icsi.berkeley.edu/ 2http://verbs.colorado.edu/mpalmer/ projects/ace.html 106 dencies of referential chaining across sentences.</citsent>
<aftsection>
<nextsent>but so far there have been few attempts to find links between argument structures across clause and sentence boundaries explicitly on the basis of semantic relations between the predicates involved.
</nextsent>
<nextsent>two no table exceptions are fillmore and baker (2001) and burchardt et al  (2005).
</nextsent>
<nextsent>fillmore and baker (2001) analyse short newspaper article and discuss how frame semantics could benefit discourse processing but without making concrete suggestions of how to model this.
</nextsent>
<nextsent>burchardt et al  (2005) provide detailed analysis of the links between the local semantic argument structures in short text; however their system is not fully implemented either.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1185">
<title id=" W10-0711.xml">turkerassisted paraphrasing for english arabic machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, the nist open machine translation evaluation (openmt) 2009 (garofolo, 2009) constrained arabic-englishdevelopment and evaluation data includes four english translations for each arabic source sentence, as english is the usual target language.
</prevsent>
<prevsent>however, when considering this data to tune and evaluate an english-to-arabic system, each english sentence has single arabic translation and such translations are often identical.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
with at most one reference translation for each source sentence, standard minimum error rate training (och, 2003) <papid> P03-1021 </papid>to the bleu metric (papineni et al, 2002) <papid> P02-1040 </papid>becomes problematic, asbleu relies on the availability of multiple refer ences.</citsent>
<aftsection>
<nextsent>we describe semi-automatic paraphrasing technique that addresses this problem by identifying paraphrases that can be used to create new reference translations based on valid phrase substitutions on existing references.
</nextsent>
<nextsent>paraphrases are automatically extracted from large parallel corpus and filtered by quality judgments collected from human annotators using amazon mechanical turk.
</nextsent>
<nextsent>as turkers are not trained to complete natural language processing (nlp) tasks and can dishonestly submit random judgments, we develop task type that is able to catch problem turkers while remaining simple enough for untrained annotators to understand.
</nextsent>
<nextsent>the parallel corpus used for paraphrasing consists of all arabic-english sentence pairs in the nist openmt evaluation 2009 (garofolo, 2009) constrained training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1186">
<title id=" W10-0711.xml">turkerassisted paraphrasing for english arabic machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, the nist open machine translation evaluation (openmt) 2009 (garofolo, 2009) constrained arabic-englishdevelopment and evaluation data includes four english translations for each arabic source sentence, as english is the usual target language.
</prevsent>
<prevsent>however, when considering this data to tune and evaluate an english-to-arabic system, each english sentence has single arabic translation and such translations are often identical.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
with at most one reference translation for each source sentence, standard minimum error rate training (och, 2003) <papid> P03-1021 </papid>to the bleu metric (papineni et al, 2002) <papid> P02-1040 </papid>becomes problematic, asbleu relies on the availability of multiple refer ences.</citsent>
<aftsection>
<nextsent>we describe semi-automatic paraphrasing technique that addresses this problem by identifying paraphrases that can be used to create new reference translations based on valid phrase substitutions on existing references.
</nextsent>
<nextsent>paraphrases are automatically extracted from large parallel corpus and filtered by quality judgments collected from human annotators using amazon mechanical turk.
</nextsent>
<nextsent>as turkers are not trained to complete natural language processing (nlp) tasks and can dishonestly submit random judgments, we develop task type that is able to catch problem turkers while remaining simple enough for untrained annotators to understand.
</nextsent>
<nextsent>the parallel corpus used for paraphrasing consists of all arabic-english sentence pairs in the nist openmt evaluation 2009 (garofolo, 2009) constrained training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1187">
<title id=" W10-0306.xml">representing story plans in sumo </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>people combine words and events from their knowledge source of words, their meanings and their relationships in order to tell stories about their lives, their communities, and their daily experiences.
</prevsent>
<prevsent>in order for computers to achieve the same level of expressiveness to provide more fluent man-machine interaction, they must be provided with the same collection of knowledge about the basic relationships between things and events.
</prevsent>
</prevsection>
<citsent citstr=" W09-2009 ">
picture books (solis et al 2009), <papid> W09-2009 </papid>an automatic story generator that generates story text for children from given input set of picture elements (backgrounds, characters and objects), utilized semantic ontology whose design has been adapted from concept net (liu and singh, 2004).</citsent>
<aftsection>
<nextsent>the background serves as the setting of the story and is also used to determine the theme.
</nextsent>
<nextsent>semantic concepts needed by the story planner, specifically objects, story events, and character actions are classified according to the semantic categories of concept net, namely things, spatial, events, actions, and functions.
</nextsent>
<nextsent>this mapping approach constrained the flexibility of the system, as new themes would entail re populating the sequences of possible events manually into the knowledge base.
</nextsent>
<nextsent>events and actions are selected according to their associated themes, and not marked with preconditions that specify constraints under which certain actions can be performed and the corresponding consequential events that may arise.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1189">
<title id=" W09-2503.xml">subsentencial paraphrasing by contextual pivot translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a step towards meaning-aware paraphrasing can be done by appropriate use of the context in which paraphrasing occurrence occurs.
</prevsent>
<prevsent>at the lowest level, deciding automatically when word can be substituted with synonym is complex issue (connor and roth, 2007).
</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
when attempting paraphrasing on higher level, such as arbitrary phrases or full sentences (barzilay and lee, 2003; pang et al, 2003; <papid> N03-1024 </papid>quirk et al, 2004; <papid> W04-3219 </papid>bannard and callison-burch, 2005; zhao et al,2008<papid> P08-1116 </papid>a), first issue concerns the acquisition of elementary units, which in the general case do not exist in predefined dictionaries.</citsent>
<aftsection>
<nextsent>some paraphrasing strategy must then follow, which may consider the context of substitution to guide the selection of appropriate units (callison-burch, 2008; max, 2008).
</nextsent>
<nextsent>an important limitation to this family of works is the scarcity of corpora that can be used as reliable supervised training data.
</nextsent>
<nextsent>indeed, strictly parallel sentence pairs, for instance, are not naturally produced in human activities.1 as consequence, works on paraphrasing have recourse to costly human evaluation procedures, and an objective of automatic evaluation metrics is to rely onas little gold standard data as possible (callison burch et al, 2008).a text revision task is an application of paraphrase generation where context may be used in an effective way.
</nextsent>
<nextsent>when local change is made to text, it occurs within textual envelope?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1190">
<title id=" W09-2503.xml">subsentencial paraphrasing by contextual pivot translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a step towards meaning-aware paraphrasing can be done by appropriate use of the context in which paraphrasing occurrence occurs.
</prevsent>
<prevsent>at the lowest level, deciding automatically when word can be substituted with synonym is complex issue (connor and roth, 2007).
</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
when attempting paraphrasing on higher level, such as arbitrary phrases or full sentences (barzilay and lee, 2003; pang et al, 2003; <papid> N03-1024 </papid>quirk et al, 2004; <papid> W04-3219 </papid>bannard and callison-burch, 2005; zhao et al,2008<papid> P08-1116 </papid>a), first issue concerns the acquisition of elementary units, which in the general case do not exist in predefined dictionaries.</citsent>
<aftsection>
<nextsent>some paraphrasing strategy must then follow, which may consider the context of substitution to guide the selection of appropriate units (callison-burch, 2008; max, 2008).
</nextsent>
<nextsent>an important limitation to this family of works is the scarcity of corpora that can be used as reliable supervised training data.
</nextsent>
<nextsent>indeed, strictly parallel sentence pairs, for instance, are not naturally produced in human activities.1 as consequence, works on paraphrasing have recourse to costly human evaluation procedures, and an objective of automatic evaluation metrics is to rely onas little gold standard data as possible (callison burch et al, 2008).a text revision task is an application of paraphrase generation where context may be used in an effective way.
</nextsent>
<nextsent>when local change is made to text, it occurs within textual envelope?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1191">
<title id=" W09-2503.xml">subsentencial paraphrasing by contextual pivot translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a step towards meaning-aware paraphrasing can be done by appropriate use of the context in which paraphrasing occurrence occurs.
</prevsent>
<prevsent>at the lowest level, deciding automatically when word can be substituted with synonym is complex issue (connor and roth, 2007).
</prevsent>
</prevsection>
<citsent citstr=" P08-1116 ">
when attempting paraphrasing on higher level, such as arbitrary phrases or full sentences (barzilay and lee, 2003; pang et al, 2003; <papid> N03-1024 </papid>quirk et al, 2004; <papid> W04-3219 </papid>bannard and callison-burch, 2005; zhao et al,2008<papid> P08-1116 </papid>a), first issue concerns the acquisition of elementary units, which in the general case do not exist in predefined dictionaries.</citsent>
<aftsection>
<nextsent>some paraphrasing strategy must then follow, which may consider the context of substitution to guide the selection of appropriate units (callison-burch, 2008; max, 2008).
</nextsent>
<nextsent>an important limitation to this family of works is the scarcity of corpora that can be used as reliable supervised training data.
</nextsent>
<nextsent>indeed, strictly parallel sentence pairs, for instance, are not naturally produced in human activities.1 as consequence, works on paraphrasing have recourse to costly human evaluation procedures, and an objective of automatic evaluation metrics is to rely onas little gold standard data as possible (callison burch et al, 2008).a text revision task is an application of paraphrase generation where context may be used in an effective way.
</nextsent>
<nextsent>when local change is made to text, it occurs within textual envelope?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1194">
<title id=" W09-2503.xml">subsentencial paraphrasing by contextual pivot translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>different sources have been considered for paraphrase acquisition techniques.
</prevsent>
<prevsent>(pang et al, 2003), <papid> N03-1024 </papid>for example, apply syntactic fusion to multiple translations of individual sentences.</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
(barzilay andlee, 2003; dolan et al, 2004) <papid> C04-1051 </papid>acquire short paraphrases from comparable corpora, while (bhagat and ravichandran, 2008) considered the issue of acquiring short paraphrase patterns from huge amounts of comparable corpora.</citsent>
<aftsection>
<nextsent>(bannardand callison-burch, 2005) introduced pivot approach to acquire short paraphrases from multilingual parallel corpora, resource much more readily available than their monolingual counterpart.
</nextsent>
<nextsent>(zhao et al, 2008<papid> P08-1116 </papid>b) acquire paraphrase patterns from bilingual corpora and report the various types obtained.3 (callison-burch, 2008) improves the pivot paraphrase acquisition technique by using syntactic constraints at the level of constituents during phrase extraction.</nextsent>
<nextsent>this works also uses syntactic constraints during phrase substitution, resulting in improvements in both grammat3the types of their paraphrase patterns are the following (numbers in parentheses indicate frequency in their database): phrase replacements (267); trivial changes (79);structural paraphrases (71); phrase reorderings (56); and addition of deletion of information that are claimed to not alter meaning (27).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1204">
<title id=" W09-2503.xml">subsentencial paraphrasing by contextual pivot translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several works have tackled full sentence paraphrasing as monolingual translation task relying on statistical machine translation (smt).
</prevsent>
<prevsent>for instance, (quirk et al., 2004) <papid> W04-3219 </papid>used phrase-based smt decoder that uses local paraphrases acquired from comparable corpora to produce monotone sentential paraphrases.</prevsent>
</prevsection>
<citsent citstr=" W07-0716 ">
(zhao et al, 2008<papid> P08-1116 </papid>a) acquired monolingual biphrases from various sources and used them with phrase-based smt decoder, and (madnani et al, 2007) <papid> W07-0716 </papid>combined rules of their hierarchical decoders by pivot to obtain monolingual grammar.</citsent>
<aftsection>
<nextsent>these works were not motivated by the generation of high-quality paraphrases that could, for example, be reused in documents.
</nextsent>
<nextsent>the lack of structural information, the local nature of the paraphrasing performed and the fact that the context of the original sentences was not taken into account in the phrase-based approaches make it difficult to control meaning preservation during paraphrasing.
</nextsent>
<nextsent>context has been shown to play crucial role in machine translation, where in particular proper word sense disambiguation (wsd) is required in many cases.
</nextsent>
<nextsent>a variety of works have integrated context with some success into phrase-based and hierarchical decoders.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1205">
<title id=" W09-2503.xml">subsentencial paraphrasing by contextual pivot translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, (carpuat andwu, 2007) disambiguate phrases using state-of the-art wsd classifier, and (stroppa et al, 2007)use global memory-based classifier to find appropriate phrase translations in context.
</prevsent>
<prevsent>context is often defined as local linguistic features such as surrounding words and their part-of-speech, butsome works have experimented with more syntactic features (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W08-0302 ">
(gimpel and smith, 2008; <papid> W08-0302 </papid>max et al, 2008; haque et al, 2009)).</citsent>
<aftsection>
<nextsent>using an intermediate pivot language with bilingual translation in which given language pair is low-resourced has led to improvements in translation performance (wu and wang, 2007;<papid> P07-1108 </papid>bertoldi et al, 2008), but to our knowledge this approach has not been applied to full sentence paraphrasing.</nextsent>
<nextsent>several reasons may explain this, in particular the relative low quality of current mt approaches on full sentence translation, and the difficulties in controlling what is paraphrased and how.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1207">
<title id=" W09-2503.xml">subsentencial paraphrasing by contextual pivot translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>context is often defined as local linguistic features such as surrounding words and their part-of-speech, butsome works have experimented with more syntactic features (e.g.
</prevsent>
<prevsent>(gimpel and smith, 2008; <papid> W08-0302 </papid>max et al, 2008; haque et al, 2009)).</prevsent>
</prevsection>
<citsent citstr=" P07-1108 ">
using an intermediate pivot language with bilingual translation in which given language pair is low-resourced has led to improvements in translation performance (wu and wang, 2007;<papid> P07-1108 </papid>bertoldi et al, 2008), but to our knowledge this approach has not been applied to full sentence paraphrasing.</citsent>
<aftsection>
<nextsent>several reasons may explain this, in particular the relative low quality of current mt approaches on full sentence translation, and the difficulties in controlling what is paraphrased and how.
</nextsent>
<nextsent>19
</nextsent>
<nextsent>sub-sentential paraphrasing although many works have addressed the issue of local paraphrase acquisition, effective use of such paraphrases for paraphrase generation has only be achieved at the level of text units corresponding to short contiguous phrases.
</nextsent>
<nextsent>recent works have proposed approaches to exploit context in orderto correctly replace text fragment with paraphrase, but they are limited to known text units and therefore suffer from scarcity of data.4in this work, we address the case of sub sentential paraphrase generation, an intermediate case between local paraphrasing using text units for which paraphrases are available and full sentence paraphrasing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1211">
<title id=" W09-2503.xml">subsentencial paraphrasing by contextual pivot translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we have conducted experiments motivated by text revision task that we report in this section by describing our baseline and context-aware sub sentential paraphrasing systems and the results of small-scale manual evaluation.
</prevsent>
<prevsent>4.1 data and systems.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
we built two-way french-english smt systems using 188,115 lines of the europarl corpus (koehn, 2005) of parliamentary debates with moses (koehn et al, 2007) <papid> P07-2045 </papid>10.</citsent>
<aftsection>
<nextsent>our corpus was analyzed by the xip robust parser (at-mokhtar et al, 2002) and its output tokenization was used.
</nextsent>
<nextsent>we built standard systems, as well as contextual system for french english as described in section 3.2 using an additional contextual score ob 8reordering is allowed in the paraphrased fragment.
</nextsent>
<nextsent>9as consequence, minimal paraphrases may differ by only one full word.
</nextsent>
<nextsent>this can however be used advantageously when the sought type of paraphrasing aims at normalizing?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1212">
<title id=" W10-0716.xml">using the amazon mechanical turk to transcribe and annotate meeting speech for extractive summarization </title>
<section> annotating for summarization.  </section>
<citcontext>
<prevsection>
<prevsent>the latter annotation is particularly useful for training and evaluating extractive summarization systems systems that create summaries by selecting subset of the utterances.
</prevsent>
<prevsent>due to the subjectivity involved, we find very low inter-annotator agreement for this labeling task.
</prevsent>
</prevsection>
<citsent citstr=" P08-2051 ">
liu and liu (2008) <papid> P08-2051 </papid>reported kappa agreement scores of between 0.11 and 0.35 across 6 annotators, penn and zhu (2008) <papid> P08-1054 </papid>reported 0.38 on telephone conversation and 0.37 on lecture speech, using 3 annotators, and galley (2006) <papid> W06-1643 </papid>reported 0.32 on meeting data.</citsent>
<aftsection>
<nextsent>such low levels of agreement imply that the resulting training data is likely to contain great deal of noise utterances labeled in summary?
</nextsent>
<nextsent>or out of summary?, when in fact they are not good examples of those classes.
</nextsent>
<nextsent>disagreements arise due to the fact that utterance importance is spectrum.
</nextsent>
<nextsent>while some utterances are clearly important or unimportant, there are many utterances that lie between these extremes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1213">
<title id=" W10-0716.xml">using the amazon mechanical turk to transcribe and annotate meeting speech for extractive summarization </title>
<section> annotating for summarization.  </section>
<citcontext>
<prevsection>
<prevsent>the latter annotation is particularly useful for training and evaluating extractive summarization systems systems that create summaries by selecting subset of the utterances.
</prevsent>
<prevsent>due to the subjectivity involved, we find very low inter-annotator agreement for this labeling task.
</prevsent>
</prevsection>
<citsent citstr=" P08-1054 ">
liu and liu (2008) <papid> P08-2051 </papid>reported kappa agreement scores of between 0.11 and 0.35 across 6 annotators, penn and zhu (2008) <papid> P08-1054 </papid>reported 0.38 on telephone conversation and 0.37 on lecture speech, using 3 annotators, and galley (2006) <papid> W06-1643 </papid>reported 0.32 on meeting data.</citsent>
<aftsection>
<nextsent>such low levels of agreement imply that the resulting training data is likely to contain great deal of noise utterances labeled in summary?
</nextsent>
<nextsent>or out of summary?, when in fact they are not good examples of those classes.
</nextsent>
<nextsent>disagreements arise due to the fact that utterance importance is spectrum.
</nextsent>
<nextsent>while some utterances are clearly important or unimportant, there are many utterances that lie between these extremes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1214">
<title id=" W10-0716.xml">using the amazon mechanical turk to transcribe and annotate meeting speech for extractive summarization </title>
<section> annotating for summarization.  </section>
<citcontext>
<prevsection>
<prevsent>the latter annotation is particularly useful for training and evaluating extractive summarization systems systems that create summaries by selecting subset of the utterances.
</prevsent>
<prevsent>due to the subjectivity involved, we find very low inter-annotator agreement for this labeling task.
</prevsent>
</prevsection>
<citsent citstr=" W06-1643 ">
liu and liu (2008) <papid> P08-2051 </papid>reported kappa agreement scores of between 0.11 and 0.35 across 6 annotators, penn and zhu (2008) <papid> P08-1054 </papid>reported 0.38 on telephone conversation and 0.37 on lecture speech, using 3 annotators, and galley (2006) <papid> W06-1643 </papid>reported 0.32 on meeting data.</citsent>
<aftsection>
<nextsent>such low levels of agreement imply that the resulting training data is likely to contain great deal of noise utterances labeled in summary?
</nextsent>
<nextsent>or out of summary?, when in fact they are not good examples of those classes.
</nextsent>
<nextsent>disagreements arise due to the fact that utterance importance is spectrum.
</nextsent>
<nextsent>while some utterances are clearly important or unimportant, there are many utterances that lie between these extremes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1215">
<title id=" W10-0716.xml">using the amazon mechanical turk to transcribe and annotate meeting speech for extractive summarization </title>
<section> annotating for summarization.  </section>
<citcontext>
<prevsection>
<prevsent>although this formulation creates two decision boundaries, instead of the single one in the 2-class formulation, the expectation is that large number of utterances with middling importance will simply be assigned to the in between?
</prevsent>
<prevsent>class, thus reducing the amount of noise in the data.
</prevsent>
</prevsection>
<citsent citstr=" W09-3910 ">
indeed we have shown (baner jee and rudnicky, 2009) <papid> W09-3910 </papid>that in-house annotators achieve high inter-annotator agreement when provided with the 3-class formulation.</citsent>
<aftsection>
<nextsent>another way to alleviate the problem of low agreement is to obtain annotations from many annotators, and identify the utterances that majority of the annotators appear to agree on; such utterances may be considered as good examples of their class.
</nextsent>
<nextsent>using multiple annotators is typically not feasible due to cost.
</nextsent>
<nextsent>in this paper we investigate using mturk to create 3-class-based summarization annotations from multiple annotators per meeting, and to combine and filter these annotations to create high quality labels.
</nextsent>
<nextsent>5.1 challenges of using mechanical turk.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1216">
<title id=" W10-0702.xml">corpus creation for new genres a crowd sourced approach to pp attachment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our analysis shows that this two-step approach is capable of producing reliable annotations on informal and potentially noisy blog text, and this semi-automated strategy holds promise for similar annotation projects in new genres.
</prevsent>
<prevsent>recent decades have seen rapid development in natural language processing tools for parsing, semantic role-labeling, machine translation, etc., and much ofthis success can be attributed to the study of statistical techniques and the availability of large annotated corpora for training.
</prevsent>
</prevsection>
<citsent citstr=" D07-1112 ">
however, the performance of these systems is heavily dependent on the domain and genre of their training data, i.e. systems trained on data from particular domain tend to perform poorly when applied to other domains and adaptation techniques are not always able to compensate (dredze et al, 2007).<papid> D07-1112 </papid></citsent>
<aftsection>
<nextsent>for this reason, achieving high performance on new domains and genres frequently necessitates the collection of annotated training data from those domains and genres, time consuming and frequently expensive process.
</nextsent>
<nextsent>this paper examines the problem of collecting high-quality annotations for new genres with focuson time and cost efficiency.
</nextsent>
<nextsent>we explore the well studied but non-trivial task of prepositional phrase(pp) attachment and describe semi-automated system for identifying accurate attachments in blog data, which is frequently noisy and difficult to parse.
</nextsent>
<nextsent>pp attachment disambiguation involves finding correct attachment for prepositional phrase in sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1217">
<title id=" W10-0702.xml">corpus creation for new genres a crowd sourced approach to pp attachment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this simplification of the pp attachment task to choice between two alternative sis unrealistic when considering the potential long distance attachments encountered in real-world text.while blogs and other web text, such as discussion forums and emails, have been studied for variety of tasks such as information extraction (hong and davison, 2009), social networking (gruhl et al., 2004), and sentiment analysis (leshed andkaye, 2006), we are not aware of any previous efforts to gather syntactic data (such as pp attach ments) in the genre.
</prevsent>
<prevsent>syntactic methods such as pos tagging, parsing and structural disambiguation are commonly used when analyzing well-structured text.
</prevsent>
</prevsection>
<citsent citstr=" P05-1063 ">
including the use of syntactic information has yielded improvements inaccuracy in speech recognition (chelba and jelenik, 1998; collins et al., 2005) <papid> P05-1063 </papid>and machine translation (deneefe and knight, 2009; <papid> D09-1076 </papid>carreras and collins, 2009).<papid> D09-1021 </papid></citsent>
<aftsection>
<nextsent>we anticipate that datasets such as ours could be useful for such tasks as well.
</nextsent>
<nextsent>amazons mechanical turk (mturk) has become very popular for manual annotation tasks and has been shown to perform equally well over labeling tasks such as affect recognition, word similarity, recognizing textual entailment, event temporal ordering and word sense disambiguation, when compared to annotations from experts (snow et al, 2008).
</nextsent>
<nextsent>while these tasks were small in scale and intended to demonstrate the viability of annotation via mturk,it has also proved effective in large-scale tasks including the collection of accurate speech transcriptions (gruenstein et al, 2009).
</nextsent>
<nextsent>in this paper we explore method for corpus building on large scale in order to extend annotation into new domains and genres.we previously evaluated crowd sourced pp attachment annotation by using mturk workers to reproduce pp attachments from the wall street journal corpus (rosenthal et al, 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1218">
<title id=" W10-0702.xml">corpus creation for new genres a crowd sourced approach to pp attachment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this simplification of the pp attachment task to choice between two alternative sis unrealistic when considering the potential long distance attachments encountered in real-world text.while blogs and other web text, such as discussion forums and emails, have been studied for variety of tasks such as information extraction (hong and davison, 2009), social networking (gruhl et al., 2004), and sentiment analysis (leshed andkaye, 2006), we are not aware of any previous efforts to gather syntactic data (such as pp attach ments) in the genre.
</prevsent>
<prevsent>syntactic methods such as pos tagging, parsing and structural disambiguation are commonly used when analyzing well-structured text.
</prevsent>
</prevsection>
<citsent citstr=" D09-1076 ">
including the use of syntactic information has yielded improvements inaccuracy in speech recognition (chelba and jelenik, 1998; collins et al., 2005) <papid> P05-1063 </papid>and machine translation (deneefe and knight, 2009; <papid> D09-1076 </papid>carreras and collins, 2009).<papid> D09-1021 </papid></citsent>
<aftsection>
<nextsent>we anticipate that datasets such as ours could be useful for such tasks as well.
</nextsent>
<nextsent>amazons mechanical turk (mturk) has become very popular for manual annotation tasks and has been shown to perform equally well over labeling tasks such as affect recognition, word similarity, recognizing textual entailment, event temporal ordering and word sense disambiguation, when compared to annotations from experts (snow et al, 2008).
</nextsent>
<nextsent>while these tasks were small in scale and intended to demonstrate the viability of annotation via mturk,it has also proved effective in large-scale tasks including the collection of accurate speech transcriptions (gruenstein et al, 2009).
</nextsent>
<nextsent>in this paper we explore method for corpus building on large scale in order to extend annotation into new domains and genres.we previously evaluated crowd sourced pp attachment annotation by using mturk workers to reproduce pp attachments from the wall street journal corpus (rosenthal et al, 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1219">
<title id=" W10-0702.xml">corpus creation for new genres a crowd sourced approach to pp attachment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this simplification of the pp attachment task to choice between two alternative sis unrealistic when considering the potential long distance attachments encountered in real-world text.while blogs and other web text, such as discussion forums and emails, have been studied for variety of tasks such as information extraction (hong and davison, 2009), social networking (gruhl et al., 2004), and sentiment analysis (leshed andkaye, 2006), we are not aware of any previous efforts to gather syntactic data (such as pp attach ments) in the genre.
</prevsent>
<prevsent>syntactic methods such as pos tagging, parsing and structural disambiguation are commonly used when analyzing well-structured text.
</prevsent>
</prevsection>
<citsent citstr=" D09-1021 ">
including the use of syntactic information has yielded improvements inaccuracy in speech recognition (chelba and jelenik, 1998; collins et al., 2005) <papid> P05-1063 </papid>and machine translation (deneefe and knight, 2009; <papid> D09-1076 </papid>carreras and collins, 2009).<papid> D09-1021 </papid></citsent>
<aftsection>
<nextsent>we anticipate that datasets such as ours could be useful for such tasks as well.
</nextsent>
<nextsent>amazons mechanical turk (mturk) has become very popular for manual annotation tasks and has been shown to perform equally well over labeling tasks such as affect recognition, word similarity, recognizing textual entailment, event temporal ordering and word sense disambiguation, when compared to annotations from experts (snow et al, 2008).
</nextsent>
<nextsent>while these tasks were small in scale and intended to demonstrate the viability of annotation via mturk,it has also proved effective in large-scale tasks including the collection of accurate speech transcriptions (gruenstein et al, 2009).
</nextsent>
<nextsent>in this paper we explore method for corpus building on large scale in order to extend annotation into new domains and genres.we previously evaluated crowd sourced pp attachment annotation by using mturk workers to reproduce pp attachments from the wall street journal corpus (rosenthal et al, 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1220">
<title id=" W09-2108.xml">a method for unsupervised broad coverage lexical error detection and correction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a problem with this use of search engines like google is that such searches commonly provide false positives, hits for strings that contain errors.
</prevsent>
<prevsent>lexbar accepts as user input candidate strings of english to be checked for acceptability and, where errors are detected, offers corrections.
</prevsent>
</prevsection>
<citsent citstr=" A00-2019 ">
among the many works on error detection, recently unsupervised error detection approaches have been proposed, such as [chodorow and leacock, 2000] <papid> A00-2019 </papid>and [quixal and badia 2008].</citsent>
<aftsection>
<nextsent>these use contextual features and statistical word association measurement to decide if the detected bigram or trigram is an error or not.
</nextsent>
<nextsent>to our knowledge, such unsupervised methods have not been applied in error correction.
</nextsent>
<nextsent>[gamon et al 2008] <papid> I08-1059 </papid>and [felice and pulman 2008] propose unsupervised approaches to build probabilistic model for detecting errors (prepositions and arti cles) and providing correct answers.</nextsent>
<nextsent>they also typically focus on particular type of error, usually limited to specific word class such as preposition errors, often in pre-determined paradigmatic slot.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1221">
<title id=" W09-2108.xml">a method for unsupervised broad coverage lexical error detection and correction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>these use contextual features and statistical word association measurement to decide if the detected bigram or trigram is an error or not.
</prevsent>
<prevsent>to our knowledge, such unsupervised methods have not been applied in error correction.
</prevsent>
</prevsection>
<citsent citstr=" I08-1059 ">
[gamon et al 2008] <papid> I08-1059 </papid>and [felice and pulman 2008] propose unsupervised approaches to build probabilistic model for detecting errors (prepositions and arti cles) and providing correct answers.</citsent>
<aftsection>
<nextsent>they also typically focus on particular type of error, usually limited to specific word class such as preposition errors, often in pre-determined paradigmatic slot.
</nextsent>
<nextsent>our approach reported here is unsupervised in both detection and correction and is not tailored to specific target error subtype or targeted to specific position in string.
</nextsent>
<nextsent>more generally the family of error types suitable for this approach are lexical or lexico-grammatical errors since detection and correction are based on patterns of word use detected statistically.
</nextsent>
<nextsent>at the core of our approach is bank of what we call hybrid n-grams?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1222">
<title id=" W10-0208.xml">evaluation of unsupervised emotion models to textual affect recognition </title>
<section> emotion-labeled data.  </section>
<citcontext>
<prevsection>
<prevsent>three emotional datasets, with sentence-level emotion annotations, were employed for the evaluation described in the next section.
</prevsent>
<prevsent>the first dataset is affective text?
</prevsent>
</prevsection>
<citsent citstr=" W07-2013 ">
from the semeval 2007 task (strapparava and mihalcea 2007).<papid> W07-2013 </papid></citsent>
<aftsection>
<nextsent>1 we also use the isear (international survey on emotion antecedents and reactions) dataset, which consists of 7,666 sentences (scherer and wall bott 1994), with regard to our experiments.
</nextsent>
<nextsent>this dataset consists of news headlines excerpted from newspapers and news web sites.
</nextsent>
<nextsent>headlines are suitable for our experiments because headlines are typically intended to express emotions in order to draw the readers?
</nextsent>
<nextsent>attention.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1223">
<title id=" W09-2012.xml">how creative is your writing </title>
<section> candidate features for predicting.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 the computer science perspective:.
</prevsent>
<prevsent>language modeling we start from the following hypothesis: if the wordsin the sentence frequently co-occur with the key word z, then is probably not creative.
</prevsent>
</prevsection>
<citsent citstr=" D07-1090 ">
this is of course an over-simplification, as many creative sentences are about novel usage of common words1.nonetheless, this hypothesis inspires some candidate features that can be computed from large cor pus.in this study, we use the google web 1t 5 gram corpus (brants et al, 2007).<papid> D07-1090 </papid></citsent>
<aftsection>
<nextsent>this corpus was generated from about 1012 word tokens from web pages.
</nextsent>
<nextsent>it consists of counts of n-gram for = 1, . . .
</nextsent>
<nextsent>, 5.
</nextsent>
<nextsent>we denote the words in sentence by = x1, . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1224">
<title id=" W09-2012.xml">how creative is your writing </title>
<section> candidate features for predicting.  </section>
<citcontext>
<prevsection>
<prevsent>for each sentence x, we first exclude the keyword from the sentence.
</prevsent>
<prevsent>we also remove punctuations, and map all words to lowercase.
</prevsent>
</prevsection>
<citsent citstr=" W02-0109 ">
we further remove all stop words using the snowball stop word list (porter, 2001), and stem all words in the sentence and the norm word list using nltk (loper and bird, 2002).<papid> W02-0109 </papid></citsent>
<aftsection>
<nextsent>we then count the number of words xi that appear in the norm list of the keyword in the leuven data.
</nextsent>
<nextsent>let this count be cnorm(x, z).
</nextsent>
<nextsent>the feature is the fraction of such norm words in the original sentence: f4(x, z) = cnorm(x, z)n .
</nextsent>
<nextsent>(8) it is worth noting that the leuven dataset is relatively small, with less than two thousand keywords.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1226">
<title id=" W09-2501.xml">multiword expressions in textual inference much ado about nothing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(rte) task (dagan et al , 2006).
</prevsent>
<prevsent>our analysis ties in with the pivotal question of what types of knowledge are beneficial for rte.
</prevsent>
</prevsection>
<citsent citstr=" W07-1414 ">
a number of papers have suggested that paraphrase knowledge plays very important role (bar-haim et al , 2005; marsi et al ,2007; <papid> W07-1414 </papid>dinu and wang, 2009).</citsent>
<aftsection>
<nextsent>for example, bar haim et al  (2005) conclude: our analysis also shows that paraphrases stand out as dominant contributor to the entailment task.the term paraphrase?
</nextsent>
<nextsent>is however often construed broadly.
</nextsent>
<nextsent>in bar-haim et al  (2005), it refer sto the ability of relating lexico-syntactic reformulations such as dia thesis alternations, passivizations, or symmetrical predicates (x lent his bmw to y/y borrowed xs bmw).
</nextsent>
<nextsent>if paraphrase?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1227">
<title id=" W09-2501.xml">multiword expressions in textual inference much ado about nothing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the absence of universally accepted definition of mwes, we define mwes in the rte setting as multi-word alignments, i.e., words that participate in more than one word alignment link between premise and hypothesis: (1) pre: he died.
</prevsent>
<prevsent>hyp: he kicked the bucket.the exclusion of mwes that do not lead to multiword alignments (i.e., which can be aligned word by word) is not significant loss, since these cases are unlikely to cause significant problems for rte.
</prevsent>
</prevsection>
<citsent citstr=" W07-1402 ">
in addition, an alignment-based approach has the advantage of generality: almost all existing rte models align the linguistic material of the premise 1and hypothesis and base at least part of their decision on properties of this alignment (burchardt et al , 2007; <papid> W07-1402 </papid>hickl and bensley, 2007; <papid> W07-1428 </papid>iftene and balahur-dobrescu, 2007; zanzotto et al , 2007).<papid> W07-1412 </papid></citsent>
<aftsection>
<nextsent>we proceed in three steps.
</nextsent>
<nextsent>first, we analyze the microsoft research (msr) manual word alignments (brockett, 2007) for the rte2 dataset (barhaim et al , 2006), shedding light on the relationship between alignments and multi-word expressions.
</nextsent>
<nextsent>we provide frequency estimates anda coarse-grained classification scheme for multiword expressions on textual entailment data.
</nextsent>
<nextsent>next, we analyze two widely used types of paraphrase resources with respect to their modeling of mwes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1228">
<title id=" W09-2501.xml">multiword expressions in textual inference much ado about nothing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the absence of universally accepted definition of mwes, we define mwes in the rte setting as multi-word alignments, i.e., words that participate in more than one word alignment link between premise and hypothesis: (1) pre: he died.
</prevsent>
<prevsent>hyp: he kicked the bucket.the exclusion of mwes that do not lead to multiword alignments (i.e., which can be aligned word by word) is not significant loss, since these cases are unlikely to cause significant problems for rte.
</prevsent>
</prevsection>
<citsent citstr=" W07-1428 ">
in addition, an alignment-based approach has the advantage of generality: almost all existing rte models align the linguistic material of the premise 1and hypothesis and base at least part of their decision on properties of this alignment (burchardt et al , 2007; <papid> W07-1402 </papid>hickl and bensley, 2007; <papid> W07-1428 </papid>iftene and balahur-dobrescu, 2007; zanzotto et al , 2007).<papid> W07-1412 </papid></citsent>
<aftsection>
<nextsent>we proceed in three steps.
</nextsent>
<nextsent>first, we analyze the microsoft research (msr) manual word alignments (brockett, 2007) for the rte2 dataset (barhaim et al , 2006), shedding light on the relationship between alignments and multi-word expressions.
</nextsent>
<nextsent>we provide frequency estimates anda coarse-grained classification scheme for multiword expressions on textual entailment data.
</nextsent>
<nextsent>next, we analyze two widely used types of paraphrase resources with respect to their modeling of mwes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1229">
<title id=" W09-2501.xml">multiword expressions in textual inference much ado about nothing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the absence of universally accepted definition of mwes, we define mwes in the rte setting as multi-word alignments, i.e., words that participate in more than one word alignment link between premise and hypothesis: (1) pre: he died.
</prevsent>
<prevsent>hyp: he kicked the bucket.the exclusion of mwes that do not lead to multiword alignments (i.e., which can be aligned word by word) is not significant loss, since these cases are unlikely to cause significant problems for rte.
</prevsent>
</prevsection>
<citsent citstr=" W07-1412 ">
in addition, an alignment-based approach has the advantage of generality: almost all existing rte models align the linguistic material of the premise 1and hypothesis and base at least part of their decision on properties of this alignment (burchardt et al , 2007; <papid> W07-1402 </papid>hickl and bensley, 2007; <papid> W07-1428 </papid>iftene and balahur-dobrescu, 2007; zanzotto et al , 2007).<papid> W07-1412 </papid></citsent>
<aftsection>
<nextsent>we proceed in three steps.
</nextsent>
<nextsent>first, we analyze the microsoft research (msr) manual word alignments (brockett, 2007) for the rte2 dataset (barhaim et al , 2006), shedding light on the relationship between alignments and multi-word expressions.
</nextsent>
<nextsent>we provide frequency estimates anda coarse-grained classification scheme for multiword expressions on textual entailment data.
</nextsent>
<nextsent>next, we analyze two widely used types of paraphrase resources with respect to their modeling of mwes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1230">
<title id=" W09-2501.xml">multiword expressions in textual inference much ado about nothing </title>
<section> multi-word expressions in alignment.  </section>
<citcontext>
<prevsection>
<prevsent>the computation of word alignments is usually phrased as an optimization task.
</prevsent>
<prevsent>the search space is based on lexical similarities, but usually extended with structural biases in order to obtain alignments with desirable properties, such as the contiguous alignment of adjacent words, orthe mapping of different source words on to different target words.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
one prominent constraint of the ibm word alignment models (brown et al , 1993) <papid> J93-2003 </papid>is functional alignment, that is each target word is mapped onto at most one source word.</citsent>
<aftsection>
<nextsent>other models produce only one-to-one alignments, where both alignment directions must be functional.mwes that involve many-to-many or one-to many alignments like ex.
</nextsent>
<nextsent>(1) present problem for such constrained word alignment models.
</nextsent>
<nextsent>a functional alignment model can still handle cases like ex.
</nextsent>
<nextsent>(1) correctly in one direction (from bottom to top), but not in the other one.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1231">
<title id=" W09-2501.xml">multiword expressions in textual inference much ado about nothing </title>
<section> mwes in paraphrase resources.  </section>
<citcontext>
<prevsection>
<prevsent>alignments (capital punishment ? death penalty) as well as for one-to-many?
</prevsent>
<prevsent>alignments (vote ? cast ballots).
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
such similarities are not present in standard lexical resources like wordnet or dekang lins thesaurus (lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>the best class of candidate resources to provide wide-coverage of multi-word similarities seems tobe paraphrase resources.
</nextsent>
<nextsent>in this section, we examine to what extent two of the most widely used paraphrase resource types provide supporting evidence for the true mwes in the msr data.
</nextsent>
<nextsent>we deliberately use corpus-derived, noisy resources, since we are interested in the real-world (ratherthan idealized) prospects for accurate mwe align ment.dependency-based paraphrases.
</nextsent>
<nextsent>lin and pantel (2002)s dirt model collects lexicalized dependency paths with two slots at either end.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1232">
<title id=" W09-2501.xml">multiword expressions in textual inference much ado about nothing </title>
<section> impact of mwes on practical.  </section>
<citcontext>
<prevsection>
<prevsent>results of this experiment are not guaranteed to transfer to other rte system architectures, or to future, improved paraphrase resources, it provides current snapshot of the practical impact of mwe handling.
</prevsent>
<prevsent>5.1 the stanford rte system.
</prevsent>
</prevsection>
<citsent citstr=" N06-1006 ">
we base our experiments on the stanford rte system which uses staged architecture (maccartney et al , 2006).<papid> N06-1006 </papid></citsent>
<aftsection>
<nextsent>after the linguistic analysis which produces dependency graphs for premise and hypothesis, the alignment stage creates links between the nodes of the two dependency trees.
</nextsent>
<nextsent>in the inference stage, the system produces roughly 70 features for the aligned premise-hypothesis pair, almost all of which are implementations of small linguistictheories?
</nextsent>
<nextsent>whose activation indicates lexical, syntactic and semantic matches and mismatches of different types.
</nextsent>
<nextsent>the entailment decision is computed using logistic regression on these features.the stanford system supports the use of different align ers without touching the rest of the pipeline.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1233">
<title id=" W09-2501.xml">multiword expressions in textual inference much ado about nothing </title>
<section> impact of mwes on practical.  </section>
<citcontext>
<prevsection>
<prevsent>whose activation indicates lexical, syntactic and semantic matches and mismatches of different types.
</prevsent>
<prevsent>the entailment decision is computed using logistic regression on these features.the stanford system supports the use of different align ers without touching the rest of the pipeline.
</prevsent>
</prevsection>
<citsent citstr=" D08-1084 ">
we compare two aligners: one-to-one aligner, which cannot construct mwe alignments (uniq), and many-to-many aligner (manli)(maccartney et al , 2008), <papid> D08-1084 </papid>which can.</citsent>
<aftsection>
<nextsent>both align ers use around 10 large-coverage lexical resources of semantic similarities, both manually compiled resources (such as wordnet and nombank) and automatically induced resources (such as dekang lins distributional thesaurus or infomap).
</nextsent>
<nextsent>uniq: one-to-one aligner.
</nextsent>
<nextsent>uniq constructs an alignment between dependency graphs as the highest-scoring mapping from each word in the hypothesis to one word in the premise, or to null.
</nextsent>
<nextsent>mappings are scored by summing the alignment scores of all individual word pairs (provided by the lexical resources), plus edge alignment scores that 5use the syntactic structure of premise and hypothesis to introduce bias for syntactic parallelism.the large number of possible alignments (expo nential in the number of hypothesis words) makes exhaustive search intractable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1234">
<title id=" W09-2501.xml">multiword expressions in textual inference much ado about nothing </title>
<section> impact of mwes on practical.  </section>
<citcontext>
<prevsection>
<prevsent>substitution edits also use contextual features, including distortion score and matching-neighbors feature.
</prevsent>
<prevsent>4 due to the dependence between alignment and segmentation decisions, manli uses simulated annealing strategy to traverse the resulting large search space.even though manli is our current best candidate at recovering mwe alignments, it currently has an important architectural limitation: it workson textual phrases rather than dependency tree fragments, and therefore misses all mwes that are not contiguous (e.g., due to inserted articles or adver 3 positive weights for all operation types ensure that manli prefers small over large edits where appropriate.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
4 an adaptation of the averaged perceptron algorithm (collins, 2002) <papid> W02-1001 </papid>is used to tune the model parameters.</citsent>
<aftsection>
<nextsent>micro-avg r 1 uniq w/o para 80.4 80.8 80.6 manli w/o para 77.0 85.5 81.0 w/ para 76.7 85.4 80.8 table 4: evaluation of align ers and resources against the manual msr rte2 test annotations.
</nextsent>
<nextsent>bials).
</nextsent>
<nextsent>this accounts for roughly 9% of the mwes in rte2 data.
</nextsent>
<nextsent>other work on rte has targeted specifically this observation and has described paraphrases on dependency level (marsi et al , 2007; <papid> W07-1414 </papid>dinu and wang, 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1236">
<title id=" W10-0202.xml">emotion detection in email customer care </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude the paper by highlighting the main contribution of this work in section 5.
</prevsent>
<prevsent>extensive work has been done on emotion detection.
</prevsent>
</prevsection>
<citsent citstr=" H05-1073 ">
in the context of human-computer dialogs, although richer features including acoustic and intonation are available, there is general consensus (lit man and forbes-riley, 2004b; lee and narayanan,2005) about the use of lexical features to significantly improve the accuracy of emotion detection.research has also been done in predicting basic emotions (also referred to as affects) within text (alm et al, 2005; <papid> H05-1073 </papid>liu et al, 2003).</citsent>
<aftsection>
<nextsent>to render speech with prosodic contour conveying the emotional content of the text, one of 6 types of human emotions(e.g., angry, disgusted, fearful, happy, sad, and surprised) are identified for each sentence in the running text.
</nextsent>
<nextsent>deducing such emotions from lexical constructs is hard problem evidenced by little agreement among humans.
</nextsent>
<nextsent>a kappa value of 0.24-0.51 was shown in alm et al (2005).<papid> H05-1073 </papid></nextsent>
<nextsent>liu et al (2003) have argued that the absence of affect laden surface features i.e., keywords, from the text does not imply absence of emotions, therefore they have relied more on common-sense knowledge.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1238">
<title id=" W10-0202.xml">emotion detection in email customer care </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of deducing types emotions in each sentence, we are interested in knowing if the entire email is emotional or not.
</prevsent>
<prevsent>additionally we are also interested in the intensity and the cause of those emotions.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
there is also body of work in areas of creating semantic orientation (so) dictionaries (hatzivas siloglou and mckeown, 1997; <papid> P97-1023 </papid>turney and littman, 2003; esuli and sebastiani, 2005) and their use in identifying emotions laden sentences and polarity (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>kim and hovy, 2004; <papid> C04-1200 </papid>hu and liu, 2004) of those emotions.</citsent>
<aftsection>
<nextsent>while such dictionaries provide useful starting point, their use alone does not yield satisfactory results.
</nextsent>
<nextsent>in wilson et al (2005), <papid> H05-1044 </papid>classification of phrases containing positive, negative or neutral emotions is dis cussed.</nextsent>
<nextsent>for this problem they show high agreement among human annotators (kappa of 0.84).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1239">
<title id=" W10-0202.xml">emotion detection in email customer care </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of deducing types emotions in each sentence, we are interested in knowing if the entire email is emotional or not.
</prevsent>
<prevsent>additionally we are also interested in the intensity and the cause of those emotions.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
there is also body of work in areas of creating semantic orientation (so) dictionaries (hatzivas siloglou and mckeown, 1997; <papid> P97-1023 </papid>turney and littman, 2003; esuli and sebastiani, 2005) and their use in identifying emotions laden sentences and polarity (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>kim and hovy, 2004; <papid> C04-1200 </papid>hu and liu, 2004) of those emotions.</citsent>
<aftsection>
<nextsent>while such dictionaries provide useful starting point, their use alone does not yield satisfactory results.
</nextsent>
<nextsent>in wilson et al (2005), <papid> H05-1044 </papid>classification of phrases containing positive, negative or neutral emotions is dis cussed.</nextsent>
<nextsent>for this problem they show high agreement among human annotators (kappa of 0.84).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1240">
<title id=" W10-0202.xml">emotion detection in email customer care </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of deducing types emotions in each sentence, we are interested in knowing if the entire email is emotional or not.
</prevsent>
<prevsent>additionally we are also interested in the intensity and the cause of those emotions.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
there is also body of work in areas of creating semantic orientation (so) dictionaries (hatzivas siloglou and mckeown, 1997; <papid> P97-1023 </papid>turney and littman, 2003; esuli and sebastiani, 2005) and their use in identifying emotions laden sentences and polarity (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>kim and hovy, 2004; <papid> C04-1200 </papid>hu and liu, 2004) of those emotions.</citsent>
<aftsection>
<nextsent>while such dictionaries provide useful starting point, their use alone does not yield satisfactory results.
</nextsent>
<nextsent>in wilson et al (2005), <papid> H05-1044 </papid>classification of phrases containing positive, negative or neutral emotions is dis cussed.</nextsent>
<nextsent>for this problem they show high agreement among human annotators (kappa of 0.84).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1241">
<title id=" W10-0202.xml">emotion detection in email customer care </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>there is also body of work in areas of creating semantic orientation (so) dictionaries (hatzivas siloglou and mckeown, 1997; <papid> P97-1023 </papid>turney and littman, 2003; esuli and sebastiani, 2005) and their use in identifying emotions laden sentences and polarity (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>kim and hovy, 2004; <papid> C04-1200 </papid>hu and liu, 2004) of those emotions.</prevsent>
<prevsent>while such dictionaries provide useful starting point, their use alone does not yield satisfactory results.</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
in wilson et al (2005), <papid> H05-1044 </papid>classification of phrases containing positive, negative or neutral emotions is dis cussed.</citsent>
<aftsection>
<nextsent>for this problem they show high agreement among human annotators (kappa of 0.84).
</nextsent>
<nextsent>they also show that labeling phrases as positive, negative or neutral only on the basis of presence of key word from such dictionaries yields classification accuracy of 48%.
</nextsent>
<nextsent>an obvious reason for this poor performance is that semantic orientations of words are context dependent.
</nextsent>
<nextsent>works reported in wilson et al (2005); <papid> H05-1044 </papid>pang et al (2002) <papid> W02-1011 </papid>and dave et al (2003) have attempted to mitigate this problem by using supervised methods.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1243">
<title id=" W10-0202.xml">emotion detection in email customer care </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they also show that labeling phrases as positive, negative or neutral only on the basis of presence of key word from such dictionaries yields classification accuracy of 48%.
</prevsent>
<prevsent>an obvious reason for this poor performance is that semantic orientations of words are context dependent.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
works reported in wilson et al (2005); <papid> H05-1044 </papid>pang et al (2002) <papid> W02-1011 </papid>and dave et al (2003) have attempted to mitigate this problem by using supervised methods.</citsent>
<aftsection>
<nextsent>they report classification results using number of different sets of features, including unigramword features.
</nextsent>
<nextsent>wilson et al (2005) <papid> H05-1044 </papid>reports an improvement (63% to 65.7% accuracy) in performance by using host of features extracted from syntactic dependencies.</nextsent>
<nextsent>similarly, gamon (2004) <papid> C04-1121 </papid>shows that the use of deep semantic features along with word unigrams improve performances.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1246">
<title id=" W10-0202.xml">emotion detection in email customer care </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they report classification results using number of different sets of features, including unigramword features.
</prevsent>
<prevsent>wilson et al (2005) <papid> H05-1044 </papid>reports an improvement (63% to 65.7% accuracy) in performance by using host of features extracted from syntactic dependencies.</prevsent>
</prevsection>
<citsent citstr=" C04-1121 ">
similarly, gamon (2004) <papid> C04-1121 </papid>shows that the use of deep semantic features along with word unigrams improve performances.</citsent>
<aftsection>
<nextsent>pang et al (2002) <papid> W02-1011 </papid>and dave et al (2003) on the other hand confirmed that word unigrams provide the best classification results.</nextsent>
<nextsent>this is in line with our experience as well and could be due to sparseness of the data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1249">
<title id=" W09-2704.xml">some challenges in the design of comparative and evaluative question answering systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since evaluation is necessary, the answer is not lifted from source text, as in the case of answering factoid, definition, or list questions.
</prevsent>
<prevsent>instead, natural language answers will have to be constructed from the results of numeric and non-numeric evaluations of the criteria.currently, to our knowledge, there are no systems that answer comparative and evaluative questions.
</prevsent>
</prevsection>
<citsent citstr=" P89-1021 ">
the closest applications to comparing or evaluating information are implemented through natural language database interfaces (olawsky,1989) <papid> P89-1021 </papid>and database queries (e.g., via sql state ments).</citsent>
<aftsection>
<nextsent>in the former, the user is prompted to choose among set of candidate interpretations of comparative expressions to indicate his intent.
</nextsent>
<nextsent>the comparisons are based on quantifiable predicates (i.e., those measurable by count, mass, or value).using database queries restrict the possible questions that can be raised and is far less natural and user-friendly than using human language.
</nextsent>
<nextsent>it also does not allow producing cooperative responses.recent researches in linguistics on the semantics of comparatives and superlatives (kennedy,2006) can be used as basis in answering comparative and evaluative questions.
</nextsent>
<nextsent>the next section discusses some challenges we have identified as crucial for the development of comparative and evaluative qa systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1250">
<title id=" W09-2704.xml">some challenges in the design of comparative and evaluative question answering systems </title>
<section> challenges.  </section>
<citcontext>
<prevsection>
<prevsent>in the second case, relevant data must be searched on the web.
</prevsent>
<prevsent>a straightforward procedure 16consists of extracting keywords from the question, then getting results from search engines from which, via local grammars associated to properties, relevant values may be extracted.
</prevsent>
</prevsection>
<citsent citstr=" W06-1808 ">
we already successfully conducted such an experiment for numerical data fusion (moriceau, 2006).<papid> W06-1808 </papid></citsent>
<aftsection>
<nextsent>2.3 response generation.
</nextsent>
<nextsent>the answer cannot be lifted from the source text, thus response generator component should be part of comparative and evaluative qa system.
</nextsent>
<nextsent>as response is to be generated from the resultsof numeric and textual comparisons of the criteria, it is necessary to go through complex sentence generation, involving comparative expressions.
</nextsent>
<nextsent>incase the response is not direct, it is also necessary to elaborate adapted forms of cooperativity,by providing the user with adequate forms of explanations, elaborations, examples (of properties), and other relevant information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1251">
<title id=" W10-0408.xml">exploring individual differences in student writing with a narrative composition support environment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>sourcers apprentice is web-based learning environment to help high school students gather, evaluate, and integrate information for writing essays about history topics (britt et al, 2004), although sourcers apprentice did not seek to apply nlp tools to understand or scaffold students?
</prevsent>
<prevsent>compositions directly.
</prevsent>
</prevsection>
<citsent citstr=" N07-1058 ">
other work on intelligent tutoring for language arts, such as project listen (mo stow and aist, 2001) and reap (heilman et al, 2007), <papid> N07-1058 </papid>has addressed vocabulary learning and reading comprehension.</citsent>
<aftsection>
<nextsent>to investigate narrative composition in novice writers, study was conducted with more than one hundred middle grade students using narrative composition support environment.
</nextsent>
<nextsent>the narrative theatre (figure 1) is an interactive environment designed to capture both the process and products of writing.1 targeting user population of sixth grade students (age typically 12 years) and the genre of fables, the narrative theatre enables students to create stories in an environment that was specifically designed to scaffold novices?
</nextsent>
<nextsent>composition activities during timed story plan 1 the version of the narrative theatre used in the study.
</nextsent>
<nextsent>reported in this paper is the forerunner of more general creativity support environment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1252">
<title id=" W10-0609.xml">using fmri activation to conceptual stimuli to evaluate methods for extracting conceptual representations from corpora </title>
<section> semantic models.  </section>
<citcontext>
<prevsection>
<prevsent>sim 3http://www.cs.cmu.edu/tom/science2008/ semanticfeaturevectors.html 4in baroni et al implementation context window of 5 (baroni and lenci, 2008) or 20 (baroni et al , 2009) words either side of the target word was used instead; we chose sentence-based context window as it is analogous to the context used in our experimental method (described in the following section).
</prevsent>
<prevsent>ilarity between pairs of target words was calculated as the cosine between their vectors, and for each of the 60 concept words in the experimental stimuli we chose the 200 most similar target words to act as the feature terms extracted by the model.
</prevsent>
</prevsection>
<citsent citstr=" C94-1103 ">
the corpus used with this model was the british national corpus (bnc) (leech et al , 1994).<papid> C94-1103 </papid></citsent>
<aftsection>
<nextsent>2.3 novel extraction method.
</nextsent>
<nextsent>finally we implemented novel extraction method,which aims to extract property-norm-like, psychologically meaningful features from corpus data(kelly et al , 2010).
</nextsent>
<nextsent>the method aims to extract semantically unconstrained feature triples of the form concept-relation-feature , where feature is feature (either noun or adjective) of the target concept and relation is verb representing the semantic relationship between them.
</nextsent>
<nextsent>examples of extracted triplesinclude: swan be white, swan have neck and screwdriver be tool.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1253">
<title id=" W10-0609.xml">using fmri activation to conceptual stimuli to evaluate methods for extracting conceptual representations from corpora </title>
<section> semantic models.  </section>
<citcontext>
<prevsection>
<prevsent>the method aims to extract semantically unconstrained feature triples of the form concept-relation-feature , where feature is feature (either noun or adjective) of the target concept and relation is verb representing the semantic relationship between them.
</prevsent>
<prevsent>examples of extracted triplesinclude: swan be white, swan have neck and screwdriver be tool.
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
the model uses corpus parsed for grammatical relations (grs) using robust accurate statistical parsing (rasp) (briscoe et al , 2006).<papid> P06-4020 </papid></citsent>
<aftsection>
<nextsent>for each sentence containing target concept, the set of grs for that sentence are examined to test whether they match manually-created rules.
</nextsent>
<nextsent>these rules include prototypical feature-relation gr structures connecting elements of the sentence and represent dependency patterns which encode potential semantic relationships between the concept and candidate feature terms occurring in the sentence.
</nextsent>
<nextsent>alarge set of candidate triples are extracted by applying these rules to each sentence in the corpus containing target concept, and the triples for each concept are ranked by their frequency of extraction.
</nextsent>
<nextsent>in the second stage of the method, the extracted triples are re weighted on the basis of probabilistichigh-level semantic information obtained from human property norm data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1254">
<title id=" W10-0718.xml">opinion mining of spanish customer comments with non expert annotations on mechanical turk </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this reason, crowdsourcing platforms such as amazons mechanical turk1, crowdflower2 and others have recently attracted lot of attention from both companies and academia.
</prevsent>
<prevsent>crowdsourcing enables requesters to tap from global pool of non-experts to obtain rapid and affordable answers to simple human intelligence tasks (hits), which 1https://www.mturk.com 2http://crowdflower.com/can be subsequently used to train data-driven applications.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
a number of recent papers on this subject point out that non-expert annotations, if produced insufficient quantity, can rival and even surpass the quality of expert annotations, often at much lower cost (snow et al, 2008), (<papid> D08-1027 </papid>su et al, 2007).</citsent>
<aftsection>
<nextsent>however, this possible increase in quality depends on the task at hand and on an adequate hit design (kittur et al, 2008).
</nextsent>
<nextsent>in this paper, we evaluate the usefulness of mturk annotations to train an opinion mining system to detect opinionated contents (polarity detection) in spanish customer comments on car brands.
</nextsent>
<nextsent>currently, large majority of mturk tasks is designed for english speakers.
</nextsent>
<nextsent>one of our reasons for participating in this shared task was to find out how easyit is to obtain annotated data for spanish.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1256">
<title id=" W10-0514.xml">labelling and spatiotemporal grounding of news events </title>
<section> introduction: news event analysis.  </section>
<citcontext>
<prevsection>
<prevsent>1.2 news event location.
</prevsent>
<prevsent>we use the edinburgh geo parser (tobin et al, 2010) to recognise location names and ground them to the geo names gazetteer.2 besides latitudes, longitudes and geo names ids, we also assign population size and type of location (e.g. populated place, country etc.).
</prevsent>
</prevsection>
<citsent citstr=" L08-1017 ">
our geo parser yields 81.2% accuracy when evaluating on spatialml (mani et al, 2008).<papid> L08-1017 </papid></citsent>
<aftsection>
<nextsent>it also compares favourably with yahoo!
</nextsent>
<nextsent>placemaker3 in an end-to-end run.
</nextsent>
<nextsent>2http://www.geonames.org 3http://developer.yahoo.com/geo/ place maker 27 we only consider locations grounded to lat/longvalues as potential news item locations, therefore restricting the set to more accurately recognised ones.we select the first location in the label and description or (if none can be found) either the first or most frequent location in the news item.
</nextsent>
<nextsent>the news item location associated with the most representative cluster label is selected as the news event location.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1257">
<title id=" W09-2010.xml">an unsupervised model for text message normalization </title>
<section> text messaging.  </section>
<citcontext>
<prevsection>
<prevsent>normalization of non-standard forms?
</prevsent>
<prevsent>converting non-standard forms to their standard formsis challenge that must be tackled before other types of natural language processing can take place (sproat et al , 2001).
</prevsent>
</prevsection>
<citsent citstr=" P06-2005 ">
in the case of text messages, text-to-speech synthesis may be 1the number of characters in text message may also be limited to 160 characters, although this is not always the case.particularly useful for the visually impaired; automatic translation has also been considered (e.g., aw et al , 2006).<papid> P06-2005 </papid></citsent>
<aftsection>
<nextsent>for texting language, given the abundance of creative forms, and the wide-ranging possibilities for creating new forms, normalization is particularly important problem, and has indeed received some attention in computational linguistics (e.g., aw et al , 2006; <papid> P06-2005 </papid>choudhury et al , 2007; kobus et al , 2008).</nextsent>
<nextsent>in this paper we propose an unsupervised noisy channel method for texting language normalization,that gives performance on par with that of supervised system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1259">
<title id=" W09-2010.xml">an unsupervised model for text message normalization </title>
<section> an unsupervised noisy channel model.  </section>
<citcontext>
<prevsection>
<prevsent>the other less frequent formations phonetic abbreviations, spelling errors, and suffix clippings are not modeled; we hypothesize that the similarity of these formation processes to those we do model will allow the system to perform reasonably well on them.
</prevsent>
<prevsent>3.1.1 stylistic variation swe propose probabilistic version of edit distance referred to here as edit-probability?
</prevsent>
</prevsection>
<citsent citstr=" P00-1037 ">
inspired by brill and moore (2000) <papid> P00-1037 </papid>to modelp (ti|si, stylistic variation).</citsent>
<aftsection>
<nextsent>to compute edit probability, we consider the probability of each edit operation substitution, insertion, and deletion instead of its cost, as in edit-distance.
</nextsent>
<nextsent>we then simply multiply the probabilities of edits as opposed to summing their costs.in this version of edit-probability, we allow two character edits.
</nextsent>
<nextsent>ideally, we would compute the edit probability of two strings as the sum of the edit probability of each partitioning of those strings into one or two character segments.
</nextsent>
<nextsent>however, following brill and moore, we approximate this by the probability of the partition with maximum probability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1260">
<title id=" W09-2010.xml">an unsupervised model for text message normalization </title>
<section> an unsupervised noisy channel model.  </section>
<citcontext>
<prevsection>
<prevsent>we compute edit-probability between the graphemes of si and ti.
</prevsent>
<prevsent>when filling each cell in the chart, we consider edit operations between 73 segments of si and ti of length 02, referred to as and b, respectively.
</prevsent>
</prevsection>
<citsent citstr=" N07-1047 ">
if aligns with phonemes in si, we also consider those phonemes, p. in our lexicon, the graphemes and phonemes of each word are aligned according to the method of jiampojamarn et al  (2007).<papid> N07-1047 </papid></citsent>
<aftsection>
<nextsent>for example, the alignment for without is given in table 2.
</nextsent>
<nextsent>the probability of each edit operation is then determined by three properties the length of a, whether aligns with any phonemes in si, and if so, pas shown below: |a|= 0 or 1, not aligned w/ si phonemes: (b|a, pos) |a|= 2, not aligned w/ si phonemes: 0 |a|= 1 or 2, aligned w/ si phonemes: (b|p, a, pos) 3.1.2 sub sequence abbreviations we model sub sequence abbreviations according to the equation below: (ti|si, subseq abrv) = { if ti is subseq of si 0 otherwise where is constant.
</nextsent>
<nextsent>note that this is similar to the error model for spelling correction presented by mays et al  (1991), in which all words (in our terms, all si) within specified edit-distance of the out-of-vocabulary word (ti in our model) are given equal probability.
</nextsent>
<nextsent>the key difference is that in our formulation, we only consider standard forms for which the texting form is potentially sub sequence abbreviation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1266">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>especially, we refresh the concepts of the structure reordering rules and the dis contiguous phrase rules.
</prevsent>
<prevsent>this novel classification system may supports the smt research community with some helpful references.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
phrase-based statistical machine translation models (marcu and wong, 2002; <papid> W02-1018 </papid>koehn et al, 2003; <papid> N03-1017 </papid>och and ney, 2004; <papid> J04-4002 </papid>koehn, 2004; koehn et al, 2007)<papid> P07-2045 </papid>have achieved significant improvements in translation accuracy over the original ibm word-based model.</citsent>
<aftsection>
<nextsent>however, there are still many limitations in phrase based models.
</nextsent>
<nextsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</nextsent>
<nextsent>to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</nextsent>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1267">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>especially, we refresh the concepts of the structure reordering rules and the dis contiguous phrase rules.
</prevsent>
<prevsent>this novel classification system may supports the smt research community with some helpful references.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
phrase-based statistical machine translation models (marcu and wong, 2002; <papid> W02-1018 </papid>koehn et al, 2003; <papid> N03-1017 </papid>och and ney, 2004; <papid> J04-4002 </papid>koehn, 2004; koehn et al, 2007)<papid> P07-2045 </papid>have achieved significant improvements in translation accuracy over the original ibm word-based model.</citsent>
<aftsection>
<nextsent>however, there are still many limitations in phrase based models.
</nextsent>
<nextsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</nextsent>
<nextsent>to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</nextsent>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1268">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>especially, we refresh the concepts of the structure reordering rules and the dis contiguous phrase rules.
</prevsent>
<prevsent>this novel classification system may supports the smt research community with some helpful references.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
phrase-based statistical machine translation models (marcu and wong, 2002; <papid> W02-1018 </papid>koehn et al, 2003; <papid> N03-1017 </papid>och and ney, 2004; <papid> J04-4002 </papid>koehn, 2004; koehn et al, 2007)<papid> P07-2045 </papid>have achieved significant improvements in translation accuracy over the original ibm word-based model.</citsent>
<aftsection>
<nextsent>however, there are still many limitations in phrase based models.
</nextsent>
<nextsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</nextsent>
<nextsent>to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</nextsent>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1269">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>especially, we refresh the concepts of the structure reordering rules and the dis contiguous phrase rules.
</prevsent>
<prevsent>this novel classification system may supports the smt research community with some helpful references.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
phrase-based statistical machine translation models (marcu and wong, 2002; <papid> W02-1018 </papid>koehn et al, 2003; <papid> N03-1017 </papid>och and ney, 2004; <papid> J04-4002 </papid>koehn, 2004; koehn et al, 2007)<papid> P07-2045 </papid>have achieved significant improvements in translation accuracy over the original ibm word-based model.</citsent>
<aftsection>
<nextsent>however, there are still many limitations in phrase based models.
</nextsent>
<nextsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</nextsent>
<nextsent>to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</nextsent>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1270">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are still many limitations in phrase based models.
</prevsent>
<prevsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</citsent>
<aftsection>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.
</nextsent>
<nextsent>although remarkable progresses have been reported, the strict syntactic constraint(the both sides of the rules should strictly be sub tree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.
</nextsent>
<nextsent>to alleviate this constraint, few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.
</nextsent>
<nextsent>for example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (liu et al, 2006; <papid> P06-1077 </papid>liu et al, 2007).<papid> P07-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1271">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are still many limitations in phrase based models.
</prevsent>
<prevsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</prevsent>
</prevsection>
<citsent citstr=" J07-2003 ">
to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</citsent>
<aftsection>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.
</nextsent>
<nextsent>although remarkable progresses have been reported, the strict syntactic constraint(the both sides of the rules should strictly be sub tree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.
</nextsent>
<nextsent>to alleviate this constraint, few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.
</nextsent>
<nextsent>for example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (liu et al, 2006; <papid> P06-1077 </papid>liu et al, 2007).<papid> P07-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1273">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are still many limitations in phrase based models.
</prevsent>
<prevsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</citsent>
<aftsection>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.
</nextsent>
<nextsent>although remarkable progresses have been reported, the strict syntactic constraint(the both sides of the rules should strictly be sub tree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.
</nextsent>
<nextsent>to alleviate this constraint, few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.
</nextsent>
<nextsent>for example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (liu et al, 2006; <papid> P06-1077 </papid>liu et al, 2007).<papid> P07-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1274">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are still many limitations in phrase based models.
</prevsent>
<prevsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</citsent>
<aftsection>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.
</nextsent>
<nextsent>although remarkable progresses have been reported, the strict syntactic constraint(the both sides of the rules should strictly be sub tree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.
</nextsent>
<nextsent>to alleviate this constraint, few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.
</nextsent>
<nextsent>for example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (liu et al, 2006; <papid> P06-1077 </papid>liu et al, 2007).<papid> P07-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1275">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are still many limitations in phrase based models.
</prevsent>
<prevsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</prevsent>
</prevsection>
<citsent citstr=" P07-1089 ">
to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</citsent>
<aftsection>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.
</nextsent>
<nextsent>although remarkable progresses have been reported, the strict syntactic constraint(the both sides of the rules should strictly be sub tree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.
</nextsent>
<nextsent>to alleviate this constraint, few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.
</nextsent>
<nextsent>for example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (liu et al, 2006; <papid> P06-1077 </papid>liu et al, 2007).<papid> P07-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1277">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are still many limitations in phrase based models.
</prevsent>
<prevsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</prevsent>
</prevsection>
<citsent citstr=" P08-1064 ">
to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</citsent>
<aftsection>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.
</nextsent>
<nextsent>although remarkable progresses have been reported, the strict syntactic constraint(the both sides of the rules should strictly be sub tree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.
</nextsent>
<nextsent>to alleviate this constraint, few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.
</nextsent>
<nextsent>for example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (liu et al, 2006; <papid> P06-1077 </papid>liu et al, 2007).<papid> P07-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1329">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are still many limitations in phrase based models.
</prevsent>
<prevsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</citsent>
<aftsection>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.
</nextsent>
<nextsent>although remarkable progresses have been reported, the strict syntactic constraint(the both sides of the rules should strictly be sub tree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.
</nextsent>
<nextsent>to alleviate this constraint, few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.
</nextsent>
<nextsent>for example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (liu et al, 2006; <papid> P06-1077 </papid>liu et al, 2007).<papid> P07-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1330">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are still many limitations in phrase based models.
</prevsent>
<prevsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</prevsent>
</prevsection>
<citsent citstr=" N04-1035 ">
to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</citsent>
<aftsection>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.
</nextsent>
<nextsent>although remarkable progresses have been reported, the strict syntactic constraint(the both sides of the rules should strictly be sub tree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.
</nextsent>
<nextsent>to alleviate this constraint, few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.
</nextsent>
<nextsent>for example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (liu et al, 2006; <papid> P06-1077 </papid>liu et al, 2007).<papid> P07-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1331">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are still many limitations in phrase based models.
</prevsent>
<prevsent>the most frequently pointed limitation is its in efficacy to modeling the structure reordering and the dis contiguous corresponding.
</prevsent>
</prevsection>
<citsent citstr=" W06-1606 ">
to overcome these limitations, many syntax based smt models have been proposed (wu, 1997; <papid> J97-3002 </papid>chiang, 2007; <papid> J07-2003 </papid>ding et al, 2005; eisner, 2003; <papid> P03-2041 </papid>quirk et al, 2005; <papid> P05-1034 </papid>liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2007; zhang et al, 2008<papid> P08-1064 </papid>a; zhang et al, 2008<papid> P08-1064 </papid>b; gildea, 2003; <papid> P03-1011 </papid>galley et al, 2004; <papid> N04-1035 </papid>marcu et al, 2006; <papid> W06-1606 </papid>bod, 2007).</citsent>
<aftsection>
<nextsent>the basic motivation behind syntax-basedmodel is that the syntax information has the potential to model the structure reordering and dis contiguous corresponding by the intrinsic structural generalization ability.
</nextsent>
<nextsent>although remarkable progresses have been reported, the strict syntactic constraint(the both sides of the rules should strictly be sub tree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.
</nextsent>
<nextsent>to alleviate this constraint, few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.
</nextsent>
<nextsent>for example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (liu et al, 2006; <papid> P06-1077 </papid>liu et al, 2007).<papid> P07-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1332">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although remarkable progresses have been reported, the strict syntactic constraint(the both sides of the rules should strictly be sub tree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.
</prevsent>
<prevsent>to alleviate this constraint, few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.
</prevsent>
</prevsection>
<citsent citstr=" P06-1077 ">
for example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (liu et al, 2006; <papid> P06-1077 </papid>liu et al, 2007).<papid> P07-1089 </papid></citsent>
<aftsection>
<nextsent>zhang et al (2008<papid> P08-1064 </papid>a) made it possible to utilize the non-syntactic rules and even the phrases which are used in phrase based modelby advancing general tree sequence to tree sequence framework based on the tree-to-tree model presented in (zhang et al, 2007).</nextsent>
<nextsent>in these models, various kinds of rules can be employed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1392">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>in other words,some comprehensive rule classifications are necessary to make the rule analyses feasible.
</prevsent>
<prevsent>the motivation of this paper is to present such rule classification.
</prevsent>
</prevsection>
<citsent citstr=" D07-1079 ">
a few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories (liu et al, 2007; <papid> P07-1089 </papid>zhang et al, 2008<papid> P08-1064 </papid>a;deneefe et al, 2007)<papid> D07-1079 </papid></citsent>
<aftsection>
<nextsent>liu et al (2007) <papid> P07-1089 </papid>differentiated the rules in their tree-to-string model which integrated with forest1-to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels.</nextsent>
<nextsent>as an extension, zhang et al (2008<papid> P08-1064 </papid>a) proposed two more categories: structure reordering rules (srr) and dis contiguous phrase rules (dpr).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1567">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> rules classifications.  </section>
<citcontext>
<prevsection>
<prevsent>the reordering rules involving the reordering between content word terminals and non-terminal (such as r5 in figure 2) also can model the useful structure reorderings.moreover, it is not uncommon that rule demonstrates the reorderings between two non-terminals as well as the reorderings between one non-terminal and one content word terminal.
</prevsent>
<prevsent>the reason for our emphasis of content word terminal is that there orderings between the non-terminals and function word are less meaningful.
</prevsent>
</prevsection>
<citsent citstr=" H05-1095 ">
one of the theoretical problems with phrase based smt models is that they can not effectively mode lthe dis contiguous translations and numerous attempts have been made on this issue (simard et al, 2005; <papid> H05-1095 </papid>quirk and menezes, 2006; <papid> N06-1002 </papid>wellington et al, 2006; <papid> P06-1123 </papid>bod, 2007; zhang et al, 2007).</citsent>
<aftsection>
<nextsent>what seems to be lacking, however, is explicit definition to the dis contiguous translation.
</nextsent>
<nextsent>the definition of dpr in (zhang et al, 2008<papid> P08-1064 </papid>a) is explicit but somewhat rough and not very accurate.</nextsent>
<nextsent>for example, in figure 3(a), non-terminal node pair ([0,???], [0,love?]</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1568">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> rules classifications.  </section>
<citcontext>
<prevsection>
<prevsent>the reordering rules involving the reordering between content word terminals and non-terminal (such as r5 in figure 2) also can model the useful structure reorderings.moreover, it is not uncommon that rule demonstrates the reorderings between two non-terminals as well as the reorderings between one non-terminal and one content word terminal.
</prevsent>
<prevsent>the reason for our emphasis of content word terminal is that there orderings between the non-terminals and function word are less meaningful.
</prevsent>
</prevsection>
<citsent citstr=" N06-1002 ">
one of the theoretical problems with phrase based smt models is that they can not effectively mode lthe dis contiguous translations and numerous attempts have been made on this issue (simard et al, 2005; <papid> H05-1095 </papid>quirk and menezes, 2006; <papid> N06-1002 </papid>wellington et al, 2006; <papid> P06-1123 </papid>bod, 2007; zhang et al, 2007).</citsent>
<aftsection>
<nextsent>what seems to be lacking, however, is explicit definition to the dis contiguous translation.
</nextsent>
<nextsent>the definition of dpr in (zhang et al, 2008<papid> P08-1064 </papid>a) is explicit but somewhat rough and not very accurate.</nextsent>
<nextsent>for example, in figure 3(a), non-terminal node pair ([0,???], [0,love?]</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1569">
<title id=" W09-2306.xml">a study of translation rule classification for syntax based statistical machine translation </title>
<section> rules classifications.  </section>
<citcontext>
<prevsection>
<prevsent>the reordering rules involving the reordering between content word terminals and non-terminal (such as r5 in figure 2) also can model the useful structure reorderings.moreover, it is not uncommon that rule demonstrates the reorderings between two non-terminals as well as the reorderings between one non-terminal and one content word terminal.
</prevsent>
<prevsent>the reason for our emphasis of content word terminal is that there orderings between the non-terminals and function word are less meaningful.
</prevsent>
</prevsection>
<citsent citstr=" P06-1123 ">
one of the theoretical problems with phrase based smt models is that they can not effectively mode lthe dis contiguous translations and numerous attempts have been made on this issue (simard et al, 2005; <papid> H05-1095 </papid>quirk and menezes, 2006; <papid> N06-1002 </papid>wellington et al, 2006; <papid> P06-1123 </papid>bod, 2007; zhang et al, 2007).</citsent>
<aftsection>
<nextsent>what seems to be lacking, however, is explicit definition to the dis contiguous translation.
</nextsent>
<nextsent>the definition of dpr in (zhang et al, 2008<papid> P08-1064 </papid>a) is explicit but somewhat rough and not very accurate.</nextsent>
<nextsent>for example, in figure 3(a), non-terminal node pair ([0,???], [0,love?]</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1649">
<title id=" W10-0207.xml">wishful thinking  finding suggestions and rsquobuyrsquo wishes from product reviews </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our work is not sub-task of sentiment analysis, but supplements the area.
</prevsent>
<prevsent>a similar example of classification task that works on the sentence level and is also related to sentiment analysis is jindal and liu (2006) which aims to identify comparisons between two entities in texts such as product reviews.
</prevsent>
</prevsection>
<citsent citstr=" N09-1030 ">
goldberg et al (2009) <papid> N09-1030 </papid>introduced the novel task of identifying wishes.</citsent>
<aftsection>
<nextsent>this used wish?
</nextsent>
<nextsent>corpus derived from web site that collected new years wishes.
</nextsent>
<nextsent>goldberg et al (2009) <papid> N09-1030 </papid>studied the corpus in detail, describing the nature, geography, and scope of topics found in them.</nextsent>
<nextsent>the paper also looked at building wish detectors?, which were applied on corpus of political comments and product reviews.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1679">
<title id=" W09-2001.xml">discourse topics and metaphors </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>this is, to our knowledge, the first quantitative demonstration of the connection between metaphoricity of given word and its role in the relevant background discourse.
</prevsent>
<prevsent>it complements the traditionally localistic?
</prevsent>
</prevsection>
<citsent citstr=" J91-1003 ">
outlook on metaphors that is based on the observation that metaphorically used item creates local incongruity because there is violation of selectional restrictions between verbs and their arguments (fass, 1991; <papid> J91-1003 </papid>mason, 2004; <papid> J04-1002 </papid>gedigian et al , 2006; <papid> W06-3506 </papid>birke and sarkar, 2006) <papid> E06-1042 </papid>or in the adjective-noun pairs (krishnakumaran and zhu, 2007).<papid> W07-0103 </papid></citsent>
<aftsection>
<nextsent>global discourse-level information can potentially be used to focus metaphor detectors operating at the local level on items with higher metaphoric potential.
</nextsent>
<nextsent>reining and lonneker-rodman (2007) use minimal topical information to focus their search for metaphors.
</nextsent>
<nextsent>working with french-language 7 corpus discussing european politics, reining and lonneker-rodman (2007) proposed harvesting salient collocates of the lemma europe, that represents the main topic of discussion and is thus hypothesized to be the main target domain of metaphors in this corpus.
</nextsent>
<nextsent>indeed, numerous instances of metaphors were collected using 4-word window around the lemma in their corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1680">
<title id=" W09-2001.xml">discourse topics and metaphors </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>this is, to our knowledge, the first quantitative demonstration of the connection between metaphoricity of given word and its role in the relevant background discourse.
</prevsent>
<prevsent>it complements the traditionally localistic?
</prevsent>
</prevsection>
<citsent citstr=" J04-1002 ">
outlook on metaphors that is based on the observation that metaphorically used item creates local incongruity because there is violation of selectional restrictions between verbs and their arguments (fass, 1991; <papid> J91-1003 </papid>mason, 2004; <papid> J04-1002 </papid>gedigian et al , 2006; <papid> W06-3506 </papid>birke and sarkar, 2006) <papid> E06-1042 </papid>or in the adjective-noun pairs (krishnakumaran and zhu, 2007).<papid> W07-0103 </papid></citsent>
<aftsection>
<nextsent>global discourse-level information can potentially be used to focus metaphor detectors operating at the local level on items with higher metaphoric potential.
</nextsent>
<nextsent>reining and lonneker-rodman (2007) use minimal topical information to focus their search for metaphors.
</nextsent>
<nextsent>working with french-language 7 corpus discussing european politics, reining and lonneker-rodman (2007) proposed harvesting salient collocates of the lemma europe, that represents the main topic of discussion and is thus hypothesized to be the main target domain of metaphors in this corpus.
</nextsent>
<nextsent>indeed, numerous instances of metaphors were collected using 4-word window around the lemma in their corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1681">
<title id=" W09-2001.xml">discourse topics and metaphors </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>this is, to our knowledge, the first quantitative demonstration of the connection between metaphoricity of given word and its role in the relevant background discourse.
</prevsent>
<prevsent>it complements the traditionally localistic?
</prevsent>
</prevsection>
<citsent citstr=" W06-3506 ">
outlook on metaphors that is based on the observation that metaphorically used item creates local incongruity because there is violation of selectional restrictions between verbs and their arguments (fass, 1991; <papid> J91-1003 </papid>mason, 2004; <papid> J04-1002 </papid>gedigian et al , 2006; <papid> W06-3506 </papid>birke and sarkar, 2006) <papid> E06-1042 </papid>or in the adjective-noun pairs (krishnakumaran and zhu, 2007).<papid> W07-0103 </papid></citsent>
<aftsection>
<nextsent>global discourse-level information can potentially be used to focus metaphor detectors operating at the local level on items with higher metaphoric potential.
</nextsent>
<nextsent>reining and lonneker-rodman (2007) use minimal topical information to focus their search for metaphors.
</nextsent>
<nextsent>working with french-language 7 corpus discussing european politics, reining and lonneker-rodman (2007) proposed harvesting salient collocates of the lemma europe, that represents the main topic of discussion and is thus hypothesized to be the main target domain of metaphors in this corpus.
</nextsent>
<nextsent>indeed, numerous instances of metaphors were collected using 4-word window around the lemma in their corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1682">
<title id=" W09-2001.xml">discourse topics and metaphors </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>this is, to our knowledge, the first quantitative demonstration of the connection between metaphoricity of given word and its role in the relevant background discourse.
</prevsent>
<prevsent>it complements the traditionally localistic?
</prevsent>
</prevsection>
<citsent citstr=" E06-1042 ">
outlook on metaphors that is based on the observation that metaphorically used item creates local incongruity because there is violation of selectional restrictions between verbs and their arguments (fass, 1991; <papid> J91-1003 </papid>mason, 2004; <papid> J04-1002 </papid>gedigian et al , 2006; <papid> W06-3506 </papid>birke and sarkar, 2006) <papid> E06-1042 </papid>or in the adjective-noun pairs (krishnakumaran and zhu, 2007).<papid> W07-0103 </papid></citsent>
<aftsection>
<nextsent>global discourse-level information can potentially be used to focus metaphor detectors operating at the local level on items with higher metaphoric potential.
</nextsent>
<nextsent>reining and lonneker-rodman (2007) use minimal topical information to focus their search for metaphors.
</nextsent>
<nextsent>working with french-language 7 corpus discussing european politics, reining and lonneker-rodman (2007) proposed harvesting salient collocates of the lemma europe, that represents the main topic of discussion and is thus hypothesized to be the main target domain of metaphors in this corpus.
</nextsent>
<nextsent>indeed, numerous instances of metaphors were collected using 4-word window around the lemma in their corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1683">
<title id=" W09-2001.xml">discourse topics and metaphors </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>this is, to our knowledge, the first quantitative demonstration of the connection between metaphoricity of given word and its role in the relevant background discourse.
</prevsent>
<prevsent>it complements the traditionally localistic?
</prevsent>
</prevsection>
<citsent citstr=" W07-0103 ">
outlook on metaphors that is based on the observation that metaphorically used item creates local incongruity because there is violation of selectional restrictions between verbs and their arguments (fass, 1991; <papid> J91-1003 </papid>mason, 2004; <papid> J04-1002 </papid>gedigian et al , 2006; <papid> W06-3506 </papid>birke and sarkar, 2006) <papid> E06-1042 </papid>or in the adjective-noun pairs (krishnakumaran and zhu, 2007).<papid> W07-0103 </papid></citsent>
<aftsection>
<nextsent>global discourse-level information can potentially be used to focus metaphor detectors operating at the local level on items with higher metaphoric potential.
</nextsent>
<nextsent>reining and lonneker-rodman (2007) use minimal topical information to focus their search for metaphors.
</nextsent>
<nextsent>working with french-language 7 corpus discussing european politics, reining and lonneker-rodman (2007) proposed harvesting salient collocates of the lemma europe, that represents the main topic of discussion and is thus hypothesized to be the main target domain of metaphors in this corpus.
</nextsent>
<nextsent>indeed, numerous instances of metaphors were collected using 4-word window around the lemma in their corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1684">
<title id=" W10-0203.xml">toward plot units automatic affect state analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common phenomena are affect states that result from character being acted upon in positive or negative way.
</prevsent>
<prevsent>for example, the cat ate the mouse?
</prevsent>
</prevsection>
<citsent citstr=" W06-0301 ">
produces positive affect state for the cat and negative affect 1this is somewhat analogous to, but not exactly the same as, associating opinion words with their targets or topics (kim and hovy, 2006; <papid> W06-0301 </papid>stoyanov and cardie, 2008).<papid> C08-1103 </papid></citsent>
<aftsection>
<nextsent>17 the father and his sons (s1) father had family of sons who were perpetually quarreling among themselves.
</nextsent>
<nextsent>(s2) when he failed to heal their disputes by his exhortations, he determined to give them practical illustration of the evils of disunion; and for this purpose he one day told them to bring him bundle of sticks.
</nextsent>
<nextsent>(s3) when they had done so, he placed the faggot into the hands of each of them in succession, and ordered them to break it in pieces.
</nextsent>
<nextsent>(s4) they tried with all their strength, and were not able to do it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1685">
<title id=" W10-0203.xml">toward plot units automatic affect state analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common phenomena are affect states that result from character being acted upon in positive or negative way.
</prevsent>
<prevsent>for example, the cat ate the mouse?
</prevsent>
</prevsection>
<citsent citstr=" C08-1103 ">
produces positive affect state for the cat and negative affect 1this is somewhat analogous to, but not exactly the same as, associating opinion words with their targets or topics (kim and hovy, 2006; <papid> W06-0301 </papid>stoyanov and cardie, 2008).<papid> C08-1103 </papid></citsent>
<aftsection>
<nextsent>17 the father and his sons (s1) father had family of sons who were perpetually quarreling among themselves.
</nextsent>
<nextsent>(s2) when he failed to heal their disputes by his exhortations, he determined to give them practical illustration of the evils of disunion; and for this purpose he one day told them to bring him bundle of sticks.
</nextsent>
<nextsent>(s3) when they had done so, he placed the faggot into the hands of each of them in succession, and ordered them to break it in pieces.
</nextsent>
<nextsent>(s4) they tried with all their strength, and were not able to do it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1686">
<title id=" W10-0203.xml">toward plot units automatic affect state analysis </title>
<section> aesop: automatic affect state analysis.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 step 1: assigning affect tags to words.
</prevsent>
<prevsent>4.1.1 sentiment analysis resources aesop incorporates several existing sentiment analysis resources to recognize affect states associated with emotions and speech acts.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
opinionfinder2 (wilson et al, 2005) (<papid> H05-1044 </papid>version 1.4) is used to identify all three types of states.</citsent>
<aftsection>
<nextsent>weuse the +/- labels assigned by its contextual polarity classifier (wilson, 2005) to create +/- affect tags.
</nextsent>
<nextsent>the mpqasd tags produced by its direct subjective and speech event identifier (choi et al, 2006) <papid> W06-1651 </papid>are used as affect tags.?</nextsent>
<nextsent>subjectivity lexicon3 (wilson, 2005): the positive/negative words in this list are assigned +/- affect tags, when they occur with the designated part of-speech (pos).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1687">
<title id=" W10-0203.xml">toward plot units automatic affect state analysis </title>
<section> aesop: automatic affect state analysis.  </section>
<citcontext>
<prevsection>
<prevsent>opinionfinder2 (wilson et al, 2005) (<papid> H05-1044 </papid>version 1.4) is used to identify all three types of states.</prevsent>
<prevsent>weuse the +/- labels assigned by its contextual polarity classifier (wilson, 2005) to create +/- affect tags.</prevsent>
</prevsection>
<citsent citstr=" W06-1651 ">
the mpqasd tags produced by its direct subjective and speech event identifier (choi et al, 2006) <papid> W06-1651 </papid>are used as affect tags.?</citsent>
<aftsection>
<nextsent>subjectivity lexicon3 (wilson, 2005): the positive/negative words in this list are assigned +/- affect tags, when they occur with the designated part of-speech (pos).
</nextsent>
<nextsent>semantic orientation lexicon4 (takamura et al., 2005): <papid> P05-1017 </papid>the positive/negative words in this list are assigned +/- affect tags, when they occur with the designated part-of-speech.</nextsent>
<nextsent>a list of 228 speech act verbs compiled from (wierzbicka, 1987)5, which are used for states.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1688">
<title id=" W10-0203.xml">toward plot units automatic affect state analysis </title>
<section> aesop: automatic affect state analysis.  </section>
<citcontext>
<prevsection>
<prevsent>the mpqasd tags produced by its direct subjective and speech event identifier (choi et al, 2006) <papid> W06-1651 </papid>are used as affect tags.?</prevsent>
<prevsent>subjectivity lexicon3 (wilson, 2005): the positive/negative words in this list are assigned +/- affect tags, when they occur with the designated part of-speech (pos).</prevsent>
</prevsection>
<citsent citstr=" P05-1017 ">
semantic orientation lexicon4 (takamura et al., 2005): <papid> P05-1017 </papid>the positive/negative words in this list are assigned +/- affect tags, when they occur with the designated part-of-speech.</citsent>
<aftsection>
<nextsent>a list of 228 speech act verbs compiled from (wierzbicka, 1987)5, which are used for states.
</nextsent>
<nextsent>4.1.2 patient polarity verbs as we discussed in section 3.4, existing resources are not sufficient to identify affect states that arise from character being acted upon.
</nextsent>
<nextsent>sentiment lexicons, for example, assign polarity to verbs irrespective of their agents or patients.
</nextsent>
<nextsent>to fill this gap, we tried to automatically acquire verbs that have strong patient polarity (i.e., the patient will be in good or bad state by virtue of being acted upon).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1689">
<title id=" W09-2309.xml">reordering model using syntactic information of a source tree for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this indicates the validity of the proposed reordering model.
</prevsent>
<prevsent>statistical machine translation has been wiedely applied in many state-of-the-art translation systems.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
a popular statistical machine translation paradigms is the phrase-based model (koehn et al , 2003; ochand ney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>in phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems.
</nextsent>
<nextsent>to resolve this problem, many word-reordering constraint techniques have been proposed.
</nextsent>
<nextsent>these techniques are categorized intotwo types.
</nextsent>
<nextsent>the first type is linguistically syntax based.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1690">
<title id=" W09-2309.xml">reordering model using syntactic information of a source tree for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these techniques are categorized intotwo types.
</prevsent>
<prevsent>the first type is linguistically syntax based.
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
in this approach, tree structures for the source(quirk et al , 2005; <papid> P05-1034 </papid>huang et al , 2006), target (ya mada and knight, 2000; marcu et al , 2006), <papid> W06-1606 </papid>or both (melamed, 2004) are used for model training.</citsent>
<aftsection>
<nextsent>the second type is formal constraints on word permutations.
</nextsent>
<nextsent>ibm constraints (berger et al , 1996), <papid> J96-1002 </papid>the lexical word reordering model (tillmann, 2004), <papid> N04-4026 </papid>and inversion transduction grammar (itg) constraints(wu, 1995; wu, 1997) <papid> J97-3002 </papid>belong to this type of ap proach.</nextsent>
<nextsent>for itg constraints, the target-side word order is obtained by rotating nodes of the source side binary tree.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1691">
<title id=" W09-2309.xml">reordering model using syntactic information of a source tree for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these techniques are categorized intotwo types.
</prevsent>
<prevsent>the first type is linguistically syntax based.
</prevsent>
</prevsection>
<citsent citstr=" W06-1606 ">
in this approach, tree structures for the source(quirk et al , 2005; <papid> P05-1034 </papid>huang et al , 2006), target (ya mada and knight, 2000; marcu et al , 2006), <papid> W06-1606 </papid>or both (melamed, 2004) are used for model training.</citsent>
<aftsection>
<nextsent>the second type is formal constraints on word permutations.
</nextsent>
<nextsent>ibm constraints (berger et al , 1996), <papid> J96-1002 </papid>the lexical word reordering model (tillmann, 2004), <papid> N04-4026 </papid>and inversion transduction grammar (itg) constraints(wu, 1995; wu, 1997) <papid> J97-3002 </papid>belong to this type of ap proach.</nextsent>
<nextsent>for itg constraints, the target-side word order is obtained by rotating nodes of the source side binary tree.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1692">
<title id=" W09-2309.xml">reordering model using syntactic information of a source tree for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this approach, tree structures for the source(quirk et al , 2005; <papid> P05-1034 </papid>huang et al , 2006), target (ya mada and knight, 2000; marcu et al , 2006), <papid> W06-1606 </papid>or both (melamed, 2004) are used for model training.</prevsent>
<prevsent>the second type is formal constraints on word permutations.</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
ibm constraints (berger et al , 1996), <papid> J96-1002 </papid>the lexical word reordering model (tillmann, 2004), <papid> N04-4026 </papid>and inversion transduction grammar (itg) constraints(wu, 1995; wu, 1997) <papid> J97-3002 </papid>belong to this type of ap proach.</citsent>
<aftsection>
<nextsent>for itg constraints, the target-side word order is obtained by rotating nodes of the source side binary tree.
</nextsent>
<nextsent>in these node rotations, the source binary tree instance is not considered.
</nextsent>
<nextsent>imposinga source tree on itg (ist-itg) constraints (yamamoto et al , 2008) <papid> W08-0401 </papid>is an extension of itg constraints and hybrid of the first and second type of approach.</nextsent>
<nextsent>ist-itg constraints directly introduce source sentence tree structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1693">
<title id=" W09-2309.xml">reordering model using syntactic information of a source tree for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this approach, tree structures for the source(quirk et al , 2005; <papid> P05-1034 </papid>huang et al , 2006), target (ya mada and knight, 2000; marcu et al , 2006), <papid> W06-1606 </papid>or both (melamed, 2004) are used for model training.</prevsent>
<prevsent>the second type is formal constraints on word permutations.</prevsent>
</prevsection>
<citsent citstr=" N04-4026 ">
ibm constraints (berger et al , 1996), <papid> J96-1002 </papid>the lexical word reordering model (tillmann, 2004), <papid> N04-4026 </papid>and inversion transduction grammar (itg) constraints(wu, 1995; wu, 1997) <papid> J97-3002 </papid>belong to this type of ap proach.</citsent>
<aftsection>
<nextsent>for itg constraints, the target-side word order is obtained by rotating nodes of the source side binary tree.
</nextsent>
<nextsent>in these node rotations, the source binary tree instance is not considered.
</nextsent>
<nextsent>imposinga source tree on itg (ist-itg) constraints (yamamoto et al , 2008) <papid> W08-0401 </papid>is an extension of itg constraints and hybrid of the first and second type of approach.</nextsent>
<nextsent>ist-itg constraints directly introduce source sentence tree structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1694">
<title id=" W09-2309.xml">reordering model using syntactic information of a source tree for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this approach, tree structures for the source(quirk et al , 2005; <papid> P05-1034 </papid>huang et al , 2006), target (ya mada and knight, 2000; marcu et al , 2006), <papid> W06-1606 </papid>or both (melamed, 2004) are used for model training.</prevsent>
<prevsent>the second type is formal constraints on word permutations.</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
ibm constraints (berger et al , 1996), <papid> J96-1002 </papid>the lexical word reordering model (tillmann, 2004), <papid> N04-4026 </papid>and inversion transduction grammar (itg) constraints(wu, 1995; wu, 1997) <papid> J97-3002 </papid>belong to this type of ap proach.</citsent>
<aftsection>
<nextsent>for itg constraints, the target-side word order is obtained by rotating nodes of the source side binary tree.
</nextsent>
<nextsent>in these node rotations, the source binary tree instance is not considered.
</nextsent>
<nextsent>imposinga source tree on itg (ist-itg) constraints (yamamoto et al , 2008) <papid> W08-0401 </papid>is an extension of itg constraints and hybrid of the first and second type of approach.</nextsent>
<nextsent>ist-itg constraints directly introduce source sentence tree structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1695">
<title id=" W09-2309.xml">reordering model using syntactic information of a source tree for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for itg constraints, the target-side word order is obtained by rotating nodes of the source side binary tree.
</prevsent>
<prevsent>in these node rotations, the source binary tree instance is not considered.
</prevsent>
</prevsection>
<citsent citstr=" W08-0401 ">
imposinga source tree on itg (ist-itg) constraints (yamamoto et al , 2008) <papid> W08-0401 </papid>is an extension of itg constraints and hybrid of the first and second type of approach.</citsent>
<aftsection>
<nextsent>ist-itg constraints directly introduce source sentence tree structure.
</nextsent>
<nextsent>therefore, ist-itg can obtain stronger constraints for word reordering than the original itg constraints.
</nextsent>
<nextsent>for example, ist itg constraints allows only eight word orderings for four-word sentence, even though twenty-two word orderings are possible with respect to the originalitg constraints.
</nextsent>
<nextsent>although ist-itg constraints efficiently suppress erroneous target word orderings,the method cannot assign the probability to the target word orderings.this paper presents reordering model using syntactic information of source tree for phrase-basedstatistical machine translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1696">
<title id=" W09-2309.xml">reordering model using syntactic information of a source tree for statistical machine translation </title>
<section> reordering model using syntactic.  </section>
<citcontext>
<prevsection>
<prevsent>we obtained the word alignments using koehn et al method (2003), 71 32 2,34 2,3,41 figure 2: example of source-side parse-tree with word alignments using the training algorithm of the proposed model.
</prevsent>
<prevsent>which is based on och and neys work (2004).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
this involves running giza++ (och and ney,2003) <papid> J03-1002 </papid>on the corpus in both directions, and applying refinement rules (the variant they designate is final-and?)</citsent>
<aftsection>
<nextsent>to obtain single many-to many word alignment for each sentence.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>source-side parse-trees are created using a. source language phrase structure parser, which annotates each node with syntactic label.
</nextsent>
<nextsent>asource-side parse-tree consists of several subtrees with syntactic labels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1697">
<title id=" W09-2309.xml">reordering model using syntactic information of a source tree for statistical machine translation </title>
<section> reordering model using syntactic.  </section>
<citcontext>
<prevsection>
<prevsent>some are due to linguistic reasons, structual differences such as negation (french ne...pas?
</prevsent>
<prevsent>and english not?), adverb, modal and soon.
</prevsent>
</prevsection>
<citsent citstr=" W02-1039 ">
others are due to non-linguistic reasons, errors of automatic word alignments, syntactic analysis, or human translation (fox, 2002).<papid> W02-1039 </papid></citsent>
<aftsection>
<nextsent>the proposed method discards such problematic cases.
</nextsent>
<nextsent>in figure 3, the subtree s1 is then removed from training samples, and the subtrees s2 and s3 are used as training samples.
</nextsent>
<nextsent>3.3 decoding using the proposed reordering.
</nextsent>
<nextsent>model in this section, we describe one-pass phrase-baseddecoding algorithm that uses the proposed reordering model in the decoder.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1699">
<title id=" W09-2309.xml">reordering model using syntactic information of a source tree for statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for language model training, we used the sri language model toolkit(stolcke, 2002), and 1.0m sentences for the translation model training.
</prevsent>
<prevsent>the language model type was word 5-gram smoothed by kneser-ney discounting(kneser and ney, 1995).
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
to tune the decoder parameters, we conducted minimum error rate training (och, 2003) <papid> P03-1021 </papid>with respect to the word bleu score(papineni et al , 2002) <papid> P02-1040 </papid>using 2.0k development sentence pairs.</citsent>
<aftsection>
<nextsent>the test set with 2.0k sentences is used.in the evaluation and development sets, single reference was used.
</nextsent>
<nextsent>for the creation of english sentence parse trees and segmentation of the english, we used the charniak parser (charniak, 2000).
</nextsent>
<nextsent>weused chasen for segmentation of the japanese sentences.
</nextsent>
<nextsent>for decoding, we used an in-house decoder that is close relative of the moses decoder.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1700">
<title id=" W09-2309.xml">reordering model using syntactic information of a source tree for statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for language model training, we used the sri language model toolkit(stolcke, 2002), and 1.0m sentences for the translation model training.
</prevsent>
<prevsent>the language model type was word 5-gram smoothed by kneser-ney discounting(kneser and ney, 1995).
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
to tune the decoder parameters, we conducted minimum error rate training (och, 2003) <papid> P03-1021 </papid>with respect to the word bleu score(papineni et al , 2002) <papid> P02-1040 </papid>using 2.0k development sentence pairs.</citsent>
<aftsection>
<nextsent>the test set with 2.0k sentences is used.in the evaluation and development sets, single reference was used.
</nextsent>
<nextsent>for the creation of english sentence parse trees and segmentation of the english, we used the charniak parser (charniak, 2000).
</nextsent>
<nextsent>weused chasen for segmentation of the japanese sentences.
</nextsent>
<nextsent>for decoding, we used an in-house decoder that is close relative of the moses decoder.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1701">
<title id=" W09-2703.xml">qast question answering system for thai wikipedia </title>
<section> question analyzer ? the question ana-.  </section>
<citcontext>
<prevsection>
<prevsent>return the top spans in as answers..
</prevsent>
<prevsent>in the actual implementation, we set equal to500 so that only the top 500 documents are considered.
</prevsent>
</prevsection>
<citsent citstr=" P02-1005 ">
although retrieving more texts from the corpus would likely increase the chance of finding the answer (moldovan et al, 2002), <papid> P02-1005 </papid>our trial and-error showed that 500 documents seem to bea good trade-off between speed and content cov erage.</citsent>
<aftsection>
<nextsent>to look for an occurrence of hint terms, each span is stretched backward and forward for 10 terms (i.e., = 10).
</nextsent>
<nextsent>finally, we set equal to 5 to return only the top five spans as the answers.
</nextsent>
<nextsent>2.3 answer processor.
</nextsent>
<nextsent>this sub-system contains two modules: answer ranker and answer generator.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1702">
<title id=" W09-2208.xml">a simple semi supervised algorithm for named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semi-supervised learning involves the utilization of unlabeled data to mitigate the effect of insufficient labeled data on classifier accuracy.
</prevsent>
<prevsent>one variety of semi-supervised learning essentially attempts to automatically generate high-quality training data from an unlabeled corpus.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
algorithms such as co-training (blum and mitchell, 1998)(collins and singer, 1999)(<papid> W99-0613 </papid>pierce and cardie, 2001) <papid> W01-0501 </papid>andthe yarowsky algorithm (yarowsky, 1995) <papid> P95-1026 </papid>make assumptions about the data that permit such an ap proach.the main requirement for the automatically generated training data in addition to high accuracy, is that it covers regions in the feature space withlow probability density.</citsent>
<aftsection>
<nextsent>furthermore, it is necessary that all the classes are represented according totheir prior probabilities in every region in the feature space.
</nextsent>
<nextsent>one approach to achieve these goals is to select unlabeled data that has been classified withlow confidence by the classifier trained on the original training data, but whose labels are known with high precision from independent evidence.
</nextsent>
<nextsent>here independence means that the high-precision decision rule that classifies these low confidence instances uses information that is independent of the features used by the classifier.we propose two ways of obtaining such independent evidence for ner.
</nextsent>
<nextsent>the first is based onthe fact that multiple mentions of capitalized tokens are likely to have the same label and occur in independently chosen contexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1703">
<title id=" W09-2208.xml">a simple semi supervised algorithm for named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semi-supervised learning involves the utilization of unlabeled data to mitigate the effect of insufficient labeled data on classifier accuracy.
</prevsent>
<prevsent>one variety of semi-supervised learning essentially attempts to automatically generate high-quality training data from an unlabeled corpus.
</prevsent>
</prevsection>
<citsent citstr=" W01-0501 ">
algorithms such as co-training (blum and mitchell, 1998)(collins and singer, 1999)(<papid> W99-0613 </papid>pierce and cardie, 2001) <papid> W01-0501 </papid>andthe yarowsky algorithm (yarowsky, 1995) <papid> P95-1026 </papid>make assumptions about the data that permit such an ap proach.the main requirement for the automatically generated training data in addition to high accuracy, is that it covers regions in the feature space withlow probability density.</citsent>
<aftsection>
<nextsent>furthermore, it is necessary that all the classes are represented according totheir prior probabilities in every region in the feature space.
</nextsent>
<nextsent>one approach to achieve these goals is to select unlabeled data that has been classified withlow confidence by the classifier trained on the original training data, but whose labels are known with high precision from independent evidence.
</nextsent>
<nextsent>here independence means that the high-precision decision rule that classifies these low confidence instances uses information that is independent of the features used by the classifier.we propose two ways of obtaining such independent evidence for ner.
</nextsent>
<nextsent>the first is based onthe fact that multiple mentions of capitalized tokens are likely to have the same label and occur in independently chosen contexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1704">
<title id=" W09-2208.xml">a simple semi supervised algorithm for named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semi-supervised learning involves the utilization of unlabeled data to mitigate the effect of insufficient labeled data on classifier accuracy.
</prevsent>
<prevsent>one variety of semi-supervised learning essentially attempts to automatically generate high-quality training data from an unlabeled corpus.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
algorithms such as co-training (blum and mitchell, 1998)(collins and singer, 1999)(<papid> W99-0613 </papid>pierce and cardie, 2001) <papid> W01-0501 </papid>andthe yarowsky algorithm (yarowsky, 1995) <papid> P95-1026 </papid>make assumptions about the data that permit such an ap proach.the main requirement for the automatically generated training data in addition to high accuracy, is that it covers regions in the feature space withlow probability density.</citsent>
<aftsection>
<nextsent>furthermore, it is necessary that all the classes are represented according totheir prior probabilities in every region in the feature space.
</nextsent>
<nextsent>one approach to achieve these goals is to select unlabeled data that has been classified withlow confidence by the classifier trained on the original training data, but whose labels are known with high precision from independent evidence.
</nextsent>
<nextsent>here independence means that the high-precision decision rule that classifies these low confidence instances uses information that is independent of the features used by the classifier.we propose two ways of obtaining such independent evidence for ner.
</nextsent>
<nextsent>the first is based onthe fact that multiple mentions of capitalized tokens are likely to have the same label and occur in independently chosen contexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1710">
<title id=" W09-2208.xml">a simple semi supervised algorithm for named entity recognition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>at the token level, the seed rules they proposed do not necessarily work.
</prevsent>
<prevsent>in addition, parsing sentences into word sequences is not trivial task, and also not necessary for ner, in our opinion.
</prevsent>
</prevsection>
<citsent citstr=" P06-1027 ">
jiao et al  propose semi-supervised conditional random fields (jiao et al , 2006) <papid> P06-1027 </papid>that try to maximize the conditional log-likelihood on the training data and simultaneously minimize the conditional entropy of the class labels on the unlabeled data.</citsent>
<aftsection>
<nextsent>this approach is reminiscent of the semi-supervisedlearning algorithms that try to discourage the boundary from being in regions with high density of unlabeled data.
</nextsent>
<nextsent>the resulting objective function is no longer convex and may result in local optima.
</nextsent>
<nextsent>our approach in contrast avoids changing the crf training procedure, which guarantees global maximum.
</nextsent>
<nextsent>as long as independent evidence exists for one type of ne, our method can be directly applied to classify such ne.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1711">
<title id=" W09-2208.xml">a simple semi supervised algorithm for named entity recognition </title>
<section> named entity recognition.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 features for ner.
</prevsent>
<prevsent>one big advantage of crf is that it can naturally represent rich domain knowledge with features.
</prevsent>
</prevsection>
<citsent citstr=" W03-0430 ">
3.2.1 standard features part of the features we used for our crf classifier are common features that are widely used inner (mccallum and li, 2003), <papid> W03-0430 </papid>as shown below.</citsent>
<aftsection>
<nextsent>1) lexicon.
</nextsent>
<nextsent>each token is itself feature.
</nextsent>
<nextsent>2) orthography.
</nextsent>
<nextsent>orthographic information is used to identify whether token is capitalized, or an acronym, or pure number, or punctuation, or has mixed letters and digits, etc.3) single/multiple-token list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1712">
<title id=" W09-2208.xml">a simple semi supervised algorithm for named entity recognition </title>
<section> semi-supervised learning algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>thus the classifier ckis expected to get better monotonic ally as the training data gets updated.
</prevsent>
<prevsent>table 2: the semi-supervised ner algorithm given: - small set of labeled training data - unlabeled data loop for iterations: step 1: train classifier ck based on l; step 2: extract new data based on ck; step 3: add to l;at each iteration, the classifier trained on the previous training data (using the features introduced in the previous section) is used to tag the unlabeleddata.
</prevsent>
</prevsection>
<citsent citstr=" N04-4028 ">
in addition, for each token and ne segment, confidence score is computed using the con 61 strained forward-backward algorithm (culotta and mccallum, 2004), <papid> N04-4028 </papid>which calculates the lcx , the sum of the probabilities of all the paths passing through the constrained segment (constrained to be the assigned labels).</citsent>
<aftsection>
<nextsent>one way to increase the size of the training datais to add all the tokens classified with high confidence to the training set.
</nextsent>
<nextsent>this scheme is unlikely to improve the accuracy of the classifier at the next iteration because the newly added data is unlikely to include new patterns.
</nextsent>
<nextsent>instead, we use the high confidence data to tag other data by exploiting independent features.
</nextsent>
<nextsent>tagging org if sequence of tokens has been classified as org?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1713">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling (srl) aims to identify and label all the arguments for each predicate in sentence.
</prevsent>
<prevsent>specifically, it involves identifying portions of the sentence that represent the predicates arguments and assigning pre-specified semantic roles to them.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
[a0seller ports of call inc.] reached agreements to [vverb sell] [a1thing its remaining seven aircraft] [a2buyer to buyers that werent disclosed] .is an example of srl annotation from the propbank corpus (palmer et al, 2005), <papid> J05-1004 </papid>where the sub scripted information maps the semantic roles a0, a1 and a2 to arguments for the predicate sell as defined in the propbank frame scheme.the availability of annotated corpora like propbank and framenet (fillmore et al, 2001) have provided rapid development of research into srl (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>pradhan et al, 2005).<papid> P05-1072 </papid></citsent>
<aftsection>
<nextsent>the shared tasks in conll2004 (carreras and ma`rquez, 2004), conll2005 (carreras and ma`rquez, 2005) and conll 2008 (surdeanu et al, 2008) were all focused on srl.
</nextsent>
<nextsent>srl systems (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002) <papid> P02-1031 </papid>have extensively used features defined over penn treebank phrase structure trees.</nextsent>
<nextsent>other syntactic representations such as ccg derivations (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and dependency trees (hacioglu,2004; surdeanu et al, 2008) have also been ex plored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1714">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling (srl) aims to identify and label all the arguments for each predicate in sentence.
</prevsent>
<prevsent>specifically, it involves identifying portions of the sentence that represent the predicates arguments and assigning pre-specified semantic roles to them.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
[a0seller ports of call inc.] reached agreements to [vverb sell] [a1thing its remaining seven aircraft] [a2buyer to buyers that werent disclosed] .is an example of srl annotation from the propbank corpus (palmer et al, 2005), <papid> J05-1004 </papid>where the sub scripted information maps the semantic roles a0, a1 and a2 to arguments for the predicate sell as defined in the propbank frame scheme.the availability of annotated corpora like propbank and framenet (fillmore et al, 2001) have provided rapid development of research into srl (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>pradhan et al, 2005).<papid> P05-1072 </papid></citsent>
<aftsection>
<nextsent>the shared tasks in conll2004 (carreras and ma`rquez, 2004), conll2005 (carreras and ma`rquez, 2005) and conll 2008 (surdeanu et al, 2008) were all focused on srl.
</nextsent>
<nextsent>srl systems (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002) <papid> P02-1031 </papid>have extensively used features defined over penn treebank phrase structure trees.</nextsent>
<nextsent>other syntactic representations such as ccg derivations (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and dependency trees (hacioglu,2004; surdeanu et al, 2008) have also been ex plored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1716">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling (srl) aims to identify and label all the arguments for each predicate in sentence.
</prevsent>
<prevsent>specifically, it involves identifying portions of the sentence that represent the predicates arguments and assigning pre-specified semantic roles to them.
</prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
[a0seller ports of call inc.] reached agreements to [vverb sell] [a1thing its remaining seven aircraft] [a2buyer to buyers that werent disclosed] .is an example of srl annotation from the propbank corpus (palmer et al, 2005), <papid> J05-1004 </papid>where the sub scripted information maps the semantic roles a0, a1 and a2 to arguments for the predicate sell as defined in the propbank frame scheme.the availability of annotated corpora like propbank and framenet (fillmore et al, 2001) have provided rapid development of research into srl (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>pradhan et al, 2005).<papid> P05-1072 </papid></citsent>
<aftsection>
<nextsent>the shared tasks in conll2004 (carreras and ma`rquez, 2004), conll2005 (carreras and ma`rquez, 2005) and conll 2008 (surdeanu et al, 2008) were all focused on srl.
</nextsent>
<nextsent>srl systems (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002) <papid> P02-1031 </papid>have extensively used features defined over penn treebank phrase structure trees.</nextsent>
<nextsent>other syntactic representations such as ccg derivations (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and dependency trees (hacioglu,2004; surdeanu et al, 2008) have also been ex plored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1717">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling (srl) aims to identify and label all the arguments for each predicate in sentence.
</prevsent>
<prevsent>specifically, it involves identifying portions of the sentence that represent the predicates arguments and assigning pre-specified semantic roles to them.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
[a0seller ports of call inc.] reached agreements to [vverb sell] [a1thing its remaining seven aircraft] [a2buyer to buyers that werent disclosed] .is an example of srl annotation from the propbank corpus (palmer et al, 2005), <papid> J05-1004 </papid>where the sub scripted information maps the semantic roles a0, a1 and a2 to arguments for the predicate sell as defined in the propbank frame scheme.the availability of annotated corpora like propbank and framenet (fillmore et al, 2001) have provided rapid development of research into srl (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>pradhan et al, 2005).<papid> P05-1072 </papid></citsent>
<aftsection>
<nextsent>the shared tasks in conll2004 (carreras and ma`rquez, 2004), conll2005 (carreras and ma`rquez, 2005) and conll 2008 (surdeanu et al, 2008) were all focused on srl.
</nextsent>
<nextsent>srl systems (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002) <papid> P02-1031 </papid>have extensively used features defined over penn treebank phrase structure trees.</nextsent>
<nextsent>other syntactic representations such as ccg derivations (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and dependency trees (hacioglu,2004; surdeanu et al, 2008) have also been ex plored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1718">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling (srl) aims to identify and label all the arguments for each predicate in sentence.
</prevsent>
<prevsent>specifically, it involves identifying portions of the sentence that represent the predicates arguments and assigning pre-specified semantic roles to them.
</prevsent>
</prevsection>
<citsent citstr=" W03-1006 ">
[a0seller ports of call inc.] reached agreements to [vverb sell] [a1thing its remaining seven aircraft] [a2buyer to buyers that werent disclosed] .is an example of srl annotation from the propbank corpus (palmer et al, 2005), <papid> J05-1004 </papid>where the sub scripted information maps the semantic roles a0, a1 and a2 to arguments for the predicate sell as defined in the propbank frame scheme.the availability of annotated corpora like propbank and framenet (fillmore et al, 2001) have provided rapid development of research into srl (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>pradhan et al, 2005).<papid> P05-1072 </papid></citsent>
<aftsection>
<nextsent>the shared tasks in conll2004 (carreras and ma`rquez, 2004), conll2005 (carreras and ma`rquez, 2005) and conll 2008 (surdeanu et al, 2008) were all focused on srl.
</nextsent>
<nextsent>srl systems (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002) <papid> P02-1031 </papid>have extensively used features defined over penn treebank phrase structure trees.</nextsent>
<nextsent>other syntactic representations such as ccg derivations (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and dependency trees (hacioglu,2004; surdeanu et al, 2008) have also been ex plored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1720">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling (srl) aims to identify and label all the arguments for each predicate in sentence.
</prevsent>
<prevsent>specifically, it involves identifying portions of the sentence that represent the predicates arguments and assigning pre-specified semantic roles to them.
</prevsent>
</prevsection>
<citsent citstr=" W03-1008 ">
[a0seller ports of call inc.] reached agreements to [vverb sell] [a1thing its remaining seven aircraft] [a2buyer to buyers that werent disclosed] .is an example of srl annotation from the propbank corpus (palmer et al, 2005), <papid> J05-1004 </papid>where the sub scripted information maps the semantic roles a0, a1 and a2 to arguments for the predicate sell as defined in the propbank frame scheme.the availability of annotated corpora like propbank and framenet (fillmore et al, 2001) have provided rapid development of research into srl (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>pradhan et al, 2005).<papid> P05-1072 </papid></citsent>
<aftsection>
<nextsent>the shared tasks in conll2004 (carreras and ma`rquez, 2004), conll2005 (carreras and ma`rquez, 2005) and conll 2008 (surdeanu et al, 2008) were all focused on srl.
</nextsent>
<nextsent>srl systems (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002) <papid> P02-1031 </papid>have extensively used features defined over penn treebank phrase structure trees.</nextsent>
<nextsent>other syntactic representations such as ccg derivations (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and dependency trees (hacioglu,2004; surdeanu et al, 2008) have also been ex plored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1721">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling (srl) aims to identify and label all the arguments for each predicate in sentence.
</prevsent>
<prevsent>specifically, it involves identifying portions of the sentence that represent the predicates arguments and assigning pre-specified semantic roles to them.
</prevsent>
</prevsection>
<citsent citstr=" W04-3212 ">
[a0seller ports of call inc.] reached agreements to [vverb sell] [a1thing its remaining seven aircraft] [a2buyer to buyers that werent disclosed] .is an example of srl annotation from the propbank corpus (palmer et al, 2005), <papid> J05-1004 </papid>where the sub scripted information maps the semantic roles a0, a1 and a2 to arguments for the predicate sell as defined in the propbank frame scheme.the availability of annotated corpora like propbank and framenet (fillmore et al, 2001) have provided rapid development of research into srl (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>pradhan et al, 2005).<papid> P05-1072 </papid></citsent>
<aftsection>
<nextsent>the shared tasks in conll2004 (carreras and ma`rquez, 2004), conll2005 (carreras and ma`rquez, 2005) and conll 2008 (surdeanu et al, 2008) were all focused on srl.
</nextsent>
<nextsent>srl systems (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002) <papid> P02-1031 </papid>have extensively used features defined over penn treebank phrase structure trees.</nextsent>
<nextsent>other syntactic representations such as ccg derivations (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and dependency trees (hacioglu,2004; surdeanu et al, 2008) have also been ex plored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1722">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling (srl) aims to identify and label all the arguments for each predicate in sentence.
</prevsent>
<prevsent>specifically, it involves identifying portions of the sentence that represent the predicates arguments and assigning pre-specified semantic roles to them.
</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
[a0seller ports of call inc.] reached agreements to [vverb sell] [a1thing its remaining seven aircraft] [a2buyer to buyers that werent disclosed] .is an example of srl annotation from the propbank corpus (palmer et al, 2005), <papid> J05-1004 </papid>where the sub scripted information maps the semantic roles a0, a1 and a2 to arguments for the predicate sell as defined in the propbank frame scheme.the availability of annotated corpora like propbank and framenet (fillmore et al, 2001) have provided rapid development of research into srl (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>pradhan et al, 2005).<papid> P05-1072 </papid></citsent>
<aftsection>
<nextsent>the shared tasks in conll2004 (carreras and ma`rquez, 2004), conll2005 (carreras and ma`rquez, 2005) and conll 2008 (surdeanu et al, 2008) were all focused on srl.
</nextsent>
<nextsent>srl systems (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002) <papid> P02-1031 </papid>have extensively used features defined over penn treebank phrase structure trees.</nextsent>
<nextsent>other syntactic representations such as ccg derivations (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and dependency trees (hacioglu,2004; surdeanu et al, 2008) have also been ex plored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1723">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling (srl) aims to identify and label all the arguments for each predicate in sentence.
</prevsent>
<prevsent>specifically, it involves identifying portions of the sentence that represent the predicates arguments and assigning pre-specified semantic roles to them.
</prevsent>
</prevsection>
<citsent citstr=" P05-1072 ">
[a0seller ports of call inc.] reached agreements to [vverb sell] [a1thing its remaining seven aircraft] [a2buyer to buyers that werent disclosed] .is an example of srl annotation from the propbank corpus (palmer et al, 2005), <papid> J05-1004 </papid>where the sub scripted information maps the semantic roles a0, a1 and a2 to arguments for the predicate sell as defined in the propbank frame scheme.the availability of annotated corpora like propbank and framenet (fillmore et al, 2001) have provided rapid development of research into srl (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>pradhan et al, 2005).<papid> P05-1072 </papid></citsent>
<aftsection>
<nextsent>the shared tasks in conll2004 (carreras and ma`rquez, 2004), conll2005 (carreras and ma`rquez, 2005) and conll 2008 (surdeanu et al, 2008) were all focused on srl.
</nextsent>
<nextsent>srl systems (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002) <papid> P02-1031 </papid>have extensively used features defined over penn treebank phrase structure trees.</nextsent>
<nextsent>other syntactic representations such as ccg derivations (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and dependency trees (hacioglu,2004; surdeanu et al, 2008) have also been ex plored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1730">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other syntactic representations such as ccg derivations (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and dependency trees (hacioglu,2004; surdeanu et al, 2008) have also been ex plored.</prevsent>
<prevsent>it has been previously noted that ltag, which has the useful property of extended domain of locality (edl), is well-suited to address the srl task, c.f.</prevsent>
</prevsection>
<citsent citstr=" D07-1062 ">
(chen and rambow, 2003; <papid> W03-1006 </papid>liu and sarkar, 2007).<papid> D07-1062 </papid></citsent>
<aftsection>
<nextsent>however, ltag elementary trees were extracted from the derived parse trees by using magerman-collins style head-percolation based heuristic rules (liu and sarkar, 2007).<papid> D07-1062 </papid></nextsent>
<nextsent>theltag-spinal treebank (shen et al, 2008) provided corpus of derivation trees where elementary trees were extracted from the penn treebank in combination with the propbank predicate argument annotation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1736">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, ltag elementary trees were extracted from the derived parse trees by using magerman-collins style head-percolation based heuristic rules (liu and sarkar, 2007).<papid> D07-1062 </papid></prevsent>
<prevsent>theltag-spinal treebank (shen et al, 2008) provided corpus of derivation trees where elementary trees were extracted from the penn treebank in combination with the propbank predicate argument annotation.</prevsent>
</prevsection>
<citsent citstr=" H05-1102 ">
the ltag-spinal treebank can be used to overcome some of the limitations of the previous work on srl using ltag: (liu and sarkar, 2007) <papid> D07-1062 </papid>uses ltag-based features extracted from phrase-structure trees as an additional source of features and combined them with features from phrase-structure based srl framework; (chenand rambow, 2003) <papid> W03-1006 </papid>only considers those comple ment/adjunct semantic roles that can be localized in ltag elementary trees, which leads to loss of over 17% instances of semantic roles even from gold-standard trees.the ltag-spinal formalism was initially proposed for automatic treebank extraction and statistical parsing (shen and joshi, 2005).<papid> H05-1102 </papid></citsent>
<aftsection>
<nextsent>however, its propbank-guided treebank extraction process further strengthens the connection between the ltag-spinal and semantic role labeling.
</nextsent>
<nextsent>in this paper, we present an srl system that was built to 1explore the utility of this new formalism, its treebank and the output of its statistical parser.
</nextsent>
<nextsent>experiments show that our ltag-spinal based srl system achieves very high precision on both gold standard and automatic parses, and significantly outperforms the one using ccgbank.
</nextsent>
<nextsent>more importantly, it shows that ltag-spinal is an useful resource for semantic role labeling, with the potential for further improvement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1737">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> ltag-spinal, its treebank and parsers.  </section>
<citcontext>
<prevsection>
<prevsent>and rambow, 2001).
</prevsent>
<prevsent>compared to traditional ltag, the two types of elementary trees (e-tree for short), initial and auxiliary trees, are in spinal form with no substitution nodes for arguments appearing in the predicate etree: spinal initial tree is composed of lexical spine from the root to the anchor, and nothing else; spinal auxiliary tree is composed of lexical spine and recursive spine from the root to the foot node.
</prevsent>
</prevsection>
<citsent citstr=" J94-1004 ">
for example, in figure 1 (from(shen et al, 2008)), the lexical spine for the auxiliary tree is b1, .., bi, .., bn, the recursive spine is b1, .., bi, .., b1 . two operations attachment and adjunction are defined in ltag-spinal where adjunction is the same as adjunction in the traditional ltag; attachment stems from sister adjunction as defined in tree insertion grammar (tig) (schabes and shieber, 1994), <papid> J94-1004 </papid>which corresponds to the case where the root of an initial tree is taken as child of another spinal e-tree.</citsent>
<aftsection>
<nextsent>the two operations are applied to ltag-spinal e-tree pairs resulting in anltag derivation tree which is similar to dependency tree (see figure 2).
</nextsent>
<nextsent>in figure 2, e-tree anchored with continue is the only auxiliary tree; all other e-trees are initial trees.
</nextsent>
<nextsent>the arrow is directed from parent to child, with the type of operation labeled on the arc. the operation types are: att denotes attachment operation; adj denotes adjunction operation.
</nextsent>
<nextsent>the sibling nodes may have differ an b1a1 bn initial: auxiliary: b1* bi figure 1: spinal elementary tree sent landing site along the parent spine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1740">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> ltag-spinal, its treebank and parsers.  </section>
<citcontext>
<prevsection>
<prevsent>each argument (a0, a1, etc.) is referred to as and the predicate is called . in most cases, the argument is found locally in the derivation tree due to the extended domain of locality in e-trees.
</prevsent>
<prevsent>thus, most arguments are identified by the pattern ? or ? a. the next section contains discussion of such patterns in more detail.
</prevsent>
</prevsection>
<citsent citstr=" D08-1052 ">
two statistical parsers have been developed by libin shen specifically for training on the ltag-spinal treebank: left-to-right incremental parser (shen and joshi, 2005) <papid> H05-1102 </papid>and bidirectional incremental parser (shen and joshi, 2008).<papid> D08-1052 </papid></citsent>
<aftsection>
<nextsent>if one compares the output of these two parsers, the leftto-right parser produces full ltag-spinal derivation trees (including all the information about specific elementary trees used in the derivation and the attachment information within the e-trees) while the bidirectional parser produces derivation trees without information about elementary treesor attachment points (similar to output from dependency parser).
</nextsent>
<nextsent>in this paper, we use the left to-right incremental parser for its richer output because our srl system uses feature functions that use information about the elementary trees inthe derivation tree and the attachment points between e-trees.
</nextsent>
<nextsent>the landing site of child node along the parent spine is useful for identifying different types of arguments in srl.
</nextsent>
<nextsent>for example, assume the parent spine is s-vp-vb-anchor?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1746">
<title id=" W09-2601.xml">exploration of the ltagspinal formalism and treebank for semantic role labeling </title>
<section> pcoordpxa 180 1.76 </section>
<citcontext>
<prevsection>
<prevsent>however, ccgbank appears to have large degree of mismatch.
</prevsent>
<prevsent>in this sense,root/head word based scoring provides fair comparison between ltag-spinal srl system and the ccgbank srl system.
</prevsent>
</prevsection>
<citsent citstr=" L08-1039 ">
recent work (boxwell and white, 2008)<papid> L08-1039 </papid>changes some structures in the ccgbank to correspond more closely with the prob bank annota tions.</citsent>
<aftsection>
<nextsent>they also resolve split arguments that occurin propbank and add these annotations into revised version of the ccgbank.
</nextsent>
<nextsent>as result they show that the oracle f-score improves by over 2 points over the (gildea and hockenmaier, 2003)<papid> W03-1008 </papid> oracle results for the numbered arguments only (a0, . . ., a5).</nextsent>
<nextsent>it remains an open question whether full srl system based on ccg parser trained on this new version of the ccgbank will be competitive against the ltag-spinal based and phrase structure based srl systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1749">
<title id=" W09-2402.xml">making sense of word sense variation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it has been widely observed that usage features such as vocabulary and syntax vary across corpora of different genres and registers (biber, 1995), and that serve different functions (kittredgeet al, 1991).
</prevsent>
<prevsent>still, we are far from able to predict specific morphosyntactic and lexical variations across corpora (kilgarriff, 2001), much less quantify them in way that makes it possible to apply the same analysis tools (taggers, parsers) without retraining.
</prevsent>
</prevsection>
<citsent citstr=" J91-4003 ">
in comparison to morphosyntactic properties of language, word and phrasal meaning is fluid, and to some degree, generative (pustejovsky, 1991; <papid> J91-4003 </papid>nunberg, 1979).</citsent>
<aftsection>
<nextsent>based on our initial observations from word sense annotation task for relatively pol ysemous words, carried out by multiple annotator son heterogeneous corpus, we hypothesize that different words lead to greater or lesser interannota tor agreement (ia) for reasons that in the long run should be explicitly modelled in order for natural language processing (nlp) applications to handle usage differences more robustly.
</nextsent>
<nextsent>this pilot study is step in that direction.
</nextsent>
<nextsent>we present related work in the next section, then describe the annotation task in the following one.
</nextsent>
<nextsent>in section 4, we present examples of variation in agreement on matched subset of words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1750">
<title id=" W09-2402.xml">making sense of word sense variation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in section 5 we discuss why we believe the observed variation depends on the words and present three lexical use factors we hypothesize to lead to greater or lesser ia. in section 6, we use association rules to mineour data for systematic differences among annotators, thus to explain the variations in ia. we conclude with summary of our findings goals.
</prevsent>
<prevsent>there has been decade-long community-wide effort to evaluate word sense disambiguation (wsd)systems across languages in the four senseval efforts (1998, 2001, 2004, and 2007, cf.
</prevsent>
</prevsection>
<citsent citstr=" W02-0812 ">
(kilgarriff, 1998; pedersen, 2002<papid> W02-0812 </papid>a; pedersen, 2002<papid> W02-0812 </papid>b; palme ret al, 2005)), with corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (palmer et al, 2005).</citsent>
<aftsection>
<nextsent>differences in ia and system performance across part-of-speech have been examined, as in (ng et al, 1999; <papid> W99-0502 </papid>palmer et al, 2 word pos no.</nextsent>
<nextsent>senses no.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1764">
<title id=" W09-2402.xml">making sense of word sense variation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there has been decade-long community-wide effort to evaluate word sense disambiguation (wsd)systems across languages in the four senseval efforts (1998, 2001, 2004, and 2007, cf.
</prevsent>
<prevsent>(kilgarriff, 1998; pedersen, 2002<papid> W02-0812 </papid>a; pedersen, 2002<papid> W02-0812 </papid>b; palme ret al, 2005)), with corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (palmer et al, 2005).</prevsent>
</prevsection>
<citsent citstr=" W99-0502 ">
differences in ia and system performance across part-of-speech have been examined, as in (ng et al, 1999; <papid> W99-0502 </papid>palmer et al, 2 word pos no.</citsent>
<aftsection>
<nextsent>senses no.
</nextsent>
<nextsent>occurrences fair adj 10 463 long adj 9 2706 quiet adj 6 244 land noun 11 1288 time noun 10 21790 work noun 7 5780 know verb 11 10334 say verb 11 20372 show verb 12 11877 tell verb 8 4799 table 1: ten words2005).
</nextsent>
<nextsent>pedersen (pedersen, 2002<papid> W02-0812 </papid>a) examines variation across individual words in evaluating wsd systems, but does not attempt to explain it.</nextsent>
<nextsent>factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multi labels (veronis, 1998; ide et al, 2002; <papid> W02-0808 </papid>passonneau et al, 2006), the number or granularity of senses (ng et al, 1999), <papid> W99-0502 </papid>merging of related senses (snow et al, 2007),<papid> D07-1107 </papid>sense similarity (chugur et al, 2002), <papid> W02-0805 </papid>sense perplexity (diab, 2004), <papid> P04-1039 </papid>entropy (diab, 2004; <papid> P04-1039 </papid>palmer etal., 2005), and in psycho linguistic experiments, reactions times required to distinguish senses (klein and murphy, 2002; ide and wilks, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1772">
<title id=" W09-2402.xml">making sense of word sense variation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>occurrences fair adj 10 463 long adj 9 2706 quiet adj 6 244 land noun 11 1288 time noun 10 21790 work noun 7 5780 know verb 11 10334 say verb 11 20372 show verb 12 11877 tell verb 8 4799 table 1: ten words2005).
</prevsent>
<prevsent>pedersen (pedersen, 2002<papid> W02-0812 </papid>a) examines variation across individual words in evaluating wsd systems, but does not attempt to explain it.</prevsent>
</prevsection>
<citsent citstr=" W02-0808 ">
factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multi labels (veronis, 1998; ide et al, 2002; <papid> W02-0808 </papid>passonneau et al, 2006), the number or granularity of senses (ng et al, 1999), <papid> W99-0502 </papid>merging of related senses (snow et al, 2007),<papid> D07-1107 </papid>sense similarity (chugur et al, 2002), <papid> W02-0805 </papid>sense perplexity (diab, 2004), <papid> P04-1039 </papid>entropy (diab, 2004; <papid> P04-1039 </papid>palmer etal., 2005), and in psycho linguistic experiments, reactions times required to distinguish senses (klein and murphy, 2002; ide and wilks, 2006).</citsent>
<aftsection>
<nextsent>with respect to using multiple annotators, snow et al included disambiguation of the word preside nta relatively non-polysemous word with three sense sin set of tasks given to amazon mechanical turkers, aimed at determining how to combine data from multiple non-experts for machine learning tasks.
</nextsent>
<nextsent>the word sense task comprised 177sentences taken from the semeval word sense disambiguation lexical sample task.
</nextsent>
<nextsent>majority voting among three annotators achieve 99% accuracy.
</nextsent>
<nextsent>the manually annotated sub-corpus (masc) project is creating small, representative corpus of american english written and spoken texts drawn from the open american national corpus (oanc).1 the masc corpus includes hand validated or manually produced annotations for variety of linguistic phenomena.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1775">
<title id=" W09-2402.xml">making sense of word sense variation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>occurrences fair adj 10 463 long adj 9 2706 quiet adj 6 244 land noun 11 1288 time noun 10 21790 work noun 7 5780 know verb 11 10334 say verb 11 20372 show verb 12 11877 tell verb 8 4799 table 1: ten words2005).
</prevsent>
<prevsent>pedersen (pedersen, 2002<papid> W02-0812 </papid>a) examines variation across individual words in evaluating wsd systems, but does not attempt to explain it.</prevsent>
</prevsection>
<citsent citstr=" D07-1107 ">
factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multi labels (veronis, 1998; ide et al, 2002; <papid> W02-0808 </papid>passonneau et al, 2006), the number or granularity of senses (ng et al, 1999), <papid> W99-0502 </papid>merging of related senses (snow et al, 2007),<papid> D07-1107 </papid>sense similarity (chugur et al, 2002), <papid> W02-0805 </papid>sense perplexity (diab, 2004), <papid> P04-1039 </papid>entropy (diab, 2004; <papid> P04-1039 </papid>palmer etal., 2005), and in psycho linguistic experiments, reactions times required to distinguish senses (klein and murphy, 2002; ide and wilks, 2006).</citsent>
<aftsection>
<nextsent>with respect to using multiple annotators, snow et al included disambiguation of the word preside nta relatively non-polysemous word with three sense sin set of tasks given to amazon mechanical turkers, aimed at determining how to combine data from multiple non-experts for machine learning tasks.
</nextsent>
<nextsent>the word sense task comprised 177sentences taken from the semeval word sense disambiguation lexical sample task.
</nextsent>
<nextsent>majority voting among three annotators achieve 99% accuracy.
</nextsent>
<nextsent>the manually annotated sub-corpus (masc) project is creating small, representative corpus of american english written and spoken texts drawn from the open american national corpus (oanc).1 the masc corpus includes hand validated or manually produced annotations for variety of linguistic phenomena.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1776">
<title id=" W09-2402.xml">making sense of word sense variation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>occurrences fair adj 10 463 long adj 9 2706 quiet adj 6 244 land noun 11 1288 time noun 10 21790 work noun 7 5780 know verb 11 10334 say verb 11 20372 show verb 12 11877 tell verb 8 4799 table 1: ten words2005).
</prevsent>
<prevsent>pedersen (pedersen, 2002<papid> W02-0812 </papid>a) examines variation across individual words in evaluating wsd systems, but does not attempt to explain it.</prevsent>
</prevsection>
<citsent citstr=" W02-0805 ">
factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multi labels (veronis, 1998; ide et al, 2002; <papid> W02-0808 </papid>passonneau et al, 2006), the number or granularity of senses (ng et al, 1999), <papid> W99-0502 </papid>merging of related senses (snow et al, 2007),<papid> D07-1107 </papid>sense similarity (chugur et al, 2002), <papid> W02-0805 </papid>sense perplexity (diab, 2004), <papid> P04-1039 </papid>entropy (diab, 2004; <papid> P04-1039 </papid>palmer etal., 2005), and in psycho linguistic experiments, reactions times required to distinguish senses (klein and murphy, 2002; ide and wilks, 2006).</citsent>
<aftsection>
<nextsent>with respect to using multiple annotators, snow et al included disambiguation of the word preside nta relatively non-polysemous word with three sense sin set of tasks given to amazon mechanical turkers, aimed at determining how to combine data from multiple non-experts for machine learning tasks.
</nextsent>
<nextsent>the word sense task comprised 177sentences taken from the semeval word sense disambiguation lexical sample task.
</nextsent>
<nextsent>majority voting among three annotators achieve 99% accuracy.
</nextsent>
<nextsent>the manually annotated sub-corpus (masc) project is creating small, representative corpus of american english written and spoken texts drawn from the open american national corpus (oanc).1 the masc corpus includes hand validated or manually produced annotations for variety of linguistic phenomena.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1777">
<title id=" W09-2402.xml">making sense of word sense variation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>occurrences fair adj 10 463 long adj 9 2706 quiet adj 6 244 land noun 11 1288 time noun 10 21790 work noun 7 5780 know verb 11 10334 say verb 11 20372 show verb 12 11877 tell verb 8 4799 table 1: ten words2005).
</prevsent>
<prevsent>pedersen (pedersen, 2002<papid> W02-0812 </papid>a) examines variation across individual words in evaluating wsd systems, but does not attempt to explain it.</prevsent>
</prevsection>
<citsent citstr=" P04-1039 ">
factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multi labels (veronis, 1998; ide et al, 2002; <papid> W02-0808 </papid>passonneau et al, 2006), the number or granularity of senses (ng et al, 1999), <papid> W99-0502 </papid>merging of related senses (snow et al, 2007),<papid> D07-1107 </papid>sense similarity (chugur et al, 2002), <papid> W02-0805 </papid>sense perplexity (diab, 2004), <papid> P04-1039 </papid>entropy (diab, 2004; <papid> P04-1039 </papid>palmer etal., 2005), and in psycho linguistic experiments, reactions times required to distinguish senses (klein and murphy, 2002; ide and wilks, 2006).</citsent>
<aftsection>
<nextsent>with respect to using multiple annotators, snow et al included disambiguation of the word preside nta relatively non-polysemous word with three sense sin set of tasks given to amazon mechanical turkers, aimed at determining how to combine data from multiple non-experts for machine learning tasks.
</nextsent>
<nextsent>the word sense task comprised 177sentences taken from the semeval word sense disambiguation lexical sample task.
</nextsent>
<nextsent>majority voting among three annotators achieve 99% accuracy.
</nextsent>
<nextsent>the manually annotated sub-corpus (masc) project is creating small, representative corpus of american english written and spoken texts drawn from the open american national corpus (oanc).1 the masc corpus includes hand validated or manually produced annotations for variety of linguistic phenomena.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1779">
<title id=" W09-2402.xml">making sense of word sense variation </title>
<section> observation: varying agreement,.  </section>
<citcontext>
<prevsection>
<prevsent>while there are no hard and fast criteria for what constitutes good ia, landis and koch (landis and koch, 1977) consider values between 0.40 and 0.60 to represent moderately good agreement, and values above 0.60 as quite good; krippendorff (krippen dorff, 1980) considers values above 0.67 moderately good, and values above 0.80 as quite good.
</prevsent>
<prevsent>(cf.
</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
(art stein and poesio, 2008) <papid> J08-4004 </papid>for discussion of agreement measurement for computational linguistic tasks.)</citsent>
<aftsection>
<nextsent>table 2 shows ia for pair of adjectives, nouns and verbs from our sample for which the ia scores are at the extremes (high and low) in each pair: the average delta is 0.24.
</nextsent>
<nextsent>note that the agreement decreases as part-of-speech varies from adjectives to nouns to verbs, but for all three parts-of-speech, there is wide spread of values.
</nextsent>
<nextsent>it is striking, given that the same annotators did all words, that one in each pair has relatively better agreement.
</nextsent>
<nextsent>3?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1791">
<title id=" W09-1908.xml">proactive learning for building machine translation systems for minority languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to lower the barriers surrounding mt system creation, we must reduce the time and resources needed to developmt for new language pairs.
</prevsent>
<prevsent>syntax based mt has proven to be good choice for minority language scenario(lavie et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
while the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems (koehn et al, 2003), <papid> N03-1017 </papid>the variety of linguistic annotation required is greater.</citsent>
<aftsection>
<nextsent>syntax based mt systems require lexicons that provide coverage for the target translations, synchronous grammar rules that define the divergences in word-orderacross the language-pair.
</nextsent>
<nextsent>in case of minority languages one can only expect to find meagre amount of such data, if any.
</nextsent>
<nextsent>building such resources effectively, within constrained budget, and deploying an mt system is the need of the day.
</nextsent>
<nextsent>we first consider active learning?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1793">
<title id=" W09-1908.xml">proactive learning for building machine translation systems for minority languages </title>
<section> syntax based machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude with some challenges that still remain in applying proactive learning for mt.
</prevsent>
<prevsent>in recent years, corpus based approaches to machine translation have become predominant, with phrase based statistical machine translation (pbsmt) (koehn et al, 2003) <papid> N03-1017 </papid>being the most actively progressing area.</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
recent research in syntax based machine translation (yamada and knight,2001; <papid> P01-1067 </papid>chiang, 2005) <papid> P05-1033 </papid>incorporates syntactic information to ameliorate the reordering problem faced by pb-smt approaches.</citsent>
<aftsection>
<nextsent>while traditional approaches to syntax based mt were dependent on availability of manual grammar, more recent approaches operate within the resources of pb-smt and induce hierarchical or linguistic grammars from existingphrasal units, to provide better generality and structure for reordering (yamada and knight, 2001; <papid> P01-1067 </papid>chiang, 2005; <papid> P05-1033 </papid>wu, 1997).<papid> J97-3002 </papid></nextsent>
<nextsent>2.1 resources for syntax mt. syntax based approaches to mt seek to leverage the structure of natural language to automatically induce mt systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1794">
<title id=" W09-1908.xml">proactive learning for building machine translation systems for minority languages </title>
<section> syntax based machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude with some challenges that still remain in applying proactive learning for mt.
</prevsent>
<prevsent>in recent years, corpus based approaches to machine translation have become predominant, with phrase based statistical machine translation (pbsmt) (koehn et al, 2003) <papid> N03-1017 </papid>being the most actively progressing area.</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
recent research in syntax based machine translation (yamada and knight,2001; <papid> P01-1067 </papid>chiang, 2005) <papid> P05-1033 </papid>incorporates syntactic information to ameliorate the reordering problem faced by pb-smt approaches.</citsent>
<aftsection>
<nextsent>while traditional approaches to syntax based mt were dependent on availability of manual grammar, more recent approaches operate within the resources of pb-smt and induce hierarchical or linguistic grammars from existingphrasal units, to provide better generality and structure for reordering (yamada and knight, 2001; <papid> P01-1067 </papid>chiang, 2005; <papid> P05-1033 </papid>wu, 1997).<papid> J97-3002 </papid></nextsent>
<nextsent>2.1 resources for syntax mt. syntax based approaches to mt seek to leverage the structure of natural language to automatically induce mt systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1797">
<title id=" W09-1908.xml">proactive learning for building machine translation systems for minority languages </title>
<section> syntax based machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>in recent years, corpus based approaches to machine translation have become predominant, with phrase based statistical machine translation (pbsmt) (koehn et al, 2003) <papid> N03-1017 </papid>being the most actively progressing area.</prevsent>
<prevsent>recent research in syntax based machine translation (yamada and knight,2001; <papid> P01-1067 </papid>chiang, 2005) <papid> P05-1033 </papid>incorporates syntactic information to ameliorate the reordering problem faced by pb-smt approaches.</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
while traditional approaches to syntax based mt were dependent on availability of manual grammar, more recent approaches operate within the resources of pb-smt and induce hierarchical or linguistic grammars from existingphrasal units, to provide better generality and structure for reordering (yamada and knight, 2001; <papid> P01-1067 </papid>chiang, 2005; <papid> P05-1033 </papid>wu, 1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>2.1 resources for syntax mt. syntax based approaches to mt seek to leverage the structure of natural language to automatically induce mt systems.
</nextsent>
<nextsent>depending upon the mt system and the paradigm, the resource requirements may varyand could also include modules such as morphological analyzers, sense disambiguation modules, generators etc. detailed discussion of the comprehensive pipeline, may be out of the scope of this paper, more so because such resources can not be expected in low-resource language scenario.
</nextsent>
<nextsent>we only focus on the quintessential set of modules for mtpipeline - data acquisition, word-alignment, syntactic analysis etc. the resources can broadly be categorized as monolingual?
</nextsent>
<nextsent>vs bilingual?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1798">
<title id=" W09-1908.xml">proactive learning for building machine translation systems for minority languages </title>
<section> active learning for mt.  </section>
<citcontext>
<prevsection>
<prevsent>np v
</prevsent>
<prevsent>modern syntax based mt rides on the success of both statistical machine translation and statisticalparsing.
</prevsent>
</prevsection>
<citsent citstr=" J04-3001 ">
active learning has been applied to statistical parsing (hwa, 2004; <papid> J04-3001 </papid>baldridge and osborne,2003) <papid> W03-0403 </papid>to improve sample selection for manual anno tation.</citsent>
<aftsection>
<nextsent>in case of mt, active learning has remained largely unexplored.
</nextsent>
<nextsent>some attempts include training multiple statistical mt systems on varying amounts of data, and exploring committee based selection for re-ranking the data to be translated and included for re-training.
</nextsent>
<nextsent>but this does not apply to training in low-resource scenario where data is scarce.
</nextsent>
<nextsent>in the rest of the section we discuss the different scenarios that arise in gathering of annotation for mt under traditional active learning?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1799">
<title id=" W09-1908.xml">proactive learning for building machine translation systems for minority languages </title>
<section> active learning for mt.  </section>
<citcontext>
<prevsection>
<prevsent>np v
</prevsent>
<prevsent>modern syntax based mt rides on the success of both statistical machine translation and statisticalparsing.
</prevsent>
</prevsection>
<citsent citstr=" W03-0403 ">
active learning has been applied to statistical parsing (hwa, 2004; <papid> J04-3001 </papid>baldridge and osborne,2003) <papid> W03-0403 </papid>to improve sample selection for manual anno tation.</citsent>
<aftsection>
<nextsent>in case of mt, active learning has remained largely unexplored.
</nextsent>
<nextsent>some attempts include training multiple statistical mt systems on varying amounts of data, and exploring committee based selection for re-ranking the data to be translated and included for re-training.
</nextsent>
<nextsent>but this does not apply to training in low-resource scenario where data is scarce.
</nextsent>
<nextsent>in the rest of the section we discuss the different scenarios that arise in gathering of annotation for mt under traditional active learning?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1800">
<title id=" W09-1908.xml">proactive learning for building machine translation systems for minority languages </title>
<section> active learning for mt.  </section>
<citcontext>
<prevsection>
<prevsent>we can elicit translations for building parallel corpus from bilingual speakers who speak both the languages with certain accuracy or from linguist who is well educated in the formal senseof the languages.
</prevsent>
<prevsent>with the success of collaborative sites like amazons mechanical turk?
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
1, one 1http://www.mturk.com/ 59can provide the task of annotation to multiple oracles on the internet (snow et al, 2008).<papid> D08-1027 </papid></citsent>
<aftsection>
<nextsent>the taskof word alignment can be posed in similar fashion too.
</nextsent>
<nextsent>more interestingly, there are statistical tools like giza 2 that take as input un-annotated parallel data and propose automatic correspondences between words in the language-pair, giving scope to machine oracles?.
</nextsent>
<nextsent>3.2 varying quality and reliability.
</nextsent>
<nextsent>oracles also vary on the correctness of the answers they provide (quality) as well as their availability (robustness) to answer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1801">
<title id=" W09-2416.xml">semeval2010 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it should also promote paraphrase-based approaches to the problem, which can benefit many nlp applications.
</prevsent>
<prevsent>noun compounds (ncs) ? sequences of two or more nouns acting as single noun,1 e.g., colon cancer tumor suppressor protein ? are abundant in english and pose major challenge to the automatic analysis of written text.
</prevsent>
</prevsection>
<citsent citstr=" W04-0404 ">
baldwin and tanaka (2004) <papid> W04-0404 </papid>calculated that 3.9% and 2.6% of the tokens in the reuters corpus and the british national corpus (bnc), respectively, are part of noun compound.compounding is also an extremely productive process in english.</citsent>
<aftsection>
<nextsent>the frequency spectrum of compound types follows zipfian or power-law distribution (o? seaghdha, 2008), so in practice many compound tokens encountered belong to long tail?
</nextsent>
<nextsent>of low-frequency types.
</nextsent>
<nextsent>for example, over half of the two-noun nc types in the bnc occur just once (lapata and lascarides, 2003).<papid> E03-1073 </papid></nextsent>
<nextsent>even for relatively frequent ncs that occur ten or more times in thebnc, static english dictionaries give only 27% coverage (tanaka and baldwin, 2003).<papid> W03-1803 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1802">
<title id=" W09-2416.xml">semeval2010 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the frequency spectrum of compound types follows zipfian or power-law distribution (o? seaghdha, 2008), so in practice many compound tokens encountered belong to long tail?
</prevsent>
<prevsent>of low-frequency types.
</prevsent>
</prevsection>
<citsent citstr=" E03-1073 ">
for example, over half of the two-noun nc types in the bnc occur just once (lapata and lascarides, 2003).<papid> E03-1073 </papid></citsent>
<aftsection>
<nextsent>even for relatively frequent ncs that occur ten or more times in thebnc, static english dictionaries give only 27% coverage (tanaka and baldwin, 2003).<papid> W03-1803 </papid></nextsent>
<nextsent>taken together, 1we follow the definition in (downing, 1977).the factors of high frequency and high productivity mean that achieving robust nc interpretation isan important goal for broad-coverage semantic pro cessing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1803">
<title id=" W09-2416.xml">semeval2010 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>of low-frequency types.
</prevsent>
<prevsent>for example, over half of the two-noun nc types in the bnc occur just once (lapata and lascarides, 2003).<papid> E03-1073 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1803 ">
even for relatively frequent ncs that occur ten or more times in thebnc, static english dictionaries give only 27% coverage (tanaka and baldwin, 2003).<papid> W03-1803 </papid></citsent>
<aftsection>
<nextsent>taken together, 1we follow the definition in (downing, 1977).the factors of high frequency and high productivity mean that achieving robust nc interpretation isan important goal for broad-coverage semantic processing.
</nextsent>
<nextsent>ncs provide concise means of evoking arelationship between two or more nouns, and natural language processing (nlp) systems that do not try to recover these implicit relations from ncs are effectively discarding valuable semantic information.
</nextsent>
<nextsent>broad coverage should therefore be achieved by post-hoc interpretation rather than pre-hoc enumeration, since it is impossible to build lexicon of all ncs likely to be encountered.the challenges presented by ncs and their semantics have generated significant ongoing interest in nc interpretation in the nlp community.
</nextsent>
<nextsent>representative publications include (butnariu and veale, 2008; <papid> C08-1011 </papid>girju, 2007; kim and baldwin, 2006; <papid> P06-2064 </papid>nakov, 2008b; nastase and szpakowicz, 2003; o?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1804">
<title id=" W09-2416.xml">semeval2010 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ncs provide concise means of evoking arelationship between two or more nouns, and natural language processing (nlp) systems that do not try to recover these implicit relations from ncs are effectively discarding valuable semantic information.
</prevsent>
<prevsent>broad coverage should therefore be achieved by post-hoc interpretation rather than pre-hoc enumeration, since it is impossible to build lexicon of all ncs likely to be encountered.the challenges presented by ncs and their semantics have generated significant ongoing interest in nc interpretation in the nlp community.
</prevsent>
</prevsection>
<citsent citstr=" C08-1011 ">
representative publications include (butnariu and veale, 2008; <papid> C08-1011 </papid>girju, 2007; kim and baldwin, 2006; <papid> P06-2064 </papid>nakov, 2008b; nastase and szpakowicz, 2003; o?</citsent>
<aftsection>
<nextsent>seaghdha and copestake, 2007).
</nextsent>
<nextsent>applications that have been suggested include question answering, machine translation, information retrieval and informationextraction.
</nextsent>
<nextsent>for example, question-answering system may need to determine whether headaches induced by caffeine withdrawal is good paraphrase for caffeine headaches when answering questions about the causes of headaches, while an information extraction system may need to decide whether caffeine withdrawal headache and caffeine headache refer to the same concept when used in the same document.
</nextsent>
<nextsent>similarly, machine translation system facing the unknown nc wto geneva headquarters might benefit from the ability to paraphrase it as geneva headquarters of the wto or as wto headquarters located in geneva.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1805">
<title id=" W09-2416.xml">semeval2010 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ncs provide concise means of evoking arelationship between two or more nouns, and natural language processing (nlp) systems that do not try to recover these implicit relations from ncs are effectively discarding valuable semantic information.
</prevsent>
<prevsent>broad coverage should therefore be achieved by post-hoc interpretation rather than pre-hoc enumeration, since it is impossible to build lexicon of all ncs likely to be encountered.the challenges presented by ncs and their semantics have generated significant ongoing interest in nc interpretation in the nlp community.
</prevsent>
</prevsection>
<citsent citstr=" P06-2064 ">
representative publications include (butnariu and veale, 2008; <papid> C08-1011 </papid>girju, 2007; kim and baldwin, 2006; <papid> P06-2064 </papid>nakov, 2008b; nastase and szpakowicz, 2003; o?</citsent>
<aftsection>
<nextsent>seaghdha and copestake, 2007).
</nextsent>
<nextsent>applications that have been suggested include question answering, machine translation, information retrieval and informationextraction.
</nextsent>
<nextsent>for example, question-answering system may need to determine whether headaches induced by caffeine withdrawal is good paraphrase for caffeine headaches when answering questions about the causes of headaches, while an information extraction system may need to decide whether caffeine withdrawal headache and caffeine headache refer to the same concept when used in the same document.
</nextsent>
<nextsent>similarly, machine translation system facing the unknown nc wto geneva headquarters might benefit from the ability to paraphrase it as geneva headquarters of the wto or as wto headquarters located in geneva.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1807">
<title id=" W09-2416.xml">semeval2010 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> models of relational semantics in ncs.  </section>
<citcontext>
<prevsection>
<prevsent>both schemes are intended to be suitable for broad-coverage analysis of text.
</prevsent>
<prevsent>for specialized applications, however, it is often useful to use domain-specific relations.
</prevsent>
</prevsection>
<citsent citstr=" W01-0511 ">
for example,rosario and hearst (2001) <papid> W01-0511 </papid>propose 18 abstract relations for interpreting ncs in biomedical text, e.g., defect, material, person affiliated, attribute of clinical study.inventory-based analyses offer significant advantages.</citsent>
<aftsection>
<nextsent>abstract relations such as location?
</nextsent>
<nextsent>and pos session?
</nextsent>
<nextsent>capture valuable generalizations about nc semantics in parsimonious framework.
</nextsent>
<nextsent>unlike paraphrase-based analyses (section 2.2), they arenot tied to specific lexical items, which may themselves be semantically ambiguous.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1808">
<title id=" W09-2416.xml">semeval2010 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> models of relational semantics in ncs.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, manyncs cannot be paraphrased adequately with prepositions, e.g., woman driver, honey bee.a richer, more flexible paraphrasing model is afforded by the use of verbs.
</prevsent>
<prevsent>in such model, honeybee is bee that produces honey, sleeping pill is pill that induces sleeping and headache pill is pill that relieves headaches.
</prevsent>
</prevsection>
<citsent citstr=" P84-1109 ">
in some previous computational work on nc interpretation, manually constructed dictionaries provided typical activities or functions associated with nouns (finin, 1980; isabelle, 1984; <papid> P84-1109 </papid>johnston and busa, 1996).<papid> W96-0309 </papid></citsent>
<aftsection>
<nextsent>it is, how ever, impractical to build large structured lexicons for broad-coverage systems; these methods can only be applied to specialized domains.
</nextsent>
<nextsent>on the other hand, we expect that the ready availability of large text corpora should facilitate the automatic mining of rich paraphrase information.
</nextsent>
<nextsent>the semeval-2010 task we present here builds on the work of nakov (nakov and hearst, 2006; nakov, 2007; nakov, 2008b), where ncs are paraphrased by combinations of verbs and prepositions.
</nextsent>
<nextsent>given the problem of synonymy, we do not provide single correct paraphrase forgiven nc but probability distribution over range of candidates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1809">
<title id=" W09-2416.xml">semeval2010 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> models of relational semantics in ncs.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, manyncs cannot be paraphrased adequately with prepositions, e.g., woman driver, honey bee.a richer, more flexible paraphrasing model is afforded by the use of verbs.
</prevsent>
<prevsent>in such model, honeybee is bee that produces honey, sleeping pill is pill that induces sleeping and headache pill is pill that relieves headaches.
</prevsent>
</prevsection>
<citsent citstr=" W96-0309 ">
in some previous computational work on nc interpretation, manually constructed dictionaries provided typical activities or functions associated with nouns (finin, 1980; isabelle, 1984; <papid> P84-1109 </papid>johnston and busa, 1996).<papid> W96-0309 </papid></citsent>
<aftsection>
<nextsent>it is, how ever, impractical to build large structured lexicons for broad-coverage systems; these methods can only be applied to specialized domains.
</nextsent>
<nextsent>on the other hand, we expect that the ready availability of large text corpora should facilitate the automatic mining of rich paraphrase information.
</nextsent>
<nextsent>the semeval-2010 task we present here builds on the work of nakov (nakov and hearst, 2006; nakov, 2007; nakov, 2008b), where ncs are paraphrased by combinations of verbs and prepositions.
</nextsent>
<nextsent>given the problem of synonymy, we do not provide single correct paraphrase forgiven nc but probability distribution over range of candidates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1813">
<title id=" W09-2416.xml">semeval2010 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> models of relational semantics in ncs.  </section>
<citcontext>
<prevsection>
<prevsent>as described in section 3.3, set of gold standard paraphrase distributions can be constructed by collating responses from large number of human subjects.in this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an nc.
</prevsent>
<prevsent>nakov (2008b) and butnariu and veale(2008) <papid> C08-1011 </papid>have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in 102 semeval-2010 task 9 will further develop suitable techniques for this problem.</prevsent>
</prevsection>
<citsent citstr=" P08-1052 ">
paraphrases of this kind have been shown to be useful in applications such as machine translation (nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (kim and baldwin, 2006; <papid> P06-2064 </papid>nakov and hearst, 2008).<papid> P08-1052 </papid></citsent>
<aftsection>
<nextsent>progress in paraphrasing is therefore likely to have follow-on benefits in many areas.
</nextsent>
<nextsent>the description of the task we present below is preliminary.
</nextsent>
<nextsent>we invite the interested reader to visit the official website of semeval-2010 task 9, where up to-date information will be published; there is also discussion group and mailing list.2 3.1 preliminary study.
</nextsent>
<nextsent>in preliminary study, we asked 25-30 human subjects to paraphrase 250 noun-noun compounds using suitable paraphrasing verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1814">
<title id=" W09-2416.xml">semeval2010 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>following the methodology of nakov (2008b), we will use the amazon mechanical turk web service4 to recruit human subjects.
</prevsent>
<prevsent>this service offers an inexpensive way to recruit subjects for tasks that require human intelligence, and provides an api which allows computer program to easily run tasks and collate the responses from human subjects.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
the mechanical turk is becoming popular means to elicit and collect linguistic intuitions for nlp research; see snowet al (2008) <papid> D08-1027 </papid>for an overview and discussion of issues that arise.</citsent>
<aftsection>
<nextsent>we intend to recruit 100 annotators for each nc, and we will require each annotator to paraphrase at least five ncs.
</nextsent>
<nextsent>annotators will be given clear instructions and will be asked to produce one or more paraphrases forgiven nc.
</nextsent>
<nextsent>to help us filter out subjects with an insufficient grasp of english or an insufficient interest in the task, annotators willbe asked to complete short and simple multiple choice pretest on nc comprehension before proceeding to the paraphrasing step.
</nextsent>
<nextsent>post-processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1816">
<title id=" W09-1903.xml">estimating annotation cost for active learning in a multi annotator environment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate that combination of instance, annotator and annotation task characteristics are important for developing an accurate estimator, and argue that both correlation coefficient and root mean square error should be used for evaluating annotation cost estimators.
</prevsent>
<prevsent>active learning is the process of selectively querying the user to annotate examples with the goalof minimizing the total annotation cost.
</prevsent>
</prevsection>
<citsent citstr=" L08-1367 ">
annotation cost has been traditionally measured in terms of the number of examples annotated, but it has been widely acknowledged that different examples may require different annotation effort (settles et al, 2008; ringger et al, 2008).<papid> L08-1367 </papid></citsent>
<aftsection>
<nextsent>ideally, we would use actual human annotation cost for evaluating selective sampling strategies, but this will require conducting several user studies, one per strategy on the same dataset.
</nextsent>
<nextsent>alternatively, wemay be able to simulate the real user by an annotation cost estimator that can then be used to evaluate several selective sampling strategies without having to run new user study each time.
</nextsent>
<nextsent>an annotation cost estimator models the characteristics that can differentiate the examples in terms of their annotation time.
</nextsent>
<nextsent>the characteristics that strongly correlate with the annotation time can be used as criterion in selective sampling strategies to minimize the total annotation cost.in some domains, the annotation cost of an example is known or can be calculated exactly before querying the user.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1818">
<title id=" W09-1903.xml">estimating annotation cost for active learning in a multi annotator environment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the characteristics that strongly correlate with the annotation time can be used as criterion in selective sampling strategies to minimize the total annotation cost.in some domains, the annotation cost of an example is known or can be calculated exactly before querying the user.
</prevsent>
<prevsent>for example, in biological experiments it might be calculable from the cost of the equipment and the material used (king et al, 2004).
</prevsent>
</prevsection>
<citsent citstr=" D07-1051 ">
in nlp, sometimes simplifying assumption is made that the annotation cost for an example can be measured in terms of its length (e.g. seconds of voicemail annotated (kapoor et al, 2007); number of tokens annotated (tomanek et al, 2007)).<papid> D07-1051 </papid></citsent>
<aftsection>
<nextsent>another assumption is that the number of user annotation actions can be used as proxy for annotation cost of an example (e.g. number of brackets added for parsing sentence (hwa, 2000); <papid> W00-1306 </papid>number of clicks for correcting named entities (kristjannson et al, 2004)).</nextsent>
<nextsent>while these are important factors in determining the annotation cost, none of them alone can fully substitute for the actual annotation cost.for example, short sentence with lot of embedded clauses may be more costly to annotate than longer sentence with simpler grammatical structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1819">
<title id=" W09-1903.xml">estimating annotation cost for active learning in a multi annotator environment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, in biological experiments it might be calculable from the cost of the equipment and the material used (king et al, 2004).
</prevsent>
<prevsent>in nlp, sometimes simplifying assumption is made that the annotation cost for an example can be measured in terms of its length (e.g. seconds of voicemail annotated (kapoor et al, 2007); number of tokens annotated (tomanek et al, 2007)).<papid> D07-1051 </papid></prevsent>
</prevsection>
<citsent citstr=" W00-1306 ">
another assumption is that the number of user annotation actions can be used as proxy for annotation cost of an example (e.g. number of brackets added for parsing sentence (hwa, 2000); <papid> W00-1306 </papid>number of clicks for correcting named entities (kristjannson et al, 2004)).</citsent>
<aftsection>
<nextsent>while these are important factors in determining the annotation cost, none of them alone can fully substitute for the actual annotation cost.for example, short sentence with lot of embedded clauses may be more costly to annotate than longer sentence with simpler grammatical structure.
</nextsent>
<nextsent>similarly, short sentence with multiple verbs and discontinuous arguments may take more time to annotate with semantic roles than longer sentence with single verb and simple subject-verb-object structure (carreras and marquez, 2004).what further complicates the estimation of annotation cost is that even for the same example, annotation cost may vary across annotators (settles et al, 2008).
</nextsent>
<nextsent>for example, non-native speakers of english were found to take longer time to annotate part of 18 speech tags (ringger et al, 2008).<papid> L08-1367 </papid></nextsent>
<nextsent>often multiple annotators are used for creating an annotated corpus to avoid annotator bias, and we may not know all our annotators beforehand.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1827">
<title id=" W09-1903.xml">estimating annotation cost for active learning in a multi annotator environment </title>
<section> analysis and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the data we used were collected as part of graduate course.
</prevsent>
<prevsent>twenty annotators (students and instructors) were grouped into five groups of four each.
</prevsent>
</prevsection>
<citsent citstr=" N07-1033 ">
the groups were created such that each group had similar variance in annotator characteristics such as department, educational experience, programming experience, etc. we used the first 200 movie reviews from the dataset provided by zaidan et al (2007), <papid> N07-1033 </papid>with an equal distribution of positive and negative examples.</citsent>
<aftsection>
<nextsent>each group annotated 25 movie reviews randomly selected from the 200 reviews and all annotators in each group annotated all 25 reviews.
</nextsent>
<nextsent>in addition to voting positive or negative for review, annotators also annotated rationales (zaidan et al, 2007), <papid> N07-1033 </papid>spans of text in the review that support their vote.rationales can be used to guide the model by identifying the most discriminant features.</nextsent>
<nextsent>in related work(arora and nyberg, 2009), we ascertain that with rationales the same performance can be achieved with less annotated data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1830">
<title id=" W09-1903.xml">estimating annotation cost for active learning in a multi annotator environment </title>
<section> analysis and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>among the 20 annotators in the study, there were 8 annotators with native ness value of 1, and 6 each for native ness values of 2 and 3.
</prevsent>
<prevsent>table 1 shows the average and standard deviation of the native ness score in each group.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
21 feature definition intuition character length (cl) length of review in terms of number of characters longer document stake longer to annotate polar word count (pc) number of words that are polar (strong subjective words from the lexicon (wilson et al, 2005)) <papid> H05-1044 </papid>more subjectivity implies user would need more time to judge positive vs. negative stop word percent (sc) percentage of words that are stop words high percentage of stop words implies that the text is not very complex and hence easier to read.avg.</citsent>
<aftsection>
<nextsent>sentence length (sl)average of the character length of sentences in the review long sentences in review may make it harder to read.
</nextsent>
<nextsent>table 2: instance characteristics feature mean standard deviation cl 2.25 0.92 pc 41.50 20.39 sp 0.45 0.03 sl 121.90 28.72 nr 4.80 2.30table 3: mean and the standard deviation for the feature occurrences in the data.
</nextsent>
<nextsent>3.3 evaluation metric.
</nextsent>
<nextsent>we use both root mean square (rms) error and correlation coefficient (crcoef) to evaluate our model, since the two metrics evaluate different aspects of an estimate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1832">
<title id=" W10-0608.xml">acquiring human like feature based conceptual representations from corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>61 of norms lists features for over 500 concepts, the relatively small size of property norm sets still gives cause for concern.
</prevsent>
<prevsent>larger sets of norms would be useful to psycholinguists; however, large-scale property norming studies are time-consuming and costly.
</prevsent>
</prevsection>
<citsent citstr=" P08-1079 ">
in nlp, researchers have developed methods for extracting and classifying generic relationships from data, e.g. pantel and pennacchiotti (2008), davidov and rappoport (2008<papid> P08-1079 </papid>a), davidov and rappoport (2008<papid> P08-1079 </papid>b).</citsent>
<aftsection>
<nextsent>in recent years, researchers have also begun to develop methods which can automatically extract feature norm-like representations from corpora, e.g. almuhareb and poesio (2005), barbu (2008), baroni et al  (2009).
</nextsent>
<nextsent>the automatic approach is capable of gathering large-scale distributional data, and furthermore it iscost-effective.
</nextsent>
<nextsent>corpora contain natural-language instances of words denoting concepts and their features, and therefore serve as ideal material for feature generation tasks.
</nextsent>
<nextsent>however, current methods are restricted to specific relations between concepts and their features, or target concept-feature pairs only.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1840">
<title id=" W10-0608.xml">acquiring human like feature based conceptual representations from corpora </title>
<section> extraction method.  </section>
<citcontext>
<prevsection>
<prevsent>the first of these(wiki500) consists of the wikipedia articles corresponding to each of the mcrae concepts.
</prevsent>
<prevsent>it contains c. 500 articles (1.1 million words).
</prevsent>
</prevsection>
<citsent citstr=" C94-1103 ">
the second sub corpus is comprised of those articles where the title is fewer than five words long and contains one of the mcrae concept words.3 this corpus, called wiki110k, holds 109,648 plain text articles (36.5 million words).we also employ the 100-million word british national corpus (bnc) (leech et al , 1994) <papid> C94-1103 </papid>which contains written (90%) and spoken (10%) english.</citsent>
<aftsection>
<nextsent>it was designed to represent broad cross-section of modern british english.
</nextsent>
<nextsent>this corpus provides an interesting contrast with wikipedia, since we assume that any features contained in such wide-ranging corpus would be presented in an incidental fashion rather than explicitly.
</nextsent>
<nextsent>the bnc may contain useful features which are encoded in everyday speech and text but not in wikipedia, perhaps due to their ambiguity for encyclopedic purposes, or due to their non-scientific but rather common-sense nature.
</nextsent>
<nextsent>for example, eaten by monkeys is listed as feature of banana in the mcrae norms, but the word monkey does not appear in the wikipedia banana article.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1841">
<title id=" W10-0608.xml">acquiring human like feature based conceptual representations from corpora </title>
<section> extraction method.  </section>
<citcontext>
<prevsection>
<prevsent>in the first stage, we extract large sets of candidateconcept-relation-feature triples for each target concept from parsed corpus data.
</prevsent>
<prevsent>in the second stage, we re-rank and filter these triples with the intention of retaining only those triples which are likely to be true semantic features.
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
in the first stage, the corpora are parsed using the robust accurate statistical parsing (rasp) system(briscoe et al , 2006).<papid> P06-4020 </papid></citsent>
<aftsection>
<nextsent>for each sentence in the corpora, this yields the most probable analysis returned by the parser in the form of set of grammatical relations (grs).
</nextsent>
<nextsent>the gr sets for each sentence containing the target concept noun are then retrieved from the corpus.
</nextsent>
<nextsent>these grs form an undirected acyclic graph, whose nodes are labelled with words in the sentence and their pos, and whose edges are labelled with the gr types linking the nodes together.
</nextsent>
<nextsent>using this graph we generate all possible paths which are rooted at our target concept node using breadth-first search.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1842">
<title id=" W10-0608.xml">acquiring human like feature based conceptual representations from corpora </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this underscores the usefulness of parsing for semantically meaningful feature extraction.
</prevsent>
<prevsent>this is consistent with recent work in the field of computational lexical semantics, although gr data has not previously been successfully applied to feature extraction.we showed that semantic information about cooccurring concept and feature clusters can be used to enhance feature acquisition.
</prevsent>
</prevsection>
<citsent citstr=" D09-1067 ">
we employed the mcrae norms for our analysis, however we could also employ other knowledge resources and cluster relation verbs using recent methods, e.g. sun and korhonen (2009), <papid> D09-1067 </papid>vlachos et al  (2009).<papid> W09-0210 </papid>our paper has also investigated methods of evaluation, which is critical but difficult issue for feature extraction.</citsent>
<aftsection>
<nextsent>most recent approaches have been evaluated against the esslli sub-set of the mcrae norms which expands the set of features in the norms with their synonyms.
</nextsent>
<nextsent>yet even expansion sets likethe esslli norms do not facilitate adequate evaluation because they are not complete in the sense that there are true features which are not included in the norms.
</nextsent>
<nextsent>our qualitative analysis shows that many of the errors against the recoded norms are in fact correct or plausible features.
</nextsent>
<nextsent>future work can aim for larger-scale qualitative evaluation using multiple judges as well as investigating other task based evaluations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1843">
<title id=" W10-0608.xml">acquiring human like feature based conceptual representations from corpora </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this underscores the usefulness of parsing for semantically meaningful feature extraction.
</prevsent>
<prevsent>this is consistent with recent work in the field of computational lexical semantics, although gr data has not previously been successfully applied to feature extraction.we showed that semantic information about cooccurring concept and feature clusters can be used to enhance feature acquisition.
</prevsent>
</prevsection>
<citsent citstr=" W09-0210 ">
we employed the mcrae norms for our analysis, however we could also employ other knowledge resources and cluster relation verbs using recent methods, e.g. sun and korhonen (2009), <papid> D09-1067 </papid>vlachos et al  (2009).<papid> W09-0210 </papid>our paper has also investigated methods of evaluation, which is critical but difficult issue for feature extraction.</citsent>
<aftsection>
<nextsent>most recent approaches have been evaluated against the esslli sub-set of the mcrae norms which expands the set of features in the norms with their synonyms.
</nextsent>
<nextsent>yet even expansion sets likethe esslli norms do not facilitate adequate evaluation because they are not complete in the sense that there are true features which are not included in the norms.
</nextsent>
<nextsent>our qualitative analysis shows that many of the errors against the recoded norms are in fact correct or plausible features.
</nextsent>
<nextsent>future work can aim for larger-scale qualitative evaluation using multiple judges as well as investigating other task based evaluations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1844">
<title id=" W09-2207.xml">an analysis of bootstrapping for the recognition of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this problem is subpart ofa task called tern (temporal expression recognition and normalization), where temporal expressions are first identified in text and then its intended temporal meaning is represented in canonical format.
</prevsent>
<prevsent>tern was first proposed as an independent task in the 2004 edition of the ace conferences1.
</prevsent>
</prevsection>
<citsent citstr=" W01-1313 ">
the most widely used standard for the annotation of temporal expressions is timex (ferro et al, 2005).the most common approach to temporal expression recognition in the past has been the use of hand-made grammars to capture the expressions (see(wiebe et al, 1998; filatova and hovy, 2001; <papid> W01-1313 </papid>saquete et al, 2004) for examples), which can then be easily expanded with additional attributes for the normalization task, based on computing distance and direction (past or future) with respect to reference time.</citsent>
<aftsection>
<nextsent>this approach achieves an f1-measureof approximately 85% for recognition and normalization.
</nextsent>
<nextsent>the use of machine learning techniques ? mainly statistical?
</nextsent>
<nextsent>for this task is more recent development, either alongside the traditional hand grammar approach to learn to distinguish specific difficult cases (mani and wilson, 2000), <papid> P00-1010 </papid>or on its own (hacioglu et al, 2005).</nextsent>
<nextsent>the latter apply svmsto the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an f1-measure of 87.8%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1845">
<title id=" W09-2207.xml">an analysis of bootstrapping for the recognition of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this approach achieves an f1-measureof approximately 85% for recognition and normalization.
</prevsent>
<prevsent>the use of machine learning techniques ? mainly statistical?
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
for this task is more recent development, either alongside the traditional hand grammar approach to learn to distinguish specific difficult cases (mani and wilson, 2000), <papid> P00-1010 </papid>or on its own (hacioglu et al, 2005).</citsent>
<aftsection>
<nextsent>the latter apply svmsto the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an f1-measure of 87.8%.
</nextsent>
<nextsent>1http://www.nist.gov/speech/tests/ace/ 49 bootstrapping techniques have been used for such diverse nlp problems as: word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>named entity classification (collins and singer, 1999), <papid> W99-0613 </papid>ie pattern acquisition (riloff, 1996; yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>stevenson and greenwood, 2005), <papid> P05-1047 </papid>document classification (surdeanu et al, 2006), <papid> W06-2207 </papid>fact extraction from the web (pasca et al, 2006) and hyponymy relation extraction (kozareva et al, 2008).(<papid> P08-1119 </papid>yarowsky, 1995) <papid> P95-1026 </papid>used bootstrapping to train decision list classifiers to disambiguate between two senses of word, achieving impressive classificationaccuracy.</nextsent>
<nextsent>(collins and singer, 1999) <papid> W99-0613 </papid>applied bootstrapping to extract rules for named entity (ne) classification, seeding the sytem with few handcrafted rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1846">
<title id=" W09-2207.xml">an analysis of bootstrapping for the recognition of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this task is more recent development, either alongside the traditional hand grammar approach to learn to distinguish specific difficult cases (mani and wilson, 2000), <papid> P00-1010 </papid>or on its own (hacioglu et al, 2005).</prevsent>
<prevsent>the latter apply svmsto the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an f1-measure of 87.8%.</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
1http://www.nist.gov/speech/tests/ace/ 49 bootstrapping techniques have been used for such diverse nlp problems as: word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>named entity classification (collins and singer, 1999), <papid> W99-0613 </papid>ie pattern acquisition (riloff, 1996; yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>stevenson and greenwood, 2005), <papid> P05-1047 </papid>document classification (surdeanu et al, 2006), <papid> W06-2207 </papid>fact extraction from the web (pasca et al, 2006) and hyponymy relation extraction (kozareva et al, 2008).(<papid> P08-1119 </papid>yarowsky, 1995) <papid> P95-1026 </papid>used bootstrapping to train decision list classifiers to disambiguate between two senses of word, achieving impressive classificationaccuracy.</citsent>
<aftsection>
<nextsent>(collins and singer, 1999) <papid> W99-0613 </papid>applied bootstrapping to extract rules for named entity (ne) classification, seeding the sytem with few handcrafted rules.</nextsent>
<nextsent>their main innovation was to split training in two alternate stages: during one step, only contextual rules are sought; during the second step, the new contextual rules are used to tag further nes and these are used to produce new spelling rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1847">
<title id=" W09-2207.xml">an analysis of bootstrapping for the recognition of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this task is more recent development, either alongside the traditional hand grammar approach to learn to distinguish specific difficult cases (mani and wilson, 2000), <papid> P00-1010 </papid>or on its own (hacioglu et al, 2005).</prevsent>
<prevsent>the latter apply svmsto the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an f1-measure of 87.8%.</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
1http://www.nist.gov/speech/tests/ace/ 49 bootstrapping techniques have been used for such diverse nlp problems as: word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>named entity classification (collins and singer, 1999), <papid> W99-0613 </papid>ie pattern acquisition (riloff, 1996; yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>stevenson and greenwood, 2005), <papid> P05-1047 </papid>document classification (surdeanu et al, 2006), <papid> W06-2207 </papid>fact extraction from the web (pasca et al, 2006) and hyponymy relation extraction (kozareva et al, 2008).(<papid> P08-1119 </papid>yarowsky, 1995) <papid> P95-1026 </papid>used bootstrapping to train decision list classifiers to disambiguate between two senses of word, achieving impressive classificationaccuracy.</citsent>
<aftsection>
<nextsent>(collins and singer, 1999) <papid> W99-0613 </papid>applied bootstrapping to extract rules for named entity (ne) classification, seeding the sytem with few handcrafted rules.</nextsent>
<nextsent>their main innovation was to split training in two alternate stages: during one step, only contextual rules are sought; during the second step, the new contextual rules are used to tag further nes and these are used to produce new spelling rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1848">
<title id=" W09-2207.xml">an analysis of bootstrapping for the recognition of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this task is more recent development, either alongside the traditional hand grammar approach to learn to distinguish specific difficult cases (mani and wilson, 2000), <papid> P00-1010 </papid>or on its own (hacioglu et al, 2005).</prevsent>
<prevsent>the latter apply svmsto the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an f1-measure of 87.8%.</prevsent>
</prevsection>
<citsent citstr=" C00-2136 ">
1http://www.nist.gov/speech/tests/ace/ 49 bootstrapping techniques have been used for such diverse nlp problems as: word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>named entity classification (collins and singer, 1999), <papid> W99-0613 </papid>ie pattern acquisition (riloff, 1996; yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>stevenson and greenwood, 2005), <papid> P05-1047 </papid>document classification (surdeanu et al, 2006), <papid> W06-2207 </papid>fact extraction from the web (pasca et al, 2006) and hyponymy relation extraction (kozareva et al, 2008).(<papid> P08-1119 </papid>yarowsky, 1995) <papid> P95-1026 </papid>used bootstrapping to train decision list classifiers to disambiguate between two senses of word, achieving impressive classificationaccuracy.</citsent>
<aftsection>
<nextsent>(collins and singer, 1999) <papid> W99-0613 </papid>applied bootstrapping to extract rules for named entity (ne) classification, seeding the sytem with few handcrafted rules.</nextsent>
<nextsent>their main innovation was to split training in two alternate stages: during one step, only contextual rules are sought; during the second step, the new contextual rules are used to tag further nes and these are used to produce new spelling rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1849">
<title id=" W09-2207.xml">an analysis of bootstrapping for the recognition of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this task is more recent development, either alongside the traditional hand grammar approach to learn to distinguish specific difficult cases (mani and wilson, 2000), <papid> P00-1010 </papid>or on its own (hacioglu et al, 2005).</prevsent>
<prevsent>the latter apply svmsto the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an f1-measure of 87.8%.</prevsent>
</prevsection>
<citsent citstr=" P03-1044 ">
1http://www.nist.gov/speech/tests/ace/ 49 bootstrapping techniques have been used for such diverse nlp problems as: word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>named entity classification (collins and singer, 1999), <papid> W99-0613 </papid>ie pattern acquisition (riloff, 1996; yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>stevenson and greenwood, 2005), <papid> P05-1047 </papid>document classification (surdeanu et al, 2006), <papid> W06-2207 </papid>fact extraction from the web (pasca et al, 2006) and hyponymy relation extraction (kozareva et al, 2008).(<papid> P08-1119 </papid>yarowsky, 1995) <papid> P95-1026 </papid>used bootstrapping to train decision list classifiers to disambiguate between two senses of word, achieving impressive classificationaccuracy.</citsent>
<aftsection>
<nextsent>(collins and singer, 1999) <papid> W99-0613 </papid>applied bootstrapping to extract rules for named entity (ne) classification, seeding the sytem with few handcrafted rules.</nextsent>
<nextsent>their main innovation was to split training in two alternate stages: during one step, only contextual rules are sought; during the second step, the new contextual rules are used to tag further nes and these are used to produce new spelling rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1850">
<title id=" W09-2207.xml">an analysis of bootstrapping for the recognition of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this task is more recent development, either alongside the traditional hand grammar approach to learn to distinguish specific difficult cases (mani and wilson, 2000), <papid> P00-1010 </papid>or on its own (hacioglu et al, 2005).</prevsent>
<prevsent>the latter apply svmsto the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an f1-measure of 87.8%.</prevsent>
</prevsection>
<citsent citstr=" P05-1047 ">
1http://www.nist.gov/speech/tests/ace/ 49 bootstrapping techniques have been used for such diverse nlp problems as: word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>named entity classification (collins and singer, 1999), <papid> W99-0613 </papid>ie pattern acquisition (riloff, 1996; yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>stevenson and greenwood, 2005), <papid> P05-1047 </papid>document classification (surdeanu et al, 2006), <papid> W06-2207 </papid>fact extraction from the web (pasca et al, 2006) and hyponymy relation extraction (kozareva et al, 2008).(<papid> P08-1119 </papid>yarowsky, 1995) <papid> P95-1026 </papid>used bootstrapping to train decision list classifiers to disambiguate between two senses of word, achieving impressive classificationaccuracy.</citsent>
<aftsection>
<nextsent>(collins and singer, 1999) <papid> W99-0613 </papid>applied bootstrapping to extract rules for named entity (ne) classification, seeding the sytem with few handcrafted rules.</nextsent>
<nextsent>their main innovation was to split training in two alternate stages: during one step, only contextual rules are sought; during the second step, the new contextual rules are used to tag further nes and these are used to produce new spelling rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1851">
<title id=" W09-2207.xml">an analysis of bootstrapping for the recognition of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this task is more recent development, either alongside the traditional hand grammar approach to learn to distinguish specific difficult cases (mani and wilson, 2000), <papid> P00-1010 </papid>or on its own (hacioglu et al, 2005).</prevsent>
<prevsent>the latter apply svmsto the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an f1-measure of 87.8%.</prevsent>
</prevsection>
<citsent citstr=" W06-2207 ">
1http://www.nist.gov/speech/tests/ace/ 49 bootstrapping techniques have been used for such diverse nlp problems as: word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>named entity classification (collins and singer, 1999), <papid> W99-0613 </papid>ie pattern acquisition (riloff, 1996; yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>stevenson and greenwood, 2005), <papid> P05-1047 </papid>document classification (surdeanu et al, 2006), <papid> W06-2207 </papid>fact extraction from the web (pasca et al, 2006) and hyponymy relation extraction (kozareva et al, 2008).(<papid> P08-1119 </papid>yarowsky, 1995) <papid> P95-1026 </papid>used bootstrapping to train decision list classifiers to disambiguate between two senses of word, achieving impressive classificationaccuracy.</citsent>
<aftsection>
<nextsent>(collins and singer, 1999) <papid> W99-0613 </papid>applied bootstrapping to extract rules for named entity (ne) classification, seeding the sytem with few handcrafted rules.</nextsent>
<nextsent>their main innovation was to split training in two alternate stages: during one step, only contextual rules are sought; during the second step, the new contextual rules are used to tag further nes and these are used to produce new spelling rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1852">
<title id=" W09-2207.xml">an analysis of bootstrapping for the recognition of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this task is more recent development, either alongside the traditional hand grammar approach to learn to distinguish specific difficult cases (mani and wilson, 2000), <papid> P00-1010 </papid>or on its own (hacioglu et al, 2005).</prevsent>
<prevsent>the latter apply svmsto the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an f1-measure of 87.8%.</prevsent>
</prevsection>
<citsent citstr=" P08-1119 ">
1http://www.nist.gov/speech/tests/ace/ 49 bootstrapping techniques have been used for such diverse nlp problems as: word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>named entity classification (collins and singer, 1999), <papid> W99-0613 </papid>ie pattern acquisition (riloff, 1996; yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>stevenson and greenwood, 2005), <papid> P05-1047 </papid>document classification (surdeanu et al, 2006), <papid> W06-2207 </papid>fact extraction from the web (pasca et al, 2006) and hyponymy relation extraction (kozareva et al, 2008).(<papid> P08-1119 </papid>yarowsky, 1995) <papid> P95-1026 </papid>used bootstrapping to train decision list classifiers to disambiguate between two senses of word, achieving impressive classificationaccuracy.</citsent>
<aftsection>
<nextsent>(collins and singer, 1999) <papid> W99-0613 </papid>applied bootstrapping to extract rules for named entity (ne) classification, seeding the sytem with few handcrafted rules.</nextsent>
<nextsent>their main innovation was to split training in two alternate stages: during one step, only contextual rules are sought; during the second step, the new contextual rules are used to tag further nes and these are used to produce new spelling rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1860">
<title id=" W09-2207.xml">an analysis of bootstrapping for the recognition of temporal expressions </title>
<section> lemma pes: match tokens with given.  </section>
<citcontext>
<prevsection>
<prevsent>ous, but the lemma of the headword may be any in given semantic similarity class.
</prevsent>
<prevsent>the semantic similarity class of word is define das the word itself plus group of other semantically similar words.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
for computing these, we employ lins corpus of pairwise distributional similarities among words (nouns, verbs and adjectives) (lin, 1998), <papid> P98-2127 </papid>filtered to include only those words whose similarity value is above both an absolute (highest n) and relative (to the highest similarity value in theclass) threshold.</citsent>
<aftsection>
<nextsent>even after filtering, lins similarities can be noisy?, since the corpus has been constructed relying on purely statistical means.
</nextsent>
<nextsent>therefore, we are employing in addition set of manually defined semantic classes (hardcoded lists) sensitive to our domain of temporal expressions, such that these lists override?
</nextsent>
<nextsent>the lins similarity corpus whenever the semantic class of word present in them is involved.
</nextsent>
<nextsent>the manually defined semantic classes include: the written form of cardinals; ordi nals; days of the week (plus today, tomorrow and yesterday); months of the year; date trigger words(e.g. day, week); time trigger words (e.g. hour, sec ond); frequency adverbs (e.g. hourly, monthly); date adjectives (e.g. two- day, @@-week-long); and time adjectives (e.g. three-hour, @@-minute-long).we use dynamic window for the amount of context that is encoded into pattern, that is, we generate all the possible patterns with the same infix, and anything between 0 and the specified length of the context window pes in the prefix and the postfix, and let the selection step decide which variations get accepted into the next iteration.the modifiers field in the pattern representation has been devised as an extension mechanism.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1863">
<title id=" W09-2502.xml">a proposal on evaluation measures for rte </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>average precision does not properly reflect this symmetry.
</prevsent>
<prevsent>in this paper, we will first summarize relevant aspects of the current methodology, and outline these three problems in greater depth.
</prevsent>
</prevsection>
<citsent citstr=" W07-1401 ">
1 see the reports on rte-1 (dagan et al , 2005), rte-2 (bar-haim et al , 2006), rte-3 (giampiccolo et al , 2007), <papid> W07-1401 </papid>the rte-3 pilot (voorhees, 2008), <papid> P08-1008 </papid>rte-4 (giampicolo et al , 2008), and rte-5 (tac, 2009) the problem of bias is quite general and widely known.</citsent>
<aftsection>
<nextsent>artstein and poesio (2005) discuss it in the context of cohens kappa (cohen, 1960), which is one way of addressing the problem.
</nextsent>
<nextsent>yet, it has not received sufficient attention in the rte community, which is why we will show how it applies torte, in particular, and why it is an especially pressing concern for rte.
</nextsent>
<nextsent>average precision has been imported into therte evaluation methodology from ir, tacitly assuming great level of analogy between ir and rte.
</nextsent>
<nextsent>however, we will argue that the analogy is flawed, and that average precision is not suitable for rte evaluation.then, we will then reframe the problem in information theoretic terms, advocating mutual information as new evaluation measure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1864">
<title id=" W09-2502.xml">a proposal on evaluation measures for rte </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>average precision does not properly reflect this symmetry.
</prevsent>
<prevsent>in this paper, we will first summarize relevant aspects of the current methodology, and outline these three problems in greater depth.
</prevsent>
</prevsection>
<citsent citstr=" P08-1008 ">
1 see the reports on rte-1 (dagan et al , 2005), rte-2 (bar-haim et al , 2006), rte-3 (giampiccolo et al , 2007), <papid> W07-1401 </papid>the rte-3 pilot (voorhees, 2008), <papid> P08-1008 </papid>rte-4 (giampicolo et al , 2008), and rte-5 (tac, 2009) the problem of bias is quite general and widely known.</citsent>
<aftsection>
<nextsent>artstein and poesio (2005) discuss it in the context of cohens kappa (cohen, 1960), which is one way of addressing the problem.
</nextsent>
<nextsent>yet, it has not received sufficient attention in the rte community, which is why we will show how it applies torte, in particular, and why it is an especially pressing concern for rte.
</nextsent>
<nextsent>average precision has been imported into therte evaluation methodology from ir, tacitly assuming great level of analogy between ir and rte.
</nextsent>
<nextsent>however, we will argue that the analogy is flawed, and that average precision is not suitable for rte evaluation.then, we will then reframe the problem in information theoretic terms, advocating mutual information as new evaluation measure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1866">
<title id=" W09-2502.xml">a proposal on evaluation measures for rte </title>
<section> the other trivial systems have a.  </section>
<citcontext>
<prevsection>
<prevsent>3.
</prevsent>
<prevsent>on statistical grounds, one can account for.
</prevsent>
</prevsection>
<citsent citstr=" J04-1005 ">
the possibility of random agreement in the presence of bias using cohens kappa (artstein and poesio, 2005; di eugenio and glass, 2004).<papid> J04-1005 </papid></citsent>
<aftsection>
<nextsent>we will outline mutual information as an alternative, arguing that it has additional advantages.
</nextsent>
<nextsent>4 average precision.
</nextsent>
<nextsent>the purpose of average precision is to evaluate against the gold standard labelling the system assigned ranking ?, rather than directly comparing the two label lings and l. this is done by deriving from the ranking ? series of binary labellings.
</nextsent>
<nextsent>the i-th labelling in that series is that which labels all instances up to rank as 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1867">
<title id=" W09-2805.xml">unsupervised induction of sentence compression rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first one is based on template translation learning, method inherited from the machine translation field, which learns lexical transformation rules 2 , by observing set of 1500 (sentence, sentence reduced ) pair, selected from newsagency and manually tuned to obtain the training data.
</prevsent>
<prevsent>due to complexity difficulties found for the application of this big lexical ruleset, they proposed an improvement where stochastic hidden markov model is trained to help in the decision of which sequence of possible lexical reduction rules should be applied to specific case.
</prevsent>
</prevsection>
<citsent citstr=" P05-1036 ">
an unsupervised approach was included in thework of (turner &amp; charniak, 2005), <papid> P05-1036 </papid>where training data are automatically extracted from the penn treebank corpus, to fit noisy channel model, similar to the one used by (knight &amp; marcu, 2002).</citsent>
<aftsection>
<nextsent>although it seems an interesting approach to provide new training instances, it still be dependent upon data manually labeled.
</nextsent>
<nextsent>more recently, the work of (clarke &amp; lapata,2006) <papid> P06-2019 </papid>devise different and quite curious approach, where the sentence compression task is defined as an optimization goal, from an integer programming problem.</nextsent>
<nextsent>several constraints are defined, according to language models, linguistic,and syntactical features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1868">
<title id=" W09-2805.xml">unsupervised induction of sentence compression rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an unsupervised approach was included in thework of (turner &amp; charniak, 2005), <papid> P05-1036 </papid>where training data are automatically extracted from the penn treebank corpus, to fit noisy channel model, similar to the one used by (knight &amp; marcu, 2002).</prevsent>
<prevsent>although it seems an interesting approach to provide new training instances, it still be dependent upon data manually labeled.</prevsent>
</prevsection>
<citsent citstr=" P06-2019 ">
more recently, the work of (clarke &amp; lapata,2006) <papid> P06-2019 </papid>devise different and quite curious approach, where the sentence compression task is defined as an optimization goal, from an integer programming problem.</citsent>
<aftsection>
<nextsent>several constraints are defined, according to language models, linguistic,and syntactical features.
</nextsent>
<nextsent>although this is an unsupervised approach, without using any paralel corpus, it is completely knowledge driven, like set of crafted rules and heuristics incorporated into system to solve certain problem.
</nextsent>
<nextsent>1.2 our proposal.
</nextsent>
<nextsent>in this paper, we propose new approach to this research field, which follows an unsupervised methodology to learn sentence compression rules 2 those rules are named there as template-reduction rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1869">
<title id=" W09-2805.xml">unsupervised induction of sentence compression rules </title>
<section> data preparation.  </section>
<citcontext>
<prevsection>
<prevsent>our system collects web news stories on daily basis, and organized them into clusters, which are exclusively related to different and unique events, happening each day: company acqui sition?, presidential speech?, bomb attack?, etc. usually, such clusters contain near 30 smallor medium news articles, collected from different media sources.
</prevsent>
<prevsent>this environment proves to be very fruitful for paraphrase extraction, since wehave many sentences conveying similar information yet written in different form.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
a few unsupervised metrics have been applied to automatic paraphrase identification and extraction (barzilay &amp; lee, 2003; <papid> N03-1003 </papid>dolan et al, 2004).<papid> C04-1051 </papid></citsent>
<aftsection>
<nextsent>however, these unsupervised methodologies show major drawback by extracting quasi-exact or even exact match pairs of sentences as they relyon classical string similarity measures such as the edit distance in the case of (dolan et al, 2004) <papid> C04-1051 </papid>and word n-gram overlap for (barzilay &amp; lee, 2003).<papid> N03-1003 </papid></nextsent>
<nextsent>such pairs are useless for our purpose, since we aim to identify asymmetrical paraphrase pairs to be used for sentence compression rule induction, as explained in (cordeiro et al, oct 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1870">
<title id=" W09-2805.xml">unsupervised induction of sentence compression rules </title>
<section> data preparation.  </section>
<citcontext>
<prevsection>
<prevsent>our system collects web news stories on daily basis, and organized them into clusters, which are exclusively related to different and unique events, happening each day: company acqui sition?, presidential speech?, bomb attack?, etc. usually, such clusters contain near 30 smallor medium news articles, collected from different media sources.
</prevsent>
<prevsent>this environment proves to be very fruitful for paraphrase extraction, since wehave many sentences conveying similar information yet written in different form.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
a few unsupervised metrics have been applied to automatic paraphrase identification and extraction (barzilay &amp; lee, 2003; <papid> N03-1003 </papid>dolan et al, 2004).<papid> C04-1051 </papid></citsent>
<aftsection>
<nextsent>however, these unsupervised methodologies show major drawback by extracting quasi-exact or even exact match pairs of sentences as they relyon classical string similarity measures such as the edit distance in the case of (dolan et al, 2004) <papid> C04-1051 </papid>and word n-gram overlap for (barzilay &amp; lee, 2003).<papid> N03-1003 </papid></nextsent>
<nextsent>such pairs are useless for our purpose, since we aim to identify asymmetrical paraphrase pairs to be used for sentence compression rule induction, as explained in (cordeiro et al, oct 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZF1874">
<title id=" W09-2805.xml">unsupervised induction of sentence compression rules </title>
<section> data preparation.  </section>
<citcontext>
<prevsection>
<prevsent>from corpus of asymmetrical paraphrases, we then use biology-based gene alignment algorithms to align the words contained in each of the two sentences within each paraphrase.
</prevsent>
<prevsent>for that purpose, we implemented two well established algorithms, one identifying local alignments (smith &amp; waterman, 1981) and the other one computing global alignments (needleman &amp; wunsch, 1970).
</prevsent>
</prevsection>
<citsent citstr=" W07-1429 ">
we also proposed convenient dynamic strategy (cordeiro et al, 2007), <papid> W07-1429 </papid>which chooses the best alignment algorithm to be applied to specific case at runtime.the difference between local and global sequence alignments is illustrated below, where we use letters, instead of words, to better fit our paper space constraints.</citsent>
<aftsection>
<nextsent>suppose that we have the following two sequences: [d,h,m,s,t,p,r,q,i,s] and [t,p,q,i,s,d,h,s] global alignment would produce the following pair.
</nextsent>
<nextsent>d m t r i _ _ _ _ _ _ _ p _ i d s for the same two sequences, local alignment strategy could generate two or more aligned sub sequences as follows.
</nextsent>
<nextsent>17 |d m s| |t r i s| |d _ s| |t _ i s| hence, at this stage of the process, we end with corpus of aligned 7 asymmetrical paraphrases.
</nextsent>
<nextsent>in figure 4, we present the alignment of the paraphrase of figure 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>