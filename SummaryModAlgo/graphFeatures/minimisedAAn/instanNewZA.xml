<paper>
<cited id="ZA0">
<title id=" W04-1101.xml">segmentation of chinese long sentences using commas </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>chinese is language with less morphology and no case marker.
</prevsent>
<prevsent>in chinese, subordinate clause or coordinate clause is sometimes connected without any conjunctions in sentence.
</prevsent>
</prevsection>
<citsent citstr=" P03-1056 ">
because of these characteristics, chinese has rather different set of salient ambiguities from the perspective of statistical parsing (levy and manning, 2003).<papid> P03-1056 </papid></citsent>
<aftsection>
<nextsent>in addition, the work for clause segmentation is also rather different compared with other languages.
</nextsent>
<nextsent>however, in written chinese, the comma is used more frequently (lin, 2000).
</nextsent>
<nextsent>in english, the average use of comma per sentence is 0.869 (jones, 1996<papid> P96-1052 </papid>a)1 ~1.04(hill, 1996), and in chinese it is 1.792, which is one and half to two more times as it is used in english.</nextsent>
<nextsent>in korean, the comma is used even less than it is in english (lin, 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1">
<title id=" W04-1101.xml">segmentation of chinese long sentences using commas </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, the work for clause segmentation is also rather different compared with other languages.
</prevsent>
<prevsent>however, in written chinese, the comma is used more frequently (lin, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P96-1052 ">
in english, the average use of comma per sentence is 0.869 (jones, 1996<papid> P96-1052 </papid>a)1 ~1.04(hill, 1996), and in chinese it is 1.792, which is one and half to two more times as it is used in english.</citsent>
<aftsection>
<nextsent>in korean, the comma is used even less than it is in english (lin, 2000).
</nextsent>
<nextsent>since chinese has less morphology and no case marker, and the comma is frequently used, the comma becomes an important cue for long chinese sentence parsing.
</nextsent>
<nextsent>because more commas may appear in longer sentences, the necessity of analyzing the comma also increases.
</nextsent>
<nextsent>some handbooks about standard chinese grammars list ten to twenty uses of the comma, according to the context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2">
<title id=" W04-1101.xml">segmentation of chinese long sentences using commas </title>
<section> the calculation is based on peoples daily corpus98..  </section>
<citcontext>
<prevsection>
<prevsent>long sentence segmentation is way to avoid the problem.
</prevsent>
<prevsent>many studies have been made on clause segmentation (carreras and marquez, 2002, leffa, 1998, sang and dejean,2001).
</prevsent>
</prevsection>
<citsent citstr=" J97-2002 ">
in addition, many studies also have been done on long sentences segmentation by certain patterns (kim and zhang, 2001, li and pei, 1990, palmer and hearst, 1997).<papid> J97-2002 </papid></citsent>
<aftsection>
<nextsent>however, some researchers merely ignore punctuation, including the comma, and some researchers use comma as one feature to detect the segmentation point, not fully using the information from the comma.
</nextsent>
<nextsent>2.2 related work for punctuation proc-.
</nextsent>
<nextsent>essing several researchers have provided descriptive treatment of the role of punctuations: jones (1996<papid> P96-1052 </papid>b) determined the syntactic function of the punctuation mark.</nextsent>
<nextsent>bayraktar and akman (1998) classified commas by means of the syntax-patterns in which they occur.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA4">
<title id=" W04-1101.xml">segmentation of chinese long sentences using commas </title>
<section> the calculation is based on peoples daily corpus98..  </section>
<citcontext>
<prevsection>
<prevsent>however, theoretical forays into the syntactic roles of punctuation were limited.
</prevsent>
<prevsent>many researchers have used the punctuation mark for syntactic analysis and insist that punctuation indicates useful information.
</prevsent>
</prevsection>
<citsent citstr=" C94-1069 ">
jones (1994) <papid> C94-1069 </papid>successfully shows that grammar with punctuation outperforms one without punctuation.</citsent>
<aftsection>
<nextsent>briscoe and carroll 1995) also show the importance of punctuation in reducing syntactic ambiguity.
</nextsent>
<nextsent>collins (1999), in his statistical parser, treats comma as an important feature.
</nextsent>
<nextsent>shiuan and ann (1996) separate complex sentences with respect to the link word, including the comma.
</nextsent>
<nextsent>as result, their syntactic parser performs an error reduction of 21.2% in its accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA5">
<title id=" W04-2010.xml">robust ending guessing rules with application to slavonic languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>pos guessing.
</prevsent>
<prevsent>kupiec (1992) uses pre-specified suffixes and performs statistical learning for pos guessing.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
the xerox tagger comes with list of built-in ending guessing rules (cutting et al,1992).<papid> A92-1018 </papid></citsent>
<aftsection>
<nextsent>in addition to the ending, weischedel et al (1993) <papid> J93-2006 </papid>exploit capitalisation.</nextsent>
<nextsent>thede and harper (1997) <papid> W97-0124 </papid>consider contextual information, word endings, entropy and open-class smoothing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA6">
<title id=" W04-2010.xml">robust ending guessing rules with application to slavonic languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kupiec (1992) uses pre-specified suffixes and performs statistical learning for pos guessing.
</prevsent>
<prevsent>the xerox tagger comes with list of built-in ending guessing rules (cutting et al,1992).<papid> A92-1018 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
in addition to the ending, weischedel et al (1993) <papid> J93-2006 </papid>exploit capitalisation.</citsent>
<aftsection>
<nextsent>thede and harper (1997) <papid> W97-0124 </papid>consider contextual information, word endings, entropy and open-class smoothing.</nextsent>
<nextsent>a similar approach is presented in (schmid,1995).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA7">
<title id=" W04-2010.xml">robust ending guessing rules with application to slavonic languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the xerox tagger comes with list of built-in ending guessing rules (cutting et al,1992).<papid> A92-1018 </papid></prevsent>
<prevsent>in addition to the ending, weischedel et al (1993) <papid> J93-2006 </papid>exploit capitalisation.</prevsent>
</prevsection>
<citsent citstr=" W97-0124 ">
thede and harper (1997) <papid> W97-0124 </papid>consider contextual information, word endings, entropy and open-class smoothing.</citsent>
<aftsection>
<nextsent>a similar approach is presented in (schmid,1995).
</nextsent>
<nextsent>ruch et al (2000) <papid> W00-0722 </papid>combine pos guessing, contextual rules and markov models to build pos tagger for biomedical text.</nextsent>
<nextsent>a very influential is the work of brill (1997), who induces more linguistically motivated rules exploiting both tagged corpus and lexi con.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA8">
<title id=" W04-2010.xml">robust ending guessing rules with application to slavonic languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>thede and harper (1997) <papid> W97-0124 </papid>consider contextual information, word endings, entropy and open-class smoothing.</prevsent>
<prevsent>a similar approach is presented in (schmid,1995).</prevsent>
</prevsection>
<citsent citstr=" W00-0722 ">
ruch et al (2000) <papid> W00-0722 </papid>combine pos guessing, contextual rules and markov models to build pos tagger for biomedical text.</citsent>
<aftsection>
<nextsent>a very influential is the work of brill (1997), who induces more linguistically motivated rules exploiting both tagged corpus and lexicon.
</nextsent>
<nextsent>he does not look at the affixes only, but also checks their pos class in lexicon.
</nextsent>
<nextsent>mikheev (1997) <papid> J97-3003 </papid>proposes similar approach, but learns the rules from raw as opposed to tagged text.</nextsent>
<nextsent>daciuk (1999) speeds up the process by means of finite state transducers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA9">
<title id=" W04-2010.xml">robust ending guessing rules with application to slavonic languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a very influential is the work of brill (1997), who induces more linguistically motivated rules exploiting both tagged corpus and lexicon.
</prevsent>
<prevsent>he does not look at the affixes only, but also checks their pos class in lexicon.
</prevsent>
</prevsection>
<citsent citstr=" J97-3003 ">
mikheev (1997) <papid> J97-3003 </papid>proposes similar approach, but learns the rules from raw as opposed to tagged text.</citsent>
<aftsection>
<nextsent>daciuk (1999) speeds up the process by means of finite state transducers.
</nextsent>
<nextsent>general morphology.
</nextsent>
<nextsent>nakov et al (2003) use ending guessing rules to predict the morphological class of unknown german nouns.
</nextsent>
<nextsent>schone and jurafsky (2000) <papid> W00-0712 </papid>apply latent semantic analysis for knowledge-free morphology induction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA11">
<title id=" W04-2010.xml">robust ending guessing rules with application to slavonic languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>general morphology.
</prevsent>
<prevsent>nakov et al (2003) use ending guessing rules to predict the morphological class of unknown german nouns.
</prevsent>
</prevsection>
<citsent citstr=" W00-0712 ">
schone and jurafsky (2000) <papid> W00-0712 </papid>apply latent semantic analysis for knowledge-free morphology induction.</citsent>
<aftsection>
<nextsent>dejean (1998), hafer and weiss (1974) follow successor variety approach: the word is cut, if the number of distinct letters after pre-specified sequence surpasses threshold.
</nextsent>
<nextsent>goldsmith (2001) performs minimum description length analysis of the morphology of several european languages using corpora.
</nextsent>
<nextsent>gaussier (1999) <papid> W99-0904 </papid>induces derivational morphology from lexicon by means of p-similarity based splitting.</nextsent>
<nextsent>jacquemin (1997) focuses on the morphological processes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA12">
<title id=" W04-2010.xml">robust ending guessing rules with application to slavonic languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>dejean (1998), hafer and weiss (1974) follow successor variety approach: the word is cut, if the number of distinct letters after pre-specified sequence surpasses threshold.
</prevsent>
<prevsent>goldsmith (2001) performs minimum description length analysis of the morphology of several european languages using corpora.
</prevsent>
</prevsection>
<citsent citstr=" W99-0904 ">
gaussier (1999) <papid> W99-0904 </papid>induces derivational morphology from lexicon by means of p-similarity based splitting.</citsent>
<aftsection>
<nextsent>jacquemin (1997) focuses on the morphological processes.
</nextsent>
<nextsent>vanden bosch and daelemans (1999) propose memory-based approach, which maps directly from letters in context to categories that encode morphological boundaries, syntactic class labels and spelling changes.
</nextsent>
<nextsent>yarowsky and wicentowski (2000) <papid> P00-1027 </papid>present corpus-ba sed approach for morphological analysis of both regular and irregular forms based on four models including: relative corpus frequency, context similarity, weighted string similarity and incremental retraining of inflectional transduction probabilities.</nextsent>
<nextsent>another interesting work, exploiting capitalisation and fixed/variable suffixes, is presented in cucerzan and yarowsky (2000).<papid> P00-1035 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA13">
<title id=" W04-2010.xml">robust ending guessing rules with application to slavonic languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>jacquemin (1997) focuses on the morphological processes.
</prevsent>
<prevsent>vanden bosch and daelemans (1999) propose memory-based approach, which maps directly from letters in context to categories that encode morphological boundaries, syntactic class labels and spelling changes.
</prevsent>
</prevsection>
<citsent citstr=" P00-1027 ">
yarowsky and wicentowski (2000) <papid> P00-1027 </papid>present corpus-ba sed approach for morphological analysis of both regular and irregular forms based on four models including: relative corpus frequency, context similarity, weighted string similarity and incremental retraining of inflectional transduction probabilities.</citsent>
<aftsection>
<nextsent>another interesting work, exploiting capitalisation and fixed/variable suffixes, is presented in cucerzan and yarowsky (2000).<papid> P00-1035 </papid></nextsent>
<nextsent>as the related work above shows, large lexical database is often needed for the automatic identification of good diagnostic word endings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA14">
<title id=" W04-2010.xml">robust ending guessing rules with application to slavonic languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>vanden bosch and daelemans (1999) propose memory-based approach, which maps directly from letters in context to categories that encode morphological boundaries, syntactic class labels and spelling changes.
</prevsent>
<prevsent>yarowsky and wicentowski (2000) <papid> P00-1027 </papid>present corpus-ba sed approach for morphological analysis of both regular and irregular forms based on four models including: relative corpus frequency, context similarity, weighted string similarity and incremental retraining of inflectional transduction probabilities.</prevsent>
</prevsection>
<citsent citstr=" P00-1035 ">
another interesting work, exploiting capitalisation and fixed/variable suffixes, is presented in cucerzan and yarowsky (2000).<papid> P00-1035 </papid></citsent>
<aftsection>
<nextsent>as the related work above shows, large lexical database is often needed for the automatic identification of good diagnostic word endings.
</nextsent>
<nextsent>in particular, in our experiments we used the large grammatical dictionary of bulgarian (paskaleva,2003), created at the linguistic modelling department of the bulgarian academy of sciences (clpp-bas) and comprising approximately 995,000 word forms (about 65,000 lemmas), encoded in delaf format (silberztein,1993).
</nextsent>
<nextsent>the following information is listed for each wordform: 1) lemma; 2) lemma properties (pos, additional grammatical features related to the word formation: gender, e.g. for the no uns; degree, e.g. for the adjectives; transit ivity, for verbs; kind for pronouns/numerals, etc.); and 3) properties of the word form as member of the lemma paradigm.
</nextsent>
<nextsent>the first group of properties represent our primary learning resource, as we focus on the extraction of ending rules for whole word classes and not for individual words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA18">
<title id=" W04-2010.xml">robust ending guessing rules with application to slavonic languages </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>we did not address the problem of selecting the best threshold (although it is clear that it should be low, maybe around 0.50).
</prevsent>
<prevsent>one way to do this is to split the training set into rules training and threshold-training sets.
</prevsent>
</prevsection>
<citsent citstr=" N04-1016 ">
next, it looks promising to try to estimate the dictionary word frequencies using search engine instead of text corpus, as proposed by lapata and keller (2004).<papid> N04-1016 </papid></citsent>
<aftsection>
<nextsent>while the exact algorithm performed worse due to insufficient coverage10, we believe it has potential, e.g. if extended with some approximate rules.
</nextsent>
<nextsent>note that the way the exact rules were built is very similar to the standard algorithm for decision tree construction.
</nextsent>
<nextsent>thus the corresponding tree cutting criteria used to prevent over fitting can help decide when to go further and look at longer endings and when to stop.
</nextsent>
<nextsent>it is interesting to see how the proposed rules perform for other slavonic languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA19">
<title id=" W04-0209.xml">exploiting semantic information for manual anaphoric annotation in cast3lb corpus </title>
<section> semantic annotation.  </section>
<citcontext>
<prevsection>
<prevsent>this corpus is formed by portion of the brown corpus and the novel the red badge of courage.
</prevsent>
<prevsent>altogether, it is formed by approximately 250.000 words, where nouns, verbs,adjectives and adverbs have been manually annotated with wordnet senses (miller, 1990).
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
another corpus with wordnet-based semantic annotation isthe dso corpus (ng and lee, 1996).<papid> P96-1006 </papid></citsent>
<aftsection>
<nextsent>in this corpus, the most frequent english ambiguous nouns and verbs had been annotated with the correct sense (121 nouns and 70 verbs).
</nextsent>
<nextsent>the corpus is formed by 192.800 sentences from the brown corpus and the wall street journal, and it has also been manuallyannotated.
</nextsent>
<nextsent>finally, the senseval forum has developed few sense annotated corpora for the evaluation of word sense disambiguation systems (kil garriff and palmer, 2000), some of which also use wordnet as lexical resource.we have decided to use spanish wordnet for several reasons.
</nextsent>
<nextsent>first of all, spanish wordnet is, up to now, the more commonly used lexical resource in word sense disambiguation tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA20">
<title id=" W04-0410.xml">frozen sentences of portuguese formal descriptions for nlp </title>
<section> application to texts: some experiences.  </section>
<citcontext>
<prevsection>
<prevsent>in order to evaluate the performance of the electronic dictionary on real texts, experiences were made using intex6 on three different texts.
</prevsent>
<prevsent>two smaller texts, one obtained from the on-line edition of the expresso weekly newspaper 7 and the other composite text 8 obtained from several sources and used on the morpholimpics evalua 6 we also used an electronic dictionary of simple.
</prevsent>
</prevsection>
<citsent citstr=" W99-0511 ">
words of portuguese (ranchhod et al 1999), <papid> W99-0511 </papid>from the public linguistic resources built by label: http://label.ist.utl.pt.</citsent>
<aftsection>
<nextsent>7 http://www.expresso.pt/.
</nextsent>
<nextsent>this is 976 kb text, with 83,269 (5,764 different) words.
</nextsent>
<nextsent>8 http://acdc.linguateca.pt/aval_conjunta/morfolimpia das/ts_ml.txt [29-03-2004] this is 215 kb text, with 35,053 (10,070 different) words.
</nextsent>
<nextsent>tion campaign for portuguese 9.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA21">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much attention has recently been given to calculating the similarity of word senses, in support of various natural language learning and processing tasks.
</prevsent>
<prevsent>such techniques apply within semantic hierarchy, or ontology,such as wordnet.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
typical methods comprise an edge distance measurement over the two sense nodes being compared within the hierarchy (leacock and chodorow, 1998; rada et al, 1989; wu and palmer, 1994)<papid> P94-1019 </papid></citsent>
<aftsection>
<nextsent>other approaches instead assume probability distribution over the entire sense hierarchy; similarity is captured between individual senses by formula over the information content (negative log probabilities) of relevant nodes (e.g., jiang and conrath, 1997; lin, 1998).
</nextsent>
<nextsent>the latter case assumes that there is single wordnet probability distribution of interest, which is estimated by populating the hierarchy with word frequencies from an appropriate corpus (e.g, jiang and conrath, 1997).
</nextsent>
<nextsent>butsome problems more naturally give rise to multiple conditional probability distributions estimated from counts that are conditioned on various contexts, such as different corpora or differing word usage within single corpus.
</nextsent>
<nextsent>each of these contexts would yield distinct wordnet probability distribution, or what we will call sense profile.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA23">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate our method on problem that arise sin lexical acquisition, of determining whether two different argument positions across syntactic usages of verb are assigned the same semantic role.
</prevsent>
<prevsent>for example, even though the truck shows up in two different syntactic positions, it is the destination of the action in both of the sentences loaded the truck with hay and loaded hay onto the truck.
</prevsent>
</prevsection>
<citsent citstr=" C00-2148 ">
automatic detection of such argument alternations is important to acquisition of verb lexical semantics (dang et al, 2000; <papid> C00-2148 </papid>dorr and jones, 2000; merlo and stevenson, 2001; <papid> J01-3003 </papid>schulte im walde and brew, 2002;tsang et al, 2002), <papid> C02-1146 </papid>and moreover, may play role in automatic processing of language for applied tasks, such asquestion-answering (katz et al, 2001), <papid> W01-1009 </papid>information extraction (riloff and schmelzenbach, 1998), <papid> W98-1106 </papid>detection oftext relations (teufel, 1999), and determination of verb particle constructions (bannard, 2002).</citsent>
<aftsection>
<nextsent>we focus on this problem to illustrate how our general method works, and how it aids in particular natural language learning task.
</nextsent>
<nextsent>as in mccarthy (2000), <papid> A00-2034 </papid>we cast argument alternation detection as comparison of sense profiles across two different argument positions of verb.</nextsent>
<nextsent>our method differs, however, in two important respects.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA24">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate our method on problem that arise sin lexical acquisition, of determining whether two different argument positions across syntactic usages of verb are assigned the same semantic role.
</prevsent>
<prevsent>for example, even though the truck shows up in two different syntactic positions, it is the destination of the action in both of the sentences loaded the truck with hay and loaded hay onto the truck.
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
automatic detection of such argument alternations is important to acquisition of verb lexical semantics (dang et al, 2000; <papid> C00-2148 </papid>dorr and jones, 2000; merlo and stevenson, 2001; <papid> J01-3003 </papid>schulte im walde and brew, 2002;tsang et al, 2002), <papid> C02-1146 </papid>and moreover, may play role in automatic processing of language for applied tasks, such asquestion-answering (katz et al, 2001), <papid> W01-1009 </papid>information extraction (riloff and schmelzenbach, 1998), <papid> W98-1106 </papid>detection oftext relations (teufel, 1999), and determination of verb particle constructions (bannard, 2002).</citsent>
<aftsection>
<nextsent>we focus on this problem to illustrate how our general method works, and how it aids in particular natural language learning task.
</nextsent>
<nextsent>as in mccarthy (2000), <papid> A00-2034 </papid>we cast argument alternation detection as comparison of sense profiles across two different argument positions of verb.</nextsent>
<nextsent>our method differs, however, in two important respects.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA25">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate our method on problem that arise sin lexical acquisition, of determining whether two different argument positions across syntactic usages of verb are assigned the same semantic role.
</prevsent>
<prevsent>for example, even though the truck shows up in two different syntactic positions, it is the destination of the action in both of the sentences loaded the truck with hay and loaded hay onto the truck.
</prevsent>
</prevsection>
<citsent citstr=" C02-1146 ">
automatic detection of such argument alternations is important to acquisition of verb lexical semantics (dang et al, 2000; <papid> C00-2148 </papid>dorr and jones, 2000; merlo and stevenson, 2001; <papid> J01-3003 </papid>schulte im walde and brew, 2002;tsang et al, 2002), <papid> C02-1146 </papid>and moreover, may play role in automatic processing of language for applied tasks, such asquestion-answering (katz et al, 2001), <papid> W01-1009 </papid>information extraction (riloff and schmelzenbach, 1998), <papid> W98-1106 </papid>detection oftext relations (teufel, 1999), and determination of verb particle constructions (bannard, 2002).</citsent>
<aftsection>
<nextsent>we focus on this problem to illustrate how our general method works, and how it aids in particular natural language learning task.
</nextsent>
<nextsent>as in mccarthy (2000), <papid> A00-2034 </papid>we cast argument alternation detection as comparison of sense profiles across two different argument positions of verb.</nextsent>
<nextsent>our method differs, however, in two important respects.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA26">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate our method on problem that arise sin lexical acquisition, of determining whether two different argument positions across syntactic usages of verb are assigned the same semantic role.
</prevsent>
<prevsent>for example, even though the truck shows up in two different syntactic positions, it is the destination of the action in both of the sentences loaded the truck with hay and loaded hay onto the truck.
</prevsent>
</prevsection>
<citsent citstr=" W01-1009 ">
automatic detection of such argument alternations is important to acquisition of verb lexical semantics (dang et al, 2000; <papid> C00-2148 </papid>dorr and jones, 2000; merlo and stevenson, 2001; <papid> J01-3003 </papid>schulte im walde and brew, 2002;tsang et al, 2002), <papid> C02-1146 </papid>and moreover, may play role in automatic processing of language for applied tasks, such asquestion-answering (katz et al, 2001), <papid> W01-1009 </papid>information extraction (riloff and schmelzenbach, 1998), <papid> W98-1106 </papid>detection oftext relations (teufel, 1999), and determination of verb particle constructions (bannard, 2002).</citsent>
<aftsection>
<nextsent>we focus on this problem to illustrate how our general method works, and how it aids in particular natural language learning task.
</nextsent>
<nextsent>as in mccarthy (2000), <papid> A00-2034 </papid>we cast argument alternation detection as comparison of sense profiles across two different argument positions of verb.</nextsent>
<nextsent>our method differs, however, in two important respects.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA27">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate our method on problem that arise sin lexical acquisition, of determining whether two different argument positions across syntactic usages of verb are assigned the same semantic role.
</prevsent>
<prevsent>for example, even though the truck shows up in two different syntactic positions, it is the destination of the action in both of the sentences loaded the truck with hay and loaded hay onto the truck.
</prevsent>
</prevsection>
<citsent citstr=" W98-1106 ">
automatic detection of such argument alternations is important to acquisition of verb lexical semantics (dang et al, 2000; <papid> C00-2148 </papid>dorr and jones, 2000; merlo and stevenson, 2001; <papid> J01-3003 </papid>schulte im walde and brew, 2002;tsang et al, 2002), <papid> C02-1146 </papid>and moreover, may play role in automatic processing of language for applied tasks, such asquestion-answering (katz et al, 2001), <papid> W01-1009 </papid>information extraction (riloff and schmelzenbach, 1998), <papid> W98-1106 </papid>detection oftext relations (teufel, 1999), and determination of verb particle constructions (bannard, 2002).</citsent>
<aftsection>
<nextsent>we focus on this problem to illustrate how our general method works, and how it aids in particular natural language learning task.
</nextsent>
<nextsent>as in mccarthy (2000), <papid> A00-2034 </papid>we cast argument alternation detection as comparison of sense profiles across two different argument positions of verb.</nextsent>
<nextsent>our method differs, however, in two important respects.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA28">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatic detection of such argument alternations is important to acquisition of verb lexical semantics (dang et al, 2000; <papid> C00-2148 </papid>dorr and jones, 2000; merlo and stevenson, 2001; <papid> J01-3003 </papid>schulte im walde and brew, 2002;tsang et al, 2002), <papid> C02-1146 </papid>and moreover, may play role in automatic processing of language for applied tasks, such asquestion-answering (katz et al, 2001), <papid> W01-1009 </papid>information extraction (riloff and schmelzenbach, 1998), <papid> W98-1106 </papid>detection oftext relations (teufel, 1999), and determination of verb particle constructions (bannard, 2002).</prevsent>
<prevsent>we focus on this problem to illustrate how our general method works, and how it aids in particular natural language learning task.</prevsent>
</prevsection>
<citsent citstr=" A00-2034 ">
as in mccarthy (2000), <papid> A00-2034 </papid>we cast argument alternation detection as comparison of sense profiles across two different argument positions of verb.</citsent>
<aftsection>
<nextsent>our method differs, however, in two important respects.
</nextsent>
<nextsent>first, our measure can be used on any probability distribution, while mccarthys approach applies only to very narrow form of sense profile known as tree cut.1 the dependence on tree cuts greatly limits the applicability of her measure in both this and other problems, since only particular method can be used for populating the wordnet hierarchy with probability estimates.
</nextsent>
<nextsent>second, our approach provides much finer-grained measure of the distance between the two profiles.
</nextsent>
<nextsent>mccarthys method rewards probability mass that occurs in the same subtree across two distributions, but does not take into account the distance between the classes that carry the probability mass.our new spd method integrates comparison of probability distributions over wordnet with node distancemeasure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA33">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>spd thus enables us to calculate more detailed comparison over the probability patterns of wordnet classes.
</prevsent>
<prevsent>as our results indicate, this has advantages for argument alternation detection, but more importantly, we think it is crucial for generalizing the method to wider range of problems.
</prevsent>
</prevsection>
<citsent citstr=" J98-2002 ">
1a tree cut for tree is set of nodes in such that every leaf node of has exactly one member of on path between it and the root (li and abe, 1998).<papid> J98-2002 </papid></citsent>
<aftsection>
<nextsent>as sense profile, tree cut will have non-zero probability associated with every node in c, and zero probability for all other nodes in t. figure 1 in section 3 has examples of two tree cuts.
</nextsent>
<nextsent>in the next section, we present background work on comparing sense profiles, and on using them to detect alternations.
</nextsent>
<nextsent>in section 3, we describe our new spd measure, and show how it captures both the general differences between wordnet probability distributions, as well as the fine-grained semantic distances between the nodes that comprise them.
</nextsent>
<nextsent>section 4 presents our corpus methodology and experimental set-up.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA41">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in its strength of selection for an object.
</prevsent>
<prevsent>a drawback of this approach for generalizing to other sense profile comparisons is the assumption in relative entropy of an asymmetry between the two probability distributions.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
similarly, mccarthy (2000) <papid> A00-2034 </papid>uses skew divergence (avariant of kl divergence proposed by lee, 1999) <papid> P99-1004 </papid>to compare the sense profile of one argument of verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transi tive), to determine if the verb participates in an argument alternation involving the two positions.</citsent>
<aftsection>
<nextsent>for example, the causative alternation in sentences (1) and (2) illustrate show the subject of the in transitive is the same underlying semantic argument (i.e., the theme the argument undergoing the action) as the object of the transitive: (1) the snow melted.
</nextsent>
<nextsent>(2) the sun melted the snow.
</nextsent>
<nextsent>because we demonstrate our new spd measure on thesame problem as mccarthy (2000), <papid> A00-2034 </papid>we provide more detail of her method here, for comparison.</nextsent>
<nextsent>the first step is to create the sense profiles for the relevant verb/slotpairs (e.g., the in transitive subject of melt, and the transitive object of melt, if determining whether melt under goes the causative alternation, as illustrated above).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA63">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> materials and methods.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 corpus data.
</prevsent>
<prevsent>our materials are drawn from 35m-word portion of the british national corpus (bnc).
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
the text is parsed using the rasp parser (briscoe and carroll, 2002), and sub categorizations are extracted using the system of briscoe and carroll (1997).<papid> A97-1052 </papid></citsent>
<aftsection>
<nextsent>the subcategorization frame entry ofeach verb includes the frequency count and list of argument heads per slot.
</nextsent>
<nextsent>the target slots in this work are the subject of the in transitive and the object of the transitive.
</nextsent>
<nextsent>4.2 verb selection.
</nextsent>
<nextsent>we evaluate our method on the causative alternation in order for comparison to the earlier method of mccarthy (2000).<papid> A00-2034 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA69">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> materials and methods.  </section>
<citcontext>
<prevsection>
<prevsent>for both our development and test sets, we chose filler verbs randomly, as long as the verb classes they belong to do not allow subject/objectalternation as in the causative.
</prevsent>
<prevsent>verbs must occur minimum of 10 times per syntactic slot to be chosen.
</prevsent>
</prevsection>
<citsent citstr=" W04-2605 ">
note that we did not hand-verify that individual verbs allowed or disallowed the alternation, as mccarthy (2000) <papid> A00-2034 </papid>had done, because we wanted to evaluate our method in the presence of noise of this kind.in pilot experiment on smaller, domain-specific corpus (6m words, medical domain) (tsang and stevenson, 2004), <papid> W04-2605 </papid>we randomly picked 18 causative verbs and 18 filler verbs for development and 20 causative verbs and 20 filler verbs for testing.</citsent>
<aftsection>
<nextsent>in this pilot experiment, spd is consistently the best performer in both development andtesting.
</nextsent>
<nextsent>spd achieves best accuracy of 69% in development and 65% in testing (chance accuracy of 50%).given more data (35m words) in our current experiments, we randomly select additional verbs to make up total of 60 causative verbs and 60 filler verbs, half of these for development and half for testing.
</nextsent>
<nextsent>each set of verbs is further divided into high frequency band (withat least 450 instances of one target slot), medium frequency band (with between 150 and 400 instances in one target slot), and low frequency band (with between 20 and 100 instances of one target slot).
</nextsent>
<nextsent>each band has 20 verbs (10 causative and 10 non-causative).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA71">
<title id=" W04-2411.xml">calculating semantic distance between word sense probability distributions </title>
<section> materials and methods.  </section>
<citcontext>
<prevsection>
<prevsent>note that in the bnc, these verbs are not evenly distributed across the bands, hence we can only experiment with the mixed frequencies condition.
</prevsent>
<prevsent>4.3 experimental set-up.
</prevsent>
</prevsection>
<citsent citstr=" J02-2003 ">
using (verb,slot,noun) tuples from the corpus, we experimented with several ways of building sense profiles of each verbs target argument slots (resnik, 1993; li andabe, 1998; <papid> J98-2002 </papid>clark and weir, 2002).<papid> J02-2003 </papid>5 in both our pilot experiment and current development work, we found thatthe method of clark and weir (2002)<papid> J02-2003 </papid>overall gave better performance, and so we limit our discussion here to the results on their model.</citsent>
<aftsection>
<nextsent>briefly, clark and weir (2002)<papid> J02-2003 </papid>populate the wordnet hierarchy based on corpus frequencies (of all nouns for verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using  to determine whether to generalize an estimate to parent node in the hierarchy.</nextsent>
<nextsent>we compare spd to other measures applied directly to the (unpropagated) probability profiles given by the clark-weir method: the probability distribution distance given by skew divergence (skew) (lee, 1999), <papid> P99-1004 </papid>as well as the general vector distance given by cosine (cos).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA98">
<title id=" W04-2325.xml">causes and strategies for requesting clarification in dialogue </title>
<section> clarification requests.  </section>
<citcontext>
<prevsection>
<prevsent>note that confirmation request can be realised as an alternative question, as in (5-b-ii) and (4) b, or as y/n-question, as in (1-c).
</prevsent>
<prevsent>lastly, we also distinguish between crs that point out problematic element in the original utterance, and those that dont. the former category is illustrated by the crs in (1) and (5-b), the latter by those in (5-a) and (5-c).
</prevsent>
</prevsection>
<citsent citstr=" W01-1616 ">
we call this dimension extent.3 2(purver et al, 2001) <papid> W01-1616 </papid>investigates such mapping, based on the classification discussed below in section 2.2.3the dimensions for classification introduced here are related to, but different in some aspects from those used in (lars son, 2003).</citsent>
<aftsection>
<nextsent>our term cr?
</nextsent>
<nextsent>covers what larsson calls negative feedback as well as what he calls checking feedback, whereas before we finally come to the description of the dimension level of understanding in the next section, we will briefly look at an earlier analysis of cr that does not make these distinctions.
</nextsent>
<nextsent>2.2 previous analyses.
</nextsent>
<nextsent>in number of papers (ginzburg and cooper, 2001; <papid> P01-1031 </papid>purver et al, 2001), <papid> W01-1616 </papid>jonathan ginzburg and colleagues have developed an influential analysis of cr.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA99">
<title id=" W04-2325.xml">causes and strategies for requesting clarification in dialogue </title>
<section> clarification requests.  </section>
<citcontext>
<prevsection>
<prevsent>covers what larsson calls negative feedback as well as what he calls checking feedback, whereas before we finally come to the description of the dimension level of understanding in the next section, we will briefly look at an earlier analysis of cr that does not make these distinctions.
</prevsent>
<prevsent>2.2 previous analyses.
</prevsent>
</prevsection>
<citsent citstr=" P01-1031 ">
in number of papers (ginzburg and cooper, 2001; <papid> P01-1031 </papid>purver et al, 2001), <papid> W01-1616 </papid>jonathan ginzburg and colleagues have developed an influential analysis of cr.</citsent>
<aftsection>
<nextsent>the authors define two readings that can be ascribed to crs, which they name the constituent reading and the clausal read ing.4 (8-b) shows paraphrases of these readings for the cr in (8-a).5 (8) a. a: did bo leave?
</nextsent>
<nextsent>b: bo?
</nextsent>
<nextsent>b. clausal: are you asking whether bo left?
</nextsent>
<nextsent>constituent: whos bo?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA103">
<title id=" W04-0808.xml">an evaluation exercise for romanian word sense disambiguation </title>
<section> open mind word expert.  </section>
<citcontext>
<prevsection>
<prevsent>five teams with total of seven systems have tackled this task.
</prevsent>
<prevsent>we present in this paper the data used and how it was obtained, and the performance of the participating systems.
</prevsent>
</prevsection>
<citsent citstr=" W02-0817 ">
the sense annotated corpus required for this task was built using the open mind word expert system(chklovski and mihalcea, 2002), <papid> W02-0817 </papid>adapted to roma nian1.</citsent>
<aftsection>
<nextsent>to overcome the current lack of sense tagged data and the limitations imposed by the creation of such data using trained lexicographers, the open mind word expert system enables the collection of semantically annotated corpora over the web.sense tagged examples are collected using web based application that allows contributors to annotate words with their meanings.
</nextsent>
<nextsent>the tagging exercise proceeds as follows.
</nextsent>
<nextsent>foreach target word the system extracts set of sentences from large textual corpus.
</nextsent>
<nextsent>these examples are presented to the contributors, who are asked to select the most appropriate sense for the target wordin each sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA104">
<title id=" W04-0808.xml">an evaluation exercise for romanian word sense disambiguation </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the corpus was tokenized and part-of-speechtagged using racais tools (tufis, 1999).
</prevsent>
<prevsent>the to kenizer recognizes and adequately segments various constructs: clitics, dates, abbreviations, multiword expressions, proper nouns, etc. the tagging followed the tiered tagging approach with the hidden layer of tagging being taken care of by thorsten brants?
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
tnt (brants, 2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>the upper level of the tiered tagger removed from the assigned tags all the attributes irrelevant for this wsd exercise.
</nextsent>
<nextsent>the estimated accuracy of the part-of-speech tagging is around 98%.
</nextsent>
<nextsent>while several sense annotation schemes have been previously proposed, including single or dual annotations, or the tag until two agree?
</nextsent>
<nextsent>scheme used during senseval-2, we decided to use new scheme and collect four tags per item, which allowed us to conduct and compare inter-annotator agreement evaluations for two-, three-, and four-way agreement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA105">
<title id=" W04-0842.xml">wsd system based on specialized hidden markov model upvshmmeaw </title>
<section> description of the wsd system.  </section>
<citcontext>
<prevsection>
<prevsent>this technique has been successfully applied to other disambiguation tasks such as part-of-speech tagging (pla and molina, 2004) and shallow parsing (molina and pla, 2002).
</prevsent>
<prevsent>other hmm-based approaches have also been applied to wsd.
</prevsent>
</prevsection>
<citsent citstr=" W97-0811 ">
in (segond et al, 1997), <papid> W97-0811 </papid>they estimated bigram model of ambiguity classes from the semcor corpus for the task of disambiguating the semantic categories corresponding to the lexico gra pher level.</citsent>
<aftsection>
<nextsent>these semantic categories are codified into the lex sense field.
</nextsent>
<nextsent>a second-order hmm was used in (loupy et al, 1998) in two-step strategy.first, they determined the semantic category associated to word.
</nextsent>
<nextsent>then, they assigned the most probable sense according to the word and the semantic category.
</nextsent>
<nextsent>a shmm consists of changing the topology of the hmm in order to get more accurate model association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems which includes more information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA106">
<title id=" W04-0311.xml">dynamic dependency parsing </title>
<section> basic dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>in particular, variable in stantiations evaluated formerly might now be judged differently.
</prevsent>
<prevsent>as this could entail serious computational problems we try to keep changes in the cost function monotonic, that is re-evaluation shall only give lower penalties than before, i.e. instantiations that become inconsistent once dont get consistent later on again.
</prevsent>
</prevsection>
<citsent citstr=" P90-1005 ">
using constraint satisfaction techniques for natural language parsing was introduced first in(maruyama, 1990) <papid> P90-1005 </papid>by defining constraint dependency grammar (cdg) that maps nicely on the notion of csp.</citsent>
<aftsection>
<nextsent>a cdg is quadruple (?, r, l, c), where ? is lexicon of known words, is set of roles of word.
</nextsent>
<nextsent>a role represents level of language like syn?
</nextsent>
<nextsent>or sem?.
</nextsent>
<nextsent>l is set of labels for each role (e.g. {subj?, obj?}, {agent?,patient?}), and is constraint grammar consisting of atomic logical formulas.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA107">
<title id=" W04-0311.xml">dynamic dependency parsing </title>
<section> weighted dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>the second constraint subj-dist is soft one, such as every edge with label subj attached more than two words away induces penalty calculated by the term 2.9 / x.length.
</prevsent>
<prevsent>note, that the maximal edge length in subj-dist is quite arbitrary and should be extracted from corpus automatically as well as the grade of increasing penal ization.
</prevsent>
</prevsection>
<citsent citstr=" E03-1052 ">
a realistic grammar consists of about500 such handwritten constraints like the currently developed grammar for german (daum et al, 2003).<papid> E03-1052 </papid>the notation used for constraints in this paper is expressing valid formulas interpret able by the wcdg constraint system.</citsent>
<aftsection>
<nextsent>the following definitions explain some of the primitives that are part of the constraint language: ? is variable for dependency edge of the form ei,j = r, wi, l, wj?, ref sem syn semantic constraints syntax constraints reference constraints syntax semantic constraints syntax?
</nextsent>
<nextsent>reference constraints semantic?
</nextsent>
<nextsent>reference constraints lexicon chunker tagger ontology figure 1: architecture of wcdg ? x@word (x^word) refers to the word form wi ? ?
</nextsent>
<nextsent>(wj ? ?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA108">
<title id=" W04-1207.xml">event based information extraction for the biomedical domain the caderige project </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the extraction patterns are thus easier to acquire or learn, more abstract and easier to maintain beyond extraction patterns, it is also possible to acquire from the corpus, via ml methods, part of the knowledge necessary for text normalization as shown here.
</prevsent>
<prevsent>this paper gives an overview of current research activities and achievements of the caderige project.
</prevsent>
</prevsection>
<citsent citstr=" E99-1043 ">
the paper first presents our approach and compares it with the one developed in the framework of similar project called genia (collier et al 1999).<papid> E99-1043 </papid></citsent>
<aftsection>
<nextsent>we then propose an account of caderige techniques on various filtering and normalization tasks, namely, sentence filtering, resolution of named entity synonymy, syntactic parsing, and ontology learning.
</nextsent>
<nextsent>finally, we show how extraction patterns can be learned from normalized and annotated documents, all applied to biological texts.
</nextsent>
<nextsent>in this section, we give some details about the motivations and choices of implementation.
</nextsent>
<nextsent>we then briefly compare our approach with the one of the genia project.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA110">
<title id=" W04-1612.xml">automatic diacritization of arabic for acoustic modeling in speech recognition </title>
<section> motivation and prior work.  </section>
<citcontext>
<prevsection>
<prevsent>it was found that the diacritization error rate (percentage of missing and wrongly identified or inserted diacritics) on msa ranged between 9% and 28%, depending on whether or not case vowel endings were counted.
</prevsent>
<prevsent>however,on the eca text, the diacritization software obtained an error rate of 48%.
</prevsent>
</prevsection>
<citsent citstr=" W02-0504 ">
a fully automatic approach to diacritizationwas presented in (gal, 2002), <papid> W02-0504 </papid>where an hmm based bigram model was used for decodingdiacritized sentences from non-diacritized sen tences.</citsent>
<aftsection>
<nextsent>the technique was applied to the quranand achieved 14% word error (incorrectly dia crit ized words).
</nextsent>
<nextsent>a first attempt at developing an automatic diacritizer for dialectal speech was reported in (kirchhoff et al, 2002).
</nextsent>
<nextsent>the basic approach was to use small set of parallel script and dia crit ized data (obtained from the eca call home corpus) and to derive diacritization rules in anexample-based way.
</nextsent>
<nextsent>this entirely knowledge free approach achieved 16.6% word error rate.other studies (el-imam, 2003) have addressed problems of grapheme-to-phoneme conversion in arabic, e.g. for the purpose of speech synthesis, but have assumed that fully dia crit ized version of the text is already available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA111">
<title id=" W04-1010.xml">template filtered headline summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a lossy summarizer that translates?
</prevsent>
<prevsent>news stories into target summaries using the ibm-style?
</prevsent>
</prevsection>
<citsent citstr=" P00-1041 ">
statistical machine translation (mt) model was shown in (banko, et al, 2000).<papid> P00-1041 </papid></citsent>
<aftsection>
<nextsent>conditional probabilities for limited vocabulary and bigram transition probabilities as headline syntax approximation were incorporated into the translation model.
</nextsent>
<nextsent>it was shown to have worked surprisingly well with stand-alone evaluation of quantitative analysis on content coverage.
</nextsent>
<nextsent>the use of noisy-channel model and viterbi search was shown in another mt-inspired headline summarization system (zajic, et al, 2002).
</nextsent>
<nextsent>the method was automatically evaluated by bilingual evaluation understudy (bleu) (papineni, et al, 2001) and scored 0.1886 with its limited length model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA112">
<title id=" W04-1010.xml">template filtered headline summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the use of noisy-channel model and viterbi search was shown in another mt-inspired headline summarization system (zajic, et al, 2002).
</prevsent>
<prevsent>the method was automatically evaluated by bilingual evaluation understudy (bleu) (papineni, et al, 2001) and scored 0.1886 with its limited length model.
</prevsent>
</prevsection>
<citsent citstr=" W03-0501 ">
a non statistical system, coupled with linguistically motivated heuristics, using parse-and-trim approach based on parse trees was reported in (dorr, et al, 2003).<papid> W03-0501 </papid></citsent>
<aftsection>
<nextsent>it achieved 0.1341 on bleu with an average of 8.5 words.
</nextsent>
<nextsent>even though human evaluations were conducted in the past, we still do not have sufficient material to perform comprehensive comparative evaluation on large enough scale to claim that one method is superior to others.
</nextsent>
<nextsent>it is difficult to formulate rule set that defines how headlines are written.
</nextsent>
<nextsent>however, we may discover how headlines are related to the templates derived from them using training set of 60933 (headline, text) pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA114">
<title id=" W04-1805.xml">discovering specific semantic relationships between nouns and verbs in a specialized french corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>results, measured with large test set, show that our acquisition technique outperforms classical statistical methods used for collocation acquisition.
</prevsent>
<prevsent>moreover, the inferred patterns yield interesting clues on which structures are more likely to convey the target semantic link.
</prevsent>
</prevsection>
<citsent citstr=" W02-1403 ">
recent literature in computational terminology has shown an increasing interest in identifying various semantic relationships between terms.different strategies have been developed in order to identify pairs of terms that share specific semantic relationship (such as hyperonymy or meronymy) or to build classes of terms.however, most strategies are based on in ternal or external methods (grabar and zweigenbaum, 2002), <papid> W02-1403 </papid>i.e. methods that relyon the form of terms or on the information gathered from contexts.</citsent>
<aftsection>
<nextsent>(in some cases, an additional resource, such as dictionary or thesaurus, is used during the identification process.)
</nextsent>
<nextsent>thework reported here infers specific semantic relationships based on sets of examples and counter examples.
</nextsent>
<nextsent>in this paper, the method is applied to french corpus on computing to find noun-verb combinations in which verbs convey meaning of realization.
</nextsent>
<nextsent>the work is carried out in order to assist terminographers in the enrichment of adictionary on computing that includes collocational information (l homme, 2004).even though this work is carried out for ter mino graphical and lexico graphical purposes, it can certainly be of use in other applications, namely information retrieval.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA115">
<title id=" W04-1805.xml">discovering specific semantic relationships between nouns and verbs in a specialized french corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>other semantic relationships could be sought in the future.
</prevsent>
<prevsent>a number of applications have relied on distributional analysis (harris, 1971) in order to build classes of semantically related terms.
</prevsent>
</prevsection>
<citsent citstr=" C96-1083 ">
this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (habert et al, 1996, <papid> C96-1083 </papid>forex ample), does not specify the relationship itself.</citsent>
<aftsection>
<nextsent>hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.
</nextsent>
<nextsent>more recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships betweenterms: daille (2003) uses derivational morphol ogy; grabar and zweigenbaum (2002) <papid> W02-1403 </papid>use, as starting point, number of identical characters.</nextsent>
<nextsent>up to now, the focus has been on nouns and adjectives, since these structuring methods have been applied to lists of extracted candidate terms (habert et al, 1996; <papid> C96-1083 </papid>daille, 2003) or tolists of admitted terms (grabar and zweigenbaum, 2002).<papid> W02-1403 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA119">
<title id=" W04-1805.xml">discovering specific semantic relationships between nouns and verbs in a specialized french corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as consequence, relationships considered have been mostly synonymic or taxonomic, or defined as term variations.on the other hand, other work has been carried out in order to acquire collocations.
</prevsent>
<prevsent>most ofthese endeavours have focused on purely statistical acquisition techniques (church and hanks, 1 however, our interpretation of lfs in this work is much looser, since we admitted verbs that would not be considered to be members of true collocations as mel uk et al (1984 1999) define them, i.e. groups of lexical units that share restricted cooccurrence relationship.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
1990), on linguisitic acquisition (by the use of part-of-speech filters hand-crafted by linguist)(oueslati, 1999) or, more frequently, on combination of the two (smadja, 1993; <papid> J93-1007 </papid>kilgarriff and tugwell, 2001, for example).</citsent>
<aftsection>
<nextsent>it is worth noting that although these techniques are able to identify n-v pairs, they do not specify the relationship between and v, nor are they capable of focusing on subset of n-v pairs.
</nextsent>
<nextsent>the original acquisition methodology we present in the next section will allow us to overcome this limitation.
</nextsent>
<nextsent>noun-verb pairs this section is devoted to the description of the methodology and the data we use to acquire semantically related noun-verb pairs.
</nextsent>
<nextsent>we first describe the specialized corpus used in this experiment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA120">
<title id=" W04-1805.xml">discovering specific semantic relationships between nouns and verbs in a specialized french corpus </title>
<section> methodology for finding valid.  </section>
<citcontext>
<prevsection>
<prevsent>set, since no information concerning verbs that are not semantically related is available in the terminological database.
</prevsent>
<prevsent>to obtain list of invalid n-v pairs, we acquire them from our corpus using statistical technique.this produces list of all n-v pairs that appear in the same sentence, and assigns each ascore.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
many statistical coefficients exist (man ning and schtze, 1999); most of them can be easily expressed with the help of contingency table similar to that reproduced in table 1 and by noting = + + + d. for example, the vj vk, 6= ni b nl, 6= c table 1: contingency table for the pair ni-vj mutual information coefficient is defined as: mi = log2 a(a+ b)(a+ c) and the loglike coefficient (dunning, 1993) <papid> J93-1003 </papid>as: log = log + log + log + log ?</citsent>
<aftsection>
<nextsent>(a + computerm 2004 - 3rd international workshop on computational terminology 41 b) log(a+ b)?
</nextsent>
<nextsent>(a+ c) log(a+ c)?
</nextsent>
<nextsent>(b+ d) log(b+ d)?
</nextsent>
<nextsent>(c+ d) log(c+ d) + log s. in the work presented here, we have adopted the loglike coefficient.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA121">
<title id=" W04-0850.xml">the duluth lexical sample systems in senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the unsupervised system uses measures of semantic relatedness to find the sense of the target word that is most related to the senses of its neighbors.
</prevsent>
<prevsent>it participated in the english lexical sample task.
</prevsent>
</prevsection>
<citsent citstr=" N01-1011 ">
the duluth systems participated in various lexical sample tasks in senseval-3, using both supervised and unsupervised methodologies.the supervised lexical sample system that participated in senseval-3 is the duluth3 (english) orduluth8 (spanish) system as used in senseval 2 (pedersen, 2001<papid> N01-1011 </papid>b).</citsent>
<aftsection>
<nextsent>it has been renamed forsenseval-3 as duluth-xlss, where is one letter abbreviation of the language to which it is being applied, and lss stands for lexical sample supervised.
</nextsent>
<nextsent>the idea behind this system is to learn three bagged decision trees, one using unigram features, another using bigram features, and third using cooccurrences with the target word as features.
</nextsent>
<nextsent>this system only uses surface lexical features, so it can be easily applied to wide rangeof languages.
</nextsent>
<nextsent>for senseval-3 this system participated in the english, spanish, basque, catalan, romanian, and multilingual (english-hindi) tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA123">
<title id=" W04-0850.xml">the duluth lexical sample systems in senseval3 </title>
<section> lexical sample supervised.  </section>
<citcontext>
<prevsection>
<prevsent>in other words, an informative feature set will result inaccurate disambiguation when used with wide range of learning algorithms, but there is no learning algorithm that can perform well given an uninformative or misleading set of features.
</prevsent>
<prevsent>therefore, our interest in these experiments is more in the effect of the different features sets than in the variations that would be possible if we used learning algorithms other than decision trees.we are satisfied that decision trees are reason able choice of learning algorithm.
</prevsent>
</prevsection>
<citsent citstr=" W96-0208 ">
they have long history of use in word sense disambiguation, dating back to early work by (black, 1988), and have fared well in comparative studies such as (mooney,1996) <papid> W96-0208 </papid>and (pedersen and bruce, 1997).</citsent>
<aftsection>
<nextsent>in the former they were used with unigram features and in the latter they were used with small set of features that included the part-of-speech of neighboring words,three collocations, and the morphology of the ambiguous word.
</nextsent>
<nextsent>in (pedersen, 2001<papid> N01-1011 </papid>a) we introduced the use of decision trees based strictly on bigram features.</nextsent>
<nextsent>while we might squeeze out few extra points of performance by using more complicated methods, we believe that this would obscure our ability to study and understand the effects of different kinds of features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA126">
<title id=" W04-0850.xml">the duluth lexical sample systems in senseval3 </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>there were 178 instances where the wordnet sense found by our system was not mapped to wordsmyth.
</prevsent>
<prevsent>rather than attempt to create our own mapping of wordnet to wordsmyth,we simply threw these instances out of the evaluation set, which does lead to somewhat less coverage for the unsupervised system for the verbs.
</prevsent>
</prevsection>
<citsent citstr=" A00-2009 ">
the duluth-xlss system was originally inspired by (pedersen, 2000), <papid> A00-2009 </papid>which presents an ensemble of eighty-one naive bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features.</citsent>
<aftsection>
<nextsent>however, the duluth-elss system only uses three member ensemble to explore the efficacy of combinations of different lexical features via simple ensembles.
</nextsent>
<nextsent>we plan to carry out more detailed analysis of the degree to which unigram, bigram, and cooccurrence features are useful sources of information for disambiguation.we will also conduct an analysis of the complementary and redundant nature of lexical and syntactic features, as we have done in (mohammad and pedersen, 2004<papid> W04-2404 </papid>a) for the senseval-1, senseval2, and line, hard, serve, and interest data.</nextsent>
<nextsent>the syn talex system (mohammad and pedersen, 2004<papid> W04-2404 </papid>b) also participated in the english lexical sample taskof senseval3 and is sister system to duluth elss.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA127">
<title id=" W04-0850.xml">the duluth lexical sample systems in senseval3 </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>the duluth-xlss system was originally inspired by (pedersen, 2000), <papid> A00-2009 </papid>which presents an ensemble of eighty-one naive bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features.</prevsent>
<prevsent>however, the duluth-elss system only uses three member ensemble to explore the efficacy of combinations of different lexical features via simple ensembles.</prevsent>
</prevsection>
<citsent citstr=" W04-2404 ">
we plan to carry out more detailed analysis of the degree to which unigram, bigram, and cooccurrence features are useful sources of information for disambiguation.we will also conduct an analysis of the complementary and redundant nature of lexical and syntactic features, as we have done in (mohammad and pedersen, 2004<papid> W04-2404 </papid>a) for the senseval-1, senseval2, and line, hard, serve, and interest data.</citsent>
<aftsection>
<nextsent>the syn talex system (mohammad and pedersen, 2004<papid> W04-2404 </papid>b) also participated in the english lexical sample taskof senseval3 and is sister system to duluth elss.</nextsent>
<nextsent>it uses lexical and syntactic features with bagged decision trees and serves as convenient point of comparison.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA133">
<title id=" W04-0106.xml">induction of a simple morphology for highlyinflecting languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>existing morphology learning algorithms are commonly based on the item and arrangement model, i.e., words are formed by concatenation of morphemes, which are the smallest meaning bearing units in language.
</prevsent>
<prevsent>the methods segment words, and the resulting segments are supposed to be close to linguistic morphemes.
</prevsent>
</prevsection>
<citsent citstr=" P03-1036 ">
in addition to producing segmentation of words the aim is of ten to discover structure, such as knowledge of which word forms belong to the same inflectional paradigm.typically, generative models are used, either formulated in bayesian framework, e.g., (brent,1999; creutz, 2003); <papid> P03-1036 </papid>or applying the minimum description length (mdl) principle, e.g., (de marcken, 1996; goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2002).<papid> W02-0603 </papid></citsent>
<aftsection>
<nextsent>there is another approach, inspired by the works of zellig harris, where morpheme boundary is suggested at locations where the predictability of the next letter in letter sequence is low, cf.
</nextsent>
<nextsent>e.g., (dejean, 1998).<papid> W98-1239 </papid></nextsent>
<nextsent>as it is necessary to learn both which segments are plausible morphemes and what sequences of morphemes are possible, the learning task is al barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the huumori taju ttom uute nne humor of sense -less -ness your figure 1: morpheme segmentation of the finnish word huumorintajuttomuutenne?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA134">
<title id=" W04-0106.xml">induction of a simple morphology for highlyinflecting languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>existing morphology learning algorithms are commonly based on the item and arrangement model, i.e., words are formed by concatenation of morphemes, which are the smallest meaning bearing units in language.
</prevsent>
<prevsent>the methods segment words, and the resulting segments are supposed to be close to linguistic morphemes.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
in addition to producing segmentation of words the aim is of ten to discover structure, such as knowledge of which word forms belong to the same inflectional paradigm.typically, generative models are used, either formulated in bayesian framework, e.g., (brent,1999; creutz, 2003); <papid> P03-1036 </papid>or applying the minimum description length (mdl) principle, e.g., (de marcken, 1996; goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2002).<papid> W02-0603 </papid></citsent>
<aftsection>
<nextsent>there is another approach, inspired by the works of zellig harris, where morpheme boundary is suggested at locations where the predictability of the next letter in letter sequence is low, cf.
</nextsent>
<nextsent>e.g., (dejean, 1998).<papid> W98-1239 </papid></nextsent>
<nextsent>as it is necessary to learn both which segments are plausible morphemes and what sequences of morphemes are possible, the learning task is al barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the huumori taju ttom uute nne humor of sense -less -ness your figure 1: morpheme segmentation of the finnish word huumorintajuttomuutenne?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA135">
<title id=" W04-0106.xml">induction of a simple morphology for highlyinflecting languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>existing morphology learning algorithms are commonly based on the item and arrangement model, i.e., words are formed by concatenation of morphemes, which are the smallest meaning bearing units in language.
</prevsent>
<prevsent>the methods segment words, and the resulting segments are supposed to be close to linguistic morphemes.
</prevsent>
</prevsection>
<citsent citstr=" W02-0603 ">
in addition to producing segmentation of words the aim is of ten to discover structure, such as knowledge of which word forms belong to the same inflectional paradigm.typically, generative models are used, either formulated in bayesian framework, e.g., (brent,1999; creutz, 2003); <papid> P03-1036 </papid>or applying the minimum description length (mdl) principle, e.g., (de marcken, 1996; goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2002).<papid> W02-0603 </papid></citsent>
<aftsection>
<nextsent>there is another approach, inspired by the works of zellig harris, where morpheme boundary is suggested at locations where the predictability of the next letter in letter sequence is low, cf.
</nextsent>
<nextsent>e.g., (dejean, 1998).<papid> W98-1239 </papid></nextsent>
<nextsent>as it is necessary to learn both which segments are plausible morphemes and what sequences of morphemes are possible, the learning task is al barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the huumori taju ttom uute nne humor of sense -less -ness your figure 1: morpheme segmentation of the finnish word huumorintajuttomuutenne?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA136">
<title id=" W04-0106.xml">induction of a simple morphology for highlyinflecting languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition to producing segmentation of words the aim is of ten to discover structure, such as knowledge of which word forms belong to the same inflectional paradigm.typically, generative models are used, either formulated in bayesian framework, e.g., (brent,1999; creutz, 2003); <papid> P03-1036 </papid>or applying the minimum description length (mdl) principle, e.g., (de marcken, 1996; goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2002).<papid> W02-0603 </papid></prevsent>
<prevsent>there is another approach, inspired by the works of zellig harris, where morpheme boundary is suggested at locations where the predictability of the next letter in letter sequence is low, cf.</prevsent>
</prevsection>
<citsent citstr=" W98-1239 ">
e.g., (dejean, 1998).<papid> W98-1239 </papid></citsent>
<aftsection>
<nextsent>as it is necessary to learn both which segments are plausible morphemes and what sequences of morphemes are possible, the learning task is al barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the huumori taju ttom uute nne humor of sense -less -ness your figure 1: morpheme segmentation of the finnish word huumorintajuttomuutenne?
</nextsent>
<nextsent>(your lack of sense of humor?).
</nextsent>
<nextsent>levi ated by making simplifying assumptions about word structure.
</nextsent>
<nextsent>often words are assumed to consist of one stem followed by one, possibly empty, suffix as in, e.g., (dejean, 1998; <papid> W98-1239 </papid>snover and brent,2001).<papid> P01-1063 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA138">
<title id=" W04-0106.xml">induction of a simple morphology for highlyinflecting languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(your lack of sense of humor?).
</prevsent>
<prevsent>levi ated by making simplifying assumptions about word structure.
</prevsent>
</prevsection>
<citsent citstr=" P01-1063 ">
often words are assumed to consist of one stem followed by one, possibly empty, suffix as in, e.g., (dejean, 1998; <papid> W98-1239 </papid>snover and brent,2001).<papid> P01-1063 </papid></citsent>
<aftsection>
<nextsent>in (goldsmith, 2001) <papid> J01-2001 </papid>recursive structure is proposed, such that stems can consist of sub-stem and suffix.</nextsent>
<nextsent>also prefixes are possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA143">
<title id=" W04-0106.xml">induction of a simple morphology for highlyinflecting languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1 for an example).
</prevsent>
<prevsent>these resemble algorithms that segment text without blanks (or transcribed speech) into words, e.g., (de marcken, 1996; brent, 1999), in that they do not distinguish between stems and affixes, but split words into socalled morphs, which carry no explicit category in formation.some algorithms do not relyon the item and arrangement (ia) model, but learn relationships between words by comparing the orthographic similarity of pairs of words.
</prevsent>
</prevsection>
<citsent citstr=" W02-0604 ">
in (neuvel and fulop, 2002), <papid> W02-0604 </papid>morphological learner based on the theory of whole word morphology is outlined.</citsent>
<aftsection>
<nextsent>full words are related to other full words, and complex word forms are analyzed into variable and non-variablecomponent.
</nextsent>
<nextsent>conceivably, in this framework nonconcatenative morphological processes, such as um laut in german, should not be as problematic as in the ia model.
</nextsent>
<nextsent>other algorithms combine information of both orthographic and semantic similarity of words (schone and jurafsky, 2000; <papid> W00-0712 </papid>baroni et al, 2002).<papid> W02-0606 </papid>semantic similarity is measured in terms of similar word contexts.</nextsent>
<nextsent>if two ortho graphically similar words occur in the context of roughly the same set of other words they probably share the same base form, e.g. german vertrag?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA144">
<title id=" W04-0106.xml">induction of a simple morphology for highlyinflecting languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>full words are related to other full words, and complex word forms are analyzed into variable and non-variablecomponent.
</prevsent>
<prevsent>conceivably, in this framework nonconcatenative morphological processes, such as um laut in german, should not be as problematic as in the ia model.
</prevsent>
</prevsection>
<citsent citstr=" W00-0712 ">
other algorithms combine information of both orthographic and semantic similarity of words (schone and jurafsky, 2000; <papid> W00-0712 </papid>baroni et al, 2002).<papid> W02-0606 </papid>semantic similarity is measured in terms of similar word contexts.</citsent>
<aftsection>
<nextsent>if two ortho graphically similar words occur in the context of roughly the same set of other words they probably share the same base form, e.g. german vertrag?
</nextsent>
<nextsent>vs. vertragen?
</nextsent>
<nextsent>(treaty).
</nextsent>
<nextsent>further cues for morphological learning are presented in (schone and jurafsky, 2001) <papid> N01-1024 </papid>and(yarowsky and wicentowsky, 2000).<papid> P00-1027 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA145">
<title id=" W04-0106.xml">induction of a simple morphology for highlyinflecting languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>full words are related to other full words, and complex word forms are analyzed into variable and non-variablecomponent.
</prevsent>
<prevsent>conceivably, in this framework nonconcatenative morphological processes, such as um laut in german, should not be as problematic as in the ia model.
</prevsent>
</prevsection>
<citsent citstr=" W02-0606 ">
other algorithms combine information of both orthographic and semantic similarity of words (schone and jurafsky, 2000; <papid> W00-0712 </papid>baroni et al, 2002).<papid> W02-0606 </papid>semantic similarity is measured in terms of similar word contexts.</citsent>
<aftsection>
<nextsent>if two ortho graphically similar words occur in the context of roughly the same set of other words they probably share the same base form, e.g. german vertrag?
</nextsent>
<nextsent>vs. vertragen?
</nextsent>
<nextsent>(treaty).
</nextsent>
<nextsent>further cues for morphological learning are presented in (schone and jurafsky, 2001) <papid> N01-1024 </papid>and(yarowsky and wicentowsky, 2000).<papid> P00-1027 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA146">
<title id=" W04-0106.xml">induction of a simple morphology for highlyinflecting languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>vs. vertragen?
</prevsent>
<prevsent>(treaty).
</prevsent>
</prevsection>
<citsent citstr=" N01-1024 ">
further cues for morphological learning are presented in (schone and jurafsky, 2001) <papid> N01-1024 </papid>and(yarowsky and wicentowsky, 2000).<papid> P00-1027 </papid></citsent>
<aftsection>
<nextsent>the latter utilizes frequency distributions over different inflectional forms (e.g., how often an english verb occurs in its past tense form in comparison to its base form).
</nextsent>
<nextsent>the algorithm is not entirely unsupervised.
</nextsent>
<nextsent>however, none of these non-ia models suits highly-inflecting languages as they assume only two or three constituents per word, analogous to stem and suffix.
</nextsent>
<nextsent>in order to cope with broader range of languages we would need the following: on the one hand, words should be allowed to consist of any number of alternating stems and affixes, making the model more flexible than, e.g., the model in (gold smith, 2001).<papid> J01-2001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA147">
<title id=" W04-0106.xml">induction of a simple morphology for highlyinflecting languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>vs. vertragen?
</prevsent>
<prevsent>(treaty).
</prevsent>
</prevsection>
<citsent citstr=" P00-1027 ">
further cues for morphological learning are presented in (schone and jurafsky, 2001) <papid> N01-1024 </papid>and(yarowsky and wicentowsky, 2000).<papid> P00-1027 </papid></citsent>
<aftsection>
<nextsent>the latter utilizes frequency distributions over different inflectional forms (e.g., how often an english verb occurs in its past tense form in comparison to its base form).
</nextsent>
<nextsent>the algorithm is not entirely unsupervised.
</nextsent>
<nextsent>however, none of these non-ia models suits highly-inflecting languages as they assume only two or three constituents per word, analogous to stem and suffix.
</nextsent>
<nextsent>in order to cope with broader range of languages we would need the following: on the one hand, words should be allowed to consist of any number of alternating stems and affixes, making the model more flexible than, e.g., the model in (gold smith, 2001).<papid> J01-2001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA158">
<title id=" W04-1705.xml">indexing student essays paragraphs using lsa over an integrated onto logical space </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on this distinction, four main approaches to essay assessment have been reported (williams, 2001).
</prevsent>
<prevsent>early systems such as peg (page, 1966) relied mainly on syntactic and linguistic features and required sample of the essays to be marked by number of human judges.
</prevsent>
</prevsection>
<citsent citstr=" W98-0303 ">
e-rater (burstein et al, 1998) <papid> W98-0303 </papid>uses combination of statistical and natural language processing techniques for the purpose of extracting linguistic features of the essays to be graded.</citsent>
<aftsection>
<nextsent>again, the essays are evaluated against set of human graded essays acting as benchmark.
</nextsent>
<nextsent>in the lsa method of essay grading, an lsa space is constructed based on domain specific material and the student essays.
</nextsent>
<nextsent>lsa grading performance is about as reliable as human graders (foltz, 1996).
</nextsent>
<nextsent>text categorisation (larkey, 1998) also requires database of graded essays, so that new essays can be categorised in relation to them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA159">
<title id=" W04-2322.xml">a rule based approach to discourse parsing </title>
<section> the classical linguistic dis-.  </section>
<citcontext>
<prevsection>
<prevsent>in section 5, we sketch the architecture of the palsumm system, summarization system being developed at fx palo alto laboratory that uses algorithms operating on discourse representations generated by u-ldm parser to summarize written english prose texts.
</prevsent>
<prevsent>in section 6 we present our conclusions and suggest directions for future research.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
course model (c-ldm) unlike the discourse structures model (dsm) of grosz and sidner (1986), <papid> J86-3001 </papid>pragmatic and psychological theory that aims to clarify the relationship between speakers?</citsent>
<aftsection>
<nextsent>intentions and their focus of attention in discourse, or the rhetorical model of rhetorical structures theory (mann and thompson, 1988) that is designed to identify the coherence relations between segments of text, the linguistic discourse model (ldm) (polanyi and scha, 1984; <papid> P84-1085 </papid>polanyi, 1988; polanyi and vanden berg, 1996) is syntactically informed, semantically driven model developed to provide proper semantic interpretation for every utterance in discourse despite the apparent discontinuities that are present even in well structured written texts.</nextsent>
<nextsent>in its focus on understanding discourse meaning, the ldm is close in spirit to structured discourse representation theory (s drt) (asher, 1993).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA160">
<title id=" W04-2322.xml">a rule based approach to discourse parsing </title>
<section> the classical linguistic dis-.  </section>
<citcontext>
<prevsection>
<prevsent>in section 6 we present our conclusions and suggest directions for future research.
</prevsent>
<prevsent>course model (c-ldm) unlike the discourse structures model (dsm) of grosz and sidner (1986), <papid> J86-3001 </papid>pragmatic and psychological theory that aims to clarify the relationship between speakers?</prevsent>
</prevsection>
<citsent citstr=" P84-1085 ">
intentions and their focus of attention in discourse, or the rhetorical model of rhetorical structures theory (mann and thompson, 1988) that is designed to identify the coherence relations between segments of text, the linguistic discourse model (ldm) (polanyi and scha, 1984; <papid> P84-1085 </papid>polanyi, 1988; polanyi and vanden berg, 1996) is syntactically informed, semantically driven model developed to provide proper semantic interpretation for every utterance in discourse despite the apparent discontinuities that are present even in well structured written texts.</citsent>
<aftsection>
<nextsent>in its focus on understanding discourse meaning, the ldm is close in spirit to structured discourse representation theory (s drt) (asher, 1993).
</nextsent>
<nextsent>while s-drt attempts to account for discourse structure purely semantically, the ldm framework is concerned to maintain separation between discourse syntactic?
</nextsent>
<nextsent>structure, on the one hand, and discourse interpretation on the other.
</nextsent>
<nextsent>therefore, like dsm and rst, the ldm incorporates an explicit tree structured model of relationships between discourse segments as its model of discourse syn tax?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA163">
<title id=" W04-2322.xml">a rule based approach to discourse parsing </title>
<section> greetings, discourse push/pop markers and.  </section>
<citcontext>
<prevsection>
<prevsent>2) syntactic information parallel syntactic structure; topic/focus and centering information, syntactic status of reused lexemes, pre-posed adverbial constituents, etc. 3) semantic information realis status, genericity, tense, aspect, point of view etc. in the mbdu 9 this process is too complex to describe in de-.
</prevsent>
<prevsent>tail here but it involves looking at both the fstructure of the sentential parsing information returned by the xle and applying discourse rules to the bdus identified.
</prevsent>
</prevsection>
<citsent citstr=" N03-1030 ">
soricut and marcu (2003) <papid> N03-1030 </papid>also build up rst sentential trees to use in discourse parsing.</citsent>
<aftsection>
<nextsent>both the information and methods used to construct rst trees as well as the trees themselves differ from ours.
</nextsent>
<nextsent>4) constituents of incomplete n-ary constructions on the right edge questions, initial greetings, genre-internal units like sections and sub-sections, etc. 5) structure of both the local attachment point and the bdu-tree while we are still experimenting with understanding the complexities involved in attachment, we believe that different types of evidence have different weights10 and that the combined weight of evidence determines the attachment point.
</nextsent>
<nextsent>we have noted, however even at this stage of our investigations, that the weight given to each type of information differs for attachment site selection and relationship determination.
</nextsent>
<nextsent>lexical information, for example, is often very important in determining site, while semantic and syntactic information is most relevant in determining relationship.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA164">
<title id=" W04-2322.xml">a rule based approach to discourse parsing </title>
<section> greetings, discourse push/pop markers and.  </section>
<citcontext>
<prevsection>
<prevsent>reported speech/reporting clause subordination m-bdu realis status differs from status of ap (mbdu is irrealis; ap is realis or mbdu is realis; ap is irrealis) nary-attachment (intrasentential) tense(ap) = past tense(mbdu) = pluperfect ap is time-reference for mbdu nary-attachment (intrasentential) verbclass(ap)=speechact?
</prevsent>
<prevsent>type(mbdu) = adjunct nary-attachment (intrasentential) tense(ap) = present tense(mbdu) = past ap is time-reference for mbdu coordinate parent(ap) is coordination parent(ap) would coordinate with mbdu ap would coordinate with mbdu subordination tense(ap) = past genericty(ap) = specific tense(mbdu) = present genericty(mbdu) = generic subordination m-bdu genericity status differs from status of ap (mbdu is specific; ap is generic or mbdu is generic; ap is specific) subordination subj(mbdu) = obj(ap) subordination subj(mbdu) = xcomp(ap) subordination mbdu/lexeme is subcase of ap/lexeme role(ap/lexeme) = role(mbdu/lexeme) right headed subordination (intrasentential) type(ap) = adjunct type(mbdu) = nary-attach (intrasentential) pred(adjunct(ap)) = if?
</prevsent>
</prevsection>
<citsent citstr=" W98-0315 ">
ap is irrealis mbdu is realis nary-attachment (intrasentential) ap and mbdu related by logical connective (cf webber&amp; joshi, 1998; <papid> W98-0315 </papid>forbes (2003) subordination tense(ap) = past tense(mbdu) = pluperfect subordination tense(ap) = present tense(mbdu) = past subordinate ap is bottom of dpt m-bdu is footnote or parenthetical coordinate ap is narrative( = specific, punctual ,event) mbdu is narrative coordinate tense(ap) = tense(mbdu) aspect(ap) = aspect(mbdu) coordinate mbdu/lexeme is synonym or antonym of ap/lexeme role(ap/lexeme) = role(mbdu/lexeme) subordinate ap is bottom of dpt table 3.</citsent>
<aftsection>
<nextsent>discourse attachment rules ordered to express priority of the rules.
</nextsent>
<nextsent>ap denotes (potential) attachment point.
</nextsent>
<nextsent>dcu consisting of the parent of both attachment point and incoming mbdu according to constraints of the discourse relation selected.
</nextsent>
<nextsent>if multiple attachments at different sites are possible, ambiguous parses are generated; less preferred attachments are discarded and the remaining attachment choices generate valid parse trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA165">
<title id=" W04-0817.xml">semantic role labelling with similarity based generalization using embased clustering </title>
<section> data and instances.  </section>
<citcontext>
<prevsection>
<prevsent>       fiff   as:
</prevsent>
<prevsent>flffi !
</prevsent>
</prevsection>
<citsent citstr=" C00-2094 ">
&amp;   estimation was performed using variant of the expectation-maximisation algorithm (prescher et al., 2000).<papid> C00-2094 </papid></citsent>
<aftsection>
<nextsent>we used this model both as feature andin the generalisation described in sec.
</nextsent>
<nextsent>5.
</nextsent>
<nextsent>in second model, we clustered pairs of target:role and the association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems syntactic properties of the role fillers; the resulting model was only used for generalisation.
</nextsent>
<nextsent>constituent features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA166">
<title id=" W04-0817.xml">semantic role labelling with similarity based generalization using embased clustering </title>
<section> classification.  </section>
<citcontext>
<prevsection>
<prevsent>)   the value of feature ) for class  , and : the weight assigned to 9 . the model is trained by optimising.
</prevsent>
<prevsent>the weights :subject to the maximum entropy constraint which ensures that the least committal optimal model is learnt.
</prevsent>
</prevsection>
<citsent citstr=" W03-1007 ">
maximum entropy (maxent) models have been successfully applied to semantic role labelling (fleischman et al, 2003).<papid> W03-1007 </papid></citsent>
<aftsection>
<nextsent>we usedthe estimate software for estimation, which implements the lmvm algorithm (malouf, 2002) <papid> W02-2018 </papid>and was kindly provided by rob malouf.</nextsent>
<nextsent>memory-based learning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA167">
<title id=" W04-0817.xml">semantic role labelling with similarity based generalization using embased clustering </title>
<section> classification.  </section>
<citcontext>
<prevsection>
<prevsent>the weights :subject to the maximum entropy constraint which ensures that the least committal optimal model is learnt.
</prevsent>
<prevsent>maximum entropy (maxent) models have been successfully applied to semantic role labelling (fleischman et al, 2003).<papid> W03-1007 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
we usedthe estimate software for estimation, which implements the lmvm algorithm (malouf, 2002) <papid> W02-2018 </papid>and was kindly provided by rob malouf.</citsent>
<aftsection>
<nextsent>memory-based learning.
</nextsent>
<nextsent>our second learner implements an instance of memory-based learning (mbl) algorithm, namely the ; -nearest neighbour algorithm.
</nextsent>
<nextsent>this algorithm classifies test instances by assigning them the label of the most similar examples from the training set.
</nextsent>
<nextsent>its parameters are the number of training examples to be considered, the similarity metric, and the feature weighting scheme.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA169">
<title id=" W04-0817.xml">semantic role labelling with similarity based generalization using embased clustering </title>
<section> jlk.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, the official scoring scheme determines the memory-based learners performance at more than10 points f-score below the maxent learner.
</prevsent>
<prevsent>we intend to run the memory-based learner with generalisation data for more comprehensive comparison.generalisation.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
gildea and jurafsky (2002) <papid> J02-3001 </papid>report an improvement of 1.6% through generalisation, which is roughly comparable to our fig ures.</citsent>
<aftsection>
<nextsent>the two strategies share the common idea of exploiting role similarities, but the realisations are converse: gildea and jurafsky manually compact similar frame elements into 18 abstract, frame independent roles, whereas we keep the roles frame specific but augment the training data for each by automatically discovered similarities.
</nextsent>
<nextsent>one reason for the disappointing performance ofthe framenet hierarchy-based generalisation strategies may be simply the amount of data, as shown by table 4: fn-h (sem) and fn-h (syn) each only yield 10,000 additional instances as compared to around 1,000,000 for em head.
</nextsent>
<nextsent>that the reliability of the results roughly seems to go up with the number of additional instances generated (periph erals: ca.
</nextsent>
<nextsent>50,000, em-path: ca.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA170">
<title id=" W04-0817.xml">semantic role labelling with similarity based generalization using embased clustering </title>
<section> jlk.  </section>
<citcontext>
<prevsection>
<prevsent>in future work, we will try to combine the em head strategy with the framenet hierarchy to derive more input for the clustering model to see if this can improve the present generalisation results.
</prevsent>
<prevsent>comparison with conll.
</prevsent>
</prevsection>
<citsent citstr=" W04-2413 ">
we recently studied semantic role labelling in the context of the conll shared task (baldewein et al, 2004).<papid> W04-2413 </papid></citsent>
<aftsection>
<nextsent>the two key differences to this study were that the semantic rolesin question were propbank roles and that only shallow information was available.
</nextsent>
<nextsent>our system there showed two main differences to the current sys tem: the overall level of accuracy was lower, andem-based clustering did not improve the performance.
</nextsent>
<nextsent>while the performance difference is evidently consequence of only shallow information being available, it remains an interesting open question why em-based clustering could improve one system, but not the other.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA171">
<title id=" W04-2324.xml">discourse dependency structures as constrained dags </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>the point want to make now is that one could argue that the nuclearity interpretation should be adopted instead, but one should not feel free to use both interpretations for the same tree.
</prevsent>
<prevsent>this is however what is done by some authors.
</prevsent>
</prevsection>
<citsent citstr=" J03-4002 ">
for example, in (webber et al, 2003), <papid> J03-4002 </papid>the tree in figure 4 is the discourse structure associated with (1).(1) a. although john is very generous b. if you need some money,c. you only have to ask him for it d. hes very hard to find.</citsent>
<aftsection>
<nextsent>let us show that some predicate-argument relations are given by the nuclearity interpretation and other ones by the standard interpretation in their tree.
</nextsent>
<nextsent>from (1), (2) can be inferred.
</nextsent>
<nextsent>this is evidence that the arguments of the discourse relation concession?
</nextsent>
<nextsent>in their tree are and d. these predicate-argument dependencies are given by the nuclearity interpretation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA174">
<title id=" W04-1108.xml">combining neural networks and statistics for chinese word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation (wsd) is method to determine the sense of ambiguous word given the context circumstance.
</prevsent>
<prevsent>wsd, long-standing problem in nlp, has been very active research topic,, which can be well applied in many nlp systems, such as information retrieval, text mining, machine translation, text categorization, text summarization, speech recognition, text to speech, and so on.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
with rising of corpus linguistics, the machine learning methods based on statistics are booming (yarowsky, 1992).<papid> C92-2070 </papid></citsent>
<aftsection>
<nextsent>these methods draw the support from the high-powered computers, get the statistics of large real-world corpus, find and acquire knowledge of linguistics automatically.
</nextsent>
<nextsent>they deal with all change by in variability, thus it is easy to trace the evaluation and development of natural language.
</nextsent>
<nextsent>so the statistic methods of nlp has attracted the attention of professional researchers and become the mainstream bit by bit.
</nextsent>
<nextsent>corpus-based statistical approaches are decision tree (pedersen, 2001), decision list, genetic algorithm, naive-bayesian classifier (escudero, 2000)maximum entropy model (adam, 1996; li, 1999), and so on.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA175">
<title id=" W03-2907.xml">unsupervised learning of bulgarian pos tags </title>
<section> previous approaches.  </section>
<citcontext>
<prevsection>
<prevsent>since the goal of this paperis to devise model which will induce pos categories automatically from an untagged text, withno prior knowledge of the structure of the language, we will be using these tagged corpora as agold standard to evaluate the performance of competing models.
</prevsent>
<prevsent>while this study is unique in attempting to incorporate both syntactic and morphological factors, previous work by other researchers has explored unsupervised methods of deriving clusters of words based on their linguistic behavior.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
(brown et al , 1992) <papid> J92-4003 </papid>is one of the first works to use statistical methods of distributional analysis to induce clusters of words.</citsent>
<aftsection>
<nextsent>these authors define an initial, very fine categorization of the vocabulary of corpus, in which each word is the sole member of its own category, and then iteratively merge these word classes until the desired levelof granularity is achieved.
</nextsent>
<nextsent>the objective function which they use to determine the optimal set of word classes   for corpus is the inter-class mutual information between adjacent words in thecorpus.
</nextsent>
<nextsent>since there is no practical way of determining the classification   which maximizes this quantity forgiven corpus, (brown et al , 1992) <papid> J92-4003 </papid>use greedy algorithm which proceeds from the initial classification, performing the merge which results in the least loss in mutual information ateach stage.</nextsent>
<nextsent>(lee, 1997) pursues similar approach in clustering nouns which occur as direct objects to verbs, but uses soft clustering algorithm in place of the agglomerative clustering algorithm used by brown et al , and lee uses the kl divergence between the nouns?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA181">
<title id=" W03-2907.xml">unsupervised learning of bulgarian pos tags </title>
<section> previous approaches.  </section>
<citcontext>
<prevsection>
<prevsent>(lee, 1997) pursues similar approach in clustering nouns which occur as direct objects to verbs, but uses soft clustering algorithm in place of the agglomerative clustering algorithm used by brown et al , and lee uses the kl divergence between the nouns?
</prevsent>
<prevsent>distributions as ameasure of closeness, rather than the loss in inter class mutual information.
</prevsent>
</prevsection>
<citsent citstr=" J96-2003 ">
(mcmahon and smith, 1996) <papid> J96-2003 </papid>employ similar algorithm to that of brown et al , but use top-down search in determining word clusters, rather than bottom-up one.</citsent>
<aftsection>
<nextsent>a number of other studies have attempted to use distributional analysis to derive pos categories.(brill et al , 1990) <papid> H90-1055 </papid>use an ad-hoc similarity metric to cluster words into pos-like classes, but the problem is significantly simplified by their preprocessing of the data to replace infrequent open class words with their correct pos tags.</nextsent>
<nextsent>finch&amp; chater (1994) describe model based on clustering words in vector space derived from their corpus contexts, but perform this analysis only for 1,0002,000 common words in their usenet corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA182">
<title id=" W03-2907.xml">unsupervised learning of bulgarian pos tags </title>
<section> previous approaches.  </section>
<citcontext>
<prevsection>
<prevsent>distributions as ameasure of closeness, rather than the loss in inter class mutual information.
</prevsent>
<prevsent>(mcmahon and smith, 1996) <papid> J96-2003 </papid>employ similar algorithm to that of brown et al , but use top-down search in determining word clusters, rather than bottom-up one.</prevsent>
</prevsection>
<citsent citstr=" H90-1055 ">
a number of other studies have attempted to use distributional analysis to derive pos categories.(brill et al , 1990) <papid> H90-1055 </papid>use an ad-hoc similarity metric to cluster words into pos-like classes, but the problem is significantly simplified by their preprocessing of the data to replace infrequent open class words with their correct pos tags.</citsent>
<aftsection>
<nextsent>finch&amp; chater (1994) describe model based on clustering words in vector space derived from their corpus contexts, but perform this analysis only for 1,0002,000 common words in their usenet corpus.
</nextsent>
<nextsent>hinrich schutze (1995) presents perhaps the most sophisticated model of word clustering for pos identification.
</nextsent>
<nextsent>schutze first construc tsa context vector to represent each words cooccurrence properties, and then trains recurrent neural network to predict the words location in the space based on the context vectors for surrounding words.
</nextsent>
<nextsent>the output vectors of the network are then clustered to produce pos-like classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA183">
<title id=" W03-2907.xml">unsupervised learning of bulgarian pos tags </title>
<section> model components.  </section>
<citcontext>
<prevsection>
<prevsent>this model architecture, which classifies word in context, allows the same word to be tagged differently, depending on how it is used.
</prevsent>
<prevsent>the approach to the identification of pos categories which we pursue in this paper attempts to incrementally home in on an optimal set of categories through the incorporation of morphological information and local syntactic information.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
the procedure is uses gradient descent training ofa hidden neural network model, including an embedded morphological model based on information from the linguist ica (goldsmith, 2001) <papid> J01-2001 </papid>engine for unsupervised morphological analysis.</citsent>
<aftsection>
<nextsent>be cause this morphological information is the output of completely unsupervised process of induction,our model of pos category induction is also an unsupervised one.
</nextsent>
<nextsent>in the following subsections, we provide short summary of each of these components of our model of part-of-speech learning, and in section 4, we present the results of testing our model on tagged bulgarian corpus.
</nextsent>
<nextsent>3.1 hidden neural networks.
</nextsent>
<nextsent>the model which we use for inducing clusters ofword tokens in corpus (which ought to correspond to parts of speech) is actually generalization of hidden markov model, called hidden neural network (hnn) (baldi and chauvin, 1996; riis, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA194">
<title id=" W04-0856.xml">pattern abstraction and term similarity for word sense disambiguation irst at senseval3 </title>
<section> lexical sample systems: pattern.  </section>
<citcontext>
<prevsection>
<prevsent>th-cut prec recall attempted 0.5 0.661 0.496 75.01 table 3: ddd-lsa on the english all-words task.
</prevsent>
<prevsent>abstraction and kernel methods one of the most discriminative features for lexical disambiguation is the lexical/syntactic pattern in which the word appears.
</prevsent>
</prevsection>
<citsent citstr=" H93-1052 ">
a well known issue in the wsd area is the one sense per collocation claim (yarowsky, 1993) <papid> H93-1052 </papid>stating that the word meanings are strongly associated with the particular collocation in which the word is located.</citsent>
<aftsection>
<nextsent>collocations are sequences of words in the context of the word to disambiguate, and can be associated to word senses performing supervised learning.
</nextsent>
<nextsent>another important knowledge source for wsd isthe shallow-syntactic pattern in which word appears.
</nextsent>
<nextsent>syntactic patterns, like lexical patterns, canbe obtained by exploiting pattern abstraction techniques on pos sequences.
</nextsent>
<nextsent>in the wsd literature both lexical and syntactic patterns have been usedas features in supervised learning schema by representing each instance using bigrams and trigram sin the surrounding context of the word to be ana lyzed2.2more recently deep-syntactic features have been also considered by several systems, as for example modifiers of nouns and verbs, object and subject of the sentence, etc. in order to representing each instance by bag of features?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA195">
<title id=" W04-2203.xml">qualitative evaluation of automatically calculated accept ion based mldb </title>
<section> taxonomy of evaluation criteria.  </section>
<citcontext>
<prevsection>
<prevsent>we propose classification of mldb evaluation criteria into four classes, according to their nature.
</prevsent>
<prevsent>3.1 golden-standard-based criteria.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
in the domain of machine translation systems, an increasingly accepted way to measure thequality of system is to compare the outputs it produces with set of reference translations, considered as an approximation of golden standard (papineni et al, 2002; <papid> P02-1040 </papid>hovy et al., 2002).</citsent>
<aftsection>
<nextsent>by analogy, one can define golden standard multilingual lexical database to compare to database generated by system such asjeminie, that both contain axies that link to lexies in the same monolingual databases.
</nextsent>
<nextsent>considering that two axies are the same if they contain links to exactly the same lexies, the quality of machine generated multilingual lexical database would then be measured with two metrics adapted from machine translation system evaluation (ahrenberg et al, 2000): recall and precision.
</nextsent>
<nextsent>recall (coverage) is the number of axies that are defined in both the generated database and in the golden standard database, divided by the number of axies in the golden standard.
</nextsent>
<nextsent>precision is the number of axies that are defined in both the generated database and in the golden standard database, divided by the number of axies in the generated database.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA196">
<title id=" W03-2801.xml">reuse and challenges in evaluating language generation systems position paper </title>
<section> the challenge: automatic.  </section>
<citcontext>
<prevsection>
<prevsent>an integral part of the development of machine learning approaches to nlp tasks is the ability to perform automatic quantitative evaluation in orderto measure differences between different configurations of the module and also allow comparative evaluation with other approaches.
</prevsent>
<prevsent>for example, the muc corpora and the associated scoring tool are frequently used by researchers working on machine learning for information extraction both as part of the development process and also asmeans for comparison of the performance of dif4the miakt project is sponsored by the uk engineering and physical sciences research council (grant gr/r85150/01) and involves the university of southampton, university of sheffield, the open university, university of oxford, and kings college london.
</prevsent>
</prevsection>
<citsent citstr=" M98-1002 ">
ferent systems (see e.g., (marsh and perzanowski,1998)).<papid> M98-1002 </papid></citsent>
<aftsection>
<nextsent>similarly, automatic quantitative evaluation of content planners needs:   an annotated corpus;  an evaluation metric and scoring tool, implementing this metric.
</nextsent>
<nextsent>below we will discuss each of these component sand highlight the outstanding problems and challenges.
</nextsent>
<nextsent>4.1 evaluation corpora for content.
</nextsent>
<nextsent>planning research on content planning comes from two fields: document summarisation which uses some nlg techniques to generate the summaries; and natural language generation where the systems generate from some semantic representation, e.g., domain knowledge base or numeric weather data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA197">
<title id=" W03-2801.xml">reuse and challenges in evaluating language generation systems position paper </title>
<section> the challenge: automatic.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, reuse and some automation are paramount.
</prevsent>
<prevsent>so far, only very small semantically annotated corpora for nlg have been created.
</prevsent>
</prevsection>
<citsent citstr=" P01-1023 ">
for example, (duboue and mckeown, 2001)<papid> P01-1023 </papid>have collected an annotated corpus of 24 transcripts of medical briefings.</citsent>
<aftsection>
<nextsent>they use 29 categories to classify the 200 tags used in their tagset.
</nextsent>
<nextsent>each transcript had an average of 33 tags with some tags being much more frequent than others.
</nextsent>
<nextsent>since the tags need to convey the semantics of the text units, they are highly domain specific, which means that any other nlg system or learning approach that would want to use this corpus for evaluation will have to be re targetted to this domain.
</nextsent>
<nextsent>4.1.2 the proposed approach for miakt as evident from this discussion, there are still number of problems that need to be solved so that semantically annotated corpus of useful size 5available at http://www.cs.columbia.edu/ noemie/ordering/.can be created, thus enabling the comparative evaluation of different learning strategies and content planning components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA201">
<title id=" W03-2801.xml">reuse and challenges in evaluating language generation systems position paper </title>
<section> the challenge: automatic.  </section>
<citcontext>
<prevsection>
<prevsent>for example, (barzilay et al, 2002) asked humans to grade the summaries, while (duboue and mckeown, 2001)<papid> P01-1023 </papid>manually analysed the derived constraints by comparing them to an existing text planner.</prevsent>
<prevsent>however,this is not sufficient if different planners or versions of the same planner are to be compared in aquantitative fashion.</prevsent>
</prevsection>
<citsent citstr=" W00-1401 ">
in contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (bangalore et al, 2000) <papid> W00-1401 </papid>andthey have been shown to correlate well with human judgement for quality and understandability.</citsent>
<aftsection>
<nextsent>these metrics are two kinds: using string edit distance and using tree-based metrics.
</nextsent>
<nextsent>the string edit distance ones measure the insertion, deletion,and substitution errors between the reference sentences in the corpus and the generated ones.
</nextsent>
<nextsent>two different measures were evaluated and the one that treats deletions in one place and insertion in the other as single movement error was found to bemore appropriate.
</nextsent>
<nextsent>in the context of content planning we intend use the string edit distance metrics by comparing the proposition sequence generated by the planner against the ideal?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA202">
<title id=" W04-0810.xml">a first evaluation of logic form identification systems </title>
<section> general guidelines.  </section>
<citcontext>
<prevsection>
<prevsent>for this first trial, we did not make the distinction between the two and thus the accepted representation would be eat:v (e2, x3, x2, x4) - see below.
</prevsent>
<prevsent>output: earth:n (x1) provide:v (e1, x1, x2) food:n (x2) we(x3) eat:v (e2, x3, x2, x4) day:n (x4) predicates are formed by the concatenation of the base form of the word and its lexical category as encoded in wordnet (since only nouns, verbs, adjectives and adverbs are encoded in wordnet, only predicates for those lexical categories have the category attached to the predicate).
</prevsent>
</prevsection>
<citsent citstr=" P01-1052 ">
to ease the task, the notation was relaxed by adopting few simplifications similar, to some extent, to the simplications in (moldovan andrus, 2001): <papid> P01-1052 </papid>determiners, plurals, negation, auxiliaries and verb tenses, punctuation are ingnored.</citsent>
<aftsection>
<nextsent>collocations, such as new york, should be considered single predicate as well as verbs having particles (e.g.give up).
</nextsent>
<nextsent>for cases when an argument is underspecified, such as the logical subject in jim was told to association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems say something, an artificial argument should be generated.
</nextsent>
<nextsent>the advantages of the lf notation are mantfold:   it allows simple syntax/semantics interface   it is user friendly   it has positional syntactic arguments that ease other nlp tasks such as textual interpretationa and textual inference   if predicates are disambiguated with respect to general ontology such as wordnet it leads to concept predicates  it is easily customizable (for example to distinguish between arguments and adjuncts) for details about the principles of logic forms read chapter 2 in (rus, 2002), (rus and moldovan, 2002) and (hobbs, 1986).<papid> H86-1003 </papid></nextsent>
<nextsent>the lf notation proposed for the lfi competition is novel, and different from the one described in the previous references since it distinguishes between complements and adjuncts among other differ ences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA203">
<title id=" W04-0810.xml">a first evaluation of logic form identification systems </title>
<section> general guidelines.  </section>
<citcontext>
<prevsection>
<prevsent>collocations, such as new york, should be considered single predicate as well as verbs having particles (e.g.give up).
</prevsent>
<prevsent>for cases when an argument is underspecified, such as the logical subject in jim was told to association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems say something, an artificial argument should be generated.
</prevsent>
</prevsection>
<citsent citstr=" H86-1003 ">
the advantages of the lf notation are mantfold:   it allows simple syntax/semantics interface   it is user friendly   it has positional syntactic arguments that ease other nlp tasks such as textual interpretationa and textual inference   if predicates are disambiguated with respect to general ontology such as wordnet it leads to concept predicates  it is easily customizable (for example to distinguish between arguments and adjuncts) for details about the principles of logic forms read chapter 2 in (rus, 2002), (rus and moldovan, 2002) and (hobbs, 1986).<papid> H86-1003 </papid></citsent>
<aftsection>
<nextsent>the lf notation proposed for the lfi competition is novel, and different from the one described in the previous references since it distinguishes between complements and adjuncts among other differences.
</nextsent>
<nextsent>a web page for the lfi task is available at http://www.cs.iusb.edu/ vasile/logic/indexlf.html and discussion group, called logic form, was opened at yahoo.groups.com which can also be consulted.
</nextsent>
<nextsent>the test data was compiled so that the impact of external tools that different sytems might use in the lf identification process be minimal.
</nextsent>
<nextsent>for example,it is well-known that the accuracy of automatic syntactic parsing drops drastically for sentences larger than 40 words and thus we kept the size of the collected sentences below the 40 words threshold.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA204">
<title id=" W04-0810.xml">a first evaluation of logic form identification systems </title>
<section> annotation guidelines.  </section>
<citcontext>
<prevsection>
<prevsent>for the logic form identification task the following steps were applied to obtain the correct lf for the test data: 1.
</prevsent>
<prevsent>logic forms for the test data were automatically obtained using an extended version of the lf derivation engine developed in (rus, 2002) for lfi of wordnet glosses.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
as part of this step,sentences were preprocessed: tokenized (sepa rating punctuation from words) using the penn treebank guidelines, tagged with brills tagger(brill, 1992) <papid> A92-1021 </papid>and then parsed with collins?</citsent>
<aftsection>
<nextsent>statistical parser (collins, 1996).<papid> P96-1025 </papid>2.</nextsent>
<nextsent>a first manual checking of the previously generated lf was done.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA205">
<title id=" W04-0810.xml">a first evaluation of logic form identification systems </title>
<section> annotation guidelines.  </section>
<citcontext>
<prevsection>
<prevsent>logic forms for the test data were automatically obtained using an extended version of the lf derivation engine developed in (rus, 2002) for lfi of wordnet glosses.
</prevsent>
<prevsent>as part of this step,sentences were preprocessed: tokenized (sepa rating punctuation from words) using the penn treebank guidelines, tagged with brills tagger(brill, 1992) <papid> A92-1021 </papid>and then parsed with collins?</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
statistical parser (collins, 1996).<papid> P96-1025 </papid>2.</citsent>
<aftsection>
<nextsent>a first manual checking of the previously generated lf was done.
</nextsent>
<nextsent>3.
</nextsent>
<nextsent>a second manual checking was done by another annotator.4.
</nextsent>
<nextsent>quality assurance of the previous steps was performed by individual annotators by checking specific cases (ditransitives, relative pronouns, etc.) with much emphasis on consistency.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA206">
<title id=" W04-0834.xml">supervised word sense disambiguation with support vector machines and multiple knowledge sources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the distinction is that for the translation and sense subtask, the english sense of the target ambiguous word   is also provided (for both training and test data).
</prevsent>
<prevsent>in all, we submitted 3 systems: system nusels for the english lexical sample task, system nusmlst for the translation subtask, and system nusmlsts for the translation and sense subtask.
</prevsent>
</prevsection>
<citsent citstr=" W02-1006 ">
all systems were based on the supervised word sense disambiguation (wsd) system of lee and ng (2002), <papid> W02-1006 </papid>and used support vector machines (svm) learning.</citsent>
<aftsection>
<nextsent>only the training examples provided in the official training corpus were used to train the systems, and no other external resources were used.
</nextsent>
<nextsent>in particular, we did not use any external dictionary or the sample sentences in the provided dictionary.the knowledge sources used included part-of speech (pos) of neighboring words, single words inthe surrounding context, local collocations, and syntactic relations, as described in lee and ng (2002).<papid> W02-1006 </papid>for the translation and sense subtask of the multilingual lexical sample task, the english sense given for the target word was also used as an additional knowledge source.</nextsent>
<nextsent>all features encoding these knowledge sources were used, without any feature selection.we next describe svm learning and the combined knowledge sources adopted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA211">
<title id=" W04-0834.xml">supervised word sense disambiguation with support vector machines and multiple knowledge sources </title>
<section> multiple knowledge sources.  </section>
<citcontext>
<prevsection>
<prevsent>to disambiguate word occurrence   , systems nusels and nusmlst used the first four knowledge sources listed below.
</prevsent>
<prevsent>system nusmlsts used the english sense given for the target ambiguous word  as an additional knowledge source.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
previous research (ng and lee, 1996; <papid> P96-1006 </papid>stevenson and wilks, 2001; <papid> J01-3001 </papid>florian et al, 2002; lee and ng, 2002) <papid> W02-1006 </papid>has shown that combination of knowledge sources improves wsd accuracy.</citsent>
<aftsection>
<nextsent>our experiments on the provided training data of the senseval-3 translation and sense subtask also indicated that the additional knowledge source of the english sense of the target word further improved accuracy (see section 4.3 for details).
</nextsent>
<nextsent>we did not attempt feature selection since our previous research (lee and ng, 2002) <papid> W02-1006 </papid>indicated that svm performs better without feature selection.</nextsent>
<nextsent>3.1 part-of-speech (pos) of neighboring.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA212">
<title id=" W04-0834.xml">supervised word sense disambiguation with support vector machines and multiple knowledge sources </title>
<section> multiple knowledge sources.  </section>
<citcontext>
<prevsection>
<prevsent>to disambiguate word occurrence   , systems nusels and nusmlst used the first four knowledge sources listed below.
</prevsent>
<prevsent>system nusmlsts used the english sense given for the target ambiguous word  as an additional knowledge source.
</prevsent>
</prevsection>
<citsent citstr=" J01-3001 ">
previous research (ng and lee, 1996; <papid> P96-1006 </papid>stevenson and wilks, 2001; <papid> J01-3001 </papid>florian et al, 2002; lee and ng, 2002) <papid> W02-1006 </papid>has shown that combination of knowledge sources improves wsd accuracy.</citsent>
<aftsection>
<nextsent>our experiments on the provided training data of the senseval-3 translation and sense subtask also indicated that the additional knowledge source of the english sense of the target word further improved accuracy (see section 4.3 for details).
</nextsent>
<nextsent>we did not attempt feature selection since our previous research (lee and ng, 2002) <papid> W02-1006 </papid>indicated that svm performs better without feature selection.</nextsent>
<nextsent>3.1 part-of-speech (pos) of neighboring.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA217">
<title id=" W04-0834.xml">supervised word sense disambiguation with support vector machines and multiple knowledge sources </title>
<section> multiple knowledge sources.  </section>
<citcontext>
<prevsection>
<prevsent>words we use 7 features to encode this knowledge source:
</prevsent>
<prevsent>       , where
</prevsent>
</prevsection>
<citsent citstr=" A97-1004 ">
(    ) is the pos of the  th token to the left (right) of   , and   is the pos of   . token can be word or punctuation symbol, and each of these neighboring tokens must be in the same sentence as   . we use asentence segmentation program (reynar and ratnaparkhi, 1997) <papid> A97-1004 </papid>and pos tagger (ratnaparkhi, 1996) <papid> W96-0213 </papid>to segment the tokens surrounding   into sentences and assign pos tags to these tokens.</citsent>
<aftsection>
<nextsent>for example, to disambiguate the word bars in the pos-tagged sentence reid/nnp saw/vbd me/prp looking/vbg at/in the/dt iron/nn bars/nns ./.?, the pos feature vector is   ff fifl ffi  ffi ! $#$#% where # denotes the pos tag of null token.
</nextsent>
<nextsent>3.2 single words in the surrounding context.
</nextsent>
<nextsent>for this knowledge source, we consider all single words (unigrams) in the surrounding context of   , and these words can be in different sentence from   . for each training or test example, thesenseval-3 official dataset provides few sentences as the surrounding context.
</nextsent>
<nextsent>in the results reported here, we consider all words in the provided context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA218">
<title id=" W04-0834.xml">supervised word sense disambiguation with support vector machines and multiple knowledge sources </title>
<section> multiple knowledge sources.  </section>
<citcontext>
<prevsection>
<prevsent>words we use 7 features to encode this knowledge source:
</prevsent>
<prevsent>       , where
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
(    ) is the pos of the  th token to the left (right) of   , and   is the pos of   . token can be word or punctuation symbol, and each of these neighboring tokens must be in the same sentence as   . we use asentence segmentation program (reynar and ratnaparkhi, 1997) <papid> A97-1004 </papid>and pos tagger (ratnaparkhi, 1996) <papid> W96-0213 </papid>to segment the tokens surrounding   into sentences and assign pos tags to these tokens.</citsent>
<aftsection>
<nextsent>for example, to disambiguate the word bars in the pos-tagged sentence reid/nnp saw/vbd me/prp looking/vbg at/in the/dt iron/nn bars/nns ./.?, the pos feature vector is   ff fifl ffi  ffi ! $#$#% where # denotes the pos tag of null token.
</nextsent>
<nextsent>3.2 single words in the surrounding context.
</nextsent>
<nextsent>for this knowledge source, we consider all single words (unigrams) in the surrounding context of   , and these words can be in different sentence from   . for each training or test example, thesenseval-3 official dataset provides few sentences as the surrounding context.
</nextsent>
<nextsent>in the results reported here, we consider all words in the provided context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA220">
<title id=" W04-0834.xml">supervised word sense disambiguation with support vector machines and multiple knowledge sources </title>
<section> multiple knowledge sources.  </section>
<citcontext>
<prevsection>
<prevsent>$) , and
</prevsent>
<prevsent>$)  . this.
</prevsent>
</prevsection>
<citsent citstr=" W97-0323 ">
set of 11 features is the union of the collocation features used in ng and lee (1996) <papid> P96-1006 </papid>and ng (1997).<papid> W97-0323 </papid></citsent>
<aftsection>
<nextsent>note that each collocation
</nextsent>
<nextsent> ) * is represented byone feature that can have many possible feature values (the local collocation strings), whereas each distinct surrounding word is represented by one feature that takes binary values (indicating presence or absence of that word).
</nextsent>
<nextsent>for example, if   is the word bars and suppose the set of collocations for
</nextsent>
<nextsent>is   chocolate, the wine, the iron ( , then the feature value for collocation
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA221">
<title id=" W04-0834.xml">supervised word sense disambiguation with support vector machines and multiple knowledge sources </title>
<section> multiple knowledge sources.  </section>
<citcontext>
<prevsection>
<prevsent>is the iron.
</prevsent>
<prevsent>3.4 syntactic relations.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
we first parse the sentence containing   with statistical parser (charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>the constituent tree structure generated by charniak parser is then converted into dependency tree in which every word points to parent headword.
</nextsent>
<nextsent>for example, in the sentence reid saw me looking at the iron bars .?, the word reid points to the parent headword saw.
</nextsent>
<nextsent>similarly, the word me also points to the parent headword saw.we use different types of syntactic relations, depending on the pos of   . if   is noun, we use four features: its parent headword 1 , the pos of 1 , the voice of 1 (active, passive, or 2 if 1 is not verb), 1(a) attention (noun) 1(b) he turned his attention to the workbench . 1(c)  turned, vbd, active, left % 2(a) turned (verb) 2(b) he turned his attention to the workbench . 2(c)  he, attention, prp, nn, vbd, active % 3(a) green (adj) 3(b) the modern tram is green machine . 3(c)  machine, nn % table 1: examples of syntactic relations and the relative position of 1 from   (whether 1 is to the left or right of   ).
</nextsent>
<nextsent>if   is verb, we use six features: the nearest word   to the left of   such that   is the parent headword of   , the nearest word
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA222">
<title id=" W04-0905.xml">evaluating the performance of the ontosem semantic analyzer </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at both stages, the analyzer has to eal with ambiguity, incongruity between the input nd expectations recorded in the static knowledge ources, unknown words, and non-literal language.
</prevsent>
<prevsent>igure 2 summarizes the types of heuristics that the nalyzer uses at the first stage.
</prevsent>
</prevsection>
<citsent citstr=" W03-0904 ">
while all of rocedures using them have been implemented see nirenburg et al 2003), <papid> W03-0904 </papid>the version of the nalyzer we evaluated involved only subset of hem.</citsent>
<aftsection>
<nextsent>we plan to evaluate the analyzer with all the vail able recovery procedures in the near future.
</nextsent>
<nextsent>etc., as well as about 3,000 of the most frequent verbs.
</nextsent>
<nextsent>we illustrate the structure of the lexicon entry on the example of the first verbal sense of alert (presented in simplified format): alert-v1 example  he alerted us morph regular syn-struc root root $var0 cat subject root $var root $var object pp-adjunct root $var2 cat prep root to opt + ect obj root $va s m- cept warn ;an onto logical con agen value ^$var1 sem huma value ^$var3 theme beneficiary value ^$var4 instrument value ^$var1 sem (or artifact ev ^$var2 null-sem + ntax-semantics dependency linking; the caret ?^ is read the meaning of.?
</nextsent>
<nextsent>in this ex ple, am if ^$var1 or descendant of , it occupies to logical concepts by rize the ar from recently processed text about colin accept-70 neficiary organization-71 ask hor-time)) st-action-69 on-71 ations f. resolution done -action -72 (colin powell), whose beneficiary is organization-71 (united bered instances of onto logical concepts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA223">
<title id=" W04-0905.xml">evaluating the performance of the ontosem semantic analyzer </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>in fact, we will use different combinations of the procedures for residual ambiguity resolution and recovery from unexpected?
</prevsent>
<prevsent>input to dete analysis (not only wsd but also semantic dependency determination).
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
the evaluation of semantic dependency determination is different from that suggested by gildea and jurafsky (2002) <papid> J02-3001 </papid>who designed system to automatically learn the semantic roles of unknown predicates.</citsent>
<aftsection>
<nextsent>first, that system does not actually do wsd; second, it makes assumptions that our work does not: it does not use any language-independent meta language to record meaning and concent restrictions, far more limited inventory than the set of all possible relations between concepts provided in our ontology.
</nextsent>
<nextsent>the evaluation environment we have developed reduces the amount of time necessary to produce sense that it is very important enabling element for larger-scale evaluation work that from this point on will become standard proc gold standard output for each of the three stages of our analysis process quite dramatically.
</nextsent>
<nextsent>it is in this edure in our work on building semantic analyzers baseline baseline as is correct pre processor correct syntax abbreviations, numbers, etc. 3/2 3/2 3/2 5/0 5/0 named entities 14/10 14/10 14/10 24/0 24/0 parts of speech 121/83 121/83 121/83 204/0 204/0 pre processor total 0.59 0.59 0.59 1.0 1.0 phrase boundary score 0.81 0.8 0.91 0.97 1.0 phrase heads 129/48 127/50 159/25 180/12 182/0 attachments 86/38 87/37 100/53 166/15 `81/0 syntax total 0.74 0.77 0.81 0.94 1.0 wsd 57/54 59/52 63/48 86/25 98/15 wsd complexity 0.61 0.62 0.64 0.85 0.96 wsd distance 0.79 0.80 0.83 0.92 0.96 semantic dependencies 104/182 113/173 136/150 198/88 229/43 table 2.
</nextsent>
<nextsent>results of the initial evaluation of the ontosem semantic analyzer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA224">
<title id=" W04-0215.xml">live tree an integrated workbench for discourse processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the live tree environment provides tools for manual and automatic u-ldm segmentation and discourse parsing.
</prevsent>
<prevsent>document management, grammar testing, manipulation of discourse structures and creation and editing of discourse relations are also supported.
</prevsent>
</prevsection>
<citsent citstr=" W04-2322 ">
in this paper, we introduce live tree, core component of lidas (the linguistic discourse analysis system) for automatic discourse parsing with the unified linguistic discourse model (u-ldm) (po lanyi et al 2004).<papid> W04-2322 </papid></citsent>
<aftsection>
<nextsent>the u-ldm is theory of discourse structure and semantics that has as its goal assigning the correct interpretation to natural language utterances.
</nextsent>
<nextsent>1.1 overview of livetree.
</nextsent>
<nextsent>live tree is an integrated workbench for supervised and unsupervised creation, storage and manipulation of the discourse structure of text documents under the ldm.
</nextsent>
<nextsent>live tree does not support speech, dialog or interaction annotation (bernsen et al 2002, 2003 and over view of systems in bernsen et al 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA225">
<title id=" W04-2117.xml">language resources for a network based dictionary </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W02-1118 ">
in order to facilitate the use of dictionary for language production and language learning we propose the construction of new network-based electronic dictionary along the lines of zock (2002).<papid> W02-1118 </papid></citsent>
<aftsection>
<nextsent>however, contrary to zock who proposes just paradig matic network with information about the various ways in which words are similar we would like to present several existing language resources (lrs)which will be integrated in such network resulting in more linguistic levels than one with paradig matic ally associated words.
</nextsent>
<nextsent>we argue that just asthe mental lexicon exhibits various, possibly interwoven layers of networks, electronic lrs containing syntagmatic, morphological and phonological information need to be integrated into an associative electronic dictionary.
</nextsent>
<nextsent>traditional dictionaries are mainly used for language reception, even though they were also developed to be used for language production.
</nextsent>
<nextsent>however the form-based structure following orthographic conventions which could also be called one-dimensional?, makes it difficult to access the information by meaning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA226">
<title id=" W04-2117.xml">language resources for a network based dictionary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>apart from specifying paradigmatic information which is usually also part of the definition of lemma, syntagmatic information representing collocations and cooccurrances is an important resource for language learners.
</prevsent>
<prevsent>knowledge about collocations is kind of linguistic knowledge which is language-specific andnot systematically derivable making collocations especially difficult to learn.
</prevsent>
</prevsection>
<citsent citstr=" C02-1007 ">
even though there are some studies that compare the results from statistically computed association measures with word association norms from psy cho linguistic experiments (landauer et al, 1998; rapp, 2002) <papid> C02-1007 </papid>there has not been any research onthe usage of digital, network-based dictionary reflecting the organisation of the mental lexicon to our knowledge.</citsent>
<aftsection>
<nextsent>apart from studies using so called mind maps or concept maps to visualize world knowledge3 (novak, 1998) nothing is known about the psycho linguistic aspects which need to be considered for the construction of network-based dictionary.
</nextsent>
<nextsent>in the following section we will summarize the information made available by the various lrs weplan to integrate into our system.
</nextsent>
<nextsent>the ideas presented here were developed in preparation of project at the university of osnabruck.
</nextsent>
<nextsent>zock (2002) <papid> W02-1118 </papid>proposes the use of only one type of information structure in his network, namely type of semantic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA231">
<title id=" W04-2117.xml">language resources for a network based dictionary </title>
<section> language resources.  </section>
<citcontext>
<prevsection>
<prevsent>with more elaborate set of rules describing the phonology of language amore complex analysis is possible which even allows the determination of words that rhyme.7 setting suitable threshold for some measure of similarity network should evolve with phonologically similar words being connected with each other.
</prevsent>
<prevsent>a related approach to spelling correction is the use of so called tries?
</prevsent>
</prevsection>
<citsent citstr=" J96-1003 ">
for the efficient storage of lexical data (oflazer, 1996).<papid> J96-1003 </papid></citsent>
<aftsection>
<nextsent>the calculation of minimal editing distance between an unknown word and aword in trie determines possible correct candidate.
</nextsent>
<nextsent>contrary to zock (2002) <papid> W02-1118 </papid>who suggests this as an analysis step on its own we think that the phonological and morphological similarity can be exploited to form yet another layer in network-based dictio nary.</nextsent>
<nextsent>zocks example of the looked-for relegate may than be connected to renegade?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA234">
<title id=" W04-0707.xml">discourse new detectors for definite description resolution a survey and a preliminary proposal </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J00-4003 ">
vieira and poesio (2000) <papid> J00-4003 </papid>proposed an algorithm for definite description (dd) resolution that incorporates number of heuristics for detecting discourse new descriptions.</citsent>
<aftsection>
<nextsent>the inclusion of such detectors was motivated by the observation that more than 50% of definite descriptions (dds) in an average corpus are discourse new (poesio and vieira, 1998), <papid> J98-2001 </papid>but whereas the inclusion of detectors fornon-anaphoric pronouns in algorithms such as lap pin and leass?</nextsent>
<nextsent>(1994) leads to clear improvement sin precision, the improvements in anaphoric dd resolution (as opposed to classification) brought about by the detectors were rather small.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA236">
<title id=" W04-0707.xml">discourse new detectors for definite description resolution a survey and a preliminary proposal </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>vieira and poesio (2000) <papid> J00-4003 </papid>proposed an algorithm for definite description (dd) resolution that incorporates number of heuristics for detecting discourse new descriptions.</prevsent>
</prevsection>
<citsent citstr=" J98-2001 ">
the inclusion of such detectors was motivated by the observation that more than 50% of definite descriptions (dds) in an average corpus are discourse new (poesio and vieira, 1998), <papid> J98-2001 </papid>but whereas the inclusion of detectors fornon-anaphoric pronouns in algorithms such as lap pin and leass?</citsent>
<aftsection>
<nextsent>(1994) leads to clear improvement sin precision, the improvements in anaphoric dd resolution (as opposed to classification) brought about by the detectors were rather small.
</nextsent>
<nextsent>in fact, ng and cardie (2002<papid> P02-1014 </papid>a) challenged the motivation for the inclusion of such detectors, reporting no improvements, or even worse performance.</nextsent>
<nextsent>we re-examinethe literature on the topic in detail, and propose revised algorithm, taking advantage of the improved discourse-new detection techniques developed by uryupina (2003).<papid> P03-2012 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA237">
<title id=" W04-0707.xml">discourse new detectors for definite description resolution a survey and a preliminary proposal </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>the inclusion of such detectors was motivated by the observation that more than 50% of definite descriptions (dds) in an average corpus are discourse new (poesio and vieira, 1998), <papid> J98-2001 </papid>but whereas the inclusion of detectors fornon-anaphoric pronouns in algorithms such as lap pin and leass?</prevsent>
<prevsent>(1994) leads to clear improvement sin precision, the improvements in anaphoric dd resolution (as opposed to classification) brought about by the detectors were rather small.</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
in fact, ng and cardie (2002<papid> P02-1014 </papid>a) challenged the motivation for the inclusion of such detectors, reporting no improvements, or even worse performance.</citsent>
<aftsection>
<nextsent>we re-examinethe literature on the topic in detail, and propose revised algorithm, taking advantage of the improved discourse-new detection techniques developed by uryupina (2003).<papid> P03-2012 </papid></nextsent>
<nextsent>although many theories of definite ness and many anaphora resolution algorithms are based on the assumption that definite descriptions are anaphoric,in fact in most corpora at least half of definite descriptions are discourse-new (prince, 1992), as shown by the following examples, both of which are the first sentences of texts from the penn treebank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA243">
<title id=" W04-0707.xml">discourse new detectors for definite description resolution a survey and a preliminary proposal </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>(1994) leads to clear improvement sin precision, the improvements in anaphoric dd resolution (as opposed to classification) brought about by the detectors were rather small.
</prevsent>
<prevsent>in fact, ng and cardie (2002<papid> P02-1014 </papid>a) challenged the motivation for the inclusion of such detectors, reporting no improvements, or even worse performance.</prevsent>
</prevsection>
<citsent citstr=" P03-2012 ">
we re-examinethe literature on the topic in detail, and propose revised algorithm, taking advantage of the improved discourse-new detection techniques developed by uryupina (2003).<papid> P03-2012 </papid></citsent>
<aftsection>
<nextsent>although many theories of definite ness and many anaphora resolution algorithms are based on the assumption that definite descriptions are anaphoric,in fact in most corpora at least half of definite descriptions are discourse-new (prince, 1992), as shown by the following examples, both of which are the first sentences of texts from the penn treebank.
</nextsent>
<nextsent>(1) a. toni johnson pulls tape measure across the front of what was once stately victorian home.
</nextsent>
<nextsent>b. the federal communications commission allowed american telephone &amp; telegraph co. to continue offering discount phone services for large-business customers and said it would soon re-examine its regulation of the long-distance market.
</nextsent>
<nextsent>vieira and poesio (2000) <papid> J00-4003 </papid>proposed an algorithm for definite description resolution that incorporates number of heuristics for detecting discourse-new (henceforth: dn) descriptions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA256">
<title id=" W04-0707.xml">discourse new detectors for definite description resolution a survey and a preliminary proposal </title>
<section> detecting discourse-new definite.  </section>
<citcontext>
<prevsection>
<prevsent>we reexamine the literature on the topic in detail, and propose revised algorithm, taking advantage of the improved dn detection techniques developed by uryupina (2003).<papid> P03-2012 </papid></prevsent>
<prevsent>descriptions 2.1 vieira and poesio.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
poesio and vieira (1998) <papid> J98-2001 </papid>carried out corpus studies indicating that in corpora like the wall street journal portion of the penn treebank (marcus et al., 1993), <papid> J93-2004 </papid>around 52% of dds are discourse-new(prince, 1992), and another 15% or so are bridging references, for total of about 66-67% first mention.</citsent>
<aftsection>
<nextsent>these results led vieira and poesio to propose definite description resolution algorithm incorporating independent heuristic strategies for recognizing dn definite descriptions (vieira, 1998; vieira and poesio, 2000).<papid> J00-4003 </papid></nextsent>
<nextsent>the heuristics proposed by vieira and poesio assumed parsed input (the penn treebank) and aimed at identifying five categories of dds licensed to occur as first mention on semantic or pragmatic grounds on the basis of work on definite ness including loebners account (1987): 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA258">
<title id=" W04-0707.xml">discourse new detectors for definite description resolution a survey and a preliminary proposal </title>
<section> larger situation definite descriptions.  </section>
<citcontext>
<prevsection>
<prevsent>two results are shown for the hand-coded decision tree:in one version, the system doesnt attempt to classify all dds; in the other, all unclassified dds are classified as discourse-new.
</prevsent>
<prevsent>version of the system r baseline 50.8 100 67.4 discourse-new detection only 69 72 70 hand-coded dt: partial 62 85 71.7 hand-coded dt: total 77 77 77 id3 75 75 75 table 1: overall results by vieira and poesio 2.2 bean and riloff.
</prevsent>
</prevsection>
<citsent citstr=" P99-1048 ">
bean and riloff (1999) <papid> P99-1048 </papid>developed system for identifying discourse-new dds1 that incorporates, in addition to syntax-based heuristics aimed at recognizing predicative and established dds using postmod ifi cation heuristics similar to those used by vieira and poesio, additional techniques for mining from corpora unfamiliar dds including proper names, larger situation, and semantically functional.</citsent>
<aftsection>
<nextsent>two 1bean and riloff use the term existential for these dds.
</nextsent>
<nextsent>of the techniques proposed by bean and riloff are particularly worth noticing.
</nextsent>
<nextsent>the sentence-one(s1) extraction heuristic identifies as discourse new every dd found in the first sentence of text.
</nextsent>
<nextsent>more general patterns can then be extracted from the dds initially found by s1-extraction, using the existential head pattern method which, e.g., would extract the n+ government from the salvadoran government and the guatemalan government.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA268">
<title id=" W04-0707.xml">discourse new detectors for definite description resolution a survey and a preliminary proposal </title>
<section> larger situation definite descriptions.  </section>
<citcontext>
<prevsection>
<prevsent>there is also consensus on the fact that dn detection cannot be isolated from anaphoric resolution (wit ness the ng and cardie results).
</prevsent>
<prevsent>one problem with some of the machine learning approaches to coreference is that these systems donot achieve very good results on pronoun and definite description resolution in comparison with specialized algorithms: e.g., although ng and cardiesbest version achieves f=65.8 on all anaphoric expressions, it only achieves f=29.6 for definite descriptions (cfr.
</prevsent>
</prevsection>
<citsent citstr=" J01-4003 ">
vieira and poesios best result off=77), and f=28.2 for pronouns (as opposed to results as high as f=80 obtained by the pronoun resolution algorithms evaluated in (tetreault, 2001)).<papid> J01-4003 </papid>clearly these systems can only be properly compared by evaluating them all on the same corpora and the same data, and discussion such as (mitkov, 2000) suggest caution in interpreting some of the results discussed in the literature as pre- and postprocessing often plays crucial role, but we feel that evaluating dn detectors in conjunction with high performing systems would give better idea of the improvements that one may hope to achieve.</citsent>
<aftsection>
<nextsent>3 do discourse-new detectors help?.
</nextsent>
<nextsent>preliminary evaluations vieira and poesio did not test their system without dn-detection, but ng and cardies results indicate that dn detection does improve results, if not dramatically, provided that the same_head test is run first although their dn detector does not appear to improve results for pronouns, the one category for which detection of non-anaphoricity has been shown to be essential (lappin and leass, 1994).<papid> J94-4002 </papid></nextsent>
<nextsent>in order to evaluate how much improvement can we expect by just improving the dn detector, we dida few preliminary evaluations both with reimple men tation of vieira and poesios algorithm which does not include discourse-new detector, running over treebank text as the original algorithm, andwith simple statistical coreference re solver attempting to resolve all anaphoric expressions and running over un parsed text, using uryupinas features for discourse-new detection, and over the same corpus used by ng and cardie (muc-7).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA269">
<title id=" W04-0707.xml">discourse new detectors for definite description resolution a survey and a preliminary proposal </title>
<section> larger situation definite descriptions.  </section>
<citcontext>
<prevsection>
<prevsent>vieira and poesios best result off=77), and f=28.2 for pronouns (as opposed to results as high as f=80 obtained by the pronoun resolution algorithms evaluated in (tetreault, 2001)).<papid> J01-4003 </papid>clearly these systems can only be properly compared by evaluating them all on the same corpora and the same data, and discussion such as (mitkov, 2000) suggest caution in interpreting some of the results discussed in the literature as pre- and postprocessing often plays crucial role, but we feel that evaluating dn detectors in conjunction with high performing systems would give better idea of the improvements that one may hope to achieve.</prevsent>
<prevsent>3 do discourse-new detectors help?.</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
preliminary evaluations vieira and poesio did not test their system without dn-detection, but ng and cardies results indicate that dn detection does improve results, if not dramatically, provided that the same_head test is run first although their dn detector does not appear to improve results for pronouns, the one category for which detection of non-anaphoricity has been shown to be essential (lappin and leass, 1994).<papid> J94-4002 </papid></citsent>
<aftsection>
<nextsent>in order to evaluate how much improvement can we expect by just improving the dn detector, we dida few preliminary evaluations both with reimple men tation of vieira and poesios algorithm which does not include discourse-new detector, running over treebank text as the original algorithm, andwith simple statistical coreference re solver attempting to resolve all anaphoric expressions and running over un parsed text, using uryupinas features for discourse-new detection, and over the same corpus used by ng and cardie (muc-7).
</nextsent>
<nextsent>3.1 how much does dn-detection help the.
</nextsent>
<nextsent>vieira / poesio algorithm?
</nextsent>
<nextsent>guitar (poesio and alexandrov-kabadjov, 2004)is general-purpose anaphoric re solver that includes an implementation of the vieira / poesio algorithm for definite descriptions and of mitkovs algorithm for pronoun resolution (mitkov, 1998).<papid> P98-2143 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA270">
<title id=" W04-0707.xml">discourse new detectors for definite description resolution a survey and a preliminary proposal </title>
<section> larger situation definite descriptions.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 how much does dn-detection help the.
</prevsent>
<prevsent>vieira / poesio algorithm?
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
guitar (poesio and alexandrov-kabadjov, 2004)is general-purpose anaphoric re solver that includes an implementation of the vieira / poesio algorithm for definite descriptions and of mitkovs algorithm for pronoun resolution (mitkov, 1998).<papid> P98-2143 </papid></citsent>
<aftsection>
<nextsent>it is implemented in java, takes its input in xml format and returns as output its input augmented with the anaphoric relations it has discovered.
</nextsent>
<nextsent>guitar hasbeen implemented in such way as to be fully modular, making it possible, for example, to replace thedd resolution method with alternative implementations.
</nextsent>
<nextsent>it includes pre-processor incorporating chunker so that it can run over both hand-parsed and raw text.
</nextsent>
<nextsent>a version of guitar without the dn detection aspects of the vieira / poesio algorithm was evaluated on the gnome corpus (poesio, 2000; poesio et al., 2004), <papid> J04-3003 </papid>which contains 554 definite descriptions,of which 180 anaphoric, and 305 third-person pronouns, of which 217 anaphoric.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA271">
<title id=" W04-0707.xml">discourse new detectors for definite description resolution a survey and a preliminary proposal </title>
<section> larger situation definite descriptions.  </section>
<citcontext>
<prevsection>
<prevsent>guitar hasbeen implemented in such way as to be fully modular, making it possible, for example, to replace thedd resolution method with alternative implementations.
</prevsent>
<prevsent>it includes pre-processor incorporating chunker so that it can run over both hand-parsed and raw text.
</prevsent>
</prevsection>
<citsent citstr=" J04-3003 ">
a version of guitar without the dn detection aspects of the vieira / poesio algorithm was evaluated on the gnome corpus (poesio, 2000; poesio et al., 2004), <papid> J04-3003 </papid>which contains 554 definite descriptions,of which 180 anaphoric, and 305 third-person pronouns, of which 217 anaphoric.</citsent>
<aftsection>
<nextsent>the results for definite descriptions over hand-parsed text are shown in table 6.
</nextsent>
<nextsent>total res corr nm wm sm p 180 182 121 43 16 45 67.2 66.5 66.8 table 6: evaluation of the guitar system without dn detection over hand-annotated treebank guitar without dn recognizer takes 182 dds(res) as anaphoric, resolving 121 of them correctly (corr); of the 182 dds it attempts to resolve, only 16 are incorrectly resolved (wm); almost three times that number (45) are spurious matches (sm), i.e., discourse-new dds incorrectly interpreted as anaphoric.
</nextsent>
<nextsent>(res=corr+wm+sm.)
</nextsent>
<nextsent>the system cant find an antecedent for 43 of the 180 anaphoric dds.when endowed with perfect dn detector, guitar could achieve precision p=88.3 which, assuming recall stays the same (r=67.2) would mean f=76.3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA274">
<title id=" W04-2318.xml">prosodic cues to discourse segment boundaries in human computer dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>contemporary theories of discourse, both computational and descriptive, postulate tree-structured hierarchical model of discourse.
</prevsent>
<prevsent>these structures may be viewed as corresponding tointentional?
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
structure of discourse segment purposes in the view of (grosz and sidner, 1986), <papid> J86-3001 </papid>to plan and subplan structure directly in the view of(allen and litman, 1990) , to nuclei and satellite rhetorical relations in the rhetorical structure theory of (mann and thompson, 1987), or to information structures as in (traum and hinkelman, 1992).</citsent>
<aftsection>
<nextsent>despite this diversity ofviews on the sources of structural organization, these theories agree on the decomposition of discourse into segments and sub segments in hierarchical structure.discourse segments help to establish the domain of interpretation for referents or anaphors.
</nextsent>
<nextsent>(grosz, 1977) discourse segmentation can also provide guidance for summarization or retrieval by identifying the topical structure of extended text spans.
</nextsent>
<nextsent>as result, an understanding of the mechanisms that signal discourse structure is highly desirable.
</nextsent>
<nextsent>while substantial work has been done on identifying and automatically recognizing the textual and prosodic correlates of discourse structure in monologue, comparable cues for dialogue or multi-party conversation, and in particular human-computer dialogue remain relatively less studied.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA275">
<title id=" W04-2318.xml">prosodic cues to discourse segment boundaries in human computer dialogue </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally we conclude and present some future work.
</prevsent>
<prevsent>cues for and automatic segmentation of discourse structure have been most extensively studied for written and spoken monologue.
</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
for written narrative, discourse segment boundaries have been identified based on textual topic similarity with variety of approaches based on hearsts textiling(hearst, 1994).<papid> P94-1002 </papid></citsent>
<aftsection>
<nextsent>more complex rheto rial structure theory trees have also been extracted based heavily on cue phrases and discourse markers by (marcu, 2000).
</nextsent>
<nextsent>in spoken monologue, prosodic cues to discourse structure and segmentation have been explored by (nakatani et al, 1995; swerts, 1997).
</nextsent>
<nextsent>increases in pitch range, amplitude, and silence duration appear to signal discourse segment boundaries across different domains - voicemail, broadcast news, descriptive narrative - and across different languages, such as english and dutch.comparable prosodic cues have been applied to there lated task of news story segmentation, in conjunction with textual cues to topicality, by (tur et al, 2001), <papid> J01-1002 </papid>where large pitch differences between pre- and post- boundary positions play the most significant role among prosodic cues.in spoken dialogue, research has focused on the identification of dialogue acts and dialogue games.</nextsent>
<nextsent>integration of textual and prosodic cues, such as particular pitch accent or contour types, have been found useful for identifying act type(shriberg et al, 1998; taylor et al, 1998).specific classes of dialogue act, such as corrections (re quest repair), have received particular interest in work by (levow, 1998; <papid> P98-1122 </papid>swerts et al, 2000) in the context of human-computer error resolution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA276">
<title id=" W04-2318.xml">prosodic cues to discourse segment boundaries in human computer dialogue </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>more complex rheto rial structure theory trees have also been extracted based heavily on cue phrases and discourse markers by (marcu, 2000).
</prevsent>
<prevsent>in spoken monologue, prosodic cues to discourse structure and segmentation have been explored by (nakatani et al, 1995; swerts, 1997).
</prevsent>
</prevsection>
<citsent citstr=" J01-1002 ">
increases in pitch range, amplitude, and silence duration appear to signal discourse segment boundaries across different domains - voicemail, broadcast news, descriptive narrative - and across different languages, such as english and dutch.comparable prosodic cues have been applied to there lated task of news story segmentation, in conjunction with textual cues to topicality, by (tur et al, 2001), <papid> J01-1002 </papid>where large pitch differences between pre- and post- boundary positions play the most significant role among prosodic cues.in spoken dialogue, research has focused on the identification of dialogue acts and dialogue games.</citsent>
<aftsection>
<nextsent>integration of textual and prosodic cues, such as particular pitch accent or contour types, have been found useful for identifying act type(shriberg et al, 1998; taylor et al, 1998).specific classes of dialogue act, such as corrections (re quest repair), have received particular interest in work by (levow, 1998; <papid> P98-1122 </papid>swerts et al, 2000) in the context of human-computer error resolution.</nextsent>
<nextsent>recent work on the icsi multi-party meeting recorder data has demonstrated some very preliminary results on multi-party segmentation (galley et al, 2003); <papid> P03-1071 </papid>prosodic information in this case was limited to silence duration.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA277">
<title id=" W04-2318.xml">prosodic cues to discourse segment boundaries in human computer dialogue </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in spoken monologue, prosodic cues to discourse structure and segmentation have been explored by (nakatani et al, 1995; swerts, 1997).
</prevsent>
<prevsent>increases in pitch range, amplitude, and silence duration appear to signal discourse segment boundaries across different domains - voicemail, broadcast news, descriptive narrative - and across different languages, such as english and dutch.comparable prosodic cues have been applied to there lated task of news story segmentation, in conjunction with textual cues to topicality, by (tur et al, 2001), <papid> J01-1002 </papid>where large pitch differences between pre- and post- boundary positions play the most significant role among prosodic cues.in spoken dialogue, research has focused on the identification of dialogue acts and dialogue games.</prevsent>
</prevsection>
<citsent citstr=" P98-1122 ">
integration of textual and prosodic cues, such as particular pitch accent or contour types, have been found useful for identifying act type(shriberg et al, 1998; taylor et al, 1998).specific classes of dialogue act, such as corrections (re quest repair), have received particular interest in work by (levow, 1998; <papid> P98-1122 </papid>swerts et al, 2000) in the context of human-computer error resolution.</citsent>
<aftsection>
<nextsent>recent work on the icsi multi-party meeting recorder data has demonstrated some very preliminary results on multi-party segmentation (galley et al, 2003); <papid> P03-1071 </papid>prosodic information in this case was limited to silence duration.</nextsent>
<nextsent>with the exception of work on error resolution, most work on dialogue has focused human-human interaction and on identification of particular act or game types.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA278">
<title id=" W04-2318.xml">prosodic cues to discourse segment boundaries in human computer dialogue </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>increases in pitch range, amplitude, and silence duration appear to signal discourse segment boundaries across different domains - voicemail, broadcast news, descriptive narrative - and across different languages, such as english and dutch.comparable prosodic cues have been applied to there lated task of news story segmentation, in conjunction with textual cues to topicality, by (tur et al, 2001), <papid> J01-1002 </papid>where large pitch differences between pre- and post- boundary positions play the most significant role among prosodic cues.in spoken dialogue, research has focused on the identification of dialogue acts and dialogue games.</prevsent>
<prevsent>integration of textual and prosodic cues, such as particular pitch accent or contour types, have been found useful for identifying act type(shriberg et al, 1998; taylor et al, 1998).specific classes of dialogue act, such as corrections (re quest repair), have received particular interest in work by (levow, 1998; <papid> P98-1122 </papid>swerts et al, 2000) in the context of human-computer error resolution.</prevsent>
</prevsection>
<citsent citstr=" P03-1071 ">
recent work on the icsi multi-party meeting recorder data has demonstrated some very preliminary results on multi-party segmentation (galley et al, 2003); <papid> P03-1071 </papid>prosodic information in this case was limited to silence duration.</citsent>
<aftsection>
<nextsent>with the exception of work on error resolution, most work on dialogue has focused human-human interaction and on identification of particular act or game types.
</nextsent>
<nextsent>herewe concentrate on the general question of discourse segmentation in voice-only human-computer interaction.
</nextsent>
<nextsent>we ask whether the cues to segment structure identified for monologue are robust to the change in number and type of conversational participant.
</nextsent>
<nextsent>3.1 speech system description.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA279">
<title id=" W04-0830.xml">the university of jaen word sense disambiguation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have used the learning vector quant ization, which is supervised learning algorithm based on the kohonen neural model.
</prevsent>
<prevsent>our system for senseval-3 uses supervised learning algorithm for word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
the method suggested trains neural network using the learning vector quant ization (lvq) algorithm, integrating several semantic relations of wordnet (fellbaum, 1998) and semcor corpus (miller et al, 1993).<papid> H93-1061 </papid></citsent>
<aftsection>
<nextsent>the university of jan system has been used in english-lexical-sample and eng lish-all-words tasks.
</nextsent>
<nextsent>the presented disambiguator uses the vector space model (vsm) as an information representation model.
</nextsent>
<nextsent>each sense of word is represented as vector in an n-dimensional space where is the number of words in all its contexts.
</nextsent>
<nextsent>the accuracy of the disambiguator depends essentially on the word weights.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA280">
<title id=" W03-2114.xml">managing dialogue interaction a multilayered approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>environment-level processes fill in the detail of these goals and handle contingencies which may otherwise prevent the achievement of these goals.
</prevsent>
<prevsent>a number of interaction-management techniques are present in the current implementation, including:   back-up recognition pass, using statistical processing to extend grammar-based coverage and provide immediate user help?
</prevsent>
</prevsection>
<citsent citstr=" E03-1075 ">
feedback for unrecognized utterances (hockey et al , 2003);   <papid> E03-1075 </papid>turn management timing of system output is governed by monitoring the speech channel and the (prioritized) agenda of speech outputs.</citsent>
<aftsection>
<nextsent>if the system need to take the turn, it grabs it using only low-level processing;   handling user barge-inuser speech interrupts system output and automatically grabs the turn;  immediate grounding of recognized commands (e.g. system says ok? immediately after recognizing the user: fly to the tower?);   np selection ? choosing anaphoric or salient noun-phrases at the point of generation;   incremental aggregation of system-generated utterances ? appropriately condensing and forming elliptical system output at the point of generation.
</nextsent>
<nextsent>while this accounts for only small number of signals that arise during natural dialogue, the architecture provides framework for incorporating further technique sin particular, using shallow 2note: we are talking about very different parallel thread shere than those which occur in multi-modal fusion, such as occurs in the smartkom (wahlster, 2002) system.processingfor making use of such signals to provide more natural and robust interactions between dialogue systems and human participants.in the next section, we describe work from the linguistic and psychology literature that demonstrates the importance of asynchronous interaction-levelprocessing.
</nextsent>
<nextsent>in section 3, we propose specific architecture that provides framework for integrating various processes for channel-management.
</nextsent>
<nextsent>in sections 4 and 5, we describe specifics of the csliimplementation, outlining first the more abstract dialogue management layer, followed by techniques employed at the interaction layer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA281">
<title id=" W03-2114.xml">managing dialogue interaction a multilayered approach </title>
<section> a two-level architecture </section>
<citcontext>
<prevsection>
<prevsent>for example thelow level includes turn manager which manipulates the speech channel to ensure that:   user inputs are respected without interruption (except when necessary);   turn passes to the appropriate participant, basedon the highest priority agenda item and the dialogue move that generated it;   generated outputs are natural and timely;   recognized user inputs are acknowledged quickly using simple feedback utterances.
</prevsent>
<prevsent>the upper level is responsible for modeling other aspects of the conversational context, as well as communicative goals and intentions.
</prevsent>
</prevsection>
<citsent citstr=" W02-0216 ">
the content (i.e. logical forms) of user utterances are processed using the dialogue model (e.g. updates and adding nodes to the dialogue move tree (lemon et al ., 2002<papid> W02-0216 </papid>b)), and system utterances are constructed which are in line with the systems communicative dialogue move tree activity model context mgr conversation planner agent - intentions - goals - plans - observations content layer: - utterance planning - communicative intentions - grounding - content management - interaction with agent arch speech recogition and parsing backup shallow processor (helper) speech channel turn mgr tts generation module output agenda attention monitor interaction layer - timing - form - engagement - acknowledgement generation: - anaphora - pronouns - aggregation - echoing ack figure 2: system architecture goals and intentions, whether they be imparting information to the user or requesting clarification or further information.</citsent>
<aftsection>
<nextsent>the higher level also interacts with the rest of the agent architecture, mediated by an activity model (i.e. representation of the agent activities about which dialogue may occur (gruenstein, 2002)).
</nextsent>
<nextsent>the agent may wish to communicate its own goals, the progress of its activities, or report on any observations it makes regarding its environment.
</nextsent>
<nextsent>as with multi-layered agent architectures, thetwo levels operate semi-autonomously and asynchronously: the lower level is driven by tight interaction with the user, while the upper level is driven by longer-range communicative goals fromits activities and responses to user utterances.
</nextsent>
<nextsent>how ever, various types of information exchange connect the two levels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA282">
<title id=" W03-2114.xml">managing dialogue interaction a multilayered approach </title>
<section> top-level context management.  </section>
<citcontext>
<prevsection>
<prevsent>in the rest of the paper, we discuss our dialogue management architecture and, in particular, the techniques employed so far at each of the two levels described here to enhance user experience and improve overall system performance.
</prevsent>
<prevsent>the current implementation based on the above architecture is still being refined; we focus on the features that have already been implemented.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
the approach to dialogue modeling we have implemented is based on the theory of dialogue games (carlson, 1983; power, 1979), and, for task-oriented dialogues, discourse segments (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>these accounts relyon the observation that answers generally follow questions, commands are usually acknowledged, and so on, so that dialogues can be partially described as consisting of adjacencypairs of such dialogue moves.
</nextsent>
<nextsent>the notion of attach ment?
</nextsent>
<nextsent>of dialogue moves on dialogue move tree (dmt) (lemon et al , 2002<papid> W02-0216 </papid>b) embodies this idea.an activity tree represents hierarchical and temporal information about the task-state of the dia logue.</nextsent>
<nextsent>activities are the joint tasks managed by the dialogue: e.g. booking flight or moving robot?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA288">
<title id=" W04-0110.xml">segment predictability as a cue in word segmentation application to modern greek </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>computational simulations of word segmentation have also focused mainly on data from english corpora, and should also be extended to cover broader range of the corpora available.
</prevsent>
<prevsent>the line of research proposed here is twofold: on the one hand we wish to understand the nature of the cues present in modern greek, on the other we wish to establish framework for orderly comparison of word segmentation algorithms across the desired broad range of languages.
</prevsent>
</prevsection>
<citsent citstr=" W98-0905 ">
finite-state techniques, used by e.g., belz (1998) <papid> W98-0905 </papid>in modeling phonotactic constraints and syllable within various languages, provide one straightforward way to formulate some of these comparisons, and may be useful in future testing of multiple cues.</citsent>
<aftsection>
<nextsent>previous research (rytting, 2004) <papid> N04-2008 </papid>examined the role of utterance-boundary information in modern greek, implementing variant of aslin and colleagues?</nextsent>
<nextsent>(1996) model within finite-state framework.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA289">
<title id=" W04-0110.xml">segment predictability as a cue in word segmentation application to modern greek </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the line of research proposed here is twofold: on the one hand we wish to understand the nature of the cues present in modern greek, on the other we wish to establish framework for orderly comparison of word segmentation algorithms across the desired broad range of languages.
</prevsent>
<prevsent>finite-state techniques, used by e.g., belz (1998) <papid> W98-0905 </papid>in modeling phonotactic constraints and syllable within various languages, provide one straightforward way to formulate some of these comparisons, and may be useful in future testing of multiple cues.</prevsent>
</prevsection>
<citsent citstr=" N04-2008 ">
previous research (rytting, 2004) <papid> N04-2008 </papid>examined the role of utterance-boundary information in modern greek, implementing variant of aslin and colleagues?</citsent>
<aftsection>
<nextsent>(1996) model within finite-state framework.
</nextsent>
<nextsent>the present paper examines more closely the proposed cue of segment predictability.
</nextsent>
<nextsent>these two studies lay the groundwork for examining the relative worth of various cues, separately and as an ensemble.
</nextsent>
<nextsent>1.1 infant studies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA295">
<title id=" W04-2703.xml">annotating discourse connectives and their arguments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we provide detailed preliminary analysis of inter-annotator agreement ? both the level of agreement and the types of inter-annotator variation.
</prevsent>
<prevsent>large scale annotated corpora have played critical rolein speech and natural language research.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the penn treebank (ptb) is an example of such resource with world wide impact on natural language processing (marcus et al., 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>however, the ptb deals with text only atthe sentence level: with the demand for more powerful nlp applications comes need for greater richness in annotation.
</nextsent>
<nextsent>at the sentence level, penn propbank is adding predicate-argument annotation to sentences inptb (kingsbury and palmer, 2002).
</nextsent>
<nextsent>at the discourse level are efforts to produce corpora annotated with rhetorical relations (carlson et al, 2003).
</nextsent>
<nextsent>this paper describes more basic discourse-level annotation project ? the penn discourse treebank (pdtb) ? that aims to produce large-scale corpus in which discourse connectives are annotated, along with their arguments.there have been several approaches to describing discourse in terms of discourse relations (mann and thompson, 1988; asher and lascarides, 1998; polanyi and vanden berg, 1996).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA296">
<title id=" W04-2703.xml">annotating discourse connectives and their arguments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in these approaches, the additional meaning the discourse contributes beyond the sentence derives from discourse relations.
</prevsent>
<prevsent>specification of the discourse relations for discourse thus constitutes description of certain level of discourse structure.rather than starting from (abstract) discourse relations, we describe an approach to annotating large scale corpus in terms of more basic characterisation of discourse structure in terms of discourse connective sand their arguments.
</prevsent>
</prevsection>
<citsent citstr=" W98-0315 ">
the motivation for such an approach stems from work by webber and joshi (1998),<papid> W98-0315 </papid>webber et al (1999<papid> P99-1006 </papid>a), webber et al (2000) which integrates sentence level structures with discourse level structure (using tree-adjoining grammars for both cases, ltagand dltag, respectively).1 this allows structural composition and its associated semantic composition at the sentence level to be smoothly carried over to the discourse level, goal also shared by gardent (1997), schilder (1997) and polanyi and vanden berg (1996), among others.2discourse connectives and their arguments can be successfully annotated with high reliability (cf.</citsent>
<aftsection>
<nextsent>section4).
</nextsent>
<nextsent>this is not surprising, given that the task resembles that of annotating verbs and their arguments at the sentence level (kingsbury and palmer, 2002).
</nextsent>
<nextsent>infact, we use fine-grained, lexically grounded annotation in which argument labels are specific to the dis 1in the pdtb annotations, we have deliberately adopted policy to make the annotations independent of the dltag framework for two reasons: (1) to make the annotated corpus widely useful to researchers working in different frameworks and (2) to make the annotators?
</nextsent>
<nextsent>task easier, thereby increasing inter annotator reliability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA297">
<title id=" W04-2703.xml">annotating discourse connectives and their arguments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in these approaches, the additional meaning the discourse contributes beyond the sentence derives from discourse relations.
</prevsent>
<prevsent>specification of the discourse relations for discourse thus constitutes description of certain level of discourse structure.rather than starting from (abstract) discourse relations, we describe an approach to annotating large scale corpus in terms of more basic characterisation of discourse structure in terms of discourse connective sand their arguments.
</prevsent>
</prevsection>
<citsent citstr=" P99-1006 ">
the motivation for such an approach stems from work by webber and joshi (1998),<papid> W98-0315 </papid>webber et al (1999<papid> P99-1006 </papid>a), webber et al (2000) which integrates sentence level structures with discourse level structure (using tree-adjoining grammars for both cases, ltagand dltag, respectively).1 this allows structural composition and its associated semantic composition at the sentence level to be smoothly carried over to the discourse level, goal also shared by gardent (1997), schilder (1997) and polanyi and vanden berg (1996), among others.2discourse connectives and their arguments can be successfully annotated with high reliability (cf.</citsent>
<aftsection>
<nextsent>section4).
</nextsent>
<nextsent>this is not surprising, given that the task resembles that of annotating verbs and their arguments at the sentence level (kingsbury and palmer, 2002).
</nextsent>
<nextsent>infact, we use fine-grained, lexically grounded annotation in which argument labels are specific to the dis 1in the pdtb annotations, we have deliberately adopted policy to make the annotations independent of the dltag framework for two reasons: (1) to make the annotated corpus widely useful to researchers working in different frameworks and (2) to make the annotators?
</nextsent>
<nextsent>task easier, thereby increasing inter annotator reliability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA302">
<title id=" W04-2703.xml">annotating discourse connectives and their arguments </title>
<section> theoretical background.  </section>
<citcontext>
<prevsection>
<prevsent>section 4 presents data analysis based on current annotations as well as results from inter-annotator agreement.
</prevsent>
<prevsent>section 5 wraps up with summary of the work.
</prevsent>
</prevsection>
<citsent citstr=" J03-4002 ">
the annotation project presented in this paper builds on basic ideas presented in webber and joshi (1998),<papid> W98-0315 </papid> webber et al (1999<papid> P99-1006 </papid>b) and webber et al (2003) <papid> J03-4002 </papid>that connectives are discourse-level predicates which project predicate-argument structure on par with verbs at the sentence level.</citsent>
<aftsection>
<nextsent>webber and joshi (1998) <papid> W98-0315 </papid>propose tree adjoining grammar for discourse (dltag) in which compositional aspects of discourse meaning are formally defined, thus teasing apart compositional from non compositional layers of meaning.</nextsent>
<nextsent>in this framework, connectives are grouped into natural classes depending on the structure that they project at the discourse level.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA308">
<title id=" W04-2703.xml">annotating discourse connectives and their arguments </title>
<section> data analysis.  </section>
<citcontext>
<prevsection>
<prevsent>arguments are shown in square brackets, while spans providing supplementary information are shown in parentheses.
</prevsent>
<prevsent>(15) although [started in 1965], [wedtech didnt really get rolling until 1975] (when mr. neuberger discovered the federal governments section 8 minority business pro gram).
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
to test the reliability of the annotation, we first considered the kappa statistic (siegel and castel lan, 1988)which is used extensively in empirical studies of discourse (carletta, 1996).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>the kappa coefficient provide san inter-annotator agreement figure for any number of annotators by measuring pairwise agreement between themand by correcting for chance expected agreement.
</nextsent>
<nextsent>how ever, the statistic requires the data tokens to be classified into discrete categories, and as result, we could not apply it to our data since the pdtb annotation tokens can not be classified as such.
</nextsent>
<nextsent>rather, annotation in the pdtbconstitutes either selection of span of text for the arguments of connectives which can be of indeterminate length or providing explicit expressions for implicit connectives from an open-ended class of expressions.
</nextsent>
<nextsent>8for preliminary corpus-based analysis of the arguments of instead?, see miltsakaki et al (2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA309">
<title id=" W04-0710.xml">reference resolution over a restricted domain references to documents </title>
<section> challenges of reference resolution over.  </section>
<citcontext>
<prevsection>
<prevsent>section 7 outlines the applications of the ref2doc algorithm to the exploitation of documents in meeting processing and retrieval applications.
</prevsent>
<prevsent>a restricted domain from cognitive point of view, the role of referring expressions in discourse is to specify the entities about which the speaker talks.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
it has long been observed that more accurate view is that res rather specify representations of entities in the speakers or hearers mind, an abstraction called discourse entities or des (sidner, 1983; grosz et al, 1995)<papid> J95-2003 </papid>reference resolution can be defined as the construction of the discourse entities specified by referring expressions, or rather, the construction of computational representations of des.</citsent>
<aftsection>
<nextsent>this difficult but important task in discourse understanding by computers appears to be more tractable when enough knowledge about domain is available to system (gaizauskas and humphreys, 1997), orwhen the representations are considerably simplified (popescu-belis et al, 1998).
</nextsent>
<nextsent>the coreference and anaphoric links, that is,links between res only, are somewhat different aspects of the phenomenon of reference (devitt andsterelny, 1999; lycan, 2000).
</nextsent>
<nextsent>coreference is there lation between two res that specify the same de.anaphora is relation between two res, called antecedent re and anaphoric re, where the de specified by the latter is determined by knowledge of the de specified by the former.
</nextsent>
<nextsent>in other terms, the despecified by the anaphoric re cannot be fully determined without knowledge of the antecedent re.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA311">
<title id=" W04-0710.xml">reference resolution over a restricted domain references to documents </title>
<section> challenges of reference resolution over.  </section>
<citcontext>
<prevsection>
<prevsent>reference resolution in restricted domain presents similarities with problems in natural language generation (nlg) and in command dialogs,that is, when the sets of referents are known priori to the system.
</prevsent>
<prevsent>in nlg, the problem is to generate res from existing computational descriptions of entities see paraboni and van deemter (2002) for an application to intra-document references.
</prevsent>
</prevsection>
<citsent citstr=" J95-1003 ">
in command dialogs, the problem is to match theres produced by the user against the objects managed by the interface, again known formally to the system (huls et al, 1995; <papid> J95-1003 </papid>skantze, 2002).</citsent>
<aftsection>
<nextsent>ref2doc system 3.1 overview.
</nextsent>
<nextsent>within the overall goal of fully automated understanding of references to documents in meeting dialogs, several related sub-tasks can be distinguished, most simply envisaged as separate processes in computational architecture: 1.
</nextsent>
<nextsent>generate transcript of the utterances pro-.
</nextsent>
<nextsent>duced by each speaker.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA313">
<title id=" W04-2607.xml">non classical lexical semantic relations </title>
<section> theoretical background.  </section>
<citcontext>
<prevsection>
<prevsent>(halliday and hasan, 1976, p. 320, emphasis added).
</prevsent>
<prevsent>lexical semantic relations are the building blocks of lexical cohesion ,and so clear understanding of their nature and behavior is crucial.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
lexical cohesion analysis has been used in such nlp applications as determining the structure oftext (morris and hirst, 1991) <papid> J91-1002 </papid>and automatic text summarization (barzilay and elhadad, 1999).</citsent>
<aftsection>
<nextsent>in recent lexical cohesion research in linguistics (hasan, 1984; halliday and hasan, 1989; martin, 1992) non-classical relations are largely ignored, and the sameis true in implementations of lexical cohesion in computational linguistics (barzilay and elhadad, 1999; silber and mccoy, 2002), <papid> J02-4004 </papid>as the lexical resource used is wordnet.</nextsent>
<nextsent>it is notable, however, that the original view of lexical semantic relations in the lexical cohesion work of halliday and hasan (1976) was very broad and general; the only criterion was that there had to be are cognizable relation between two words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA314">
<title id=" W04-2607.xml">non classical lexical semantic relations </title>
<section> theoretical background.  </section>
<citcontext>
<prevsection>
<prevsent>lexical semantic relations are the building blocks of lexical cohesion ,and so clear understanding of their nature and behavior is crucial.
</prevsent>
<prevsent>lexical cohesion analysis has been used in such nlp applications as determining the structure oftext (morris and hirst, 1991) <papid> J91-1002 </papid>and automatic text summarization (barzilay and elhadad, 1999).</prevsent>
</prevsection>
<citsent citstr=" J02-4004 ">
in recent lexical cohesion research in linguistics (hasan, 1984; halliday and hasan, 1989; martin, 1992) non-classical relations are largely ignored, and the sameis true in implementations of lexical cohesion in computational linguistics (barzilay and elhadad, 1999; silber and mccoy, 2002), <papid> J02-4004 </papid>as the lexical resource used is wordnet.</citsent>
<aftsection>
<nextsent>it is notable, however, that the original view of lexical semantic relations in the lexical cohesion work of halliday and hasan (1976) was very broad and general; the only criterion was that there had to be are cognizable relation between two words.
</nextsent>
<nextsent>most research on lexical semantic relations in linguistics(cruse, 1986) and psychology has also ignored non classical relations (with the exception of chaffin and herrmann, 1984); however there have been recent calls to broaden the focus and include non-classical relations as well (mcrae and boisvert, 1998; hodgson, 1991).a notable exception to this trend is in library and information science (lis), and is likely pragmatic reflection of the fact that it is field with large user base that demanded this type of access to reference materials.
</nextsent>
<nextsent>in lis thesauri, most of the word pairs that are classed as related terms (rts) are related non-classically, but unfortunately are listed as an undifferentiated group.
</nextsent>
<nextsent>standards for their use have been developed (iso, 1986); but since 1985, the library of congress has been encouraging minimization of their use (el-hoshy,2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA317">
<title id=" W04-2607.xml">non classical lexical semantic relations </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this work would incorporate and build on the related ideas discussed above of cruse (1986), hasan (1984), and barsalou (1989), along with the actual relation types and word group interactions found by readers.
</prevsent>
<prevsent>we are also interested in how text-specific the word groups and relations are, since nontext-specific information can be added to existing resources, but text specific knowledge will require further complex interaction with the rest of the text.
</prevsent>
</prevsection>
<citsent citstr=" W97-0713 ">
we intend to investigate any potential linkages between the word groups in the texts and other theories that provide pre-determined structures of text, such as rhetorical structure theory (marcu, 1997).<papid> W97-0713 </papid></citsent>
<aftsection>
<nextsent>it will also be useful for computational purposes to have clearer understanding of what aspects of text understanding exist in it?
</nextsent>
<nextsent>and what can be expected to contribute to subjectivity of interpretation or individual differences incomprehension.
</nextsent>
<nextsent>acknowledgments this research was supported by grant and scholarship from the natural sciences and engineering research council of canada.
</nextsent>
<nextsent>we are grateful to clare beghtol for ongoing comments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA318">
<title id=" W04-0837.xml">using automatically acquired predominant senses for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for accurate wsd the first sense heuristic should be used only as back-off, where the evidence from the context is not strong enough.
</prevsent>
<prevsent>in this paper however, we examine the performance of the automatically acquired first sense in isolation since it turned out that the first sense taken from semcor outperformed many systems in senseval-2.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
the first sense heuristic which is often used as baseline for supervised wsd systems outperforms many of these systems which take surrounding context into account (mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>the high performance of the first sense baseline is due to the skewed frequency distribution of word senses.
</nextsent>
<nextsent>even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (hoste et al, 2001).the first sense heuristic is powerful one.
</nextsent>
<nextsent>using the first sense listed in semcor on the senseval-2 english all-words data we obtained there sults given in table 1, (where the pos was given bythe gold-standard data in the senseval-2 data it self).
</nextsent>
<nextsent>1 recall is lower than precision because there are many words which do not occur in semcor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA321">
<title id=" W04-0837.xml">using automatically acquired predominant senses for word sense disambiguation </title>
<section> english all-words data.  </section>
<citcontext>
<prevsection>
<prevsent>2 method.
</prevsent>
<prevsent>the method is described in (mccarthy et al, 2004), <papid> P04-1036 </papid>which we summarise here.</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
we acquire thesauruses for nouns, verbs, adjectives and adverbs based onthe method proposed by lin (1998) <papid> P98-2127 </papid>using grammatical relations output from the rasp parser (briscoe and carroll, 2002).</citsent>
<aftsection>
<nextsent>the grammatical contexts used are listed in table 3, but there is scope for extending or restricting the contexts forgiven pos.
</nextsent>
<nextsent>we use the thesauruses for ranking the senses of the target words.
</nextsent>
<nextsent>each target word (  ) e.g. plant in the thesaurus is associated with list of nearest pos grammatical contexts noun verb in direct object or subject relation adjective or noun modifier verb noun as direct object or subject adjective modified noun, modifing adverb adverb modified adjective or verb table 3: grammatical contexts used for acquiring the thesauruses neighbours (   ) with distributional similarity scores (    ) e.g. factory 0.28, refinery 0.17,tree 0.14 etc...
</nextsent>
<nextsent>3 distributional similarity is measure indicating the degree that two words, word and its neighbour, occur in similar contexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA322">
<title id=" W04-0837.xml">using automatically acquired predominant senses for word sense disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>system precision recall autops 49 43 autopsnvs 56 35 table 5: using the automatically acquired first sense on the senseval-3 english all-words data in germanet for specific domains using the words in given synset, and those related by hyponymy,and term relevance measure taken from information retrieval.
</prevsent>
<prevsent>buitelaar and bogdan have evaluated their method on identifying domain specific concepts, rather than for wsd.
</prevsent>
</prevsection>
<citsent citstr=" J04-1003 ">
in recent work, la pata and brew (2004) <papid> J04-1003 </papid>obtain predominant senses of verbs occurring in subcategorization frames, where the senses of verbs are defined using levin classes (levin, 1993).</citsent>
<aftsection>
<nextsent>they demonstrate that these priors are useful for wsd of verbs.
</nextsent>
<nextsent>our ranking method is related to work by pantel and lin (2002) who use automatic thesauruses for discovering word senses from corpora, rather than for detecting predominance.
</nextsent>
<nextsent>in their work, the lists of neighbours are themselves clustered to bring outthe various senses of the word.
</nextsent>
<nextsent>they evaluate using wordnet similarity measure to determine the precision and recall of these discovered classes with respect to wordnet synsets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA323">
<title id=" W04-0837.xml">using automatically acquired predominant senses for word sense disambiguation </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>we have demonstrated that it is possible to acquire predominant senses from raw textual corpora, and that these can be used as an unsupervised first sense heuristic that does not not relyon manually produced corpora such as semcor.
</prevsent>
<prevsent>this approach is useful for words where there is no manually-tagged data available.
</prevsent>
</prevsection>
<citsent citstr=" W04-0861 ">
our predominant senses have been used within wsd system as back-off method when data is not available from other resources (villarejo et al, 2004).<papid> W04-0861 </papid></citsent>
<aftsection>
<nextsent>the method could be particularly useful when tailoring wsd system to particular domain.we intend to experiment further using wider variety of grammatical relations, which we hope will improve performance for verbs, and with data from larger corpora, such as the gigaword corpus and the web, which should allow us to cover great many more words which do not occur in manually created resources such as semcor.
</nextsent>
<nextsent>we also intend to apply our method to domain specific text.
</nextsent>
<nextsent>acknowledgements we would like to thank siddharth patwardhan andted pedersen for making the wn similarity package publically available.
</nextsent>
<nextsent>this work was funded by eu-2001-34460 project meaning: developing multilingual web-scale language technologies, and uk epsrc project robust accurate statistical parsing (rasp).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA324">
<title id=" W04-1213.xml">introduction to the bio entity recognition task at jnlpba </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 classification models.
</prevsent>
<prevsent>roughly four types of classification models we reapplied by the eight participating systems; support vector machines (svms), hidden markov models (hmms), maximum entropy markov models (memms) and conditional random fields (crfs).
</prevsent>
</prevsection>
<citsent citstr=" W04-1214 ">
the most frequently applied 72 models were svms with totally five systems adopting svms as the classification models either in isolation (park et al, 2004; <papid> W04-1214 </papid>lee et al., 2004) <papid> W04-1215 </papid>or in combination with other models (zhou and su, 2004; song et al, 2004;<papid> W04-1220 </papid>rossler, 2004).<papid> W04-1218 </papid></citsent>
<aftsection>
<nextsent>hmms were employed by one system in isolation (zhao, 2004) <papid> W04-1216 </papid>and by two systems in combination with svms (zhou and su, 2004; rossler, 2004).<papid> W04-1218 </papid></nextsent>
<nextsent>similarly, crfs were employed by one system in isolation (settles, 2004) <papid> W04-1221 </papid>and by another system in combination with svms (song et al, 2004).<papid> W04-1220 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA325">
<title id=" W04-1213.xml">introduction to the bio entity recognition task at jnlpba </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 classification models.
</prevsent>
<prevsent>roughly four types of classification models we reapplied by the eight participating systems; support vector machines (svms), hidden markov models (hmms), maximum entropy markov models (memms) and conditional random fields (crfs).
</prevsent>
</prevsection>
<citsent citstr=" W04-1215 ">
the most frequently applied 72 models were svms with totally five systems adopting svms as the classification models either in isolation (park et al, 2004; <papid> W04-1214 </papid>lee et al., 2004) <papid> W04-1215 </papid>or in combination with other models (zhou and su, 2004; song et al, 2004;<papid> W04-1220 </papid>rossler, 2004).<papid> W04-1218 </papid></citsent>
<aftsection>
<nextsent>hmms were employed by one system in isolation (zhao, 2004) <papid> W04-1216 </papid>and by two systems in combination with svms (zhou and su, 2004; rossler, 2004).<papid> W04-1218 </papid></nextsent>
<nextsent>similarly, crfs were employed by one system in isolation (settles, 2004) <papid> W04-1221 </papid>and by another system in combination with svms (song et al, 2004).<papid> W04-1220 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA326">
<title id=" W04-1213.xml">introduction to the bio entity recognition task at jnlpba </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 classification models.
</prevsent>
<prevsent>roughly four types of classification models we reapplied by the eight participating systems; support vector machines (svms), hidden markov models (hmms), maximum entropy markov models (memms) and conditional random fields (crfs).
</prevsent>
</prevsection>
<citsent citstr=" W04-1220 ">
the most frequently applied 72 models were svms with totally five systems adopting svms as the classification models either in isolation (park et al, 2004; <papid> W04-1214 </papid>lee et al., 2004) <papid> W04-1215 </papid>or in combination with other models (zhou and su, 2004; song et al, 2004;<papid> W04-1220 </papid>rossler, 2004).<papid> W04-1218 </papid></citsent>
<aftsection>
<nextsent>hmms were employed by one system in isolation (zhao, 2004) <papid> W04-1216 </papid>and by two systems in combination with svms (zhou and su, 2004; rossler, 2004).<papid> W04-1218 </papid></nextsent>
<nextsent>similarly, crfs were employed by one system in isolation (settles, 2004) <papid> W04-1221 </papid>and by another system in combination with svms (song et al, 2004).<papid> W04-1220 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA328">
<title id=" W04-1213.xml">introduction to the bio entity recognition task at jnlpba </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 classification models.
</prevsent>
<prevsent>roughly four types of classification models we reapplied by the eight participating systems; support vector machines (svms), hidden markov models (hmms), maximum entropy markov models (memms) and conditional random fields (crfs).
</prevsent>
</prevsection>
<citsent citstr=" W04-1218 ">
the most frequently applied 72 models were svms with totally five systems adopting svms as the classification models either in isolation (park et al, 2004; <papid> W04-1214 </papid>lee et al., 2004) <papid> W04-1215 </papid>or in combination with other models (zhou and su, 2004; song et al, 2004;<papid> W04-1220 </papid>rossler, 2004).<papid> W04-1218 </papid></citsent>
<aftsection>
<nextsent>hmms were employed by one system in isolation (zhao, 2004) <papid> W04-1216 </papid>and by two systems in combination with svms (zhou and su, 2004; rossler, 2004).<papid> W04-1218 </papid></nextsent>
<nextsent>similarly, crfs were employed by one system in isolation (settles, 2004) <papid> W04-1221 </papid>and by another system in combination with svms (song et al, 2004).<papid> W04-1220 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA330">
<title id=" W04-1213.xml">introduction to the bio entity recognition task at jnlpba </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>roughly four types of classification models we reapplied by the eight participating systems; support vector machines (svms), hidden markov models (hmms), maximum entropy markov models (memms) and conditional random fields (crfs).
</prevsent>
<prevsent>the most frequently applied 72 models were svms with totally five systems adopting svms as the classification models either in isolation (park et al, 2004; <papid> W04-1214 </papid>lee et al., 2004) <papid> W04-1215 </papid>or in combination with other models (zhou and su, 2004; song et al, 2004;<papid> W04-1220 </papid>rossler, 2004).<papid> W04-1218 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-1216 ">
hmms were employed by one system in isolation (zhao, 2004) <papid> W04-1216 </papid>and by two systems in combination with svms (zhou and su, 2004; rossler, 2004).<papid> W04-1218 </papid></citsent>
<aftsection>
<nextsent>similarly, crfs were employed by one system in isolation (settles, 2004) <papid> W04-1221 </papid>and by another system in combination with svms (song et al, 2004).<papid> W04-1220 </papid></nextsent>
<nextsent>it is somewhat surprising that maximum entropy models were applied by only one system (finkel et al, 2004), <papid> W04-1217 </papid>while it was the most successfully applied model in the conll-2003 shared task of named entity recognition, and at this time also the memmsystem yields quite good performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA333">
<title id=" W04-1213.xml">introduction to the bio entity recognition task at jnlpba </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>the most frequently applied 72 models were svms with totally five systems adopting svms as the classification models either in isolation (park et al, 2004; <papid> W04-1214 </papid>lee et al., 2004) <papid> W04-1215 </papid>or in combination with other models (zhou and su, 2004; song et al, 2004;<papid> W04-1220 </papid>rossler, 2004).<papid> W04-1218 </papid></prevsent>
<prevsent>hmms were employed by one system in isolation (zhao, 2004) <papid> W04-1216 </papid>and by two systems in combination with svms (zhou and su, 2004; rossler, 2004).<papid> W04-1218 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-1221 ">
similarly, crfs were employed by one system in isolation (settles, 2004) <papid> W04-1221 </papid>and by another system in combination with svms (song et al, 2004).<papid> W04-1220 </papid></citsent>
<aftsection>
<nextsent>it is somewhat surprising that maximum entropy models were applied by only one system (finkel et al, 2004), <papid> W04-1217 </papid>while it was the most successfully applied model in the conll-2003 shared task of named entity recognition, and at this time also the memmsystem yields quite good performance.</nextsent>
<nextsent>one interpretation on this may be the crf is often regarded as kind of version-upped model ofthe memm (in the sense that both are conditional, exponential models) and thus is replacing memm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA336">
<title id=" W04-1213.xml">introduction to the bio entity recognition task at jnlpba </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>hmms were employed by one system in isolation (zhao, 2004) <papid> W04-1216 </papid>and by two systems in combination with svms (zhou and su, 2004; rossler, 2004).<papid> W04-1218 </papid></prevsent>
<prevsent>similarly, crfs were employed by one system in isolation (settles, 2004) <papid> W04-1221 </papid>and by another system in combination with svms (song et al, 2004).<papid> W04-1220 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-1217 ">
it is somewhat surprising that maximum entropy models were applied by only one system (finkel et al, 2004), <papid> W04-1217 </papid>while it was the most successfully applied model in the conll-2003 shared task of named entity recognition, and at this time also the memmsystem yields quite good performance.</citsent>
<aftsection>
<nextsent>one interpretation on this may be the crf is often regarded as kind of version-upped model ofthe memm (in the sense that both are conditional, exponential models) and thus is replacing memm.
</nextsent>
<nextsent>5.2 features and external resources.
</nextsent>
<nextsent>it has been found that utilizing various sources of information is crucial to get good performance in this kind of task.
</nextsent>
<nextsent>table 4 outlines some of the features exploited by the systems participating in the jnlpba 2004 shared task(the table also lists the classification models employed and external resources exploited by the systems to provide the outline of the systems at glance).lexical features (words) were widely exploited by three systems that didnt employ svms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA352">
<title id=" W04-1213.xml">introduction to the bio entity recognition task at jnlpba </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>this fact suggests that global optimization over whole sequence (e.g, viterbi optimization) is crucial in this type of tasks.
</prevsent>
<prevsent>as is well known, the outputs of svms are not easy to use in global optimization.
</prevsent>
</prevsection>
<citsent citstr=" W04-1219 ">
it seems (zhou and su, 2004) <papid> W04-1219 </papid>overcomes the drawback ofsvms by mapping the svm output into probability, and complementing it with markov mod els.</citsent>
<aftsection>
<nextsent>their remarkable performance seems due to the well designed classification model and the4a comprehensive report of systems performance is available at http://www-tsujii.is.s.u tokyo.ac.jp/genia/ertask/report.html.
</nextsent>
<nextsent>73 cm lx af or sh gn wv ln gz po np sy tr ab ca do pa pr ext.zho sh - + + - + - - + + - - + + + - - + fin + + - + - - - + + - + - + - + + + b, set + + + + - - - (+) - - - (+) - - - - + (w) son sc * + + - - - - - + + - - - - - - + zha + - - - - - - - - - - - - - - - + ros sh - + + - + - + - - - - - - - - - + (m) par - + + + + + - - + + - + - - - - - m, lee * + - - - - - - + - - - - - - - - y, table 4: overview of participating systems in terms of classification models, main features and external resources, sorted by performance.
</nextsent>
<nextsent>classification model (cm): s: svm; h: hmm; m: memm; c: crf; lx: lexical features; af: affix information (character n-grams); or: orthographic information; sh: word shapes; gn: gene sequences (atcg sequences); wv: word variations; ln: word length; gz: gazetteers; po: part-of-speech tags; np: noun phrase tags; sy: syntactic tags; tr: word triggers; ab: abbreviations; ca: cascaded entities; do: global document information; pa: parentheses handling; pr: previously predicted entity tags; external resources (ext): b: british national corpus; m: medline corpus; p: penn treebank ii corpus; w: world wide web; v: virtually generated corpus; y: yapex; g: gapscore.
</nextsent>
<nextsent>1978-1989 set 1990-1999 set 2000-2001 set s/1998-2001 set total zho 75.3 / 69.5 / 72.3 77.1 / 69.2 / 72.9 75.6 / 71.3 / 73.8 75.8 / 69.5 / 72.5 76.0 / 69.4 / 72.6 fin 66.9 / 70.4 / 68.6 73.8 / 69.4 / 71.5 72.6 / 69.3 / 70.9 71.8 / 67.5 / 69.6 71.6 / 68.6 / 70.1 set 63.6 / 71.4 / 67.3 72.2 / 68.7 / 70.4 71.3 / 69.6 / 70.5 71.3 / 68.8 / 70.1 70.3 / 69.3 / 69.8 son 60.3 / 66.2 / 63.1 71.2 / 65.6 / 68.2 69.5 / 65.8 / 67.6 68.3 / 64.0 / 66.1 67.8 / 64.8 / 66.3 zha 63.2 / 60.4 / 61.8 72.5 / 62.6 / 67.2 69.1 / 60.2 / 64.7 69.2 / 60.3 / 64.4 69.1 / 61.0 / 64.8 ros 59.2 / 60.3 / 59.8 70.3 / 61.8 / 65.8 68.4 / 61.5 / 64.8 68.3 / 60.4 / 64.1 67.4 / 61.0 / 64.0 par 62.8 / 55.9 / 59.2 70.3 / 61.4 / 65.6 65.1 / 60.4 / 62.7 65.9 / 59.7 / 62.7 66.5 / 59.8 / 63.0 lee 42.5 / 42.0 / 42.2 52.5 / 49.1 / 50.8 53.8 / 50.9 / 52.3 52.3 / 48.1 / 50.1 50.8 / 47.6 / 49.1 bl 47.1 / 33.9 / 39.4 56.8 / 45.5 / 50.5 51.7 / 46.3 / 48.8 52.6 / 46.0 / 49.1 52.6 / 43.6 / 47.7 table 5: performance of each participating system and baseline model (bl) (recall / precision / f-score) rich set of features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA364">
<title id=" W04-2108.xml">dictionaries merger for text expansion in question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>vocabulary of the question.
</prevsent>
<prevsent>qualc (ferret et al, 1999) adds stemming expansion prior to using search engine.
</prevsent>
</prevsection>
<citsent citstr=" P00-1071 ">
the falcon system (moldovan et al, 2000) <papid> P00-1071 </papid>uses some semantic relations from wordnet when it expands the question.</citsent>
<aftsection>
<nextsent>in this paper, present way to process dictionaries to make them consistent with the needs of the application.
</nextsent>
<nextsent>i first describe the lexical needs of my qa application.
</nextsent>
<nextsent>i secondly outline the issue of the use of several incompatible dictionaries.
</nextsent>
<nextsent>then show the way distribute information from additional dictionaries to reference one: synonyms, derivative forms and taxonomy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA365">
<title id=" W04-2108.xml">dictionaries merger for text expansion in question answering </title>
<section> what the qa method needs.  </section>
<citcontext>
<prevsection>
<prevsent>i decided to process the documents to build an enriched informative structure (jacquemin, 2004).
</prevsent>
<prevsent>but this feature falls outside the scope of this paper.
</prevsent>
</prevsection>
<citsent citstr=" C00-1020 ">
my semantic disambiguator (jacquemin etal., 2002) is an evolution of tool previously developed for both french and english at xrce (brun, 2000; <papid> C00-1020 </papid>brunet al, 2001).</citsent>
<aftsection>
<nextsent>the idea is to use dictionary as tagged corpus to extract semantic disambiguation rules.
</nextsent>
<nextsent>the contextual data (syntactic, lexico-syntactic and semantico syntactic) forgiven sense of word are seen as differential indications.
</nextsent>
<nextsent>so when the schema is found in the context of this word in sentence, the corresponding sense is assigned.
</nextsent>
<nextsent>in figure 1, we can see how disambiguation rule is extracted from an informative field ofdubois?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA366">
<title id=" W04-2417.xml">a transformation based approach to argument labeling </title>
<section> overview </section>
<citcontext>
<prevsection>
<prevsent>consequently, some careis needed in posing the task in tbl framework.
</prevsent>
<prevsent>in the closed challenge of the conll shared task, the system is charged with both identifying argument boundaries, and correctly labeling the arguments with the correct semantic role, without using parser to suggest candidate phrases.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
transformation-based learning (brill, 1995) <papid> J95-4004 </papid>is well-suited to simultaneously addressing this dual task of identifying and labeling semantic arguments of predicate, because it allows intermediate hypotheses to influence the ultimate decisions made.</citsent>
<aftsection>
<nextsent>more concretely, the category of an argument may decisively influence how the system places its boundaries, and conversely, the shape of an argument is an important factor in predicting its category.
</nextsent>
<nextsent>we treat the task as word-by-word tagging problem, using variant of the iob2 labeling scheme.
</nextsent>
<nextsent>tbl is general machine learning tool for assigning classes to sequence of observations.
</nextsent>
<nextsent>tbl induces set of transformational rules, which apply in sequence to change the class assigned to observations which meet the rules?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA367">
<title id=" W04-2417.xml">a transformation based approach to argument labeling </title>
<section> transformation-based learning.  </section>
<citcontext>
<prevsection>
<prevsent>conditions.
</prevsent>
<prevsent>we use the software package fntbl to design the model described here.
</prevsent>
</prevsection>
<citsent citstr=" N01-1006 ">
this package, and the tbl framework itself, are described in detail by ngai and florian (2001).<papid> N01-1006 </papid></citsent>
<aftsection>
<nextsent>defining the task of semantic role labeling in tbl terms requires four basic steps.
</nextsent>
<nextsent>first, the problem has to be reduced to that of assigning an appropriate tag to each wordin sentence.
</nextsent>
<nextsent>second, we must define the features associated with each word in the sentence, on which the transformational rules will operate.
</nextsent>
<nextsent>third, we must decide on the exact forms the transformational rules will be allowed to take (the rule templates).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA368">
<title id=" W04-2417.xml">a transformation based approach to argument labeling </title>
<section> task definition.  </section>
<citcontext>
<prevsection>
<prevsent>tag the words part-of-speech tag, as predicted by the system of gimenez and ma`rquez (2003).
</prevsent>
<prevsent>chunk the chunk label of the word, as predicted by the system of carreras and ma`rquez (2003).
</prevsent>
</prevsection>
<citsent citstr=" W03-0423 ">
entity the named-entity label of the word, as predicted by the system of chieu and ng (2003).<papid> W03-0423 </papid></citsent>
<aftsection>
<nextsent>l/r feature indicating whether the word is to the left (l) or right (r) of the target verb.indent this feature indicates the clause level of the current word with respect to the target predicate.
</nextsent>
<nextsent>using the clause boundaries predicted by the system of carreras and ma`rquez (2003), we compute feature based on the linguistic notion of c-command.1 if both the predicate and the current word are inthe same basic clause, indent=0.
</nextsent>
<nextsent>if the predicate commands the current word, and the current word is one clause level lower, indent=1.
</nextsent>
<nextsent>if it is two clause levels lower, indent=2, and so on.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA369">
<title id=" W04-0832.xml">senseval automatic labeling of semantic roles using maximum entropy models </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>the sequence of stepson sample sentence having target word tied?.
</prevsent>
<prevsent>we train the me models using the gis algorithm (darroch and rat cliff, 1972) as implemented in the yasmet me package (och, 2002).
</prevsent>
</prevsection>
<citsent citstr=" E03-1055 ">
we use the yasmet me tagger (bender et al. 2003) <papid> E03-1055 </papid>to perform the viterbi search for choosing the most probable tag sequence for sentence using the probabilities computed during training.</citsent>
<aftsection>
<nextsent>feature weights are smoothed using gaussian priors with mean 0 (chen and rosenfeld, 1999).
</nextsent>
<nextsent>2.1 sentence segmentation.
</nextsent>
<nextsent>we segment sentence into sequence of nonoverlapping constituents instead of all individual constituents.
</nextsent>
<nextsent>there are number of advantages to applying sentence segmentation before fe 2 we are currently ignoring null instantiations..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA370">
<title id=" W04-0832.xml">senseval automatic labeling of semantic roles using maximum entropy models </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems boundary identification.
</prevsent>
<prevsent>first, it allows us to utilize sentence-wide features for fe identification.
</prevsent>
</prevsection>
<citsent citstr=" W03-1007 ">
the sentence-wide features, containing dependent information between frame element such as the previously identified class or the syntactic pattern, have previously been shown to be powerful features for role classification (fleischman et al, 2003).<papid> W03-1007 </papid></citsent>
<aftsection>
<nextsent>further, it allows us to reduce the number of candidate constituents for fe, which reduces the convergence time in training.
</nextsent>
<nextsent>the constituents are derived from syntactic parse tree3.
</nextsent>
<nextsent>although we need to consider all combinations of various level constituents in parse tree, we know the given target word should be separate segment because target word is not part of other fes.4 since most frame elements tend to be in higher levels of the parse tree, we decide to use the highest constituents (the parse constituents having the maximum number of words) while separating the target word.
</nextsent>
<nextsent>figure 2 shows an example of the segmentation for an actual sentence in framenet with the target word tied?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA371">
<title id=" W04-0832.xml">senseval automatic labeling of semantic roles using maximum entropy models </title>
<section> we use charniak parser :.  </section>
<citcontext>
<prevsection>
<prevsent>frame element identification is executed for segments to classify into the classes on fe, target, or none.
</prevsent>
<prevsent>when constituent is both target and frame element, we set it as frame element when training because we are interested in identifying frame elements not target.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
the initial features are adopted from (gildea and juraksky 2002) <papid> J02-3001 </papid>and (fleischman, kwon, and hovy 2003), and few additional features are also used.</citsent>
<aftsection>
<nextsent>the features are: ? target predicate (target): the target is the principal lexical item in sentence.
</nextsent>
<nextsent>target lexical name (lexunit): the formal lexical name of target predicate is the string of the original form of target word and grammatical type.
</nextsent>
<nextsent>for example, when the target is tied?, the lexical name is tie.v?.
</nextsent>
<nextsent>target type (ltype): the target type is part of lexunit representing verb, noun, or adjective.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA372">
<title id=" W04-0203.xml">using a probabilistic model of discourse relations to investigate word order variation </title>
<section> additional meaning of non-canonical.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 concludes and suggests improvements and applications of the model.
</prevsent>
<prevsent>syntax: discourse relations the meaning of multi-utterance text is composed not only of the meaning of each individual utterance but also of the relations holding between the utterances.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
these relations have syntactic aspects, such that single utterances can be grouped together and combined into segments recursively and are often modeled as hierarchical tree structure (grosz andsidner, 1986; <papid> J86-3001 </papid>webber et al, 1999).<papid> P99-1006 </papid></citsent>
<aftsection>
<nextsent>discourse relations may also have semantic or meaning com ponent; this property, when treated in the literature, is often referred to as coherence, subject matter, or rhetorical relations (kehler, 2002; halliday, 1985; mann and thompson, 1988).
</nextsent>
<nextsent>the use of an utterance with non-canonical word order helps hearers make inferences about both the syntactic and semantic properties of discourse relations between the utterance and the rest of the discourse.
</nextsent>
<nextsent>for both aspects of discourse relations, it is the fact that the non-canonical order marks part ofthe utterances information as salient or discourse old that assists these inferences.
</nextsent>
<nextsent>2.1 syntax of discourse relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA373">
<title id=" W04-0203.xml">using a probabilistic model of discourse relations to investigate word order variation </title>
<section> additional meaning of non-canonical.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 concludes and suggests improvements and applications of the model.
</prevsent>
<prevsent>syntax: discourse relations the meaning of multi-utterance text is composed not only of the meaning of each individual utterance but also of the relations holding between the utterances.
</prevsent>
</prevsection>
<citsent citstr=" P99-1006 ">
these relations have syntactic aspects, such that single utterances can be grouped together and combined into segments recursively and are often modeled as hierarchical tree structure (grosz andsidner, 1986; <papid> J86-3001 </papid>webber et al, 1999).<papid> P99-1006 </papid></citsent>
<aftsection>
<nextsent>discourse relations may also have semantic or meaning com ponent; this property, when treated in the literature, is often referred to as coherence, subject matter, or rhetorical relations (kehler, 2002; halliday, 1985; mann and thompson, 1988).
</nextsent>
<nextsent>the use of an utterance with non-canonical word order helps hearers make inferences about both the syntactic and semantic properties of discourse relations between the utterance and the rest of the discourse.
</nextsent>
<nextsent>for both aspects of discourse relations, it is the fact that the non-canonical order marks part ofthe utterances information as salient or discourse old that assists these inferences.
</nextsent>
<nextsent>2.1 syntax of discourse relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA375">
<title id=" W04-0203.xml">using a probabilistic model of discourse relations to investigate word order variation </title>
<section> probabilistic model of discourse.  </section>
<citcontext>
<prevsection>
<prevsent>the left dislocation supports the additional inference that the exemplification described above holds too.
</prevsent>
<prevsent>relations and non-canonical syntax to provide evidence beyond individual examples for the phenomena in section 2, we need to measure the correlation between discourse relations and syntactic form, but annotating discourse relations directly is problematic.
</prevsent>
</prevsection>
<citsent citstr=" W99-0307 ">
annotation of hierarchical discourse structure is difficult and subjective although efforts have been made (creswell et al,2002; marcu et al, 1999).<papid> W99-0307 </papid></citsent>
<aftsection>
<nextsent>even annotating linear segmentation is challenging, particularly in the vicinity of segment boundaries (passonneau and litman, 1997).<papid> J97-1005 </papid></nextsent>
<nextsent>annotation of the semantics of discourse relations requires predetermined set of relation types, on which theories vary widely, makingtheory-neutral generalizations about the role of non canonical syntax impossible.this project attempts to overcome these difficulties by indirectly deriving discourse relations by mapping from their known correlates to the use of certain non-canonical forms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA376">
<title id=" W04-0203.xml">using a probabilistic model of discourse relations to investigate word order variation </title>
<section> probabilistic model of discourse.  </section>
<citcontext>
<prevsection>
<prevsent>relations and non-canonical syntax to provide evidence beyond individual examples for the phenomena in section 2, we need to measure the correlation between discourse relations and syntactic form, but annotating discourse relations directly is problematic.
</prevsent>
<prevsent>annotation of hierarchical discourse structure is difficult and subjective although efforts have been made (creswell et al,2002; marcu et al, 1999).<papid> W99-0307 </papid></prevsent>
</prevsection>
<citsent citstr=" J97-1005 ">
even annotating linear segmentation is challenging, particularly in the vicinity of segment boundaries (passonneau and litman, 1997).<papid> J97-1005 </papid></citsent>
<aftsection>
<nextsent>annotation of the semantics of discourse relations requires predetermined set of relation types, on which theories vary widely, makingtheory-neutral generalizations about the role of non canonical syntax impossible.this project attempts to overcome these difficulties by indirectly deriving discourse relations by mapping from their known correlates to the use of certain non-canonical forms.
</nextsent>
<nextsent>the correlates used here are referential relations across utterance boundaries and the presence and type of lexical discourse markers or cue words.
</nextsent>
<nextsent>these features are annotated with respect to three-utterance window centered on target utterance ui, shown schematically in figure 1.
</nextsent>
<nextsent>these referential and lexical features build on the work of passonneau and litman (1997), <papid> J97-1005 </papid>who use them in discourse segmentation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA379">
<title id=" W04-0508.xml">answering questions in the genomics domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the process of variation is well investigated in terminological research (daille et al, 1996).
</prevsent>
<prevsent>in the biomedical domain, an example of system that deals with terminological variants (also called aliases?)
</prevsent>
</prevsection>
<citsent citstr=" W02-0312 ">
can be found in (pustejovsky et al, 2002).<papid> W02-0312 </papid>in the rest of this paper we will first briefly describe our existing question answering system, ex trans (section 2).</citsent>
<aftsection>
<nextsent>in the following section (3) we detail the specific problems encountered in the new domain and the steps that we have taken to solve them.
</nextsent>
<nextsent>we conclude the paper with an overview of related research (section 4).
</nextsent>
<nextsent>figure 1: example of document to be analyzed
</nextsent>
<nextsent>extrans is question answering system aimed at restricted domains, in particular terminology-rich domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA380">
<title id=" W04-0508.xml">answering questions in the genomics domain </title>
<section> moving to the new domain.  </section>
<citcontext>
<prevsection>
<prevsent>the remaining 4483 candidate terms involve novel head with at least one gene or pathway as modifier.
</prevsent>
<prevsent>once the terminology is available, it is necessary to detect relations among terms in order to exploit argentine methyl ation of stat1 modulates ifn -induced transcription np1 vbz subj np2 np3 prepmodpp obj figure 4: an example of syntactic analysis it.
</prevsent>
</prevsection>
<citsent citstr=" W03-1801 ">
we have focused our attention in particular to the relations of synonymy and hyponymy, which are detected as described in (dowdall et al, 2003) <papid> W03-1801 </papid>and gathered in thesaurus.</citsent>
<aftsection>
<nextsent>the organizing unit isthe wordnet style synset which includes strict syn onymy as well as three weaker synonymy relations.
</nextsent>
<nextsent>these sets are further organized into isa hierarchy based on two definitions of hyponymy.
</nextsent>
<nextsent>one of the most serious problems that we have encountered in working in restricted domains is the syntactic ambiguity generated by multi-word units, in particular technical terms.
</nextsent>
<nextsent>any generic parser, unless developed specifically for the do main at hand, will have serious problems dealing with those multi-words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA381">
<title id=" W04-0508.xml">answering questions in the genomics domain </title>
<section> moving to the new domain.  </section>
<citcontext>
<prevsection>
<prevsent>in comparison to deep linguistic, formal grammar-based parsers, however, the output of probabilistic parsers is relatively shallow, pure context-free grammar (cfg) constituency output,tree structures that do not include grammatical function annotation nor co-indexation and empty nodes annotation expressing long-distance dependencies (ldd).
</prevsent>
<prevsent>in simple example sentence john wantsto leave?, deep-linguistic syntactic analysis expresses the identity of the explicit matrix clause subject and implicit subordinate clause subject bymeans of co-indexing the explicit and the empty implicit subject trace t: ?[john1 wants [t1 to leave]]?.a parser that fails to recognize these implicit subjects, so-called control subjects, misses very important information, quantitatively about 3 % of all subjects.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
although ldd annotation is actually provided in treebanks such as the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>over which they are typically trained, most probabilistic parsers largely or fully ignore this in formation.</citsent>
<aftsection>
<nextsent>this means that the extraction of lddsand the mapping to shallow semantic representations such as mlf is not always possible, because first co-indexation information is not available, second single parsing error across tree fragment containing an ldd makes its extraction impossible, third some syntactic relations cannot be recovered figure 5: dependency tree output of the swi prolog graphical implementation of the parser on configurational grounds only.we therefore adapt extrans to use new statistical broad-coverage parser that is as fast as probabilistic parser but more deep-linguistic because it delivers grammatical relation structures which are closer to predicate-argument structures and shallow semantic structures like mlf, and more informative if non-local dependencies are involved (schneider, 2003).
</nextsent>
<nextsent>it has been evaluated and shown to have state-of-the-art performance.the parser expresses distinctions that are especially important for predicate-argument based shallow semantic representation, as far as they are expressed in the penn treebank training data, such as pp-attachment, most ldds, relative clause anaphora, participles, gerunds, and the argu ment/adjunct distinction for nps.
</nextsent>
<nextsent>in some cases functional relations distinctions that are not expressed in the penn treebank are made.
</nextsent>
<nextsent>commas are e.g. disambiguated between apposition and conjunction, or the penn tag in isdisambiguated between preposition and subordinating conjunction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA382">
<title id=" W04-0508.xml">answering questions in the genomics domain </title>
<section> moving to the new domain.  </section>
<citcontext>
<prevsection>
<prevsent>3.4 mlfs.
</prevsent>
<prevsent>the deep-linguistic dependency based parser partly simplifies the construction of mlf.
</prevsent>
</prevsection>
<citsent citstr=" P02-1018 ">
first, the mapping between labeled dependencies and surface semantic representation is often more direct than across complex constituency subtree (schneider, 2003), and often more accurate (johnson, 2002).<papid> P02-1018 </papid>dedicated labels can directly express complex relations, the lexical participants needed for the construction are more locally available.let us look at the example sentence adenovirus infection and transfection were used to model changes in susceptibility to cell killing caused by e1a expression?.</citsent>
<aftsection>
<nextsent>the control relation (infectionis the implicit subject of model) and the pp relation (including the description noun) are available locally.
</nextsent>
<nextsent>the reduced relative clause killing caused by is expressed by local dedicated label (modpart).only the conjunction infection and transfection, expressed here by bracketing, needs to be searched across the syntactic hierarchy.
</nextsent>
<nextsent>this leads to the following mlfs: object(infection, o1, [o1]).
</nextsent>
<nextsent>object(transfection, o2, [o2]).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA383">
<title id=" W04-0508.xml">answering questions in the genomics domain </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in(o3, o4).
</prevsent>
<prevsent>question answering in biomedicine is surveyed in detail in (zweigenbaum, 2003), in particular regarding clinical questions.
</prevsent>
</prevsection>
<citsent citstr=" W03-1310 ">
an example of system applied to such questions is presented in (niu et al,2003), <papid> W03-1310 </papid>where it is applied in setting for evidence based medicine.</citsent>
<aftsection>
<nextsent>this system identifies specificroles?
</nextsent>
<nextsent>within the document sentences and the questions, determining the answers is then matter of comparing the roles in each.
</nextsent>
<nextsent>to this aim, natural language questions are translated into the pico format (sackett et al, 2000).
</nextsent>
<nextsent>automatic knowledge extraction (or strategies for improving these methods) over medline articles are numerous.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA384">
<title id=" W04-0305.xml">look ahead in deterministic left corner parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>minis tic parsing takes the extreme position that there can only be one analysis for any sentence prefix.
</prevsent>
<prevsent>we investigate methods which make such strong constraint feasible, in particular the use of lookahead.in this paper we do not try to construct single deterministic parser, but instead consider family of deterministic parsers and empirically measure the optimal performance of deterministic parser in this family.
</prevsent>
</prevsection>
<citsent citstr=" C00-1017 ">
as has been previously proposed by brants and crocker (2000),<papid> C00-1017 </papid>we take corpus-based approach to this empirical investigation, using previously defined statistical parser (henderson, 2003).<papid> N03-1014 </papid></citsent>
<aftsection>
<nextsent>the statistical parser uses an incremental history-based probability model based on left-corner parsing, and the parameters of this model are estimated using neural network.
</nextsent>
<nextsent>performance of this basic model is state-of-the-art, making these results likely to generalize beyond this specific system.
</nextsent>
<nextsent>we specify the family of deterministic parser sin terms of pruning the search for the most probable parse.
</nextsent>
<nextsent>both deterministic parsing and the use of k-word look ahead are characterized as constraints on pruning this search.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA385">
<title id=" W04-0305.xml">look ahead in deterministic left corner parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>minis tic parsing takes the extreme position that there can only be one analysis for any sentence prefix.
</prevsent>
<prevsent>we investigate methods which make such strong constraint feasible, in particular the use of lookahead.in this paper we do not try to construct single deterministic parser, but instead consider family of deterministic parsers and empirically measure the optimal performance of deterministic parser in this family.
</prevsent>
</prevsection>
<citsent citstr=" N03-1014 ">
as has been previously proposed by brants and crocker (2000),<papid> C00-1017 </papid>we take corpus-based approach to this empirical investigation, using previously defined statistical parser (henderson, 2003).<papid> N03-1014 </papid></citsent>
<aftsection>
<nextsent>the statistical parser uses an incremental history-based probability model based on left-corner parsing, and the parameters of this model are estimated using neural network.
</nextsent>
<nextsent>performance of this basic model is state-of-the-art, making these results likely to generalize beyond this specific system.
</nextsent>
<nextsent>we specify the family of deterministic parser sin terms of pruning the search for the most probable parse.
</nextsent>
<nextsent>both deterministic parsing and the use of k-word look ahead are characterized as constraints on pruning this search.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA388">
<title id=" W04-0305.xml">look ahead in deterministic left corner parsing </title>
<section> a generative left-corner </section>
<citcontext>
<prevsection>
<prevsent>by expressing the family of deterministic parsers with look ahead in terms of pruning strategy on basic parsing model, we are able to easily investigate the effects of different looka head lengths on the maximum performance ofa deterministic parser in this family.
</prevsent>
<prevsent>to complete the specification of the family of deterministic parsers, we simple have to specify the basic parsing model, as done in the next section.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
probability model as with several previous statistical parsers(collins, 1999; charniak, 2000), <papid> A00-2018 </papid>we use generative history-based probability model of parsing.</citsent>
<aftsection>
<nextsent>designing history-based model of parsing involves two steps, first choosing mapping from the set of phrase structure trees to the set of parses, and then choosing probability model in which the probability of each parser decision is conditioned on the history of previous decisions in the parse.
</nextsent>
<nextsent>for the model to be generative, these decisions must include predicting the words of the sentence.
</nextsent>
<nextsent>to support incremental parsing, we want to map phrase structure treesto parses which predict the words of the sentence in their left-to-right order.
</nextsent>
<nextsent>to support deterministic parsing, we want our parses to specify information about the phrase structure tree at appropriate points in the sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA396">
<title id=" W04-0305.xml">look ahead in deterministic left corner parsing </title>
<section> the experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the non-deterministic pruning also reduces the set of partial parses which are chosen between during the subsequent deterministic pruning.
</prevsent>
<prevsent>but this undoubtedly has no significant effect, since experimental results have shown that the level of non-deterministicpruning discussed above does not effect performance even without deterministic pruning.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
to investigate the effects of look ahead on our family of deterministic parsers, we ran empirical experiments on the standard the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>datasets.</citsent>
<aftsection>
<nextsent>the input to the network is sequence of tag-word pairs.1 we report results for vocabulary size of 508 tag-word pairs (a frequency threshold of 200).we first trained network to estimate the parameters of the basic probability model.
</nextsent>
<nextsent>we determined appropriate training parameters and network size based on intermediate validation 1we used publicly available tagger (ratnaparkhi, 1996) <papid> W96-0213 </papid>to provide the tags.</nextsent>
<nextsent>this tagger is run before the parser, so there may be some information about future words which is available in the disambiguated tag which is not available in the word itself.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA397">
<title id=" W04-0305.xml">look ahead in deterministic left corner parsing </title>
<section> the experiments.  </section>
<citcontext>
<prevsection>
<prevsent>to investigate the effects of look ahead on our family of deterministic parsers, we ran empirical experiments on the standard the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>datasets.</prevsent>
<prevsent>the input to the network is sequence of tag-word pairs.1 we report results for vocabulary size of 508 tag-word pairs (a frequency threshold of 200).we first trained network to estimate the parameters of the basic probability model.</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
we determined appropriate training parameters and network size based on intermediate validation 1we used publicly available tagger (ratnaparkhi, 1996) <papid> W96-0213 </papid>to provide the tags.</citsent>
<aftsection>
<nextsent>this tagger is run before the parser, so there may be some information about future words which is available in the disambiguated tag which is not available in the word itself.
</nextsent>
<nextsent>we dont think this has had significant impact on the results reported here, but currently we are working on doing the tagging internally to the parser to avoid this problem.
</nextsent>
<nextsent>80 82 84 86 88 90 0 2 4 6 8 10 12 14 16 deterministic recall deterministic precision non-deterministic recall non-deterministic precision figure 1: labeled constituent recall and precision as function of the number of words of look ahead used by deterministic parser.curves reach their non-deterministic performance with large lookahead.
</nextsent>
<nextsent>results and our previous experience.2 we trained several networks and chose the best ones based on their validation performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA398">
<title id=" W04-0305.xml">look ahead in deterministic left corner parsing </title>
<section> the experiments.  </section>
<citcontext>
<prevsection>
<prevsent>to avoid repeated testing on the standard testing set, we measured the performance of the different models on section 0 of the penn treebank (which is not included in either the training or validation sets).
</prevsent>
<prevsent>standard measures of accuracy for different look ahead lengths are plotted in figure 1.3 first we should note thatthe non-deterministic parser has state-of-the art accuracy (89.0% f-measure), considering its vocabulary size.
</prevsent>
</prevsection>
<citsent citstr=" E03-1005 ">
a moderately larger vocabulary version (4215 tag-word pairs) of this parser achieves 89.8% f-measure on section 0, where the best current result on the testing set is 90.7% (bod, 2003).<papid> E03-1005 </papid></citsent>
<aftsection>
<nextsent>as expected, the deterministic parsers do worse than the non-deterministic one, and this difference becomes less as the look ahead is lengthened.
</nextsent>
<nextsent>what is surprising about the curves in figure 1 is that there is very large increase in performance from zero words of look ahead 2the best network had 80 hidden units for the history representation.
</nextsent>
<nextsent>weight decay regularization was applied at the beginning of training but reduced to near 0 by the end of training.
</nextsent>
<nextsent>training was stopped when maximum performance was reached on the validation set, using post-word beam width of 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA399">
<title id=" W04-1120.xml">a new chinese natural language understanding architecture based on multilayer search mechanism </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>without correct word-seg results, however the syntax and semantic parser cannot obtain correct analysis.
</prevsent>
<prevsent>it is chain debts problem.
</prevsent>
</prevsection>
<citsent citstr=" C94-1032 ">
people have tried to solve the error-multipliedproblem by integrating multi-layers into uniform model (gao et al, 2001; nagata, 1994).<papid> C94-1032 </papid></citsent>
<aftsection>
<nextsent>but with the increasing number of integrated layers, the model becomes too complex to build or solve.
</nextsent>
<nextsent>the feedback mechanism (wu and jiang,1998) helps to use the information of high layers to control the final result.
</nextsent>
<nextsent>if the analysis at feedback point cannot be passed, the whole analysis will be denied.
</nextsent>
<nextsent>this mechanism places too much burden on the function of feedback point.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA400">
<title id=" W04-0863.xml">joining forces to resolve lexical ambiguity east meets west in barcelona </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>effort.
</prevsent>
<prevsent>this paper describes the two joint component models of the swat-hk systems entered into four of the word sense disambiguation lexical sample tasksin senseval-3: basque, catalan, italian and romanian, as well as combination model for each language.
</prevsent>
</prevsection>
<citsent citstr=" W04-0862 ">
the feature engineering (and construction of three other component models which are described in (wicentowski et al, 2004)) <papid> W04-0862 </papid>was performed at swarthmore college, while the hongkong team constructed two component models based on well known machine learning algorithms.</citsent>
<aftsection>
<nextsent>the combination model, which was constructed at swarthmore, uses voting to combine all five models.
</nextsent>
<nextsent>a full description of the experimental features for all four tasks can be found in the report submitted by the swarthmore college senseval team (wicentowski et al, 2004).<papid> W04-0862 </papid></nextsent>
<nextsent>briefly, the systems used lexical and syntactic features in the context of the target word:?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA402">
<title id=" W04-0863.xml">joining forces to resolve lexical ambiguity east meets west in barcelona </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>since our systems were all supervised, all the data used was provided by the senseval organizers; no additional (unlabeled) data was included.
</prevsent>
<prevsent>the systems that were constructed by this team included two component models: boosting mod eland maximum entropy model as well ascom bination system.
</prevsent>
</prevsection>
<citsent citstr=" W04-0845 ">
the component models were also used in other senseval-3 tasks: semantic role labeling (ngai et al, 2004) <papid> W04-0845 </papid>and the lexical sample tasks for chinese and english, as well as the multilingual task (carpuat et al, 2004).<papid> W04-0822 </papid>to perform parameter tuning for the two component models, 20% of the samples from the training set were held out into validation set.</citsent>
<aftsection>
<nextsent>since we did not expect the senses of different words to share any information, the training data was partitioned by the ambiguous word in question.
</nextsent>
<nextsent>a model was then trained for each ambiguous word type.
</nextsent>
<nextsent>in total, we had 40 models for basque, 27 models for catalan, 45 models for italian and 39 models for romanian.
</nextsent>
<nextsent>3.1 boosting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA403">
<title id=" W04-0863.xml">joining forces to resolve lexical ambiguity east meets west in barcelona </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>since our systems were all supervised, all the data used was provided by the senseval organizers; no additional (unlabeled) data was included.
</prevsent>
<prevsent>the systems that were constructed by this team included two component models: boosting mod eland maximum entropy model as well ascom bination system.
</prevsent>
</prevsection>
<citsent citstr=" W04-0822 ">
the component models were also used in other senseval-3 tasks: semantic role labeling (ngai et al, 2004) <papid> W04-0845 </papid>and the lexical sample tasks for chinese and english, as well as the multilingual task (carpuat et al, 2004).<papid> W04-0822 </papid>to perform parameter tuning for the two component models, 20% of the samples from the training set were held out into validation set.</citsent>
<aftsection>
<nextsent>since we did not expect the senses of different words to share any information, the training data was partitioned by the ambiguous word in question.
</nextsent>
<nextsent>a model was then trained for each ambiguous word type.
</nextsent>
<nextsent>in total, we had 40 models for basque, 27 models for catalan, 45 models for italian and 39 models for romanian.
</nextsent>
<nextsent>3.1 boosting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA405">
<title id=" W04-0840.xml">senseval3 logic forms a system and possible improvements </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P01-1052 ">
logic forms, particular powerful logic representations presented in moldovan andrus (2001), <papid> P01-1052 </papid>are simple yet highly ef fective.</citsent>
<aftsection>
<nextsent>in this paper, the structure of logic forms and their generation from input text are described.
</nextsent>
<nextsent>the results of an evaluation comparing the logic forms generated by hand with those generated automatically are also reported.
</nextsent>
<nextsent>finally, we suggest some improvements to the representation used in the lfi task based on our results.
</nextsent>
<nextsent>logic forms are first order logic representations of natural language text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA407">
<title id=" W04-0840.xml">senseval3 logic forms a system and possible improvements </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a detailed description of the notation is presented in moldovan andrus (2001).<papid> P01-1052 </papid></prevsent>
<prevsent>logic forms can be utilized by wide variety of applications.</prevsent>
</prevsection>
<citsent citstr=" N03-1022 ">
a logic prover (rus, 2002; moldovan et al, 2003) <papid> N03-1022 </papid>utilizing the axioms generated by the logic form generation system boosts the performance of the question answering system.</citsent>
<aftsection>
<nextsent>the prover essentially takes as input the logic forms of the question and one or more answers and then proceeds to justify (and rank) the answers based on (i) world knowledge axioms, and (ii) nlp axioms.
</nextsent>
<nextsent>the logic prover developed at language computer corporation has increased the performance of the qa system by 30%.
</nextsent>
<nextsent>2.1 parse tree construction.
</nextsent>
<nextsent>logic forms are derived from the output of syntactic parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA409">
<title id=" W04-1121.xml">aligning bilingual corpora using sentences location information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results show that it can achieve good aligned performance and be robust and language independent.
</prevsent>
<prevsent>it can resolve the alignment problem on real bilingual text.
</prevsent>
</prevsection>
<citsent citstr=" P91-1022 ">
there have been number of papers on aligning parallel texts at the sentence level in the last century, e.g., (brown et al 1991; <papid> P91-1022 </papid>gale and church, 1993; <papid> P93-1001 </papid>simard et al 1992; wu dekai 1994).</citsent>
<aftsection>
<nextsent>on clean inputs, such as the canadian hansa rds and the hong kang hansa rds, these methods have been very successful.
</nextsent>
<nextsent>(church, kenneth w, 1993; chen, stanley, 1993) proposed some methods to resolve the problem in noisy bilingual texts.
</nextsent>
<nextsent>cognate information between indo-european languages pairs are used to align n- oisy texts.
</nextsent>
<nextsent>but these methods are limited when aligning the language pairs which are not in the same genre or have no cognate information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA410">
<title id=" W04-1121.xml">aligning bilingual corpora using sentences location information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results show that it can achieve good aligned performance and be robust and language independent.
</prevsent>
<prevsent>it can resolve the alignment problem on real bilingual text.
</prevsent>
</prevsection>
<citsent citstr=" P93-1001 ">
there have been number of papers on aligning parallel texts at the sentence level in the last century, e.g., (brown et al 1991; <papid> P91-1022 </papid>gale and church, 1993; <papid> P93-1001 </papid>simard et al 1992; wu dekai 1994).</citsent>
<aftsection>
<nextsent>on clean inputs, such as the canadian hansa rds and the hong kang hansa rds, these methods have been very successful.
</nextsent>
<nextsent>(church, kenneth w, 1993; chen, stanley, 1993) proposed some methods to resolve the problem in noisy bilingual texts.
</nextsent>
<nextsent>cognate information between indo-european languages pairs are used to align n- oisy texts.
</nextsent>
<nextsent>but these methods are limited when aligning the language pairs which are not in the same genre or have no cognate information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA412">
<title id=" W04-1121.xml">aligning bilingual corpora using sentences location information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>their algorithms need some high frequency word pairs as anchor points.
</prevsent>
<prevsent>when processing the texts that include less high frequency words, these methods will perform weakly and with less precision because of the scarcity of the data problem.
</prevsent>
</prevsection>
<citsent citstr=" P96-1018 ">
(haruno and yamazaki, 1996) <papid> P96-1018 </papid>tried to align short texts without enough repeated words in structurally different languages, such as english and japanese.</citsent>
<aftsection>
<nextsent>they applied the pos information of content words and an online dictionary to find matching word pairs.
</nextsent>
<nextsent>but this is only suitable for the short texts.
</nextsent>
<nextsent>the real text always includes some noisy information.
</nextsent>
<nextsent>it has the following characteristics as fol lows: 1) there are no strict aligned paragraph boundaries in real bilingual text; 2) some paragraphs may be merged into larger paragraph because of the translators individual idea; 3) there are many complex translation patterns in real text; 4) there exist different styles and themes; 5) different genres have different inherent characteristics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA414">
<title id=" W04-1121.xml">aligning bilingual corpora using sentences location information </title>
<section> anchors selection during alignment.  </section>
<citcontext>
<prevsection>
<prevsent>the horizontal axis denotes the sentence number in chinese text, and the vertical axis denotes the sentence number in english text.
</prevsent>
<prevsent>-20 0 20 40 60 80 100 120 140 160 180 -20 0 20 40 60 80 100 120 140 160 180 200 se nte nc n um be in ng lis t ex sentence number in chinese text beads statistical results show that more than 85% sentence beads are (1:1) sentence beads in bilingual texts and their distributions obey an obvious law well.
</prevsent>
</prevsection>
<citsent citstr=" P94-1012 ">
(dekai wu, 1994) <papid> P94-1012 </papid>offered that (1:1) sentence beads occupied 89% in english-chinese as well.</citsent>
<aftsection>
<nextsent>if we select these style sentence beads as candidate anchors, the alignment method will be general on any other language pairs.
</nextsent>
<nextsent>the main points of our alignment method using sentences location information are: locating by the whole text, col locating by sentence length and checking by bilingual dictionary.
</nextsent>
<nextsent>location information of any sentence pair is used fully.
</nextsent>
<nextsent>three lengths are used: are sentence length, upper context length above the sentence pair and nether context length below the sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA417">
<title id=" W04-1215.xml">annotating multiple types of biomedical entities a single word classification approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several attempts have been made to mine knowledge from biomedical documents, such as identifying gene/protein names, recognizing protein interactions, and capturing specific relations in databases.
</prevsent>
<prevsent>among these, named entity recognition is fundamental step to mine knowledge from biological articles.
</prevsent>
</prevsection>
<citsent citstr=" C02-1110 ">
previous approaches on biological named entity extraction can be classified into two types ? rule based (fukuda et al, 1998; olsson et al, 2002; <papid> C02-1110 </papid>tanabe and wilbur, 2002) and corpus-based (collier et al, 2000; <papid> C00-1030 </papid>chang et al, 2004).</citsent>
<aftsection>
<nextsent>yapex (olsson et al, 2002) <papid> C02-1110 </papid>implemented some heuristic steps described by fukuda, et al, and applied filters and knowledge bases to remove false alarms.</nextsent>
<nextsent>syntactic information obtained from the parser was incorporated as well.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA418">
<title id=" W04-1215.xml">annotating multiple types of biomedical entities a single word classification approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several attempts have been made to mine knowledge from biomedical documents, such as identifying gene/protein names, recognizing protein interactions, and capturing specific relations in databases.
</prevsent>
<prevsent>among these, named entity recognition is fundamental step to mine knowledge from biological articles.
</prevsent>
</prevsection>
<citsent citstr=" C00-1030 ">
previous approaches on biological named entity extraction can be classified into two types ? rule based (fukuda et al, 1998; olsson et al, 2002; <papid> C02-1110 </papid>tanabe and wilbur, 2002) and corpus-based (collier et al, 2000; <papid> C00-1030 </papid>chang et al, 2004).</citsent>
<aftsection>
<nextsent>yapex (olsson et al, 2002) <papid> C02-1110 </papid>implemented some heuristic steps described by fukuda, et al, and applied filters and knowledge bases to remove false alarms.</nextsent>
<nextsent>syntactic information obtained from the parser was incorporated as well.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA420">
<title id=" W04-1215.xml">annotating multiple types of biomedical entities a single word classification approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the models includes naive bayes (manning and schutze, 1999), maximum entropy (ratnaparkhi, 1998) and support vector machines (burges, 1998).
</prevsent>
<prevsent>gap score also used brills tagger (brill, 1994) to get the pos tag to filter out some words that are clearly not gene or protein names.
</prevsent>
</prevsection>
<citsent citstr=" W03-1306 ">
efforts have been made (hou and chen, 2002, 2003; tsuruoka and tsujii, 2003) <papid> W03-1306 </papid>to improve the performance.</citsent>
<aftsection>
<nextsent>the nature of classification makes it possible to integrate existing approaches by extracting good features from them.
</nextsent>
<nextsent>several works employing svm classifier have been done (kazama et al, 2002; <papid> W02-0301 </papid>lee et al, 2003; <papid> W03-1305 </papid>takeuchi and collier, 2003; <papid> W03-1308 </papid>yamamoto et al, 2003), <papid> W03-1309 </papid>and will be discussed further in the rest of this paper.</nextsent>
<nextsent>collocation denotes two or more words having strong relationships (manning and schutze, 1999).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA421">
<title id=" W04-1215.xml">annotating multiple types of biomedical entities a single word classification approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>efforts have been made (hou and chen, 2002, 2003; tsuruoka and tsujii, 2003) <papid> W03-1306 </papid>to improve the performance.</prevsent>
<prevsent>the nature of classification makes it possible to integrate existing approaches by extracting good features from them.</prevsent>
</prevsection>
<citsent citstr=" W02-0301 ">
several works employing svm classifier have been done (kazama et al, 2002; <papid> W02-0301 </papid>lee et al, 2003; <papid> W03-1305 </papid>takeuchi and collier, 2003; <papid> W03-1308 </papid>yamamoto et al, 2003), <papid> W03-1309 </papid>and will be discussed further in the rest of this paper.</citsent>
<aftsection>
<nextsent>collocation denotes two or more words having strong relationships (manning and schutze, 1999).
</nextsent>
<nextsent>hou and chen (2003) <papid> W03-1304 </papid>showed that protein/gene collocates are capable of assisting existing protein/gene taggers.</nextsent>
<nextsent>in this paper, we addressed this task as multi-class classification problem with svms and extended the idea of collocation to generate features at word and pattern level in our method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA422">
<title id=" W04-1215.xml">annotating multiple types of biomedical entities a single word classification approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>efforts have been made (hou and chen, 2002, 2003; tsuruoka and tsujii, 2003) <papid> W03-1306 </papid>to improve the performance.</prevsent>
<prevsent>the nature of classification makes it possible to integrate existing approaches by extracting good features from them.</prevsent>
</prevsection>
<citsent citstr=" W03-1305 ">
several works employing svm classifier have been done (kazama et al, 2002; <papid> W02-0301 </papid>lee et al, 2003; <papid> W03-1305 </papid>takeuchi and collier, 2003; <papid> W03-1308 </papid>yamamoto et al, 2003), <papid> W03-1309 </papid>and will be discussed further in the rest of this paper.</citsent>
<aftsection>
<nextsent>collocation denotes two or more words having strong relationships (manning and schutze, 1999).
</nextsent>
<nextsent>hou and chen (2003) <papid> W03-1304 </papid>showed that protein/gene collocates are capable of assisting existing protein/gene taggers.</nextsent>
<nextsent>in this paper, we addressed this task as multi-class classification problem with svms and extended the idea of collocation to generate features at word and pattern level in our method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA423">
<title id=" W04-1215.xml">annotating multiple types of biomedical entities a single word classification approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>efforts have been made (hou and chen, 2002, 2003; tsuruoka and tsujii, 2003) <papid> W03-1306 </papid>to improve the performance.</prevsent>
<prevsent>the nature of classification makes it possible to integrate existing approaches by extracting good features from them.</prevsent>
</prevsection>
<citsent citstr=" W03-1308 ">
several works employing svm classifier have been done (kazama et al, 2002; <papid> W02-0301 </papid>lee et al, 2003; <papid> W03-1305 </papid>takeuchi and collier, 2003; <papid> W03-1308 </papid>yamamoto et al, 2003), <papid> W03-1309 </papid>and will be discussed further in the rest of this paper.</citsent>
<aftsection>
<nextsent>collocation denotes two or more words having strong relationships (manning and schutze, 1999).
</nextsent>
<nextsent>hou and chen (2003) <papid> W03-1304 </papid>showed that protein/gene collocates are capable of assisting existing protein/gene taggers.</nextsent>
<nextsent>in this paper, we addressed this task as multi-class classification problem with svms and extended the idea of collocation to generate features at word and pattern level in our method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA424">
<title id=" W04-1215.xml">annotating multiple types of biomedical entities a single word classification approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>efforts have been made (hou and chen, 2002, 2003; tsuruoka and tsujii, 2003) <papid> W03-1306 </papid>to improve the performance.</prevsent>
<prevsent>the nature of classification makes it possible to integrate existing approaches by extracting good features from them.</prevsent>
</prevsection>
<citsent citstr=" W03-1309 ">
several works employing svm classifier have been done (kazama et al, 2002; <papid> W02-0301 </papid>lee et al, 2003; <papid> W03-1305 </papid>takeuchi and collier, 2003; <papid> W03-1308 </papid>yamamoto et al, 2003), <papid> W03-1309 </papid>and will be discussed further in the rest of this paper.</citsent>
<aftsection>
<nextsent>collocation denotes two or more words having strong relationships (manning and schutze, 1999).
</nextsent>
<nextsent>hou and chen (2003) <papid> W03-1304 </papid>showed that protein/gene collocates are capable of assisting existing protein/gene taggers.</nextsent>
<nextsent>in this paper, we addressed this task as multi-class classification problem with svms and extended the idea of collocation to generate features at word and pattern level in our method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA425">
<title id=" W04-1215.xml">annotating multiple types of biomedical entities a single word classification approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several works employing svm classifier have been done (kazama et al, 2002; <papid> W02-0301 </papid>lee et al, 2003; <papid> W03-1305 </papid>takeuchi and collier, 2003; <papid> W03-1308 </papid>yamamoto et al, 2003), <papid> W03-1309 </papid>and will be discussed further in the rest of this paper.</prevsent>
<prevsent>collocation denotes two or more words having strong relationships (manning and schutze, 1999).</prevsent>
</prevsection>
<citsent citstr=" W03-1304 ">
hou and chen (2003) <papid> W03-1304 </papid>showed that protein/gene collocates are capable of assisting existing protein/gene taggers.</citsent>
<aftsection>
<nextsent>in this paper, we addressed this task as multi-class classification problem with svms and extended the idea of collocation to generate features at word and pattern level in our method.
</nextsent>
<nextsent>existing protein/gene recognizers were used to perform feature extraction as well.
</nextsent>
<nextsent>the rest of this paper is organized as follows.
</nextsent>
<nextsent>the methods used in this study are introduced in section 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA428">
<title id=" W04-0822.xml">augmenting ensemble classification for word sense disambiguation with a kernel pca model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this work, we investigate the potential of one promising new disambiguation model with respect 1the author would like to thank the hongkong research grants council (rgc) for supporting this research in part through research grants rgc6083/99e, rgc6256/00e, and dag03/04.eg09.
</prevsent>
<prevsent>to augmenting our existing ensemble combining maximum entropy model, boosting model, and nave bayes modela combination representing some of the best stand-alone wsd models currently known.
</prevsent>
</prevsection>
<citsent citstr=" P04-1081 ">
the new wsd model, proposed by wu et al  (2004), <papid> P04-1081 </papid>is method for disambiguat ing word senses that exploits nonlinear kernel principal component analysis (kpca) technique.</citsent>
<aftsection>
<nextsent>that the kpca-based model could potentially bea good candidate for new voting model is suggested by wu et al empirical results showing that it yielded higher accuracies on senseval-2 datasets than other models that included maximum entropy, nave bayes, and svm based models.in the following sections, we begin with description of the experimental setup, which utilizesa number of individual classifiers in voting ensemble.
</nextsent>
<nextsent>we then describe the kpca-based model to be added to the baseline ensemble.
</nextsent>
<nextsent>the accuracy results of the three submitted models are examined, and also the individual voting models are compared.
</nextsent>
<nextsent>subsequently, we analyze the degree of difference in voting bias of the kpca-based model from the others, and finally show that this does indeed usually lead to accuracy gains in the voting ensemble.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA429">
<title id=" W04-0822.xml">augmenting ensemble classification for word sense disambiguation with a kernel pca model </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 ensemble classification.
</prevsent>
<prevsent>the wsd models presented here consist of ensembles utilizing various combinations of four voting models, as follows.
</prevsent>
</prevsection>
<citsent citstr=" W04-0863 ">
some of these component models were also evaluated on other senseval-3 tasks: the basque, catalan, italian, and romanian lexical sample tasks (wicentowski et al , 2004), <papid> W04-0863 </papid>as well as semantic role labeling (ngai et al , 2004).<papid> W04-0845 </papid></citsent>
<aftsection>
<nextsent>the first voting model, nave bayes model, was built as yarowsky and florian (2002) found this model to be the most accurate classifier in comparative study on subset of senseval-2 english lexical sample data.
</nextsent>
<nextsent>the second voting model, maximum entropy model (jaynes, 1978), was built as klein and manning (2002) <papid> W02-1002 </papid>found that it yielded higher accuracy than nave bayes in subsequent comparison of wsd performance.</nextsent>
<nextsent>however, note that different subset of either senseval-1 or senseval-2 english lexical sample data was used.the third voting model, boosting model (fre und and schapire, 1997), was built as boosting has consistently turned in very competitive scores on related tasks such as named entity classification (car reras et al , 2002)(<papid> W02-2004 </papid>wu et al , 2002).<papid> W02-2035 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA430">
<title id=" W04-0822.xml">augmenting ensemble classification for word sense disambiguation with a kernel pca model </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 ensemble classification.
</prevsent>
<prevsent>the wsd models presented here consist of ensembles utilizing various combinations of four voting models, as follows.
</prevsent>
</prevsection>
<citsent citstr=" W04-0845 ">
some of these component models were also evaluated on other senseval-3 tasks: the basque, catalan, italian, and romanian lexical sample tasks (wicentowski et al , 2004), <papid> W04-0863 </papid>as well as semantic role labeling (ngai et al , 2004).<papid> W04-0845 </papid></citsent>
<aftsection>
<nextsent>the first voting model, nave bayes model, was built as yarowsky and florian (2002) found this model to be the most accurate classifier in comparative study on subset of senseval-2 english lexical sample data.
</nextsent>
<nextsent>the second voting model, maximum entropy model (jaynes, 1978), was built as klein and manning (2002) <papid> W02-1002 </papid>found that it yielded higher accuracy than nave bayes in subsequent comparison of wsd performance.</nextsent>
<nextsent>however, note that different subset of either senseval-1 or senseval-2 english lexical sample data was used.the third voting model, boosting model (fre und and schapire, 1997), was built as boosting has consistently turned in very competitive scores on related tasks such as named entity classification (car reras et al , 2002)(<papid> W02-2004 </papid>wu et al , 2002).<papid> W02-2035 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA431">
<title id=" W04-0822.xml">augmenting ensemble classification for word sense disambiguation with a kernel pca model </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>some of these component models were also evaluated on other senseval-3 tasks: the basque, catalan, italian, and romanian lexical sample tasks (wicentowski et al , 2004), <papid> W04-0863 </papid>as well as semantic role labeling (ngai et al , 2004).<papid> W04-0845 </papid></prevsent>
<prevsent>the first voting model, nave bayes model, was built as yarowsky and florian (2002) found this model to be the most accurate classifier in comparative study on subset of senseval-2 english lexical sample data.</prevsent>
</prevsection>
<citsent citstr=" W02-1002 ">
the second voting model, maximum entropy model (jaynes, 1978), was built as klein and manning (2002) <papid> W02-1002 </papid>found that it yielded higher accuracy than nave bayes in subsequent comparison of wsd performance.</citsent>
<aftsection>
<nextsent>however, note that different subset of either senseval-1 or senseval-2 english lexical sample data was used.the third voting model, boosting model (fre und and schapire, 1997), was built as boosting has consistently turned in very competitive scores on related tasks such as named entity classification (car reras et al , 2002)(<papid> W02-2004 </papid>wu et al , 2002).<papid> W02-2035 </papid></nextsent>
<nextsent>specifically, we employed an adaboost.mh model (schapire and singer, 2000), which is multi-class generalization of the original boosting algorithm, with boosting on top of decision stump classifiers (decision trees of depth one).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA432">
<title id=" W04-0822.xml">augmenting ensemble classification for word sense disambiguation with a kernel pca model </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>the first voting model, nave bayes model, was built as yarowsky and florian (2002) found this model to be the most accurate classifier in comparative study on subset of senseval-2 english lexical sample data.
</prevsent>
<prevsent>the second voting model, maximum entropy model (jaynes, 1978), was built as klein and manning (2002) <papid> W02-1002 </papid>found that it yielded higher accuracy than nave bayes in subsequent comparison of wsd performance.</prevsent>
</prevsection>
<citsent citstr=" W02-2004 ">
however, note that different subset of either senseval-1 or senseval-2 english lexical sample data was used.the third voting model, boosting model (fre und and schapire, 1997), was built as boosting has consistently turned in very competitive scores on related tasks such as named entity classification (car reras et al , 2002)(<papid> W02-2004 </papid>wu et al , 2002).<papid> W02-2035 </papid></citsent>
<aftsection>
<nextsent>specifically, we employed an adaboost.mh model (schapire and singer, 2000), which is multi-class generalization of the original boosting algorithm, with boosting on top of decision stump classifiers (decision trees of depth one).
</nextsent>
<nextsent>the fourth voting model, the kpca-based model, is described below.all classifier models were selected for their ability to able to handle large numbers of sparse features, many of which may be irrelevant.
</nextsent>
<nextsent>more over, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent.
</nextsent>
<nextsent>2.3 controlled feature set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA433">
<title id=" W04-0822.xml">augmenting ensemble classification for word sense disambiguation with a kernel pca model </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>the first voting model, nave bayes model, was built as yarowsky and florian (2002) found this model to be the most accurate classifier in comparative study on subset of senseval-2 english lexical sample data.
</prevsent>
<prevsent>the second voting model, maximum entropy model (jaynes, 1978), was built as klein and manning (2002) <papid> W02-1002 </papid>found that it yielded higher accuracy than nave bayes in subsequent comparison of wsd performance.</prevsent>
</prevsection>
<citsent citstr=" W02-2035 ">
however, note that different subset of either senseval-1 or senseval-2 english lexical sample data was used.the third voting model, boosting model (fre und and schapire, 1997), was built as boosting has consistently turned in very competitive scores on related tasks such as named entity classification (car reras et al , 2002)(<papid> W02-2004 </papid>wu et al , 2002).<papid> W02-2035 </papid></citsent>
<aftsection>
<nextsent>specifically, we employed an adaboost.mh model (schapire and singer, 2000), which is multi-class generalization of the original boosting algorithm, with boosting on top of decision stump classifiers (decision trees of depth one).
</nextsent>
<nextsent>the fourth voting model, the kpca-based model, is described below.all classifier models were selected for their ability to able to handle large numbers of sparse features, many of which may be irrelevant.
</nextsent>
<nextsent>more over, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent.
</nextsent>
<nextsent>2.3 controlled feature set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA435">
<title id=" W04-0822.xml">augmenting ensemble classification for word sense disambiguation with a kernel pca model </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>we briefly summarize the kpca-based model here; for full details including illustrative examples and graphical interpretation, please refer to wu et al  (2004).<papid> P04-1081 </papid>kernel pca kernel principal component analysis is nonlinear kernel method for extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are non linearly mapped from their original space rn to high-dimensional feature space where linear pca is performed, yielding transform by which the input vectors can be mapped non linearly to new set of vectors (scholkopf et al , 1998).</prevsent>
<prevsent>as with other kernel methods, major advantage of kpca over other common analysis techniques isthat it can inherently take combinations of predictive features into account when optimizing dimensionality reduction.</prevsent>
</prevsection>
<citsent citstr=" P03-1004 ">
for wsd and indeed many natural language tasks, significant accuracy gains can often be achieved by generalizing over relevant feature combinations (see, e.g., kudo and matsumoto(2003)).<papid> P03-1004 </papid></citsent>
<aftsection>
<nextsent>a further advantage of kpca in the context of the wsd problem is that the dimensionality of the input data is generally very large, condition where kernel methods excel.
</nextsent>
<nextsent>nonlinear principal components (diamantaras and kung, 1996) are defined as follows.
</nextsent>
<nextsent>suppose we are given training set of pairs (xt, ct) where the observed vectors xt ? rn in an n-dimensional input space represent the context of the target word being disambiguated, and the correct class ct represents the sense of the word, for = 1, ..,m . suppose ? is nonlinear mapping from the input space rn to the feature space . without loss of generality we assume the vectors are centered vectors in the feature space, i.e., t=1 ?(xt) = 0;uncentered vectors can easily be converted to centered vectors (scholkopf et al , 1998).
</nextsent>
<nextsent>we wish to diagonalize the co variance matrix in : = 1m ? j=1 ?(xj) (xj) (1) to do this requires solving the equation = cv for eigenvalues ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA438">
<title id=" W04-0835.xml">explorations in disambiguation using xml text representation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the descriptions below focus on the integration of disambiguation technology in larger system and do not present any advancements in this technology.
</prevsent>
<prevsent>1 the senseval-3 all-words task.
</prevsent>
</prevsection>
<citsent citstr=" W02-0807 ">
our procedures for performing this task and our results were largely unchanged from senseval-2 (litkowski, 2001; litkowski, 2002).<papid> W02-0807 </papid></citsent>
<aftsection>
<nextsent>our system is unsupervised, instead relying on information in whatever dictionary is being used to disambiguate the words.
</nextsent>
<nextsent>in this case, as in senseval-2, wordnet 1.7.1 was used.
</nextsent>
<nextsent>the main types of information used are default sense selection, idiomatic usage, syntactic and semantic clues, subcategorization patterns, word forms, syntactic usage, context, and topics or subject fields.
</nextsent>
<nextsent>as pointed out in litkowski (2002), <papid> W02-0807 </papid>the amount of information available in wordnet is problematic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA443">
<title id=" W04-0835.xml">explorations in disambiguation using xml text representation </title>
<section> automatic labeling of semantic roles.  </section>
<citcontext>
<prevsection>
<prevsent>words 15179 0.449 0.345
</prevsent>
<prevsent>the senseval-3 task to label sentence constituents with semantic roles was designed to replicate the tagging and identification of frame elements performed in the framenet project (johnson et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
this task was modeled on the study of automatic labeling by gildea &amp; jurafsky (2002), <papid> J02-3001 </papid>to allow other participants to investigate methods for assigning semantic roles.</citsent>
<aftsection>
<nextsent>that study was based on framenet 1.0, whereas this task used data from framenet 1.1, which considerably expanded the number of frames and the corpus sentences that were tagged by framenet lexicographers.
</nextsent>
<nextsent>the test data for this task consisted of 200 sentences that had been labeled with frame elements for 40 different frames.
</nextsent>
<nextsent>participants were provided with the sentences, the target word (along with its beginning and ending positions in the sentence), and the frame name (i.e., no attempt was made to determine the applicable frame).
</nextsent>
<nextsent>specific training data for the task consisted of all sentences not in the test set for the individual frame (ranging from slightly fewer than 200 sentences to as many as 1500 sentences).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA444">
<title id=" W04-2503.xml">using answer set programming to answer complex queries </title>
<section> framenet and events.  </section>
<citcontext>
<prevsection>
<prevsent>the semantic representation makes use of symbols based upon the lexicon of english.
</prevsent>
<prevsent>the success of our endeavor requires that there be an axiomatization of the relationship between the symbols representing functions and predicate symbols in our various ansprolog theories (e.g., m1 ? m4) and the symbols(based upon the lexicon of english) used in the semantic representation of the english queries and the narrative texts.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the online lexical database, framenet(baker et al, 1998) <papid> P98-1013 </papid>provides such connection, especially for events.</citsent>
<aftsection>
<nextsent>this is done through the notion of frame semantics that underlies framenet.
</nextsent>
<nextsent>frame semantics assumes that lexical items draw their meaning from conceptual structures or frames that provide an abstract or sce matic description of particular types of events.
</nextsent>
<nextsent>the frames are structured into an inheritance hierarchy.
</nextsent>
<nextsent>each frame includes number of frame elements (fes) or roles that make up the conceptual struc ture.for example, our travelling?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA445">
<title id=" W04-2503.xml">using answer set programming to answer complex queries </title>
<section> framenet and events.  </section>
<citcontext>
<prevsection>
<prevsent>the coverage of framenet is not sufficient.
</prevsent>
<prevsent>it will be necessary to augment our use of framenet with other online sources such as wordnet and to also increase the number of frames within framenet.
</prevsent>
</prevsection>
<citsent citstr=" P00-1065 ">
there has been some related work on using the frame of framenet for reasoning (chang et al, 2002) and also on the automatic annotation of english texts with regard to the relevant frames (gildea and jurafsky, 2000) <papid> P00-1065 </papid>and frame elements.</citsent>
<aftsection>
<nextsent>an ansprolog knowledge base consists of rules of the form: l0 ? l1, . . .
</nextsent>
<nextsent>, lm, not lm+1, . . .
</nextsent>
<nextsent>, not ln (4.1)where each of the lis is literal, i.e. an atom, a, or its classical negation, -a and not is logical connective called negation as failure or default negation.
</nextsent>
<nextsent>while -a states that is false, an expression not says that there is no reason to believe in l. the answer set semantics of logic program ? assigns to ? collection of answer sets ? consistent sets of ground liter als corresponding to beliefs which can be built by ara tional reasoner on the basis of rules of ?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA446">
<title id=" W04-0407.xml">representation and treatment of multiword expressions in basque </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this case, the interpretation as an instance of the mwlu would also be unambiguous.
</prevsent>
<prevsent>however, this srs only allows the inflection of the first component as absolut ive case (non-definite).
</prevsent>
</prevsection>
<citsent citstr=" P98-1063 ">
3.3 4 different information requirements in lemmatization and syntax processing the first prototype for the treatment of mwes in basque habil (ezeiza et al, 1998; <papid> P98-1063 </papid>ezeiza, 2003) was built for lemmatization purposes.</citsent>
<aftsection>
<nextsent>however, we are nowadays involved in the construction of deep syntactic parser (aduriz et al, 2004) and the mwes seem to need different treatment.
</nextsent>
<nextsent>the fact that many mwes may be syntactically regular but, above all, that an external element may have dependency relation with one of the constituents, forces us to analyze the elements independently.
</nextsent>
<nextsent>for example, in the verb beldur izan  to be afraid (of)  an external noun phrase may have modifier noun dependency relation with beldur  fear  as in sugeen beldur naiz  m afraid of snakes .
</nextsent>
<nextsent>in loak hartu  to fall asleep  there is subject-verb relation as in loak hartu nau  have fallen asleep , literally  sleep has caught me ; therefore subject-auxiliary verb agreement would fade if both components were analyzed as one.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA447">
<title id=" W04-0407.xml">representation and treatment of multiword expressions in basque </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some approaches treat them at tokenization stage, identifying fixed phrases, such as prepositional phrases or compounds, included in list (carmona et al, 1998; karlsson et al, 1995).
</prevsent>
<prevsent>other approaches relyon morphological analysis to better identify the features of the mwe using finite state technology (breidt et al, 1996).
</prevsent>
</prevsection>
<citsent citstr=" C94-1103 ">
finally, there is another approach that identifies them after the tagging process, allowing the correction of some tagging errors (leech et al, 1994).<papid> C94-1103 </papid></citsent>
<aftsection>
<nextsent>all of these approaches are based on the use of closed set of mwlus that could be included in list or database.
</nextsent>
<nextsent>however, some groups of mwes are not subject to be included in database, because they comprise an open class of expressions.
</nextsent>
<nextsent>that is the case of collocations, compounds or named entities.
</nextsent>
<nextsent>the group of collocations and compounds should be delimited using statistical approaches, such as xtract (smadja, 1993) <papid> J93-1007 </papid>or local max (silva et al, 1999), so that only the most relevant those of higher frequency?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA448">
<title id=" W04-0407.xml">representation and treatment of multiword expressions in basque </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, some groups of mwes are not subject to be included in database, because they comprise an open class of expressions.
</prevsent>
<prevsent>that is the case of collocations, compounds or named entities.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
the group of collocations and compounds should be delimited using statistical approaches, such as xtract (smadja, 1993) <papid> J93-1007 </papid>or local max (silva et al, 1999), so that only the most relevant those of higher frequency?</citsent>
<aftsection>
<nextsent>are included in the database.
</nextsent>
<nextsent>named entity recognition task has been solved for large set of languages.
</nextsent>
<nextsent>most of these works are linked to the message understanding conference (chinchor, 1997).
</nextsent>
<nextsent>there is variety of methods that have been used in ne recognition, such as hmm, maximum entropy models, decision trees, boosting and voted perceptron (collins, 2002), <papid> P02-1062 </papid>syntactic structure based approaches and wordnet-based approaches (magnini et al, 2002; <papid> W02-1109 </papid>arvalo, 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA449">
<title id=" W04-0407.xml">representation and treatment of multiword expressions in basque </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>named entity recognition task has been solved for large set of languages.
</prevsent>
<prevsent>most of these works are linked to the message understanding conference (chinchor, 1997).
</prevsent>
</prevsection>
<citsent citstr=" P02-1062 ">
there is variety of methods that have been used in ne recognition, such as hmm, maximum entropy models, decision trees, boosting and voted perceptron (collins, 2002), <papid> P02-1062 </papid>syntactic structure based approaches and wordnet-based approaches (magnini et al, 2002; <papid> W02-1109 </papid>arvalo, 2002).</citsent>
<aftsection>
<nextsent>most references on ne task might be accessed at http://www.muc.saic.com.
</nextsent>
<nextsent>4.1 processing mwes with habil we have implemented habil, tool for the treatment of multiword expressions (mwe), based 3 currently we are studying the mwlus in the lexical.
</nextsent>
<nextsent>database in order to determine which of them deserve to be parsed as separate elements.
</nextsent>
<nextsent>we have not defined yet how this will be formally represented in the database.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA450">
<title id=" W04-0407.xml">representation and treatment of multiword expressions in basque </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>named entity recognition task has been solved for large set of languages.
</prevsent>
<prevsent>most of these works are linked to the message understanding conference (chinchor, 1997).
</prevsent>
</prevsection>
<citsent citstr=" W02-1109 ">
there is variety of methods that have been used in ne recognition, such as hmm, maximum entropy models, decision trees, boosting and voted perceptron (collins, 2002), <papid> P02-1062 </papid>syntactic structure based approaches and wordnet-based approaches (magnini et al, 2002; <papid> W02-1109 </papid>arvalo, 2002).</citsent>
<aftsection>
<nextsent>most references on ne task might be accessed at http://www.muc.saic.com.
</nextsent>
<nextsent>4.1 processing mwes with habil we have implemented habil, tool for the treatment of multiword expressions (mwe), based 3 currently we are studying the mwlus in the lexical.
</nextsent>
<nextsent>database in order to determine which of them deserve to be parsed as separate elements.
</nextsent>
<nextsent>we have not defined yet how this will be formally represented in the database.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA451">
<title id=" W04-0213.xml">the potsdam commentary corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the paper explains the design decisions taken in the annotations, and describes number of applications using this corpus with its multi-layer annotation.
</prevsent>
<prevsent>a corpus of german newspaper commentaries has been assembled at potsdam university, and annotated with different linguistic information, to different degrees.
</prevsent>
</prevsection>
<citsent citstr=" W03-0909 ">
two aspects of the corpus have been presented in previous papers ((re itter, stede 2003) <papid> W03-0909 </papid>on underspecified rhetorical structure; (stede 2003) <papid> W03-0909 </papid>on the perspective of knowledge-based summarization).</citsent>
<aftsection>
<nextsent>this paper, however, provides comprehensive overview of the data collection effort and its current state.at present, the potsdam commentary cor pus?
</nextsent>
<nextsent>(henceforth pcc?
</nextsent>
<nextsent>for short) consists of 170 commentaries from markische allgemeine zeitung, german regional daily.
</nextsent>
<nextsent>the choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA461">
<title id=" W04-0213.xml">the potsdam commentary corpus </title>
<section> past, present, future applications.  </section>
<citcontext>
<prevsection>
<prevsent>future work along these lines will incorporate other layers of annotation, in particular the syntax information.
</prevsent>
<prevsent>9www.ling.uni-potsdam.de/sfb/ figure 2: screen shot of annis linguistic database 3.3 symbolic and knowledge-based.
</prevsent>
</prevsection>
<citsent citstr=" N03-2011 ">
rhetorical analysis we are experimenting with hybrid statistical and knowledge-based system for discourse parsing and summarization (stede 2003), (<papid> W03-0909 </papid>hanneforth et al 2003), <papid> N03-2011 </papid>again targeting the genre of commentaries.</citsent>
<aftsection>
<nextsent>the idea is to have pipeline of shallow-analysis modules (tagging, chunking, discourse parsing based on connectives) and map the resulting underspecified rhetorical tree (see section 2.4) into knowledge base that may contain domain and world knowledge for enriching the representation, e.g., to resolve references that cannot be handled by shallow methods, or to hypothesize coherence relations.
</nextsent>
<nextsent>in the rhetorical tree, nuclearity information is then used to extract kernel tree?
</nextsent>
<nextsent>that supposedly represents the key information from which the summary can be generated (which in turn may involve co-reference information, as we want to avoid dangling pronouns in summary).
</nextsent>
<nextsent>thus we are interested not in extraction, but actual generation from representations that may be developed to different degrees of granularity.in order to evaluate and advance this approach, it helps to feed into the knowledge base data that is already enriched with some of the desired information ? as in pcc.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA462">
<title id=" W04-0213.xml">the potsdam commentary corpus </title>
<section> past, present, future applications.  </section>
<citcontext>
<prevsection>
<prevsent>besides information structure, the second main goal is to enhance current models of rhetorical structure.
</prevsent>
<prevsent>as already pointed out in section 2.4, current theories diverge not only on the number and definition of relations but also on apects of structure, i.e., whether tree is sufficient as representational device or general graphs are required (and if so, whether any restrictions can be placed on these graphs structures ? cf.
</prevsent>
</prevsection>
<citsent citstr=" J03-4002 ">
(webber et al, 2003)).<papid> J03-4002 </papid></citsent>
<aftsection>
<nextsent>again, the idea is that having picture of syntax, co-reference, and sentence-internal information structure at ones disposal should aid in finding models of discourse structure that are more explanatory and can be empirically supported.
</nextsent>
<nextsent>the pcc is not the result of funded project.
</nextsent>
<nextsent>instead, the designs of the various annotation layers and the actual annotation work are results of series of diploma theses, of students?
</nextsent>
<nextsent>work in course projects, and to some extent of paid assistentships.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA463">
<title id=" W04-0910.xml">paraphrastic grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.
</prevsent>
<prevsent>for instance, (lin and pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
similarly, (barzi lay and lee, 2003) <papid> N03-1003 </papid>and (shinyanma et al, 2002) learn sentence level paraphrase templates from corpus of news articles stemming from different news source.</citsent>
<aftsection>
<nextsent>and (glickman and dagan, 2003) use clustering and similarity measures to identify similar contexts in single corpus and extract verbal paraphrases from these contexts.
</nextsent>
<nextsent>such machine learning approaches have known pros and cons.
</nextsent>
<nextsent>on the one hand, they produce large scale resources at little man labour cost.
</nextsent>
<nextsent>on the other hand, the degree of descriptive abstraction offered by the list of inference or paraphrase rules they output is low.we chose to investigate an alternative research direction by aiming to develop paraphrastic grammar?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA464">
<title id=" W04-0910.xml">paraphrastic grammars </title>
<section> developing paraphrase test suite.  </section>
<citcontext>
<prevsection>
<prevsent>indeed, when developing grammar, it is necessary to have some means of assessing both the coverage of the grammar (doesit generate all the sentences of the described lan guage?)
</prevsent>
<prevsent>and its degree of over generation (does it generate only the sentences of the described language?)
</prevsent>
</prevsection>
<citsent citstr=" H91-1060 ">
while corpus driven efforts along the parseval lines (black et al, 1991) <papid> H91-1060 </papid>are good at giving some measure of grammar coverage, they are not suitable for finer grained analysis and in particular,for progress evaluation, regression testing and comparative report generation.</citsent>
<aftsection>
<nextsent>another known method consists in developing and using test suite that is, set of negative and positive items against which the grammar can be systematically tested.
</nextsent>
<nextsent>for english, there is for instance the 15 year old hewlett packard test suite, simple text file listing test sentences and grouping them according to linguistics phenomena (flickinger et al, 1987); and more recently, the much more sophisticated tsnlp (testsuite for natural language processing) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information (oepen and flickinger, 1998).yet because they do not take into account these mantic dimension, none of these tools are adequate for evaluating the paraphrastic power of grammar.
</nextsent>
<nextsent>to remedy this, we propose to develop paraphrase test suite based on the paraphrase typology described in the previous section.
</nextsent>
<nextsent>in such test suite, test items pair semantic representation with setof paraphrases verbal ising this semantics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA465">
<title id=" W04-0910.xml">paraphrastic grammars </title>
<section> a paraphrastic grammar </section>
<citcontext>
<prevsection>
<prevsent>semantic grammars?
</prevsent>
<prevsent>already exist which describe not only the syntax but also the semantics of natural language.
</prevsent>
</prevsection>
<citsent citstr=" P01-1019 ">
thus for instance, (copestake and flickinger, 2000; copestake et al, 2001) <papid> P01-1019 </papid>describes head driven phrase structure grammar (hpsg) which supports the parallel construction of phrase structure (or derived) tree and of semantic representation and (dalrymple, 1999) show how to equip lexical functional grammar (lfg) with glue se mantics.</citsent>
<aftsection>
<nextsent>these grammars are both efficient and large scalein that they cover an important fragment of the natural language they describe and can be processed by parsers and generators in almost real time.
</nextsent>
<nextsent>for instance, the lfg grammar parses sentences from the wall street journal and the erg hpsg grammar will produce semantic representations for about 83 percent of the utterances in corpus of some 10 000 utterances varying in length between one and thirty words.
</nextsent>
<nextsent>parsing times vary between few ms for short sentences and several tens of seconds for longer ones.
</nextsent>
<nextsent>nonetheless, from semantics viewpoint, these grammars fail to yield clear account of the paraphrastic relation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA467">
<title id=" W04-0910.xml">paraphrastic grammars </title>
<section> a paraphrastic grammar </section>
<citcontext>
<prevsection>
<prevsent>thus the semantic representations we assume are simply set of liter als of the form n(x1, . . .
</prevsent>
<prevsent>, xn) where n is predicate of arity and xi is either constant or unification variable whose value will be instantiated during processing.
</prevsent>
</prevsection>
<citsent citstr=" E03-1030 ">
semantic construction proceeds from the derived tree (gardent and kallmeyer, 2003) <papid> E03-1030 </papid>rather than ? as is more common in tag ? from the derivation tree.</citsent>
<aftsection>
<nextsent>this is done by associating each elementary tree with semantic representation and by decorating relevant tree nodes with unification variable sand constants occuring in associated semantic representation.
</nextsent>
<nextsent>the association between tree nodes and unification variables encodes the syntax/semanticsinterface ? it specifies which node in the tree provides the value for which variable in the final semantic representation.
</nextsent>
<nextsent>as trees combine during derivation, (i) variables are unified ? both in the tree and in the associated semantic representation ? and (ii) the semantics of the derived tree is constructed from the conjunction of the semantics of the combined trees.
</nextsent>
<nextsent>a simple example will illustrate this.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA468">
<title id=" W04-0910.xml">paraphrastic grammars </title>
<section> a paraphrastic grammar </section>
<citcontext>
<prevsection>
<prevsent>cases of intra categorial synonymy are relativelystraigthtforward as several electronic synonym dic tionnaries for french are available (ploux, 1997).
</prevsent>
<prevsent>multiword expressions however remain problem as they are often not or only partially included insuch dictionnaries.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
for these or for specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (pereira et al, 1993; <papid> P93-1024 </papid>lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>techniques.for inter categorial synonymy involving derivational morphology link, some resources are available which however are only partial in that they only store morphological families that is, sets of items that are morphologically related.
</nextsent>
<nextsent>lexical semantics information still need to be included.intercategorial synonymy not involving deriva tional morphology link has been little studied and resources are lacking.
</nextsent>
<nextsent>however as for other types of synonymy, distributional analysis and clustering techniques can be used to develop such resources.
</nextsent>
<nextsent>for shuffling paraphrases, french alternations are partially described in (saint-dizier, 1999) andre source is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA469">
<title id=" W04-0910.xml">paraphrastic grammars </title>
<section> a paraphrastic grammar </section>
<citcontext>
<prevsection>
<prevsent>cases of intra categorial synonymy are relativelystraigthtforward as several electronic synonym dic tionnaries for french are available (ploux, 1997).
</prevsent>
<prevsent>multiword expressions however remain problem as they are often not or only partially included insuch dictionnaries.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
for these or for specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (pereira et al, 1993; <papid> P93-1024 </papid>lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>techniques.for inter categorial synonymy involving derivational morphology link, some resources are available which however are only partial in that they only store morphological families that is, sets of items that are morphologically related.
</nextsent>
<nextsent>lexical semantics information still need to be included.intercategorial synonymy not involving deriva tional morphology link has been little studied and resources are lacking.
</nextsent>
<nextsent>however as for other types of synonymy, distributional analysis and clustering techniques can be used to develop such resources.
</nextsent>
<nextsent>for shuffling paraphrases, french alternations are partially described in (saint-dizier, 1999) andre source is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA470">
<title id=" W04-0206.xml">discourse level annotation for investigating information structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>what is needed is further systematization of terminologies,formalization and computational modeling, and empirical and corpus-based studies.the goal of the muli (multilingual information structure) project is to contribute to this effort by empirically analyzing is in german and english newspaper texts.
</prevsent>
<prevsent>for this, we designed annotation schemes for enriching existing linguistically interpreted language resources with information at the levels of syntax, discourse semantics and prosody.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
the muli corpus consists of extracts from the tiger treebank for german (brants et al, to appear)1 and the penn treebank for english (marcus et al, 1994)<papid> H94-1020 </papid>2.</citsent>
<aftsection>
<nextsent>it comprises 250 sentences in german (app.
</nextsent>
<nextsent>3,500 tokens) and 320 sentences in english (app.
</nextsent>
<nextsent>7,000 tokens).
</nextsent>
<nextsent>the muli corpus has been created by extracting continuous stretch of 21 relatively short texts from the tiger treebank, and set of 10 texts from the penn treebank.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA471">
<title id=" W04-0206.xml">discourse level annotation for investigating information structure </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast to other projects in which is is annotated and investigated, we do not annotate theory-biased abstract categories like topic-focusor theme-rheme.
</prevsent>
<prevsent>since we are particularly interested in the correlations and co-occurrences of features on different linguistic levels that can be interpreted as indicators of the abstract is categories,we needed an annotation scheme to be as theory neutral as possible: it should allow for description of the phenomena, from which any?
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
theory specific explanatory mechanisms can subsequently be derived (skut et al, 1997).<papid> A97-1014 </papid></citsent>
<aftsection>
<nextsent>we therefore concentrate instead on features pertaining, on the onehand, to the surface realization of linguistic expressions (the levels of syntax and prosody), and, onthe other hand, to the semantic character of the discourse referents (the discourse level).in designing our annotation schemes, we followed the guidelines of the text encoding ini tiative3 and the discourse resource initiative (carletta et al, 1997).
</nextsent>
<nextsent>in line with these standards, we define for each annotation level (i) the mark able expressions, (ii) the attributes of markables, and (iii) the links between markables (if any).syntax the tiger treebank and the penn treebank we use as the starting point already contain syntactic information.
</nextsent>
<nextsent>the additional syntactic features annotated in the muli project pertain to clauses as mark able units, and encode the presence of structures with non canonical word order that typically serve to put the focus on certain syntactic elements.
</nextsent>
<nextsent>we include cleft, pseudo-cleft, reversed pseudo-cleft, extra position, fronting and expletives, as well as voice distinctions (active, medio passive and passive).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA473">
<title id=" W04-0206.xml">discourse level annotation for investigating information structure </title>
<section> conclusions and perspectives.  </section>
<citcontext>
<prevsection>
<prevsent>the corpus also makes it possible to construct computational models from the corpus data.theory-neutrality enhances re usability of linguistic resources, because it facilitates the integration with other, theory-neutral resources.
</prevsent>
<prevsent>to some extent we have already explored this in muli, combining e.g. tiger annotation with discourse-level annotation.
</prevsent>
</prevsection>
<citsent citstr=" P03-1068 ">
another possibility to explore is the to integrate muli annotation with, e.g., the salsa corpus (erk et al, 2003), <papid> P03-1068 </papid>which provides more detailed semantico-pragmatic information in the style of framenet.our initial investigation also reveals where additional annotation would be needed.</citsent>
<aftsection>
<nextsent>for instance,the text example discussed above constitutes concession scheme, which we cannot identify without annotating discourse/rhetorical relations.
</nextsent>
<nextsent>this in turn requires extending the annotation scheme to non-nominal markables.
</nextsent>
<nextsent>acknowledgement swe would like to thank saarland university for funding the muli pilot project.
</nextsent>
<nextsent>thanks also to stella neumann, erich steiner, elke teich, stefan baumann, caren brinckmann, silvia hansen-schirra and hans uszkoreit for discussions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA474">
<title id=" W04-2119.xml">application adaptive electronic dictionary with intelligent interface </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>transdict features powerful environment for acquisition, editing, browsing, defaulting and coherence checking.
</prevsent>
<prevsent>it is implemented in c++ as an integral part of 32-bit windows applications for windows 95/98/2000/nt.
</prevsent>
</prevsection>
<citsent citstr=" W03-2008 ">
1 autopat, aptrans, auto read, - computer systems for authoring, translation and improving readability of paten claims, correspondingly (sheremetyeva, 2003)<papid> W03-2008 </papid></citsent>
<aftsection>
<nextsent>a vast amount of research in the field of electronic dictionaries concentrate on data unification, representation, organization and management with the major focus on multilingual dictionaries as, for example, in (wong, 2000; boitet et al,2002).<papid> W02-1705 </papid></nextsent>
<nextsent>multilingual electronic dictionaries often include database of cross-referenced uni lingual dictionaries with the use of interlingua such as ontology (onyshkevich and nirenburg, 1994)) or pivotal language (boitet et al,cf.).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA475">
<title id=" W04-2119.xml">application adaptive electronic dictionary with intelligent interface </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it is implemented in c++ as an integral part of 32-bit windows applications for windows 95/98/2000/nt.
</prevsent>
<prevsent>1 autopat, aptrans, auto read, - computer systems for authoring, translation and improving readability of paten claims, correspondingly (sheremetyeva, 2003)<papid> W03-2008 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1705 ">
a vast amount of research in the field of electronic dictionaries concentrate on data unification, representation, organization and management with the major focus on multilingual dictionaries as, for example, in (wong, 2000; boitet et al,2002).<papid> W02-1705 </papid></citsent>
<aftsection>
<nextsent>multilingual electronic dictionaries often include database of cross-referenced uni lingual dictionaries with the use of interlingua such as ontology (onyshkevich and nirenburg, 1994)) or pivotal language (boitet et al,cf.).
</nextsent>
<nextsent>the architecture of such dictionaries normally include lexical database and set of tools for data management, - visualisers, editors, defaulters, etc.
</nextsent>
<nextsent>(khatchadourian, 1992).
</nextsent>
<nextsent>a user-friendly interface is one of the major issues still uderdeveloped (bilac and zock, 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA476">
<title id=" W04-1910.xml">bootstrapping parallel treebanks </title>
<section> bootstrapping german-swedish.  </section>
<citcontext>
<prevsection>
<prevsent>in 100 trials we observed 89 correct node labels and 93% correct edge labels (for305 edges).
</prevsent>
<prevsent>if we assume that manual inspection of correct suggestions takes about third of the time of manual annotation, and if we also assume that the correction of erroneous suggestions takes the same amount of time as manual annotation, then the employment of the german chunker for swedish saves about 60% of the annotation time.reusing chunker for bootstrapping parallel treebank between closely related languages like german and swedish is only first step towards reusing annotation (be it automatic or manual) in one language for another language.but it points to promising research direction.
</prevsent>
</prevsection>
<citsent citstr=" H01-1035 ">
(yarowsky et al, 2001) <papid> H01-1035 </papid>have reported interesting results of an annotation-projection technique for pos tagging, named entities and morphology.</citsent>
<aftsection>
<nextsent>and (cabezas et al, 2001) have explored projecting syntactic dependency relations from english to basque.
</nextsent>
<nextsent>this idea was followed by (hwa et al, 2002) <papid> P02-1050 </papid>who investigated english to chinese projections based on the direct correspondence assumption.</nextsent>
<nextsent>they conclude that annotation projections are nearly70% accurate (in terms of un labelled dependen cies) when some linguistic knowledge is used.we believe that annotation projection is difficult field but even if we only succeed in limited number of cases, it will be valuable for increased speed in the development of parallel treebanks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA477">
<title id=" W04-1910.xml">bootstrapping parallel treebanks </title>
<section> bootstrapping german-swedish.  </section>
<citcontext>
<prevsection>
<prevsent>(yarowsky et al, 2001) <papid> H01-1035 </papid>have reported interesting results of an annotation-projection technique for pos tagging, named entities and morphology.</prevsent>
<prevsent>and (cabezas et al, 2001) have explored projecting syntactic dependency relations from english to basque.</prevsent>
</prevsection>
<citsent citstr=" P02-1050 ">
this idea was followed by (hwa et al, 2002) <papid> P02-1050 </papid>who investigated english to chinese projections based on the direct correspondence assumption.</citsent>
<aftsection>
<nextsent>they conclude that annotation projections are nearly70% accurate (in terms of un labelled dependen cies) when some linguistic knowledge is used.we believe that annotation projection is difficult field but even if we only succeed in limited number of cases, it will be valuable for increased speed in the development of parallel treebanks.
</nextsent>
<nextsent>3.1 alignment.
</nextsent>
<nextsent>the alignment in our experimental treebank isbased on the nodes, not the edge labels.
</nextsent>
<nextsent>figure 1 shows the phrase alignment as thick lines across the trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA478">
<title id=" W04-0849.xml">class based collocations for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>three separate sources of word relatedness are used for these collocations: 1) wordnet hypernym relations; 2) cluster-based word similarity classes; and 3) dictionary definition analysis.
</prevsent>
<prevsent>supervised systems for word-sense disambiguation (wsd) often rely upon word collocations (i.e., sense-specific keywords) to provide clues on the most likely sense for word given the context.
</prevsent>
</prevsection>
<citsent citstr=" C02-1039 ">
in the second senseval competition, these features figured predominantly among the feature sets for the leading systems (mihalcea, 2002; <papid> C02-1039 </papid>yarowsky et al, 2001; seo et al, 2001).</citsent>
<aftsection>
<nextsent>a limitation of such features is that the words selected must occur in the test data in order for the features to apply.
</nextsent>
<nextsent>to alleviate this problem,class-based approaches augment word-level features with category-level ones (ide and veronis,1998; <papid> J98-1001 </papid>jurafsky and martin, 2000).</nextsent>
<nextsent>when applied to collocational features, this approach effectively uses class labels rather than word forms in deriving the collocational features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA479">
<title id=" W04-0849.xml">class based collocations for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the second senseval competition, these features figured predominantly among the feature sets for the leading systems (mihalcea, 2002; <papid> C02-1039 </papid>yarowsky et al, 2001; seo et al, 2001).</prevsent>
<prevsent>a limitation of such features is that the words selected must occur in the test data in order for the features to apply.</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
to alleviate this problem,class-based approaches augment word-level features with category-level ones (ide and veronis,1998; <papid> J98-1001 </papid>jurafsky and martin, 2000).</citsent>
<aftsection>
<nextsent>when applied to collocational features, this approach effectively uses class labels rather than word forms in deriving the collocational features.
</nextsent>
<nextsent>this research focuses on the determination of class-based collocations to improve word sense disambiguation.
</nextsent>
<nextsent>we do not address refinement of existing algorithms for machine learning.
</nextsent>
<nextsent>therefore, commonly used decision tree algorithm is employed to combine the various features when performing classification.this paper describes the nmsu-pitt unca system we developed for the third senseval competition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA480">
<title id=" W04-0849.xml">class based collocations for word sense disambiguation </title>
<section> system overview.  </section>
<citcontext>
<prevsection>
<prevsent>ity is 20% or higher: (p (s|w) ? (s)) (s) ? 0.20.
</prevsent>
<prevsent>this threshold was determined to be effective via an optimization search over the senseval-2 data.
</prevsent>
</prevsection>
<citsent citstr=" W98-1126 ">
word coll represents set of non-sensespecific collocations (i.e., not necessarily indicative of any one sense), chosen via the g2 criteria (wiebe et al, 1998).<papid> W98-1126 </papid></citsent>
<aftsection>
<nextsent>in contrast to word coll , each of which is separate binary feature, the words contained in the set word coll ? serve as values in single enumerated feature.these features are augmented with class based collocational features that represent information about word relationships derived from three separate sources: 1) wordnet (miller, 1990) hypernym relations (hypercoll);2) cluster-based word similarity classes (similarcoll); and 3) relatedness inferred from dictionary definition analysis (dictcoll).
</nextsent>
<nextsent>the information inherent in the sources from which these class-based features are derived allows words that do not occur in the training data context to be considered as collocations during classification.
</nextsent>
<nextsent>the hyper coll features are intended to capt urea portion of the information in the wordnet hypernyms links (i.e., is-a relations).
</nextsent>
<nextsent>hypernym based collocations are formulated by replacing each word in the context of the target word (e.g., in the same sentence as the target word) with its complete hypernym ancestry from wordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA482">
<title id=" W04-0849.xml">class based collocations for word sense disambiguation </title>
<section> class-based collocations.  </section>
<citcontext>
<prevsection>
<prevsent>(more of these features could be used, but they are limited for tractability.)
</prevsent>
<prevsent>for more details on hypernym collocations, see (ohara, forthcoming).
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
word-similarity classes (lin, 1998) <papid> P98-2127 </papid>derived from clustering are also used to expand thepool of potential collocations; this type of semantic relatedness among words is expressed inthe similar coll feature.</citsent>
<aftsection>
<nextsent>for the dictcoll features, definition analysis (ohara, forthcoming) is used to determine the semantic relatedness of the defining words.
</nextsent>
<nextsent>differences between these two sources of word relations are illustrated by looking at the information they provide for bal lerina?: word-clusters: dancer:0.115 baryshnikov:0.072 pianist:0.056 choreographer:0.049 ...
</nextsent>
<nextsent>[18 other words] nicole:0.041 wrestler:0.040 tibetans:0.040 clown:0.040 definition words: dancer:0.0013 female:0.0013 ballet:0.0004 this shows that word clusters capture wider range of relatedness than the dictionary definitions at the expense of incidental associations (e.g., nicole?).
</nextsent>
<nextsent>again, because context words are not disambiguated, the relations for all senses of context word are conflated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA486">
<title id=" W04-0204.xml">on the use of automatic tools for largescale semantic analyses of causal connectives </title>
<section> techniques and tools.  </section>
<citcontext>
<prevsection>
<prevsent>the causal segments the extraction of the relevant linguistic material was fulfilled by automatic syntactic analysis techniques.
</prevsent>
<prevsent>as basis for our analyses we worked with the first six months of dutch newspaper corpus of more than 30 million words2.
</prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
this material was pos-tagged using mbt (memory based tagger) (daelemans et al,1996).<papid> W96-0102 </papid></citsent>
<aftsection>
<nextsent>we then discarded the items with few content words: sports results, television programs, crosswords and puzzles, stock exchange reports, service information from the newspaper editor, etc. we also cleaned?
</nextsent>
<nextsent>the corpus material of irregularities caused by the incompatibility between the source file and the tagging program (mostly nonsense words generated by the program).
</nextsent>
<nextsent>this eventually led to dataset of approximately 16,500,000 words.
</nextsent>
<nextsent>the pos-tagging permitted to segment the corpus in sentences and to label the words grammatically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA487">
<title id=" W04-0204.xml">on the use of automatic tools for largescale semantic analyses of causal connectives </title>
<section> techniques and tools.  </section>
<citcontext>
<prevsection>
<prevsent>landauer et al (1998) stress that this technique can be viewed from two sides.
</prevsent>
<prevsent>at theoretical level, it is meant to be used to develop simulations of the cognitive processes running during language comprehension, including, for instance, computational model of metaphor treatment (kintsch, 2000 ; lemaire et al, 2001), but also to analyse the coherence of texts (foltz et al., 1998 ; pirard et al, 2004).
</prevsent>
</prevsection>
<citsent citstr=" W01-0514 ">
at more applied level, it is technique which enables to infer and to represent the meaning of words on the basis of their actual use in text so that the similarity of the meaning of words, sentences or paragraphs can be estimated (bestgen, 2002; choi et al, 2001).<papid> W01-0514 </papid></citsent>
<aftsection>
<nextsent>it is this latter aspect which draws our attention here.
</nextsent>
<nextsent>the point of departure of the analysis is lexical table (lebart and salem, 1992) containing the frequencies of every word in each of the documents included in the text material, document being text, paragraph, or sentence.
</nextsent>
<nextsent>to derive semantic relations between words from the lexical table the analysis of mere co-occurrences will not do, the major problem being that even in large corpus most words are relatively rare.
</nextsent>
<nextsent>consequently the co-occurrences of words are even rarer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA488">
<title id=" W04-0204.xml">on the use of automatic tools for largescale semantic analyses of causal connectives </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the second is that the analysis is mostly fully automatic, especially with respect to the coding of the fragments.
</prevsent>
<prevsent>it is especially this latter feature that should appeal to the linguistic community, and makes our method more robust.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
the interco der reliability is constant concern of everyone working with corpora to test linguistic hypotheses (car letta, 1996), <papid> J96-2004 </papid>and the more so when one is coding for semanto-pragmatic interpretations, as in the case of the analysis of connectives.</citsent>
<aftsection>
<nextsent>a third reason is that our method combines two techniques of automatic text analysis, which allows us to formulate our hypotheses to be tested more fine-grained than possible with either one separately.
</nextsent>
<nextsent>moreover, hypothesis formulation and testing goes further: we can use the methodology to formulate new hypotheses.
</nextsent>
<nextsent>an interesting possibility is to use lsa to find neighbours of terms in the dictionary, thus extending the dictionary.
</nextsent>
<nextsent>a further interesting venue is to test the linguistic hypotheses for different genres.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA489">
<title id=" W04-1902.xml">inflectional syncretism and corpora </title>
<section> morphological theory and frequency.  </section>
<citcontext>
<prevsection>
<prevsent>in other words we use the sum of the token occurrences of the following ambiguous forms: i) all dative and locative singular occurrences of class ii nouns; ii) all nominative and accusative singular occurrences of class iv nouns, class iii nouns and class nouns which are inanimate; iii) all accusative and genitive singular occurrences of class nouns which are animate; iv) all genitive, dative and locative singular nouns from class iii.
</prevsent>
<prevsent>it is likely that one can disambiguate most morphological syncretisms in russian readily from the syntactic context, but our purpose is to demonstrate how often morphologically well endowed language such as russian still leaves much work to syntax.
</prevsent>
</prevsection>
<citsent citstr=" J96-2002 ">
irregularity was implemented in the lexical knowledge representation language datr (evans and gazdar 1996) <papid> J96-2002 </papid>and is available at the datr archive from the datr web pages (http://www.datr.org).</citsent>
<aftsection>
<nextsent>the morphological model is structured as hierarchy in which information is pushed as far up the hierarchy as it can go capturing as many generalisations as possible.
</nextsent>
<nextsent>thus at the top of the hierarchy in figure 2 we find information associated with all nouns (such as the inflections for the dative, instrumental and locative plural) and that information is propagated to others by inheritance, and at the bottom we find information which is unique to particular instances.
</nextsent>
<nextsent>node n_i (representing class i) and node n_iv (representing class iv) both inherit from node n_0 (representing class o), sharing the inflections for the genitive, dative, instrumental and locative singular.
</nextsent>
<nextsent>datr is default inheritance formalism, which means that information specified under particular class node takes precedence over that what is inherited, overriding the inherited information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA490">
<title id=" W04-0208.xml">temporal discourse models for narrative structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we can see that even for interpreting such relatively simple discourses, system might require variety of sources of linguistic knowledge, including knowledge of tense, aspect, temporal adverbials, discourse relations, as well as background knowledge.
</prevsent>
<prevsent>of course, other inferences are clearly possible, e.g., that the running stopped after the twisting, but when viewed as defaults, these latter inferences seem to be more easily violated.
</prevsent>
</prevsection>
<citsent citstr=" E95-1035 ">
the need for commonsense inferences has motivated computational approaches that are domain specific, using hand-coded knowledge (e.g., asher and lascarides 2003, hitzeman et al 1995).<papid> E95-1035 </papid></citsent>
<aftsection>
<nextsent>a number of theories have postulated the existence of various discourse relations that relate elements in the text to produce global model of discourse, e.g., (mann and thompson 1988), (hobbs 1985), (hovy 1990) <papid> W90-0117 </papid>and others.</nextsent>
<nextsent>in rst (mann and thompson 1988), (marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA491">
<title id=" W04-0208.xml">temporal discourse models for narrative structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>of course, other inferences are clearly possible, e.g., that the running stopped after the twisting, but when viewed as defaults, these latter inferences seem to be more easily violated.
</prevsent>
<prevsent>the need for commonsense inferences has motivated computational approaches that are domain specific, using hand-coded knowledge (e.g., asher and lascarides 2003, hitzeman et al 1995).<papid> E95-1035 </papid></prevsent>
</prevsection>
<citsent citstr=" W90-0117 ">
a number of theories have postulated the existence of various discourse relations that relate elements in the text to produce global model of discourse, e.g., (mann and thompson 1988), (hobbs 1985), (hovy 1990) <papid> W90-0117 </papid>and others.</citsent>
<aftsection>
<nextsent>in rst (mann and thompson 1988), (marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses.
</nextsent>
<nextsent>in sdrt (asher and lascarides 2003), these relations are between representations of propositional content, called discourse representation structures (kamp and reyle, 1993).
</nextsent>
<nextsent>despite considerable amount of very productive research, annotating such discourse relations has proved problematic.
</nextsent>
<nextsent>this is due to the fact that discourse markers may be absent (i.e., implicit) or ambiguous; but more importantly, because in many cases the precise nature of these discourse relations is unclear.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA492">
<title id=" W04-0208.xml">temporal discourse models for narrative structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>c is set of temporal ordering constraints using the ordering relation,   (temporal precedence) as well as (for states, clarified below) minimal restrictions?
</prevsent>
<prevsent>on the above temporal inclusion relation (expressed as min).
</prevsent>
</prevsection>
<citsent citstr=" W01-1605 ">
al. 1999) (carlson et al 2001) <papid> W01-1605 </papid>reported relatively high levels of inter-annotator agreement, this was based on an annotation procedure where the annotators were allowed to iteratively revise the instructions based on joint discussion.</citsent>
<aftsection>
<nextsent>while we appreciate the importance of representing rhetorical relations in order to carry out temporal inferences about event ordering, we believe that there are substantial advantages in isolating the temporal aspects and modeling them separately as tdms.
</nextsent>
<nextsent>this greatly simplifies the representation, which we discuss next.
</nextsent>
<nextsent>a tdm is tree-structured syntactic model of global discourse structure, where temporal relations are used as surrogates for discourse relations, and where abstract events corresponding to entire discourses are introduced as nodes in the tree.
</nextsent>
<nextsent>in (1) the embedding nodes e0 and e1 were abstract, but textually mentioned events can also create embed dings, as in (2) (example from (spejewski 1988)): we begin by illustrating the basic intuition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA493">
<title id=" W04-0208.xml">temporal discourse models for narrative structure </title>
<section> temporal discourse models.  </section>
<citcontext>
<prevsection>
<prevsent>a tdm is tree-structured syntactic model of global discourse structure, where temporal relations are used as surrogates for discourse relations, and where abstract events corresponding to entire discourses are introduced as nodes in the tree.
</prevsent>
<prevsent>in (1) the embedding nodes e0 and e1 were abstract, but textually mentioned events can also create embed dings, as in (2) (example from (spejewski 1988)): we begin by illustrating the basic intuition.
</prevsent>
</prevsection>
<citsent citstr=" J88-2006 ">
consider discourse (1), from (webber 1988): (<papid> J88-2006 </papid>2) a. edmond made his own christmas presents this year.</citsent>
<aftsection>
<nextsent>b. first he dried bunch of tomatoes in his oven.
</nextsent>
<nextsent>c. then he made booklet of recipes that use dried tomatoes.
</nextsent>
<nextsent>d. he scanned in the recipes from his gourmet magazines.
</nextsent>
<nextsent>e. he gave these gifts to his family.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA494">
<title id=" W04-0208.xml">temporal discourse models for narrative structure </title>
<section> temporal discourse models.  </section>
<citcontext>
<prevsection>
<prevsent>note that the mentioned events are ordered left to right in text order for notational convenience, but no temporal ordering is directly represented in the tree.
</prevsent>
<prevsent>since the nodes in this representation are at semantic level, the tree structure is not necessarily isomorphic to representation at the text level, although t1 happens to be isomorphic.
</prevsent>
</prevsection>
<citsent citstr=" W01-1311 ">
c2 = {ea   ee, eb   ec} note that the partial ordering can be extended using and temporal closure axioms (setzer and gaizauskas 2001), (<papid> W01-1311 </papid>verhagen 2004), so that in the case of  t2, c2 , we can infer, for example, that eb   ed, ed   ee, and so forth.</citsent>
<aftsection>
<nextsent>in representing states, we take conservative approach to the problems of ramification and change (mccarthy and hayes 1969).
</nextsent>
<nextsent>this is the classic problem of recognizing when states (the effects of actions) change as result of actions.
</nextsent>
<nextsent>any tensed stative predicate will be represented as node in the tree (progressives are here treated as stative).
</nextsent>
<nextsent>consider an example like john walked home.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA496">
<title id=" W04-0208.xml">temporal discourse models for narrative structure </title>
<section> prerequisites.  </section>
<citcontext>
<prevsection>
<prevsent>likewise, time expressions are flagged, and their values normalized, so that thursday in he left on thursday would get resolved iso time value depending on context (timex2 2004).
</prevsent>
<prevsent>finally, temporal relations between events and time expressions (e.g., that the leaving occurs during thursday) are recorded by means of temporal links (tlinks) that express allen-style interval relations (allen 1984).
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
several automatic tools have been developed in conjunction with timeml, including event taggers (pustejovsky et al 2003), time expression taggers (mani and wilson 2000), <papid> P00-1010 </papid>and an exploratory link extractor (mani et al 2003).<papid> N03-2019 </papid></citsent>
<aftsection>
<nextsent>temporal reasoning algorithms have also been developed, that apply transit ivity axioms to expand the links using temporal closure algorithms (setzer and gaizauskas 2001), (<papid> W01-1311 </papid>pustejovsky et al 2003).</nextsent>
<nextsent>however, timeml is inadequate as temporal model of discourse: it constructs no global representation of the narrative structure, instead annotating complex graph that links primitive events and times.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA497">
<title id=" W04-0208.xml">temporal discourse models for narrative structure </title>
<section> prerequisites.  </section>
<citcontext>
<prevsection>
<prevsent>likewise, time expressions are flagged, and their values normalized, so that thursday in he left on thursday would get resolved iso time value depending on context (timex2 2004).
</prevsent>
<prevsent>finally, temporal relations between events and time expressions (e.g., that the leaving occurs during thursday) are recorded by means of temporal links (tlinks) that express allen-style interval relations (allen 1984).
</prevsent>
</prevsection>
<citsent citstr=" N03-2019 ">
several automatic tools have been developed in conjunction with timeml, including event taggers (pustejovsky et al 2003), time expression taggers (mani and wilson 2000), <papid> P00-1010 </papid>and an exploratory link extractor (mani et al 2003).<papid> N03-2019 </papid></citsent>
<aftsection>
<nextsent>temporal reasoning algorithms have also been developed, that apply transit ivity axioms to expand the links using temporal closure algorithms (setzer and gaizauskas 2001), (<papid> W01-1311 </papid>pustejovsky et al 2003).</nextsent>
<nextsent>however, timeml is inadequate as temporal model of discourse: it constructs no global representation of the narrative structure, instead annotating complex graph that links primitive events and times.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA505">
<title id=" W03-2911.xml">morphosyntactic clues for terminological processing in serbian </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although naming conventions do exist for some types of concepts (e.g. gene and protein names in biomedicine), these are only guidelines and as such do not impose restrictions to domain experts, who frequently introduce ad-hoc terms.
</prevsent>
<prevsent>thus, the lack of clear naming conventions makes the automatic term recognition (atr) task difficult even for languages that are not morphologically and derivation ally rich.
</prevsent>
</prevsection>
<citsent citstr=" C94-2167 ">
atr tools have been developed for english (frantzi et al, 2000), french (jacquemin, 2001), japanese (nakagawa and mori, 2000), etc. some methods rely purely on linguistic information, namely morpho-syntactic features of term candidates (ananiadou, 1994).<papid> C94-2167 </papid></citsent>
<aftsection>
<nextsent>hybrid approaches combining linguistic patterns and statistical measures (e.g.
</nextsent>
<nextsent>(frantzi et al, 2000)) and ma chine-learning techniques (e.g.
</nextsent>
<nextsent>(hatzivassiloglou et al, 2001)) have been also used.
</nextsent>
<nextsent>however, few studies have been done for morphologically rich slavic languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA506">
<title id=" W04-1408.xml">bilingual concordancers and translation memories a comparative evaluation </title>
<section> general introduction to bcs and tms.  </section>
<citcontext>
<prevsection>
<prevsent>problems can arise, for example, if single source text sentence has been translated by multiple target language sentences, or vice versa, or if information has been omitted from or added to the target text (e.g. to handle cultural references).
</prevsent>
<prevsent>paragraphs.
</prevsent>
</prevsection>
<citsent citstr=" J93-1004 ">
alignment at the sentence level is achieved by applying the gale-church algorithm (gale and church 1993).<papid> J93-1004 </papid></citsent>
<aftsection>
<nextsent>to make adjustments to the alignment, the user can examine the aligned segments and either merge or split particular segments, as necessary.
</nextsent>
<nextsent>one important thing to note is that the aligned units remain situated within the larger surrounding text.
</nextsent>
<nextsent>once the texts are aligned, the translator can consult the corpus.
</nextsent>
<nextsent>by choosing the basic search command, the translator can retrieve all examples of word or phrase (or part of word) from the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA507">
<title id=" W04-0802.xml">the senseval3 multilingual englishadhindi lexical sample task </title>
<section> open mind word expert.  </section>
<citcontext>
<prevsection>
<prevsent>for the english target words are their translations in hindi.
</prevsent>
<prevsent>this paper outlines some of the major issues that arose in the creation of this task, and then describes the participating systems and summarizes their results.
</prevsent>
</prevsection>
<citsent citstr=" W02-0817 ">
the annotated corpus required for this task was built using the open mind word expert system(chklovski and mihalcea, 2002), <papid> W02-0817 </papid>adapted for multilingual annotations 1.</citsent>
<aftsection>
<nextsent>to overcome the current lack of tagged data and the limitations imposed by the creation of such data using trained lexicographers, the open mind word 1multilingual open mind word expert can be accessed at http://teach-computers.org/word-expert/english-hindi expert system enables the collection of semantically annotated corpora over the web.
</nextsent>
<nextsent>tagged examples are collected using web-based application that allows contributors to annotate words with their meanings.
</nextsent>
<nextsent>the tagging exercise proceeds as follows.
</nextsent>
<nextsent>foreach target word the system extracts set of sentences from large textual corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA508">
<title id=" W04-1018.xml">chinese text summarization based on thematic area detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it is ideal to employ this method while dealing with those documents with standard discourse structure, because it can effectively avoid the problems caused by the summarization method without discourse structure analysis.
</prevsent>
<prevsent>yet when the writing style of document is rather free and the distribution of the themes is variable, that is the same theme can be distributed in several paragraphs not adjacent to each other, then the use of this method cant be equally effective.
</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
to deal with lot of chinese documents which have free style of writing and flexible themes, sentence-extraction summarization method created by detecting thematic areas is tried following such work as (nomoto and matsumoto, 2001; salton et al., 1996; salton et al, 1997; carbonell and goldstein, 1998; lin and hovy, 2000).<papid> C00-1072 </papid></citsent>
<aftsection>
<nextsent>the thematic areas detection in document is obtained through the adaptive clustering of paragraphs (cf.
</nextsent>
<nextsent>moens et al 1999), so it can overcome in certain degree the defects of the above methods in dealing with the documents with rather flexible theme distribution.
</nextsent>
<nextsent>in this section, the proposed method will be introduced in detail.
</nextsent>
<nextsent>the method consists of the following three main stages: stage 1: find the different thematic areas in the document through paragraph clustering and clustering analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA509">
<title id=" W04-1018.xml">chinese text summarization based on thematic area detection </title>
<section> the algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 stage 2: selection of the thematic.
</prevsent>
<prevsent>representative sentences to select most suitable representative sentence from each thematic area, the author proposes the following method.
</prevsent>
</prevsection>
<citsent citstr=" W00-0403 ">
this is in contrast with method proposed by radev (radev et al, 2000 ), <papid> W00-0403 </papid>where the centro id of cluster is selected as the representative one.</citsent>
<aftsection>
<nextsent>method: select the sentence which is most similar to the thematic area semantically as representative one.
</nextsent>
<nextsent>before carrying out the method in detail, there are two problems to be solved: 1) the vector representation of sentence and thematic area the vector representation of sentence and thematic area is similar to that of paragraph introduced before.
</nextsent>
<nextsent>we only need to change the weight calculation field of the terms from the interior of paragraph to the interior of sentence or thematic area.
</nextsent>
<nextsent>accordingly, we can describe the sentence vector and thematic area vector as follows vsj= ( wsj1,wsj2,?,wsjn) vak= ( wak1,wak2,?,wakn) 2) the semantic similarity calculation between sentence and thematic area the calculation of semantic similarity of sentence and thematic area can be achieved by calculating the vector distance between sentence vector and thematic area vector.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA510">
<title id=" W04-0105.xml">priors in bayesian learning of phonological rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our goal is to learn transformation rules of the formx ? / , where and are individual characters (or the empty character ) and is some representation of the context licensing the transformation.
</prevsent>
<prevsent>our input is an existing segmentation of words from the penn treebank (marcus et al, 93) into stems and suffixes.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
this segmentation is provided by the linguist ica morphological analyzer (goldsmith, 2001; <papid> J01-2001 </papid>goldsmith, 2004b), itself an unsupervised algorithm.</citsent>
<aftsection>
<nextsent>using the transformation rules we learn, we are able to output new segmentation that more closely matches our linguistic intuitions.1 we are not the first to apply bayesian learning techniques for unsupervised learning of morphology and phonology.
</nextsent>
<nextsent>several other researchers have also pursued these methods, usually within minimum description length (mdl) framework (ris sanen, 1989).
</nextsent>
<nextsent>in mdl approaches, ? log pr(h) is taken to be proportional to the length of in some standard encoding, and ? log pr(d|h) is the length of using the encoding specified by . mdl-based systems have been relatively successful for tasks including word segmentation (de marcken, 1996; brent and cartwright, 1996), morphological1since we use ordinary text, rather than phonological transcriptions, as input, the rules we learn are really spelling rules, not phonological rules.
</nextsent>
<nextsent>we believe that the work discussed here would be equally applicable, and possibly more successful, with phonological transcriptions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA514">
<title id=" W04-0105.xml">priors in bayesian learning of phonological rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lift jump roll walk . . .
</prevsent>
<prevsent> ed ing . . .
</prevsent>
</prevsection>
<citsent citstr=" W02-0603 ">
figure 1: an example signature segmentation (goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus,2002), <papid> W02-0603 </papid>discovery of syllabicity and sonority (ellison, 1993), and learning constraints on vowel harmony and consonant clusters (ellison, 1994).</citsent>
<aftsection>
<nextsent>how ever, our work shows that straightforward mdl approach, where the prior ? log pr(h) depends on the length of the phonological rules and the rest of the grammar in the obvious way, does not result in successful system for learning phonological rules.we discuss why this is so, and then present several changes that can be made to the prior in orderto learn phonological rules successfully.
</nextsent>
<nextsent>our conclusion is that, although bayesian techniques canbe successful for unsupervised learning of linguistic information, careful choice of the prior, with attention to both linguistic and statistical factors, is important.
</nextsent>
<nextsent>in the remainder of this paper, we first review the basics behind goldsmiths linguist ica program, which serves as the starting point for our own work.we then explain the additional framework necessary for learning phonological rules, and describe our search algorithm.
</nextsent>
<nextsent>in section 5, we describe the results of two experiments using our search algorithm, first with an mdl prior, then with modifiedprior.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA516">
<title id=" W04-0105.xml">priors in bayesian learning of phonological rules </title>
<section> search algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>given set of similar signature pairs, the rules relating them, and the possible contexts for those rules, we need to determine which rules are actually phonologically legitimate and which are simply accidents of the data.
</prevsent>
<prevsent>we do this by simply considering each rule and context in turn, proceeding from the most attested to least attested rules and from most likely to least likely contexts.
</prevsent>
</prevsection>
<citsent citstr=" P84-1070 ">
for each rule-context pair, we add the rule to the grammar 4the reasoning we use to finding conditioning contexts for deletion rules was also described by goldsmith (2004a), and is similar to the much earlier work of johnson (1984).<papid> P84-1070 </papid></citsent>
<aftsection>
<nextsent>findphonorules() 1 g?
</nextsent>
<nextsent>grammar produced by linguist ica 2 r?
</nextsent>
<nextsent>ordered set of possible rules 3 for each ? 4 do 6 ? ?
</nextsent>
<nextsent>7 while cr 6= ? 8 do c?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA521">
<title id=" W04-2208.xml">multilingual aligned parallel treebank corpus reflecting contextual information and its applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this framework, parallel translations whose source language sentence is similar to given sentence can be semi automatically generated.
</prevsent>
<prevsent>in this paper we show that the framework can be achieved by using our aligned parallel treebank corpus.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
recently, accurate machine translation systems can be constructed by using parallel corpora (och and ney, 2000; <papid> P00-1056 </papid>germann et al, 2001).<papid> P01-1030 </papid>however, almost all existing machine translation systems do not consider the problem of translating given sentence into natural sentence reflecting its contextual information in the target language.</citsent>
<aftsection>
<nextsent>one of the main reasons for this is that we had many problems that had tobe solved by one-sentence to one-sentence machine translation before we could solve the contextual problem.
</nextsent>
<nextsent>another reason is that it was difficult to simply investigate the influence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-to-one, and are usually one-to-many or many-to-many.
</nextsent>
<nextsent>on the other hand, high-quality treebanks such as the penn treebank (marcus et al, 1993)<papid> J93-2004 </papid>and the kyoto university text corpus (kuro hashi and nagao, 1997) have contributed to improving the accuracies of fundamental techniques for natural language processing such as morphological analysis and syntactic structureanalysis.</nextsent>
<nextsent>however, almost all of these high quality treebanks are based on monolingual corpora and do not have bilingual or multilingual information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA522">
<title id=" W04-2208.xml">multilingual aligned parallel treebank corpus reflecting contextual information and its applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this framework, parallel translations whose source language sentence is similar to given sentence can be semi automatically generated.
</prevsent>
<prevsent>in this paper we show that the framework can be achieved by using our aligned parallel treebank corpus.
</prevsent>
</prevsection>
<citsent citstr=" P01-1030 ">
recently, accurate machine translation systems can be constructed by using parallel corpora (och and ney, 2000; <papid> P00-1056 </papid>germann et al, 2001).<papid> P01-1030 </papid>however, almost all existing machine translation systems do not consider the problem of translating given sentence into natural sentence reflecting its contextual information in the target language.</citsent>
<aftsection>
<nextsent>one of the main reasons for this is that we had many problems that had tobe solved by one-sentence to one-sentence machine translation before we could solve the contextual problem.
</nextsent>
<nextsent>another reason is that it was difficult to simply investigate the influence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-to-one, and are usually one-to-many or many-to-many.
</nextsent>
<nextsent>on the other hand, high-quality treebanks such as the penn treebank (marcus et al, 1993)<papid> J93-2004 </papid>and the kyoto university text corpus (kuro hashi and nagao, 1997) have contributed to improving the accuracies of fundamental techniques for natural language processing such as morphological analysis and syntactic structureanalysis.</nextsent>
<nextsent>however, almost all of these high quality treebanks are based on monolingual corpora and do not have bilingual or multilingual information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA523">
<title id=" W04-2208.xml">multilingual aligned parallel treebank corpus reflecting contextual information and its applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the main reasons for this is that we had many problems that had tobe solved by one-sentence to one-sentence machine translation before we could solve the contextual problem.
</prevsent>
<prevsent>another reason is that it was difficult to simply investigate the influence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-to-one, and are usually one-to-many or many-to-many.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
on the other hand, high-quality treebanks such as the penn treebank (marcus et al, 1993)<papid> J93-2004 </papid>and the kyoto university text corpus (kuro hashi and nagao, 1997) have contributed to improving the accuracies of fundamental techniques for natural language processing such as morphological analysis and syntactic structureanalysis.</citsent>
<aftsection>
<nextsent>however, almost all of these high quality treebanks are based on monolingual corpora and do not have bilingual or multilingual information.
</nextsent>
<nextsent>there are few high-qualitybilingual or multilingual treebank corpora be cause parallel corpora have mainly been actively used for machine translation between related languages such as english and french, therefore their syntactic structures are not require dso much for aligning words or phrases.
</nextsent>
<nextsent>however, syntactic structures are necessary for machine translation between languages whose syntactic structures are different from each other, such as in japanese-english, japanese-chinese,and chinese-english machine translations, be cause it is more difficult to automatically align words or phrases between two unrelated languages than between two related languages.
</nextsent>
<nextsent>actually, it has been reported that syntactic structures contribute to improving the accuracy of word alignment between japanese and english (yamada and knight, 2001).<papid> P01-1067 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA524">
<title id=" W04-2208.xml">multilingual aligned parallel treebank corpus reflecting contextual information and its applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are few high-qualitybilingual or multilingual treebank corpora be cause parallel corpora have mainly been actively used for machine translation between related languages such as english and french, therefore their syntactic structures are not require dso much for aligning words or phrases.
</prevsent>
<prevsent>however, syntactic structures are necessary for machine translation between languages whose syntactic structures are different from each other, such as in japanese-english, japanese-chinese,and chinese-english machine translations, be cause it is more difficult to automatically align words or phrases between two unrelated languages than between two related languages.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
actually, it has been reported that syntactic structures contribute to improving the accuracy of word alignment between japanese and english (yamada and knight, 2001).<papid> P01-1067 </papid></citsent>
<aftsection>
<nextsent>therefore, if we had high-quality parallel treebank corpus, the accuracies of machine translation between languages whose syntactic structures are different from each other would improve.
</nextsent>
<nextsent>further more, if the parallel treebank corpus had word or phrase alignment, the accuracy of automatic word or phrase alignment would increase by using the parallel treebank corpus as trainingdata.
</nextsent>
<nextsent>however, so far, there is no aligned parallel treebank corpus whose domain is notre stricted.
</nextsent>
<nextsent>for example, the japanese electronics industry development associations (jeidas) bilingual corpus (isahara and haruno, 2000)has sentence, phrase, and proper noun alignment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA527">
<title id=" W04-2208.xml">multilingual aligned parallel treebank corpus reflecting contextual information and its applications </title>
<section> translations of proper nouns.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, in many cases, english sentences canbe generated just by reordering the selected expressions.
</prevsent>
<prevsent>the english word order was estimated manually in this experiment.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
however,we can automatically estimate english word order by using language model or an english surface sentence generator such as fergus (bangalore and rambow, 2000).<papid> C00-1007 </papid></citsent>
<aftsection>
<nextsent>unnatural or ungrammatical parallel translations are sometimes generated in the above steps.
</nextsent>
<nextsent>however, comprehensible translations can be generated as shown in figure 4.
</nextsent>
<nextsent>the biggest advantage of this framework is that comprehensible target sentences can be generated basically by refer ring only to source sentences.
</nextsent>
<nextsent>although it is costly to search and select appropriate translation pairs, we believe that human labor can be reduced by developing human interface.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA528">
<title id=" W04-2208.xml">multilingual aligned parallel treebank corpus reflecting contextual information and its applications </title>
<section> translations of proper nouns.  </section>
<citcontext>
<prevsection>
<prevsent>the biggest advantage of this framework is that comprehensible target sentences can be generated basically by refer ring only to source sentences.
</prevsent>
<prevsent>although it is costly to search and select appropriate translation pairs, we believe that human labor can be reduced by developing human interface.
</prevsent>
</prevsection>
<citsent citstr=" C02-1064 ">
for example, when we use japanese text generation system from keywords (uchimoto et al,2002), <papid> C02-1064 </papid>users should only select appropriate key words.</citsent>
<aftsection>
<nextsent>we are investigating whether or not we can generate similar parallel translations to all of the japanese sentences appearing on january17, 1995.
</nextsent>
<nextsent>so far, we found that we can generate similar parallel translations to 691 out of 840 sentences (the average number of bunsetsu sis about 10.3) including the 102 sentences described in section 3.3.
</nextsent>
<nextsent>we found that we could not generate similar parallel translations to 149 out of 840 sentences.in the proposed framework of similar parallel translation generation, the language appearing in corpus corresponds to controlled language, and users are allowed to use only the controlled language to write sentences in the source language.
</nextsent>
<nextsent>we believe that high-qualitybilingual or multilingual documents can be generated by letting us adapt ourselves to the controlled environment in this way.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA529">
<title id=" W04-1903.xml">the szeged corpus a pos tagged and syntactically annotated hungarian natural language corpus </title>
<section> annotation of the szeged corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the applicability of these algorithms in hungarian nlp was extensively studied in the past couple of years (horvth et al, 1999), (hcza et al, 2003).
</prevsent>
<prevsent>researchers of the university of szeged experimented with different kind of pos tagging methods and compared their results based on accuracy.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
brills transformation based learning method (brill, 1995) <papid> J95-4004 </papid>worked with 96.52% per word accuracy when trained and tested on the corpus.</citsent>
<aftsection>
<nextsent>the hmm-based tnt tagger (brants, 2000) <papid> A00-1031 </papid>performed 96.18%, while the rglearn rule-based tagger (hcza et al, 2003) produced 94.54% accuracy.</nextsent>
<nextsent>researchers also experimented with the combination of the different learning methods in order to increase accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA530">
<title id=" W04-1903.xml">the szeged corpus a pos tagged and syntactically annotated hungarian natural language corpus </title>
<section> annotation of the szeged corpus.  </section>
<citcontext>
<prevsection>
<prevsent>researchers of the university of szeged experimented with different kind of pos tagging methods and compared their results based on accuracy.
</prevsent>
<prevsent>brills transformation based learning method (brill, 1995) <papid> J95-4004 </papid>worked with 96.52% per word accuracy when trained and tested on the corpus.</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
the hmm-based tnt tagger (brants, 2000) <papid> A00-1031 </papid>performed 96.18%, while the rglearn rule-based tagger (hcza et al, 2003) produced 94.54% accuracy.</citsent>
<aftsection>
<nextsent>researchers also experimented with the combination of the different learning methods in order to increase accuracy.
</nextsent>
<nextsent>the best accuracy result, delivered by combining the above three methods, was 96.95%.
</nextsent>
<nextsent>overall results showed that despite the agglutinating nature of hungarian language and the structural differences between hungarian and other indo european languages, all of the mentioned methods can be used effectively for learning pos tagging.
</nextsent>
<nextsent>the applicability of machine learning methods for learning np recognition rules was also investigated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA531">
<title id=" W04-1903.xml">the szeged corpus a pos tagged and syntactically annotated hungarian natural language corpus </title>
<section> annotation of the szeged corpus.  </section>
<citcontext>
<prevsection>
<prevsent>5 related work.
</prevsent>
<prevsent>corpus-based methods play an important role in empirical linguistics as well as in the application of machine learning algorithms.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
annotated reference corpora, such as the brown corpus (kucera, francis, 1967), the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>and the bnc (leech et al, 2001.), have helped both the development of english computational linguistics tools and english corpus linguistics.</citsent>
<aftsection>
<nextsent>manual pos tagging and syntactic annotation are costly but allow one to build and improve sizable linguistic resources and also to train and evaluate automated analysers.
</nextsent>
<nextsent>the negra (skut at al., 1997) pos tagged.
</nextsent>
<nextsent>and syntactically annotated corpus of 355 thousand tokens was the first initiative in corpus linguistics for german.
</nextsent>
<nextsent>the more recent tiger treebank project (brants et al, 2002) aims at building the largest and most extensively annotated treebank for german.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA532">
<title id=" W04-1903.xml">the szeged corpus a pos tagged and syntactically annotated hungarian natural language corpus </title>
<section> annotation of the szeged corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the more recent tiger treebank project (brants et al, 2002) aims at building the largest and most extensively annotated treebank for german.
</prevsent>
<prevsent>currently, it comprises 700 thousand tokens of newspaper text that were automatically analysed and manually checked.
</prevsent>
</prevsection>
<citsent citstr=" E03-2015 ">
considerable results were achieved for czech in the framework of the prague dependency treebank project (hajic, 1998), and for bulgarian in the bultreebank project (simov et al, 2003) <papid> E03-2015 </papid>as well.</citsent>
<aftsection>
<nextsent>the szeged corpus project is comparable both in size and indepth of analysis to the corpus and treebank initiatives mentioned above7.
</nextsent>
<nextsent>as the first such like initiative for hungarian language, it is valuable source for linguistic research and suitable training and testing basis for machine applications and automated induction of linguistic knowledge.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA533">
<title id=" W04-0859.xml">the university of alicante systems at senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at senseval-2 the average level of accuracy achieved rounded 70%, which is insufficient for such other nlp tasks as information retrieval, machine translation, or question answering.
</prevsent>
<prevsent>the dlsi-ua systems were applied to threesenseval-3 tasks: english all-words, english lexical sample and spanish lexical sample.
</prevsent>
</prevsection>
<citsent citstr=" H93-1021 ">
our systems use both corpus-based and knowledge-based approaches: maximum entropy(me) (lau et al, 1993; <papid> H93-1021 </papid>berger et al, 1996; <papid> J96-1002 </papid>ratnaparkhi, 1998) is corpus-based and supervised method based on linguistic features; me is the core of bootstrapping algorithm that we call re-training inspired ? this paper has been partially supported by the spanish government (cicyt) under project number tic-2003-7180 and the valencia government (ocyt) under project number ctidib-2002-151by co-training (blum and mitchell, 1998); relevant domains (rd) (montoyo et al, 2003) is resource built from wordnet domains (magniniand cavaglia, 2000) that is used in an unsupervised method that assigns domain and sense labels; specification marks(sp) (montoyo and palomar, 2000) exploits the relations between synsets stored in wordnet (miller et al, 1993) and does not need any training corpora; commutative test (ct)(nica et al, 2003), based on the sense discrimi nators device derived from ewn (vossen, 1998), disambiguates nouns inside their syntactic patterns,with the help of information extracted from raw cor pus.</citsent>
<aftsection>
<nextsent>a resume of which methods and how were used in which senseval-3 tasks is shown in table 1.
</nextsent>
<nextsent>dlsi-ua method combined systems results all-nosu rd no ls-eng-su re-t nols-eng nosu rd no ls-spa-su me+re-t no ls-spa-nosu sm + me nouns: sm verbs and adj.: me ls-spa- pattern-nica nouns: sm pattern + me verbs and adj.: me table 1: dlsi-ua systems at senseval-3 most of these methods are relatively new and ourgoal when participating at senseval-3 is to evaluate for the first time such approaches.
</nextsent>
<nextsent>at the moment of writing this paper we can conclude that these are promising contributions in order to im prove current wsd systems.
</nextsent>
<nextsent>in the following section each method is described briefly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA534">
<title id=" W04-0859.xml">the university of alicante systems at senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at senseval-2 the average level of accuracy achieved rounded 70%, which is insufficient for such other nlp tasks as information retrieval, machine translation, or question answering.
</prevsent>
<prevsent>the dlsi-ua systems were applied to threesenseval-3 tasks: english all-words, english lexical sample and spanish lexical sample.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
our systems use both corpus-based and knowledge-based approaches: maximum entropy(me) (lau et al, 1993; <papid> H93-1021 </papid>berger et al, 1996; <papid> J96-1002 </papid>ratnaparkhi, 1998) is corpus-based and supervised method based on linguistic features; me is the core of bootstrapping algorithm that we call re-training inspired ? this paper has been partially supported by the spanish government (cicyt) under project number tic-2003-7180 and the valencia government (ocyt) under project number ctidib-2002-151by co-training (blum and mitchell, 1998); relevant domains (rd) (montoyo et al, 2003) is resource built from wordnet domains (magniniand cavaglia, 2000) that is used in an unsupervised method that assigns domain and sense labels; specification marks(sp) (montoyo and palomar, 2000) exploits the relations between synsets stored in wordnet (miller et al, 1993) and does not need any training corpora; commutative test (ct)(nica et al, 2003), based on the sense discrimi nators device derived from ewn (vossen, 1998), disambiguates nouns inside their syntactic patterns,with the help of information extracted from raw cor pus.</citsent>
<aftsection>
<nextsent>a resume of which methods and how were used in which senseval-3 tasks is shown in table 1.
</nextsent>
<nextsent>dlsi-ua method combined systems results all-nosu rd no ls-eng-su re-t nols-eng nosu rd no ls-spa-su me+re-t no ls-spa-nosu sm + me nouns: sm verbs and adj.: me ls-spa- pattern-nica nouns: sm pattern + me verbs and adj.: me table 1: dlsi-ua systems at senseval-3 most of these methods are relatively new and ourgoal when participating at senseval-3 is to evaluate for the first time such approaches.
</nextsent>
<nextsent>at the moment of writing this paper we can conclude that these are promising contributions in order to im prove current wsd systems.
</nextsent>
<nextsent>in the following section each method is described briefly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA535">
<title id=" W04-0859.xml">the university of alicante systems at senseval3 </title>
<section> methods and algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>this is an unsupervised wsd method based on the wordnet domains lexical resource (magnini and cavaglia, 2000).
</prevsent>
<prevsent>the underlying working hypothesis is that domain labels, such as architecture, sport and medicine provide natural way to establish semantic relations between word senses, that can be used during the disambiguation process.
</prevsent>
</prevsection>
<citsent citstr=" W00-0804 ">
this resource has already been used onword sense disambiguation (magnini and strapparava, 2000), <papid> W00-0804 </papid>but it has not made use of glosses infor mation.</citsent>
<aftsection>
<nextsent>so our approach make use of new lexical resource obtained from glosses information named relevant domains.first step is to obtain the relevant domains resource from wordnet glosses.
</nextsent>
<nextsent>for this task is necessary previous part-of-speech tagging of wordnet glosses (each gloss has associated domain la bel).
</nextsent>
<nextsent>so we extract all nouns, verbs, adjectives andadverbs from glosses and assign them their associated domain label.
</nextsent>
<nextsent>with this information and using the association ratio formula (w=word,d=domainlabel), in (1), we obtain the relevant domains resource.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA536">
<title id=" W04-1220.xml">posbiotmner in the shared task of bionlpnlpba 2004 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and recently several svm-based named entity recognition models have been proposed.
</prevsent>
<prevsent>lee et. al.
</prevsent>
</prevsection>
<citsent citstr=" W03-1305 ">
([lee et. al., 2003]) <papid> W03-1305 </papid>proposed two-phrase svm recognition model.</citsent>
<aftsection>
<nextsent>yamamoto et. al.
</nextsent>
<nextsent>([yamamoto et. al., 2003]) <papid> W03-1309 </papid>proposed svm-based recognition method which uses various morphological information and input features such as base noun phrase information, stemmed forms of word, etc. however, notable limitation of svm is its low speed both for training and recognition.</nextsent>
<nextsent>on the other hand, conditional random fields (crfs) ([lafferty, 2001]) is probabilistic framework for labelling and segmenting sequential data, which is much faster comparing with svm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA537">
<title id=" W04-1220.xml">posbiotmner in the shared task of bionlpnlpba 2004 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>([lee et. al., 2003]) <papid> W03-1305 </papid>proposed two-phrase svm recognition model.</prevsent>
<prevsent>yamamoto et. al.</prevsent>
</prevsection>
<citsent citstr=" W03-1309 ">
([yamamoto et. al., 2003]) <papid> W03-1309 </papid>proposed svm-based recognition method which uses various morphological information and input features such as base noun phrase information, stemmed forms of word, etc. however, notable limitation of svm is its low speed both for training and recognition.</citsent>
<aftsection>
<nextsent>on the other hand, conditional random fields (crfs) ([lafferty, 2001]) is probabilistic framework for labelling and segmenting sequential data, which is much faster comparing with svm.
</nextsent>
<nextsent>the conditional probability of the label sequence can depend on arbitrary, non-independent features of the observation sequence without forcing the model to account for the distribution of those dependencies.
</nextsent>
<nextsent>named entity recognition problem can be taken as assigning the named entity class tag sequences to the input sentences.
</nextsent>
<nextsent>we adopt crf to be the complementary scheme of svm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA538">
<title id=" W04-1304.xml">grammatical inference and first language acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then discuss the applicability of these results to parametric and non parametric models.
</prevsent>
<prevsent>for some years, the relevance of formal results in grammatical inference to the empirical question of first language acquisition by infant children hasbeen recognised (wexler and culicover, 1980).
</prevsent>
</prevsection>
<citsent citstr=" C88-1001 ">
unfortunately, for many researchers, with few no table exceptions (abe, 1988), <papid> C88-1001 </papid>this begins and ends with golds famous negative results in the identification in the limit paradigm.</citsent>
<aftsection>
<nextsent>this paradigm, though still widely used in the grammatical inference community, is clearly of limited relevance to the issue at hand, since it requires the model to be able to exactly identify the target language even when an adversary can pick arbitrarily misleading sequences of examples to provide.
</nextsent>
<nextsent>moreover, the paradigm as stated has no bounds on the amount of data or computation required for the learner.
</nextsent>
<nextsent>inspite of the inapplicability of this particular paradigm, in suitable analysis there are quite strong arguments that bear directly on this problem.
</nextsent>
<nextsent>grammatical inference is the study of machine learning of formal languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA539">
<title id=" W04-1304.xml">grammatical inference and first language acquisition </title>
<section> applying gi to fla.  </section>
<citcontext>
<prevsection>
<prevsent>when dealing with regular languages, for example, though the class of languages defined by deterministic automata is the same as that defined by non-deterministic languages, the same is not true for their stochastic variants.
</prevsent>
<prevsent>additionally, one can have exponential blow-ups in the number of states when determinising automata.
</prevsent>
</prevsection>
<citsent citstr=" P99-1070 ">
similarly,with cfgs, (abney et al, 1999) <papid> P99-1070 </papid>showed that converting between two parametrisations of stochastic context free languages are equivalent but that there are blow-ups in both directions.</citsent>
<aftsection>
<nextsent>we do not have completely satisfactory solution to this problem at the moment; an alternative is to consider learning the distributions rather than the languages.
</nextsent>
<nextsent>in the case of learning distributions, we have thesame framework, but the samples are drawn according to the distribution being learned , and we require that the hypothesis has small divergence from the target: d(t ||h)   .
</nextsent>
<nextsent>since the divergence is infinite if the hypothesis gives probability zero to string in the target, this will have the consequence that the target must assign non-zero probability to every string.
</nextsent>
<nextsent>now that we have fairly clear idea of various waysof formalising the situation we can consider the extent to which formal results apply.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA540">
<title id=" W04-1304.xml">grammatical inference and first language acquisition </title>
<section> positive results.  </section>
<citcontext>
<prevsection>
<prevsent>since these are acyclic the languages they define are always finite.
</prevsent>
<prevsent>this additional criterion of distin guishability suffices to guarantee learnability.
</prevsent>
</prevsection>
<citsent citstr=" C04-1013 ">
this work can be extended to cyclic automata (clark and thollard, 2004<papid> C04-1013 </papid>a; clark and thollard, 2004<papid> C04-1013 </papid>b), andthus the class of all regular languages, with the addition of further parameter which bounds the expected length of string generated from any state.</citsent>
<aftsection>
<nextsent>the use of distinguish ability seems innocuous; in syntactic terms it is consequence of the plausible condition that for any pair of distinct non-terminals there is some fairly likely string generated by one and not the other.
</nextsent>
<nextsent>similarly strings of symbols in natural language tend to have limited length.
</nextsent>
<nextsent>an alternate way of formalising this is to define classof distinguishable automata, where the distinguish ability of the automata is lower bounded by an inverse polynomial in the number of states.
</nextsent>
<nextsent>this is formally equivalent, but avoids adding terms to the sample complexity polynomial.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA544">
<title id=" W04-2614.xml">fine grained lexical semantic representations and compositionally derived events in mandarin chinese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in machine translation, good?
</prevsent>
<prevsent>representation of verbs can straightforwardly capture cross-linguistic divergences in the expression of arguments.
</prevsent>
</prevsection>
<citsent citstr=" C00-2148 ">
in question answering, lexical semantics can be leveraged to bridge the gap between the way question is asked and the way an answer is stated.this paper explores fine-grained lexical semantic representations approaches that view verb as more than simple predicate of its arguments (e.g., dang et al, 2000).<papid> C00-2148 </papid></citsent>
<aftsection>
<nextsent>this contrasts with recent semantic annotation projects such as propbank (kingsbury and palmer, 2002) and framenet (baker et al, 1998).<papid> P98-1013 </papid></nextsent>
<nextsent>for example, while it is undeniable that throw(john, the ball, mary), is valid representation for the sentence john threw the ball to mary?, it is widely believed (at least by theoretical linguists) that decomposing verbs in terms of more basic primitives can better capture generalizations about verb meaning and argument realization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA545">
<title id=" W04-2614.xml">fine grained lexical semantic representations and compositionally derived events in mandarin chinese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>representation of verbs can straightforwardly capture cross-linguistic divergences in the expression of arguments.
</prevsent>
<prevsent>in question answering, lexical semantics can be leveraged to bridge the gap between the way question is asked and the way an answer is stated.this paper explores fine-grained lexical semantic representations approaches that view verb as more than simple predicate of its arguments (e.g., dang et al, 2000).<papid> C00-2148 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
this contrasts with recent semantic annotation projects such as propbank (kingsbury and palmer, 2002) and framenet (baker et al, 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>for example, while it is undeniable that throw(john, the ball, mary), is valid representation for the sentence john threw the ball to mary?, it is widely believed (at least by theoretical linguists) that decomposing verbs in terms of more basic primitives can better capture generalizations about verb meaning and argument realization.
</nextsent>
<nextsent>i will argue that finer-grained semantics is not only theoretically motivated, but necessary for building applications.i first provide brief overview of theories of verbal argument structure, and then contrast the typology of mandarin verbs with that of english verbs.
</nextsent>
<nextsent>i will present evidence from chinese that verb meaning is compositionally built up?
</nextsent>
<nextsent>from primitive notions of stat ivity and activity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA546">
<title id=" W04-2614.xml">fine grained lexical semantic representations and compositionally derived events in mandarin chinese </title>
<section> event types.  </section>
<citcontext>
<prevsection>
<prevsent>why is it, for example, that love takes an obligatory agent and an obligatory patient why is the instrument role in open optional?
</prevsent>
<prevsent>these theories cannot offer satisfactory answers because they do not directly refer to the meaning of predicates.
</prevsent>
</prevsection>
<citsent citstr=" J91-4003 ">
recognizing the drawbacks of theories based purely on semantic roles, there is now general consensus among linguists that argument structure is (to large extent)predictable from event semantics hence, patterns of argument realization should be infer able from lexical semantic representations grounded in theory of events.these event representations typically decompose seman 1see (dowty, 1991) and (levin and rappaport hovav, 1996) tic roles in terms of primitive predicates representing concepts such as causality, agent ivity, inchoativity, and stat ivity (dowty, 1979; jackendoff, 1983; pustejovsky, 1991<papid> J91-4003 </papid>b; rappaport hovav and levin, 1998).</citsent>
<aftsection>
<nextsent>although aristotle (metaphysics 1048b) observed thatthe meanings of some verbs involve an end?
</nextsent>
<nextsent>or result?, and other do not, it wasnt until the twentieth century that philosophers and linguists developed classification of event types which captures logical entail ments and the co-occurrence restrictions between verb sand other syntactic elements such as tenses and adver bials.
</nextsent>
<nextsent>vendlers (1957) classification of events into states,activities, accomplishments, and achievements is groundbreaking in this respect.
</nextsent>
<nextsent>in his event ontology, activities and states both depict situations that are inherently temporally unbounded (atelic); states denote static situations, whereas activities denote on-going dynamic situations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA547">
<title id=" W04-2614.xml">fine grained lexical semantic representations and compositionally derived events in mandarin chinese </title>
<section> the mandarin verbal system.  </section>
<citcontext>
<prevsection>
<prevsent>this contrasts with my analysis of it as signal of inchoativity.
</prevsent>
<prevsent>how are these two approaches to be reconciled?
</prevsent>
</prevsection>
<citsent citstr=" N04-2004 ">
in (lin, 2004<papid> N04-2004 </papid>b),i argue that le is reflex, rather than an overt realization of the underlying inchoative marker.</citsent>
<aftsection>
<nextsent>as generally defined, perfect ive aspect is not compatible with stative predicates.
</nextsent>
<nextsent>however, the addition of covert inchoative functional head, in effect, licenses the perfect ive aspect.
</nextsent>
<nextsent>why is this peculiar organization of the mandarin verbal system important for lexical semantic representations designed for language applications?
</nextsent>
<nextsent>it demonstrates that, at least for languages such as mandarin chinese, the verb phrase must be rich in internal structure; verb cannot be simply viewed as predicate of its arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA565">
<title id=" W04-1205.xml">zone identification in biology articles as a basis for information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we explore qualitative analysis of zone identification (zi) in biology articles and provide stronger support for our framework toward automatic annotation of zones.
</prevsent>
<prevsent>specifically we; 1) illustrate the linguistic and other features of each zone, which have been extracted through our pilot study of total of 20 articles randomly selected from four major online journals (embo, pnas, nar and jcb), 2) discuss controversial cases for zi and nested annotation to elaborate the scheme, 3) discuss multiple features relevant to zi, and 4) summarize the investigation and outline future steps related to machine learning and applications.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
previous work on rhetorical analysis of scientific articles focus on either; 1) hierarchical discourse relations between sentences (e.g. mann and thompson, 1987), 2) genre analysis within descriptive framework (e.g. swales 1990), or 3) zi in flat structure and statistical evaluation of the annotation scheme from machine learning perspective (e.g. teufel and moens, 2002).<papid> J02-4002 </papid></citsent>
<aftsection>
<nextsent>we follow the lines of (teufel and moens, 2002) <papid> J02-4002 </papid>and apply zi to the domain of biology.</nextsent>
<nextsent>but our approach is unique in that we focus on experimental results and on qualitative analysis of zi as basis for automatic zi.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA567">
<title id=" W04-0104.xml">automatic acquisition of feature based phonotactic resources </title>
<section> phonotactic automata and typed.  </section>
<citcontext>
<prevsection>
<prevsent>these additional type label tapes are discussed in more detail in the following section.
</prevsent>
<prevsent>feature structures lexical knowledge representation in computationalphonology has already made extensive use of inheritance hierarchies to model lexical generalisations ranging from higher level prosodic categories to the phonological segment.
</prevsent>
</prevsection>
<citsent citstr=" J96-2002 ">
in contrast to the approach presented in this section, the work described in (cahill et al, 2000) is set in an untyped feature system using datr to define inheritance networks with path-value equations (evans and gaz dar, 1996).<papid> J96-2002 </papid></citsent>
<aftsection>
<nextsent>the merits of applying type discipline even to untyped feature structures is considered in wintner and sarkar (2002) <papid> J02-3005 </papid>from general perspective and in neugebauer (2003b) with special reference to phonological lexica.</nextsent>
<nextsent>previous proposals to cast phonological structure in typed feature system can be found in bird and klein (1994) <papid> J94-3010 </papid>and walther (1999).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA568">
<title id=" W04-0104.xml">automatic acquisition of feature based phonotactic resources </title>
<section> phonotactic automata and typed.  </section>
<citcontext>
<prevsection>
<prevsent>feature structures lexical knowledge representation in computationalphonology has already made extensive use of inheritance hierarchies to model lexical generalisations ranging from higher level prosodic categories to the phonological segment.
</prevsent>
<prevsent>in contrast to the approach presented in this section, the work described in (cahill et al, 2000) is set in an untyped feature system using datr to define inheritance networks with path-value equations (evans and gaz dar, 1996).<papid> J96-2002 </papid></prevsent>
</prevsection>
<citsent citstr=" J02-3005 ">
the merits of applying type discipline even to untyped feature structures is considered in wintner and sarkar (2002) <papid> J02-3005 </papid>from general perspective and in neugebauer (2003b) with special reference to phonological lexica.</citsent>
<aftsection>
<nextsent>previous proposals to cast phonological structure in typed feature system can be found in bird and klein (1994) <papid> J94-3010 </papid>and walther (1999).</nextsent>
<nextsent>however, there are two major differences with regard to ourwork.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA569">
<title id=" W04-0104.xml">automatic acquisition of feature based phonotactic resources </title>
<section> phonotactic automata and typed.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast to the approach presented in this section, the work described in (cahill et al, 2000) is set in an untyped feature system using datr to define inheritance networks with path-value equations (evans and gaz dar, 1996).<papid> J96-2002 </papid></prevsent>
<prevsent>the merits of applying type discipline even to untyped feature structures is considered in wintner and sarkar (2002) <papid> J02-3005 </papid>from general perspective and in neugebauer (2003b) with special reference to phonological lexica.</prevsent>
</prevsection>
<citsent citstr=" J94-3010 ">
previous proposals to cast phonological structure in typed feature system can be found in bird and klein (1994) <papid> J94-3010 </papid>and walther (1999).</citsent>
<aftsection>
<nextsent>however, there are two major differences with regard to ourwork.
</nextsent>
<nextsent>first, while types may denote sets of segments, we go beyond the idea of sets as arc labels infinite-state automata (bird and ellison, 1994; <papid> J94-1003 </papid>eisner, 1997; <papid> P97-1040 </papid>van noord and gerdemann, 2001) which says that boolean combinations of finitely-valued features can be stored as set on just one arc, rather than being multiplied out as disjunctive collection of arcs.</nextsent>
<nextsent>this choice has no theoretical consequences but is merely convenience for grammar development (bird and ellison, 1994).<papid> J94-1003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA570">
<title id=" W04-0104.xml">automatic acquisition of feature based phonotactic resources </title>
<section> phonotactic automata and typed.  </section>
<citcontext>
<prevsection>
<prevsent>previous proposals to cast phonological structure in typed feature system can be found in bird and klein (1994) <papid> J94-3010 </papid>and walther (1999).</prevsent>
<prevsent>however, there are two major differences with regard to ourwork.</prevsent>
</prevsection>
<citsent citstr=" J94-1003 ">
first, while types may denote sets of segments, we go beyond the idea of sets as arc labels infinite-state automata (bird and ellison, 1994; <papid> J94-1003 </papid>eisner, 1997; <papid> P97-1040 </papid>van noord and gerdemann, 2001) which says that boolean combinations of finitely-valued features can be stored as set on just one arc, rather than being multiplied out as disjunctive collection of arcs.</citsent>
<aftsection>
<nextsent>this choice has no theoretical consequences but is merely convenience for grammar development (bird and ellison, 1994).<papid> J94-1003 </papid></nextsent>
<nextsent>the difference in our approach consists in the hierarchical ordering of types (or sets) which relates each arc label to any other type in given phonological typed feature system; such type-augmented automata have been formally defined in neugebauer (2003c).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA571">
<title id=" W04-0104.xml">automatic acquisition of feature based phonotactic resources </title>
<section> phonotactic automata and typed.  </section>
<citcontext>
<prevsection>
<prevsent>previous proposals to cast phonological structure in typed feature system can be found in bird and klein (1994) <papid> J94-3010 </papid>and walther (1999).</prevsent>
<prevsent>however, there are two major differences with regard to ourwork.</prevsent>
</prevsection>
<citsent citstr=" P97-1040 ">
first, while types may denote sets of segments, we go beyond the idea of sets as arc labels infinite-state automata (bird and ellison, 1994; <papid> J94-1003 </papid>eisner, 1997; <papid> P97-1040 </papid>van noord and gerdemann, 2001) which says that boolean combinations of finitely-valued features can be stored as set on just one arc, rather than being multiplied out as disjunctive collection of arcs.</citsent>
<aftsection>
<nextsent>this choice has no theoretical consequences but is merely convenience for grammar development (bird and ellison, 1994).<papid> J94-1003 </papid></nextsent>
<nextsent>the difference in our approach consists in the hierarchical ordering of types (or sets) which relates each arc label to any other type in given phonological typed feature system; such type-augmented automata have been formally defined in neugebauer (2003c).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA574">
<title id=" W03-2912.xml">russian morphology res sources and java software application </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we introdu cee the system based on java and oracle 9i dbms.
</prevsent>
<prevsent>up-to-date language technologies contain efficient morphological analyzers for romance, germanic (karttunen, 1983; karttunen, koskenniemi, kaplan, 1987; zaenen, uszkoreit, 1996) and some slavic (chanod, 1997) languages.
</prevsent>
</prevsection>
<citsent citstr=" C90-3058 ">
in the last 15 years russian computational morphology has advanced at great rate from first quite restricted systems towards large-scale practical morphological analyzers ( ashmanov i., 1995; belonogov, zelenkov, 1989; belyaev, surcis, yablonsky, 1993; bolshakov, 1990; <papid> C90-3058 </papid>mikheev, liubushkina, 1995; segalovich, 1995).</citsent>
<aftsection>
<nextsent>this paper attempts to introduce results of 15 years ongoing project on developing of russian resources and software for building advance russian language morphological analyzers and their applications that enable different forms of text indexing and retrieval, and direct benefit from the russian morphological analyzers in: ? information-acquisition tools, ? authoring tools, ? language-learning tools, ? translation-tools, ? summarizers, ? semantic web etc. the objectives of this project are not unique.
</nextsent>
<nextsent>several analogous projects have been carried out to different stages.
</nextsent>
<nextsent>in the late eighties of xx century we developed one of the first russian morphologic analyzers on pc (yablonsky s., 1990; belyaev b.m., surcis a.s., yablonsky s.a., 1993; yablonsky s.a., 1998; yablonsky s.a., 1999).
</nextsent>
<nextsent>now we are developing set of platform independent internet/intranet russian language processing tools based on java and oracle technologies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA575">
<title id=" W04-0306.xml">an efficient algorithm to induce minimum average look ahead grammars for incremental lr parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in contrast, our approach permits greater flexibility and finer gradations, where the average degree of look ahead required can be minimized with the aim of assisting grammar induction.
</prevsent>
<prevsent>since marcus?
</prevsent>
</prevsection>
<citsent citstr=" N01-1021 ">
work, significant body of workon incremental parsing has developed in the sentence processing community, but much of this work has actually suggested models with an increased amount of non determinism, often with probabilistic weights (e.g., narayanan &amp; jurafsky (1998); hale (2001)).<papid> N01-1021 </papid></citsent>
<aftsection>
<nextsent>meanwhile, in the way of formal methods, tomita (1986) introduced generalized lr parsing,which offers an interesting hybrid of nondetermin istic dynamic programming surrounding lr parsing methods that were originally deterministic.
</nextsent>
<nextsent>additionally, methods for determinizing and minimizing finite-state machines are well known(e.g., mohri (2000),   al &amp; carton (1968)).
</nextsent>
<nextsent>however, such methods (a) do not operate at the context free level, (b) do not directly minimize lookahead,and (c) do not induce grammars under environmental constraints.unfortunately, there has still been relatively little work on automatic learning of grammars forde terministic parsers to date.
</nextsent>
<nextsent>hermjakob &amp; mooney (1997) <papid> P97-1062 </papid>describe semi-automatic procedure for learning deterministic parser from treebank, which requires the intervention of human expert in the loop to determine appropriate derivation order, to resolve parsing conflicts between certain actions such as merge?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA576">
<title id=" W04-0306.xml">an efficient algorithm to induce minimum average look ahead grammars for incremental lr parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>additionally, methods for determinizing and minimizing finite-state machines are well known(e.g., mohri (2000),   al &amp; carton (1968)).
</prevsent>
<prevsent>however, such methods (a) do not operate at the context free level, (b) do not directly minimize lookahead,and (c) do not induce grammars under environmental constraints.unfortunately, there has still been relatively little work on automatic learning of grammars forde terministic parsers to date.
</prevsent>
</prevsection>
<citsent citstr=" P97-1062 ">
hermjakob &amp; mooney (1997) <papid> P97-1062 </papid>describe semi-automatic procedure for learning deterministic parser from treebank, which requires the intervention of human expert in the loop to determine appropriate derivation order, to resolve parsing conflicts between certain actions such as merge?</citsent>
<aftsection>
<nextsent>and add-into?, and to identify specific features for disambiguating actions.
</nextsent>
<nextsent>in our earlier work we described deterministic parser with fully automatically learned decision algorithm (wong and wu, 1999).
</nextsent>
<nextsent>but unlike our present work, the decision algorithms in both hermjakob &mooney; (1997) <papid> P97-1062 </papid>and wong &amp; wu (1999) are pro cedural; there is no explicit representation of the grammar that can be meaningfully inspected.</nextsent>
<nextsent>finally, we observe that there are also trainable stochastic shift-reduce parser models (briscoe and carroll, 1993), <papid> J93-1002 </papid>which are theoretically related toshift-reduce parsing, but operate in highly nonde terministic fashion during parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA578">
<title id=" W04-0306.xml">an efficient algorithm to induce minimum average look ahead grammars for incremental lr parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our earlier work we described deterministic parser with fully automatically learned decision algorithm (wong and wu, 1999).
</prevsent>
<prevsent>but unlike our present work, the decision algorithms in both hermjakob &mooney; (1997) <papid> P97-1062 </papid>and wong &amp; wu (1999) are pro cedural; there is no explicit representation of the grammar that can be meaningfully inspected.</prevsent>
</prevsection>
<citsent citstr=" J93-1002 ">
finally, we observe that there are also trainable stochastic shift-reduce parser models (briscoe and carroll, 1993), <papid> J93-1002 </papid>which are theoretically related toshift-reduce parsing, but operate in highly nonde terministic fashion during parsing.</citsent>
<aftsection>
<nextsent>we believe the shortage of learning models for deterministic parsing is in no small part due to the difficulty of overcoming computational complexity barriers in the optimization problems this would involve.
</nextsent>
<nextsent>many types of factors need to be optimized in learning, because deterministic parsing is much more sensitive to incorrect choice of structural features (e.g., categories, rules) than non deterministic parsing that employ robustness mechanisms such as weighted charts.
</nextsent>
<nextsent>consequently, we suggest shifting attention to the development of new methods that directly address the problem of optimizing criteria associated with deterministic parsing, in computationally feasible ways.
</nextsent>
<nextsent>in particular, we aim in this paper to develop method that efficiently searches for parser under minimum average look ahead cost function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA579">
<title id=" W04-2401.xml">a linear programming formulation for global inference in natural language tasks </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>table 1: some patterns used in relation classification the relation; (2) conjunctions of the features from the two arguments; (3) some patterns extracted from the sentence or between the two arguments.
</prevsent>
<prevsent>some features in category (3) are the number of words between arg1 and arg2 ?, whether arg1 and arg2 are the same word?, or arg1 is the beginning of the sentence and has words that consist of all capitalized characters?, where arg1 and arg2 represent the first and second argument entities respectively.
</prevsent>
</prevsection>
<citsent citstr=" C02-1151 ">
in addition, table 1 presents some patterns we use.the learning algorithm used is variation of the win now update rule incorporated in snow (roth, 1998;roth and yih, 2002), <papid> C02-1151 </papid>multi-class classifier that is specifically tailored for large scale learning tasks.</citsent>
<aftsection>
<nextsent>snow learns sparse network of linear functions, in which the targets(entity classes or relation classes, in this case) are represented as linear functions over common feature space.while snow can be used as classifier and predicts using winner-take-all mechanism over the activation value of the target classes, we can also rely directly on the raw activation value it outputs, which is the weighted linear sum of the active features, to estimate the posteriors.
</nextsent>
<nextsent>it can be verified that the resulting values are monotonic with the confidence in the prediction, therefore provide good source of probability estimation.
</nextsent>
<nextsent>we use softmax(bishop, 1995) over the raw activation values as conditional probabilities.
</nextsent>
<nextsent>specifically, suppose the number of classes is n, and the raw activation values of class is acti.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA580">
<title id=" W03-2127.xml">annotating emotion in dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>keywords: dialogue, emotions, annotation
</prevsent>
<prevsent>dialogue annotation is fundamental stage of much of the research conducted on both human-human and human-machine dialogue.
</prevsent>
</prevsection>
<citsent citstr=" W01-1627 ">
we are fortunate to have access to valuable corpus of 37 dialogues between nurses and patients, each comprising 200-1200 utterances (wood,2001).<papid> W01-1627 </papid></citsent>
<aftsection>
<nextsent>these consultations contain genuine emotional speech and form the ideal basis for studies of realistic conversational dialogue.
</nextsent>
<nextsent>the emotional state of participants affects the way in which the dialogue is conducted.
</nextsent>
<nextsent>i propose that annotating emotion in dialogue alongside currently annotated phenomena will reveal interesting and useful correlations that will improve our understanding of dialogue and benefit natural language applications.
</nextsent>
<nextsent>the overall aim of this research is to develop scheme for annotating expressions of emotion, to create an annotated corpus of dialogue containing emotion and to study the effects that aparticipants emotional state has on their communicative behaviour.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA581">
<title id=" W03-2127.xml">annotating emotion in dialogue </title>
<section> annotating emotion in dialogue.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 current dialogue annotation schemes.
</prevsent>
<prevsent>in order to learn how current annotation schemes accommodate emotion, we aligned the layers in anumber of schemes.
</prevsent>
</prevsection>
<citsent citstr=" H01-1015 ">
(core and allen, 1997; di eugenio et al, 1998; traum, 1996; walker et al, 1996; macwhinney, 1996; jekat et al, 1995; anderson et al., 1991; condon and cech, 1996; van vark et al, 1996; walker and passonneau, 2001).<papid> H01-1015 </papid></citsent>
<aftsection>
<nextsent>layers from different schemes are grouped according to the similar phenomena that they label.
</nextsent>
<nextsent>table 1 shows this alignment.
</nextsent>
<nextsent>in this section we will look at these layers and discuss how they may relate to annotating emotion in dialogue.
</nextsent>
<nextsent>information level when analysing task dialogue,we may be interested in knowing whether an utterance pertains to the management of the communications channel, advancement of task, discussing of task etc. in the previous section we suggested that we are more likely to find emotional speech in conversational rather than task dialogue because the latter is more of mechanical process than conversation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA582">
<title id=" W03-2127.xml">annotating emotion in dialogue </title>
<section> annotating emotion in dialogue.  </section>
<citcontext>
<prevsection>
<prevsent>p. poor girl youve got to listen to all that however, from observations of our emotional dialogues it appears that short question-answer, offer-acceptance exchanges tend to be formal.
</prevsent>
<prevsent>emotion tends to build though sub-dialogue on topic that speakers find funny, feel anxious about etc.dialogue grammars are used to exploit the expected sequences of speech acts.
</prevsent>
</prevsection>
<citsent citstr=" J00-3003 ">
these can be used in dialogue act classification to predict the next act in series of utterances (stolcke et al, 2000).<papid> J00-3003 </papid></citsent>
<aftsection>
<nextsent>it may be possible that complementary approach may be used to automatically identify emotional utterances.
</nextsent>
<nextsent>one way would be to develop grammars based on patterns discovered in emotional sections of dialogue where particular sequence of acts may indicate the proceedings have become emotional.
</nextsent>
<nextsent>another may be to apply established grammars to dialogue so that deviations from the grammar may highlight interesting or emotional passages.
</nextsent>
<nextsent>topic several annotation schemes contain layer that labels the topic discussed in an utterance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA583">
<title id=" W04-2403.xml">a semantic kernel for predicate argument classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they include functional tags,e.g. argm-dir indicates directional, argm-loc indicates locative and argm-tmp stands for temporal.
</prevsent>
<prevsent>an example of propbank markup is:1other arguments are: arg 2 for indirect object or benefac tive or instrument or attribute or end state, arg 3 for start point or benefactive or attribute, arg4 for end point and so on.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
[arg10 analysts ] have been [predicate1 expecting ] [arg11 gm-jaguar pact ] that would [predicate2 give ] [arg22 the u.s. carmaker ] [arg21 an eventual 30% state in the british company ].automatically recognizing the boundaries and classifying the type of arguments allows natural language processing systems (e.g. information extraction, question answering or summarization) to answer questions such as who?, when?, what?, where?, why?, and so on.given the importance of this task for natural language processing applications, several machine learning approaches for argument identification and classification have been developed (gildea and jurasky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>hacioglu et al, 2003; chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003).<papid> W03-1008 </papid></citsent>
<aftsection>
<nextsent>their common characteristic is the adoption of feature spaces that model predicate-argument structures in flat representation.
</nextsent>
<nextsent>the major problem of this choice is that there isno linguistic theory that supports the selection of syntactic features to recognize semantic structures.
</nextsent>
<nextsent>as consequence, researchers are still trying to extend the basic features with other ones, e.g.
</nextsent>
<nextsent>(surdeanu et al, 2003), <papid> P03-1002 </papid>to improve the flat feature space.convolution kernels are viable alternative to flat feature representation that aims to capture the structural information in term of sub-structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA585">
<title id=" W04-2403.xml">a semantic kernel for predicate argument classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they include functional tags,e.g. argm-dir indicates directional, argm-loc indicates locative and argm-tmp stands for temporal.
</prevsent>
<prevsent>an example of propbank markup is:1other arguments are: arg 2 for indirect object or benefac tive or instrument or attribute or end state, arg 3 for start point or benefactive or attribute, arg4 for end point and so on.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
[arg10 analysts ] have been [predicate1 expecting ] [arg11 gm-jaguar pact ] that would [predicate2 give ] [arg22 the u.s. carmaker ] [arg21 an eventual 30% state in the british company ].automatically recognizing the boundaries and classifying the type of arguments allows natural language processing systems (e.g. information extraction, question answering or summarization) to answer questions such as who?, when?, what?, where?, why?, and so on.given the importance of this task for natural language processing applications, several machine learning approaches for argument identification and classification have been developed (gildea and jurasky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>hacioglu et al, 2003; chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003).<papid> W03-1008 </papid></citsent>
<aftsection>
<nextsent>their common characteristic is the adoption of feature spaces that model predicate-argument structures in flat representation.
</nextsent>
<nextsent>the major problem of this choice is that there isno linguistic theory that supports the selection of syntactic features to recognize semantic structures.
</nextsent>
<nextsent>as consequence, researchers are still trying to extend the basic features with other ones, e.g.
</nextsent>
<nextsent>(surdeanu et al, 2003), <papid> P03-1002 </papid>to improve the flat feature space.convolution kernels are viable alternative to flat feature representation that aims to capture the structural information in term of sub-structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA588">
<title id=" W04-2403.xml">a semantic kernel for predicate argument classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they include functional tags,e.g. argm-dir indicates directional, argm-loc indicates locative and argm-tmp stands for temporal.
</prevsent>
<prevsent>an example of propbank markup is:1other arguments are: arg 2 for indirect object or benefac tive or instrument or attribute or end state, arg 3 for start point or benefactive or attribute, arg4 for end point and so on.
</prevsent>
</prevsection>
<citsent citstr=" W03-1006 ">
[arg10 analysts ] have been [predicate1 expecting ] [arg11 gm-jaguar pact ] that would [predicate2 give ] [arg22 the u.s. carmaker ] [arg21 an eventual 30% state in the british company ].automatically recognizing the boundaries and classifying the type of arguments allows natural language processing systems (e.g. information extraction, question answering or summarization) to answer questions such as who?, when?, what?, where?, why?, and so on.given the importance of this task for natural language processing applications, several machine learning approaches for argument identification and classification have been developed (gildea and jurasky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>hacioglu et al, 2003; chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003).<papid> W03-1008 </papid></citsent>
<aftsection>
<nextsent>their common characteristic is the adoption of feature spaces that model predicate-argument structures in flat representation.
</nextsent>
<nextsent>the major problem of this choice is that there isno linguistic theory that supports the selection of syntactic features to recognize semantic structures.
</nextsent>
<nextsent>as consequence, researchers are still trying to extend the basic features with other ones, e.g.
</nextsent>
<nextsent>(surdeanu et al, 2003), <papid> P03-1002 </papid>to improve the flat feature space.convolution kernels are viable alternative to flat feature representation that aims to capture the structural information in term of sub-structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA591">
<title id=" W04-2403.xml">a semantic kernel for predicate argument classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they include functional tags,e.g. argm-dir indicates directional, argm-loc indicates locative and argm-tmp stands for temporal.
</prevsent>
<prevsent>an example of propbank markup is:1other arguments are: arg 2 for indirect object or benefac tive or instrument or attribute or end state, arg 3 for start point or benefactive or attribute, arg4 for end point and so on.
</prevsent>
</prevsection>
<citsent citstr=" W03-1008 ">
[arg10 analysts ] have been [predicate1 expecting ] [arg11 gm-jaguar pact ] that would [predicate2 give ] [arg22 the u.s. carmaker ] [arg21 an eventual 30% state in the british company ].automatically recognizing the boundaries and classifying the type of arguments allows natural language processing systems (e.g. information extraction, question answering or summarization) to answer questions such as who?, when?, what?, where?, why?, and so on.given the importance of this task for natural language processing applications, several machine learning approaches for argument identification and classification have been developed (gildea and jurasky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>hacioglu et al, 2003; chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003).<papid> W03-1008 </papid></citsent>
<aftsection>
<nextsent>their common characteristic is the adoption of feature spaces that model predicate-argument structures in flat representation.
</nextsent>
<nextsent>the major problem of this choice is that there isno linguistic theory that supports the selection of syntactic features to recognize semantic structures.
</nextsent>
<nextsent>as consequence, researchers are still trying to extend the basic features with other ones, e.g.
</nextsent>
<nextsent>(surdeanu et al, 2003), <papid> P03-1002 </papid>to improve the flat feature space.convolution kernels are viable alternative to flat feature representation that aims to capture the structural information in term of sub-structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA594">
<title id=" W04-2403.xml">a semantic kernel for predicate argument classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that is, we do not need to understand which syntactic feature may besuited for representing semantic data.
</prevsent>
<prevsent>we need only to define the similarity function between two semantic structures.
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
an example of convolution kernel on the parse-tree space is given in (collins and duffy, 2002).<papid> P02-1034 </papid>the aim was to design novel syntactic parser by looking at the similarity between the testing parse-trees and the correct parse-trees available for training.in this paper, we define kernel in semantic structure space to learn the classification function of predicatearguments.</citsent>
<aftsection>
<nextsent>the main idea is to select portions of syn tactic/semantic trees that include the target  predicate, argument  pair and to define kernel function between these objects.
</nextsent>
<nextsent>if our similarity function is well defined the learning model will converge and provide an effective argument classification.
</nextsent>
<nextsent>experiments on propbank data show not only that support vector machines (svms) trained with the proposed semantic kernel converge but also that they have higher accuracy than svms trained with linear kernel on the standard features proposed in (gildea and jurasky, 2002).<papid> J02-3001 </papid></nextsent>
<nextsent>this provides piece of evidence that convolution kernel can be used to learn semantic linguistic structures.moreover, interesting research lines on the use of kernel for nlp are enabled, e.g. question classification in question/answering or automatic template designing in information extraction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA603">
<title id=" W04-2403.xml">a semantic kernel for predicate argument classification </title>
<section> the experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for example, in figure 3 are shown fragments like [np[dt] [n]] or [np [dt a] [n talk]] which explicitly encode the phrase type feature np for arg 1 in figure 2.b. the predicate word is represented by the fragment [v delivers] and the head word is present as [n talk].finally, governing category, position and voice can not be expressed by sk.
</prevsent>
<prevsent>this suggests that combination of the flat features (especially the named entity class (sur deanu et al, 2003)) <papid> P03-1002 </papid>with sk could furthermore improve the predicate argument representation.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
for the experiments, we used propbank(www.cis.upenn.edu/ace) along with penn treebank5 2 (www.cis.upenn.edu/treebank) (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>this corpus contains about 53,700 sentences and fixed split between training and testing which has been used in other researches (gildea and jurasky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>hacioglu et al,2003; chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>pradhan et al, 2003).</nextsent>
<nextsent>in this split, sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA613">
<title id=" W04-2403.xml">a semantic kernel for predicate argument classification </title>
<section> the experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this suggests that combination of the flat features (especially the named entity class (sur deanu et al, 2003)) <papid> P03-1002 </papid>with sk could furthermore improve the predicate argument representation.</prevsent>
<prevsent>for the experiments, we used propbank(www.cis.upenn.edu/ace) along with penn treebank5 2 (www.cis.upenn.edu/treebank) (marcus et al, 1993).<papid> J93-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
this corpus contains about 53,700 sentences and fixed split between training and testing which has been used in other researches (gildea and jurasky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>hacioglu et al,2003; chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>pradhan et al, 2003).</citsent>
<aftsection>
<nextsent>in this split, sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set.
</nextsent>
<nextsent>we considered all propbank arguments from arg0 to arg9, arga and argm even if only arg0 from arg4 and argm contain enough training/testing data to affect the global performance.
</nextsent>
<nextsent>in table 2 some characteristics of the corpus used in our experiments are reported.
</nextsent>
<nextsent>the classifier evaluations were carried out using the svm-light software (joachims, 1999) available athttp://svmlight.joachims.org/ with the default linear kernel for the standard feature evaluations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA628">
<title id=" W04-2321.xml">on the use of confidence for statistical decision in dialogue strategies </title>
<section> structured n-best list.  </section>
<citcontext>
<prevsection>
<prevsent>the cost function for transition corresponds to the acoustic score of the word emitted.
</prevsent>
<prevsent>the first step in the word lattice processing consists of rescoring each transition of by means of 3-gramlanguage model (lm) in order to obtain the probabilities (w ) of equation 1.
</prevsent>
</prevsection>
<citsent citstr=" P03-1006 ">
this is done by composing the word lattice with 3-gram lm also coded as an fsm (see (allauzen et al, 2003) <papid> P03-1006 </papid>for more details about statistical lms and fsms).the resulting fsm is then composed with the transducer tconcept in order to obtain the word-to-concept transducer l?.</citsent>
<aftsection>
<nextsent>a path in l?
</nextsent>
<nextsent>corresponds to word string if only the input symbols of the transducer are considered and its score is the one expressed by equation 1; similarly by considering only the output symbols, path in l?
</nextsent>
<nextsent>corresponds to concept tag string.
</nextsent>
<nextsent>the structured n-best list is directly obtained from l?: by extracting the n-best concept tag strings (output labelpaths) we obtain an n-best list on the conceptual interpretations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA629">
<title id=" W04-2321.xml">on the use of confidence for statistical decision in dialogue strategies </title>
<section> use of correctness probabilities.  </section>
<citcontext>
<prevsection>
<prevsent>one classifier is trained for each concept tag in the following way: each utterance of training corpus is labeled with atag, manually checked, indicating if given concept occurs or not in the utterance.
</prevsent>
<prevsent>in order to let the classifier model the context of occurrence of concept rather than its value we removed most of the concept headwords from the list of criterion used by the classifier.during the decision process, if the interpretation evaluated contains 2 concepts c1 and c2, then the classifiers corresponding to c1 and c2 are used to give to the utterance confidence score of containing these two concepts.
</prevsent>
</prevsection>
<citsent citstr=" P00-1011 ">
the text classifier used in the experimental section is decision-tree classifier based on the semantic classification-trees introduced for the atis taskby (kuhn and mori, 1995) and used for semantic disambiguation in (bechet et al, 2000).<papid> P00-1011 </papid></citsent>
<aftsection>
<nextsent>5.1.4 rank confidence measure (r) to the previous confidence measures we added the rank of each candidate in its n-best.
</nextsent>
<nextsent>this rank contains two numbers: the rank of the interpretation of the utterance and the rank of the utterance among those having the same interpretation.
</nextsent>
<nextsent>5.2 decision tree based strategy.
</nextsent>
<nextsent>as the dependencies of these measures are difficult to establish, their values are transformed into symbols by vector quant ization (vq) and conjunctions of these symbols expressing relevant statistical dependencies are obtained by decision tree which is trained with development set of examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA630">
<title id=" W04-1003.xml">the effects of human variation in duc summarization evaluation </title>
<section> initial design ? duc-2001.  </section>
<citcontext>
<prevsection>
<prevsent>one type of evaluation supported by see was coverage, i.e., how well did the peer summaries (i.e., those being evaluated) cover the content of the documents (as expressed by the model summary).
</prevsent>
<prevsent>a pairwise summary comparison was used in this part of the evaluation and judges were asked to do detailed coverage comparisons.
</prevsent>
</prevsection>
<citsent citstr=" N03-1030 ">
see allowed the judges to step through predefined units of the model summary (elementary discourse units/edus) (soricut and marcu, 2003) <papid> N03-1030 </papid>and for each unit of that summary, mark the sentences in the peer summary that expressed [all(4), most(3), some(2), hardly any(1)or none(0)] of the content in the current model summary unit.</citsent>
<aftsection>
<nextsent>the resulting ordered category scale[0 4] is treated as an interval scale in the coverage score based on feedback from the judges on how it was used.
</nextsent>
<nextsent>the coverage score forgiven peer summary is the mean of its scores against the edus of the associated model (?
</nextsent>
<nextsent>4 edus per summary for the 50-word model summaries).
</nextsent>
<nextsent>this process is much more complex than doing simple overall comparison using the entire summary but past evaluation experiences indicated that judges had more difficulty making an overall decision than they did making decisions at each edu.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA631">
<title id=" W04-1003.xml">the effects of human variation in duc summarization evaluation </title>
<section> initial design ? duc-2001.  </section>
<citcontext>
<prevsection>
<prevsent>there is large variation across document sets,with some sets having much wider ranges in coverage score differences based on the two different models.
</prevsent>
<prevsent>looking across all 20 document sets, the average absolute coverage difference is 0.437or 47.8% of the highest scoring model for the 50word summaries and 0.318 (42.5%) for the 200 word summaries.
</prevsent>
</prevsection>
<citsent citstr=" W02-0406 ">
this large difference in scores is coming solely from the model difference since judgment is being made by the same person (although some self-inconsistency is involved (lin and hovy, 2002)).<papid> W02-0406 </papid></citsent>
<aftsection>
<nextsent>2.4 relationship between model differences.
</nextsent>
<nextsent>and coverage scores does small unigram overlap in terms for the models in given document set predict wide difference in coverage scores for peers judged against the models in that document set?
</nextsent>
<nextsent>comparing figures 2 and 3, or indeed graphing overlap against coverage(figure 4) shows that there is little correlation between these two.
</nextsent>
<nextsent>one suspects that the humans are able to compensate for different word choice andthat the coverage differences shown in figure 3 represent differences in content in the models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA632">
<title id=" W04-0107.xml">unsupervised induction of natural language morphology inflection classes </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a vital first step in rule-based machine translation system is morphological analysis.
</prevsent>
<prevsent>this paper outlines framework for automatic natural language morphology induction inspired by the traditional and linguistic concept of inflection classes.
</prevsent>
</prevsection>
<citsent citstr=" P04-2012 ">
additional details concerning the candidate inflection class framework can be found in monson (2004).<papid> P04-2012 </papid></citsent>
<aftsection>
<nextsent>this paper then goes on to describe one implemented search strategy within this framework, presenting both simple summary of results and an indepth error analysis.
</nextsent>
<nextsent>while the intent of this research direction is to define techniques applicable to low-density languages, this paper employs english to illustrate the main conjectures and spanish, language with reasonably complex morphological system, for quantitative analysis.
</nextsent>
<nextsent>all experiments detailed in this paper are over spanish newswire corpus of 40,011 tokens and 6,975 types.
</nextsent>
<nextsent>it is possible to organize much of the recent work on unsupervised morphology induction by considering the bias each approach has toward discovering morphologically related words that are also ortho graphically similar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA633">
<title id=" W04-0107.xml">unsupervised induction of natural language morphology inflection classes </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>all experiments detailed in this paper are over spanish newswire corpus of 40,011 tokens and 6,975 types.
</prevsent>
<prevsent>it is possible to organize much of the recent work on unsupervised morphology induction by considering the bias each approach has toward discovering morphologically related words that are also ortho graphically similar.
</prevsent>
</prevsection>
<citsent citstr=" H01-1035 ">
at one end of the spectrum is the work of yarowsky et al (2001), <papid> H01-1035 </papid>who derive morphological analyzer for language, l, by projecting the morphological analysis of resource-rich language onto through clever application of statistical machine translation style word alignment prob abilities.</citsent>
<aftsection>
<nextsent>the word alignments are trained over sentence aligned parallel bilingual text for the language pair.
</nextsent>
<nextsent>while the probabilistic model they use to generalize their initial system contains bias toward orthographic similarity, the un embellished algorithm contains no assumptions on the orthographic shape of related word forms.
</nextsent>
<nextsent>next along the spectrum of orthographic similar barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the ity bias is the work of schone and jurafsky (2000), <papid> W00-0712 </papid>who first acquire list of pairs of potential morphological variants (ppmvs) using an orthographic similarity technique due to gaussier (1999), <papid> W99-0904 </papid>in which pairs of words from corpus vocabulary with the same initial string are identified.</nextsent>
<nextsent>they then apply latent semantic analysis (lsa) to score each ppmv with semantic distance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA634">
<title id=" W04-0107.xml">unsupervised induction of natural language morphology inflection classes </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the word alignments are trained over sentence aligned parallel bilingual text for the language pair.
</prevsent>
<prevsent>while the probabilistic model they use to generalize their initial system contains bias toward orthographic similarity, the un embellished algorithm contains no assumptions on the orthographic shape of related word forms.
</prevsent>
</prevsection>
<citsent citstr=" W00-0712 ">
next along the spectrum of orthographic similar barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the ity bias is the work of schone and jurafsky (2000), <papid> W00-0712 </papid>who first acquire list of pairs of potential morphological variants (ppmvs) using an orthographic similarity technique due to gaussier (1999), <papid> W99-0904 </papid>in which pairs of words from corpus vocabulary with the same initial string are identified.</citsent>
<aftsection>
<nextsent>they then apply latent semantic analysis (lsa) to score each ppmv with semantic distance.
</nextsent>
<nextsent>pairs measuring small distance, those whose potential variants tend to occur where neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther.
</nextsent>
<nextsent>in later work, schone and jurafsky (2001) <papid> N01-1024 </papid>extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over corpus.</nextsent>
<nextsent>goldsmith (2001), <papid> J01-2001 </papid>by searching over space of morphology models limited to substitution of suffixes, ties morphology yet closer to orthography.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA635">
<title id=" W04-0107.xml">unsupervised induction of natural language morphology inflection classes </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the word alignments are trained over sentence aligned parallel bilingual text for the language pair.
</prevsent>
<prevsent>while the probabilistic model they use to generalize their initial system contains bias toward orthographic similarity, the un embellished algorithm contains no assumptions on the orthographic shape of related word forms.
</prevsent>
</prevsection>
<citsent citstr=" W99-0904 ">
next along the spectrum of orthographic similar barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the ity bias is the work of schone and jurafsky (2000), <papid> W00-0712 </papid>who first acquire list of pairs of potential morphological variants (ppmvs) using an orthographic similarity technique due to gaussier (1999), <papid> W99-0904 </papid>in which pairs of words from corpus vocabulary with the same initial string are identified.</citsent>
<aftsection>
<nextsent>they then apply latent semantic analysis (lsa) to score each ppmv with semantic distance.
</nextsent>
<nextsent>pairs measuring small distance, those whose potential variants tend to occur where neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther.
</nextsent>
<nextsent>in later work, schone and jurafsky (2001) <papid> N01-1024 </papid>extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over corpus.</nextsent>
<nextsent>goldsmith (2001), <papid> J01-2001 </papid>by searching over space of morphology models limited to substitution of suffixes, ties morphology yet closer to orthography.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA636">
<title id=" W04-0107.xml">unsupervised induction of natural language morphology inflection classes </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they then apply latent semantic analysis (lsa) to score each ppmv with semantic distance.
</prevsent>
<prevsent>pairs measuring small distance, those whose potential variants tend to occur where neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther.
</prevsent>
</prevsection>
<citsent citstr=" N01-1024 ">
in later work, schone and jurafsky (2001) <papid> N01-1024 </papid>extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over corpus.</citsent>
<aftsection>
<nextsent>goldsmith (2001), <papid> J01-2001 </papid>by searching over space of morphology models limited to substitution of suffixes, ties morphology yet closer to orthography.</nextsent>
<nextsent>segmenting word forms in corpus, goldsmith creates an inventory of stems and suffixes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA637">
<title id=" W04-0107.xml">unsupervised induction of natural language morphology inflection classes </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>pairs measuring small distance, those whose potential variants tend to occur where neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther.
</prevsent>
<prevsent>in later work, schone and jurafsky (2001) <papid> N01-1024 </papid>extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over corpus.</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
goldsmith (2001), <papid> J01-2001 </papid>by searching over space of morphology models limited to substitution of suffixes, ties morphology yet closer to orthography.</citsent>
<aftsection>
<nextsent>segmenting word forms in corpus, goldsmith creates an inventory of stems and suffixes.
</nextsent>
<nextsent>suffixes which can interchangeably concatenate onto set of stems form signature.
</nextsent>
<nextsent>after defining the space of signatures, goldsmith searches for that choice of word segment ations resulting in minimum description length local optimum.
</nextsent>
<nextsent>finally, the work of harris (1955), work of harris (1967), and later hafer and weiss (1974), has direct bearing on the approach taken in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA640">
<title id=" W04-0819.xml">semantic parsing based on framenet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this method employs four different feature sets, one of which being first reported herein.
</prevsent>
<prevsent>the combination of features as well as the extended training data we considered have produced in the senseval-3 experiments an f1-score of 92.5% for the unrestricted case and of 76.3% for the restricted case.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the evaluation of the senseval-3 task for automatic labeling of semantic roles is based on the annotations made available by the framenet project (baker et al, 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>the idea of automatically identifying and labeling frame-specific roles, as defined by the semantic frames, was first introduced by (gildea andjurasfky, 2002).
</nextsent>
<nextsent>each semantic frame is characterized by set of target words which can be nouns,verbs or adjectives.
</nextsent>
<nextsent>this helps abstracting the thematic roles and adding semantics to the given frame, highlighting the characteristic semantic features.
</nextsent>
<nextsent>frames are characterized by (1) target words or lexical predicates whose meaning includes aspects of the frame; (2) frame elements (fes) which represent the semantic roles of the frame and (3) examples of annotations performed on the british national corpus (bnc) for instances of each target word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA641">
<title id=" W04-0819.xml">semantic parsing based on framenet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the restricted case requires systems to (i) recognize the boundaries of the fes for each evaluated frame as well as to (ii) assign label to it.
</prevsent>
<prevsent>both cases can be cast as two different classifications: (1) classification of the role when its boundaries are known and (2) classification of the sentence words as either belonging to role or not1.a similar approach was used for automatically identifying predicate-argument structures in english sentences.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
the propbank annotations (www.cis.upenn.edu/ace) enable training for two distinct learning techniques: (1) decision trees (sur deanu et al, 2003) <papid> P03-1002 </papid>and (2) support vector machines (svms) (pradhan et al, 2004).</citsent>
<aftsection>
<nextsent>the svms produced the best results, therefore we decided to use the same learning framework for the senseval-3 task for automatic labeling of semantic roles.
</nextsent>
<nextsent>additionally, we have performed the following enhancements: ? we created multi-class classifier for each frame, thus achieving improved accuracy and efficiency; ? we combined some new features with features from (gildea and jurasfky, 2002; surdeanu et al, 2003; <papid> P03-1002 </papid>pradhan et al, 2004); ? we resolved the data sparsity problem generated by limited training data for each frame, when using the examples associated with any other frame from framenet that had at least one fe shared with each frame that was evaluated; ? we crafted heuristics that improved mappings from the syntactic constituents to the semantic roles.we believe that the combination of these four extensions are responsible for our results in senseval-3.the remainder of this paper is organized as follows.</nextsent>
<nextsent>section 2 describes our methods of classifying semantic roles whereas section 3 describes our method of identifying role boundaries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA645">
<title id=" W04-0819.xml">semantic parsing based on framenet </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>we used an additional setof 66,687 sentences (hereafter extended data) as extended data produced when using the examples associated with any other frame from framenet that had at least one fe shared with any of the 40 frames evaluated in senseval-3.
</prevsent>
<prevsent>these sentences were parsed with the collins?
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
parser (collins, 1997).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>the classifier experiments were carried out using the svm-light software (joachims, 1999) available athttp://svmlight.joachims.org/with poly nomial kernel2 (degree=3).
</nextsent>
<nextsent>5.1 unrestricted task experiments.
</nextsent>
<nextsent>for this task we devised four different experiments that used four different combination of features: (1)fs1 indicates using only feature set 1; (2) +h indicates that we added the heuristics; (3) +fs2+fs3 indicates that we add the feature set 2 and 3; and (4) +e indicates that the extended data has also been used.
</nextsent>
<nextsent>for each of the four experiments we trained 40 multi-class classifiers, (one for each frame) for totalof 385 binary role classifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA646">
<title id=" W03-2007.xml">patent claim processing for readability  structure analysis and term explanation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(burgun der, 1995).
</prevsent>
<prevsent>therefore, we believe that patent corpus processing should be centered around patent claim processing.it is common that japanese patent claims are described in one sentence with peculiar style and wording and that they are difficult to read and under stand for ordinary people.
</prevsent>
</prevsection>
<citsent citstr=" W03-2003 ">
after surveying related literature and investigating ntcir3 patent collection (iwayama et al, 2003), <papid> W03-2003 </papid>we found the difficulty has two aspects: structural difficulty and term diffi culty.</citsent>
<aftsection>
<nextsent>in this paper, we first present the characteristics of patent claims.
</nextsent>
<nextsent>next, we present our work on the structure analysis of patent claims.
</nextsent>
<nextsent>third, we introduce our on-going research on term explanation for patent claims.
</nextsent>
<nextsent>typical japanese patent claims taken from two patents are shown in figure 1 and 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA647">
<title id=" W04-2406.xml">word sense discrimination by clustering contexts in vector and similarity spaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is motivated by (miller and charles, 1991), who hypothesize that words with similar meanings are often used in similar contexts.
</prevsent>
<prevsent>hence, word sense discrimination reduces to the problem of finding classes of similar contexts such that each class represents single word sense.
</prevsent>
</prevsection>
<citsent citstr=" W97-0322 ">
put another way, contexts that are grouped together in the same class represent particular word sense.while there has been some previous work in sense discrimination (e.g., (schutze, 1992), (pedersen and bruce, 1997), (<papid> W97-0322 </papid>pedersen and bruce, 1998), (schutze, 1998), (fukumoto and suzuki, 1999)), <papid> E99-1028 </papid>by comparison it is much less than that devoted to word sense disambiguation, which is the process of assigning meaning to word from predefined set of possibilities.</citsent>
<aftsection>
<nextsent>however, solutions to disambiguation usually require the availability of an external knowledge source or manually created sense tagged training data.
</nextsent>
<nextsent>as such these are knowledge intensive methods that are difficult to adapt to new domains.by contrast, word sense discrimination is an unsupervised clustering problem.
</nextsent>
<nextsent>this is an attractive methodology because it is knowledge lean approach based on evidence found in simple raw text.
</nextsent>
<nextsent>manually sense tagged text is not required, nor are specific knowledge rich resources like dictionaries or ontologies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA649">
<title id=" W04-2406.xml">word sense discrimination by clustering contexts in vector and similarity spaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is motivated by (miller and charles, 1991), who hypothesize that words with similar meanings are often used in similar contexts.
</prevsent>
<prevsent>hence, word sense discrimination reduces to the problem of finding classes of similar contexts such that each class represents single word sense.
</prevsent>
</prevsection>
<citsent citstr=" E99-1028 ">
put another way, contexts that are grouped together in the same class represent particular word sense.while there has been some previous work in sense discrimination (e.g., (schutze, 1992), (pedersen and bruce, 1997), (<papid> W97-0322 </papid>pedersen and bruce, 1998), (schutze, 1998), (fukumoto and suzuki, 1999)), <papid> E99-1028 </papid>by comparison it is much less than that devoted to word sense disambiguation, which is the process of assigning meaning to word from predefined set of possibilities.</citsent>
<aftsection>
<nextsent>however, solutions to disambiguation usually require the availability of an external knowledge source or manually created sense tagged training data.
</nextsent>
<nextsent>as such these are knowledge intensive methods that are difficult to adapt to new domains.by contrast, word sense discrimination is an unsupervised clustering problem.
</nextsent>
<nextsent>this is an attractive methodology because it is knowledge lean approach based on evidence found in simple raw text.
</nextsent>
<nextsent>manually sense tagged text is not required, nor are specific knowledge rich resources like dictionaries or ontologies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA656">
<title id=" W04-2406.xml">word sense discrimination by clustering contexts in vector and similarity spaces </title>
<section> clustering.  </section>
<citcontext>
<prevsection>
<prevsent>agglomerative algorithms start with each instance in separate cluster and merge pair of clusters at each iteration until there is only single cluster remaining.
</prevsent>
<prevsent>divisive methods start with all instances in the same cluster and split one cluster into two during each iteration until all instances are in their own cluster.the most widely known criteria functions used with hierarchical agglomerative algorithms are single link, complete link, and average link, also known as upgma.
</prevsent>
</prevsection>
<citsent citstr=" N03-3004 ">
(schutze, 1998) points out that single link clustering tends to place all instances into single elongated cluster, whereas (pedersen and bruce, 1997) <papid> W97-0322 </papid>and (purandare, 2003) <papid> N03-3004 </papid>show that hierarchical agglomerative clustering using average link (via mcquittys method) fares well.</citsent>
<aftsection>
<nextsent>thus, we have chosen to use average link/upgma as our criteria function for the agglomerative experiments.
</nextsent>
<nextsent>in similarity space, each instance can be viewed as node in weighted graph.
</nextsent>
<nextsent>the weights on edges joining two nodes indicate their pairwise similarity as measured by the cosine between the context vectors that represent the pair of instances.
</nextsent>
<nextsent>when agglomerative clustering starts, each node is in its own cluster and is considered to be the centro id of that cluster.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA657">
<title id=" W04-0813.xml">the basque country university system english and basque tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a richset of features was used for english, including syntactic dependencies and domain information, extracted with different tools, and also from external resources like wordnet domains (magnini and cavaglia?, 2000).
</prevsent>
<prevsent>the features for basque were different, as basque is an agglu tina tive language, and syntactic information isgiven by inflectional suffixes.
</prevsent>
</prevsection>
<citsent citstr=" C00-1001 ">
we tried to represent this information in local features, relying on the analysis of deep morphological analyzer developed in our group (aduriz et al, 2000).<papid> C00-1001 </papid>in order to improve the performance of the algorithms, different smoothing techniques we retested on the english senseval-2 lexical sample data (agirre and martinez, 2004), and applied to senseval-3.</citsent>
<aftsection>
<nextsent>these methods helped to obtain better estimations for the features, and to avoid the problem of 0 counts decision lists and naive bayes.this paper is organized as follows.
</nextsent>
<nextsent>the learning algorithms are first introduced in section 2, and section 3 describes the features applied toeach task.
</nextsent>
<nextsent>in section 4, we present the experiments performed on training data before submission; this section also covers the final configuration of each algorithm, and the performance obtained on training data.
</nextsent>
<nextsent>finally, the official results in senseval-3 are presented and discussed in section 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA658">
<title id=" W04-0813.xml">the basque country university system english and basque tasks </title>
<section> learning algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>in section 4, we present the experiments performed on training data before submission; this section also covers the final configuration of each algorithm, and the performance obtained on training data.
</prevsent>
<prevsent>finally, the official results in senseval-3 are presented and discussed in section 5.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
the algorithms presented in this section relyon features extracted from the context of the target word to make their decisions.the decision list (dl) algorithm is described in (yarowsky, 1995<papid> P95-1026 </papid>b).</citsent>
<aftsection>
<nextsent>in this algorithm the sense with the highest weighted feature is selected, as shown below.
</nextsent>
<nextsent>we can avoid undetermined values by discarding features that have 0 probability in the divisor.
</nextsent>
<nextsent>more sophisticated smoothing techniques have also been tried (cf.
</nextsent>
<nextsent>section 4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA660">
<title id=" W04-0813.xml">the basque country university system english and basque tasks </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>the following relations were used: object, subject, noun-modifier, preposition, and sibling.
</prevsent>
<prevsent>bag-of-words features: we extract thelemmas of the content words in the whole context, and in 4-word window around the target.
</prevsent>
</prevsection>
<citsent citstr=" N01-1011 ">
we also obtain salient bigrams in the context, with the methods and the software described in (pedersen, 2001).<papid> N01-1011 </papid></citsent>
<aftsection>
<nextsent>domain features: the wordnet domains resource was used to identify the most relevant domains in the context.
</nextsent>
<nextsent>following the relevance formula presented in (magnini and cavaglia?, 2000), we defined 2 feature types: (1) the most relevant domain, and (2) list of domains abovea predefined threshold3.
</nextsent>
<nextsent>other experiments using domains from sumo, the euro wordnet 1the pos tagging was performed with the fntbl toolkit (ngai and florian, 2001).<papid> N01-1006 </papid></nextsent>
<nextsent>2this software was kindly provided by david yarowskys group, from johns hopkins university.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA661">
<title id=" W04-0813.xml">the basque country university system english and basque tasks </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>domain features: the wordnet domains resource was used to identify the most relevant domains in the context.
</prevsent>
<prevsent>following the relevance formula presented in (magnini and cavaglia?, 2000), we defined 2 feature types: (1) the most relevant domain, and (2) list of domains abovea predefined threshold3.
</prevsent>
</prevsection>
<citsent citstr=" N01-1006 ">
other experiments using domains from sumo, the euro wordnet 1the pos tagging was performed with the fntbl toolkit (ngai and florian, 2001).<papid> N01-1006 </papid></citsent>
<aftsection>
<nextsent>2this software was kindly provided by david yarowskys group, from johns hopkins university.
</nextsent>
<nextsent>3the software to obtain the relevant domains was kindly provided by gerard escuderos group, from universitat politecnica de catalunya top-ontology, and wordnets semantic fields were performed, but these features were discarded from the final set.
</nextsent>
<nextsent>3.2 features for basque.
</nextsent>
<nextsent>basque is an agglutinative language, and syntactic information is given by inflectional suffixes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA665">
<title id=" W04-0813.xml">the basque country university system english and basque tasks </title>
<section> experiments on training data.  </section>
<citcontext>
<prevsection>
<prevsent>as this method seems sensitive to the feature types and the amount of examples, we tested 3 dl versions: dl smooth (using smoothed.
</prevsent>
<prevsent>probabilities), dl fixed (replacing 0 counts with0.1), and dl discard (discarding features appearing with only one sense).
</prevsent>
</prevsection>
<citsent citstr=" W97-0323 ">
nb: we applied simple smoothing method presented in (ng, 1997), <papid> W97-0323 </papid>where zero counts are replaced by the probability of the given sense divided by the number of examples.</citsent>
<aftsection>
<nextsent>v: the same smoothing method used for nbwas applied for vectors.
</nextsent>
<nextsent>for basque, two versions were tested: as the basque parser can return ambiguous analyses, partial weights are assigned to the features in the context, and we can chose to use these partial weights (p), or assign the full weight to all features (f).svm: no smoothing was applied.
</nextsent>
<nextsent>we estimated the soft margin using greedy process in cross-validation on the training data per each word.
</nextsent>
<nextsent>combination: single voting was used, where each system voted for its best ranked sense, and the most voted sense was chosen.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA666">
<title id=" W04-1503.xml">a simple string rewriting formalism for dependency grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we present simple generative formalism for dependency grammars based on extended context-free grammar, along with parser; the formalism captures the intuitions of previous formalizations while deviating minimally from the much-used context-free grammar.
</prevsent>
<prevsent>dependency grammar has long tradition in syntactic theory, dating back to at least tesnie`res work from the thirties.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
recently, it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words(see, e.g., (collins, 1997)), <papid> P97-1003 </papid>which is what dependency grammars model explicitly, but context-free phrase-structure grammars do not.</citsent>
<aftsection>
<nextsent>in this paper, we address an important issue in using grammar for malisms: the compact representation of the parse forest.
</nextsent>
<nextsent>why is this an important issue?
</nextsent>
<nextsent>it is well known that for non-toy grammars and non-toy examples, sentence can have staggeringly large number of analyses (for example, using context free grammar (cfg) extracted from the penn tree bank, sentence of 25 words may easily have1,000,000 or more analyses).
</nextsent>
<nextsent>by way of an example of an ambiguous sentence (though with only two readings), the two dependency representations for the ambiguous sentence (1) are given in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA667">
<title id=" W04-1503.xml">a simple string rewriting formalism for dependency grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we use head (or mother) and dependent (or daughter) to refer to nodesin tree.
</prevsent>
<prevsent>sometimes, in the formal and parsing literature, modifier is used to designate any dependent node, but we consider that usage confusing because of the related but different meaning of the term modifier that is well-established in the linguistic literature.4in fact, much of our formalism is very similar to (lom bardo, 1996), who however does not discuss parsing (only recognition), nor the representation of the parse forest.corresponding finite-state machines which straightforwardly allows for binary-branching representation of the derivation structure for the purpose of parsing, and thus for compact (polynomial and not exponential) representation of the parse forest.
</prevsent>
</prevsection>
<citsent citstr=" P98-1106 ">
this formalism is based on previous work presentedin (kahane et al, 1998), <papid> P98-1106 </papid>which has been substantially reformulated in order to simplify it.5 in particular, we do not address non-projectivity here, but acknowledge that for certain languages it is crucial issue.</citsent>
<aftsection>
<nextsent>we will extend our basic approach in the spirit of (kahane et al, 1998) <papid> P98-1106 </papid>in future work.</nextsent>
<nextsent>the paper is structured as follows.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA670">
<title id=" W04-1503.xml">a simple string rewriting formalism for dependency grammar </title>
<section> previous formalizations of dependency.  </section>
<citcontext>
<prevsection>
<prevsent>in the formalism presented in this paper, they have been collapsed into one.
</prevsent>
<prevsent>6we leave aside here work on tree rewriting systems such as tree adjoining grammar, which, when lexicalized, have derivation structures which are very similar to dependency trees.
</prevsent>
</prevsection>
<citsent citstr=" J01-1004 ">
see (rambow and joshi, 1997) for discussion related to tag, and see (rambow et al, 2001) <papid> J01-1004 </papid>for the definition of tree-rewriting system which can be used to develop grammars whose derivations faithfully mirror syntactic dependency.</citsent>
<aftsection>
<nextsent>saw  hh pilar man  hh with telescope saw   hh hh pilar man with telescope figure 1: two dependency trees pilar a man vv saw man sawn with pilar n d telesope p with telesoped figure 2: two dependency trees with lexical categories tures, these approaches cannot be formalized in straightforward manner as context-free rewriting formalisms.in the third approach, which includes forma liza tions of dependency structure such as dependency tree grammar of modina (see (dikovsky and mod ina, 2000) for an overview), link grammar (sleatorand temperley, 1993) or the tree-composition approach of nasr (1996), rules construct the dependency tree incrementally; in these approaches, the grammar licenses dependency relations which, in derivation, are added to the tree one by one, or ingroups.
</nextsent>
<nextsent>in contrast, we are interested in string rewriting system; in such system, we cannot add dependency relations incrementally: all daughters of node must be added at once to represent single rewrite step.
</nextsent>
<nextsent>in the fourth approach, the dependency grammar is converted into headed context-free grammar (abney, 1996; holan et al, 1998), <papid> W98-0503 </papid>also the basic dependency grammar of beletskij (1967) as citedin (dikovsky and modina, 2000).</nextsent>
<nextsent>this approach allows for the recovery of the dependency structure both from the derivation tree and from parse forest represented in polynomial space.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA671">
<title id=" W04-1503.xml">a simple string rewriting formalism for dependency grammar </title>
<section> previous formalizations of dependency.  </section>
<citcontext>
<prevsection>
<prevsent>saw  hh pilar man  hh with telescope saw   hh hh pilar man with telescope figure 1: two dependency trees pilar a man vv saw man sawn with pilar n d telesope p with telesoped figure 2: two dependency trees with lexical categories tures, these approaches cannot be formalized in straightforward manner as context-free rewriting formalisms.in the third approach, which includes forma liza tions of dependency structure such as dependency tree grammar of modina (see (dikovsky and mod ina, 2000) for an overview), link grammar (sleatorand temperley, 1993) or the tree-composition approach of nasr (1996), rules construct the dependency tree incrementally; in these approaches, the grammar licenses dependency relations which, in derivation, are added to the tree one by one, or ingroups.
</prevsent>
<prevsent>in contrast, we are interested in string rewriting system; in such system, we cannot add dependency relations incrementally: all daughters of node must be added at once to represent single rewrite step.
</prevsent>
</prevsection>
<citsent citstr=" W98-0503 ">
in the fourth approach, the dependency grammar is converted into headed context-free grammar (abney, 1996; holan et al, 1998), <papid> W98-0503 </papid>also the basic dependency grammar of beletskij (1967) as citedin (dikovsky and modina, 2000).</citsent>
<aftsection>
<nextsent>this approach allows for the recovery of the dependency structure both from the derivation tree and from parse forest represented in polynomial space.
</nextsent>
<nextsent>(in fact, our parsing algorithm draws on this work.)
</nextsent>
<nextsent>however, the approach of course requires the introduction of additional nonterminal nodes.
</nextsent>
<nextsent>finally, we observe that recursive transition networks (woods, 1970)can be used to encode grammar whose derivation trees are dependency trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA672">
<title id=" W04-1503.xml">a simple string rewriting formalism for dependency grammar </title>
<section> previous formalizations of dependency.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we observe that recursive transition networks (woods, 1970)can be used to encode grammar whose derivation trees are dependency trees.
</prevsent>
<prevsent>however, they are more general framework for encoding grammars than specific type of grammar (for example, wecan also use them to encode cfgs).
</prevsent>
</prevsection>
<citsent citstr=" J00-1004 ">
in somewhat related manner, alshawi et al (2000) <papid> J00-1004 </papid>use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized.</citsent>
<aftsection>
<nextsent>eisner (2000) provides formalization of system that uses two different automata to generate left and right children of head.
</nextsent>
<nextsent>his formalism is very close to the one we present, but it is not string-rewriting formalism (and not really generative at all).
</nextsent>
<nextsent>weare looking for precise formulation of generative dependency grammar, and the question has remained open whether there is an alternate formalism which allows for an unbounded number of adjuncts, introduces all daughter nodes at once in string-rewriting step, and avoids the introduction of additional nonterminal nodes.
</nextsent>
<nextsent>in this section we first review the definition of extended context-free grammar and then show howwe use it to model dependency derivations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA673">
<title id=" W04-1503.xml">a simple string rewriting formalism for dependency grammar </title>
<section> empirical results.  </section>
<citcontext>
<prevsection>
<prevsent>in first step, we determine which rules of the ecfg should be used for each word in the input sentence.
</prevsent>
<prevsent>(recall that grammar rule encodes the active and passive valency, as well as how any arguments are realized, for example, fronted or in canonical posi tion.)
</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
this step is called super tagging and has been suggested and studied in the context of tree adjoining grammar by bangalore and joshi (1999).<papid> J99-2004 </papid></citsent>
<aftsection>
<nextsent>in second step, we use probabilistic ecfg where the probabilities are non-lexical and are based entirely on the grammar rules.
</nextsent>
<nextsent>we extract the most probable derivation from the compact parse forest using dynamic programming in the usual manner.
</nextsent>
<nextsent>this non-lexical probability model is used because the super tagging step already takes the words in the sentence into account.
</nextsent>
<nextsent>the probabilities can be encoded directly as weights on the transitions in the rule-fsms used by the parser.the ecfg grammar we use has been automatically extracted from the penn treebank (ptb).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA674">
<title id=" W04-1503.xml">a simple string rewriting formalism for dependency grammar </title>
<section> empirical results.  </section>
<citcontext>
<prevsection>
<prevsent>this non-lexical probability model is used because the super tagging step already takes the words in the sentence into account.
</prevsent>
<prevsent>the probabilities can be encoded directly as weights on the transitions in the rule-fsms used by the parser.the ecfg grammar we use has been automatically extracted from the penn treebank (ptb).
</prevsent>
</prevsection>
<citsent citstr=" W00-1307 ">
infact, we first extract tree insertion grammar following the work of (xia et al, 2000; <papid> W00-1307 </papid>chen, 2001; chiang, 2000), <papid> P00-1058 </papid>and then directly convert the trees of the obtained tag into automata for the parser.</citsent>
<aftsection>
<nextsent>it is clear that one could also derive an explicitecfg in the same manner.
</nextsent>
<nextsent>the extracted grammar has about 4.800 rules.
</nextsent>
<nextsent>the probabilities are estimated from the corpus during extraction.
</nextsent>
<nextsent>note that there are many different ways of extracting anecfg from the ptb, corresponding to different theories of syntactic dependency.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA675">
<title id=" W04-1503.xml">a simple string rewriting formalism for dependency grammar </title>
<section> empirical results.  </section>
<citcontext>
<prevsection>
<prevsent>this non-lexical probability model is used because the super tagging step already takes the words in the sentence into account.
</prevsent>
<prevsent>the probabilities can be encoded directly as weights on the transitions in the rule-fsms used by the parser.the ecfg grammar we use has been automatically extracted from the penn treebank (ptb).
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
infact, we first extract tree insertion grammar following the work of (xia et al, 2000; <papid> W00-1307 </papid>chen, 2001; chiang, 2000), <papid> P00-1058 </papid>and then directly convert the trees of the obtained tag into automata for the parser.</citsent>
<aftsection>
<nextsent>it is clear that one could also derive an explicitecfg in the same manner.
</nextsent>
<nextsent>the extracted grammar has about 4.800 rules.
</nextsent>
<nextsent>the probabilities are estimated from the corpus during extraction.
</nextsent>
<nextsent>note that there are many different ways of extracting anecfg from the ptb, corresponding to different theories of syntactic dependency.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA676">
<title id=" W04-1016.xml">generic sentence fusion is an ill defined summarization task </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>most typical are summaries of single news document down to headline orshort summary, or of collection of news documents down to headline or short summary (hahn and harman, 2002).
</prevsent>
<prevsent>a few researchers have focused on other aspects of summarization, including single sentence (knight and marcu, 2002), paragraph or short document (daume?
</prevsent>
</prevsection>
<citsent citstr=" P00-1038 ">
iii and marcu, 2002), query-focused (berger and mittal, 2000), <papid> P00-1038 </papid>or speech (hori et al, 2003).</citsent>
<aftsection>
<nextsent>the techniques relevant to, and the challenges faced in each of these tasks can be quite different.nevertheless, they all relyon one critical assump tion: there exists notion of (relative) importance between pieces of information in document (or ut terance), regardless of whether we can detect this or not.
</nextsent>
<nextsent>indeed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions.
</nextsent>
<nextsent>the first partition aims to develop manual evaluation criteria for determining thequality of summary, and is typified by the extensive research done in single-document summarization by halteren and teufel (2003) and by the evaluation strategy proposed by nenkova and passonneau (2004).<papid> N04-1019 </papid></nextsent>
<nextsent>the other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA677">
<title id=" W04-1016.xml">generic sentence fusion is an ill defined summarization task </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>the techniques relevant to, and the challenges faced in each of these tasks can be quite different.nevertheless, they all relyon one critical assump tion: there exists notion of (relative) importance between pieces of information in document (or ut terance), regardless of whether we can detect this or not.
</prevsent>
<prevsent>indeed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions.
</prevsent>
</prevsection>
<citsent citstr=" N04-1019 ">
the first partition aims to develop manual evaluation criteria for determining thequality of summary, and is typified by the extensive research done in single-document summarization by halteren and teufel (2003) and by the evaluation strategy proposed by nenkova and passonneau (2004).<papid> N04-1019 </papid></citsent>
<aftsection>
<nextsent>the other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them).
</nextsent>
<nextsent>work in this area includes that of lin and hovy (2003) <papid> N03-1020 </papid>and pastra and saggion (2003), both of whom inspect the use of bleu-like metrics (papineni et al, 2002) <papid> P02-1040 </papid>in summarization.</nextsent>
<nextsent>the results of these investigations have beenmixed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA678">
<title id=" W04-1016.xml">generic sentence fusion is an ill defined summarization task </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>the first partition aims to develop manual evaluation criteria for determining thequality of summary, and is typified by the extensive research done in single-document summarization by halteren and teufel (2003) and by the evaluation strategy proposed by nenkova and passonneau (2004).<papid> N04-1019 </papid></prevsent>
<prevsent>the other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them).</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
work in this area includes that of lin and hovy (2003) <papid> N03-1020 </papid>and pastra and saggion (2003), both of whom inspect the use of bleu-like metrics (papineni et al, 2002) <papid> P02-1040 </papid>in summarization.</citsent>
<aftsection>
<nextsent>the results of these investigations have beenmixed.
</nextsent>
<nextsent>in the duc competitions (hahn and harman, 2002), when manual evaluation has been employed, it has been commonly observed that human written summaries grossly outscore any machine produced summary.
</nextsent>
<nextsent>all machine-produced summaries tend to show little (statistically significant) difference from one another.
</nextsent>
<nextsent>moreover, baseline system that simply takes the first sentences of document performs just as well or better than intelligently crafted systems when summarizing news stories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA679">
<title id=" W04-1016.xml">generic sentence fusion is an ill defined summarization task </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>the first partition aims to develop manual evaluation criteria for determining thequality of summary, and is typified by the extensive research done in single-document summarization by halteren and teufel (2003) and by the evaluation strategy proposed by nenkova and passonneau (2004).<papid> N04-1019 </papid></prevsent>
<prevsent>the other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them).</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
work in this area includes that of lin and hovy (2003) <papid> N03-1020 </papid>and pastra and saggion (2003), both of whom inspect the use of bleu-like metrics (papineni et al, 2002) <papid> P02-1040 </papid>in summarization.</citsent>
<aftsection>
<nextsent>the results of these investigations have beenmixed.
</nextsent>
<nextsent>in the duc competitions (hahn and harman, 2002), when manual evaluation has been employed, it has been commonly observed that human written summaries grossly outscore any machine produced summary.
</nextsent>
<nextsent>all machine-produced summaries tend to show little (statistically significant) difference from one another.
</nextsent>
<nextsent>moreover, baseline system that simply takes the first sentences of document performs just as well or better than intelligently crafted systems when summarizing news stories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA681">
<title id=" W04-1016.xml">generic sentence fusion is an ill defined summarization task </title>
<section> our study.  </section>
<citcontext>
<prevsection>
<prevsent>ithas been further observed that simply compressing sentences individually and concatenating there sults leads to sub optimal summaries (daume?
</prevsent>
<prevsent>iii andmarcu, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
the use of sentence fusion in multi document summarization has been extensively explored by barzilay in her thesis (barzilay, 2003; barzilay et al, 1999), <papid> P99-1071 </papid>though in the multi-documentsetting, one has redundancy to fall back on.</citsent>
<aftsection>
<nextsent>additionally, the sentence fusion task is sufficiently constrained that it makes possible more complex and linguistically motivated manipulations than are reasonable for full document or multi-document summaries (and for which simple extraction techniques are unlikely to suffice).
</nextsent>
<nextsent>our data comes from collection of computer product reviews from the ziff-davis corporation.
</nextsent>
<nextsent>this corpus consists of roughly seven thousand documents paired with human written abstracts.
</nextsent>
<nextsent>the average document was 1080 words in length, with an abstract of length 136 words, compression rate of roughly 87.5%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA682">
<title id=" W04-1016.xml">generic sentence fusion is an ill defined summarization task </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, using generalization of hidden markov model, we are able to create (in an unsupervised fashion) similar alignments for all of the documents (daume?
</prevsent>
<prevsent>iii and marcu, 2004).
</prevsent>
</prevsection>
<citsent citstr=" J02-4006 ">
this system achieves precision, recall and f-score of0.528, 0.668 and 0.590, respectively (which is significant increase in performance (f = 0.407) over the ibm models or the cut &amp; paste method (jing, 2002)).<papid> J02-4006 </papid>based on these alignments (be they manually created or automatically created), we are able to look for examples of sentence fusions within the data.in particular, we search for sentences in the abstracts which are aligned to exactly two document sentences, for which at least 80% of the summary sentence is aligned and for which at least 20% of the words in the summary sentence come from each of the two document sentences.this leaves us with pairs that consist of two document sentences and one abstract sentence, exactly the sort of data we are looking to use.</citsent>
<aftsection>
<nextsent>we randomly select 25 such pairs from the data collected from the human-aligned portion of the corpus and 25 pairs from the automatically aligned portion, giving us 50 pairs in all.
</nextsent>
<nextsent>3.2 examples based on elicitation.
</nextsent>
<nextsent>in addition to collecting data from the ziff-davis corpus, we also elicited data from human subjects with variety of different backgrounds (though all were familiar with computers and technology).these people were presented with the pairs of document sentences and, independently of the rest of the document, asked to produce single summary sentence that contained the important?
</nextsent>
<nextsent>information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA686">
<title id=" W04-2207.xml">identifying correspondences between words an approach based on a bilingual syntactic analysis of french english parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common assumption is that the alignment of words or phrases raises real challenge, since it is neither one-to-one, nor sequential, nor compact?, and thus the correspondences are fuzzy and contextual?
</prevsent>
<prevsent>(debili, 1997).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
indeed, it is even often diffult for human to determine which source unit correspond to which target unit within aligned sentences (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>most alignment systems working on parallel corpora relyon statistical models, in particular the em ones (brown, della pietra and mercer, 1993).
</nextsent>
<nextsent>quite recently attempts have been made in order to incorporate different types of linguistic information sources into word and phrase alignment systems.
</nextsent>
<nextsent>the idea is to take into account the specific problems arising from the alignment at the word or phrase level mentioned in particular by debili (1997).
</nextsent>
<nextsent>different types of linguistic knowledge are exploited: morphological, lexical and syntactic ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA687">
<title id=" W04-2207.xml">identifying correspondences between words an approach based on a bilingual syntactic analysis of french english parallel corpora </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>two kinds of methods have been basically proposed in order to address the problem of bilingual lexicon extraction.
</prevsent>
<prevsent>on the one hand, terms are recognized in both source and target language and then they are mapped to each other (daille, gaussier and lang?, 1994).
</prevsent>
</prevsection>
<citsent citstr=" P98-1074 ">
on the other hand, only source terms are extracted and the target ones are discovered through the alignment process (gaussier, 1998; <papid> P98-1074 </papid>hull, 2001).</citsent>
<aftsection>
<nextsent>the alignment between terms is obtained either by computing association probabilities (gaussier, 1998 ; <papid> P98-1074 </papid>daille, gaussier and lang?, 1994) or by identifying, forgiven source term, sequence of words in the target language which is likely to contain or to correspond to its translation (hull, 2001).</nextsent>
<nextsent>in so far as the precision rate may be affected by the number of alignments obtained (daille, gaussier and lang?, 1994; gaussier, 1998), <papid> P98-1074 </papid>the results achieved basically range between 80% and 90%, for the first 500 alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA690">
<title id=" W04-2207.xml">identifying correspondences between words an approach based on a bilingual syntactic analysis of french english parallel corpora </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 word alignment.
</prevsent>
<prevsent>quite recently attempts have been made in order to incorporate different types of linguistic information sources into word alignment systems and to combine them with statistical knowledge.
</prevsent>
</prevsection>
<citsent citstr=" W03-0302 ">
various and more or less complex sources of linguistic knowledge are exploited: morphological, lexical (arhenberg, andersson and merkel, 2000) and syntactic knowledge (wu, 2000; lin and cherry, 2003).<papid> W03-0302 </papid></citsent>
<aftsection>
<nextsent>the contribution of these information sources to the alignment process with respect to the statistical data varies according to the considered system.
</nextsent>
<nextsent>however, as pointed out by arhenberg, andersson and merkel (2000) as well as lin and cherry (2003), <papid> W03-0302 </papid>the introduction of linguistic knowledge leads to significant improvement in alignment quality.</nextsent>
<nextsent>in the first case, the accuracy goes from 91% for baseline configuration up to 96.7% for linguistic knowledge based one.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA694">
<title id=" W04-2207.xml">identifying correspondences between words an approach based on a bilingual syntactic analysis of french english parallel corpora </title>
<section> identification of anchor pairs.  </section>
<citcontext>
<prevsection>
<prevsent>in this study, we chose to extract lexicon out of the corpus, the anchor pairs being located both by projecting the lexicon at the level of aligned sentences and processing the identical and fuzzy cognates.
</prevsent>
<prevsent>inra joc aligned sentences 7056 8774 anchor pairs 42570 58771 words/source sentence 21 25 words/target sentence 24 30 anchor pairs/sentence 6.38 6.77 precision (%) 98 99.3
</prevsent>
</prevsection>
<citsent citstr=" H91-1026 ">
to derive list of words which are likely to be used to initiate the syntactic propagation process out of the corpus, we implemented widely used method described notably in (gale and church, 1991; <papid> H91-1026 </papid>ahrenberg, andersson and merkel, 2000) which is based on the assumption that the words which appear frequently in aligned text segments are potential translation equivalents.</citsent>
<aftsection>
<nextsent>for each source (english) and target (french) unit, respectively u1 and u2, extracted by syntex, the translation equivalents are searched for by counting co-occurrences of (u1, u2) in aligned sentences in comparison with their overall occurrences in the corpus and then an association score is computed.
</nextsent>
<nextsent>in this study, we chose the jaccard association score which is calculated as follows:
</nextsent>
<nextsent>6.1 two types of propagation.
</nextsent>
<nextsent>the syntactic propagation may be performed according to two different directions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA699">
<title id=" W03-2904.xml">the multexteast morphosyntactic specification for slavic languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the mid-nineties saw ? to large extent via eu projects ? the rapid development of multilingual language resources and standards for human language technologies.
</prevsent>
<prevsent>however, while the development of resources, tools, and standards was well on its way for eu languages, there had been no comparable efforts for the languages of central and eastern europe.
</prevsent>
</prevsection>
<citsent citstr=" C94-1097 ">
the multext-east project (multilingual text tools and corpora for eastern and central european languages) was spin-off of the eu multext project (ide and vronis, 1994)<papid> C94-1097 </papid>; <papid> C94-1097 </papid>it developed standardised language resources forsix languages (dimitrova et al, 1998): <papid> P98-1050 </papid>bulgarian, czech, estonian, hungarian, romanian, and slovene, as well as for english, the hub?</citsent>
<aftsection>
<nextsent>language of the project.
</nextsent>
<nextsent>the main results of the project were an annotated multilingual corpus (erjavec and ide, 1998), comprising speech corpus, comparable corpus and parallel corpus; lexical resources (ideet al, 1998); and tool resources for the seven languages.
</nextsent>
<nextsent>one of the objectives of multext-east hasbeen to make its resources freely available for research purposes.
</nextsent>
<nextsent>in the scope of the telri concerted action the results of multext-east have been extended with several new languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA701">
<title id=" W03-2904.xml">the multexteast morphosyntactic specification for slavic languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the mid-nineties saw ? to large extent via eu projects ? the rapid development of multilingual language resources and standards for human language technologies.
</prevsent>
<prevsent>however, while the development of resources, tools, and standards was well on its way for eu languages, there had been no comparable efforts for the languages of central and eastern europe.
</prevsent>
</prevsection>
<citsent citstr=" P98-1050 ">
the multext-east project (multilingual text tools and corpora for eastern and central european languages) was spin-off of the eu multext project (ide and vronis, 1994)<papid> C94-1097 </papid>; <papid> C94-1097 </papid>it developed standardised language resources forsix languages (dimitrova et al, 1998): <papid> P98-1050 </papid>bulgarian, czech, estonian, hungarian, romanian, and slovene, as well as for english, the hub?</citsent>
<aftsection>
<nextsent>language of the project.
</nextsent>
<nextsent>the main results of the project were an annotated multilingual corpus (erjavec and ide, 1998), comprising speech corpus, comparable corpus and parallel corpus; lexical resources (ideet al, 1998); and tool resources for the seven languages.
</nextsent>
<nextsent>one of the objectives of multext-east hasbeen to make its resources freely available for research purposes.
</nextsent>
<nextsent>in the scope of the telri concerted action the results of multext-east have been extended with several new languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA703">
<title id=" W04-1014.xml">evaluation measures considering sentence concatenation for automatic summarization by sentence or word extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently, summarizing multiple documents with the same topic has been made target.
</prevsent>
<prevsent>the major approach to extracting sentences that have significant information is statistical, i.e., supervised learning from parallel corpora consisting of original texts and their summarization (kupiec et1summac in the tipster project by darpa (http://www nlpir.nist.gov/related projects/tipster summac) and duc in the tides project (http://duc.nist.gov/) in the u.s. tsc(http://research.nii.ac.jp/ntcir/) in the ntcir by nii (the national institute of informatica) in japan.
</prevsent>
</prevsection>
<citsent citstr=" P98-1009 ">
al., 1995) (aone et al, 1998) (<papid> P98-1009 </papid>mani and bloedorn, 1998).several summarization techniques for multimedia including image, speech, and text have been re searched.</citsent>
<aftsection>
<nextsent>manually transcribed newswire speech (tdt data) and meeting speech (zechner, 2003) have been set as summarization targets.
</nextsent>
<nextsent>the need to automatically generate summaries from speech has led to research on summarizing transcription results obtained by automatic speech recognition instead of manually transcribed speech (hori and furui, 2000a).
</nextsent>
<nextsent>this summarization approach is word extraction (sentence compaction) that attempts to extract significant information, exclude acoustically and linguistically unreliable words, and maintain the meanings of the original speech.
</nextsent>
<nextsent>the summarization approaches that have been mainly researched so far are extracting sentences or words from original text or transcribed speech.there has also been research on generating an ab stract?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA704">
<title id=" W04-1014.xml">evaluation measures considering sentence concatenation for automatic summarization by sentence or word extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this summarization approach is word extraction (sentence compaction) that attempts to extract significant information, exclude acoustically and linguistically unreliable words, and maintain the meanings of the original speech.
</prevsent>
<prevsent>the summarization approaches that have been mainly researched so far are extracting sentences or words from original text or transcribed speech.there has also been research on generating an ab stract?
</prevsent>
</prevsection>
<citsent citstr=" J02-4006 ">
like the much higher level summarization composed freely by human experts (jing, 2002).<papid> J02-4006 </papid>this approach includes not only extracting sentences but also combining sentences to generate new sentences, replacing words, reconstructing syntactic structure, and so on.</citsent>
<aftsection>
<nextsent>evaluation measures for summarization metrics that can be used to accurately evaluate the various appropriateness to summarization are needed.the simplest and probably the ideal way of evaluating automatic summarization is to have human subjects read the summaries and evaluate them in terms of the appropriateness of summarization.
</nextsent>
<nextsent>however, this type of evaluation is too expensive for comparing the efficiencies of many different approaches precisely and repeatedly.
</nextsent>
<nextsent>we thus need automatic evaluation metrics to numerically validate the efficiency of various approaches repeatedly and consistently.automatic summaries can be evaluated by comparing them with manual summaries generated by humans.
</nextsent>
<nextsent>the similarities between the targets and the automatically processed results provide metrics indicating the extent to which the task was accomplished.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA705">
<title id=" W04-1014.xml">evaluation measures considering sentence concatenation for automatic summarization by sentence or word extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, concatena tions between sentences or words have meanings, so some concatenations of sentences or words in the automatic summaries sometimes generate meanings different from the original.
</prevsent>
<prevsent>the evaluation metrics for summarization should thus consider each concatenation between components in the automatic results.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
to evaluate sentence automatically generated with taking consideration word concatenation intoby using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (hori and furui, 2000b) for summarization through word extraction, rouge (lin and hovy, 2003) <papid> N03-1020 </papid>for abstracts, and bleu (papineni et al, 2002) <papid> P02-1040 </papid>for machine translation.</citsent>
<aftsection>
<nextsent>evaluation metrics based on word accuracy, summarization accuracy(sumaccy), using word network made by merging manual summaries has been proposed (hori and furui, 2001).
</nextsent>
<nextsent>in addition, to solve the problems for the coverage of correct answers and the reliability of manual summaries as correct answers, weighted summarization accuracy (wsumaccy) in whichsumaccy is weighted by the majority of the humans?
</nextsent>
<nextsent>selections, has been proposed (hori and furui, 2003a).in contrast, summarization through sentence extraction has been evaluated using only single sentence precision.
</nextsent>
<nextsent>sentence extraction should also be evaluated using measures that take into account sentence concatenations, the coverage of correct answers, and the reliability of manual summaries.this paper presents evaluation results of automatic summarization through sentence or word extraction using the abovementioned metrics based on n-gram precision and sentence/word accuracy and examines how well these measures reflect the judgments of humans as well.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA706">
<title id=" W04-1014.xml">evaluation measures considering sentence concatenation for automatic summarization by sentence or word extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, concatena tions between sentences or words have meanings, so some concatenations of sentences or words in the automatic summaries sometimes generate meanings different from the original.
</prevsent>
<prevsent>the evaluation metrics for summarization should thus consider each concatenation between components in the automatic results.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
to evaluate sentence automatically generated with taking consideration word concatenation intoby using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (hori and furui, 2000b) for summarization through word extraction, rouge (lin and hovy, 2003) <papid> N03-1020 </papid>for abstracts, and bleu (papineni et al, 2002) <papid> P02-1040 </papid>for machine translation.</citsent>
<aftsection>
<nextsent>evaluation metrics based on word accuracy, summarization accuracy(sumaccy), using word network made by merging manual summaries has been proposed (hori and furui, 2001).
</nextsent>
<nextsent>in addition, to solve the problems for the coverage of correct answers and the reliability of manual summaries as correct answers, weighted summarization accuracy (wsumaccy) in whichsumaccy is weighted by the majority of the humans?
</nextsent>
<nextsent>selections, has been proposed (hori and furui, 2003a).in contrast, summarization through sentence extraction has been evaluated using only single sentence precision.
</nextsent>
<nextsent>sentence extraction should also be evaluated using measures that take into account sentence concatenations, the coverage of correct answers, and the reliability of manual summaries.this paper presents evaluation results of automatic summarization through sentence or word extraction using the abovementioned metrics based on n-gram precision and sentence/word accuracy and examines how well these measures reflect the judgments of humans as well.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA707">
<title id=" W04-0401.xml">statistical measures of the semi productivity of light verb constructions </title>
<section> light verb constructions.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W03-1809 ">
much research on multiword expressions involving verbs has focused on verb-particle constructions (vpcs), such as scale up or put down (e.g., bannard et al, 2003; <papid> W03-1809 </papid>mccarthy et al, 2003; <papid> W03-1810 </papid>villavicencio,2003).</citsent>
<aftsection>
<nextsent>another kind of verb-based multiword expression is light verb constructions (lvcs), such as the examples in (1).
</nextsent>
<nextsent>(1) a. sara took stroll along the beach.
</nextsent>
<nextsent>b. paul gave knock on the door.
</nextsent>
<nextsent>c. jamie made pass to her teammate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA708">
<title id=" W04-0401.xml">statistical measures of the semi productivity of light verb constructions </title>
<section> light verb constructions.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W03-1810 ">
much research on multiword expressions involving verbs has focused on verb-particle constructions (vpcs), such as scale up or put down (e.g., bannard et al, 2003; <papid> W03-1809 </papid>mccarthy et al, 2003; <papid> W03-1810 </papid>villavicencio,2003).</citsent>
<aftsection>
<nextsent>another kind of verb-based multiword expression is light verb constructions (lvcs), such as the examples in (1).
</nextsent>
<nextsent>(1) a. sara took stroll along the beach.
</nextsent>
<nextsent>b. paul gave knock on the door.
</nextsent>
<nextsent>c. jamie made pass to her teammate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA709">
<title id=" W04-0401.xml">statistical measures of the semi productivity of light verb constructions </title>
<section> our proposal.  </section>
<citcontext>
<prevsection>
<prevsent>the lexical semantic classes of levin (1993) have been used asa standard verb classification within the computational linguistics community.
</prevsent>
<prevsent>we thus propose using these classes as the semantically similar groups over which to compare acceptability of potential complements with given light verb.2our approach is related to the idea of substi tut ability in multiword expressions.
</prevsent>
</prevsection>
<citsent citstr=" P99-1041 ">
substituting pieces of multiword expression with semantically similar words from thesaurus can be used to determine productivity higher degree of substitutability indicating higher productivity (lin, 1999; <papid> P99-1041 </papid>mccarthy et al, 2003).<papid> W03-1810 </papid>3 instead of using thesaurus based measure, villavicencio (2003) uses substi tut ability over semantic verb classes to determine potential verb-particle combinations.our method is somewhat different from these earlier approaches, not only in focusing on lvcs, but in the precise goal.</citsent>
<aftsection>
<nextsent>while villavicencio (2003) uses verb classes to generalize over verbs and then confirms whether an expression is attested, we seek to determine how good an expression is. specifically, we aim to develop computational approach not only for characterizing the set of complements that can occur with given light verb in these lvcs, but also to quantify the acceptability.in investigating light verbs and their combination with complements from various verb semantic classes, we expect that these lvcs are not fully idiosyncratic, but exhibit systematic behaviour.
</nextsent>
<nextsent>most importantly, we hypothesize that they show class based behaviouri.e., that the same light verb will show distinct patterns of acceptability with complements across different verb classes.
</nextsent>
<nextsent>we also ex 2we also need to compare generalizability over semantic noun classes to further test the linguistic hypothesis.
</nextsent>
<nextsent>we initially performed such experiments on noun classes in wordnet, but, due to the difficulty of deciding an appropriate level of generalization in the hierarchy, we left this as future work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA712">
<title id=" W04-0401.xml">statistical measures of the semi productivity of light verb constructions </title>
<section> materials and methods.  </section>
<citcontext>
<prevsection>
<prevsent>even the 100m words of the british national corpus (bnc reference guide, 2000) do not give an acceptable level of lvc coverage: very common lvc such as take stroll, for instance, is attested only 23 times.
</prevsent>
<prevsent>to ensure sufficient data to detect less common lvcs, we instead use the web as our corpus (in particular, the subsection indexed by the google search engine, http://www.google.com).
</prevsent>
</prevsection>
<citsent citstr=" W02-1030 ">
using the web to overcome data sparseness hasbeen attempted before (keller et al, 2002); <papid> W02-1030 </papid>however, there are issues: misspellings, typographic errors, and pages in other languages all contribute tonoise in the results.</citsent>
<aftsection>
<nextsent>moreover, punctuation is ig 4cf.
</nextsent>
<nextsent>i took the hike that was recommended.
</nextsent>
<nextsent>this finding supports statistical corpus-based approach to lvcs, as their usage may be more nuanced than linguistic theory suggests.
</nextsent>
<nextsent>determiner search strings indefinite give/gives/gave cry definite give/gives/gave the cry demons.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA713">
<title id=" W04-1211.xml">creating a test corpus of clinical notes manually tagged for partofspeech information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>state-of-the-art automated pos taggers achieve accuracy of 93% - 98% and the most successful implementations are based on statistical approaches to pos tagging.
</prevsent>
<prevsent>taggers based on hidden markoff model (hmm) technology currently appear to be in the lead.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
the prime public domain examples of such implementations include the trigramsntags tagger (brandts 2000), xerox tagger (cutting et al 1992) <papid> A92-1018 </papid>and lt pos tagger (mikheev 1997).<papid> J97-3003 </papid></citsent>
<aftsection>
<nextsent>maximum entropy (maxent) based taggers also seem to perform very well (ratnaparkhi 1996, <papid> W96-0213 </papid>jason baldridge, tom morton, and gann bierner http://maxent.sourceforge.net ).</nextsent>
<nextsent>one of the issues with statistical pos taggers is that most of them need representative amount of hand-labeled training data either in the form of comprehensive lexicon and corpus of untagged data or large corpus of text annotated for pos or combination of the two.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA714">
<title id=" W04-1211.xml">creating a test corpus of clinical notes manually tagged for partofspeech information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>state-of-the-art automated pos taggers achieve accuracy of 93% - 98% and the most successful implementations are based on statistical approaches to pos tagging.
</prevsent>
<prevsent>taggers based on hidden markoff model (hmm) technology currently appear to be in the lead.
</prevsent>
</prevsection>
<citsent citstr=" J97-3003 ">
the prime public domain examples of such implementations include the trigramsntags tagger (brandts 2000), xerox tagger (cutting et al 1992) <papid> A92-1018 </papid>and lt pos tagger (mikheev 1997).<papid> J97-3003 </papid></citsent>
<aftsection>
<nextsent>maximum entropy (maxent) based taggers also seem to perform very well (ratnaparkhi 1996, <papid> W96-0213 </papid>jason baldridge, tom morton, and gann bierner http://maxent.sourceforge.net ).</nextsent>
<nextsent>one of the issues with statistical pos taggers is that most of them need representative amount of hand-labeled training data either in the form of comprehensive lexicon and corpus of untagged data or large corpus of text annotated for pos or combination of the two.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA715">
<title id=" W04-1211.xml">creating a test corpus of clinical notes manually tagged for partofspeech information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>taggers based on hidden markoff model (hmm) technology currently appear to be in the lead.
</prevsent>
<prevsent>the prime public domain examples of such implementations include the trigramsntags tagger (brandts 2000), xerox tagger (cutting et al 1992) <papid> A92-1018 </papid>and lt pos tagger (mikheev 1997).<papid> J97-3003 </papid></prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
maximum entropy (maxent) based taggers also seem to perform very well (ratnaparkhi 1996, <papid> W96-0213 </papid>jason baldridge, tom morton, and gann bierner http://maxent.sourceforge.net ).</citsent>
<aftsection>
<nextsent>one of the issues with statistical pos taggers is that most of them need representative amount of hand-labeled training data either in the form of comprehensive lexicon and corpus of untagged data or large corpus of text annotated for pos or combination of the two.
</nextsent>
<nextsent>currently, most of the pos tagger accuracy reports are based on the experiments involving penn treebank data (marcus, 1993).
</nextsent>
<nextsent>the texts in treebank represent the general english domain.
</nextsent>
<nextsent>it is not entirely clear how representative the general english language vocabulary and structure are of specialized sub domain such as clinical reports.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA717">
<title id=" W04-1211.xml">creating a test corpus of clinical notes manually tagged for partofspeech information </title>
<section> annotator agreement.  </section>
<citcontext>
<prevsection>
<prevsent>two types of measures of consistency were computed ? absolute agreement and kappa coefficient.
</prevsent>
<prevsent>the absolute agreement (abs agr) was calculated by dividing the total number of times all annotators agreed on tag over the total number of tags.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
kappa coefficient is given in (1) (carletta 1996) (<papid> J96-2004 </papid>1) )(1 )()( ep epapkappa ? ?= where p(a) is the proportion of times the annotators actually agree and p(e) is the proportion of times the annotators are expected to agree due to chance3.</citsent>
<aftsection>
<nextsent>the absolute agreement is most informative when computed over several sets of labels and where one of the sets represents the authoritative?
</nextsent>
<nextsent>set.
</nextsent>
<nextsent>in this case, the ratio of matches among all the sets including the authoritative?
</nextsent>
<nextsent>set to the total number of labels shows how close the other sets are to the authoritative?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA719">
<title id=" W04-0827.xml">gambl genetic algorithm optimization of memory based wsd </title>
<section> memory-based wsd.  </section>
<citcontext>
<prevsection>
<prevsent>it has been claimed,e.g. in (daelemans et al, 1999), that lazy learning has the right bias for learning natural language processing tasks as it makes possible learning from atypical and low-frequency events that are usually discarded by eager learning methods.
</prevsent>
<prevsent>architecture.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
previous work on memory-basedwsd includes work from ng and lee (1996), <papid> P96-1006 </papid>veenstra et al (2000), hoste et al (2002) and mihalcea (2002).<papid> C02-1039 </papid></citsent>
<aftsection>
<nextsent>the current design of our wsd system is largely based on hoste et al (2002).
</nextsent>
<nextsent>figure 1 gives an overview of the design of our wsd system: the training text is first linguisticallyanalyzed.
</nextsent>
<nextsent>for each word-lemmapos-tag combination, we check if it (i) is in our sense lexicon, (ii) has more than one sense and (iii) has frequency in the training text above certain threshold.
</nextsent>
<nextsent>for all combinations matching these three conditions, we train word expert module.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA720">
<title id=" W04-0827.xml">gambl genetic algorithm optimization of memory based wsd </title>
<section> memory-based wsd.  </section>
<citcontext>
<prevsection>
<prevsent>it has been claimed,e.g. in (daelemans et al, 1999), that lazy learning has the right bias for learning natural language processing tasks as it makes possible learning from atypical and low-frequency events that are usually discarded by eager learning methods.
</prevsent>
<prevsent>architecture.
</prevsent>
</prevsection>
<citsent citstr=" C02-1039 ">
previous work on memory-basedwsd includes work from ng and lee (1996), <papid> P96-1006 </papid>veenstra et al (2000), hoste et al (2002) and mihalcea (2002).<papid> C02-1039 </papid></citsent>
<aftsection>
<nextsent>the current design of our wsd system is largely based on hoste et al (2002).
</nextsent>
<nextsent>figure 1 gives an overview of the design of our wsd system: the training text is first linguisticallyanalyzed.
</nextsent>
<nextsent>for each word-lemmapos-tag combination, we check if it (i) is in our sense lexicon, (ii) has more than one sense and (iii) has frequency in the training text above certain threshold.
</nextsent>
<nextsent>for all combinations matching these three conditions, we train word expert module.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA722">
<title id=" W04-1218.xml">adapting an ner system for german to the biomedical domain </title>
<section> adapting the system.  </section>
<citcontext>
<prevsection>
<prevsent>table 1: the table shows the feature sets f1-f4extracted for all words of 6-word window.
</prevsent>
<prevsent>feature set f4 is described in section 3.
</prevsent>
</prevsection>
<citsent citstr=" W03-1307 ">
after adding atcg sequence (see shen et al 2003) <papid> W03-1307 </papid>and greek letter (see collier et al 2000) <papid> C00-1030 </papid>asdomain-specific deterministic word-surface features, we ran first experiments on the genia (2003) corpus.</citsent>
<aftsection>
<nextsent>while inspecting the results we noticed that special attention was necessary to address the correct boundary detection of the enti 1 all experiments were conducted with the svmlight.
</nextsent>
<nextsent>software package, freely available at: http://svmlight.joachims.org.
</nextsent>
<nextsent>ties and the transformation of the output of the svm-classifiers to the iob-notation.
</nextsent>
<nextsent>a first step to improve the boundary detection is based on the output of second-order markov model in order to support the svms that are not optimised to tag linear sequences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA723">
<title id=" W04-1218.xml">adapting an ner system for german to the biomedical domain </title>
<section> adapting the system.  </section>
<citcontext>
<prevsection>
<prevsent>table 1: the table shows the feature sets f1-f4extracted for all words of 6-word window.
</prevsent>
<prevsent>feature set f4 is described in section 3.
</prevsent>
</prevsection>
<citsent citstr=" C00-1030 ">
after adding atcg sequence (see shen et al 2003) <papid> W03-1307 </papid>and greek letter (see collier et al 2000) <papid> C00-1030 </papid>asdomain-specific deterministic word-surface features, we ran first experiments on the genia (2003) corpus.</citsent>
<aftsection>
<nextsent>while inspecting the results we noticed that special attention was necessary to address the correct boundary detection of the enti 1 all experiments were conducted with the svmlight.
</nextsent>
<nextsent>software package, freely available at: http://svmlight.joachims.org.
</nextsent>
<nextsent>ties and the transformation of the output of the svm-classifiers to the iob-notation.
</nextsent>
<nextsent>a first step to improve the boundary detection is based on the output of second-order markov model in order to support the svms that are not optimised to tag linear sequences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA724">
<title id=" W04-1218.xml">adapting an ner system for german to the biomedical domain </title>
<section> adapting the system.  </section>
<citcontext>
<prevsection>
<prevsent>the correct labelling of these occurrences is the actual task of ner.
</prevsent>
<prevsent>the discourse level describes all occurrences of word form within text unit and the semantic labels assigned to them.
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
addressing word sense disambiguation, gale et al (1992) <papid> H92-1045 </papid>introduced the idea of word sense located at the discourse-level and observed 93 strong one-sense-per-discourse tendency, i.e. several occurrences of polysemous word form have tendency to belong to the same semantic class within one discourse.</citsent>
<aftsection>
<nextsent>it is common practice inner to utilize the discourse level to disambiguate items in non predictive contexts (see e.g. mikheev et al, 1999).<papid> E99-1001 </papid></nextsent>
<nextsent>the corpus level describes all occurrences of word form within all texts available for the application.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA725">
<title id=" W04-1218.xml">adapting an ner system for german to the biomedical domain </title>
<section> adapting the system.  </section>
<citcontext>
<prevsection>
<prevsent>the discourse level describes all occurrences of word form within text unit and the semantic labels assigned to them.
</prevsent>
<prevsent>addressing word sense disambiguation, gale et al (1992) <papid> H92-1045 </papid>introduced the idea of word sense located at the discourse-level and observed 93 strong one-sense-per-discourse tendency, i.e. several occurrences of polysemous word form have tendency to belong to the same semantic class within one discourse.</prevsent>
</prevsection>
<citsent citstr=" E99-1001 ">
it is common practice inner to utilize the discourse level to disambiguate items in non predictive contexts (see e.g. mikheev et al, 1999).<papid> E99-1001 </papid></citsent>
<aftsection>
<nextsent>the corpus level describes all occurrences of word form within all texts available for the application.
</nextsent>
<nextsent>the larger the corpus, the more likely particular word form is seen as member of two or more semantic classes.
</nextsent>
<nextsent>in order to utilize the discourse level, all words tagged as entity within one medline abstract are stored in dynamic lexicon.
</nextsent>
<nextsent>then, the processed discourse unit is matched against the dynamic lexicon in order to detect entities in non-predictive contexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA726">
<title id=" W04-1218.xml">adapting an ner system for german to the biomedical domain </title>
<section> adapting the system.  </section>
<citcontext>
<prevsection>
<prevsent>tem is trained on surface words.
</prevsent>
<prevsent>complex units and longer distant phenomena will lead to further progress in ne-tagging.
</prevsent>
</prevsection>
<citsent citstr=" W03-1308 ">
for the biomedical domain, the work of takeuchi and collier (2003) <papid> W03-1308 </papid>demonstrates the successful incorporation of shallow parsing.for future research, we plan to address these issues by focusing on learning external evidence, i.e.triggers and longer-distant phenomena from unlabeled texts.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA727">
<title id=" W04-0705.xml">applying coreference to improve name recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, we show how these gains can be magnified by clustering documents and using cross-document coreference in these clusters.
</prevsent>
<prevsent>these combined methods yield an absolute improvement of about 3.1% in tagger score.
</prevsent>
</prevsection>
<citsent citstr=" C96-1079 ">
the problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the muc 6 evaluation (grishman and sundheim, 1996).<papid> C96-1079 </papid></citsent>
<aftsection>
<nextsent>a. wide variety of machine learning methods have been applied to this problem, including hidden markov models (bikel et al  1997), <papid> A97-1029 </papid>maximum entropy methods (borthwick et al  1998, <papid> W98-1118 </papid>chieu and ng 2002), <papid> C02-1025 </papid>decision trees (sekine et al  1998), <papid> W98-1120 </papid>conditional random fields (mccallum and li 2003), <papid> W03-0430 </papid>class-based language model (sun et al  2002), <papid> C02-1012 </papid>agent-based approach (ye et al  2002) <papid> C02-1080 </papid>and support vector machines.</nextsent>
<nextsent>however, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA728">
<title id=" W04-0705.xml">applying coreference to improve name recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these combined methods yield an absolute improvement of about 3.1% in tagger score.
</prevsent>
<prevsent>the problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the muc 6 evaluation (grishman and sundheim, 1996).<papid> C96-1079 </papid></prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
a. wide variety of machine learning methods have been applied to this problem, including hidden markov models (bikel et al  1997), <papid> A97-1029 </papid>maximum entropy methods (borthwick et al  1998, <papid> W98-1118 </papid>chieu and ng 2002), <papid> C02-1025 </papid>decision trees (sekine et al  1998), <papid> W98-1120 </papid>conditional random fields (mccallum and li 2003), <papid> W03-0430 </papid>class-based language model (sun et al  2002), <papid> C02-1012 </papid>agent-based approach (ye et al  2002) <papid> C02-1080 </papid>and support vector machines.</citsent>
<aftsection>
<nextsent>however, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ.
</nextsent>
<nextsent>in particular, most of these methods classify an instance of name based on the information about that instance alone, and very local context of that instance ? typically, one or 1 the best results reported for chinese named entity recognition, on the met-2 test corpus, are 0.92 to 0.95 f-measure for the different name types (ye et al  2002).<papid> C02-1080 </papid></nextsent>
<nextsent>two words preceding and following the name.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA730">
<title id=" W04-0705.xml">applying coreference to improve name recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these combined methods yield an absolute improvement of about 3.1% in tagger score.
</prevsent>
<prevsent>the problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the muc 6 evaluation (grishman and sundheim, 1996).<papid> C96-1079 </papid></prevsent>
</prevsection>
<citsent citstr=" W98-1118 ">
a. wide variety of machine learning methods have been applied to this problem, including hidden markov models (bikel et al  1997), <papid> A97-1029 </papid>maximum entropy methods (borthwick et al  1998, <papid> W98-1118 </papid>chieu and ng 2002), <papid> C02-1025 </papid>decision trees (sekine et al  1998), <papid> W98-1120 </papid>conditional random fields (mccallum and li 2003), <papid> W03-0430 </papid>class-based language model (sun et al  2002), <papid> C02-1012 </papid>agent-based approach (ye et al  2002) <papid> C02-1080 </papid>and support vector machines.</citsent>
<aftsection>
<nextsent>however, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ.
</nextsent>
<nextsent>in particular, most of these methods classify an instance of name based on the information about that instance alone, and very local context of that instance ? typically, one or 1 the best results reported for chinese named entity recognition, on the met-2 test corpus, are 0.92 to 0.95 f-measure for the different name types (ye et al  2002).<papid> C02-1080 </papid></nextsent>
<nextsent>two words preceding and following the name.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA731">
<title id=" W04-0705.xml">applying coreference to improve name recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these combined methods yield an absolute improvement of about 3.1% in tagger score.
</prevsent>
<prevsent>the problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the muc 6 evaluation (grishman and sundheim, 1996).<papid> C96-1079 </papid></prevsent>
</prevsection>
<citsent citstr=" C02-1025 ">
a. wide variety of machine learning methods have been applied to this problem, including hidden markov models (bikel et al  1997), <papid> A97-1029 </papid>maximum entropy methods (borthwick et al  1998, <papid> W98-1118 </papid>chieu and ng 2002), <papid> C02-1025 </papid>decision trees (sekine et al  1998), <papid> W98-1120 </papid>conditional random fields (mccallum and li 2003), <papid> W03-0430 </papid>class-based language model (sun et al  2002), <papid> C02-1012 </papid>agent-based approach (ye et al  2002) <papid> C02-1080 </papid>and support vector machines.</citsent>
<aftsection>
<nextsent>however, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ.
</nextsent>
<nextsent>in particular, most of these methods classify an instance of name based on the information about that instance alone, and very local context of that instance ? typically, one or 1 the best results reported for chinese named entity recognition, on the met-2 test corpus, are 0.92 to 0.95 f-measure for the different name types (ye et al  2002).<papid> C02-1080 </papid></nextsent>
<nextsent>two words preceding and following the name.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA732">
<title id=" W04-0705.xml">applying coreference to improve name recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these combined methods yield an absolute improvement of about 3.1% in tagger score.
</prevsent>
<prevsent>the problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the muc 6 evaluation (grishman and sundheim, 1996).<papid> C96-1079 </papid></prevsent>
</prevsection>
<citsent citstr=" W98-1120 ">
a. wide variety of machine learning methods have been applied to this problem, including hidden markov models (bikel et al  1997), <papid> A97-1029 </papid>maximum entropy methods (borthwick et al  1998, <papid> W98-1118 </papid>chieu and ng 2002), <papid> C02-1025 </papid>decision trees (sekine et al  1998), <papid> W98-1120 </papid>conditional random fields (mccallum and li 2003), <papid> W03-0430 </papid>class-based language model (sun et al  2002), <papid> C02-1012 </papid>agent-based approach (ye et al  2002) <papid> C02-1080 </papid>and support vector machines.</citsent>
<aftsection>
<nextsent>however, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ.
</nextsent>
<nextsent>in particular, most of these methods classify an instance of name based on the information about that instance alone, and very local context of that instance ? typically, one or 1 the best results reported for chinese named entity recognition, on the met-2 test corpus, are 0.92 to 0.95 f-measure for the different name types (ye et al  2002).<papid> C02-1080 </papid></nextsent>
<nextsent>two words preceding and following the name.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA733">
<title id=" W04-0705.xml">applying coreference to improve name recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these combined methods yield an absolute improvement of about 3.1% in tagger score.
</prevsent>
<prevsent>the problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the muc 6 evaluation (grishman and sundheim, 1996).<papid> C96-1079 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-0430 ">
a. wide variety of machine learning methods have been applied to this problem, including hidden markov models (bikel et al  1997), <papid> A97-1029 </papid>maximum entropy methods (borthwick et al  1998, <papid> W98-1118 </papid>chieu and ng 2002), <papid> C02-1025 </papid>decision trees (sekine et al  1998), <papid> W98-1120 </papid>conditional random fields (mccallum and li 2003), <papid> W03-0430 </papid>class-based language model (sun et al  2002), <papid> C02-1012 </papid>agent-based approach (ye et al  2002) <papid> C02-1080 </papid>and support vector machines.</citsent>
<aftsection>
<nextsent>however, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ.
</nextsent>
<nextsent>in particular, most of these methods classify an instance of name based on the information about that instance alone, and very local context of that instance ? typically, one or 1 the best results reported for chinese named entity recognition, on the met-2 test corpus, are 0.92 to 0.95 f-measure for the different name types (ye et al  2002).<papid> C02-1080 </papid></nextsent>
<nextsent>two words preceding and following the name.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA734">
<title id=" W04-0705.xml">applying coreference to improve name recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these combined methods yield an absolute improvement of about 3.1% in tagger score.
</prevsent>
<prevsent>the problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the muc 6 evaluation (grishman and sundheim, 1996).<papid> C96-1079 </papid></prevsent>
</prevsection>
<citsent citstr=" C02-1012 ">
a. wide variety of machine learning methods have been applied to this problem, including hidden markov models (bikel et al  1997), <papid> A97-1029 </papid>maximum entropy methods (borthwick et al  1998, <papid> W98-1118 </papid>chieu and ng 2002), <papid> C02-1025 </papid>decision trees (sekine et al  1998), <papid> W98-1120 </papid>conditional random fields (mccallum and li 2003), <papid> W03-0430 </papid>class-based language model (sun et al  2002), <papid> C02-1012 </papid>agent-based approach (ye et al  2002) <papid> C02-1080 </papid>and support vector machines.</citsent>
<aftsection>
<nextsent>however, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ.
</nextsent>
<nextsent>in particular, most of these methods classify an instance of name based on the information about that instance alone, and very local context of that instance ? typically, one or 1 the best results reported for chinese named entity recognition, on the met-2 test corpus, are 0.92 to 0.95 f-measure for the different name types (ye et al  2002).<papid> C02-1080 </papid></nextsent>
<nextsent>two words preceding and following the name.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA735">
<title id=" W04-0705.xml">applying coreference to improve name recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these combined methods yield an absolute improvement of about 3.1% in tagger score.
</prevsent>
<prevsent>the problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the muc 6 evaluation (grishman and sundheim, 1996).<papid> C96-1079 </papid></prevsent>
</prevsection>
<citsent citstr=" C02-1080 ">
a. wide variety of machine learning methods have been applied to this problem, including hidden markov models (bikel et al  1997), <papid> A97-1029 </papid>maximum entropy methods (borthwick et al  1998, <papid> W98-1118 </papid>chieu and ng 2002), <papid> C02-1025 </papid>decision trees (sekine et al  1998), <papid> W98-1120 </papid>conditional random fields (mccallum and li 2003), <papid> W03-0430 </papid>class-based language model (sun et al  2002), <papid> C02-1012 </papid>agent-based approach (ye et al  2002) <papid> C02-1080 </papid>and support vector machines.</citsent>
<aftsection>
<nextsent>however, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ.
</nextsent>
<nextsent>in particular, most of these methods classify an instance of name based on the information about that instance alone, and very local context of that instance ? typically, one or 1 the best results reported for chinese named entity recognition, on the met-2 test corpus, are 0.92 to 0.95 f-measure for the different name types (ye et al  2002).<papid> C02-1080 </papid></nextsent>
<nextsent>two words preceding and following the name.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA739">
<title id=" W04-0705.xml">applying coreference to improve name recognition </title>
<section> baseline systems.  </section>
<citcontext>
<prevsection>
<prevsent>we have extensive rules for name-name coreference, including rules specific to the particular name types.
</prevsent>
<prevsent>for these experiments, we do not attempt to resolve pronouns, and we only resolve names with nominals when the name and nominal appear in close proximity in specific structure, as listed in table 1.
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
we have used the muc coreference scoring metric (vilain et al  1995) <papid> M95-1005 </papid>to evaluate this re solver, excluding all pronouns and limiting ourselves to noun phrases of semantic type per, org, and gpe.</citsent>
<aftsection>
<nextsent>using perfect (hand-generated) set of mentions, we obtain recall of 82.7% and precision of 95.1%, for an score of 88.47%.
</nextsent>
<nextsent>2 this class is used in the u.s. governments ace evaluations; it excludes locations without governments, such as bodies of water and mountains.
</nextsent>
<nextsent>using the mentions generated by our extraction system, we obtain recall of 74.3%, precision of 84.5%, and an score of 79.07%.3
</nextsent>
<nextsent>in order to decide when we need to relyon global (coreference) information for name tagging, we want to have some assessment of the confidence that the name tagger has in individual tagging decisions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA742">
<title id=" W04-0409.xml">integrating morphology with multiword expression processing in turkish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical approaches require corpus that contains significant numbers of occurrences ofmulti-word expressions.
</prevsent>
<prevsent>but even if the corpus consists of millions of words, usually, the frequencies of multi-word expressions are too low for statistical extraction.
</prevsent>
</prevsection>
<citsent citstr=" W02-2001 ">
baldwin and villavicencio (2002)<papid> W02-2001 </papid>indicate that two-thirds of verb-particle constructions occur at most three times in the overall corpus, meaning that any extraction method must be ableto handle extremely sparse data.?</citsent>
<aftsection>
<nextsent>they use rule based method to extract multi-word expressions in the form of head verb and single obligatory preposition employing tagger augmented with an second acl workshop on multiword expressions: integrating processing, july 2004, pp.
</nextsent>
<nextsent>64-71existing chunking system with which they first identify the particle chunked and then turn back for the verb part of the construction.piao et al  (2003) <papid> W03-1807 </papid>employ their semantic field annotator usas, containing 37,000 words and template list of 16,000 multi-word units, all constructed manually from various resources, in order to extract multi-word expressions.</nextsent>
<nextsent>the evaluation indicates high precision (over 90%) but the estimated recall is about 40%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA743">
<title id=" W04-0409.xml">integrating morphology with multiword expression processing in turkish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>baldwin and villavicencio (2002)<papid> W02-2001 </papid>indicate that two-thirds of verb-particle constructions occur at most three times in the overall corpus, meaning that any extraction method must be ableto handle extremely sparse data.?</prevsent>
<prevsent>they use rule based method to extract multi-word expressions in the form of head verb and single obligatory preposition employing tagger augmented with an second acl workshop on multiword expressions: integrating processing, july 2004, pp.</prevsent>
</prevsection>
<citsent citstr=" W03-1807 ">
64-71existing chunking system with which they first identify the particle chunked and then turn back for the verb part of the construction.piao et al  (2003) <papid> W03-1807 </papid>employ their semantic field annotator usas, containing 37,000 words and template list of 16,000 multi-word units, all constructed manually from various resources, in order to extract multi-word expressions.</citsent>
<aftsection>
<nextsent>the evaluation indicates high precision (over 90%) but the estimated recall is about 40%.
</nextsent>
<nextsent>deeper investigation on the corpus has indicated that two-thirds of the multi-word expressions occur in the corpus once or twice, verifying the fact that the statistical methods filtering low frequencies would fail.urizar et al  (2000) describe basque terminology extraction system which covered multi-word term extraction as subset.
</nextsent>
<nextsent>as basque is highly inflected agglutinative language like turkish, morphological information is exploited to better definemulti-word patterns.
</nextsent>
<nextsent>their lemmatizer/tagger euslem, consists of tokenizer followed by two subsystems for the treatment of single word and multiword expressions, and disambiguator.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA744">
<title id=" W04-2204.xml">automatic construction of a transfer dictionary considering directionality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are many ways of dictionary building.
</prevsent>
<prevsent>for machine translation, bilingual transfer dictionary is most important resource.
</prevsent>
</prevsection>
<citsent citstr=" W02-1705 ">
an interesting approach is the papillon project that focuses on building multilingual lexical database to construct large, detailed and principled dictionaries (boitet et al, 2002).<papid> W02-1705 </papid></citsent>
<aftsection>
<nextsent>the main source of multilingual dictionaries is monolingual dictionaries.
</nextsent>
<nextsent>each monolingual dictionary is connected to inter lingual links.
</nextsent>
<nextsent>to make this possible, we need many contributors, ex ? some of this research was done while at atr.perts and the donated data.
</nextsent>
<nextsent>one of the studies related to the papillon project tried to link the words using definitions between english and french, but the method can be extended to other language pairs (lafourcade, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA745">
<title id=" W04-2204.xml">automatic construction of a transfer dictionary considering directionality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to make this possible, we need many contributors, ex ? some of this research was done while at atr.perts and the donated data.
</prevsent>
<prevsent>one of the studies related to the papillon project tried to link the words using definitions between english and french, but the method can be extended to other language pairs (lafourcade, 2002).
</prevsent>
</prevsection>
<citsent citstr=" C94-1048 ">
other research that focuses on the automatic building of bilingual dictionaries include tanaka and umemura (1994), <papid> C94-1048 </papid>shirai and yamamoto (2001), shirai et al (2001), bond et al (2001), and paik et al (2001).</citsent>
<aftsection>
<nextsent>our main concern is automatically building bilingual dictionary, especially with different combinations of dictionaries.
</nextsent>
<nextsent>none of the research on building dictionaries seriously considers the characteristics of dictionaries.
</nextsent>
<nextsent>a dictionary has peculiar characteristic according to its directionality.
</nextsent>
<nextsent>for example, we use ajapanese-to-english (henceforth, je) dictionary mainly used by japanese often when they write or speak in english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA750">
<title id=" W04-0604.xml">the semantics of markup mapping legacy markup schemas to a common semantics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>as such, it is in the spirit of efforts such as sperberg-mcqueen et al (2000), who define the meaning of markup as the set of inferences licensed by it.
</prevsent>
<prevsent>however, their model does not provide for the general comparison of documents.
</prevsent>
</prevsection>
<citsent citstr=" W03-1901 ">
it is also in the spirit of the proposal for linguistic annotation framework (laf) underdevelopment by working group 1-1 of iso tc 37 sc 4 [www.tc37sc4.org] (ide and romary, 2003; <papid> W03-1901 </papid>ide, romary and de la clergerie, 2003), but differs from it in some significant ways.</citsent>
<aftsection>
<nextsent>for example, our strategy does not require that the source annotations be mapped to an xml pivot format?.
</nextsent>
<nextsent>on the other hand, the laf does not require that the source annotations be in xml to begin with.
</nextsent>
<nextsent>the data categories?
</nextsent>
<nextsent>of the laf correspond to the concepts in gold; however the creation of an ontology of annotation classes and types?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA752">
<title id=" W04-0404.xml">translation by machine of complex nominals getting it right </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>combining these observations, we see that translator ormt system attempting to translate one of these corpora will run across nn compounds with high frequency, but that each individual nn compound will occur only few times (with around 45-60% occuring only once).
</prevsent>
<prevsent>the upshot of this for mt system sand translators is that nn compounds are too varied to be able to pre-compile an exhaustive list of translated nn compounds, and must instead be able to deal with novel nn compounds on the fly.
</prevsent>
</prevsection>
<citsent citstr=" W03-1803 ">
this claim is supported by tanaka and baldwin (2003<papid> W03-1803 </papid>a), who found that static bilingual dictionaries had atype coverage of around 84% and 94% over the top250 most frequent english and japanese nn compounds, respectively, but only 27% and 60%, respectively, over random sample of nn compounds occurring more than 10 times in the corpus.</citsent>
<aftsection>
<nextsent>we develop and test method for translating nn compounds based on japanese   english mt. the method can act as standalone module in an mt second acl workshop on multiword expressions: integrating processing, july 2004, pp.
</nextsent>
<nextsent>24-31 system, translating nn compounds according to the best-scoring translation candidate produced by the method, and it is primarly in this context that we present and evaluate the method.
</nextsent>
<nextsent>this is congruent with the findings of koehn and knight (2003) <papid> P03-1040 </papid>that, in the context of statistical mt, overall translation performance improves when source language noun phrases are prescriptively translated as noun phrases in the target language.</nextsent>
<nextsent>alternatively, the proposed method can be used to generate list of plausible translation candidates for each nn compound, for human translator or mt system to select between based on the full translation context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA757">
<title id=" W04-0404.xml">translation by machine of complex nominals getting it right </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we develop and test method for translating nn compounds based on japanese   english mt. the method can act as standalone module in an mt second acl workshop on multiword expressions: integrating processing, july 2004, pp.
</prevsent>
<prevsent>24-31 system, translating nn compounds according to the best-scoring translation candidate produced by the method, and it is primarly in this context that we present and evaluate the method.
</prevsent>
</prevsection>
<citsent citstr=" P03-1040 ">
this is congruent with the findings of koehn and knight (2003) <papid> P03-1040 </papid>that, in the context of statistical mt, overall translation performance improves when source language noun phrases are prescriptively translated as noun phrases in the target language.</citsent>
<aftsection>
<nextsent>alternatively, the proposed method can be used to generate list of plausible translation candidates for each nn compound, for human translator or mt system to select between based on the full translation context.
</nextsent>
<nextsent>in the remainder of the paper, we describe the translation procedure and resources used in this research ( - 2), and outline the translation candidate selection method, benchmark selection method and pre-processors our method relies on ( - 3).
</nextsent>
<nextsent>we then evaluate the method using variety of data sources( - 4), and finally compare our method to related research ( - 5).
</nextsent>
<nextsent>2.1 translation procedure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA758">
<title id=" W04-0404.xml">translation by machine of complex nominals getting it right </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>we then evaluate the method using variety of data sources( - 4), and finally compare our method to related research ( - 5).
</prevsent>
<prevsent>2.1 translation procedure.
</prevsent>
</prevsection>
<citsent citstr=" C02-1011 ">
we translate nn compounds by way of two-phase procedure, incorporating generation and selection (similarly to cao and li (2002) <papid> C02-1011 </papid>and langkilde and knight (1998)).<papid> P98-1116 </papid></citsent>
<aftsection>
<nextsent>generation consists of looking up word-level translations for each word in the nn compound to be translated, and running them through set of constructional translation templates to generate translation candidates.
</nextsent>
<nextsent>in order to translate      kankei  kaizen improvement in relations?, for example, possible word-level translations for  are relation, connection and relationship, and translations for   are improvement and betterment.
</nextsent>
<nextsent>constructional templates are of the form [n
</nextsent>
<nextsent>in n
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA759">
<title id=" W04-0404.xml">translation by machine of complex nominals getting it right </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>we then evaluate the method using variety of data sources( - 4), and finally compare our method to related research ( - 5).
</prevsent>
<prevsent>2.1 translation procedure.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
we translate nn compounds by way of two-phase procedure, incorporating generation and selection (similarly to cao and li (2002) <papid> C02-1011 </papid>and langkilde and knight (1998)).<papid> P98-1116 </papid></citsent>
<aftsection>
<nextsent>generation consists of looking up word-level translations for each word in the nn compound to be translated, and running them through set of constructional translation templates to generate translation candidates.
</nextsent>
<nextsent>in order to translate      kankei  kaizen improvement in relations?, for example, possible word-level translations for  are relation, connection and relationship, and translations for   are improvement and betterment.
</nextsent>
<nextsent>constructional templates are of the form [n
</nextsent>
<nextsent>in n
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA765">
<title id=" W04-0404.xml">translation by machine of complex nominals getting it right </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 translation data.
</prevsent>
<prevsent>in order to generate english and japanese nn compound test data, we first extracted out all nn bigrams from the reuters corpus and mainichi shimbun corpus.
</prevsent>
</prevsection>
<citsent citstr=" N01-1006 ">
the reuters copus was first tagged and chunked using fntbl (ngai and florian, 2001), <papid> N01-1006 </papid>and lemmatised using morph (minnen et al, 2001), while the mainichi shimbun was segmented and tagged using chasen (matsumoto et al, 1999).</citsent>
<aftsection>
<nextsent>for both english and japanese, we took only those nn bigrams adjoined by non-nouns to ensure that they were not part of larger compound nominal.
</nextsent>
<nextsent>we additionally measured the entropy of the left and right contexts for each nn type, and filtered out all compounds where either entropy value was  .2 this was done in an attempt to, once again, exclude nns which were embedded in larger mwes, such asser vice department in social service department.
</nextsent>
<nextsent>we next calculated the frequency of occurrence of each nn compound type identified in the english and japanese corpora, and ranked the nn compound types in order of corpus frequency.
</nextsent>
<nextsent>based on this ranking, we split the nn compound types into three partitions of equal token frequency, and from each partition, randomly selected 250 nn compounds.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA777">
<title id=" W04-0404.xml">translation by machine of complex nominals getting it right </title>
<section> selection methodology.  </section>
<citcontext>
<prevsection>
<prevsent>the corpus frequencies were extracted from the same three corpora as were described in - 1: the bnc and reuters corpus for english, and mainichi shimbun corpus for japanese.
</prevsent>
<prevsent>we chose to use thebnc and reuters corpus because of their complementary nature: the bnc is balanced corpus and hence has rounded coverage of nn compounds (see table 1), whereas the reuters corpus contains newswire data which aligns relatively well in content with the newspaper articles in the mainichi shimbun corpus.
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
we calculated the corpus frequencies based on the tag and dependency output of rasp (briscoe and carroll, 2002) for english, and cabocha (kudo and matsumoto, 2002) <papid> W02-2016 </papid>for japanese.</citsent>
<aftsection>
<nextsent>rasp is tag sequence grammar-based stochastic parser which attempts to exhaustively resolve inter-word dependencies in the input.
</nextsent>
<nextsent>cabocha, on the other hand, chunks the input into head-annotated bunsetsu?
</nextsent>
<nextsent>orbase phrases, and resolves only inter-phrase dependencies.
</nextsent>
<nextsent>we thus independently determined the intra-phrasal structure from the cabocha output based on pos-conditioned templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA784">
<title id=" W04-0404.xml">translation by machine of complex nominals getting it right </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they report an impressive f-score of 0.73 over dataset of 1000instances, although they also cite prior-based score (equivalent to our baseline) of 0.70 for thetask, such that the particular dataset they are dealing with would appear to be less complex than that which we have targeted.
</prevsent>
<prevsent>having said this, contextual similarity is an orthogonal data source to those used in this research, and has the potential to further improve the accuracy of our method.
</prevsent>
</prevsection>
<citsent citstr=" W01-1413 ">
nagata et al (2001) <papid> W01-1413 </papid>use partially bilingual?</citsent>
<aftsection>
<nextsent>web pages, that is web pages which are predominantly japanese, say, but interspersed with english words,to extract translation pairs.
</nextsent>
<nextsent>they do this by accessing web pages containing given japanese expression, and looking for the english expression which occurs most reliably in its immediate vicinity.
</nextsent>
<nextsent>the method achieves an impressive gold-standard accuracy of 0.62, at recall of 0.68, over combination of simplex nouns and compound nominals.grefenstette (1999) uses web data to select english translations for compositional german and spanish noun compounds, and achieves an impressive accuracy of 0.860.87.
</nextsent>
<nextsent>the translation task grefenstette targets is intrinsically simpler than that described in this paper, however, in that he considers only those compounds which translate into nn compounds in english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA785">
<title id=" W04-0301.xml">competence and performance grammar in incremental processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>different algorithms result in different possibilities for the configurations of partial structures that the parser builds.
</prevsent>
<prevsent>for example, bottom up algorithm will never build partial structure with nonterminal leaf nodes.
</prevsent>
</prevsection>
<citsent citstr=" J01-2004 ">
the standard approach is to assign this responsibility to the parsing algorithm, whether the grammar is based on standard context-freeformalisms (roark, 2001), <papid> J01-2004 </papid>on generative syntactic theories based on context-free backbone (crocker, 1992), or on categorial approaches, like e.g. combinatory categorial grammar (ccg ?</citsent>
<aftsection>
<nextsent>(steedman, 2000)).
</nextsent>
<nextsent>a different method is to assign this responsibility to the competence component.
</nextsent>
<nextsent>in this case the space of possible configurations of partial structures is constrained by the grammatical derivation process itself, and the parsing algorithm needs tobe aligned with these requirements.
</nextsent>
<nextsent>this approach is exemplified by the works of kempson et al (2000) and phillips (2003), who argue that many problems in theoretical syntax, like the definition of constituency, can be solved by extending this responsability to the competence grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA789">
<title id=" W04-2012.xml">answer validation by keyword association </title>
<section> answer validation by keyword.  </section>
<citcontext>
<prevsection>
<prevsent>and choice are affected by the hits of the choice alone.
</prevsent>
<prevsent>therefore some normalization might be required.
</prevsent>
</prevsection>
<citsent citstr=" P03-2020 ">
based on the analysis above, we employ the metrics proposed by sato and sasaki (2003).<papid> P03-2020 </papid>sato and sasaki (2003) <papid> P03-2020 </papid>has proposed two metrics for evaluating the strength of the relation of two terms.</citsent>
<aftsection>
<nextsent>suppose that be the set of keywords and be the choice.
</nextsent>
<nextsent>in this paper, we call the hits of conjunct query consisting of keywords and choice , which is normalized by the hits of x, as forward association fa(x, ).
</nextsent>
<nextsent>we also call the hits of conjunct query and , which is normalized by the hits of , as backward association ba(x, ).
</nextsent>
<nextsent>fa(x, ) = hits(x ? {y })/hits(x) ba(x, ) = hits(x ? {y })/hits({y })note that when is fixed, fa(x, ) is proportional to hits(x ? {y }).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA791">
<title id=" W04-2012.xml">answer validation by keyword association </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the method proposed by brill et al (2002),answer candidates are picked up from the summary pages returned by search engine.
</prevsent>
<prevsent>then, each answer candidate is validated by searching for relevant documents in the trec qa document collection.
</prevsent>
</prevsection>
<citsent citstr=" P02-1054 ">
both methods do not consider the number of hits returned by the search en gine.magnini et al (2002) <papid> P02-1054 </papid>proposed an answer validation method which uses the number of search engine hits.</citsent>
<aftsection>
<nextsent>they formulate search engine queries using alta vistas or and near operators.
</nextsent>
<nextsent>major difference between the method of magnini et al (2002) <papid> P02-1054 </papid>and ours is in keyword se lection.</nextsent>
<nextsent>in the method of magnini et al (2002), <papid> P02-1054 </papid>the initial keywords are content words extracted from question sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA804">
<title id=" W04-0911.xml">lexical semantic interpretation of language input in mathematical dialogs </title>
<section> interpretation strategy.  </section>
<citcontext>
<prevsection>
<prevsent>(c ? d), etc. next, the pre-processed input is parsed with lexically-based syntactic/semantic parser builton multi-modal combinatory categorial grammar (baldridge, 2002; baldridge and kruijff, 2003).the task of the deep parser is to produce an fgd based linguistic meaning representation of syntactically well-formed sentences and fragments.
</prevsent>
<prevsent>the linguistic meaning is represented in the formalism of hybrid logic dependency semantics.
</prevsent>
</prevsection>
<citsent citstr=" P02-1041 ">
details on the semantic construction in this formalism can be found in (baldridge and kruijff, 2002).<papid> P02-1041 </papid>to derive our set of trs we generalize and simplify the collection of praguian tectogrammaticalrelations from (hajicova?</citsent>
<aftsection>
<nextsent>et al, 2000).
</nextsent>
<nextsent>one reason for simplification is to distinguish which relations are to be understood metaphorically given the domain-specific sub-language.
</nextsent>
<nextsent>the most commonly occurring relations in our context (aside fromthe roles of actor and patient) are cause, condition, and result-conclusion (which coincide withthe rhetorical relations in the argumentative structure of the proof): da [a ? k(b) gilt] cause , alle x, die in sind sind nicht in [as ak(b) applies, all that are in are not in b] wenn [a ? k(b)] cond , dann ? b=?
</nextsent>
<nextsent>[if ak(b), then ab=?]for example, in one of the readings of en thaelt ? a?, the verb enthaelten?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA805">
<title id=" W04-1115.xml">combining prosodic and text features for segmentation of mandarin broadcast news </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>natural spoken discourse is composed of sequence of utterances, not independently generated or randomly strung together, but rather organized according to basic structural principles.
</prevsent>
<prevsent>this structure inturn guides the interpretation of individual utterances and the discourse as whole.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
formal written discourse signals hierarchical, tree-based discourse structure explicitly by the division of the text into chapters, sections, paragraphs, and sentences.this structure, in turn, identifies domains for in terpretation; many systems for anaphora resolution relyon some notion of locality (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>similarly, this structure represents topical organization, and thus would be useful in information retrieval to select documents where the primary sections are on-topic, and, for summarization, to select information covering the different aspects of the topic.
</nextsent>
<nextsent>unfortunately, spoken discourse does not include the orthographic conventions that signal structural organization in written discourse.
</nextsent>
<nextsent>instead, one must infer the hierarchical structure of spoken discourse from other cues.
</nextsent>
<nextsent>prior research (nakatani et al,1995; swerts, 1997) has shown that human label ers can more sharply, consistently, and confidently identify discourse structure in word-level transcription when an original audio recording is available than they can on the basis of the transcribed text alone.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA806">
<title id=" W04-1115.xml">combining prosodic and text features for segmentation of mandarin broadcast news </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>not only is the use of prosodic cues to topic segmentation much less well-studied in general than is the use of text cues, but the use of prosodic cueshas been largely limited to english and other european languages.
</prevsent>
<prevsent>most prior research on automatic topic segmentation has been applied to clean text only and thus used textual features.
</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (hearst,1994) <papid> P94-1002 </papid>and also discourse markers to identify discourse structure (marcu, 2000).the topic detection and tracking (tdt) evaluations focused on segmentation of both text and speech sources.</citsent>
<aftsection>
<nextsent>this framework introduced new challenges in dealing with error ful automatic transcriptions as well as new opportunities to exploit cues in the original speech.
</nextsent>
<nextsent>the most successful approach (beeferman et al, 1999) produced automatic segment ations that yielded retrieval results approaching those with manual segment ations, using text and silence features.
</nextsent>
<nextsent>(tur et al, 2001) <papid> J01-1002 </papid>applied both prosody-only and mixed text-prosody model to segmentation of tdt english broadcast news, with the best results combining text and prosodic features.</nextsent>
<nextsent>(hirschberg and nakatani, 1998) also examined automatic topic segmentation basedon prosodic cues, in the domain of english broadcast news, while (hirschberg et al, 2001) applied similar cues to segmentation of voicemail.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA807">
<title id=" W04-1115.xml">combining prosodic and text features for segmentation of mandarin broadcast news </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this framework introduced new challenges in dealing with error ful automatic transcriptions as well as new opportunities to exploit cues in the original speech.
</prevsent>
<prevsent>the most successful approach (beeferman et al, 1999) produced automatic segment ations that yielded retrieval results approaching those with manual segment ations, using text and silence features.
</prevsent>
</prevsection>
<citsent citstr=" J01-1002 ">
(tur et al, 2001) <papid> J01-1002 </papid>applied both prosody-only and mixed text-prosody model to segmentation of tdt english broadcast news, with the best results combining text and prosodic features.</citsent>
<aftsection>
<nextsent>(hirschberg and nakatani, 1998) also examined automatic topic segmentation basedon prosodic cues, in the domain of english broadcast news, while (hirschberg et al, 2001) applied similar cues to segmentation of voicemail.
</nextsent>
<nextsent>work in discourse analysis (nakatani et al, 1995;swerts, 1997) in both english and dutch has identified features such as changes in pitch range, intensity, and speaking rate associated with segment boundaries and with boundaries of different strengths.
</nextsent>
<nextsent>they also demonstrated that access to acoustic cues improves the ease and quality of human labeling.
</nextsent>
<nextsent>in this paper we focus on topic segmentation in mandarin chinese broadcast news.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA808">
<title id=" W04-1217.xml">exploiting context for biomedical entity recognition from syntax to the web </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper we describe machine learning system incorporating diverse set of features and various external resources to accomplish this task.
</prevsent>
<prevsent>we describe our system in detail and also discuss some sources of error.
</prevsent>
</prevsection>
<citsent citstr=" W03-0428 ">
our system is maximum entropy markov model, which further develops system earlier used for the conll 2003 shared task (klein et al, 2003) <papid> W03-0428 </papid>and the 2004 bio creative critical assessment of information.</citsent>
<aftsection>
<nextsent>extraction systems, task that involved identifying gene and protein name mentions but not distinguishing between them (dingare et al, 2004).
</nextsent>
<nextsent>unlike the above two tasks, many of the entities in the current task do not have good internal cues for distinguishing the class of entity: various systematic pol ysemies and the widespread use of acronyms mean that internal cues are lacking.
</nextsent>
<nextsent>the challenge wasthus to make better use of contextual features, including local and syntactic features, and external resources in order to succeed at this task.
</nextsent>
<nextsent>2.1 local features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA809">
<title id=" W04-1217.xml">exploiting context for biomedical entity recognition from syntax to the web </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>would become xxxxx, mrna?
</prevsent>
<prevsent>would become xxxx, and cpa1would become xxxd.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
we also incorporated part-of speech tagging, using the tnt tagger(brants, 2000)<papid> A00-1031 </papid>retrained on the genia corpus gold standard part of-speech tagging.</citsent>
<aftsection>
<nextsent>we also used various interaction terms (conjunctions) of these base-level features in various ways.
</nextsent>
<nextsent>the full set of local features is outlined in table 1.
</nextsent>
<nextsent>2.2 external resources.
</nextsent>
<nextsent>we made use of number of external resources, including gazette ers, web-querying, use of the surrounding abstract, and frequency counts from the british national corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA810">
<title id=" W04-1217.xml">exploiting context for biomedical entity recognition from syntax to the web </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>was assigned.
</prevsent>
<prevsent>this value was also assigned to all words whose frequency was higher than 10 (using yet another value for words with higher frequency did not improve the taggers performance).
</prevsent>
</prevsection>
<citsent citstr=" W03-0424 ">
2.2.4 abstracts number of ner systems have made effective use of how the same token was tagged in different parts of the same document (see (curran and clark, 2003) <papid> W03-0424 </papid>and (mikheev et al, 1999)).<papid> E99-1001 </papid></citsent>
<aftsection>
<nextsent>a token which appear sin an un indicative context in one sentence may appear in very obvious context in another sentence in the same abstract.
</nextsent>
<nextsent>to leverage this we tagged each abstract twice, providing for each token feature indicating whether it was tagged as an entity elsewhere in the abstract.
</nextsent>
<nextsent>this information wasonly useful when combined with information on frequency.
</nextsent>
<nextsent>2.3 deeper syntactic features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA811">
<title id=" W04-1217.xml">exploiting context for biomedical entity recognition from syntax to the web </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>was assigned.
</prevsent>
<prevsent>this value was also assigned to all words whose frequency was higher than 10 (using yet another value for words with higher frequency did not improve the taggers performance).
</prevsent>
</prevsection>
<citsent citstr=" E99-1001 ">
2.2.4 abstracts number of ner systems have made effective use of how the same token was tagged in different parts of the same document (see (curran and clark, 2003) <papid> W03-0424 </papid>and (mikheev et al, 1999)).<papid> E99-1001 </papid></citsent>
<aftsection>
<nextsent>a token which appear sin an un indicative context in one sentence may appear in very obvious context in another sentence in the same abstract.
</nextsent>
<nextsent>to leverage this we tagged each abstract twice, providing for each token feature indicating whether it was tagged as an entity elsewhere in the abstract.
</nextsent>
<nextsent>this information wasonly useful when combined with information on frequency.
</nextsent>
<nextsent>2.3 deeper syntactic features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA812">
<title id=" W04-1217.xml">exploiting context for biomedical entity recognition from syntax to the web </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 deeper syntactic features.
</prevsent>
<prevsent>while the local features discussed earlier are all fairly surface level, our system also makes use of deeper syntactic features.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we fully parsed the training and testing data using the stanford parser of (klein and manning, 2003) <papid> P03-1054 </papid>operating on the tntpart-of-speech tagging ? we believe that the un lexicalized nature of this parser makes it particularly suitable statistical parser to use when there is large domain mismatch between the training material (wall street journal text) and the target domain, but have not yet carefully evaluated this.then, for each word in the sentence which is in side noun phrase, the head and governor of the noun phrase are extracted.</citsent>
<aftsection>
<nextsent>these features are not very useful when identifying only two classes (such as gene and other in the bio creative task), but they were quite useful for this task because of the large number of classes which the system needed to distinguish between.
</nextsent>
<nextsent>because the classifier is now 89 choosing between classes where members can look very similar, longer distance information can provide better representation of the context in which the word appears.
</nextsent>
<nextsent>for instance, the word phospho ryl ation occurs in the training corpus 492 times, 482 of which it is was classified as other.
</nextsent>
<nextsent>however, it is the governor of 738 words, of which 443 are protein, 292 are other and only 3 are cell line.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA813">
<title id=" W04-1217.xml">exploiting context for biomedical entity recognition from syntax to the web </title>
<section> results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>otherwise confusions between some named entity and being nothing aremost of the errors, although protein/dna and cell line/cell-type confusions are also noticeable.
</prevsent>
<prevsent>analysis of performance in biomedical named entity recognition tends to be dominated by the perceived poor ness of the results, stemming from the twin beliefs that performance of roughly ninety percent is the state-of-the-art and that performance of 100% (or close to that) is possible and the goalto be aimed for.
</prevsent>
</prevsection>
<citsent citstr=" M98-1021 ">
both of these beliefs are questionable, as the top muc 7 performance of 93.39% entity precision recall f-score fully correct protein 77.40% 68.48% 72.67% dna 66.19% 69.62% 67.86% rna 72.03% 65.89% 68.83% cell line 59.00% 47.12% 52.40% cell type 62.62% 76.97% 69.06% overall 71.62% 68.56% 70.06% left boundary correct protein 82.89% 73.34% 77.82% dna 68.47% 72.01% 70.19% rna 75.42% 68.99% 72.06% cell line 63.80% 50.96% 56.66% cell type 63.93% 78.57% 70.49% overall 75.72% 72.48% 74.07% right boundary correct protein 84.70% 74.96% 79.53% dna 74.43% 78.29% 76.31% rna 78.81% 72.09% 75.30% cell line 70.2% 56.07% 62.34% cell type 71.68% 88.10% 79.05% overall 79.65% 76.24% 77.91% table 2: results on the evaluation data (mikheev et al, 1998) <papid> M98-1021 </papid>in the domain of newswire text used an easier performance metric where incorrect boundaries were given partial credit, while both the biomedical ner shared tasks to date have usedan exact match criterion where one is doubly penalized (both as fp and as fn) for incorrect boundaries.</citsent>
<aftsection>
<nextsent>however, the difference in metric clearly cannot account entirely for the performance discrepancy between newswire ner and biomedical ner.
</nextsent>
<nextsent>biomedical ner appears to be harder task dueto the widespread ambiguity of terms out of context, the complexity of medical language, and the apparent need for expert domain knowledge.
</nextsent>
<nextsent>these are problems that more sophisticated machine learning systems using resources such as ontologies anddeep processing might be able to overcome.
</nextsent>
<nextsent>however, one should also consider the inherent fuzzi ness?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA814">
<title id=" W04-2103.xml">linguistic preprocessing for distributional classification of words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one attractive idea to address this problem is to elicit the meanings of new words automatically from corpus relevant to the application domain.
</prevsent>
<prevsent>to do this, many approaches to lexical acquisition employ the distributional model of word meaning induced from the distribution of the word across various lexical contexts of its occurrence found in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
the approach is now being actively explored for wide range of semantics-related tasks including automatic construction of thesauri (lin, 1998; <papid> P98-2127 </papid>caraballo, 1999), <papid> P99-1016 </papid>their enrichment (alfonseca and manandhar, 2002; pekar and staab,2002), <papid> C02-1090 </papid>acquisition of bilingual lexica from nonaligned (kay and rscheisen, 1993) <papid> J93-1006 </papid>and non parallel corpora (fung and yee, 1998), <papid> P98-1069 </papid>learning of information extraction patterns from un-annotated text (riloff and schmelzenbach, 1998).<papid> W98-1106 </papid></citsent>
<aftsection>
<nextsent>however, because of irregularities in corpus data, corpus statistics cannot guarantee optimal performance, notably for rare lexical items.
</nextsent>
<nextsent>in order to improve robustness, recent research has attempted variety of ways to incorporate external knowledge into the distributional model.
</nextsent>
<nextsent>in this paper we investigate the impact produced by the introduction of different types of linguistic knowledge into the model.
</nextsent>
<nextsent>linguistic knowledge, i.e., the knowledge about linguistically relevant units of text and relations holding between them, is particularly convenient way to enhance the distributional model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA815">
<title id=" W04-2103.xml">linguistic preprocessing for distributional classification of words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one attractive idea to address this problem is to elicit the meanings of new words automatically from corpus relevant to the application domain.
</prevsent>
<prevsent>to do this, many approaches to lexical acquisition employ the distributional model of word meaning induced from the distribution of the word across various lexical contexts of its occurrence found in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" P99-1016 ">
the approach is now being actively explored for wide range of semantics-related tasks including automatic construction of thesauri (lin, 1998; <papid> P98-2127 </papid>caraballo, 1999), <papid> P99-1016 </papid>their enrichment (alfonseca and manandhar, 2002; pekar and staab,2002), <papid> C02-1090 </papid>acquisition of bilingual lexica from nonaligned (kay and rscheisen, 1993) <papid> J93-1006 </papid>and non parallel corpora (fung and yee, 1998), <papid> P98-1069 </papid>learning of information extraction patterns from un-annotated text (riloff and schmelzenbach, 1998).<papid> W98-1106 </papid></citsent>
<aftsection>
<nextsent>however, because of irregularities in corpus data, corpus statistics cannot guarantee optimal performance, notably for rare lexical items.
</nextsent>
<nextsent>in order to improve robustness, recent research has attempted variety of ways to incorporate external knowledge into the distributional model.
</nextsent>
<nextsent>in this paper we investigate the impact produced by the introduction of different types of linguistic knowledge into the model.
</nextsent>
<nextsent>linguistic knowledge, i.e., the knowledge about linguistically relevant units of text and relations holding between them, is particularly convenient way to enhance the distributional model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA816">
<title id=" W04-2103.xml">linguistic preprocessing for distributional classification of words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one attractive idea to address this problem is to elicit the meanings of new words automatically from corpus relevant to the application domain.
</prevsent>
<prevsent>to do this, many approaches to lexical acquisition employ the distributional model of word meaning induced from the distribution of the word across various lexical contexts of its occurrence found in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" C02-1090 ">
the approach is now being actively explored for wide range of semantics-related tasks including automatic construction of thesauri (lin, 1998; <papid> P98-2127 </papid>caraballo, 1999), <papid> P99-1016 </papid>their enrichment (alfonseca and manandhar, 2002; pekar and staab,2002), <papid> C02-1090 </papid>acquisition of bilingual lexica from nonaligned (kay and rscheisen, 1993) <papid> J93-1006 </papid>and non parallel corpora (fung and yee, 1998), <papid> P98-1069 </papid>learning of information extraction patterns from un-annotated text (riloff and schmelzenbach, 1998).<papid> W98-1106 </papid></citsent>
<aftsection>
<nextsent>however, because of irregularities in corpus data, corpus statistics cannot guarantee optimal performance, notably for rare lexical items.
</nextsent>
<nextsent>in order to improve robustness, recent research has attempted variety of ways to incorporate external knowledge into the distributional model.
</nextsent>
<nextsent>in this paper we investigate the impact produced by the introduction of different types of linguistic knowledge into the model.
</nextsent>
<nextsent>linguistic knowledge, i.e., the knowledge about linguistically relevant units of text and relations holding between them, is particularly convenient way to enhance the distributional model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA817">
<title id=" W04-2103.xml">linguistic preprocessing for distributional classification of words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one attractive idea to address this problem is to elicit the meanings of new words automatically from corpus relevant to the application domain.
</prevsent>
<prevsent>to do this, many approaches to lexical acquisition employ the distributional model of word meaning induced from the distribution of the word across various lexical contexts of its occurrence found in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" J93-1006 ">
the approach is now being actively explored for wide range of semantics-related tasks including automatic construction of thesauri (lin, 1998; <papid> P98-2127 </papid>caraballo, 1999), <papid> P99-1016 </papid>their enrichment (alfonseca and manandhar, 2002; pekar and staab,2002), <papid> C02-1090 </papid>acquisition of bilingual lexica from nonaligned (kay and rscheisen, 1993) <papid> J93-1006 </papid>and non parallel corpora (fung and yee, 1998), <papid> P98-1069 </papid>learning of information extraction patterns from un-annotated text (riloff and schmelzenbach, 1998).<papid> W98-1106 </papid></citsent>
<aftsection>
<nextsent>however, because of irregularities in corpus data, corpus statistics cannot guarantee optimal performance, notably for rare lexical items.
</nextsent>
<nextsent>in order to improve robustness, recent research has attempted variety of ways to incorporate external knowledge into the distributional model.
</nextsent>
<nextsent>in this paper we investigate the impact produced by the introduction of different types of linguistic knowledge into the model.
</nextsent>
<nextsent>linguistic knowledge, i.e., the knowledge about linguistically relevant units of text and relations holding between them, is particularly convenient way to enhance the distributional model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA818">
<title id=" W04-2103.xml">linguistic preprocessing for distributional classification of words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one attractive idea to address this problem is to elicit the meanings of new words automatically from corpus relevant to the application domain.
</prevsent>
<prevsent>to do this, many approaches to lexical acquisition employ the distributional model of word meaning induced from the distribution of the word across various lexical contexts of its occurrence found in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
the approach is now being actively explored for wide range of semantics-related tasks including automatic construction of thesauri (lin, 1998; <papid> P98-2127 </papid>caraballo, 1999), <papid> P99-1016 </papid>their enrichment (alfonseca and manandhar, 2002; pekar and staab,2002), <papid> C02-1090 </papid>acquisition of bilingual lexica from nonaligned (kay and rscheisen, 1993) <papid> J93-1006 </papid>and non parallel corpora (fung and yee, 1998), <papid> P98-1069 </papid>learning of information extraction patterns from un-annotated text (riloff and schmelzenbach, 1998).<papid> W98-1106 </papid></citsent>
<aftsection>
<nextsent>however, because of irregularities in corpus data, corpus statistics cannot guarantee optimal performance, notably for rare lexical items.
</nextsent>
<nextsent>in order to improve robustness, recent research has attempted variety of ways to incorporate external knowledge into the distributional model.
</nextsent>
<nextsent>in this paper we investigate the impact produced by the introduction of different types of linguistic knowledge into the model.
</nextsent>
<nextsent>linguistic knowledge, i.e., the knowledge about linguistically relevant units of text and relations holding between them, is particularly convenient way to enhance the distributional model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA819">
<title id=" W04-2103.xml">linguistic preprocessing for distributional classification of words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one attractive idea to address this problem is to elicit the meanings of new words automatically from corpus relevant to the application domain.
</prevsent>
<prevsent>to do this, many approaches to lexical acquisition employ the distributional model of word meaning induced from the distribution of the word across various lexical contexts of its occurrence found in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" W98-1106 ">
the approach is now being actively explored for wide range of semantics-related tasks including automatic construction of thesauri (lin, 1998; <papid> P98-2127 </papid>caraballo, 1999), <papid> P99-1016 </papid>their enrichment (alfonseca and manandhar, 2002; pekar and staab,2002), <papid> C02-1090 </papid>acquisition of bilingual lexica from nonaligned (kay and rscheisen, 1993) <papid> J93-1006 </papid>and non parallel corpora (fung and yee, 1998), <papid> P98-1069 </papid>learning of information extraction patterns from un-annotated text (riloff and schmelzenbach, 1998).<papid> W98-1106 </papid></citsent>
<aftsection>
<nextsent>however, because of irregularities in corpus data, corpus statistics cannot guarantee optimal performance, notably for rare lexical items.
</nextsent>
<nextsent>in order to improve robustness, recent research has attempted variety of ways to incorporate external knowledge into the distributional model.
</nextsent>
<nextsent>in this paper we investigate the impact produced by the introduction of different types of linguistic knowledge into the model.
</nextsent>
<nextsent>linguistic knowledge, i.e., the knowledge about linguistically relevant units of text and relations holding between them, is particularly convenient way to enhance the distributional model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA820">
<title id=" W04-2103.xml">linguistic preprocessing for distributional classification of words </title>
<section> different syntactically motivated methods of.  </section>
<citcontext>
<prevsection>
<prevsent>the questions we seek answers to are: are syntactically related words indeed more revealing about the meaning of the target word than spatially adjacent ones?
</prevsent>
<prevsent>which types of syntactic dependencies should be preferred for delimiting the context of target words occurrence?
</prevsent>
</prevsection>
<citsent citstr=" W93-0113 ">
practice of preprocessing distributional data is to remove rare word co-occurrences, thus aiming to reduce noise from idiosyncratic word uses and linguistic processing errors and at the same time form more compact word representations (e.g., grefenstette, 1993; <papid> W93-0113 </papid>ciaramita, 2002).<papid> W02-0903 </papid></citsent>
<aftsection>
<nextsent>on the other hand, even single occurrence word pairs make up very large portion of the data and many of them are clearly meaningful.
</nextsent>
<nextsent>we compare the quality of the distributional representations with and without context words that occurred only once with the target word.
</nextsent>
<nextsent>3 evaluation.
</nextsent>
<nextsent>3.1 experimental task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA821">
<title id=" W04-2103.xml">linguistic preprocessing for distributional classification of words </title>
<section> different syntactically motivated methods of.  </section>
<citcontext>
<prevsection>
<prevsent>the questions we seek answers to are: are syntactically related words indeed more revealing about the meaning of the target word than spatially adjacent ones?
</prevsent>
<prevsent>which types of syntactic dependencies should be preferred for delimiting the context of target words occurrence?
</prevsent>
</prevsection>
<citsent citstr=" W02-0903 ">
practice of preprocessing distributional data is to remove rare word co-occurrences, thus aiming to reduce noise from idiosyncratic word uses and linguistic processing errors and at the same time form more compact word representations (e.g., grefenstette, 1993; <papid> W93-0113 </papid>ciaramita, 2002).<papid> W02-0903 </papid></citsent>
<aftsection>
<nextsent>on the other hand, even single occurrence word pairs make up very large portion of the data and many of them are clearly meaningful.
</nextsent>
<nextsent>we compare the quality of the distributional representations with and without context words that occurred only once with the target word.
</nextsent>
<nextsent>3 evaluation.
</nextsent>
<nextsent>3.1 experimental task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA824">
<title id=" W04-2103.xml">linguistic preprocessing for distributional classification of words </title>
<section> different syntactically motivated methods of.  </section>
<citcontext>
<prevsection>
<prevsent>thus we were left with 101 classes, each containing 2 or 3 nouns.
</prevsent>
<prevsent>3.3 classification methods.
</prevsent>
</prevsection>
<citsent citstr=" W97-0803 ">
two classification algorithms were used in the study: nave bayes and rocchio, which were previously shown to be quite robust on highly dimensional representations on tasks including word classification (e.g., tokunaga et al, 1997, <papid> W97-0803 </papid>ciaramita, 2002).<papid> W02-0903 </papid></citsent>
<aftsection>
<nextsent>the nave bayes algorithm classifies test instance by finding class that maximizes p(c|nr ).
</nextsent>
<nextsent>assuming independence between features, the goal of the algorithm can be stated as: )|()(maxarg)|(maxarg nv iiii cvpcpncp ? ?
</nextsent>
<nextsent>where p(ci) and p(v|ci) are estimated during the training process from the corpus data.
</nextsent>
<nextsent>the nave bayes classifier was the binary independence model, which estimates p(v|ci) assuming the binomial distribution of features across classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA833">
<title id=" W04-2314.xml">bootstrapping spoken dialog systems with data reuse </title>
<section> at&t; spoken dialog system.  </section>
<citcontext>
<prevsection>
<prevsent>the dm keeps track of the specific discourse context and provides disambiguation and clarification strategies when the slu call-types are low controller atn larification ule-based output rocessor input rocessor context augmented rans ition network now ledge tree ules concepts actions voice xml lu output (xml) figure 2: dialog manager architecture ambiguous or have associated low confidence scores.
</prevsent>
<prevsent>it also extracts other information from the slu response in order to complete the information necessary to provide service.
</prevsent>
</prevsection>
<citsent citstr=" P99-1025 ">
previous work on dialog management (abella andgorin, 1999) <papid> P99-1025 </papid>shows how an object inheritance hierarchy is convenient way of representing the task knowledge and the relationships among the objects.</citsent>
<aftsection>
<nextsent>a formally defined construct algebra describes the set of operations necessary to execute actions (e.g. replies to the user or motivators).
</nextsent>
<nextsent>each dialog motivator consists ofa small processing unit which can be combined accordingly to the object hierarchy to build the application.
</nextsent>
<nextsent>although this approach demonstrated effective results in different domains (gorin et al, 1997; buntschuh et al, 1998), it proposes model which substantially differs from the call ow model broadly used to specify the human-machine interaction.
</nextsent>
<nextsent>building and maintaining large-scale voice-enabled applications requires more direct mapping between specifications and programming model, together with authoring tools that simplifies the time consuming implementation, debugging, and testing phases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA834">
<title id=" W04-2314.xml">bootstrapping spoken dialog systems with data reuse </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>noting the weakness of our current call type library we expect even better performances as we augment more call-types from on-going applications.
</prevsent>
<prevsent>4.3 dialog level evaluation.
</prevsent>
</prevsection>
<citsent citstr=" W01-0902 ">
evaluation of spoken dialog system performances is complex task and depends on the purpose of the desired dialog metric (paek, 2001).<papid> W01-0902 </papid></citsent>
<aftsection>
<nextsent>while asr and slu can be fairly assessed off-line using utterances collected in previous runs of the baseline system, the dialog manager requires interaction with real motivated user who will cooperate with the system to complete the task.
</nextsent>
<nextsent>ideally, the bootstrap system has to be deployed in the field andthe dialogs have to be manually labeled to provide accurate measure of task completion rate.
</nextsent>
<nextsent>usability metrics also require direct feedback from the caller to properly measure the user satisfaction (specifically, task success and dialog cost) (walker et al, 1997).<papid> P97-1035 </papid></nextsent>
<nextsent>however, we are more interested in automatically comparing the bootstrap system performances with reference system, working on the same domain and with identical dialog strategies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA835">
<title id=" W04-2314.xml">bootstrapping spoken dialog systems with data reuse </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>while asr and slu can be fairly assessed off-line using utterances collected in previous runs of the baseline system, the dialog manager requires interaction with real motivated user who will cooperate with the system to complete the task.
</prevsent>
<prevsent>ideally, the bootstrap system has to be deployed in the field andthe dialogs have to be manually labeled to provide accurate measure of task completion rate.
</prevsent>
</prevsection>
<citsent citstr=" P97-1035 ">
usability metrics also require direct feedback from the caller to properly measure the user satisfaction (specifically, task success and dialog cost) (walker et al, 1997).<papid> P97-1035 </papid></citsent>
<aftsection>
<nextsent>however, we are more interested in automatically comparing the bootstrap system performances with reference system, working on the same domain and with identical dialog strategies.
</nextsent>
<nextsent>as first order of approximation, we reused the 3,082 baseline test dialogs (5,537 utterances) collected by the live reference system and applied the same dialog turn sequence to evaluate the bootstrap system.
</nextsent>
<nextsent>according to the reference system call flow, the 97 call-types covered by the reference classifier are clustered into 32 dm categories (dmc).
</nextsent>
<nextsent>a dmc is generalization of more specific intents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA836">
<title id=" W03-2113.xml">some empirical findings on dialogue management and domain ontologies in dialogue systems  implications from an evaluation ofa0 bird quest </title>
<section> combining ie with dialogue interaction.  </section>
<citcontext>
<prevsection>
<prevsent>(hagen, 1999)), or domain related (alexandersson and reithinger, 1995).
</prevsent>
<prevsent>the dialogue manager instead utilise the focal parameter sto control interaction (cf.
</prevsent>
</prevsection>
<citsent citstr=" P98-1103 ">
(jokinen et al, 1998; <papid> P98-1103 </papid>denecke, 1997; jonsson, 1995)).</citsent>
<aftsection>
<nextsent>in malin dialogue history is represented in dialogue objects with parameter termed objects, which identify set of primary referents, and the parameter properties which denote complex predicate ascribed to this set.
</nextsent>
<nextsent>inbirdquest objects are normally birds and properties model information about the birds, such as appearance, number of eggs and feed.the domain knowledge manager receives requests from the dialogue manager and process them further using domain knowledge, for example, disambiguation and mapping of vague concepts to ones more suitable for database access.
</nextsent>
<nextsent>it then retrieve sand coordinates information from available information sources, such as data and knowledge bases.
</nextsent>
<nextsent>ifa request is under-specified or contains inconsistencies from the domain knowledge managers point of view, specification of what clarifying information is needed will be returned to the dialogue manager to help the formulation of clarification question to the user.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA838">
<title id=" W04-1012.xml">automatic evaluation of summaries using document graphs </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>content-based measure computes the similarity at the vocabulary level (donaway, 2000 and mani, 2001).
</prevsent>
<prevsent>the evaluation is done by creating term frequency vectors for both the summary and the model summary, and measuring the cosine similarity (salton, 1988) between these two vectors.
</prevsent>
</prevsection>
<citsent citstr=" W02-0406 ">
of course, the higher the cosine similarity measure, the higher the quality of the summary is. lin and hovy (2002) <papid> W02-0406 </papid>used accumulative n-gram matching scores between model summaries and the summaries to be evaluated as performance indicator in multi-document summaries.</citsent>
<aftsection>
<nextsent>they achieved their best results by giving more credit to longer gram matches with the use of porter stemmer.
</nextsent>
<nextsent>a problem raised in the evaluation approaches that use the cosine measure is that the summaries may use different key terms than those in the original documents or model summaries.
</nextsent>
<nextsent>since term frequency is the base to score summaries, it is possible that high quality summary will get lower score if the terms used in the summary are not the same terms used in most of the documents text.
</nextsent>
<nextsent>donaway et al  (2000) <papid> W00-0408 </papid>discussed using common tool in information retrieval: latent semantic indexing (lsi) (deerwester et al , 1990) to address this problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA839">
<title id=" W04-1012.xml">automatic evaluation of summaries using document graphs </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a problem raised in the evaluation approaches that use the cosine measure is that the summaries may use different key terms than those in the original documents or model summaries.
</prevsent>
<prevsent>since term frequency is the base to score summaries, it is possible that high quality summary will get lower score if the terms used in the summary are not the same terms used in most of the documents text.
</prevsent>
</prevsection>
<citsent citstr=" W00-0408 ">
donaway et al  (2000) <papid> W00-0408 </papid>discussed using common tool in information retrieval: latent semantic indexing (lsi) (deerwester et al , 1990) to address this problem.</citsent>
<aftsection>
<nextsent>the use of lsi reduces the effect of near-synonymy problem on the similarity score.
</nextsent>
<nextsent>this is done by penalizing the summary less in the reduced dimension model when there are infrequent terms synonymous to frequent terms.
</nextsent>
<nextsent>lsi averages the weights of terms that co occur frequently with other mutual terms.
</nextsent>
<nextsent>for example, both bank?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA843">
<title id=" W04-1012.xml">automatic evaluation of summaries using document graphs </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>here, we presented our approach to evaluate the summaries base on document graphs, which is generated automatically.
</prevsent>
<prevsent>it is not very surprising that different measures rank summaries differently.
</prevsent>
</prevsection>
<citsent citstr=" P03-1048 ">
a similar observation has been reported previously (radev, et al  2003).<papid> P03-1048 </papid></citsent>
<aftsection>
<nextsent>our document graph approach on summarization evaluation is new automatic way to evaluate machine-generated summaries, which measures the summaries from the point of view of informativeness.
</nextsent>
<nextsent>it has the potential to evaluate the quality of summaries, including extracts, abstracts, and multi-document summaries, without human involvement.
</nextsent>
<nextsent>to improve the performance of our system and better represent the content of the summaries and source documents, we are working in several areas: 1) improve the results of natural language processing to capture information more accurately; 2) incorporate knowledge base, such as wordnet (fellbaum, 1998), to address the synonymy problem; and, 3) use more heuristics in our relation extraction and generation.
</nextsent>
<nextsent>we are also going to extend our experiments by comparing our approach to content-based measure approaches, such as cosine similarity based on term frequencies and lsi approaches, in both extracts and abstracts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA844">
<title id=" W04-0839.xml">complementarity of lexical and simple syntactic features the syntalex approach to senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they are descendants of the duluth systems that participated in senseval-2.
</prevsent>
<prevsent>the syntalex systems are supervised learners that identify the intended sense of word (target word) given its context.
</prevsent>
</prevsection>
<citsent citstr=" N01-1011 ">
they are derived from the duluth systems that participated in senseval-2, and which are more fully described in (pedersen, 2001<papid> N01-1011 </papid>b).the context of word is rich source of discrete features which lend themselves nicely to decision tree learning.</citsent>
<aftsection>
<nextsent>prior research (e.g., (mcroy, 1992), (<papid> J92-1001 </papid>ng and lee, 1996), (<papid> P96-1006 </papid>stevenson and wilks, 2001), (<papid> J01-3001 </papid>yarowsky and florian, 2002)) suggests thatuse of both syntactic and lexical features will im prove disambiguation accuracies.</nextsent>
<nextsent>there has also been considerable work on word sense disambiguation using various supervised learning algorithms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA846">
<title id=" W04-0839.xml">complementarity of lexical and simple syntactic features the syntalex approach to senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the syntalex systems are supervised learners that identify the intended sense of word (target word) given its context.
</prevsent>
<prevsent>they are derived from the duluth systems that participated in senseval-2, and which are more fully described in (pedersen, 2001<papid> N01-1011 </papid>b).the context of word is rich source of discrete features which lend themselves nicely to decision tree learning.</prevsent>
</prevsection>
<citsent citstr=" J92-1001 ">
prior research (e.g., (mcroy, 1992), (<papid> J92-1001 </papid>ng and lee, 1996), (<papid> P96-1006 </papid>stevenson and wilks, 2001), (<papid> J01-3001 </papid>yarowsky and florian, 2002)) suggests thatuse of both syntactic and lexical features will im prove disambiguation accuracies.</citsent>
<aftsection>
<nextsent>there has also been considerable work on word sense disambiguation using various supervised learning algorithms.
</nextsent>
<nextsent>however, both (pedersen, 2001<papid> N01-1011 </papid>a) and (lee and ng,2002) <papid> W02-1006 </papid>show that different learning algorithms produce similar results and that the use of appropriate features may dramatically improve results.</nextsent>
<nextsent>thus, our focus is not on the learning algorithm but on the features used and their dynamics.our systems use bigrams and part of speech features individually, in simple ensemble and as part of single classifier using both kinds of features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA848">
<title id=" W04-0839.xml">complementarity of lexical and simple syntactic features the syntalex approach to senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the syntalex systems are supervised learners that identify the intended sense of word (target word) given its context.
</prevsent>
<prevsent>they are derived from the duluth systems that participated in senseval-2, and which are more fully described in (pedersen, 2001<papid> N01-1011 </papid>b).the context of word is rich source of discrete features which lend themselves nicely to decision tree learning.</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
prior research (e.g., (mcroy, 1992), (<papid> J92-1001 </papid>ng and lee, 1996), (<papid> P96-1006 </papid>stevenson and wilks, 2001), (<papid> J01-3001 </papid>yarowsky and florian, 2002)) suggests thatuse of both syntactic and lexical features will im prove disambiguation accuracies.</citsent>
<aftsection>
<nextsent>there has also been considerable work on word sense disambiguation using various supervised learning algorithms.
</nextsent>
<nextsent>however, both (pedersen, 2001<papid> N01-1011 </papid>a) and (lee and ng,2002) <papid> W02-1006 </papid>show that different learning algorithms produce similar results and that the use of appropriate features may dramatically improve results.</nextsent>
<nextsent>thus, our focus is not on the learning algorithm but on the features used and their dynamics.our systems use bigrams and part of speech features individually, in simple ensemble and as part of single classifier using both kinds of features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA849">
<title id=" W04-0839.xml">complementarity of lexical and simple syntactic features the syntalex approach to senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the syntalex systems are supervised learners that identify the intended sense of word (target word) given its context.
</prevsent>
<prevsent>they are derived from the duluth systems that participated in senseval-2, and which are more fully described in (pedersen, 2001<papid> N01-1011 </papid>b).the context of word is rich source of discrete features which lend themselves nicely to decision tree learning.</prevsent>
</prevsection>
<citsent citstr=" J01-3001 ">
prior research (e.g., (mcroy, 1992), (<papid> J92-1001 </papid>ng and lee, 1996), (<papid> P96-1006 </papid>stevenson and wilks, 2001), (<papid> J01-3001 </papid>yarowsky and florian, 2002)) suggests thatuse of both syntactic and lexical features will im prove disambiguation accuracies.</citsent>
<aftsection>
<nextsent>there has also been considerable work on word sense disambiguation using various supervised learning algorithms.
</nextsent>
<nextsent>however, both (pedersen, 2001<papid> N01-1011 </papid>a) and (lee and ng,2002) <papid> W02-1006 </papid>show that different learning algorithms produce similar results and that the use of appropriate features may dramatically improve results.</nextsent>
<nextsent>thus, our focus is not on the learning algorithm but on the features used and their dynamics.our systems use bigrams and part of speech features individually, in simple ensemble and as part of single classifier using both kinds of features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA852">
<title id=" W04-0839.xml">complementarity of lexical and simple syntactic features the syntalex approach to senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>prior research (e.g., (mcroy, 1992), (<papid> J92-1001 </papid>ng and lee, 1996), (<papid> P96-1006 </papid>stevenson and wilks, 2001), (<papid> J01-3001 </papid>yarowsky and florian, 2002)) suggests thatuse of both syntactic and lexical features will im prove disambiguation accuracies.</prevsent>
<prevsent>there has also been considerable work on word sense disambiguation using various supervised learning algorithms.</prevsent>
</prevsection>
<citsent citstr=" W02-1006 ">
however, both (pedersen, 2001<papid> N01-1011 </papid>a) and (lee and ng,2002) <papid> W02-1006 </papid>show that different learning algorithms produce similar results and that the use of appropriate features may dramatically improve results.</citsent>
<aftsection>
<nextsent>thus, our focus is not on the learning algorithm but on the features used and their dynamics.our systems use bigrams and part of speech features individually, in simple ensemble and as part of single classifier using both kinds of features.
</nextsent>
<nextsent>we also show that state of the art results (72.1%, coarse grained accuracy) can be achieved using just these simple sets of features.
</nextsent>
<nextsent>simple lexical and syntactic features are used to represent the context.
</nextsent>
<nextsent>the lexical features used areword bigrams.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA853">
<title id=" W04-0839.xml">complementarity of lexical and simple syntactic features the syntalex approach to senseval3 </title>
<section> experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the part of speech of words in sentence have local influence.
</prevsent>
<prevsent>the part of speech of words further away from the target wordare not expected to be as strong indicators of intended sense as the immediate neighbors.
</prevsent>
</prevsection>
<citsent citstr=" W04-2404 ">
however,inclusion of such features has been shown to im prove accuracies (mohammad and pedersen, 2004).<papid> W04-2404 </papid></citsent>
<aftsection>
<nextsent>the nodes in the decision trees are features of the form:  =  tag  ,
</nextsent>
<nextsent>=  tag  ,   =  tag  , p
</nextsent>
<nextsent>=  tag  or  =  tag  . the system achieves fine grained and coarse grained accuracy of 61.8% and 68.4%, respectively.
</nextsent>
<nextsent>4.3 syntalex-3: ensemble of lexical and.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA861">
<title id=" W04-0711.xml">bioar anaphora resolution for relating protein names to proteome database entries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the need for identifying the antecedents of anaphoric expressions in the literature is well recognized.
</prevsent>
<prevsent>most previous approaches assume that anaphoric expressions and their antecedents would appear in the same documents.
</prevsent>
</prevsection>
<citsent citstr=" W03-0105 ">
however, further work is called for when such antecedents need to be associated with actual entities in the world, where the task of establishing the denot ation of named entity with respect to the world or model is known as named entity grounding (leidner et al, 2003).<papid> W03-0105 </papid></citsent>
<aftsection>
<nextsent>in the biomedical domain where the phrases in the literature tend to refer to actual biological entities such as proteins, the phrases should be associated with the actual entries of external resources (hacheyet al, 2004).
</nextsent>
<nextsent>in this paper, we present biomedical anaphora resolution system, bioar, in order to identify the actual referents of those phrases in the biomedical literature and to annotate the phrases, especially those that refer to proteins, with the entries of proteome databases such as swiss-prot by suit able anaphora resolution.anaphora resolution indicates the process of determining the antecedent of an anaphoric expression.
</nextsent>
<nextsent>traditional approaches to anaphora resolution in general domain utilize various constraints or preferences from the morphological, syntactic,and semantic points of view.
</nextsent>
<nextsent>the most prominent proposal for anaphora resolution is centering theory (grosz et al (1995)<papid> J95-2003 </papid>which identifies the antecedents of pronouns with respect to discourse structures, based on the observation that those entities that have already been mentioned and are more central than others tend to be referred back by pronouns subsequently.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA862">
<title id=" W04-0711.xml">bioar anaphora resolution for relating protein names to proteome database entries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we present biomedical anaphora resolution system, bioar, in order to identify the actual referents of those phrases in the biomedical literature and to annotate the phrases, especially those that refer to proteins, with the entries of proteome databases such as swiss-prot by suit able anaphora resolution.anaphora resolution indicates the process of determining the antecedent of an anaphoric expression.
</prevsent>
<prevsent>traditional approaches to anaphora resolution in general domain utilize various constraints or preferences from the morphological, syntactic,and semantic points of view.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
the most prominent proposal for anaphora resolution is centering theory (grosz et al (1995)<papid> J95-2003 </papid>which identifies the antecedents of pronouns with respect to discourse structures, based on the observation that those entities that have already been mentioned and are more central than others tend to be referred back by pronouns subsequently.</citsent>
<aftsection>
<nextsent>byron (2002) <papid> P02-1011 </papid>proposed to identify the antecedents of pronominal reference sin spoken dialogues by utilizing discourse structures with discourse entities and semantic filtering.</nextsent>
<nextsent>castano et al (2002) adopted knowledge poor method, which focuses on resolving pronouns robustly, for example with part-of-speech information, positions of the candidate antecedents, agree(1) the yeast and mammalian branch point sequence binding proteins (bbp and mbbp/ sf1) contain both kh domain and znknuckle rna-binding motifs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA864">
<title id=" W04-0711.xml">bioar anaphora resolution for relating protein names to proteome database entries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>traditional approaches to anaphora resolution in general domain utilize various constraints or preferences from the morphological, syntactic,and semantic points of view.
</prevsent>
<prevsent>the most prominent proposal for anaphora resolution is centering theory (grosz et al (1995)<papid> J95-2003 </papid>which identifies the antecedents of pronouns with respect to discourse structures, based on the observation that those entities that have already been mentioned and are more central than others tend to be referred back by pronouns subsequently.</prevsent>
</prevsection>
<citsent citstr=" P02-1011 ">
byron (2002) <papid> P02-1011 </papid>proposed to identify the antecedents of pronominal reference sin spoken dialogues by utilizing discourse structures with discourse entities and semantic filtering.</citsent>
<aftsection>
<nextsent>castano et al (2002) adopted knowledge poor method, which focuses on resolving pronouns robustly, for example with part-of-speech information, positions of the candidate antecedents, agree(1) the yeast and mammalian branch point sequence binding proteins (bbp and mbbp/ sf1) contain both kh domain and znknuckle rna-binding motifs.
</nextsent>
<nextsent> therefore, we propose that all three of these accessory rna-binding modules bind the phosphate backbone, whereas the kh domain interacts specifically with the bases of the bps.
</nextsent>
<nextsent>(pmid:9701290) table 1: protein domain-referring phrase example ments and lexical features, in addressing problems in the biomedical domain (cf.
</nextsent>
<nextsent>mitkov et al (1998)).in the biomedical literature, an anaphoric expression works as the device of making an abbreviated and indirect reference to some biological object or objects.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA868">
<title id=" W04-2009.xml">recovering coherent interpretations using semantic integration of partial parses </title>
<section> the grammar.  </section>
<citcontext>
<prevsection>
<prevsent>the evokes operator makes the evoked schema locally available under the given alias.
</prevsent>
<prevsent>the throw action schema evokes its frame, the cause-motion frame schema.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the cause-motion-frame schema is the ecg representation of frame nets cause-motion frame(baker et al, 1998; <papid> P98-1013 </papid>the framenet project, 2004).</citsent>
<aftsection>
<nextsent>because throw is lexical unit associated with this frame, the corresponding throw-action schema evokes the cause-motion-frame schema so that their roles can be coindexed.
</nextsent>
<nextsent>in this case, the thrower is bound to the agent while the throwee is bound to the theme.
</nextsent>
<nextsent>the only commitment an evoking schema makes when it evokes some other schema is that the twoschemas are related.
</nextsent>
<nextsent>in this way, the evokes operator provides mechanism for under specifying the relation between the evoking schema and the evoked schema.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA869">
<title id=" W04-2005.xml">evaluating getaruns parser with greval test suite </title>
<section> parsing and robust techniques.  </section>
<citcontext>
<prevsection>
<prevsent>av=adverb 14.
</prevsent>
<prevsent>x=punctuation tab.
</prevsent>
</prevsection>
<citsent citstr=" A94-1008 ">
1: pre terminal symbols used for look ahead as has been reported in the literature (see tapanainen and voutilainen 1994; <papid> A94-1008 </papid>brants and samuelsson 1995), english is language with high level of homography: readings per word are around 2 (i.e. each word can be assigned in average two different tags depending on the tagset).</citsent>
<aftsection>
<nextsent>look ahead in our system copes with most cases of ambiguity: however, we also had to introduce disambiguating tool before the input string could be safely passed to the parser.
</nextsent>
<nextsent>disambiguation is applied to the look ahead stack and is operated by means of finite state automata.
</nextsent>
<nextsent>the reason why we use fsa is simply due to the fact that for some important categories, english has unambiguous tags which can be used as anchoring in the input string, to reduce ambiguity.
</nextsent>
<nextsent>i am now referring to the class of determiners which is used to tell apart words belonging to the ambiguity class [verb,noun], the most frequent in occurrence in english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA870">
<title id=" W04-2310.xml">anaphora resolution in multi person dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, any comprehensive anaphora resolution scheme is expected to entail the use of rich semantic and pragmatic knowledge representation and processing, and is, therefore, complex problem.
</prevsent>
<prevsent>as result of such problems, several heuristics-based approaches have been developed and adopted over the years to achieve partial solutions to the problem.the pioneering work in the area of anaphora resolution was done by hobbs (jerry r. hobbs, 1978) who designed several early syntactic and semantic heuristics for the same.
</prevsent>
</prevsection>
<citsent citstr=" J81-2001 ">
(hirst, 1981) <papid> J81-2001 </papid>discusses several early approaches to anaphora resolution in discourses.</citsent>
<aftsection>
<nextsent>(denber,1998) and (lappin and leass, 1994) <papid> J94-4002 </papid>describe several syntactic heuristics for reflexive, reciprocal and pleonasticanaphora, among others.</nextsent>
<nextsent>often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on limited corpus, such as in (mitkov,1998).<papid> P98-2143 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA871">
<title id=" W04-2310.xml">anaphora resolution in multi person dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as result of such problems, several heuristics-based approaches have been developed and adopted over the years to achieve partial solutions to the problem.the pioneering work in the area of anaphora resolution was done by hobbs (jerry r. hobbs, 1978) who designed several early syntactic and semantic heuristics for the same.
</prevsent>
<prevsent>(hirst, 1981) <papid> J81-2001 </papid>discusses several early approaches to anaphora resolution in discourses.</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
(denber,1998) and (lappin and leass, 1994) <papid> J94-4002 </papid>describe several syntactic heuristics for reflexive, reciprocal and pleonasticanaphora, among others.</citsent>
<aftsection>
<nextsent>often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on limited corpus, such as in (mitkov,1998).<papid> P98-2143 </papid></nextsent>
<nextsent>(ng and cardie, 2002) <papid> W02-1008 </papid>proposes machine learning approach to anaphora resolution but generally statistical learning approaches suffer from the problems of small corp uses and corpus dependent learning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA872">
<title id=" W04-2310.xml">anaphora resolution in multi person dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(hirst, 1981) <papid> J81-2001 </papid>discusses several early approaches to anaphora resolution in discourses.</prevsent>
<prevsent>(denber,1998) and (lappin and leass, 1994) <papid> J94-4002 </papid>describe several syntactic heuristics for reflexive, reciprocal and pleonasticanaphora, among others.</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on limited corpus, such as in (mitkov,1998).<papid> P98-2143 </papid></citsent>
<aftsection>
<nextsent>(ng and cardie, 2002) <papid> W02-1008 </papid>proposes machine learning approach to anaphora resolution but generally statistical learning approaches suffer from the problems of small corp uses and corpus dependent learning.</nextsent>
<nextsent>a more general and comprehensive overview of state-of-the-art in anaphora resolution is given in (mitkov, 1999) and also in (mitkov et al, 2001).<papid> J01-4001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA873">
<title id=" W04-2310.xml">anaphora resolution in multi person dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(denber,1998) and (lappin and leass, 1994) <papid> J94-4002 </papid>describe several syntactic heuristics for reflexive, reciprocal and pleonasticanaphora, among others.</prevsent>
<prevsent>often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on limited corpus, such as in (mitkov,1998).<papid> P98-2143 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1008 ">
(ng and cardie, 2002) <papid> W02-1008 </papid>proposes machine learning approach to anaphora resolution but generally statistical learning approaches suffer from the problems of small corp uses and corpus dependent learning.</citsent>
<aftsection>
<nextsent>a more general and comprehensive overview of state-of-the-art in anaphora resolution is given in (mitkov, 1999) and also in (mitkov et al, 2001).<papid> J01-4001 </papid></nextsent>
<nextsent>few systems have been developed that are specifically aimed at the task of anaphora resolution in discourses.rosana, an algorithm for anaphora resolution that focuses on robustness against information deficiency in the parsed output, is described in (stuckardt, 2001).<papid> J01-4002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA874">
<title id=" W04-2310.xml">anaphora resolution in multi person dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on limited corpus, such as in (mitkov,1998).<papid> P98-2143 </papid></prevsent>
<prevsent>(ng and cardie, 2002) <papid> W02-1008 </papid>proposes machine learning approach to anaphora resolution but generally statistical learning approaches suffer from the problems of small corp uses and corpus dependent learning.</prevsent>
</prevsection>
<citsent citstr=" J01-4001 ">
a more general and comprehensive overview of state-of-the-art in anaphora resolution is given in (mitkov, 1999) and also in (mitkov et al, 2001).<papid> J01-4001 </papid></citsent>
<aftsection>
<nextsent>few systems have been developed that are specifically aimed at the task of anaphora resolution in discourses.rosana, an algorithm for anaphora resolution that focuses on robustness against information deficiency in the parsed output, is described in (stuckardt, 2001).<papid> J01-4002 </papid></nextsent>
<nextsent>mars,the mitkov anaphora resolution system, is another automatic, knowledge-poor anaphora resolution system that has been implemented for several languages including english, bulgarian and japanese.in this paper, we describe the design and implementation of jepthah1, rule-based system for resolving wide variety of anaphora occurring in multi-person dialogues in english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA875">
<title id=" W04-2310.xml">anaphora resolution in multi person dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(ng and cardie, 2002) <papid> W02-1008 </papid>proposes machine learning approach to anaphora resolution but generally statistical learning approaches suffer from the problems of small corp uses and corpus dependent learning.</prevsent>
<prevsent>a more general and comprehensive overview of state-of-the-art in anaphora resolution is given in (mitkov, 1999) and also in (mitkov et al, 2001).<papid> J01-4001 </papid></prevsent>
</prevsection>
<citsent citstr=" J01-4002 ">
few systems have been developed that are specifically aimed at the task of anaphora resolution in discourses.rosana, an algorithm for anaphora resolution that focuses on robustness against information deficiency in the parsed output, is described in (stuckardt, 2001).<papid> J01-4002 </papid></citsent>
<aftsection>
<nextsent>mars,the mitkov anaphora resolution system, is another automatic, knowledge-poor anaphora resolution system that has been implemented for several languages including english, bulgarian and japanese.in this paper, we describe the design and implementation of jepthah1, rule-based system for resolving wide variety of anaphora occurring in multi-person dialogues in english.
</nextsent>
<nextsent>in this system, we integrate several different knowledge-poor constraints and heuristics, and operate them over naive character model of the entire dialogue to perform effective anaphora resolution.
</nextsent>
<nextsent>in addition tousing standard heuristics, we have developed our own semantic and pragmatic heuristics, specific to dialogue situations, that operate on this character model.
</nextsent>
<nextsent>there is weight assigned to each of these heuristics and these weights are fine-tuned using learning mechanism implemented by genetic algorithms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA888">
<title id=" W03-2123.xml">dipper description and formalisation of an information state update dialogue system architecture </title>
<section> end while.  </section>
<citcontext>
<prevsection>
<prevsent>the number of successful spoken dialogue prototypes implemented using dipper is convincingproof-of-concept.
</prevsent>
<prevsent>applications include conversafigure 1: the graphical user interface of the dipper dme, showing the current information state, the last applied update rule, and system messages.
</prevsent>
</prevsection>
<citsent citstr=" C02-1067 ">
tion with domestic appliances, as initiated by theeu project dhomme (bos and oka, 2002), <papid> C02-1067 </papid>explaining route descriptions to mobile robot in miniature town, an epsrc-funded project (lauria et al, 2001), and meaningful conversation with mobile robot in the basement of our department(theobalt et al, 2002).</citsent>
<aftsection>
<nextsent>currently we are working on prototype dialogue system including the greta three-dimensional talking head (pasquarielloand pelachaud, 2001) as part of the eu project mag icster.
</nextsent>
<nextsent>we presented the dipper framework for building spoken dialogue systems, based on the information state theory of dialogue management.
</nextsent>
<nextsent>in comparison to trindikit, we showed that dipper provides transparent and elegant way of declaring update rules independent of any particular programming language, and with the ability to use arbitrary procedural attachment via oaa.
</nextsent>
<nextsent>the system incorporates many off-the-shelf oaa agents,which we described, as well as variety of support agents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA889">
<title id=" W03-2123.xml">dipper description and formalisation of an information state update dialogue system architecture </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>the dipper resources are available at http://www.ltg.ed.ac.uk/dipper.we also presented the formal syntax and semantics of our information-state update language.
</prevsent>
<prevsent>although it is up to the developer to ensure the validity of update rules, this formalisation could form the basis of implementing an interpreter that proves validity of update rules.
</prevsent>
</prevsection>
<citsent citstr=" E03-3005 ">
this is an attractive task for future work, and similar directions have been suggested by (ljunglof, 2000; fernandez, 2003) <papid> E03-3005 </papid>for proving generic properties of dialogue systems.</citsent>
<aftsection>
<nextsent>acknowledgements part of this work was supported by the eu project magic ster (ist 1999-29078).
</nextsent>
<nextsent>we thank nuance for permission to use their software and tools.
</nextsent>
<nextsent>j.
</nextsent>
<nextsent>bos and t. oka.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA890">
<title id=" W04-1216.xml">named entity recognition in biomedical texts using an hmm model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiment shows that the word similarity-based smoothing can improve the performance by using huge unlabeled data.
</prevsent>
<prevsent>while many systems have laboriously hand-coded rules for all kinds of word features, we show that word similarity is potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information.
</prevsent>
</prevsection>
<citsent citstr=" W00-0904 ">
in the message understanding conference (muc), named entity recognition aims to classify proper nouns, dates, time, measures and locations, etc. many researchers adapt their systems from muc to the biomedical domain, such as (fukuda et al 1998), (proux et al 1998), (nobata et al 2000), (<papid> W00-0904 </papid>collier et al 2000), (<papid> C00-1030 </papid>gaizauskas et al 2000), (kazama et al 2002), (<papid> W02-0301 </papid>takeuchi et al 2002), (lee et al 2003) <papid> W03-1305 </papid>and (zhou et al 2004).</citsent>
<aftsection>
<nextsent>as opposed to rule-based systems, machine learning-based systems could train their models on labeled data.
</nextsent>
<nextsent>but due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems.
</nextsent>
<nextsent>this work requires domain specific knowledge.
</nextsent>
<nextsent>how to get the domain knowledge automatically is question that has not been fully investigated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA891">
<title id=" W04-1216.xml">named entity recognition in biomedical texts using an hmm model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiment shows that the word similarity-based smoothing can improve the performance by using huge unlabeled data.
</prevsent>
<prevsent>while many systems have laboriously hand-coded rules for all kinds of word features, we show that word similarity is potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information.
</prevsent>
</prevsection>
<citsent citstr=" C00-1030 ">
in the message understanding conference (muc), named entity recognition aims to classify proper nouns, dates, time, measures and locations, etc. many researchers adapt their systems from muc to the biomedical domain, such as (fukuda et al 1998), (proux et al 1998), (nobata et al 2000), (<papid> W00-0904 </papid>collier et al 2000), (<papid> C00-1030 </papid>gaizauskas et al 2000), (kazama et al 2002), (<papid> W02-0301 </papid>takeuchi et al 2002), (lee et al 2003) <papid> W03-1305 </papid>and (zhou et al 2004).</citsent>
<aftsection>
<nextsent>as opposed to rule-based systems, machine learning-based systems could train their models on labeled data.
</nextsent>
<nextsent>but due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems.
</nextsent>
<nextsent>this work requires domain specific knowledge.
</nextsent>
<nextsent>how to get the domain knowledge automatically is question that has not been fully investigated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA892">
<title id=" W04-1216.xml">named entity recognition in biomedical texts using an hmm model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiment shows that the word similarity-based smoothing can improve the performance by using huge unlabeled data.
</prevsent>
<prevsent>while many systems have laboriously hand-coded rules for all kinds of word features, we show that word similarity is potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information.
</prevsent>
</prevsection>
<citsent citstr=" W02-0301 ">
in the message understanding conference (muc), named entity recognition aims to classify proper nouns, dates, time, measures and locations, etc. many researchers adapt their systems from muc to the biomedical domain, such as (fukuda et al 1998), (proux et al 1998), (nobata et al 2000), (<papid> W00-0904 </papid>collier et al 2000), (<papid> C00-1030 </papid>gaizauskas et al 2000), (kazama et al 2002), (<papid> W02-0301 </papid>takeuchi et al 2002), (lee et al 2003) <papid> W03-1305 </papid>and (zhou et al 2004).</citsent>
<aftsection>
<nextsent>as opposed to rule-based systems, machine learning-based systems could train their models on labeled data.
</nextsent>
<nextsent>but due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems.
</nextsent>
<nextsent>this work requires domain specific knowledge.
</nextsent>
<nextsent>how to get the domain knowledge automatically is question that has not been fully investigated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA893">
<title id=" W04-1216.xml">named entity recognition in biomedical texts using an hmm model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiment shows that the word similarity-based smoothing can improve the performance by using huge unlabeled data.
</prevsent>
<prevsent>while many systems have laboriously hand-coded rules for all kinds of word features, we show that word similarity is potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information.
</prevsent>
</prevsection>
<citsent citstr=" W03-1305 ">
in the message understanding conference (muc), named entity recognition aims to classify proper nouns, dates, time, measures and locations, etc. many researchers adapt their systems from muc to the biomedical domain, such as (fukuda et al 1998), (proux et al 1998), (nobata et al 2000), (<papid> W00-0904 </papid>collier et al 2000), (<papid> C00-1030 </papid>gaizauskas et al 2000), (kazama et al 2002), (<papid> W02-0301 </papid>takeuchi et al 2002), (lee et al 2003) <papid> W03-1305 </papid>and (zhou et al 2004).</citsent>
<aftsection>
<nextsent>as opposed to rule-based systems, machine learning-based systems could train their models on labeled data.
</nextsent>
<nextsent>but due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems.
</nextsent>
<nextsent>this work requires domain specific knowledge.
</nextsent>
<nextsent>how to get the domain knowledge automatically is question that has not been fully investigated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA894">
<title id=" W04-1216.xml">named entity recognition in biomedical texts using an hmm model </title>
<section> distributional word similarity.  </section>
<citcontext>
<prevsection>
<prevsent>words that tend to appear in the same contexts tend to have similar meanings.?
</prevsent>
<prevsent>(harris 1968).
</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
for example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc. many methods have been proposed to compute distributional similarity between words, e.g., (hindle, 1990), (<papid> P90-1034 </papid>pereira et al  1993), (<papid> P93-1024 </papid>grefenstette 1994) and (lin 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>almost all of the methods represent word by feature vector where each feature corresponds to type of context in which the word appeared.
</nextsent>
<nextsent>84 3.1 proximity-based similarity.
</nextsent>
<nextsent>it is natural to use dependency relationship (mel uk, 1987) as features, but parser has to be available.
</nextsent>
<nextsent>since biomedical text is highly irregular, and is very different from text like newspaper, parser developed for the newspaper domain may not perform very well on biomedical text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA895">
<title id=" W04-1216.xml">named entity recognition in biomedical texts using an hmm model </title>
<section> distributional word similarity.  </section>
<citcontext>
<prevsection>
<prevsent>words that tend to appear in the same contexts tend to have similar meanings.?
</prevsent>
<prevsent>(harris 1968).
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
for example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc. many methods have been proposed to compute distributional similarity between words, e.g., (hindle, 1990), (<papid> P90-1034 </papid>pereira et al  1993), (<papid> P93-1024 </papid>grefenstette 1994) and (lin 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>almost all of the methods represent word by feature vector where each feature corresponds to type of context in which the word appeared.
</nextsent>
<nextsent>84 3.1 proximity-based similarity.
</nextsent>
<nextsent>it is natural to use dependency relationship (mel uk, 1987) as features, but parser has to be available.
</nextsent>
<nextsent>since biomedical text is highly irregular, and is very different from text like newspaper, parser developed for the newspaper domain may not perform very well on biomedical text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA896">
<title id=" W04-1216.xml">named entity recognition in biomedical texts using an hmm model </title>
<section> distributional word similarity.  </section>
<citcontext>
<prevsection>
<prevsent>words that tend to appear in the same contexts tend to have similar meanings.?
</prevsent>
<prevsent>(harris 1968).
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
for example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc. many methods have been proposed to compute distributional similarity between words, e.g., (hindle, 1990), (<papid> P90-1034 </papid>pereira et al  1993), (<papid> P93-1024 </papid>grefenstette 1994) and (lin 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>almost all of the methods represent word by feature vector where each feature corresponds to type of context in which the word appeared.
</nextsent>
<nextsent>84 3.1 proximity-based similarity.
</nextsent>
<nextsent>it is natural to use dependency relationship (mel uk, 1987) as features, but parser has to be available.
</nextsent>
<nextsent>since biomedical text is highly irregular, and is very different from text like newspaper, parser developed for the newspaper domain may not perform very well on biomedical text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA897">
<title id=" W04-1216.xml">named entity recognition in biomedical texts using an hmm model </title>
<section> distributional word similarity.  </section>
<citcontext>
<prevsection>
<prevsent>it is natural to use dependency relationship (mel uk, 1987) as features, but parser has to be available.
</prevsent>
<prevsent>since biomedical text is highly irregular, and is very different from text like newspaper, parser developed for the newspaper domain may not perform very well on biomedical text.
</prevsent>
</prevsection>
<citsent citstr=" N03-1032 ">
since most dependency relationships involve words that are situated close to one another, the dependency relationships can often be approximated by cooccurrence relationships within small window (turney 2001); (terra and clarke 2003).<papid> N03-1032 </papid></citsent>
<aftsection>
<nextsent>we define the features of the word to be the first non-stop word on either side of and the intervening stop words (which can be defined as the top-k most frequent words in the corpus).
</nextsent>
<nextsent>for example, for sentence he got job from this company.?
</nextsent>
<nextsent>(considering a, from and this to be stop words.), the features of job provided by this sentence are shown in table 1.
</nextsent>
<nextsent>features frequency (left, got) 0.50 (left, a) 0.50 (right ,from) 0.33 (right, this) 0.33 (right, company) 0.33 ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA898">
<title id=" W04-1216.xml">named entity recognition in biomedical texts using an hmm model </title>
<section> hmm model and smoothing schema.  </section>
<citcontext>
<prevsection>
<prevsent>in order to compute formula (1), we can use the back-off (katz 1987); (bikel et al 1999) approach.
</prevsent>
<prevsent>baseline1 and baseline2 in our system use different back-off schema.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
the following formula is introduced in (lee 1999) <papid> P99-1004 </papid>for word similarity-based smoothing: )4( ),( )|(),( )|( )( )( 1 ? ?</citsent>
<aftsection>
<nextsent>= tt tt wsw tt wsw tttt tt wwsim wtagpwwsim wtagp where s(w) is set of candidate similar words and sim(w,w?) is the similarity between word and w?.
</nextsent>
<nextsent>word similarity-based smoothing approach is used in our system to make advantage of the huge unlabeled corpus.
</nextsent>
<nextsent>in order to plug the word similarity-based smoothing into our hmm model, we made several extensions to formula (4).
</nextsent>
<nextsent>for each word w, we define as the distribution of ws tags, which are annotated in the training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA899">
<title id=" W04-2421.xml">semantic role labeling via generalized inference over classifiers </title>
<section> snow learning architecture.  </section>
<citcontext>
<prevsection>
<prevsent>however, the set of possible role-labelings is restricted by structural and linguistic constraints.
</prevsent>
<prevsent>we encode these constraints using linear functions and use integer programming to ensure the final prediction is consistent (see section 4).
</prevsent>
</prevsection>
<citsent citstr=" C02-1151 ">
the learning algorithm used is variation of the winnow update rule incorporated in snow (roth, 1998; roth andyih, 2002), <papid> C02-1151 </papid>multi-class classifier that is specifically tailored for large scale learning tasks.</citsent>
<aftsection>
<nextsent>snow learns sparse network of linear functions, in which the targets (phrase border predictions or argument type predictions, in this case) are represented as linear functions over common feature space.
</nextsent>
<nextsent>it incorporates several improvements overthe basic winnow update rule.
</nextsent>
<nextsent>in particular, regularization term is added, which has the affect of trying to separate the data with think separator (grove and roth, 2001; hang et al, 2002).
</nextsent>
<nextsent>in the work presented here we use this regularization with fixed parameter.experimental evidence has shown that snow activations are monotonic with the confidence in the prediction therefore, it can provide good source of probability estimation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA900">
<title id=" W04-2421.xml">semantic role labeling via generalized inference over classifiers </title>
<section> no duplicate argument classes for a0-a5,v..  </section>
<citcontext>
<prevsection>
<prevsent>constraints 5, 8, and 9 are valid for only subset of the arguments.these constraints are easy to transform into linear constraints (for example, for each class c, constraint 5 becomes i=1[si = c] ? 1) 2.
</prevsent>
<prevsent>then the optimum solution of the cost function given in equation 2 can be found by integer linear programming3.
</prevsent>
</prevsection>
<citsent citstr=" W04-2401 ">
a similar method was used for entity/relation recognition (roth and yih, 2004).<papid> W04-2401 </papid></citsent>
<aftsection>
<nextsent>almost all previous work on shallow parsing and phrase classification has used constraint 4 to ensure that there are no overlapping phrases.
</nextsent>
<nextsent>by considering additional constraints, we show improved performance (see table 1).
</nextsent>
<nextsent>5 results.
</nextsent>
<nextsent>in this section, we present results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA901">
<title id=" W04-1116.xml">automatic semantic role assignment for a tree structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for natural language understanding, the process of fine-grain semantic role assignment is one of the prominent steps, which provides semantic relations between constituents.
</prevsent>
<prevsent>the sense and sense relations between constituents are core meaning of sentence.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
conventionally there are two kinds of methods for role assignments, one is using only statistical information (gildea and jurafsky, 2002) <papid> J02-3001 </papid>and the other is combining with grammar rules (gildea and hockenmaier, 2003).<papid> W03-1008 </papid></citsent>
<aftsection>
<nextsent>however using only grammar rules to assign semantic roles could lead to low coverage.
</nextsent>
<nextsent>on the other hand, performance of statistical methods relies on significant dependent features.
</nextsent>
<nextsent>data driven is suitable strategy for semantic roles assignments of general texts.
</nextsent>
<nextsent>we use the sinica treebank as information resource because of its various domains texts including politics, society, literature etc and it is chinese treebank with semantic role assigned for each constituent (chen etc., 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA902">
<title id=" W04-1116.xml">automatic semantic role assignment for a tree structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for natural language understanding, the process of fine-grain semantic role assignment is one of the prominent steps, which provides semantic relations between constituents.
</prevsent>
<prevsent>the sense and sense relations between constituents are core meaning of sentence.
</prevsent>
</prevsection>
<citsent citstr=" W03-1008 ">
conventionally there are two kinds of methods for role assignments, one is using only statistical information (gildea and jurafsky, 2002) <papid> J02-3001 </papid>and the other is combining with grammar rules (gildea and hockenmaier, 2003).<papid> W03-1008 </papid></citsent>
<aftsection>
<nextsent>however using only grammar rules to assign semantic roles could lead to low coverage.
</nextsent>
<nextsent>on the other hand, performance of statistical methods relies on significant dependent features.
</nextsent>
<nextsent>data driven is suitable strategy for semantic roles assignments of general texts.
</nextsent>
<nextsent>we use the sinica treebank as information resource because of its various domains texts including politics, society, literature etc and it is chinese treebank with semantic role assigned for each constituent (chen etc., 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA903">
<title id=" W04-1009.xml">hybrid text summarization combining external relevance measures with structural analysis </title>
<section> the palsumm system.  </section>
<citcontext>
<prevsection>
<prevsent>palsumm summarization algorithms operate on data structures generated by fx palo altos linguistic discourse analysis system (lidas).
</prevsent>
<prevsent>lidas is computational discourse parser implementing the unified linguistic discourse model (u-ldm).
</prevsent>
</prevsection>
<citsent citstr=" W04-2322 ">
a description of the lidas system and the u-ldm as well as summary of an article from the new yorker are described in earlier work (polanyi et al  2004<papid> W04-2322 </papid>a, b, thione 2004).</citsent>
<aftsection>
<nextsent>due to space limitations we can only sketch the main points of the system here.
</nextsent>
<nextsent>the lidas parser itself is purely symbolic.
</nextsent>
<nextsent>it parses text discourse segment by discourse segment to construct tree that captures discourse continuity and accessibility relations between the segments.
</nextsent>
<nextsent>the tree identifies what discourse constituents are available for further development and what information given by discourse constituents is available to be referred to.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA909">
<title id=" W04-1009.xml">hybrid text summarization combining external relevance measures with structural analysis </title>
<section> the palsumm system.  </section>
<citcontext>
<prevsection>
<prevsent>2 one reviewer remarked, quite correctly: how sentence is attached to the.
</prevsent>
<prevsent>emerging representation of the structure of the discourse ? is the heart of the algorithm?.
</prevsent>
</prevsection>
<citsent citstr=" W04-0215 ">
this issue is discussed in detail in polanyi et al , 2004<papid> W04-2322 </papid>a,b ; thione et al  2004.<papid> W04-0215 </papid></citsent>
<aftsection>
<nextsent>evidence attachment is subordination syntactic promotion: if the subject of an m-bdu co-refers with the object of the ap.
</nextsent>
<nextsent>sub-cases: if the subject of the m-bdu refers to sub-case of the subject of the ap.
</nextsent>
<nextsent>sub-cases include subsets (all children /some children), sub-types (peo ple/children), etc. verbal properties: if the tense, aspect, modality or genericity of the verbs are different.
</nextsent>
<nextsent>evidence attachment is coordination narrative: if the verbs express events.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA910">
<title id=" W04-1009.xml">hybrid text summarization combining external relevance measures with structural analysis </title>
<section> pruning palsumm trees.  </section>
<citcontext>
<prevsection>
<prevsent>to do so, we begin by seeding every leaf node with statistical seed s(l) using the mead statistical summarizer.
</prevsent>
<prevsent>each segment is scored by mead in the context of the full document, with score that mirrors its judgment of the relevance of that segment for summary.
</prevsent>
</prevsection>
<citsent citstr=" J93-3003 ">
meads metrics include: tf/idf cosine similarity between segment and the document ? optionally skewed towards query entered by the user, the relative position of segment within the document, an adverse score against segments deemed as too similar to the current summary, and our own implementation of feature concerning the presence of certain cue words (hirschberg and litman, 1993).<papid> J93-3003 </papid></citsent>
<aftsection>
<nextsent>after scoring, the values are percolated up through the tree, as before.
</nextsent>
<nextsent>during percolation of both structurally and statistically obtained scores, the new value of node that receives higher score from child node is percolated downwards through all non-subordinated children.
</nextsent>
<nextsent>children of 3 we use the publicly available mead (radev et al  2003).
</nextsent>
<nextsent>adopting sen-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA911">
<title id=" W04-1610.xml">automatic arabic document categorization based on the naive bayes algorithm </title>
<section> preprocessing of document.  </section>
<citcontext>
<prevsection>
<prevsent>then roots are extracted for words in the document.
</prevsent>
<prevsent>in arabic, however, the use of stems will not yield satisfactory categorization.
</prevsent>
</prevsection>
<citsent citstr=" W98-1009 ">
this is mainly due to the fact that arabic is non-concatenative language (al-shalabi and evens, 1998), <papid> W98-1009 </papid>and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root.</citsent>
<aftsection>
<nextsent>the infix form (or stem) needs further to be processed in order to obtain the root.
</nextsent>
<nextsent>this processing is not straightforward: it necessitates expert knowledge in arabic language word morphology (al-shalabi and evens, 1998).<papid> W98-1009 </papid></nextsent>
<nextsent>as an example, two close roots (i.e., roots made of the same letters), but semantically different, can yield the same infix form thus creating ambiguity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA914">
<title id=" W04-2601.xml">ontosem methods for processing semantic ellipsis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1
</prevsent>
<prevsent>syntactic ellipsis ? the non-expression of syntactically obligatory elements ? has been widely studied in computational (not to mention other branches of) linguistics, largely because accounting for missing syntactic elements is crucial aspect of achieving full parse, and parsing is required for many approaches to nlp.1 much less attention has been devoted to what we will call semantic ellipsis, or the non-expression of elements that, while not syntactically obligatory, are required for full semantic interpretation of text.2 naturally, semantic ellipsis is important only in truly knowledge-rich ap 1 examples of nlp efforts to resolve syntactic ellipsis in-.
</prevsent>
</prevsection>
<citsent citstr=" P97-1051 ">
clude hobbs and kehler 1997; <papid> P97-1051 </papid>kehler and shieber 1997; <papid> J97-3005 </papid>and lappin 1992, among many others.</citsent>
<aftsection>
<nextsent>here are described in the literature (e.g., pustejovsky 1995) in theoretical terms, not as heuristic algorithms.
</nextsent>
<nextsent>this is due, in large part, to lack of knowledge sources for semantic reasoning in those contributions.
</nextsent>
<nextsent>pro aches to nlp, which few current non-toy systems pursue.
</nextsent>
<nextsent>all definitions of ellipsis derive from stated or implied notion of completeness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA915">
<title id=" W04-2601.xml">ontosem methods for processing semantic ellipsis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1
</prevsent>
<prevsent>syntactic ellipsis ? the non-expression of syntactically obligatory elements ? has been widely studied in computational (not to mention other branches of) linguistics, largely because accounting for missing syntactic elements is crucial aspect of achieving full parse, and parsing is required for many approaches to nlp.1 much less attention has been devoted to what we will call semantic ellipsis, or the non-expression of elements that, while not syntactically obligatory, are required for full semantic interpretation of text.2 naturally, semantic ellipsis is important only in truly knowledge-rich ap 1 examples of nlp efforts to resolve syntactic ellipsis in-.
</prevsent>
</prevsection>
<citsent citstr=" J97-3005 ">
clude hobbs and kehler 1997; <papid> P97-1051 </papid>kehler and shieber 1997; <papid> J97-3005 </papid>and lappin 1992, among many others.</citsent>
<aftsection>
<nextsent>here are described in the literature (e.g., pustejovsky 1995) in theoretical terms, not as heuristic algorithms.
</nextsent>
<nextsent>this is due, in large part, to lack of knowledge sources for semantic reasoning in those contributions.
</nextsent>
<nextsent>pro aches to nlp, which few current non-toy systems pursue.
</nextsent>
<nextsent>all definitions of ellipsis derive from stated or implied notion of completeness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA916">
<title id=" W04-2601.xml">ontosem methods for processing semantic ellipsis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>its agent is nation-213, which refers to the united states of americain our fact repository.
</prevsent>
<prevsent>the theme of the event is the 7th instantiation of war activity in this text.
</prevsent>
</prevsection>
<citsent citstr=" W03-0904 ">
details of this approach to text processing can be found, e.g., in nirenburg and raskin 2004, beale et al 2003, nirenburg et al 2003<papid> W03-0904 </papid>a,b. the ontology itself, brief ontology tutorial, and an extensive lexicon tutorial can be viewed at http://ilit.umbc.edu.</citsent>
<aftsection>
<nextsent>since ontosem text processing attempts to do it all ? meaning that any phenomenon in any language we are processing is within the purview of our approach ? work on any given problem is carried out in spiral fashion: first at rough grain size, then at finer grain size with each iterative improvement of the system.
</nextsent>
<nextsent>in order both to drive and to organize work, we develop mic rothe ory?
</nextsent>
<nextsent>for each aspect of text processing we treat: e.g., we have micro theories of mood, time, reference resolution, and many more.
</nextsent>
<nextsent>one of the benefits of conceiving work on given topic in terms of micro theory is that conceptual, algorithmic progress can occur separately from its realization in specific application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA917">
<title id=" W04-2601.xml">ontosem methods for processing semantic ellipsis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>blame assignment is determined by processing each sentence multiple times: first without manual intervention, then with the correction of pre processor errors, then with the correction of syntax errors.
</prevsent>
<prevsent>the rationale behind these loops of correction and reevaluation is that low level?
</prevsent>
</prevsection>
<citsent citstr=" W04-0905 ">
mistakes like pre processor errors or lack of coverage of some syntactic construction require different development action than more weighty (from our point of view) errors in semantic interpretation that might result from gaps in knowledge, insufficient reasoning engines, etc. the first experiment with our new evaluation regime produced the following results (reported on in detail in nirenburg et al  2004): <papid> W04-0905 </papid>the analyzer was shown to carry out word sense disambiguation at over 90% and semantic dependency determination at 87% on the basis of correct syntactic analysis and on sentences of an average length of over 25 words with 1.33 unknown words on average per input sentence.</citsent>
<aftsection>
<nextsent>outstanding errors in semantic analysis were due, in most cases, to non-literal use of language (which is one of our topics of ongoing investigation).
</nextsent>
<nextsent>although this first formal experiment was limited to wsd and semantic dependencies, testing of other modules ? like those for reference resolution and ellipsis ? will soon be added to the formal evaluation regime.
</nextsent>
<nextsent>at this stage, evaluation work is slow, but we are well into the development of an evaluation and correction environment that promises to significantly speed up both evaluation and system enhancement.
</nextsent>
<nextsent>closing thoughts the type of work presented in this paper might be termed practical, progressive long-term effort.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA918">
<title id=" W04-0807.xml">the senseval3 english lexical sample task </title>
<section> building sense tagged corpus with.  </section>
<citcontext>
<prevsection>
<prevsent>this task is follow-up to similar tasks organized during the senseval-1 (kilgarriff and palmer, 2000) and senseval-2 (preiss and yarowsky, 2001) evaluations.
</prevsent>
<prevsent>the main changes in this years evaluation consist of new methodology for collecting annotated data (with contributions from web users, as opposed to trained lexicographers), and new sense inventory used for verb entries (wordsmyth).
</prevsent>
</prevsection>
<citsent citstr=" W02-0817 ">
volunteer contributions over the web the sense annotated corpus required for this task was built using the open mind word expert system (chklovski and mihalcea, 2002) <papid> W02-0817 </papid>1.</citsent>
<aftsection>
<nextsent>to overcome the current lack of sense tagged data and the limitations imposed by the creation of such data using trainedlexicographers, the omwe system enables the collection of semantically annotated corpora over the web.
</nextsent>
<nextsent>sense tagged examples are collected using1open mind word expert can be accessed at http://teach computers.org/ web-based application that allows contributors to annotate words with their meanings.
</nextsent>
<nextsent>the tagging exercise proceeds as follows.
</nextsent>
<nextsent>foreach target word the system extracts set of sentences from large textual corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA919">
<title id=" W04-0807.xml">the senseval3 english lexical sample task </title>
<section> building sense tagged corpus with.  </section>
<citcontext>
<prevsection>
<prevsent>this results in an overall agreement of about 67.3% which is reasonable and closely comparable with previous figures.
</prevsent>
<prevsent>note that these figures are collected for the entire omwe dataset build so far, which consists of annotated data for more than 350 words.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
in addition to raw inter-tagger agreement, the kappa statistic, which removes from the agreement rate the amount of agreement that is expected by chance(carletta, 1996), <papid> J96-2004 </papid>was also determined.</citsent>
<aftsection>
<nextsent>we measure two figures: micro-average   , where number of senses, agreement by chance, and   are determined as an average for all words in the set, and macro-average   , where inter-tagger agreement,agreement by chance, and   are individually determined for each of the words in the set, and then combined in an overall average.
</nextsent>
<nextsent>with an average offive senses per word, the average value for the agreement by chance is measured at 0.20, resulting in micro-   statistic of 0.58.
</nextsent>
<nextsent>for macro-   estimations, we assume that word senses follow the distribution observed in the omwe annotated data, and under this assumption, the macro-   is 0.35.
</nextsent>
<nextsent>27 teams participated in this word sense disambiguation task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA920">
<title id=" W03-1903.xml">ontology based linguistic annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular machine-learning based approaches to part-of-speech tagging, word sense disambiguation, information extraction or anaphora resolution - just to name few - relyon corpora annotated with the corresponding phenomenon to be trained and tested on.
</prevsent>
<prevsent>in this paper, we argue that linguistic annotation can to some extent be considered special case of semantic annotation with regard to an ontology.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
part-of-speech (pos) annotation for example can be seen as the task of choosing the appropriate tag for word from an ontology of word categories (compare for example the penn treebank pos tagset as described in (marcus et al , 1993)).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>the annotation of word senses suchas used by machine-learning based word sense disambiguation (wsd) tools corresponds to the task of selecting the correct semantic class or concept for word from an underlying ontology such as wordnet (resnik, 1997).<papid> W97-0209 </papid></nextsent>
<nextsent>annotation by template filling such as used to train machine-learning based information extraction (ie) systems as (ciravegna, 2001) can beseen as the task of finding and marking all the attributes of given onto logical concept in text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA921">
<title id=" W03-1903.xml">ontology based linguistic annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we argue that linguistic annotation can to some extent be considered special case of semantic annotation with regard to an ontology.
</prevsent>
<prevsent>part-of-speech (pos) annotation for example can be seen as the task of choosing the appropriate tag for word from an ontology of word categories (compare for example the penn treebank pos tagset as described in (marcus et al , 1993)).<papid> J93-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" W97-0209 ">
the annotation of word senses suchas used by machine-learning based word sense disambiguation (wsd) tools corresponds to the task of selecting the correct semantic class or concept for word from an underlying ontology such as wordnet (resnik, 1997).<papid> W97-0209 </papid></citsent>
<aftsection>
<nextsent>annotation by template filling such as used to train machine-learning based information extraction (ie) systems as (ciravegna, 2001) can beseen as the task of finding and marking all the attributes of given onto logical concept in text.
</nextsent>
<nextsent>an onto logical concept in this sense can be launching event, management succession event or person together with attributes such as name, affiliation, position, etc. the annotation of anaphoric or bridging relations is actually the task of identifying these mantic relation between two linguistic expressions representing certain onto logical concept.
</nextsent>
<nextsent>most linguistic annotation tools make use of schema specifying what can actually be annotated.
</nextsent>
<nextsent>these schema can in fact be understood as formal representation of the conceptualization underlying the annotation task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA922">
<title id=" W03-1903.xml">ontology based linguistic annotation </title>
<section> the ontology-based linguistic.  </section>
<citcontext>
<prevsection>
<prevsent>e the ontology-based linguistic annotation model offers the kind of flexibility mentioned in (ide, 2002) in the sense that it is general enough to be applied to broad variety of annotation tasks.e the fact that annotation is performed with respect to an onto logical hierarchy offers annotators the possibility to choose the appropriate level of annotation detail such that they are never forced to over specify, i.e. to annotate more specifically than they actually feel comfortable with.
</prevsent>
<prevsent>2http://www.w3.org/tr/owl-ref/ in addition, hierarchical annotation offers further possibilities regarding the computation of the agreement between different annotators as well as the evaluation of system against certain annotation.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
in this sense, instead of measuring only the categorial agreement between annotators with the kappa statistic (carletta, 1996) <papid> J96-2004 </papid>or the performance of system in terms of precision/recall, we could take into account the hierarchical organization of the categories or concepts by making use of measures considering the hierarchical distance?</citsent>
<aftsection>
<nextsent>between two concepts such as proposed by (hahn and schnattinger, 1998) or (madche et al , 2002).
</nextsent>
<nextsent>furthermore, the use of an ontology-based and thus more semantic framework for linguistic annotation has two further, very interesting properties.
</nextsent>
<nextsent>on the one hand, the use of an ontology helps to constrain the possible relations between two concepts, thus reducing the amount of errors in the annotation process.
</nextsent>
<nextsent>for example when annotating coreference relations in text, it seems obvious that an event and an entity will never be co referring and in fact such an erroneous annotation can be avoided if the underlying onto logical model actually forbids such an annotation (see below).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA923">
<title id=" W03-1903.xml">ontology based linguistic annotation </title>
<section> discussion of related work.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, (muller and strube, 2001) propose to specify antecedence with regard to equivalence classes rather than with regard to particular antecedents.however, this has the disadvantage that the information about the actual antecedent an annotator has selected is actually lost.
</prevsent>
<prevsent>thus in our annotation proposal the fact that the coreference relation forms equivalence classes is modeled by an underlying axiom system which can be exploited in the evaluation of system against the annotation standard.
</prevsent>
</prevsection>
<citsent citstr=" J98-2001 ">
the annotation scheme proposed by poesio et al (poesio and vieira, 1998) <papid> J98-2001 </papid>is product of corpus based analysis of definite description (dd) use showing that more than 50% of the dds in their corpus are discourse new or unfamiliar.</citsent>
<aftsection>
<nextsent>thus in poesio et al annotation scheme definite descriptions are also explicitly annotated as discourse new.
</nextsent>
<nextsent>the muc coreference scheme (hirschman and chinchor, 1997) is restricted to the annotation of coreference relations, where coreference is also defined as an equivalence relation.
</nextsent>
<nextsent>though this annotation scheme may seem quite simple, we agree with (hirschman and chinchor, 1997) that it is complex enough when taking into account the agreement of the annotators on task.
</nextsent>
<nextsent>in fact, it has been shown that the agreement of subjects annotating bridging (poesio and vieira, 1998) <papid> J98-2001 </papid>or discourse (cimiano,2003) relations can be too low for tentative conclusion to be drawn (carletta, 1996).<papid> J96-2004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA928">
<title id=" W04-0914.xml">semantic forensics an application of onto logical semantics to information assurance </title>
<section> human deception detection and its.  </section>
<citcontext>
<prevsection>
<prevsent>barwise and perry 1983)); but what she wants to accomplish is for the wife to infer, incorrectly, that this is all the bossis doing downtown.
</prevsent>
<prevsent>it is the latter, linguistically interesting type that was the focus of raskin (1987).
</prevsent>
</prevsection>
<citsent citstr=" W02-1303 ">
application function implementation reference mnemonic string generator generates jingles corresponding to random-generated passwords pilot raskin et al  2001a syntactic nl water marking embeds the watermark in the syntactic tree of sentence pilot/demo atallah et al  2001 semantic nl water marking embeds the watermark in the tmr tree of sentence pilot atallah et al  2002 nl tamper proofing embeds brittle watermark to detect any changes to the text pilot atallah et al  2002 nl sanit ization seamlessly removes and replaces sensitive information proof of concept mohamed 2001 automatic terminology standardizer translates different terminological dialects in ias into tmrs proof of concept raskin et al  2002<papid> W02-1303 </papid>a perimeter protection sanitizes outgoing e-mail online proof of concept raskin et al  2001b nl streaming processor interprets incoming information before it is complete research raskin et al  2002<papid> W02-1303 </papid>b nl steganalysis detects the presence of hidden message research raskin et al  2002<papid> W02-1303 </papid>b semantic mimicking creates meaningful cohesive text to hide secret message research bennett 2003 web crawler for planned attacks crawls the web in search of credible information on computer attacks research raskin et al  2002<papid> W02-1303 </papid>b onto logical support for non-nl data helps to classify incoming strings in computer attack initial research raskin 2004 table 1: nl ias applications new tmr contradicting previously processed one should lead to fact repository flag, and this is where we are moving next.</citsent>
<aftsection>
<nextsent>deception detection the fact repository (frsee nirenburg and raskin 2004: 350-1), so far the least developed static resource in onse, records the remembered event instances.
</nextsent>
<nextsent>in principle, it should record all of them.
</nextsent>
<nextsent>realistically, it records them selectively to suit the needs of an implementation.
</nextsent>
<nextsent>thus, in cresp, small qa system for queries about the 2000 olympics in sydney, the fr remembered all.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA953">
<title id=" W04-0914.xml">semantic forensics an application of onto logical semantics to information assurance </title>
<section> using scripts of complex events for.  </section>
<citcontext>
<prevsection>
<prevsent>what is important to realize is that some nlp systems with sf capabilities are within reach in onse, using the already available resources, possibly with some modifications, primarily if not entirely on the static side, and that is not much different than changing domains for regular?
</prevsent>
<prevsent>nlp system (see raskin et al  2002<papid> W02-1303 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W03-0905 ">
deception detection main tool for dd, in particular tsf, is the expansion of the ontology by acquiring scripts of complex events, already found necessary for other higher-end nlp tasks (see raskin et al  2003).<papid> W03-0905 </papid></citsent>
<aftsection>
<nextsent>there are strong connections among elements of many texts.
</nextsent>
<nextsent>these have to do with the understanding that individual propositions may hold well-defined places in routine,?
</nextsent>
<nextsent>typical?
</nextsent>
<nextsent>sequences of events (often called complex events, scripts or scenarios) that happen in the world, with well-specified set of object-like entities that appear in different roles throughout that sequence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA954">
<title id=" W04-2201.xml">multilinguality in etap3 reuse of lexical resources </title>
<section> unl module in etap.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 architecture.
</prevsent>
<prevsent>since etap-3 is an nlp system based on rich linguistic knowledge, it is natural to maximally reuse its knowledge base and the whole architecture of the system in all applications.
</prevsent>
</prevsection>
<citsent citstr=" C00-1013 ">
our approach to unl (described in boguslavsky et al  2000) <papid> C00-1013 </papid>is to build bridge between unl and one of the internal representations of etap, namely normalized syntactic structure (normss), and in this way link unl with all other levels of text representation, including the conventional orthographic form of the text.</citsent>
<aftsection>
<nextsent>the level of normss is best suited for establishing correspondence with unl, as unl expressions and normss show strong similarities.
</nextsent>
<nextsent>the most important of them are as follows: a) both unl expressions and normsss occupy an intermediate position between the surface and the semantic levels of representation.
</nextsent>
<nextsent>they roughly correspond to the so-called deep-syntactic level.
</nextsent>
<nextsent>at this level the meaning of lexical items is not decomposed into semantic primitives, and the relations between lexical items are language independent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA955">
<title id=" W04-1510.xml">extensible dependency grammar a new methodology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>xdg avoids the problem of placing too high burden on the interfaces, and allows interactions between all and not only adjacent dimensions.
</prevsent>
<prevsent>from mono-stratal approaches, xdg adopts high degree of integration, facilitating concurrent processing and the treatment of ambiguity.
</prevsent>
</prevsection>
<citsent citstr=" P01-1024 ">
at the same time, xdg does not lose its modularity.xdg is descendant of topological dependency grammar (tdg) (duchier and debusmann, 2001), <papid> P01-1024 </papid>pushing the underlying methodology further by generalizing it in two aspects: ? number of dimensions: two in tdg (id and lp), arbitrary many in xdg ? set of principles: fixed in tdg, extensible principle library in xdg the structure of this paper is as follows: in 2, we introduce xdg and the xdg solver used for parsing and generation.</citsent>
<aftsection>
<nextsent>in 3, we introduce number of xdg principles informally, before making use of them in an idealized example grammar in 4.
</nextsent>
<nextsent>in 5we argue why xdg has the potential to be an improvement over multi-stratal and mono-stratal approaches, before we conclude in 6.
</nextsent>
<nextsent>in this section, we introduce xdg formally and mention briefly the constraint-based xdg solver for parsing and generation.
</nextsent>
<nextsent>2.1 formalization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA956">
<title id=" W04-1510.xml">extensible dependency grammar a new methodology </title>
<section> extensible dependency grammar.  </section>
<citcontext>
<prevsection>
<prevsent>this means that the solver can infer information about one dimension from information on an other, if there is either multi-dimensional principle linking the two dimensions, or by the synchronization induced by the lexical entries.
</prevsent>
<prevsent>for instance, not only can syntactic information trigger inferences in syntax, but also vice versa.
</prevsent>
</prevsection>
<citsent citstr=" P02-1003 ">
because xdg allows us to write grammars with completely free word order, xdg solving is an np-complete problem (koller and striegnitz, 2002).<papid> P02-1003 </papid></citsent>
<aftsection>
<nextsent>this means that the worst-case complexity of the solver is exponential.
</nextsent>
<nextsent>the average-case complexity of many smaller-scale grammars that we have experimented with seems polynomial, but it remain sto be seen whether we can scale this up to large scale grammars.
</nextsent>
<nextsent>the well-formedness conditions of xdg analyses are stipulated by principles.
</nextsent>
<nextsent>principles are parametrizable, e.g. by the dimensions on which they are applied, or by lexical features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA957">
<title id=" W04-1510.xml">extensible dependency grammar a new methodology </title>
<section> principles.  </section>
<citcontext>
<prevsection>
<prevsent>climbing(i, j) the graph on dimension must be flatter than the graph on dimension j. the climbing principle is non-lexicalized andtwo-dimensional.
</prevsent>
<prevsent>it is parametrized by the two dimensions and j. for instance, the tree in (11) is flatter than the corresponding tree in (10).
</prevsent>
</prevsection>
<citsent citstr=" P98-1106 ">
this concept was introduced as lifting in (kahane et al, 1998).<papid> P98-1106 </papid></citsent>
<aftsection>
<nextsent>2again, we restrict ourselves to case for simplicity.
</nextsent>
<nextsent>3the node labels are defined in (2) below.
</nextsent>
<nextsent>4the projectivity principle of course only makes sense in combination with the order principle.
</nextsent>
<nextsent>3.9 linking principle.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA958">
<title id=" W04-1710.xml">the syntax students companion an elearning tool designed for computational linguistics students </title>
<section> current work.  </section>
<citcontext>
<prevsection>
<prevsent>plan to use this for the tutorial of the program.
</prevsent>
<prevsent>on the content side, several ideas have been submitted and will be implemented depending on time.
</prevsent>
</prevsection>
<citsent citstr=" W99-0807 ">
notably, it seems particularly interesting to provide actual linguistic data from corpora to students from which grammars can be inferred, as in (borin and dahllof, 1999).<papid> W99-0807 </papid></citsent>
<aftsection>
<nextsent>a new exercise type will ask students to write grammar accounting forgiven small corpus, which could already be morphologically annotated or not.
</nextsent>
<nextsent>lexicons will be separated from grammars, in order to make them reusable when possible.
</nextsent>
<nextsent>feature structures will also be supported, both for the edition of grammars and for the validation of syntactic derivations.
</nextsent>
<nextsent>a number of new features concern the graphical display of trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA959">
<title id=" W04-1710.xml">the syntax students companion an elearning tool designed for computational linguistics students </title>
<section> perspectives and conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>the range of topics covered by the program could be extended.
</prevsent>
<prevsent>the learning of syntax could probably be supported by the integration of parsers, which could be of particular interest to computational linguistics students (see e.g.
</prevsent>
</prevsection>
<citsent citstr=" W02-0103 ">
(meurers et al, 2002; <papid> W02-0103 </papid>van halteren, 2002)).</citsent>
<aftsection>
<nextsent>the integration of generators would also allow students to inspect the productions of their grammars to attempt to identify why they could overgenerate.
</nextsent>
<nextsent>furthermore, we would liketo reuse what already exists for the morphological analysis of words in terms of inflections and derivations, as well as for compositional semantic analysis.the program we have presented puts particular emphasis on its central users, who are students in (computational) linguistics.
</nextsent>
<nextsent>initial evaluation has shown that this kind of support was very welcome by the learners?
</nextsent>
<nextsent>community, and we hope that it will be more widely adopted by the teachers?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA960">
<title id=" W04-0601.xml">techniques for text planning with xslt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the work presented here continues in the tradition of several recent nlg systems that use what could be called generalized template-based processing.
</prevsent>
<prevsent>by generalized, we mean that, rather than manipulating flat strings with no underlying linguistic representation, these systems instead work with structured fragments, which are often processed recursively.
</prevsent>
</prevsection>
<citsent citstr=" W98-1428 ">
other systems that fall into this category include exemplars (white and caldwell, 1998), <papid> W98-1428 </papid>d2s (van deemter et al, 1999), interact  xsl:template match= one-of    !-- recursive pruning step --   xsl:variable name= pruned-alts    xsl:for-each select= *    xsl:variable name= pruned-alt    xsl:apply-templates select= . /   /xsl:variable   xsl:if test= not(xalan:nodeset($pruned-alt)//fail)    xsl:copy-of select= $pruned-alt /   /xsl:if   /xsl:for-each   /xsl:variable   xsl:variable name= num-remaining  select= count(xalan:nodeset($pruned-alts)/*) /   !-- propagation step --   xsl:choose   !-- keep one-of when multiple alts succeed --   xsl:when test= $num-remaining   1    one-of   xsl:copy-of select= $pruned-alts /   /one-of   /xsl:when   !-- filter out one-of when just one choice remains --   xsl:when test= $num-remaining = 1    xsl:copy-of select= $pruned-alts /   /xsl:when   !-- fail if none remain --   xsl:otherwise   fail/   /xsl:otherwise   /xsl:choose   /xsl:template  figure 10: failure-pruning template (wilcock, 2001; wilcock, 2003), <papid> E03-2016 </papid>and smartkom (becker, 2002).</citsent>
<aftsection>
<nextsent>the main novel contribution of the text-planningapproach described here is in its use of an external realizer that processes logical forms with embedded alternatives.
</nextsent>
<nextsent>this eliminates the need to use backtracking ai planner (becker, 2002) or to make arbitrary choices when multiple alternatives are available (van deemter et al, 1999).
</nextsent>
<nextsent>the real izer also uses completely different algorithm thanthe xslt template processingbottom-up, chart based search rather than top-down rule expansion?
</nextsent>
<nextsent>which allows it to deal with those aspects of nlg that are more easily addressed using this kind of processing strategy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA961">
<title id=" W04-0601.xml">techniques for text planning with xslt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the work presented here continues in the tradition of several recent nlg systems that use what could be called generalized template-based processing.
</prevsent>
<prevsent>by generalized, we mean that, rather than manipulating flat strings with no underlying linguistic representation, these systems instead work with structured fragments, which are often processed recursively.
</prevsent>
</prevsection>
<citsent citstr=" E03-2016 ">
other systems that fall into this category include exemplars (white and caldwell, 1998), <papid> W98-1428 </papid>d2s (van deemter et al, 1999), interact  xsl:template match= one-of    !-- recursive pruning step --   xsl:variable name= pruned-alts    xsl:for-each select= *    xsl:variable name= pruned-alt    xsl:apply-templates select= . /   /xsl:variable   xsl:if test= not(xalan:nodeset($pruned-alt)//fail)    xsl:copy-of select= $pruned-alt /   /xsl:if   /xsl:for-each   /xsl:variable   xsl:variable name= num-remaining  select= count(xalan:nodeset($pruned-alts)/*) /   !-- propagation step --   xsl:choose   !-- keep one-of when multiple alts succeed --   xsl:when test= $num-remaining   1    one-of   xsl:copy-of select= $pruned-alts /   /one-of   /xsl:when   !-- filter out one-of when just one choice remains --   xsl:when test= $num-remaining = 1    xsl:copy-of select= $pruned-alts /   /xsl:when   !-- fail if none remain --   xsl:otherwise   fail/   /xsl:otherwise   /xsl:choose   /xsl:template  figure 10: failure-pruning template (wilcock, 2001; wilcock, 2003), <papid> E03-2016 </papid>and smartkom (becker, 2002).</citsent>
<aftsection>
<nextsent>the main novel contribution of the text-planningapproach described here is in its use of an external realizer that processes logical forms with embedded alternatives.
</nextsent>
<nextsent>this eliminates the need to use backtracking ai planner (becker, 2002) or to make arbitrary choices when multiple alternatives are available (van deemter et al, 1999).
</nextsent>
<nextsent>the real izer also uses completely different algorithm thanthe xslt template processingbottom-up, chart based search rather than top-down rule expansion?
</nextsent>
<nextsent>which allows it to deal with those aspects of nlg that are more easily addressed using this kind of processing strategy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA962">
<title id=" W04-0601.xml">techniques for text planning with xslt </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>recall of the information that the system presents to them, where that information is generated at different levels of detail.at the moment, the logical form for each message is created in isolation.
</prevsent>
<prevsent>in future versions of comic, we plan to use ideas from centering theory to help ensure coherence by planning coherent sequence of logical forms for description.
</prevsent>
</prevsection>
<citsent citstr=" W00-1411 ">
we will implement this in way similar to that described by kibble and power (2000).<papid> W00-1411 </papid></citsent>
<aftsection>
<nextsent>we will also incorporate model of the users preferences into later version of comic.
</nextsent>
<nextsent>the model will be used both to rank the options to be presented to the user, and to generate user-tailored descriptions of those options, as in flights (moore et al, 2004).
</nextsent>
<nextsent>finally, we plan to extend the use of data-driven techniques in the realizer, and to make use of these techniques to help in choosing among alternatives in the other comic output modalities.
</nextsent>
<nextsent>acknowledgements thanks to jon oberlander, johanna moore, and the anonymous reviewers for helpful comments and discussion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA963">
<title id=" W04-1202.xml">using argumentation to retrieve articles with similar citations from medline </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>digital libraries aim at structuring their records to facilitate user navigation.
</prevsent>
<prevsent>interfaces visualizing 8 overlapping relationships of the standard library fields such as author and title in document collections are usually the most accessible to the user.
</prevsent>
</prevsection>
<citsent citstr=" C00-1030 ">
beyond these well-known targets, researchers (see de bruijn and martin, 2002, or hirschman and al. 2002, for survey) interested in information extraction and retrieval for biomedical applications have mostly focused on studying specific biological interactions (stapley and benoit, 2000; ndellec et al, 2002; dobrokhotov et al, 2003) and related entities (collier et al, 2000; <papid> C00-1030 </papid>humphreys et al, 2000; yu et al, 2002; yamamoto et al, 2003; <papid> W03-1309 </papid>albert et al, 2003) or using terms in biomedical vocabularies (nazarenko et al, 2001; ruch et al, 2004; srinivasan and hristovski, 2004).</citsent>
<aftsection>
<nextsent>the use of bibliographical and argumentative information (mcknight and srinivasan 2003) has been less well studied by researchers interested in applying natural language processing to biomedical texts.
</nextsent>
<nextsent>2.1 citations.
</nextsent>
<nextsent>originating from bibliometrics, citation analysis (white, 2003) has been used to visualize field via representative slice of its literature.
</nextsent>
<nextsent>co-citation techniques make it possible to cluster documents by scientific paradigm or hypothesis (noyons et al., 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA964">
<title id=" W04-1202.xml">using argumentation to retrieve articles with similar citations from medline </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>digital libraries aim at structuring their records to facilitate user navigation.
</prevsent>
<prevsent>interfaces visualizing 8 overlapping relationships of the standard library fields such as author and title in document collections are usually the most accessible to the user.
</prevsent>
</prevsection>
<citsent citstr=" W03-1309 ">
beyond these well-known targets, researchers (see de bruijn and martin, 2002, or hirschman and al. 2002, for survey) interested in information extraction and retrieval for biomedical applications have mostly focused on studying specific biological interactions (stapley and benoit, 2000; ndellec et al, 2002; dobrokhotov et al, 2003) and related entities (collier et al, 2000; <papid> C00-1030 </papid>humphreys et al, 2000; yu et al, 2002; yamamoto et al, 2003; <papid> W03-1309 </papid>albert et al, 2003) or using terms in biomedical vocabularies (nazarenko et al, 2001; ruch et al, 2004; srinivasan and hristovski, 2004).</citsent>
<aftsection>
<nextsent>the use of bibliographical and argumentative information (mcknight and srinivasan 2003) has been less well studied by researchers interested in applying natural language processing to biomedical texts.
</nextsent>
<nextsent>2.1 citations.
</nextsent>
<nextsent>originating from bibliometrics, citation analysis (white, 2003) has been used to visualize field via representative slice of its literature.
</nextsent>
<nextsent>co-citation techniques make it possible to cluster documents by scientific paradigm or hypothesis (noyons et al., 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA965">
<title id=" W04-1202.xml">using argumentation to retrieve articles with similar citations from medline </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>the term-weighting schema 1 http://lithwww.epfl.ch/~ruch/softs/softs.html.
</prevsent>
<prevsent>10 composed of combinations of term frequency, inverse document frequency and length normalization was varied to determine the most relevant output ranking.
</prevsent>
</prevsection>
<citsent citstr=" C02-1109 ">
table 1 gives the most common term weighting factors (atc.atn, ltc.atn); the first letter triplet applies to the document, the second letter triplet applies to the query (ruch, 2002).<papid> C02-1109 </papid></citsent>
<aftsection>
<nextsent>table 1.
</nextsent>
<nextsent>weighting parameters, following smart conventions.
</nextsent>
<nextsent>3.4 argumentative classification.
</nextsent>
<nextsent>the classifier segmented the abstracts into 4 argumentative moves: purpose, methods, results, and conclusion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA966">
<title id=" W04-1202.xml">using argumentation to retrieve articles with similar citations from medline </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>while the improvement brought by boosting purpose and conclusion features, when measured by map is modest (3-4%), the improvement observed by their optimal combination reached significant improvement: + 5.48%.
</prevsent>
<prevsent>the various combinations of results and methods sections did not lead to any improvement.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
purp meth resu conc ltc.atn + map 0.0958 (62.5%) 0.0251 (16.4%) 0.0270 (17.6%) 0.0858 (56.0%) 0.1532 argumentation has typically been studied in relation to summarization (teufel and moens, 2002).<papid> J02-4002 </papid></citsent>
<aftsection>
<nextsent>its impact on information retrieval is more difficult to establish although recent experiments (ruch et al, 2003) tend to confirm that argumentation is useful for information extraction, as demonstrated by the extraction of gene functions for locus link curation.
</nextsent>
<nextsent>similarly, using the argumentative structure of scientific articles has been proposed to reduce noise (camon et al, 2004) in the assignment of gene ontology codes as investigated in the bio creative challenge.
</nextsent>
<nextsent>in particular, it was seen that the use of material and methods?
</nextsent>
<nextsent>sentences should be avoided.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA967">
<title id=" W03-2103.xml">answering clarification questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present some results from an investigation using the bnc which show some general correlations between clarification request type, likelihood of answering, answer typeand distance between question and answer.
</prevsent>
<prevsent>we then describe new experimental technique for integrating manipulations into text-based synchronous dialogue, and give more specific results concerning the effect of word category and level of grounding on interpretation and response type.
</prevsent>
</prevsection>
<citsent citstr=" P01-1031 ">
requesting clarification is vital part of the communicative process and has received attention from both the formal semantic (ginzburg and cooper,2001; <papid> P01-1031 </papid>ginzburg and cooper, forthcoming) and conversation analytic traditions (schegloff, 1987), but little in the computational dialogue system commu nity.</citsent>
<aftsection>
<nextsent>in theory, perfect dialogue system should be able to interpret and deal with clarification requests(crs) made by the user in order to elicit clarification of some part of system utterance, and be ableto request clarification itelf of some part of user utterance.
</nextsent>
<nextsent>this is no easy task ? crs may take many different forms (often highly elliptical), and can be intended to be interpreted with many different readings which query different aspects of the original utterance.
</nextsent>
<nextsent>as result, dialogue system design has traditionally attempted to avoid the necessity for cr interpretation by making system utterances as clear and precise as possible, and avoid having to generate all but the most simple crs by using robust shallow methods of interpretation or by relying on highlydomain-dependent lexicons and grammars.
</nextsent>
<nextsent>how ever, as systems become more human-like, it seems likely that we will have to cope with user crs at some stage; and the ability to generate system crs can be useful in order to repair misunderstanding, disambiguate other utterances, and learn new words ? see (knight, 1996; dusan and flanagan, 2002; purver, 2002).<papid> W02-0222 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA968">
<title id=" W03-2103.xml">answering clarification questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is no easy task ? crs may take many different forms (often highly elliptical), and can be intended to be interpreted with many different readings which query different aspects of the original utterance.
</prevsent>
<prevsent>as result, dialogue system design has traditionally attempted to avoid the necessity for cr interpretation by making system utterances as clear and precise as possible, and avoid having to generate all but the most simple crs by using robust shallow methods of interpretation or by relying on highlydomain-dependent lexicons and grammars.
</prevsent>
</prevsection>
<citsent citstr=" W02-0222 ">
how ever, as systems become more human-like, it seems likely that we will have to cope with user crs at some stage; and the ability to generate system crs can be useful in order to repair misunderstanding, disambiguate other utterances, and learn new words ? see (knight, 1996; dusan and flanagan, 2002; purver, 2002).<papid> W02-0222 </papid></citsent>
<aftsection>
<nextsent>the investigations presented here had two main aims: to examine (a) how crs are interpreted, and (b) how they are responded to.
</nextsent>
<nextsent>the two are clearly dependent ? the response must depend on the interpretation ? but there are many other influencing factors such as cr form, context and level of grounding.
</nextsent>
<nextsent>answers to (a) should help us with the following questions:?
</nextsent>
<nextsent>what factors can help us disambiguate and correctly interpret user crs??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA969">
<title id=" W03-2103.xml">answering clarification questions </title>
<section> clarification requests.  </section>
<citcontext>
<prevsection>
<prevsent>section 3 describes further corpus work which gives some general results concerning response type.
</prevsent>
<prevsent>section 4 then describesa text-based dialogue experiment examining the detailed effects on interpretation and response of part of-speech (pos) type and level of grounding for one particular cr form, and section 5 then draws some general conclusions.
</prevsent>
</prevsection>
<citsent citstr=" W01-1616 ">
purver et al (2001), <papid> W01-1616 </papid>purver et al (2002) presented taxonomy of cr forms and readings derived from corpus study using the british national corpus (bnc) ? see (burnard, 2000).</citsent>
<aftsection>
<nextsent>this showed that some forms showed high correlation with certain readings, but that some were highly ambiguous.
</nextsent>
<nextsent>purver et al (2002)s taxonomy of cr forms is given in table 1 and cr readings in table 21.
</nextsent>
<nextsent>some crs (the non-reprise class) explicitly identify the clarification required, e.g. what did you say??
</nextsent>
<nextsent>or what do you mean??, and some forms (e.g. literal reprises) appear to favour particular reading almost exclusively, but most are more ambiguous.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA970">
<title id=" W03-2103.xml">answering clarification questions </title>
<section> experimental work.  </section>
<citcontext>
<prevsection>
<prevsent>the server records all turns, together with each key press from both clients, for later analysis.
</prevsent>
<prevsent>this data is also used on the fly to control the speed and capitalisation of artificially generated turns, to be as realistic simulation of the relevant subject as possible.
</prevsent>
</prevsection>
<citsent citstr=" J97-3003 ">
nlp component the nlp component consists of perl text-processing module which communicates with various external nlp modules as required: pos tagging can be performed using ltpos (mikheev, 1997), <papid> J97-3003 </papid>word rarity/frequency tagging using custom tagger based on the bnc (kilgarriff, 1997), and synonym generation using wordnet (fellbaum, 1998).</citsent>
<aftsection>
<nextsent>experimental parameters are specified as set ofrules which are applied to each word in turn.
</nextsent>
<nextsent>preconditions for the application of the rule can be specified in terms of pos, word frequency and the word itself, together with contextual factors such as the time since the last artificial turn was generated, anda probability threshold to prevent behaviour appearing too regular.
</nextsent>
<nextsent>the effect of the rule can be to transform the word in question (by substitution with another word, synonym or randomly generated non-word, or by letter order scrambling) or to trigger an artificially generated turn sequence (currently reprise fragment, followed by an acknowledgement, although other turn types are possible).
</nextsent>
<nextsent>the current experimental setup consists of rules which generate pairs of rfs and subsequent acknowledgements6, for proper nouns, common nouns, verbs, determiners and prepositions, with probabilities determined during pilot experiment to give reasonable numbers of rfs per subject.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA971">
<title id=" W04-0858.xml">word sense disambiguation by web mining for word cooccurrence probabilities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after the raw examples are converted to feature vectors, the weka machine learning software is used to induce model of the training dataand predict the classes of the testing examples (wit ten and frank, 1999).the syntactic features are based on part-of speech tags, assigned by rule-based tagger (brill,1994).
</prevsent>
<prevsent>the main innovation of the nrc wsd system is the method for generating the semantic features, which are derived from word co-occurrence probabilities.
</prevsent>
</prevsection>
<citsent citstr=" N03-1032 ">
we estimated these probabilities using the waterloo multi text system with corpus of about one tera byte of unlabeled text, collected bya web crawler (clarke et al, 1995; clarke and cormack, 2000; terra and clarke, 2003).<papid> N03-1032 </papid></citsent>
<aftsection>
<nextsent>in section 2, we describe the nrc wsd system.
</nextsent>
<nextsent>our experimental results are presented in section 3 and we conclude in section 4.
</nextsent>
<nextsent>this section presents various aspects of the system in roughly the order in which they are executed.
</nextsent>
<nextsent>the following definitions will simplify the description.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA977">
<title id=" W04-1904.xml">towards user adaptive annotation guidelines </title>
<section> guideline users.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 user objectives.
</prevsent>
<prevsent>people are interested in annotation guidelines for different reasons.
</prevsent>
</prevsection>
<citsent citstr=" W99-0302 ">
according to their respective objectives, we define five user profiles.3the annotator annotators assign linguistic features to language data, according to criteria and 2http://www.ling.uni-potsdam.de/sfb/ 3in similar way, carletta and isard (1999) <papid> W99-0302 </papid>define three user types: the coder, the coding consumer, and the coding de veloper.</citsent>
<aftsection>
<nextsent>these classes, however, refer to users of annotation work benches rather than annotation guidelines.
</nextsent>
<nextsent>instructions specified in the annotation guidelines.
</nextsent>
<nextsent>important annotation criteria are consistency and speed.the corpus explorer the group of corpus explorers encompasses all those who aim at exploiting linguistic data in order to find evidence for or against linguistic hypotheses.
</nextsent>
<nextsent>these people need to know (i) how to find instances of specific phenomena theyare interested in, and (ii) how to interpret the annotations of the phenomena in question.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA978">
<title id=" W04-2402.xml">semantic lexicon construction learning from unlabeled data via spectral analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>starting from small number of labeled examples (seeds), e.g.,   car?,plane?, ship?
</prevsent>
<prevsent>labeled as vehicles, we seek to automatically collect more of these.this task is sometimes called the semi-automatic construction of semantic lexicons, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W97-0313 ">
(riloff and shepherd, 1997; <papid> W97-0313 </papid>roark and charniak, 1998; <papid> P98-2182 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>phillips and riloff, 2002).<papid> W02-1017 </papid></citsent>
<aftsection>
<nextsent>a common trend inprior studies is bootstrapping, which is an iterative process to collect new words and regard the words newly collected with high confidence as additional labeled examples for the next iteration.
</nextsent>
<nextsent>the aim of bootstrapping is to compensate for the paucity of labeled examples.
</nextsent>
<nextsent>however, its potential danger is label contamination?
</nextsent>
<nextsent>namely, wrongly (automatically) labeled examples may1our argument in this paper holds for relatively small linguistic objects including words, phrases, collocations, and so forth.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA979">
<title id=" W04-2402.xml">semantic lexicon construction learning from unlabeled data via spectral analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>starting from small number of labeled examples (seeds), e.g.,   car?,plane?, ship?
</prevsent>
<prevsent>labeled as vehicles, we seek to automatically collect more of these.this task is sometimes called the semi-automatic construction of semantic lexicons, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P98-2182 ">
(riloff and shepherd, 1997; <papid> W97-0313 </papid>roark and charniak, 1998; <papid> P98-2182 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>phillips and riloff, 2002).<papid> W02-1017 </papid></citsent>
<aftsection>
<nextsent>a common trend inprior studies is bootstrapping, which is an iterative process to collect new words and regard the words newly collected with high confidence as additional labeled examples for the next iteration.
</nextsent>
<nextsent>the aim of bootstrapping is to compensate for the paucity of labeled examples.
</nextsent>
<nextsent>however, its potential danger is label contamination?
</nextsent>
<nextsent>namely, wrongly (automatically) labeled examples may1our argument in this paper holds for relatively small linguistic objects including words, phrases, collocations, and so forth.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA980">
<title id=" W04-2402.xml">semantic lexicon construction learning from unlabeled data via spectral analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>starting from small number of labeled examples (seeds), e.g.,   car?,plane?, ship?
</prevsent>
<prevsent>labeled as vehicles, we seek to automatically collect more of these.this task is sometimes called the semi-automatic construction of semantic lexicons, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W02-1028 ">
(riloff and shepherd, 1997; <papid> W97-0313 </papid>roark and charniak, 1998; <papid> P98-2182 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>phillips and riloff, 2002).<papid> W02-1017 </papid></citsent>
<aftsection>
<nextsent>a common trend inprior studies is bootstrapping, which is an iterative process to collect new words and regard the words newly collected with high confidence as additional labeled examples for the next iteration.
</nextsent>
<nextsent>the aim of bootstrapping is to compensate for the paucity of labeled examples.
</nextsent>
<nextsent>however, its potential danger is label contamination?
</nextsent>
<nextsent>namely, wrongly (automatically) labeled examples may1our argument in this paper holds for relatively small linguistic objects including words, phrases, collocations, and so forth.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA981">
<title id=" W04-2402.xml">semantic lexicon construction learning from unlabeled data via spectral analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>starting from small number of labeled examples (seeds), e.g.,   car?,plane?, ship?
</prevsent>
<prevsent>labeled as vehicles, we seek to automatically collect more of these.this task is sometimes called the semi-automatic construction of semantic lexicons, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W02-1017 ">
(riloff and shepherd, 1997; <papid> W97-0313 </papid>roark and charniak, 1998; <papid> P98-2182 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>phillips and riloff, 2002).<papid> W02-1017 </papid></citsent>
<aftsection>
<nextsent>a common trend inprior studies is bootstrapping, which is an iterative process to collect new words and regard the words newly collected with high confidence as additional labeled examples for the next iteration.
</nextsent>
<nextsent>the aim of bootstrapping is to compensate for the paucity of labeled examples.
</nextsent>
<nextsent>however, its potential danger is label contamination?
</nextsent>
<nextsent>namely, wrongly (automatically) labeled examples may1our argument in this paper holds for relatively small linguistic objects including words, phrases, collocations, and so forth.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA988">
<title id=" W04-2402.xml">semantic lexicon construction learning from unlabeled data via spectral analysis </title>
<section> related work and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>thelen and riloff (2002)<papid> W02-1028 </papid>s bootstrapping method iteratively performs feature selection and word selection for each class.</prevsent>
<prevsent>it outperformed the best-performing bootstrapping method for this task at the time.</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
we also note that there are number of bootstrapping methods successfully applied to text ? e.g., word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>named entity instance classification (collins and singer, 1999), <papid> W99-0613 </papid>and the extraction ofparts?</citsent>
<aftsection>
<nextsent>word given the whole?
</nextsent>
<nextsent>word (berland and charniak, 1999).<papid> P99-1008 </papid></nextsent>
<nextsent>in section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with thelen and riloff (2002)<papid> W02-1028 </papid>s bootstrapping method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA989">
<title id=" W04-2402.xml">semantic lexicon construction learning from unlabeled data via spectral analysis </title>
<section> related work and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>thelen and riloff (2002)<papid> W02-1028 </papid>s bootstrapping method iteratively performs feature selection and word selection for each class.</prevsent>
<prevsent>it outperformed the best-performing bootstrapping method for this task at the time.</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
we also note that there are number of bootstrapping methods successfully applied to text ? e.g., word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>named entity instance classification (collins and singer, 1999), <papid> W99-0613 </papid>and the extraction ofparts?</citsent>
<aftsection>
<nextsent>word given the whole?
</nextsent>
<nextsent>word (berland and charniak, 1999).<papid> P99-1008 </papid></nextsent>
<nextsent>in section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with thelen and riloff (2002)<papid> W02-1028 </papid>s bootstrapping method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA990">
<title id=" W04-2402.xml">semantic lexicon construction learning from unlabeled data via spectral analysis </title>
<section> related work and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>we also note that there are number of bootstrapping methods successfully applied to text ? e.g., word sense disambiguation (yarowsky, 1995), <papid> P95-1026 </papid>named entity instance classification (collins and singer, 1999), <papid> W99-0613 </papid>and the extraction ofparts?</prevsent>
<prevsent>word given the whole?</prevsent>
</prevsection>
<citsent citstr=" P99-1008 ">
word (berland and charniak, 1999).<papid> P99-1008 </papid></citsent>
<aftsection>
<nextsent>in section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with thelen and riloff (2002)<papid> W02-1028 </papid>s bootstrapping method.</nextsent>
<nextsent>4.3 techniques for learning from unlabeled data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA993">
<title id=" W04-2402.xml">semantic lexicon construction learning from unlabeled data via spectral analysis </title>
<section> related work and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the classifiers are trained with the new augmented labeled data, and the process repeats.
</prevsent>
<prevsent>its theoretical foundations are based on the assumptions that two views are redundantly sufficient and conditionally independent given classes.
</prevsent>
</prevsection>
<citsent citstr=" P02-1046 ">
abney (2002) <papid> P02-1046 </papid>presents an analysis to relax the (fairly strong) conditional independence assumption to weak rule dependence.nigam and ghani (2000) study the effectiveness of co training through experiments on the text categorization task.</citsent>
<aftsection>
<nextsent>pierce and cardie (2001)<papid> W01-0501 </papid>investigate the scala bility of co-training on the base noun phrase bracketing task,which typically requires larger number of labeled examples than text categorization.</nextsent>
<nextsent>they propose to manually correct labels to counteract the degradation of automatically assigned labels on large data sets.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA994">
<title id=" W04-2402.xml">semantic lexicon construction learning from unlabeled data via spectral analysis </title>
<section> related work and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>its theoretical foundations are based on the assumptions that two views are redundantly sufficient and conditionally independent given classes.
</prevsent>
<prevsent>abney (2002) <papid> P02-1046 </papid>presents an analysis to relax the (fairly strong) conditional independence assumption to weak rule dependence.nigam and ghani (2000) study the effectiveness of co training through experiments on the text categorization task.</prevsent>
</prevsection>
<citsent citstr=" W01-0501 ">
pierce and cardie (2001)<papid> W01-0501 </papid>investigate the scala bility of co-training on the base noun phrase bracketing task,which typically requires larger number of labeled examples than text categorization.</citsent>
<aftsection>
<nextsent>they propose to manually correct labels to counteract the degradation of automatically assigned labels on large datasets.
</nextsent>
<nextsent>we use these two empirical studies as references for the implementation of co-training in our experiments.
</nextsent>
<nextsent>co-em (nigam and ghani, 2000) combines the essence of co-training and em in an elegant way.
</nextsent>
<nextsent>classifier is initially trained with the labeled data, and computes probabilistically-weighted labels for all the unlabeled data (as in e-step).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1002">
<title id=" W04-0304.xml">incremental parsing with reference interaction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these results suggest that the architecture holds promise as platform for incremental parsing supporting continuous understanding.
</prevsent>
<prevsent>humans process language incrementally, as has been shown by classic psycho linguistic discussions surrounding the garden-path phenomenon and parsing preferences (altmann and steedman, 1988;konieczny, 1996; phillips, 1996).
</prevsent>
</prevsection>
<citsent citstr=" P93-1008 ">
moreover, variety of eye-tracking experiments (cooper, 1974; tanenhaus and spivey, 1996; allo penna et al,1998; sedivy et al, 1999) suggest that complex semantic and referential constraints are incorporated on an incremental basis in human parsing decisions.computational parsers, however, still tend to operate an entire sentence at time, despite the advent of speech-to-intention dialogue systems such as verb mobil (kasper et al, 1996; noth et al, 2000; pinkal et al, 2000), gemini (dowding et al, 1993; <papid> P93-1008 </papid>dowding et al, 1994; <papid> P94-1016 </papid>moore et al, 1995) and trips(allen et al, 1996; ferguson et al, 1996; ferguson and allen, 1998).</citsent>
<aftsection>
<nextsent>naturalness, robustness, and interactivity are goals of such systems, but control flow is typically the sequential execution of modules, each operating on the output of its predeces sor; only after the entire sentence has been parsed do higher-level modules such as intention recognition and reference resolution get involved.in contrast to this sequential model is the continuous understanding approach, in which all levels of language analysis occur simultaneously, from speech recognition to intention recognition.
</nextsent>
<nextsent>as well as being psycholinguistically motivated, continuous understanding models offer potential computational advantages, including accuracy and efficiency improvements for real-time spoken language understanding and better support for the spontaneities of natural human speech.
</nextsent>
<nextsent>continuous understanding is necessary if the system is to respond before the entire utterance is analyzed, prerequisite for incremental confirmation and clarification.
</nextsent>
<nextsent>the major computational advantage of continuous understanding models is that high-level expectations and feedback should be able to influence the search of lower level processes, thus leading to focused search through hypotheses that are plausible at all levels of processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1003">
<title id=" W04-0304.xml">incremental parsing with reference interaction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these results suggest that the architecture holds promise as platform for incremental parsing supporting continuous understanding.
</prevsent>
<prevsent>humans process language incrementally, as has been shown by classic psycho linguistic discussions surrounding the garden-path phenomenon and parsing preferences (altmann and steedman, 1988;konieczny, 1996; phillips, 1996).
</prevsent>
</prevsection>
<citsent citstr=" P94-1016 ">
moreover, variety of eye-tracking experiments (cooper, 1974; tanenhaus and spivey, 1996; allo penna et al,1998; sedivy et al, 1999) suggest that complex semantic and referential constraints are incorporated on an incremental basis in human parsing decisions.computational parsers, however, still tend to operate an entire sentence at time, despite the advent of speech-to-intention dialogue systems such as verb mobil (kasper et al, 1996; noth et al, 2000; pinkal et al, 2000), gemini (dowding et al, 1993; <papid> P93-1008 </papid>dowding et al, 1994; <papid> P94-1016 </papid>moore et al, 1995) and trips(allen et al, 1996; ferguson et al, 1996; ferguson and allen, 1998).</citsent>
<aftsection>
<nextsent>naturalness, robustness, and interactivity are goals of such systems, but control flow is typically the sequential execution of modules, each operating on the output of its predeces sor; only after the entire sentence has been parsed do higher-level modules such as intention recognition and reference resolution get involved.in contrast to this sequential model is the continuous understanding approach, in which all levels of language analysis occur simultaneously, from speech recognition to intention recognition.
</nextsent>
<nextsent>as well as being psycholinguistically motivated, continuous understanding models offer potential computational advantages, including accuracy and efficiency improvements for real-time spoken language understanding and better support for the spontaneities of natural human speech.
</nextsent>
<nextsent>continuous understanding is necessary if the system is to respond before the entire utterance is analyzed, prerequisite for incremental confirmation and clarification.
</nextsent>
<nextsent>the major computational advantage of continuous understanding models is that high-level expectations and feedback should be able to influence the search of lower level processes, thus leading to focused search through hypotheses that are plausible at all levels of processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1004">
<title id=" W04-0304.xml">incremental parsing with reference interaction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>continuous understanding is necessary if the system is to respond before the entire utterance is analyzed, prerequisite for incremental confirmation and clarification.
</prevsent>
<prevsent>the major computational advantage of continuous understanding models is that high-level expectations and feedback should be able to influence the search of lower level processes, thus leading to focused search through hypotheses that are plausible at all levels of processing.
</prevsent>
</prevsection>
<citsent citstr=" P98-1028 ">
one of the major current applications of parsers that operate incrementally is for language modelling in speech recognition (brill et al, 1998; <papid> P98-1028 </papid>jelinek and chelba, 1999).</citsent>
<aftsection>
<nextsent>this work is important not only for its ability to improve performance on the speech recognition task; it also models the interactions between speech recognition and parsing in continuous understanding system.
</nextsent>
<nextsent>our research attempts to further the quest for continuous understanding by moving one step up the hierarchy, building an incremental parser which is the advisee rather than the advisor.
</nextsent>
<nextsent>we begin by presenting general architecture for incremental interaction between the parser andhigher-level modules, and then discuss specific in stantiation of this general architecture in which reference resolution module provides feedback tothe parser on the suitability of noun phrases.
</nextsent>
<nextsent>experiments with incremental feedback from refer client(parser) mediator advisor(reference) inform inform feedback modify chart figure 1: general architecture for incremental parsing ence resolution module and an np suitability oracle are reported, and the ability of the implementation to incrementally instantiate semantically underspecified pronouns is outlined.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1005">
<title id=" W04-0304.xml">incremental parsing with reference interaction </title>
<section> an incremental parsing architecture.  </section>
<citcontext>
<prevsection>
<prevsent>experiments with incremental feedback from refer client(parser) mediator advisor(reference) inform inform feedback modify chart figure 1: general architecture for incremental parsing ence resolution module and an np suitability oracle are reported, and the ability of the implementation to incrementally instantiate semantically underspecified pronouns is outlined.
</prevsent>
<prevsent>we believe this research provides an important start towards developing end to-end continuous understanding models.
</prevsent>
</prevsection>
<citsent citstr=" H92-1026 ">
many current parsers fall into the class of history based grammars (black et al, 1992).<papid> H92-1026 </papid></citsent>
<aftsection>
<nextsent>the independence assumptions of these models make the parsing problem both stochastic ally and computationally tractable, but represent simplification and may therefore be source of error.
</nextsent>
<nextsent>in continuous understanding framework, higher-level modules may have additional information that suggests loci for improvement, recognizing either invalid independence assumptions or errors in the underlying probability model.
</nextsent>
<nextsent>we have designed general incremental parsing architecture (figure 1) in which the client, dynamic programming parser, performs its calculations, the results of which are incrementally passed on via mediator to an advisor with access tohigher-level information.
</nextsent>
<nextsent>this higher-level advisor sends feedback to the mediator which has access to the clients chart, and which then surreptitiously changes and/or adds to the chart in order to make the judgments conform more closely to thoseof the advisor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1006">
<title id=" W04-0304.xml">incremental parsing with reference interaction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>evaluation using held-out data suggested that value of ? = 0.2 would be optimal.
</prevsent>
<prevsent>this style of feedback is an example of chart subversion, as it is direct modification of constituent probabilities by the mediator, defining new probability distribution.
</prevsent>
</prevsection>
<citsent citstr=" W04-0214 ">
the monroe domain (tetreault et al, 2004; <papid> W04-0214 </papid>stent, 2001) is series of task-oriented dialogues between human participants set in simulated rescue operation domain, where participants collaboratively plan responses to emergency calls.</citsent>
<aftsection>
<nextsent>dialogues were recorded, broken up into utterances, and then transcribed by hand, removing speech repairs from the parser input.
</nextsent>
<nextsent>these transcriptions served as input for all experiments reported below.a probabilistic grammar was trained from supervised data, assigning pcfg probabilities for therule expansions in the cfg backbone of the handcrafted, semantically constrained grammar.
</nextsent>
<nextsent>the parser was run using this grammar, but without any incremental interaction whatsoever, in order to establish baseline accuracy and efficiency numbers.
</nextsent>
<nextsent>the corpus consists of six task-oriented dialogues; four were used for the pcfg training, one was held out to establish appropriate parameter values, and one was selected for testing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1007">
<title id=" W04-0304.xml">incremental parsing with reference interaction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this sort of capability is key for an end-to-end incremental system, because neither the reference module nor the parser is capable, by itself, of determining incrementally that the reference in question must be the bus?.
</prevsent>
<prevsent>if we want an end-to-end system which can interact incrementally with the user, this type of decision-making must be made in an incremental fashion.this ability is also key in the presence of soft constraints or other advisors which prefer one possible moveable referent to another; under incremental parsing, these constraints would have the chance tobe applied during the parsing process, whereas sequential system has no alternatives to the default, underspecified pronoun, and so cannot apply these restrictions to discriminate between referents.our implementation performs the semantic vetting discussed above, but we have done no large scale experiments in this area.
</prevsent>
</prevsection>
<citsent citstr=" P98-2229 ">
there are instances in the literature of incremental parsers that pass forward information to higher-level modules, but none, to our knowledge, are design edas continuous understanding systems, where all levels of language analysis occur (virtually) simultane ously.for example, there are number of robust semantic processing systems (pinkal et al, 2000; rose, 2000; worm, 1998; <papid> P98-2229 </papid>zechner, 1998) <papid> P98-2236 </papid>which contain incremental parsers that pass on partial results immediately to the robust semantic analysis component, which begins to work on combining these sentence fragments.</citsent>
<aftsection>
<nextsent>if the parser cannot find aparse, then the semantic analysis program has already done at least part of its work.
</nextsent>
<nextsent>however, none of the above systems have feedback loop between the semantic analysis component and the incremental parser.
</nextsent>
<nextsent>so, while all of these are in some sense examples of incremental parsing, they are not continuous understanding models.
</nextsent>
<nextsent>schuler (2002) <papid> C02-1024 </papid>describes parser which builds both syntactic tree and denotation-based semantic analysis as it parses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1008">
<title id=" W04-0304.xml">incremental parsing with reference interaction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this sort of capability is key for an end-to-end incremental system, because neither the reference module nor the parser is capable, by itself, of determining incrementally that the reference in question must be the bus?.
</prevsent>
<prevsent>if we want an end-to-end system which can interact incrementally with the user, this type of decision-making must be made in an incremental fashion.this ability is also key in the presence of soft constraints or other advisors which prefer one possible moveable referent to another; under incremental parsing, these constraints would have the chance tobe applied during the parsing process, whereas sequential system has no alternatives to the default, underspecified pronoun, and so cannot apply these restrictions to discriminate between referents.our implementation performs the semantic vetting discussed above, but we have done no large scale experiments in this area.
</prevsent>
</prevsection>
<citsent citstr=" P98-2236 ">
there are instances in the literature of incremental parsers that pass forward information to higher-level modules, but none, to our knowledge, are design edas continuous understanding systems, where all levels of language analysis occur (virtually) simultane ously.for example, there are number of robust semantic processing systems (pinkal et al, 2000; rose, 2000; worm, 1998; <papid> P98-2229 </papid>zechner, 1998) <papid> P98-2236 </papid>which contain incremental parsers that pass on partial results immediately to the robust semantic analysis component, which begins to work on combining these sentence fragments.</citsent>
<aftsection>
<nextsent>if the parser cannot find aparse, then the semantic analysis program has already done at least part of its work.
</nextsent>
<nextsent>however, none of the above systems have feedback loop between the semantic analysis component and the incremental parser.
</nextsent>
<nextsent>so, while all of these are in some sense examples of incremental parsing, they are not continuous understanding models.
</nextsent>
<nextsent>schuler (2002) <papid> C02-1024 </papid>describes parser which builds both syntactic tree and denotation-based semantic analysis as it parses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1009">
<title id=" W04-0304.xml">incremental parsing with reference interaction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, none of the above systems have feedback loop between the semantic analysis component and the incremental parser.
</prevsent>
<prevsent>so, while all of these are in some sense examples of incremental parsing, they are not continuous understanding models.
</prevsent>
</prevsection>
<citsent citstr=" C02-1024 ">
schuler (2002) <papid> C02-1024 </papid>describes parser which builds both syntactic tree and denotation-based semantic analysis as it parses.</citsent>
<aftsection>
<nextsent>the denot ations of constituents in the environment are used to inform parsing decisions, much as we use the static database of place names.
</nextsent>
<nextsent>however, the feedback in our syst emis richer, based on the context provided by the preceding discourse.
</nextsent>
<nextsent>furthermore, as an instantiation of the general architecture presented in section 2, our system is more easily extensible to other forms of feedback.
</nextsent>
<nextsent>there is catch-22 in that the accurate reference information necessary to improve parsing accuracy is dependent on an accurate discourse context which is reliant on accurate parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1011">
<title id=" W03-2802.xml">the peace slds understanding evaluation paradigm of the french media campaign </title>
<section> overview of slds evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>paradise (walker et al, 1998) can be seenas sort of meta-paradigm which correlates objective and subjective measurements.
</prevsent>
<prevsent>its grounding hypothesis states that the goal of any slds is to achieve user-satisfaction, which in turn can be predicted through task success and various interaction costs.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
with the help of the kappa coefficient (carletta, 1996) <papid> J96-2004 </papid>proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation.</citsent>
<aftsection>
<nextsent>paradise has been tested in the communicator project (walker et al, 2001) <papid> P01-1066 </papid>with 9 systems working on the same task over different databases.</nextsent>
<nextsent>with four basic measures (e.g. task completion) the protocol has been able to predict 37% of user satisfaction variation, and 42% with the help of few extra measurements on dialog acts and subtasks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1012">
<title id=" W03-2802.xml">the peace slds understanding evaluation paradigm of the french media campaign </title>
<section> overview of slds evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>its grounding hypothesis states that the goal of any slds is to achieve user-satisfaction, which in turn can be predicted through task success and various interaction costs.
</prevsent>
<prevsent>with the help of the kappa coefficient (carletta, 1996) <papid> J96-2004 </papid>proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation.</prevsent>
</prevsection>
<citsent citstr=" P01-1066 ">
paradise has been tested in the communicator project (walker et al, 2001) <papid> P01-1066 </papid>with 9 systems working on the same task over different databases.</citsent>
<aftsection>
<nextsent>with four basic measures (e.g. task completion) the protocol has been able to predict 37% of user satisfaction variation, and 42% with the help of few extra measurements on dialog acts and subtasks.
</nextsent>
<nextsent>one critic, one can make about paradise concern its cost (real user tests are costly) and the use of subjective assessment.
</nextsent>
<nextsent>the adaption of the dqr text understanding evaluation methodology (sabatier et al, 2000) to speech resulted in generic and qualitative procedure.
</nextsent>
<nextsent>each element of its test set holds three parts, the declaration to define the context, question which bears on point present in the context and the response.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1013">
<title id=" W03-2802.xml">the peace slds understanding evaluation paradigm of the french media campaign </title>
<section> the peace paradigm.  </section>
<citcontext>
<prevsection>
<prevsent>this paradigm which is as basement for the media project willbe refined by all the partners and use for an evaluation campaign between seven systems of industrial and academic sites.
</prevsent>
<prevsent>3.1 description.
</prevsent>
</prevsection>
<citsent citstr=" H94-1022 ">
the peace paradigm relies on the idea that for database querying tasks, it is possible to define common semantic representation, onto which allthe systems are able to convert their own representation (moore, 1994).<papid> H94-1022 </papid></citsent>
<aftsection>
<nextsent>the paradigm based ondata extracted from real corpus, includes both literal and contextual understanding test sets.
</nextsent>
<nextsent>more precisely, it provides:  the definition of semantic representation (see 3.1.1),  the definition of model for dia logic contexts (see 3.1.2), the definition and typology of linguistic phenomena and dia logic functions used to selectively diagnoze the system language capabilities (anaphora resolution, constraints relaxation, etc.)
</nextsent>
<nextsent>(see 3.1.3),  data structuring method.
</nextsent>
<nextsent>the format of the annotated data will be adapted to language resource standard annotations implemented (see 3.1.4), and evaluation metrics with the corresponding evaluation tool (see 3.1.5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1014">
<title id=" W04-2416.xml">semantic role labeling by tagging syntactic chunks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this structure word (most frequently verb) is specified as predicate, and number of word groups are considered as arguments accompanying the word (or predicate).
</prevsent>
<prevsent>in this paper, we select support vector machines (svms) (vapnik, 1995; burges, 1998) to implement the semantic role classifiers, due to their ability to handle an extremely large number of (overlapping) features with quite strong generalization properties.
</prevsent>
</prevsection>
<citsent citstr=" N03-2009 ">
support vector machines for semantic role chunking were first used this research was partially supported by the arda aquaint program via contract ocg4423b and by the nsf via grant iis-9978025in (hacioglu and ward, 2003) <papid> N03-2009 </papid>as word-by-word (w-by w) classifiers.</citsent>
<aftsection>
<nextsent>the system was then applied to theconstituent-by-constituent (c-by-c) classification in (hacioglu et al, 2003).
</nextsent>
<nextsent>in (pradhan et al, 2003; pradhan et al, 2004), several extensions to the basic system have been proposed, extensively studied and systematically compared to other systems.
</nextsent>
<nextsent>in this paper, we implement system that classifies syntactic chunks (i.e. base phrases) instead of words or the constituents derived from syntactic trees.
</nextsent>
<nextsent>this system is referred to as the phraseby-phrase (p-by-p) semantic role classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1015">
<title id=" W04-2605.xml">using selectional profile distance to detect verb alternations </title>
<section> kiva ate his lunch./kiva ate./*his lunch ate..  </section>
<citcontext>
<prevsection>
<prevsent>the verb eat in (2), like melt, allows both transitive and in transitive forms, but these are related by the unspecified object alternation, as opposed to causativization.
</prevsent>
<prevsent>based largely on the influence of levin (1993), it has become widely accepted that alternations such as these can serve as basis for the formation of semantic classes of verbs.
</prevsent>
</prevsection>
<citsent citstr=" C00-2148 ">
correspondingly, the relation between alternation patterns and meaning is key focus in the computational study of the lexical semantics of verbs (e.g., allen, 1997; dang et al, 2000; <papid> C00-2148 </papid>dorr and jones, 2000; merlo and stevenson, 2001; <papid> J01-3003 </papid>schulte im walde and brew,2002; tsang et al, 2002).<papid> C02-1146 </papid></citsent>
<aftsection>
<nextsent>furthermore, we note that recent work indicates that verb alternations may also play role in automatic processing of language for applied tasks, such as question-answering (katz et al, 2001), <papid> W01-1009 </papid>detection of text relations (teufel, 1999), and determination of verb-particle constructions (bannard, 2002).the theoretical and practical implications of alternations mean that it is important to identify verbs which undergo an alternation, and to discover the range of alternations.</nextsent>
<nextsent>manual annotation of verbs is labour intensive, and new verbs (or new uses of known verbs) may be encountered in any given domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1016">
<title id=" W04-2605.xml">using selectional profile distance to detect verb alternations </title>
<section> kiva ate his lunch./kiva ate./*his lunch ate..  </section>
<citcontext>
<prevsection>
<prevsent>the verb eat in (2), like melt, allows both transitive and in transitive forms, but these are related by the unspecified object alternation, as opposed to causativization.
</prevsent>
<prevsent>based largely on the influence of levin (1993), it has become widely accepted that alternations such as these can serve as basis for the formation of semantic classes of verbs.
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
correspondingly, the relation between alternation patterns and meaning is key focus in the computational study of the lexical semantics of verbs (e.g., allen, 1997; dang et al, 2000; <papid> C00-2148 </papid>dorr and jones, 2000; merlo and stevenson, 2001; <papid> J01-3003 </papid>schulte im walde and brew,2002; tsang et al, 2002).<papid> C02-1146 </papid></citsent>
<aftsection>
<nextsent>furthermore, we note that recent work indicates that verb alternations may also play role in automatic processing of language for applied tasks, such as question-answering (katz et al, 2001), <papid> W01-1009 </papid>detection of text relations (teufel, 1999), and determination of verb-particle constructions (bannard, 2002).the theoretical and practical implications of alternations mean that it is important to identify verbs which undergo an alternation, and to discover the range of alternations.</nextsent>
<nextsent>manual annotation of verbs is labour intensive, and new verbs (or new uses of known verbs) may be encountered in any given domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1018">
<title id=" W04-2605.xml">using selectional profile distance to detect verb alternations </title>
<section> kiva ate his lunch./kiva ate./*his lunch ate..  </section>
<citcontext>
<prevsection>
<prevsent>the verb eat in (2), like melt, allows both transitive and in transitive forms, but these are related by the unspecified object alternation, as opposed to causativization.
</prevsent>
<prevsent>based largely on the influence of levin (1993), it has become widely accepted that alternations such as these can serve as basis for the formation of semantic classes of verbs.
</prevsent>
</prevsection>
<citsent citstr=" C02-1146 ">
correspondingly, the relation between alternation patterns and meaning is key focus in the computational study of the lexical semantics of verbs (e.g., allen, 1997; dang et al, 2000; <papid> C00-2148 </papid>dorr and jones, 2000; merlo and stevenson, 2001; <papid> J01-3003 </papid>schulte im walde and brew,2002; tsang et al, 2002).<papid> C02-1146 </papid></citsent>
<aftsection>
<nextsent>furthermore, we note that recent work indicates that verb alternations may also play role in automatic processing of language for applied tasks, such as question-answering (katz et al, 2001), <papid> W01-1009 </papid>detection of text relations (teufel, 1999), and determination of verb-particle constructions (bannard, 2002).the theoretical and practical implications of alternations mean that it is important to identify verbs which undergo an alternation, and to discover the range of alternations.</nextsent>
<nextsent>manual annotation of verbs is labour intensive, and new verbs (or new uses of known verbs) may be encountered in any given domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1019">
<title id=" W04-2605.xml">using selectional profile distance to detect verb alternations </title>
<section> kiva ate his lunch./kiva ate./*his lunch ate..  </section>
<citcontext>
<prevsection>
<prevsent>based largely on the influence of levin (1993), it has become widely accepted that alternations such as these can serve as basis for the formation of semantic classes of verbs.
</prevsent>
<prevsent>correspondingly, the relation between alternation patterns and meaning is key focus in the computational study of the lexical semantics of verbs (e.g., allen, 1997; dang et al, 2000; <papid> C00-2148 </papid>dorr and jones, 2000; merlo and stevenson, 2001; <papid> J01-3003 </papid>schulte im walde and brew,2002; tsang et al, 2002).<papid> C02-1146 </papid></prevsent>
</prevsection>
<citsent citstr=" W01-1009 ">
furthermore, we note that recent work indicates that verb alternations may also play role in automatic processing of language for applied tasks, such as question-answering (katz et al, 2001), <papid> W01-1009 </papid>detection of text relations (teufel, 1999), and determination of verb-particle constructions (bannard, 2002).the theoretical and practical implications of alternations mean that it is important to identify verbs which undergo an alternation, and to discover the range of alternations.</citsent>
<aftsection>
<nextsent>manual annotation of verbs is labour intensive, and new verbs (or new uses of known verbs) may be encountered in any given domain.
</nextsent>
<nextsent>in response, some researchers have begun to investigate ways to detect alternations automatically in corpus.
</nextsent>
<nextsent>some of this work has focused on subcategorization patterns as the clear syntactic cueto an alternation (lapata, 1999; <papid> P99-1051 </papid>lapata and brew, 1999; <papid> W99-0632 </papid>schulte im walde and brew, 2002).</nextsent>
<nextsent>other work has observed, however, that detecting an alternation involves more than observing the use of particular subcategorization sit must also be determined whether the semantic arguments are mapped to the appropriate po sitions.1 to address this issue, it has been suggested that, if verb participates in an alternation, then there should be similarity in the kinds of nouns that show up in the syn 1for example, melt (as in (1) above) undergoes causative alternation because the theme argument that surfaces as subject of the in transitive surfaces as object of the transitive, with the addition of causal agent as the subject of the latter.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1020">
<title id=" W04-2605.xml">using selectional profile distance to detect verb alternations </title>
<section> kiva ate his lunch./kiva ate./*his lunch ate..  </section>
<citcontext>
<prevsection>
<prevsent>manual annotation of verbs is labour intensive, and new verbs (or new uses of known verbs) may be encountered in any given domain.
</prevsent>
<prevsent>in response, some researchers have begun to investigate ways to detect alternations automatically in corpus.
</prevsent>
</prevsection>
<citsent citstr=" P99-1051 ">
some of this work has focused on subcategorization patterns as the clear syntactic cueto an alternation (lapata, 1999; <papid> P99-1051 </papid>lapata and brew, 1999; <papid> W99-0632 </papid>schulte im walde and brew, 2002).</citsent>
<aftsection>
<nextsent>other work has observed, however, that detecting an alternation involves more than observing the use of particular subcategorization sit must also be determined whether the semantic arguments are mapped to the appropriate po sitions.1 to address this issue, it has been suggested that, if verb participates in an alternation, then there should be similarity in the kinds of nouns that show up in the syn 1for example, melt (as in (1) above) undergoes causative alternation because the theme argument that surfaces as subject of the in transitive surfaces as object of the transitive, with the addition of causal agent as the subject of the latter.
</nextsent>
<nextsent>it is not the case that any optionally in transitive verb undergoes this alternation, as shown by eat in (2).tactic positions (or slots) that alternate such as snow occurring as in transitive subject and transitive object in the causative alternation in (1) (merlo and stevenson, 2001; <papid> J01-3003 </papid>mccarthy, 2000).<papid> A00-2034 </papid></nextsent>
<nextsent>as cueto this alternation, merlo and stevenson (2001) <papid> J01-3003 </papid>create bag of head nouns for each of the two potentially alternating slots, and compare them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1021">
<title id=" W04-2605.xml">using selectional profile distance to detect verb alternations </title>
<section> kiva ate his lunch./kiva ate./*his lunch ate..  </section>
<citcontext>
<prevsection>
<prevsent>manual annotation of verbs is labour intensive, and new verbs (or new uses of known verbs) may be encountered in any given domain.
</prevsent>
<prevsent>in response, some researchers have begun to investigate ways to detect alternations automatically in corpus.
</prevsent>
</prevsection>
<citsent citstr=" W99-0632 ">
some of this work has focused on subcategorization patterns as the clear syntactic cueto an alternation (lapata, 1999; <papid> P99-1051 </papid>lapata and brew, 1999; <papid> W99-0632 </papid>schulte im walde and brew, 2002).</citsent>
<aftsection>
<nextsent>other work has observed, however, that detecting an alternation involves more than observing the use of particular subcategorization sit must also be determined whether the semantic arguments are mapped to the appropriate po sitions.1 to address this issue, it has been suggested that, if verb participates in an alternation, then there should be similarity in the kinds of nouns that show up in the syn 1for example, melt (as in (1) above) undergoes causative alternation because the theme argument that surfaces as subject of the in transitive surfaces as object of the transitive, with the addition of causal agent as the subject of the latter.
</nextsent>
<nextsent>it is not the case that any optionally in transitive verb undergoes this alternation, as shown by eat in (2).tactic positions (or slots) that alternate such as snow occurring as in transitive subject and transitive object in the causative alternation in (1) (merlo and stevenson, 2001; <papid> J01-3003 </papid>mccarthy, 2000).<papid> A00-2034 </papid></nextsent>
<nextsent>as cueto this alternation, merlo and stevenson (2001) <papid> J01-3003 </papid>create bag of head nouns for each of the two potentially alternating slots, and compare them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1024">
<title id=" W04-2605.xml">using selectional profile distance to detect verb alternations </title>
<section> kiva ate his lunch./kiva ate./*his lunch ate..  </section>
<citcontext>
<prevsection>
<prevsent>some of this work has focused on subcategorization patterns as the clear syntactic cueto an alternation (lapata, 1999; <papid> P99-1051 </papid>lapata and brew, 1999; <papid> W99-0632 </papid>schulte im walde and brew, 2002).</prevsent>
<prevsent>other work has observed, however, that detecting an alternation involves more than observing the use of particular subcategorization sit must also be determined whether the semantic arguments are mapped to the appropriate po sitions.1 to address this issue, it has been suggested that, if verb participates in an alternation, then there should be similarity in the kinds of nouns that show up in the syn 1for example, melt (as in (1) above) undergoes causative alternation because the theme argument that surfaces as subject of the in transitive surfaces as object of the transitive, with the addition of causal agent as the subject of the latter.</prevsent>
</prevsection>
<citsent citstr=" A00-2034 ">
it is not the case that any optionally in transitive verb undergoes this alternation, as shown by eat in (2).tactic positions (or slots) that alternate such as snow occurring as in transitive subject and transitive object in the causative alternation in (1) (merlo and stevenson, 2001; <papid> J01-3003 </papid>mccarthy, 2000).<papid> A00-2034 </papid></citsent>
<aftsection>
<nextsent>as cueto this alternation, merlo and stevenson (2001) <papid> J01-3003 </papid>create bag of head nouns for each of the two potentially alternating slots, and compare them.</nextsent>
<nextsent>in contrast to comparing head nouns directly, mccarthy (2000) <papid> A00-2034 </papid>instead compares the selectional preferences foreach of the two slots (captured by probability distribution over wordnet).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1044">
<title id=" W04-2605.xml">using selectional profile distance to detect verb alternations </title>
<section> kiva ate his lunch./kiva ate./*his lunch ate..  </section>
<citcontext>
<prevsection>
<prevsent>selectional preference refers to the general notion of how much verb favours (or disfavours) particular noun as semantic argument.
</prevsent>
<prevsent>for example, informally we would say that eat has strong selectional preference for nouns of type food as its theme argument.
</prevsent>
</prevsection>
<citsent citstr=" J98-2002 ">
formalization of this notion has been difficult, but several computational methods have now been proposed that capture selectional preference of verb as probability distribution over the wordnet hierarchy (resnik, 1993; li and abe, 1998; <papid> J98-2002 </papid>clark and weir, 2002).<papid> J02-2003 </papid>2 the key task that each of these proposals address is how to generalize appropriately from counts of observed nouns in the relevant verb argument position (in corpus), to probabilistic representation of selectional strength over classes.</citsent>
<aftsection>
<nextsent>we will refer in there mainder of the paper to such probability distribution over wordnet as selectional profile.?
</nextsent>
<nextsent>as mentioned above, mccarthy (2000) <papid> A00-2034 </papid>suggested the use of selectional profiles to capture generalizations over argument slots, so that two argument slots could be effectively compared for detecting alternations.</nextsent>
<nextsent>after extracting the argument heads of the target slots of each verb (e.g., the in transitive subject and the transitive object for the causative alternation), she then determined their selectional profiles using minimum description length tree cut model (li and abe, 1998)<papid> J98-2002 </papid>3 the two slot profiles were compared using skew divergence (a variant of2resniks proposed measure is not actually probability distribution, but difference between probability distributions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1046">
<title id=" W04-2605.xml">using selectional profile distance to detect verb alternations </title>
<section> kiva ate his lunch./kiva ate./*his lunch ate..  </section>
<citcontext>
<prevsection>
<prevsent>selectional preference refers to the general notion of how much verb favours (or disfavours) particular noun as semantic argument.
</prevsent>
<prevsent>for example, informally we would say that eat has strong selectional preference for nouns of type food as its theme argument.
</prevsent>
</prevsection>
<citsent citstr=" J02-2003 ">
formalization of this notion has been difficult, but several computational methods have now been proposed that capture selectional preference of verb as probability distribution over the wordnet hierarchy (resnik, 1993; li and abe, 1998; <papid> J98-2002 </papid>clark and weir, 2002).<papid> J02-2003 </papid>2 the key task that each of these proposals address is how to generalize appropriately from counts of observed nouns in the relevant verb argument position (in corpus), to probabilistic representation of selectional strength over classes.</citsent>
<aftsection>
<nextsent>we will refer in there mainder of the paper to such probability distribution over wordnet as selectional profile.?
</nextsent>
<nextsent>as mentioned above, mccarthy (2000) <papid> A00-2034 </papid>suggested the use of selectional profiles to capture generalizations over argument slots, so that two argument slots could be effectively compared for detecting alternations.</nextsent>
<nextsent>after extracting the argument heads of the target slots of each verb (e.g., the in transitive subject and the transitive object for the causative alternation), she then determined their selectional profiles using minimum description length tree cut model (li and abe, 1998)<papid> J98-2002 </papid>3 the two slot profiles were compared using skew divergence (a variant of2resniks proposed measure is not actually probability distribution, but difference between probability distributions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1061">
<title id=" W04-2605.xml">using selectional profile distance to detect verb alternations </title>
<section> selectional profile distance.  </section>
<citcontext>
<prevsection>
<prevsent>naturally, we look tothe current research comparing semantic similarity between word senses (e.g., budanitsky and hirst, 2001;lin, 1998).
</prevsent>
<prevsent>we choose to implement two straightforward methods.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
for one, we invert (to obtain distance) the wordnet similarity measure of wu and palmer (1994), <papid> P94-1019 </papid>yielding: `ha : lt@*]&r;   fl % z# :  e fl % z# :  e -2fl % z# : \ : lt@*z&r; ee * (3) where cn.</citsent>
<aftsection>
<nextsent>  khjmh  is the lowest common subsumer of dge and dgf . the other method we use is the simple edge distance between nodes, vh]ikj2h .6thus far, we have defined spd as sum of propagated profile scores multiplied by the distance travelled?
</nextsent>
<nextsent>(equation 1).
</nextsent>
<nextsent>we have also considered propagating other values as function of profile scores.
</nextsent>
<nextsent>lets return to the same example but redistribute some of the probability mass of
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1062">
<title id=" W04-2605.xml">using selectional profile distance to detect verb alternations </title>
<section> materials and methods.  </section>
<citcontext>
<prevsection>
<prevsent>and drug information?
</prevsent>
<prevsent>sections of the medlineplus website (http://www.nlm.nih.gov/ medlineplus/).
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
the text is parsed using the raspparser (briscoe and carroll, 2002), and subcategorizations are extracted using the system of briscoe and car roll (1997).<papid> A97-1052 </papid></citsent>
<aftsection>
<nextsent>the subcategorization frame entry of each verb includes the frequency count and list of argument heads per slot.
</nextsent>
<nextsent>the target slots in this work are the subject of the in transitive and the object of the transitive.
</nextsent>
<nextsent>4.2 verb selection.
</nextsent>
<nextsent>we evaluate our method on the causative alternation in order for comparison to the earlier methods of mccarthy(2000) <papid> A00-2034 </papid>and merlo and stevenson (2001).<papid> J01-3003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1096">
<title id=" W04-0857.xml">generative models for semantic role labeling </title>
<section> role labeler.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, the role sequence in figure 1 is replaced by single node containing all roles.
</prevsent>
<prevsent>this can be compared to case-based approach that memorizes all seen role sequences and calculates their likelihood.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
it is also similar to gildea &amp; jurafskys (2002) <papid> J02-3001 </papid>frame element groups, though we distinguish between different role orderings, whereas association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems they do not.</citsent>
<aftsection>
<nextsent>however, we still model constituent generation sequentially.
</nextsent>
<nextsent>the framenet corpus contains annotations for allof the model components described above.
</nextsent>
<nextsent>we represent each constituent by its phrasal category together with its head word.
</nextsent>
<nextsent>as in gildea &amp; jurafskys (2002) <papid> J02-3001 </papid>approach, we determine headwords from the sentences syntactic parse, using simple heuristic1when syntactic alignment with parse is not avail able.we estimate most of the model parameters using straightforward maximum likelihood estimate based on fully labeled training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1100">
<title id=" W04-0857.xml">generative models for semantic role labeling </title>
<section> constituent classification for role.  </section>
<citcontext>
<prevsection>
<prevsent>we use naive bayes classifier from the weka machine learning toolkit (witten and frank, 2000) to classify every sentence constituent as role-bearing or not.
</prevsent>
<prevsent>in our cross validation studies, naive bayes was both accurate and efficient.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
to generate the training examples forthe classifier, we generate parse tree for every sentence in the senseval-3 training data, using the collins (1996) <papid> P96-1025 </papid>statistical parser.</citsent>
<aftsection>
<nextsent>we call each nodein this tree constituent.
</nextsent>
<nextsent>once it is trained, the classifier can sift through new constituent list and decide which are likely to be role-bearing.
</nextsent>
<nextsent>the selected constituents are passed on to the role labeler for labeling with semantic roles, as described in section 4.
</nextsent>
<nextsent>we train the classifier on examples extracted from the senseval-3 training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1102">
<title id=" W04-0857.xml">generative models for semantic role labeling </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>forex ample, in preliminary studies, bottom-up search for positive constituents in the parse tree seems toyield better results than our current top-down approach.
</prevsent>
<prevsent>second, since false positives cannot be entirely avoided, the labeler needs to better handle constituents that should not be labeled with role.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
to solve this problem, we will adapt the idea of null generated words from machine translation (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>instead of having word in the target language that corresponds to no word in the source language, we have constituent that corresponds to no state in the role sequence.
</nextsent>
<nextsent>finally, we will address roles that do not label aconstituent, called null-instantiated roles.
</nextsent>
<nextsent>an example is the sentence the man drove to the station,in which the vehicle role does not have constituent, but is implicitly there, since the man obviously drove something to the station.
</nextsent>
<nextsent>this problem is more difficult, since it involves obtaining information not actually in the sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1103">
<title id=" W04-1011.xml">handling figures in document summarization </title>
<section> prospects for automation.  </section>
<citcontext>
<prevsection>
<prevsent>clearly, automation of diagram summarization presents new set of challenges and is no easier than text summarization.
</prevsent>
<prevsent>large scale evaluation of diagram summarization will offer its own challenges, cf.
</prevsent>
</prevsection>
<citsent citstr=" P03-1048 ">
text summarization evaluation (radev et al, 2003).<papid> P03-1048 </papid></citsent>
<aftsection>
<nextsent>automated text summarization has advanced substantially in the last decade.
</nextsent>
<nextsent>see for example, the major collection of papers, (mani &amp; maybury, 1999) and the special journal issue (radev, hovy, &amp; mckeown, 2002).
</nextsent>
<nextsent>reviews include (hovy, 2002; marcu, 2003).
</nextsent>
<nextsent>a recent useful monograph is (mani, 2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1104">
<title id=" W04-1011.xml">handling figures in document summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another recent work is (barzilay, 2003), focused on mult idocument summarization and going beyond sentence extraction to consider phrases.
</prevsent>
<prevsent>paradoxically, work on the summarization of scientific articles is inhibited by the fact that virtually all scientific articles have abstracts as sta dard component.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
but there are other tasks such as developing user-tailored summaries (teufel &amp; moens, 2002).<papid> J02-4002 </papid></citsent>
<aftsection>
<nextsent>the generation of coordinated explanations involving text and graphics offers insight into the relations between them (feiner &amp; mckeown, 1990).
</nextsent>
<nextsent>this task involves dealing with the internal structure of diagrams, as do problems of image retrieval, which can be aided by developing ontol ogy-based descriptions of the images (hyvnen, styrman, &amp; saarela, 2002).
</nextsent>
<nextsent>diagrams form part of coordinated discourse, so that diagram summarization can profit from the work done on text summarization that focuses on discourse structure.
</nextsent>
<nextsent>examples of dis course-related approaches include (boguraev &amp; neff, 2000; marcu, 1997<papid> W97-0713 </papid>a, 1997b; teufel &amp; moens, 2002).<papid> J02-4002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1106">
<title id=" W04-1011.xml">handling figures in document summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this task involves dealing with the internal structure of diagrams, as do problems of image retrieval, which can be aided by developing ontol ogy-based descriptions of the images (hyvnen, styrman, &amp; saarela, 2002).
</prevsent>
<prevsent>diagrams form part of coordinated discourse, so that diagram summarization can profit from the work done on text summarization that focuses on discourse structure.
</prevsent>
</prevsection>
<citsent citstr=" W97-0713 ">
examples of dis course-related approaches include (boguraev &amp; neff, 2000; marcu, 1997<papid> W97-0713 </papid>a, 1997b; teufel &amp; moens, 2002).<papid> J02-4002 </papid></citsent>
<aftsection>
<nextsent>document summarization including diagrams seems both possible and desirable.
</nextsent>
<nextsent>work in this area is waiting on the development of corpus of parsed object-based diagrams.
</nextsent>
<nextsent>the vector ization and parsing systems required are under development.
</nextsent>
<nextsent>this material is based upon work supported by the national science foundation under grants no.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1109">
<title id=" W04-0915.xml">interpreting communicative goals in constrained domains using generation and interactive negotiation </title>
<section> interactive validation of the correct.  </section>
<citcontext>
<prevsection>
<prevsent>figure 3: fuzzy inverted generation highest similarity score with the input document is then considered to be the most likely candidate.
</prevsent>
<prevsent>communicative content relying solely on information retrieval techniques to associate normalized content representation to an input document is unfortunately unlikely to yield good results, even if linguistically-oriented techniques can improve accuracy (arampatzis et al, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P03-2017 ">
we have advocated an interactive approach to text understanding (dymetman et al, 2003) <papid> P03-2017 </papid>where the input text is used as source of information to assist the user in re-authoring its content.</citsent>
<aftsection>
<nextsent>following fuzzy inverted generation, an interactive negotiation can take place between the system and its hypotheses (the candidate content representations) on the one hand, and human expert on the second.
</nextsent>
<nextsent>a naive way would be to let the expert choose which hypothesis is correct based on the normalized text associated with each one of them.
</nextsent>
<nextsent>but this would be tedious and error-prone process.
</nextsent>
<nextsent>rather, under specifications from analysis can be found by building compact representation of the candidates, and then used to engage in negotiations over local interpretation issues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1110">
<title id=" W04-0915.xml">interpreting communicative goals in constrained domains using generation and interactive negotiation </title>
<section> interactive document normalization.  </section>
<citcontext>
<prevsection>
<prevsent>system systems implementing controlled document authoring (hartley and paris, 1997) are based on an interaction with an author who makes semantic choices that define the content of document, from which multilingual textual versions can be produced.
</prevsent>
<prevsent>therefore, these systems integrate resources that can be used to represent document content and to generate textual versions of the documents.
</prevsent>
</prevsection>
<citsent citstr=" C00-1036 ">
the mda system developed at xrce (dymetman et al, 2000; <papid> C00-1036 </papid>brunet al, 2000) <papid> W00-1404 </papid>uses formalism inspired from definite clause grammars (pereira and warren, 1980) that encodes both the abstract semantic syntax of well-formed documents and the concrete syntax for the documents in several languages.5 mda grammars contain the definition of semantic objects of given semantic type, which are usedto build typed abstract semantic trees.</citsent>
<aftsection>
<nextsent>importantly, the formalism can encode the three levels for normalization model that we described in our introduction: semantic objects can be of any granularity and can thus be communicative goals; the communicative structure is described by the abstract semantic syntax, which can be used to express semantic dependencies across subtrees; and the text generated is entirely under control, so normalized texts can be associated with communicative goals.
</nextsent>
<nextsent>5this is achieved by developing parallel grammars that share the same abstract semantic syntax, but specify concrete syntax for particular language.figure 4: architecture of our document normalization system for the reasons given above, we used the formalism of mda for our implementation.
</nextsent>
<nextsent>the architecture of our normalization system is shown on figure 4.
</nextsent>
<nextsent>textual descriptors (wordnet synsets in our current implementation) are first extracted from the text of the input document to build the profile of the input document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1111">
<title id=" W04-0915.xml">interpreting communicative goals in constrained domains using generation and interactive negotiation </title>
<section> interactive document normalization.  </section>
<citcontext>
<prevsection>
<prevsent>system systems implementing controlled document authoring (hartley and paris, 1997) are based on an interaction with an author who makes semantic choices that define the content of document, from which multilingual textual versions can be produced.
</prevsent>
<prevsent>therefore, these systems integrate resources that can be used to represent document content and to generate textual versions of the documents.
</prevsent>
</prevsection>
<citsent citstr=" W00-1404 ">
the mda system developed at xrce (dymetman et al, 2000; <papid> C00-1036 </papid>brunet al, 2000) <papid> W00-1404 </papid>uses formalism inspired from definite clause grammars (pereira and warren, 1980) that encodes both the abstract semantic syntax of well-formed documents and the concrete syntax for the documents in several languages.5 mda grammars contain the definition of semantic objects of given semantic type, which are usedto build typed abstract semantic trees.</citsent>
<aftsection>
<nextsent>importantly, the formalism can encode the three levels for normalization model that we described in our introduction: semantic objects can be of any granularity and can thus be communicative goals; the communicative structure is described by the abstract semantic syntax, which can be used to express semantic dependencies across subtrees; and the text generated is entirely under control, so normalized texts can be associated with communicative goals.
</nextsent>
<nextsent>5this is achieved by developing parallel grammars that share the same abstract semantic syntax, but specify concrete syntax for particular language.figure 4: architecture of our document normalization system for the reasons given above, we used the formalism of mda for our implementation.
</nextsent>
<nextsent>the architecture of our normalization system is shown on figure 4.
</nextsent>
<nextsent>textual descriptors (wordnet synsets in our current implementation) are first extracted from the text of the input document to build the profile of the input document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1112">
<title id=" W04-1901.xml">the hinoki treebank working toward text understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show how this treebank can be used to extract thesaurus information from definition sentences ina language-neutral way using minimal recur sion semantics.
</prevsent>
<prevsent>in this paper we describe the current state of new lexical resource: the hinoki treebank.
</prevsent>
</prevsection>
<citsent citstr=" C04-1193 ">
the motivation and initial construction was described in detail in bond et al (2004<papid> C04-1193 </papid>a).</citsent>
<aftsection>
<nextsent>the ultimate goal of our research is natural language understanding ? we aimto create system that can parse text into some useful semantic representation.
</nextsent>
<nextsent>ideally this would be such that the output can be used to actually update our semantic models.
</nextsent>
<nextsent>this is an ambitious goal, and this paper does not present completed solution, but rather road-map to the solution, with some progress along the way.
</nextsent>
<nextsent>the mid-term goal is to build thesaurus from dictionary definition sentences and use it to enhancea stochastic parse ranking model that combines syntactic and semantic information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1115">
<title id=" W04-1901.xml">the hinoki treebank working toward text understanding </title>
<section> the hinoki treebank.  </section>
<citcontext>
<prevsection>
<prevsent>driver?
</prevsent>
<prevsent>isgiven in figure 1, with english glosses added (un der lined features are those added by hinoki).
</prevsent>
</prevsection>
<citsent citstr=" C02-2025 ">
the structure of our treebank is inspired by the redwoods treebank of english in which utterances are parsed and the annotator selects the best parse from the full analyses derived by the grammar (oepen et al., 2002).<papid> C02-2025 </papid></citsent>
<aftsection>
<nextsent>we had four main reasons for selecting this approach.
</nextsent>
<nextsent>the first was that we wanted to develop precise broad-coverage grammar in tandem with the treebank, as part of our research into natural language understanding.
</nextsent>
<nextsent>tree banking the out ??
</nextsent>
<nextsent>index
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1116">
<title id=" W04-1901.xml">the hinoki treebank working toward text understanding </title>
<section> the hinoki treebank.  </section>
<citcontext>
<prevsection>
<prevsent>driver?
</prevsent>
<prevsent>(with english glosses) put of the parser allows us to immediately identify problems in the grammar, and improving the grammar directly improves the quality of the treebank in mutually beneficial feedback loop (oepen et al, 2004).the second reason is that we wanted to annotate to high level of detail, marking not only dependency and constituent structure but also detailed semantic relations.
</prevsent>
</prevsection>
<citsent citstr=" W02-1210 ">
by using japanese grammar (jacy: siegel and bender (2002)) <papid> W02-1210 </papid>based on monostratal theory of grammar (hpsg: pollard andsag (1994)) we could simultaneously annotate syntactic and semantic structure without overburdening the annotator.</citsent>
<aftsection>
<nextsent>the treebank records the complete syntacto-semantic analysis provided by the hpsg grammar, along with an annotators choice of the most appropriate parse.
</nextsent>
<nextsent>from this record, all kinds of information can be extracted at various levels of granularity.
</nextsent>
<nextsent>in particular, traditional syntactic structure (e.g., in the form of labeled trees), dependency relations between words and full meaning representations using minimal recur sion semantics(mrs: copestake et al (1999)).
</nextsent>
<nextsent>a simplified example of the labeled tree, mrs and dependency views for the definition of
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1125">
<title id=" W04-1104.xml">adaptive compression based approach for chinese pinyin input </title>
<section> prediction by partial matching.  </section>
<citcontext>
<prevsection>
<prevsent>ppm compression methods are significantly better than practical compression utilities like unix gzip and compress except escape method but they are slower during compression.
</prevsent>
<prevsent>the compression rates for escape method and are both higher than escape method c. order-2 model (trigram)is slightly better that order-1 and order-3 models for escape method d. in our experiment we use escape method to calculate the escape probability as escape method is slightly better than other escape methods in compressing text although method is the best here.
</prevsent>
</prevsection>
<citsent citstr=" J00-3004 ">
teahan (teahan et al, 2000) <papid> J00-3004 </papid>has successfully applied escape method to segment chinese text.</citsent>
<aftsection>
<nextsent>we use 220mb people daily (91-95) as the training corpus and 58m people daily (96) and stories download from internet (400k) as the test corpus.
</nextsent>
<nextsent>we used srilm language tools (stolcke, 2002) to collect trigram counts and applied modified kneser-ney smoothing method tobuild the language model.
</nextsent>
<nextsent>then we used disam big to translate pinyin to chinese characters.
</nextsent>
<nextsent>in ppm model we used the same count data collected by srilm tools.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1126">
<title id=" W04-0309.xml">the information processing difficulty of incremental parsing </title>
<section> procedure.  </section>
<citcontext>
<prevsection>
<prevsent>derivation trees on both grammars were obtained5for each of keenan and hawkins?
</prevsent>
<prevsent>(1987) twenty four stimulus sentences6.
</prevsent>
</prevsection>
<citsent citstr=" J99-1004 ">
branches of these derivation trees were viewed as pcfg rules with probabilities set according to the usual relative-frequency estimation technique (chi, 1999).<papid> J99-1004 </papid></citsent>
<aftsection>
<nextsent>however, because the stimuli were intentionally constructed to have5derivations were obtained using parser described in appendix of hale (2003) 6to eliminate number agreement as source of derivationaluncertainty, the results were calculated using modified stimulus set in which four noun phrases were changed from plural to singular.exactly four examples of each structure, these sentences were weighted in accordance with corpus study (keenan, 1975) to make their relative frequencies more realistic.
</nextsent>
<nextsent>the summed entropy reductions exhibit significant correlation with the repetition accuracy scores collected by keenan and hawkins (1987).
</nextsent>
<nextsent>the correlation in figure 7(a) obtains only on the grammar expressing the kaynian promotion analysis, and not on the grammar expressing the standard adjunction analysis (figure 7(b)).
</nextsent>
<nextsent>nor do log probabilities for stimulus sentences on the grammar 250 300 350 400 450 500error score 30 35 40 45 50 55 total bits reduced accessibility hierarchy promotion grammar r2=0.45, 0.001 250 300 350 400 450 500error score 50 55 60 65 70 75 total bits reduced accessibility hierarchy adjunction grammar r2=0.02, n.s. figure 7: predictions of two probabilistic minimalist grammars through the lens of the erh exhibit significant correlation with repetition accuracy scores.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1127">
<title id=" W04-2008.xml">an algorithm for open text semantic parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>these interfaces make the semantic parser more flexible, robust, and easier to integrate into other systems that achieve high level meaning processing and understanding.
</prevsent>
<prevsent>there are several statistical approaches for automatic semantic role labeling based on propbank and framenet.
</prevsent>
</prevsection>
<citsent citstr=" P00-1065 ">
(gildea and jurafsky, 2000) <papid> P00-1065 </papid>proposed statistical approach based on framenet data for annotation of semantic roles.</citsent>
<aftsection>
<nextsent>fleischman(fleischman et al, 2003) <papid> W03-1007 </papid>used framenet annotations in maximum entropy framework.</nextsent>
<nextsent>a more flexible generative model is proposed in (thomp sonet al, 2003), where null-instantiated roles can be also identified, and frames are not assumed to be known a-priori.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1128">
<title id=" W04-2008.xml">an algorithm for open text semantic parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there are several statistical approaches for automatic semantic role labeling based on propbank and framenet.
</prevsent>
<prevsent>(gildea and jurafsky, 2000) <papid> P00-1065 </papid>proposed statistical approach based on framenet data for annotation of semantic roles.</prevsent>
</prevsection>
<citsent citstr=" W03-1007 ">
fleischman(fleischman et al, 2003) <papid> W03-1007 </papid>used framenet annotations in maximum entropy framework.</citsent>
<aftsection>
<nextsent>a more flexible generative model is proposed in (thomp sonet al, 2003), where null-instantiated roles can be also identified, and frames are not assumed to be known a-priori.
</nextsent>
<nextsent>these approaches exclusively focuson semantic roles labeling based on statistical methods, rather than analysis of the full structure of sentence semantics.
</nextsent>
<nextsent>however, rule-based approach is closer to the way humans interpret the semantic structure of sentence.
</nextsent>
<nextsent>moreover, as mentioned earlier, the framenet data is not meant to be statistically representative?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1129">
<title id=" W04-0805.xml">the italian lexical sample task at senseval3 </title>
<section> automatic annotation.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, subjectivity in manual tagging was considerably reduced by adjusting the sense repository and selecting manually each single instance, but it could not be eliminated.
</prevsent>
<prevsent>we provided participants with three data sets: labeled training data (twice larger than the test set), unlabeled training data (about 10 times the labeledinstances) and test data.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
in order to facilitate participation, we pos-tagged the labeled datasets using an italian version of the tnt pos-tagger (brants, 2000), <papid> A00-1031 </papid>trained on the elsnet corpus.</citsent>
<aftsection>
<nextsent>3.1 participants?
</nextsent>
<nextsent>results.
</nextsent>
<nextsent>three groups participated in the italian lexical sample task, testing six systems: two developed by itc-irst - italy - (irst-kernels and irst-ties),three by swarthmore college - u.s.a. - (swat-hk italian, italian-swat_hk-bo and swat-italian) and one by uned - spain.
</nextsent>
<nextsent>table 2 below reports the participants?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1130">
<title id=" W04-0907.xml">making sense of japanese relative clause constructions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our objective in this paper is, given taxonomy of japanese rcc semantic types (baldwin, 1998) and gold-standard set of japanese rcc instances,to investigate the success of various parameter configurations in interpreting rccs.
</prevsent>
<prevsent>one feature of the proposed method is that it is based on shallow analysis, centring principally around basic case frame and verb class description.
</prevsent>
</prevsection>
<citsent citstr=" E03-1040 ">
that is, we attempt tomake maximum use of surface information in performing deep semantic task, in the same vein, e.g., as joanis and stevenson (2003) <papid> E03-1040 </papid>for english verb classification and lapata (2002) <papid> J02-3004 </papid>in disambiguating nominalisations.</citsent>
<aftsection>
<nextsent>relative clause interpretation is core component of text understanding, as demonstrated in the context of the muc conference series (cardie, 1992;<papid> P92-1028 </papid>hobbs et al, 1997).</nextsent>
<nextsent>it also has immediate applications in, e.g., japanese english machine translation: for case-slot gapping rccs such as (1), we ex trapose the head np from the appropriate argument position in the english relative clause (producing,e.g., the hat   [</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1131">
<title id=" W04-0907.xml">making sense of japanese relative clause constructions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our objective in this paper is, given taxonomy of japanese rcc semantic types (baldwin, 1998) and gold-standard set of japanese rcc instances,to investigate the success of various parameter configurations in interpreting rccs.
</prevsent>
<prevsent>one feature of the proposed method is that it is based on shallow analysis, centring principally around basic case frame and verb class description.
</prevsent>
</prevsection>
<citsent citstr=" J02-3004 ">
that is, we attempt tomake maximum use of surface information in performing deep semantic task, in the same vein, e.g., as joanis and stevenson (2003) <papid> E03-1040 </papid>for english verb classification and lapata (2002) <papid> J02-3004 </papid>in disambiguating nominalisations.</citsent>
<aftsection>
<nextsent>relative clause interpretation is core component of text understanding, as demonstrated in the context of the muc conference series (cardie, 1992;<papid> P92-1028 </papid>hobbs et al, 1997).</nextsent>
<nextsent>it also has immediate applications in, e.g., japanese english machine translation: for case-slot gapping rccs such as (1), we ex trapose the head np from the appropriate argument position in the english relative clause (producing,e.g., the hat   [</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1132">
<title id=" W04-0907.xml">making sense of japanese relative clause constructions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one feature of the proposed method is that it is based on shallow analysis, centring principally around basic case frame and verb class description.
</prevsent>
<prevsent>that is, we attempt tomake maximum use of surface information in performing deep semantic task, in the same vein, e.g., as joanis and stevenson (2003) <papid> E03-1040 </papid>for english verb classification and lapata (2002) <papid> J02-3004 </papid>in disambiguating nominalisations.</prevsent>
</prevsection>
<citsent citstr=" P92-1028 ">
relative clause interpretation is core component of text understanding, as demonstrated in the context of the muc conference series (cardie, 1992;<papid> P92-1028 </papid>hobbs et al, 1997).</citsent>
<aftsection>
<nextsent>it also has immediate applications in, e.g., japanese english machine translation: for case-slot gapping rccs such as (1), we ex trapose the head np from the appropriate argument position in the english relative clause (producing,e.g., the hat   [
</nextsent>
<nextsent>  bought yesterday]?), and for at tributive rccs such as (2), we generate the english relative clause without extra position and select the relative pronoun according to the head np (produc ing, e.g., the reason that the hat was bought?).rcc interpretation is dogged by analytical ambiguity, in particular for phrase boundary, phrase head/attachment and word sense ambiguity.
</nextsent>
<nextsent>the first two of these concerns can be dealt with by parser such as knp (kurohashi and nagao, 1998)or cabocha (kudo and matsumoto, 2002), <papid> W02-2016 </papid>or alternatively tag sequence-based technique such as that proposed by siddharthan (2002) for english.</nextsent>
<nextsent>word sense ambiguity is an issue if we wish to determine the valence of the verb and make use of selectionalrestrictions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1133">
<title id=" W04-0907.xml">making sense of japanese relative clause constructions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it also has immediate applications in, e.g., japanese english machine translation: for case-slot gapping rccs such as (1), we ex trapose the head np from the appropriate argument position in the english relative clause (producing,e.g., the hat   [
</prevsent>
<prevsent>  bought yesterday]?), and for at tributive rccs such as (2), we generate the english relative clause without extra position and select the relative pronoun according to the head np (produc ing, e.g., the reason that the hat was bought?).rcc interpretation is dogged by analytical ambiguity, in particular for phrase boundary, phrase head/attachment and word sense ambiguity.
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
the first two of these concerns can be dealt with by parser such as knp (kurohashi and nagao, 1998)or cabocha (kudo and matsumoto, 2002), <papid> W02-2016 </papid>or alternatively tag sequence-based technique such as that proposed by siddharthan (2002) for english.</citsent>
<aftsection>
<nextsent>word sense ambiguity is an issue if we wish to determine the valence of the verb and make use of selectionalrestrictions.
</nextsent>
<nextsent>we sidestep full-on verb sense disambiguation by associating unique case frame with each verb stem type and encoding common alternations in the verb class.
</nextsent>
<nextsent>even here, however, we must have some means of dealing with verb homonymy and integrating analyses for co subordinated relativeclauses.
</nextsent>
<nextsent>we investigate various techniques to resolve such ambiguity and combine the analysis of multiple component clauses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1134">
<title id=" W04-0907.xml">making sense of japanese relative clause constructions </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the accuracy for their method over task where they distinguished between attributive and 6 types of case-slot gappingrccs (defined according to case marker) was relatively modest 65.3%.
</prevsent>
<prevsent>for binary attributive vs.case-slot gapping task, the accuracy was more respectable 88.8%, but still considerably lower than that achieved in this research.
</prevsent>
</prevsection>
<citsent citstr=" P98-2125 ">
an alternate point of reference is found in the work of li et al (1998) <papid> P98-2125 </papid>on korean rccs, which display the same structural ambiguities as japanese rccs.</citsent>
<aftsection>
<nextsent>li et al (1998) <papid> P98-2125 </papid>attain an accuracy of 90.4% through statistical analysis of the distribution of verb-case filler collocates, except that they classify relative clauses according to only 5 categories and consider only case-slot gapping rccs.</nextsent>
<nextsent>with our method, restricting analysis to only gapping rccs(still retaining total of nineteen rcc types) produces an accuracy of 94.1% for the andci system with c4.5.in conclusion, we have proposed method for interpreting japanese relative clause constructions according to surface evidence and generalised semantic representation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1138">
<title id=" W04-0411.xml">lexical encoding of mwes </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>different strategies for encoding mwes have been employed by different lexical resources with varying degrees of success, depending on the type of mwe.
</prevsent>
<prevsent>one case is the alvey tools lexicon (carroll and grover, 1989), which has good coverage of phrasal verbs,providing extensive information about their syntactic aspects (variation in word order, subcategorisation, etc), but it does not distinguish compositional from non-compositional entries neither does it specify entries that can be productively formed.
</prevsent>
</prevsection>
<citsent citstr=" W98-0707 ">
wordnet, on the other hand, covers large number ofmwes (fellbaum, 1998), <papid> W98-0707 </papid>but does not provide information about their variability.</citsent>
<aftsection>
<nextsent>neither of these resources covers idioms.
</nextsent>
<nextsent>the challenge in designing adequate lexical resources for mwes, is to ensure that the variability and the extra dimensions required by the different types of mwe can be captured.
</nextsent>
<nextsent>such move is called for by calzolari et al  (2002) and copestake et al  (2002).
</nextsent>
<nextsent>calzolari et al  (2002) discuss these problems while attempting to establish the standards for mwe description in the context of multilingual lexical resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1139">
<title id=" W04-0411.xml">lexical encoding of mwes </title>
<section> verb particle constructions.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, some of the compositional vpcs seem to follow productive patterns (e.g. the resultative combinations walk/jump/run up/down/out/in/away/around/... from joining these verbs and the directional/locative particles up,down, out, in, away, around, ...).
</prevsent>
<prevsent>this is discussed in fraser (1976), who notes that the semantic properties of verbs seem to affect their possibility of combination with particles.
</prevsent>
</prevsection>
<citsent citstr=" W03-1808 ">
for productive vpcs, one possibility is then to use the entries of verbs already listed in lexical resource to productively generate vpc entries by combining them with particles according to their semantic classes, as discussed by villavicencio (2003).<papid> W03-1808 </papid></citsent>
<aftsection>
<nextsent>however, there are also cases of semi-productivity, since the possibilities of combinations are not fully predictable from particular verb and particle (e.g. phone/ring/call/*telephone up).
</nextsent>
<nextsent>thus, although some classes of vpcs can be productively generated from verb entries, to avoid over generation we adopt an approach where the remaining vpcs need to be explicitly licensed by the specification of the appropriate vpc entry.to sum up, for vpc entries an appropriate encoding needs to maintain the link between vpc and the corresponding simplex form, from where the vpc inherits many of its characteristics, including inflectional morphology and for compositional cases, the semantics of the verb.
</nextsent>
<nextsent>on the other hand, for non-compositional entry, like get al ng, it is necessary to specify the resulting semantics.
</nextsent>
<nextsent>in this case, the semantics defined in the vpc entry overrides that inherited by default from its components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1141">
<title id=" W04-0216.xml">animacy encoding in english why and how </title>
<section> the animacy dimension in natural.  </section>
<citcontext>
<prevsection>
<prevsent>599 dialogues were annotated.
</prevsent>
<prevsent>6 coding reliability.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
the reliability of the annotation was evaluated using the kappa statistic (carletta, 1996).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>although there are no hard and fast rules about what makes an acceptable kappa coefficient it depends on the use to which the data will be putmany researchers in the computational linguistics community have adopted the rule of thumb that discourse annotation should have kappa of at least .8.
</nextsent>
<nextsent>for the reliability study, we had three individuals work separately to code the same four dialogues with the animacy scheme.
</nextsent>
<nextsent>markables (in this case nps and possessives) had been extracted automatically from the data, leading the coders to mark around 10% of the overall set with category that indicated that they were not proper markables and therefore not to be coded.
</nextsent>
<nextsent>omitting these (non-) markables, for the dataset overall, k=.92 (k=3, n=1081).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1142">
<title id=" W04-2104.xml">standards going concrete  from lmf to morphalou </title>
<section> standards for lexical resources.  </section>
<citcontext>
<prevsection>
<prevsent>as consequence, many relevant projects such as the tlfi 5 ( dendien &amp; pierrel, 2003) have designed their own proprietary structure for the description of their lexical archives.
</prevsent>
<prevsent>if one moves away from classical dictionaries proper and considers lexical resources dedicated to the domain of nlp, there are numerous projects that have worked toward the definition of standardized lexical structures in the domain of nlp (multext for basic morphological lexica; genelex, simple, isle/mile for complex multilingual entries; olif 1&amp;2 for translation lexica, etc.), but none of them has lead to standard that reflects wide international consensus and that is effectively maintained by an authoritative body.
</prevsent>
</prevsection>
<citsent citstr=" C92-2089 ">
from more theoretical point of view, it has been shown that such lexical structures can be modelled as feature structures (ide et al i, 1995; veronis &amp; ide, 1992), <papid> C92-2089 </papid>leading to inheritance properties within entries (ide et al i, 2000), as partially implemented in the tei print dictionary chapter (ide &veronis;, 1995).</citsent>
<aftsection>
<nextsent>it has also been 1 text encoding initiative (http://www.tei-c.or) 2 http://dictionary.oed.com/ 3 http://www.dwb.uni-trier.de 4 http://www.mhra.org.uk/ 5 http://www.atilf.fr/_ns/produits/tlfi.htmshown that, with respect to describing the microstructure of such lexica, at least three configurations are possible: 2-layered, 3-layered and 7-layered models.
</nextsent>
<nextsent>in the 2-layered approach, following ferdinand de saussure (1974), word is described by signifier/signified pair, corresponding to morphological/semantic description.
</nextsent>
<nextsent>the syntactic behaviour of the word is then systematically attached to the semantic description.
</nextsent>
<nextsent>this is the approach that has been retained for lmf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1143">
<title id=" W04-2104.xml">standards going concrete  from lmf to morphalou </title>
<section> the lexical markup framework project.  </section>
<citcontext>
<prevsection>
<prevsent>the lmf proposal, as currently being developed in iso committee tc 37/sc 4, is conceived as generic platform for the specification of lexical structures at any level of linguistic description.
</prevsent>
<prevsent>as such, it does not provide one single model, but rather mechanism by which implementers combine elementary lexical subsystems to design models that can be both as close as possible to their needs and comparable to any other lexical models based on the same principles and, possibly, on the same components.
</prevsent>
</prevsection>
<citsent citstr=" W03-1901 ">
the underlying data model for lmf follows the general principles of the linguistic annotation scheme design stated in ide &amp; romary, 2003 <papid> W03-1901 </papid>and implemented in the context of iso standard 16642 for the representation of terminological data (romary, 2001).</citsent>
<aftsection>
<nextsent>those principles provide mechanism for combining given structural 6 http://www2.crl.go.jp/kk/e416/edr/index.html 7 http://www.papillon-dictionary.org/ meta model that informs the general organization ofa certain level of linguistic information (morphology, syntax, etc.) with elementary descriptors (so called data categories).
</nextsent>
<nextsent>data categories reflect basic linguistic concepts (e.g. /part of speech/, /grammatical number/, /paucal number/, etc.) and allow for recording language-specific properties independently of linguistic level specific models.
</nextsent>
<nextsent>in order to share data categories within the community, on-going work (in iso/tc 37) is in the process of deploying an on-line registry 8 of them, especially for use in conjunction with the other standardization activities.
</nextsent>
<nextsent>according to these principles, lmf consists of the following elements: ? core meta model (i.e. the structural skeleton shared by any linguistic description at the lexical level); ? mechanisms for attaching lexical extensions (see below) to the core meta model in order to build up more complex metamodels; ? mechanisms for selecting data categories used for lexical description and for determining how they relate to metamodel; ? mechanisms for expressing any combination of the core meta model and data categories as xml structures, i.e. by deciding to implement given data category (/gender/) as an xml element rather than as an attribute and by providing the corresponding vocabularies (gen?, gender?, genre?); ? methods for describing how to extend lmf to analyze, design, and describe variety of more specific lexical resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1144">
<title id=" W04-0821.xml">the university of maryland senseval3 system descriptions </title>
<section> supervised sense tagging for lexical.  </section>
<citcontext>
<prevsection>
<prevsent>for example, in the sentence the u.s. government announced new visa waiver policy, the word government would have syntactic features like det:the, mod:u.s., and subj-of:announced.
</prevsent>
<prevsent>  expanded context.
</prevsent>
</prevsection>
<citsent citstr=" H01-1060 ">
in information retrieval, we and other researchers have found that it can be useful to expand the representation of document to include informative words from similar documents (levow et al, 2001).<papid> H01-1060 </papid></citsent>
<aftsection>
<nextsent>ina similar spirit, we create set of expanded context features  by (a) treating the wsd context as bag of words, (b) issuing it as query to standard information retrieval system that has indexed large collection of documents, and (c) including the non stop word vocabulary of the top  documents returned.
</nextsent>
<nextsent>so, for example, in context containing the sentence the u.s. government announced new visa waiver policy, the query might retrieve news articles like us to extend fingerprinting to europeans, japanese?
</nextsent>
<nextsent>(bloomberg.com, april 2, 2004), leading to the addition of features like ext:european, ext:japanese, ext:fingerprinting ext:visitors, ext:tourism, and so forth.
</nextsent>
<nextsent>association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems lexical sample coarse (prec/rec) fine (prec/rec) umd-sst 0.643/0.643 0.568/0.568 umd-sst-gram 0.600/0.600 0.576/0.576 umd-sst-docexp 0.541/0.542 0.516/0.491 table 1: umd-sst variations on the senseval-2 english lexical sample task as described by cabezas et al (2001), we have adopted the framework of support vector machines(svms) in order to perform supervised classification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1145">
<title id=" W04-0821.xml">the university of maryland senseval3 system descriptions </title>
<section> unsupervised sense tagging using.  </section>
<citcontext>
<prevsection>
<prevsent>although we have not yet reached any firm conclusions, we conjecture that value potentially added by these features may have been offset by the expansion in the size of the feature space; in future work we plan to explore feature selection and alternative learning frameworks.
</prevsent>
<prevsent>bilingual text 2.1 probabilistic sense model.
</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
for the past several years, the university of maryland group has been exploring unsupervised approaches to word sense disambiguation that take advantage of parallel corpora (diab and resnik, 2002; <papid> P02-1033 </papid>diab, 2003).</citsent>
<aftsection>
<nextsent>recently, bhattacharya et al (2004) (<papid> P04-1037 </papid>in umd/montreal collaboration) have develop eda variation on this bilingual approach that is inspired by the central insight of diabs work, but re casts it in probabilistic framework.</nextsent>
<nextsent>a generative model, it is variant of the graphical model of bengio and kermorvant (2003), which groups semantically related words from the two languages intosenses?; translations are generated by probabilistically choosing sense and then words from the sense.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1146">
<title id=" W04-0821.xml">the university of maryland senseval3 system descriptions </title>
<section> unsupervised sense tagging using.  </section>
<citcontext>
<prevsection>
<prevsent>bilingual text 2.1 probabilistic sense model.
</prevsent>
<prevsent>for the past several years, the university of maryland group has been exploring unsupervised approaches to word sense disambiguation that take advantage of parallel corpora (diab and resnik, 2002; <papid> P02-1033 </papid>diab, 2003).</prevsent>
</prevsection>
<citsent citstr=" P04-1037 ">
recently, bhattacharya et al (2004) (<papid> P04-1037 </papid>in umd/montreal collaboration) have develop eda variation on this bilingual approach that is inspired by the central insight of diabs work, but re casts it in probabilistic framework.</citsent>
<aftsection>
<nextsent>a generative model, it is variant of the graphical model of bengio and kermorvant (2003), which groups semantically related words from the two languages intosenses?; translations are generated by probabilistically choosing sense and then words from the sense.
</nextsent>
<nextsent>briefly, the model of bhattacharya et al uses probabilistic analysis and independence assumptions: it assumes that senses and words have certain occurrence probabilities and that the choice of the word can be made independently once the sense has been decided.
</nextsent>
<nextsent>here interaction between different words arising from the same sense comes into play, even if the words are not related through translations, and this interdependence of the senses through common words plays role in sense disambiguation.
</nextsent>
<nextsent>the model takes as its starting point the idea of translation pair?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1147">
<title id=" W04-0821.xml">the university of maryland senseval3 system descriptions </title>
<section> unsupervised sense tagging using.  </section>
<citcontext>
<prevsection>
<prevsent>for example, in the english-spanish sentence pair me gusta la ciudad/i like the city, one would find the translation pairs
</prevsent>
<prevsent>   , and
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
fiffflfffl .2 those familiar with statistical machine translation (mt) models will note that translation pair is equivalent to link in word-level alignment, and in fact we obtain translation pairs from sentence-aligned parallel text by training statistical mt model (using giza++, (och and ney,2003)) <papid> J03-1002 </papid>and using the word-level alignments that re sult.the probabilistic sense model makes the assumption that the english word ffi and the non-english word !</citsent>
<aftsection>
<nextsent>in translation pair share the same precise sense, or, in other words, that the set of sense labels for the words in the two languages is the same andmay be collapsed into one set of senses that is responsible for both english and non-english words.
</nextsent>
<nextsent>thus the one latent variable in the model is the sense label   generating both words, represented by variables # ffi and #$ . the model also makes the assumption that words in both languages are conditionally independent given the sense label.
</nextsent>
<nextsent>the generative parameters % for the model are the prior probability &amp;
</nextsent>
<nextsent> of each sense  and the conditional probabilities &amp;
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1150">
<title id=" W04-0821.xml">the university of maryland senseval3 system descriptions </title>
<section> using the model for wsd.  </section>
<citcontext>
<prevsection>
<prevsent>language pair precision (%) recall (%) english-chinese 0.445 0.445 english-spanish 0.444 0.444 english-french 0.445 0.445 table 4: unsupervised probabilistic model results(fine-grained) on the senseval-2 english all words task united nations proceedings, and newswire translations from fbis (the foreign broadcast information service).
</prevsent>
<prevsent>  french-english: set of 1,008,591 sentence pairs from the europarl corpus (koehn, 2003)   chinese-english: set of 440,223 sentence pairs from fbis.in order to tag new test sentences, we used machine translation from english test items into each of spanish, french, and chinese.
</prevsent>
</prevsection>
<citsent citstr=" N03-1019 ">
we used systran for spanish and french, and for chinese we usedan implementation of the alignment template framework for statistical mt (kumar and byrne, 2003).<papid> N03-1019 </papid>once having obtained the translations for test sentences, we used giza++ to create word-level alignments within which translation pairs could be iden tified.</citsent>
<aftsection>
<nextsent>we used the probabilistic model only for wsd of nouns, where nouns were identified using an automatic part-of-speech tagger.
</nextsent>
<nextsent>for other parts of speech, we used the first-listed wordnet sense.
</nextsent>
<nextsent>time limitations prevented us from completingsenseval-3 runs in time for this writing.
</nextsent>
<nextsent>table 4 shows the performance of the system on thesenseval-2 english all-words task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1151">
<title id=" W04-1602.xml">developing an arabic treebank methods guidelines procedures and tools </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(maamouri and cieri, 2002) the atb corpora are annotated for morphological information, part-of-speech, english gloss (all in the part-of-speech?
</prevsent>
<prevsent>phase of annotation), and for syntactic structure (treebank ii style).
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
(marcus, et al, 1993), (<papid> J93-2004 </papid>marcus, et al, 1994) <papid> H94-1020 </papid>in addition to the usual issues involved with the complex annotation of data, we have come to terms with number of issues that are specific to highly inflected language with rich history of traditional grammar.</citsent>
<aftsection>
<nextsent>modern standard arabic 2.1 defining the specificities of modern.
</nextsent>
<nextsent>standard arabic?
</nextsent>
<nextsent>modern standard arabic (msa), the natural language under investigation, is not nat ively spoken by arabs, who acquire it only through formal schooling.
</nextsent>
<nextsent>msa is the only form of written communication in the whole of the arab world.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1152">
<title id=" W04-1602.xml">developing an arabic treebank methods guidelines procedures and tools </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(maamouri and cieri, 2002) the atb corpora are annotated for morphological information, part-of-speech, english gloss (all in the part-of-speech?
</prevsent>
<prevsent>phase of annotation), and for syntactic structure (treebank ii style).
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
(marcus, et al, 1993), (<papid> J93-2004 </papid>marcus, et al, 1994) <papid> H94-1020 </papid>in addition to the usual issues involved with the complex annotation of data, we have come to terms with number of issues that are specific to highly inflected language with rich history of traditional grammar.</citsent>
<aftsection>
<nextsent>modern standard arabic 2.1 defining the specificities of modern.
</nextsent>
<nextsent>standard arabic?
</nextsent>
<nextsent>modern standard arabic (msa), the natural language under investigation, is not nat ively spoken by arabs, who acquire it only through formal schooling.
</nextsent>
<nextsent>msa is the only form of written communication in the whole of the arab world.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1153">
<title id=" W04-1118.xml">do we need chinese word segmentation for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore,the usual method is to segment chinese character sequence into chinese words?.
</prevsent>
<prevsent>many investigations have been performed concerning chinese word segmentation.
</prevsent>
</prevsection>
<citsent citstr=" P97-1041 ">
for example, (palmer, 1997) <papid> P97-1041 </papid>developed chinese word segmenter using manually segmented corpus.</citsent>
<aftsection>
<nextsent>the segmentation rules were learned automatically from this corpus.
</nextsent>
<nextsent>(sproat and shih, 1990) and (sun et al, 1998) <papid> P98-2206 </papid>used method that does not relyon dictionary or manually segmented corpus.</nextsent>
<nextsent>the characters of the unsegmented chinese text are grouped into pairs with the highest value of mutual informa tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1154">
<title id=" W04-1118.xml">do we need chinese word segmentation for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, (palmer, 1997) <papid> P97-1041 </papid>developed chinese word segmenter using manually segmented corpus.</prevsent>
<prevsent>the segmentation rules were learned automatically from this corpus.</prevsent>
</prevsection>
<citsent citstr=" P98-2206 ">
(sproat and shih, 1990) and (sun et al, 1998) <papid> P98-2206 </papid>used method that does not relyon dictionary or manually segmented corpus.</citsent>
<aftsection>
<nextsent>the characters of the unsegmented chinese text are grouped into pairs with the highest value of mutual information.
</nextsent>
<nextsent>this mutual information can be learned from an unsegmented chinese corpus.we will present new method for segmenting the chinese text without using manually segmented corpus or predefined dictionary.
</nextsent>
<nextsent>in statistical machine translation, we have bilingual corpus available, which is used to obtaina segmentation of the chinese text in the following way.
</nextsent>
<nextsent>first, we train the statistical translation models with the unsegmented bilingual corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1155">
<title id=" W04-1118.xml">do we need chinese word segmentation for statistical machine translation </title>
<section> review of the baseline system for.  </section>
<citcontext>
<prevsection>
<prevsent>sentence ei1 =e1 . . .
</prevsent>
<prevsent>ei . . .
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
ei . among all possible target language sentences, we will choose the sentence with the highest probability: ei1 = argmax ei1 {pr(ei1|fj1 ) } (1) = argmax ei1 {pr(ei1) ? pr(fj1 |ei1) } (2) the decomposition into two knowledge sources in equation 2 is known as the source-channel approach to statistical machine translation (brown et al, 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>it allows an independent modeling of target language model pr(ei1) and translation model pr(fj1 |ei1)1.
</nextsent>
<nextsent>the target language model describes the well-formedness of the target language sentence.
</nextsent>
<nextsent>the translation model links the source language sentence to the target language sentence.
</nextsent>
<nextsent>the argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1156">
<title id=" W04-1118.xml">do we need chinese word segmentation for statistical machine translation </title>
<section> review of the baseline system for.  </section>
<citcontext>
<prevsection>
<prevsent>to denote general probability distributions with (nearly) no specific assumptions.
</prevsent>
<prevsent>in contrast, for model-based probability distributions, we use the generic symbol p(?).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
a mapping from source position to target position aj . the relationship between the translation model and the alignment model is given by: pr(fj1 |ei1) = ? aj1 pr(fj1 , aj1 |ei1) (3)in this paper, we use the models ibm-1, ibm4 from (brown et al, 1993) <papid> J93-2003 </papid>and the hidden markov alignment model (hmm) from (vogel etal., 1996).<papid> C96-2141 </papid></citsent>
<aftsection>
<nextsent>all these models provide different de compositions of the probability pr(fj1 , aj1 |ei1).
</nextsent>
<nextsent>a detailed description of these models can be found in (och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>a viterbi alignment aj1 of specific model is an alignment for which the following equation holds: aj1 = argmax aj1 pr(fj1 , aj1 |ei1).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1157">
<title id=" W04-1118.xml">do we need chinese word segmentation for statistical machine translation </title>
<section> review of the baseline system for.  </section>
<citcontext>
<prevsection>
<prevsent>to denote general probability distributions with (nearly) no specific assumptions.
</prevsent>
<prevsent>in contrast, for model-based probability distributions, we use the generic symbol p(?).
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
a mapping from source position to target position aj . the relationship between the translation model and the alignment model is given by: pr(fj1 |ei1) = ? aj1 pr(fj1 , aj1 |ei1) (3)in this paper, we use the models ibm-1, ibm4 from (brown et al, 1993) <papid> J93-2003 </papid>and the hidden markov alignment model (hmm) from (vogel etal., 1996).<papid> C96-2141 </papid></citsent>
<aftsection>
<nextsent>all these models provide different de compositions of the probability pr(fj1 , aj1 |ei1).
</nextsent>
<nextsent>a detailed description of these models can be found in (och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>a viterbi alignment aj1 of specific model is an alignment for which the following equation holds: aj1 = argmax aj1 pr(fj1 , aj1 |ei1).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1158">
<title id=" W04-1118.xml">do we need chinese word segmentation for statistical machine translation </title>
<section> review of the baseline system for.  </section>
<citcontext>
<prevsection>
<prevsent>a mapping from source position to target position aj . the relationship between the translation model and the alignment model is given by: pr(fj1 |ei1) = ? aj1 pr(fj1 , aj1 |ei1) (3)in this paper, we use the models ibm-1, ibm4 from (brown et al, 1993) <papid> J93-2003 </papid>and the hidden markov alignment model (hmm) from (vogel etal., 1996).<papid> C96-2141 </papid></prevsent>
<prevsent>all these models provide different de compositions of the probability pr(fj1 , aj1 |ei1).</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
a detailed description of these models can be found in (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>a viterbi alignment aj1 of specific model is an alignment for which the following equation holds: aj1 = argmax aj1 pr(fj1 , aj1 |ei1).
</nextsent>
<nextsent>(4)the alignment models are trained on bilingual corpus using giza++(och et al, 1999;<papid> W99-0604 </papid>och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>the training is done it eratively in succession on the same data, where the final parameter estimates of simpler model serve as starting point for more complex model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1160">
<title id=" W04-1118.xml">do we need chinese word segmentation for statistical machine translation </title>
<section> review of the baseline system for.  </section>
<citcontext>
<prevsection>
<prevsent>a detailed description of these models can be found in (och and ney, 2003).<papid> J03-1002 </papid></prevsent>
<prevsent>a viterbi alignment aj1 of specific model is an alignment for which the following equation holds: aj1 = argmax aj1 pr(fj1 , aj1 |ei1).</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
(4)the alignment models are trained on bilingual corpus using giza++(och et al, 1999;<papid> W99-0604 </papid>och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>the training is done it eratively in succession on the same data, where the final parameter estimates of simpler model serve as starting point for more complex model.
</nextsent>
<nextsent>the result of the training procedure isthe viterbi alignment of the final training iteration for the whole training corpus.
</nextsent>
<nextsent>2.3 alignment template approach.
</nextsent>
<nextsent>in the translation approach from section 2.1,one disadvantage is that the contextual information is only taken into account by the language model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1166">
<title id=" W04-1118.xml">do we need chinese word segmentation for statistical machine translation </title>
<section> review of the baseline system for.  </section>
<citcontext>
<prevsection>
<prevsent>this method willbe referred to as translation with no segmen tation?
</prevsent>
<prevsent>(see section 5.2).
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
they will also go to hangzhou for visit figure 2: example of word aligned sentence pair and some possible alignment templates.in the chinese english darpa tides evaluations in june 2002 and may 2003, carried out by nist (nist, 2003), the alignment template approach performed very well and was ranked among the best translation systems.further details on the alignment template approach are described in (och et al, 1999;<papid> W99-0604 </papid> och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>in section 5.3, we will present results for chinese english translation task.
</nextsent>
<nextsent>the domain of this task is news articles.
</nextsent>
<nextsent>as bilingual training data, we use corpus composed of the english translations of chinese treebank.
</nextsent>
<nextsent>this corpus is provided by the linguistic data consortium (ldc), catalog number ldc2002e17.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1167">
<title id=" W04-0862.xml">the swarthmore college senseval3 system </title>
<section> systems.  </section>
<citcontext>
<prevsection>
<prevsent>the second system used was nave bayes classifier where the similarity between an instance, , and sense class, sj , is defined as: sim(i, sj) = (i, sj) = (sj)p (i|sj)we then choose the sense class, sj , which maximized the similarity function above, making standard independence assumptions.
</prevsent>
<prevsent>3.3 decision list.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
the final system was decision list classifier that found the log-likelihoods of the correspondence be association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems tween features and senses, using plus-one smoothing (yarowsky, 1994).<papid> P94-1013 </papid></citsent>
<aftsection>
<nextsent>the features were ordered from most to least indicative to form the decision list.
</nextsent>
<nextsent>a separate decision list was constructed for each set of lexical samples in the training data.
</nextsent>
<nextsent>for each test instance, the first matching feature found in the associated decision list was used to determine the classification of the instance.
</nextsent>
<nextsent>instances which failed to match any rule in the decision list were labeled with the most frequent sense, as calculated from the training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1168">
<title id=" W04-0862.xml">the swarthmore college senseval3 system </title>
<section> classifier combination.  </section>
<citcontext>
<prevsection>
<prevsent>we then did the same for the clustering system and the nave bayes system, yielding total of three new systems.
</prevsent>
<prevsent>these three systems were then voted together to form the final system.
</prevsent>
</prevsection>
<citsent citstr=" W04-0863 ">
the two-tiered voting was performed to ensure equal voting in the case of our joint work (wicentowski et al, 2004) <papid> W04-0863 </papid>where the five systems that needed to be combined were run on different numbers of feature subsets.</citsent>
<aftsection>
<nextsent>4.1 combination errors.
</nextsent>
<nextsent>there were two mistakes we made when voting our systems together.
</nextsent>
<nextsent>we caught one mistake after the submission deadline but before notification of re sults; the other we realized only while evaluating our systems after receiving our results.
</nextsent>
<nextsent>for this reason, there are three sets of results that we will report here:?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1170">
<title id=" W04-0862.xml">the swarthmore college senseval3 system </title>
<section> collaborative work.  </section>
<citcontext>
<prevsection>
<prevsent>their team then sent us back two sets of results: the output of their maximum entropy system and their boosting system.
</prevsent>
<prevsent>these two results were then combined with the three systems written by swarthmore college.details on this joint effort can be found in (wicen towski et al, 2004).<papid> W04-0863 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-0845 ">
in addition, the decision list system described here was used in the semantic role labeling task submitted by (ngai et al, 2004).<papid> W04-0845 </papid></citsent>
<aftsection>
<nextsent>the authors thank the following swarthmore college students for their assistance and guidance: ben mitchell 05, charles bell 06, lisa spitalewitz 06, and michael stone 07.
</nextsent>
<nextsent>their efforts as part of thefall 2003 information retrieval and natural language processing?
</nextsent>
<nextsent>class laid the foundation for our successful entry into the senseval3 competition.
</nextsent>
<nextsent>in addition, the authors express their gratitude to grace ngai, dekai wu, and all the members of their joint team, for asking us to participate in their semantic role labeling system.finally, the authors thank the organizers, especially rada mihalcea, for their support of our participation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1171">
<title id=" W04-0908.xml">carsim a system to visualize written road accident reports as animated 3d scenes </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, some people have difficulties in imagining situations and may need visual aids pre designed by professional analysts.in this paper, we will describe carsim, text-toscene converter that automates the generation of images from texts.
</prevsent>
<prevsent>the conversion of natural language texts into graphics has been investigated in few projects.
</prevsent>
</prevsection>
<citsent citstr=" P84-1106 ">
nalig (adorni et al, 1984; <papid> P84-1106 </papid>manzo et al, 1986) is an early example of them that was aimed at recreating static2d scenes.</citsent>
<aftsection>
<nextsent>one of its major goals was to study relationships between space and prepositions.
</nextsent>
<nextsent>naligconsidered simple phrases in italian of the type subject, preposition, object that inspite of their simplicity can have ambiguous interpretations.
</nextsent>
<nextsent>from whatis described in the papers, nalig has not been extended to process sentences and even less to texts.wordseye (coyne and sproat, 2001) is an impressive system that recreates 3d animated scenes from short descriptions.
</nextsent>
<nextsent>the number of 3d objectswordseye uses ? 12,000 ? gives an idea of its ambition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1172">
<title id=" W04-0908.xml">carsim a system to visualize written road accident reports as animated 3d scenes </title>
<section> carsim.  </section>
<citcontext>
<prevsection>
<prevsent>the system is limited to the visualization of single vehicle maneuvers at an intersection as the one described in this two-sentence narrative: car came from kriegstrasse.
</prevsent>
<prevsent>it turned left at the intersection.the authors give no further details on the text corpus and no precise description of the results.
</prevsent>
</prevsection>
<citsent citstr=" W01-1301 ">
carsim (egges et al, 2001; dupuy et al, 2001) <papid> W01-1301 </papid>isa program that analyzes texts describing car accidents and visualizes them in 3d environment.</citsent>
<aftsection>
<nextsent>it has been developed using real-world texts.
</nextsent>
<nextsent>the carsim architecture is divided into two parts that communicate using formal representation of input text linguistic component formal description visualizer component output animation figure 1: the carsim architecture.the accident.
</nextsent>
<nextsent>carsims first part is linguistic module that extracts information from the report and fills the frame slots.
</nextsent>
<nextsent>the second part is virtual scene generator that takes the structured representation as input, creates the visual entities, and animates them (figure 1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1174">
<title id=" W04-0908.xml">carsim a system to visualize written road accident reports as animated 3d scenes </title>
<section> the information extraction module.  </section>
<citcontext>
<prevsection>
<prevsent>[en personbil]actor korde [vid femtiden]t ime[pa? torsdagseftermiddagen]t ime [in ett rad hus]v ictim [i ett aldreboende]loc [pa? alvagen]loc [i enebyberg]loc [norr om stockholm]loc . [about five]t ime [on thursday afternoon]t ime , [a car]actor crashed [into row house]v ictim [in an old peoples home]loc [at alvagen street]loc [in enebyberg]loc [north of stockholm]loc.
</prevsent>
<prevsent>figure 4: sentence tagged with semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
gildea and jurafsky (2002) <papid> J02-3001 </papid>describe an algorithm to label automatically semantic roles in generalcontext.</citsent>
<aftsection>
<nextsent>they use the semantic frames and associated roles defined in framenet (baker et al, 1998) <papid> P98-1013 </papid>and train their classifier on the framenet corpus.</nextsent>
<nextsent>they report performance of 82 percent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1175">
<title id=" W04-0908.xml">carsim a system to visualize written road accident reports as animated 3d scenes </title>
<section> the information extraction module.  </section>
<citcontext>
<prevsection>
<prevsent>figure 4: sentence tagged with semantic roles.
</prevsent>
<prevsent>gildea and jurafsky (2002) <papid> J02-3001 </papid>describe an algorithm to label automatically semantic roles in generalcontext.</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
they use the semantic frames and associated roles defined in framenet (baker et al, 1998) <papid> P98-1013 </papid>and train their classifier on the framenet corpus.</citsent>
<aftsection>
<nextsent>they report performance of 82 percent.
</nextsent>
<nextsent>carsim uses classification algorithm similar to the one described in this paper.
</nextsent>
<nextsent>however, as there is no lexical resource such as framenet for swedish and no widely available parser, we adapted it.
</nextsent>
<nextsent>our classifier uses more local strategy as well as different set of attributes.the analysis starts from the words in our dictionary for which we designed specific set of frames and associated roles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1176">
<title id=" W04-0908.xml">carsim a system to visualize written road accident reports as animated 3d scenes </title>
<section> conclusion and perspectives.  </section>
<citcontext>
<prevsection>
<prevsent>as far as we know, carsim is the onlytext-to-scene conversion system working on non invented narratives.
</prevsent>
<prevsent>we are currently improving carsim and we hopein future work to obtain better results in the resolution of coreferences.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
we are implementing and adapting algorithms such as the one described in (soon et al, 2001) <papid> J01-4004 </papid>to handle this.</citsent>
<aftsection>
<nextsent>we also intend to improve the visualizer to handle more complex scenes and animations.the current aim of the carsim project is to visualize the content of text as accurately as possible, with no external knowledge.
</nextsent>
<nextsent>in the future, we would like to integrate additional knowledge sources in order to make the visualization more realistic and understandable.
</nextsent>
<nextsent>geographical and meteorological information systems are good examples of this, which could be helpful to improve the realism.
</nextsent>
<nextsent>another topic, which has been prominent in our discussions with traffic safety experts, is how to reconcile different narratives that describe same accident.in our work on the information extraction module, we have concentrated on the extraction of data which are relevant for the visual reconstruction ofthe scene.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1177">
<title id=" W04-2202.xml">a model for fine grained alignment of multilingual texts </title>
<section> alignment model.  </section>
<citcontext>
<prevsection>
<prevsent>9all figures are at the end of the paper.
</prevsent>
<prevsent>10see e. g.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
(marcus et al, 1994).<papid> H94-1020 </papid></citsent>
<aftsection>
<nextsent>cates and arguments and may eventually speed up the annotation process in semi-automatic way.
</nextsent>
<nextsent>but, as the examples above have shown, predicate-argument structure goes beyond the assignment of phrasal categories and grammatical functions, because the grammatical category of predicate expressions and consequently the grammatical functions of their arguments can vary considerably.
</nextsent>
<nextsent>also, the predicate-argument structure licenses the alignment relation by showing explicitly what it is based on.
</nextsent>
<nextsent>3.2 binding layer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1178">
<title id=" W03-2909.xml">a reconfigurable stochastic tagger for languages with complex tag structure </title>
<section> comments and possible extensions.  </section>
<citcontext>
<prevsection>
<prevsent>comparing with publications on similar tasks, our minimal overall error rate is twice as big as for slovene (deroski et al, 2000) and 3 times as big as for czech (hajic?
</prevsent>
<prevsent>and hladk?, 1998).
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
it has been remarked that hmm trigram taggers using single multivalued context features can perform better and run faster than maxent taggers trying to combine conditional probabilities for multitude of binary features (brants, 2000; <papid> A00-1031 </papid>megyesi, 2001), even for large structured tagsets and slavonic free word-order (hajic?</citsent>
<aftsection>
<nextsent>et al, 2001).
</nextsent>
<nextsent>our intention was to try out hybrid approach inwhich small number of multivalued context features is used.
</nextsent>
<nextsent>we have thought it can help solving data sparseness problems.
</nextsent>
<nextsent>now we do not know exactly what is the most important causeof our present high error rate: in homogenous polish corpus annotation, deficiencies of morphological analysis, low efficiency of the manual model search, or the assumption of probabilistic independence of different tag attributes (used succes fully by hajic?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1179">
<title id=" W04-2610.xml">support vector machines applied to the classification of semantic relations in nominal ized noun phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.2 previous work on the discovery of semantic.
</prevsent>
<prevsent>relations the development of large semantically annotated corpora, such as penn treebank2 and, more recently, propbank (kingsbury, et al  2002), as well as semantic knowledge bases, such as framenet (baker, fillmore, and lowe 1998), have stimulated high interest in the automatic acquisition of semantic relations, and especially of semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" A00-2031 ">
in the last few years, many researchers (blaheta and charniak 2000), (<papid> A00-2031 </papid>gildea and jurafsky 2002), (<papid> J02-3001 </papid>gildea and palmer 2002), (<papid> P02-1031 </papid>pradhan etal.</citsent>
<aftsection>
<nextsent>2003) have focused on the automatic prediction of semantic roles using statistical techniques.
</nextsent>
<nextsent>these statistical techniques operate on the output of probabilistic parsers and take advantage of the characteristic features of the semantic roles that are then employed in learning algorithm.
</nextsent>
<nextsent>while these systems focus on verb-argument semantic relations, called semantic roles, in this paper we investigate predicate-argument semantic relations in nominal ized noun phrases and present method for their automatic detection in open-text.
</nextsent>
<nextsent>1.3 approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1180">
<title id=" W04-2610.xml">support vector machines applied to the classification of semantic relations in nominal ized noun phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.2 previous work on the discovery of semantic.
</prevsent>
<prevsent>relations the development of large semantically annotated corpora, such as penn treebank2 and, more recently, propbank (kingsbury, et al  2002), as well as semantic knowledge bases, such as framenet (baker, fillmore, and lowe 1998), have stimulated high interest in the automatic acquisition of semantic relations, and especially of semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
in the last few years, many researchers (blaheta and charniak 2000), (<papid> A00-2031 </papid>gildea and jurafsky 2002), (<papid> J02-3001 </papid>gildea and palmer 2002), (<papid> P02-1031 </papid>pradhan etal.</citsent>
<aftsection>
<nextsent>2003) have focused on the automatic prediction of semantic roles using statistical techniques.
</nextsent>
<nextsent>these statistical techniques operate on the output of probabilistic parsers and take advantage of the characteristic features of the semantic roles that are then employed in learning algorithm.
</nextsent>
<nextsent>while these systems focus on verb-argument semantic relations, called semantic roles, in this paper we investigate predicate-argument semantic relations in nominal ized noun phrases and present method for their automatic detection in open-text.
</nextsent>
<nextsent>1.3 approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1181">
<title id=" W04-2610.xml">support vector machines applied to the classification of semantic relations in nominal ized noun phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.2 previous work on the discovery of semantic.
</prevsent>
<prevsent>relations the development of large semantically annotated corpora, such as penn treebank2 and, more recently, propbank (kingsbury, et al  2002), as well as semantic knowledge bases, such as framenet (baker, fillmore, and lowe 1998), have stimulated high interest in the automatic acquisition of semantic relations, and especially of semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
in the last few years, many researchers (blaheta and charniak 2000), (<papid> A00-2031 </papid>gildea and jurafsky 2002), (<papid> J02-3001 </papid>gildea and palmer 2002), (<papid> P02-1031 </papid>pradhan etal.</citsent>
<aftsection>
<nextsent>2003) have focused on the automatic prediction of semantic roles using statistical techniques.
</nextsent>
<nextsent>these statistical techniques operate on the output of probabilistic parsers and take advantage of the characteristic features of the semantic roles that are then employed in learning algorithm.
</nextsent>
<nextsent>while these systems focus on verb-argument semantic relations, called semantic roles, in this paper we investigate predicate-argument semantic relations in nominal ized noun phrases and present method for their automatic detection in open-text.
</nextsent>
<nextsent>1.3 approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1182">
<title id=" W04-2610.xml">support vector machines applied to the classification of semantic relations in nominal ized noun phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the distribution of the semantic relations is studied across different np pattern sand the similarities and differences among resulting semantic spaces are analyzed.
</prevsent>
<prevsent>a thorough understanding of the syntactic and semantic characteristics of nps provides valuable insights into defining the most representative feature vectors that ultimately drive the discriminating learning models.an important characteristic of this work is that it relies heavily on state-of-the-art natural language processing and machine learning methods.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
prior to the discovery of semantic relations, the text is syntactically parsed withcharniaks parser (charniak 2001) <papid> P01-1017 </papid>and words are semantically disambiguated and mapped into their appropriate wordnet senses.</citsent>
<aftsection>
<nextsent>the word sense disambiguation is done manually for training and automatically for testing with state-of-the-art wsd module, an improved version of system with which we have participated successfully in senseval 2 and which has an accuracy of 81% when disambiguating nouns in open-domain.
</nextsent>
<nextsent>the discovery of semantic relations is based on learning lexical, syntactic,semantic and contextual constraints that effectively identify the most probable relation for each np construction considered.
</nextsent>
<nextsent>phrases in this paper we study the behavior of semantic relation sat the noun phrase level when one of the nouns is nominalized.
</nextsent>
<nextsent>the following np level constructions are con sidered: complex nominals, genitives, adjective phrases, and adjective clauses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1183">
<title id=" W04-2610.xml">support vector machines applied to the classification of semantic relations in nominal ized noun phrases </title>
<section> nominalizations and mapping of nps.  </section>
<citcontext>
<prevsection>
<prevsent>the classification is the result of our observations of nominal ization patterns at noun phrase level.
</prevsent>
<prevsent>category the number of randomly selected sentences, the number of instances found in these sentences, and finally the number of nominal ized instances our group managed to annotate by hand.
</prevsent>
</prevsection>
<citsent citstr=" W04-2609 ">
the annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (moldovan et al  2004).<papid> W04-2609 </papid></citsent>
<aftsection>
<nextsent>inter-annotator agreement the annotators, four phd students in computational semantics worked in groups of two, each group focusing on one half of the corpus to annotate.
</nextsent>
<nextsent>besides the type of relation, the annotators were asked to provide information about the order of the modifier and the head nouns in the syntactic constructions if applicable.
</nextsent>
<nextsent>for example, owner of the car?
</nextsent>
<nextsent>and car of the owner?.the annotators were also asked to indicate if the instance was nominal ization and if yes, which of the noun constituents was derived from verb (e.g. the head noun nominal ization student protest?, or the modifier noun nominal ization working woman??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1185">
<title id=" W04-2610.xml">support vector machines applied to the classification of semantic relations in nominal ized noun phrases </title>
<section> nominalizations and mapping of nps.  </section>
<citcontext>
<prevsection>
<prevsent>the key to successful semantic classification of np constructions is the identification of their most specific lexical, syntactic, semantic and contextual features.
</prevsent>
<prevsent>we developed algorithms for finding their values automatically.
</prevsent>
</prevsection>
<citsent citstr=" C94-1042 ">
the values of these features are determined with the help of some important resources mentioned below.comlex (grishman et al 1994) <papid> C94-1042 </papid>is computational lexicon providing syntactic information for more than 38,000english headwords.</citsent>
<aftsection>
<nextsent>it contains detailed syntactic information about the attributes of each lexical item and the subcategorization frames when words have arguments.
</nextsent>
<nextsent>this last feature is the most useful for our task as the senses of verbs are clustered by the syntactic frames.
</nextsent>
<nextsent>we will use comlex in combination with verblex to map the syntactic behaviors to verb semantic classes.verblex is an in-house verb lexicon built by enriching verbnet (kipper et al  2000) with verb synsets from wordnet and verbs extracted from the semantic frames of framenet.
</nextsent>
<nextsent>it contains information about the semantic roles that can appear within class of verbs together with the selectional restrictions for their lexical realizations, syntactic subcategorization and wordnet verb senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1191">
<title id=" W04-2422.xml">learning transformation rules for semantic role labeling </title>
<section> introduction to transformation-based.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J95-4004 ">
error-driven learning for the 2004 conference on computational natural language learning (conll), our team has applied the methodology popularized by eric brill for part-of-speech tagging and linguistic parsing (brill, 1995; <papid> J95-4004 </papid>brill, 1993).</citsent>
<aftsection>
<nextsent>in this methodology, illustrated in figure 1, system learns sequence of rules that best labels training data.
</nextsent>
<nextsent>these rules are then used to annotate previously unseen data.
</nextsent>
<nextsent>according to (brill, 1995), <papid> J95-4004 </papid>transformation-based error-driven learning application is defined by: 1.</nextsent>
<nextsent>the initial annotation scheme.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1197">
<title id=" W04-2211.xml">semiautomatic construction of korean chinese verb patterns based on translation equivalency </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pbmt (pattern-based machine translation) approach has been adopted by many mt researchers, mainly due to the portability, customizability and the scala bility of the approach.
</prevsent>
<prevsent>cf.
</prevsent>
</prevsection>
<citsent citstr=" C96-2211 ">
hong et al (2003a), takeda (1996), <papid> C96-2211 </papid>watanabe &amp; takeda (1998).<papid> P98-2223 </papid></citsent>
<aftsection>
<nextsent>however, major drawback of the approach is that it is often very costly and time-consuming to construct large amount of data enough to assure the performance of the pbmt system.
</nextsent>
<nextsent>from this reason many studies from pbmt research circles have been focused on the data acquisition issue.
</nextsent>
<nextsent>most of the data acquisition studies were about automatic acquisition of lexical resources from bilingual corpus.
</nextsent>
<nextsent>since 2001, etri has developed korean chinese mt system, tellus k-c, under the auspices of the mic (ministry of information and communication) of korean government.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1198">
<title id=" W04-2211.xml">semiautomatic construction of korean chinese verb patterns based on translation equivalency </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pbmt (pattern-based machine translation) approach has been adopted by many mt researchers, mainly due to the portability, customizability and the scala bility of the approach.
</prevsent>
<prevsent>cf.
</prevsent>
</prevsection>
<citsent citstr=" P98-2223 ">
hong et al (2003a), takeda (1996), <papid> C96-2211 </papid>watanabe &amp; takeda (1998).<papid> P98-2223 </papid></citsent>
<aftsection>
<nextsent>however, major drawback of the approach is that it is often very costly and time-consuming to construct large amount of data enough to assure the performance of the pbmt system.
</nextsent>
<nextsent>from this reason many studies from pbmt research circles have been focused on the data acquisition issue.
</nextsent>
<nextsent>most of the data acquisition studies were about automatic acquisition of lexical resources from bilingual corpus.
</nextsent>
<nextsent>since 2001, etri has developed korean chinese mt system, tellus k-c, under the auspices of the mic (ministry of information and communication) of korean government.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1199">
<title id=" W04-0507.xml">a practical qa system in restricted domains </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>unstructured natural language sentences are indexed in the form of ternary expressions and stored in rdb.
</prevsent>
<prevsent>the start system covers much wider domain of questions than ours, however, it seems that the system returns more wrong answers than ours, because we extract the answer only from semi-structured documents.the jupiter system (zue et al, 2000) is conversational system that provides weather information over the phone.
</prevsent>
</prevsection>
<citsent citstr=" J92-1004 ">
based on the galaxy architecture (goddeau et al, 1994), jupiter recognizes user question over the phone, parses the question with the tina language understanding system (seneff,1992) <papid> J92-1004 </papid>and generates sql and natural language answer with the genesis system (baptist and sen eff, 2000).</citsent>
<aftsection>
<nextsent>the generated answer is synthesized with the envoice system.
</nextsent>
<nextsent>even the jupiter system deals with the same domain, ours can process bit wider-range of weather topics.
</nextsent>
<nextsent>our qa system can cover the question which requires inferences such as when is the best day for washing my car in thisweek?
</nextsent>
<nextsent>moreover, our system has an ability of inferring missing information from the user profile and the inferring algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1200">
<title id=" W04-0801.xml">the basque lexical sample task </title>
<section> setting of the exercise.  </section>
<citcontext>
<prevsection>
<prevsent>being basque minority language it is not easy to find the required number of occurrences for each word.
</prevsent>
<prevsent>we wanted to have both balanced and newspaper examples, but we also had to include texts extracted from the web, specially for the untagged corpus.
</prevsent>
</prevsection>
<citsent citstr=" C00-1001 ">
the procedure to find examples from the web was the following: for each target word all possible morphological declensions were automatically generated, searched in search engine, documents retrieved, automatically lemmatized (aduriz et al 2000), <papid> C00-1001 </papid>filtered using some heuristics to ensure quality of context, and finally filtered for pos mismatches.</citsent>
<aftsection>
<nextsent>table 1 shows the number of examples from each source.
</nextsent>
<nextsent>2.4 words chosen.
</nextsent>
<nextsent>basically, the words employed in this task are the same words used in senseval 2 (40 words, 15 nouns, 15 verbs and 10 adjectives), only the sense inventory changed.
</nextsent>
<nextsent>besides, in senseval 3 we replaced 5 verbs with new ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1202">
<title id=" W03-2008.xml">natural language analysis of patent claims </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>developing natural language analyzers for patents (with at least one or any combination of morphological, syntactic and semantic modules) is basic task.
</prevsent>
<prevsent>the ultimate task of such analysis is to build kind of possibly unambiguous content representation that could further be used to produce higher quality applications.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
broad coverage syntactic parsers with good performance have recently become available (charniak, 2000; <papid> A00-2018 </papid>collins, 2000), but they are not trained for patents.</citsent>
<aftsection>
<nextsent>semantic parsing is considerably less developed and shows trend to relyon ontologies rather then semantic primitives.
</nextsent>
<nextsent>(gnasa and woch, 2002).
</nextsent>
<nextsent>this paper reports on on-going project whose goal is to propose nlp methodology and an analyzer for patent claims.
</nextsent>
<nextsent>the claim is the focal point of patent disclosure, - it describes essential features of the invention and is the actual subject of legal protection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1203">
<title id=" W03-2008.xml">natural language analysis of patent claims </title>
<section> knowledge.  </section>
<citcontext>
<prevsection>
<prevsent>a semi-automatic super tagging procedure was used to label these lexemes with their supertags.
</prevsent>
<prevsent>super tagging is process of tagging lexemes with labels (or supertags), which code richer information than standard pos tags.
</prevsent>
</prevsection>
<citsent citstr=" C94-1024 ">
the use of supertags, as noted in (joshi and srinivas, 1994) <papid> C94-1024 </papid>localizes some crucial linguistic dependencies, and thus show significant performance gains.</citsent>
<aftsection>
<nextsent>the content of super tag differs from work to work and is tailored for the needs of an application.
</nextsent>
<nextsent>for example, joshi and srinivas (1994) <papid> C94-1024 </papid>who seem to coin this term use elementary trees of lexicalized tree adjoining grammar for super tagging lexical items.</nextsent>
<nextsent>in (gnasa and woch, 2002) it is grammatical structures of the ontology that are used as supertags.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1205">
<title id=" W04-1106.xml">character sense association and compounding template similarity automatic semantic classification of chinese compounds </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experiment reports high precision rate (about 38% in outside test and 61% in inside test) against the baseline one (about 18%).
</prevsent>
<prevsent>sense tagging is an important task in nlp.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
it is supposed to provide semantic information useful to the application tasks like ir and mt. as generally acknowledged, sense tagging is to assign certain sense to word in certain context by using semantic lexicon (yarowsky, 1992, <papid> C92-2070 </papid>wilks and stevenson, 1997).<papid> W97-0208 </papid></citsent>
<aftsection>
<nextsent>in addition to word sense disambiguation (wsd) for known words, sense determination for words unknown to the lexicon poses another challenge in sense tagging.
</nextsent>
<nextsent>this is especially the case in nlp of chinese, language rich in compound words.
</nextsent>
<nextsent>according to the data in (chen and lin, 2000), <papid> W00-1202 </papid>about 5.51% of unknown words is encountered in their sense-tagging task of chinese corpus.</nextsent>
<nextsent>instead of proper names, the cross-linguistically most common type of unknown words, compound words constitute the majority of unknown words in chinese text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1206">
<title id=" W04-1106.xml">character sense association and compounding template similarity automatic semantic classification of chinese compounds </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experiment reports high precision rate (about 38% in outside test and 61% in inside test) against the baseline one (about 18%).
</prevsent>
<prevsent>sense tagging is an important task in nlp.
</prevsent>
</prevsection>
<citsent citstr=" W97-0208 ">
it is supposed to provide semantic information useful to the application tasks like ir and mt. as generally acknowledged, sense tagging is to assign certain sense to word in certain context by using semantic lexicon (yarowsky, 1992, <papid> C92-2070 </papid>wilks and stevenson, 1997).<papid> W97-0208 </papid></citsent>
<aftsection>
<nextsent>in addition to word sense disambiguation (wsd) for known words, sense determination for words unknown to the lexicon poses another challenge in sense tagging.
</nextsent>
<nextsent>this is especially the case in nlp of chinese, language rich in compound words.
</nextsent>
<nextsent>according to the data in (chen and lin, 2000), <papid> W00-1202 </papid>about 5.51% of unknown words is encountered in their sense-tagging task of chinese corpus.</nextsent>
<nextsent>instead of proper names, the cross-linguistically most common type of unknown words, compound words constitute the majority of unknown words in chinese text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1207">
<title id=" W04-1106.xml">character sense association and compounding template similarity automatic semantic classification of chinese compounds </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition to word sense disambiguation (wsd) for known words, sense determination for words unknown to the lexicon poses another challenge in sense tagging.
</prevsent>
<prevsent>this is especially the case in nlp of chinese, language rich in compound words.
</prevsent>
</prevsection>
<citsent citstr=" W00-1202 ">
according to the data in (chen and lin, 2000), <papid> W00-1202 </papid>about 5.51% of unknown words is encountered in their sense-tagging task of chinese corpus.</citsent>
<aftsection>
<nextsent>instead of proper names, the cross-linguistically most common type of unknown words, compound words constitute the majority of unknown words in chinese text.
</nextsent>
<nextsent>according to chen and chen (2000), <papid> C00-1026 </papid>the three most dominant types of chinese unknown words are: compound nouns (about 51%), compound verbs (about 34%), and proper names (about 15%).</nextsent>
<nextsent>while the identification and classification of proper names is an issue already well discussed in chinese nlp researches, the sense determination of unknown compounds remains subject relatively less tackled.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1208">
<title id=" W04-1106.xml">character sense association and compounding template similarity automatic semantic classification of chinese compounds </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>according to the data in (chen and lin, 2000), <papid> W00-1202 </papid>about 5.51% of unknown words is encountered in their sense-tagging task of chinese corpus.</prevsent>
<prevsent>instead of proper names, the cross-linguistically most common type of unknown words, compound words constitute the majority of unknown words in chinese text.</prevsent>
</prevsection>
<citsent citstr=" C00-1026 ">
according to chen and chen (2000), <papid> C00-1026 </papid>the three most dominant types of chinese unknown words are: compound nouns (about 51%), compound verbs (about 34%), and proper names (about 15%).</citsent>
<aftsection>
<nextsent>while the identification and classification of proper names is an issue already well discussed in chinese nlp researches, the sense determination of unknown compounds remains subject relatively less tackled.
</nextsent>
<nextsent>1.1 shallow vs. deep classification.
</nextsent>
<nextsent>while word sense might be conceptually vague and controversial in linguistics and difficult to define (manning and schtze, 1999), sense tag is more concrete and can be defined according to the specific need of the nlp tasks in question.
</nextsent>
<nextsent>for example, in task of semantic tagging or classification, sense tag can be the semantic class from thesaurus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1219">
<title id=" W04-1106.xml">character sense association and compounding template similarity automatic semantic classification of chinese compounds </title>
<section> compound sense determination.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 compound similarity.
</prevsent>
<prevsent>as critical technique, word similarity is generally used in the example-based models of semantic classification.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
the measure of word similarity can be divided into two major approaches: taxonomy-based lexical approach (resnik 1995, lin 1998<papid> P98-2127 </papid>a, chen and chen 1998) and context-based syntactic approach (lin 1998<papid> P98-2127 </papid>b,chen and you 2002), which is not the concern in this context-free model.</citsent>
<aftsection>
<nextsent>however, two problems arise here for the taxonomy-based lexical approach.
</nextsent>
<nextsent>first, such similarity measures risk the failure to capture the similarity among some semantically highly related words, if they happen to be put under classes distant from each other according to specific ontology 4 . second, as mentioned, the appropriate senses of some characters just cannot be found in the thesaurus.
</nextsent>
<nextsent>one major reason why dictionaries do not include certain character senses is that many of such characters are used in contemporary chinese only as bound morphemes not as free words, when the senses in question are involved.
</nextsent>
<nextsent>however, such senses could be kept in the compounds in the lexicon, so they might be covert but not inextricable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1223">
<title id=" W04-1106.xml">character sense association and compounding template similarity automatic semantic classification of chinese compounds </title>
<section> compound sense determination.  </section>
<citcontext>
<prevsection>
<prevsent>when native-speaker is capable of giving synonyms to word, he is considered to understand the meaning of that word.
</prevsent>
<prevsent>in fact, such way of sense capturing is also reflected in how the senses of words can be explained in many dictionaries6.
</prevsent>
</prevsection>
<citsent citstr=" J03-2001 ">
moreover, as some researches propose, synonyms can be used to construct the semantic space forgiven word (ploux and victorri, 1998, ploux and ji, 2003).<papid> J03-2001 </papid></citsent>
<aftsection>
<nextsent>in such semantic space, each synonym with different nuance occupies certain area.
</nextsent>
<nextsent>as visually reflected in this approach, retrieving proper set of its synonyms means the ability to well capture the senses of word.
</nextsent>
<nextsent>in fact, my model of automatic sense determination for compound is exactly built upon the retrieval of its near synonyms, the t-similar compounds as previously described.
</nextsent>
<nextsent>2.4 model representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1224">
<title id=" W04-2604.xml">using prepositions to extend a verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sets of syntactic frames are associated with each verb class, and speci prepositions are often listed as well.
</prevsent>
<prevsent>we are interested in evaluating how well our lexicon predicts syntactic frames in naturally occurring data.
</prevsent>
</prevsection>
<citsent citstr=" C00-2148 ">
this will give us an estimate of its likely usefulness in extending the coverage of systems trained on one genre to other genres.this paper presents comparison between our hierarchical verb lexicon, verbnet [kipper et al 2000,dang et al 2000], <papid> C00-2148 </papid>and corpus annotated semantically with predicate-argument structure, propbank [kingsbury and palmer2002].</citsent>
<aftsection>
<nextsent>we brie describe an experiment which established baseline for the syntactic coverage of the verb lexicon and more extensively we compare and discuss the preposition mismatches found while doing this evaluation.
</nextsent>
<nextsent>we used this experiment, which used almost 50,000 verb instances, to measure how well the linguistic intuitions motivating our verb lexicon are attested to in the actual data.
</nextsent>
<nextsent>it allowed us to determine whichof the expected syntactic frames and speci prepositions occur and which do not, and also look for unexpected occurrences.
</nextsent>
<nextsent>although prepositions are generally described as restrictions on syntax, theirsigni cance goes far beyond that of syntactic restriction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1225">
<title id=" W04-2604.xml">using prepositions to extend a verb lexicon </title>
<section> verbnet components.  </section>
<citcontext>
<prevsection>
<prevsent>each verb class is completely described by the set of its members (each verb has links to the appropriate senses in wordnet, thematic roles for the predicate-argument structure of the members, selectional restrictions on these arguments to express preferred argument types, and frames.
</prevsent>
<prevsent>each frame consists of brief description, an example, syntactic description corresponding to one of levin alternations, and set of semantic predicates.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
in addition, each predicate has time function to show at what stage of the event the predicate holds true, in manner similar to the event decomposition of moens and steedman (1988) . <papid> J88-2003 </papid>in order for the members of each class to be coherent with respect to the thematic roles, selectional restrictions, syntactic frames, and semantics they allow, we re ned the original levin classes and added 74 new subclasses.</citsent>
<aftsection>
<nextsent>verbnet broad-coverage, with explicit syntax and semantics, attempts to address several gaps present in other resources.
</nextsent>
<nextsent>wordnet was designed mainly as semantic network, and contains little syntactic information.
</nextsent>
<nextsent>verbnet, in contrast, includes explicit predicate argument structures for verbs intheir classes, as well as way to systematically extend those senses based on the semantics of each class.
</nextsent>
<nextsent>framenet [baker et al 1998] <papid> P98-1013 </papid>and verbnet both contain the notion of verb groupings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1226">
<title id=" W04-2604.xml">using prepositions to extend a verb lexicon </title>
<section> verbnet components.  </section>
<citcontext>
<prevsection>
<prevsent>wordnet was designed mainly as semantic network, and contains little syntactic information.
</prevsent>
<prevsent>verbnet, in contrast, includes explicit predicate argument structures for verbs intheir classes, as well as way to systematically extend those senses based on the semantics of each class.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
framenet [baker et al 1998] <papid> P98-1013 </papid>and verbnet both contain the notion of verb groupings.</citsent>
<aftsection>
<nextsent>the groupings in framenet however are based solely on the semantic roles shared by the members of class.
</nextsent>
<nextsent>these members do not need to have the same set of syntactic frames, and lack explicit semantics other than what is provided by the semantic labels.
</nextsent>
<nextsent>unlike verbnet, which uses small set of thematic roles for all classes, framenet uses frame elements which are particular to lexical item or to small groups of frames.
</nextsent>
<nextsent>besides, one of the bene ts of constructing general lexicon like verbnet is that it allows one to extend the coverage of resources tied to speci corpora.the syntactic frames in verbnet describe the surface realization for constructions such as transitive, in transitive, prepositional phrases, resultatives, and large set of levin alternations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1227">
<title id=" W04-0845.xml">semantic role labeling with boosting svms maximum entropy snow and decision lists </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the systems represent diverse array of machine learning algorithms, from decision lists to svms to winnow-type networks.
</prevsent>
<prevsent>semantic role labeling (srl) is task that has recently received lot of attention in the nlp community.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the srl task in senseval-3 used the framenet (baker et al, 1998) <papid> P98-1013 </papid>corpus: given sentence instance from the corpus, systems job would be to identify the phrase constituents and their corresponding role.</citsent>
<aftsection>
<nextsent>the senseval-3 task was divided into restricted and non-restricted subtasks.
</nextsent>
<nextsent>in the non-restricted subtask, any and all of the gold standard annotations contained in the framenet corpus could be used.
</nextsent>
<nextsent>since this includes information on the boundaries of the parse constituents which correspond to some frame element, this effectively maps the srl task to that of role-labeling classification task: given constituent parse, identify the frame element that it belongs to.
</nextsent>
<nextsent>due to the lack of time and resources, we chose to participate only in the non-restricted subtask.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1228">
<title id=" W04-0845.xml">semantic role labeling with boosting svms maximum entropy snow and decision lists </title>
<section> experimental features.  </section>
<citcontext>
<prevsection>
<prevsent>this section describes the features that were used for the srl task.
</prevsent>
<prevsent>since the non-restricted srl taskis essentially classification task, each parse constituent that was known to correspond to frame element was considered to be sample.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
the features that we used for each sample have been previously shown to be helpful for the srl task (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>some of these features can be obtained directly from the framenet annotations: ? the name of the frame.?
</nextsent>
<nextsent>the lexical unit of the sentence ? i.e. the lexical identity of the target word in the sentence.
</nextsent>
<nextsent>the general part-of-speech tag of the target word.
</nextsent>
<nextsent>the phrase type?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1229">
<title id=" W04-0845.xml">semantic role labeling with boosting svms maximum entropy snow and decision lists </title>
<section> experimental features.  </section>
<citcontext>
<prevsection>
<prevsent>the grammatical function?
</prevsent>
<prevsent>(e.g. subject, object, modifier, etc) of the constituent, with respect to the target word.?
</prevsent>
</prevsection>
<citsent citstr=" N01-1006 ">
the position (e.g. before, after) of the constituent, with respect to the target word.in addition to the above features, we also extracted set of features which required the use of some statistical nlp tools: ? transit ivity and voice of the target word ? the sentence was first part-of-speech tagged and chunked with the fntbl transformation based learning tools (ngai and florian, 2001).<papid> N01-1006 </papid></citsent>
<aftsection>
<nextsent>simple heuristics were then used to deduce the transit ivity voice of the target word.
</nextsent>
<nextsent>head word (and its part-of-speech tag) of the constituent ? after pos tagging, syntactic parser (collins, 1997) <papid> P97-1003 </papid>was then used to obtain the parse tree for the sentence.</nextsent>
<nextsent>the head word (and the pos tag of the head word) of association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems the syntactic parse constituent whose span corresponded most closely to the candidate constituent was then assumed to be the head word of the candidate constituent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1230">
<title id=" W04-0845.xml">semantic role labeling with boosting svms maximum entropy snow and decision lists </title>
<section> experimental features.  </section>
<citcontext>
<prevsection>
<prevsent>the position (e.g. before, after) of the constituent, with respect to the target word.in addition to the above features, we also extracted set of features which required the use of some statistical nlp tools: ? transit ivity and voice of the target word ? the sentence was first part-of-speech tagged and chunked with the fntbl transformation based learning tools (ngai and florian, 2001).<papid> N01-1006 </papid></prevsent>
<prevsent>simple heuristics were then used to deduce the transit ivity voice of the target word.</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
head word (and its part-of-speech tag) of the constituent ? after pos tagging, syntactic parser (collins, 1997) <papid> P97-1003 </papid>was then used to obtain the parse tree for the sentence.</citsent>
<aftsection>
<nextsent>the head word (and the pos tag of the head word) of association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems the syntactic parse constituent whose span corresponded most closely to the candidate constituent was then assumed to be the head word of the candidate constituent.
</nextsent>
<nextsent>the resulting training dataset consisted of 51,366constituent samples with total of 151 frame element types.
</nextsent>
<nextsent>these ranged from descriptor?
</nextsent>
<nextsent>(3520constituents) to baggage?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1232">
<title id=" W04-0845.xml">semantic role labeling with boosting svms maximum entropy snow and decision lists </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>our system was constructed around the boostexter software (schapire and singer, 2000), which imple model prec.
</prevsent>
<prevsent>recall attempted single model 0.891 0.795 89.2% frame separated 0.894 0.798 89.2% baseline 0.444 0.396 89.2% table 1: boosting models: validation set results ments boosting on top of decision stumps (decision trees of one level), and was originally designed fortext classification.
</prevsent>
</prevsection>
<citsent citstr=" W04-0822 ">
the same system also participated in the senseval-3 lexical sample tasks for chinese and english, as well as the multilingual lexical sample task (carpuat et al, 2004).<papid> W04-0822 </papid>table 1 compares the results of training one single overall boosting model (single) versus training separate models for each frame (frame).</citsent>
<aftsection>
<nextsent>it can be seen that training frame-specific models produces small improvement over the single model.
</nextsent>
<nextsent>theframe-specific model was used in all of the ensemble systems, and was also entered into the competition as an individual system (hkpust-boost).
</nextsent>
<nextsent>3.2 support vector machines.
</nextsent>
<nextsent>the second of our individual systems was based on support vector machines, and implemented using the tinysvm software package (boser et al, 1992).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1236">
<title id=" W04-0845.xml">semantic role labeling with boosting svms maximum entropy snow and decision lists </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>both systems were eventually used in the some of the ensemble models but were not submitted as individual contestants.
</prevsent>
<prevsent>3.4 snow.
</prevsent>
</prevsection>
<citsent citstr=" W99-0621 ">
the fourth of our individual systems is based on snow ? sparse network of winnows (munoz et al., 1999).<papid> W99-0621 </papid>the development approach for the snow models was similar to that of the boosting models.</citsent>
<aftsection>
<nextsent>twomain model types were generated: one which generated single overall model for all the possible frame elements, and one which generated one model per frame type.
</nextsent>
<nextsent>due to bug in the coding which was not discovered until the last minute, however, the results for the frame-separated model were invalidated.
</nextsent>
<nextsent>the single model system was eventually used in some of the ensemble systems, but not entered as an official contestant.
</nextsent>
<nextsent>table 4 shows the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1237">
<title id=" W04-0845.xml">semantic role labeling with boosting svms maximum entropy snow and decision lists </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>system prec.
</prevsent>
<prevsent>recall attempted single model 0.764 0.682 89.2% baseline 0.444 0.396 89.2% table 4: snow models: validation set results 3.5 decision lists.
</prevsent>
</prevsection>
<citsent citstr=" W04-0862 ">
the final individual system was decision list implementation contributed from the swarthmore college team (wicentowski et al, 2004), <papid> W04-0862 </papid>which participated in some of the lexical sample tasks.the swarthmore team followed the frame separated approach in building the decision list models.</citsent>
<aftsection>
<nextsent>table 5 shows the result on the validation set.
</nextsent>
<nextsent>this system participated in some of the final ensemble systems as well as being an official participant (hkpust-swat-dl).
</nextsent>
<nextsent>system prec.
</nextsent>
<nextsent>recall attempted dl 0.837 0.747 89.2% baseline 0.444 0.396 89.2%table 5: decision list models: validation set results 3.6 ensemble systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1238">
<title id=" W04-0906.xml">question answering using onto logical semantics </title>
<section> the environment for qa.  </section>
<citcontext>
<prevsection>
<prevsent>the final module in the system answer formulation.
</prevsent>
<prevsent>the current step toward this functionality, albeit not yet in fully automatic way.
</prevsent>
</prevsection>
<citsent citstr=" W04-0905 ">
at this point, we relyon tmrs that are obtained automatically but improved through human interaction (see nirenburg et al  2004 <papid> W04-0905 </papid>for details).</citsent>
<aftsection>
<nextsent>note that fully automatic methods for creating structured knowledge of quality even remotely approaching that needed to support realistic qa do not at this point exist.
</nextsent>
<nextsent>few of the numerous current and recent machine learning and statistical processing experiments in nlp deal with the analysis of meaning at all; and those that do address partial tasks (e.g., determining case role fillers in terms of undisambiguated text elements in gildea and jurafsky 2002) <papid> J02-3001 </papid>in rather knowledge-lean?</nextsent>
<nextsent>manner.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1239">
<title id=" W04-0906.xml">question answering using onto logical semantics </title>
<section> the environment for qa.  </section>
<citcontext>
<prevsection>
<prevsent>at this point, we relyon tmrs that are obtained automatically but improved through human interaction (see nirenburg et al  2004 <papid> W04-0905 </papid>for details).</prevsent>
<prevsent>note that fully automatic methods for creating structured knowledge of quality even remotely approaching that needed to support realistic qa do not at this point exist.</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
few of the numerous current and recent machine learning and statistical processing experiments in nlp deal with the analysis of meaning at all; and those that do address partial tasks (e.g., determining case role fillers in terms of undisambiguated text elements in gildea and jurafsky 2002) <papid> J02-3001 </papid>in rather knowledge-lean?</citsent>
<aftsection>
<nextsent>manner.
</nextsent>
<nextsent>the results are very far away indeed from either good quality or good coverage, either in terms of phenomena and text.
</nextsent>
<nextsent>we believe that our approach, using as it does statistical as well as recorded-knowledge evidence for extracting, representing and manipulating meaning is the most practical and holds the most promise for the future.
</nextsent>
<nextsent>indeed, it is not even as expensive as many people believe.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1242">
<title id=" W04-0906.xml">question answering using onto logical semantics </title>
<section> the knowledge support infrastructure.  </section>
<citcontext>
<prevsection>
<prevsent>at the time of this writing, the ontology contains about 6,000 concepts (events, objects and properties), with, on average, 16 properties each.
</prevsent>
<prevsent>temporally and cau sally related events are encoded as values of complex events has-event-as-part property.
</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
these are essentially scripts that provide information that is very useful in general reasoning as well as reasoning for nlp (e.g., schank and abelson 1977, lin and hovy 2000, <papid> C00-1072 </papid>clark and porter 2000).</citsent>
<aftsection>
<nextsent>we use scripts in the answer content determination module of the question answering system.
</nextsent>
<nextsent>figure 4 illustrates rather simple script that supports reasoning for our example question answering session.
</nextsent>
<nextsent>the ontosem lexicon contains not only semantic information, it also supports morphological and syntactic analysis.
</nextsent>
<nextsent>semantically, it specifies what concept, concepts, property or properties of concepts defined in the ontology must be instantiated in the tmr to account for the meaning of given lexical unit of input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1243">
<title id=" W04-0307.xml">a statistical constraint dependency grammar cdg parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using tight integration of multiple knowledge sources, together with distance modeling and synergistic dependencies, this parser achieves parsing accuracy comparable to several state-of-the-art context-free grammar (cfg) based statistical parsers using dependency-based evaluation metric.
</prevsent>
<prevsent>factors contributing to the scdg parsers performance are analyzed.
</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
statistical parsing has been an important focus of recent research (magerman, 1995; <papid> P95-1037 </papid>eisner, 1996; charniak, 1997; collins, 1999; ratnaparkhi, 1999;charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>several of these parsers generate constituents by conditioning probabilities on non-terminal labels, part-of-speech (pos) tags, andsome headword information (collins, 1999; ratnaparkhi, 1999; charniak, 2000).<papid> A00-2018 </papid></nextsent>
<nextsent>they utilizenon-terminals that go beyond the level of single word and do not explicitly use lexical fea tures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1244">
<title id=" W04-0307.xml">a statistical constraint dependency grammar cdg parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using tight integration of multiple knowledge sources, together with distance modeling and synergistic dependencies, this parser achieves parsing accuracy comparable to several state-of-the-art context-free grammar (cfg) based statistical parsers using dependency-based evaluation metric.
</prevsent>
<prevsent>factors contributing to the scdg parsers performance are analyzed.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
statistical parsing has been an important focus of recent research (magerman, 1995; <papid> P95-1037 </papid>eisner, 1996; charniak, 1997; collins, 1999; ratnaparkhi, 1999;charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>several of these parsers generate constituents by conditioning probabilities on non-terminal labels, part-of-speech (pos) tags, andsome headword information (collins, 1999; ratnaparkhi, 1999; charniak, 2000).<papid> A00-2018 </papid></nextsent>
<nextsent>they utilizenon-terminals that go beyond the level of single word and do not explicitly use lexical fea tures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1252">
<title id=" W04-0307.xml">a statistical constraint dependency grammar cdg parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>collins?
</prevsent>
<prevsent>model 2 parser (1999) learns the distinction between complements and adjuncts byusing heuristics during training, distinguishes complement and adjunct non-terminals, and includesa probabilistic choice of left and right subcategorization frames, while his model 3 parser uses gap features to model wh-movement.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
charniak(charniak, 2000) <papid> A00-2018 </papid>developed state-of-the-art statistical cfg parser and then built an effective language model based on it (charniak, 2001).<papid> P01-1017 </papid></citsent>
<aftsection>
<nextsent>buthis parser and language model were originally designed to analyze complete sentences.
</nextsent>
<nextsent>among the statistical dependency grammar parsers, eisners (1996) best probabilistic dependency model used unlabeled links between words and their heads, as well as between words and their complements and adjuncts.
</nextsent>
<nextsent>however, the parser does not distinguish between complements and adjuncts or model wh movement.
</nextsent>
<nextsent>collins?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1253">
<title id=" W04-0307.xml">a statistical constraint dependency grammar cdg parser </title>
<section> cdg parsing.  </section>
<citcontext>
<prevsection>
<prevsent>px and mx([r]) represent the position of word and its modifiee (for role r), respectively.
</prevsent>
<prevsent>note that cdg parse information can be easily lexicalized at the word level.
</prevsent>
</prevsection>
<citsent citstr=" W02-1031 ">
this lexicalization isable to include not only lexical category and syntactic constraints, but also rich set of lexical features to model subcategorization and wh-movement without combinatorial explosion of the parametric space (wang and harper, 2002).<papid> W02-1031 </papid></citsent>
<aftsection>
<nextsent>cdg can distinguish between adjuncts and complements due to the use of need roles (harper and helzerman, 1995), is more powerful than cfg, and has the ability to model languages with crossing dependencies and free word ordering (hence, this research could be applicable to wide variety of languages).
</nextsent>
<nextsent>an almost-parsing lm based on cdg has been developed in (wang and harper, 2002).<papid> W02-1031 </papid></nextsent>
<nextsent>theun derlying hidden event of this lm is superarv.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1264">
<title id=" W04-0307.xml">a statistical constraint dependency grammar cdg parser </title>
<section> evaluation and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>5.2 comparing to other parsers.
</prevsent>
<prevsent>charniaks state-of-the-art pcfg parser (charniak, 2000) <papid> A00-2018 </papid>has achieved the highest parseval lp/lr when compared to collins?</prevsent>
</prevsection>
<citsent citstr=" J01-2004 ">
model 2 and model3 (collins, 1999), roarks (roark, 2001), <papid> J01-2004 </papid>ratna parkhis (ratnaparkhi, 1999), and xu &amp; chelbas(xu et al , 2002) <papid> P02-1025 </papid>parsers.</citsent>
<aftsection>
<nextsent>hence, we will compare our best loosely integrated and tightly integrated scdg parsers to charniaks parser.
</nextsent>
<nextsent>additionally, we will compare with collins?
</nextsent>
<nextsent>model table 3: results on section 23 of the wsj penn tree bank comparing models that utilize different superarvtaggers and n-best sizes with the tightly coupled implementation.
</nextsent>
<nextsent>note denotes loose coupling and denotes tight coupling.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1265">
<title id=" W04-0307.xml">a statistical constraint dependency grammar cdg parser </title>
<section> evaluation and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>5.2 comparing to other parsers.
</prevsent>
<prevsent>charniaks state-of-the-art pcfg parser (charniak, 2000) <papid> A00-2018 </papid>has achieved the highest parseval lp/lr when compared to collins?</prevsent>
</prevsection>
<citsent citstr=" P02-1025 ">
model 2 and model3 (collins, 1999), roarks (roark, 2001), <papid> J01-2004 </papid>ratna parkhis (ratnaparkhi, 1999), and xu &amp; chelbas(xu et al , 2002) <papid> P02-1025 </papid>parsers.</citsent>
<aftsection>
<nextsent>hence, we will compare our best loosely integrated and tightly integrated scdg parsers to charniaks parser.
</nextsent>
<nextsent>additionally, we will compare with collins?
</nextsent>
<nextsent>model table 3: results on section 23 of the wsj penn tree bank comparing models that utilize different superarvtaggers and n-best sizes with the tightly coupled implementation.
</nextsent>
<nextsent>note denotes loose coupling and denotes tight coupling.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1274">
<title id=" W04-2405.xml">co training and self training for word sense disambiguation </title>
<section> co-training and self-training for natural.  </section>
<citcontext>
<prevsection>
<prevsent>from this newly annotated data, the most confident predictions are sought, and subsequently added to the set of labeled data.
</prevsent>
<prevsent>the process may continue for several iterations.
</prevsent>
</prevsection>
<citsent citstr=" N01-1023 ">
in natural language learning, co-training was applied to statistical parsing (sarkar, 2001), <papid> N01-1023 </papid>reference resolution (ng and cardie, 2003), <papid> N03-1023 </papid>part of speech tagging (clark et al., 2003), <papid> W03-0407 </papid>and others, and was generally found to bring improvement over the case when no additional unlabeled data are used.one important aspect of co-training consists in there lation between the views used in learning.</citsent>
<aftsection>
<nextsent>in the original definition of co-training, (blum and mitchell, 1998) state conditional independence of the views as required criterion for co-training to work.
</nextsent>
<nextsent>in recent work, (abney,2002) <papid> P02-1046 </papid>shows that the independence assumption can be relaxed, and co-training is still effective under weaker independence assumption.</nextsent>
<nextsent>he is proposing greedy algorithm to maximize agreement on un labelled data, which produces good results in co-training experiment for named entity classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1275">
<title id=" W04-2405.xml">co training and self training for word sense disambiguation </title>
<section> co-training and self-training for natural.  </section>
<citcontext>
<prevsection>
<prevsent>from this newly annotated data, the most confident predictions are sought, and subsequently added to the set of labeled data.
</prevsent>
<prevsent>the process may continue for several iterations.
</prevsent>
</prevsection>
<citsent citstr=" N03-1023 ">
in natural language learning, co-training was applied to statistical parsing (sarkar, 2001), <papid> N01-1023 </papid>reference resolution (ng and cardie, 2003), <papid> N03-1023 </papid>part of speech tagging (clark et al., 2003), <papid> W03-0407 </papid>and others, and was generally found to bring improvement over the case when no additional unlabeled data are used.one important aspect of co-training consists in there lation between the views used in learning.</citsent>
<aftsection>
<nextsent>in the original definition of co-training, (blum and mitchell, 1998) state conditional independence of the views as required criterion for co-training to work.
</nextsent>
<nextsent>in recent work, (abney,2002) <papid> P02-1046 </papid>shows that the independence assumption can be relaxed, and co-training is still effective under weaker independence assumption.</nextsent>
<nextsent>he is proposing greedy algorithm to maximize agreement on un labelled data, which produces good results in co-training experiment for named entity classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1278">
<title id=" W04-2405.xml">co training and self training for word sense disambiguation </title>
<section> co-training and self-training for natural.  </section>
<citcontext>
<prevsection>
<prevsent>from this newly annotated data, the most confident predictions are sought, and subsequently added to the set of labeled data.
</prevsent>
<prevsent>the process may continue for several iterations.
</prevsent>
</prevsection>
<citsent citstr=" W03-0407 ">
in natural language learning, co-training was applied to statistical parsing (sarkar, 2001), <papid> N01-1023 </papid>reference resolution (ng and cardie, 2003), <papid> N03-1023 </papid>part of speech tagging (clark et al., 2003), <papid> W03-0407 </papid>and others, and was generally found to bring improvement over the case when no additional unlabeled data are used.one important aspect of co-training consists in there lation between the views used in learning.</citsent>
<aftsection>
<nextsent>in the original definition of co-training, (blum and mitchell, 1998) state conditional independence of the views as required criterion for co-training to work.
</nextsent>
<nextsent>in recent work, (abney,2002) <papid> P02-1046 </papid>shows that the independence assumption can be relaxed, and co-training is still effective under weaker independence assumption.</nextsent>
<nextsent>he is proposing greedy algorithm to maximize agreement on un labelled data, which produces good results in co-training experiment for named entity classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1279">
<title id=" W04-2405.xml">co training and self training for word sense disambiguation </title>
<section> co-training and self-training for natural.  </section>
<citcontext>
<prevsection>
<prevsent>in natural language learning, co-training was applied to statistical parsing (sarkar, 2001), <papid> N01-1023 </papid>reference resolution (ng and cardie, 2003), <papid> N03-1023 </papid>part of speech tagging (clark et al., 2003), <papid> W03-0407 </papid>and others, and was generally found to bring improvement over the case when no additional unlabeled data are used.one important aspect of co-training consists in there lation between the views used in learning.</prevsent>
<prevsent>in the original definition of co-training, (blum and mitchell, 1998) state conditional independence of the views as required criterion for co-training to work.</prevsent>
</prevsection>
<citsent citstr=" P02-1046 ">
in recent work, (abney,2002) <papid> P02-1046 </papid>shows that the independence assumption can be relaxed, and co-training is still effective under weaker independence assumption.</citsent>
<aftsection>
<nextsent>he is proposing greedy algorithm to maximize agreement on un labelled data, which produces good results in co-training experiment for named entity classification.
</nextsent>
<nextsent>moreover, (clark et al, 2003)<papid> W03-0407 </papid>show that naive co-training process that does not explicitly seek to maximize agreement on un labelled data canlead to similar performance, at much lower computational cost.</nextsent>
<nextsent>in this work, we apply co-training by identifying two different feature sets based on local versus topi cal? feature split, which represent potentially independent views for word sense classification, as shown in section 4.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1286">
<title id=" W04-2405.xml">co training and self training for word sense disambiguation </title>
<section> supervised word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>these include surrounding words and their part of speech tags, collocations, keywords in contexts.
</prevsent>
<prevsent>more recently, other possible features have been investigated: bigrams, named entities, syntactic features, semantic relations with other words in context.
</prevsent>
</prevsection>
<citsent citstr=" C02-1039 ">
table 1lists commonly used features in word sense disambiguation (list drawn from larger set of features compiled by (mihalcea, 2002)).<papid> C02-1039 </papid></citsent>
<aftsection>
<nextsent>3.3 supervised learning for word sense.
</nextsent>
<nextsent>disambiguation related work in supervised word sense disambigua tions includes experiments with variety of learning algorithms, with varying degrees of success, including bayesian learning, decision trees, decision lists, memory based learning, and others.
</nextsent>
<nextsent>(yarowsky and florian, 2002) give comprehensive examination of learning methods and their combination.
</nextsent>
<nextsent>3.4 basic classifiers for word sense disambiguation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1287">
<title id=" W04-2405.xml">co training and self training for word sense disambiguation </title>
<section> co-training and self-training for word.  </section>
<citcontext>
<prevsection>
<prevsent>there are three words (chair, holiday, spade) for which no improvement could be obtained with either co-trainingor self-training, and therefore no optimal setting is indicated.
</prevsent>
<prevsent>these are among the four words with the best performing basic classifier (baseline higher than 75%).
</prevsent>
</prevsection>
<citsent citstr=" W01-0501 ">
thefact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (pierce and cardie,2001).<papid> W01-0501 </papid></citsent>
<aftsection>
<nextsent>note that even very weak classifiers, with preci sions below 40%, can still be improved, sometimes with as much as 45% error reduction (e.g. the classifier for feeling).there are no clear commonalities between the parameters leading to maximum precision for different classifiers.
</nextsent>
<nextsent>some classifiers benefit more from an aggressive?
</nextsent>
<nextsent>augmentation of the training data with new examples ? for instance the self-trained classifier for nature achieves its highest peak for growth size of 200 from pool of 500 examples.
</nextsent>
<nextsent>others instead work better by taking short steps?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1294">
<title id=" W04-2508.xml">experiments with interactive question answering in complex scenarios </title>
<section> domain-dependent complex questions.  </section>
<citcontext>
<prevsection>
<prevsent>for most state-of-the-art q/a systems, correct answers are returned iff the system identifies the correct answer type from the syntax and semantics of the question itself.
</prevsent>
<prevsent>although current answer-type hierarchies can be fairly broad in their coverage of concepts, they do not provide an exhaustive treatment of all of the types of information that users can request for any particular domain.
</prevsent>
</prevsection>
<citsent citstr=" N03-1022 ">
in lccs current q/a system (harabagiu, moldovan, et al, 2003), <papid> N03-1022 </papid>no answer type could be detected for questions like what business was the source of john d. rockefellers fortune?</citsent>
<aftsection>
<nextsent>(trec-1909) or what 1857 u.s. supreme court decision denied that blacks were citizens?
</nextsent>
<nextsent>(trec-2259).
</nextsent>
<nextsent>the failure of our system to return answer types for these questions was attributed to identifiable gaps in our semantic ontology of answer types.
</nextsent>
<nextsent>by revising our answer type hierarchy to include classes of businesses or supreme court decisions, we could presumably enable our system to identify viable answer type for each ofthese questions, and thereby improve our chances of returning correct answer to the user.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1295">
<title id=" W04-0403.xml">what is at stake a case study of russian expressions starting with a preposition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>then we analyse the results ofthis data collection and estimate the efficiency of the collected list for the resolution of morphosyntactic and semantic ambiguity in corpus.
</prevsent>
<prevsent>computational research on multiword expressions(mwes) has mostly addressed the topic for english (sag et al, 2001).
</prevsent>
</prevsection>
<citsent citstr=" W00-1219 ">
some research has dealt with other languages, such as french (michiels and dufour, 1998) or chinese (zhang et al, 2000),<papid> W00-1219 </papid>but there has been no computationally tractable research on the topic for russian.</citsent>
<aftsection>
<nextsent>what is more, the study of mwes in english has been mostly devoted to the description of nominal groups or light verbs, e.g.
</nextsent>
<nextsent>(calzolari et al, 2002), (sag et al,2001), while constructions starting with preposition, such as in line, at large, have not been the focus of attention.
</nextsent>
<nextsent>even though the tradition of studying russian idiomatic expressions resulted in many descriptions of russian idioms and phraseological dictionaries, like (dobrovolskij, 2000) or (fedorov,1995), the studies and dictionaries often concentrate on non-decomposable colourful expressions of the kick-the-bucket?
</nextsent>
<nextsent>type, such as byt?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1296">
<title id=" W04-0403.xml">what is at stake a case study of russian expressions starting with a preposition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the former approaches assume syntactic parsing of source texts (sometimes shallow, sometimes deep to identify the semantic rolesof mwe components) and the ability to get information from thesaurus.
</prevsent>
<prevsent>detection results can be further improved by deep semantic analysis of second acl workshop on multiword expressions: integrating processing, july 2004, pp.
</prevsent>
</prevsection>
<citsent citstr=" W03-1807 ">
17-23 source texts (piao et al, 2003).<papid> W03-1807 </papid></citsent>
<aftsection>
<nextsent>when we apply such techniques to russian corpus of the size ofthe bnc, this means that we need accurate and robust parsing tools, which do not exist for russian.
</nextsent>
<nextsent>also, no electronic thesaurus, such as wordnet(miller, 1990), is available for russian.
</nextsent>
<nextsent>purely statistical approaches treat multiword expressions asa bag of words and pay no attention to the possibility of variation in the inventory and order of mwe components.
</nextsent>
<nextsent>given that the word order in russian (and other slavonic languages) is relatively free and typical word (i.e. lemma) has many forms(typically from 9 for nouns to 50 for verbs), these quences of exact n-grams are much less frequent than in english, thus rendering purely statistical approaches useless.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1297">
<title id=" W04-0403.xml">what is at stake a case study of russian expressions starting with a preposition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, we cannot use the difference in the pp structure as an indicator of an mwe.
</prevsent>
<prevsent>the fact that mwes are not fully compositional means that the meanings of their constituent words change resulting specific idiomatic meaning of the whole contstruction.
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
in this case we cannot accept the general assumption of one sense per discourse (gale et al, 1992), <papid> H92-1045 </papid>because words such as line, large in english or kljuch in russian can function in the same discourse in totally different sense.</citsent>
<aftsection>
<nextsent>however, the assumption of one sense per collocation can hold, because an mwe with prepositional phrase typically has one and the same meaning: even though line, large or techenie are ambiguous, in line, at large and pod kljuch, techenie have their specific meanings.
</nextsent>
<nextsent>3 methodology.
</nextsent>
<nextsent>the study starts with the selection of the list of the most frequent prepositions to account for alarge number of potential collocations.
</nextsent>
<nextsent>information on the frequency of prepositions (table 2) istaken from the pilot version of the russian reference corpus, which currently consists of about 55 million words (table 2 lists the relative frequency of prepositions in terms of the number of their instances per million words, ipm).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1298">
<title id=" W04-0803.xml">senseval3 task automatic labeling of semantic roles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at the present time, major paradigm for representing meaning has emerged in frame semantics, specifically in the framenet project.
</prevsent>
<prevsent>a worthy objective for the senseval community is the development of wide range of methods for automating frame semantics, specifically identifying and labeling semantic roles in sentences.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
an important baseline study of this process has recently appeared in the literature (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>the framenet project (johnson et al , 2003) has put together body of hand-labeled data and the gildea and jurafsky study has put together set of suitable metrics for evaluating the performance of an automatic system.
</nextsent>
<nextsent>1 the senseval-3 task.
</nextsent>
<nextsent>this senseval-3 task calls for the development of systems to meet the same objectives as the gildea and jurafsky study.
</nextsent>
<nextsent>the data for this task is sample of the framenet hand-annotated data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1299">
<title id=" W04-2006.xml">a step towards incremental generation of logical forms </title>
<section> case studies.  </section>
<citcontext>
<prevsection>
<prevsent>4.2.1 minimal recur sion semantics linking syntax with semantics is not an easy task.
</prevsent>
<prevsent>as allen says in (allen, 1995) there seemsto be structural inconsistency between syntactic structure and the structure of the logical form.we can ease this process by using an adequate representation language.
</prevsent>
</prevsection>
<citsent citstr=" P83-1009 ">
in fact, although the concept is not new (hobbs, 1983), <papid> P83-1009 </papid>state of the art frameworks such as (molla?</citsent>
<aftsection>
<nextsent>et al., 2003; baldridge and kruijff, 2002) <papid> P02-1041 </papid>are using flat semantic representations, taht is formulas with no embedded structures (see (molla?, 2000) for details about flatness), which simplify the syntactic-semantic interface.</nextsent>
<nextsent>at the same time, and because it is not reasonable to generate all the possible interpretations of sentence, many frameworks are using representation languages that leave underspecified semantic interpretations (also an old concept (woods, 1978)).mrs (copestake et al, 2001) uses flat representation with explicit pointers (called han dles) to encode scope effects, corresponding to recursive structures in more conventional formal semantic representations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1300">
<title id=" W04-2006.xml">a step towards incremental generation of logical forms </title>
<section> case studies.  </section>
<citcontext>
<prevsection>
<prevsent>as allen says in (allen, 1995) there seemsto be structural inconsistency between syntactic structure and the structure of the logical form.we can ease this process by using an adequate representation language.
</prevsent>
<prevsent>in fact, although the concept is not new (hobbs, 1983), <papid> P83-1009 </papid>state of the art frameworks such as (molla?</prevsent>
</prevsection>
<citsent citstr=" P02-1041 ">
et al., 2003; baldridge and kruijff, 2002) <papid> P02-1041 </papid>are using flat semantic representations, taht is formulas with no embedded structures (see (molla?, 2000) for details about flatness), which simplify the syntactic-semantic interface.</citsent>
<aftsection>
<nextsent>at the same time, and because it is not reasonable to generate all the possible interpretations of sentence, many frameworks are using representation languages that leave underspecified semantic interpretations (also an old concept (woods, 1978)).mrs (copestake et al, 2001) uses flat representation with explicit pointers (called han dles) to encode scope effects, corresponding to recursive structures in more conventional formal semantic representations.
</nextsent>
<nextsent>we have chosen this language because it has three fundamental characteristics: a) it is flatlanguage; b) it allows the treatment of quantification; c) it allows underspecification.
</nextsent>
<nextsent>un 3notice, that by choosing the universal value, in the final formula this quantifier will no longer be in the scope of negation.
</nextsent>
<nextsent>der specified mrs structures can be converted into scope-resolved structures that, according to (copestake et al, 1997), correspond to those obeyed by conventionally written bracketed structure?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1301">
<title id=" W04-0308.xml">incrementality in deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first is mainly practical and has to do with real-time applications such as speech recognition, which require continually updated analysis of the in put received so far.
</prevsent>
<prevsent>the second reason is more theoretical in that it connects parsing to cognitive modeling, where there is psycholinguis tic evidence suggesting that human parsing is largely incremental (marslen-wilson, 1973; frazier, 1987).however, most state-of-the-art parsing methods today do not adhere to the principle of in cre mentality, for different reasons.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
parsers that attempt to disambiguate the input completely ? full parsing ? typically first employ some kind of dynamic programming algorithm to derive packed parse forest and then applies probabilistic top-down model in order to select the most probable analysis (collins, 1997; <papid> P97-1003 </papid>charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>since the first step is essentiallynondeterministic, this seems to rule out incre mentality at least in strict sense.
</nextsent>
<nextsent>by contrast,parsers that only partially disambiguate the input ? partial parsing ? are usually deterministic and construct the final analysis in one pass over the input (abney, 1991; daelemans et al,1999).<papid> W99-0707 </papid></nextsent>
<nextsent>but since they normally output sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for different reason.deterministic dependency parsing has recently been proposed as robust and efficient method for syntactic parsing of unrestricted natural language text (yamada and matsumoto, 2003; nivre, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1302">
<title id=" W04-0308.xml">incrementality in deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first is mainly practical and has to do with real-time applications such as speech recognition, which require continually updated analysis of the in put received so far.
</prevsent>
<prevsent>the second reason is more theoretical in that it connects parsing to cognitive modeling, where there is psycholinguis tic evidence suggesting that human parsing is largely incremental (marslen-wilson, 1973; frazier, 1987).however, most state-of-the-art parsing methods today do not adhere to the principle of in cre mentality, for different reasons.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
parsers that attempt to disambiguate the input completely ? full parsing ? typically first employ some kind of dynamic programming algorithm to derive packed parse forest and then applies probabilistic top-down model in order to select the most probable analysis (collins, 1997; <papid> P97-1003 </papid>charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>since the first step is essentiallynondeterministic, this seems to rule out incre mentality at least in strict sense.
</nextsent>
<nextsent>by contrast,parsers that only partially disambiguate the input ? partial parsing ? are usually deterministic and construct the final analysis in one pass over the input (abney, 1991; daelemans et al,1999).<papid> W99-0707 </papid></nextsent>
<nextsent>but since they normally output sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for different reason.deterministic dependency parsing has recently been proposed as robust and efficient method for syntactic parsing of unrestricted natural language text (yamada and matsumoto, 2003; nivre, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1303">
<title id=" W04-0308.xml">incrementality in deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>parsers that attempt to disambiguate the input completely ? full parsing ? typically first employ some kind of dynamic programming algorithm to derive packed parse forest and then applies probabilistic top-down model in order to select the most probable analysis (collins, 1997; <papid> P97-1003 </papid>charniak, 2000).<papid> A00-2018 </papid></prevsent>
<prevsent>since the first step is essentiallynondeterministic, this seems to rule out incre mentality at least in strict sense.</prevsent>
</prevsection>
<citsent citstr=" W99-0707 ">
by contrast,parsers that only partially disambiguate the input ? partial parsing ? are usually deterministic and construct the final analysis in one pass over the input (abney, 1991; daelemans et al,1999).<papid> W99-0707 </papid></citsent>
<aftsection>
<nextsent>but since they normally output sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for different reason.deterministic dependency parsing has recently been proposed as robust and efficient method for syntactic parsing of unrestricted natural language text (yamada and matsumoto, 2003; nivre, 2003).
</nextsent>
<nextsent>in some ways,this approach can be seen as compromise between traditional full and partial parsing.
</nextsent>
<nextsent>essentially, it is kind of full parsing in that the goal is to build complete syntactic analysis forthe input string, not just identify major constituents.
</nextsent>
<nextsent>but it resembles partial parsing in being robust, efficient and deterministic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1304">
<title id=" W04-0308.xml">incrementality in deterministic dependency parsing </title>
<section> incrementality in dependency.  </section>
<citcontext>
<prevsection>
<prevsent>for left-dependents this is not problem, as can be seen in (5), which can be processed by alternating shift and left-reduce.
</prevsent>
<prevsent>but in(1) the sequence of reductions has to be performed from right to left as it were, which rules out strict incrementality.
</prevsent>
</prevsection>
<citsent citstr=" J00-3002 ">
however, whereas the structures exemplified in (67) can never be processed incrementally within the present framework, the structure in (1) can be handled by modifying the parsing strategy, as we shall see in the next section.it is instructive at this point to make comparison with incremental parsing based on extended categorial grammar, where the structures in (67) would normally be handled by some kind of concatenation (or product), which does not correspond to any real semantic combination of the constituents (steedman, 2000; morrill, 2000).<papid> J00-3002 </papid></citsent>
<aftsection>
<nextsent>by contrast, the structure in (1)would typically be handled by function composition, which corresponds to well-defined compositional semantic operation.
</nextsent>
<nextsent>hence, it might be argued that the treatment of (67) is only pseudo-incremental even in other frameworks.before we leave the strict bottom-up approach, it can be noted that the algorithm described in this section is essentially the algorithm used by yamada and matsumoto (2003) in combination with support vector machines, except that they allow parsing to be performed in multiple passes, where the graph produced in one pass is given as input to the next pass.1 themain motivation they give for parsing in multiple passes is precisely the fact that the bottom up strategy requires each token to have found all its dependents before it is combined with itshead, which is also what prevents the incremental parsing of structures like (1).
</nextsent>
<nextsent>in order to increase the incrementality of deterministic dependency parsing, we need to combine bottom-up and top-down processing.
</nextsent>
<nextsent>more precisely, we need to process left-dependents bottom-up and right-dependents top-down.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1305">
<title id=" W04-0308.xml">incrementality in deterministic dependency parsing </title>
<section> 222 1.3 </section>
<citcontext>
<prevsection>
<prevsent>7 26 0.2 8 3 0.0 ? 1 11399 68.9 ? 3 15609 94.3 ? 8 16545 100.0 table 1: number of connected components in (s,as) during parsing 5 experimental evaluation.
</prevsent>
<prevsent>in order to measure the degree of incrementality achieved in practical parsing, we have evaluated parser that uses the arc-eager parsing algorithm in combination with memory-based classifier for predicting the next transition.
</prevsent>
</prevsection>
<citsent citstr=" W04-2407 ">
in experiments reported in nivre et al (2004), <papid> W04-2407 </papid>aparsing accuracy of 85.7% (unlabeled attachment score) was achieved, using data from asmall treebank of swedish (einarsson, 1976), divided into training set of 5054 sentences and test set of 631 sentences.</citsent>
<aftsection>
<nextsent>however, in the present context, we are primarily interested inthe incrementality of the parser, which we measure by considering the number of connected components in (s,as) at different stages during the parsing of the test data.
</nextsent>
<nextsent>the results can be found in table 1, where we see that out of 16545 configurations used in parsing 613 sentences (with mean length of 14.0 words), 68.9% have zero or one connected component on the stack, which is what we require of strictly incremental parser.
</nextsent>
<nextsent>we also see that most violations of incrementality are fairly mild, since more than 90% of all configurations have no more than three connected components on the stack.
</nextsent>
<nextsent>many violations of incrementality are caused by sentences that cannot be parsed into well formed dependency graph, i.e. single projective dependency tree, but where the output ofthe parser is set of internally connected components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1307">
<title id=" W03-2910.xml">some aspects of the morphological processing of bulgarian </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>usually, morphological models and their implementations are the primary building blocks in nlp systems.the development of the computational morphology of given language has two main stages.the first stage is the building of the morphological database itself.
</prevsent>
<prevsent>the second stage includes applications of the morphological database in different processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" P98-1050 ">
the interaction and mutual prediction between the two stages determines the linguistic and computational decision-making of each stage.bulgarian computational morphology has developed as the result of local (paskaleva et al 1993; popov et al  1998) and international activities for the compilation of sets of morphosyntactic distinctions and the construction of electronic lexicons (dimitrova et al  1998).<papid> P98-1050 </papid></citsent>
<aftsection>
<nextsent>the need for synchronization and standardization has lead to the activities of the application to bulgarian of internationally acknowledged guidelines for morphosyntactic annotation (slavcheva and paskaleva 1997), and to the comparison of morphosyntactic tagsets (slavcheva 1997).in this paper demonstrate the production scenario of modelling morphological knowledge in bulgarian and applications of the created data setsin an integrated framework for production and manipulation of language resources, that is, the bul treebank framework (simov et al  2002).
</nextsent>
<nextsent>the production scenario is exemplified by the bulgarian verb as the morphologically richest and most problematic part-of-speech category.
</nextsent>
<nextsent>the definition of the set of morphosyntactic specifications for verbs in the lexicon is described.
</nextsent>
<nextsent>the application of the tagset in the automatic morphological analysis of text corpora is accounted for.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1308">
<title id=" W04-0712.xml">ellipsis resolution by controlled default unification for multimodal and speech dialog systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we extend default unification to non-parallel structures,which is important for speech and multimodal dialog systems.
</prevsent>
<prevsent>we introduce new control mechanisms for ellipsis resolution by considering dialog structure with respect to specification, variation and results of tasks and combine this with the analysis of relations between the information elements contained in antecedent and elliptic structures.
</prevsent>
</prevsection>
<citsent citstr=" P94-1003 ">
the application of default unification (carpenter, 1993) or priority union (kaplan, 1987; calder,1991) to discourse is attractive, because these related concepts meet the intuition that new information extends, corrects, or modifies old information,instead of deleting it, by keeping what is consis tent.1 the use of default unification as means for ellipsis resolution has been discussed in the first half of the nineties (prust et al, 1994; grover et al,1994).<papid> P94-1003 </papid></citsent>
<aftsection>
<nextsent>later, the discussion silted up, perhaps be cause the conditions on parallelism that have been imposed occured to be too strong (cf.
</nextsent>
<nextsent>(hobbs and kehler, 1997)).<papid> P97-1051 </papid></nextsent>
<nextsent>1.1 applications in dialog systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1309">
<title id=" W04-0712.xml">ellipsis resolution by controlled default unification for multimodal and speech dialog systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the application of default unification (carpenter, 1993) or priority union (kaplan, 1987; calder,1991) to discourse is attractive, because these related concepts meet the intuition that new information extends, corrects, or modifies old information,instead of deleting it, by keeping what is consis tent.1 the use of default unification as means for ellipsis resolution has been discussed in the first half of the nineties (prust et al, 1994; grover et al,1994).<papid> P94-1003 </papid></prevsent>
<prevsent>later, the discussion silted up, perhaps be cause the conditions on parallelism that have been imposed occured to be too strong (cf.</prevsent>
</prevsection>
<citsent citstr=" P97-1051 ">
(hobbs and kehler, 1997)).<papid> P97-1051 </papid></citsent>
<aftsection>
<nextsent>1.1 applications in dialog systems.
</nextsent>
<nextsent>since this discussion, default unification-based ellipsis resolution has been applied in working systems of at least two projects, where it played an essential role in discourse processing.
</nextsent>
<nextsent>the first implementations have been provided in the second half of the nineties at siemens, where the diamod project developed serial of prototypes for multi-modal human machine dialog (cf.
</nextsent>
<nextsent>(streit, 2001)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1310">
<title id=" W04-0712.xml">ellipsis resolution by controlled default unification for multimodal and speech dialog systems </title>
<section> information browsing.  </section>
<citcontext>
<prevsection>
<prevsent>browsing means to vary requests either because it is not clear in advance which information is relevant, how exactly it can be obtained, or because the user wants to gather broad information in some area.in browsing dialog, ellipsis is controlled by relations between the informational content of the antecedent and the elliptic utterance.
</prevsent>
<prevsent>according to our remarks at the beginn of the section, we omit there actions of the system in the subsequent examples.by group, we understand collection of information that is orthogonal to other information.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
by4the reader may recognize certain similarity of the considerations in this section with the approach of (grosz and sid ner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>an example: we restrict ourself to some remarks: grosz &amp; sidner focus on the segmentation of discourse along the hierarchical structure of task, while we focus on problems concerning repetition (this section) and variation of tasks (next section).
</nextsent>
<nextsent>grosz &amp; sidner are mainly concerned with anaphoric reference while we are concerned with ellipsis and related implicit inheritance of information.
</nextsent>
<nextsent>in our approach, structural relations between information is as much important as aspects concerning the processing of tasks.
</nextsent>
<nextsent>furthermore, we discuss problems in relation to special resolution mechanism, i.e., default unification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1311">
<title id=" W04-1112.xml">chinese term extraction from web pages based on compound term productivity </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>there are no explicit word boundary marker in chinese, we first have to do morphological analysis which segments out words from sentence and does pos tagging at the same time.
</prevsent>
<prevsent>after pos tagging, the complex structures mentioned above are extracted as term candidates.
</prevsent>
</prevsection>
<citsent citstr=" P90-1032 ">
previous studies have proposed many promising ways for this purpose, for instance, smadja and mckeown (1990), <papid> P90-1032 </papid>and frantzi and ananiadou (1996) <papid> C96-1009 </papid>tried to treat more general structures like collocations.</citsent>
<aftsection>
<nextsent>2.3 scoring.
</nextsent>
<nextsent>the next step of atr is to assign each term candidate its score in order to rank them in descending order of termhood.
</nextsent>
<nextsent>many researchers have sought the definition of term candidates score which approximates termhood.
</nextsent>
<nextsent>in fact, many of those proposals make use of statistics of actual use in corpus such as term frequency which is so powerful and simple that many researchers directly or indirectly have used it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1312">
<title id=" W04-1112.xml">chinese term extraction from web pages based on compound term productivity </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>there are no explicit word boundary marker in chinese, we first have to do morphological analysis which segments out words from sentence and does pos tagging at the same time.
</prevsent>
<prevsent>after pos tagging, the complex structures mentioned above are extracted as term candidates.
</prevsent>
</prevsection>
<citsent citstr=" C96-1009 ">
previous studies have proposed many promising ways for this purpose, for instance, smadja and mckeown (1990), <papid> P90-1032 </papid>and frantzi and ananiadou (1996) <papid> C96-1009 </papid>tried to treat more general structures like collocations.</citsent>
<aftsection>
<nextsent>2.3 scoring.
</nextsent>
<nextsent>the next step of atr is to assign each term candidate its score in order to rank them in descending order of termhood.
</nextsent>
<nextsent>many researchers have sought the definition of term candidates score which approximates termhood.
</nextsent>
<nextsent>in fact, many of those proposals make use of statistics of actual use in corpus such as term frequency which is so powerful and simple that many researchers directly or indirectly have used it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1314">
<title id=" W04-1507.xml">categorial type logic meets dependency grammar to annotate an italian corpus </title>
<section> a distributional study of italian </section>
<citcontext>
<prevsection>
<prevsent>part-of-speech tagging in order to annotate the coris corpus with theory-neutral set of pos tags, we plan to carry out distributional study of its lexicon.
</prevsent>
<prevsent>early approaches to this problem were basedon the hypothesis that if two words are syntactically and semantically different, they will appear in different contexts.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
there are number of studies that, starting from this hypothesis, have built automatic or semi-automaticprocedures for clustering words (brill and marcus, 1992; pereira et al, 1993; <papid> P93-1024 </papid>martinet al,1998), especially in the field of cognitive sciences (redington et al, 1998; gobet and pine,1997; clark, 2000).<papid> W00-0717 </papid></citsent>
<aftsection>
<nextsent>they examine the distributional behaviour of some target words, comparing the lexical distribution of their respective collocates using quantitative measures of distributional similarity (lee, 1999).<papid> P99-1004 </papid>in (brill and marcus, 1992) it is given semiautomatic procedure that, starting from lexical statistical data collected from large corpus, aims to arrange target words in tree (more precisely dendrogram), instead of clustering them automatically.</nextsent>
<nextsent>this procedure requires linguistic examination of the resulting tree, in order to identify the word classes that are most appropriate to describe the phenomenon underinvestigation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1315">
<title id=" W04-1507.xml">categorial type logic meets dependency grammar to annotate an italian corpus </title>
<section> a distributional study of italian </section>
<citcontext>
<prevsection>
<prevsent>part-of-speech tagging in order to annotate the coris corpus with theory-neutral set of pos tags, we plan to carry out distributional study of its lexicon.
</prevsent>
<prevsent>early approaches to this problem were basedon the hypothesis that if two words are syntactically and semantically different, they will appear in different contexts.
</prevsent>
</prevsection>
<citsent citstr=" W00-0717 ">
there are number of studies that, starting from this hypothesis, have built automatic or semi-automaticprocedures for clustering words (brill and marcus, 1992; pereira et al, 1993; <papid> P93-1024 </papid>martinet al,1998), especially in the field of cognitive sciences (redington et al, 1998; gobet and pine,1997; clark, 2000).<papid> W00-0717 </papid></citsent>
<aftsection>
<nextsent>they examine the distributional behaviour of some target words, comparing the lexical distribution of their respective collocates using quantitative measures of distributional similarity (lee, 1999).<papid> P99-1004 </papid>in (brill and marcus, 1992) it is given semiautomatic procedure that, starting from lexical statistical data collected from large corpus, aims to arrange target words in tree (more precisely dendrogram), instead of clustering them automatically.</nextsent>
<nextsent>this procedure requires linguistic examination of the resulting tree, in order to identify the word classes that are most appropriate to describe the phenomenon underinvestigation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1316">
<title id=" W04-1507.xml">categorial type logic meets dependency grammar to annotate an italian corpus </title>
<section> a distributional study of italian </section>
<citcontext>
<prevsection>
<prevsent>early approaches to this problem were basedon the hypothesis that if two words are syntactically and semantically different, they will appear in different contexts.
</prevsent>
<prevsent>there are number of studies that, starting from this hypothesis, have built automatic or semi-automaticprocedures for clustering words (brill and marcus, 1992; pereira et al, 1993; <papid> P93-1024 </papid>martinet al,1998), especially in the field of cognitive sciences (redington et al, 1998; gobet and pine,1997; clark, 2000).<papid> W00-0717 </papid></prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
they examine the distributional behaviour of some target words, comparing the lexical distribution of their respective collocates using quantitative measures of distributional similarity (lee, 1999).<papid> P99-1004 </papid>in (brill and marcus, 1992) it is given semiautomatic procedure that, starting from lexical statistical data collected from large corpus, aims to arrange target words in tree (more precisely dendrogram), instead of clustering them automatically.</citsent>
<aftsection>
<nextsent>this procedure requires linguistic examination of the resulting tree, in order to identify the word classes that are most appropriate to describe the phenomenon underinvestigation.
</nextsent>
<nextsent>in this sense, they use semiautomatic word-class generator method.a similar procedure has been applied on italian in (tamburini et al, 2002).
</nextsent>
<nextsent>the novelty ofthis work is that it derives the distributional information on words from very basic set of pos tags, namely nouns, verbs and adjectives.
</nextsent>
<nextsent>this method, completely avoiding the sparseness of the data affecting brill and marcus?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1317">
<title id=" W03-2806.xml">intrinsic versus extrinsic evaluations of parsing systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(galliers and sparck jones, 1993, p22).
</prevsent>
<prevsent>thus, an intrinsic evaluation of parser would analyse the accuracy of the results returned by the parser as stand-alone system, whereas an extrinsic evaluation would analyse the impact of the parser within the context of broader nlp application.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
there are currently several parsing systems that attempt to achieve wide coverage of the english language (such as those developed by collins (1996), <papid> P96-1025 </papid>jarvinen and tapanainen (1997), and sleator and temperley (1993)).</citsent>
<aftsection>
<nextsent>there is also substantial literature on parsing evaluation (see, for example, work by sutcliffe et al (1996), black (1996), carroll et al (1998), and bangalore et al (1998)).
</nextsent>
<nextsent>recently there hasbeen shift from constituency-based (e.g. counting crossing brackets (black et al, 1991)) <papid> H91-1060 </papid>to dependency-based evaluation (lin, 1995; carroll et al, 1998).</nextsent>
<nextsent>those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1318">
<title id=" W03-2806.xml">intrinsic versus extrinsic evaluations of parsing systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are currently several parsing systems that attempt to achieve wide coverage of the english language (such as those developed by collins (1996), <papid> P96-1025 </papid>jarvinen and tapanainen (1997), and sleator and temperley (1993)).</prevsent>
<prevsent>there is also substantial literature on parsing evaluation (see, for example, work by sutcliffe et al (1996), black (1996), carroll et al (1998), and bangalore et al (1998)).</prevsent>
</prevsection>
<citsent citstr=" H91-1060 ">
recently there hasbeen shift from constituency-based (e.g. counting crossing brackets (black et al, 1991)) <papid> H91-1060 </papid>to dependency-based evaluation (lin, 1995; carroll et al, 1998).</citsent>
<aftsection>
<nextsent>those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations).
</nextsent>
<nextsent>in this paper we report on the comparison between an intrinsic evaluation and an evaluation of the impact of the parser in real application (an extrinsic evaluation).we have chosen answer extraction as an example of practical application within which to test the parsing systems.
</nextsent>
<nextsent>in particular, the extrinsic evaluation uses extrans, an answer extraction system that operates over unix manual pages (mollaet al, 2000).
</nextsent>
<nextsent>the two grammar systems to compare are link grammar (sleator and temperley, 1993) and the conexor functional dependency grammar parser (tapanainen and jarvinen, 1997) <papid> A97-1011 </papid>henceforth referred to as conexor fdg).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1320">
<title id=" W03-2806.xml">intrinsic versus extrinsic evaluations of parsing systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we report on the comparison between an intrinsic evaluation and an evaluation of the impact of the parser in real application (an extrinsic evaluation).we have chosen answer extraction as an example of practical application within which to test the parsing systems.
</prevsent>
<prevsent>in particular, the extrinsic evaluation uses extrans, an answer extraction system that operates over unix manual pages (mollaet al, 2000).
</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
the two grammar systems to compare are link grammar (sleator and temperley, 1993) and the conexor functional dependency grammar parser (tapanainen and jarvinen, 1997) <papid> A97-1011 </papid>henceforth referred to as conexor fdg).</citsent>
<aftsection>
<nextsent>these parsing systems were chosen because both include dependency-based parser and comprehensive grammar of english.
</nextsent>
<nextsent>however, the structures returned are so different that direct comparison between them is not straightforward.
</nextsent>
<nextsent>in section 2 we review the main differences between link grammar and conexor fdg.
</nextsent>
<nextsent>in section 3 we present the intrinsic comparison of parsers, and in section 4 we comment on the extrinsic comparison within the context of answer extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1325">
<title id=" W03-2806.xml">intrinsic versus extrinsic evaluations of parsing systems </title>
<section> extrinsic evaluations.  </section>
<citcontext>
<prevsection>
<prevsent>logical forms are called minimal logical forms (mlfs) because they encode the minimum information required for effective answer extraction.
</prevsent>
<prevsent>in particular, only the main dependencies between the verb and arguments are expressed,plus modifier and adjunct relations.
</prevsent>
</prevsection>
<citsent citstr=" P85-1008 ">
thus, complex quantification, tense and aspect, temporal relations, plurality, and modality are not expressed.the mlfs use reification to achieve flat expressions, very much in the line of davidson (1967), hobbs (1985), <papid> P85-1008 </papid>and copestake et al (1997).</citsent>
<aftsection>
<nextsent>in the current implementation only reification to objects, eventualities (events or states), and properties is applied.
</nextsent>
<nextsent>for example, the mlf of the sentence cp will quickly copy files is: holds(e4), object(cp,o1,[x1]), object(s command,o2,[x1]), evt(s copy,e4,[x1,x6]), object(s file,o3,[x6]), prop(quickly,p3,[e4]).in other words, there is an entity x1 which represents an object of type command;2 there is an entity x6 (a file); there is an entity e4, which represents copying event where the first argument is x1 and the second argument is x6; there is an entity p3which states that e4 is done quickly, and the event e4, that is, the copying, holds.
</nextsent>
<nextsent>extrans finds the answers to the questions by converting the mlfs of the questions into prologqueries and then running prologs default resolution mechanism to find those mlfs that can prove the question.this default search procedure is called the synonym mode since extrans uses small wordnet style thesaurus (fellbaum, 1998) to convert all the synonyms into synonym representative.
</nextsent>
<nextsent>extrans also has an approximate mode which, besides normalising all synonyms, scores all document sentences on the basis of the maximum number of predicates that unify between the mlfs of the query and the answer candidate (molla?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1326">
<title id=" W04-0408.xml">multiword expressions as dependency subgraphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, dependency grammar (dg) (tesnie`re, 1959; sgall et al, 1986; melcuk, 1988) has received resurgent interest.
</prevsent>
<prevsent>core concepts such as grammatical functions, valency and the head-dependent asymmetry have now found their way into most grammar formalisms, including phrase structure-based ones such as hpsg, lfg and tag.
</prevsent>
</prevsection>
<citsent citstr=" P98-1086 ">
this renewed interest in dg has also given rise to new grammar formalisms based directly on dg (nasr, 1995; heinecke et al, 1998; <papid> P98-1086 </papid>broker, 1999; gerdes and kahane, 2001; <papid> P01-1029 </papid>kruijff, 2001; joshi and rambow, 2003).a controversy among dg grammar ians circles around the question of assuming 1:1 correspondence between words and nodes in the dependency graph.</citsent>
<aftsection>
<nextsent>this assumption simplifies the formalization of dgs substantially, and is often required for parsing.
</nextsent>
<nextsent>but as soon as semantics comes in, the picture changes.
</nextsent>
<nextsent>clearly, the 1:1-correspondence between words and nodes does not hold anymore for multiword expressions (mwes), where one semantic unit, represented by node in semantically oriented dependency graph, corresponds not to one, but to more than one word.
</nextsent>
<nextsent>most dgs interested in semantics have thus weakened the 1:1-assumption, starting withtesnie`res work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1327">
<title id=" W04-0408.xml">multiword expressions as dependency subgraphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, dependency grammar (dg) (tesnie`re, 1959; sgall et al, 1986; melcuk, 1988) has received resurgent interest.
</prevsent>
<prevsent>core concepts such as grammatical functions, valency and the head-dependent asymmetry have now found their way into most grammar formalisms, including phrase structure-based ones such as hpsg, lfg and tag.
</prevsent>
</prevsection>
<citsent citstr=" P01-1029 ">
this renewed interest in dg has also given rise to new grammar formalisms based directly on dg (nasr, 1995; heinecke et al, 1998; <papid> P98-1086 </papid>broker, 1999; gerdes and kahane, 2001; <papid> P01-1029 </papid>kruijff, 2001; joshi and rambow, 2003).a controversy among dg grammar ians circles around the question of assuming 1:1 correspondence between words and nodes in the dependency graph.</citsent>
<aftsection>
<nextsent>this assumption simplifies the formalization of dgs substantially, and is often required for parsing.
</nextsent>
<nextsent>but as soon as semantics comes in, the picture changes.
</nextsent>
<nextsent>clearly, the 1:1-correspondence between words and nodes does not hold anymore for multiword expressions (mwes), where one semantic unit, represented by node in semantically oriented dependency graph, corresponds not to one, but to more than one word.
</nextsent>
<nextsent>most dgs interested in semantics have thus weakened the 1:1-assumption, starting withtesnie`res work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1328">
<title id=" W04-0408.xml">multiword expressions as dependency subgraphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this can happen e.g. through paraphrasing rules implemented by lexical functions (melcuk, 1996).
</prevsent>
<prevsent>unfortunately, these attempts to break the 1:1-correspondence have not yet been formalized in declarative way.
</prevsent>
</prevsection>
<citsent citstr=" P01-1024 ">
extensible dependency grammar (xdg) is new grammar formalism based on topologicaldependency grammar (tdg) (duchier and debusmann, 2001).<papid> P01-1024 </papid></citsent>
<aftsection>
<nextsent>from tdg, it inherits declarative word order constraints, the ability to distinguish multiple dimensions of linguistic description, and an axiomatization as constraint satisfaction problem (duchier, 2003) solvable using constraint programming (apt, 2003).
</nextsent>
<nextsent>one of the benefits of this axiomatization is that the linear order of the words can be left underspecified, with the effect that the constraint solver can be applied for both parsing and generation.xdg solving is efficient at least for the smaller scale example grammars tested so far, but these good results hinge substantially on the assump second acl workshop on multiword expressions: integrating processing, july 2004, pp.
</nextsent>
<nextsent>56-63 tion of 1:1-correspondence between words and nodes.
</nextsent>
<nextsent>as xdg has been created to cover not only syntax but also semantics, we have no choice but to weaken the 1:1-correspondence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1339">
<title id=" W04-0509.xml">analysis of semantic classes in medical text for question answering </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>currently, the system accepts keyword queries in pico format (sack ett et al, 2000).
</prevsent>
<prevsent>in this format, clinical question is represented by set of four fields that correspond to the basic elements of the question: p: description of the patient (or the problem); i: an intervention; c: comparison or control intervention (may be omitted); o: the clinical outcome.for example, the question shown above can be represented in pico format as follows: p: myocardial infarction i: thrombolysis c: ? o: mortality our work in the project is to extend the keyword retrieval to system that can answer questions expressed in natural language.
</prevsent>
</prevsection>
<citsent citstr=" W03-1310 ">
in our earlier work (niu et al, 2003), <papid> W03-1310 </papid>we showed that current technologies for factoid question answering (qa) are not adequate for clinical questions, whose answers must often be obtained by synthesizing relevant context.</citsent>
<aftsection>
<nextsent>to adapt to this new characteristic of qa in the medical domain, we exploit semantic classes and relations between them in medical text.
</nextsent>
<nextsent>semantic classes are important for our task because the information contained in them isoften good candidate for answering clinical questions.
</nextsent>
<nextsent>in the example above, pico elements correspond to three semantic classes: disease (medical problem of the patient), intervention (med ication applied to the disease) and the clinical outcome.
</nextsent>
<nextsent>they together constitute scenario of treatment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1340">
<title id=" W04-0509.xml">analysis of semantic classes in medical text for question answering </title>
<section> identifying semantic classes in medical.  </section>
<citcontext>
<prevsection>
<prevsent>we also apply an automatic classification process to determine the polarity of an outcome, as it is important in answering clinical questions.
</prevsent>
<prevsent>text 2.1 diseases and medications.
</prevsent>
</prevsection>
<citsent citstr=" W03-1305 ">
the identification of named entities (nes) in the biomedical area, such as proteins and cells, has been extensively explored; e.g., lee et al (2003), <papid> W03-1305 </papid>shen et al (2003).<papid> W03-1307 </papid></citsent>
<aftsection>
<nextsent>however, we are not aware ofany satisfactory solution that focuses on the recognition of semantic classes such as medication and disease.
</nextsent>
<nextsent>to straightforwardly identify disease and medication in the text, we use the knowledge base unified medical language system (umls) (lindberg et al, 1993) and the software metamap (aronson, 2001).
</nextsent>
<nextsent>umls contains three knowledge sources: themetathesaurus, the semantic network, and the specialist lexicon.
</nextsent>
<nextsent>given an input sentence, metamapseparates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categories to them according to the knowledge in umls.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1341">
<title id=" W04-0509.xml">analysis of semantic classes in medical text for question answering </title>
<section> identifying semantic classes in medical.  </section>
<citcontext>
<prevsection>
<prevsent>we also apply an automatic classification process to determine the polarity of an outcome, as it is important in answering clinical questions.
</prevsent>
<prevsent>text 2.1 diseases and medications.
</prevsent>
</prevsection>
<citsent citstr=" W03-1307 ">
the identification of named entities (nes) in the biomedical area, such as proteins and cells, has been extensively explored; e.g., lee et al (2003), <papid> W03-1305 </papid>shen et al (2003).<papid> W03-1307 </papid></citsent>
<aftsection>
<nextsent>however, we are not aware ofany satisfactory solution that focuses on the recognition of semantic classes such as medication and disease.
</nextsent>
<nextsent>to straightforwardly identify disease and medication in the text, we use the knowledge base unified medical language system (umls) (lindberg et al, 1993) and the software metamap (aronson, 2001).
</nextsent>
<nextsent>umls contains three knowledge sources: themetathesaurus, the semantic network, and the specialist lexicon.
</nextsent>
<nextsent>given an input sentence, metamapseparates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categories to them according to the knowledge in umls.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1342">
<title id=" W04-0509.xml">analysis of semantic classes in medical text for question answering </title>
<section> identifying semantic classes in medical.  </section>
<citcontext>
<prevsection>
<prevsent>our approach does not extract the whole outcome at once.
</prevsent>
<prevsent>instead, it tries to identify the different parts of an outcome that may be scattered in the sentence, and then combines them to form the complete outcome.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
2.2.1 related workrule-based methods and machine-learning approaches have been used for similar problems.gildea and jurafsky (2002) <papid> J02-3001 </papid>used supervised learning method to learn both the identifier of the semantic roles defined in framenet such as theme, target, goal, and the boundaries of the roles (baker et al, 2003).</citsent>
<aftsection>
<nextsent>a set of features were learned from large training set, and then applied to the unseen data to detect the roles.
</nextsent>
<nextsent>the performance of the system was quite good.
</nextsent>
<nextsent>however, it requires large training set for related roles, which is not available in many tasks, including tasks in the medical area.
</nextsent>
<nextsent>rule-based methods are explored in information extraction (ie) to identify roles to fill slots in some pre-defined templates (catala` et al, 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1343">
<title id=" W04-0509.xml">analysis of semantic classes in medical text for question answering </title>
<section> the polarity of outcomes.  </section>
<citcontext>
<prevsection>
<prevsent>thus the polarity of an outcome is often determined by how change happens: if bad thing (e.g., mortality) is reduced then it is positive outcome; if the bad thing is increased, then the outcome is negative.
</prevsent>
<prevsent>we try to capture this observation by adding context feature sto the feature set.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
the way they were added is similar to incorporating the negation effect described by pang et al (2002).<papid> W02-1011 </papid></citsent>
<aftsection>
<nextsent>but instead of just finding negation word?
</nextsent>
<nextsent>(not, isnt, didnt, etc.), we need to find two groups of words: those indicating more and those indicating less.
</nextsent>
<nextsent>in the training text, we found 9 words in the first group and 7 words in the second group.
</nextsent>
<nextsent>when pre-processing text for classification, following the method of pang et al, we attached the tag more to all words between the more-words and the following punctuation mark, and the tag less to the words after the less-words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1345">
<title id=" W04-2603.xml">a powerful and general approach to context exploitation in natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our approach to meaning?
</prevsent>
<prevsent>representation for lex emes is to provide set of similar elements that are grammatically and/or semantically interchangeable with given lexeme.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
others have constructed lexical similarity clusters using order-dependent co-occurrence statistics, particularly with n-gram models see brown et al (1992) <papid> J92-4003 </papid>for an example where words are sorted into exclusive classes based on bigram statistics.</citsent>
<aftsection>
<nextsent>the occurrence statistics of bigrams do stabilize for frequent words given training corpus of hundreds of millions of words.
</nextsent>
<nextsent>however, beyond tri-grams, the theoretical size of training corpus required for completeness is unreasonable.
</nextsent>
<nextsent>our method uses only pairwise conditionals.
</nextsent>
<nextsent>to analyze given text stream, we use hierarchy consisting of word-level representation and conceptual-unit-level representation to analyze arbitrary single-clause english sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1346">
<title id=" W04-2603.xml">a powerful and general approach to context exploitation in natural language processing </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>our sre expansion method provides context-specific meaning?
</prevsent>
<prevsent>representation providing application builders with features that could be applied to problems including word sense disambiguation and named entity recognition.
</prevsent>
</prevsection>
<citsent citstr=" N04-1043 ">
miller et al (2004) <papid> N04-1043 </papid>describe relevant technique for the latter.</citsent>
<aftsection>
<nextsent>to quantify the quality of our sre expansions will require an end-user application demonstration that we are unable to provide at this time.
</nextsent>
<nextsent>our approach uses very large training corpus, hierarchical architecture, and nine independent pairwise co-occurrence knowledge bases.
</nextsent>
<nextsent>individually, these components have, in some form, been applied to computational natural language processing by other researchers.
</nextsent>
<nextsent>however, the combination of these components in our biologically-inspired framework has already produced novel methods that may prove useful to the computational linguistics community.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1347">
<title id=" W03-2115.xml">ontology based contextual coherence scoring </title>
<section> annotation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we wanted to test whether human subjects were ableto annotate the data reliably according to our annotation schemata.
</prevsent>
<prevsent>we had two annotators specially trained for each of these particular annotation tasks.
</prevsent>
</prevsection>
<citsent citstr=" W02-0207 ">
in an earlier annotation experiment reported in gurevych et al (2002), <papid> W02-0207 </papid>the task of annotators was to classify subset of the corpus of srhs as either coherent or incoherent.</citsent>
<aftsection>
<nextsent>here we randomly mixed srhs in order to avoid contextual priming.2 in the first new experiment, sub-corpus of 552 utterances was annotated within the discourse context, i.e. thesrhs were presented in their original dialogue order.
</nextsent>
<nextsent>for each srh, decision again had to be made whether it is semantically coherent or incoherent with respect to the best srh representing the previous user utterance.
</nextsent>
<nextsent>given total of 1,375 markables, the annotators reached an agreement of 79.71%, i.e. 1,096 markables.in the second new annotation experiment, the annotators saw the srhs together with the transcribed user utterances.
</nextsent>
<nextsent>the task of annotators was to determine the best srh from the n-best list of srhs corresponding to single user utterance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1348">
<title id=" W03-2115.xml">ontology based contextual coherence scoring </title>
<section> annotation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the task of annotators was to determine the best srh from the n-best list of srhs corresponding to single user utterance.
</prevsent>
<prevsent>the decision had to be made on the basis of several criteria.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
themost important criteria was how well the srh captures the intentional content of the users utterance.2as reported elsewhere the resulting kappa statistics (carletta, 1996) <papid> J96-2004 </papid>over the annotated data yields ? = 0.7, which indicates that human annotators can reliably distinguish between coherent samples and incoherent ones.if none of the srhs captured the users intention adequately, the decision had to be made by looking at the actual word error rate.</citsent>
<aftsection>
<nextsent>in this experiment the inter-annotator agreement was 90.69%, i.e. 1,247markables out of 1,375.3 each corpus was then tran formed into an evaluation gold standard by means of the annotators agreeing on single solution for the cases of disagreement.
</nextsent>
<nextsent>the aim of the work presented here, then, was to provide knowledge-based score, that can be employed by any nlu system to select the best hypothesis from given n-best list.
</nextsent>
<nextsent>the corresponding on toscore system will be described below, followed by its evaluation against the human gold standards.
</nextsent>
<nextsent>in this section, we provide description of the underlying algorithm and knowledge sources employed by the original onto score system (in press).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1349">
<title id=" W03-2115.xml">ontology based contextual coherence scoring </title>
<section> the knowledge base and onto score.  </section>
<citcontext>
<prevsection>
<prevsent>or owl,5 is converted into graph, consisting of the class hierarchy, with each class corresponding to concept representing either an entity or proces sand their slots, i.e. the named edges of the graph corresponding to the class properties, constraints and restrictions.
</prevsent>
<prevsent>the ontology employed for the evaluation has about 730 concepts and 200 relations.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
it includes generic top-level ontology whose purpose is to provide basic structure of the world, i.e. abstract classes to divide the universe indistinct parts as resulting from the onto logical analysis.6 the modeling of processes and physical objects as kind of event that is continuous and homogeneous in nature,follows the frame semantic analysis used for generating the framenet data (baker et al, 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>the hierarchy of processes is connected to the hierarchy of physical objects via slot-constraint definitions.see also (gurevych et al, 2003<papid> W03-0903 </papid>b) for further description of the ontology.</nextsent>
<nextsent>onto score performs number of processing steps.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1350">
<title id=" W03-2115.xml">ontology based contextual coherence scoring </title>
<section> the knowledge base and onto score.  </section>
<citcontext>
<prevsection>
<prevsent>the ontology employed for the evaluation has about 730 concepts and 200 relations.
</prevsent>
<prevsent>it includes generic top-level ontology whose purpose is to provide basic structure of the world, i.e. abstract classes to divide the universe indistinct parts as resulting from the onto logical analysis.6 the modeling of processes and physical objects as kind of event that is continuous and homogeneous in nature,follows the frame semantic analysis used for generating the framenet data (baker et al, 1998).<papid> P98-1013 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-0903 ">
the hierarchy of processes is connected to the hierarchy of physical objects via slot-constraint definitions.see also (gurevych et al, 2003<papid> W03-0903 </papid>b) for further description of the ontology.</citsent>
<aftsection>
<nextsent>onto score performs number of processing steps.
</nextsent>
<nextsent>a first preprocessing step is to convert each srh into concept representation (cr).
</nextsent>
<nextsent>for that purpose we augmented the systems lexicon with specific concept mappings.
</nextsent>
<nextsent>that is, for each entry in the lexicon either zero, one or many corresponding concepts where added.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1354">
<title id=" W04-0212.xml">annotation and data mining of the penn discourse treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>its use of stand off annotation allows integration with stand-off version of the penn treebank (syntactic structure) and propbank (verbs and their arguments), which adds value for both linguistic discovery and discourse modeling.
</prevsent>
<prevsent>here we describe the pdtb and some experiments in linguistic discovery based on the pdtb alone, as well as on the linked ptb and pdtb corpora.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
large scale annotated corpora such as the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>have played central role in speech and natural language research.</citsent>
<aftsection>
<nextsent>however, with the demand for more powerful nlp applications comes need for greater richness in annotation ? hence, the development of propbank(kingsbury and palmer, 2002), which adds basic semantics to the ptb in the form of verb predicate argument annotation and eventually similar annotation of nominalizations.
</nextsent>
<nextsent>we have been developing yet another annotation layer above these both.
</nextsent>
<nextsent>the penn discourse treebank (pdtb) adds low-leveldiscourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels.
</nextsent>
<nextsent>with this added knowledge, the pdtb (together with the ptb and propbank) should support more in-depth nlp research and more powerful applications.work on the pdtb is grounded in lexicalized approach to discourse ? dltag (webber and joshi, 1998; <papid> W98-0315 </papid>webber et al, 1999<papid> P99-1006 </papid>a; webber et al,2000; webber et al, 2003).<papid> J03-4002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1355">
<title id=" W04-0212.xml">annotation and data mining of the penn discourse treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have been developing yet another annotation layer above these both.
</prevsent>
<prevsent>the penn discourse treebank (pdtb) adds low-leveldiscourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels.
</prevsent>
</prevsection>
<citsent citstr=" W98-0315 ">
with this added knowledge, the pdtb (together with the ptb and propbank) should support more in-depth nlp research and more powerful applications.work on the pdtb is grounded in lexicalized approach to discourse ? dltag (webber and joshi, 1998; <papid> W98-0315 </papid>webber et al, 1999<papid> P99-1006 </papid>a; webber et al,2000; webber et al, 2003).<papid> J03-4002 </papid></citsent>
<aftsection>
<nextsent>here, low-level discourse structure and semantics are taken to result (in part) from composing elementary predicate argument relations whose predicates come mainly from discourse connectives1 and whose arguments1despite this, we have deliberately adopted policy of hav come from units of discourse ? clausal, sentential or multi-sentential units.
</nextsent>
<nextsent>the pdtb therefore differs from the rst-annotated corpus (carlson et al,2003) which starts with (abstract) rhetorical relations (mann and thompson, 1988) and annotates subset of the penn wsj corpus with those relations that can be taken to hold between (primarily) pairs of discourse spans identified in the corpus.the current paper focuses on what can be discovered through analyzing pdtb annotation, both on its own and together with the penn treebank.section 2 of the paper briefly reviews the theoretical background of the project, its current state, the guidelines given to annotators, the annotation tool they used (wordfreak), and the extent of inter annotator agreement.
</nextsent>
<nextsent>section 3 shows how we have used pdtb annotation, along with the ptb, to extract several features pertaining to discourse connectives and their arguments, and discusses the relevance of these features for nlp research and applications.
</nextsent>
<nextsent>section 4 concludes with the summary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1356">
<title id=" W04-0212.xml">annotation and data mining of the penn discourse treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have been developing yet another annotation layer above these both.
</prevsent>
<prevsent>the penn discourse treebank (pdtb) adds low-leveldiscourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels.
</prevsent>
</prevsection>
<citsent citstr=" P99-1006 ">
with this added knowledge, the pdtb (together with the ptb and propbank) should support more in-depth nlp research and more powerful applications.work on the pdtb is grounded in lexicalized approach to discourse ? dltag (webber and joshi, 1998; <papid> W98-0315 </papid>webber et al, 1999<papid> P99-1006 </papid>a; webber et al,2000; webber et al, 2003).<papid> J03-4002 </papid></citsent>
<aftsection>
<nextsent>here, low-level discourse structure and semantics are taken to result (in part) from composing elementary predicate argument relations whose predicates come mainly from discourse connectives1 and whose arguments1despite this, we have deliberately adopted policy of hav come from units of discourse ? clausal, sentential or multi-sentential units.
</nextsent>
<nextsent>the pdtb therefore differs from the rst-annotated corpus (carlson et al,2003) which starts with (abstract) rhetorical relations (mann and thompson, 1988) and annotates subset of the penn wsj corpus with those relations that can be taken to hold between (primarily) pairs of discourse spans identified in the corpus.the current paper focuses on what can be discovered through analyzing pdtb annotation, both on its own and together with the penn treebank.section 2 of the paper briefly reviews the theoretical background of the project, its current state, the guidelines given to annotators, the annotation tool they used (wordfreak), and the extent of inter annotator agreement.
</nextsent>
<nextsent>section 3 shows how we have used pdtb annotation, along with the ptb, to extract several features pertaining to discourse connectives and their arguments, and discusses the relevance of these features for nlp research and applications.
</nextsent>
<nextsent>section 4 concludes with the summary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1358">
<title id=" W04-0212.xml">annotation and data mining of the penn discourse treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have been developing yet another annotation layer above these both.
</prevsent>
<prevsent>the penn discourse treebank (pdtb) adds low-leveldiscourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels.
</prevsent>
</prevsection>
<citsent citstr=" J03-4002 ">
with this added knowledge, the pdtb (together with the ptb and propbank) should support more in-depth nlp research and more powerful applications.work on the pdtb is grounded in lexicalized approach to discourse ? dltag (webber and joshi, 1998; <papid> W98-0315 </papid>webber et al, 1999<papid> P99-1006 </papid>a; webber et al,2000; webber et al, 2003).<papid> J03-4002 </papid></citsent>
<aftsection>
<nextsent>here, low-level discourse structure and semantics are taken to result (in part) from composing elementary predicate argument relations whose predicates come mainly from discourse connectives1 and whose arguments1despite this, we have deliberately adopted policy of hav come from units of discourse ? clausal, sentential or multi-sentential units.
</nextsent>
<nextsent>the pdtb therefore differs from the rst-annotated corpus (carlson et al,2003) which starts with (abstract) rhetorical relations (mann and thompson, 1988) and annotates subset of the penn wsj corpus with those relations that can be taken to hold between (primarily) pairs of discourse spans identified in the corpus.the current paper focuses on what can be discovered through analyzing pdtb annotation, both on its own and together with the penn treebank.section 2 of the paper briefly reviews the theoretical background of the project, its current state, the guidelines given to annotators, the annotation tool they used (wordfreak), and the extent of inter annotator agreement.
</nextsent>
<nextsent>section 3 shows how we have used pdtb annotation, along with the ptb, to extract several features pertaining to discourse connectives and their arguments, and discusses the relevance of these features for nlp research and applications.
</nextsent>
<nextsent>section 4 concludes with the summary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1366">
<title id=" W04-0212.xml">annotation and data mining of the penn discourse treebank </title>
<section> project overview.  </section>
<citcontext>
<prevsection>
<prevsent>implicit connectives are taken to occur between adjacent sentences not related by any explicit connective.
</prevsent>
<prevsent>they are annotated with whatever explicit connective the annotator feels could be inserted, with the original meaning retained.
</prevsent>
</prevsection>
<citsent citstr=" W04-2703 ">
assessment of inter-annotator agreement groups these annotations into five coarse classes (miltsakaki et al, 2004).<papid> W04-2703 </papid>currently, we are not annotating implicit connectives intra-sententially (such as between main clause and free adjunct) or across paragraphs.what counts as legal argument?</citsent>
<aftsection>
<nextsent>the simplest argument to connective is what we take to be the minimum unit of discourse.
</nextsent>
<nextsent>because we take discourse relations to hold between abstract objects, we require that an argument contain at least one clause-level predication (usually verb ? tense dor untensed), though it may span as much as sequence of clauses or sentences.
</nextsent>
<nextsent>the two exceptions are nominal phrases that express an event or state,and discourse deictics that denote an abstract object.
</nextsent>
<nextsent>what we describe to annotators as arguments to discourse connectives are actually the textual span from which the argument is derived (webber et al, 1999<papid> P99-1006 </papid>a; webber et al, 2003).<papid> J03-4002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1373">
<title id=" W04-0212.xml">annotation and data mining of the penn discourse treebank </title>
<section> data mining.  </section>
<citcontext>
<prevsection>
<prevsent>providing empirical evidence for the dltagclaim that discourse adverbials get one argument anaphoric ally, while structural connectives such as conjunctions establish relations between adjacent units of text (creswell et al, 2002).
</prevsent>
<prevsent>acquiring common usage patterns of connectives and identifying their dependencies, in order to support natural?
</prevsent>
</prevsection>
<citsent citstr=" P97-1011 ">
choices in natural language generation (di eugenio et al, 1997; <papid> P97-1011 </papid>moser and moore, 1995; <papid> P95-1018 </papid>williams and reiter, 2003).</citsent>
<aftsection>
<nextsent>developing decision procedures for resolving and interpreting discourse adverbials (milt sakaki et al, 2003) which can be built on top of discourse parsing systems (forbes et al, 2003).
</nextsent>
<nextsent>developing word sense disambiguation?
</nextsent>
<nextsent>procedures for distinguishing among different senses of connective and hence interpreting connectives correctly (e.g., distinguishing between temporal and explanatory since, between hypothetical and counter factual if, between epistemic and semantic because, etc.)
</nextsent>
<nextsent>providing empirical evidence for theories of anaphoric phenomena such as verb phrase ellipsis that see them as sensitive to the type of discourse relation in which they are expressed (hardt and romero, 2002; kehler, 2002).the value of carrying out such studies using single corpus with multiple layers of annotation is that relationships between phenomena are clearer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1374">
<title id=" W04-0212.xml">annotation and data mining of the penn discourse treebank </title>
<section> data mining.  </section>
<citcontext>
<prevsection>
<prevsent>providing empirical evidence for the dltagclaim that discourse adverbials get one argument anaphoric ally, while structural connectives such as conjunctions establish relations between adjacent units of text (creswell et al, 2002).
</prevsent>
<prevsent>acquiring common usage patterns of connectives and identifying their dependencies, in order to support natural?
</prevsent>
</prevsection>
<citsent citstr=" P95-1018 ">
choices in natural language generation (di eugenio et al, 1997; <papid> P97-1011 </papid>moser and moore, 1995; <papid> P95-1018 </papid>williams and reiter, 2003).</citsent>
<aftsection>
<nextsent>developing decision procedures for resolving and interpreting discourse adverbials (milt sakaki et al, 2003) which can be built on top of discourse parsing systems (forbes et al, 2003).
</nextsent>
<nextsent>developing word sense disambiguation?
</nextsent>
<nextsent>procedures for distinguishing among different senses of connective and hence interpreting connectives correctly (e.g., distinguishing between temporal and explanatory since, between hypothetical and counter factual if, between epistemic and semantic because, etc.)
</nextsent>
<nextsent>providing empirical evidence for theories of anaphoric phenomena such as verb phrase ellipsis that see them as sensitive to the type of discourse relation in which they are expressed (hardt and romero, 2002; kehler, 2002).the value of carrying out such studies using single corpus with multiple layers of annotation is that relationships between phenomena are clearer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1376">
<title id=" W04-0212.xml">annotation and data mining of the penn discourse treebank </title>
<section> data mining.  </section>
<citcontext>
<prevsection>
<prevsent>conn arg1-arg2 arg2-arg1 total when 545 (54%) 465 (46%) 1010 because 822 (90%) 93 (10%) 915 even though 77 (75%) 26 (25%) 103 although 129 (37%) 218 (63%) 347 so that 33 (100%) 0 (0%) 33 total 1606 (67%) 802 (33%) 2408table 3: distribution for argument order for subordinating conjunctions there are few interesting things to note here.first, even if one considers only the four subordinating conjunctions with  100 tokens, no two of them pattern in the same way.
</prevsent>
<prevsent>second, with when, the almost equal distribution of pre posed and post posed tokens suggests either free variation of the two patterns or different usesof the two patterns, with each use favoring different pattern.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
the latter would accord with theoretical distinction that has been made between post posed when expressing purely temporal relation between the two clauses, and pre posed when expressing contingent relation between them (moens and steedman, 1988).<papid> J88-2003 </papid></citsent>
<aftsection>
<nextsent>integrated evidence from the ptb and propbank may help distinguish the two possibilities.third, there is striking contrast between the patterning of although and even though, especially if one assumes that even though (like even when, even after, even if, etc.) involves application of the topi calizer even to the subordinate clause, just as it can apply to other constituents.
</nextsent>
<nextsent>further annotation and analysis of the pdtb will reveal whether all subordinating conjunctions that co-occur with even pattern like even though, or whether this is specific to the concessive.finally, when williams and reiter (2003) examined 342 texts from the rst annotation of the penn treebank corpus (carlson et al, 2003), they reported that 77% of the instances of concessive relations that they examined appeared in the order arg2-arg1.
</nextsent>
<nextsent>(the eleven instances of although that they examined and the three instances of even though appeared in concessive relations, along with instances of but, despite, however, etc.) if we were to collapse together all instances of although and even though annotated in the pdtb (totalling 450),we would find that 46% (206) patterned as arg1arg2, and 54% of them (244) patterned as arg2arg1.
</nextsent>
<nextsent>this might lead us to draw similar conclusion to williams and reiter (2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1377">
<title id=" W04-0212.xml">annotation and data mining of the penn discourse treebank </title>
<section> data mining.  </section>
<citcontext>
<prevsection>
<prevsent>this suggests (1) that making the feature extraction procedure specific to particular connectives, as in the pdtb, will reveal distributional patterns that are lost when more abstract relations are the focus of the annotation, and (2) that larger set of annotated tokens can show more reliable distributional patterns.in sum, data mining of pdtb with respect to subordinating conjunctions has shown radically different distribution patterns regarding the relative position of the arguments.
</prevsent>
<prevsent>some of these have confirmed and strengthened previous theoretical claim sand some have suggested new and promising research directions.
</prevsent>
</prevsection>
<citsent citstr=" P04-1011 ">
further work in this area will also be extremely relevant for nlg sentence planning components employing discourse relations (walker et al (2003), stent et al (2004), <papid> P04-1011 </papid>among others), where the sentence planner needs to make decisions regarding cue placement.</citsent>
<aftsection>
<nextsent>finally, while our approach is syntactic?, with the distribution of the connectives and their arguments being explored interms of whether they are subordinating conjunctions, coordinating conjunctions, or adverbial connectives, one can also explore the patterning of connectives in terms of semantic categories, once their semantic role annotation is complete (cf.
</nextsent>
<nextsent>section 2.2).
</nextsent>
<nextsent>the latter could be especially interesting to cross-linguistic studies of discourse, as well as to applications such as multilingual generation and mt are envisaged.11
</nextsent>
<nextsent>in this paper we have presented the penn discourse treebank (pdtb), large-scale discourse level annotated corpus that is being developed towards the creation of multi-layered annotated corpus, integrating the penn treebank, propbank and 11we thank an anonymous reviewer for pointing this out.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1378">
<title id=" W04-2311.xml">the importance of discourse context for statistical natural language generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>humans choose contextually-appropriate options from these many ways with little conscious effort and with rather effective communicative results.
</prevsent>
<prevsent>statistical approaches to natural language generation are based on the assumption that often many of these options will be equally good, e.g.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
(bangalore and rambow, 2000).<papid> C00-1007 </papid></citsent>
<aftsection>
<nextsent>in this paper, we argue that, in fact, not all options are equivalent, based on linguistic data both from english, language with relatively static word order, and from finnish, language with much more flexible word order.
</nextsent>
<nextsent>we show that statistical nlg algorithm based only on counts of trees cannot capture the appropriate use of word order.
</nextsent>
<nextsent>we provide an alternative method which has been implemented elsewhere and show thatit dramatically outperforms the statistical approach.
</nextsent>
<nextsent>finally, we explain how the alternative method could be used to augment present statistical approaches and draw some lessons for future development of statistical nlg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1380">
<title id=" W04-2311.xml">the importance of discourse context for statistical natural language generation </title>
<section> statistical nlg: brief summary.  </section>
<citcontext>
<prevsection>
<prevsent>we provide an alternative method which has been implemented elsewhere and show thatit dramatically outperforms the statistical approach.
</prevsent>
<prevsent>finally, we explain how the alternative method could be used to augment present statistical approaches and draw some lessons for future development of statistical nlg.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
in recent years, new approach to nlg has emerged, which hopes to build on the success of the use of probabilistic models in natural language understanding (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000; <papid> C00-1007 </papid>ratnaparkhi, 2000).<papid> A00-2026 </papid></citsent>
<aftsection>
<nextsent>building an nlg system is highly labor-intensive.
</nextsent>
<nextsent>for the system to be robust, large amounts of world and linguistic knowledge must be hand coded.
</nextsent>
<nextsent>the goal of statistical approaches is to minimizehand-coding and instead rely upon information automatically extracted from linguistic corpora when selecting linguistic realization of some conceptual representation.
</nextsent>
<nextsent>the underlying concept of these statistical approaches is that the form generated to express particular meaning should be selected on the basis of counts of that form (either strings or trees) in corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1382">
<title id=" W04-2311.xml">the importance of discourse context for statistical natural language generation </title>
<section> statistical nlg: brief summary.  </section>
<citcontext>
<prevsection>
<prevsent>we provide an alternative method which has been implemented elsewhere and show thatit dramatically outperforms the statistical approach.
</prevsent>
<prevsent>finally, we explain how the alternative method could be used to augment present statistical approaches and draw some lessons for future development of statistical nlg.
</prevsent>
</prevsection>
<citsent citstr=" A00-2026 ">
in recent years, new approach to nlg has emerged, which hopes to build on the success of the use of probabilistic models in natural language understanding (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000; <papid> C00-1007 </papid>ratnaparkhi, 2000).<papid> A00-2026 </papid></citsent>
<aftsection>
<nextsent>building an nlg system is highly labor-intensive.
</nextsent>
<nextsent>for the system to be robust, large amounts of world and linguistic knowledge must be hand coded.
</nextsent>
<nextsent>the goal of statistical approaches is to minimizehand-coding and instead rely upon information automatically extracted from linguistic corpora when selecting linguistic realization of some conceptual representation.
</nextsent>
<nextsent>the underlying concept of these statistical approaches is that the form generated to express particular meaning should be selected on the basis of counts of that form (either strings or trees) in corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1385">
<title id=" W04-0713.xml">an algorithm for resolving individual and abstract anaphora in danish texts and dialogues </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the dar-algorithm for resolving inter sentential pronominal anaphors referring to individual and abstract entities in danish texts and dialogues.
</prevsent>
<prevsent>individual entities are resolved combining models which identify high degree of salience with high degree of givenness (topicality) of entities in the hearers cognitive model, e.g.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
(grosz et al , 1995), <papid> J95-2003 </papid>with hajicova?</citsent>
<aftsection>
<nextsent>et al (1990) salience account which assigns the highest degree of salience to entities in the focal part of an utterance in information structure terms.
</nextsent>
<nextsent>these focal entities often introduce new information in discourse.
</nextsent>
<nextsent>anaphors referring to abstract entities are resolved with an extension of the algorithm presented by eckert and strube (2000).
</nextsent>
<nextsent>manual tests ofthe dar-algorithm and other well-known resolution algorithms on the same data show that dar performs significantly better on most types of anaphor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1389">
<title id=" W04-0713.xml">an algorithm for resolving individual and abstract anaphora in danish texts and dialogues </title>
<section> background for dar.  </section>
<citcontext>
<prevsection>
<prevsent>nearly all salience-based models identify high degree of salience with high degree of givenness of des.
</prevsent>
<prevsent>in fact, although the various algorithms use different criteria for ranking des such as linear order, hierarchy of grammatical roles, information structure, princes familiarity scale (prince, 1981), they all assign the highest prominence to the des which are most topical, known, bound, familiar and thus given, i.a.
</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
(grosz et al , 1995; <papid> J95-2003 </papid>brennan et al , 1987; <papid> P87-1022 </papid>strube and hahn, 1996; <papid> P96-1036 </papid>strube, 1998).<papid> P98-2204 </papid></citsent>
<aftsection>
<nextsent>grosz et al .
</nextsent>
<nextsent>(1995) also suggest that continuing speaking about the same elements in discourse segment is perceived as more coherent than shifting the focus of attention.
</nextsent>
<nextsent>they implement this by the following ranking of transition states: continue   retain   shift.one salience model departs from the given ness2 assumption.
</nextsent>
<nextsent>it has been proposed by hajicova?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1390">
<title id=" W04-0713.xml">an algorithm for resolving individual and abstract anaphora in danish texts and dialogues </title>
<section> background for dar.  </section>
<citcontext>
<prevsection>
<prevsent>nearly all salience-based models identify high degree of salience with high degree of givenness of des.
</prevsent>
<prevsent>in fact, although the various algorithms use different criteria for ranking des such as linear order, hierarchy of grammatical roles, information structure, princes familiarity scale (prince, 1981), they all assign the highest prominence to the des which are most topical, known, bound, familiar and thus given, i.a.
</prevsent>
</prevsection>
<citsent citstr=" P96-1036 ">
(grosz et al , 1995; <papid> J95-2003 </papid>brennan et al , 1987; <papid> P87-1022 </papid>strube and hahn, 1996; <papid> P96-1036 </papid>strube, 1998).<papid> P98-2204 </papid></citsent>
<aftsection>
<nextsent>grosz et al .
</nextsent>
<nextsent>(1995) also suggest that continuing speaking about the same elements in discourse segment is perceived as more coherent than shifting the focus of attention.
</nextsent>
<nextsent>they implement this by the following ranking of transition states: continue   retain   shift.one salience model departs from the given ness2 assumption.
</nextsent>
<nextsent>it has been proposed by hajicova?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1391">
<title id=" W04-0713.xml">an algorithm for resolving individual and abstract anaphora in danish texts and dialogues </title>
<section> background for dar.  </section>
<citcontext>
<prevsection>
<prevsent>nearly all salience-based models identify high degree of salience with high degree of givenness of des.
</prevsent>
<prevsent>in fact, although the various algorithms use different criteria for ranking des such as linear order, hierarchy of grammatical roles, information structure, princes familiarity scale (prince, 1981), they all assign the highest prominence to the des which are most topical, known, bound, familiar and thus given, i.a.
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
(grosz et al , 1995; <papid> J95-2003 </papid>brennan et al , 1987; <papid> P87-1022 </papid>strube and hahn, 1996; <papid> P96-1036 </papid>strube, 1998).<papid> P98-2204 </papid></citsent>
<aftsection>
<nextsent>grosz et al .
</nextsent>
<nextsent>(1995) also suggest that continuing speaking about the same elements in discourse segment is perceived as more coherent than shifting the focus of attention.
</nextsent>
<nextsent>they implement this by the following ranking of transition states: continue   retain   shift.one salience model departs from the given ness2 assumption.
</nextsent>
<nextsent>it has been proposed by hajicova?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1393">
<title id=" W04-0713.xml">an algorithm for resolving individual and abstract anaphora in danish texts and dialogues </title>
<section> the dar-algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>an example of rule identifying ipas is the following: adjectival constructions in which the prepositional complement only subcategorises for concrete entities such as let for (easy for x), fuld af (full of x).
</prevsent>
<prevsent>4.1 search space and de lists.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
dar presupposes the discourse structure described by grosz and sidner (1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>the minimal discourse unit is the utterance . paragraphs correspond to discourse segments in texts.
</nextsent>
<nextsent>discourse segments in dialogues were manually marked.
</nextsent>
<nextsent>the dialogues were structured with synchronising units (su) according to the definitions in es00.
</nextsent>
<nextsent>the immediate antecedent search space of apronoun in utterance un is the previous utterance, un1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1397">
<title id=" W04-0713.xml">an algorithm for resolving individual and abstract anaphora in danish texts and dialogues </title>
<section> tests and evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>expletive and cataphoricuses of pronouns have been marked and excluded from the tests.
</prevsent>
<prevsent>dialogue act units were marked and classified by three persons following the strategy proposed in (eckert and strube, 2000).
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
the reliability for the two annotation tasks (?-statistics (carletta, 1996)) <papid> J96-2004 </papid>was of 0.94 and 0.90 respectively.</citsent>
<aftsection>
<nextsent>pronominal anaphors were marked, classified and resolved by two annotators.
</nextsent>
<nextsent>the ?-statistics for the pronoun classification was 0.86.
</nextsent>
<nextsent>when the annotators did not agree upon resolution, the pronoun was marked as ambiguous and excluded from evaluation.
</nextsent>
<nextsent>the results obtained for bfp and str98 are given in table 1, while the results of dars resolve ipa are given in table 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1398">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P03-1013 ">
we present and evaluate an implemented statistical minimal parsing strategy exploiting dgcharateristics to permit fast, robust, deep linguistic analysis of unrestricted text, and compare its probability model to (collins, 1999) and an adaptation, (dubey and keller, 2003)<papid> P03-1013 </papid></citsent>
<aftsection>
<nextsent>we show that dg allows for the expression of the majority of english ldds in context-free wayand oers simple yet powerful statistical models.
</nextsent>
<nextsent>we present fast, deep-linguistic statistical parser that pro ts from dg characteristics and that uses am minimal parsing strategy.
</nextsent>
<nextsent>first, we relyon nite-state based approaches as longas possible, secondly where parsing is necessary we keep it context-free as long as possible 1 .for low-level syntactic tasks, tagging and base np chunking is used, parsing only takes place between heads of chunks.
</nextsent>
<nextsent>robust, successful parsers (abney, 1995; collins, 1999) have shown that this division of labour is particularly attractive for dg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1401">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>deep-linguistic, formal grammar parser shave carefully crafted grammars written by professional linguists.
</prevsent>
<prevsent>but unrestricted real-world texts still pose problem to nlp systems thatare based on formal grammars.
</prevsent>
</prevsection>
<citsent citstr=" P02-1035 ">
few handcrafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (riezler et al, 2002), (<papid> P02-1035 </papid>burke et al, 2004) and (hockenmaier and steedman, 2002) <papid> P02-1043 </papid>for exceptions), and speed remains serious challenge.</citsent>
<aftsection>
<nextsent>the typical problems can be grouped as follows.
</nextsent>
<nextsent>grammar complexity fully comprehensive grammars are dicult to maintain and consid 1 non-subject wh-question pronouns and support verbs cannot be treated context-free with our approach.
</nextsent>
<nextsent>we use simple pre-parsing step to analyze them erably increase parsing complexity.parsing complexity typical formal grammar parser complexity is much higher than the o(n 3 ) for cfg.
</nextsent>
<nextsent>the complexity of some formal grammars is still unknown.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1402">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>deep-linguistic, formal grammar parser shave carefully crafted grammars written by professional linguists.
</prevsent>
<prevsent>but unrestricted real-world texts still pose problem to nlp systems thatare based on formal grammars.
</prevsent>
</prevsection>
<citsent citstr=" P02-1043 ">
few handcrafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (riezler et al, 2002), (<papid> P02-1035 </papid>burke et al, 2004) and (hockenmaier and steedman, 2002) <papid> P02-1043 </papid>for exceptions), and speed remains serious challenge.</citsent>
<aftsection>
<nextsent>the typical problems can be grouped as follows.
</nextsent>
<nextsent>grammar complexity fully comprehensive grammars are dicult to maintain and consid 1 non-subject wh-question pronouns and support verbs cannot be treated context-free with our approach.
</nextsent>
<nextsent>we use simple pre-parsing step to analyze them erably increase parsing complexity.parsing complexity typical formal grammar parser complexity is much higher than the o(n 3 ) for cfg.
</nextsent>
<nextsent>the complexity of some formal grammars is still unknown.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1403">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ranking returning all syntactically possible analyses for sentence is not what is expected of syntactic analyzer.
</prevsent>
<prevsent>a clear indication of preference is needed.pruning in order to keep search spaces manageable it is necessary to discard unconvincing alternatives already during the parsing process.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
a number of robust statistical parsers that oer solutions to these problems have become available (charniak, 2000; <papid> A00-2018 </papid>collins, 1999; henderson, 2003).<papid> N03-1014 </papid></citsent>
<aftsection>
<nextsent>in statistical parser, the ranking of intermediate structures occurs naturally and based on empirical grounds, while most rule-based systems relyon ad hoc heuristics.with an aggressive beam for parse-time pruning (so in our parser), real-world parsing time can be reduced to near-linear.
</nextsent>
<nextsent>if one were to assume constantly full xed beam, or uses an oracle (nivre, 2004) it is linear in practice 3 . also worst-case complexity for exhaustive parsing is low, as these parsers are cfg based (eisner, 2000) 4 . but they typically pro-.
</nextsent>
<nextsent>duce cfg constituency data as output, trees that do not express long-distance dependencies.
</nextsent>
<nextsent>although grammatical function and empty 2 for tree-adjoining grammars (tag) it is o(n 7 ) or o(n 8 ) depending on the implementation (eisner, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1404">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ranking returning all syntactically possible analyses for sentence is not what is expected of syntactic analyzer.
</prevsent>
<prevsent>a clear indication of preference is needed.pruning in order to keep search spaces manageable it is necessary to discard unconvincing alternatives already during the parsing process.
</prevsent>
</prevsection>
<citsent citstr=" N03-1014 ">
a number of robust statistical parsers that oer solutions to these problems have become available (charniak, 2000; <papid> A00-2018 </papid>collins, 1999; henderson, 2003).<papid> N03-1014 </papid></citsent>
<aftsection>
<nextsent>in statistical parser, the ranking of intermediate structures occurs naturally and based on empirical grounds, while most rule-based systems relyon ad hoc heuristics.with an aggressive beam for parse-time pruning (so in our parser), real-world parsing time can be reduced to near-linear.
</nextsent>
<nextsent>if one were to assume constantly full xed beam, or uses an oracle (nivre, 2004) it is linear in practice 3 . also worst-case complexity for exhaustive parsing is low, as these parsers are cfg based (eisner, 2000) 4 . but they typically pro-.
</nextsent>
<nextsent>duce cfg constituency data as output, trees that do not express long-distance dependencies.
</nextsent>
<nextsent>although grammatical function and empty 2 for tree-adjoining grammars (tag) it is o(n 7 ) or o(n 8 ) depending on the implementation (eisner, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1405">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>3 whnp np *t* 10,659 wh trace the woman who you saw *t*.
</prevsent>
<prevsent>(4) *u* 9,202 empty units $ 25 *u* (5) 0 7,057 empty complementizers sam said 0 sasha snores (6) s *t* 5,035 moved clauses sam had to go, sasha said *t* 7 whadvp advp *t* 3,181 wh-trace sam explained how to leave *t*.
</prevsent>
</prevsection>
<citsent citstr=" P02-1018 ">
(8) sbar 2,513 empty clauses sam had to go, said sasha (sbar) (9) whnp 0 2,139 empty relative pronouns the woman 0 we saw (10) whadvp 0 726 empty relative pronouns the reason 0 to leave table 1: the distribution of the 10 most frequent types of empty nodes and their antecedents in the penn treebank (adapted from (johnson, 2002)).<papid> P02-1018 </papid></citsent>
<aftsection>
<nextsent>bracketed line numbers only involve ldds as grammar artifact nodes annotation expressing long-distance dependencies are provided in treebanks such as the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>most statistical treebank trained parsers fully or largely ignore them 5, which entails two problems: rst, the training cannot pro from valuable annotation data.</nextsent>
<nextsent>second, the extraction of long-distance dependencies (ldd) and the mapping to shallow semantic representations is not always possible from the output of these parsers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1406">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(4) *u* 9,202 empty units $ 25 *u* (5) 0 7,057 empty complementizers sam said 0 sasha snores (6) s *t* 5,035 moved clauses sam had to go, sasha said *t* 7 whadvp advp *t* 3,181 wh-trace sam explained how to leave *t*.
</prevsent>
<prevsent>(8) sbar 2,513 empty clauses sam had to go, said sasha (sbar) (9) whnp 0 2,139 empty relative pronouns the woman 0 we saw (10) whadvp 0 726 empty relative pronouns the reason 0 to leave table 1: the distribution of the 10 most frequent types of empty nodes and their antecedents in the penn treebank (adapted from (johnson, 2002)).<papid> P02-1018 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
bracketed line numbers only involve ldds as grammar artifact nodes annotation expressing long-distance dependencies are provided in treebanks such as the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>most statistical treebank trained parsers fully or largely ignore them 5, which entails two problems: rst, the training cannot pro from valuable annotation data.</citsent>
<aftsection>
<nextsent>second, the extraction of long-distance dependencies (ldd) and the mapping to shallow semantic representations is not always possible from the output of these parsers.
</nextsent>
<nextsent>this limitation is aggravated by lack of co-indexation information and parsing errors across an ldd.
</nextsent>
<nextsent>in fact, some syntactic relations cannot be recovered on con gurational grounds only.
</nextsent>
<nextsent>for these reasons, (johnson, 2002) <papid> P02-1018 </papid>refers to them as \half-grammars .an approach that relies heavily on dg characteristics is explored in this paper.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1409">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> the benet of dg characteristics.  </section>
<citcontext>
<prevsection>
<prevsent>phrase modpp draft of paper noun{participle modpart report written verb{complementizer compl to eat apples noun{preposition prep to the house table 2: important pro3gres dependency types spanning several constituency levels, including empty nodes and functional penn treebank labels, by purely local dg relation 6 . the selec-.
</prevsent>
<prevsent>tive mapping patterns for mle counts of passive subjects and control subjects from the penn treebank, the most frequent np traces [line 1], are e.g.
</prevsent>
</prevsection>
<citsent citstr=" P04-1041 ">
(@ stands for arbitrary nestedness): ? h h ( ( ( ( np-sbj-x@ noun vp@ h ( ( ( passive verb np-none *-x ? h h ( ( ( ( np-sbj-x@ noun vp@ h ( ( ( control-verb np-sbj-none *-xour approach employs nite-state approximations of long-distance dependencies, described in (schneider, 2003) for dg and (cahill et al, 2004) <papid> P04-1041 </papid>for lexical functional grammar (lfg)it leaves empty nodes underspeci ed but largelyrecoverable.</citsent>
<aftsection>
<nextsent>table 2 gives an overview of important dependencies.
</nextsent>
<nextsent>2.5 monostratalism and functionalism.
</nextsent>
<nextsent>while multistratal dgs exist and several dependency levels can be distinguished (mel cuk, 1988) we follow conservative view close to the original (tesniere, 1959), which basically parses directly for simple lfg f-structure without needing c-structure detour.
</nextsent>
<nextsent>6 in addition to taking less decisions due to the gainedhigh-level shallowness, it is ensured that the lexical information that matters is available in one central place,allowing the parser to take one well-informed decision instead of several brittle decisions plagued by sparseness.collapsing deeply nested structures into single dependency relation is less complex but has similar eect as selecting what goes in to the parse history in history based approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1410">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> the benet of dg characteristics.  </section>
<citcontext>
<prevsection>
<prevsent>a statistical lexicalized post-processing module in pro3gres transforms selected subtrees into graphs, e.g. in order to express control.
</prevsent>
<prevsent>2.7 transformation to semantic layer.
</prevsent>
</prevsection>
<citsent citstr=" W04-0508 ">
pro3gres is currently being applied in question answering system speci cally targeted at technical domains (rinaldi et al, 2004<papid> W04-0508 </papid>b).</citsent>
<aftsection>
<nextsent>one of the main advantages of dg parser such as pro3gres over other parsing approaches is thata mapping from the syntactic layer to semantic layer (meaning representation) is partly sim plied (molla et al, 2000).
</nextsent>
<nextsent>2.8 tesniere translations.
</nextsent>
<nextsent>the possible functional changes of word called translations (tesniere, 1959) are an exception to endocentricity.
</nextsent>
<nextsent>they are an important contribution to trace less theory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1412">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> the statistical dependency model.  </section>
<citcontext>
<prevsection>
<prevsent>b ! ab; e:g: nn ! dt nn (2) ! ab; e:g: b ! b pp (3) labeled dg rules additionally use syntactic relation label r. non-lexicalized model would be: p(rja ! ab)  = #(r;a ! ab) #(a ! ab) (4) research on pcfg and pp-attachment has shown the importance of probabilizing on lexical heads (a and b).
</prevsent>
<prevsent>p(rja ! ab;a; b)  = #(r;a ! ab; a; b) #(a ! ab; a; b) (5)all that ! ab expresses is that the dependency relation is towards the right.
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
p(rjright; a; b)  = #(r; right; a; b) #(right; a; b) (6) e.g. for the verb-pp attachment relation pobj (following (collins and brooks, 1995) <papid> W95-0103 </papid>including the description noun 7 ) p(pobjjright; verb; prep; desc:noun)  = #(pobj; right; verb; prep; desc:noun) #(right; verb; prep; desc:noun) the distance (measured in chunks) between head and dependent is limiting factor for the probability of dependency between them.</citsent>
<aftsection>
<nextsent>p(r; distjright; a; b)  = #(r; dist; right; a; b) #(right; a; b) (7) 7 pp is considered to be an exocentric category, since both the preposition and the description noun can be seen as head; in lfg they appear as double-headmany relations are only allowed towards one direction, the left/right factor is absent for them.typical distances mainly depend on the relation.
</nextsent>
<nextsent>objects usually immediately follow the verb, while pp attached to the verb may easily follow only at the second or third position, after the object and other pps etc. by application ofthe chain rule and assuming that distance is independent of the lexical heads we get: p(r; distja; b)  = #(r; a; b) #(a; b)
</nextsent>
<nextsent>#(r; dist) #r (8) we now explore pro3gres  main probability model by comparing it to (collins, 1999), and an adaptation of it, (dubey and keller, 2003)<papid> P03-1013 </papid></nextsent>
<nextsent>3.1 relation of pro3gres to collins.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1418">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> the statistical dependency model.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 relation to dubey &amp; keller 03.
</prevsent>
<prevsent>(dubey and keller, 2003)<papid> P03-1013 </papid>address the question whether models such as collins also improve performance on freer word order languages, in their case german.</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
german is considerably more in ect ional which means that discarding functional information is more harmful, and which explains why the negra annotation has been conceived to be quite at (skut et al, 1997).<papid> A97-1014 </papid></citsent>
<aftsection>
<nextsent>(dubey and keller, 2003)<papid> P03-1013 </papid>observe that models such as collins when applied directly perform worse than an unlexical ized pcfg baseline.</nextsent>
<nextsent>the fact that learning curves converge early indicates that this is not mainly sparse data eect.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1423">
<title id=" W04-1505.xml">fast deep linguistic statistical dependency parsing </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>two such evaluations are reported now.
</prevsent>
<prevsent>first, general-purpose evaluation using hand-compiled gold standard corpus (carroll etal., 1999), which contains the grammatical relation data of 500 random sentences from the susanne corpus.
</prevsent>
</prevsection>
<citsent citstr=" E03-1025 ">
the performance (table 3), according to (preiss, 2003), <papid> E03-1025 </papid>is similar to large selection of statistical parsers and grammatical relation nder.</citsent>
<aftsection>
<nextsent>relations involving ldds form part of these relations.
</nextsent>
<nextsent>a selection of them is also given: wh-subject (whs), wh-object(who), passive subject (psubj), control subject (csubj), and the anaphor of the relative clause pronoun (rclsubja).
</nextsent>
<nextsent>9 since normalized probabilities are used (t(h ); l(h )jt(h ); t(h 0 ); l(h 0 ; d(i))) = (t(h ); d(i)jt(h ); t(h 0 ); l(h ); l(h 0 )) carroll percentages for some relations, general, on carroll testset only ldd-involving subject object noun-pp verb-pp subord.
</nextsent>
<nextsent>clause whs who psubj csubj rclsubja precision 91 89 73 74 68 92 60 n/a 80 89 recall 81 83 67 83 n/a 90 86 83 n/a 63 genia percentages for some relations, general, on genia corpus subject object noun-pp verb-pp subord.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1426">
<title id=" W04-0854.xml">kunlp system in senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>relatives, especially those in synonym class, usually have related meanings and tend to share similar contexts.
</prevsent>
<prevsent>hence, some wordnet-based approaches extract relatives of each sense of polysemous word from wordnet, collect example sentences of the relatives from raw corpus, and learn the senses from the example sentences for wsd.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
yarowsky (1992) <papid> C92-2070 </papid>first proposed this approach, but used international rogets thesaurus as hierarchical lexical database instead of wordnet.</citsent>
<aftsection>
<nextsent>however, the approach seems to suffer from examples irrelevant to the senses of polysemous word since many of the relatives arepolysemous.
</nextsent>
<nextsent>leacock et al (1998) <papid> J98-1006 </papid>attempted to exclude irrelevant or spurious examples by using only monosemous relatives in wordnet.</nextsent>
<nextsent>however, some senses do not have short distance monosemous relatives through relation such as synonym, child, and parent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1427">
<title id=" W04-0854.xml">kunlp system in senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>yarowsky (1992) <papid> C92-2070 </papid>first proposed this approach, but used international rogets thesaurus as hierarchical lexical database instead of wordnet.</prevsent>
<prevsent>however, the approach seems to suffer from examples irrelevant to the senses of polysemous word since many of the relatives arepolysemous.</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
leacock et al (1998) <papid> J98-1006 </papid>attempted to exclude irrelevant or spurious examples by using only monosemous relatives in wordnet.</citsent>
<aftsection>
<nextsent>however, some senses do not have short distance monosemous relatives through relation such as synonym, child, and parent.
</nextsent>
<nextsent>a possible alternative of using onlymonosemous relatives in the long distance, how ever, is problematic because the longer the distance of two synsets in wordnet, the weaker the relationship between them.
</nextsent>
<nextsent>in other words, the monosemous relatives in the long distance may provide irrelevant examples for wsd.our approach is somewhat similar to the wordnet based approach of leacock et al (1998) <papid> J98-1006 </papid>in that it acquires relatives of target word from wordnet and extracts co-occurrence frequencies of the relatives from raw corpus, but our system uses poly semous as well as monosemous relatives.</nextsent>
<nextsent>to avoida negative effect of polysemous relatives on the cooccurrence frequency calculation, our system handles the example sentences of each relative separately instead of putting together the example sentences of all relatives into pool.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1431">
<title id=" W04-1810.xml">quantitative portraits of lexical elements </title>
<section> methods of term weighting.  </section>
<citcontext>
<prevsection>
<prevsent>has an information theoretic meaning within the given set of documents (figure 2).
</prevsent>
<prevsent>3.2 term representativeness.
</prevsent>
</prevsection>
<citsent citstr=" C00-1047 ">
hisamitsu, et al (2000<papid> C00-1047 </papid>a) proposed measure of term representativeness?, in order to overcome the computerm 2004 poster session - 3rd international workshop on computational terminology 75 text text text text text text text set of actual texts (targets of ir) term term term termtermterm term term terms as attributes of concrete set of documents figure 2: the position of  </citsent>
<aftsection>
<nextsent>. text text text text text text text set of actual texts (a manifestation of discourse) textual sphere / theoretical sphere of discourse term term term termtermterm term term terms as attributes of theoretical discourse represented by the given set of documents figure 3: the position of term representativeness.excessive sensitivity of weighting measures to token frequencies.
</nextsent>
<nextsent>they hypothesised that, for term   , if the term is representative, ffi  (the set of all documents containing   ) have some specific characteristic.
</nextsent>
<nextsent>they define measure which calculates the distance between distributional characteristic of words around   and the same distributional characteristic in the whole document set.in order to remove the factor of data size dependency, hisamitsu et al (2000<papid> C00-1047 </papid>a) defines the baseline function,?</nextsent>
<nextsent>which indicates the distance between the distribution of words in the original document set and the distribution of words in randomly selected document subsets for each size.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1433">
<title id=" W04-0109.xml">multilingual noise robust supervised morphological analysis using the word frame model </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>while we make no claims that the model works equally well for all languages, its ability to analyze inflections in 32 diverse languages with median accuracy of 97.5% attests to its flexibility in learning wide range of morphological phenomena.
</prevsent>
<prevsent>the effectiveness of the model when trained from noisy data makes it well-suited for co-training with low-accuracy unsupervised algorithms.
</prevsent>
</prevsection>
<citsent citstr=" P00-1027 ">
the development of the word frame model was motivated by work originally presented in yarowsky and wicentowski (2000).<papid> P00-1027 </papid></citsent>
<aftsection>
<nextsent>in that work, suite of unsupervised learning algorithms and supervised morphological learner are co-trained to achieve high accuracies for english and spanish verb inflections.
</nextsent>
<nextsent>the supervised learner employed nave approach to morphology, only capable of learning word-final stem changes between inflections and roots.
</nextsent>
<nextsent>this end-of-string model?
</nextsent>
<nextsent>of morphology was used again in yarowsky et al (2001) <papid> H01-1035 </papid>where it was applied to english, french and czech.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1434">
<title id=" W04-0109.xml">multilingual noise robust supervised morphological analysis using the word frame model </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the supervised learner employed nave approach to morphology, only capable of learning word-final stem changes between inflections and roots.
</prevsent>
<prevsent>this end-of-string model?
</prevsent>
</prevsection>
<citsent citstr=" H01-1035 ">
of morphology was used again in yarowsky et al (2001) <papid> H01-1035 </papid>where it was applied to english, french and czech.</citsent>
<aftsection>
<nextsent>(morecomplete details of the end-of-string model are presented in section 3.3.1.)though simplistic, this end-of-string model is robust to noise, especially important in co-training with low-accuracy unsupervised learners.
</nextsent>
<nextsent>however,the end-of-string model relied heavily upon externally provided, noise-free lists of affixes in order to correctly align inflections to roots.
</nextsent>
<nextsent>the word frame model allows, but does not require, such affix lists, thereby eliminating direct human supervision.much previous work has been done in automatically acquiring such affix lists, most recently the generative models built by snover and brent (2001) <papid> P01-1063 </papid>which are able to identify suffixes in english andpolish.</nextsent>
<nextsent>schone and jurafsky (2001) <papid> N01-1024 </papid>use latent semantic analysis to find prefixes, suffixes and circumfixes in german, dutch and english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1435">
<title id=" W04-0109.xml">multilingual noise robust supervised morphological analysis using the word frame model </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>(morecomplete details of the end-of-string model are presented in section 3.3.1.)though simplistic, this end-of-string model is robust to noise, especially important in co-training with low-accuracy unsupervised learners.
</prevsent>
<prevsent>however,the end-of-string model relied heavily upon externally provided, noise-free lists of affixes in order to correctly align inflections to roots.
</prevsent>
</prevsection>
<citsent citstr=" P01-1063 ">
the word frame model allows, but does not require, such affix lists, thereby eliminating direct human supervision.much previous work has been done in automatically acquiring such affix lists, most recently the generative models built by snover and brent (2001) <papid> P01-1063 </papid>which are able to identify suffixes in english andpolish.</citsent>
<aftsection>
<nextsent>schone and jurafsky (2001) <papid> N01-1024 </papid>use latent semantic analysis to find prefixes, suffixes and circumfixes in german, dutch and english.</nextsent>
<nextsent>baroni (2003) treats morphology as data compression problem to find english prefixes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1436">
<title id=" W04-0109.xml">multilingual noise robust supervised morphological analysis using the word frame model </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>however,the end-of-string model relied heavily upon externally provided, noise-free lists of affixes in order to correctly align inflections to roots.
</prevsent>
<prevsent>the word frame model allows, but does not require, such affix lists, thereby eliminating direct human supervision.much previous work has been done in automatically acquiring such affix lists, most recently the generative models built by snover and brent (2001) <papid> P01-1063 </papid>which are able to identify suffixes in english andpolish.</prevsent>
</prevsection>
<citsent citstr=" N01-1024 ">
schone and jurafsky (2001) <papid> N01-1024 </papid>use latent semantic analysis to find prefixes, suffixes and circumfixes in german, dutch and english.</citsent>
<aftsection>
<nextsent>baroni (2003) treats morphology as data compression problem to find english prefixes.
</nextsent>
<nextsent>goldsmith (2001) <papid> J01-2001 </papid>uses minimum description length to successfully find paradigmatic classes of suffixes in number of european languages, including dutch and russian, though the approach has been less successful in handling prefixation.the boas project (oflazer et al, 2001), (<papid> J01-1003 </papid>hakkani tur et al, 2000), and (oflazer and nirenburg, 1999)<papid> W99-0703 </papid>has produced excellent results bootstrapping morphological analyzer, but relyon direct human supervision to produce two-level rules (koskenniemi, barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the1983) which are then compiled into finite state ma chine.</nextsent>
<nextsent>3.1 motivation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1437">
<title id=" W04-0109.xml">multilingual noise robust supervised morphological analysis using the word frame model </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>schone and jurafsky (2001) <papid> N01-1024 </papid>use latent semantic analysis to find prefixes, suffixes and circumfixes in german, dutch and english.</prevsent>
<prevsent>baroni (2003) treats morphology as data compression problem to find english prefixes.</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
goldsmith (2001) <papid> J01-2001 </papid>uses minimum description length to successfully find paradigmatic classes of suffixes in number of european languages, including dutch and russian, though the approach has been less successful in handling prefixation.the boas project (oflazer et al, 2001), (<papid> J01-1003 </papid>hakkani tur et al, 2000), and (oflazer and nirenburg, 1999)<papid> W99-0703 </papid>has produced excellent results bootstrapping morphological analyzer, but relyon direct human supervision to produce two-level rules (koskenniemi, barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the1983) which are then compiled into finite state ma chine.</citsent>
<aftsection>
<nextsent>3.1 motivation.
</nextsent>
<nextsent>the supervised morphological learner presented in yarowsky and wicentowski (2000) <papid> P00-1027 </papid>modeled lemmatization as word-final stem change plus suffix taken from (possibly empty) list of potentialsuffixes.</nextsent>
<nextsent>though effective for suffix ation, this end of-string (eos) based model can not model other morphological phenomena, such as prefixation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1438">
<title id=" W04-0109.xml">multilingual noise robust supervised morphological analysis using the word frame model </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>schone and jurafsky (2001) <papid> N01-1024 </papid>use latent semantic analysis to find prefixes, suffixes and circumfixes in german, dutch and english.</prevsent>
<prevsent>baroni (2003) treats morphology as data compression problem to find english prefixes.</prevsent>
</prevsection>
<citsent citstr=" J01-1003 ">
goldsmith (2001) <papid> J01-2001 </papid>uses minimum description length to successfully find paradigmatic classes of suffixes in number of european languages, including dutch and russian, though the approach has been less successful in handling prefixation.the boas project (oflazer et al, 2001), (<papid> J01-1003 </papid>hakkani tur et al, 2000), and (oflazer and nirenburg, 1999)<papid> W99-0703 </papid>has produced excellent results bootstrapping morphological analyzer, but relyon direct human supervision to produce two-level rules (koskenniemi, barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the1983) which are then compiled into finite state ma chine.</citsent>
<aftsection>
<nextsent>3.1 motivation.
</nextsent>
<nextsent>the supervised morphological learner presented in yarowsky and wicentowski (2000) <papid> P00-1027 </papid>modeled lemmatization as word-final stem change plus suffix taken from (possibly empty) list of potentialsuffixes.</nextsent>
<nextsent>though effective for suffix ation, this end of-string (eos) based model can not model other morphological phenomena, such as prefixation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1439">
<title id=" W04-0109.xml">multilingual noise robust supervised morphological analysis using the word frame model </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>schone and jurafsky (2001) <papid> N01-1024 </papid>use latent semantic analysis to find prefixes, suffixes and circumfixes in german, dutch and english.</prevsent>
<prevsent>baroni (2003) treats morphology as data compression problem to find english prefixes.</prevsent>
</prevsection>
<citsent citstr=" W99-0703 ">
goldsmith (2001) <papid> J01-2001 </papid>uses minimum description length to successfully find paradigmatic classes of suffixes in number of european languages, including dutch and russian, though the approach has been less successful in handling prefixation.the boas project (oflazer et al, 2001), (<papid> J01-1003 </papid>hakkani tur et al, 2000), and (oflazer and nirenburg, 1999)<papid> W99-0703 </papid>has produced excellent results bootstrapping morphological analyzer, but relyon direct human supervision to produce two-level rules (koskenniemi, barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the1983) which are then compiled into finite state ma chine.</citsent>
<aftsection>
<nextsent>3.1 motivation.
</nextsent>
<nextsent>the supervised morphological learner presented in yarowsky and wicentowski (2000) <papid> P00-1027 </papid>modeled lemmatization as word-final stem change plus suffix taken from (possibly empty) list of potentialsuffixes.</nextsent>
<nextsent>though effective for suffix ation, this end of-string (eos) based model can not model other morphological phenomena, such as prefixation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1445">
<title id=" W04-0109.xml">multilingual noise robust supervised morphological analysis using the word frame model </title>
<section> word frame + eos.  </section>
<citcontext>
<prevsection>
<prevsent>our french test set included many examples of this, including roots that ended -tenir, -venir, -mettre, and -duire.
</prevsent>
<prevsent>for most languages however, the performance on the irregular set was not that good.
</prevsent>
</prevsection>
<citsent citstr=" W02-0606 ">
we propose no new solutions to handling irregular verb forms, but suggest using non-string-based techniques, such as those presented in (yarowsky and wicentowski, 2000), (<papid> P00-1027 </papid>baroni et al, 2002) <papid> W02-0606 </papid>and (wicentowski, 2002).</citsent>
<aftsection>
<nextsent>5.3 accuracy, precision and coverage.
</nextsent>
<nextsent>all of the previous results assumed that each inflection must be aligned to exactly one root, though one can improve precision by relaxing this constraint.
</nextsent>
<nextsent>the wf+eos model transforms an inflection intoa new string which we can compare against dictionary, word list, or large corpus.
</nextsent>
<nextsent>in determining the final inflection-root alignment, we can down weight, or even throw away, all proposed roots which are are not found in such wordlist.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1446">
<title id=" W04-1808.xml">discovering synonyms and other related words </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>for completely disjoint feature sets, j(p, q) = 1.
</prevsent>
<prevsent>the formula is symmetric but does not satisfy the triangle inequality.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
for speed the estimate may be calculated from the shared features alone (lee, 1999).<papid> P99-1004 </papid></citsent>
<aftsection>
<nextsent>after calculating all the pairwise estimates, we retained lists of the 100 most similar nouns for each of the nouns in the corpus data.
</nextsent>
<nextsent>no other data is used in the similarity calculations.
</nextsent>
<nextsent>computerm 2004 - 3rd international workshop on computational terminology 65 3.3 low-dimensional similarity.
</nextsent>
<nextsent>measures performing all the calculations in high dimensional feature space is time-consuming.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1447">
<title id=" W04-0302.xml">stochastic ally evaluating the validity of partial parse trees in incremental parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>real-time spoken language processing systems,such as simultaneous machine interpretation systems, are required to quickly respond to users?
</prevsent>
<prevsent>utterances.
</prevsent>
</prevsection>
<citsent citstr=" C94-2120 ">
to fulfill the requirement, the system needsto understand spoken language at least incrementally (allen et al, 2001; inagaki and matsubara,1995; milward and cooper, 1994), <papid> C94-2120 </papid>that is, to analyze each input sentence from left to right and acquire the content.</citsent>
<aftsection>
<nextsent>several incremental parsing methods have been proposed to date (costa et al, 2001; haddock, 1987; matsubara et al, 1997; milward, 1995; <papid> E95-1017 </papid>roark, 2001).<papid> J01-2004 </papid></nextsent>
<nextsent>these methods construct candidate partial parse trees for initial fragments of the input sentence on word-by-word basis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1448">
<title id=" W04-0302.xml">stochastic ally evaluating the validity of partial parse trees in incremental parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>utterances.
</prevsent>
<prevsent>to fulfill the requirement, the system needsto understand spoken language at least incrementally (allen et al, 2001; inagaki and matsubara,1995; milward and cooper, 1994), <papid> C94-2120 </papid>that is, to analyze each input sentence from left to right and acquire the content.</prevsent>
</prevsection>
<citsent citstr=" E95-1017 ">
several incremental parsing methods have been proposed to date (costa et al, 2001; haddock, 1987; matsubara et al, 1997; milward, 1995; <papid> E95-1017 </papid>roark, 2001).<papid> J01-2004 </papid></citsent>
<aftsection>
<nextsent>these methods construct candidate partial parse trees for initial fragments of the input sentence on word-by-word basis.
</nextsent>
<nextsent>however, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence.
</nextsent>
<nextsent>on the other hand, marcus proposed method of deterministically constructing valid partial parse trees by looking ahead several words (marcus, 1980), while kato et al proposed an incremental parsing which delays the decision of valid partial parse trees (kato et al, 2000).
</nextsent>
<nextsent>however, it is hard tosay that these methods realize broad-coverage incremental parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1449">
<title id=" W04-0302.xml">stochastic ally evaluating the validity of partial parse trees in incremental parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>utterances.
</prevsent>
<prevsent>to fulfill the requirement, the system needsto understand spoken language at least incrementally (allen et al, 2001; inagaki and matsubara,1995; milward and cooper, 1994), <papid> C94-2120 </papid>that is, to analyze each input sentence from left to right and acquire the content.</prevsent>
</prevsection>
<citsent citstr=" J01-2004 ">
several incremental parsing methods have been proposed to date (costa et al, 2001; haddock, 1987; matsubara et al, 1997; milward, 1995; <papid> E95-1017 </papid>roark, 2001).<papid> J01-2004 </papid></citsent>
<aftsection>
<nextsent>these methods construct candidate partial parse trees for initial fragments of the input sentence on word-by-word basis.
</nextsent>
<nextsent>however, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence.
</nextsent>
<nextsent>on the other hand, marcus proposed method of deterministically constructing valid partial parse trees by looking ahead several words (marcus, 1980), while kato et al proposed an incremental parsing which delays the decision of valid partial parse trees (kato et al, 2000).
</nextsent>
<nextsent>however, it is hard tosay that these methods realize broad-coverage incremental parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1453">
<title id=" W04-0302.xml">stochastic ally evaluating the validity of partial parse trees in incremental parsing </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>in the experiment, the inputs of the incremental parser are pos sequences rather than word sequences.
</prevsent>
<prevsent>we used 47247 initial trees and 2931 auxiliary trees for the experiment.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the elementary trees were extracted from the parse trees in sections 02-21 of the wall street journal in penn tree bank (marcus et al, 1993), <papid> J93-2004 </papid>which is transformed by using parent-child annotation and left factoring (roark and johnson, 1999).<papid> P99-1054 </papid></citsent>
<aftsection>
<nextsent>we set the beam-width at 500.
</nextsent>
<nextsent>the labeled precision and recall of the parsing are 80.8% and 78.5%, respectively for the section 23 in penn treebank.
</nextsent>
<nextsent>we used the set of sentences for which the outputs of the incremental parser are identical to the correct parse trees in the penn tree bank.
</nextsent>
<nextsent>the number of these sentences is 451.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1454">
<title id=" W04-0302.xml">stochastic ally evaluating the validity of partial parse trees in incremental parsing </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>in the experiment, the inputs of the incremental parser are pos sequences rather than word sequences.
</prevsent>
<prevsent>we used 47247 initial trees and 2931 auxiliary trees for the experiment.
</prevsent>
</prevsection>
<citsent citstr=" P99-1054 ">
the elementary trees were extracted from the parse trees in sections 02-21 of the wall street journal in penn tree bank (marcus et al, 1993), <papid> J93-2004 </papid>which is transformed by using parent-child annotation and left factoring (roark and johnson, 1999).<papid> P99-1054 </papid></citsent>
<aftsection>
<nextsent>we set the beam-width at 500.
</nextsent>
<nextsent>the labeled precision and recall of the parsing are 80.8% and 78.5%, respectively for the section 23 in penn treebank.
</nextsent>
<nextsent>we used the set of sentences for which the outputs of the incremental parser are identical to the correct parse trees in the penn tree bank.
</nextsent>
<nextsent>the number of these sentences is 451.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1455">
<title id=" W04-1203.xml">analysis of link grammar on biomedical dependency corpus targeted at protein protein interactions </title>
<section> evaluation criteria.  </section>
<citcontext>
<prevsection>
<prevsent>366 interaction subgraphs were identified from the corpus, one for each described interaction.the interaction subgraphs can be partially overlapping, because single link can be part of more than one interaction subgraph.
</prevsent>
<prevsent>figure 1shows an example of an annotated text fragment.
</prevsent>
</prevsection>
<citsent citstr=" P99-1065 ">
we evaluated the performance of the lg parser according to the following three quantitative cri teria: ? number of dependencies recovered ? number of fully correct linkages ? number of interaction subgraphs recovered the number of recovered dependencies gives an estimate of the probability that dependency will be correctly identified by the lg parser (this criterion is also employed by, e.g., coll inset al (1999)).<papid> P99-1065 </papid></citsent>
<aftsection>
<nextsent>the number of fully correct linkages, i.e. linkages where all annotated dependencies are recovered, measures the fraction of sentences that are parsed without error.
</nextsent>
<nextsent>how ever, fully correct linkage is not necessary to extract protein-protein interactions from sen tence; to estimate how many interactions can potentially be recovered, we measure the number of interaction subgraphs for which all dependencies were recovered.for each criterion, we measure the performance for the first linkage returned by the parser.
</nextsent>
<nextsent>however, the first linkage as ordered by the heuristics of the lg parser was often not the best (according to the criteria above) of the linkages returned by the parser.
</nextsent>
<nextsent>to separate the effect of the heuristics from overall lg performance, we identify separately for each of the three criteria the best linkage among the linkages returned by the parser, and we also report performance for the best linkages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1456">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the gnome corpus was created to study the discourse and semantic properties of discourse entities that affect their realization and interpretation, and particularly salience.
</prevsent>
<prevsent>we discuss what information was annotated and the methods we followed.
</prevsent>
</prevsection>
<citsent citstr=" P04-1019 ">
the gnome corpus was created to study the aspects of discourse that appear to affect generation, especially salience (pearson et al, 2000; poesio and di eugenio, 2001; poesio and nissim, 2001; poesio et al, 2004<papid> P04-1019 </papid>b).</citsent>
<aftsection>
<nextsent>particular attention was paid to the factors affecting the generation of pronouns (pearson et al, 2000; henschel et al, 2000), <papid> C00-1045 </papid>demon strat ives (poesio and nygren-modjeska, to appear) possess ives (poesio and nissim, 2001) and definites in general (poesio, 2004<papid> W04-2327 </papid>a).</nextsent>
<nextsent>these results, and the annotated corpus, were used in the development ofboth symbolic and statistical natural language generation algorithms for sentence planning (poesio, 2000a; henschel et al, 2000; <papid> C00-1045 </papid>cheng et al, 2001),<papid> N01-1002 </papid>aggregation (cheng, 2001) and text planning (karamanis, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1466">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we discuss what information was annotated and the methods we followed.
</prevsent>
<prevsent>the gnome corpus was created to study the aspects of discourse that appear to affect generation, especially salience (pearson et al, 2000; poesio and di eugenio, 2001; poesio and nissim, 2001; poesio et al, 2004<papid> P04-1019 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" C00-1045 ">
particular attention was paid to the factors affecting the generation of pronouns (pearson et al, 2000; henschel et al, 2000), <papid> C00-1045 </papid>demon strat ives (poesio and nygren-modjeska, to appear) possess ives (poesio and nissim, 2001) and definites in general (poesio, 2004<papid> W04-2327 </papid>a).</citsent>
<aftsection>
<nextsent>these results, and the annotated corpus, were used in the development ofboth symbolic and statistical natural language generation algorithms for sentence planning (poesio, 2000a; henschel et al, 2000; <papid> C00-1045 </papid>cheng et al, 2001),<papid> N01-1002 </papid>aggregation (cheng, 2001) and text planning (karamanis, 2003).</nextsent>
<nextsent>the empirical side of the project involved both psychological experiments and corpus annotation, based on scheme based on the mate proposals, as well as on detailed annotation manual (poesio, 2000b), the reliability of whose instructions was tested by extensive experiments (poe sio, 2000a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1467">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we discuss what information was annotated and the methods we followed.
</prevsent>
<prevsent>the gnome corpus was created to study the aspects of discourse that appear to affect generation, especially salience (pearson et al, 2000; poesio and di eugenio, 2001; poesio and nissim, 2001; poesio et al, 2004<papid> P04-1019 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W04-2327 ">
particular attention was paid to the factors affecting the generation of pronouns (pearson et al, 2000; henschel et al, 2000), <papid> C00-1045 </papid>demon strat ives (poesio and nygren-modjeska, to appear) possess ives (poesio and nissim, 2001) and definites in general (poesio, 2004<papid> W04-2327 </papid>a).</citsent>
<aftsection>
<nextsent>these results, and the annotated corpus, were used in the development ofboth symbolic and statistical natural language generation algorithms for sentence planning (poesio, 2000a; henschel et al, 2000; <papid> C00-1045 </papid>cheng et al, 2001),<papid> N01-1002 </papid>aggregation (cheng, 2001) and text planning (karamanis, 2003).</nextsent>
<nextsent>the empirical side of the project involved both psychological experiments and corpus annotation, based on scheme based on the mate proposals, as well as on detailed annotation manual (poesio, 2000b), the reliability of whose instructions was tested by extensive experiments (poe sio, 2000a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1474">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the gnome corpus was created to study the aspects of discourse that appear to affect generation, especially salience (pearson et al, 2000; poesio and di eugenio, 2001; poesio and nissim, 2001; poesio et al, 2004<papid> P04-1019 </papid>b).</prevsent>
<prevsent>particular attention was paid to the factors affecting the generation of pronouns (pearson et al, 2000; henschel et al, 2000), <papid> C00-1045 </papid>demon strat ives (poesio and nygren-modjeska, to appear) possess ives (poesio and nissim, 2001) and definites in general (poesio, 2004<papid> W04-2327 </papid>a).</prevsent>
</prevsection>
<citsent citstr=" N01-1002 ">
these results, and the annotated corpus, were used in the development ofboth symbolic and statistical natural language generation algorithms for sentence planning (poesio, 2000a; henschel et al, 2000; <papid> C00-1045 </papid>cheng et al, 2001),<papid> N01-1002 </papid>aggregation (cheng, 2001) and text planning (karamanis, 2003).</citsent>
<aftsection>
<nextsent>the empirical side of the project involved both psychological experiments and corpus annotation, based on scheme based on the mate proposals, as well as on detailed annotation manual (poesio, 2000b), the reliability of whose instructions was tested by extensive experiments (poe sio, 2000a).
</nextsent>
<nextsent>more recently, the corpus has also been used to develop and evaluate anaphora resolution systems, with special focus on the resolution of bridging references (poesio, 2003; poesio and alexandrov-kabadjov, 2004; poesio et al, 2004<papid> P04-1019 </papid>a) although the results of the studies using the gnome corpus mentioned above have been published in number of papers, and although detailed annotation manual was written and has been available on the web for few years (poesio,2000b), none of the previously published papers discusses in detail the goals of the annotation and the methodology that was followed, especially for the non-anaphoric aspects.</nextsent>
<nextsent>in this paper we discuss the methods used to identify possible utterances,?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1485">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> identifying utterances.  </section>
<citcontext>
<prevsection>
<prevsent>the total size of the annotated corpus is about 60k.
</prevsent>
<prevsent>in order to use corpus to study salience, it is essential to find way to annotate what in center 1the museum sub corpus extends the corpus collected to support the ilex and sole projects at the university of edinburgh (oberlander et al, 1998).
</prevsent>
</prevsection>
<citsent citstr=" W98-1427 ">
2the leaflets in the pharmaceutical sub corpus are subset of the collection of all patient leaflets in the uk which was digitized to support the iconoclast project at the university of brighton (scott et al, 1998).<papid> W98-1427 </papid>ing theory (grosz et al, 1995) <papid> J95-2003 </papid>are called utterances, i.e., the units of text after which the local focus is updated.</citsent>
<aftsection>
<nextsent>in most annotations concerned with salience, predefined notion of utterance was adopted, typically sentences (miltsakaki, 2002) <papid> J02-3003 </papid>or (finite) clauses (kameyama, 1998).</nextsent>
<nextsent>this approach, however, precludes using the corpus to compare possible definitions of utterance, one of the goals of the gnome annotation (poesio et al, 2004<papid> P04-1019 </papid>b).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1486">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> identifying utterances.  </section>
<citcontext>
<prevsection>
<prevsent>the total size of the annotated corpus is about 60k.
</prevsent>
<prevsent>in order to use corpus to study salience, it is essential to find way to annotate what in center 1the museum sub corpus extends the corpus collected to support the ilex and sole projects at the university of edinburgh (oberlander et al, 1998).
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
2the leaflets in the pharmaceutical sub corpus are subset of the collection of all patient leaflets in the uk which was digitized to support the iconoclast project at the university of brighton (scott et al, 1998).<papid> W98-1427 </papid>ing theory (grosz et al, 1995) <papid> J95-2003 </papid>are called utterances, i.e., the units of text after which the local focus is updated.</citsent>
<aftsection>
<nextsent>in most annotations concerned with salience, predefined notion of utterance was adopted, typically sentences (miltsakaki, 2002) <papid> J02-3003 </papid>or (finite) clauses (kameyama, 1998).</nextsent>
<nextsent>this approach, however, precludes using the corpus to compare possible definitions of utterance, one of the goals of the gnome annotation (poesio et al, 2004<papid> P04-1019 </papid>b).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1487">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> identifying utterances.  </section>
<citcontext>
<prevsection>
<prevsent>in order to use corpus to study salience, it is essential to find way to annotate what in center 1the museum sub corpus extends the corpus collected to support the ilex and sole projects at the university of edinburgh (oberlander et al, 1998).
</prevsent>
<prevsent>2the leaflets in the pharmaceutical sub corpus are subset of the collection of all patient leaflets in the uk which was digitized to support the iconoclast project at the university of brighton (scott et al, 1998).<papid> W98-1427 </papid>ing theory (grosz et al, 1995) <papid> J95-2003 </papid>are called utterances, i.e., the units of text after which the local focus is updated.</prevsent>
</prevsection>
<citsent citstr=" J02-3003 ">
in most annotations concerned with salience, predefined notion of utterance was adopted, typically sentences (miltsakaki, 2002) <papid> J02-3003 </papid>or (finite) clauses (kameyama, 1998).</citsent>
<aftsection>
<nextsent>this approach, however, precludes using the corpus to compare possible definitions of utterance, one of the goals of the gnome annotation (poesio et al, 2004<papid> P04-1019 </papid>b).</nextsent>
<nextsent>in order to do this, we marked all spans of text that might be claimed to update the local focus, including sentences (defined as all units of text ending with full stop, question mark, or an exclamation point) as well as what we called (discourse) units.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1498">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> identifying utterances.  </section>
<citcontext>
<prevsection>
<prevsent>subject: for verbed units, whether they have full subject, an empty subject (expletive, as inthere sentences), or no subject (e.g., for infini tival clauses).
</prevsent>
<prevsent>annotation issues marking up sentences proved to be quite easy; marking up units, on the other hand, required extensive annotator training.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
the agreement on identifying the boundaries of units, using the ? statistic discussed in (carletta, 1996), <papid> J96-2004 </papid>was ? = .9 (for two annotators and 500 units); the agreement on features (2 annotators and at least 200 units) was as follows: utype: ?=.76; verbed: ?=.9; finite: ?=.81.</citsent>
<aftsection>
<nextsent>the main problems when marking units were to identify complements, to distinguish clausal adjuncts from prepositional phrases, and how to mark up coordinated units.
</nextsent>
<nextsent>the main problem with complements was to distinguish non-finite complements of verbs such as want from the non-finite part of verbal complexes containing modal auxiliaries such as get, let, make, and have: (2) a.
</nextsent>
<nextsent>(i would like (to be able to travel)) b.
</nextsent>
<nextsent>(i let him do his homework)one problem that proved fairly difficult to handle (and which, in fact, we didnt entirely solve)was clausal coordination.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1500">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> properties of discourse entities and.  </section>
<citcontext>
<prevsection>
<prevsent>utype=complement?
</prevsent>
<prevsent>verbed=verbed-yes?  to link  ne id= ne5  cat= pers-pro  per= per3  num= sing  gen= neut  gf= obj  lftype= term  onto= concrete  ani= inanimate  deix= deix-yes  count= count-yes  structure= atom  generic= generic-no  loeb= disc-function   it  /ne  ...
</prevsent>
</prevsection>
<citsent citstr=" W99-0309 ">
the gnome instructions for identifying nps derive from those proposed inmate (poesio et al, 1999), <papid> W99-0309 </papid>in turn derived from drama (passonneau, 1997)and muc-7 (hirschman, 1998).</citsent>
<aftsection>
<nextsent>an important difference between the instructions used for gnome and those developed for mate is that instead of attempting to get the annotators to recognize the np that realize discourse entities and only mark those, in gnome all nps were marked with ne?
</nextsent>
<nextsent>elements;the separate lf type attribute was used to distinguish between nps with different types of denota tions (see below).
</nextsent>
<nextsent>this change made the process of identifying nominal entities easier and potentially automatic (even though the identification of mark ables was still done by hand).
</nextsent>
<nextsent>as in the case of units, the main problem with marking up nps was coordination.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1502">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> properties of discourse entities and.  </section>
<citcontext>
<prevsection>
<prevsent>this problem was solved by introducing special undersp-gen value; indeed, underspecified values were provided for all attributes.
</prevsent>
<prevsent>the agreement values for these features were: gen: ? = .89; num: ? = .84; per: ? = .9.gf this attribute was used to annotate the grammatical function of the np, property generally taken to play an important role in determining the salience of the discourse entity it realizes (grosz et al, 1995).<papid> J95-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
our instructions for this attribute are derived from those used in the framenet project ((baker et al, 1998); <papid> P98-1013 </papid>see also http://www.icsi.berkeley.edu/framenet/).</citsent>
<aftsection>
<nextsent>the values are subj, obj, predicate (used for post-verbal objects in copular sentences, such as this is (a production watch)), there-obj (for post-verbal objects in there-sentences), comp (for indirect objects), adjunct (for the argument of pps modifying vps), gen (for nps in determiner position in possessive nps), np-compl, np-part, np-mod, adj-mod, and no-gf (for nps occurring by themselves - eg., in titles).
</nextsent>
<nextsent>the agreement values for gf is ? = .85.
</nextsent>
<nextsent>lf type not all nps realize discourse entities: some of them realize quantifiers (e.g., each coffer in each coffer has lid) or predicates (e.g., nps in appositive position, such as the oldest son of louis xiv in the 1689 inventory of the grand dauphin, the oldest son of louis xiv, lists jewel coffer of similar form and decoration.
</nextsent>
<nextsent>as said above, in the gnome annotation all nps are treated as markables, but the lf type attribute is used to indicate the type of semantic object denoted by an np: term, quant or pred.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1503">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> properties of discourse entities and.  </section>
<citcontext>
<prevsection>
<prevsent>the last version of the instructions (not yet added to the overall annotation manual) asked annotators to try to identify generic sentences before attempting to determine the value of the generic attribute.with these instructions, we finally reached reason able agreement (?
</prevsent>
<prevsent>= .82).
</prevsent>
</prevsection>
<citsent citstr=" J98-2001 ">
loeb poesio and vieira (1998) <papid> J98-2001 </papid>found that of the 1,400 definite descriptions in their corpus, only about 50% were subsequent mention or bridging references, whereas 50% were first mentions.</citsent>
<aftsection>
<nextsent>ofthe first mentions, about half (i.e., 25% of the to tal) were what hawkins (1978) would call larger situation?
</nextsent>
<nextsent>definites, i.e., definite descriptions like the pope whose referent is supposed to be part of shared knowledge; whereas the other half includes what loebner (1987) calls semantically functional definites, like the first man on the moon.loebner claimed that the paradigmatic case of def initeness are not anaphoric nps, as suggested by familiarity theories such as heims (1982), but semantically functional ones such as the first person ever to row across the pacific on his own.
</nextsent>
<nextsent>in order to test loebners theory and compare it with one based on familiarity, we annotated the nps referring to discourse entities according to whether they were functional, relational, or sortal (poesio, 2004<papid> W04-2327 </papid>a).</nextsent>
<nextsent>we achieved good reliability on this attribute (?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1539">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> anaphora.  </section>
<citcontext>
<prevsection>
<prevsent>gf=no-gf?  an engraving of  ne id=ne549?
</prevsent>
<prevsent>gf=np-compl? it  /ne  /ne   /unit , ...
</prevsent>
</prevsection>
<citsent citstr=" J99-3001 ">
/unit   ante current= ne549  rel= ident    anchor id= ne547    /ante  work such as (sidner, 1979; strube and hahn, 1999), <papid> J99-3001 </papid>as well as our own preliminary analysis,suggested that indirect realization can play crucial role in maintaining the cb.</citsent>
<aftsection>
<nextsent>however, previous attempts at marking anaphoric information, particularly in the context of the muc initiative, suggested that while agreement on identity relations is6the presence of more than one anchor?
</nextsent>
<nextsent>element indicates that the anaphoric expression is ambiguous.
</nextsent>
<nextsent>fairly easy to achieve, marking bridging references is hard; this was confirmed by poesio and vieira(1998).<papid> J98-2001 </papid></nextsent>
<nextsent>for these reasons, and to reduce the annotators?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1541">
<title id=" W04-0210.xml">discourse annotation and semantic annotation in the gnome corpus </title>
<section> automatically computing the local.  </section>
<citcontext>
<prevsection>
<prevsent>of theory of the local focus, and then use scripts to automatically compute the cb.
</prevsent>
<prevsent>there are two advantages to this approach: first of all, agreement on the building blocks?
</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
is much easier to reach than agreement on the cbin our preliminary experiments we didnt go beyond ? = .6 when trying to directly identify the cb using the definitions from (brennan et al, 1987).<papid> P87-1022 </papid></citsent>
<aftsection>
<nextsent>and secondly, this approach makes it possible to compute the cb according to different ways of instantiating what we call the parameters of center ing?
</nextsent>
<nextsent>e.g., ranking.we developed such scripts for the work discussed in (poesio et al, 2004<papid> P04-1019 </papid>b); they can be tested on the web site associated with that paper, http://cswww.essex.ac.uk/staff/poesio/ cbc/.</nextsent>
<nextsent>these scripts have been subsequently used to compute the cb in, e.g., (poesio and nissim, 2001; poesio and nygren-modjeska, to appear).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1560">
<title id=" W04-0214.xml">discourse annotation in the monroe corpus </title>
<section> aims of monroe project.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 parser development.
</prevsent>
<prevsent>one of the aims of the monroe project was to develop wide coverage grammar for spoken dialogue.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
since parsing is just an initial stage of natural language understanding, the project was focused not just on obtaining syntactic trees alone (as is done in many other parsed corpora, for example, penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>or tiger (brants and plaehn, 2000)).</citsent>
<aftsection>
<nextsent>instead, we aimed to develop parser and grammar for the production of syntactic parses and semantic representations useful in discourse processing.the parser produces domain-independent semantic representation with information necessary for referential and discourse processing, in particular, domain-independent representations of determiners and quantifiers (to be resolved by our reference module), domain-independent representations for discourse adverbials, and tense, aspect and modality information.
</nextsent>
<nextsent>this necessitated the development of domain-independent logical form syntax and domain-independent ontology as source of semantic types for our representations (dzikovska et al, 2004).
</nextsent>
<nextsent>in subsequent sections we discuss how the parser-generated representations are used as basis for discourse annotation.
</nextsent>
<nextsent>2.2 reference resolution development.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1561">
<title id=" W04-0214.xml">discourse annotation in the monroe corpus </title>
<section> aims of monroe project.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 discourse segmentation.
</prevsent>
<prevsent>another research area that can benefit from discourse-annotated corpus is discourse structure.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
there has been plenty of theoretical work such as (grosz and sidner, 1986), (<papid> J86-3001 </papid>moser and moore, 1996)<papid> J96-3006 </papid>which shows that just as sentences can be decomposed into smaller constituents, discourse can be decomposed into smaller units called discourse seg ments.</citsent>
<aftsection>
<nextsent>though there are many different ways to segment discourse, the common themes are that some sequences are more closely related than others (discourse segments) and that discourse can be organized as tree, with the leaves being the individual utterances and the interior nodes being discourse segments.
</nextsent>
<nextsent>the embedded ness of segment effects which previous segments, and thus their entities, are accessible.
</nextsent>
<nextsent>as discourse progresses, segments close and unless they are close to the root ofthe tree (have low embedding) may not be acces sible.discourse segmentation has implications for spoken dialogue systems.
</nextsent>
<nextsent>properly detecting discourse structure can lead to improved reference resolution accuracy since competing antecedents in inaccessible clauses may be removed from consideration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1562">
<title id=" W04-0214.xml">discourse annotation in the monroe corpus </title>
<section> aims of monroe project.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 discourse segmentation.
</prevsent>
<prevsent>another research area that can benefit from discourse-annotated corpus is discourse structure.
</prevsent>
</prevsection>
<citsent citstr=" J96-3006 ">
there has been plenty of theoretical work such as (grosz and sidner, 1986), (<papid> J86-3001 </papid>moser and moore, 1996)<papid> J96-3006 </papid>which shows that just as sentences can be decomposed into smaller constituents, discourse can be decomposed into smaller units called discourse seg ments.</citsent>
<aftsection>
<nextsent>though there are many different ways to segment discourse, the common themes are that some sequences are more closely related than others (discourse segments) and that discourse can be organized as tree, with the leaves being the individual utterances and the interior nodes being discourse segments.
</nextsent>
<nextsent>the embedded ness of segment effects which previous segments, and thus their entities, are accessible.
</nextsent>
<nextsent>as discourse progresses, segments close and unless they are close to the root ofthe tree (have low embedding) may not be acces sible.discourse segmentation has implications for spoken dialogue systems.
</nextsent>
<nextsent>properly detecting discourse structure can lead to improved reference resolution accuracy since competing antecedents in inaccessible clauses may be removed from consideration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1564">
<title id=" W04-0214.xml">discourse annotation in the monroe corpus </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>speakers will interrupt each other.
</prevsent>
<prevsent>as result, many empirical methods that work well in very formal, structured domains such as newspaper texts or manuals tend to suffer.
</prevsent>
</prevsection>
<citsent citstr=" J01-4003 ">
for example, many leading pronoun resolution methods perform around 80% accuracy over corpus of syntactically-parsed wall street journal articles (e.g., (tetreault, 2001) <papid> J01-4003 </papid>and (ge et al, 1998)), <papid> W98-1119 </papid>but in spoken dialogue the performance of these algorithms drops significantly (byron, 2002).<papid> P02-1011 </papid></citsent>
<aftsection>
<nextsent>however, by including semantic and discourse information, one is able to improve performance.our preliminary results show that using the semantic feature lists associated with each entity as filter for reference increases performance to 59% from 44%.
</nextsent>
<nextsent>adding discourse segmentation boosts that figure to 66% over some parts of the corpus.
</nextsent>
<nextsent>we have presented description of our corpus annotation in the monroe domain.
</nextsent>
<nextsent>it is novel in that it incorporates rich semantic information with reference and discourse information, rarity for spoken dialogue domains which are typically very difficult to annotate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1565">
<title id=" W04-0214.xml">discourse annotation in the monroe corpus </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>speakers will interrupt each other.
</prevsent>
<prevsent>as result, many empirical methods that work well in very formal, structured domains such as newspaper texts or manuals tend to suffer.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
for example, many leading pronoun resolution methods perform around 80% accuracy over corpus of syntactically-parsed wall street journal articles (e.g., (tetreault, 2001) <papid> J01-4003 </papid>and (ge et al, 1998)), <papid> W98-1119 </papid>but in spoken dialogue the performance of these algorithms drops significantly (byron, 2002).<papid> P02-1011 </papid></citsent>
<aftsection>
<nextsent>however, by including semantic and discourse information, one is able to improve performance.our preliminary results show that using the semantic feature lists associated with each entity as filter for reference increases performance to 59% from 44%.
</nextsent>
<nextsent>adding discourse segmentation boosts that figure to 66% over some parts of the corpus.
</nextsent>
<nextsent>we have presented description of our corpus annotation in the monroe domain.
</nextsent>
<nextsent>it is novel in that it incorporates rich semantic information with reference and discourse information, rarity for spoken dialogue domains which are typically very difficult to annotate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1566">
<title id=" W04-0214.xml">discourse annotation in the monroe corpus </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>speakers will interrupt each other.
</prevsent>
<prevsent>as result, many empirical methods that work well in very formal, structured domains such as newspaper texts or manuals tend to suffer.
</prevsent>
</prevsection>
<citsent citstr=" P02-1011 ">
for example, many leading pronoun resolution methods perform around 80% accuracy over corpus of syntactically-parsed wall street journal articles (e.g., (tetreault, 2001) <papid> J01-4003 </papid>and (ge et al, 1998)), <papid> W98-1119 </papid>but in spoken dialogue the performance of these algorithms drops significantly (byron, 2002).<papid> P02-1011 </papid></citsent>
<aftsection>
<nextsent>however, by including semantic and discourse information, one is able to improve performance.our preliminary results show that using the semantic feature lists associated with each entity as filter for reference increases performance to 59% from 44%.
</nextsent>
<nextsent>adding discourse segmentation boosts that figure to 66% over some parts of the corpus.
</nextsent>
<nextsent>we have presented description of our corpus annotation in the monroe domain.
</nextsent>
<nextsent>it is novel in that it incorporates rich semantic information with reference and discourse information, rarity for spoken dialogue domains which are typically very difficult to annotate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1567">
<title id=" W04-1807.xml">detecting semantic relations between terms in definitions </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the former one extracts defining statements on the basis of the match of linguistic clues, when they are relayed in the sentence by some linguistic rules.
</prevsent>
<prevsent>these rules are deve lopped by the author, withing the schema defined in the contextual exploration?
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
methodology (descls, 1996).pearson (1996) and rebeyrolle (2000) have followed the methodology described by hearst (1992), <papid> C92-2082 </papid>up to now mainly applied to discover hyponymous terms.</citsent>
<aftsection>
<nextsent>it consists in describing the lexico-syntactic context of an occurrence of pair of terms known to share semantic relation.
</nextsent>
<nextsent>modelling the context in which they occur provides pattern?
</nextsent>
<nextsent>to apply to the corpus, in order to extract other pairs of terms connected by the same relation.
</nextsent>
<nextsent>pearson and rebeyrollehave modelled lexico-syntactic contexts around lexical clues interpreted as definition markers?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1569">
<title id=" W04-1504.xml">axiomatization of restricted non projective dependency trees through finite state constraints that analyse crossing bracketings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>annotated sentences) are structural descriptions that are assigned to their sub sequences (cf.
</prevsent>
<prevsent>generated strings or yields of trees) where brackets and other special-purpose characters have been omitted.
</prevsent>
</prevsection>
<citsent citstr=" P98-1106 ">
recently, many dependency syntactic parsers using finite-state machines (fsms) have been presented (kahane et al, 1998; <papid> P98-1106 </papid>elworthy, 2000; nasr et al,2002; oflazer, 2003; <papid> J03-4001 </papid>yli-jyra?, 2004a).</citsent>
<aftsection>
<nextsent>this article shows that finite-state equivalent grammatical system is capable of assigning even non-projectivesyntactic dependency trees ? or their representations ? to terminal strings.
</nextsent>
<nextsent>an appropriate representation is conveniently defined through set of axioms presented in this work.
</nextsent>
<nextsent>the complexity of the structures assigned is bounded by some special parameters.
</nextsent>
<nextsent>1.1 motivation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1570">
<title id=" W04-1504.xml">axiomatization of restricted non projective dependency trees through finite state constraints that analyse crossing bracketings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>annotated sentences) are structural descriptions that are assigned to their sub sequences (cf.
</prevsent>
<prevsent>generated strings or yields of trees) where brackets and other special-purpose characters have been omitted.
</prevsent>
</prevsection>
<citsent citstr=" J03-4001 ">
recently, many dependency syntactic parsers using finite-state machines (fsms) have been presented (kahane et al, 1998; <papid> P98-1106 </papid>elworthy, 2000; nasr et al,2002; oflazer, 2003; <papid> J03-4001 </papid>yli-jyra?, 2004a).</citsent>
<aftsection>
<nextsent>this article shows that finite-state equivalent grammatical system is capable of assigning even non-projectivesyntactic dependency trees ? or their representations ? to terminal strings.
</nextsent>
<nextsent>an appropriate representation is conveniently defined through set of axioms presented in this work.
</nextsent>
<nextsent>the complexity of the structures assigned is bounded by some special parameters.
</nextsent>
<nextsent>1.1 motivation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1574">
<title id=" W04-1504.xml">axiomatization of restricted non projective dependency trees through finite state constraints that analyse crossing bracketings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure 1: this tree analyses the latin sentence ultima cumaei venit iam car minis aetas?
</prevsent>
<prevsent>(vergil: eclogues iv.4) that means the last era of the cumean song has now arrived?.
</prevsent>
</prevsection>
<citsent citstr=" J90-4003 ">
the analysis is adapted from covington (1990).<papid> J90-4003 </papid></citsent>
<aftsection>
<nextsent>we added the sentence-initial node (
</nextsent>
<nextsent>    ) and the arc labels.the graphical representation for the d-tree is interpreted as follows.
</nextsent>
<nextsent>if   is (directed) arc between two nodes, we will say that   depends immediately on   (or, conversely,  governs  im mediately), and that   is an immediate syntactic dependent of   (and   is the immediate syntactic governor of  ).
</nextsent>
<nextsent>(melcuk 1988.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1575">
<title id=" W04-1002.xml">extending document summarization to information graphics </title>
<section> the role of intention in graphics.  </section>
<citcontext>
<prevsection>
<prevsent>however, it is widely acknowledged that to truly understand textand produce the best summary, one must under stand the document and recognize the intentions of the author.
</prevsent>
<prevsent>recent work in text summarization has personal filings delaware bankruptcy 3000 2500 1000 1500 2000 1998 1999 2000 2001 figure 1: graphic from city newspaper 60 70 80 90 01 $15 10 5 black women white women median income in thousands of 2001 dollars 1948 figure 2: graphic from newsweek magazine begun to address this issue.
</prevsent>
</prevsection>
<citsent citstr=" J00-3005 ">
for example, (marcu,2000) <papid> J00-3005 </papid>presents algorithms for automatically identifying the rhetorical structure of text and argues that the hypothesized rhetorical structure can be successfully used in text summarization.</citsent>
<aftsection>
<nextsent>information graphics are an important component of many documents.
</nextsent>
<nextsent>in some cases, information graphics are stand-alone and constitute the entiredocument.
</nextsent>
<nextsent>this is the case for many graphics appearing in newspapers, such as the graphic shown in figure 1.
</nextsent>
<nextsent>on the other hand, when an article is comprised of text and graphics, the graphic generally expands on the text and contributes to the discourse purpose (grosz and sidner, 1986) <papid> J86-3001 </papid>of the arti cle.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1576">
<title id=" W04-1002.xml">extending document summarization to information graphics </title>
<section> the role of intention in graphics.  </section>
<citcontext>
<prevsection>
<prevsent>in some cases, information graphics are stand-alone and constitute the entiredocument.
</prevsent>
<prevsent>this is the case for many graphics appearing in newspapers, such as the graphic shown in figure 1.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
on the other hand, when an article is comprised of text and graphics, the graphic generally expands on the text and contributes to the discourse purpose (grosz and sidner, 1986) <papid> J86-3001 </papid>of the arti cle.</citsent>
<aftsection>
<nextsent>for example, figure 2 illustrates graphic from newsweek showing that the income of black women has risen dramatically over the last decade and has reached the level of white women.
</nextsent>
<nextsent>although this information is not conveyed elsewhere in the article, it contributes to the overall communicative intention of this portion of the article ? namely, that there has been monumental shifting of the sands?
</nextsent>
<nextsent>with regard to the achievements of black women.our project is concerned with the understanding and summarization of information graphics: bar charts, line graphs, pie charts, etc. we contend that analyzing the data points underlying an information graphic is insufficient.
</nextsent>
<nextsent>one must instead identify the message that the graphic designer intended to convey via the design choices that were made in constructing the graphic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1577">
<title id=" W04-1002.xml">extending document summarization to information graphics </title>
<section> graphic summarization.  </section>
<citcontext>
<prevsection>
<prevsent>serves as evidence for the salience of recognize relative difference task.
</prevsent>
<prevsent>in the future, we plan to capture the influence of surrounding text by identifying the important concepts from the text using lexical chains.
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
lexical chains have been used in text summarization (barzilay et al, 1999), <papid> P99-1071 </papid>and our linear time algorithm (silber and mccoy, 2002) <papid> J02-4004 </papid>makes their computation feasible even for large texts.</citsent>
<aftsection>
<nextsent>whether task is salient and the method by which it was made salient are used as evidence in our plan inference system.
</nextsent>
<nextsent>the graphic design makes some tasks easier than others.
</nextsent>
<nextsent>we use set of rules, based on research by cognitive psychologists, to estimate the relative effort of performing different perceptual and cognitive tasks.
</nextsent>
<nextsent>these rules, described in (elzer et al,2004), have been validated by eye-tracking experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1578">
<title id=" W04-1002.xml">extending document summarization to information graphics </title>
<section> graphic summarization.  </section>
<citcontext>
<prevsection>
<prevsent>serves as evidence for the salience of recognize relative difference task.
</prevsent>
<prevsent>in the future, we plan to capture the influence of surrounding text by identifying the important concepts from the text using lexical chains.
</prevsent>
</prevsection>
<citsent citstr=" J02-4004 ">
lexical chains have been used in text summarization (barzilay et al, 1999), <papid> P99-1071 </papid>and our linear time algorithm (silber and mccoy, 2002) <papid> J02-4004 </papid>makes their computation feasible even for large texts.</citsent>
<aftsection>
<nextsent>whether task is salient and the method by which it was made salient are used as evidence in our plan inference system.
</nextsent>
<nextsent>the graphic design makes some tasks easier than others.
</nextsent>
<nextsent>we use set of rules, based on research by cognitive psychologists, to estimate the relative effort of performing different perceptual and cognitive tasks.
</nextsent>
<nextsent>these rules, described in (elzer et al,2004), have been validated by eye-tracking experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1579">
<title id=" W04-1502.xml">relative clauses in hindi and arabic a panini an dependency grammar analysis </title>
<section> dependency grammar in the.  </section>
<citcontext>
<prevsection>
<prevsent>panini an grammar framework 2.1 background.
</prevsent>
<prevsent>the modern formulation of dependency structure is frequently attributed to (tesnie`re, 1959).it is interesting to note that among contemporary proponents of dependency structure, there are those, such as (hudson, 1984), who maintain general principle of projectivity for their dependency structures, and devise additional means of coping with discontinuity when it arises.
</prevsent>
</prevsection>
<citsent citstr=" J90-4003 ">
others, such as (melcuk, 1988), (bharati et al, 1996b) and (covington, 1990) <papid> J90-4003 </papid>allow non-projective dependency structures and rely upon separate means of linearisation.</citsent>
<aftsection>
<nextsent>most recently, (debusmann and duchier, 2003)have presented new formulation of dependency grammar which generali ses multi-stratalapproaches to an n-dimensional formalism of interacting dependency graphs.
</nextsent>
<nextsent>the panini an grammar framework proposed by (bharati et al, 1996b) is particularly aimed at treating heavily inflected free word order languages such as hindi and other indian languages.
</nextsent>
<nextsent>like hindi, arabic is heavily inflected with overt case marking of nouns, noun-verbagreement, as well as incorporation of pronom inals into verb forms.
</nextsent>
<nextsent>although arabic has word order (vso) that is more fixed compared to hindi (canonically sov but with significant word order freedom), there is also significant word order variation found in nominal and top icalised sentences, thus making alternate word orders such as svo quite common.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1580">
<title id=" W04-1502.xml">relative clauses in hindi and arabic a panini an dependency grammar analysis </title>
<section> dependency grammar in the.  </section>
<citcontext>
<prevsection>
<prevsent>2paninis grammar of sanskrit asserted that everykaraka relation in an utterance must have phonetic realisation (bharati et al, 1996b, p. 187), rather than via an intermediate syntactic role or sentential position.
</prevsent>
<prevsent>inthis sense, there is tighter binding between karaka relations and the surface-level syntax than would normally be seen in typical theta role analysis.despite its orientation toward inflected languages, some work has been done on generalising the notion of vibhakti to include linear position where this has syntactic significance (bharati et al, 1996a).
</prevsent>
</prevsection>
<citsent citstr=" P01-1024 ">
the generalisation of vibhakti to account for word order essentially introduces facility analogous to topolog ical fields (as found in topological dependency grammar (duchier and debusmann, 2001) <papid> P01-1024 </papid>anddachs (broker, 1998)) <papid> P98-1026 </papid>to pgf.</citsent>
<aftsection>
<nextsent>it is important to note however that these word order constraints are treated in the same way as other vibhakti (morphological constraints), and do not use separate representation.
</nextsent>
<nextsent>in this sense,pgf does not attempt to separate linear precedence from immediate dominance at formal level.
</nextsent>
<nextsent>in the following sections we explore how both morphology and word order constraints in arabic can be captured through mapping of vib hakti to karaka relations.
</nextsent>
<nextsent>arabic to briefly summarise salient features of hindi and arabic: ? hindi word order is relatively free?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1581">
<title id=" W04-1502.xml">relative clauses in hindi and arabic a panini an dependency grammar analysis </title>
<section> dependency grammar in the.  </section>
<citcontext>
<prevsection>
<prevsent>2paninis grammar of sanskrit asserted that everykaraka relation in an utterance must have phonetic realisation (bharati et al, 1996b, p. 187), rather than via an intermediate syntactic role or sentential position.
</prevsent>
<prevsent>inthis sense, there is tighter binding between karaka relations and the surface-level syntax than would normally be seen in typical theta role analysis.despite its orientation toward inflected languages, some work has been done on generalising the notion of vibhakti to include linear position where this has syntactic significance (bharati et al, 1996a).
</prevsent>
</prevsection>
<citsent citstr=" P98-1026 ">
the generalisation of vibhakti to account for word order essentially introduces facility analogous to topolog ical fields (as found in topological dependency grammar (duchier and debusmann, 2001) <papid> P01-1024 </papid>anddachs (broker, 1998)) <papid> P98-1026 </papid>to pgf.</citsent>
<aftsection>
<nextsent>it is important to note however that these word order constraints are treated in the same way as other vibhakti (morphological constraints), and do not use separate representation.
</nextsent>
<nextsent>in this sense,pgf does not attempt to separate linear precedence from immediate dominance at formal level.
</nextsent>
<nextsent>in the following sections we explore how both morphology and word order constraints in arabic can be captured through mapping of vib hakti to karaka relations.
</nextsent>
<nextsent>arabic to briefly summarise salient features of hindi and arabic: ? hindi word order is relatively free?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1582">
<title id=" W04-1502.xml">relative clauses in hindi and arabic a panini an dependency grammar analysis </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>however systematic comparison ofpgf to more recent dependency grammar formalisms, such as dachs and tdg, has not yetbeen done.
</prevsent>
<prevsent>given the strong parallels with recent work in these formalisms (cf.
</prevsent>
</prevsection>
<citsent citstr=" E03-1054 ">
(kruijff and duchier, 2003)), <papid> E03-1054 </papid>such an investigation is now essential.</citsent>
<aftsection>
<nextsent>our thanks go to professor rajeev sangal and the language technology research centre staff at the international institute of information technology, hyderabad, for their ongoing support of our efforts in applying the panini an grammar framework, petr pajas for assistance with the tred diagram ming tool used for laying out the dependency diagrams, and to professor joachim diederich and the workshop review panel for their helpful comments during the preparation of this paper.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1583">
<title id=" W04-2309.xml">but what do they mean an exploration into the range of cross turn expectations denied by but </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the practical utility of distinguishing these relations arises from discovering ways in which to both represent and utilise this information for nlg (among other applications), so we will address these issues in section 4.
</prevsent>
<prevsent>we show how the information state (is) (matheson.
</prevsent>
</prevsection>
<citsent citstr=" A00-2001 ">
et al, 2000) representing the state of the dialogue in the ptt (poesio and traum, 1998) model of dialogue mustbe updated to reflect this new information, with conversational acts (matheson et al, 2000) <papid> A00-2001 </papid>that do not simply indicate dofe as in (thomas and matheson, 2003), butalso annotate the relation underlying the expectation being denied.</citsent>
<aftsection>
<nextsent>finally we will demonstrate how system incorporating this information can improve generation of responses to dofe depending on its model of beliefs regarding the relation underlying the denied expectation in dofe dialogues.
</nextsent>
<nextsent>the main motivation behind modelling cross-turn relations is to get at what expectations and beliefs speakers might have upon interpreting the previous turn in the dialogue.
</nextsent>
<nextsent>inferring the relations speakers perceive in cases where the related material spans speaker turns sheds light on how they interpret the previous speakers turn, which in turn enables response generation that can specifically address these implicit relations.
</nextsent>
<nextsent>here we focus on cases involving dofe, where the speaker of the but?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1587">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the ambiguity arises whenever prepositional phrase can modify preceding verb or noun, as in the canonical example saw the man with the telescope.
</prevsent>
<prevsent>in syntactic terms, the prepositional phrase attaches either tothe noun phrase or the verb phrase.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
many kinds of syntactic ambiguity can be resolved using structural information alone (briscoe and carroll, 1995; lin, 1998a; klein and manning, 2003), <papid> P03-1054 </papid>but in this case both candidate structures are perfectly grammatical and roughly equallylikely.</citsent>
<aftsection>
<nextsent>therefore ambiguous prepositional phrases require some kind of additional context to disambiguatecorrectly.
</nextsent>
<nextsent>in some cases small amount of lexical knowledge is sufficient: for example of almost always modifies the noun.
</nextsent>
<nextsent>other cases, such as the telescope example, are potentially much harder since discourse or world knowledge might be required.
</nextsent>
<nextsent>fortunately it is possible to do well at this task justby considering the lexical preferences of the words making up the pp.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1588">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, saw and telescope are more likely to occur together than man and telescope, so wecan infer that the correct attachment is likely to be verbal.
</prevsent>
<prevsent>the most useful lexical preferences are captured by the quadruple (v, n1, p, n2) where is the verb, n1 is the head of the direct object, is the preposition and n2 is the head of the prepositional phrase.
</prevsent>
</prevsection>
<citsent citstr=" H94-1048 ">
a benchmark dataset of 27,937 such quadruples was extracted from the wall street journal corpus by ratnaparkhi et al (1994) <papid> H94-1048 </papid>and has been the basis of many subsequent studies comparing machine learning algorithms and lexical resources.</citsent>
<aftsection>
<nextsent>this paper examines the effect of particular smoothing algorithms on the performance of an existing statistical pp model.a major problem faced by any statistical attachment algorithm is sparse data, which occurs when plausible ppsare not well-represented in the training data.
</nextsent>
<nextsent>for example, if the observed frequency of pp in the training is zero then the maximum likelihood estimate is also zero.
</nextsent>
<nextsent>since the training corpus only represents fraction of all possible pps, this is probably an underestimate of the true probability.
</nextsent>
<nextsent>an appealing course of action when faced with an unknown pp is to consider similar known examples instead.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1590">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>general purpose thesauruses list words that tend to occur together in free text; wewant to find words that behave in similar ways specifically within prepositional phrases.
</prevsent>
<prevsent>to this end we create pp thesaurus using existing similarity metrics but usinga corpus consisting of automatically extracted prepositional phrases.a thesaurus alone is not sufficient to solve the pp attachment problem; we also need model of the lexical preferences of prepositional phrases.
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
here we use the back-off model described in (collins and brooks, 1995) <papid> W95-0103 </papid>but with maximum likelihood estimates smoothed using similar pps discovered using thesaurus.</citsent>
<aftsection>
<nextsent>suchsimilarity-based smoothing methods have been successfully used in other nlp applications but our use of them here is novel.
</nextsent>
<nextsent>a key difference is that smoothing is not done over individual words but over entire prepositional phrases.
</nextsent>
<nextsent>similar pps are generated by replacing each component word with distributionally similar word, and we define similarity functions for comparing pps.
</nextsent>
<nextsent>we find that using score based on the rank of word in the similarity list is more accurate than the actual similarity scores provided by the thesaurus, which tend to weight less similar words too highly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1591">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 pp attachment.
</prevsent>
<prevsent>early work on pp attachment disambiguation used strictly syntactic or high-level pragmatic rules to decide on an attachment (frazier, 1979; altman and steedman, 1988).
</prevsent>
</prevsection>
<citsent citstr=" P90-1004 ">
however, work by whittemore et al (1990) <papid> P90-1004 </papid>and hindle and rooth (1993) <papid> J93-1005 </papid>showed that simple lexical preferences alone can deliver reasonable accuracy.</citsent>
<aftsection>
<nextsent>hindle and rooths approach was to use mostly unambiguous (v, n1, p) triples extracted from automatically parsed text to train maximum likelihood classifier.
</nextsent>
<nextsent>this achieved around 80% accuracy on ambiguous samples.
</nextsent>
<nextsent>this marked flowering in the field of pp attachment, with succession of papers bringing the whole armoury of machine learning techniques to bear on the problem.
</nextsent>
<nextsent>ratnaparkhi et al (1994) <papid> H94-1048 </papid>trained maximum entropy model on (v, n1, p, n2) quadruples extracted from the wall street journal corpus and achieved 81.6% accuracy.the collins and brooks (1995) <papid> W95-0103 </papid>model scores 84.5% accuracy on this task, and is one of the most accurate models that do not use additional supervision.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1592">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 pp attachment.
</prevsent>
<prevsent>early work on pp attachment disambiguation used strictly syntactic or high-level pragmatic rules to decide on an attachment (frazier, 1979; altman and steedman, 1988).
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
however, work by whittemore et al (1990) <papid> P90-1004 </papid>and hindle and rooth (1993) <papid> J93-1005 </papid>showed that simple lexical preferences alone can deliver reasonable accuracy.</citsent>
<aftsection>
<nextsent>hindle and rooths approach was to use mostly unambiguous (v, n1, p) triples extracted from automatically parsed text to train maximum likelihood classifier.
</nextsent>
<nextsent>this achieved around 80% accuracy on ambiguous samples.
</nextsent>
<nextsent>this marked flowering in the field of pp attachment, with succession of papers bringing the whole armoury of machine learning techniques to bear on the problem.
</nextsent>
<nextsent>ratnaparkhi et al (1994) <papid> H94-1048 </papid>trained maximum entropy model on (v, n1, p, n2) quadruples extracted from the wall street journal corpus and achieved 81.6% accuracy.the collins and brooks (1995) <papid> W95-0103 </papid>model scores 84.5% accuracy on this task, and is one of the most accurate models that do not use additional supervision.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1598">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>this marked flowering in the field of pp attachment, with succession of papers bringing the whole armoury of machine learning techniques to bear on the problem.
</prevsent>
<prevsent>ratnaparkhi et al (1994) <papid> H94-1048 </papid>trained maximum entropy model on (v, n1, p, n2) quadruples extracted from the wall street journal corpus and achieved 81.6% accuracy.the collins and brooks (1995) <papid> W95-0103 </papid>model scores 84.5% accuracy on this task, and is one of the most accurate models that do not use additional supervision.</prevsent>
</prevsection>
<citsent citstr=" W97-0109 ">
the current state of the art is 88% reported by stetina and nagao (1997) <papid> W97-0109 </papid>using the wsj text in conjunction with wordnet.</citsent>
<aftsection>
<nextsent>thenext section discusses other specific approaches that incorporate smoothing techniques.
</nextsent>
<nextsent>2.2 similarity-based smoothing.
</nextsent>
<nextsent>smoothing for statistical models involves adjusting probability estimates away from the maximum likelihood estimates to avoid the low probabilities caused by sparsedata.
</nextsent>
<nextsent>typically this involves mixing in probability distributions that have less context and are less likely to suffer from sparse data problems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1599">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>typically this involves mixing in probability distributions that have less context and are less likely to suffer from sparse data problems.
</prevsent>
<prevsent>for example, if the probability of an attachment given pp p(a|v, n1, p, n2) is undefined because that quadruple was not seen in the training data, then less specific distribution such as p(a|v, n1, p) can be used instead.
</prevsent>
</prevsection>
<citsent citstr=" P96-1041 ">
a wide range of different technique shave been proposed (chen and goodman, 1996) <papid> P96-1041 </papid>including the backing-off technique used by collins?</citsent>
<aftsection>
<nextsent>model (see section 3).
</nextsent>
<nextsent>an alternative but complementary approach is to mixin probabilities from distributions over similar?
</nextsent>
<nextsent>contexts.
</nextsent>
<nextsent>this is the idea behind both similarity-based andclass-based smoothing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1600">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>this is the idea behind both similarity-based andclass-based smoothing.
</prevsent>
<prevsent>class-based methods cluster similar words into classes which are then used in place of actual words.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
for example the class-based language model of (brown et al, 1992) <papid> J92-4003 </papid>is defined as: p(w2|w1) = p(w2|c2)p(c2|c1) (1) this helps solve the sparse data problem since the number of classes is usually much smaller than the number of words.class-based methods have been applied to the pp attachment task in several guises, using both automatic clustering and hand-crafted classes such as wordnet.</citsent>
<aftsection>
<nextsent>liand abe (1998) <papid> P98-2124 </papid>use both wordnet and an automatic clustering algorithm to achieve 85.2% accuracy on the wsj dataset.</nextsent>
<nextsent>the maximum entropy approach of ratnaparkhi et al (1994) <papid> H94-1048 </papid>uses the mutual information clustering algorithm described in (brown et al, 1992).<papid> J92-4003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1601">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>class-based methods cluster similar words into classes which are then used in place of actual words.
</prevsent>
<prevsent>for example the class-based language model of (brown et al, 1992) <papid> J92-4003 </papid>is defined as: p(w2|w1) = p(w2|c2)p(c2|c1) (1) this helps solve the sparse data problem since the number of classes is usually much smaller than the number of words.class-based methods have been applied to the pp attachment task in several guises, using both automatic clustering and hand-crafted classes such as wordnet.</prevsent>
</prevsection>
<citsent citstr=" P98-2124 ">
liand abe (1998) <papid> P98-2124 </papid>use both wordnet and an automatic clustering algorithm to achieve 85.2% accuracy on the wsj dataset.</citsent>
<aftsection>
<nextsent>the maximum entropy approach of ratnaparkhi et al (1994) <papid> H94-1048 </papid>uses the mutual information clustering algorithm described in (brown et al, 1992).<papid> J92-4003 </papid></nextsent>
<nextsent>although class based smoothing is shown to improve the model in both cases, some researchers have suggested that clustering words is counterproductive since the information lost byconflating words into broader classes outweighs the benefits derived from reducing data sparseness.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1604">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, lins similarity metric (lin,1998b) used in this paper is based on an information theoretic comparison between pair of co-occurrence probability distributions.
</prevsent>
<prevsent>this language model was incorporated into speech recognition system with some success (dagan et al,1999).
</prevsent>
</prevsection>
<citsent citstr=" P97-1008 ">
similarity-based methods have also been successfully applied word sense disambiguation (dagan etal., 1997) <papid> P97-1008 </papid>and extraction of grammatical relations (grishman and sterling, 1994).<papid> C94-2119 </papid></citsent>
<aftsection>
<nextsent>similarity-based smoothing techniques of the kind described here have not yet been applied to probabilistic pp attachment models.
</nextsent>
<nextsent>the memory-based learning approach of (zavrel et al, 1997) <papid> W97-1016 </papid>is the closest point of contact and shares many of the same ideas, although the details are quite different.</nextsent>
<nextsent>memory based learning consults similar previously-seen example sto make decision, but the similarity judgements are usually based on strict feature matching measure rather than on co-occurrence statistics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1605">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, lins similarity metric (lin,1998b) used in this paper is based on an information theoretic comparison between pair of co-occurrence probability distributions.
</prevsent>
<prevsent>this language model was incorporated into speech recognition system with some success (dagan et al,1999).
</prevsent>
</prevsection>
<citsent citstr=" C94-2119 ">
similarity-based methods have also been successfully applied word sense disambiguation (dagan etal., 1997) <papid> P97-1008 </papid>and extraction of grammatical relations (grishman and sterling, 1994).<papid> C94-2119 </papid></citsent>
<aftsection>
<nextsent>similarity-based smoothing techniques of the kind described here have not yet been applied to probabilistic pp attachment models.
</nextsent>
<nextsent>the memory-based learning approach of (zavrel et al, 1997) <papid> W97-1016 </papid>is the closest point of contact and shares many of the same ideas, although the details are quite different.</nextsent>
<nextsent>memory based learning consults similar previously-seen example sto make decision, but the similarity judgements are usually based on strict feature matching measure rather than on co-occurrence statistics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1606">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>similarity-based methods have also been successfully applied word sense disambiguation (dagan etal., 1997) <papid> P97-1008 </papid>and extraction of grammatical relations (grishman and sterling, 1994).<papid> C94-2119 </papid></prevsent>
<prevsent>similarity-based smoothing techniques of the kind described here have not yet been applied to probabilistic pp attachment models.</prevsent>
</prevsection>
<citsent citstr=" W97-1016 ">
the memory-based learning approach of (zavrel et al, 1997) <papid> W97-1016 </papid>is the closest point of contact and shares many of the same ideas, although the details are quite different.</citsent>
<aftsection>
<nextsent>memory based learning consults similar previously-seen example sto make decision, but the similarity judgements are usually based on strict feature matching measure rather than on co-occurrence statistics.
</nextsent>
<nextsent>under this scheme pizza and pasta are as different as pizza and paris.
</nextsent>
<nextsent>to overcome this zavrel et al also experiment with features based on reduced-dimensionality vector of co-occurrence statistics and note small (0.2%) increase in performance, leading to final accuracy of 84.4%.
</nextsent>
<nextsent>our use of specialist thesauruses for this task is also novel, although in they have been used in the some what related field of selectional preference acquisition by p(a|v, n1, p, n2) = 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1613">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> smoothing.  </section>
<citcontext>
<prevsection>
<prevsent>the similarity set for the pp open plant in korea might contain open tree in korea but the latters frequency is likely to be zero.
</prevsent>
<prevsent>generating triples is riskier since there is less context to rule out unlikely pps: the triple tree in korea is more plausible and possibly misleading.
</prevsent>
</prevsection>
<citsent citstr=" W97-0322 ">
but our model does have natural preference for the most frequent sense in the thesaurus training corpus, which is useful heuristic for word sense disambiguation (pedersen and bruce, 1997).<papid> W97-0322 </papid></citsent>
<aftsection>
<nextsent>for example, if the thesaurus is trained on business text then factory will be ranked higher than tree when the thesaurus trained ona business corpus (this issue is discussed further in section 5.2).
</nextsent>
<nextsent>finally, to complete our pp attachment scheme weneed to define similarity function between pps, expressed fully as ?
</nextsent>
<nextsent>( (v, n1, p, n2), (v?, n1, ?, n2) ) . the.
</nextsent>
<nextsent>raw materials we have to work with are the similarity scores for matching pairs of verbs and nouns as given by the thesaurus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1615">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for our experiments we use the wall street journal dataset created by ratnaparkhi et al (1994).<papid> H94-1048 </papid></prevsent>
<prevsent>this is divided into training set of 20,801 words, development set of 4,039 words and test set of 3,097 words.</prevsent>
</prevsection>
<citsent citstr=" W00-1427 ">
each word was reduced to its morphological root using the morphological analyser described in (minnen et al, 2000).<papid> W00-1427 </papid></citsent>
<aftsection>
<nextsent>strings of four digits beginning with 1 or 2are replaced with year and all other digit strings including those including commas and full stops were replaced with num.
</nextsent>
<nextsent>our implementation of collins?
</nextsent>
<nextsent>algorithm only achieves 84.3% on the test data, with the short fall of 0.2% primarily due to the different morphological analysers used3 5.1 smoothing.
</nextsent>
<nextsent>firstly we compare the different pp similarity functions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1616">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we hypothesised that using syntactic rather than semantic neighbours could be desirable, but in this case it often generates contexts that are unlikely to occur: pour price of profit as neighbour of fetch price of profit, for example.
</prevsent>
<prevsent>although this may be flaw in the approach,we may simply be using too few contexts to create reliable thesaurus.
</prevsent>
</prevsection>
<citsent citstr=" P02-1030 ">
previous research has found that using more data leads to better quality thesauruses (curran andmoens, 2002).<papid> P02-1030 </papid></citsent>
<aftsection>
<nextsent>we are also conflating attachment preferences, since word must appear with similar contexts inboth noun and verb modifying pps to achieve high sim thesaurus acc.
</nextsent>
<nextsent>size (n) cov.
</nextsent>
<nextsent>none 84.30 - 30.5 lin 85.02 13,850 72.1 wasps 85.05 17,843 60.1 specialist 84.50 5,669 61.0 table 2: accuracy on the test data using ? = 0.05 and = 5; the size of the noun section of each thesaurus, and coverage of smoothed 4- and 3-tuples 84 84.2 84.4 84.6 84.8 85 85.2 0 5 10 15 20 25 ac cu ra cy   num.
</nextsent>
<nextsent>neighbours (k) wasps lin specialist figure 4: accuracy of the three different thesauruses on the development set using rank smoothing with ? = 0.05 method accuracy wn?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1619">
<title id=" W04-2410.xml">thesauruses for prepositional phrase attachment </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>a second finding is that distributional similarity scores provided by all thesaurusesweight dissimilar neighbours too highly, and more aggressive weighting schemes are better for smoothing.
</prevsent>
<prevsent>our aim is to apply similarity-based smoothing with both generic and specialist thesauruses to other areas in lexicalised parse selection, particularly other overtly lexical problems such as noun-noun modifiers and conjunction scope.
</prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
lexical information has lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (gildea, 2001).<papid> W01-0521 </papid></citsent>
<aftsection>
<nextsent>appropriately trained thesauruses and similarity-based techniques should help to alleviate both problems.
</nextsent>
<nextsent>acknowledgements many thanks to julie weeds and adam kilgarriff for providing the specialist and wasps thesauruses, and for useful discussions.
</nextsent>
<nextsent>thanks also to the anonymous reviewers for many helpful comments.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1620">
<title id=" W04-1209.xml">support vector machine approach to extracting gene references into function from biological documents </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>combining these two models, we got totally 94,563 features.
</prevsent>
<prevsent>since we are extracting sentences discussing gene functions, its reasonable to expect gene or protein names in the generif sentence.
</prevsent>
</prevsection>
<citsent citstr=" C02-1110 ">
therefore, we employed yapex (olsson et al, 2002) <papid> C02-1110 </papid>and gap score (chang et al, 2004) protein/gene name detectors to count the number of protein/gene names in each of the 9 sentences, resulting in 94,581 features.</citsent>
<aftsection>
<nextsent>3.3.2 training svms the whole process related to svm was done via libsvm ? library for support vector machines (hsu et al, 2003).
</nextsent>
<nextsent>radial basis kernel was adopted based on our previous experience.
</nextsent>
<nextsent>however, further verification showed that the combined model with either linear or polynomial kernel only slightly surpassed the baseline, attaining 50.67% for cd.
</nextsent>
<nextsent>in order to get the best-performing classifier, we tuned two parameters, and gamma.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1621">
<title id=" W04-1708.xml">automatic measuring of english language proficiency using mt evaluation technology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the proposed method selects adequate test sentences from an existing corpus.
</prevsent>
<prevsent>then, it automatically evaluates the translations oftest sentences done by users.
</prevsent>
</prevsection>
<citsent citstr=" C92-2067 ">
the core technology of the proposed method, i.e., the automatic evaluation of translations, was developed in research aiming at the efficient development of machine translation (mt) technology (su et al., 1992; <papid> C92-2067 </papid>papineni et al, 2002; <papid> P02-1040 </papid>nist, 2002).</citsent>
<aftsection>
<nextsent>in the proposed method, we apply these mt evaluation technologies to the measurement ofhuman english language proficiency.
</nextsent>
<nextsent>the proposed method focuses on measuring the communicative skill of structuring sentences, which is indispensable for writing and speaking.
</nextsent>
<nextsent>it does not measure elementary capabilities including vocabulary or grammar.
</nextsent>
<nextsent>this method also proposes test sentence selection scheme to enable efficient testing.section 2 describes several automatic evaluation methods applied to the proposed method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1622">
<title id=" W04-1708.xml">automatic measuring of english language proficiency using mt evaluation technology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the proposed method selects adequate test sentences from an existing corpus.
</prevsent>
<prevsent>then, it automatically evaluates the translations oftest sentences done by users.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the core technology of the proposed method, i.e., the automatic evaluation of translations, was developed in research aiming at the efficient development of machine translation (mt) technology (su et al., 1992; <papid> C92-2067 </papid>papineni et al, 2002; <papid> P02-1040 </papid>nist, 2002).</citsent>
<aftsection>
<nextsent>in the proposed method, we apply these mt evaluation technologies to the measurement ofhuman english language proficiency.
</nextsent>
<nextsent>the proposed method focuses on measuring the communicative skill of structuring sentences, which is indispensable for writing and speaking.
</nextsent>
<nextsent>it does not measure elementary capabilities including vocabulary or grammar.
</nextsent>
<nextsent>this method also proposes test sentence selection scheme to enable efficient testing.section 2 describes several automatic evaluation methods applied to the proposed method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1626">
<title id=" W04-1313.xml">combining utterance boundary and predictability approaches to speech segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(brent and cartwright, 1996).
</prevsent>
<prevsent>the first approach we discuss relies on the utterance-boundary strategy, which consists in reusing the information provided by the occurrence of specific phoneme sequences at utterance beginnings or endings in order to hypoth 1to avoid latent ambiguity, it should be stated that speech segmentation refers here to process taking as input sequence of symbols (usually phonemes) and producing as output sequence of higher-level units (usually words).
</prevsent>
</prevsection>
<citsent citstr=" C69-1201 ">
esize boundaries inside utterances (aslin et al, 1996; christiansen et al, 1998; xanthos, 2004).the second approach is based on the predictability strategy, which assumes that speech should be segmented at locations where some measure of the uncertainty about the next symbol(phoneme or syllable for instance) is high (har ris, 1955; gammon, 1969; <papid> C69-1201 </papid>saffran et al, 1996; hutchens and adler, 1998; xanthos, 2003).our implementation of the utterance boundary strategy is based on n-grams statistics.</citsent>
<aftsection>
<nextsent>it was previously found to perform safe?
</nextsent>
<nextsent>word segmentation, that is with arather high precision, but also too conservative as witnessed by not so high recall (xanthos, 2004).
</nextsent>
<nextsent>as regards the predictability strategy, we have implemented an incremental interpretation of the classical successor count (harris, 1955).
</nextsent>
<nextsent>this approach also relies on the observation of phoneme sequences, the length of which is however not restricted to fixed value.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1630">
<title id=" W04-1313.xml">combining utterance boundary and predictability approaches to speech segmentation </title>
<section> conclusions and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in his work on 99
</prevsent>
<prevsent>    figure 5: average successor count for n-grams (based on the corpus described in section 4.1).
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
computational morphology, goldsmith (2001)<papid> J01-2001 </papid>uses harris?</citsent>
<aftsection>
<nextsent>successor count as means to reduce the search space of more powerful algorithm based on minimum description length (marcken, 1996).
</nextsent>
<nextsent>we go one step further and show that an utterance-boundary heuristic can be used in order to reduce the complexity of the successor count algorithm11.besides complexity issues, there is problem of data sparseness with the successor count, as it decreases very quickly while the size ofthe context grows.
</nextsent>
<nextsent>in the case of our quite redundant child-oriented corpus, the (weighted) average of the successor count12 for random n-gram wsn f(w) succ(w) gets lower than 1 for ? 9 (see figure 5).
</nextsent>
<nextsent>this means that inmost utterances, no more boundary can be inserted after the first 9 phonemes (respectively before the last 9 phonemes) unless we get close enough to the other extremity of the utterance for the predecessor (respectively successor)count to operate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1632">
<title id=" W04-2323.xml">unifying annotated discourse hierarchies to create a gold standard </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a novel method for unification that minimizes conflicts among annotators outperforms methods that require consensus among majority for the   and precision metrics, while capturing much of the structure of the discourse.
</prevsent>
<prevsent>when high recall is preferred, methods requiring majority are preferable tothose that demand full consensus among annotators.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
the linguistic structure of discourse is composed of utterances that exhibit meaningful hierarchical relationships (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>automatic segmentation of discourse forms the basis for many applications, from information retrieval and text summarization to anaphora resolution (hearst, 1997).<papid> J97-1003 </papid></nextsent>
<nextsent>these automatic methods, usually based on supervised machine learning techniques,require manually annotated corpus of data for train ing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1633">
<title id=" W04-2323.xml">unifying annotated discourse hierarchies to create a gold standard </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when high recall is preferred, methods requiring majority are preferable tothose that demand full consensus among annotators.
</prevsent>
<prevsent>the linguistic structure of discourse is composed of utterances that exhibit meaningful hierarchical relationships (grosz and sidner, 1986).<papid> J86-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
automatic segmentation of discourse forms the basis for many applications, from information retrieval and text summarization to anaphora resolution (hearst, 1997).<papid> J97-1003 </papid></citsent>
<aftsection>
<nextsent>these automatic methods, usually based on supervised machine learning techniques,require manually annotated corpus of data for training.
</nextsent>
<nextsent>the creation of these corpora often involves multiple judges annotating the same discourses, so as to avoid bias from using single judges annotations as ground truth.usually, for particular discourse, these multiple annotations are unified into single annotation, either manually by the annotators?
</nextsent>
<nextsent>discussions or automatically.
</nextsent>
<nextsent>however, annotation unification approaches have not been formally evaluated, and although manual unification mightbe the best approach, it can be time-consuming.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1635">
<title id=" W04-2323.xml">unifying annotated discourse hierarchies to create a gold standard </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discussions or automatically.
</prevsent>
<prevsent>however, annotation unification approaches have not been formally evaluated, and although manual unification mightbe the best approach, it can be time-consuming.
</prevsent>
</prevsection>
<citsent citstr=" P96-1038 ">
indeed, much of the work on automatic recognition of discourse structure has focused on linear, rather than hierarchical segmentation (hearst, 1997; <papid> J97-1003 </papid>hirschberg and nakatani, 1996), <papid> P96-1038 </papid>because of the difficulties of obtaining consistent hierarchical annotations.</citsent>
<aftsection>
<nextsent>in addition, those approaches that do handle hierarchical segmentation do not address automatic unification methods (carlson et al, 2001; <papid> W01-1605 </papid>marcu, 2000).</nextsent>
<nextsent>there are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations.first, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators havemet with difficulties.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1636">
<title id=" W04-2323.xml">unifying annotated discourse hierarchies to create a gold standard </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, annotation unification approaches have not been formally evaluated, and although manual unification mightbe the best approach, it can be time-consuming.
</prevsent>
<prevsent>indeed, much of the work on automatic recognition of discourse structure has focused on linear, rather than hierarchical segmentation (hearst, 1997; <papid> J97-1003 </papid>hirschberg and nakatani, 1996), <papid> P96-1038 </papid>because of the difficulties of obtaining consistent hierarchical annotations.</prevsent>
</prevsection>
<citsent citstr=" W01-1605 ">
in addition, those approaches that do handle hierarchical segmentation do not address automatic unification methods (carlson et al, 2001; <papid> W01-1605 </papid>marcu, 2000).</citsent>
<aftsection>
<nextsent>there are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations.first, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators havemet with difficulties.
</nextsent>
<nextsent>rotondo (1984) reported that hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words.?
</nextsent>
<nextsent>passonneau and litman (1993) <papid> P93-1020 </papid>conducted pilot study in which subjects found it difficult and time-consuming?</nextsent>
<nextsent>to identify hierarchical relations in discourse.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1637">
<title id=" W04-2323.xml">unifying annotated discourse hierarchies to create a gold standard </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations.first, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators havemet with difficulties.
</prevsent>
<prevsent>rotondo (1984) reported that hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words.?
</prevsent>
</prevsection>
<citsent citstr=" P93-1020 ">
passonneau and litman (1993) <papid> P93-1020 </papid>conducted pilot study in which subjects found it difficult and time-consuming?</citsent>
<aftsection>
<nextsent>to identify hierarchical relations in discourse.
</nextsent>
<nextsent>other attempts have had more success using improved annotation tools and more precise instructions (grosz and hirschberg, 1992; hirschberg and nakatani, 1996).<papid> P96-1038 </papid></nextsent>
<nextsent>second, hierarchical segmentation of discourse is subjective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1648">
<title id=" W04-2323.xml">unifying annotated discourse hierarchies to create a gold standard </title>
<section> measures of evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the   coefficient is computed as follows:   /
</prevsent>
<prevsent>ffi d
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
ffi carletta (1996) <papid> J96-2004 </papid>reports that content analysis researchers generally think of   !</citsent>
<aftsection>
<nextsent>1$# % as good reliability,?
</nextsent>
<nextsent>with 1$# &amp;( %)   )1$# % allowing tentative conclusions to be drawn.?
</nextsent>
<nextsent>all that remains is to define the chance agreement probability
</nextsent>
<nextsent>ffi . let
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1650">
<title id=" W04-2323.xml">unifying annotated discourse hierarchies to create a gold standard </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>some corpora, such as the penn treebank, require its annotators to meet whenever there is conflict so that the conflict can be resolved before the corpus is publicly released.
</prevsent>
<prevsent>penn has now begun discourse treebank aswell (creswell et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" P99-1032 ">
wiebe et al (1999) <papid> P99-1032 </papid>use statistical methods to automatically correct the biases in annotations of speaker subjectivity.</citsent>
<aftsection>
<nextsent>the corrections are then used as basis for further conflict resolution.
</nextsent>
<nextsent>carlson et al (2001) <papid> W01-1605 </papid>also used conflict resolution when creating their discourse-tagged corpus.</nextsent>
<nextsent>one interesting area of research would be to compare how annotators choose to resolve their conflicts compared to the different automatic approaches of finding gold standard.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1652">
<title id=" W04-0818.xml">the mitre logical form generation system </title>
<section> comments on the evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>one way in which to motivate guidelines, define scoring metrics, etc. is to include more goal directed task description.
</prevsent>
<prevsent>the last two decades of research in computational linguistics have cemented the crucial role of system evaluation, but the summary in (hirschman and thompson, 1996) makes it clear that the best evaluations are defined with specific task in mind.
</prevsent>
</prevsection>
<citsent citstr=" C96-1079 ">
in previous attempt to define predicate-argument structure, semeval, the effort was abandoned because so many constructs would require detailed attention and resolution, and be cause most information-extraction systems did not generate full predicate-argument structures (most likely because the task did not require it) (grishmanand sundheim, 1996).<papid> C96-1079 </papid></citsent>
<aftsection>
<nextsent>while introducing task creates its own problems by removing domain independence, the constraints it provides are worth consideration.
</nextsent>
<nextsent>for example, in task such as question answering, certain distinctions in the logic-form presented here may serve no purpose or perhaps finer grained distinctions are required.as another example of this issue, the scorer provided for this task computes the precision and recall for both predicates and predicate arguments in the logic forms.
</nextsent>
<nextsent>in some circumstances, the scorer assigns the same score for predication of an incorrect, independently specified variable (e.g., x2 instead of x1 as the first argument of loves in figure 1) as for predication of an otherwise unspecified variable(e.g., x3 instead of x1).
</nextsent>
<nextsent>this may be an informative scoring strategy, but having more specific task would help make this decision.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1653">
<title id=" W04-2606.xml">extended lexical semantic classification of english verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate the utility ofour novel classes by using them to support automatic subcategorization acquisition and show that the resulting extended classification has extensive coverage over the english verb lexicon.
</prevsent>
<prevsent>lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P98-1046 ">
(pinker, 1989; jackendoff, 1990; levin, 1993; dorr, 1997; dang et al, 1998; <papid> P98-1046 </papid>merlo and stevenson, 2001)).<papid> J01-3003 </papid></citsent>
<aftsection>
<nextsent>such classes can capture generalizations over range of (cross-)linguistic properties, and can therefore be used as valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge.verb classes have proved useful in various (multilingual) natural language processing (nlp) tasks and applications, such as computational lexicography (kipperet al, 2000), language generation (stede, 1998), <papid> J98-3003 </papid>machine translation (dorr, 1997), word sense disambiguation (prescher et al, 2000), <papid> C00-2094 </papid>document classification (kla vans and kan, 1998), <papid> P98-1112 </papid>and subcategorization acquisition (korhonen, 2002).</nextsent>
<nextsent>fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore critical component of any nlp system which needs to recoverpredicate-argument structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1654">
<title id=" W04-2606.xml">extended lexical semantic classification of english verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate the utility ofour novel classes by using them to support automatic subcategorization acquisition and show that the resulting extended classification has extensive coverage over the english verb lexicon.
</prevsent>
<prevsent>lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
(pinker, 1989; jackendoff, 1990; levin, 1993; dorr, 1997; dang et al, 1998; <papid> P98-1046 </papid>merlo and stevenson, 2001)).<papid> J01-3003 </papid></citsent>
<aftsection>
<nextsent>such classes can capture generalizations over range of (cross-)linguistic properties, and can therefore be used as valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge.verb classes have proved useful in various (multilingual) natural language processing (nlp) tasks and applications, such as computational lexicography (kipperet al, 2000), language generation (stede, 1998), <papid> J98-3003 </papid>machine translation (dorr, 1997), word sense disambiguation (prescher et al, 2000), <papid> C00-2094 </papid>document classification (kla vans and kan, 1998), <papid> P98-1112 </papid>and subcategorization acquisition (korhonen, 2002).</nextsent>
<nextsent>fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore critical component of any nlp system which needs to recoverpredicate-argument structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1655">
<title id=" W04-2606.xml">extended lexical semantic classification of english verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.
</prevsent>
<prevsent>(pinker, 1989; jackendoff, 1990; levin, 1993; dorr, 1997; dang et al, 1998; <papid> P98-1046 </papid>merlo and stevenson, 2001)).<papid> J01-3003 </papid></prevsent>
</prevsection>
<citsent citstr=" J98-3003 ">
such classes can capture generalizations over range of (cross-)linguistic properties, and can therefore be used as valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge.verb classes have proved useful in various (multilingual) natural language processing (nlp) tasks and applications, such as computational lexicography (kipperet al, 2000), language generation (stede, 1998), <papid> J98-3003 </papid>machine translation (dorr, 1997), word sense disambiguation (prescher et al, 2000), <papid> C00-2094 </papid>document classification (kla vans and kan, 1998), <papid> P98-1112 </papid>and subcategorization acquisition (korhonen, 2002).</citsent>
<aftsection>
<nextsent>fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore critical component of any nlp system which needs to recoverpredicate-argument structure.
</nextsent>
<nextsent>in many operational contexts, lexical information must be acquired from small application and/or domain-specific corpora.
</nextsent>
<nextsent>the predictive power of classes can help compensate for lack of sufficient data fully exemplifying the behaviour of relevant words, through use of back-off smoothing or similar techniques.
</nextsent>
<nextsent>although several classifications are now available for english verbs (e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1656">
<title id=" W04-2606.xml">extended lexical semantic classification of english verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.
</prevsent>
<prevsent>(pinker, 1989; jackendoff, 1990; levin, 1993; dorr, 1997; dang et al, 1998; <papid> P98-1046 </papid>merlo and stevenson, 2001)).<papid> J01-3003 </papid></prevsent>
</prevsection>
<citsent citstr=" C00-2094 ">
such classes can capture generalizations over range of (cross-)linguistic properties, and can therefore be used as valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge.verb classes have proved useful in various (multilingual) natural language processing (nlp) tasks and applications, such as computational lexicography (kipperet al, 2000), language generation (stede, 1998), <papid> J98-3003 </papid>machine translation (dorr, 1997), word sense disambiguation (prescher et al, 2000), <papid> C00-2094 </papid>document classification (kla vans and kan, 1998), <papid> P98-1112 </papid>and subcategorization acquisition (korhonen, 2002).</citsent>
<aftsection>
<nextsent>fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore critical component of any nlp system which needs to recoverpredicate-argument structure.
</nextsent>
<nextsent>in many operational contexts, lexical information must be acquired from small application and/or domain-specific corpora.
</nextsent>
<nextsent>the predictive power of classes can help compensate for lack of sufficient data fully exemplifying the behaviour of relevant words, through use of back-off smoothing or similar techniques.
</nextsent>
<nextsent>although several classifications are now available for english verbs (e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1657">
<title id=" W04-2606.xml">extended lexical semantic classification of english verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.
</prevsent>
<prevsent>(pinker, 1989; jackendoff, 1990; levin, 1993; dorr, 1997; dang et al, 1998; <papid> P98-1046 </papid>merlo and stevenson, 2001)).<papid> J01-3003 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1112 ">
such classes can capture generalizations over range of (cross-)linguistic properties, and can therefore be used as valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge.verb classes have proved useful in various (multilingual) natural language processing (nlp) tasks and applications, such as computational lexicography (kipperet al, 2000), language generation (stede, 1998), <papid> J98-3003 </papid>machine translation (dorr, 1997), word sense disambiguation (prescher et al, 2000), <papid> C00-2094 </papid>document classification (kla vans and kan, 1998), <papid> P98-1112 </papid>and subcategorization acquisition (korhonen, 2002).</citsent>
<aftsection>
<nextsent>fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore critical component of any nlp system which needs to recoverpredicate-argument structure.
</nextsent>
<nextsent>in many operational contexts, lexical information must be acquired from small application and/or domain-specific corpora.
</nextsent>
<nextsent>the predictive power of classes can help compensate for lack of sufficient data fully exemplifying the behaviour of relevant words, through use of back-off smoothing or similar techniques.
</nextsent>
<nextsent>although several classifications are now available for english verbs (e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1659">
<title id=" W04-2606.xml">extended lexical semantic classification of english verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, the largest and the most widely deployed classification in english, levins (1993) taxonomy, mainly deals with verbs taking noun and prepositional phrase complements, and does not provide large numbers of exemplars of the classes.
</prevsent>
<prevsent>the fact that no comprehensive classification is available limits the usefulness of the classes for practical nlp.
</prevsent>
</prevsection>
<citsent citstr=" P03-1009 ">
some experiments have been reported recently which indicate that it should be possible, in the future, to automatically supplement extant classifications with novel verb classes and member verbs from corpus data (brew and schulte im walde, 2002; merlo and stevenson, 2001; <papid> J01-3003 </papid>korhonen et al, 2003).<papid> P03-1009 </papid></citsent>
<aftsection>
<nextsent>while the automatic approach will avoid the expensive overhead of manual classification, the very development of the technology capable of large-scale automatic classification will require access to target classification and gold standard exemplification of it more extensive than that available currently.in this paper, we address these problems by introducing substantial extension to levins classification which incorporates 57 novel classes for verbs not covered (com prehensively) by levin.
</nextsent>
<nextsent>these classes, many of them drawn initially from linguistic resources, were created semi-automatically by looking for dia thesis alternations shared by candidate verbs.
</nextsent>
<nextsent>106 new alternations not covered by levin were identified for this work.
</nextsent>
<nextsent>we demonstrate the usefulness of our novel classes by using themto improve the performance of our extant subcategorization acquisition system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1661">
<title id=" W04-2606.xml">extended lexical semantic classification of english verbs </title>
<section> instrument subject alternation:.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 novel dia thesis alternations.
</prevsent>
<prevsent>when constructing novel dia thesis alternations, we took as starting point the subcategorization classification of briscoe (2000).
</prevsent>
</prevsection>
<citsent citstr=" P87-1027 ">
this fairly comprehensive classification incorporates 163 different subcategorization frames (scfs), superset of those listed in the anlt (boguraev et al, 1987) <papid> P87-1027 </papid>and comlex syntax dictionaries (grishman et al, 1994).<papid> C94-1042 </papid></citsent>
<aftsection>
<nextsent>the scfs define mappings from surface arguments to predicate-argument structure for bounded dependency constructions, but abstract over specific particles and prepositions, as these can be trivially inst anti ated when the frame is associated with specific verb.
</nextsent>
<nextsent>as most dia thesis alternations are only semi-predictable on verb-by-verb basis, distinct scf is defined for every such construction, and thus all alternations can be represented as mappings between such scfs.
</nextsent>
<nextsent>we considered possible alternations between pairs of scfs in this classification, focusing in particular on thosescfs not covered by levin.
</nextsent>
<nextsent>the identification of alternations was done manually, using criteria similar to levins:the scfs alternating should preserve the sense in question, or modify it systematically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1662">
<title id=" W04-2606.xml">extended lexical semantic classification of english verbs </title>
<section> instrument subject alternation:.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 novel dia thesis alternations.
</prevsent>
<prevsent>when constructing novel dia thesis alternations, we took as starting point the subcategorization classification of briscoe (2000).
</prevsent>
</prevsection>
<citsent citstr=" C94-1042 ">
this fairly comprehensive classification incorporates 163 different subcategorization frames (scfs), superset of those listed in the anlt (boguraev et al, 1987) <papid> P87-1027 </papid>and comlex syntax dictionaries (grishman et al, 1994).<papid> C94-1042 </papid></citsent>
<aftsection>
<nextsent>the scfs define mappings from surface arguments to predicate-argument structure for bounded dependency constructions, but abstract over specific particles and prepositions, as these can be trivially inst anti ated when the frame is associated with specific verb.
</nextsent>
<nextsent>as most dia thesis alternations are only semi-predictable on verb-by-verb basis, distinct scf is defined for every such construction, and thus all alternations can be represented as mappings between such scfs.
</nextsent>
<nextsent>we considered possible alternations between pairs of scfs in this classification, focusing in particular on thosescfs not covered by levin.
</nextsent>
<nextsent>the identification of alternations was done manually, using criteria similar to levins:the scfs alternating should preserve the sense in question, or modify it systematically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1664">
<title id=" W04-2606.xml">extended lexical semantic classification of english verbs </title>
<section> try attempt, try.  </section>
<citcontext>
<prevsection>
<prevsent>57.
</prevsent>
<prevsent>adopt assume, adopt.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
table 2: new verb classes 1000 citations, on average, for each verb.our method for scf acquisition (korho nen, 2002) involves first using the system of briscoe and carroll (1997) <papid> A97-1052 </papid>to acquire putative scf distribution for each test verb from corpus data.</citsent>
<aftsection>
<nextsent>this system employs robust statistical parser (briscoe and carroll, 2002) which yields complete though shallow parses from the pos tagged data.
</nextsent>
<nextsent>the parse contexts around verbs are passed to comprehensive scf classifier, which selects one of the 163 scfs.
</nextsent>
<nextsent>the scf distribution is then smoothed with the back-off distribution corresponding to the semantic class of the predominant sense of verb.
</nextsent>
<nextsent>although many of the test verbs are polysemic, we relied on the knowledge that the majority of english verbs have single predominating sense in balanced corpus data (korhonen and preiss, 2003).<papid> P03-1007 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1665">
<title id=" W04-2606.xml">extended lexical semantic classification of english verbs </title>
<section> try attempt, try.  </section>
<citcontext>
<prevsection>
<prevsent>the parse contexts around verbs are passed to comprehensive scf classifier, which selects one of the 163 scfs.
</prevsent>
<prevsent>the scf distribution is then smoothed with the back-off distribution corresponding to the semantic class of the predominant sense of verb.
</prevsent>
</prevsection>
<citsent citstr=" P03-1007 ">
although many of the test verbs are polysemic, we relied on the knowledge that the majority of english verbs have single predominating sense in balanced corpus data (korhonen and preiss, 2003).<papid> P03-1007 </papid></citsent>
<aftsection>
<nextsent>the back-off estimates were obtained by the following method: (i) few individual verbs were chosen from new verb class whose predominant sense according to the wordnet frequency data belongs to this class,(ii) scf distributions were built for these verbs by manually analysing c. 300 occurrences of each verb in the bnc, (iii) the resulting scf distributions were merged.
</nextsent>
<nextsent>an empirically-determined threshold was finally set on the probability estimates from smoothing to reject noisy scfs caused by errors during the statistical parsing phase.
</nextsent>
<nextsent>this method for scf acquisition is highly sensitive to the accuracy of the lexical-semantic classes.
</nextsent>
<nextsent>where class adequately predicts the syntactic behaviour of the predominant sense of test verb, significant improvement is seen in scf acquisition, as accurate back-off estimates help to correct the acquired scf distribution and deal with sparse data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1666">
<title id=" W04-2606.xml">extended lexical semantic classification of english verbs </title>
<section> try attempt, try.  </section>
<citcontext>
<prevsection>
<prevsent>while it is possible that thenew classes require further adjustment until optimal accuracy can be obtained, it is clear that many of our test verbs (and verbs in our new classes in general) are more polysemic on average and thus more difficult?
</prevsent>
<prevsent>than those employed by korhonen (2002).
</prevsent>
</prevsection>
<citsent citstr=" W02-2014 ">
our subcategorization acquisition method, based on predominant sense heuristics, is less adequate for these verbs ? rather, method based on word sense disambiguation and the use of multi 3for the details of these measures and their application to this task see korhonen and krymolowski (2002).<papid> W02-2014 </papid></citsent>
<aftsection>
<nextsent>4korhonen (2002) reports 17.8% absolute improvement in  -measure with the back-off scheme on 45 test verbs.
</nextsent>
<nextsent>ple classes should be employed to establish the true upper bound on performance.
</nextsent>
<nextsent>korhonen and preiss (2003) <papid> P03-1007 </papid>have proposed such method, but the method is not currently applicable to our test data.</nextsent>
<nextsent>4.2 evaluation of coverage.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1670">
<title id=" W04-0829.xml">wsd based on mutual information and syntactic patterns </title>
<section> selection of the closest variant.  </section>
<citcontext>
<prevsection>
<prevsent>this last problem has been addressed in very straightforward manner, since we have discarded the senses for word with relative frequency below 10%.the first problem might very well improve by it self when larger untagged corpora are available and increasing computing power eliminates the need fora limited controlled vocabulary in the mi calculations.
</prevsent>
<prevsent>anyway, solution that we have tried to implement for this source of problems, that is, cumulative errors in estimates biasing the final result, consists in restricting the application of the mi measure to promising candidates.
</prevsent>
</prevsection>
<citsent citstr=" H93-1052 ">
an interesting criterion for the selection of these candidates is to select those words in the context that form collocation with the word to be disam biguated, in the sense that is defined in (yarowsky, 1993).<papid> H93-1052 </papid></citsent>
<aftsection>
<nextsent>yarowsky claimed that collocations are nearly monosemous, so identifying them would allow us to focus on very local context, which should make the disambiguation process, if not more efficient, at least easier to interpretate.one example of test item that was incorrectly disambiguated by the systems described in (fernandez-amoros et al, 2001) is the word church in the sentence : association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems an ancient stone church stands amid the fields, the sound of bells cascading from its tower, calling the faithful to evensong.
</nextsent>
<nextsent>the applicable collocation here would be noun/noun so that stone is the context word to be used.to address the second problem, the use of non relevant words in the glosses, we have decided to consider only the variants (the synonyms in synset,in the case of wordnet) of each sense.
</nextsent>
<nextsent>these synonyms (i.e. variants of sense) constitute the intimate matter of wordnet synsets, change in asynset implies change in the senses of the corresponding words, while the glosses are just additional information of secondary importance in the design of the sense inventory.
</nextsent>
<nextsent>to continue with the example, the synonyms for the three synsets for church in wordnet are (excluding church itself, which is obviously common to all the synsets) :?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1671">
<title id=" W04-0829.xml">wsd based on mutual information and syntactic patterns </title>
<section> selection of the closest variant.  </section>
<citcontext>
<prevsection>
<prevsent>the numbers in parenthesis indicate the mi 1 between the term and stone.
</prevsent>
<prevsent>in this case we have clear and strong preference for the second sense, which happens to be in accordance with the gold standard.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
unfortunately, we didnt have the time to finish collocation detection procedure, we just had enough time to pos-tag the text with the brill tagger (brill, 1992) <papid> A92-1021 </papid>and parse it with the collins parser (collins, 1999).</citsent>
<aftsection>
<nextsent>that effort was put to use in the syntactic pattern-matching heuristic in the next section, so in this case we just limited ourselves to detect, for each variant, the context word with the highest mi.
</nextsent>
<nextsent>it is important to note that this heuristic is not dependent on the glosses and it is completely unsupervised, so that it is possible to apply it to any language with sense inventory based on variants, as is the case with the languages in euro wordnet, and an untagged corpus.
</nextsent>
<nextsent>we have evaluated this heuristic and the results are shown in table 1 1for words and b, mi(a,b)= p(ab)p(a)p(b) , the probabilities are estimated in corpus.
</nextsent>
<nextsent>task attempted prec recall all words 1215 / 2041 .58 .35 lexical sample 938 / 3944 .45 .11 table 1: closest variant heuristic results the dt art nn
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1672">
<title id=" W04-2214.xml">revising the wordnet domains hierarchy semantics coverage and balancing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, as the english synset {court, tribunal, judicature} was annotated with the domain law, also the italian synset {corte, tribunale}, which is aligned with the corresponding english synset, results automatically annotated with the law domain.
</prevsent>
<prevsent>this procedure can be applied to any other wordnet (or part of it) aligned with princeton wordnet (see for instance the spanish wordnet).
</prevsent>
</prevsection>
<citsent citstr=" W02-1304 ">
it is worth noticing that two of the main on going projects addressing the construction of multilingual resources, that is meaning (rigau et al 2002) <papid> W02-1304 </papid>and balkan et (see web site), make use of wordnet domains.</citsent>
<aftsection>
<nextsent>finally, wordnet domains is being profitably used by the nlp community mainly for word sense disambiguation tasks in various languages.
</nextsent>
<nextsent>another application of domain hierarchies can be found in the field of corpus creation.
</nextsent>
<nextsent>in many existing corpora (see for instance the bnc, the anc, the brown and lob corpora) domain is one of the most used criteria for text selection and/or classification.
</nextsent>
<nextsent>given that domain hierarchy is language independent, if the same domain hierarchy is used to build reference corpora for different languages, then it would be easy to create (a first approximation of) comparable corpora by putting in correspondence corpora sections belonging to the same domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1673">
<title id=" W03-2804.xml">a quantitative method for machine translation evaluation </title>
<section> metrics in mt evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>it presents the drawback of requiring great human effort before actually being able to use it.
</prevsent>
<prevsent>however, the effort is worthwhile, if it can be later used for hundreds of evaluations.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
bleu score: bleu is an automatic metric designed by ibm, which uses several references (papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>the main problem of mwer is that all possible reference translations cannot be introduced.
</nextsent>
<nextsent>the bleu score try to solve this problem by combining the available references.
</nextsent>
<nextsent>in simplified manner we could say that it measures how many word sequences in the sentence under evaluation match the word sequences of some reference sentence.
</nextsent>
<nextsent>the bleu score also includes penalty for translations whose length differs significantly from that of the reference translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1674">
<title id=" W03-2804.xml">a quantitative method for machine translation evaluation </title>
<section> metrics in mt evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the evaluation process can be carried out very quickly, if one takes as the starting point the result obtained by the wer or the mwer.
</prevsent>
<prevsent>the idea consists of visualising the incorrect words detected by one of these methods (editing operations).
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
the evaluator just needs to indicate whether each of the marked items is an actual error or whether it can rather be considered as an alternative translation this metric resembles very much the one proposed in (brown et al 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>that work suggested for measuring the translation quality counting the number of times an evaluator would have to press the keyboard keys in order to make the proposed sentence correct.
</nextsent>
<nextsent>all references sentence error rate (aser): the ser metric presents the drawback of working with only one reference.
</nextsent>
<nextsent>therefore, it does not really measure the number of wrong sentences, but rather those that do not match exactly the reference.
</nextsent>
<nextsent>for this reason we thought it would be interesting to introduce metric that could indicate the percentage of sentences whose acronym name on references description wer word error rate word 1 % of words which are to be inserted, deleted or replaced in order to obtain the reference.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1675">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>accordingly, to accomplish the task of topic identification, we have to solve the problem of zero anaphora resolution.
</prevsent>
<prevsent>there are several methods of anaphora resolution.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
one method is to integrate different knowledge sources or factors (e.g. gender and number agreement, c-command constraints, semantic information) that discount unlikely candidates until minimal set of plausible candidates is obtained (grosz et al, 1995; <papid> J95-2003 </papid>lappin and leass, 1994; <papid> J94-4002 </papid>okumura and tamura, 1996; <papid> C96-2147 </papid>walker et al, 1998; yeh and chen, 2001).</citsent>
<aftsection>
<nextsent>anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge.
</nextsent>
<nextsent>however, it is very labor-intensive and time-consuming to construct domain knowledge base.
</nextsent>
<nextsent>another method employs statistical models or ai techniques, such as machine learning, to compute the most likely candidate (aone and bennett, 1995; connoly et al, 1994; ge et al, 1998; <papid> W98-1119 </papid>seki et al, 2002).<papid> C02-1078 </papid></nextsent>
<nextsent>this method can sort out the above problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1676">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>accordingly, to accomplish the task of topic identification, we have to solve the problem of zero anaphora resolution.
</prevsent>
<prevsent>there are several methods of anaphora resolution.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
one method is to integrate different knowledge sources or factors (e.g. gender and number agreement, c-command constraints, semantic information) that discount unlikely candidates until minimal set of plausible candidates is obtained (grosz et al, 1995; <papid> J95-2003 </papid>lappin and leass, 1994; <papid> J94-4002 </papid>okumura and tamura, 1996; <papid> C96-2147 </papid>walker et al, 1998; yeh and chen, 2001).</citsent>
<aftsection>
<nextsent>anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge.
</nextsent>
<nextsent>however, it is very labor-intensive and time-consuming to construct domain knowledge base.
</nextsent>
<nextsent>another method employs statistical models or ai techniques, such as machine learning, to compute the most likely candidate (aone and bennett, 1995; connoly et al, 1994; ge et al, 1998; <papid> W98-1119 </papid>seki et al, 2002).<papid> C02-1078 </papid></nextsent>
<nextsent>this method can sort out the above problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1677">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>accordingly, to accomplish the task of topic identification, we have to solve the problem of zero anaphora resolution.
</prevsent>
<prevsent>there are several methods of anaphora resolution.
</prevsent>
</prevsection>
<citsent citstr=" C96-2147 ">
one method is to integrate different knowledge sources or factors (e.g. gender and number agreement, c-command constraints, semantic information) that discount unlikely candidates until minimal set of plausible candidates is obtained (grosz et al, 1995; <papid> J95-2003 </papid>lappin and leass, 1994; <papid> J94-4002 </papid>okumura and tamura, 1996; <papid> C96-2147 </papid>walker et al, 1998; yeh and chen, 2001).</citsent>
<aftsection>
<nextsent>anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge.
</nextsent>
<nextsent>however, it is very labor-intensive and time-consuming to construct domain knowledge base.
</nextsent>
<nextsent>another method employs statistical models or ai techniques, such as machine learning, to compute the most likely candidate (aone and bennett, 1995; connoly et al, 1994; ge et al, 1998; <papid> W98-1119 </papid>seki et al, 2002).<papid> C02-1078 </papid></nextsent>
<nextsent>this method can sort out the above problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1678">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge.
</prevsent>
<prevsent>however, it is very labor-intensive and time-consuming to construct domain knowledge base.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
another method employs statistical models or ai techniques, such as machine learning, to compute the most likely candidate (aone and bennett, 1995; connoly et al, 1994; ge et al, 1998; <papid> W98-1119 </papid>seki et al, 2002).<papid> C02-1078 </papid></citsent>
<aftsection>
<nextsent>this method can sort out the above problems.
</nextsent>
<nextsent>however, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (stuckardt, 2002).
</nextsent>
<nextsent>our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable nlp tools such as part of-speech (pos) tagger and shallow parsers (baldwin, 1997; <papid> W97-1306 </papid>ferrndez et al, 1998; kennedy and boguraev, 1996; <papid> C96-1021 </papid>mitkov, 1998; <papid> P98-2143 </papid>yeh and chen, 2003).</nextsent>
<nextsent>the resolution process works from the output of pos tagger enriched with annotations of grammatical function of lexical items in the input text stream.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1679">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge.
</prevsent>
<prevsent>however, it is very labor-intensive and time-consuming to construct domain knowledge base.
</prevsent>
</prevsection>
<citsent citstr=" C02-1078 ">
another method employs statistical models or ai techniques, such as machine learning, to compute the most likely candidate (aone and bennett, 1995; connoly et al, 1994; ge et al, 1998; <papid> W98-1119 </papid>seki et al, 2002).<papid> C02-1078 </papid></citsent>
<aftsection>
<nextsent>this method can sort out the above problems.
</nextsent>
<nextsent>however, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (stuckardt, 2002).
</nextsent>
<nextsent>our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable nlp tools such as part of-speech (pos) tagger and shallow parsers (baldwin, 1997; <papid> W97-1306 </papid>ferrndez et al, 1998; kennedy and boguraev, 1996; <papid> C96-1021 </papid>mitkov, 1998; <papid> P98-2143 </papid>yeh and chen, 2003).</nextsent>
<nextsent>the resolution process works from the output of pos tagger enriched with annotations of grammatical function of lexical items in the input text stream.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1680">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this method can sort out the above problems.
</prevsent>
<prevsent>however, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (stuckardt, 2002).
</prevsent>
</prevsection>
<citsent citstr=" W97-1306 ">
our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable nlp tools such as part of-speech (pos) tagger and shallow parsers (baldwin, 1997; <papid> W97-1306 </papid>ferrndez et al, 1998; kennedy and boguraev, 1996; <papid> C96-1021 </papid>mitkov, 1998; <papid> P98-2143 </papid>yeh and chen, 2003).</citsent>
<aftsection>
<nextsent>the resolution process works from the output of pos tagger enriched with annotations of grammatical function of lexical items in the input text stream.
</nextsent>
<nextsent>the shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents.
</nextsent>
<nextsent>in the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence.
</nextsent>
<nextsent>in section 3 we describe the details of shallow parsing we employed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1681">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this method can sort out the above problems.
</prevsent>
<prevsent>however, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (stuckardt, 2002).
</prevsent>
</prevsection>
<citsent citstr=" C96-1021 ">
our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable nlp tools such as part of-speech (pos) tagger and shallow parsers (baldwin, 1997; <papid> W97-1306 </papid>ferrndez et al, 1998; kennedy and boguraev, 1996; <papid> C96-1021 </papid>mitkov, 1998; <papid> P98-2143 </papid>yeh and chen, 2003).</citsent>
<aftsection>
<nextsent>the resolution process works from the output of pos tagger enriched with annotations of grammatical function of lexical items in the input text stream.
</nextsent>
<nextsent>the shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents.
</nextsent>
<nextsent>in the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence.
</nextsent>
<nextsent>in section 3 we describe the details of shallow parsing we employed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1682">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this method can sort out the above problems.
</prevsent>
<prevsent>however, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (stuckardt, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable nlp tools such as part of-speech (pos) tagger and shallow parsers (baldwin, 1997; <papid> W97-1306 </papid>ferrndez et al, 1998; kennedy and boguraev, 1996; <papid> C96-1021 </papid>mitkov, 1998; <papid> P98-2143 </papid>yeh and chen, 2003).</citsent>
<aftsection>
<nextsent>the resolution process works from the output of pos tagger enriched with annotations of grammatical function of lexical items in the input text stream.
</nextsent>
<nextsent>the shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents.
</nextsent>
<nextsent>in the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence.
</nextsent>
<nextsent>in section 3 we describe the details of shallow parsing we employed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1683">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> centering model.  </section>
<citcontext>
<prevsection>
<prevsent>the method of topic identification in chinese sentences is illustrated in section 5.
</prevsent>
<prevsent>in the last section the conclusions are made.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
in the centering theory (grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al 1995; <papid> J95-2003 </papid>walker et al, 1994; <papid> J94-2003 </papid>strube and hahn, 1996), <papid> P96-1036 </papid>the  attentional state  was identified as basic component of discourse structure that consisted of two levels of focusing: global and local.</citsent>
<aftsection>
<nextsent>for grosz and sidner, the centering theory provided model for monitoring local focus and yielded the centering model which was designed to account for the difference in the perceived coherence of discourses.
</nextsent>
<nextsent>in the centering model, each utterance in discourse segment has two structures associated with it, called forward looking centers, cf(u), and backward-looking center, cb(u).
</nextsent>
<nextsent>the forward-looking centers of un, cf(un), depend only on the expressions that constitute that utterance.
</nextsent>
<nextsent>they are not constrained by features of any previous utterance in the discourse segment (ds), and the elements of cf(un) are partially ordered to reflect relative prominence in un.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1686">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> centering model.  </section>
<citcontext>
<prevsection>
<prevsent>the method of topic identification in chinese sentences is illustrated in section 5.
</prevsent>
<prevsent>in the last section the conclusions are made.
</prevsent>
</prevsection>
<citsent citstr=" J94-2003 ">
in the centering theory (grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al 1995; <papid> J95-2003 </papid>walker et al, 1994; <papid> J94-2003 </papid>strube and hahn, 1996), <papid> P96-1036 </papid>the  attentional state  was identified as basic component of discourse structure that consisted of two levels of focusing: global and local.</citsent>
<aftsection>
<nextsent>for grosz and sidner, the centering theory provided model for monitoring local focus and yielded the centering model which was designed to account for the difference in the perceived coherence of discourses.
</nextsent>
<nextsent>in the centering model, each utterance in discourse segment has two structures associated with it, called forward looking centers, cf(u), and backward-looking center, cb(u).
</nextsent>
<nextsent>the forward-looking centers of un, cf(un), depend only on the expressions that constitute that utterance.
</nextsent>
<nextsent>they are not constrained by features of any previous utterance in the discourse segment (ds), and the elements of cf(un) are partially ordered to reflect relative prominence in un.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1687">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> centering model.  </section>
<citcontext>
<prevsection>
<prevsent>the method of topic identification in chinese sentences is illustrated in section 5.
</prevsent>
<prevsent>in the last section the conclusions are made.
</prevsent>
</prevsection>
<citsent citstr=" P96-1036 ">
in the centering theory (grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al 1995; <papid> J95-2003 </papid>walker et al, 1994; <papid> J94-2003 </papid>strube and hahn, 1996), <papid> P96-1036 </papid>the  attentional state  was identified as basic component of discourse structure that consisted of two levels of focusing: global and local.</citsent>
<aftsection>
<nextsent>for grosz and sidner, the centering theory provided model for monitoring local focus and yielded the centering model which was designed to account for the difference in the perceived coherence of discourses.
</nextsent>
<nextsent>in the centering model, each utterance in discourse segment has two structures associated with it, called forward looking centers, cf(u), and backward-looking center, cb(u).
</nextsent>
<nextsent>the forward-looking centers of un, cf(un), depend only on the expressions that constitute that utterance.
</nextsent>
<nextsent>they are not constrained by features of any previous utterance in the discourse segment (ds), and the elements of cf(un) are partially ordered to reflect relative prominence in un.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1693">
<title id=" W04-0714.xml">topic identification in chinese based on centering model </title>
<section> ranking of elements in cf(ui) guides.  </section>
<citcontext>
<prevsection>
<prevsent>(securities stocks) fell by close one after another.
</prevsent>
<prevsent>3 shallow parsing.
</prevsent>
</prevsection>
<citsent citstr=" W01-0706 ">
shallow (or partial) parsing which is an inexpensive, fast and reliable method does not deliver full syntactic analysis but is limited to parsing smaller constituents such as noun phrases or verb phrases (abney, 1996; li and roth, 2001; <papid> W01-0706 </papid>mitkov, 1999).</citsent>
<aftsection>
<nextsent>for example, the sentence (2) can be divided as follows: (2) !  # $% &amp;  ( )* hualien became the popular tourist attraction.
</nextsent>
<nextsent>[np ! ] [vp  # ] [np $% &amp;  ( )*] 1 we use b a?
</nextsent>
<nextsent>to denote zero anaphor, where the sub script is the index of the zero anaphor itself and the super script is the index of the referent.
</nextsent>
<nextsent>a single
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1694">
<title id=" W04-0402.xml">paraphrasing of japanese light verb constructions based on lexical conceptual structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results show that our lcs-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases.
</prevsent>
<prevsent>automatic paraphrase generation technology offers the potential to bridge gaps between the authors and readers of documents.
</prevsent>
</prevsection>
<citsent citstr=" W03-1602 ">
for example, system that is capable of simplifying given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting reader (carroll et al, 1999; inui et al, 2003).<papid> W03-1602 </papid>in japanese, like other languages, there are several classes of paraphrasing that exhibit degree of regularity that allows them to be explained by handful of sophisticated general rules and lexical semantic knowledge.</citsent>
<aftsection>
<nextsent>for example, paraphrases associated with voice alteration, verb/case alteration, compounds, and lexical derivations all fall into such classes.
</nextsent>
<nextsent>in this paper, we focus our discussion on another useful class of paraphrases, namely, the paraphrasing of light-verb constructions (lvcs), and propose computational model for generating paraphrases of this class.
</nextsent>
<nextsent>sentence (1s) is an example of an lvc1.
</nextsent>
<nextsent>an lvcis verb phrase (kandou-o ataeta (made an impres sion)?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1695">
<title id=" W04-0402.xml">paraphrasing of japanese light verb constructions based on lexical conceptual structure </title>
<section> motivation, target, and related work.  </section>
<citcontext>
<prevsection>
<prevsent>and that potential?
</prevsent>
<prevsent>can be paraphrased into possibility.?
</prevsent>
</prevsection>
<citsent citstr=" W01-0814 ">
several attempts have been made to develop such resources manually (sato, 1999; dras, 1999; inui and nogami, 2001); <papid> W01-0814 </papid>those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct sufficiently comprehensive resource for practical applications.</citsent>
<aftsection>
<nextsent>there is another trend in the research in this field,namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (barzilay and mckeown, 2001; <papid> P01-1008 </papid>lin and pantel, 2001; pang et al., 2003; <papid> N03-1024 </papid>shinyama and sekine, 2003, <papid> W03-1609 </papid>etc.).</nextsent>
<nextsent>this type of approach may be able to reduce the cost of resource development.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1696">
<title id=" W04-0402.xml">paraphrasing of japanese light verb constructions based on lexical conceptual structure </title>
<section> motivation, target, and related work.  </section>
<citcontext>
<prevsection>
<prevsent>can be paraphrased into possibility.?
</prevsent>
<prevsent>several attempts have been made to develop such resources manually (sato, 1999; dras, 1999; inui and nogami, 2001); <papid> W01-0814 </papid>those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct sufficiently comprehensive resource for practical applications.</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
there is another trend in the research in this field,namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (barzilay and mckeown, 2001; <papid> P01-1008 </papid>lin and pantel, 2001; pang et al., 2003; <papid> N03-1024 </papid>shinyama and sekine, 2003, <papid> W03-1609 </papid>etc.).</citsent>
<aftsection>
<nextsent>this type of approach may be able to reduce the cost of resource development.
</nextsent>
<nextsent>there are problems that must be overcome, however, before they can work practically.
</nextsent>
<nextsent>first, automatically acquired patterns tend to be complex.
</nextsent>
<nextsent>for example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: is purchased by ? buys x.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1697">
<title id=" W04-0402.xml">paraphrasing of japanese light verb constructions based on lexical conceptual structure </title>
<section> motivation, target, and related work.  </section>
<citcontext>
<prevsection>
<prevsent>can be paraphrased into possibility.?
</prevsent>
<prevsent>several attempts have been made to develop such resources manually (sato, 1999; dras, 1999; inui and nogami, 2001); <papid> W01-0814 </papid>those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct sufficiently comprehensive resource for practical applications.</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
there is another trend in the research in this field,namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (barzilay and mckeown, 2001; <papid> P01-1008 </papid>lin and pantel, 2001; pang et al., 2003; <papid> N03-1024 </papid>shinyama and sekine, 2003, <papid> W03-1609 </papid>etc.).</citsent>
<aftsection>
<nextsent>this type of approach may be able to reduce the cost of resource development.
</nextsent>
<nextsent>there are problems that must be overcome, however, before they can work practically.
</nextsent>
<nextsent>first, automatically acquired patterns tend to be complex.
</nextsent>
<nextsent>for example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: is purchased by ? buys x.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1698">
<title id=" W04-0402.xml">paraphrasing of japanese light verb constructions based on lexical conceptual structure </title>
<section> motivation, target, and related work.  </section>
<citcontext>
<prevsection>
<prevsent>can be paraphrased into possibility.?
</prevsent>
<prevsent>several attempts have been made to develop such resources manually (sato, 1999; dras, 1999; inui and nogami, 2001); <papid> W01-0814 </papid>those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct sufficiently comprehensive resource for practical applications.</prevsent>
</prevsection>
<citsent citstr=" W03-1609 ">
there is another trend in the research in this field,namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (barzilay and mckeown, 2001; <papid> P01-1008 </papid>lin and pantel, 2001; pang et al., 2003; <papid> N03-1024 </papid>shinyama and sekine, 2003, <papid> W03-1609 </papid>etc.).</citsent>
<aftsection>
<nextsent>this type of approach may be able to reduce the cost of resource development.
</nextsent>
<nextsent>there are problems that must be overcome, however, before they can work practically.
</nextsent>
<nextsent>first, automatically acquired patterns tend to be complex.
</nextsent>
<nextsent>for example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: is purchased by ? buys x.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1699">
<title id=" W04-0402.xml">paraphrasing of japanese light verb constructions based on lexical conceptual structure </title>
<section> lexical conceptual structure.  </section>
<citcontext>
<prevsection>
<prevsent>of nominal ized verb n, case particle c, and base form of verb from the parsed5 sentences of newspaper articles6.
</prevsent>
<prevsent>4a sahen-noun is verbal noun in japanese, which acts as verb in the form of sahen-noun + suru?.
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
5we used the statistical japanese dependency parser cabocha (kudo and matsumoto, 2002) <papid> W02-2016 </papid>for parsing.</citsent>
<aftsection>
<nextsent>http://chasen.naist.jp/taku/software/cabocha/ 6excerpts from 9 years of the mainichi shinbun and 10 years of the nihon keizai shinbun, giving total of 25,061,504 sentences, were used.
</nextsent>
<nextsent>table 2: extensions of lcs verb verb phrase and its lcs representation ext.1 hankou-suru [[ken]y be against [parents]z] (resist) ken-ga oya-ni hankou-suru.
</nextsent>
<nextsent>ken-nom parents-dat resist-pres (ken resists his parents.)
</nextsent>
<nextsent>ext.2 ukeru [become [[salesclerk]z be with [[complaint]y move from [customer]x to [salesclerk]z]]] (receive) tenin-ga kyaku-kara kujo-o ukeru.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1700">
<title id=" W04-2115.xml">multiword lexical acquisition and dictionary formalization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, however, there has been growing awareness in the nlp community of the problems that mwes pose and the need for their robust handling.
</prevsent>
<prevsent>several major conferences and satellite workshops have been dedicated to the subject (acl, eacl, lrec, for instance); major publications devote thematic issues to mwes.
</prevsent>
</prevsection>
<citsent citstr=" W99-0511 ">
anticipating that growing interest, over the last years, significant part of labels research has been devoted to the development of large-scale, linguistically precise language resources, namely to the construction of computational lexicons for simple and multiword units (eleutrio et al, 1995; ranchhod et al, 1999; <papid> W99-0511 </papid>ranchhod et al, 2004).</citsent>
<aftsection>
<nextsent>in fact, we have observed that mwes are used frequently in both everyday language and technical and scientific texts to express ideas and concepts that in general cannot be stated by free?
</nextsent>
<nextsent>linguistic structures.
</nextsent>
<nextsent>they include large range of different linguistic phenomena, such as: (i) lexical compounds (nouns: cellular phone, rush hour, new jersey; adjectives: well-known; adverbs: for the time being, in short; prepositions and conjunctions: 1 label (laboratrio de engenharia da linguagem) http://label.ist.utl.pt inspite of, in order to) (ii) phrasal verbs (give up); (iii) light verbs (give lecture); (iv) fixed (proverbs and maxims) and semi-fixed sentences (to see the light at the end of the tunnel; to take the lord?
</nextsent>
<nextsent>name in vain).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1701">
<title id=" W04-2115.xml">multiword lexical acquisition and dictionary formalization </title>
<section> characterization of multiword nouns.  </section>
<citcontext>
<prevsection>
<prevsent>regarding inflection, general rules presented by grammar ians do apply to some cases, but most compounds exhibit inflectional restrictions on gender or number that cannot be described by the morphological properties of their constituents.
</prevsent>
<prevsent>table 1 presents few examples of the most representative classes of compound nouns in portuguese.
</prevsent>
</prevsection>
<citsent citstr=" P01-1058 ">
2 cetempublico is journalistic corpus containing about 180 million words (see santos and rocha, 2001 <papid> P01-1058 </papid>for techical information).</citsent>
<aftsection>
<nextsent>                                                                                   !                             # $                                       table 1: some binary compound noun classes these classes represent binary compounds, comprised of two content words (where one of them is noun), eventually connected by grammatical word3.
</nextsent>
<nextsent>the classification criteria are based on the nouns internal structures, which are generally associated with characteristic inflectional pattern.
</nextsent>
<nextsent>for instance, compound nouns belonging to the na class usually allow the inflection in gender and/or number of both constituents (e.g. bomba atmica, bombas atmicas); on the contrary, in the majority of ndn compound nouns, only the first noun can inflect (conselho de guerra, conselhos de guerra).
</nextsent>
<nextsent>in the following sections, further relevant information on inflection, formalization and generation of inflected forms will be given.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1702">
<title id=" W04-0912.xml">under specification of meaning the case of russian imperfect ive aspect </title>
<section> ipf aspect in russian.  </section>
<citcontext>
<prevsection>
<prevsent>the relation thus obtained constitutes the aspect ual value.
</prevsent>
<prevsent>the decisive units for this selection are phases and boundaries (bickel, 1996).
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
presuming tripartite event structure (moens and steedman, 1988) <papid> J88-2003 </papid>consisting of preparation phase (dynamic phase dyn), culmination point (boundary ?) and consequent state (static phase stat), there are three possibilities for aspect to select.</citsent>
<aftsection>
<nextsent>english and turkish both have dyn-selecting aspect ual markers, turkish also marker for explicit stat selection; russian pf aspect explicitly selects ?.
</nextsent>
<nextsent>the unmarked members of the aspect ual oppositions may assert anything else ? russian ipf aspect may assert anything but the explicit selection of boundary.
</nextsent>
<nextsent>as truth-evaluation and assertion have to be kept apart, the selected and asserted parts have to be related an interval of time, the validation interval, where they are asserted to hold and truth conditionally evaluated.
</nextsent>
<nextsent>padueva (1996) refers to this interval with the notion of toka otseta (henceforth to), an aspect ual reference interval, which can be retrospective or synchronous (bounded or unbouded) with respect to the asserted part.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1703">
<title id=" W04-1219.xml">exploring deep knowledge resources in biomedical name recognition </title>
<section> the baseline system.  </section>
<citcontext>
<prevsection>
<prevsent>various widely used lexical-level features are explored in the baseline system.
</prevsent>
<prevsent>word formation pattern (fwfp): the purpose of this feature is to capture capitalization, digital ization and other word formation information.
</prevsent>
</prevsection>
<citsent citstr=" W03-1307 ">
in this paper, the same feature as in shen et al 2003 <papid> W03-1307 </papid>is used.</citsent>
<aftsection>
<nextsent>morphological pattern (fmp): morphological information, such as prefix and suffix, is considered as an important cue for terminology identification.
</nextsent>
<nextsent>same as shen et al 2003, <papid> W03-1307 </papid>we use statistical method to get the most useful prefixes/suffixes from the training data.</nextsent>
<nextsent>part-of-speech (fpos): since many of the words in biomedical entity names are in lowercase, capitalization information in the biomedical domain is not as evidential as that in the newswire domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1706">
<title id=" W04-0902.xml">solving logic puzzles from robust processing to precise semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper presents intial work on system that bridges from robust, broad-coverage natural language processing to precise semantics and automated reasoning, focusing on solving logic puzzles drawn from sources such as the law school admission test (lsat) and the analytic section of the graduate record exam (gre).
</prevsent>
<prevsent>we highlight key challenges, and discuss the representations and performance of the prototype system.
</prevsent>
</prevsection>
<citsent citstr=" J82-3002 ">
traditional approaches to natural language understanding (woods, 1973; warren and pereira, 1982; <papid> J82-3002 </papid>alshawi, 1992) provided good account of mapping from surface forms to semantic representations, when confined to very limited vocabulary, syntax, and world model, and resulting low levels of syntactic/semantic ambi guity.</citsent>
<aftsection>
<nextsent>it is, however, difficult to scale these methods to unrestricted, general-domain natural language input because of the overwhelming problems of grammar coverage, unknown words,unresolvable ambiguities, and incomplete do main knowledge.
</nextsent>
<nextsent>recent work in nlp has consequently focused on more robust, broad coverage techniques, but with the effect of overall shallower levels of processing.
</nextsent>
<nextsent>thus, state-of-the-art work on probabilistic parsing (e.g., (collins, 1999)) provides good solution to robust, broad coverage parsing with automatic and frequently successful ambiguity resolution, but has largely ignored issues of semantic interpretation.
</nextsent>
<nextsent>the field of question answering (pasca and harabagiu, 2001; moldovan et al,2003) <papid> N03-1022 </papid>focuses on simple-fact queries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1707">
<title id=" W04-0902.xml">solving logic puzzles from robust processing to precise semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent work in nlp has consequently focused on more robust, broad coverage techniques, but with the effect of overall shallower levels of processing.
</prevsent>
<prevsent>thus, state-of-the-art work on probabilistic parsing (e.g., (collins, 1999)) provides good solution to robust, broad coverage parsing with automatic and frequently successful ambiguity resolution, but has largely ignored issues of semantic interpretation.
</prevsent>
</prevsection>
<citsent citstr=" N03-1022 ">
the field of question answering (pasca and harabagiu, 2001; moldovan et al,2003) <papid> N03-1022 </papid>focuses on simple-fact queries.</citsent>
<aftsection>
<nextsent>and so called semantic parsing (gildea and jurafsky,2002) <papid> J02-3001 </papid>provides as end output only flat classification of semantic arguments of predicates, ignoring much of the semantic content, such as quantifiers.a major research question that remains unanswered is whether there are methods for getting from robust parse-anything?</nextsent>
<nextsent>statistical parser to semantic representation precise enough for knowledge representation and automated reasoning, without falling afoul of thesame problems that stymied the broad application of traditional approaches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1708">
<title id=" W04-0902.xml">solving logic puzzles from robust processing to precise semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, state-of-the-art work on probabilistic parsing (e.g., (collins, 1999)) provides good solution to robust, broad coverage parsing with automatic and frequently successful ambiguity resolution, but has largely ignored issues of semantic interpretation.
</prevsent>
<prevsent>the field of question answering (pasca and harabagiu, 2001; moldovan et al,2003) <papid> N03-1022 </papid>focuses on simple-fact queries.</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
and so called semantic parsing (gildea and jurafsky,2002) <papid> J02-3001 </papid>provides as end output only flat classification of semantic arguments of predicates, ignoring much of the semantic content, such as quantifiers.a major research question that remains unanswered is whether there are methods for getting from robust parse-anything?</citsent>
<aftsection>
<nextsent>statistical parser to semantic representation precise enough for knowledge representation and automated reasoning, without falling afoul of thesame problems that stymied the broad application of traditional approaches.
</nextsent>
<nextsent>this paper presents initial work on system that addresses this question.
</nextsent>
<nextsent>the chosen task is solving logic puzzles of the sort found in the law school admission test (lsat) and the old analytic section of the graduate record exam (gre) (seefigure 1 for typical example).
</nextsent>
<nextsent>the system integrates statistical parsing, on-the-fly?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1709">
<title id=" W04-0902.xml">solving logic puzzles from robust processing to precise semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, the task has clear evaluation metric because the puzzle texts are designed to yield exactly one correct answer to each multiple choice question.
</prevsent>
<prevsent>moreover, the domain is an other example of found test material?
</prevsent>
</prevsection>
<citsent citstr=" P99-1042 ">
in the sense of (hirschman et al, 1999): <papid> P99-1042 </papid>puzzle texts were developed with goal independent of the evaluation of natural language processing systems, and so provide more realistic evaluation framework than specially-designed tests such as trec qa.</citsent>
<aftsection>
<nextsent>while our current system is not real world application, we believe that the methods being developed could be used in applications such asa computerized office assistant that must understand requests such as: put each file containing task description in different directory.?
</nextsent>
<nextsent>this section explains the languages we use to represent the content of puzzle.
</nextsent>
<nextsent>computing the representations from text is complex process with several stages, as shown in figure 2.most of the stages are independent of the puzzles domain.
</nextsent>
<nextsent>section 3 reviews the main challenges in this process, and later sections outline the various processing stages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1710">
<title id=" W04-0902.xml">solving logic puzzles from robust processing to precise semantics </title>
<section> morpho-syntactic analysis.  </section>
<citcontext>
<prevsection>
<prevsent>while traditional hand-built grammars often include rich semantics, we have found their coverage inadequate for the logic puzzles task.
</prevsent>
<prevsent>for example, the english resource grammar (copestake and flickinger, 2000) fails to parse any of the sentences in figure 1 for lack of coverage of some words and of several different syntactic structures; and parsable simplified versions of the text produce dozens of unranked parse trees.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
for this reason, we use broad coverage statistical parser (klein and manning,2003) <papid> P03-1054 </papid>trained on the penn treebank.</citsent>
<aftsection>
<nextsent>in addition to robustness, treebank-trained statistical parsers have the benefit of extensive research on accurate ambiguity resolution.
</nextsent>
<nextsent>qualitatively, we have found that the output of the parser onlogic puzzles is quite good (see 10).
</nextsent>
<nextsent>after parsing, each word in the resulting parse trees is converted to base form by stemmer.
</nextsent>
<nextsent>a few tree-transformation rules are applied on the parse trees to make them more convenient for combinatorial semantics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1711">
<title id=" W04-0902.xml">solving logic puzzles from robust processing to precise semantics </title>
<section> morpho-syntactic analysis.  </section>
<citcontext>
<prevsection>
<prevsent>most of them are general, e.g. imposing binary branching structure on verb phrases, and grouping expressions like more than?.
</prevsent>
<prevsent>a few of them correct some parsing errors, such as nouns marked asnames and vice-versa.
</prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
there is growing awareness in the probabilistic parsing literature that mismatches between training and test set genre can degrade parse accuracy, and that small amounts of correct-genre data can be more important than large amounts of wrong-genre data(gildea, 2001); <papid> W01-0521 </papid>we have found corroborating evidence in misparsings of noun phrases common in puzzle texts, such as sculptures and e?, which do not appear in the wall street journalcorpus.</citsent>
<aftsection>
<nextsent>depending on the severity of this problem, we may hand-annotate small amount of puzzle texts to include in parser training data.
</nextsent>
<nextsent>work in nlp has shifted from hand-built grammars that need to cover explicitly every sentence structure and that break down on unexpected inputs to more robust statistical parsing.however, grammars that involve precise semantics are still largely hand-built (e.g.
</nextsent>
<nextsent>(carpenter, 1998; copestake and flickinger, 2000)).
</nextsent>
<nextsent>we aimat extending the robustness trend to the semantics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1712">
<title id=" W04-1802.xml">meta linguistic information extraction for terminology </title>
<section> filtering out non-metalinguistic sentences:.  </section>
<citcontext>
<prevsection>
<prevsent>the different number of positions to the left and right of our training sentences, as well as the nature of the features selected (there are many more word-types than pos tags) ensured that our 3-part vector introduced wide range of features against our 2 possible labels.
</prevsent>
<prevsent>the best results of each algorithms restricted to the lexeme call, are presented in the next table.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
figures 1 and 2 present best results in the learning experiments for the complete set of patterns used in the collocation approach, over two of our evaluation corpora.11 type positions tags/words features accuracy precision recall gis 1 1254 0.97 0.96 0.98 iis 1 136 0.95 0.96 0.94 nb 1 136 0.88 0.97 0.84 9 see rish, 2001, ratnaparkhi, 1997 and berger et al 1996 <papid> J96-1002 </papid>for formal description of these algorithms.</citsent>
<aftsection>
<nextsent>10 in other words, given known data statistics,.
</nextsent>
<nextsent>construct model that best represents them but is otherwise as uniform as possible.
</nextsent>
<nextsent>11 legend: p: precision; r: recall; f: f-measure.
</nextsent>
<nextsent>nb: nave.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1713">
<title id=" W04-1802.xml">meta linguistic information extraction for terminology </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>lexicographers and terminologist can use them as tools for their own labour-intensive work of reviewing and compiling special-domain vocabularies.
</prevsent>
<prevsent>mids could, in principle, also supply new interpretation rules in ai applications when inferences wont succeed because the state of the lexico-conceptual system has changed.15 neo logism or word in an unexpected technical sense could stump nlp system that assumes it will be able to use the default information from machine-readable dictionary or tkb.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
the kind of sortal information implicit in many definitions can also help improve anaphora resolution, semantic typing, acronym identification or bootstrapping of ontologies and taxonomies (hearst, 1992; <papid> C92-2082 </papid>cond amines &amp; rebeyrolle, 2001; pustejovsky et al., 2002; malais?</citsent>
<aftsection>
<nextsent>et al 2004).
</nextsent>
<nextsent>although our approach might miss some of the important conceptual relations between terms, many of the mids we have obtained using language-centred contexts are rich sources of information.
</nextsent>
<nextsent>in addition, terminological information can be more specific than that obtainable by glossary lookup, and might be better suited for the interpretation of certain texts.
</nextsent>
<nextsent>the locality of such information can be seen as advantageous for specialized lexicography.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1714">
<title id=" W04-2206.xml">a method of creating new bilingual valency entries using alternations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we present method of adding new entries to an existing bilingual valency dictionary, using information about verbal alternations.the classic approach to acquiring lexical information is to build resources by hand.
</prevsent>
<prevsent>this produces useful resources but is expensive.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
this is still the approach taken by large projects such as framenet (baker et al, 1998) <papid> P98-1013 </papid>or ontosem.therefore, there is need to extend these handmade resources quickly and economically.</citsent>
<aftsection>
<nextsent>another approach is to attempt to learn information from corpora.
</nextsent>
<nextsent>there has been much research based on this, but due to the inevitable errors,there are few examples of lexicons being constructed fully automatically.
</nextsent>
<nextsent>korhonen (2002) <papid> W02-0907 </papid>reports that the ceiling on the performance of mono-lingual subcategorization acquisition from corpora is generally around 80%, level that still requires manual intervention.</nextsent>
<nextsent>yet another approach is to combine knowledge sources: for example to build lexicon and then try to extend it using corpus data or to enrich monolingual data using multilingual lexicons (fujita and bond, 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1715">
<title id=" W04-2206.xml">a method of creating new bilingual valency entries using alternations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another approach is to attempt to learn information from corpora.
</prevsent>
<prevsent>there has been much research based on this, but due to the inevitable errors,there are few examples of lexicons being constructed fully automatically.
</prevsent>
</prevsection>
<citsent citstr=" W02-0907 ">
korhonen (2002) <papid> W02-0907 </papid>reports that the ceiling on the performance of mono-lingual subcategorization acquisition from corpora is generally around 80%, level that still requires manual intervention.</citsent>
<aftsection>
<nextsent>yet another approach is to combine knowledge sources: for example to build lexicon and then try to extend it using corpus data or to enrich monolingual data using multilingual lexicons (fujita and bond, 2002).
</nextsent>
<nextsent>the aim of this research is not to create lexicon from scratch, but rather to add further entries to an existing lexicon.
</nextsent>
<nextsent>we propose method of acquiring detailed information about predicates, including argument structure, semantic restrictions on the arguments and translation equivalents.
</nextsent>
<nextsent>it combines two heterogeneous knowledge sources: an existing bilingual valency lexicon (the seed lexicon), and information about verbal alternations.most verbs have more than one possible argument structure (subcat).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1716">
<title id=" W04-2206.xml">a method of creating new bilingual valency entries using alternations </title>
<section> the nature of the s=o alternation.  </section>
<citcontext>
<prevsection>
<prevsent>baldwin et al (1999) hypothesized that selectional restrictions (srs) stay constantin the different syntactic positions.
</prevsent>
<prevsent>dorr (1997), who generates both alternations from single underlying representation, implicitly makes this assumption.
</prevsent>
</prevsection>
<citsent citstr=" E93-1026 ">
in addition, kilgarriff (1993) <papid> E93-1026 </papid>specifically makes the ?+sentient, +volition?, while the is ?+changes-state, +causally affected?.</citsent>
<aftsection>
<nextsent>however, we know of no quantitative studies of the similarities of alternating verbs.
</nextsent>
<nextsent>exploiting the machine translation lexicon for linguistic research, we compare the srs of with both and for verbs that take the s=o alternation.the srs take the form of list of semantic classes, strings or *.
</nextsent>
<nextsent>strings only match specific words, while * matches anything, even non-nouns.
</nextsent>
<nextsent>the semantic classes are from the goitaikei ontology of 2,710 categories (ikehara et al, 1997).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1718">
<title id=" W04-2206.xml">a method of creating new bilingual valency entries using alternations </title>
<section> method of creating valency entries.  </section>
<citcontext>
<prevsection>
<prevsent>for each dependent ni if ni participates in the alternation if ni has an alternate in the target then map to it else delete ni else transfer [non-alternating dependent] ? if the alternation requires dependent not in the source add the default argument we use the most frequent argument in existing valency entries as default.
</prevsent>
<prevsent>specific examples of creating = alternations are given in the next section.although we only discuss the selectional restrictions and subcat information here, we alsomap the verb classes (given as verbal semantic attributes (nakaiwa and ikehara, 1997)).
</prevsent>
</prevsection>
<citsent citstr=" A00-2034 ">
the mapping for the dependents in the alternation can be taken from existing lexical resources (dorr, 1997), learned from corpora (mccarthy, 2000) <papid> A00-2034 </papid>or learned from existing lexicons (bond et al, 2002).</citsent>
<aftsection>
<nextsent>4.1 target.
</nextsent>
<nextsent>in this experiment, we look at one family of alternations, the = alternation.
</nextsent>
<nextsent>the candidate words are thus in transitive verbs with no transitive alternate, or transitive entries with no in transitive alternate.
</nextsent>
<nextsent>alternations should be between senses, but the alternation list is only of words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1720">
<title id=" W04-1906.xml">corpus based induction of an lfg syntax semantics interface for frame semantic processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is growing insight that high-quality nlp applications for information access are in need of deeper, in particular, semantic analysis.
</prevsent>
<prevsent>a bottle neck for semantic processing is the lack of large domain-independent lexical semantic resources.there are now efforts for the creation of large lexical semantic resources that provide information on predicate-argument structure.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
framenet (baker et al., 1998), <papid> P98-1013 </papid>building on fill mores theory of frame semantics, provides definitions of frames and their semantic roles, lexical database and manually annotated corpus of example sentences.</citsent>
<aftsection>
<nextsent>a strictlycorpus-based approach is carried out with prop bank?
</nextsent>
<nextsent>a manual predicate-argument annotation on top of the penn treebank (kingsbury et al, 2002).
</nextsent>
<nextsent>first approaches for learning stochastic models for semantic role assignment from annotated corpora have emerged with gildea and jurafsky (2002)<papid> J02-3001 </papid>and fleischman et al (2003).<papid> W03-1007 </papid></nextsent>
<nextsent>while current competitions explore the potential of shallow parsing1the research reported here was conducted in cooperation project of the german research center for artificial intelligence, dfki saarbrucken with the computational linguistics department of the university of the saarland at saarbrucken.for role labelling, gildea and palmer (2002) <papid> P02-1031 </papid>emphasise the role of deeper syntactic analysis for semantic role labelling.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1721">
<title id=" W04-1906.xml">corpus based induction of an lfg syntax semantics interface for frame semantic processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a strictlycorpus-based approach is carried out with prop bank?
</prevsent>
<prevsent>a manual predicate-argument annotation on top of the penn treebank (kingsbury et al, 2002).
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
first approaches for learning stochastic models for semantic role assignment from annotated corpora have emerged with gildea and jurafsky (2002)<papid> J02-3001 </papid>and fleischman et al (2003).<papid> W03-1007 </papid></citsent>
<aftsection>
<nextsent>while current competitions explore the potential of shallow parsing1the research reported here was conducted in cooperation project of the german research center for artificial intelligence, dfki saarbrucken with the computational linguistics department of the university of the saarland at saarbrucken.for role labelling, gildea and palmer (2002) <papid> P02-1031 </papid>emphasise the role of deeper syntactic analysis for semantic role labelling.</nextsent>
<nextsent>we follow this line and explore the potential of deep syntactic analysis for role labelling, choosing lexical functional grammar as underlying syntactic framework.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1722">
<title id=" W04-1906.xml">corpus based induction of an lfg syntax semantics interface for frame semantic processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a strictlycorpus-based approach is carried out with prop bank?
</prevsent>
<prevsent>a manual predicate-argument annotation on top of the penn treebank (kingsbury et al, 2002).
</prevsent>
</prevsection>
<citsent citstr=" W03-1007 ">
first approaches for learning stochastic models for semantic role assignment from annotated corpora have emerged with gildea and jurafsky (2002)<papid> J02-3001 </papid>and fleischman et al (2003).<papid> W03-1007 </papid></citsent>
<aftsection>
<nextsent>while current competitions explore the potential of shallow parsing1the research reported here was conducted in cooperation project of the german research center for artificial intelligence, dfki saarbrucken with the computational linguistics department of the university of the saarland at saarbrucken.for role labelling, gildea and palmer (2002) <papid> P02-1031 </papid>emphasise the role of deeper syntactic analysis for semantic role labelling.</nextsent>
<nextsent>we follow this line and explore the potential of deep syntactic analysis for role labelling, choosing lexical functional grammar as underlying syntactic framework.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1723">
<title id=" W04-1906.xml">corpus based induction of an lfg syntax semantics interface for frame semantic processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a manual predicate-argument annotation on top of the penn treebank (kingsbury et al, 2002).
</prevsent>
<prevsent>first approaches for learning stochastic models for semantic role assignment from annotated corpora have emerged with gildea and jurafsky (2002)<papid> J02-3001 </papid>and fleischman et al (2003).<papid> W03-1007 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
while current competitions explore the potential of shallow parsing1the research reported here was conducted in cooperation project of the german research center for artificial intelligence, dfki saarbrucken with the computational linguistics department of the university of the saarland at saarbrucken.for role labelling, gildea and palmer (2002) <papid> P02-1031 </papid>emphasise the role of deeper syntactic analysis for semantic role labelling.</citsent>
<aftsection>
<nextsent>we follow this line and explore the potential of deep syntactic analysis for role labelling, choosing lexical functional grammar as underlying syntactic framework.
</nextsent>
<nextsent>we aim at computational interface for frame semantics processing that can be used to (semi-)automatically extend thesize of current training corpora for learning stochastic models for role labelling, and ? ultimately ? as basis for automatic frame assignment in nlp tasks, based on the acquired stochastic models.we discuss advantages of semantic role assignment on the basis of functional syntactic analyses as provided by lfg parsing, and present anlfg syntax-semantics interface for frame semantics, building on first study in frank and erk(2004).
</nextsent>
<nextsent>in the present paper we focus on the corpus based induction of computational lfg interface for frame semantics from semantically annotated corpus.
</nextsent>
<nextsent>we describe the methods used to derive an lfg-based frame semantic lexicon, and discuss the treatment of special (since non-isomorphic) mappings in the syntax-semantics interface.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1724">
<title id=" W04-1906.xml">corpus based induction of an lfg syntax semantics interface for frame semantic processing </title>
<section> corpus and grammar resources.  </section>
<citcontext>
<prevsection>
<prevsent>in section 5 we apply and evaluate the frame projection rules inan lfg parsing architecture.
</prevsent>
<prevsent>in section 6 we summarise our results and discuss future directions.
</prevsent>
</prevsection>
<citsent citstr=" P03-1068 ">
frame semantic corpus annotations the basis for our work is corpus of manual frame annotations, the salsa/tiger corpus (erk et al, 2003).<papid> P03-1068 </papid>2 the annotation follows the framenet definitions of frames and their semantic roles.3 underlying this corpus is syntactically annotated corpus of german newspaper text, the tiger treebank (brants et al, 2002).</citsent>
<aftsection>
<nextsent>tiger syntactic annotations consist of relatively flat constituent graph representations,with edge labels that indicate functional information, such as head (hd), subject (sb), cf.
</nextsent>
<nextsent>figure 1.
</nextsent>
<nextsent>the salsa frame annotations are flat graphs connected to syntactic constituents.
</nextsent>
<nextsent>figure 1 displays frame annotations where the request frameis triggered by the (discontinuous) frame evoking element (fee) fordert ... auf (requests).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1726">
<title id=" W04-1906.xml">corpus based induction of an lfg syntax semantics interface for frame semantic processing </title>
<section> corpus and grammar resources.  </section>
<citcontext>
<prevsection>
<prevsent>this german lfg grammar has already been used for semi-automatic syntactic annotation of the tiger corpus, with reported coverage of 50%, and 70% 2http://www.coli.uni-sb.de/lexicon 3see http://www.icsi.berkeley.edu/framenet precision (brants et al, 2002).
</prevsent>
<prevsent>the grammar runs on the xle grammar processing platform, which provides stochastic training and online disambiguation packages.
</prevsent>
</prevsection>
<citsent citstr=" P02-1035 ">
currently, the grammar is further extended, and will be enhanced with stochastic disambiguation, along the lines of (riezler et al, 2002).<papid> P02-1035 </papid></citsent>
<aftsection>
<nextsent>lfg corpus resource next to the german lfg grammar, (forst, 2003) has derived parallel?
</nextsent>
<nextsent>lfgf-structure corpus from the tiger treebank, by applying methods for treebank conversion.
</nextsent>
<nextsent>we makeuse of the parallel treebank to induce lfg frame annotation rules from the salsa/tiger annotations.
</nextsent>
<nextsent>lexical functional grammar (bresnan, 2001) assumes multiple levels of representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1730">
<title id=" W04-1114.xml">the construction of a chinese shallow treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is language resource containing annotations of information at various linguistic levels such as words, phrases, clauses and sentences to form bank of linguistic trees?.
</prevsent>
<prevsent>there are many treebanks built for different languages such as the penn treebank (marcus 1993), ice-gb (wallis 2003), and so on.
</prevsent>
</prevsection>
<citsent citstr=" C02-1145 ">
the penn chinese treebank is an important resource (xia et al 2000; xue et al 2002).<papid> C02-1145 </papid></citsent>
<aftsection>
<nextsent>its annotation is based on head-driven phrase structure grammar (hpsg).
</nextsent>
<nextsent>the corpus of 100,000 chinese words has been manually annotated with strict quality assurance process.
</nextsent>
<nextsent>another important work is the sinica treebank at the academic sinica, taiwan ( chen et al. 1999; chen et al 2003).
</nextsent>
<nextsent>information-based case grammar (icg) was selected as the language framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1731">
<title id=" W04-1114.xml">the construction of a chinese shallow treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from linguistics viewpoint, collocation occurs only in words within phrase, or between the headwords of related phrases (zhang and lin 1992).
</prevsent>
<prevsent>therefore, the use of syntactic information is naturally considered an effective way to improve the performance of collocation extraction systems.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
the typical problems like doctor-nurse (church and hanks 1990) <papid> J90-1003 </papid>could be avoided by using such information.</citsent>
<aftsection>
<nextsent>when employing syntactical information in collocation extraction, we restrict ourselves to identify the stable phrases in the sentences with certain levels of nesting.
</nextsent>
<nextsent>thus it has motivated us to produce shallow treebank.
</nextsent>
<nextsent>a natural way to obtain shallow treebank is through extracting shallow structures from fully parsed treebank.
</nextsent>
<nextsent>unfortunately, all the available fully parsed treebank, such as the penn treebank and the sinica treebank, are annotated using different grammars than our chosen phrase-based grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1732">
<title id=" W04-0814.xml">the university of amsterdam at senseval3 semantic roles and logic forms </title>
<section> syntactic processing.  </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation is not part of the task, so the predicates need not specify wordnet senses.
</prevsent>
<prevsent>system evaluation is based on precision and recall of predicates and predicates together with all their arguments as compared to gold standard.
</prevsent>
</prevsection>
<citsent citstr=" P04-1040 ">
for both tasks, sr and lf, the core of our systems was the syntactic analysis module described in detail in (jijkoun and de rijke, 2004).<papid> P04-1040 </papid></citsent>
<aftsection>
<nextsent>we only have space here to give short overview of the module.
</nextsent>
<nextsent>every sentence was part-of-speech tagged using maximum entropy tagger (ratnaparkhi, 1996) <papid> W96-0213 </papid>and parsed using state-of-the-art wide coverage phrase structure parser (collins, 1999).</nextsent>
<nextsent>both the tagger and the parser are trained on the penn treebank wall street journal corpus (wsj in the rest of this paper) and thus produce structures similar to those in the penn treebank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1734">
<title id=" W04-0814.xml">the university of amsterdam at senseval3 semantic roles and logic forms </title>
<section> syntactic processing.  </section>
<citcontext>
<prevsection>
<prevsent>for both tasks, sr and lf, the core of our systems was the syntactic analysis module described in detail in (jijkoun and de rijke, 2004).<papid> P04-1040 </papid></prevsent>
<prevsent>we only have space here to give short overview of the module.</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
every sentence was part-of-speech tagged using maximum entropy tagger (ratnaparkhi, 1996) <papid> W96-0213 </papid>and parsed using state-of-the-art wide coverage phrase structure parser (collins, 1999).</citsent>
<aftsection>
<nextsent>both the tagger and the parser are trained on the penn treebank wall street journal corpus (wsj in the rest of this paper) and thus produce structures similar to those in the penn treebank.
</nextsent>
<nextsent>unfortunately, the parser does not deliver some of the information available in wsj that is potentially useful for our two applications: penn functional tags (e.g., subject, temporal, closely related, logical subject in passive) and non-local dependencies (e.g., subject and object control, argument extraction in relative clauses).
</nextsent>
<nextsent>our syntactic module tries to compensate for this and make this information explicit in the resulting syntactic analy ses.as first step, we converted phrase trees produced by the parser to dependency structures, by detecting heads of constituents and then propagating the lexical head information up the syntactic tree, similarly to (collins, 1999).
</nextsent>
<nextsent>the resulting dependency structures were labeled with dependency labels derived from corresponding penn phrase labels: e.g., verb phrase (vp) modified by prepositional phrase (pp) resulted in dependency with label vp|pp?.then, the information available in the wsj (func association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems vp to seek np seats vp planned directors this month np np planned directors vp|s s|np s|np month this np|dt to seek seats vp|npvp|to planned directors vp|ss|npsbj s|nptmp s|npsbj month this np|dt to seek seats vp|npvp|to (a) (b) (c)figure 1: stages of the syntactic processing: (a) the parsers output, (b) the result of conversion to dependency structure, (c) final output of our syntactic module tional tags, non-local dependencies) was added to dependency structures using memory-based learning (daelemans et al, 2003): we trained the learner to change dependency labels, or add new nodes or arcs to dependency structures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1738">
<title id=" W04-0814.xml">the university of amsterdam at senseval3 semantic roles and logic forms </title>
<section> automatic labeling of semantic roles.  </section>
<citcontext>
<prevsection>
<prevsent>parser, (b) the phrase structure tree converted to ade pendency structure and (c) the transformed dependency structure with added functional tags and anon-local dependency the final output of our syntactic module.
</prevsent>
<prevsent>dependencies are shown as arcs from heads to dependents.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
for the sr task, we applied method very similar tothe one used in (jijkoun and de rijke, 2004) <papid> P04-1040 </papid>for recovering syntactic structures and somewhat similar to the first method for automatic semantic role identification described in (gildea and jurafsky, 2002).<papid> J02-3001 </papid>essentially, our method consists of extracting possible syntactic patterns (paths in syntactic dependency structures), introducing semantic relations from at raining corpus, and then using machine learning classifier to predict which syntactic paths correspond to which frame elements.</citsent>
<aftsection>
<nextsent>our main assumption was that frame elements, as annotated in framenet, correspond directly to constituents (constituents being complete subtrees of dependency structures).
</nextsent>
<nextsent>similarly to (gildea and jurafsky, 2002), <papid> J02-3001 </papid>our own evaluation showed that about 15% of frame elements in framenet 1.1 do not correspond to constituents, even when applying some straighforward heuristics (see below) to compensate for this mismatch.</nextsent>
<nextsent>this observation puts an upper bound of around 85% on the accuracy of our system (with strict evaluation, i.e., if frame element boundaries must match the gold standard exactly).note, though, that these 15% of erroneous?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1741">
<title id=" W04-1513.xml">synchronous dependency insertion grammars a grammar formalism for syntax based statistical mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the early use of dependency structures in machine translation tasks mainly fall into the category of transfer based mt, where the dependency structure of the source language is first analyzed, then transferred to the target language by using set of transduction rules or transfer lexicon, and finally the linear form of the target language sentence is generated.
</prevsent>
<prevsent>while the above approach seems to be plausible, the transfer process demands intense human effort in creating working transduction rule set or transfer lexicon, which largely limits the performance and application domain of the resultant machine translation system.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
in the early 1990s, (brown et. al. 1993) <papid> J93-2003 </papid>introduced the idea of statistical machine translation, where the word to word translation probabilities and sentence reordering probabilities are estimated from large set of parallel sentence pairs.</citsent>
<aftsection>
<nextsent>by having the advantage of leveraging large parallel corpora, the statistical mt approach outperforms the traditional transfer based approaches in tasks for which adequate parallel corpora is available (och, 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>however, major criticism of this approach is that it is void of any internal representation for syntax or semantics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1742">
<title id=" W04-1513.xml">synchronous dependency insertion grammars a grammar formalism for syntax based statistical mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while the above approach seems to be plausible, the transfer process demands intense human effort in creating working transduction rule set or transfer lexicon, which largely limits the performance and application domain of the resultant machine translation system.
</prevsent>
<prevsent>in the early 1990s, (brown et. al. 1993) <papid> J93-2003 </papid>introduced the idea of statistical machine translation, where the word to word translation probabilities and sentence reordering probabilities are estimated from large set of parallel sentence pairs.</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
by having the advantage of leveraging large parallel corpora, the statistical mt approach outperforms the traditional transfer based approaches in tasks for which adequate parallel corpora is available (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>however, major criticism of this approach is that it is void of any internal representation for syntax or semantics.
</nextsent>
<nextsent>in recent years, hybrid approaches, which aim at applying statistical learning to structured data, began to emerge.
</nextsent>
<nextsent>syntax based statistical mt approaches began with (wu 1997), <papid> J97-3002 </papid>who introduced polyno mial-time solution for the alignment problem based on synchronous binary trees.</nextsent>
<nextsent>(alshawi et al, 2000) <papid> J00-1004 </papid>extended the tree-based approach by representing each production in parallel dependency trees as finite-state transducer.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1743">
<title id=" W04-1513.xml">synchronous dependency insertion grammars a grammar formalism for syntax based statistical mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, major criticism of this approach is that it is void of any internal representation for syntax or semantics.
</prevsent>
<prevsent>in recent years, hybrid approaches, which aim at applying statistical learning to structured data, began to emerge.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
syntax based statistical mt approaches began with (wu 1997), <papid> J97-3002 </papid>who introduced polyno mial-time solution for the alignment problem based on synchronous binary trees.</citsent>
<aftsection>
<nextsent>(alshawi et al, 2000) <papid> J00-1004 </papid>extended the tree-based approach by representing each production in parallel dependency trees as finite-state transducer.</nextsent>
<nextsent>(yamada and knight, 2001, <papid> P01-1067 </papid>2002) model translation as sequence of operations transforming syntactic tree in one language into the string of the second language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1744">
<title id=" W04-1513.xml">synchronous dependency insertion grammars a grammar formalism for syntax based statistical mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, hybrid approaches, which aim at applying statistical learning to structured data, began to emerge.
</prevsent>
<prevsent>syntax based statistical mt approaches began with (wu 1997), <papid> J97-3002 </papid>who introduced polyno mial-time solution for the alignment problem based on synchronous binary trees.</prevsent>
</prevsection>
<citsent citstr=" J00-1004 ">
(alshawi et al, 2000) <papid> J00-1004 </papid>extended the tree-based approach by representing each production in parallel dependency trees as finite-state transducer.</citsent>
<aftsection>
<nextsent>(yamada and knight, 2001, <papid> P01-1067 </papid>2002) model translation as sequence of operations transforming syntactic tree in one language into the string of the second language.</nextsent>
<nextsent>the syntax based statistical approaches have been faced with the major problem of pervasive structural divergence between languages, due to both systematic differences between languages (dorr, 1994) <papid> J94-4004 </papid>and the vagaries of loose translations in real corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1745">
<title id=" W04-1513.xml">synchronous dependency insertion grammars a grammar formalism for syntax based statistical mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>syntax based statistical mt approaches began with (wu 1997), <papid> J97-3002 </papid>who introduced polyno mial-time solution for the alignment problem based on synchronous binary trees.</prevsent>
<prevsent>(alshawi et al, 2000) <papid> J00-1004 </papid>extended the tree-based approach by representing each production in parallel dependency trees as finite-state transducer.</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
(yamada and knight, 2001, <papid> P01-1067 </papid>2002) model translation as sequence of operations transforming syntactic tree in one language into the string of the second language.</citsent>
<aftsection>
<nextsent>the syntax based statistical approaches have been faced with the major problem of pervasive structural divergence between languages, due to both systematic differences between languages (dorr, 1994) <papid> J94-4004 </papid>and the vagaries of loose translations in real corpora.</nextsent>
<nextsent>while we would like to use syntactic information in both languages, the problem of non isomorphism grows when trees in both languages are required to match.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1746">
<title id=" W04-1513.xml">synchronous dependency insertion grammars a grammar formalism for syntax based statistical mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(alshawi et al, 2000) <papid> J00-1004 </papid>extended the tree-based approach by representing each production in parallel dependency trees as finite-state transducer.</prevsent>
<prevsent>(yamada and knight, 2001, <papid> P01-1067 </papid>2002) model translation as sequence of operations transforming syntactic tree in one language into the string of the second language.</prevsent>
</prevsection>
<citsent citstr=" J94-4004 ">
the syntax based statistical approaches have been faced with the major problem of pervasive structural divergence between languages, due to both systematic differences between languages (dorr, 1994) <papid> J94-4004 </papid>and the vagaries of loose translations in real corpora.</citsent>
<aftsection>
<nextsent>while we would like to use syntactic information in both languages, the problem of non isomorphism grows when trees in both languages are required to match.
</nextsent>
<nextsent>to allow the syntax based machine translation approaches to work as generative process, certain isomorphism assumptions have to be made.
</nextsent>
<nextsent>hence reasonable question to ask is: to what extent should the grammar formalism, which we choose to represent syntactic language transfer, assume isomor phism between the structures of the two languages?
</nextsent>
<nextsent>(hajic et al, 2002) allows for limited non isomorphism in that n-to-m matching of nodes in the two trees is permitted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1747">
<title id=" W04-1513.xml">synchronous dependency insertion grammars a grammar formalism for syntax based statistical mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence reasonable question to ask is: to what extent should the grammar formalism, which we choose to represent syntactic language transfer, assume isomor phism between the structures of the two languages?
</prevsent>
<prevsent>(hajic et al, 2002) allows for limited non isomorphism in that n-to-m matching of nodes in the two trees is permitted.
</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
however, even after extending this model by allowing cloning operations on subtrees, (gildea, 2003) <papid> P03-1011 </papid>found that parallel trees over-constrained the alignment problem, and achieved better results with tree-to-string model using one input tree than with tree-to-tree model using two.</citsent>
<aftsection>
<nextsent>at the same time, grammar theoreticians have proposed various generative synchronous grammar formalisms for mt, such as synchronous context free grammars (s-cfg) (wu, 1997) <papid> J97-3002 </papid>or synchronous tree adjoining grammars (s-tag) (shieber and schabes, 1990).<papid> C90-3045 </papid></nextsent>
<nextsent>mathematically, generative synchronous grammars share many good properties similar to their monolingual counterparts such as cfg or tag (joshi and schabes, 1992).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1750">
<title id=" W04-1513.xml">synchronous dependency insertion grammars a grammar formalism for syntax based statistical mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(hajic et al, 2002) allows for limited non isomorphism in that n-to-m matching of nodes in the two trees is permitted.
</prevsent>
<prevsent>however, even after extending this model by allowing cloning operations on subtrees, (gildea, 2003) <papid> P03-1011 </papid>found that parallel trees over-constrained the alignment problem, and achieved better results with tree-to-string model using one input tree than with tree-to-tree model using two.</prevsent>
</prevsection>
<citsent citstr=" C90-3045 ">
at the same time, grammar theoreticians have proposed various generative synchronous grammar formalisms for mt, such as synchronous context free grammars (s-cfg) (wu, 1997) <papid> J97-3002 </papid>or synchronous tree adjoining grammars (s-tag) (shieber and schabes, 1990).<papid> C90-3045 </papid></citsent>
<aftsection>
<nextsent>mathematically, generative synchronous grammars share many good properties similar to their monolingual counterparts such as cfg or tag (joshi and schabes, 1992).
</nextsent>
<nextsent>if such synchronous grammar could be learnt from parallel corpora, the mt task would become mathematically clean generative process.
</nextsent>
<nextsent>however, the problem of inducing synchronous grammar from empirical data was never solved.
</nextsent>
<nextsent>for example, synchronous tags, proposed by (shieber and schabes, 1990), <papid> C90-3045 </papid>which were introduced primarily for semantics but were later also proposed for translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1752">
<title id=" W04-1513.xml">synchronous dependency insertion grammars a grammar formalism for syntax based statistical mt </title>
<section> without the unrealistic word-to-word isomor-.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 specifies the synchronous dig and section 6 gives the probabilistic extension of sdig.
</prevsent>
<prevsent>2 issues with dependency grammars.
</prevsent>
</prevsection>
<citsent citstr=" W02-1039 ">
2.1 dependency grammars and statistical mt. according to (fox, 2002), <papid> W02-1039 </papid>dependency representations have the best phrasal cohesion properties across languages.</citsent>
<aftsection>
<nextsent>the percentage of head crossings perchance is 12.62% and that of modifier crossings perchance is 9.22%.
</nextsent>
<nextsent>observing this fact, it is reasonable to propose formalism that handles language transfer based on dependency structures.
</nextsent>
<nextsent>what is more, if formalism based on dependency structures is made possible, it will have the nice property of being simple, as expressed in the following table: cfg tag dg node# 2n 2n lexicalized?
</nextsent>
<nextsent>no yes yes node types 2 2 1* operation types 1 2 1* (*: will be shown later in this paper) figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1755">
<title id=" W04-1513.xml">synchronous dependency insertion grammars a grammar formalism for syntax based statistical mt </title>
<section> synchronous dig.  </section>
<citcontext>
<prevsection>
<prevsent>in the testing data, each input chinese sentence has 4 english translations as references, so that the result of the mt system can be evaluated using bleu and nist machine translation evaluation software.
</prevsent>
<prevsent>1-gram 2-gram 3-gram 4-gram nist: 4.3753 4.9773 5.0579 5.0791 bleu: 0.5926 0.3417 0.2060 0.1353 figure 15 the above table shows the cumulative bleu and nist n-gram scores for our current implementation; with the final bleu score 0.1353 with average input sentence length of 26.3 words.
</prevsent>
</prevsection>
<citsent citstr=" P02-1039 ">
in comparison, in (yamada and knight, 2002), <papid> P02-1039 </papid>which was phrasal structure based statistical mt system for chinese to english translation, the bleu score reported for short sentences (less than 14 words) is 0.099 to 0.102.</citsent>
<aftsection>
<nextsent>please note that the bleu/nist scorers, while based on n-gram matching, do not model syntax during evaluation, which means direct comparison between syntax based mt system and string based statistical mt system using the above scorer would favor the string based systems.
</nextsent>
<nextsent>we believe that our results can be improved using more sophisticated machine translation pipeline which has separate components that handle specific language phenomena such as named entities.
</nextsent>
<nextsent>larger training corpora can also be helpful.
</nextsent>
<nextsent>8 conclusion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1756">
<title id=" W04-0706.xml">using word similarity lists for resolving indirect anaphora </title>
<section> indirect anaphora.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, it might be just co referent, in the sense that the entity has been mentioned before in the text.
</prevsent>
<prevsent>in this work, we focus on the expressions that are anaphoric and co referent, and restricting even more,just the indirect cases, when the antecedent head noun and the anaphor head-noun are not same but semantically related.
</prevsent>
</prevsection>
<citsent citstr=" J98-2001 ">
to clarify what we mean by indirect anaphora,we detail the classification we adopted in our previous work (vieira et al, 2002; vieira et al, 2003).our classes of analyses were based on the analyses of english texts presented in (poesio and vieira,1998), <papid> J98-2001 </papid>with the difference that we divided the bridging class of their analyses into two different classes,separating co referent (indirect anaphora) and non co referent (other anaphora) cases.</citsent>
<aftsection>
<nextsent>each definite description (d) is classified into one of the following four classes: 1.
</nextsent>
<nextsent>direct anaphora: co refers with previous.
</nextsent>
<nextsent>expression a; and have the same nominal head: a. comisso tem conhecimento do livro...
</nextsent>
<nextsent>(the commission knows the book) d. comisso cons tata ainda que livro no sedebrua sobre actividade das vrias...
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1757">
<title id=" W04-0706.xml">using word similarity lists for resolving indirect anaphora </title>
<section> lexical resource.  </section>
<citcontext>
<prevsection>
<prevsent>anaphora in art our heuristics were implemented as an xsl style sheet on the basis of the anaphora resolution tool (art) (vieira et al, 2003).the tool integrates set of heuristics corresponding to one or more style sheets to resolve different sorts of anaphora.
</prevsent>
<prevsent>the heuristics may be applied in sequence defined by the user.
</prevsent>
</prevsection>
<citsent citstr=" P99-1048 ">
as resolving direct anaphoric descriptions (the ones where anaphor and antecedent have the same head noun) is much simpler problem with high performance rates as shown in previous results (vieira et al, 2000; bean and riloff, 1999), <papid> P99-1048 </papid>these heuristics should be applied first in system that resolves definite descriptions.</citsent>
<aftsection>
<nextsent>in this work, however, we decided to consider forthe experiments just the anaphoras that were previously annotated as indirect and check if the proposed heuristic is able to find the correct antecedent.
</nextsent>
<nextsent>art allows the user to define the set of anaphors to be resolved, in our case they are selected from previously classified definite descriptions.
</nextsent>
<nextsent>the style sheet for indirect anaphora takes as input this list of indirect anaphors, list of the candidates and the similarity lists.
</nextsent>
<nextsent>we consider all nps in the text as candidates, and for each anaphor we consider just the candidates that appear before it in the text (we are ignoring cataphora at moment).all the input and output data is in xml format, based on the data format used by mmax.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1758">
<title id=" W04-0706.xml">using word similarity lists for resolving indirect anaphora </title>
<section> lexical resource.  </section>
<citcontext>
<prevsection>
<prevsent>in 26% of the false positive cases, the correct antecedent (a proper name) was in the anaphor similarity list (but was not selected due to the weighting strategy).the experiment with the similarity lists that include proper names was able to solve more cases,but experiment 1 got better precision and recall values.
</prevsent>
<prevsent>8 related work.
</prevsent>
</prevsection>
<citsent citstr=" W97-1301 ">
an evaluation of the use of wordnet for treating bridging descriptions is presented in (poesio et al,1997).<papid> W97-1301 </papid></citsent>
<aftsection>
<nextsent>this evaluation considers 204 bridging descriptions, distributed as follows, where npj is the anaphora and npi is antecedent.
</nextsent>
<nextsent>synonymy relation between npj and npi: 12 cases; ? hypernymy relation between npj and npi: 14 cases; ? meronymy between npj and npi: 12; ? npj related with npi being proper name: 49; ? npj sharing same noun in npi other than head (compound nouns): 25; ? npj with antecedent being an event 40;?
</nextsent>
<nextsent>npj with antecedents being an implicit discourse topic: 15; ? other types of inferences holding between npj and antecedent: 37.
</nextsent>
<nextsent>due to the nature of the relations, only some of them were expected to be found in wordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1763">
<title id=" W04-1609.xml">an unsupervised approach for bootstrapping arabic sense tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, most arabic dictionaries list the entries in terms of roots rather than surface forms.
</prevsent>
<prevsent>in this paper, we present an approach, salaam (sense annotations leveraging alignments and multilinguality), to bootstrap wsd for arabic text presented in surface form.
</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
the approach of salaam is based on work by (diab and resnik, 2002) <papid> P02-1033 </papid>but it goes beyond it in the sense of extending the approach to the tagging of arabicas target lan guage.(diab, 2003) salaam uses cross-linguistic correspondences for characterizing word meanings in natural language.</citsent>
<aftsection>
<nextsent>this idea is explored by several researchers, (resnik and yarowsky, 1998; chugur et al, 2002; <papid> W02-0805 </papid>ide, 2000; dyvik, 1998).</nextsent>
<nextsent>basically, word meaning or word sense is quantifiable as much as it is uniquely translated in some language or set of languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1764">
<title id=" W04-1609.xml">an unsupervised approach for bootstrapping arabic sense tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we present an approach, salaam (sense annotations leveraging alignments and multilinguality), to bootstrap wsd for arabic text presented in surface form.
</prevsent>
<prevsent>the approach of salaam is based on work by (diab and resnik, 2002) <papid> P02-1033 </papid>but it goes beyond it in the sense of extending the approach to the tagging of arabicas target lan guage.(diab, 2003) salaam uses cross-linguistic correspondences for characterizing word meanings in natural language.</prevsent>
</prevsection>
<citsent citstr=" W02-0805 ">
this idea is explored by several researchers, (resnik and yarowsky, 1998; chugur et al, 2002; <papid> W02-0805 </papid>ide, 2000; dyvik, 1998).</citsent>
<aftsection>
<nextsent>basically, word meaning or word sense is quantifiable as much as it is uniquely translated in some language or set of languages.
</nextsent>
<nextsent>salaam is an empirical validation of this very notion of characterizing word meaning using cross-linguistic correspondences.
</nextsent>
<nextsent>since automated lexical resources are virtually non-existent for arabic, salaam leverages sense ambiguity resolution for arabic off of existing english lexical resources and an arabic english parallel corpus, thereby providing bilingual solution to the wsd problem.the paper is organized as follows: section 2 describes the salaam system; section 3 presents an evaluation of the approach followed by section 4which discusses the chosen sense inventory in relation to the arabic data; we conclude with summary and some final remarks in section 6.
</nextsent>
<nextsent>salaam exploits parallel corpora for sense annotation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1765">
<title id=" W04-1609.xml">an unsupervised approach for bootstrapping arabic sense tagging </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>therefore the test set corpus is the senseval2 english all words test corpus which comprises three articles from the wall street journal discussing religious practice, medicine and education.
</prevsent>
<prevsent>the test corpus does not exist in arabic.
</prevsent>
</prevsection>
<citsent citstr=" W00-0801 ">
due to the high expense of manually creating parallel corpus, i.e. using human translators, we opt for automatic translation systems in fashion similar to (diab, 2000).<papid> W00-0801 </papid></citsent>
<aftsection>
<nextsent>to our knowledge there exist two off the shelf english arabic machine translation (mt) systems: tarjim and almisbar.3 we use both mt systems to translate the test corpus into arabic.
</nextsent>
<nextsent>we merge the outputs of both in an attempt to achieve more variability in translation as an approximation to human quality translation.
</nextsent>
<nextsent>the merging process is based on the assumption that the mt systems relyon different sources of knowledge, different dictionaries in the least, in their translation process.
</nextsent>
<nextsent>fortunately, the mt systems produce sentence aligned parallel corpora.4 however, salaam expects token aligned parallel corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1766">
<title id=" W04-1609.xml">an unsupervised approach for bootstrapping arabic sense tagging </title>
<section> general discussion.  </section>
<citcontext>
<prevsection>
<prevsent>metonymy is more pragmatic than regular polysemy (cruse,1986); for example, tea in english has the following metonymic sense from wn1.7pre: ? reception or party at which tea is served;  we met at the deans tea for newcomers this sense of tea does not have correspondent in the arabic $ay ( .!
</prevsent>
<prevsent>yet, the english lamb has the metonymic sense of meat which exists in arabic.
</prevsent>
</prevsection>
<citsent citstr=" W99-0512 ">
researchers buildingeurowordnet have been able to devise number of consistent metonymic relations that hold cross linguistically such as fabric/material, an imal/food, building/organization (vossen et al,1999; <papid> W99-0512 </papid>wim peters and wilks, 2001).</citsent>
<aftsection>
<nextsent>in general, in arabic, these defined classes seem to hold, however, the specific case of tea and party does not exist.
</nextsent>
<nextsent>in arabic, the english sense would be expressed as compound tea party or hflp $ay (.!
</nextsent>
<nextsent> 8:ff
</nextsent>
<nextsent> arabic word equivalent to specific english sense(s) in this evaluation set, there are 138 instances where the arabic word is equivalent to sub sense(s) of the corresponding english word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1768">
<title id=" W04-2004.xml">syntactic parser combination for improved dependency analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the platform combines several analyses of the same utterance, and then computes the best data to produce the best possible analysis.
</prevsent>
<prevsent>our approach is based on the method called vote by majority?, the more common to the different parsers one data will be, the stronger its weight will be, and also based on training method which adapts each vote according to the typologies of utterances (domain, style) and the abilities of the parsers.
</prevsent>
</prevsection>
<citsent citstr=" A00-2009 ">
the approach used, called o b n t o approach, has known lots of success in speech recognition (fiscus 1997, schwenck and gauvain 2000), part of speech tagging (halteren and al. 1998, brill and al. 1998, marquez et padro 1998), named entity recognition (borthwick and al. 1998), word sense disambiguation (pedersen, 2000) <papid> A00-2009 </papid>and recently in parsing (henderson and brill 1999), <papid> W99-0623 </papid>inui and inui 2000, <papid> C00-1051 </papid>monceaux and robba 2003).</citsent>
<aftsection>
<nextsent>these works prove that combining different systems provides an improvement in comparison to the best system.
</nextsent>
<nextsent>our work in syntactic analysis are distinguished from our predecessors by the combination methods that we use.
</nextsent>
<nextsent>our platform is made up of statistical processing, correspondence processing and reconstruction processing.
</nextsent>
<nextsent>furthermore, we base our platform on dependency representation that describes the syntactic relations between words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1769">
<title id=" W04-2004.xml">syntactic parser combination for improved dependency analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the platform combines several analyses of the same utterance, and then computes the best data to produce the best possible analysis.
</prevsent>
<prevsent>our approach is based on the method called vote by majority?, the more common to the different parsers one data will be, the stronger its weight will be, and also based on training method which adapts each vote according to the typologies of utterances (domain, style) and the abilities of the parsers.
</prevsent>
</prevsection>
<citsent citstr=" W99-0623 ">
the approach used, called o b n t o approach, has known lots of success in speech recognition (fiscus 1997, schwenck and gauvain 2000), part of speech tagging (halteren and al. 1998, brill and al. 1998, marquez et padro 1998), named entity recognition (borthwick and al. 1998), word sense disambiguation (pedersen, 2000) <papid> A00-2009 </papid>and recently in parsing (henderson and brill 1999), <papid> W99-0623 </papid>inui and inui 2000, <papid> C00-1051 </papid>monceaux and robba 2003).</citsent>
<aftsection>
<nextsent>these works prove that combining different systems provides an improvement in comparison to the best system.
</nextsent>
<nextsent>our work in syntactic analysis are distinguished from our predecessors by the combination methods that we use.
</nextsent>
<nextsent>our platform is made up of statistical processing, correspondence processing and reconstruction processing.
</nextsent>
<nextsent>furthermore, we base our platform on dependency representation that describes the syntactic relations between words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1770">
<title id=" W04-2004.xml">syntactic parser combination for improved dependency analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the platform combines several analyses of the same utterance, and then computes the best data to produce the best possible analysis.
</prevsent>
<prevsent>our approach is based on the method called vote by majority?, the more common to the different parsers one data will be, the stronger its weight will be, and also based on training method which adapts each vote according to the typologies of utterances (domain, style) and the abilities of the parsers.
</prevsent>
</prevsection>
<citsent citstr=" C00-1051 ">
the approach used, called o b n t o approach, has known lots of success in speech recognition (fiscus 1997, schwenck and gauvain 2000), part of speech tagging (halteren and al. 1998, brill and al. 1998, marquez et padro 1998), named entity recognition (borthwick and al. 1998), word sense disambiguation (pedersen, 2000) <papid> A00-2009 </papid>and recently in parsing (henderson and brill 1999), <papid> W99-0623 </papid>inui and inui 2000, <papid> C00-1051 </papid>monceaux and robba 2003).</citsent>
<aftsection>
<nextsent>these works prove that combining different systems provides an improvement in comparison to the best system.
</nextsent>
<nextsent>our work in syntactic analysis are distinguished from our predecessors by the combination methods that we use.
</nextsent>
<nextsent>our platform is made up of statistical processing, correspondence processing and reconstruction processing.
</nextsent>
<nextsent>furthermore, we base our platform on dependency representation that describes the syntactic relations between words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1771">
<title id=" W04-2004.xml">syntactic parser combination for improved dependency analysis </title>
<section> dependency structure construction.  </section>
<citcontext>
<prevsection>
<prevsent>for example, the words of the sentence on avait dnombr?
</prevsent>
<prevsent>cent vingt-neuf candidats?
</prevsent>
</prevsection>
<citsent citstr=" C88-1013 ">
have to snode: on[1,2], avait[3-7], dnombr?[8-15], etc. this data is based on the proposal of (boitet and zaharin,1988) <papid> C88-1013 </papid>structured string-tree correspondences (sstc).</citsent>
<aftsection>
<nextsent>l: set which contains the nodes of the normalized structures linked to the node rs . the first step consists in creating an initial sn for each dependency structure.
</nextsent>
<nextsent>each initial node sn of an initial sn is created according to node i of the dependency structure k : snode(n sn )=snode(s .n ) &amp; l(n sn )={s .n } then the nodes of the initial snare inserted into the lattice according to their appearance order into the sentence (according to their snode).
</nextsent>
<nextsent>in the following, we take two dependency structures 1 and 2 , and their initial segmentation networks sn 1 and sn 2 : let us do the correspondences between sn 1 and sn 2 . first, the initial network sn.
</nextsent>
<nextsent>1 is chosen as the basic sn, called sn base . we use two rules to.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1772">
<title id=" W04-2501.xml">strategies for advanced question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our fundamental premise is that progress in q/a cannot be achieved only by enhancing the processing components, but it also requires generating the best strategies for processing each individual question.
</prevsent>
<prevsent>thus we believe that q/a systems capable of successfully processing complex questions should employ multiple strategies instead of the current pipeline approach, consisting of (1) question processing, (2) passage retrieval and (3) answer selection.
</prevsent>
</prevsection>
<citsent citstr=" P00-1071 ">
the pipeline architecture was reported in (prager et al, 2000; moldovan et al, 2000; <papid> P00-1071 </papid>hovy et al, 2001).</citsent>
<aftsection>
<nextsent>recently, novel approach based on combinations of multiple independent agents implementing different answer finding strategies (multi strategy) and multiple search spaces (multiple-source) was developed by the ibm qa group (chu-carroll et al., 2003).
</nextsent>
<nextsent>in (echihabi and marcu, 2003) <papid> P03-1003 </papid>another form of combining strategies for advanced qa is proposed: (1) knowledge-based q/a implementation based on syntactic/semantic processing is combined using maximum-entropy framework with (2) statistical noisy-channel algorithm for q/a and (3) pattern-based approach that learn from web data.</nextsent>
<nextsent>in this project we propose different form of finding optimal strategies of advanced qa which is based on (a) question decomposition, (b) answer fusion and feedback from (c) interactive q&a; and (d) user background recognition.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1773">
<title id=" W04-2501.xml">strategies for advanced question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the pipeline architecture was reported in (prager et al, 2000; moldovan et al, 2000; <papid> P00-1071 </papid>hovy et al, 2001).</prevsent>
<prevsent>recently, novel approach based on combinations of multiple independent agents implementing different answer finding strategies (multi strategy) and multiple search spaces (multiple-source) was developed by the ibm qa group (chu-carroll et al., 2003).</prevsent>
</prevsection>
<citsent citstr=" P03-1003 ">
in (echihabi and marcu, 2003) <papid> P03-1003 </papid>another form of combining strategies for advanced qa is proposed: (1) knowledge-based q/a implementation based on syntactic/semantic processing is combined using maximum-entropy framework with (2) statistical noisy-channel algorithm for q/a and (3) pattern-based approach that learn from web data.</citsent>
<aftsection>
<nextsent>in this project we propose different form of finding optimal strategies of advanced qa which is based on (a) question decomposition, (b) answer fusion and feedback from (c) interactive q&a; and (d) user background recognition.
</nextsent>
<nextsent>we argue that all this new architectures operate under the assumption that there is concept-based or pat tern-based method for identifying the correct answer for any question that will be processed.
</nextsent>
<nextsent>however, we believe that there are complex questions that need first to be decomposed into simple questions, for which con cept-based or pattern-based resolving techniques either exists or may be developed.
</nextsent>
<nextsent>for instance, when asking q1: how have thefts impacted on the safety of russias nuclear navy, and has the theft problem been increased or decreased over time??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1775">
<title id=" W04-2501.xml">strategies for advanced question answering </title>
<section> answer fusion, ranking and reliability.  </section>
<citcontext>
<prevsection>
<prevsent>the block architecture for answer fusion is illustrated in figure 2.
</prevsent>
<prevsent>the system functionality is demonstrated with the example illustrated in figure 3.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
our method first converts the extracted answers into series of open-domain templates, which are based on predicate-argument frames (surdeanu et al 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>the next component detects generic inter-template relations.
</nextsent>
<nextsent>typical greedy?
</nextsent>
<nextsent>approaches in information extraction (hobbs et al 1997; surdeanu and harabagiu, 2002) use heuristics that favor proximity for template merging.
</nextsent>
<nextsent>the example in figure 3 proves that this is not always the best decision, even for templates that share the same predicate and have compatible slots.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1776">
<title id=" W04-2501.xml">strategies for advanced question answering </title>
<section> answer fusion, ranking and reliability.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 open-domain template representation.
</prevsent>
<prevsent>a key issue to the proposed approach is the open domain template representation.
</prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
while template-based representations have been proposed for information merging in the past (radev and mckeown, 1998), <papid> J98-3005 </papid>they considered only domain-specific scenarios.</citsent>
<aftsection>
<nextsent>based on our recent successes with the extraction of predicate argument frames (surdeanu et al 2003), <papid> P03-1002 </papid>we propose template representation that is direct mapping of predicate-argument frames.</nextsent>
<nextsent>for example, the first template in figure 3 is generated from the frame detected for the predicate assassinate?: the first slot ? arg0 ? typically stands for subject or agent; the second slot ? arg1 ? stands for the predicate object, and the modifier arguments argm-loc and argm-tmp indicate the location and date of the event.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1779">
<title id=" W04-2506.xml">a novel approach to focus identification in question answering systems </title>
<section> textual question answering.  </section>
<citcontext>
<prevsection>
<prevsent>in this case another word from the question needs to be used to determine the semantic class of the expectedanswer.
</prevsent>
<prevsent>in particular, the additional word is semantically classified against an ontology of semantic classes.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
to determine which word indicates the semantic classof the expected answer, the syntactic dependencies1 between the question words may be employed (harabagiu 1syntactic parsers publicly available, e.g., (charniak, 2000; <papid> A00-2018 </papid>et al, 2000; pasca and harabagiu, 2001; harabagiu et al, 2001).<papid> P01-1037 </papid></citsent>
<aftsection>
<nextsent>sometimes the semantic class of the expected answers cannot be identified or is erroneously identified causing the selection of erroneous answers.
</nextsent>
<nextsent>the use of text classification aims to filter out the incorrect set of answers that q/a systems provide.
</nextsent>
<nextsent>2.2 paragraph retrieval.
</nextsent>
<nextsent>once the question processing has chosen the relevant keywords of questions, some term expansion techniques are applied: all nouns and adjectives as well as morphological variations of nouns are inserted in list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1780">
<title id=" W04-2506.xml">a novel approach to focus identification in question answering systems </title>
<section> textual question answering.  </section>
<citcontext>
<prevsection>
<prevsent>in this case another word from the question needs to be used to determine the semantic class of the expectedanswer.
</prevsent>
<prevsent>in particular, the additional word is semantically classified against an ontology of semantic classes.
</prevsent>
</prevsection>
<citsent citstr=" P01-1037 ">
to determine which word indicates the semantic classof the expected answer, the syntactic dependencies1 between the question words may be employed (harabagiu 1syntactic parsers publicly available, e.g., (charniak, 2000; <papid> A00-2018 </papid>et al, 2000; pasca and harabagiu, 2001; harabagiu et al, 2001).<papid> P01-1037 </papid></citsent>
<aftsection>
<nextsent>sometimes the semantic class of the expected answers cannot be identified or is erroneously identified causing the selection of erroneous answers.
</nextsent>
<nextsent>the use of text classification aims to filter out the incorrect set of answers that q/a systems provide.
</nextsent>
<nextsent>2.2 paragraph retrieval.
</nextsent>
<nextsent>once the question processing has chosen the relevant keywords of questions, some term expansion techniques are applied: all nouns and adjectives as well as morphological variations of nouns are inserted in list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1781">
<title id=" W04-2506.xml">a novel approach to focus identification in question answering systems </title>
<section> textual question answering.  </section>
<citcontext>
<prevsection>
<prevsent>to learn the comparison function we use simple neural network, namely, the perceptron, to compute relative comparison between any two sentences.
</prevsent>
<prevsent>this score is computed by considering four different features for each sentence as explained in (pasca and harabagiu, 2001).
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
step 3) answer extraction: we select the top 5 ranked sentences and return them as collins, 1997), <papid> P97-1003 </papid>can be used to capture the binary dependencies between the head of each phrase.</citsent>
<aftsection>
<nextsent>answers.
</nextsent>
<nextsent>if we lead fewer than 5 sentences to select from, we return all of them.once the answers are extracted we can apply an additional filter based on text categories.
</nextsent>
<nextsent>the idea is to match the categories of the answers against those of the questions.
</nextsent>
<nextsent>next section addresses the problem of question and answer categorization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1782">
<title id=" W04-2302.xml">stochastic language generation in a dialogue system toward a domain independent generator </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the approach is attractive in comparison to templates and rule-based approaches because the language models implicitly encode the natural ordering of english.
</prevsent>
<prevsent>recently, the results from corpus-based surface generation in dialogue systems have been within specific domains, the vast majority of which have used the air travel domain with air travel corpora.
</prevsent>
</prevsection>
<citsent citstr=" A00-2026 ">
ratnaparkhi (ratnaparkhi, 2000; <papid> A00-2026 </papid>ratnaparkhi, 2002) and oh and rudnicky (oh and rudnicky, 2000) <papid> W00-0306 </papid>both studied surface generators for the air travel domain.</citsent>
<aftsection>
<nextsent>their input semantic form is set of attribute-value pairs that are specific to the airline reservation task.
</nextsent>
<nextsent>the language models were standard n-gram approaches that depended on tagged air travel corpus for the attribute types.
</nextsent>
<nextsent>both groups ran human evaluations; ratnaparkhi studied 2 subject evaluation (with marks of ok,good,bad) and ohand rudnicky studied 12 subjects that compared the out put between template generator and the corpus-based approach.
</nextsent>
<nextsent>the latter showed no significant difference.most recently, chen et al utilized fergus (banga lore and rambow, 2000) <papid> C00-1007 </papid>and attempted to make it more domain independent in (chen et al, 2002).<papid> C02-1138 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1783">
<title id=" W04-2302.xml">stochastic language generation in a dialogue system toward a domain independent generator </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the approach is attractive in comparison to templates and rule-based approaches because the language models implicitly encode the natural ordering of english.
</prevsent>
<prevsent>recently, the results from corpus-based surface generation in dialogue systems have been within specific domains, the vast majority of which have used the air travel domain with air travel corpora.
</prevsent>
</prevsection>
<citsent citstr=" W00-0306 ">
ratnaparkhi (ratnaparkhi, 2000; <papid> A00-2026 </papid>ratnaparkhi, 2002) and oh and rudnicky (oh and rudnicky, 2000) <papid> W00-0306 </papid>both studied surface generators for the air travel domain.</citsent>
<aftsection>
<nextsent>their input semantic form is set of attribute-value pairs that are specific to the airline reservation task.
</nextsent>
<nextsent>the language models were standard n-gram approaches that depended on tagged air travel corpus for the attribute types.
</nextsent>
<nextsent>both groups ran human evaluations; ratnaparkhi studied 2 subject evaluation (with marks of ok,good,bad) and ohand rudnicky studied 12 subjects that compared the out put between template generator and the corpus-based approach.
</nextsent>
<nextsent>the latter showed no significant difference.most recently, chen et al utilized fergus (banga lore and rambow, 2000) <papid> C00-1007 </papid>and attempted to make it more domain independent in (chen et al, 2002).<papid> C02-1138 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1784">
<title id=" W04-2302.xml">stochastic language generation in a dialogue system toward a domain independent generator </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the language models were standard n-gram approaches that depended on tagged air travel corpus for the attribute types.
</prevsent>
<prevsent>both groups ran human evaluations; ratnaparkhi studied 2 subject evaluation (with marks of ok,good,bad) and ohand rudnicky studied 12 subjects that compared the out put between template generator and the corpus-based approach.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
the latter showed no significant difference.most recently, chen et al utilized fergus (banga lore and rambow, 2000) <papid> C00-1007 </papid>and attempted to make it more domain independent in (chen et al, 2002).<papid> C02-1138 </papid></citsent>
<aftsection>
<nextsent>there are two stochastic processes in fergus; tree chooser that maps an input syntactic tree to tag tree, and trigram language model that chooses the best sentence in the lattice.
</nextsent>
<nextsent>they found that domain-specific corpus performs better than wall street journal (wsj) corpus for the trigram lm.
</nextsent>
<nextsent>work was done to try and use an independent lm, but (rambow et al, 2001) <papid> H01-1055 </papid>found interrogatives to be unrepresented by wsj model and fell back on air travel models.</nextsent>
<nextsent>this problem was not discussed in (chen et al, 2002).<papid> C02-1138 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1785">
<title id=" W04-2302.xml">stochastic language generation in a dialogue system toward a domain independent generator </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the language models were standard n-gram approaches that depended on tagged air travel corpus for the attribute types.
</prevsent>
<prevsent>both groups ran human evaluations; ratnaparkhi studied 2 subject evaluation (with marks of ok,good,bad) and ohand rudnicky studied 12 subjects that compared the out put between template generator and the corpus-based approach.
</prevsent>
</prevsection>
<citsent citstr=" C02-1138 ">
the latter showed no significant difference.most recently, chen et al utilized fergus (banga lore and rambow, 2000) <papid> C00-1007 </papid>and attempted to make it more domain independent in (chen et al, 2002).<papid> C02-1138 </papid></citsent>
<aftsection>
<nextsent>there are two stochastic processes in fergus; tree chooser that maps an input syntactic tree to tag tree, and trigram language model that chooses the best sentence in the lattice.
</nextsent>
<nextsent>they found that domain-specific corpus performs better than wall street journal (wsj) corpus for the trigram lm.
</nextsent>
<nextsent>work was done to try and use an independent lm, but (rambow et al, 2001) <papid> H01-1055 </papid>found interrogatives to be unrepresented by wsj model and fell back on air travel models.</nextsent>
<nextsent>this problem was not discussed in (chen et al, 2002).<papid> C02-1138 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1787">
<title id=" W04-2302.xml">stochastic language generation in a dialogue system toward a domain independent generator </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>there are two stochastic processes in fergus; tree chooser that maps an input syntactic tree to tag tree, and trigram language model that chooses the best sentence in the lattice.
</prevsent>
<prevsent>they found that domain-specific corpus performs better than wall street journal (wsj) corpus for the trigram lm.
</prevsent>
</prevsection>
<citsent citstr=" H01-1055 ">
work was done to try and use an independent lm, but (rambow et al, 2001) <papid> H01-1055 </papid>found interrogatives to be unrepresented by wsj model and fell back on air travel models.</citsent>
<aftsection>
<nextsent>this problem was not discussed in (chen et al, 2002).<papid> C02-1138 </papid></nextsent>
<nextsent>perhaps automatically extracted trees from the corpora are able to create many good and few bad possibilities that the lm might choose.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1791">
<title id=" W04-2302.xml">stochastic language generation in a dialogue system toward a domain independent generator </title>
<section> stochastic generation (halogen).  </section>
<citcontext>
<prevsection>
<prevsent>we used the halogen framework (langkilde-geary,2002) for our surface generation.
</prevsent>
<prevsent>halogen was originally created for domain within mt and is sentence planner and surface realizer.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
analysis and mt applications can be found in (langkilde and knight, 1998; <papid> P98-1116 </papid>knight and langkilde, 2000).</citsent>
<aftsection>
<nextsent>halogen accepts feature-value structure ranging from high-level semantics to shallow syntax.
</nextsent>
<nextsent>figure 1 shows mixture of both as an example.
</nextsent>
<nextsent>given this input, generation is two step process.
</nextsent>
<nextsent>first, the input formis converted into word forest (a more efficient representation of word lattice) as described in (langkilde geary, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1792">
<title id=" W04-2302.xml">stochastic language generation in a dialogue system toward a domain independent generator </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the first uses human evaluators to score the out put with some pre-defined ranking measure.
</prevsent>
<prevsent>the second uses quantitative automatic approach usually based onn-gram presence and word ordering.
</prevsent>
</prevsection>
<citsent citstr=" W00-1401 ">
bangalore et al describe some of the quantitative measures that have been used in (bangalore et al, 2000).<papid> W00-1401 </papid></citsent>
<aftsection>
<nextsent>callaway recently used quantitative measures in an evaluation between symbolic and stochastic surface generators in (callaway, 2003).
</nextsent>
<nextsent>the most common quantitative measure is simple string accuracy.
</nextsent>
<nextsent>this metric uses an ideal output string and compares it to generated string using metric that combines three word error counts; insertion, deletion, and substitution.
</nextsent>
<nextsent>one variation on this approach is tree-based metrics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1793">
<title id=" W04-0506.xml">cooperative question answering in restricted domains the webcoop experiment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(grice, 1975) maxims of conversation namely the quality, quantity, relation and style maxims are frequently used as abasis for designing cooperative answering systems.
</prevsent>
<prevsent>an overview of cooperative answering techniques is given in (gaasterland et al, 1994).
</prevsent>
</prevsection>
<citsent citstr=" N03-1022 ">
in cogex (moldovan et al, 2003), <papid> N03-1022 </papid>recent qa system, authors used automated reasoning for qa and showed that it is feasible, effective and scalable.</citsent>
<aftsection>
<nextsent>this logical prover aims at checking and extracting all kinds of lexical relationships between the question and its candidate answers using world knowledge axioms,supplied by wordnet glosses, as well as rewriting rules representing equivalent classes of linguistic patterns.
</nextsent>
<nextsent>such inference techniques (e.g.lexical equivalence, unification on logical representations of texts) are not sufficient for providing intelligent or cooperative responses.
</nextsent>
<nextsent>indeed,advanced strategies for qa requires, as we explain in this paper, the integration of reasoning components operating over variety of knowledge bases, encoding common sense knowledge as well as knowledge specific to variety of do mains.we relate in this paper, an experiment forde signing logic based qa system, webcoop, that integrates knowledge representation and advanced reasoning procedures to generate cooperative responses to natural language (nl)queries on the web.
</nextsent>
<nextsent>this experiment is first carried out on relatively restricted domain that includes number of aspects of tourism (ac commodation and transportation, which have very different characteristics on the web).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1794">
<title id=" W04-0506.xml">cooperative question answering in restricted domains the webcoop experiment </title>
<section> evaluation of webcoop.  </section>
<citcontext>
<prevsection>
<prevsent>they should be portable provided that the knowledge resources of the new domain can be implemented using webcoop format, which is quite generic.
</prevsent>
<prevsent>but, besides qa annotations, which is very useful perspective, the adequacy of inferences can only be evaluated posteriori.
</prevsent>
</prevsection>
<citsent citstr=" W01-0906 ">
in future stage, we plan to use what barrand klavans (barr and klavans, 2001) <papid> W01-0906 </papid>call component performance evaluation which consists of assessing the performance of system components and determining their impact on the over all system performance.</citsent>
<aftsection>
<nextsent>4.2 evaluating response intelligibility.
</nextsent>
<nextsent>finally, since webcoop produces response sin nl, some of which on template basis (dif ferent from trec which simply reproduces textextracts), it is important to evaluate the portability of those templates.
</nextsent>
<nextsent>we propose method based on experimental psychology, that aims at evaluating the cooperative responses generated in the know-how component of webcoop.
</nextsent>
<nextsent>our methodology involves the following steps:- evaluating templates within single do main (tourism in our case).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1795">
<title id=" W04-1911.xml">word order variation in german main clauses a corpus analysis </title>
<section> corpus analysis.  </section>
<citcontext>
<prevsection>
<prevsent>in such cases, shared general knowledge of the relations between objects and their components or attributes is assumed.
</prevsent>
<prevsent>whenever more specific knowledge is required to establish such relation, however, complements are considered to be new.
</prevsent>
</prevsection>
<citsent citstr=" P00-1023 ">
the distinction between general and specific knowledge is3a recent study by morton (2000) <papid> P00-1023 </papid>showed for singular pronouns, that 98.7% of the times the antecedents were available within two preceding sentences.</citsent>
<aftsection>
<nextsent>his findings are similar with those reported by hobbs (1976).since the context in our study regularly introduces complements, we believe that it gives an adequate picture of the interaction between information status and word order.particularly hard to maintain, since the distinction is often clearly not binary.
</nextsent>
<nextsent>for instance, geographic familiarity with the catchment areaof the frankfurter rundschau is considered specific knowledge: in (4), waldstadion?
</nextsent>
<nextsent>is one of the local soccer stadiums in frankfurt.
</nextsent>
<nextsent>even though many local readers of the frankfurter rundschau will know this it can not be assumed to be known by all readers of the newspaper, and is therefore coded as new information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1796">
<title id=" W04-2505.xml">intentions implicatures and processing of complex questions </title>
<section> question complexity.  </section>
<citcontext>
<prevsection>
<prevsent>example of definition questions are what is golden parachute??
</prevsent>
<prevsent>or what is eta in spain??.
</prevsent>
</prevsection>
<citsent citstr=" P03-1003 ">
in (echihabi and marcu, 2003) <papid> P03-1003 </papid>noisy channel model for q/a was introduced.</citsent>
<aftsection>
<nextsent>this model is based on theidea that if given sentence sa contains an answer substring to question q, then sa can be re-written into through sequence of stochastic operators.
</nextsent>
<nextsent>not only justification of the answer is produced, but the conditional probability p(qsa) re-ranks all candidate answers.a different viewpoint of q/a was reported in (ittycheriah et al, 2000).
</nextsent>
<nextsent>finding the answers to question was considered classification problem that maximizes the conditional probability p(aq).
</nextsent>
<nextsent>this model is not tractable currently, because (a) the search space is too large for text collection like the trec or the aquaintcorpora; and (b) the training data is insufficient.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1797">
<title id=" W04-2505.xml">intentions implicatures and processing of complex questions </title>
<section> models of question answering.  </section>
<citcontext>
<prevsection>
<prevsent>thus we may have questions asking about people,organizations, time or locations.
</prevsent>
<prevsent>since open domain q/a systems process questions regardless of the domain of interest, question processing must be based on an extended ontology of answer types.
</prevsent>
</prevsection>
<citsent citstr=" P01-1037 ">
the identification of the expected answer type is based either on binary semantic dependencies extracted from the syntactic parse ofthe question (harabagiu et al, 2001) <papid> P01-1037 </papid>or on the predicate argument structure of the question.</citsent>
<aftsection>
<nextsent>in both cases, there lation to the question stem (i.e. what, who, when) enables the classification.
</nextsent>
<nextsent>figure 2 illustrates factoid question generated as an intended question and the derivation of its expected answer type.
</nextsent>
<nextsent>however, many times the expected answer type needsto be identified from an ontology that has high lexico semantic coverage.
</nextsent>
<nextsent>many q/a systems use the wordnet database for this purpose.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1798">
<title id=" W04-2505.xml">intentions implicatures and processing of complex questions </title>
<section> models of question answering.  </section>
<citcontext>
<prevsection>
<prevsent>figure 5 shows the structure of the novel model of q/a we propose.
</prevsent>
<prevsent>both question processing and document processing have the recognition of predicate-argumentstructures as crux of their models.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
as reported in (sur deanu et al, 2003), <papid> P03-1002 </papid>the recognition of predicate-argumentstructures depends on features made available by full syntactic parses and by named entity recognizers.</citsent>
<aftsection>
<nextsent>as weshall show in this paper, the predicate-argument structures enable the recognition of question pattern, the question focus and the intentional structure associated with question syntactic parse named entity recognition identification of predicate argument structure structure recognition of answer based on extended indexing &amp; retrieval lexico semantic knowledge named entityrecognitionsyntactic parse intentional structure identification ofpredicateargumentstructures question pattern recognition of identification of question focus recognition of answer structure keyword extraction validation of implied information answer structure recognition of recognition and extention of intentional structure reference resolution question processing answer processing document processing answer figure 5: novel question/answering architecture.
</nextsent>
<nextsent>a question.
</nextsent>
<nextsent>when the intentions are known, the answer structure can be identified and the keywords extracted.
</nextsent>
<nextsent>for better retrieval of candidate answers, documents are indexed and retrieved based on the predicate-argumentstructures as well as on complex semantic structure associated with different question patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1799">
<title id=" W04-2505.xml">intentions implicatures and processing of complex questions </title>
<section> predicate-argument structures.  </section>
<citcontext>
<prevsection>
<prevsent>often this requires reference resolution.
</prevsent>
<prevsent>the implied information coerced from both the question and the candidate answer is also validated before deciding on the answer correctness.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
to identify predicate-argument structures in questions and passages, we have: (1) used the proposition bank or propbank as training data; and (2) mode for predicting argument roles similar to the one employed by (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>propbank is one million word corpus annotated withpredicate-argument structures on top of the penn tree bank 2 wall street journal texts.
</nextsent>
<nextsent>for any given predicate, the expected arguments are labeled sequentially from arg 0 to arg 4.
</nextsent>
<nextsent>generally, arg 0 stands for agent, arg 1 for direct object or theme or patient, arg 2 for indirect object or benefactive or instrument or attribute or end state, arg 3 for start point or benefactive or attribute and arg4 forend point.
</nextsent>
<nextsent>in addition to these core arguments, adjunc tative arguments are marked up.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1804">
<title id=" W04-2505.xml">intentions implicatures and processing of complex questions </title>
<section> predicate-argument structures.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 parsing sentence into predicate argument.
</prevsent>
<prevsent>structures for the experiments, we used propbank(www.cis.upenn.edu/ace) along with penn treebank3 2 (www.cis.upenn.edu/treebank) (echihabi and marcu, 2003).<papid> P03-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1006 ">
this corpus contains about 53,700 sentences and fixed split between training and testing which has been used in other researches (gildea and jurafsky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>hacioglu etal., 2003; chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>pradhan et al, 2003).</citsent>
<aftsection>
<nextsent>in this split, sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set.
</nextsent>
<nextsent>we considered all propbank arguments from arg0 to arg9, arga and argm even if only arg0 from arg4 and argm contain enough training/testing 2this is basic method to pass from binary categorization into multi-class categorization problem; several optimization have been proposed, e.g.
</nextsent>
<nextsent>(goh et al, 2001).
</nextsent>
<nextsent>3we point out that we removed from the penn treebank th especial tags of noun phrases like subj and tmp as parsers usually are not able to provide this information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1805">
<title id=" W04-2505.xml">intentions implicatures and processing of complex questions </title>
<section> predicate-argument structures.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 parsing sentence into predicate argument.
</prevsent>
<prevsent>structures for the experiments, we used propbank(www.cis.upenn.edu/ace) along with penn treebank3 2 (www.cis.upenn.edu/treebank) (echihabi and marcu, 2003).<papid> P03-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1008 ">
this corpus contains about 53,700 sentences and fixed split between training and testing which has been used in other researches (gildea and jurafsky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>hacioglu etal., 2003; chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>pradhan et al, 2003).</citsent>
<aftsection>
<nextsent>in this split, sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set.
</nextsent>
<nextsent>we considered all propbank arguments from arg0 to arg9, arga and argm even if only arg0 from arg4 and argm contain enough training/testing 2this is basic method to pass from binary categorization into multi-class categorization problem; several optimization have been proposed, e.g.
</nextsent>
<nextsent>(goh et al, 2001).
</nextsent>
<nextsent>3we point out that we removed from the penn treebank th especial tags of noun phrases like subj and tmp as parsers usually are not able to provide this information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1806">
<title id=" W04-2505.xml">intentions implicatures and processing of complex questions </title>
<section> predicate-argument structures.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 parsing sentence into predicate argument.
</prevsent>
<prevsent>structures for the experiments, we used propbank(www.cis.upenn.edu/ace) along with penn treebank3 2 (www.cis.upenn.edu/treebank) (echihabi and marcu, 2003).<papid> P03-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
this corpus contains about 53,700 sentences and fixed split between training and testing which has been used in other researches (gildea and jurafsky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>hacioglu etal., 2003; chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>pradhan et al, 2003).</citsent>
<aftsection>
<nextsent>in this split, sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set.
</nextsent>
<nextsent>we considered all propbank arguments from arg0 to arg9, arga and argm even if only arg0 from arg4 and argm contain enough training/testing 2this is basic method to pass from binary categorization into multi-class categorization problem; several optimization have been proposed, e.g.
</nextsent>
<nextsent>(goh et al, 2001).
</nextsent>
<nextsent>3we point out that we removed from the penn treebank th especial tags of noun phrases like subj and tmp as parsers usually are not able to provide this information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1807">
<title id=" W04-2505.xml">intentions implicatures and processing of complex questions </title>
<section> intentional structures.  </section>
<citcontext>
<prevsection>
<prevsent>the model proposed in (sanda harabagiu and yukawa, 1996) uncovered relationship between (a) the coherence of text segment; (b) its cohesion expressed by the lexical paths and (c) the implicatures that can be drawn, mostly to account for pragmatic knowledge.
</prevsent>
<prevsent>this relationship can be extended across documents and across topics, to learn patterns of textual and q&a; implicatures and the methods of deriving knowledge that enables their recog nition.the derivation of pragmatic knowledge combines information from three different sources: (1) lexical knowledge bases (e.g. wordnet), (2) expert knowledge bases that can be rapidly formatted for many domains (e.g. japanese political knowledge); and (3) knowledge supported from the textual information available from documents.
</prevsent>
</prevsection>
<citsent citstr=" C00-1043 ">
the methodology of combining these three sources of information is novel.for question qi , the starting point is the concept identified as cue for the expected answer type through methods described in (harabagiu et al, 2000).<papid> C00-1043 </papid></citsent>
<aftsection>
<nextsent>this concept is lexicalized by the verb-object pair survive-crisis.
</nextsent>
<nextsent>verb survive has four distinct senses in the wordnet 1.6database, whereas noun crisis has two senses.
</nextsent>
<nextsent>the poly semy of the expected answer type increases the difficulty of the derivation of pragmatic knowledge, but it does not presupposes the word sense disambiguation of the expression.
</nextsent>
<nextsent>the information available in the glosses defining the wordnet synsets provides helpful information for expanding the multi-word term defining the expected answer type.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1808">
<title id=" W04-2505.xml">intentions implicatures and processing of complex questions </title>
<section> intentional structures.  </section>
<citcontext>
<prevsection>
<prevsent>the structure of the topic is define along three semantic dimensions: (1)hyponyms or examples of other types of the same category as the topic; (2) the meronyms or components; and (3) the functionality or the usage.
</prevsent>
<prevsent>the derivation of such large set of intentional structures helped us learn how to coerce pragmatic knowledge.
</prevsent>
</prevsection>
<citsent citstr=" J03-2004 ">
we have developed probabilistic approach extending the metonymy work of (lapata and lascarides, 2003).<papid> J03-2004 </papid></citsent>
<aftsection>
<nextsent>lapata and lascarides report model of interpretation of verbal metonymy as the point distribution (e, o, v) of three variables: the metonymy verb v, its object, and the sought after interpretation i. for example verb ? object relation that needs to be metonymycally interpreted, is enjoy ? movie.
</nextsent>
<nextsent>in this case = enjoy, = movie and ? {making, watching, directing}.
</nextsent>
<nextsent>the variables of the distribution reordered as  i, v,  to help factoring (i, v, o) = (i) ? (v|i) ? (o|i, v).
</nextsent>
<nextsent>each of the probabilities (i), (v|i) and (o|i, v) can be estimated using maximum likelihood.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1810">
<title id=" W04-2007.xml">using an incremental robust parser to automatically generate semantic unl graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the representation of an utterance in the unl interlingua is hypergraph where nodes bear universal words (interlingual acceptions) with semantic attributes and arcs denote semantic relations.
</prevsent>
<prevsent>any natural language utterance can be enconverted (encoded) into unl expression that can then be used as pivot in variety of possible applications (multilingual information retrieval, automatic translation, etc.).enconverting into unl is thus to be understood as the process by which unl expression is generated from the analysis of natural language utterance.
</prevsent>
</prevsection>
<citsent citstr=" C00-2111 ">
this process can be carried out by different strategies, ranging from fully automatic to fully human enconverting.within the unl project, number of software tools exist for different languages, mainly dictionnaries and deconverters (for french(serasset and boitet, 2000), <papid> C00-2111 </papid>for tamil (dhan abalan and geeta, 2003), etc.).</citsent>
<aftsection>
<nextsent>however, there are few tools for enconversion (for german (hong and streiter, 1999), for spanish1, etc.).
</nextsent>
<nextsent>as they are not full automatic enconverters, these systems have not yet proved to be suitable for dealing with huge amounts of heterogeneous data.for french, there is currently version under development of an enconverter that uses theariane-g5 platform (boitet et al, 1982), <papid> C82-1004 </papid>an environment for multilingual machine translation, for the analysis of the natural language input.</nextsent>
<nextsent>however, this approach has several drawbacks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1811">
<title id=" W04-2007.xml">using an incremental robust parser to automatically generate semantic unl graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this process can be carried out by different strategies, ranging from fully automatic to fully human enconverting.within the unl project, number of software tools exist for different languages, mainly dictionnaries and deconverters (for french(serasset and boitet, 2000), <papid> C00-2111 </papid>for tamil (dhan abalan and geeta, 2003), etc.).</prevsent>
<prevsent>however, there are few tools for enconversion (for german (hong and streiter, 1999), for spanish1, etc.).</prevsent>
</prevsection>
<citsent citstr=" C82-1004 ">
as they are not full automatic enconverters, these systems have not yet proved to be suitable for dealing with huge amounts of heterogeneous data.for french, there is currently version under development of an enconverter that uses theariane-g5 platform (boitet et al, 1982), <papid> C82-1004 </papid>an environment for multilingual machine translation, for the analysis of the natural language input.</citsent>
<aftsection>
<nextsent>however, this approach has several drawbacks.
</nextsent>
<nextsent>first, the size of the linguistic input that it can process is limited to 200-250 words.
</nextsent>
<nextsent>second, the output produced contains all the possible complete linguistic analysis for sentence (multi ple syntactic and logico-semantic trees).
</nextsent>
<nextsent>this implies an interactive disambiguation step to choose the appropriate linguistic analysis for theenconverter.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1812">
<title id=" W04-2007.xml">using an incremental robust parser to automatically generate semantic unl graphs </title>
<section> an incremental robust parser.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 overview of the parser.
</prevsent>
<prevsent>xip (ait-mokhtar et al, 2002; hagege androux, 2002) is rule-based platform for building robust incremental parsers.
</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
it is deve lopped at the xerox research centre europe(xrce) and shares the same computation nal paradigm as the pnlpl approach (jensen, 1992) and the fdgp approach (tapanainen and jarvinen, 1997).<papid> A97-1011 </papid></citsent>
<aftsection>
<nextsent>at present, various grammars for xip have been built for english and french.
</nextsent>
<nextsent>the different phases of linguistic processing are organized incrementally : syntactic analysis is done by first chunking (abney, 1991) morphosyntactic annotated input text and then extracting func tionnal dependencies (links between the words).
</nextsent>
<nextsent>the aim of the system is to produce list of syntactic dependencies which may be later usedin applications such as information retrieval, semantic disambiguation, coreference resolution, etc. 3.2 incremental approach.
</nextsent>
<nextsent>a xip parser, like the french parser (that wewill call xipf hereafter), is composed of different modules that transform and process incrementally the linguistic information given as in put.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1813">
<title id=" W04-2407.xml">memory based dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>deterministic parsing means that we always derive single analysis for each input string.
</prevsent>
<prevsent>moreover, this single analysis is derived in monotonic fashion with no redundancy or backtracking, which makes it possible to parse natural language sentences in linear time (nivre, 2003).in this paper, we report experiments using memory based learning (daelemans, 1999) to guide the parser described in nivre (2003), using data from small treebank of swedish (einarsson, 1976).
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
unlike most previous work on data-driven dependency parsing (eisner, 1996; <papid> C96-1058 </papid>collins et al, 1999; <papid> P99-1065 </papid>yamada and matsumoto, 2003;nivre, 2003), we assume that dependency graphs are labeled with dependency types, although the evaluation will give results for both labeled and unlabeled represen tations.</citsent>
<aftsection>
<nextsent>the paper is structured as follows.
</nextsent>
<nextsent>section 2 gives the necessary background definitions and introduces the idea of guided parsing as well as memory-based learning.
</nextsent>
<nextsent>section 3 describes the data used in the experiments, the evaluation metrics, and the models and algorithms used in the learning process.
</nextsent>
<nextsent>results from the experiments are given in section 4, while conclusions and suggestions for further research are presented in section 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1816">
<title id=" W04-2407.xml">memory based dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>deterministic parsing means that we always derive single analysis for each input string.
</prevsent>
<prevsent>moreover, this single analysis is derived in monotonic fashion with no redundancy or backtracking, which makes it possible to parse natural language sentences in linear time (nivre, 2003).in this paper, we report experiments using memory based learning (daelemans, 1999) to guide the parser described in nivre (2003), using data from small treebank of swedish (einarsson, 1976).
</prevsent>
</prevsection>
<citsent citstr=" P99-1065 ">
unlike most previous work on data-driven dependency parsing (eisner, 1996; <papid> C96-1058 </papid>collins et al, 1999; <papid> P99-1065 </papid>yamada and matsumoto, 2003;nivre, 2003), we assume that dependency graphs are labeled with dependency types, although the evaluation will give results for both labeled and unlabeled represen tations.</citsent>
<aftsection>
<nextsent>the paper is structured as follows.
</nextsent>
<nextsent>section 2 gives the necessary background definitions and introduces the idea of guided parsing as well as memory-based learning.
</nextsent>
<nextsent>section 3 describes the data used in the experiments, the evaluation metrics, and the models and algorithms used in the learning process.
</nextsent>
<nextsent>results from the experiments are given in section 4, while conclusions and suggestions for further research are presented in section 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1818">
<title id=" W04-2407.xml">memory based dependency parsing </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 data.
</prevsent>
<prevsent>it is standard practice in data-driven approaches to natural language parsing to use treebanks both for training and evaluation.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
thus, the penn treebank of american english (marcus et al, 1993) <papid> J93-2004 </papid>has been used to train and evaluate the best available parsers of unrestricted english text (collins, 1999; charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>one problem when developing parser for swedish is that there is no comparable large-scale treebank available for swedish.
</nextsent>
<nextsent>for the experiments reported in this paper we have used manually annotated corpus of written swedish, created at lund university in the 1970s and consisting mainly of informative texts from official sources (einars son, 1976).
</nextsent>
<nextsent>although the original annotation scheme isan eclectic combination of constituent structure, dependency structure, and topological fields (teleman, 1974), it has proven possible to convert the annotated sentences to dependency graphs with fairly high accuracy.in the conversion process, we have reduced the original fine-grained classification of grammatical functions to more restricted set of 16 dependency types, which are listed in table 2.
</nextsent>
<nextsent>we have also replaced the original (manual) part-of-speech annotation by using the same automatic tagger that is used for preprocessing in the parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1819">
<title id=" W04-2407.xml">memory based dependency parsing </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 data.
</prevsent>
<prevsent>it is standard practice in data-driven approaches to natural language parsing to use treebanks both for training and evaluation.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
thus, the penn treebank of american english (marcus et al, 1993) <papid> J93-2004 </papid>has been used to train and evaluate the best available parsers of unrestricted english text (collins, 1999; charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>one problem when developing parser for swedish is that there is no comparable large-scale treebank available for swedish.
</nextsent>
<nextsent>for the experiments reported in this paper we have used manually annotated corpus of written swedish, created at lund university in the 1970s and consisting mainly of informative texts from official sources (einars son, 1976).
</nextsent>
<nextsent>although the original annotation scheme isan eclectic combination of constituent structure, dependency structure, and topological fields (teleman, 1974), it has proven possible to convert the annotated sentences to dependency graphs with fairly high accuracy.in the conversion process, we have reduced the original fine-grained classification of grammatical functions to more restricted set of 16 dependency types, which are listed in table 2.
</nextsent>
<nextsent>we have also replaced the original (manual) part-of-speech annotation by using the same automatic tagger that is used for preprocessing in the parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1834">
<title id=" W04-1506.xml">towards a dependency parser for basque </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>syntactic processing for basque.
</prevsent>
<prevsent>the dependency parser has been performed in order to improve the syntactic analysis achieved so far, in the sense that, apart from the surface structural properties, we have added information about deeper structures by expressing the relation between the head and the dependent in an explicit manner.
</prevsent>
</prevsection>
<citsent citstr=" J03-4001 ">
additionally, we have adopted solutions to overcome problems that have emerged in doing this analysis (such as discontinuous constituents, subordinate clauses, etc. this approach has been used in several projects (jrvinen &amp; tapanainen, 1998; oflazer, 2003).<papid> J03-4001 </papid></citsent>
<aftsection>
<nextsent>before carrying out the definition of the grammar for the parser, we established the syntactic tagging system in linguistic terms.
</nextsent>
<nextsent>we simultaneously have applied it to build the treebank for basque (eus3lb1) (aduriz et al, 2003) as well as to define the dependency grammar.
</nextsent>
<nextsent>the treebank would serve to evaluate and improve the dependency parser.
</nextsent>
<nextsent>this will enable us to check how robust our grammar is. the dependency syntactic tagging system is based on the framework presented in carroll et al., (1998),  (1999): each sentence in the corpus is marked up with set of grammatical relations (grs), specifying the syntactic dependency which holds between each head and its dependent(s).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1835">
<title id=" W04-1506.xml">towards a dependency parser for basque </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these are the principal features that characterize the basque language and, obviously, they have influenced us critically in our decision: 1.
</prevsent>
<prevsent>the dependency-based formalism is the one.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
that could best deal with the free word order displayed by basque syntax (skut et al, 1997).<papid> A97-1014 </papid></citsent>
<aftsection>
<nextsent>developed so far in our group facilitate either achieving dependency relations or transforming from dependency-trees to other modes of representation.
</nextsent>
<nextsent>3.
</nextsent>
<nextsent>from our viewpoint, it is less messy to.
</nextsent>
<nextsent>evaluate the relation between the elements that compose sentence rather than the relation of elements included in parenthesis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1836">
<title id=" W04-1506.xml">towards a dependency parser for basque </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>3 overview of the syntactic processing.
</prevsent>
<prevsent>of basque: from shallow parsing to deep parsing we face the creation of robust syntactic analyser by implementing it in sequential rule layers.
</prevsent>
</prevsection>
<citsent citstr=" A94-1008 ">
in most of the cases, these layers are realized in grammars defined by the constraint grammar formalism (karlsson et al , 1995; tapanainen &amp; voutilainen, 1994).<papid> A94-1008 </papid></citsent>
<aftsection>
<nextsent>each analysis layer uses the output of the previous layer as its input and enriches it with further information.
</nextsent>
<nextsent>rule layers are grouped into modules depending on the level of depth of their analysis.
</nextsent>
<nextsent>modularity helps to maintain linguistic data and makes the system easily customisable or reusable.
</nextsent>
<nextsent>figure 1 shows the architecture of the system, for more details, see aduriz et al, 2004.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1837">
<title id=" W04-2404.xml">combining lexical and syntactic features for supervised word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the training data consists of sentences which have potential target words tagged by human expert with their intended sense.
</prevsent>
<prevsent>numerous learning algorithms, such as, naive bayesian classifiers, decision trees and neural networks have been used tolearn models of disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" N01-1011 ">
however, both (peder sen, 2001<papid> N01-1011 </papid>a) and (lee and ng, 2002) <papid> W02-1006 </papid>suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed.previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (mcroy, 1992), (<papid> J92-1001 </papid>ng and lee, 1996), (<papid> P96-1006 </papid>stevenson and wilks, 2001), (<papid> J01-3001 </papid>yarowsky and florian, 2002)).</citsent>
<aftsection>
<nextsent>however, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are.
</nextsent>
<nextsent>in this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble.we find that simple combinations of lexical and syntactic features can result in very high disambiguation accuracy, via an extensive set of experiments using thesenseval-1, senseval-2, line, hard, serve and interest data.
</nextsent>
<nextsent>together, this consists of more than 50,000sense-tagged instances.
</nextsent>
<nextsent>this paper also introduces technique to quantify the optimum gain that is theoretically possible when two feature sets are combined in an ensemble.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1838">
<title id=" W04-2404.xml">combining lexical and syntactic features for supervised word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the training data consists of sentences which have potential target words tagged by human expert with their intended sense.
</prevsent>
<prevsent>numerous learning algorithms, such as, naive bayesian classifiers, decision trees and neural networks have been used tolearn models of disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" W02-1006 ">
however, both (peder sen, 2001<papid> N01-1011 </papid>a) and (lee and ng, 2002) <papid> W02-1006 </papid>suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed.previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (mcroy, 1992), (<papid> J92-1001 </papid>ng and lee, 1996), (<papid> P96-1006 </papid>stevenson and wilks, 2001), (<papid> J01-3001 </papid>yarowsky and florian, 2002)).</citsent>
<aftsection>
<nextsent>however, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are.
</nextsent>
<nextsent>in this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble.we find that simple combinations of lexical and syntactic features can result in very high disambiguation accuracy, via an extensive set of experiments using thesenseval-1, senseval-2, line, hard, serve and interest data.
</nextsent>
<nextsent>together, this consists of more than 50,000sense-tagged instances.
</nextsent>
<nextsent>this paper also introduces technique to quantify the optimum gain that is theoretically possible when two feature sets are combined in an ensemble.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1840">
<title id=" W04-2404.xml">combining lexical and syntactic features for supervised word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the training data consists of sentences which have potential target words tagged by human expert with their intended sense.
</prevsent>
<prevsent>numerous learning algorithms, such as, naive bayesian classifiers, decision trees and neural networks have been used tolearn models of disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" J92-1001 ">
however, both (peder sen, 2001<papid> N01-1011 </papid>a) and (lee and ng, 2002) <papid> W02-1006 </papid>suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed.previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (mcroy, 1992), (<papid> J92-1001 </papid>ng and lee, 1996), (<papid> P96-1006 </papid>stevenson and wilks, 2001), (<papid> J01-3001 </papid>yarowsky and florian, 2002)).</citsent>
<aftsection>
<nextsent>however, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are.
</nextsent>
<nextsent>in this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble.we find that simple combinations of lexical and syntactic features can result in very high disambiguation accuracy, via an extensive set of experiments using thesenseval-1, senseval-2, line, hard, serve and interest data.
</nextsent>
<nextsent>together, this consists of more than 50,000sense-tagged instances.
</nextsent>
<nextsent>this paper also introduces technique to quantify the optimum gain that is theoretically possible when two feature sets are combined in an ensemble.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1841">
<title id=" W04-2404.xml">combining lexical and syntactic features for supervised word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the training data consists of sentences which have potential target words tagged by human expert with their intended sense.
</prevsent>
<prevsent>numerous learning algorithms, such as, naive bayesian classifiers, decision trees and neural networks have been used tolearn models of disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
however, both (peder sen, 2001<papid> N01-1011 </papid>a) and (lee and ng, 2002) <papid> W02-1006 </papid>suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed.previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (mcroy, 1992), (<papid> J92-1001 </papid>ng and lee, 1996), (<papid> P96-1006 </papid>stevenson and wilks, 2001), (<papid> J01-3001 </papid>yarowsky and florian, 2002)).</citsent>
<aftsection>
<nextsent>however, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are.
</nextsent>
<nextsent>in this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble.we find that simple combinations of lexical and syntactic features can result in very high disambiguation accuracy, via an extensive set of experiments using thesenseval-1, senseval-2, line, hard, serve and interest data.
</nextsent>
<nextsent>together, this consists of more than 50,000sense-tagged instances.
</nextsent>
<nextsent>this paper also introduces technique to quantify the optimum gain that is theoretically possible when two feature sets are combined in an ensemble.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1842">
<title id=" W04-2404.xml">combining lexical and syntactic features for supervised word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the training data consists of sentences which have potential target words tagged by human expert with their intended sense.
</prevsent>
<prevsent>numerous learning algorithms, such as, naive bayesian classifiers, decision trees and neural networks have been used tolearn models of disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" J01-3001 ">
however, both (peder sen, 2001<papid> N01-1011 </papid>a) and (lee and ng, 2002) <papid> W02-1006 </papid>suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed.previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (mcroy, 1992), (<papid> J92-1001 </papid>ng and lee, 1996), (<papid> P96-1006 </papid>stevenson and wilks, 2001), (<papid> J01-3001 </papid>yarowsky and florian, 2002)).</citsent>
<aftsection>
<nextsent>however, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are.
</nextsent>
<nextsent>in this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble.we find that simple combinations of lexical and syntactic features can result in very high disambiguation accuracy, via an extensive set of experiments using thesenseval-1, senseval-2, line, hard, serve and interest data.
</nextsent>
<nextsent>together, this consists of more than 50,000sense-tagged instances.
</nextsent>
<nextsent>this paper also introduces technique to quantify the optimum gain that is theoretically possible when two feature sets are combined in an ensemble.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1847">
<title id=" W04-2404.xml">combining lexical and syntactic features for supervised word sense disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the system was evaluated using the interest data on which it achieved an accuracy of 87.3%.
</prevsent>
<prevsent>they studied the utility of individual features and found collocation sto be most useful, followed by part of speech and morphological form.
</prevsent>
</prevsection>
<citsent citstr=" P97-1009 ">
(lin, 1997) <papid> P97-1009 </papid>takes supervised approach that is unique as it did not create classifier for every target word.</citsent>
<aftsection>
<nextsent>the system compares the context of the target word with that of training instances which are similar to it.
</nextsent>
<nextsent>the sense of the target word most similar to these contexts is chose nas the intended sense.
</nextsent>
<nextsent>similar to mcroy, the system attempts to disambiguate all words in the text.
</nextsent>
<nextsent>lin relies on syntactic relations, such as, subject-verb agreement and verb object relations to capture the context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1852">
<title id=" W04-2404.xml">combining lexical and syntactic features for supervised word sense disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>experiments on senseval-2 data revealed that the exclusion of any of the three kinds of features resulted insignificant drop inaccuracy.
</prevsent>
<prevsent>lee and ng as well as yarowsky and florian conclude that the combination of features is beneficial.
</prevsent>
</prevsection>
<citsent citstr=" W02-0806 ">
(pedersen, 2002) <papid> W02-0806 </papid>does pairwise study of the systems that participated in senseval-2 english and spanish disambiguation exercises.</citsent>
<aftsection>
<nextsent>the study approaches the systems as black boxes, looking only at the assigned tags whatever the classifier and sources of information may be.
</nextsent>
<nextsent>he introduces measures to determine the similarity of the classifications and optimum results obtainable by combining the systems.
</nextsent>
<nextsent>he points out that pairs of systems having low similarity and high optimal accuracies are of interest as they are markedly complementary and the combination of such systems is beneficial.there still remain questions regarding the use of multiple sources of information, in particular which features should be combined and what is the upper bound on the accuracies achievable by such combinations.
</nextsent>
<nextsent>(pedersen, 2002) <papid> W02-0806 </papid>describes how to determine the upper bound when combining two systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1856">
<title id=" W04-1907.xml">the holj corpus supporting summarisation of legal texts </title>
<section> rhetorical status annotation.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 manual annotation of rhetorical status.
</prevsent>
<prevsent>the manual annotation of rhetorical roles is work in progress and so far we have 40 documents fully annotated.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
the frequency figures in table 1 are taken from this manually annotated subset of the corpus and the classifiers described in section 2.3 have been trained and evaluated on the same subset.this subset of the corpus is similar in size to the corpus reported in (teufel and moens, 2002): <papid> J02-4002 </papid>the t&m; corpus consists of 80 conference articles while ours consists of 40 holj documents.</citsent>
<aftsection>
<nextsent>the t&m; corpus label freq.
</nextsent>
<nextsent>description fact 862 the sentence recounts the events or circumstances which gave rise (8.5%) to legal proceedings.
</nextsent>
<nextsent>e.g. on analysis the package was found to contain 152 milligrams of heroin at 100% purity.
</nextsent>
<nextsent>proceedings 2434 the sentence describes legal proceedings taken in the lower courts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1857">
<title id=" W04-1907.xml">the holj corpus supporting summarisation of legal texts </title>
<section> rhetorical status annotation.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 experiments with rhetorical role.
</prevsent>
<prevsent>classification using the manually annotated subset of the corpus we have performed number of preliminary experiments to determine which classifier and which feature set would be appropriate for rhetorical role classification.
</prevsent>
</prevsection>
<citsent citstr=" W04-1007 ">
a brief summary of the micro-averaged f-score3 results is given in table 2 (detailed results in hachey and grover, 2004).<papid> W04-1007 </papid>the features with which we have been experimenting for the holj domain are broadly similar3micro-averaging weights categories by their prior probability as opposed to macro-averaging which puts equal weight on each class regardless of how sparsely populated it might be.</citsent>
<aftsection>
<nextsent>classifier features f-score c4.5 65.4 nb lcesq 51.8 winnow lcesqt 41.4 svm lcesqt 60.6table 2: micro-averaged f-score results for rhetorical classification to those used by t&m; and include many of the features which are typically used in sentence extraction approaches to automatic summarisation as well as certain other features developed specifically for rhetorical role classification.
</nextsent>
<nextsent>briefly, the feature set includes such features as: (l) location of sentence within the document and its subsections and para graphs; (c) cue phrases; (e) whether the sentence contains named entities; (s) sentence length; (t) average tf   idf term weight; and (q) whether the sentence contains quotation or is inside block quote.
</nextsent>
<nextsent>while we still expect to achieve gains over these preliminary scores, our system already exhibits an improvement over baseline similar to that achieved by the t&m; system, which is encouraging given that we have not invested any time in developing the hand-crafted cue phrases that proved the most useful feature for t&m;, but rather have attempted to simulate these through fully automatic, largely domain-independent linguistic information.we plan further experiments to investigate the effect of other cue phrase features.
</nextsent>
<nextsent>for example, subject and main verb hypernyms should allow better holxml conversion to document html automatically annotated holxml document recognition named entity identification chunking &amp; clause verb &amp; subject features sation lemmati?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1858">
<title id=" W04-1907.xml">the holj corpus supporting summarisation of legal texts </title>
<section> automatic linguistic markup.  </section>
<citcontext>
<prevsection>
<prevsent>in the token isation module we convert from the source html to holxml and then pass the data through sequence of calls to variety of xml based tools from the lt ttt and lt xml tool sets (grover et al , 2000; thompson et al , 1997).
</prevsent>
<prevsent>the core program is the lt ttt program fsgmatch, agen eral purpose transducer which processes an input stream and adds annotations using rules provided in hand-written grammar file.
</prevsent>
</prevsection>
<citsent citstr=" J97-3003 ">
the other main lt ttt program is ltpos, statistical combined part-of-speech (pos) tagger and sentence boundary disambiguation module (mikheev, 1997).<papid> J97-3003 </papid></citsent>
<aftsection>
<nextsent>the first step in the token isation modules uses fsgmatch to segment the contents of the paragraphs into word elements.
</nextsent>
<nextsent>once the word tokens have been identified, the next step uses ltpos to mark up the sentences and add part of speech attributes to word tokens.the motivation for the module that performs further linguistic analysis is to compute information tobe used to provide features for the sentence classifier.
</nextsent>
<nextsent>however, the information we compute is general purpose and makes the data useful for range of nlp research activities.
</nextsent>
<nextsent>the first step in the linguistic analysis module lemmatises the inflected words using minnen et al s(2000) morpha lemmatiser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1859">
<title id=" W04-1907.xml">the holj corpus supporting summarisation of legal texts </title>
<section> automatic linguistic markup.  </section>
<citcontext>
<prevsection>
<prevsent>table 3 shows examples of the entities we have marked up in the corpus (in our annotation scheme these are noun groups (ng) with specific type and subtype attributes).
</prevsent>
<prevsent>in thetop two blocks of the table are examples of domain specific entities such as courts, judges, acts and judgments, while in the third block we show examples of non-domain-specific entity types.
</prevsent>
</prevsection>
<citsent citstr=" W03-0424 ">
we use different strategies for the identification of the two classes of entities: for the domain-specific ones weuse hand-crafted lt ttt rules, while for the nondomain-specific ones we use the c&c; named entity tagger (curran and clark, 2003) <papid> W03-0424 </papid>trained on themuc-7 data set.</citsent>
<aftsection>
<nextsent>for some entities, the two approaches provide competing analyses, in which case the domain-specific label is to be preferred since it provides finer-grained information.
</nextsent>
<nextsent>wherever there is no competition, c&c; entities are marked up and labelled as subtype=fromcc?).
</nextsent>
<nextsent>during the rule-based entity recognition phase, an on-the-fly?
</nextsent>
<nextsent>lexicon is built from the documentheader.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1862">
<title id=" W04-2602.xml">towards full automation of lexicon construction </title>
<section> co-clustering to define surrogate.  </section>
<citcontext>
<prevsection>
<prevsent>many current taggers are tuned to relatively formal corpora, such as newswire, while many interesting domains, such as email, netnews, or physicians?
</prevsent>
<prevsent>notes, are replete with eli sions, jargon, and neologisms.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
fortunately, using distributional characteristics of term contexts, it is feasible to induce part-of-speech categories directly from corpus of sufficient size, as several papers have made clear (brown et al, 1992; <papid> J92-4003 </papid>schutze, 1993; clark, 2000).<papid> W00-0717 </papid></citsent>
<aftsection>
<nextsent>distributional information has uses beyond part of speech induction.
</nextsent>
<nextsent>for example, it is possible to augmenta fixed syntactic or semantic taxonomy with such information to good effect (hearst and schutze, 1993).
</nextsent>
<nextsent>our objective is, where possible, to work directly with the inferred syntactic categories and their underlying distributions.
</nextsent>
<nextsent>there are many applications of computational linguistics, particularly those involving shallow?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1863">
<title id=" W04-2602.xml">towards full automation of lexicon construction </title>
<section> co-clustering to define surrogate.  </section>
<citcontext>
<prevsection>
<prevsent>many current taggers are tuned to relatively formal corpora, such as newswire, while many interesting domains, such as email, netnews, or physicians?
</prevsent>
<prevsent>notes, are replete with eli sions, jargon, and neologisms.
</prevsent>
</prevsection>
<citsent citstr=" W00-0717 ">
fortunately, using distributional characteristics of term contexts, it is feasible to induce part-of-speech categories directly from corpus of sufficient size, as several papers have made clear (brown et al, 1992; <papid> J92-4003 </papid>schutze, 1993; clark, 2000).<papid> W00-0717 </papid></citsent>
<aftsection>
<nextsent>distributional information has uses beyond part of speech induction.
</nextsent>
<nextsent>for example, it is possible to augmenta fixed syntactic or semantic taxonomy with such information to good effect (hearst and schutze, 1993).
</nextsent>
<nextsent>our objective is, where possible, to work directly with the inferred syntactic categories and their underlying distributions.
</nextsent>
<nextsent>there are many applications of computational linguistics, particularly those involving shallow?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1864">
<title id=" W04-2602.xml">towards full automation of lexicon construction </title>
<section> co-clustering to define surrogate.  </section>
<citcontext>
<prevsection>
<prevsent>our objective is, where possible, to work directly with the inferred syntactic categories and their underlying distributions.
</prevsent>
<prevsent>there are many applications of computational linguistics, particularly those involving shallow?
</prevsent>
</prevsection>
<citsent citstr=" W01-0713 ">
processing, such as information extraction, which can benefit from such automatically derived information, especially as research into acquisition of grammar matures (e.g., (clark, 2001)).<papid> W01-0713 </papid></citsent>
<aftsection>
<nextsent>2.1 the co-clustering algorithm..
</nextsent>
<nextsent>our approach to inducing syntactic clusters is closely related to that described in brown, et al (1992) <papid> J92-4003 </papid>which is one of the earliest papers on the subject.</nextsent>
<nextsent>we seek to find partition of the vocabulary that maximizes the mutual information between term categories and their contexts.we achieve this in the framework of information theoretic co-clustering (dhillon et al, 2003), in which space of entities, on the one hand, and their contexts, on the other, are alternately clustered in way that maximizes mutual information between the two spaces.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1871">
<title id=" W04-2602.xml">towards full automation of lexicon construction </title>
<section> refinements.  </section>
<citcontext>
<prevsection>
<prevsent>clearly, in order for our automatically constructed resource to be useful, we must introduce these uncovered terms into the lexicon, or better still, find way to apply it to individual novel tokens.
</prevsent>
<prevsent>3.1.1 hmm tagging in light of the current state of the art in part of speech tagging, the occurrence of these unknown terms does not pose significant problem.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
it has been known for some years that good performance can be realized with partial tagging and hidden markov model (cutting et al, 1992).<papid> A92-1018 </papid></citsent>
<aftsection>
<nextsent>note that the notion of partial tagging described in cutting, et al is essentially different from what we consider here.
</nextsent>
<nextsent>whereas they assume lexicon which, for every term in the vocabulary, lists its possible parts of speech, we construct lexicon which imposes single sense (or few senses; see section 3.2) on each of thefew thousand most frequent terms, but provides no information about other terms.as in cutting, et al however, we can use baum welch re-estimation to extract information from novel terms, and apply the viterbi algorithm to dispose of particular occurrence.
</nextsent>
<nextsent>while the literature suggests that baum-welch training can degrade performance on the tagging task (elworthy, 1994; <papid> A94-1009 </papid>merialdo, 1994), <papid> J94-2001 </papid>we have found in early experiments that agreement between tagger trained in this way and the tagger from the xtag project consistently increases with each iteration of baum-welch, eventually reaching plateau, but not decreasing.</nextsent>
<nextsent>we attribute this discrepancy to the different structure of our problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1872">
<title id=" W04-2602.xml">towards full automation of lexicon construction </title>
<section> refinements.  </section>
<citcontext>
<prevsection>
<prevsent>note that the notion of partial tagging described in cutting, et al is essentially different from what we consider here.
</prevsent>
<prevsent>whereas they assume lexicon which, for every term in the vocabulary, lists its possible parts of speech, we construct lexicon which imposes single sense (or few senses; see section 3.2) on each of thefew thousand most frequent terms, but provides no information about other terms.as in cutting, et al however, we can use baum welch re-estimation to extract information from novel terms, and apply the viterbi algorithm to dispose of particular occurrence.
</prevsent>
</prevsection>
<citsent citstr=" A94-1009 ">
while the literature suggests that baum-welch training can degrade performance on the tagging task (elworthy, 1994; <papid> A94-1009 </papid>merialdo, 1994), <papid> J94-2001 </papid>we have found in early experiments that agreement between tagger trained in this way and the tagger from the xtag project consistently increases with each iteration of baum-welch, eventually reaching plateau, but not decreasing.</citsent>
<aftsection>
<nextsent>we attribute this discrepancy to the different structure of our problem.
</nextsent>
<nextsent>3.1.2 lexicon expansion note that hmm is under no constraint to handle given term inconsistent fashion.
</nextsent>
<nextsent>a single model can and often does assign single term to multiple classes, even in single document.
</nextsent>
<nextsent>when term is sufficiently frequent, more robust approach may be to assign it to category using only its summary co-occurrence statistics.the idea is straightforward: create an entry in the lexicon for the novel term and measure the change in mutual information associated with assigning it to each of the term freq.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1873">
<title id=" W04-2602.xml">towards full automation of lexicon construction </title>
<section> refinements.  </section>
<citcontext>
<prevsection>
<prevsent>note that the notion of partial tagging described in cutting, et al is essentially different from what we consider here.
</prevsent>
<prevsent>whereas they assume lexicon which, for every term in the vocabulary, lists its possible parts of speech, we construct lexicon which imposes single sense (or few senses; see section 3.2) on each of thefew thousand most frequent terms, but provides no information about other terms.as in cutting, et al however, we can use baum welch re-estimation to extract information from novel terms, and apply the viterbi algorithm to dispose of particular occurrence.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
while the literature suggests that baum-welch training can degrade performance on the tagging task (elworthy, 1994; <papid> A94-1009 </papid>merialdo, 1994), <papid> J94-2001 </papid>we have found in early experiments that agreement between tagger trained in this way and the tagger from the xtag project consistently increases with each iteration of baum-welch, eventually reaching plateau, but not decreasing.</citsent>
<aftsection>
<nextsent>we attribute this discrepancy to the different structure of our problem.
</nextsent>
<nextsent>3.1.2 lexicon expansion note that hmm is under no constraint to handle given term inconsistent fashion.
</nextsent>
<nextsent>a single model can and often does assign single term to multiple classes, even in single document.
</nextsent>
<nextsent>when term is sufficiently frequent, more robust approach may be to assign it to category using only its summary co-occurrence statistics.the idea is straightforward: create an entry in the lexicon for the novel term and measure the change in mutual information associated with assigning it to each of the term freq.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1876">
<title id=" W04-2326.xml">annotating student emotional states in spoken tutoring dialogues </title>
<section> analysis of the annotation scheme.  </section>
<citcontext>
<prevsection>
<prevsent>the rows correspond to the labels assigned by annotator 1, and the columns correspond to the labels assigned by annotator 2.
</prevsent>
<prevsent>for example, 90 negatives were agreed upon by both annotators, while6 negatives assigned by annotator 1 were labeled as neutral by annotator 2.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
the two annotators agreed on the annotations of 385/453 turns, achieving 84.99% agreement(kappa = 0.68 (carletta, 1996)).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>such agreement is expected given the difficulty of the task, and exceeds thatof prior studies of emotion annotation in naturally occur ring speech; (ang et al, 2002), for example, achieved agreement of 71% (kappa 0.47), while (lee et al, 2001) averaged around 70% agreement.
</nextsent>
<nextsent>as in (lee et al, 2001), we next performed machine learning experiment on the 385 student turns where thetwo annotators agreed on the emotion label.
</nextsent>
<nextsent>our predictive accuracy for this data was 84.75% (using 10 10cross-validation as in (litman and forbes, 2003)).
</nextsent>
<nextsent>compared to baseline accuracy of 72.74% achieved by always predicting the majority (neutral) class, our result yields relative improvement of 44.06%.5 negative neutral positive negative 90 6 4 neutral 23 280 30 positive 0 5 15 table 1: confusion matrix 1: minor neutral 5relative improvement of over = fffifl ffi , where error(x) is 100 - %accuracy(x).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1877">
<title id=" W03-2128.xml">developing a typology of dialogue acts some boundary problems </title>
<section> principles of classification.  </section>
<citcontext>
<prevsection>
<prevsent>secondly, the act system must make it possible to differentiate functions.
</prevsent>
<prevsent>thirdly, the typology must make it possible to differentiate utterances with the same linguistic realisation but with different functions.
</prevsent>
</prevsection>
<citsent citstr=" J00-3003 ">
there are several well-known typologies of dialogue acts (sinclair, coulthard 1975, francis, hun ston 1992, stenstrm 1994, bunt 1999, stolcke et al  2000, <papid> J00-3003 </papid>dybkjr 2000, allwood et al 2001, mengel et al 2001).</citsent>
<aftsection>
<nextsent>we have decided to develop our own dialogue act system because no typology seemed to fully correspond to our needs.
</nextsent>
<nextsent>the principles underlying our typology are the same as for other coding schemes (edwards 1995).
</nextsent>
<nextsent>three types of principles are used: 1) category design, 2) readability, 3) computer manipulation.
</nextsent>
<nextsent>the first type is important for our current study.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1878">
<title id=" W04-0903.xml">constructing text sense representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the analyzed text is matched against the frame database in order to construct text meaning representations.
</prevsent>
<prevsent>an example of this approach is presented by clark et. al.
</prevsent>
</prevsection>
<citsent citstr=" W03-0901 ">
(p. clark et al, 2003).<papid> W03-0901 </papid>dahlgren et. al. present kt?, complex text understanding system based on naive semantics.</citsent>
<aftsection>
<nextsent>kt also uses frames to represent semantic content.a project that is based on roughly similar notion of text meaning representation (tmr) concept sis the kosmos project (mahesh, 1996; kavi mahesh and sergei nirenburg, 1996).
</nextsent>
<nextsent>it is aimed at the creation of machine translation system that uses broad-coverage ontology and various input sources in order to translate english to spanish texts and viceversa.
</nextsent>
<nextsent>tmr concepts within kosmos are hand written frame-based data structures.
</nextsent>
<nextsent>text meaning is represented by instances thereof that are derived by semantic rules from linguistic rule database.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1879">
<title id=" W04-0102.xml">non locality all the way through emergent global constraints in the italian morphological lexicon </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>before we go into that, we briefly overview relevant machine learning work from this perspective.
</prevsent>
<prevsent>barcelona, july 2004 association for computations linguistics acl special interest group on computational phonology (sigphon) proceedings of the workshop of the
</prevsent>
</prevsection>
<citsent citstr=" C94-1037 ">
lazy learning methods such as the nearest neighbour algorithm (van den bosch et al, 1996) or the analogy-based approach (pirrelli and federici, 1994; <papid> C94-1037 </papid>pirrelli and yvon, 1999) require full storage of supervised data, and make on-line use of them with no prior or posterior lexical struc turing.</citsent>
<aftsection>
<nextsent>this makes this class of algorithms flexible and efficient, but comparatively noise-sensitive and rather poor in simulating emergent learning phenomena.
</nextsent>
<nextsent>there is no explicit sense in which the system learns how to map new exemplars to already memorised ones, since the mapping function does not change through time and the only incremental pay-off lies in the growing quantity of information stored in the exemplar data-base.
</nextsent>
<nextsent>decision tree algorithms (quinlan, 1986), on the other hand, try to build the shortest hierarchical structure that best classifies the training data, using greedy heuristics to select the most discriminative attributes near the root of the hierarchy.
</nextsent>
<nextsent>as heuristics are based on locally optimal splitting of all training data, adding new training data may lead to dramatic reorganisation of the hierarchy, and nothing is explicitly learned from having built decision tree at previous learning stage (ling and marinov, 1993).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1880">
<title id=" W04-0102.xml">non locality all the way through emergent global constraints in the italian morphological lexicon </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>decision tree algorithms (quinlan, 1986), on the other hand, try to build the shortest hierarchical structure that best classifies the training data, using greedy heuristics to select the most discriminative attributes near the root of the hierarchy.
</prevsent>
<prevsent>as heuristics are based on locally optimal splitting of all training data, adding new training data may lead to dramatic reorganisation of the hierarchy, and nothing is explicitly learned from having built decision tree at previous learning stage (ling and marinov, 1993).
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
to tackle the issue of word structure more squarely, there has been recent upsurge of interest in global paradigm-based constraints on morphology learning, as way to minimise the range of inflectional or derivational endings heuristic ally inferred from raw training data (goldsmith, 2001; <papid> J01-2001 </papid>gaussier, 1999; <papid> W99-0904 </papid>baroni, 2000).</citsent>
<aftsection>
<nextsent>it should be noted, however, that global, linguistically-inspired constraints of this sort do not interact with morphology learning in any direct way.
</nextsent>
<nextsent>rather, they are typically used as global criteria for optimal convergence on an existing repertoire of minimally redundant sets of paradigmatically related morphemes.
</nextsent>
<nextsent>candidate morpheme-like units are acquired independently of paradigm-based constraints, solely on the basis of local heuristics.
</nextsent>
<nextsent>once more, there is no clear sense in which global constraints form integral part of learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1881">
<title id=" W04-0102.xml">non locality all the way through emergent global constraints in the italian morphological lexicon </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>decision tree algorithms (quinlan, 1986), on the other hand, try to build the shortest hierarchical structure that best classifies the training data, using greedy heuristics to select the most discriminative attributes near the root of the hierarchy.
</prevsent>
<prevsent>as heuristics are based on locally optimal splitting of all training data, adding new training data may lead to dramatic reorganisation of the hierarchy, and nothing is explicitly learned from having built decision tree at previous learning stage (ling and marinov, 1993).
</prevsent>
</prevsection>
<citsent citstr=" W99-0904 ">
to tackle the issue of word structure more squarely, there has been recent upsurge of interest in global paradigm-based constraints on morphology learning, as way to minimise the range of inflectional or derivational endings heuristic ally inferred from raw training data (goldsmith, 2001; <papid> J01-2001 </papid>gaussier, 1999; <papid> W99-0904 </papid>baroni, 2000).</citsent>
<aftsection>
<nextsent>it should be noted, however, that global, linguistically-inspired constraints of this sort do not interact with morphology learning in any direct way.
</nextsent>
<nextsent>rather, they are typically used as global criteria for optimal convergence on an existing repertoire of minimally redundant sets of paradigmatically related morphemes.
</nextsent>
<nextsent>candidate morpheme-like units are acquired independently of paradigm-based constraints, solely on the basis of local heuristics.
</nextsent>
<nextsent>once more, there is no clear sense in which global constraints form integral part of learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1882">
<title id=" W04-0901.xml">interpretation in a cognitive architecture </title>
<section> semantic analysis.  </section>
<citcontext>
<prevsection>
<prevsent>erator that corresponds to the operator co extensive part-of in el.
</prevsent>
<prevsent>the expression 2  1 indicates that situation 2 is co extensive with situation 1 , that is, 1 and 2 have the same spatiotemporal location.
</prevsent>
</prevsection>
<citsent citstr=" P92-1030 ">
3 the reader is referred to (hwang and schubert, 1992) <papid> P92-1030 </papid>for details on the non incremental deindexing rules.</citsent>
<aftsection>
<nextsent>once the parameterized episodic logical form hasbeen generated, the incremental deindexer transforms the lambda expressions that abstract over the parameters introduced by the incremental semantic interpreter into episodic logical forms.
</nextsent>
<nextsent>to thisend, constants are introduced for the meta logical parameters.
</nextsent>
<nextsent>these constants stand for parameterized terms and predicates in the resulting episodic logical form.
</nextsent>
<nextsent>-conversion is then performed for each one of the -expressions in the parameterized episodic logical form.applying this procedure, the incremental dein dexer yields the episodic logical form elf . elf : (9u 1 :[[u 1 same-time now1] ^ [u 0 precedes 1 ]] [[speaker tell hearer (that (9e 1 :[[e 1 before 1 ] ^ [e 0 orients 1 ]] [[(the x:[x spy] (the y:[y cop] (the z:[z p] [[x e 1 ] [((with-instr z) (watch y))]] )))]  1 ]))]  1 ]) applying the same procedure for the competing analysis, we obtain the following episodic logical form elf0 . elf0 : (9u 1 :[[u 1 same-time now1] ^ [u 0 immediately-precedes 1 ]] [[speaker tell hearer (that (9e 1 :[[e 1 before 1 ] ^ [e 0 orients 1 ]] [[(the x:[x spy] (the y:[y cop] (the z:[[z revolver] ^ [z p]] [x watch y])))]  1 ]))]  1 ])
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1883">
<title id=" W04-1605.xml">systematic verb stem generation for arabic </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sliding window algorithms (el-affendi, 1999) use an approximate string matching approach ofinput words against lists of roots, morphological patterns, prefixes, and suffixes.
</prevsent>
<prevsent>algebraic algorithms (el-affendi, 1991), on the other hand, assign binary values to morphological patterns and input words, then perform some simple algebraic operations to decompose word into stem and affixes.
</prevsent>
</prevsection>
<citsent citstr=" W98-1009 ">
permutation algorithms (al-shalabi and evens, 1998) <papid> W98-1009 </papid>use the input words letters to generate all possible trilateral or quadrilateral sequences without violation of the original order of the letters which is then compared with items in dictionary of roots until match is found.</citsent>
<aftsection>
<nextsent>linguistic algorithms (thalouth and al-dannan, 1990; yagi and harous, 2003) remove letters from an input word that belong to prefixes and suffixes and place the remainder of the word into list.
</nextsent>
<nextsent>the members of this list are then tested for match with dictionary of morphological patterns.the primary drawback of many of these techniques is that they attempt to analyse using the information found in the letters of the input word.
</nextsent>
<nextsent>when roots form words, root letters are often transformed by replacement, fusion, inversion, or deletion, and their positions are lost between stem and affix letters.
</nextsent>
<nextsent>most attempts use various closest match algorithms, which introduce high level of uncertainty.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1884">
<title id=" W03-2805.xml">colouring summaries bleu </title>
<section> using bleu in nlp.  </section>
<citcontext>
<prevsection>
<prevsent>it is this conclusion that lin and hovy have drawn, that contradicts findings by the ibm and nist people for the importance of using multiple references when using bleu in machine translation.
</prevsent>
<prevsent>the use of either multiple references orjust single reference has been proved not to affect the reliability of the results provided by bleu (papineni et al, 2001; doddington, 2002), which seems not to be the case in summarization.
</prevsent>
</prevsection>
<citsent citstr=" W00-0408 ">
this is not surprise; comparisons of content-based metrics for summarization in (donaway et al, 2000) <papid> W00-0408 </papid>have led the authors to the conclusion that such metrics correlate highly with human judgement when the humans do not disagree substantially.</citsent>
<aftsection>
<nextsent>the fact that more than one reference summaries are needed because of the low agreement between human evaluators has been repeatedly indicated in automatic summarization evaluation (mani, 2001).
</nextsent>
<nextsent>we attempt to test bleus reliability when changing various evaluation parameters such as the source documents, the reference summaries used and even parameters unique to the evaluation of summaries, such as the compression rate of theextract.
</nextsent>
<nextsent>in doing so, we explore whether the metric is indeed reliable only when using more than single reference and whether any other testing parameter could compensate for lack of multiple references, if used appropriately.
</nextsent>
<nextsent>in this section, we will present description of the experiments themselves, along with the results obtained and their analysis, preceded by information on the corpus we used for our experiments and the tools we developed for setting their parameters and running them automatically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1885">
<title id=" W03-2805.xml">colouring summaries bleu </title>
<section> evaluation experiment.  </section>
<citcontext>
<prevsection>
<prevsent>the texts are marked up on the paragraph, sentence and word level.
</prevsent>
<prevsent>annotations with linguistic information (part of speech tags and morphological information), though marked up on the documents have not been used in our experiments at all.
</prevsent>
</prevsection>
<citsent citstr=" W00-0403 ">
three judges have assessed the sentences in each cluster and have provided ascore on scale from 0 to 10 (i.e. utility judge ment), expressing how important the sentence is for the topic of the cluster (radev et al, 2000).<papid> W00-0403 </papid></citsent>
<aftsection>
<nextsent>in our experiments, we have used three document clusters, each consisting of ten documents in english.
</nextsent>
<nextsent>3.2 summarizers.
</nextsent>
<nextsent>it is important to note, that our objective is not to demonstrate how particular summarization methodology performs, but to analyse an evaluation metric.
</nextsent>
<nextsent>the summaries used for the evaluation were produced as extracts at different sentence?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1886">
<title id=" W03-2805.xml">colouring summaries bleu </title>
<section> evaluation experiment.  </section>
<citcontext>
<prevsection>
<prevsent>the summaries used for the evaluation were produced as extracts at different sentence?
</prevsent>
<prevsent>(and not word) compression rates2.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
in order to produce summarizers for our evaluation, we use robust summarisation system (saggion,2002) that makes use of components for semantic tagging and coreference resolution developed within the gate architecture (cunningham et al, 2002).<papid> P02-1022 </papid></citsent>
<aftsection>
<nextsent>the system combines gate components with well established statistical techniques developed for the purpose of text summarisation research.
</nextsent>
<nextsent>the system supports generic?
</nextsent>
<nextsent>and query based summarisation addressing the need for useradaptation3.
</nextsent>
<nextsent>for each sentence, the system computes values for number of shallow?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1887">
<title id=" W04-1122.xml">an integrated method for chinese unknown word extraction </title>
<section> relative research works.  </section>
<citcontext>
<prevsection>
<prevsent>there are many research works on ate.
</prevsent>
<prevsent>and most successful systems are based on statistics.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
many statistical metrics have been proposed, including pointwise mutual information (mi) (church et al 1990), mean and variance, hypothesis testing (t-test, chi square test, etc.), log-likelihood ratio (lr) (dunning, 1993), <papid> J93-1003 </papid>statistic language model (tomokiyo, et al 2003), and so on.</citsent>
<aftsection>
<nextsent>point-wise mi is often used to find interesting bigrams (collocations).
</nextsent>
<nextsent>however, mi is actually better to think of it as measure of independence than of dependence (manning et al 1999).
</nextsent>
<nextsent>lr is one of the most stable methods for ate so far, and more appropriate for sparse data than other metrics.
</nextsent>
<nextsent>however, lr is still biased to two frequent words that are rarely adjacent, such as the pair (the, the) (pantel et al 2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1888">
<title id=" W04-0816.xml">word sense disambiguation based on term to term similarity in a context space </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>participants have available training corpus, set of test examples and sense inventory in each language.
</prevsent>
<prevsent>the training corpora are available in labelled and un labelled format; the former is mainly for supervised systems and the latter mainly for the unsupervised ones.
</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
several supervised ml algorithms have been applied to wsd (ide and veronis, 1998), (<papid> J98-1001 </papid>escudero et al, 2000): <papid> W00-0706 </papid>decision lists, neural networks, bayesian classifiers, boosting, exemplar based learning, etc. we report here the exemplar-based approach developed by uned and tested at the senseval-3 competition in the lexical sample tasks for english, spanish, catalan and italian.</citsent>
<aftsection>
<nextsent>after this brief introduction, sections 2 and 3 are devoted, respectively, to the training data and the processing performed over these data.
</nextsent>
<nextsent>section 4 characterizes the uned wsd system.
</nextsent>
<nextsent>first, we describe the general approach based on the representation of words, lemmas and senses in context space.
</nextsent>
<nextsent>then, we show how results are improved by applying standard similarity measures as cosine in this context space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1889">
<title id=" W04-0816.xml">word sense disambiguation based on term to term similarity in a context space </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>participants have available training corpus, set of test examples and sense inventory in each language.
</prevsent>
<prevsent>the training corpora are available in labelled and un labelled format; the former is mainly for supervised systems and the latter mainly for the unsupervised ones.
</prevsent>
</prevsection>
<citsent citstr=" W00-0706 ">
several supervised ml algorithms have been applied to wsd (ide and veronis, 1998), (<papid> J98-1001 </papid>escudero et al, 2000): <papid> W00-0706 </papid>decision lists, neural networks, bayesian classifiers, boosting, exemplar based learning, etc. we report here the exemplar-based approach developed by uned and tested at the senseval-3 competition in the lexical sample tasks for english, spanish, catalan and italian.</citsent>
<aftsection>
<nextsent>after this brief introduction, sections 2 and 3 are devoted, respectively, to the training data and the processing performed over these data.
</nextsent>
<nextsent>section 4 characterizes the uned wsd system.
</nextsent>
<nextsent>first, we describe the general approach based on the representation of words, lemmas and senses in context space.
</nextsent>
<nextsent>then, we show how results are improved by applying standard similarity measures as cosine in this context space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1890">
<title id=" W04-0816.xml">word sense disambiguation based on term to term similarity in a context space </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>both tasks have provided dictionary with additional information as examples, typical collocations and the equivalent synsets at wordnet 1.5.
</prevsent>
<prevsent>finally, the italian sense inventory is based on the multi wordnet dictionary3 (pianta et al, 2002).
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
unlike the other mentioned languages , the italian task doesnt provide separate file with the dic tionary.besides the training data provided by senseval, we have used the semcor (miller et al, 1993) <papid> H93-1061 </papid>collection in which every word is already tagged in its part of speech, sense and synset of wordnet.</citsent>
<aftsection>
<nextsent>a tokenized version of the catalan, spanish and italian corpora has been provided.
</nextsent>
<nextsent>in this version every word is tagged with its lemma and 1http://www.cogsci.princeton.edu/ wn/ 2http://www.wordsmyth.net 3http://multiwordnet.itc.it/ part of speech tag.
</nextsent>
<nextsent>this information has been manually annotated by human assessors both inthe catalan and the spanish corpora.
</nextsent>
<nextsent>the italian corpus has been processed automatically by the tnt postagger4 (brants, 2000) <papid> A00-1031 </papid>including similar tags.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1891">
<title id=" W04-0816.xml">word sense disambiguation based on term to term similarity in a context space </title>
<section> preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>in this version every word is tagged with its lemma and 1http://www.cogsci.princeton.edu/ wn/ 2http://www.wordsmyth.net 3http://multiwordnet.itc.it/ part of speech tag.
</prevsent>
<prevsent>this information has been manually annotated by human assessors both inthe catalan and the spanish corpora.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
the italian corpus has been processed automatically by the tnt postagger4 (brants, 2000) <papid> A00-1031 </papid>including similar tags.</citsent>
<aftsection>
<nextsent>the english data lacked of this information, leading us to apply the treetagger5 (schmid, 1994) tool to the training and test data as previous step to the disambiguation process.
</nextsent>
<nextsent>since the semcor collection is already tagged, the preprocessing consisted in the segmentation of texts by the paragraph tag, obtaining 5382 different fragments.
</nextsent>
<nextsent>each paragraph of semcor has been used as separate training example for the english lexical sample task.
</nextsent>
<nextsent>we applied the mapping provided by senseval to represent verbs according to the verb inventory used in senseval-3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1892">
<title id=" W03-2101.xml">understanding information graphics a discourse level problem </title>
<section> understanding information graphics.  </section>
<citcontext>
<prevsection>
<prevsent>applying this to graphical displays, it is reasonable to presume that the author of graphic similarly expects the viewer to use perceptual skills along with other knowledge sources to deduce from the graphic the message that he intended to convey.thus we are applying speech act theory in there verse direction of the auto brief project, namely to the recognition of the intended message under lying an information graphic.
</prevsent>
<prevsent>4.2 discourse level problem this section argues that interpreting information graphics is discourse-level problem ? not only is it necessary to recognize the intention ofthe graphic as noted in section 4.1, but understanding an information graphic requires similar kinds of knowledge and processing as does understanding traditional discourse.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
grosz and sidner contended that discourse has structure comprised of discourse segments.each discourse segment has discourse segment purpose that contributes to the discourse purpose or intention underlying the overall discourse(grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>when an article is comprised of text and graphics, the graphic generally expands on the text and contributes to the discourse purpose of the article.
</nextsent>
<nextsent>consider the graphic and partial surrounding text reproduced in figure 6.
</nextsent>
<nextsent>nowhere in the text is it stated thatthe income of black women has risen dramatically over the last decade and has reached the level of white women.
</nextsent>
<nextsent>yet this message is clearly conveyed by the graphic and contributes to the overall communicative intention of this portion of the article ? namely, that there has been monumental shifting of the sands?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1894">
<title id=" W03-2101.xml">understanding information graphics a discourse level problem </title>
<section> understanding information graphics.  </section>
<citcontext>
<prevsection>
<prevsent>this message can only be deduced by relating the two individual graphics and their underlying messages.
</prevsent>
<prevsent>moreover, understanding information graphics requires the use of multiple knowledge sources.
</prevsent>
</prevsection>
<citsent citstr=" J99-1001 ">
in earlier work on recognizing expressions of doubt, we developed an algorithm that combined linguistic, contextual, and world knowledge and applied it to the recognition of complex discourse acts(carberry and lambert, 1999).<papid> J99-1001 </papid></citsent>
<aftsection>
<nextsent>in the caseof information graphics, the corollary to linguistic knowledge is perceptual knowledge, by which one recognizes the individual elements of the graphic (for example, the bars in bar chart), the relation of the individual elements in the graphic to one another, the type of graphic (line graph, bar chart, pie chart, etc.), and what the different graphic types can be used to convey.
</nextsent>
<nextsent>for example, both scatter plot and pie chart can beused to portray how an entity (such as government income) is divided up among several categories (such as social welfare, military spending, etc.); however, graphic designer will choose apie chart if the intent is to convey the relative distributions as opposed to their absolute amounts.
</nextsent>
<nextsent>furthermore, particular type of graphic (such as line graph) might be appropriate for conveying several different intentions (maximum data point, data trend, data variation, etc.).contextual and world knowledge are also essential for understanding information graphics.contextual knowledge includes the caption associated with the graphic, any highlighting of graphic elements that affects the focus of attention in the graphic, and the discourse structure and focus of attention in any surrounding text.
</nextsent>
<nextsent>world knowledge consists of mutual beliefs between designer and viewer about entities of interest to the intended viewing audience.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1895">
<title id=" W04-1113.xml">using synonym relations in chinese collocation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>are three nouns with similar meanings, however, we say ? ?
</prevsent>
<prevsent>rather than ? ?, ? rather than ? ?.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction (church and hanks 1990, <papid> J90-1003 </papid>smadja 1993, <papid> J93-1007 </papid>choueka 1993, lin 1998).</citsent>
<aftsection>
<nextsent>as the lexical statistical approach is developed based on the recurrence?
</nextsent>
<nextsent>property of collocations, only collocations with reasonably good recurrence can be extracted.
</nextsent>
<nextsent>collocations with low occurrence frequency cannot be extracted, thus affecting the recall rate.
</nextsent>
<nextsent>the precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account (smadja 1993, <papid> J93-1007 </papid>lin 1997 <papid> P97-1009 </papid>and lu et al 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1896">
<title id=" W04-1113.xml">using synonym relations in chinese collocation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>are three nouns with similar meanings, however, we say ? ?
</prevsent>
<prevsent>rather than ? ?, ? rather than ? ?.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction (church and hanks 1990, <papid> J90-1003 </papid>smadja 1993, <papid> J93-1007 </papid>choueka 1993, lin 1998).</citsent>
<aftsection>
<nextsent>as the lexical statistical approach is developed based on the recurrence?
</nextsent>
<nextsent>property of collocations, only collocations with reasonably good recurrence can be extracted.
</nextsent>
<nextsent>collocations with low occurrence frequency cannot be extracted, thus affecting the recall rate.
</nextsent>
<nextsent>the precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account (smadja 1993, <papid> J93-1007 </papid>lin 1997 <papid> P97-1009 </papid>and lu et al 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1900">
<title id=" W04-1113.xml">using synonym relations in chinese collocation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>property of collocations, only collocations with reasonably good recurrence can be extracted.
</prevsent>
<prevsent>collocations with low occurrence frequency cannot be extracted, thus affecting the recall rate.
</prevsent>
</prevsection>
<citsent citstr=" P97-1009 ">
the precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account (smadja 1993, <papid> J93-1007 </papid>lin 1997 <papid> P97-1009 </papid>and lu et al 2003).</citsent>
<aftsection>
<nextsent>the low precision rate is mainly due to the low precision rate of word bigram extractions as only about 30% - 40% precision rate can be achieved for word bi-grams.
</nextsent>
<nextsent>in this paper, we propose different approach to find collocations with low recurrences.
</nextsent>
<nextsent>the main idea is to make use of synonym relations to extract synonymous collocations.
</nextsent>
<nextsent>lin (lin 1997) <papid> P97-1009 </papid>described distributional hypothesis that if two words have similar set of collocations, they are probably similar.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1904">
<title id=" W04-1113.xml">using synonym relations in chinese collocation extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>dagan (dagan 1997) applied similarity based smoothing method to solve the problem of data sparseness in statistical natural language processing.
</prevsent>
<prevsent>the experiments conducted in his later works showed that this method achieved much better results than back-off smoothing methods in word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" P03-1016 ">
similarly, hua wu (wu and zhou 2003) <papid> P03-1016 </papid>applied synonyms relationship between two different languages to automatically acquire english synonymous collocation.</citsent>
<aftsection>
<nextsent>this is the first time that the concept synonymous collocation is proposed.
</nextsent>
<nextsent>a side intuition raised here is that nature language is full of synonymous collocations.
</nextsent>
<nextsent>as many of them have low occurrences, they are failed to be retrieved by lexical statistical methods.
</nextsent>
<nextsent>even though there are chinese synonym dictionaries, such as ( tong yi ci lin), the dictionaries lack structured knowledge and synonyms are too loosely defined to be used for collocation extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1905">
<title id=" W04-2611.xml">abstraction summarization for managing the biomedical research literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>subsequent research (schank, 1975; wilks, 1976) expanded this paradigm.
</prevsent>
<prevsent>more recently, number of examples of knowledge-based applications show considerable promise.
</prevsent>
</prevsection>
<citsent citstr=" P01-1037 ">
these include systems for machine translation (viegas et al, 1998), question answering, (harabagiu et al, 2001; <papid> P01-1037 </papid>clark et al, 2003), <papid> W03-0901 </papid>and information retrieval (mihalcea and moldovan, 2000).<papid> W00-1104 </papid></citsent>
<aftsection>
<nextsent>in the biomedical domain, the medline?
</nextsent>
<nextsent>bibliographic database provides opportunities for keeping abreast of the research literature.
</nextsent>
<nextsent>however, the large size of this online resource presents potential challenges to the user.
</nextsent>
<nextsent>query results often include hundreds or thousands of citations (including title and abstract).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1906">
<title id=" W04-2611.xml">abstraction summarization for managing the biomedical research literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>subsequent research (schank, 1975; wilks, 1976) expanded this paradigm.
</prevsent>
<prevsent>more recently, number of examples of knowledge-based applications show considerable promise.
</prevsent>
</prevsection>
<citsent citstr=" W03-0901 ">
these include systems for machine translation (viegas et al, 1998), question answering, (harabagiu et al, 2001; <papid> P01-1037 </papid>clark et al, 2003), <papid> W03-0901 </papid>and information retrieval (mihalcea and moldovan, 2000).<papid> W00-1104 </papid></citsent>
<aftsection>
<nextsent>in the biomedical domain, the medline?
</nextsent>
<nextsent>bibliographic database provides opportunities for keeping abreast of the research literature.
</nextsent>
<nextsent>however, the large size of this online resource presents potential challenges to the user.
</nextsent>
<nextsent>query results often include hundreds or thousands of citations (including title and abstract).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1907">
<title id=" W04-2611.xml">abstraction summarization for managing the biomedical research literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>subsequent research (schank, 1975; wilks, 1976) expanded this paradigm.
</prevsent>
<prevsent>more recently, number of examples of knowledge-based applications show considerable promise.
</prevsent>
</prevsection>
<citsent citstr=" W00-1104 ">
these include systems for machine translation (viegas et al, 1998), question answering, (harabagiu et al, 2001; <papid> P01-1037 </papid>clark et al, 2003), <papid> W03-0901 </papid>and information retrieval (mihalcea and moldovan, 2000).<papid> W00-1104 </papid></citsent>
<aftsection>
<nextsent>in the biomedical domain, the medline?
</nextsent>
<nextsent>bibliographic database provides opportunities for keeping abreast of the research literature.
</nextsent>
<nextsent>however, the large size of this online resource presents potential challenges to the user.
</nextsent>
<nextsent>query results often include hundreds or thousands of citations (including title and abstract).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1908">
<title id=" W04-2611.xml">abstraction summarization for managing the biomedical research literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>2 2.1 2.2 background lexical semantics research in lexical semantics (cruse, 1986) provides insight into the interaction of reference and linguistic structure.
</prevsent>
<prevsent>in addition to paradigmatic lexical phenomena such as synonymy, hypernymy, and meronymy, diathe sis alternation (levin and rappaport hovav, 1996), deep case (fillmore, 1968), and the interaction of predi cat ional structure and events (tenny and pustejovsky, 2000) are being investigated.
</prevsent>
</prevsection>
<citsent citstr=" J93-2005 ">
some of the consequences of research in lexical semantics, with particular attention to natural language processing, are discussed by pustejovsky et al (1993) <papid> J93-2005 </papid>and nirenburg and raskin (1996).</citsent>
<aftsection>
<nextsent>implemented systems often draw on the information contained in wordnet (fellbaum, 1998).
</nextsent>
<nextsent>in the biomedical domain, umls knowledge provides considerable support for text-based systems.
</nextsent>
<nextsent>(burgun and bodenreider (2001) compare the umls to wordnet.)
</nextsent>
<nextsent>the umls (humphreys et al, 1998) consists of three components: the metathesaurus,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1909">
<title id=" W04-2611.xml">abstraction summarization for managing the biomedical research literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic interpretation is based on categorical analysis that is underspecified in that it is partial parse (cf.
</prevsent>
<prevsent>mcdonald, 1992).
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
this analysis depends on the specialist lexicon and the xerox part-of-speech tagger (cutting et al, 1992) <papid> A92-1018 </papid>and provides simple noun phrases that are mapped to concepts in the umls meta thesaurus using metamap (aronson, 2001).</citsent>
<aftsection>
<nextsent>the categorial analysis enhanced with metathesau rus concepts and associated semantic types provides the basis for semantic interpretation, which relies on two components: set of indicator?
</nextsent>
<nextsent>rules and an (under specified) dependency grammar.
</nextsent>
<nextsent>indicator rules map between syntactic phenomena (such as verbs, nominalizations, and prepositions) and predicates in the semantic network.
</nextsent>
<nextsent>for example, such rules stipulate that the preposition for indicates the semantic predicate treats in sumatriptan for migraine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1910">
<title id=" W04-2611.xml">abstraction summarization for managing the biomedical research literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two paradigms are being pursued: extraction and abstraction (hahn and mani, 2000).
</prevsent>
<prevsent>extraction concentrates on creating summary from the actual text occurring in the source document, relying on notions such as frequency of occurrence and cue phrases to identify important information.
</prevsent>
</prevsection>
<citsent citstr=" P99-1072 ">
abstraction, on the other hand, relies either on linguistic processing followed by structural compaction (mani et al, 1999) <papid> P99-1072 </papid>or on interpretation of the source text into semantic representation, which is then condensed to retain only the most important information asserted in the source.</citsent>
<aftsection>
<nextsent>the semantic abstraction paradigm is attractive due to its ability to manipulate information that may not have been explicitly articulated in the source document.
</nextsent>
<nextsent>however, due to the challenges in providing semantic representation, semantic abstraction has not been widely pursued, although the topic system (hahn and reimer, 1999) is notable exception.
</nextsent>
<nextsent>semantic abstraction summarization we are devising an approach to automatic summarization in the semantic abstraction paradigm, relying on semrep for semantic interpretation of source text.
</nextsent>
<nextsent>the transformation stage that condenses these predications is guided by principles articulated in terms of frequency of occurrence as well as lexical semantic phenomena.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1911">
<title id=" W04-2611.xml">abstraction summarization for managing the biomedical research literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>extending summarization to multi document input presents challenges in removing redundancies across documents an might be important.
</prevsent>
<prevsent>one issue is devising framework on which to compute similarities and differences across documents.
</prevsent>
</prevsection>
<citsent citstr=" W00-1009 ">
radev (2000) <papid> W00-1009 </papid>defines twenty-four relationships (such as equivalence, subsumption, and contradiction) that might apply at various structural levels across documents.</citsent>
<aftsection>
<nextsent>sub-events (daniel et al, 2003) <papid> W03-0502 </papid>and sub-topics (saggion and lapalme, 2002) <papid> J02-4005 </papid>also contribute to the framework used for comparing documents in multi document summarization.</nextsent>
<nextsent>a particular challenge to multi document summarization in the extraction paradigm is determining what parts of documents conform to the ining similarities and differences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1912">
<title id=" W04-2611.xml">abstraction summarization for managing the biomedical research literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one issue is devising framework on which to compute similarities and differences across documents.
</prevsent>
<prevsent>radev (2000) <papid> W00-1009 </papid>defines twenty-four relationships (such as equivalence, subsumption, and contradiction) that might apply at various structural levels across documents.</prevsent>
</prevsection>
<citsent citstr=" W03-0502 ">
sub-events (daniel et al, 2003) <papid> W03-0502 </papid>and sub-topics (saggion and lapalme, 2002) <papid> J02-4005 </papid>also contribute to the framework used for comparing documents in multi document summarization.</citsent>
<aftsection>
<nextsent>a particular challenge to multi document summarization in the extraction paradigm is determining what parts of documents conform to the ining similarities and differences.
</nextsent>
<nextsent>a recent study (kan et al, 2001) uses topic composition from text headers, but other studies in the extraction paradigm (goldstein et al, 1999), extraction coupled with rhetorical structural identification (teufel and moens, 2002), <papid> J02-4002 </papid>and syntactic abstraction paradigms use different methodologies (barzilay et al, 1999; <papid> P99-1071 </papid>mckeown et al, 1999).</nextsent>
<nextsent>our semantic abstraction summarization system naturally extends to multi document input with no modification from the system designed for single documents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1913">
<title id=" W04-2611.xml">abstraction summarization for managing the biomedical research literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one issue is devising framework on which to compute similarities and differences across documents.
</prevsent>
<prevsent>radev (2000) <papid> W00-1009 </papid>defines twenty-four relationships (such as equivalence, subsumption, and contradiction) that might apply at various structural levels across documents.</prevsent>
</prevsection>
<citsent citstr=" J02-4005 ">
sub-events (daniel et al, 2003) <papid> W03-0502 </papid>and sub-topics (saggion and lapalme, 2002) <papid> J02-4005 </papid>also contribute to the framework used for comparing documents in multi document summarization.</citsent>
<aftsection>
<nextsent>a particular challenge to multi document summarization in the extraction paradigm is determining what parts of documents conform to the ining similarities and differences.
</nextsent>
<nextsent>a recent study (kan et al, 2001) uses topic composition from text headers, but other studies in the extraction paradigm (goldstein et al, 1999), extraction coupled with rhetorical structural identification (teufel and moens, 2002), <papid> J02-4002 </papid>and syntactic abstraction paradigms use different methodologies (barzilay et al, 1999; <papid> P99-1071 </papid>mckeown et al, 1999).</nextsent>
<nextsent>our semantic abstraction summarization system naturally extends to multi document input with no modification from the system designed for single documents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1914">
<title id=" W04-2611.xml">abstraction summarization for managing the biomedical research literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sub-events (daniel et al, 2003) <papid> W03-0502 </papid>and sub-topics (saggion and lapalme, 2002) <papid> J02-4005 </papid>also contribute to the framework used for comparing documents in multi document summarization.</prevsent>
<prevsent>a particular challenge to multi document summarization in the extraction paradigm is determining what parts of documents conform to the ining similarities and differences.</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
a recent study (kan et al, 2001) uses topic composition from text headers, but other studies in the extraction paradigm (goldstein et al, 1999), extraction coupled with rhetorical structural identification (teufel and moens, 2002), <papid> J02-4002 </papid>and syntactic abstraction paradigms use different methodologies (barzilay et al, 1999; <papid> P99-1071 </papid>mckeown et al, 1999).</citsent>
<aftsection>
<nextsent>our semantic abstraction summarization system naturally extends to multi document input with no modification from the system designed for single documents.
</nextsent>
<nextsent>e disorder schema serves as the framework for identifying sub-topics, and predications retrieved across several documents must conform to its structure.
</nextsent>
<nextsent>informational equivalence (and redundancy) is computed on this basis.
</nextsent>
<nextsent>for example, all predications that conform to the schema line {treatment} treats {disorders} constitute representation of sub topic in the disorder domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1915">
<title id=" W04-2611.xml">abstraction summarization for managing the biomedical research literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sub-events (daniel et al, 2003) <papid> W03-0502 </papid>and sub-topics (saggion and lapalme, 2002) <papid> J02-4005 </papid>also contribute to the framework used for comparing documents in multi document summarization.</prevsent>
<prevsent>a particular challenge to multi document summarization in the extraction paradigm is determining what parts of documents conform to the ining similarities and differences.</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
a recent study (kan et al, 2001) uses topic composition from text headers, but other studies in the extraction paradigm (goldstein et al, 1999), extraction coupled with rhetorical structural identification (teufel and moens, 2002), <papid> J02-4002 </papid>and syntactic abstraction paradigms use different methodologies (barzilay et al, 1999; <papid> P99-1071 </papid>mckeown et al, 1999).</citsent>
<aftsection>
<nextsent>our semantic abstraction summarization system naturally extends to multi document input with no modification from the system designed for single documents.
</nextsent>
<nextsent>e disorder schema serves as the framework for identifying sub-topics, and predications retrieved across several documents must conform to its structure.
</nextsent>
<nextsent>informational equivalence (and redundancy) is computed on this basis.
</nextsent>
<nextsent>for example, all predications that conform to the schema line {treatment} treats {disorders} constitute representation of sub topic in the disorder domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1916">
<title id=" W04-0846.xml">context clustering for word sense disambiguation based on modeling pairwise context similarities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the situation is worse when considering the domain dependency of word senses.
</prevsent>
<prevsent>to avoid the knowledge bottleneck, unsupervised or weakly supervised learning approaches have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
these include the bootstrapping approach [yarowsky 1995] <papid> P95-1026 </papid>and the context clustering approach [schutze 1998].</citsent>
<aftsection>
<nextsent>although the above unsupervised or weakly supervised learning approaches are less subject to the knowledge bottleneck, some weakness exists: i) for each individual keyword, the sense number has to be provided and in the bootstrapping case, seeds for each sense are also required; ii) the modeling usually assumes some form of evidence in dependency, e.g. the vector space model used in [schutze 1998] and [niu et al 2003]: this limits the performance and its potential enhancement; iii) most wsd systems either use selectional restriction in parsing relations, and/or trigger words which co-occur within window size of the ambiguous word.
</nextsent>
<nextsent>we previously at-tempted combining both types of evidence but only achieved limited improvement due to the lack of proper modeling of information over-lapping [niu et al 2003].
</nextsent>
<nextsent>this paper presents new algorithm that addresses these problems.
</nextsent>
<nextsent>a novel context clustering scheme based on modeling the similarities between pairwise contexts at category level is presented in the bayesian framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1917">
<title id=" W04-0846.xml">context clustering for word sense disambiguation based on modeling pairwise context similarities </title>
<section> maximum entropy modeling.  </section>
<citcontext>
<prevsection>
<prevsent>ii) parsing relationships associated with the key word automatically decoded by our parser 1 note that the words that appear in the senseval-3.
</prevsent>
<prevsent>lexical sample evaluation are removed in the corpus construction process.
</prevsent>
</prevsection>
<citsent citstr=" W03-0808 ">
infoxtract [srihari et al 2003].<papid> W03-0808 </papid></citsent>
<aftsection>
<nextsent>the relationships being utilized are listed below.
</nextsent>
<nextsent>noun: subject-of, object-of, complement-of, has-adjective-modifier, has-noun modifier, modifier-of, possess, possessed-by, appositive-of verb: has-subject, has-object, has complement, has-adverb-modifier, has-prepositional-modifier adjective: modifier-of, has-adverb-modifier based on the above context features, the following three categories of context similarity features are defined: (1) context similarity based on vector space model using co-occurring trigger words: the trigger words centering around the key word are represented as vector, and the tf*idf scheme is used to weigh each trigger word.
</nextsent>
<nextsent>the cosine of the angle between two resulting vectors is used as context similarity measure.
</nextsent>
<nextsent>(2) context similarity based on latent semantic analysis (lsa) using trigger words: lsa [deerwester et al 1990] is technique used to uncover the underlying semantics based on co-occurrence data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1918">
<title id=" W03-2104.xml">an information theoretic approach for argument interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.
</prevsent>
<prevsent>we incorporate attentional focus into the.
</prevsent>
</prevsection>
<citsent citstr=" W02-0227 ">
discourse-interpretation formalism described in (zukerman and george, 2002), <papid> W02-0227 </papid>which uses the minimum message length (mml) principle(wallace and boulton, 1968) to evaluate candidate discourse interpretations.ity for the detective game described in (zuker man, 2001).in the following section, we describe our detective game, and discuss our knowledge representa tion.</citsent>
<aftsection>
<nextsent>next, we outline the argument interpretation process.
</nextsent>
<nextsent>in section 4, we provide an overview of our minimum message length approach to discourse interpretation, and describe how attentional focus is incorporated into this formalism.
</nextsent>
<nextsent>the results of our evaluation are reported in section 5.
</nextsent>
<nextsent>we then discuss related research, followed by concluding remarks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1928">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a parser that fails to recognize control subjects misses important information, quantitatively about 3 % of all subjects.
</prevsent>
<prevsent>but unrestricted real-world texts still pose aproblem to nlp systems that are based on formal grammars.
</prevsent>
</prevsection>
<citsent citstr=" P02-1035 ">
few hand-crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (riezler et al, 2002) <papid> P02-1035 </papid>for an exception, and (burke et al,2004; hockenmaier and steedman, 2002) <papid> P02-1043 </papid>for approaches extracting formal grammars from thetreebank), and speed remains serious chal lenge.</citsent>
<aftsection>
<nextsent>the typical problems can be grouped as follows.
</nextsent>
<nextsent>grammar complexity fully comprehensive grammars are difficult to maintain and considerably increase parsing complexity.
</nextsent>
<nextsent>note that statistical parsers can equally suffer from this problem, see e.g.
</nextsent>
<nextsent>(kaplan et al, 2004).<papid> N04-1013 </papid>parsing complexity typical formal grammar parser complexity is much higher thanthe o(n3) for cfg (eisner, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1929">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a parser that fails to recognize control subjects misses important information, quantitatively about 3 % of all subjects.
</prevsent>
<prevsent>but unrestricted real-world texts still pose aproblem to nlp systems that are based on formal grammars.
</prevsent>
</prevsection>
<citsent citstr=" P02-1043 ">
few hand-crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (riezler et al, 2002) <papid> P02-1035 </papid>for an exception, and (burke et al,2004; hockenmaier and steedman, 2002) <papid> P02-1043 </papid>for approaches extracting formal grammars from thetreebank), and speed remains serious chal lenge.</citsent>
<aftsection>
<nextsent>the typical problems can be grouped as follows.
</nextsent>
<nextsent>grammar complexity fully comprehensive grammars are difficult to maintain and considerably increase parsing complexity.
</nextsent>
<nextsent>note that statistical parsers can equally suffer from this problem, see e.g.
</nextsent>
<nextsent>(kaplan et al, 2004).<papid> N04-1013 </papid>parsing complexity typical formal grammar parser complexity is much higher thanthe o(n3) for cfg (eisner, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1930">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>grammar complexity fully comprehensive grammars are difficult to maintain and considerably increase parsing complexity.
</prevsent>
<prevsent>note that statistical parsers can equally suffer from this problem, see e.g.
</prevsent>
</prevsection>
<citsent citstr=" N04-1013 ">
(kaplan et al, 2004).<papid> N04-1013 </papid>parsing complexity typical formal grammar parser complexity is much higher thanthe o(n3) for cfg (eisner, 1997).</citsent>
<aftsection>
<nextsent>the complexity of some formal grammars is still unknown.
</nextsent>
<nextsent>for tree-adjoining grammars (tag)it is o(n7) or o(n8) depending on the implementation (eisner, 2000).
</nextsent>
<nextsent>(sarkar et al, 2000) state that the theoretical bound of worst time complexity for head-driven phrase structure grammar (hpsg) parsing is exponential.parsing algorithms able to treat completely unrestricted long-distance dependencies are np complete (neuhaus and broker, 1997).<papid> P97-1043 </papid></nextsent>
<nextsent>ranking returning all syntactically possible analyses for sentence is not really what is expected of syntactic analyzer if it should be of practical use, since for human there is usually only one correct?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1931">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the complexity of some formal grammars is still unknown.
</prevsent>
<prevsent>for tree-adjoining grammars (tag)it is o(n7) or o(n8) depending on the implementation (eisner, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P97-1043 ">
(sarkar et al, 2000) state that the theoretical bound of worst time complexity for head-driven phrase structure grammar (hpsg) parsing is exponential.parsing algorithms able to treat completely unrestricted long-distance dependencies are np complete (neuhaus and broker, 1997).<papid> P97-1043 </papid></citsent>
<aftsection>
<nextsent>ranking returning all syntactically possible analyses for sentence is not really what is expected of syntactic analyzer if it should be of practical use, since for human there is usually only one correct?
</nextsent>
<nextsent>interpretation.
</nextsent>
<nextsent>a clear indication of preference, by means of ranking the analyses in preference order is needed.pruning in order to keep search spaces manageable it is in fact necessary to discard unconvincing alternatives already during the parsing process.
</nextsent>
<nextsent>in statistical parser, the ranking of intermediate structures occurs naturally, whilea rule-based system has to relyon ad hoc heuristics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1933">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with beam search in parse-time pruning system, which means that the total number of alternatives kept is constant from certain search complexity onwards, real-world parsing time can be reduced to near-linear.
</prevsent>
<prevsent>if one were to assume constantly full beam, or uses an oracle (nivre, 2004) it is linear in practice.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
a number of robust statistical parsers that offer solutions to these problems have now be come available (charniak, 2000; <papid> A00-2018 </papid>collins, 1999; henderson, 2003), <papid> N03-1014 </papid>but they typically produce cfg constituency data as output, trees that do not express long-distance dependencies.</citsent>
<aftsection>
<nextsent>although grammatical function and empty nodes annotation expressing long-distance dependencies are provided in treebanks such as the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>most statistical treebank trained parsers fully or largely ignore them1, which entails two problems: first, the training cannot profit from valuable annotation data.</nextsent>
<nextsent>second, the extraction of long-distance dependencies (ldd) and the mapping to shallow semantic representations is not always possible from the output of these parsers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1934">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with beam search in parse-time pruning system, which means that the total number of alternatives kept is constant from certain search complexity onwards, real-world parsing time can be reduced to near-linear.
</prevsent>
<prevsent>if one were to assume constantly full beam, or uses an oracle (nivre, 2004) it is linear in practice.
</prevsent>
</prevsection>
<citsent citstr=" N03-1014 ">
a number of robust statistical parsers that offer solutions to these problems have now be come available (charniak, 2000; <papid> A00-2018 </papid>collins, 1999; henderson, 2003), <papid> N03-1014 </papid>but they typically produce cfg constituency data as output, trees that do not express long-distance dependencies.</citsent>
<aftsection>
<nextsent>although grammatical function and empty nodes annotation expressing long-distance dependencies are provided in treebanks such as the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>most statistical treebank trained parsers fully or largely ignore them1, which entails two problems: first, the training cannot profit from valuable annotation data.</nextsent>
<nextsent>second, the extraction of long-distance dependencies (ldd) and the mapping to shallow semantic representations is not always possible from the output of these parsers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1935">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>if one were to assume constantly full beam, or uses an oracle (nivre, 2004) it is linear in practice.
</prevsent>
<prevsent>a number of robust statistical parsers that offer solutions to these problems have now be come available (charniak, 2000; <papid> A00-2018 </papid>collins, 1999; henderson, 2003), <papid> N03-1014 </papid>but they typically produce cfg constituency data as output, trees that do not express long-distance dependencies.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
although grammatical function and empty nodes annotation expressing long-distance dependencies are provided in treebanks such as the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>most statistical treebank trained parsers fully or largely ignore them1, which entails two problems: first, the training cannot profit from valuable annotation data.</citsent>
<aftsection>
<nextsent>second, the extraction of long-distance dependencies (ldd) and the mapping to shallow semantic representations is not always possible from the output of these parsers.
</nextsent>
<nextsent>this limitation is aggravated by lack of co-indexation information and parsing errors across an ldd.
</nextsent>
<nextsent>in fact, some syntactic relations cannot be recovered on configurational grounds only.
</nextsent>
<nextsent>for these reasons, (johnson, 2002) <papid> P02-1018 </papid>provocatively refers to them as half grammars?.the paper is organized as follows.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1936">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this limitation is aggravated by lack of co-indexation information and parsing errors across an ldd.
</prevsent>
<prevsent>in fact, some syntactic relations cannot be recovered on configurational grounds only.
</prevsent>
</prevsection>
<citsent citstr=" P02-1018 ">
for these reasons, (johnson, 2002) <papid> P02-1018 </papid>provocatively refers to them as half grammars?.the paper is organized as follows.</citsent>
<aftsection>
<nextsent>we first explore deep-linguistic grammar theory for english that is inherently designed to be robust by extending the low processing complexity and the robustness of statistical approaches to more deep-linguistic level, by making careful use of under specification, grammar compression techniques and using grammar that directly delivers simple predicate-argument structures.
</nextsent>
<nextsent>this allow us to use context-free gram marat parse-time while successfully treating long distance dependencies using low-complexity approaches before and after parsing.
</nextsent>
<nextsent>our approach is to use finite-state approximations of long-distance dependencies, as they are described in (schneider, 2003<papid> N03-3006 </papid>a) for dependency grammar (dg) and (cahill et al, 2004) <papid> P04-1041 </papid>for lexical functional grammar (lfg).</nextsent>
<nextsent>(dienesand dubey, 2003) <papid> W03-1005 </papid>show that finite-state preprocessing modules can successfully deal withldds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1938">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we first explore deep-linguistic grammar theory for english that is inherently designed to be robust by extending the low processing complexity and the robustness of statistical approaches to more deep-linguistic level, by making careful use of under specification, grammar compression techniques and using grammar that directly delivers simple predicate-argument structures.
</prevsent>
<prevsent>this allow us to use context-free gram marat parse-time while successfully treating long distance dependencies using low-complexity approaches before and after parsing.
</prevsent>
</prevsection>
<citsent citstr=" N03-3006 ">
our approach is to use finite-state approximations of long-distance dependencies, as they are described in (schneider, 2003<papid> N03-3006 </papid>a) for dependency grammar (dg) and (cahill et al, 2004) <papid> P04-1041 </papid>for lexical functional grammar (lfg).</citsent>
<aftsection>
<nextsent>(dienesand dubey, 2003) <papid> W03-1005 </papid>show that finite-state preprocessing modules can successfully deal withldds.</nextsent>
<nextsent>our approach is similar in also amounting to preprocessing recognition of ldds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1940">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we first explore deep-linguistic grammar theory for english that is inherently designed to be robust by extending the low processing complexity and the robustness of statistical approaches to more deep-linguistic level, by making careful use of under specification, grammar compression techniques and using grammar that directly delivers simple predicate-argument structures.
</prevsent>
<prevsent>this allow us to use context-free gram marat parse-time while successfully treating long distance dependencies using low-complexity approaches before and after parsing.
</prevsent>
</prevsection>
<citsent citstr=" P04-1041 ">
our approach is to use finite-state approximations of long-distance dependencies, as they are described in (schneider, 2003<papid> N03-3006 </papid>a) for dependency grammar (dg) and (cahill et al, 2004) <papid> P04-1041 </papid>for lexical functional grammar (lfg).</citsent>
<aftsection>
<nextsent>(dienesand dubey, 2003) <papid> W03-1005 </papid>show that finite-state preprocessing modules can successfully deal withldds.</nextsent>
<nextsent>our approach is similar in also amounting to preprocessing recognition of ldds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1941">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this allow us to use context-free gram marat parse-time while successfully treating long distance dependencies using low-complexity approaches before and after parsing.
</prevsent>
<prevsent>our approach is to use finite-state approximations of long-distance dependencies, as they are described in (schneider, 2003<papid> N03-3006 </papid>a) for dependency grammar (dg) and (cahill et al, 2004) <papid> P04-1041 </papid>for lexical functional grammar (lfg).</prevsent>
</prevsection>
<citsent citstr=" W03-1005 ">
(dienesand dubey, 2003) <papid> W03-1005 </papid>show that finite-state preprocessing modules can successfully deal withldds.</citsent>
<aftsection>
<nextsent>our approach is similar in also amounting to preprocessing recognition of ldds.
</nextsent>
<nextsent>then we show that the implementation (pro3gres) profits from hybrid ness and is fast 1(collins, 1999) model 2 uses some of the functional labels, and model 3 some long-distance dependencies and robust enough to do large-scale parsing of totally unrestricted texts and give an overview of its applications.
</nextsent>
<nextsent>to conclude, two evaluations are given.
</nextsent>
<nextsent>generally, linguistic analysis model aims at complete and correct analysis, which means that the mapping between the text data and its syntactic and semantic analysis is sound (the model extracts correct readings) and complete (the model deals with all language phenomena).in practice, however, both goals cannot be totally reached.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1942">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> a robust deep-linguistic theory </section>
<citcontext>
<prevsection>
<prevsent>generally, linguistic analysis model aims at complete and correct analysis, which means that the mapping between the text data and its syntactic and semantic analysis is sound (the model extracts correct readings) and complete (the model deals with all language phenomena).in practice, however, both goals cannot be totally reached.
</prevsent>
<prevsent>the main obstacle for soundness is the all-pervasive characteristic of natural language to be ambiguous, where ambiguities can often only be resolved with world knowledge.
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
statistical disambiguation such as (collins and brooks, 1995) <papid> W95-0103 </papid>for pp-attachment or (collins, 1997; <papid> P97-1003 </papid>charniak, 2000) <papid> A00-2018 </papid>for generative parsing greatly improve disambiguation, but asthey model by imitation instead of by understanding, complete soundness has to remain elu sive.</citsent>
<aftsection>
<nextsent>as for completeness, already early nave statistical approaches have shown that the problem of grammar size is not solved but even aggravated by naive probabilistic parser implementation, in which e.g. all cfg rules permitted in the penn treebank are extracted.
</nextsent>
<nextsent>from his 300,000 words training part of the penn treebank (charniak, 1996) obtains more than 10,000 cfg rules, of which only about 3,000 occur more than once.
</nextsent>
<nextsent>it is therefore necessary to either discard infrequent rules, do manual editing, use different rule format such as individual dependencies (collins, 1996) <papid> P96-1025 </papid>or gain full linguistic control and insight by using hand written grammar ? each of which sacrifices total completeness.</nextsent>
<nextsent>2.1 near-full parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1944">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> a robust deep-linguistic theory </section>
<citcontext>
<prevsection>
<prevsent>generally, linguistic analysis model aims at complete and correct analysis, which means that the mapping between the text data and its syntactic and semantic analysis is sound (the model extracts correct readings) and complete (the model deals with all language phenomena).in practice, however, both goals cannot be totally reached.
</prevsent>
<prevsent>the main obstacle for soundness is the all-pervasive characteristic of natural language to be ambiguous, where ambiguities can often only be resolved with world knowledge.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
statistical disambiguation such as (collins and brooks, 1995) <papid> W95-0103 </papid>for pp-attachment or (collins, 1997; <papid> P97-1003 </papid>charniak, 2000) <papid> A00-2018 </papid>for generative parsing greatly improve disambiguation, but asthey model by imitation instead of by understanding, complete soundness has to remain elu sive.</citsent>
<aftsection>
<nextsent>as for completeness, already early nave statistical approaches have shown that the problem of grammar size is not solved but even aggravated by naive probabilistic parser implementation, in which e.g. all cfg rules permitted in the penn treebank are extracted.
</nextsent>
<nextsent>from his 300,000 words training part of the penn treebank (charniak, 1996) obtains more than 10,000 cfg rules, of which only about 3,000 occur more than once.
</nextsent>
<nextsent>it is therefore necessary to either discard infrequent rules, do manual editing, use different rule format such as individual dependencies (collins, 1996) <papid> P96-1025 </papid>or gain full linguistic control and insight by using hand written grammar ? each of which sacrifices total completeness.</nextsent>
<nextsent>2.1 near-full parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1946">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> a robust deep-linguistic theory </section>
<citcontext>
<prevsection>
<prevsent>as for completeness, already early nave statistical approaches have shown that the problem of grammar size is not solved but even aggravated by naive probabilistic parser implementation, in which e.g. all cfg rules permitted in the penn treebank are extracted.
</prevsent>
<prevsent>from his 300,000 words training part of the penn treebank (charniak, 1996) obtains more than 10,000 cfg rules, of which only about 3,000 occur more than once.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
it is therefore necessary to either discard infrequent rules, do manual editing, use different rule format such as individual dependencies (collins, 1996) <papid> P96-1025 </papid>or gain full linguistic control and insight by using hand written grammar ? each of which sacrifices total completeness.</citsent>
<aftsection>
<nextsent>2.1 near-full parsing.
</nextsent>
<nextsent>the approach we have chosen is to use manually-developed wide-coverage tag sequence grammar (abney, 1995; briscoe and carroll, 2002), and to exclude or restrict rare, marked and error-prone phenomena.
</nextsent>
<nextsent>for example, while it is generally possible for nouns to be modified by more than one pp, only nouns seen in the treebank with several pps are allowed tohave several pps.
</nextsent>
<nextsent>or, while it is generally possible for subject to occur to the immediate right of verb (said she), this is only allowed for verbs seen with subject to the right in the training corpus, typically verbs of utterance, andonly in comma-delimited or sentence-final context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1956">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> a robust deep-linguistic theory </section>
<citcontext>
<prevsection>
<prevsent>it suggests that one might improve performance by integrating parsing, empty node recovery and antecedent finding in single system ...
</prevsent>
<prevsent>(johnson, 2002).<papid> P02-1018 </papid></prevsent>
</prevsection>
<citsent citstr=" P03-2006 ">
we have applied structural patterns to thepenn treebank, where like in perfect parses precision and recall are high, and where in addition functional labels and empty nodes are available, so that patterns similar to johnsons but ? like (jijkoun, 2003) ? <papid> P03-2006 </papid>relying on functional labels and empty nodes reach precision close to 100%.</citsent>
<aftsection>
<nextsent>unlike in johnson, also patterns for local dependencies are used; non-local patterns simply stretch across more subtree-levels.
</nextsent>
<nextsent>we use the extracted lexical counts as lexical frequency training material.
</nextsent>
<nextsent>every dependency relation has group of structural extraction patterns associated with it.
</nextsent>
<nextsent>this amounts to partial mapping of the penn treebank to functional relation label example verb subject subj he sleeps verb first object obj sees it verb second object obj2 gave (her) kisses verb adjunct adj ate yesterday verbsubord.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1957">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> a robust deep-linguistic theory </section>
<citcontext>
<prevsection>
<prevsent>   passive verb np-none *-x ?
</prevsent>
<prevsent>     np-sbj-x@ noun vp@
</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
control-verb np-sbj-none *-x figure 1: the ext action patterns for passive subjects (top) and subject control (bottom) dg (hajic?, 1998), (tapanainen and jarvinen, 1997).<papid> A97-1011 </papid></citsent>
<aftsection>
<nextsent>table 1 gives an overview of the most important dependencies.
</nextsent>
<nextsent>the subj relation, for example, has the head of an arbitrarily nested np with the functional tag sbj as dependent, and the head of an arbitrarily nested vp as head for all active verbs.in passive verbs, however, movement involving an empty constituent is assumed, which corresponds to the extraction pattern in figure 1,where vp@ is an arbitrarily nested vp, and np sbj-x@ the arbitrarily nested surface subject and the co-indexed, moved element.
</nextsent>
<nextsent>movements are generally supposed to be of arbitrary length, but closer investigation reveals that this type of movement is fixed.
</nextsent>
<nextsent>the same argument can be made for other relations, for example control structures, which have the extraction pattern shown in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1962">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> a robust deep-linguistic theory </section>
<citcontext>
<prevsection>
<prevsent>much of the interesting work is determining what goes into [the history] h(c)?(charniak, 2000).(<papid> A00-2018 </papid>schneider, 2003<papid> N03-3006 </papid>a) shows that the vast majority of ldds can be treated in this way, essentially compressing non-local subtrees into dedicated relations even before grammar writing starts.</prevsent>
<prevsent>the compressed trees correspond to simple lfg f-structure.</prevsent>
</prevsection>
<citsent citstr=" W04-0508 ">
the trees obtained from parsing can be decompressed into traditional constituency trees including empty nodes and co-indexation, or into shallow semantic structures such as minimal logical forms (mlf) (rinaldi et al, 2004<papid> W04-0508 </papid>b; schneider et al, 2000; schwitter et al, 1999).</citsent>
<aftsection>
<nextsent>this approach leaves ldds underspecified, but recoverable, and makes no claims as to whether empty nodes at an automonous syntactic level exist or not.
</nextsent>
<nextsent>2.4.2 post-processingafter parsing, shared constituents can be extracted again.
</nextsent>
<nextsent>the parser explicitly does this for control, raising and semi-auxiliary relations,because the grammar does not distinguish between subordinating clauses with and without control.
</nextsent>
<nextsent>a probability model based on the verb semantics is invoked if subordinate clause without overt subject is seen, in order to decide whether the matrix clause subject or object is shared.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1969">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> robustness in the small?.  </section>
<citcontext>
<prevsection>
<prevsent>constraint to allow each complement dependency type only once per verb can be seen as way of rendering it generative in practice.
</prevsent>
<prevsent>syntax vs. semantics instead of using back-off to tags (collins, 1999), semantic classes, wordnet for nouns and levin classes for verbs, are used, in the hope that they better manage better to express selectional restrictions than tags.
</prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
practical experiments have shown, however, that, in accordance to (gildea, 2001)<papid> W01-0521 </papid>on head-lexicalisation, there is almost no in crease in performance.</citsent>
<aftsection>
<nextsent>5 applications and evaluation.
</nextsent>
<nextsent>pro3gres is currently being applied in question answering system specifically targeted at figure 3: dependency tree output of the swi prolog graphical implementation of the parser technical domains (rinaldi et al, 2004<papid> W04-0508 </papid>b).</nextsent>
<nextsent>one of the main advantages of dependency-basedparser such as pro3gres over other parsing approaches is that mapping from the syntactic layer to semantic layer (meaning representation) is partly simplified (molla?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1976">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> robustness in the small?.  </section>
<citcontext>
<prevsection>
<prevsent>(lin, 1995; carroll et al, 1999) suggest evaluating on the linguistically more meaningful level of syntactic relations.
</prevsent>
<prevsent>two evaluations on the syntactic relation level are reported in the following.
</prevsent>
</prevsection>
<citsent citstr=" E03-1025 ">
first, general-purpose evaluation using hand-compiled gold standard corpus (carroll et al, 1999), which contains the grammatical relation data of 500 random sentences from the susanne corpus.the performance, shown in table 2, is, according to (preiss, 2003), <papid> E03-1025 </papid>similar to large selection of statistical parsers and grammatical relationfinder.</citsent>
<aftsection>
<nextsent>relations involving long-distance dependencies form part of these relations.
</nextsent>
<nextsent>in order to measure specifically their performance, selection of them is also given: wh-subject (whs), wh-object (who), passive subject (psubj), control subject (csubj), and the anaphor of the relative clause pronoun (rclsubja).
</nextsent>
<nextsent>5.2 parsing highly technical language.
</nextsent>
<nextsent>while measuring general parsing performance is fundamental in the development of any parsing system there is danger of fostering domain dependence in concentrating on single domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1977">
<title id=" W04-2003.xml">a robust and hybrid deep linguistic theory applied to largescale parsing </title>
<section> robustness in the small?.  </section>
<citcontext>
<prevsection>
<prevsent>cl. whs who psubj csubj rclsubja precision 91 89 73 74 68 92 60 n/a 80 89 recall 81 83 67 83 n/a 90 86 83 n/a 63table 2: results of evaluating the parser output on carrolls test suite on subject, object, pp attachment and clause subordination relations, and selective evaluation of 5 relations involving long-distance dependencies (ldd) percentage values for some relations, general, on the genia corpus subject object noun-pp verb-pp subord.
</prevsent>
<prevsent>clause precision 90 94 83 82 71 recall 86 95 82 84 75 table 3: results of evaluating 100 random sentences from the terminology-annotated genia corpus, on subject, object, pp-attachment and clause subordination relations guage is very technical and extremely domain specific.
</prevsent>
</prevsection>
<citsent citstr=" W03-1801 ">
but the most striking characteristic of this domain is the frequency of multiword terms (mwt) which are known to cause serious problems for nlp systems (sag et al, 2002),(dowdall et al, 2003).<papid> W03-1801 </papid></citsent>
<aftsection>
<nextsent>the token to chunk ra tio: nps = 2.3 , vps = 1.3 (number of tokens divided by the number of chunks) is unusually high.the genia corpus does not include any syntactic annotation (making standard evaluation more difficult) but approx.
</nextsent>
<nextsent>100, 000 mwts are annotated and assigned semantic type from the genia ontology.
</nextsent>
<nextsent>this novel parsing application is designed to determine how parsing performance interacts with mwt recognition as well as the applicability and possible improvements to the probablistic model over this domain, to test the hypothesis if terminology is the key to successful parsing system.
</nextsent>
<nextsent>we do not discard this information, thus simulating situation in which near-perfect terminology-recognition tool is at ones disposal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1978">
<title id=" W04-0313.xml">modeling sentence processing in actr </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this chunk can then be harvested fromthe visual buffer; it includes (as slot-value specifi cations) information about the content of the item seen, its x-y coordinates, etc. one can define an action based on this information, such as retrieving chunk from dm.
</prevsent>
<prevsent>3 modeling sentence parsing in act-r.
</prevsent>
</prevsection>
<citsent citstr=" C92-1032 ">
previous research suggests that humans employ some variant of left-corner parsing (see, e.g., (resnik, 1992)), <papid> C92-1032 </papid>which in essence involves bottom-up and top-down (predictive) step.</citsent>
<aftsection>
<nextsent>we adopt this parsing strategy in the simulations.
</nextsent>
<nextsent>inorder to model the prediction of syntactic structure based on incrementally appearing input, we assume that sentence structure templates are available in declarative memory as underspecified chunks.
</nextsent>
<nextsent>these chunks are retrieved every time new word is integrated into the structure, as are prior arguments necessary for semantic integration.
</nextsent>
<nextsent>we illustrate the parsing process with simple example (figure 2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1979">
<title id=" W04-1005.xml">vocabulary usage in newswire summaries </title>
<section> the evaluation model.  </section>
<citcontext>
<prevsection>
<prevsent>test documents were tokenized by breaking the text on white space and trimming off punctuation external to the token.
</prevsent>
<prevsent>instances of each sort of item were recorded in hash table and written to file.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
tokens are an obvious and unambiguous baseline for lexical agreement, one used by such summary evaluation systems as rouge (lin and hovy, 2003).<papid> N03-1020 </papid></citsent>
<aftsection>
<nextsent>on the other hand, it is important to explain what we mean by units we call phrases; they should not be confused with syntactically correct constituents such as noun phrases or verb phrases.
</nextsent>
<nextsent>our units often are not syntactically well-formed.
</nextsent>
<nextsent>adjacent constituents not separated by stop word are unified, single constituents are divided on any embedded stop word, and those composed entirely of stop words are simply missed.
</nextsent>
<nextsent>our phrases, however, are not n-grams.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1980">
<title id=" W04-1005.xml">vocabulary usage in newswire summaries </title>
<section> pertinent research.  </section>
<citcontext>
<prevsection>
<prevsent>it may be argued that the lower counts for inter summary vocabulary agreement can be explained thus: since summary is so much smaller than its source document, lower counts should result.
</prevsent>
<prevsent>one reply to that argument is that, while acknowledging that synonymy, generalization and specialization would augment the values found, the essence of generic summary is to report the pith, the gist, the central points, of document and that these key elements should not vary so widely from one summary to the next.
</prevsent>
</prevsection>
<citsent citstr=" W02-0406 ">
previous research addressing summary vocabulary is limited, and most has been undertaken in connection with another issue: either with the problem of evaluating summary quality (mani, 2001; lin and hovy, 2002) <papid> W02-0406 </papid>or to assess sentence element suitability for use in summary (jing and mckeown, 1999).</citsent>
<aftsection>
<nextsent>in such case results arise as byproduct of the main line of research and conclusions about vocabulary must be inferred from other findings.
</nextsent>
<nextsent>mani (2001) reports that previous studies, most of which have focused on extracts, have shown evidence of low agreement among humans as to which sentences are good summary sentences.?
</nextsent>
<nextsent>lin and hovys (2002) <papid> W02-0406 </papid>discovery of low inter rater agreement in single (~40%) and multiple (~29%) summary evaluation may also pertain to our findings.</nextsent>
<nextsent>it stands to reason that individuals who disagree on sentence pertinence or do not rate the same summary highly are not likely to use the same words to write the summary.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1982">
<title id=" W04-2609.xml">models for the semantic classification of noun phrases </title>
<section> problem description.  </section>
<citcontext>
<prevsection>
<prevsent>this is called semantic blend (quirk et al 1985).
</prevsent>
<prevsent>for example, the expression texas city?
</prevsent>
</prevsection>
<citsent citstr=" C94-2125 ">
contains both location as well as part-whole relation.other researchers have identified other sets of semantic relations (levi 1979), (vanderwende 1994), (<papid> C94-2125 </papid>sowa 1994), (baker, fillmore, and lowe 1998), (rosario and hearst 2001), (<papid> W01-0511 </papid>kingsbury, et al  2002), (blaheta and charniak 2000), (<papid> A00-2031 </papid>gildea and jurafsky 2002), (<papid> J02-3001 </papid>gildea and palmer 2002).<papid> P02-1031 </papid></citsent>
<aftsection>
<nextsent>our list contains the most frequently used semantic relations we have observed on large cor pus.besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals.
</nextsent>
<nextsent>the focus here is to determine the semantic relations that hold between different concepts within thesame phrase, and to analyze the meaning of these compounds.
</nextsent>
<nextsent>several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (lauer and dras1994), (pustejovsky et al  1993).<papid> J93-2005 </papid></nextsent>
<nextsent>another popular approach focuses on the interpretation of the underlying se mantics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1983">
<title id=" W04-2609.xml">models for the semantic classification of noun phrases </title>
<section> problem description.  </section>
<citcontext>
<prevsection>
<prevsent>this is called semantic blend (quirk et al 1985).
</prevsent>
<prevsent>for example, the expression texas city?
</prevsent>
</prevsection>
<citsent citstr=" W01-0511 ">
contains both location as well as part-whole relation.other researchers have identified other sets of semantic relations (levi 1979), (vanderwende 1994), (<papid> C94-2125 </papid>sowa 1994), (baker, fillmore, and lowe 1998), (rosario and hearst 2001), (<papid> W01-0511 </papid>kingsbury, et al  2002), (blaheta and charniak 2000), (<papid> A00-2031 </papid>gildea and jurafsky 2002), (<papid> J02-3001 </papid>gildea and palmer 2002).<papid> P02-1031 </papid></citsent>
<aftsection>
<nextsent>our list contains the most frequently used semantic relations we have observed on large cor pus.besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals.
</nextsent>
<nextsent>the focus here is to determine the semantic relations that hold between different concepts within thesame phrase, and to analyze the meaning of these compounds.
</nextsent>
<nextsent>several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (lauer and dras1994), (pustejovsky et al  1993).<papid> J93-2005 </papid></nextsent>
<nextsent>another popular approach focuses on the interpretation of the underlying se mantics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1985">
<title id=" W04-2609.xml">models for the semantic classification of noun phrases </title>
<section> problem description.  </section>
<citcontext>
<prevsection>
<prevsent>this is called semantic blend (quirk et al 1985).
</prevsent>
<prevsent>for example, the expression texas city?
</prevsent>
</prevsection>
<citsent citstr=" A00-2031 ">
contains both location as well as part-whole relation.other researchers have identified other sets of semantic relations (levi 1979), (vanderwende 1994), (<papid> C94-2125 </papid>sowa 1994), (baker, fillmore, and lowe 1998), (rosario and hearst 2001), (<papid> W01-0511 </papid>kingsbury, et al  2002), (blaheta and charniak 2000), (<papid> A00-2031 </papid>gildea and jurafsky 2002), (<papid> J02-3001 </papid>gildea and palmer 2002).<papid> P02-1031 </papid></citsent>
<aftsection>
<nextsent>our list contains the most frequently used semantic relations we have observed on large cor pus.besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals.
</nextsent>
<nextsent>the focus here is to determine the semantic relations that hold between different concepts within thesame phrase, and to analyze the meaning of these compounds.
</nextsent>
<nextsent>several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (lauer and dras1994), (pustejovsky et al  1993).<papid> J93-2005 </papid></nextsent>
<nextsent>another popular approach focuses on the interpretation of the underlying se mantics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1986">
<title id=" W04-2609.xml">models for the semantic classification of noun phrases </title>
<section> problem description.  </section>
<citcontext>
<prevsection>
<prevsent>this is called semantic blend (quirk et al 1985).
</prevsent>
<prevsent>for example, the expression texas city?
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
contains both location as well as part-whole relation.other researchers have identified other sets of semantic relations (levi 1979), (vanderwende 1994), (<papid> C94-2125 </papid>sowa 1994), (baker, fillmore, and lowe 1998), (rosario and hearst 2001), (<papid> W01-0511 </papid>kingsbury, et al  2002), (blaheta and charniak 2000), (<papid> A00-2031 </papid>gildea and jurafsky 2002), (<papid> J02-3001 </papid>gildea and palmer 2002).<papid> P02-1031 </papid></citsent>
<aftsection>
<nextsent>our list contains the most frequently used semantic relations we have observed on large cor pus.besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals.
</nextsent>
<nextsent>the focus here is to determine the semantic relations that hold between different concepts within thesame phrase, and to analyze the meaning of these compounds.
</nextsent>
<nextsent>several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (lauer and dras1994), (pustejovsky et al  1993).<papid> J93-2005 </papid></nextsent>
<nextsent>another popular approach focuses on the interpretation of the underlying se mantics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1987">
<title id=" W04-2609.xml">models for the semantic classification of noun phrases </title>
<section> problem description.  </section>
<citcontext>
<prevsection>
<prevsent>this is called semantic blend (quirk et al 1985).
</prevsent>
<prevsent>for example, the expression texas city?
</prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
contains both location as well as part-whole relation.other researchers have identified other sets of semantic relations (levi 1979), (vanderwende 1994), (<papid> C94-2125 </papid>sowa 1994), (baker, fillmore, and lowe 1998), (rosario and hearst 2001), (<papid> W01-0511 </papid>kingsbury, et al  2002), (blaheta and charniak 2000), (<papid> A00-2031 </papid>gildea and jurafsky 2002), (<papid> J02-3001 </papid>gildea and palmer 2002).<papid> P02-1031 </papid></citsent>
<aftsection>
<nextsent>our list contains the most frequently used semantic relations we have observed on large cor pus.besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals.
</nextsent>
<nextsent>the focus here is to determine the semantic relations that hold between different concepts within thesame phrase, and to analyze the meaning of these compounds.
</nextsent>
<nextsent>several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (lauer and dras1994), (pustejovsky et al  1993).<papid> J93-2005 </papid></nextsent>
<nextsent>another popular approach focuses on the interpretation of the underlying se mantics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1988">
<title id=" W04-2609.xml">models for the semantic classification of noun phrases </title>
<section> problem description.  </section>
<citcontext>
<prevsection>
<prevsent>our list contains the most frequently used semantic relations we have observed on large cor pus.besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals.
</prevsent>
<prevsent>the focus here is to determine the semantic relations that hold between different concepts within thesame phrase, and to analyze the meaning of these compounds.
</prevsent>
</prevsection>
<citsent citstr=" J93-2005 ">
several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (lauer and dras1994), (pustejovsky et al  1993).<papid> J93-2005 </papid></citsent>
<aftsection>
<nextsent>another popular approach focuses on the interpretation of the underlying semantics.
</nextsent>
<nextsent>many researchers that followed this approach relied mostly on hand-coded rules (finin 1980), (van der wende 1994).<papid> C94-2125 </papid></nextsent>
<nextsent>more recently, (rosario and hearst 2001), (<papid> W01-0511 </papid>rosario, hearst, and fillmore 2002), (lapata 2002) <papid> J02-3004 </papid>have proposed automatic methods that analyze and detect noun compounds relations from text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA1992">
<title id=" W04-2609.xml">models for the semantic classification of noun phrases </title>
<section> problem description.  </section>
<citcontext>
<prevsection>
<prevsent>another popular approach focuses on the interpretation of the underlying semantics.
</prevsent>
<prevsent>many researchers that followed this approach relied mostly on hand-coded rules (finin 1980), (van der wende 1994).<papid> C94-2125 </papid></prevsent>
</prevsection>
<citsent citstr=" J02-3004 ">
more recently, (rosario and hearst 2001), (<papid> W01-0511 </papid>rosario, hearst, and fillmore 2002), (lapata 2002) <papid> J02-3004 </papid>have proposed automatic methods that analyze and detect noun compounds relations from text.</citsent>
<aftsection>
<nextsent>(rosario and hearst 2001) <papid> W01-0511 </papid>focused on the medical domain making useof lexical ontology and standard machine learning tech niques.</nextsent>
<nextsent>2.1 basic approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2005">
<title id=" W04-2609.xml">models for the semantic classification of noun phrases </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>the annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation from those listed in table 1.
</prevsent>
<prevsent>inter-annotator agreement the annotators, four phd students in computational semantics worked in groups of two, each group focusing on one half of the corpora to annotate.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
noun - noun (adjective - noun, respectively) sequences of words were extracted using the lauer heuristic (lauer 1995) which looks for consecutive pairs of nouns that are neither preceded nor succeeded by noun after each sentence was syntactically parsed with charniak parser (charniak2001) (<papid> P01-1017 </papid>for xwn we used the gold parse trees).</citsent>
<aftsection>
<nextsent>more over, they were provided with the sentence in which the pairs occurred along with their corresponding wordnetsenses.
</nextsent>
<nextsent>whenever the annotators found an example encoding semantic relation other than those provided or they didnt know what interpretation to give, they had to tag it as others?.
</nextsent>
<nextsent>besides the type of relation, the annotators were asked to provide information about the order of the modifier and the head nouns in the syntactic constructions if applicable.
</nextsent>
<nextsent>for instance, in owner of car?-possession the possessor owner is followed by the posses see car, while in car of john?-possession/r the order is reversed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2006">
<title id=" W04-2413.xml">semantic role labelling with chunk sequences </title>
<section> classification.  </section>
<citcontext>
<prevsection>
<prevsent>the value of feature   for class
</prevsent>
<prevsent>, and *  the weight assigned to &amp;  . the model is trained by optimising the weights *  subject to the maximum entropy constraint which ensures that the least committal optimal model is learnt.
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
we used the estimate software for estimation, which implements thelmvm algorithm (malouf, 2002) <papid> W02-2018 </papid>and was kindly provided by rob malouf.</citsent>
<aftsection>
<nextsent>we chose maximum entropy approach because itcan integrate many different sources of information with out assuming independence of features.
</nextsent>
<nextsent>also, models with minimal commitment are good predictors of future data.
</nextsent>
<nextsent>maxent models have found wide application in nlp during the recent years; for semantic role labelling (on framenet data) see (fleischman et al, 2003).<papid> W03-1007 </papid></nextsent>
<nextsent>3.2 classification procedure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2007">
<title id=" W04-2413.xml">semantic role labelling with chunk sequences </title>
<section> classification.  </section>
<citcontext>
<prevsection>
<prevsent>we chose maximum entropy approach because itcan integrate many different sources of information with out assuming independence of features.
</prevsent>
<prevsent>also, models with minimal commitment are good predictors of future data.
</prevsent>
</prevsection>
<citsent citstr=" W03-1007 ">
maxent models have found wide application in nlp during the recent years; for semantic role labelling (on framenet data) see (fleischman et al, 2003).<papid> W03-1007 </papid></citsent>
<aftsection>
<nextsent>3.2 classification procedure.
</nextsent>
<nextsent>the most straightforward procedure would be to have the classifier assign all argument classes plus nolabel to sequences.
</nextsent>
<nextsent>however, this proved ineffective due to the prevalence of nolabel: since this class makes up more than 80% of the training sequences, the classifier concentrates on assigning nolabel well.
</nextsent>
<nextsent>therefore, we divide the task of automatic semantic role assignment into two classification subtasks: argument identification and argument labelling.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2008">
<title id=" W04-2413.xml">semantic role labelling with chunk sequences </title>
<section> classification.  </section>
<citcontext>
<prevsection>
<prevsent>a similar feature judges this by the combination of divider and sequence.
</prevsent>
<prevsent>features based on em-based clustering.
</prevsent>
</prevsection>
<citsent citstr=" P99-1014 ">
we useem-based clustering to measure the fit between target verb, an argument position of the verb, and the head lemma (or head named entity) of sequence.em-based clustering, originally introduced for the induction of semantically annotated lexicon (rooth et al, 1999), <papid> P99-1014 </papid>regards classes as hidden variables in the context of maximum likelihood estimation from incomplete data via the expectation maximisation algorithm.in our application, we aim at deriving probability distribution ff+, on verb-argument pairs + from the trainingdata.</citsent>
<aftsection>
<nextsent>using the key idea that + is conditioned on an unobserved class
</nextsent>
<nextsent>.-0/ , we define the probability of pair +12 +435(fi+67 -98 3;: 8 6 as:   + =   #@?a
</nextsent>
<nextsent>(fi+ 2  #@?a
</nextsent>
<nextsent>  #@?a
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2009">
<title id=" W04-1501.xml">dependency and relational structure in treebank annotation </title>
<section> annotation of the relational.  </section>
<citcontext>
<prevsection>
<prevsent>and ceplova?, 2000), (bohmova?
</prevsent>
<prevsent>et al, 2003)) implements three level annotation scheme where both the analytical (surface syntactic) and tectogrammatical level (deep syntactic and topic-focus articulation) are dependency-based; the english dependency treebank (rambow et al, 2002) implements dependency-basedmono-stratal analysis which encompasses surface and deep syntax and directly represents the predicate-argument structure.
</prevsent>
</prevsection>
<citsent citstr=" C02-2025 ">
other projects adopt mixed formalisms where the sentence is split in syntactic subunits (phrases), but linked by functional or semantic relations, e.g. the negra treebank for german ((brants et al, 2003), (skut et al, 1998)), the alpino treebank for dutch (van der beek et al, 2002), and the lingo redwood treebank for english (oepen et al, 2002).<papid> C02-2025 </papid></citsent>
<aftsection>
<nextsent>also in the penn treebank ((marcus et al, 1993), (<papid> J93-2004 </papid>marcus et al, 1994)) <papid> H94-1020 </papid>limited set of relations is placed over the constituency based annotation in order to make explicit the (morpho-syntactic or semantic) roles that the constituents play.</nextsent>
<nextsent>the choice of rs-based annotation schema can depend on theoretical linguistic motivations(a rs-based schema allows for an explicit, fine grained representation of several linguistic phenomena), task-dependent motivations (the rsbased schema represents the linguistic information involved in the task(s) at hand), language dependent motivations (the relational structure is traditionally considered as the most adequate representation of the object language).theoretical motivations for exploiting representations based on forms of rs was developed in the several rs-based theoretical linguistic frameworks (e.g. lexical functional grammar, relaional grammar and dependency grammar), which allow for capturing information involved at various level (e.g. syntactic and semantic)in linguistic structures, and grammatical formalisms have been proposed with the aim to capture the linguistic knowledge represented in these frameworks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2010">
<title id=" W04-1501.xml">dependency and relational structure in treebank annotation </title>
<section> annotation of the relational.  </section>
<citcontext>
<prevsection>
<prevsent>et al, 2003)) implements three level annotation scheme where both the analytical (surface syntactic) and tectogrammatical level (deep syntactic and topic-focus articulation) are dependency-based; the english dependency treebank (rambow et al, 2002) implements dependency-basedmono-stratal analysis which encompasses surface and deep syntax and directly represents the predicate-argument structure.
</prevsent>
<prevsent>other projects adopt mixed formalisms where the sentence is split in syntactic subunits (phrases), but linked by functional or semantic relations, e.g. the negra treebank for german ((brants et al, 2003), (skut et al, 1998)), the alpino treebank for dutch (van der beek et al, 2002), and the lingo redwood treebank for english (oepen et al, 2002).<papid> C02-2025 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
also in the penn treebank ((marcus et al, 1993), (<papid> J93-2004 </papid>marcus et al, 1994)) <papid> H94-1020 </papid>limited set of relations is placed over the constituency based annotation in order to make explicit the (morpho-syntactic or semantic) roles that the constituents play.</citsent>
<aftsection>
<nextsent>the choice of rs-based annotation schema can depend on theoretical linguistic motivations(a rs-based schema allows for an explicit, fine grained representation of several linguistic phenomena), task-dependent motivations (the rsbased schema represents the linguistic information involved in the task(s) at hand), language dependent motivations (the relational structure is traditionally considered as the most adequate representation of the object language).theoretical motivations for exploiting representations based on forms of rs was developed in the several rs-based theoretical linguistic frameworks (e.g. lexical functional grammar, relaional grammar and dependency grammar), which allow for capturing information involved at various level (e.g. syntactic and semantic)in linguistic structures, and grammatical formalisms have been proposed with the aim to capture the linguistic knowledge represented in these frameworks.
</nextsent>
<nextsent>since the most immediate way to build wide-coverage grammars is to extract them directly from linguistic data (i.e. from treebanks), the type of annotation used in the data is factor of primary importance, i.e. rs-based annotation allows for the extraction of more descriptive grammar1.
</nextsent>
<nextsent>1see (mazzei and lombardo, 2004a) and (mazzei and lombardo, 2004b) for experiments of ltag extraction from tut.
</nextsent>
<nextsent>task-dependent motivations relyon how the annotation of the rs can facilitate some processing aspects of nlp applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2011">
<title id=" W04-1501.xml">dependency and relational structure in treebank annotation </title>
<section> annotation of the relational.  </section>
<citcontext>
<prevsection>
<prevsent>et al, 2003)) implements three level annotation scheme where both the analytical (surface syntactic) and tectogrammatical level (deep syntactic and topic-focus articulation) are dependency-based; the english dependency treebank (rambow et al, 2002) implements dependency-basedmono-stratal analysis which encompasses surface and deep syntax and directly represents the predicate-argument structure.
</prevsent>
<prevsent>other projects adopt mixed formalisms where the sentence is split in syntactic subunits (phrases), but linked by functional or semantic relations, e.g. the negra treebank for german ((brants et al, 2003), (skut et al, 1998)), the alpino treebank for dutch (van der beek et al, 2002), and the lingo redwood treebank for english (oepen et al, 2002).<papid> C02-2025 </papid></prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
also in the penn treebank ((marcus et al, 1993), (<papid> J93-2004 </papid>marcus et al, 1994)) <papid> H94-1020 </papid>limited set of relations is placed over the constituency based annotation in order to make explicit the (morpho-syntactic or semantic) roles that the constituents play.</citsent>
<aftsection>
<nextsent>the choice of rs-based annotation schema can depend on theoretical linguistic motivations(a rs-based schema allows for an explicit, fine grained representation of several linguistic phenomena), task-dependent motivations (the rsbased schema represents the linguistic information involved in the task(s) at hand), language dependent motivations (the relational structure is traditionally considered as the most adequate representation of the object language).theoretical motivations for exploiting representations based on forms of rs was developed in the several rs-based theoretical linguistic frameworks (e.g. lexical functional grammar, relaional grammar and dependency grammar), which allow for capturing information involved at various level (e.g. syntactic and semantic)in linguistic structures, and grammatical formalisms have been proposed with the aim to capture the linguistic knowledge represented in these frameworks.
</nextsent>
<nextsent>since the most immediate way to build wide-coverage grammars is to extract them directly from linguistic data (i.e. from treebanks), the type of annotation used in the data is factor of primary importance, i.e. rs-based annotation allows for the extraction of more descriptive grammar1.
</nextsent>
<nextsent>1see (mazzei and lombardo, 2004a) and (mazzei and lombardo, 2004b) for experiments of ltag extraction from tut.
</nextsent>
<nextsent>task-dependent motivations relyon how the annotation of the rs can facilitate some processing aspects of nlp applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2012">
<title id=" W04-1501.xml">dependency and relational structure in treebank annotation </title>
<section> annotation of the relational.  </section>
<citcontext>
<prevsection>
<prevsent>the explicit representation of predicative structures allowed by the rs can be powerful source of disambiguation.
</prevsent>
<prevsent>in fact, large amount of ambiguity (such as coordination, noun-noun compounds and relative clause attachment) can be resolved using such kind of information, and relations can provide useful interface between syntax and semantics.
</prevsent>
</prevsection>
<citsent citstr=" P91-1030 ">
(hindle and rooth, 1991) <papid> P91-1030 </papid>had shown the use of dependency in prepositional phrase disambiguation, andthe experimental results reported in (hock enmaier, 2003) <papid> P03-1046 </papid>demonstrate that language model which encodes rich notion of predicate argument structure (e.g. including long-range relations arising through coordination) can significantly improve the parsing performances.</citsent>
<aftsection>
<nextsent>moreover, the notion of predicate argument structure has been advocated as useful ina number of different large-scale language processing tasks, and the rs is convenient intermediate representation in several applications (see (bosco, 2004) for survey on this topic).
</nextsent>
<nextsent>for instance, in information extraction relations allows for recognizing different guises in which an event can appear regardless of the several different syntactic patterns that can be used to specify it (palmer et al, 2001)<papid> H01-1010 </papid>2.</nextsent>
<nextsent>in question answering, systems usually use forms of relation-based structured representations of the input texts (i.e. questions and answers) and try to match those representations (see e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2013">
<title id=" W04-1501.xml">dependency and relational structure in treebank annotation </title>
<section> annotation of the relational.  </section>
<citcontext>
<prevsection>
<prevsent>the explicit representation of predicative structures allowed by the rs can be powerful source of disambiguation.
</prevsent>
<prevsent>in fact, large amount of ambiguity (such as coordination, noun-noun compounds and relative clause attachment) can be resolved using such kind of information, and relations can provide useful interface between syntax and semantics.
</prevsent>
</prevsection>
<citsent citstr=" P03-1046 ">
(hindle and rooth, 1991) <papid> P91-1030 </papid>had shown the use of dependency in prepositional phrase disambiguation, andthe experimental results reported in (hock enmaier, 2003) <papid> P03-1046 </papid>demonstrate that language model which encodes rich notion of predicate argument structure (e.g. including long-range relations arising through coordination) can significantly improve the parsing performances.</citsent>
<aftsection>
<nextsent>moreover, the notion of predicate argument structure has been advocated as useful ina number of different large-scale language processing tasks, and the rs is convenient intermediate representation in several applications (see (bosco, 2004) for survey on this topic).
</nextsent>
<nextsent>for instance, in information extraction relations allows for recognizing different guises in which an event can appear regardless of the several different syntactic patterns that can be used to specify it (palmer et al, 2001)<papid> H01-1010 </papid>2.</nextsent>
<nextsent>in question answering, systems usually use forms of relation-based structured representations of the input texts (i.e. questions and answers) and try to match those representations (see e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2014">
<title id=" W04-1501.xml">dependency and relational structure in treebank annotation </title>
<section> annotation of the relational.  </section>
<citcontext>
<prevsection>
<prevsent>(hindle and rooth, 1991) <papid> P91-1030 </papid>had shown the use of dependency in prepositional phrase disambiguation, andthe experimental results reported in (hock enmaier, 2003) <papid> P03-1046 </papid>demonstrate that language model which encodes rich notion of predicate argument structure (e.g. including long-range relations arising through coordination) can significantly improve the parsing performances.</prevsent>
<prevsent>moreover, the notion of predicate argument structure has been advocated as useful ina number of different large-scale language processing tasks, and the rs is convenient intermediate representation in several applications (see (bosco, 2004) for survey on this topic).</prevsent>
</prevsection>
<citsent citstr=" H01-1010 ">
for instance, in information extraction relations allows for recognizing different guises in which an event can appear regardless of the several different syntactic patterns that can be used to specify it (palmer et al, 2001)<papid> H01-1010 </papid>2.</citsent>
<aftsection>
<nextsent>in question answering, systems usually use forms of relation-based structured representations of the input texts (i.e. questions and answers) and try to match those representations (see e.g.
</nextsent>
<nextsent>(litkowski, 1999), (buchholz, 2002)).
</nextsent>
<nextsent>also the in-depth understanding of the text, necessary in machine translation task, requires the use of relation-based representations where an accurate predicate argument structure is critical factor (han et al, 2000)3.
</nextsent>
<nextsent>language-dependent motivations relyon the fact that the dependency-based formalisms hasbeen traditionally considered as the most adequate for the representation of free word order languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2015">
<title id=" W04-1501.xml">dependency and relational structure in treebank annotation </title>
<section> tut: dependency-based.  </section>
<citcontext>
<prevsection>
<prevsent>in this section, we concentrate on the major features of tut annotation schema, i.e. how the ars system can describe dependency structure.
</prevsent>
<prevsent>4.1 dependency-based schema in italian the order of words is fixed in nonverbal phrases, but verbal arguments and modifiers can be freely distributed without affecting the semantic interpretation of the sentence.
</prevsent>
</prevsection>
<citsent citstr=" J89-1001 ">
a study on set of sentences of tut shows that the basic word order for italian is subjectverb-complement (svc), as known in literature (renzi, 1988), (stock, 1989), <papid> J89-1001 </papid>but in more than quarter of declarative sentences it is violated (see the following table6).</citsent>
<aftsection>
<nextsent>although the permutations occurrences v 74,26% c 11,38% c 7,98% s 3,23% s 2,29% v 0,77% table 1: italian word order svc order is well over the others, the fact thatall the other orders are represented quantitatively confirms the assumption of partial con figurationality intuitively set in the literature.
</nextsent>
<nextsent>the partial configurationality of italian can be considered as language-dependent motivation for the choice of dependency-based annotation for an italian treebank.
</nextsent>
<nextsent>the schema is similar to that of the prague dependency tree bank analytical-syntactic level with which tut6the data reported in the table refer to 1,200 annotated sentences where 1,092 verbal predicate argument structures involving subject and at least one other complement occur.shares the following basic design principles typical of the dependency paradigm:  the sentence is represented by tree where each node represents word and each edge represents dependency labelled by grammatical relation which involves head and dependent,  each single word and punctuation markis represented by single node, the so called amalgamated words, which are words composed by lexical units that can occur both in compounds and alone, e.g. verbs with clitic suffixes (amarti (to love-you) or prepositions with article (dal (from-the)), are split in more lexical units7, since the constituent structure of the sentence is implicit in dependency trees, no phrases are annotated8.
</nextsent>
<nextsent>if the partial configurationality makes the dependency-based annotation more adequate for italian, other features of this language should be well represented by exploiting negra-like format where the syntactic units are phrases rather than single words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2016">
<title id=" W04-0207.xml">text type structure and logical document structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>documents in html.
</prevsent>
<prevsent>our aim is to explore similar methods to classify thematic segments of scientific articles in xml.previous approaches to the text type structure of scientific articles have been developed in the context of automatic text summarization.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
in teufel and moens(2002), <papid> J02-4002 </papid>categorization scheme of seven rhetorical zones?</citsent>
<aftsection>
<nextsent>including background, other, own, aim, textual, contrast, basis is used for the classification of the sentences of scientific article according to their rhetorical status and subsequently finding the most suitable ones for summary.
</nextsent>
<nextsent>the schema we employ (see below) is more fine-grained, consisting of 16 categories that are hierarchically ordered (although in the present experiments we did notmake use of this hierarchical order).
</nextsent>
<nextsent>these 16 categories refer to the dimension that is discussed under problem structure in (teufel and moens, 2002), <papid> J02-4002 </papid>rather than to exclusively rhetorical zones and are viewed as types of topics.</nextsent>
<nextsent>a topic is the semantic pragmatic function that selects which concept of the contextual information will be extended with new information?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2018">
<title id=" W04-0207.xml">text type structure and logical document structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>empty  group  elements represent topic types that corresponded to the nodes in the original tree of topic types, while segment  elements correspond to leaves (termi nal categories).
</prevsent>
<prevsent>the original hierarchical structure is still represented via the id/idref attributes idand parent, similar to odonnells (2000) representation of rhetorical structure trees.for the annotation, the raw text of each article was automatically partitioned into text segments corresponding to sentences, but the annotators were allowed to modify (join or split) segments to yield proper thematic units.
</prevsent>
</prevsection>
<citsent citstr=" P01-1064 ">
the problem of finding thematic boundaries other than sentence boundaries automatically (e.g. utiyama and isahara (2001)) <papid> P01-1064 </papid>is thus not addressed in this work.</citsent>
<aftsection>
<nextsent>the annotator then provided the values of the attribute topic using the xml spy editor, choosing exactly one of the16 terminal topic types for each segment, or alternatively the category void meta for meta data such as acknowledgements.
</nextsent>
<nextsent>if more than one topic type could in principle be assigned, the annotators were instructed to choose the one that was most central tothe argumentation.
</nextsent>
<nextsent>an extract from thm annotation layer is shown in figure 3.2 the two annotators were experienced in that they had received intense training as well as annotated corpus of psychological articles according to an extended version of the schema in figure 1 earlier(bayerl et al, 2003).
</nextsent>
<nextsent>we assessed inter-rater reliability on three articles from the present linguistics corpus, which were annotated by both annotators independently according to the topic type set shown in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2019">
<title id=" W04-0207.xml">text type structure and logical document structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>3.2 annotation of syntax and morphology.
</prevsent>
<prevsent>for an annotation of grammatical categories to word form tokens in our corpus, the commercial tagger machin ese syntax by conn exor oy was employed.
</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
this tagger is rule-based, robust syntactic parser available for several languages and based on constraint grammar and functional dependency grammar (tapanainen and jarvinen, 1997).<papid> A97-1011 </papid></citsent>
<aftsection>
<nextsent>it provides morphological, surface syntactic, and functional tags for each word form and dependency structure for sentences, and besides is able to process and output simple?
</nextsent>
<nextsent>xml (that is, xml withoutattributes).
</nextsent>
<nextsent>no conflicts in terms of element overlaps can arise between our thm annotation layer and the grammatical tagging, because all tags provided by machin ese syntax pertain to word forms.the grammatical annotations could therefore be integrated with the thm annotations, forming the xml annotation layer that we call thmcnx.
</nextsent>
<nextsent>an xslt style sheet is applied to convert the thm annotations into attribute-free xml by integrating the information from attribute-value specifications into the names of their respective elements.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2020">
<title id=" W04-0838.xml">sense learner minimally supervised word sense disambiguation for all words in open text </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>for some natural language processing tasks, suchas part of speech tagging or named entity recognition, regardless of the approach considered, thereis consensus on what makes successful algorithm (resnik and yarowsky, 1997).
</prevsent>
<prevsent>instead, no such consensus has been reached yet for the task of word sense disambiguation, and previous work has considered range of knowledge sources, suchas local collocational clues, membership in semantically or topically related word class, semantic density, etc. other related work has been motivated by the intuition that syntactic information in sentence contains enough information to be able to infer the semantics of words.
</prevsent>
</prevsection>
<citsent citstr=" N01-1012 ">
for example, according to (gomez, 2001), <papid> N01-1012 </papid>the syntax of many verbs is determined by their semantics, and thus it is possible to get the later from the former.</citsent>
<aftsection>
<nextsent>on the other hand, (lin, 1997) <papid> P97-1009 </papid>proposes disambiguation algorithm that relies on the basic intuition thatif two occurrences of the same word have identical meanings, then they should have similar local context.</nextsent>
<nextsent>he then extends this assumption one step further and proposes an algorithm based on the intuition that two different words are likely to have similar meanings if they occur in an identical local context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2021">
<title id=" W04-0838.xml">sense learner minimally supervised word sense disambiguation for all words in open text </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>instead, no such consensus has been reached yet for the task of word sense disambiguation, and previous work has considered range of knowledge sources, suchas local collocational clues, membership in semantically or topically related word class, semantic density, etc. other related work has been motivated by the intuition that syntactic information in sentence contains enough information to be able to infer the semantics of words.
</prevsent>
<prevsent>for example, according to (gomez, 2001), <papid> N01-1012 </papid>the syntax of many verbs is determined by their semantics, and thus it is possible to get the later from the former.</prevsent>
</prevsection>
<citsent citstr=" P97-1009 ">
on the other hand, (lin, 1997) <papid> P97-1009 </papid>proposes disambiguation algorithm that relies on the basic intuition thatif two occurrences of the same word have identical meanings, then they should have similar local context.</citsent>
<aftsection>
<nextsent>he then extends this assumption one step further and proposes an algorithm based on the intuition that two different words are likely to have similar meanings if they occur in an identical local context.
</nextsent>
<nextsent>our goal is to use as little annotated data as possible, and at the same time make the algorithm general enough to be able to disambiguate all content words in text.
</nextsent>
<nextsent>we are therefore using (1) semcor(miller et al, 1993) ? <papid> H93-1061 </papid>balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers ? to learn se association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems mantic language model for the words seen in the training corpus; and (2) information drawn from wordnet (miller, 1995), to derive semantic generalizations for those words that did not appear in the annotated corpus.the input to the disambiguation algorithm consists of raw text.</nextsent>
<nextsent>the output is text with word meaning annotations for all open-class words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2022">
<title id=" W04-0838.xml">sense learner minimally supervised word sense disambiguation for all words in open text </title>
<section> sense learner.  </section>
<citcontext>
<prevsection>
<prevsent>he then extends this assumption one step further and proposes an algorithm based on the intuition that two different words are likely to have similar meanings if they occur in an identical local context.
</prevsent>
<prevsent>our goal is to use as little annotated data as possible, and at the same time make the algorithm general enough to be able to disambiguate all content words in text.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
we are therefore using (1) semcor(miller et al, 1993) ? <papid> H93-1061 </papid>balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers ? to learn se association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems mantic language model for the words seen in the training corpus; and (2) information drawn from wordnet (miller, 1995), to derive semantic generalizations for those words that did not appear in the annotated corpus.the input to the disambiguation algorithm consists of raw text.</citsent>
<aftsection>
<nextsent>the output is text with word meaning annotations for all open-class words.
</nextsent>
<nextsent>the algorithm starts with preprocessing stage, where the text is tokenized and annotated with parts of speech; collocations are identified using sliding window approach, where collocation is considered to be sequence of words that forms compound concept defined in wordnet; named entities are also identified at this stage.
</nextsent>
<nextsent>next, the following two main steps are applied sequentially: 1.
</nextsent>
<nextsent>semantic language model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2023">
<title id=" W04-0838.xml">sense learner minimally supervised word sense disambiguation for all words in open text </title>
<section> sense learner.  </section>
<citcontext>
<prevsection>
<prevsent>the role of this first module is to learn global model for each part of speech, which can be used to disambiguate content words in any input text.
</prevsent>
<prevsent>although significantly more general than models that are built individually for each word in test corpus as in e.g.
</prevsent>
</prevsection>
<citsent citstr=" W02-0814 ">
(hoste et al, 2002) ? <papid> W02-0814 </papid>the models can only handle words that were previously seen in the training corpus, and therefore their coverage is not 100%.</citsent>
<aftsection>
<nextsent>starting with an annotated corpus formed by all annotated files in semcor, separate training dataset is built for each part of speech.
</nextsent>
<nextsent>the following features are used to build the training models.nouns   the first noun, verb, or adjective before the target noun, within window of at most five words to the left, and its part of speech.
</nextsent>
<nextsent>verbs   the first word before and the first word after the target verb, and its part of speech.
</nextsent>
<nextsent>adj   one relying on the first noun after the target adjective, within window of at most five words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2024">
<title id=" W04-0838.xml">sense learner minimally supervised word sense disambiguation for all words in open text </title>
<section> sense learner.  </section>
<citcontext>
<prevsection>
<prevsent>using this procedure, total of 170,146 feature vectors are constructed: 86,973 vectors in the noun model, 47,838 in the verb model, and 35,335 vectors in each of the two adjective models.to annotate new text, similar vectors are created for all content-words in the raw text.
</prevsent>
<prevsent>the vectors are stored in different files based on their syntactic class, and separate learning process is run for each part-of-speech.
</prevsent>
</prevsection>
<citsent citstr=" C02-1039 ">
for learning, we are using the timbl memory based learning algorithm (daelemans et al, 2001), which was previously found useful for the task of word sense disambiguation (mihalcea, 2002).<papid> C02-1039 </papid></citsent>
<aftsection>
<nextsent>following the learning stage, each vector in thetest dataset ? and thus each content word ? is labeled with predicted word and sense.
</nextsent>
<nextsent>if the word predicted by the learning algorithm coincides with the target word in the test feature vector, then the predicted sense is used to annotate the test instance.
</nextsent>
<nextsent>otherwise, if the predicted word is different than the target word, no annotation is produced, and the word is left for annotation in later stage.during the evaluations on the senseval-3 english all-words dataset, 1,782 words were tagged using the semantic language model, resulting in an average coverage of 85.6%.
</nextsent>
<nextsent>3.2 semantic generalizations using syntactic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2026">
<title id=" W04-2209.xml">jmdict a japanese multilingual dictionary </title>
<section> related projects.  </section>
<citcontext>
<prevsection>
<prevsent>this strategy, which required the corpus to be parsed to extract set of index words for each sentence, has proved effective at the application level.
</prevsent>
<prevsent>it also has the advantage of decoupling the maintenance of the dictionary file from that of the example corpus.
</prevsent>
</prevsection>
<citsent citstr=" W02-1705 ">
apart from few small word lists involving several european languages, the only other major current project attempting to compile comprehensive multilingual database is the papillon project (e.g. boitet et al 2002).<papid> W02-1705 </papid></citsent>
<aftsection>
<nextsent>see http://www.papillon-dictionary.org/ for full list of publications.
</nextsent>
<nextsent>the papillon design involves linkages based on word-senses as proposed in (srasset, 1994) with the finer lexical structure based on meaning-text theory (mtt) (mel cuk, 1984-1996).
</nextsent>
<nextsent>at the time of writing the papillon database is still in the process of being populated with lexical information.
</nextsent>
<nextsent>closely related to the jmdict project is the japanese-multilingual named entity dictionary (jmnedict) project.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2027">
<title id=" W04-2105.xml">word lookup on the basis of associations  from an idea to a roadmap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as result,any one of the words or concepts has the potential to evoke each other.
</prevsent>
<prevsent>the likelihood for 2obviously, looking for pseudonym?
</prevsent>
</prevsection>
<citsent citstr=" W02-1118 ">
under the letter s? in dictionary wont be of great help.3temporary amnesia, known as the tot, or tip-of the-tongue problem (brown and mcneill, 1996; zock and fournier, 2001; zock, 2002) <papid> W02-1118 </papid>4http://www.onelook.com/reverse-dictionary.</citsent>
<aftsection>
<nextsent>shtml this to happen depends, among other things, on such factors as frequency (associative strength),saliency and distance (direct vs. indirect ac cess).
</nextsent>
<nextsent>as one can see, associations are very general and powerful mechanism.
</nextsent>
<nextsent>no matter what we hear, read or say, any idea is likely to remind us of something else.5 this being so, we should make use of it.6 3 search based on the relations.
</nextsent>
<nextsent>between concepts and words if one agrees with what we have just said, one could view the mental dictionary as huge semantic network composed of nodes (words and concepts) and links (associations), with either being able to activate the other.7 finding 5the idea according to which the mental dictionary (or encyclopedia) is basically an associative network,composed of nodes (words or concepts) and links (as sociations) is not new, neither is the idea of spreading activation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2028">
<title id=" W04-2105.xml">word lookup on the basis of associations  from an idea to a roadmap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, given the costs, it is impossible to repeat these experiments to take into account the evolution of society.hence, the goal is to automatically extract associations from large corpora.
</prevsent>
<prevsent>this problem was addressed by large number of researchers, butin most cases it was reduced to extraction of collocations which are proper subset of the set of associated words.
</prevsent>
</prevsection>
<citsent citstr=" W93-0310 ">
while hard to define, collocations appear often enough in corpora to be extractable by statistical and information-theory based methods.there are several basic methods for evaluating associations between words: based on frequency counts (choueka, 1988; wettler and rapp, 1993), <papid> W93-0310 </papid>information theoretic (church and hanks, 1990) <papid> J90-1003 </papid>and statistical significance (smadja, 1993).<papid> J93-1007 </papid></citsent>
<aftsection>
<nextsent>the statistical significance often evaluate whether two words are inde pendant using hypothesis tests such as t-score (church et al, 1991), the x2, the log-likelihood(dunning, 1993) <papid> J93-1003 </papid>and fishers exact test (peder sen, 1996).</nextsent>
<nextsent>extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain subset of collo cations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2029">
<title id=" W04-2105.xml">word lookup on the basis of associations  from an idea to a roadmap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, given the costs, it is impossible to repeat these experiments to take into account the evolution of society.hence, the goal is to automatically extract associations from large corpora.
</prevsent>
<prevsent>this problem was addressed by large number of researchers, butin most cases it was reduced to extraction of collocations which are proper subset of the set of associated words.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
while hard to define, collocations appear often enough in corpora to be extractable by statistical and information-theory based methods.there are several basic methods for evaluating associations between words: based on frequency counts (choueka, 1988; wettler and rapp, 1993), <papid> W93-0310 </papid>information theoretic (church and hanks, 1990) <papid> J90-1003 </papid>and statistical significance (smadja, 1993).<papid> J93-1007 </papid></citsent>
<aftsection>
<nextsent>the statistical significance often evaluate whether two words are inde pendant using hypothesis tests such as t-score (church et al, 1991), the x2, the log-likelihood(dunning, 1993) <papid> J93-1003 </papid>and fishers exact test (peder sen, 1996).</nextsent>
<nextsent>extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain subset of collo cations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2030">
<title id=" W04-2105.xml">word lookup on the basis of associations  from an idea to a roadmap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, given the costs, it is impossible to repeat these experiments to take into account the evolution of society.hence, the goal is to automatically extract associations from large corpora.
</prevsent>
<prevsent>this problem was addressed by large number of researchers, butin most cases it was reduced to extraction of collocations which are proper subset of the set of associated words.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
while hard to define, collocations appear often enough in corpora to be extractable by statistical and information-theory based methods.there are several basic methods for evaluating associations between words: based on frequency counts (choueka, 1988; wettler and rapp, 1993), <papid> W93-0310 </papid>information theoretic (church and hanks, 1990) <papid> J90-1003 </papid>and statistical significance (smadja, 1993).<papid> J93-1007 </papid></citsent>
<aftsection>
<nextsent>the statistical significance often evaluate whether two words are inde pendant using hypothesis tests such as t-score (church et al, 1991), the x2, the log-likelihood(dunning, 1993) <papid> J93-1003 </papid>and fishers exact test (peder sen, 1996).</nextsent>
<nextsent>extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain subset of collo cations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2031">
<title id=" W04-2105.xml">word lookup on the basis of associations  from an idea to a roadmap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this problem was addressed by large number of researchers, butin most cases it was reduced to extraction of collocations which are proper subset of the set of associated words.
</prevsent>
<prevsent>while hard to define, collocations appear often enough in corpora to be extractable by statistical and information-theory based methods.there are several basic methods for evaluating associations between words: based on frequency counts (choueka, 1988; wettler and rapp, 1993), <papid> W93-0310 </papid>information theoretic (church and hanks, 1990) <papid> J90-1003 </papid>and statistical significance (smadja, 1993).<papid> J93-1007 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
the statistical significance often evaluate whether two words are inde pendant using hypothesis tests such as t-score (church et al, 1991), the x2, the log-likelihood(dunning, 1993) <papid> J93-1003 </papid>and fishers exact test (peder sen, 1996).</citsent>
<aftsection>
<nextsent>extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain subset of collocations.
</nextsent>
<nextsent>the various extraction measures have been discussed in great detail in the literature (man ning and schutze, 1999; mckeown and radev, 2000), their performance has been compared (dunning, 1993; <papid> J93-1003 </papid>pedersen, 1996; evert andkrenn, 2001), <papid> P01-1025 </papid>and the methods have been combined to improve overall performance (inkpen and hirst, 2002).<papid> W02-0909 </papid></nextsent>
<nextsent>most of these methods were originally applied in large text corpora, butmore recently the web has been used as corpus (pearce, 2001; inkpen and hirst, 2002).<papid> W02-0909 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2033">
<title id=" W04-2105.xml">word lookup on the basis of associations  from an idea to a roadmap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the statistical significance often evaluate whether two words are inde pendant using hypothesis tests such as t-score (church et al, 1991), the x2, the log-likelihood(dunning, 1993) <papid> J93-1003 </papid>and fishers exact test (peder sen, 1996).</prevsent>
<prevsent>extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain subset of collo cations.</prevsent>
</prevsection>
<citsent citstr=" P01-1025 ">
the various extraction measures have been discussed in great detail in the literature (man ning and schutze, 1999; mckeown and radev, 2000), their performance has been compared (dunning, 1993; <papid> J93-1003 </papid>pedersen, 1996; evert andkrenn, 2001), <papid> P01-1025 </papid>and the methods have been combined to improve overall performance (inkpen and hirst, 2002).<papid> W02-0909 </papid></citsent>
<aftsection>
<nextsent>most of these methods were originally applied in large text corpora, butmore recently the web has been used as corpus (pearce, 2001; inkpen and hirst, 2002).<papid> W02-0909 </papid></nextsent>
<nextsent>collocation extraction methods have been usednot only for english, but for many other languages: french (ferret, 2002), <papid> C02-1033 </papid>german (ev ert and krenn, 2001) <papid> P01-1025 </papid>and japanese (nagao and mori, 1994), <papid> C94-1101 </papid>to cite but those.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2034">
<title id=" W04-2105.xml">word lookup on the basis of associations  from an idea to a roadmap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the statistical significance often evaluate whether two words are inde pendant using hypothesis tests such as t-score (church et al, 1991), the x2, the log-likelihood(dunning, 1993) <papid> J93-1003 </papid>and fishers exact test (peder sen, 1996).</prevsent>
<prevsent>extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain subset of collo cations.</prevsent>
</prevsection>
<citsent citstr=" W02-0909 ">
the various extraction measures have been discussed in great detail in the literature (man ning and schutze, 1999; mckeown and radev, 2000), their performance has been compared (dunning, 1993; <papid> J93-1003 </papid>pedersen, 1996; evert andkrenn, 2001), <papid> P01-1025 </papid>and the methods have been combined to improve overall performance (inkpen and hirst, 2002).<papid> W02-0909 </papid></citsent>
<aftsection>
<nextsent>most of these methods were originally applied in large text corpora, butmore recently the web has been used as corpus (pearce, 2001; inkpen and hirst, 2002).<papid> W02-0909 </papid></nextsent>
<nextsent>collocation extraction methods have been usednot only for english, but for many other languages: french (ferret, 2002), <papid> C02-1033 </papid>german (ev ert and krenn, 2001) <papid> P01-1025 </papid>and japanese (nagao and mori, 1994), <papid> C94-1101 </papid>to cite but those.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2038">
<title id=" W04-2105.xml">word lookup on the basis of associations  from an idea to a roadmap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the various extraction measures have been discussed in great detail in the literature (man ning and schutze, 1999; mckeown and radev, 2000), their performance has been compared (dunning, 1993; <papid> J93-1003 </papid>pedersen, 1996; evert andkrenn, 2001), <papid> P01-1025 </papid>and the methods have been combined to improve overall performance (inkpen and hirst, 2002).<papid> W02-0909 </papid></prevsent>
<prevsent>most of these methods were originally applied in large text corpora, butmore recently the web has been used as corpus (pearce, 2001; inkpen and hirst, 2002).<papid> W02-0909 </papid></prevsent>
</prevsection>
<citsent citstr=" C02-1033 ">
collocation extraction methods have been usednot only for english, but for many other languages: french (ferret, 2002), <papid> C02-1033 </papid>german (ev ert and krenn, 2001) <papid> P01-1025 </papid>and japanese (nagao and mori, 1994), <papid> C94-1101 </papid>to cite but those.</citsent>
<aftsection>
<nextsent>the most obvious question in this context is to clarify to what extent available collocation extraction techniques fulfill our needs of extracting and labeling word associations.
</nextsent>
<nextsent>since collocations are subset of association, it is possible to apply collocation extraction techniques to obtain related words, ordered in terms of the relative strength of association.
</nextsent>
<nextsent>the result of this kind of numerical extraction would be large set of numerically weighted word pairs.
</nextsent>
<nextsent>the problem with this approach is that the links are only labeled in terms of their relative associative strength, but not categorically, which makes it impossible to group and present them in meaningful way for the dictionary user.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2040">
<title id=" W04-2105.xml">word lookup on the basis of associations  from an idea to a roadmap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the various extraction measures have been discussed in great detail in the literature (man ning and schutze, 1999; mckeown and radev, 2000), their performance has been compared (dunning, 1993; <papid> J93-1003 </papid>pedersen, 1996; evert andkrenn, 2001), <papid> P01-1025 </papid>and the methods have been combined to improve overall performance (inkpen and hirst, 2002).<papid> W02-0909 </papid></prevsent>
<prevsent>most of these methods were originally applied in large text corpora, butmore recently the web has been used as corpus (pearce, 2001; inkpen and hirst, 2002).<papid> W02-0909 </papid></prevsent>
</prevsection>
<citsent citstr=" C94-1101 ">
collocation extraction methods have been usednot only for english, but for many other languages: french (ferret, 2002), <papid> C02-1033 </papid>german (ev ert and krenn, 2001) <papid> P01-1025 </papid>and japanese (nagao and mori, 1994), <papid> C94-1101 </papid>to cite but those.</citsent>
<aftsection>
<nextsent>the most obvious question in this context is to clarify to what extent available collocation extraction techniques fulfill our needs of extracting and labeling word associations.
</nextsent>
<nextsent>since collocations are subset of association, it is possible to apply collocation extraction techniques to obtain related words, ordered in terms of the relative strength of association.
</nextsent>
<nextsent>the result of this kind of numerical extraction would be large set of numerically weighted word pairs.
</nextsent>
<nextsent>the problem with this approach is that the links are only labeled in terms of their relative associative strength, but not categorically, which makes it impossible to group and present them in meaningful way for the dictionary user.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2041">
<title id=" W04-2105.xml">word lookup on the basis of associations  from an idea to a roadmap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>only once thisis done, human being could use it to navigate through large conceptual-lexical network(the dictionary) as described above.
</prevsent>
<prevsent>unfortunately, research on automatic link identification has been rather sparse.
</prevsent>
</prevsection>
<citsent citstr=" C02-1038 ">
most attempts have been devoted to the extraction of certain types of links (usually syntactic type (lin, 1998) oron extensions of wordnet with topical information contained in thesaurus (stevenson, 2002)<papid> C02-1038 </papid>or on the www (agirre et al, 2000).</citsent>
<aftsection>
<nextsent>additional methods need to be considered in orderto reveal (automatically) the kind of associations holding between words and/or concepts.
</nextsent>
<nextsent>earlier in this paper we have suggested the use of an encyclopedia as source of general world knowledge.
</nextsent>
<nextsent>it should be noted, though, that there are important differences between large corpora and encyclopedias.
</nextsent>
<nextsent>large corpora usually contain lot of repetitive texts on limited number of topics (e.g. newspaper articles) which makes them very suitable for statisticalmethods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2042">
<title id=" W03-2121.xml">speaker independent context update rules for dialogue management </title>
<section> background: declarative and procedural.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, in chart parser the process of sentence parsing is broken down into sequence of procedural operations each of which has exactly the same general form: search of the set of grammar rules, and the creation of new chart edge if the search is successful.
</prevsent>
<prevsent>in fact, the benefits of thinking of aparser as procedural module consulting declarative grammatical resource are most clearly seen inthe fact that the procedural component can be expressed systematically in this way.the declarative/procedural distinction is also increasingly common in computational treatments of extended monologues.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
there are several overtly declarative theories of the structure of such texts (many of them stemming from the work of mann and thompson (1988) and grosz and sidner (1986)), <papid> J86-3001 </papid>and several models of text generation and text interpretation which make reference to these declarative theories (see e.g. hovy (1993) andmarcu (2000) for summary of generation and interpretation methods respectively).</citsent>
<aftsection>
<nextsent>again, the most attractive feature of the declarative/procedural distinction is that the procedural algorithms envisaged are very systematic.in models of dialogue structure, clean separation between declarative and procedural models has proved more elusive.
</nextsent>
<nextsent>by analogy with the cases of sentences and mono logic discourse just described,what is required is declarative model of well formed dialogue?, together with procedural mechanisms that consult this model in systematic way to produce and interpret contributions to dialogue.to begin with, what is declarative theory of dialogue structure?
</nextsent>
<nextsent>in this paper, we will assume theoretical perspective in which dialogue moves are represented as context-update operations (see e.g. traum et al(1999)).
</nextsent>
<nextsent>an utterance in dialogue is understood as function which (when defined) takes the current dialogue context and outputs new dialogue context that constitutes the input to the next utterance in the dialogue.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2043">
<title id=" W04-1302.xml">on statistical parameter setting </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>due to space restrictions, we cannot discuss all these approaches in detail.
</prevsent>
<prevsent>we will focus on the close domain of morphology.
</prevsent>
</prevsection>
<citsent citstr=" N01-1024 ">
approaches to the induction of morphology as presented in e.g. schone and jurafsky (2001) <papid> N01-1024 </papid>or goldsmith (2001) <papid> J01-2001 </papid>show that the morphological 1 see batchelder (1998) for discussion of these.</citsent>
<aftsection>
<nextsent>aspects.
</nextsent>
<nextsent>properties of small subset of languages can be induced with high accuracy, most of the existing approaches are motivated by applied or engineering concerns, and thus make assumptions that are less cognitively plausible: a. large corpora are processed all at once, though unsupervised incremental induction of grammars is rather the approach that would be relevant from psycho linguistic perspective; b. arbitrary decisions about selections of sets of elements are made, based on frequency or frequency profile rank,2 though such decisions should rather be derived or avoided in general.
</nextsent>
<nextsent>however, the most important aspects missing in these approaches, however, are the link to different linguistic levels and the support of general learning model that makes predictions about how knowledge is induced on different linguistic levels and what the dependencies between information at these levels are.
</nextsent>
<nextsent>further, there is no study focusing on the type of supervision that might be necessary for the guidance of different algorithm types towards grammars that resemble theoretical and empirical facts about language acquisition, and processing and the final knowledge of language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2045">
<title id=" W04-1302.xml">on statistical parameter setting </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>due to space restrictions, we cannot discuss all these approaches in detail.
</prevsent>
<prevsent>we will focus on the close domain of morphology.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
approaches to the induction of morphology as presented in e.g. schone and jurafsky (2001) <papid> N01-1024 </papid>or goldsmith (2001) <papid> J01-2001 </papid>show that the morphological 1 see batchelder (1998) for discussion of these.</citsent>
<aftsection>
<nextsent>aspects.
</nextsent>
<nextsent>properties of small subset of languages can be induced with high accuracy, most of the existing approaches are motivated by applied or engineering concerns, and thus make assumptions that are less cognitively plausible: a. large corpora are processed all at once, though unsupervised incremental induction of grammars is rather the approach that would be relevant from psycho linguistic perspective; b. arbitrary decisions about selections of sets of elements are made, based on frequency or frequency profile rank,2 though such decisions should rather be derived or avoided in general.
</nextsent>
<nextsent>however, the most important aspects missing in these approaches, however, are the link to different linguistic levels and the support of general learning model that makes predictions about how knowledge is induced on different linguistic levels and what the dependencies between information at these levels are.
</nextsent>
<nextsent>further, there is no study focusing on the type of supervision that might be necessary for the guidance of different algorithm types towards grammars that resemble theoretical and empirical facts about language acquisition, and processing and the final knowledge of language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2062">
<title id=" W04-0824.xml">multicomponent word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this data is not limited to the specific synset that represents one of the senses of the word, but concerns also other synsets that are semantically similar, i.e., close in the hierarchy, to that synset.
</prevsent>
<prevsent>then, we integrate the task-specific and the external training data with multicomponent classifier that simplifies the system for hierarchical word sense disambiguation presented in (ciaramitaet al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
the classifier consists of two components based on the averaged multiclass perceptron (collins, 2002; <papid> W02-1001 </papid>crammer and singer, 2003).</citsent>
<aftsection>
<nextsent>the first component is trained on the task-specific data while the second is trained on the former andon the external training data.
</nextsent>
<nextsent>when predicting label for an instance the classifier combines the predictions of the two components.
</nextsent>
<nextsent>cross-validationexperiments on the training data show the advantages of the multicomponent architecture.
</nextsent>
<nextsent>in the following section we describe the features used by our system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2063">
<title id=" W04-0824.xml">multicomponent word sense disambiguation </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>morphology/spelling: prefixes/suffixes up to 4 characters ) 6 ; e.g.,  $   $   $ffflfi   $ffiffflfi upper case characters fi3 ; e.g., fi3 $  number/type of words components  !#  ; e.g.,  ff$  #  $%$79 a.r the same features were extracted from the given test and training data, and the additional dataset.
</prevsent>
<prevsent>pos and other syntactic features were extracted from parse trees.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
training and test data, and the wordnet glosses, were parsed with charniaks parser (charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>open class words were morphologically simplified with the morph?
</nextsent>
<nextsent>function from the wordnet library wn.h?.
</nextsent>
<nextsent>when it was not possible to identify the noun or verb in the glosses 2 we only extracted limited set of features: ws, wc, and morphological features.
</nextsent>
<nextsent>each gloss provides one training instance per synset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2065">
<title id=" W03-2107.xml">flexible spoken dialogue system based on user models and dynamic generation of voice xml scripts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, it enables various behaviors adaptive to the dialogue situations such as obtained query results.another problem to realize user-friendly interaction is how to generate cooperative responses.
</prevsent>
<prevsent>whenwe consider the responses generated from the system side, the dialogue strategies, which determine when to make guidance and what the system should tell to the user, are the essential factors in spoken dialogue systems.
</prevsent>
</prevsection>
<citsent citstr=" C00-1068 ">
there are many studies in respect ofthe dialogue strategy such as confirmation management using confidence measures of speech recognition results (komatani and kawahara, 2000; <papid> C00-1068 </papid>hazen et al, 2000), dynamic change of dialogue initiative (litman and pan, 2000; chu-carroll, 2000; lamel et al, 1999), and addition of cooperative contents to system responses (sadek, 1999).</citsent>
<aftsection>
<nextsent>nevertheless, whether particular response is cooperative or not depends on individual users characteristic.in order to adapt the systems behavior to individual users, it is necessary to model the users patterns(kass and finin, 1988).<papid> J88-3002 </papid></nextsent>
<nextsent>most of conventional studies on user models have focused on the knowledge of users.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2066">
<title id=" W03-2107.xml">flexible spoken dialogue system based on user models and dynamic generation of voice xml scripts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>whenwe consider the responses generated from the system side, the dialogue strategies, which determine when to make guidance and what the system should tell to the user, are the essential factors in spoken dialogue systems.
</prevsent>
<prevsent>there are many studies in respect ofthe dialogue strategy such as confirmation management using confidence measures of speech recognition results (komatani and kawahara, 2000; <papid> C00-1068 </papid>hazen et al, 2000), dynamic change of dialogue initiative (litman and pan, 2000; chu-carroll, 2000; lamel et al, 1999), and addition of cooperative contents to system responses (sadek, 1999).</prevsent>
</prevsection>
<citsent citstr=" J88-3002 ">
nevertheless, whether particular response is cooperative or not depends on individual users characteristic.in order to adapt the systems behavior to individual users, it is necessary to model the users patterns(kass and finin, 1988).<papid> J88-3002 </papid></citsent>
<aftsection>
<nextsent>most of conventional studies on user models have focused on the knowledge of users.
</nextsent>
<nextsent>others tried to infer and utilize users goals to generate responses adapted to the user (van beek, 1987; paris, 1988).
</nextsent>
<nextsent>elzer et al (2000) proposeda method to generate adaptive suggestions according to users?
</nextsent>
<nextsent>preferences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2067">
<title id=" W04-1006.xml">legal text summarization by exploration of the thematic structure and argumentative roles </title>
<section> context of the work.  </section>
<citcontext>
<prevsection>
<prevsent>context table 1: alignment of the units of the original judgment with the professional abstract the legal domain and legal interpretations of expressions produce many ambiguities.
</prevsent>
<prevsent>for example, theword sentence can have two very different mean ings: one is sequence of words and the other is more particular meaning in law, the decision as towhat punishment is to be imposed.
</prevsent>
</prevsection>
<citsent citstr=" J02-4005 ">
similarly disposition which means nature, effort, mental attitude or property but in legal terms it means the final part of judgement indicating the nature of decision: accept ation of inquiry or dismission.most previous systems of automatic summarization are limited to newspaper articles and scientific articles (saggion and lapalme, 2002).<papid> J02-4005 </papid></citsent>
<aftsection>
<nextsent>there are important differences between news style and the legal language: statistics of words, probability of selection of textual units, position of paragraph sand sentences, words of title and lexical chains relations between words of the title and the key ideas of the text, relations between sentences and paragraphs and structures of the text.for judgments, we show that we can identify discursive structures for the different parts of the decision and assign some argumentative roles to them.
</nextsent>
<nextsent>newspapers articles often repeat the most important message but, in law, important information may appear only once.
</nextsent>
<nextsent>the processing of legal document requires detailed attention and it is not straight forward to adapt the techniques developed for other types of document to the legal domain.
</nextsent>
<nextsent>3.1 composition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2068">
<title id=" W04-1006.xml">legal text summarization by exploration of the thematic structure and argumentative roles </title>
<section> method for producing table style.  </section>
<citcontext>
<prevsection>
<prevsent>the implementation of our approach is system called letsum (legal text summarizer), which hasbeen developed in java and perl.
</prevsent>
<prevsent>input to the system is legal judgment in english.
</prevsent>
</prevsection>
<citsent citstr=" P00-1036 ">
to determine thepart-of-speech tags, the tagger described by (hep ple, 2000) <papid> P00-1036 </papid>is used.</citsent>
<aftsection>
<nextsent>the semantic grammars and rules are developed in jape language (java annotations pattern engine) and executed by gate transducer (cunningham et al, 2002).<papid> P02-1022 </papid></nextsent>
<nextsent>4.1 components of letsum.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2069">
<title id=" W04-1006.xml">legal text summarization by exploration of the thematic structure and argumentative roles </title>
<section> method for producing table style.  </section>
<citcontext>
<prevsection>
<prevsent>input to the system is legal judgment in english.
</prevsent>
<prevsent>to determine thepart-of-speech tags, the tagger described by (hep ple, 2000) <papid> P00-1036 </papid>is used.</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
the semantic grammars and rules are developed in jape language (java annotations pattern engine) and executed by gate transducer (cunningham et al, 2002).<papid> P02-1022 </papid></citsent>
<aftsection>
<nextsent>4.1 components of letsum.
</nextsent>
<nextsent>thematic segmentation for which we performed some experiments with two statistic segmenters: one described by hearst for the tex tiling system (hearst, 1994) <papid> P94-1002 </papid>and the c99 segmenter described by choi (choi, 2000), <papid> A00-2004 </papid>both of which apply clustering function on document to find classes divided by theme.</nextsent>
<nextsent>but because the results of these numerical segment ers were not satisfactory enough to find the thematic structures of the legal judgments, we decided to develop segmentation process based on the specific knowledge of the legal field.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2070">
<title id=" W04-1006.xml">legal text summarization by exploration of the thematic structure and argumentative roles </title>
<section> method for producing table style.  </section>
<citcontext>
<prevsection>
<prevsent>the semantic grammars and rules are developed in jape language (java annotations pattern engine) and executed by gate transducer (cunningham et al, 2002).<papid> P02-1022 </papid></prevsent>
<prevsent>4.1 components of letsum.</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
thematic segmentation for which we performed some experiments with two statistic segmenters: one described by hearst for the tex tiling system (hearst, 1994) <papid> P94-1002 </papid>and the c99 segmenter described by choi (choi, 2000), <papid> A00-2004 </papid>both of which apply clustering function on document to find classes divided by theme.</citsent>
<aftsection>
<nextsent>but because the results of these numerical segment ers were not satisfactory enough to find the thematic structures of the legal judgments, we decided to develop segmentation process based on the specific knowledge of the legal field.
</nextsent>
<nextsent>category of section title linguistic markers examples of section title begin of the judgment decision, judgment, reason, order reasons for order, reasons for judgment and order introduction introduction, summary introduction, summary context facts, background the factual background, agreed statement of facts juridical analysis analysis, decision, discussion analysis and decision of the court conclusion conclusion, disposiotion, cost conclusion and costs table 3: the linguistic markers in section titles each thematical segment can be associated with an argumentative role in the judgment based on the following information: the presence of significant section titles (table 3 shows categories and features of the section titles), the absolute and relative positions of segment, the identification of direct or narrative style (as the border of context and juridical analysis segments), certain linguistic markers.
</nextsent>
<nextsent>the linguistic markers used for each thematic segment are organized as follows: context introduces the parties with the verb to be (eg.
</nextsent>
<nextsent>the application is company x), describes the application request like: advise, indicate, request and explains the situation in the past tense and narration form.in juridical analysis, the judge gives his explanation on the subject thus the style of expression is direct such as: i, we, this court, the cue phrases (paice, 1981) like: in reviewing the sections no.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2071">
<title id=" W04-1006.xml">legal text summarization by exploration of the thematic structure and argumentative roles </title>
<section> method for producing table style.  </section>
<citcontext>
<prevsection>
<prevsent>the semantic grammars and rules are developed in jape language (java annotations pattern engine) and executed by gate transducer (cunningham et al, 2002).<papid> P02-1022 </papid></prevsent>
<prevsent>4.1 components of letsum.</prevsent>
</prevsection>
<citsent citstr=" A00-2004 ">
thematic segmentation for which we performed some experiments with two statistic segmenters: one described by hearst for the tex tiling system (hearst, 1994) <papid> P94-1002 </papid>and the c99 segmenter described by choi (choi, 2000), <papid> A00-2004 </papid>both of which apply clustering function on document to find classes divided by theme.</citsent>
<aftsection>
<nextsent>but because the results of these numerical segment ers were not satisfactory enough to find the thematic structures of the legal judgments, we decided to develop segmentation process based on the specific knowledge of the legal field.
</nextsent>
<nextsent>category of section title linguistic markers examples of section title begin of the judgment decision, judgment, reason, order reasons for order, reasons for judgment and order introduction introduction, summary introduction, summary context facts, background the factual background, agreed statement of facts juridical analysis analysis, decision, discussion analysis and decision of the court conclusion conclusion, disposiotion, cost conclusion and costs table 3: the linguistic markers in section titles each thematical segment can be associated with an argumentative role in the judgment based on the following information: the presence of significant section titles (table 3 shows categories and features of the section titles), the absolute and relative positions of segment, the identification of direct or narrative style (as the border of context and juridical analysis segments), certain linguistic markers.
</nextsent>
<nextsent>the linguistic markers used for each thematic segment are organized as follows: context introduces the parties with the verb to be (eg.
</nextsent>
<nextsent>the application is company x), describes the application request like: advise, indicate, request and explains the situation in the past tense and narration form.in juridical analysis, the judge gives his explanation on the subject thus the style of expression is direct such as: i, we, this court, the cue phrases (paice, 1981) like: in reviewing the sections no.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2072">
<title id=" W04-1006.xml">legal text summarization by exploration of the thematic structure and argumentative roles </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>first, the case category, the case structure and irrelevant text units are identified basedon knowledge base represented as text grammar.
</prevsent>
<prevsent>consequently, general data and legal foundations concerning the essence of the case are extracted.
</prevsent>
</prevsection>
<citsent citstr=" W03-0505 ">
secondly, the system extracts informative text units of the alleged offences and of the opinion of the court based on the selection of representative objects.more recently, sum (grover et al, 2003) <papid> W03-0505 </papid>examined the use of rhetorical and discourse structure in level of the sentence of legal cases for finding the main verbes.</citsent>
<aftsection>
<nextsent>the methodology is based on (teufeland moens, 2002) <papid> J02-4002 </papid>where sentences are classified according to their argumentative role.these studies have shown the interest of summarization in specialized domain such as legal texts but none of these systems was implemented in an environment such as canlii which has to deal with thousands of texts and produce summaries for each.</nextsent>
<nextsent>in this paper, we have presented our approach for dealing with automatic summarization techniques.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2073">
<title id=" W04-1006.xml">legal text summarization by exploration of the thematic structure and argumentative roles </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, general data and legal foundations concerning the essence of the case are extracted.
</prevsent>
<prevsent>secondly, the system extracts informative text units of the alleged offences and of the opinion of the court based on the selection of representative objects.more recently, sum (grover et al, 2003) <papid> W03-0505 </papid>examined the use of rhetorical and discourse structure in level of the sentence of legal cases for finding the main verbes.</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
the methodology is based on (teufeland moens, 2002) <papid> J02-4002 </papid>where sentences are classified according to their argumentative role.these studies have shown the interest of summarization in specialized domain such as legal texts but none of these systems was implemented in an environment such as canlii which has to deal with thousands of texts and produce summaries for each.</citsent>
<aftsection>
<nextsent>in this paper, we have presented our approach for dealing with automatic summarization techniques.
</nextsent>
<nextsent>this work refers to the problem of processing of huge volume of electronic documents in the legal field which becomes more and more difficult to access.
</nextsent>
<nextsent>our method is based on the extraction of relevant units in the source judgment by identifying the discourse structures and determining the semantic roles of thematic segments in the document.
</nextsent>
<nextsent>the presentation of the summary is in tabular form divided by the following thematic structures: decision data, introduction, context, juridical analysis and conclusion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2074">
<title id=" W04-0405.xml">mwes as non propositional content indicators </title>
<section> background mwe resources.  </section>
<citcontext>
<prevsection>
<prevsent>, m9) m1: possibility to be modified by adnominal m2: possibility to be modified by appredicative m3: auxiliaries insert able in between its words m4: particles insert able in between its words m5: dele table particles m6: particles by which those in it are replaced m7: constituents which can be reordered m8: possibility to be nominal ized by inversion m9: possibility to be passivized f7: estimated relative frequency f6 was adopted to ensure the flexibility of mwes, while controlling the number of headings.
</prevsent>
<prevsent>thus, our lexicon is not simply list of mwes but designed as resource proliferous to total variety of idiosyncratic expressions.
</prevsent>
</prevsection>
<citsent citstr=" C80-1001 ">
(shudo et al, 1980, <papid> C80-1001 </papid>1988; shudo, 1989; yasutake et al, 1997).</citsent>
<aftsection>
<nextsent>the present study focuses on set of npcis and its relationship to the non-propositional structure of natural sentences.
</nextsent>
<nextsent>some of our multiword npcis are treated in the general, rewriting framework for mt in (shirai et al, 1993).
</nextsent>
<nextsent>let us consider the meaning of sentence; (1) ??
</nextsent>
<nextsent>karehasokoniirubekidenakatta?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2076">
<title id=" W04-0701.xml">multi document person name resolution </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>then, modified agglom erative clustering technique is used to merge the most likely instances together, forming clusters that correspond to individual referents.
</prevsent>
<prevsent>while there has been great deal of work on coreference resolution within single document, little work has focused on the challenges associated with resolving the reference of identical person names across multiple documents.
</prevsent>
</prevsection>
<citsent citstr=" W03-0405 ">
mann and yarowsky (2003) <papid> W03-0405 </papid>are amongst the few who have examined this problem.</citsent>
<aftsection>
<nextsent>they treat it as clustering task, in which, combination of features (such as, weighted bag of words and biographic information extracted from the text) are given to an agglomerative clustering algorithm, which outputs two clusters representing the two referents of the query name.
</nextsent>
<nextsent>mann and yarowsky (2003) <papid> W03-0405 </papid>report results on two types of evaluations: using hand-annotated web pages returned from truly ambiguous searches, they report precision/recall scores of 0.88/0.73; using psuedonames1 they report an accuracy of 86.4%.</nextsent>
<nextsent>1 borrowing from techniques in word sense disambigua-.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2085">
<title id=" W04-0701.xml">multi document person name resolution </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>by ran while mann and yarowsky (2003) <papid> W03-0405 </papid>describe number of useful features for multi-document person name resolution, their technique is limited by only allowing set number of referent clusters.</prevsent>
<prevsent>further, as discussed below, their use of artificial test data makes it difficult to determine how well it generalize to real world problems.</prevsent>
</prevsection>
<citsent citstr=" P98-1012 ">
bagga and baldwin (1998) <papid> P98-1012 </papid>also present an examination of multi-document person name resolu tion.</citsent>
<aftsection>
<nextsent>they first perform within-document coreference resolution to form coreference chains for each entity in each document.
</nextsent>
<nextsent>they then use the text surrounding each reference chain to create summaries about each entity in each document.
</nextsent>
<nextsent>these summaries are then converted to bag of words feature vector and are clustered using the standard vector space model often employed in ir.
</nextsent>
<nextsent>they evaluated their system on 11 entities named john smith taken from set of 173 new york times articles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2086">
<title id=" W04-0701.xml">multi document person name resolution </title>
<section> maximum entropy model.  </section>
<citcontext>
<prevsection>
<prevsent>further, that they only evaluate their system, on single person name begs the question of how well such technique would fair on more real-world challenge.
</prevsent>
<prevsent>3.1 data.
</prevsent>
</prevsection>
<citsent citstr=" P03-1001 ">
fleischman et al (2003) <papid> P03-1001 </papid>describe dataset of con cept-instance pairs extracted automatically from very large corpus of newspaper articles.</citsent>
<aftsection>
<nextsent>the dataset (referred to here as the acl dataset) contains approximately 2 million pairs (of which 93% are legitimate) in which the concept is represented by complex noun phrase (e.g. president of the united domly selecting two names from handcrafted list of 8 individuals (e.g., haifa al-faisal and tom cruise) and treat the pair as one name with two referents.
</nextsent>
<nextsent>states) and the instance by name (e.g. william jefferson clinton).2 set of 2675 legitimate concept-instance pairs was randomly selected from the acl dataset described above; each of these was then matched with another concept-instance pair that had an identical instance name, but different concept name.
</nextsent>
<nextsent>this set of matched pairs was hand tagged by human annotator to reflect whether or not the identical instance names actually referred to the same individual.
</nextsent>
<nextsent>the set was then randomly split into training set of 1875 matched pairs (84% referring to the same individual), development set of 400 matched pairs (85.5% referring to the same individual), and test set of 400 matched pairs (83.5% referring to the same individual).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2087">
<title id=" W04-0701.xml">multi document person name resolution </title>
<section> maximum entropy model.  </section>
<citcontext>
<prevsection>
<prevsent>maximum entropy (max.
</prevsent>
<prevsent>ent.)
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence, but otherwise is as uniform as possible (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>we model the probability of two instances having the same referent (r=[1,0]) given vector of features according to the max.
</nextsent>
<nextsent>ent.
</nextsent>
<nextsent>formulation below: ? = = i x xrfzxrp 0 )],(exp[1)|( ? here zx is normalization constant, fi(r,x) is feature function over values of and vector elements, is the total number of feature functions, and is the weight forgiven feature function.
</nextsent>
<nextsent>the final output of the model is the probability 83.50% 90.75% 78% 80% 82% 84% 86% 88% 90% 92% baseline max ent % or re ct given feature vector that r=1; i.e., the probability that the referents are the same.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2088">
<title id=" W04-0701.xml">multi document person name resolution </title>
<section> clustering.  </section>
<citcontext>
<prevsection>
<prevsent>optimized for each run; thus, the performance is slightly worse than in figure 1.
</prevsent>
<prevsent>11% 12% 13% 14% 15% 16% 10 100 1000 # of training examples % rr r 88.3% 83.5% 87.0% 89.3% 89.0% 88.3% 80% 81% 82% 83% 84% 85% 86% 87% 88% 89% 90% baseline stat.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
+web +name +over +sem (all feats) % or re ct following ng and cardie (2002), <papid> P02-1014 </papid>we address this problem by clustering each set of concept-instance pairs with identical names, using form of group average agglomerative clustering, in which the similarity score between instances is just the probability output by the model.</citsent>
<aftsection>
<nextsent>because standard ag glomerative clustering algorithms are o(n3) if cosign similarity metrics are not used (manning and schutze, 2001), we adapt the method to our framework.
</nextsent>
<nextsent>our algorithm operates as follows5: on input d={concept-instance pairs of same name}, build fully connected graph with vertex set d: 1) label each edge (d,d?) in with score corresponding to the probability of identity predicted by the max.
</nextsent>
<nextsent>ent.
</nextsent>
<nextsent>model 2) while the edge with max score in   threshold: a. merge the two nodes connected by the edge with the max score.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2090">
<title id=" W04-2504.xml">discourse structure for context question answering </title>
<section> semantic-rich discourse modeling.  </section>
<citcontext>
<prevsection>
<prevsent>q6: where is mount rainier?
</prevsent>
<prevsent>figure 1: an example of context questions
</prevsent>
</prevsection>
<citsent citstr=" C00-1043 ">
for processing single questions, an earlier study shows that an impressive improvement can be achieved when more knowledge-intensive nlp techniques are applied at both question and answer processing level (harabagiu et al , 2000).<papid> C00-1043 </papid></citsent>
<aftsection>
<nextsent>for context questions, parallel question would be whether rich contextual knowledge will help interpret subsequent questions and extracting answers.
</nextsent>
<nextsent>to address this question, we propose semantic-rich discourse modeling that captures both discourse roles of questions and discourse transitions between questions, and investigate its usefulness in context question answering.
</nextsent>
<nextsent>2.1 discourse roles.
</nextsent>
<nextsent>in context question answering, each question is situated in context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2091">
<title id=" W04-2504.xml">discourse structure for context question answering </title>
<section> semantic-rich discourse modeling.  </section>
<citcontext>
<prevsection>
<prevsent>the intentional perspective relates to the purpose of question.
</prevsent>
<prevsent>in fully interactive question answering environment, instead of asking questions, user may need to reply to clarification question prompted by the system or may need to simply ask for confirmation.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
therefore, it is important to capture the intention from the user (grosz and sidner 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>the informational perspective relates to the information content of question, in particular, the topic and the focus based on the semantics of the content.
</nextsent>
<nextsent>in addition to the intentional and informational aspects, there is also presentational aspect of discourse that relates to both the input modality (i.e., questions) and the output modality (i.e., answers).
</nextsent>
<nextsent>for example, user may explicitly ask for images or pictures of person or event.
</nextsent>
<nextsent>the presentation aspect is particularly important to facilitate multimodal multimedia question answering.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2092">
<title id=" W04-2504.xml">discourse structure for context question answering </title>
<section> semantic-rich discourse modeling.  </section>
<citcontext>
<prevsection>
<prevsent>focus indicates the current focus of attention given particular topic.
</prevsent>
<prevsent>focus always refers to particular aspect of topic.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
since the informational perspective of discourse should capture the semantics of what has been conveyed, topic and focus are linked with the semantic information of question, for example, semantic roles as described in (gildea and jurafsky 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>semantic roles concern with the roles of constitutes in question in terms of its predicate-argument structure.
</nextsent>
<nextsent>the discourse roles link the semantic roles of individual questions together with respect to the discourse progress through topic and focus.
</nextsent>
<nextsent>for example, topic can be of type activity or entity.
</nextsent>
<nextsent>activity can be further categorized by acttype, participant, and peripheral.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2094">
<title id=" W04-2504.xml">discourse structure for context question answering </title>
<section> semantic-rich discourse modeling.  </section>
<citcontext>
<prevsection>
<prevsent>this may be difficult task that requires rich knowledge and deep semantic processing.
</prevsent>
<prevsent>however, the recent advancement in semantic processing and discourse parsing has provided an excellent foundation for this task.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the discourse roles are higher-level abstracts of the semantic roles as those provided in framenet (baker et al , 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer 2002).</citsent>
<aftsection>
<nextsent>recent corpus-based approaches to identify semantic roles (roth et al 2002, gildea and jurafsky 2002; <papid> J02-3001 </papid>gildea and palmer 2002; <papid> P02-1031 </papid>surdeanu et al ., 2003) <papid> P03-1002 </papid>have been successful in identifying domain independent semantic relations with respect to the predicate-argument structure.</nextsent>
<nextsent>furthermore, recent work also provides discourse annotated corpora with rhetorical relations (carlson, et al , 2003) and techniques for discourse paring for texts (soricut and marcu, 2003).<papid> N03-1030 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2097">
<title id=" W04-2504.xml">discourse structure for context question answering </title>
<section> semantic-rich discourse modeling.  </section>
<citcontext>
<prevsection>
<prevsent>however, the recent advancement in semantic processing and discourse parsing has provided an excellent foundation for this task.
</prevsent>
<prevsent>the discourse roles are higher-level abstracts of the semantic roles as those provided in framenet (baker et al , 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer 2002).</prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
recent corpus-based approaches to identify semantic roles (roth et al 2002, gildea and jurafsky 2002; <papid> J02-3001 </papid>gildea and palmer 2002; <papid> P02-1031 </papid>surdeanu et al ., 2003) <papid> P03-1002 </papid>have been successful in identifying domain independent semantic relations with respect to the predicate-argument structure.</citsent>
<aftsection>
<nextsent>furthermore, recent work also provides discourse annotated corpora with rhetorical relations (carlson, et al , 2003) and techniques for discourse paring for texts (soricut and marcu, 2003).<papid> N03-1030 </papid></nextsent>
<nextsent>all these recent advances make the semantic-rich discourse modeling possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2099">
<title id=" W04-2504.xml">discourse structure for context question answering </title>
<section> semantic-rich discourse modeling.  </section>
<citcontext>
<prevsection>
<prevsent>however, the recent advancement in semantic processing and discourse parsing has provided an excellent foundation for this task.
</prevsent>
<prevsent>the discourse roles are higher-level abstracts of the semantic roles as those provided in framenet (baker et al , 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer 2002).</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
recent corpus-based approaches to identify semantic roles (roth et al 2002, gildea and jurafsky 2002; <papid> J02-3001 </papid>gildea and palmer 2002; <papid> P02-1031 </papid>surdeanu et al ., 2003) <papid> P03-1002 </papid>have been successful in identifying domain independent semantic relations with respect to the predicate-argument structure.</citsent>
<aftsection>
<nextsent>furthermore, recent work also provides discourse annotated corpora with rhetorical relations (carlson, et al , 2003) and techniques for discourse paring for texts (soricut and marcu, 2003).<papid> N03-1030 </papid></nextsent>
<nextsent>all these recent advances make the semantic-rich discourse modeling possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2100">
<title id=" W04-2504.xml">discourse structure for context question answering </title>
<section> semantic-rich discourse modeling.  </section>
<citcontext>
<prevsection>
<prevsent>the discourse roles are higher-level abstracts of the semantic roles as those provided in framenet (baker et al , 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer 2002).</prevsent>
<prevsent>recent corpus-based approaches to identify semantic roles (roth et al 2002, gildea and jurafsky 2002; <papid> J02-3001 </papid>gildea and palmer 2002; <papid> P02-1031 </papid>surdeanu et al ., 2003) <papid> P03-1002 </papid>have been successful in identifying domain independent semantic relations with respect to the predicate-argument structure.</prevsent>
</prevsection>
<citsent citstr=" N03-1030 ">
furthermore, recent work also provides discourse annotated corpora with rhetorical relations (carlson, et al , 2003) and techniques for discourse paring for texts (soricut and marcu, 2003).<papid> N03-1030 </papid></citsent>
<aftsection>
<nextsent>all these recent advances make the semantic-rich discourse modeling possible.
</nextsent>
<nextsent>for example, collection of context questions (and answers) can be annotated in terms of their discourse roles and relations.
</nextsent>
<nextsent>specifically, the following information can be either automatically identified or manually annotated: ? syntactic structures automatically identified from parser (collins, 1997); ? <papid> P97-1003 </papid>semantic roles of entities in the question (gildea and jurafsky 2002; <papid> J02-3001 </papid>gildea and palmer 2002; <papid> P02-1031 </papid>surdeanu et al , 2003); ? <papid> P03-1002 </papid>discourse roles either manually annotated or identified by rules that map directly from semantic roles to discourse roles.</nextsent>
<nextsent>discourse transitions automatically determined once discourse roles are identified for each question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2101">
<title id=" W04-2504.xml">discourse structure for context question answering </title>
<section> semantic-rich discourse modeling.  </section>
<citcontext>
<prevsection>
<prevsent>all these recent advances make the semantic-rich discourse modeling possible.
</prevsent>
<prevsent>for example, collection of context questions (and answers) can be annotated in terms of their discourse roles and relations.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
specifically, the following information can be either automatically identified or manually annotated: ? syntactic structures automatically identified from parser (collins, 1997); ? <papid> P97-1003 </papid>semantic roles of entities in the question (gildea and jurafsky 2002; <papid> J02-3001 </papid>gildea and palmer 2002; <papid> P02-1031 </papid>surdeanu et al , 2003); ? <papid> P03-1002 </papid>discourse roles either manually annotated or identified by rules that map directly from semantic roles to discourse roles.</citsent>
<aftsection>
<nextsent>discourse transitions automatically determined once discourse roles are identified for each question.
</nextsent>
<nextsent>semantic relations between questions manually annotated.
</nextsent>
<nextsent>answers provided by the system.
</nextsent>
<nextsent>based on this information, important features can be identified.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2107">
<title id=" W04-0505.xml">biographer biography questions as a restricted domain question answering task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>many qa research groups employed external knowledge sources in order to improve performance.
</prevsent>
<prevsent>for instance, (chu-carroll and prager, 2002) used wordnet to answer what is questions, using the isa hierarchy supported by wordnet.
</prevsent>
</prevsection>
<citsent citstr=" C02-1026 ">
(hovy et al, 2002; lin, 2002) <papid> C02-1026 </papid>used dictionaries such as wordnet and web search results to re-rank answers.</citsent>
<aftsection>
<nextsent>(yang et al, 2003) preformed structure analysis of the knowledge obtained from wordnet and the web in order to further improve perfor mance.we refer to (sebastiani, 2002) for extensive review about machine learning in automated text classification.
</nextsent>
<nextsent>(lewis, 1992) were among the first to use machine learning for genre detection trying to categorize reuters articles to predefined categories.
</nextsent>
<nextsent>probabilistic classifiers were used by many groups(lewis, 1998).
</nextsent>
<nextsent>much current text classification research is focused on support vector machines, first used for genre detection by (joachims, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2108">
<title id=" W03-2106.xml">the interpretation of non sentential utterances in dialogue </title>
<section> a taxonomy of fragments </section>
<citcontext>
<prevsection>
<prevsent>we will not go into more details of the types here; in section 5 we will return to select few and give formal definition of their semantics.
</prevsent>
<prevsent>note that we do not claim that this set of fragment-types is exhaustive; we discuss in the next section the coverage that can be achieve withit on test data.5 as final point, note that we subsume what is often called clarification question?(eg.
</prevsent>
</prevsection>
<citsent citstr=" P01-1031 ">
(ginzburg and cooper, 2001)) <papid> P01-1031 </papid>under elaboration to stress the similarity with normal?</citsent>
<aftsection>
<nextsent>elaborations.
</nextsent>
<nextsent>the sub scripts and indicate the message type of ? and ?; e.g. elab pq is an elaboration of proposition, performed with question.
</nextsent>
<nextsent>2.2 corpus study.
</nextsent>
<nextsent>to test the coverage that can be achieved with our taxonomy, we analysed 5087 items of general free conversation from the bnc (dialogues ksuand ksv), and 4037 items of task-oriented dialogue from the vm/redwoods corpus (the 125 dialogues on the vm-cd-rom 6).6 we proceeded 3all taxonomies of fragment-types that are known to us use classes that are at least partially determined by the rhetorical function of the fragment (eg., that of(carberry, 1990) as well as that of (fernandez and ginzburg, 2002)); nevertheless, to our knowledge ours is the only one to make this inherent relationality explicit in the formal definition of the classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2109">
<title id=" W03-2106.xml">the interpretation of non sentential utterances in dialogue </title>
<section> a taxonomy of fragments </section>
<citcontext>
<prevsection>
<prevsent>q , q-alt and contr were found only few times.
</prevsent>
<prevsent>summary items analysed 9142 fragments 931 (= 10.2%) classfd.
</prevsent>
</prevsection>
<citsent citstr=" C80-1027 ">
865 (= 93% of fragments) other 66 (= 7% of fragments) table 3: results of annotation discussion the overall percentage of fragments we have found seems to confirm the results of earlier studies (thompson, 1980; <papid> C80-1027 </papid>fernandez and ginzburg, 2002), which also classified as fragments around 10% of the utterances in the dialogues they looked at.</citsent>
<aftsection>
<nextsent>(fernandez and ginzburg, 2002)also offers taxonomy of fragment types; the authors claim to have reached coverage which ismuch higher than what we achieved (99% compared to our 93%).
</nextsent>
<nextsent>we think this can partially be explained by the fact that the classes they use are more surface-oriented.
</nextsent>
<nextsent>for example, they have aclass sluice?, which is defined as bare question denoting wh-phrases?
</nextsent>
<nextsent>(fernandez and ginzburg,2002, p.16).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2110">
<title id=" W04-1809.xml">term extraction from korean corpora via japanese </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the accuracy for all words was changed from 94.6% to 94.8% by the additional dictionary.in summary, the additional dictionary was effective for analyzing foreign words and was not associated with side effect for the overall accuracy.
</prevsent>
<prevsent>at the same time, we concede that we need larger-scale experiments to draw firmer conclusions.
</prevsent>
</prevsection>
<citsent citstr=" J96-1001 ">
a number of corpus-based methods to extract bilingual lexicons have been proposed (smadja et al,1996).<papid> J96-1001 </papid></citsent>
<aftsection>
<nextsent>in general, these methods use statistics obtained from parallel or comparable bilingual corpus and extract word or phrase pairs that are strongly associated with each other.
</nextsent>
<nextsent>however, our method uses monolingual korean corpus and japanese lexicon independent of the corpus, which can easily be obtained, compared with parallel or comparable bilingual corpora.jeong et al (1999) and oh and choi (2001) independently explored statistical approach to detect foreign words in korean text.
</nextsent>
<nextsent>although the detection accuracy is reasonably high, these methods require training corpus in which conventional and foreign words are annotated.
</nextsent>
<nextsent>our approach does not require annotated corpora, but the detection accuracy is not high enough as shown in section 3.1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2111">
<title id=" W04-2315.xml">speech graffiti habitability what do users really say </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>an overall user satisfaction rating was calculated for each user by averaging that users scores for each of the 34 response items.
</prevsent>
<prevsent>users were also asked few comparison questions, including system preference.
</prevsent>
</prevsection>
<citsent citstr=" N04-4019 ">
in this analysis we were only concerned with results from the speech graffiti movie line interactions and not the natural language movie line interactions (see tomko &amp; rosenfeld, 2004).<papid> N04-4019 </papid></citsent>
<aftsection>
<nextsent>system presentation order was balanced and had no significant effect on grammaticality measures.
</nextsent>
<nextsent>2.1 participants.
</nextsent>
<nextsent>twenty-three participants (12 female, 11 male) accessed the systems via telephone in our lab.
</nextsent>
<nextsent>most were undergraduate students from carnegie mellon university and all were native speakers of american english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2112">
<title id=" W04-2315.xml">speech graffiti habitability what do users really say </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>the user study generated 4062 speech graffiti movie line utterances, where an utterance is defined as one chunk of speech input sent to our sphinx ii speech recognizer (huang et al, 1993).
</prevsent>
<prevsent>we removed all utterances containing non-task-related or unintelligible speech, or excessive noise or feed, resulting in cleaned set of 3626 utterances (89% of the total).
</prevsent>
</prevsection>
<citsent citstr=" H90-1027 ">
we defined an utterance to be grammatical if the phoenix parser (ward, 1990) <papid> H90-1027 </papid>used by the system returns complete parse with no extraneous words.</citsent>
<aftsection>
<nextsent>82% (2987) of the utterances from the cleaned set were fully speech graffiti-grammatical.
</nextsent>
<nextsent>for individual users, grammaticality ranged from 41.1% to 98.6%, with mean of 80.5% and median of 87.4%.
</nextsent>
<nextsent>these averages are quite high, indicating that most users were able to learn and use speech graffiti reasonably well.
</nextsent>
<nextsent>the lowest individual grammaticality scores belonged to four of the six participants who preferred the natural language movie line interface to the speech graffiti one, which suggests that proficiency with the language is very important for its acceptance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2113">
<title id=" W03-2118.xml">domain specific speech acts for spoken language translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the specification document is supported by database of over 14,000 tagged sentences in english, german, and italian.
</prevsent>
<prevsent>the discourse community has long recognized the potential for improving nlp systems by identifying speaker intention.
</prevsent>
</prevsection>
<citsent citstr=" C96-1061 ">
it has been hypothesized that predicting speaker intention of the next utterance would improve speech recognition (reithinger et al, stolcke et al), or reduce ambiguity for machine translation (qu et al, 1996, <papid> C96-1061 </papid>qu et al, 1997).</citsent>
<aftsection>
<nextsent>identifying speaker intention is also critical for sentence generation.
</nextsent>
<nextsent>we argue in this paper that the explicit representation of speaker intention using domain actions can serve as the basis for an effective language-independent representation of meaning for speech-to-speech translation and that the relevant units of speaker intention are the domain specific domain action as well as the domain independent speech act.
</nextsent>
<nextsent>after brief description of our database, we present linguistic motivation for domain actions.
</nextsent>
<nextsent>we go on to show that although domain actions are domain specific, there is not an explosion or exponential growth of domain actions when we scale up to larger domain or port to new domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2114">
<title id=" W03-2118.xml">domain specific speech acts for spoken language translation </title>
<section> linguistic argument for domain ac-.  </section>
<citcontext>
<prevsection>
<prevsent>or there are?
</prevsent>
<prevsent>in the hotel reservation domain express availability of rooms in addition to their more literal meanings of possession and existence.
</prevsent>
</prevsection>
<citsent citstr=" W02-0708 ">
in the past six years, we have been successful in using domain specific domain actions as the basis for translation of limited-domain task oriented spoken language (levin et al, 1998, levin et al 2002; <papid> W02-0708 </papid>langley and lavie, 2003)</citsent>
<aftsection>
<nextsent>actions domain actions, like speech acts, convey speaker intention.
</nextsent>
<nextsent>however, domain actions also represent components of meaning and are therefore more numerous than domain independent speech acts.
</nextsent>
<nextsent>1168 unique domain actions are used in our nespole database, in contrast to only 72 speech acts.
</nextsent>
<nextsent>we show in this section that domain actions yield good coverage of task-oriented domains, that domain actions can be coded effectively by humans, and that scaling up to larger domains or porting to new domains is feasible without an explosion of domain actions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2115">
<title id=" W03-2118.xml">domain specific speech acts for spoken language translation </title>
<section> a hybrid analysis approach for pars- </section>
<citcontext>
<prevsection>
<prevsent>we can see around 70% of da tokens are covered by da types that occur in both domains.
</prevsent>
<prevsent>da types type overlap da tokens token overlap nespole travel 880 171 8477 6004 (70.8%) nespole medical 459 171 4088 2743 (67.1%) table 3: da overlap (all languages).
</prevsent>
</prevsection>
<citsent citstr=" W02-0703 ">
ing domain actions langley et al (2002; <papid> W02-0703 </papid>langley and lavie, 2003) describe the hybrid analysis approach that is used in the nespole!</citsent>
<aftsection>
<nextsent>system (lavie et al, 2002).<papid> W02-0717 </papid></nextsent>
<nextsent>the hybrid analysis approach combines gram mar-based phrasal parsing and machine learning techniques to transform utterances into our interlingua representation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2116">
<title id=" W03-2118.xml">domain specific speech acts for spoken language translation </title>
<section> a hybrid analysis approach for pars- </section>
<citcontext>
<prevsection>
<prevsent>da types type overlap da tokens token overlap nespole travel 880 171 8477 6004 (70.8%) nespole medical 459 171 4088 2743 (67.1%) table 3: da overlap (all languages).
</prevsent>
<prevsent>ing domain actions langley et al (2002; <papid> W02-0703 </papid>langley and lavie, 2003) describe the hybrid analysis approach that is used in the nespole!</prevsent>
</prevsection>
<citsent citstr=" W02-0717 ">
system (lavie et al, 2002).<papid> W02-0717 </papid></citsent>
<aftsection>
<nextsent>the hybrid analysis approach combines gram mar-based phrasal parsing and machine learning techniques to transform utterances into our interlingua representation.
</nextsent>
<nextsent>our analyzer operates in three stages to identify the domain action and arguments.
</nextsent>
<nextsent>first, an input utterance is parsed into sequence of arguments using phrase-level semantic grammars and the soup parser (gavald?, 2000).
</nextsent>
<nextsent>four grammars are defined for argument parsing: an argument grammar, pseudo-argument grammar, cross-domain grammar, and shared grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2118">
<title id=" W03-2112.xml">example based spoken dialogue system using woz system log </title>
<section> search results.  </section>
<citcontext>
<prevsection>
<prevsent>speech recognition: uses japanese dictationtoolkit(kawahara et al, 2000)?.
</prevsent>
<prevsent>the language model was created from the previously collected human-to-human dialogue corpus.
</prevsent>
</prevsection>
<citsent citstr=" C00-1004 ">
3using chasen morphological-analysis software for the japanese language (asahara and matsumoto, 2000).<papid> C00-1004 </papid></citsent>
<aftsection>
<nextsent>speech input speech recognition query generation search speech output dialogue example database (dedb) word class database (wcdb) shop information database (sidb) speech synthesis reply generation figure 7: configuration of example-based dialogue system query generation: extracts from the dedb the example closest to current input speech and conditions, modifies the query in that example according to current conditions, and outputs the result.
</nextsent>
<nextsent>search execution: accesses the sidb using the generated query and obtains search results.
</nextsent>
<nextsent>reply generation: extracts from the dedb the example closest to input speech and search results, modifies the reply in that example according to current conditions, and outputs the result.
</nextsent>
<nextsent>speech synthesis: outputs replies in voice for musing japanese tts (text to speech) software elegantalk ver.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2119">
<title id=" W04-2116.xml">empirical acquisition of differentiating relations from definitions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>large-scale lexicons for computational semantics of ten lack sufficient distinguishing information for the concepts serving to define words.
</prevsent>
<prevsent>for example,wordnet (miller, 1990) recently introduced new relations for domain category and location inversion 2.0, along with 6,000+ instances; however, about38% of the noun synsets are still not explicitly distinguished from sibling synsets.
</prevsent>
</prevsection>
<citsent citstr=" W99-0501 ">
work on the extended wordnet project(harabagiu et al, 1999) <papid> W99-0501 </papid>is achieving substantial progress in making the information in wordnet more explicit.</citsent>
<aftsection>
<nextsent>the main goal is to transform the definitions into logical form representation suit able for drawing inferences; in addition, the content words in the definitions are being disambiguated.
</nextsent>
<nextsent>in the logical form representation, separate predicates are used for each preposition, as well as for some other functional words (e.g., conjunctions); thus, ambiguity in the underlying relations implicit in the definitions is not being resolved.
</nextsent>
<nextsent>the work described here automates the process of relation disambiguation.
</nextsent>
<nextsent>this can be used to further the transformation of wordnet into an explicit lexical knowledge base.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2120">
<title id=" W04-2116.xml">empirical acquisition of differentiating relations from definitions </title>
<section> differentia extraction.  </section>
<citcontext>
<prevsection>
<prevsent>in his approach, estimated frequencies for synsets are percolated up the hierarchy, so that the frequency always increases as one proceeds up the hierarchy.
</prevsent>
<prevsent>therefore the first common ancestor for pair is the most-informative subsumer (i.e., hasmost information content).
</prevsent>
</prevsection>
<citsent citstr=" H94-1046 ">
here attested frequencies from semcor (miller et al, 1994) <papid> H94-1046 </papid>are used, so all ancestors are considered.</citsent>
<aftsection>
<nextsent>specificity is accounted for by applying scaling factor to the frequencies that decreases as one proceeds up the hierarchy.
</nextsent>
<nextsent>thus, informative?
</nextsent>
<nextsent>is used more in an intuitive sense rather than technical.
</nextsent>
<nextsent>more details on the extraction process and the subsequent disambiguation can be found in (ohara, forthcoming).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2121">
<title id=" W04-2116.xml">empirical acquisition of differentiating relations from definitions </title>
<section> differentia disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>the wordnet definitions have recently been sense-tagged as part of the extended wordnet (novischi, 2002), so these annotations areincorporated.
</prevsent>
<prevsent>for other dictionaries, use of traditional word-sense disambiguation algorithms would be required.with the emphasis on corpus analysis in computational linguistics, there has been shift away from relying on explicitly coded knowledge towards the use of knowledge inferred from naturally occurring text,in particular text that has been annotated by humans to indicate phenomena of interest.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
the penn treebank version ii (marcus et al, 1994) <papid> H94-1020 </papid>provided the first large-scale set of case role annotations for general-purpose text.</citsent>
<aftsection>
<nextsent>these are very general roles akin to fill mores (1968) case roles.
</nextsent>
<nextsent>the berkeley framenet (fillmore et al, 2001) project provides the most recent large-scale annotation of semantic roles.
</nextsent>
<nextsent>these are at much finer granularity than those in treebank, so they should prove quite useful for applications learning detailed semantics from corpora.
</nextsent>
<nextsent>ohara and wiebe (2003) <papid> W03-0411 </papid>explain howboth inventories can be used for preposition disam biguation.the goal of relation disambiguation is to determine the underlying semantic role indicated by particular words in phrase or by word order.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2122">
<title id=" W04-2116.xml">empirical acquisition of differentiating relations from definitions </title>
<section> differentia disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>the berkeley framenet (fillmore et al, 2001) project provides the most recent large-scale annotation of semantic roles.
</prevsent>
<prevsent>these are at much finer granularity than those in treebank, so they should prove quite useful for applications learning detailed semantics from corpora.
</prevsent>
</prevsection>
<citsent citstr=" W03-0411 ">
ohara and wiebe (2003) <papid> W03-0411 </papid>explain howboth inventories can be used for preposition disam biguation.the goal of relation disambiguation is to determine the underlying semantic role indicated by particular words in phrase or by word order.</citsent>
<aftsection>
<nextsent>for relations indicated directly by function words, the disambiguation can be seen as special case of word sense disambiguation (wsd).
</nextsent>
<nextsent>as an example, refining the relationship dog?, with?
</nextsent>
<nextsent>, ears??
</nextsent>
<nextsent>into dog?, has-part , ears??, is equivalent to disambiguating the preposition with,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2123">
<title id=" W04-2116.xml">empirical acquisition of differentiating relations from definitions </title>
<section> differentia disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>the drawback however is that annotations from new resources must first be mapped into the common inventory before incorporation.
</prevsent>
<prevsent>the latter approach is employed here.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
the common inventory incorporates some of the general relation types defined by gildea and jurafsky (2002)<papid> J02-3001 </papid>for their experiments in classifying semantic relations in framenet using reduced inventory.</citsent>
<aftsection>
<nextsent>relation frequency theme 0.316 goal 0.116 ground 0.080 category 0.069 agent 0.069 cause 0.061 manner 0.058 recipient 0.053 medium 0.039 characteristic 0.022 resource 0.021 means 0.021 source 0.019 path 0.017 experiencer 0.017 accompaniment 0.011 area 0.010 direction 0.001 table 1: frequency of relations extracted.
</nextsent>
<nextsent>the evaluation discussed here assesses the quality of the information that would be added to the lexicons with respect to relation disambiguation, which is the focus of the research.
</nextsent>
<nextsent>an application-oriented evaluation is discussed in (ohara, forthcoming), showing how using the extracted information improves word sense disambiguation.
</nextsent>
<nextsent>all the definitions from wordnet 1.7.1 were run through the differentia-extraction process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2124">
<title id=" W04-2116.xml">empirical acquisition of differentiating relations from definitions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their extraction process is also closely tied into the specifics of the parser, as transformation rule is developed for each grammar rule.
</prevsent>
<prevsent>this work addresses the acquisition of conceptual distinctions.
</prevsent>
</prevsection>
<citsent citstr=" J02-2001 ">
in principle, it can handle any levelof granularity given sufficient training data; however, addressing distinctions at the level of near synonyms (edmonds and hirst, 2002) <papid> J02-2001 </papid>might require customized analysis for each cluster of nearly synonymous words.</citsent>
<aftsection>
<nextsent>inkpen and hirst (2001) discuss how this can be automated by analyzing specialized synonymy dictionaries.
</nextsent>
<nextsent>decision lists of indicative keywords are learned for the broad types of pragmatic distinctions, and these are then manually split into decision lists for more-specific distinctions.
</nextsent>
<nextsent>we have presented an empirical methodology for extracting information from dictionary definitions.this differs from previous approaches by using data driven relation disambiguation, using framenet semantic roles annotations mapped into reduced inventory.
</nextsent>
<nextsent>all the definitions from wordnet 1.7.1 we reanalyzed using this process, and the results evaluated by four human judges.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2125">
<title id=" W04-1007.xml">a rhetorical status classifier for legal text summarisation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they are texts which record the proceedings of court and, due to the role that precedents play in english law, easy access to themis essential for wide range of people.
</prevsent>
<prevsent>for this reason, they are frequently manually summarised by legal experts, with summaries varying according to target audience (e.g. students, solicitors).
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
in the sum project, we are exploring methods for generating flexible summaries of legal documents, taking as our point of departure the teufel and moens (2002; <papid> J02-4002 </papid>1999a; 1999b) approach to automatic summarisation (henceforth t&m;).</citsent>
<aftsection>
<nextsent>we have chosen to work with law reports for three main reasons: (a) the existence of manual summaries means that we have evaluation material for the final summarisationsystem; (b) the existence of differing target audiences allows us to explore the issue of tailored sum maries; and (c) the texts have much in common with the academic papers that t&m; worked with, while remaining challengingly different in many respects.
</nextsent>
<nextsent>our general aims are comparable with those of the salomon project (moens et al , 1997), which also deals with summarisation of legal texts, but our choice of methodology is designed to test the portability of the t&m; approach to new domain.
</nextsent>
<nextsent>the t&m; approach is an instance of what sparckjones (1999) terms text extraction where summary typically consists of sentences selected from the source text, with some smoothing to increase the coherence between the sentences.
</nextsent>
<nextsent>since the academic texts they use are rather long and the aim is to produce flexible summaries of varying length and for various audiences, t&m; go beyond simple sentence selection and classify source sentences according to their rhetorical status (e.g. description of the main result, criticism of someone elses work, etc.).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2137">
<title id=" W04-1007.xml">a rhetorical status classifier for legal text summarisation </title>
<section> the holj corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the architecture of our system is one where range of nlp tools is used in modular, pipelinedway to add linguistic knowledge to the xml document markup.
</prevsent>
<prevsent>in the token isation module we convert from the source html to holxml and then pass the data through sequence of calls to variety of xml based tools from the lt ttt and lt xml tool sets (grover et al , 2000; thompson et al , 1997).
</prevsent>
</prevsection>
<citsent citstr=" J97-3003 ">
thecore program in our pipelines is the lt ttt program fsgmatch, general purpose transducer which processes an input stream and adds annotations using rules provided in hand-written grammar file.the other main lt ttt program is ltpos, statistical combined part-of-speech (pos) tagger and sentence boundary disambiguation module (mikheev, 1997).<papid> J97-3003 </papid></citsent>
<aftsection>
<nextsent>the first step in the token isation modules uses fsgmatch to segment the contents of the paragraphs into word tokens encoded in the xml as welements.
</nextsent>
<nextsent>once the word tokens have been identified, the next step uses ltpos to mark up the sen holxml conversion to document html automatically annotated holxml document recognition named entity identification chunking &amp; clause verb &amp; subject features sation lemmati?
</nextsent>
<nextsent>token isation pos tagging &amp; sentence identification token isation module linguistic analysis module figure 1: holj processing stages tences as sent elements and to add part of speech attributes to word tokens.the motivation for the module that performs further linguistic analysis is to compute information tobe used to provide features for the sentence classifier.
</nextsent>
<nextsent>however, the information we compute is general purpose, making the data useful for range of nlp research activities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2139">
<title id=" W04-1007.xml">a rhetorical status classifier for legal text summarisation </title>
<section> the holj corpus.  </section>
<citcontext>
<prevsection>
<prevsent>token isation pos tagging &amp; sentence identification token isation module linguistic analysis module figure 1: holj processing stages tences as sent elements and to add part of speech attributes to word tokens.the motivation for the module that performs further linguistic analysis is to compute information tobe used to provide features for the sentence classifier.
</prevsent>
<prevsent>however, the information we compute is general purpose, making the data useful for range of nlp research activities.
</prevsent>
</prevsection>
<citsent citstr=" W00-1427 ">
the first step in the linguistic analysis module lemmatises the inflected words using minnen et al (2000) <papid> W00-1427 </papid>morpha lemmatiser.</citsent>
<aftsection>
<nextsent>this program is not xml-aware so we use xmlperl (mckelvie, 1999) to provide wrapper so that it can be incorporated in the xml pipeline.
</nextsent>
<nextsent>we use similar mechanism for the other non-xml components.
</nextsent>
<nextsent>the next stage, described in figure 1 as named entity recognition, is in fact more complex layering of two kinds of named entity recognition.
</nextsent>
<nextsent>the documents in our domain contain the standard kindsof entities familiar from the muc and conll competitions (chinchor, 1998; roth and vanden bosch,2002; daelemans and osborne, 2003), such as person, organisation, location and date.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2140">
<title id=" W04-1007.xml">a rhetorical status classifier for legal text summarisation </title>
<section> the holj corpus.  </section>
<citcontext>
<prevsection>
<prevsent>table 2 shows examples of the entities we have marked up in the corpus (in our annotation scheme these are noun groups (ng) with specific type and subtype attributes).
</prevsent>
<prevsent>in the top two blocks of the table are examples of domain-specific entities such as courts, judges, acts and judgments, while inthe third block we show examples of non-domain specific entity types.
</prevsent>
</prevsection>
<citsent citstr=" W03-0424 ">
we use different strategies for the identification of the two classes of entities: for the domain-specific ones we use hand-crafted lt ttt rules, while for the non-domain-specific ones we use the c&c; named entity tagger (curran and clark, 2003) <papid> W03-0424 </papid>trained on the muc7 data set.</citsent>
<aftsection>
<nextsent>forsome entities, the two approaches provide competing analyses and in all cases the domain-specific label is to be preferred since it provides finer-grainedinformation.
</nextsent>
<nextsent>however, while the rule-based recogniser can operate incrementally over data which already contains some entity markup, the c&c; tagger is trained to operate over un labelled sentences.
</nextsent>
<nextsent>for this reason we run the c&c; tagger first and encode its results as attributes on the words.
</nextsent>
<nextsent>we then run the domain-specific tagger, encoding its results as xml elements enclosing the words, and finish with similar encoding of whichever c&c; entities can still be realised in the un labelled subparts of the sentences (these are labelled as subtype=fromcc?).part of the rule-based entity recognition component builds an on-the-fly?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2143">
<title id=" W04-1007.xml">a rhetorical status classifier for legal text summarisation </title>
<section> the holj corpus.  </section>
<citcontext>
<prevsection>
<prevsent>we do this with clause identifier (hachey, 2002) built using the conll-2001 shared task data (sang and dejean, 2001).
</prevsent>
<prevsent>clause identification is performed in three steps.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
first, two maximum entropy classifiers (berger et al , 1996) <papid> J96-1002 </papid>are applied, where the first predicts clause start labels and the second predicts clause end labels.</citsent>
<aftsection>
<nextsent>in the the third step clause segmentation is inferred from the predicted starts and ends using maximum entropy model whose sole purpose is to provide confidence values for potential clauses.the final stages of linguistic processing use hand written lt ttt components to compute features of verb and noun groups.
</nextsent>
<nextsent>for all verb groups, attributes encoding tense, aspect, modality and negation are added to the mark-up: for example, might not have been brought is analysed as  vg tense=pres?, as pect=perf?, voice=pass?, modal=yes?, neg=yes? .
</nextsent>
<nextsent>in addition, subject noun groups are identified and lemma information from the head noun of the subject and the head verb of the verb group are propagated to the verb group attribute list.
</nextsent>
<nextsent>3.1 feature sets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2155">
<title id=" W04-1007.xml">a rhetorical status classifier for legal text summarisation </title>
<section> the sentence classifier.  </section>
<citcontext>
<prevsection>
<prevsent>t&m; invested considerable amount of effort in compiling lists of such cue phrases and building hand-crafted lexicons where the cue phrases are assigned to one of number of fixed categories.
</prevsent>
<prevsent>a primary aim ofthe current research is to investigate whether the effects of t&ms; cue phrase features can be achieved using automatically computable linguistic features.if they can, then this helps to relieve the burden involved in porting systems such as these to new domains.
</prevsent>
</prevsection>
<citsent citstr=" W03-0505 ">
our preliminary cue phrase feature set includes syntactic features of the main verb (voice, tense, aspect, modality, negation), which we have shown to be correlated with rhetorical status (grover et al , 2003).<papid> W03-0505 </papid></citsent>
<aftsection>
<nextsent>we also use features indicating sentence initial part-of-speech and sentence initial word features to roughly approximate formulaic expressions which are sentence-level adverbial or prepositional phrases.
</nextsent>
<nextsent>subject features include the head lemma,entity type, and entity subtype.
</nextsent>
<nextsent>these features approximate the hand-coded agent features of t&amp;m.; main verb lemma feature simulates t&ms; type of action and feature encoding the part-of-speechafter the main verb is meant to capture basic subcategorisation information.
</nextsent>
<nextsent>3.2 classifier results and discussion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2161">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1conll-2004 shared task web page with data, software and systems?
</prevsent>
<prevsent>outputs available?
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
at http://cnts.uia.ac.be/conll2004/roles . regarding the learning component of the systems,we find pure probabilistic models (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003), <papid> W03-1008 </papid>maximum entropy (fleischman et al,2003), <papid> W03-1007 </papid>generative models (thompson et al, 2003), decision trees (surdeanu et al, 2003; <papid> P03-1002 </papid>chen and ram bow, 2003), <papid> W03-1006 </papid>and support vector machines (hacioglu and ward, 2003; <papid> N03-2009 </papid>pradhan et al, 2003a; pradhan et al, 2003b).there have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees.</citsent>
<aftsection>
<nextsent>for instance, in (pradhan et al, 2003a; hacioglu and ward, 2003), <papid> N03-2009 </papid>svm-based srl system is devised which performs an iob sequence tagging using only shallow syntactic information at the level of phrase chunks.</nextsent>
<nextsent>nowadays, there exist two main english corpora with semantic annotations from which to train srl systems: propbank (palmer et al, 2004) and framenet (fillmoreet al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2163">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1conll-2004 shared task web page with data, software and systems?
</prevsent>
<prevsent>outputs available?
</prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
at http://cnts.uia.ac.be/conll2004/roles . regarding the learning component of the systems,we find pure probabilistic models (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003), <papid> W03-1008 </papid>maximum entropy (fleischman et al,2003), <papid> W03-1007 </papid>generative models (thompson et al, 2003), decision trees (surdeanu et al, 2003; <papid> P03-1002 </papid>chen and ram bow, 2003), <papid> W03-1006 </papid>and support vector machines (hacioglu and ward, 2003; <papid> N03-2009 </papid>pradhan et al, 2003a; pradhan et al, 2003b).there have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees.</citsent>
<aftsection>
<nextsent>for instance, in (pradhan et al, 2003a; hacioglu and ward, 2003), <papid> N03-2009 </papid>svm-based srl system is devised which performs an iob sequence tagging using only shallow syntactic information at the level of phrase chunks.</nextsent>
<nextsent>nowadays, there exist two main english corpora with semantic annotations from which to train srl systems: propbank (palmer et al, 2004) and framenet (fillmoreet al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2164">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1conll-2004 shared task web page with data, software and systems?
</prevsent>
<prevsent>outputs available?
</prevsent>
</prevsection>
<citsent citstr=" W03-1008 ">
at http://cnts.uia.ac.be/conll2004/roles . regarding the learning component of the systems,we find pure probabilistic models (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003), <papid> W03-1008 </papid>maximum entropy (fleischman et al,2003), <papid> W03-1007 </papid>generative models (thompson et al, 2003), decision trees (surdeanu et al, 2003; <papid> P03-1002 </papid>chen and ram bow, 2003), <papid> W03-1006 </papid>and support vector machines (hacioglu and ward, 2003; <papid> N03-2009 </papid>pradhan et al, 2003a; pradhan et al, 2003b).there have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees.</citsent>
<aftsection>
<nextsent>for instance, in (pradhan et al, 2003a; hacioglu and ward, 2003), <papid> N03-2009 </papid>svm-based srl system is devised which performs an iob sequence tagging using only shallow syntactic information at the level of phrase chunks.</nextsent>
<nextsent>nowadays, there exist two main english corpora with semantic annotations from which to train srl systems: propbank (palmer et al, 2004) and framenet (fillmoreet al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2165">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1conll-2004 shared task web page with data, software and systems?
</prevsent>
<prevsent>outputs available?
</prevsent>
</prevsection>
<citsent citstr=" W03-1007 ">
at http://cnts.uia.ac.be/conll2004/roles . regarding the learning component of the systems,we find pure probabilistic models (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003), <papid> W03-1008 </papid>maximum entropy (fleischman et al,2003), <papid> W03-1007 </papid>generative models (thompson et al, 2003), decision trees (surdeanu et al, 2003; <papid> P03-1002 </papid>chen and ram bow, 2003), <papid> W03-1006 </papid>and support vector machines (hacioglu and ward, 2003; <papid> N03-2009 </papid>pradhan et al, 2003a; pradhan et al, 2003b).there have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees.</citsent>
<aftsection>
<nextsent>for instance, in (pradhan et al, 2003a; hacioglu and ward, 2003), <papid> N03-2009 </papid>svm-based srl system is devised which performs an iob sequence tagging using only shallow syntactic information at the level of phrase chunks.</nextsent>
<nextsent>nowadays, there exist two main english corpora with semantic annotations from which to train srl systems: propbank (palmer et al, 2004) and framenet (fillmoreet al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2166">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1conll-2004 shared task web page with data, software and systems?
</prevsent>
<prevsent>outputs available?
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
at http://cnts.uia.ac.be/conll2004/roles . regarding the learning component of the systems,we find pure probabilistic models (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003), <papid> W03-1008 </papid>maximum entropy (fleischman et al,2003), <papid> W03-1007 </papid>generative models (thompson et al, 2003), decision trees (surdeanu et al, 2003; <papid> P03-1002 </papid>chen and ram bow, 2003), <papid> W03-1006 </papid>and support vector machines (hacioglu and ward, 2003; <papid> N03-2009 </papid>pradhan et al, 2003a; pradhan et al, 2003b).there have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees.</citsent>
<aftsection>
<nextsent>for instance, in (pradhan et al, 2003a; hacioglu and ward, 2003), <papid> N03-2009 </papid>svm-based srl system is devised which performs an iob sequence tagging using only shallow syntactic information at the level of phrase chunks.</nextsent>
<nextsent>nowadays, there exist two main english corpora with semantic annotations from which to train srl systems: propbank (palmer et al, 2004) and framenet (fillmoreet al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2168">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1conll-2004 shared task web page with data, software and systems?
</prevsent>
<prevsent>outputs available?
</prevsent>
</prevsection>
<citsent citstr=" W03-1006 ">
at http://cnts.uia.ac.be/conll2004/roles . regarding the learning component of the systems,we find pure probabilistic models (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003), <papid> W03-1008 </papid>maximum entropy (fleischman et al,2003), <papid> W03-1007 </papid>generative models (thompson et al, 2003), decision trees (surdeanu et al, 2003; <papid> P03-1002 </papid>chen and ram bow, 2003), <papid> W03-1006 </papid>and support vector machines (hacioglu and ward, 2003; <papid> N03-2009 </papid>pradhan et al, 2003a; pradhan et al, 2003b).there have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees.</citsent>
<aftsection>
<nextsent>for instance, in (pradhan et al, 2003a; hacioglu and ward, 2003), <papid> N03-2009 </papid>svm-based srl system is devised which performs an iob sequence tagging using only shallow syntactic information at the level of phrase chunks.</nextsent>
<nextsent>nowadays, there exist two main english corpora with semantic annotations from which to train srl systems: propbank (palmer et al, 2004) and framenet (fillmoreet al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2169">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1conll-2004 shared task web page with data, software and systems?
</prevsent>
<prevsent>outputs available?
</prevsent>
</prevsection>
<citsent citstr=" N03-2009 ">
at http://cnts.uia.ac.be/conll2004/roles . regarding the learning component of the systems,we find pure probabilistic models (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003), <papid> W03-1008 </papid>maximum entropy (fleischman et al,2003), <papid> W03-1007 </papid>generative models (thompson et al, 2003), decision trees (surdeanu et al, 2003; <papid> P03-1002 </papid>chen and ram bow, 2003), <papid> W03-1006 </papid>and support vector machines (hacioglu and ward, 2003; <papid> N03-2009 </papid>pradhan et al, 2003a; pradhan et al, 2003b).there have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees.</citsent>
<aftsection>
<nextsent>for instance, in (pradhan et al, 2003a; hacioglu and ward, 2003), <papid> N03-2009 </papid>svm-based srl system is devised which performs an iob sequence tagging using only shallow syntactic information at the level of phrase chunks.</nextsent>
<nextsent>nowadays, there exist two main english corpora with semantic annotations from which to train srl systems: propbank (palmer et al, 2004) and framenet (fillmoreet al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2171">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>except for non-trivial cases, this situation makes the verb fairly easy to identify and, since there is one verb with each proposition, evaluating its recognition overestimates the overall performance of system.
</prevsent>
<prevsent>for this reason, the verb argument is excluded from evaluation.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the data consists of six sections of the wall street journal part of the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>and follows the setting of past editions of the conll sharedtask: training set (sections 15-18), development set (section 20) and test set (section 21).</citsent>
<aftsection>
<nextsent>we first describe annotations related to argument structure.
</nextsent>
<nextsent>then, we describe the preprocessing of input data.
</nextsent>
<nextsent>finally, we describe the format of the datasets.
</nextsent>
<nextsent>3.1 propbank.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2172">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> clauses in start-end format..  </section>
<citcontext>
<prevsection>
<prevsent>up to six different learning algorithms have been applied in the conll-2004 shared task.
</prevsent>
<prevsent>none of them is new with respect to the past editions.
</prevsent>
</prevsection>
<citsent citstr=" W04-2413 ">
two teams used the maximum entropy (me) statistical framework (baldewein et al, 2004; <papid> W04-2413 </papid>lim et al, 2004).<papid> W04-2419 </papid></citsent>
<aftsection>
<nextsent>two teams used brills transformation-based error-driven learning (tbl) (higgins, 2004; <papid> W04-2417 </papid>williams et al, 2004).<papid> W04-2422 </papid></nextsent>
<nextsent>two other groups applied memory-based learning (mbl) (van den bosch et al, 2004; kouchnir, 2004).<papid> W04-2418 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2174">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> clauses in start-end format..  </section>
<citcontext>
<prevsection>
<prevsent>up to six different learning algorithms have been applied in the conll-2004 shared task.
</prevsent>
<prevsent>none of them is new with respect to the past editions.
</prevsent>
</prevsection>
<citsent citstr=" W04-2419 ">
two teams used the maximum entropy (me) statistical framework (baldewein et al, 2004; <papid> W04-2413 </papid>lim et al, 2004).<papid> W04-2419 </papid></citsent>
<aftsection>
<nextsent>two teams used brills transformation-based error-driven learning (tbl) (higgins, 2004; <papid> W04-2417 </papid>williams et al, 2004).<papid> W04-2422 </papid></nextsent>
<nextsent>two other groups applied memory-based learning (mbl) (van den bosch et al, 2004; kouchnir, 2004).<papid> W04-2418 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2175">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> clauses in start-end format..  </section>
<citcontext>
<prevsection>
<prevsent>none of them is new with respect to the past editions.
</prevsent>
<prevsent>two teams used the maximum entropy (me) statistical framework (baldewein et al, 2004; <papid> W04-2413 </papid>lim et al, 2004).<papid> W04-2419 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-2417 ">
two teams used brills transformation-based error-driven learning (tbl) (higgins, 2004; <papid> W04-2417 </papid>williams et al, 2004).<papid> W04-2422 </papid></citsent>
<aftsection>
<nextsent>two other groups applied memory-based learning (mbl) (van den bosch et al, 2004; kouchnir, 2004).<papid> W04-2418 </papid></nextsent>
<nextsent>the remaining four teams employed vector-based linear classifiers of different types: hacioglu et al (2004) <papid> W04-2416 </papid>and park et al (2004)<papid> W04-2420 </papid>used support vector machines (svm) with polynomial kernels, carreras et al (2004)<papid> W04-2415 </papid>used voted percep trons (vp) also with polynomial kernels, and finally, punyakanok et al (2004) <papid> W04-2421 </papid>used snow, winnow-based network of linear separators.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2176">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> clauses in start-end format..  </section>
<citcontext>
<prevsection>
<prevsent>none of them is new with respect to the past editions.
</prevsent>
<prevsent>two teams used the maximum entropy (me) statistical framework (baldewein et al, 2004; <papid> W04-2413 </papid>lim et al, 2004).<papid> W04-2419 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-2422 ">
two teams used brills transformation-based error-driven learning (tbl) (higgins, 2004; <papid> W04-2417 </papid>williams et al, 2004).<papid> W04-2422 </papid></citsent>
<aftsection>
<nextsent>two other groups applied memory-based learning (mbl) (van den bosch et al, 2004; kouchnir, 2004).<papid> W04-2418 </papid></nextsent>
<nextsent>the remaining four teams employed vector-based linear classifiers of different types: hacioglu et al (2004) <papid> W04-2416 </papid>and park et al (2004)<papid> W04-2420 </papid>used support vector machines (svm) with polynomial kernels, carreras et al (2004)<papid> W04-2415 </papid>used voted percep trons (vp) also with polynomial kernels, and finally, punyakanok et al (2004) <papid> W04-2421 </papid>used snow, winnow-based network of linear separators.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2177">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> clauses in start-end format..  </section>
<citcontext>
<prevsection>
<prevsent>two teams used the maximum entropy (me) statistical framework (baldewein et al, 2004; <papid> W04-2413 </papid>lim et al, 2004).<papid> W04-2419 </papid></prevsent>
<prevsent>two teams used brills transformation-based error-driven learning (tbl) (higgins, 2004; <papid> W04-2417 </papid>williams et al, 2004).<papid> W04-2422 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-2418 ">
two other groups applied memory-based learning (mbl) (van den bosch et al, 2004; kouchnir, 2004).<papid> W04-2418 </papid></citsent>
<aftsection>
<nextsent>the remaining four teams employed vector-based linear classifiers of different types: hacioglu et al (2004) <papid> W04-2416 </papid>and park et al (2004)<papid> W04-2420 </papid>used support vector machines (svm) with polynomial kernels, carreras et al (2004)<papid> W04-2415 </papid>used voted percep trons (vp) also with polynomial kernels, and finally, punyakanok et al (2004) <papid> W04-2421 </papid>used snow, winnow-based network of linear separators.</nextsent>
<nextsent>additionally, the team of baldewein et al (2004) <papid> W04-2413 </papid>used embased clustering algorithm for feature development (see section 4.3).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2178">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> clauses in start-end format..  </section>
<citcontext>
<prevsection>
<prevsent>two teams used brills transformation-based error-driven learning (tbl) (higgins, 2004; <papid> W04-2417 </papid>williams et al, 2004).<papid> W04-2422 </papid></prevsent>
<prevsent>two other groups applied memory-based learning (mbl) (van den bosch et al, 2004; kouchnir, 2004).<papid> W04-2418 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-2416 ">
the remaining four teams employed vector-based linear classifiers of different types: hacioglu et al (2004) <papid> W04-2416 </papid>and park et al (2004)<papid> W04-2420 </papid>used support vector machines (svm) with polynomial kernels, carreras et al (2004)<papid> W04-2415 </papid>used voted percep trons (vp) also with polynomial kernels, and finally, punyakanok et al (2004) <papid> W04-2421 </papid>used snow, winnow-based network of linear separators.</citsent>
<aftsection>
<nextsent>additionally, the team of baldewein et al (2004) <papid> W04-2413 </papid>used embased clustering algorithm for feature development (see section 4.3).</nextsent>
<nextsent>as main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2179">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> clauses in start-end format..  </section>
<citcontext>
<prevsection>
<prevsent>two teams used brills transformation-based error-driven learning (tbl) (higgins, 2004; <papid> W04-2417 </papid>williams et al, 2004).<papid> W04-2422 </papid></prevsent>
<prevsent>two other groups applied memory-based learning (mbl) (van den bosch et al, 2004; kouchnir, 2004).<papid> W04-2418 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-2420 ">
the remaining four teams employed vector-based linear classifiers of different types: hacioglu et al (2004) <papid> W04-2416 </papid>and park et al (2004)<papid> W04-2420 </papid>used support vector machines (svm) with polynomial kernels, carreras et al (2004)<papid> W04-2415 </papid>used voted percep trons (vp) also with polynomial kernels, and finally, punyakanok et al (2004) <papid> W04-2421 </papid>used snow, winnow-based network of linear separators.</citsent>
<aftsection>
<nextsent>additionally, the team of baldewein et al (2004) <papid> W04-2413 </papid>used embased clustering algorithm for feature development (see section 4.3).</nextsent>
<nextsent>as main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2180">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> clauses in start-end format..  </section>
<citcontext>
<prevsection>
<prevsent>two teams used brills transformation-based error-driven learning (tbl) (higgins, 2004; <papid> W04-2417 </papid>williams et al, 2004).<papid> W04-2422 </papid></prevsent>
<prevsent>two other groups applied memory-based learning (mbl) (van den bosch et al, 2004; kouchnir, 2004).<papid> W04-2418 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-2415 ">
the remaining four teams employed vector-based linear classifiers of different types: hacioglu et al (2004) <papid> W04-2416 </papid>and park et al (2004)<papid> W04-2420 </papid>used support vector machines (svm) with polynomial kernels, carreras et al (2004)<papid> W04-2415 </papid>used voted percep trons (vp) also with polynomial kernels, and finally, punyakanok et al (2004) <papid> W04-2421 </papid>used snow, winnow-based network of linear separators.</citsent>
<aftsection>
<nextsent>additionally, the team of baldewein et al (2004) <papid> W04-2413 </papid>used embased clustering algorithm for feature development (see section 4.3).</nextsent>
<nextsent>as main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2182">
<title id=" W04-2412.xml">introduction to the conll2004 shared task semantic role labeling </title>
<section> clauses in start-end format..  </section>
<citcontext>
<prevsection>
<prevsent>two teams used brills transformation-based error-driven learning (tbl) (higgins, 2004; <papid> W04-2417 </papid>williams et al, 2004).<papid> W04-2422 </papid></prevsent>
<prevsent>two other groups applied memory-based learning (mbl) (van den bosch et al, 2004; kouchnir, 2004).<papid> W04-2418 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-2421 ">
the remaining four teams employed vector-based linear classifiers of different types: hacioglu et al (2004) <papid> W04-2416 </papid>and park et al (2004)<papid> W04-2420 </papid>used support vector machines (svm) with polynomial kernels, carreras et al (2004)<papid> W04-2415 </papid>used voted percep trons (vp) also with polynomial kernels, and finally, punyakanok et al (2004) <papid> W04-2421 </papid>used snow, winnow-based network of linear separators.</citsent>
<aftsection>
<nextsent>additionally, the team of baldewein et al (2004) <papid> W04-2413 </papid>used embased clustering algorithm for feature development (see section 4.3).</nextsent>
<nextsent>as main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2236">
<title id=" W04-0607.xml">feeding owl extracting and representing the content of pathology reports </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>semantics appropriate for their part of speech, so that they do not confuse the later parsing stage.
</prevsent>
<prevsent>(more details about this kind of robustness will be given shortly.)
</prevsent>
</prevsection>
<citsent citstr=" W02-1706 ">
6see for example (grover et al , 2002) <papid> W02-1706 </papid>for discussion of the utility of named entitiy recognition preprocessing stage for robust symbolic parsing.</citsent>
<aftsection>
<nextsent>7note that this lexicon is one single resource out of which also the domain specfic additions to the morphology-lexicon and the list of multi-word expressions are compiled.
</nextsent>
<nextsent>3.3 chunk parsing.
</nextsent>
<nextsent>next, the analyses of the tokens are transformed into feature structure format, and are passed to the parsing component.8 the output of this stageis an intermediate semantic representation of (as pects of) the content (of which the notation shown in 1 is variant).
</nextsent>
<nextsent>this format is akin to traditional logical forms and still has to be mapped into owl; we decided on this strategy because such format is closer to surface structure and hence easier tobuild compositionally (see discussion below in section 3.5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2238">
<title id=" W03-2109.xml">interpreter for highly portable spoken dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, the cost of developing new spoken dialogue system is enormous.
</prevsent>
<prevsent>the systems that have been developed so far can not be transferred to other domains easily, and highly-portable system that can be easily adapted to another domain or task urgently should be developed.
</prevsent>
</prevsection>
<citsent citstr=" P99-1025 ">
there are several examples of researches that focused on high portability and expansibility (kaspar and hoffmannn, 1998; brond sted et al, 1998; sutton et al, 1998; sasa jima et al, 1999; abella and gorin, 1999; <papid> P99-1025 </papid>levin et al, 2000).</citsent>
<aftsection>
<nextsent>in (kaspar and hoffmannn, 1998), prototype could be simply constructed even in complicated speech dialogue system using the pia system, which was implemented using visual basic.
</nextsent>
<nextsent>this system placed priority on achieving high robustness of speech recognition and high naturalness of generated dialogue.
</nextsent>
<nextsent>however, the system limited the task to the domain of knowledge search.
</nextsent>
<nextsent>e. levin et al. reported the design and implementation of theat&t; communicator mixed-initiative spoken dialogue system (levin et al, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2239">
<title id=" W04-2327.xml">the mate gnome proposals for anaphoric annotation revisited </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we discuss these first experiences with the scheme, some lessons that have been learned, and suggest few modifications.
</prevsent>
<prevsent>the mate meta-scheme?
</prevsent>
</prevsection>
<citsent citstr=" W99-0309 ">
for anaphora annotation (poesio et al, 1999) <papid> W99-0309 </papid>is one of the annotation schemes developed as part of the mate project (mckelvie et al, 2001), whose goal was to develop annotation tools suitable for different types of dialogue annotation.</citsent>
<aftsection>
<nextsent>the scheme has served as the basis for number of annotation projects,such as the development of the gnome corpus (poe sio, 2000a) and, more recently, of the venex corpus of anaphora in italian spoken dialogue and text (poesio et al., 2004<papid> J04-3003 </papid>a).</nextsent>
<nextsent>the gnome corpus has been used to study salience, particularly as formalized in centering theory(poesio et al, 2004<papid> J04-3003 </papid>c), to develop statistical models of natural language generation (e.g., (poesio, 2000a; henschel et al, 2000; <papid> C00-1045 </papid>cheng et al, 2001; <papid> N01-1002 </papid>cheng, 2001; karamanis, 2003)) and to evaluate anaphora resolution systems,with special focus on the resolution of bridging references (poesio, 2003; poesio and alexandrov-kabadjov, 2004; poesio et al, 2004<papid> J04-3003 </papid>b).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2240">
<title id=" W04-2327.xml">the mate gnome proposals for anaphoric annotation revisited </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the mate meta-scheme?
</prevsent>
<prevsent>for anaphora annotation (poesio et al, 1999) <papid> W99-0309 </papid>is one of the annotation schemes developed as part of the mate project (mckelvie et al, 2001), whose goal was to develop annotation tools suitable for different types of dialogue annotation.</prevsent>
</prevsection>
<citsent citstr=" J04-3003 ">
the scheme has served as the basis for number of annotation projects,such as the development of the gnome corpus (poe sio, 2000a) and, more recently, of the venex corpus of anaphora in italian spoken dialogue and text (poesio et al., 2004<papid> J04-3003 </papid>a).</citsent>
<aftsection>
<nextsent>the gnome corpus has been used to study salience, particularly as formalized in centering theory(poesio et al, 2004<papid> J04-3003 </papid>c), to develop statistical models of natural language generation (e.g., (poesio, 2000a; henschel et al, 2000; <papid> C00-1045 </papid>cheng et al, 2001; <papid> N01-1002 </papid>cheng, 2001; karamanis, 2003)) and to evaluate anaphora resolution systems,with special focus on the resolution of bridging references (poesio, 2003; poesio and alexandrov-kabadjov, 2004; poesio et al, 2004<papid> J04-3003 </papid>b).</nextsent>
<nextsent>aspects of the scheme have been implemented in annotation tools including mmax(muller and strube, 2003) and the annotator tool developed by ilsp.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2246">
<title id=" W04-2327.xml">the mate gnome proposals for anaphoric annotation revisited </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for anaphora annotation (poesio et al, 1999) <papid> W99-0309 </papid>is one of the annotation schemes developed as part of the mate project (mckelvie et al, 2001), whose goal was to develop annotation tools suitable for different types of dialogue annotation.</prevsent>
<prevsent>the scheme has served as the basis for number of annotation projects,such as the development of the gnome corpus (poe sio, 2000a) and, more recently, of the venex corpus of anaphora in italian spoken dialogue and text (poesio et al., 2004<papid> J04-3003 </papid>a).</prevsent>
</prevsection>
<citsent citstr=" C00-1045 ">
the gnome corpus has been used to study salience, particularly as formalized in centering theory(poesio et al, 2004<papid> J04-3003 </papid>c), to develop statistical models of natural language generation (e.g., (poesio, 2000a; henschel et al, 2000; <papid> C00-1045 </papid>cheng et al, 2001; <papid> N01-1002 </papid>cheng, 2001; karamanis, 2003)) and to evaluate anaphora resolution systems,with special focus on the resolution of bridging references (poesio, 2003; poesio and alexandrov-kabadjov, 2004; poesio et al, 2004<papid> J04-3003 </papid>b).</citsent>
<aftsection>
<nextsent>aspects of the scheme have been implemented in annotation tools including mmax(muller and strube, 2003) and the annotator tool developed by ilsp.
</nextsent>
<nextsent>as result of this work, many aspects of the proposals concerning anaphoric annotation made inmate and gnome have been subjected to thorough test.
</nextsent>
<nextsent>in this paper we discuss some of the lessons learned through this work, some issues that have been raised, and how they have been or could be addressed.
</nextsent>
<nextsent>the design of an annotation scheme involves number of decisions: what has to be annotated, how, and how the annotation should be recorded (the markup scheme).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2247">
<title id=" W04-2327.xml">the mate gnome proposals for anaphoric annotation revisited </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for anaphora annotation (poesio et al, 1999) <papid> W99-0309 </papid>is one of the annotation schemes developed as part of the mate project (mckelvie et al, 2001), whose goal was to develop annotation tools suitable for different types of dialogue annotation.</prevsent>
<prevsent>the scheme has served as the basis for number of annotation projects,such as the development of the gnome corpus (poe sio, 2000a) and, more recently, of the venex corpus of anaphora in italian spoken dialogue and text (poesio et al., 2004<papid> J04-3003 </papid>a).</prevsent>
</prevsection>
<citsent citstr=" N01-1002 ">
the gnome corpus has been used to study salience, particularly as formalized in centering theory(poesio et al, 2004<papid> J04-3003 </papid>c), to develop statistical models of natural language generation (e.g., (poesio, 2000a; henschel et al, 2000; <papid> C00-1045 </papid>cheng et al, 2001; <papid> N01-1002 </papid>cheng, 2001; karamanis, 2003)) and to evaluate anaphora resolution systems,with special focus on the resolution of bridging references (poesio, 2003; poesio and alexandrov-kabadjov, 2004; poesio et al, 2004<papid> J04-3003 </papid>b).</citsent>
<aftsection>
<nextsent>aspects of the scheme have been implemented in annotation tools including mmax(muller and strube, 2003) and the annotator tool developed by ilsp.
</nextsent>
<nextsent>as result of this work, many aspects of the proposals concerning anaphoric annotation made inmate and gnome have been subjected to thorough test.
</nextsent>
<nextsent>in this paper we discuss some of the lessons learned through this work, some issues that have been raised, and how they have been or could be addressed.
</nextsent>
<nextsent>the design of an annotation scheme involves number of decisions: what has to be annotated, how, and how the annotation should be recorded (the markup scheme).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2253">
<title id=" W04-2327.xml">the mate gnome proposals for anaphoric annotation revisited </title>
<section> the mate proposals.  </section>
<citcontext>
<prevsection>
<prevsent>b. f: alors donc / vous avez / ici /  de id= de_88   les modeles de fusees  /de  m: oui f: et vous allez essayer de vous mettre daccord sur un classe ment /hein classer  de id= de_89   les fusees qui ont bien vole?
</prevsent>
<prevsent> /de  ou  de id= de_90   qui ont moins bien vole?
</prevsent>
</prevsection>
<citsent citstr=" J98-2001 ">
/de   link href= coref.xml#id(de_89)    anchor href= coref.xml#id(de_88)  type= subset   /   /link   link href= coref.xml#id(de_90)  type= subset      anchor href= coref.xml#id(de_88) /   /link  it was pointed out, however, that the results ofpoesio and vieira (1998) <papid> J98-2001 </papid>indicated that this type of annotation could be highly unreliable.</citsent>
<aftsection>
<nextsent>references to the visual situation specialuniverse?
</nextsent>
<nextsent>element was suggested for maptask style annotations of references to visible objects.
</nextsent>
<nextsent>the universe?
</nextsent>
<nextsent>element containing one ue?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2254">
<title id=" W04-2327.xml">the mate gnome proposals for anaphoric annotation revisited </title>
<section> work based on the mate proposals.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 annotation work related to the gnome project.
</prevsent>
<prevsent>the most direct application of the ideas discussed above was found in the annotation work undertaken as part ofthe gnome project.
</prevsent>
</prevsection>
<citsent citstr=" P00-1051 ">
gnome was concerned with the empirical investigation of the aspects of discourse that appear to affect generation, especially salience (pearson et al., 2000; poesio et al, 2000; <papid> P00-1051 </papid>poesio and di eugenio, 2001; poesio and nissim, 2001; poesio et al, 2004<papid> J04-3003 </papid>c).</citsent>
<aftsection>
<nextsent>particular attention was paid to the factors affecting the generation of pronouns (pearson et al, 2000; henschel et al., 2000), <papid> C00-1045 </papid>demonstratives (poesio and nygren-modjeska, to appear) possess ives (poesio and nissim, 2001) and definites in general (poesio, 2004).</nextsent>
<nextsent>these results, and the annotated corpus, were applied to the development of both symbolic and statistical natural language generation algorithms with the application of these empirical results to natural language generation, from sentence planning (poesio, 2000a; henschel et al, 2000; <papid> C00-1045 </papid>cheng et al, 2001), <papid> N01-1002 </papid>to aggregation (cheng, 2001) and text planning(kibble and power, 2000; <papid> W00-1411 </papid>karamanis, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2263">
<title id=" W04-2327.xml">the mate gnome proposals for anaphoric annotation revisited </title>
<section> work based on the mate proposals.  </section>
<citcontext>
<prevsection>
<prevsent>gnome was concerned with the empirical investigation of the aspects of discourse that appear to affect generation, especially salience (pearson et al., 2000; poesio et al, 2000; <papid> P00-1051 </papid>poesio and di eugenio, 2001; poesio and nissim, 2001; poesio et al, 2004<papid> J04-3003 </papid>c).</prevsent>
<prevsent>particular attention was paid to the factors affecting the generation of pronouns (pearson et al, 2000; henschel et al., 2000), <papid> C00-1045 </papid>demonstratives (poesio and nygren-modjeska, to appear) possess ives (poesio and nissim, 2001) and definites in general (poesio, 2004).</prevsent>
</prevsection>
<citsent citstr=" W00-1411 ">
these results, and the annotated corpus, were applied to the development of both symbolic and statistical natural language generation algorithms with the application of these empirical results to natural language generation, from sentence planning (poesio, 2000a; henschel et al, 2000; <papid> C00-1045 </papid>cheng et al, 2001), <papid> N01-1002 </papid>to aggregation (cheng, 2001) and text planning(kibble and power, 2000; <papid> W00-1411 </papid>karamanis, 2003).</citsent>
<aftsection>
<nextsent>the empirical side of the project involved both psychological experiments and corpus annotation, based on scheme basedon the mate proposals, as well as on detailed annotation manual (poesio, 2000b), the reliability of whose instructions was tested by extensive experiments (poesio, 2000a).
</nextsent>
<nextsent>more recently, the corpus has also been used to develop and evaluate anaphora resolution systems, with special focus on the resolution of bridging references (poesio, 2003; poesio and alexandrov-kabadjov, 2004; poesio et al, 2004<papid> J04-3003 </papid>b).</nextsent>
<nextsent>the corpus the gnome corpus currently includes texts from three domains, about 3000 nps were annotated in each domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2274">
<title id=" W04-2327.xml">the mate gnome proposals for anaphoric annotation revisited </title>
<section> work based on the mate proposals.  </section>
<citcontext>
<prevsection>
<prevsent>the separate levels when needed.
</prevsent>
<prevsent>as neither the mate workbench nor any other tools basedon standoff were available by the time the gnome annotation started,7 in gnome we didnt use standoff, but integrated all levels of annotation in one file; an emacs mode was developed for the annotation.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
this decision made it very easy to use the annotated corpus for number of studies, but did resulted in number of problems, the main among which were that the annotators had to bevery careful not to damage other annotations; that annotators working on one level were occasionally confused by annotations for other levels; and that the annotation work had to be organized in careful sequential way even for levels that could have been annotated independently.the main new aspect of the markup scheme, especially as far as our studies of salience were concerned, are the elements used to annotate potential utterance sin the sense of centering (grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>in order not to prejudge the answer to the question of which text constituents are best viewed as utterances, we used generic?
</nextsent>
<nextsent>element called unit?
</nextsent>
<nextsent>to mark up finite andnon-finite clauses, but also parentheticals and apposi tions, elements of bulleted lists, etc. the following example illustrates both the use of unit?
</nextsent>
<nextsent>elements and of the elements ne?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2276">
<title id=" W04-2327.xml">the mate gnome proposals for anaphoric annotation revisited </title>
<section> work based on the mate proposals.  </section>
<citcontext>
<prevsection>
<prevsent>instantiations of the mate meta-scheme.
</prevsent>
<prevsent>one of our aims was to continue the work on bridging references annotation and interpretation in (poesio and vieira, 1998), <papid> J98-2001 </papid>which showed that marking up bridging references is quite hard.</prevsent>
</prevsection>
<citsent citstr=" J99-3001 ">
in addition, work such as (sidner, 1979; strube and hahn, 1999) <papid> J99-3001 </papid>suggested that indirect realization can play crucial role in maintaining the cb.</citsent>
<aftsection>
<nextsent>after testing few types of associative reference(hawkins, 1978), we decided to annotate only three non identity relations, as well as identity.
</nextsent>
<nextsent>these relations area subset of those proposed in the extended relations?
</nextsent>
<nextsent>version of the mate scheme: set membership (element), subset (subset), and generalized possession?
</nextsent>
<nextsent>(poss),which includes both part-of relations and ownership relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2277">
<title id=" W04-0847.xml">optimizing feature set for chinese word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>corpus based method is one of the successful lines of research on wsd.
</prevsent>
<prevsent>many supervised learning algorithms have been applied for wsd,ex.
</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
bayesian learning (leacock et al, 1998), <papid> J98-1006 </papid>exemplar based learning (ng and lee, 1996), <papid> P96-1006 </papid>decision list (yarowsky, 2000), neural network (towel and voorheest, 1998), maximum entropy method (dang et al, 2002), <papid> C02-1143 </papid>etc..</citsent>
<aftsection>
<nextsent>in this paper, we employ naive bayes classifier to perform wsd.
</nextsent>
<nextsent>resolving the ambiguity of words usually relies on the contexts of their occurrences.
</nextsent>
<nextsent>the feature set used for context representation consists of local and topical features.
</nextsent>
<nextsent>local features include partof speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and target word, etc..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2278">
<title id=" W04-0847.xml">optimizing feature set for chinese word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>corpus based method is one of the successful lines of research on wsd.
</prevsent>
<prevsent>many supervised learning algorithms have been applied for wsd,ex.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
bayesian learning (leacock et al, 1998), <papid> J98-1006 </papid>exemplar based learning (ng and lee, 1996), <papid> P96-1006 </papid>decision list (yarowsky, 2000), neural network (towel and voorheest, 1998), maximum entropy method (dang et al, 2002), <papid> C02-1143 </papid>etc..</citsent>
<aftsection>
<nextsent>in this paper, we employ naive bayes classifier to perform wsd.
</nextsent>
<nextsent>resolving the ambiguity of words usually relies on the contexts of their occurrences.
</nextsent>
<nextsent>the feature set used for context representation consists of local and topical features.
</nextsent>
<nextsent>local features include partof speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and target word, etc..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2279">
<title id=" W04-0847.xml">optimizing feature set for chinese word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>corpus based method is one of the successful lines of research on wsd.
</prevsent>
<prevsent>many supervised learning algorithms have been applied for wsd,ex.
</prevsent>
</prevsection>
<citsent citstr=" C02-1143 ">
bayesian learning (leacock et al, 1998), <papid> J98-1006 </papid>exemplar based learning (ng and lee, 1996), <papid> P96-1006 </papid>decision list (yarowsky, 2000), neural network (towel and voorheest, 1998), maximum entropy method (dang et al, 2002), <papid> C02-1143 </papid>etc..</citsent>
<aftsection>
<nextsent>in this paper, we employ naive bayes classifier to perform wsd.
</nextsent>
<nextsent>resolving the ambiguity of words usually relies on the contexts of their occurrences.
</nextsent>
<nextsent>the feature set used for context representation consists of local and topical features.
</nextsent>
<nextsent>local features include partof speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and target word, etc..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2280">
<title id=" W03-2102.xml">annotating opinions in the world press </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>private state is general term that covers mental and emotional states, which cannot be directly observed or verified (quirk et al, 1985).
</prevsent>
<prevsent>for example, we can observe evidence of someone else being happy, but we cannot directly observe their happiness.
</prevsent>
</prevsection>
<citsent citstr=" J94-2004 ">
in natural language, opinions, emotions and other private states are expressed using subjective language (banfield, 1982; wiebe, 1994).<papid> J94-2004 </papid>articles in the news are composed of mixture of factual and subjective material.</citsent>
<aftsection>
<nextsent>writers of editorials frequently include facts to support their arguments, and news reports often mix segments presenting objective facts with segments presenting opinions and verbal reactions (vandijk, 1988).
</nextsent>
<nextsent>however, natural language processing applications that retrieve or extract information from or that summarize or answer questions about news and other discourse have focused primarily on factual information and thus could benefit from knowledge of subjective language.
</nextsent>
<nextsent>traditional information extraction and information retrieval systems could learn to concentrate on objectively presented factual information.
</nextsent>
<nextsent>question answering systems could identify when an answer is speculative rather than certain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2281">
<title id=" W03-2102.xml">annotating opinions in the world press </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the ability to extract opinions when they appear in documents would benefit multi-document summarization systems seeking to summarize different opinions and perspectives, as well as multi perspective question-answering systems trying to answer opinion-based questions.
</prevsent>
<prevsent>the annotation scheme we present in this paper was developed as part of u.s. government sponsored project (arda aquaint nrrc)1 to investigate multiple perspectives in question answering (wiebe et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
we implemented the scheme in gate2, general architecture for text engineering (cunningham et al, 2002).<papid> P02-1022 </papid></citsent>
<aftsection>
<nextsent>general instructions for annotating opinions and specific instructions for downloading and using gate to perform the annotations are available at 1this work was performed in support of the northeast regional research center (nrrc) which is sponsored bythe advanced research and development activity in information technology (arda), u.s. government entity which sponsors and promotes research of import to the intelligence community which includes but is not limited to the cia, dia, nsa, nima, and nro.
</nextsent>
<nextsent>2gate is freely available from the university of sheffield at http://gate.ac.uk.
</nextsent>
<nextsent>http://www.cs.pitt.edu/ wiebe/pubs/ardasummer02.the annotated data will be available to u.s. government contractors this summer.
</nextsent>
<nextsent>we are working to resolve copyright issues to make it available to the wider research community.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2283">
<title id=" W03-2102.xml">annotating opinions in the world press </title>
<section> previous work on subjectivity.  </section>
<citcontext>
<prevsection>
<prevsent>is cao, while the nested source of claim?
</prevsent>
<prevsent>is (writer, cao, shouters).33(10) is an example of de re rather than de dicto propositional attitude report (rapaport, 1986).
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
tagging in previous work (wiebe et al, 1999), corpus of sentences from the wall street journal treebank corpus (marcus et al, 1993) <papid> J93-2004 </papid>was manually annotated with subjectivity classifications by multiple judges.</citsent>
<aftsection>
<nextsent>the judges were instructed to classify asentence as subjective if it contained any significant expressions of subjectivity, attributed to either the writer or someone mentioned in the text,and to classify the sentence as objective, otherwise.
</nextsent>
<nextsent>the judges rated the certainty of their answers on scale from 0 to 3.
</nextsent>
<nextsent>agreement in the study was summarized in terms of cohens kappa (   ) (cohen, 1960),which compares the total probability of agreement to that expected if the taggers?
</nextsent>
<nextsent>classifications were statistically independent (i.e., chance agreement?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2284">
<title id=" W03-2122.xml">a method for forming mutual beliefs for communication through human robot multimodal interaction </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>(matsui et al, 2000) focused on enabling robots to work in the real world, and integrated language with information from robots sensors by using pattern recognition.
</prevsent>
<prevsent>(inamura et al, 2000) investigated an autonomous mobile robot that controlled its actions and conversations with user based on bayesiannetwork.
</prevsent>
</prevsection>
<citsent citstr=" W02-0217 ">
the use of bayesian networks in the interpretation and generation of dialogue was also investigated by (lemon et al, 2002).<papid> W02-0217 </papid></citsent>
<aftsection>
<nextsent>in (singh etal., 2000), the learning of dialogue strategies using reinforcement learning was investigated.
</nextsent>
<nextsent>someof these works looked at beliefs held by?
</nextsent>
<nextsent>the machines themselves, but none focused on the formation of mutual beliefs between humans and machines through interaction, based on common experiences.
</nextsent>
<nextsent>the presented method enables the formation of mutual beliefs between people and robots through interaction in physical environments, and it facilitates the process of human-machine communication.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2285">
<title id=" W04-2210.xml">a generic collaborative platform for multilingual lexical database development </title>
<section> unified access to existing.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, all results will be displayed in form that fits its own structure.
</prevsent>
<prevsent>any monolingual, bilingual or multilingual dictionary may be added in this collection, provided that it is available in xml format.
</prevsent>
</prevsection>
<citsent citstr=" W04-2209 ">
with the papillon platform, giving access toa new, unknown, dictionary is matter of writing 2 xml files: dictionary description and an dictionary languages nb of entries armamenta fra eng 1116 cedictb zho eng 215424 dingc deu eng 124413 engdictd eng kor 214127 feme fra eng msa 19247 homericaf fra 441 jmdictg jp en fr de 96264 kanjidicth jpn eng 6355 papillon multi 1323 thaidicti tha 10295 vietdictj fra vie 41029 wadokujitenk jpn deu 214274 ajapanese french dictionary of armament from the french embassy in japan bchinese english from mandel shi (xiamen univ.) c(richter, 1999) d(paik and bond, 2003) e(gut et al, 1996) funiversity stendhal, grenoble iii g(breen, 2004<papid> W04-2209 </papid>a) h(breen, 2004<papid> W04-2209 </papid>b) ithai dictionary of kasetsart university j(duc, 1998) k(apel, 2004)table 1: dictionaries available through the unified access interfacexsl stylesheet.</citsent>
<aftsection>
<nextsent>for currently available dictionaries, this took an average of about one hour per dictionary.
</nextsent>
<nextsent>3.3 implementation.
</nextsent>
<nextsent>it is possible to give access to any xml dictionary, regardless of its structure.
</nextsent>
<nextsent>for this, you have to identify minimum set of information in the dictionarys xml structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2293">
<title id=" W04-0815.xml">dependency based logical form transformations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the efficiencies and deficiencies of our approach are discussed as well as considerations for further enhanc ments.
</prevsent>
<prevsent>in addition to the well-known all words and lexical sample tasks deployed in previous senseval workshops number of new tasks have been included in this sense evaluation.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
these new tasks include identification of semantic roles as in framenet (gildea and jurafsky 2002), <papid> J02-3001 </papid>disambiguation of wordnet glosses (miller 1990; fellbaum 1998; harabagiu, miller et al 1999), automatic acquisitio of subcategorisation frames (korhonen 2002; preiss and korhonen 2002), <papid> W02-0815 </papid>and logical form identific tion (lfi) (rus 2002; rus and moldovan 2002).</citsent>
<aftsection>
<nextsent>this paper discusses solution developed for the lfi task.
</nextsent>
<nextsent>the approach used here employs functional dependency parser (jrvinen and tapanainen 1997; tapanainen and jrvinen 1997) <papid> A97-1011 </papid>and uses limited number of additional resources.</nextsent>
<nextsent>this contribution is intended to demonstrate the suitability of dependency parser to the given task and also explain some of the limitations and challenges involved when using such an approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2294">
<title id=" W04-0815.xml">dependency based logical form transformations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the efficiencies and deficiencies of our approach are discussed as well as considerations for further enhanc ments.
</prevsent>
<prevsent>in addition to the well-known all words and lexical sample tasks deployed in previous senseval workshops number of new tasks have been included in this sense evaluation.
</prevsent>
</prevsection>
<citsent citstr=" W02-0815 ">
these new tasks include identification of semantic roles as in framenet (gildea and jurafsky 2002), <papid> J02-3001 </papid>disambiguation of wordnet glosses (miller 1990; fellbaum 1998; harabagiu, miller et al 1999), automatic acquisitio of subcategorisation frames (korhonen 2002; preiss and korhonen 2002), <papid> W02-0815 </papid>and logical form identific tion (lfi) (rus 2002; rus and moldovan 2002).</citsent>
<aftsection>
<nextsent>this paper discusses solution developed for the lfi task.
</nextsent>
<nextsent>the approach used here employs functional dependency parser (jrvinen and tapanainen 1997; tapanainen and jrvinen 1997) <papid> A97-1011 </papid>and uses limited number of additional resources.</nextsent>
<nextsent>this contribution is intended to demonstrate the suitability of dependency parser to the given task and also explain some of the limitations and challenges involved when using such an approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2295">
<title id=" W04-0815.xml">dependency based logical form transformations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these new tasks include identification of semantic roles as in framenet (gildea and jurafsky 2002), <papid> J02-3001 </papid>disambiguation of wordnet glosses (miller 1990; fellbaum 1998; harabagiu, miller et al 1999), automatic acquisitio of subcategorisation frames (korhonen 2002; preiss and korhonen 2002), <papid> W02-0815 </papid>and logical form identific tion (lfi) (rus 2002; rus and moldovan 2002).</prevsent>
<prevsent>this paper discusses solution developed for the lfi task.</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
the approach used here employs functional dependency parser (jrvinen and tapanainen 1997; tapanainen and jrvinen 1997) <papid> A97-1011 </papid>and uses limited number of additional resources.</citsent>
<aftsection>
<nextsent>this contribution is intended to demonstrate the suitability of dependency parser to the given task and also explain some of the limitations and challenges involved when using such an approach.
</nextsent>
<nextsent>1.1 motivation.
</nextsent>
<nextsent>part of the initial step towards the interpretation of sentence as postulated by hobbs et al (1993) involves the proof of the logical form of sentence.
</nextsent>
<nextsent>this statement entails the transformation of sentence into logical form as fundamental building block towards sentence interpretation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2296">
<title id=" W04-0815.xml">dependency based logical form transformations </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>another common source of error arose from poor handling of nominal group complexes.
</prevsent>
<prevsent>with regard to predicate performance, the decision to forfeit the use of the available multi-word item list proved costly.
</prevsent>
</prevsection>
<citsent citstr=" W99-0501 ">
harabagiu et al (1999) <papid> W99-0501 </papid>proposed scheme for attaching sense tags to predicates within the framework of transforming wordnet glosses into logical form.</citsent>
<aftsection>
<nextsent>in this way conceptual predicates may be formed to manipulate meaning representation in more significant ways.
</nextsent>
<nextsent>naturally the sense inventory must be sensitive enough to allow for meaningful and representative mutation to be applied to the meaning representation.
</nextsent>
<nextsent>dependency grammars provide natural and intuitive solution to the task of logical form identific tion.
</nextsent>
<nextsent>we have managed to demonstrate relatively good overall performance on the given task with minimal additional processing and very small amount of training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2297">
<title id=" W04-0831.xml">finding optimal parameter settings for high performance word sense disambiguation </title>
<section> introduction. rlsc algorithm, input.  </section>
<citcontext>
<prevsection>
<prevsent>and output.
</prevsent>
<prevsent>this paper is not self-contained.
</prevsent>
</prevsection>
<citsent citstr=" W04-0851 ">
the reader should read first the paper of marius popescu (popescu, 2004), <papid> W04-0851 </papid>paper that contains the full description thebase algorithm, regularized least square classification (rlsc) applied to wsd.</citsent>
<aftsection>
<nextsent>our systems used the feature extraction described in (popescu, 2004), <papid> W04-0851 </papid>with some differences.</nextsent>
<nextsent>let us fix word that is on the list of words we must be able to disambiguate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2301">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper two different approaches to the resolution of lexical ambiguities are applied to multi-domain corpus of speech recognition output produced from spontaneous utterances in spoken dialogue system.
</prevsent>
<prevsent>the resulting evaluations show that all approaches yield significant gains over the majority class baseline performance of .68, i.e. fmeasures of .79 for the knowledge-driven approach and .86 for the supervised learning approach.
</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
following ide and veronis (1998) <papid> J98-1001 </papid>we can distinguish between data- and knowledge-driven word sense disambiguation (wsd).</citsent>
<aftsection>
<nextsent>given the basic distinction between written text and spoken utterances, we follow allen et al  (2001) and differentiate further between controlled and conversational spoken dialogue systems.
</nextsent>
<nextsent>neither data- nor knowledge-driven word sense disambiguation has been performed on speech data stemming from human interactions with dialogue systems, since multi domain conversational spoken dialogue systems for human computer interaction (hci) have not existed in the past.
</nextsent>
<nextsent>now that speech data from multi-domain systems have become available, corresponding experiments and evaluations have become feasible.
</nextsent>
<nextsent>in this paper we present the results of first word sense disambiguation annotation experiments on data from spoken interactions with multi-domain dialoguesystems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2303">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we present the results of first word sense disambiguation annotation experiments on data from spoken interactions with multi-domain dialoguesystems.
</prevsent>
<prevsent>additionally, we describe the results of corresponding evaluation of data- and knowledge-driven word sense disambiguation system on that data.
</prevsent>
</prevsection>
<citsent citstr=" W03-0903 ">
for knowledge-driven disambiguation we examined whether the ontology-based method for computing semantic coherence introduced by gurevych et al  (2003<papid> W03-0903 </papid>a) can be employed to disambiguate between alternative interpretations, i.e. concept representations, of given speech recognition hypothesis (srh) at hand.</citsent>
<aftsection>
<nextsent>we will showthe results of its evaluation in the semantic interpretation task of wsd.
</nextsent>
<nextsent>for example, in speech recognition hypotheses containing forms of the german verb kom men, i.e.
</nextsent>
<nextsent>(to) come, decision had to be made whether its meaning corresponds to the motion sense or to the showing sense, i.e. becoming mapped onto either motiondirectedtransliteratedprocess or watchperceptualprocess in the terminology ofour spoken language understanding system.
</nextsent>
<nextsent>for data driven approach we employed highly supervised learning algorithm introduced by brants (2000) <papid> A00-1031 </papid>and train edit on corpus of annotated data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2313">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, in speech recognition hypotheses containing forms of the german verb kom men, i.e.
</prevsent>
<prevsent>(to) come, decision had to be made whether its meaning corresponds to the motion sense or to the showing sense, i.e. becoming mapped onto either motiondirectedtransliteratedprocess or watchperceptualprocess in the terminology ofour spoken language understanding system.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
for data driven approach we employed highly supervised learning algorithm introduced by brants (2000) <papid> A00-1031 </papid>and train edit on corpus of annotated data.</citsent>
<aftsection>
<nextsent>a second set of semantically annotated speech recognition hypotheses was employed as gold-standard for evaluating both the ontology-based and supervised learning method.
</nextsent>
<nextsent>both datasets were annotated by separate human annotators.all annotated data stems from log files of an automatic speech recognition system that was implemented in the smartkom system (wahlster et al , 2001; wahlster, 2003).
</nextsent>
<nextsent>it is important to point out that there are at least two essential differences between spontaneous speech wsd and textual wsd, i.e.,  smaller size of proces sable context as well as  imperfections, hesitations, disfluencies and speech recognition errors.
</nextsent>
<nextsent>existing spoken language understanding systems,that are not shallow and thusly produce deep syntactic and semantic representations for multiple domains, e.g. the production system approach described by engel (2002) or unification-based approaches described by crysmann et al  (2002), <papid> P02-1056 </papid>have shown to be more suitable for well-formed input but less robust in case of im perfect input.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2314">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>both datasets were annotated by separate human annotators.all annotated data stems from log files of an automatic speech recognition system that was implemented in the smartkom system (wahlster et al , 2001; wahlster, 2003).
</prevsent>
<prevsent>it is important to point out that there are at least two essential differences between spontaneous speech wsd and textual wsd, i.e.,  smaller size of proces sable context as well as  imperfections, hesitations, disfluencies and speech recognition errors.
</prevsent>
</prevsection>
<citsent citstr=" P02-1056 ">
existing spoken language understanding systems,that are not shallow and thusly produce deep syntactic and semantic representations for multiple domains, e.g. the production system approach described by engel (2002) or unification-based approaches described by crysmann et al  (2002), <papid> P02-1056 </papid>have shown to be more suitable for well-formed input but less robust in case of im perfect input.</citsent>
<aftsection>
<nextsent>for conversational and reliable dialogue systems that achieve satisfactory scores in evaluation frameworks such as proposed by walker et al  (2000) or beringer et al  (2002) for multi-modal dialogue systems, we need robust knowledge- or data-driven methods for disambiguating the sometimes less than ideal output of the large vocabulary spontaneous speech recognizers.
</nextsent>
<nextsent>inthe long run, we would also like to avoid expensive preprocessing work, which is necessary for both ontology driven and supervised learning methods, i.e. labor intensive ontology engineering and data annotation respectively.
</nextsent>
<nextsent>after work on wsd had overcome so-called early doubts (ide and veronis, 1998) <papid> J98-1001 </papid>in the 1960s, it was applied to various nlp tasks, such as machine translation, information retrieval, content and grammatical analysis and text processing.</nextsent>
<nextsent>yarowsky (1995) <papid> P95-1026 </papid>used both supervised and unsupervised wsd for correct phonetizitation of words in speech synthesis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2316">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>inthe long run, we would also like to avoid expensive preprocessing work, which is necessary for both ontology driven and supervised learning methods, i.e. labor intensive ontology engineering and data annotation respectively.
</prevsent>
<prevsent>after work on wsd had overcome so-called early doubts (ide and veronis, 1998) <papid> J98-1001 </papid>in the 1960s, it was applied to various nlp tasks, such as machine translation, information retrieval, content and grammatical analysis and text processing.</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
yarowsky (1995) <papid> P95-1026 </papid>used both supervised and unsupervised wsd for correct phonetizitation of words in speech synthesis.</citsent>
<aftsection>
<nextsent>however, there is no recorded work on processing speech recognition hypotheses resulting from speech utterances as it is done in our research.
</nextsent>
<nextsent>in general, following ide and veronis (1998) <papid> J98-1001 </papid>the various wsd approaches of the past can be divided into two types, i.e., data- and knowledge-based approaches.</nextsent>
<nextsent>2.1 data-based methods.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2320">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 data-based methods.
</prevsent>
<prevsent>data-based approaches extract their information directly from texts and are divided into supervised and unsupervised methods (yarowsky, 1995; <papid> P95-1026 </papid>stevenson, 2003).</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
supervised methods work with given (and therefore limited) set of potential classes in the learning process.for example, yarowsky (1992) <papid> C92-2070 </papid>used thesaurus to generate 1042 statistical models of the most general categories.</citsent>
<aftsection>
<nextsent>weiss (1973) already showed that disambiguation rules can successfully be learned from hand-tagged corpora.
</nextsent>
<nextsent>despite the small size of his training and test corpus, an accuracy of 90   was achieved.
</nextsent>
<nextsent>even better results on larger corpus were obtained by kelly and stone 1975 who included collocational, syntactic and part of speech information to yield an accuracy of 93   on larger corpus.
</nextsent>
<nextsent>as always, supervised methods require manually annotated learning corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2321">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>even better results on larger corpus were obtained by kelly and stone 1975 who included collocational, syntactic and part of speech information to yield an accuracy of 93   on larger corpus.
</prevsent>
<prevsent>as always, supervised methods require manually annotated learning corpus.
</prevsent>
</prevsection>
<citsent citstr=" N03-4011 ">
unsupervised methods do not determine the set of classes before the learning process, but through analysis of the given data by identifying clusters of similar cases.one example is the algorithm for clustering by committee described by pantel and lin (2003), <papid> N03-4011 </papid>which automatically discovers word senses from text.</citsent>
<aftsection>
<nextsent>generally, unsupervised methods require large amounts of data.
</nextsent>
<nextsent>in the case of spoken dialogue and speech recognition output sufficient amounts of data will hopefully become available once multi-domain spoken dialogue systems are deployed in real world applications.
</nextsent>
<nextsent>2.2 knowledge-based methods.
</nextsent>
<nextsent>knowledge-based approaches work with lexica and/or ontologies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2326">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>knowledge-based approaches work with lexica and/or ontologies.
</prevsent>
<prevsent>the kind of knowledge varies widely andmachine-readable as well as computer lexica are employed.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the knowledge-based approach employed herein (gurevych et al , 2003<papid> W03-0903 </papid>a) operates on an ontology partially derived from framenet data (baker et al , 1998) <papid> P98-1013 </papid>and described by gurevych et al  (2003<papid> W03-0903 </papid>b).</citsent>
<aftsection>
<nextsent>incomparable approach sussna (1993) worked with the lexical reference system wordnet and used similar metric for the calculation of semantic distance of number of input lexemes.
</nextsent>
<nextsent>depending on the type of semantic relation (hyperonymy, synonymy etc.) different weights are given and his metric takes account of the number of arcs of the same type leaving node and the depth of given edge in the overall tree.
</nextsent>
<nextsent>the disambiguation results on textual data reported by sussna (1993) turned out to be significantly better than chance.
</nextsent>
<nextsent>in contrast to many other work on wsd with wordnet he took into account not only the isa hierarchy, but other relational links as well.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2341">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> data and annotation experiment.  </section>
<citcontext>
<prevsection>
<prevsent>in this section we describe the data collection and annotation experiments performed in order to obtain independent datasets for training and evaluation.
</prevsent>
<prevsent>3.1 data collection.
</prevsent>
</prevsection>
<citsent citstr=" W02-0207 ">
the first dataset was used for training the supervised model is described in gurevych et al  (2002<papid> W02-0207 </papid>b) and was collected using the so-called hidden operator test (rappand strube, 2002).</citsent>
<aftsection>
<nextsent>this procedure represents simplification of classical end-to-end experiments and wizardof-oz experiments (francony et al , 1992) - as it is con ductible without the technically very complex use of areal or seemingly real conversational system.
</nextsent>
<nextsent>the subjects are prompted to ask for specific information and the system response is pre-manufactured.
</nextsent>
<nextsent>we had 29 subjects prompted to say certain inputs in 8 dialogues.
</nextsent>
<nextsent>1479 turns were recorded.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2345">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> data and annotation experiment.  </section>
<citcontext>
<prevsection>
<prevsent>for that purpose, the annotators reached an agreement on annotated items of the test data which had differed in the first place.
</prevsent>
<prevsent>the resulting gold-standard represents the highest degree of correctly disambiguated data and is used for comparison with the tagged data produced by the disambiguation systems. thirdly, for the supervised learning another correctly disambiguated dataset is needed for training the statistical model.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
2the acronym stands for visualization tool for annotation and evaluation.the class-based kappa statistic of (cohen, 1960; carletta, 1996) <papid> J96-2004 </papid>cannot be applied here, as the classes vary depending on the number of ambiguities per entry in the lexicon.</citsent>
<aftsection>
<nextsent>also an additional class, i.e., not-decidablewas allowed for cases as in srh (1c), where it is impossible to assign sensible meanings.
</nextsent>
<nextsent>the test dataset al gether was annotated with 2219 markables of ambiguous tokens, stemming from 70 ambiguous words occurring in the test corpus.
</nextsent>
<nextsent>3.4 calculating the baselines.
</nextsent>
<nextsent>for calculating the majority class baseline, which in our case corresponds to the performance of unigram tagger, we applied the method described in (porzel and malaka, 2004).<papid> W04-2802 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2346">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> data and annotation experiment.  </section>
<citcontext>
<prevsection>
<prevsent>the test dataset al gether was annotated with 2219 markables of ambiguous tokens, stemming from 70 ambiguous words occurring in the test corpus.
</prevsent>
<prevsent>3.4 calculating the baselines.
</prevsent>
</prevsection>
<citsent citstr=" W04-2802 ">
for calculating the majority class baseline, which in our case corresponds to the performance of unigram tagger, we applied the method described in (porzel and malaka, 2004).<papid> W04-2802 </papid></citsent>
<aftsection>
<nextsent>therefore, all markables in the gold-standard were counted and, corresponding to the frequency of each concept of each ambiguous lexeme, the percentage of correctly chosen concepts by means of selecting the most frequent meaning was calculated.
</nextsent>
<nextsent>this resulted in base line of 52.48   for the test dataset.
</nextsent>
<nextsent>both word sense disambiguation systems described herein were tested and developed with the smartkom research framework.
</nextsent>
<nextsent>as one of the most advanced current systems, the smartkom (wahlster, 2003) comprises large set of input and output modalities together with an efficient fusion and fission pipeline.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2347">
<title id=" W04-2312.xml">resolution of lexical ambiguities in spoken dialogue system </title>
<section> word sense disambiguation systems.  </section>
<citcontext>
<prevsection>
<prevsent>smartkom features speech input with prosodic analysis, gesture input via infrared camera, recognition of facial expressions and their emotional states.
</prevsent>
<prevsent>on the output side, the system features gesturing and speaking life-like character together with displayed generated text and multimedia graphical output.
</prevsent>
</prevsection>
<citsent citstr=" W03-0811 ">
it currently comprises nearly 50 modules running on parallel virtual machine-based integration software called multiplatform3 described in herzog et al  (2003).<papid> W03-0811 </papid></citsent>
<aftsection>
<nextsent>4.1 the knowledge-driven system.
</nextsent>
<nextsent>the ontology employed for the evaluation has about800 concepts and 200 relations (apart from the isa relations defining the general taxonomy) and is described by gurevych et al  (2003<papid> W03-0903 </papid>b).</nextsent>
<nextsent>it includes generic top level ontology whose purpose is to provide basic structure of the world, i.e. abstract classes to divide the universe indistinct parts as resulting from the ontologicalanalysis.4 the modeling of processes and physical objects as kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the framenet data (baker et al , 1998).<papid> P98-1013 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2381">
<title id=" W04-1806.xml">automatically inducing ontologies from corpora </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>one would expect that income tax?
</prevsent>
<prevsent>is much more characteristic of theirs publication, and this is borne out by the document frequencies in the table.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
we use the log likelihood ratio (llr) (dunning 1993) <papid> J93-1003 </papid>given by -2log2(ho(p;k1,n1,k2,n2)/ha(p1,p2;n1,k1,n2,k2)) llr measures the extent to which hypothesized model of the distribution of cell counts, ha, differs from the null hypothesis, ho (namely, that the percentage of documents containing this term is the same in both corpora).</citsent>
<aftsection>
<nextsent>we used binomial model for ho and ha8.
</nextsent>
<nextsent>2.3 relationship discovery.
</nextsent>
<nextsent>the main innovation in our approach is to fuse together information from multiple knowledge 7 in publication 17, each chapter?
</nextsent>
<nextsent>is document..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2382">
<title id=" W04-1806.xml">automatically inducing ontologies from corpora </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>computerm 2004 - 3rd international workshop on computational terminology 49 2.3.4 explicit patterns relations this knowledge source infers specific relations between terms based on characteristic cue-phrases which relate them.
</prevsent>
<prevsent>for example, the cue-phrase such as?
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
(hearst 1992) (<papid> C92-2082 </papid>caraballo 1999) <papid> P99-1016 </papid>suggest kind-of relation, e.g., ligand such as triethylphosphine?</citsent>
<aftsection>
<nextsent>tells us that triethylphosphene?
</nextsent>
<nextsent>is kind of ligand?.
</nextsent>
<nextsent>likewise, in the trec domain, air toxics such as benzene?
</nextsent>
<nextsent>can suggest that benzene?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2383">
<title id=" W04-1806.xml">automatically inducing ontologies from corpora </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>computerm 2004 - 3rd international workshop on computational terminology 49 2.3.4 explicit patterns relations this knowledge source infers specific relations between terms based on characteristic cue-phrases which relate them.
</prevsent>
<prevsent>for example, the cue-phrase such as?
</prevsent>
</prevsection>
<citsent citstr=" P99-1016 ">
(hearst 1992) (<papid> C92-2082 </papid>caraballo 1999) <papid> P99-1016 </papid>suggest kind-of relation, e.g., ligand such as triethylphosphine?</citsent>
<aftsection>
<nextsent>tells us that triethylphosphene?
</nextsent>
<nextsent>is kind of ligand?.
</nextsent>
<nextsent>likewise, in the trec domain, air toxics such as benzene?
</nextsent>
<nextsent>can suggest that benzene?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2385">
<title id=" W04-1806.xml">automatically inducing ontologies from corpora </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 automatic evaluation.
</prevsent>
<prevsent>while evaluation by humans is valuable, it is expensive to carry out, and this expense must be incurred each time one wants to do an evaluation.
</prevsent>
</prevsection>
<citsent citstr=" W99-0510 ">
automatic comparison of machine-generated ontology against reference ontologies constructed by humans, e.g., (zhang et al 1996) (sekine et al 1999) (<papid> W99-0510 </papid>daude et al 2001), is therefore desirable, provided suitable reference ontologies are available.</citsent>
<aftsection>
<nextsent>in this evaluation, the human-generated taxonomy for promed described in section 3.2.1 was used as reference ontology, with its unlabeled parent-child relation treated as kind-of link.
</nextsent>
<nextsent>however, the human ontology?
</nextsent>
<nextsent>was created without looking at corpus, and was developed for use with different set of goals in mind.
</nextsent>
<nextsent>although this involves comparing apples?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2389">
<title id=" W03-2902.xml">a largescale inheritance based morphological lexicon for russian </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J96-2002 ">
in this paper we describe the mapping of zaliznjaks (1977) morphological classes into the lexical representation language datr (evans and gazdar 1996).<papid> J96-2002 </papid></citsent>
<aftsection>
<nextsent>on the basis of the resulting datr theory set of fully inflected forms together with their associated morpho syntax can automatically be generated from the electronic version of zaliznjaks dictionary (ilola and mustajoki 1989).
</nextsent>
<nextsent>from this data we plan to develop wide-coverage morphosyntactic lemmatizer and tagger for russian.
</nextsent>
<nextsent>our goal is to undertake detailed corpus analysis of russian texts, focusing on the relationship between morphological ambiguity (syncretism) in nouns and adjectives and the comparative frequency of the relevant grammatical categories.
</nextsent>
<nextsent>for this purpose, we will use two corpora, the uppsala corpus (lnngren 1993, maier 1994) and corpus of russian newspaper texts from the late 1990s, for which we require detailed morphosyntactic annotation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2390">
<title id=" W04-2328.xml">multilevel dialogue act tags </title>
<section> understanding dialogue structure:.  </section>
<citcontext>
<prevsection>
<prevsent>there are about 4 million possible combinations of damsl tags, which make huge search space for automatic annotation.
</prevsent>
<prevsent>the application of damsl to the switchboard data (two-party telephone conversations) lead to swbd-damsl (jurafsky et al , 1997), smallertagset than damsl.
</prevsent>
</prevsection>
<citsent citstr=" W98-0319 ">
about 200,000 swbd utterances were first annotated with damsl tags: it was observed that only 220 combinations of tags occurred (jurafsky et al , 1998).<papid> W98-0319 </papid></citsent>
<aftsection>
<nextsent>these 220 labels were then clustered into 42 tags, such as: statement (36%), opinion (13%), agree/accept (5%), yes-no-question(2%).
</nextsent>
<nextsent>the resulting search space (42 mutually exclusive tags) was well adapted to the initial goals, viz., the automatic annotation of dialogue acts and the use of dialogue act specific language models in speech recognition (stolcke et al , 2000).<papid> J00-3003 </papid></nextsent>
<nextsent>2.3 requirements for the definition of a. da tagset in this paper, our goal is to design new da tagset for our application, with the following constraints in mind (see also the analysis by d. traum (2000)):?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2391">
<title id=" W04-2328.xml">multilevel dialogue act tags </title>
<section> understanding dialogue structure:.  </section>
<citcontext>
<prevsection>
<prevsent>about 200,000 swbd utterances were first annotated with damsl tags: it was observed that only 220 combinations of tags occurred (jurafsky et al , 1998).<papid> W98-0319 </papid></prevsent>
<prevsent>these 220 labels were then clustered into 42 tags, such as: statement (36%), opinion (13%), agree/accept (5%), yes-no-question(2%).</prevsent>
</prevsection>
<citsent citstr=" J00-3003 ">
the resulting search space (42 mutually exclusive tags) was well adapted to the initial goals, viz., the automatic annotation of dialogue acts and the use of dialogue act specific language models in speech recognition (stolcke et al , 2000).<papid> J00-3003 </papid></citsent>
<aftsection>
<nextsent>2.3 requirements for the definition of a. da tagset in this paper, our goal is to design new da tagset for our application, with the following constraints in mind (see also the analysis by d. traum (2000)):?
</nextsent>
<nextsent>relation to one or more existing theories (de scrip tive, explanatory, etc.).?
</nextsent>
<nextsent>compatibility with the observed functions of actual utterances in context, in given domain.?
</nextsent>
<nextsent>empirical validation: reliability of human application of the tagset to typical data (high inter annotator agreement, at least potentially).?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2393">
<title id=" W04-2328.xml">multilevel dialogue act tags </title>
<section> available data and annotations:.  </section>
<citcontext>
<prevsection>
<prevsent>in meetings (wrede and shriberg, 2003).
</prevsent>
<prevsent>icsi meeting recorder the volume of available annotated data suffers from the diversity of da tagsets (klein and soria, 1998).one of the most significant resources is the switchboard corpus mentioned above, but telephone conversations have many differences with multi-party meetings.
</prevsent>
</prevsection>
<citsent citstr=" W04-2319 ">
apart from the data recently available in the im2 project, results reported in this paper make use of the icsi meeting recording (mr) corpus of transcribed and annotated dialogues (morgan et al , 2003; shriberg et al , 2004)<papid> W04-2319 </papid>3.</citsent>
<aftsection>
<nextsent>3.1 overview of icsi mr corpus.
</nextsent>
<nextsent>the icsi-mr corpus consists of 75 one-hour recordings of staff meetings, each involving up to eight speakers on separate mike channels.
</nextsent>
<nextsent>each channel was manually transcribed and timed, then annotated with dialogue act and adjacency pair information(shriberg et al , 2004)<papid> W04-2319 </papid>.</nextsent>
<nextsent>following preliminary release in november 2003 (sound files, transcriptions, and annotations), the full corpus was released in february 2004 to im2 partners.the dialogue act annotation makes use of the preexisting segmentation of each channel into (prosodic)utterances, sometimes segmented further into functional utterances, each of them bearing separate dialogue act.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2396">
<title id=" W04-2328.xml">multilevel dialogue act tags </title>
<section> automatic classification.  </section>
<citcontext>
<prevsection>
<prevsent>the data was provided with individual time stamps for each word using speech recognizer in forced recognition mode: where there were errors or mismatches we discarded the words.
</prevsent>
<prevsent>5.2 results.
</prevsent>
</prevsection>
<citsent citstr=" N03-5008 ">
we use maximum entropy (me) classifier (manning and klein, 2003) <papid> N03-5008 </papid>which allows an efficient combination of many overlapping features.</citsent>
<aftsection>
<nextsent>we selected 5 meetings (6771 utterances after splitting) to use asour test set and 40 as our training set leaving further five for possible later experiments.
</nextsent>
<nextsent>as simple baseline we use the classifier which just guesses themost likely class.
</nextsent>
<nextsent>we first performed some experiments on the original tag sets to see how predictable they are.we started by defining simple six-way classification task which classifies disruption forms, and undecipherable forms as well as the four general tags defined above.
</nextsent>
<nextsent>this is an empirically very well-founded distinction: the icsi-mr group have provided some inter-annotator agreement figures(carletta et al , 1997) <papid> J97-1002 </papid>for very similar task and report kappa of 0.79.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2397">
<title id=" W04-2328.xml">multilevel dialogue act tags </title>
<section> automatic classification.  </section>
<citcontext>
<prevsection>
<prevsent>as simple baseline we use the classifier which just guesses themost likely class.
</prevsent>
<prevsent>we first performed some experiments on the original tag sets to see how predictable they are.we started by defining simple six-way classification task which classifies disruption forms, and undecipherable forms as well as the four general tags defined above.
</prevsent>
</prevsection>
<citsent citstr=" J97-1002 ">
this is an empirically very well-founded distinction: the icsi-mr group have provided some inter-annotator agreement figures(carletta et al , 1997) <papid> J97-1002 </papid>for very similar task and report kappa of 0.79.</citsent>
<aftsection>
<nextsent>our me classifier scored 77.9% (baseline 54.0%).
</nextsent>
<nextsent>we also tested few simple binary classification sto see how predictable they are.
</nextsent>
<nextsent>utterances are annotated for example with tag if they are joke.
</nextsent>
<nextsent>as would be expected, the joke/non-joke classification produced results not distinguishable from chance.the performance of the classifiers on separating disrupted utterances from non disrupted forms scored slightly above chance at 89.9% (against baseline of87.0%).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2398">
<title id=" W04-0860.xml">the r2d2 team at senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the r2d2 team has participated in two tasks:english all-words and lexical sample.
</prevsent>
<prevsent>we use several different systems both supervised and unsupervised.
</prevsent>
</prevsection>
<citsent citstr=" H93-1021 ">
the supervised methods are based on maximum entropy (me) (lau et al, 1993; <papid> H93-1021 </papid>berger et al., 1996; <papid> J96-1002 </papid>ratnaparkhi, 1998), neural network using the learning vector quant ization algorithm (kohonen, 1995) and specialized hidden markov models (pla, 2000).</citsent>
<aftsection>
<nextsent>the unsupervised methods are relevant domains (rd) (montoyo et al, 2003) and the ciaosenso wsd system which is based on conceptual density (agirre and rigau, 1995), frequency of wordnet (miller et al, 1993<papid> H93-1061 </papid>a) senses and wordnet domains (magnini and cavaglia, 2000).</nextsent>
<nextsent>in the following section we will show more complete description of the systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2399">
<title id=" W04-0860.xml">the r2d2 team at senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the r2d2 team has participated in two tasks:english all-words and lexical sample.
</prevsent>
<prevsent>we use several different systems both supervised and unsupervised.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
the supervised methods are based on maximum entropy (me) (lau et al, 1993; <papid> H93-1021 </papid>berger et al., 1996; <papid> J96-1002 </papid>ratnaparkhi, 1998), neural network using the learning vector quant ization algorithm (kohonen, 1995) and specialized hidden markov models (pla, 2000).</citsent>
<aftsection>
<nextsent>the unsupervised methods are relevant domains (rd) (montoyo et al, 2003) and the ciaosenso wsd system which is based on conceptual density (agirre and rigau, 1995), frequency of wordnet (miller et al, 1993<papid> H93-1061 </papid>a) senses and wordnet domains (magnini and cavaglia, 2000).</nextsent>
<nextsent>in the following section we will show more complete description of the systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2400">
<title id=" W04-0860.xml">the r2d2 team at senseval3 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we use several different systems both supervised and unsupervised.
</prevsent>
<prevsent>the supervised methods are based on maximum entropy (me) (lau et al, 1993; <papid> H93-1021 </papid>berger et al., 1996; <papid> J96-1002 </papid>ratnaparkhi, 1998), neural network using the learning vector quant ization algorithm (kohonen, 1995) and specialized hidden markov models (pla, 2000).</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
the unsupervised methods are relevant domains (rd) (montoyo et al, 2003) and the ciaosenso wsd system which is based on conceptual density (agirre and rigau, 1995), frequency of wordnet (miller et al, 1993<papid> H93-1061 </papid>a) senses and wordnet domains (magnini and cavaglia, 2000).</citsent>
<aftsection>
<nextsent>in the following section we will show more complete description of the systems.
</nextsent>
<nextsent>next, howsuch methods were combined in two voting systems, and the results obtained in senseval-3.
</nextsent>
<nextsent>finally, some conclusions will be presented.
</nextsent>
<nextsent>in this section the systems that have participated at senseval-3 will be described.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2416">
<title id=" W04-0860.xml">the r2d2 team at senseval3 </title>
<section> systems description.  </section>
<citcontext>
<prevsection>
<prevsent>this is an unsupervised wsd method based on the wordnet domains lexical resource (magnini and cavaglia, 2000).
</prevsent>
<prevsent>the under laying working hypothesis is that domain labels, such as architecture, sport and medicine provide natural way to establish semantic relations between word senses, that can be used during the disambiguation process.
</prevsent>
</prevsection>
<citsent citstr=" W00-0804 ">
this resource has already been used onword sense disambiguation (magnini and strapparava, 2000), <papid> W00-0804 </papid>but it has not made use of glosses infor mation.</citsent>
<aftsection>
<nextsent>so our approach make use of new lexical resource obtained from glosses information named relevant domains.first step is to obtain the relevant domains resource from wordnet glosses.
</nextsent>
<nextsent>for this task is necessary previous part-of-speech tagging of wordnet glosses (each gloss has associated domain la bel).
</nextsent>
<nextsent>so we extract all nouns, verbs, adjectives andadverbs from glosses and assign them their associated domain label.
</nextsent>
<nextsent>with this information and using the association ratio formula(w=word,d=domainlabel), in (1), we obtain the relevant domains resource.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2417">
<title id=" W04-0412.xml">noncontiguous word sequences for information retrieval </title>
<section> the use of phrases in ir.  </section>
<citcontext>
<prevsection>
<prevsent>will generate the subpair hot dogs?, but it will also generate the subpair like dogs?, whose semantical meaning is very far from that of the original sentence.
</prevsent>
<prevsent>other types of phrases.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
many efficient techniques exist to extract multiword expressions, collocations, lexical units and idioms (church and hanks, 1989; <papid> P89-1010 </papid>smadja, 1993; <papid> J93-1007 </papid>dias et al, 2000; dias, 2003).</citsent>
<aftsection>
<nextsent>unfortunately,very few have been applied to information retrieval with deep evaluation of the results.maximal frequent sequences.
</nextsent>
<nextsent>we propose maximal frequent sequences (mfs) as new alternative to account for word ordering in the modeling of textual documents.
</nextsent>
<nextsent>one of their strength is the fact that they are extracted if and only if they occur more often than given frequency threshold, which hopefully permits to avoid storing the numerous least significant phrases.
</nextsent>
<nextsent>a gap between words is allowed within the extraction process itself, permitting to deal with larger variety of language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2418">
<title id=" W04-0412.xml">noncontiguous word sequences for information retrieval </title>
<section> the use of phrases in ir.  </section>
<citcontext>
<prevsection>
<prevsent>will generate the subpair hot dogs?, but it will also generate the subpair like dogs?, whose semantical meaning is very far from that of the original sentence.
</prevsent>
<prevsent>other types of phrases.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
many efficient techniques exist to extract multiword expressions, collocations, lexical units and idioms (church and hanks, 1989; <papid> P89-1010 </papid>smadja, 1993; <papid> J93-1007 </papid>dias et al, 2000; dias, 2003).</citsent>
<aftsection>
<nextsent>unfortunately,very few have been applied to information retrieval with deep evaluation of the results.maximal frequent sequences.
</nextsent>
<nextsent>we propose maximal frequent sequences (mfs) as new alternative to account for word ordering in the modeling of textual documents.
</nextsent>
<nextsent>one of their strength is the fact that they are extracted if and only if they occur more often than given frequency threshold, which hopefully permits to avoid storing the numerous least significant phrases.
</nextsent>
<nextsent>a gap between words is allowed within the extraction process itself, permitting to deal with larger variety of language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2419">
<title id=" W04-1111.xml">a statistical model for hangeulhanja conversion in terminology domain </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>the disadvantages include, there should be enough big knowledge-base developed before, and syntactic analyzer was required for the case frame based approach.
</prevsent>
<prevsent>chinese pinyin conversion is similar task with hangeul-hanja conversion, except that all pinyin syllables are converted to chinese characters.
</prevsent>
</prevsection>
<citsent citstr=" P00-1031 ">
to convert pinyin to chinese characters h, chen and lee (2000) <papid> P00-1031 </papid>used bayes law to maximize pr(h|p), in which lm pr(h) and typing model pr(p|h) are included.</citsent>
<aftsection>
<nextsent>the typing model reflects online typing error, and also measures if the input is an english or chinese word.
</nextsent>
<nextsent>as the report, the statistical based pinyin conversion method showed better result than the rule and heuristic based pinyin conversion method.
</nextsent>
<nextsent>hangeul-hanja conversion normally does not need to convert online input.
</nextsent>
<nextsent>so we assume the user input is perfect, and employ transfer model instead of the typing model in chen and lee (2000).<papid> P00-1031 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2421">
<title id=" W04-1111.xml">a statistical model for hangeulhanja conversion in terminology domain </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>so we assume the user input is perfect, and employ transfer model instead of the typing model in chen and lee (2000).<papid> P00-1031 </papid></prevsent>
<prevsent>the third related work is transliteration.</prevsent>
</prevsection>
<citsent citstr=" C00-1056 ">
in statistical based english-korean transliteration, to convert english word to korean word k, model could use korean lm pr(k) and tm pr(e|k) (lee, 1999; kim et.al, 1999) to maximize pr(k|e), or use english lm pr(e) and tm pr(k|e) to maximize pr(e,k) (jung et, al., 2000).<papid> C00-1056 </papid></citsent>
<aftsection>
<nextsent>different from previous hangeul-hanja conversion method in korean imes, our system uses statistical information in both sino-korean word recognition and the best hanja correspondence selection.
</nextsent>
<nextsent>there are two sub models included in the model, one is hangeul hanja tm, and the other one is hanja lm.
</nextsent>
<nextsent>they provide unified approach to the whole conversion processing, including compound word tokenization, sino-korean word recognition, and the correct hanja correspondence selection.
</nextsent>
<nextsent>let be hangeul string (block) not longer than sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2425">
<title id=" W04-0833.xml">simple features for statistical word sense disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>sections 3 and 4 provide specifications of our two systems.
</prevsent>
<prevsent>section 5 discusses the results obtained and remarks on them, and finally in section 6, conclusion and our future work direction are given.
</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
in 1950, kaplan carried out one of the earliest wsd experiments and showed that the accuracy of sense resolution does not improve when more than four words around the target are considered (ide and veronis, 1998).<papid> J98-1001 </papid></citsent>
<aftsection>
<nextsent>while researchers such as masterman (1961), gougenheim and michea (1961), agree with this observation (ide and veronis, 1998), <papid> J98-1001 </papid>our results demonstrate that this does not generally apply to all words.</nextsent>
<nextsent>a large context window provides domain information which increases the accuracy for some target words such as bank.n,but not others like different.a or use.v (see section 3).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2428">
<title id=" W04-0833.xml">simple features for statistical word sense disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while researchers such as masterman (1961), gougenheim and michea (1961), agree with this observation (ide and veronis, 1998), <papid> J98-1001 </papid>our results demonstrate that this does not generally apply to all words.</prevsent>
<prevsent>a large context window provides domain information which increases the accuracy for some target words such as bank.n,but not others like different.a or use.v (see section 3).</prevsent>
</prevsection>
<citsent citstr=" C02-1039 ">
this confirms mihalceas observations (mihalcea, 2002).<papid> C02-1039 </papid></citsent>
<aftsection>
<nextsent>in our system we allow larger context window size and for most of the words such context window is selected by the system.another trend consists in defining and using semantic preferences for the target word.for example, the verb drink prefers anani mate subject in its imbibe sense.
</nextsent>
<nextsent>boguraev shows that this does not work for polysemous verbs because of metaphoric expressions (ide and veronis, 1998).<papid> J98-1001 </papid></nextsent>
<nextsent>furthermore, the grammatical structures the target word takes part in can be used as distinguishing tool: the word keep?, can be disam biguated by determining whether its object is gerund (he kept eating), adjectival phrase (he kept calm), or noun phrase (he kept record)?(reifler, 1955).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2431">
<title id=" W04-0833.xml">simple features for statistical word sense disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, the grammatical structures the target word takes part in can be used as distinguishing tool: the word keep?, can be disam biguated by determining whether its object is gerund (he kept eating), adjectival phrase (he kept calm), or noun phrase (he kept record)?(reifler, 1955).
</prevsent>
<prevsent>in our second system we approximate the syntactic structures of word, in its different senses.
</prevsent>
</prevsection>
<citsent citstr=" W96-0208 ">
mooney (mooney, 1996) <papid> W96-0208 </papid>has discussed the effect of bias on inductive learning methods.</citsent>
<aftsection>
<nextsent>in this work we also show sensitivity of nave bayes to the distribution of samples.
</nextsent>
<nextsent>words in our approach, large window and smaller sub-window are centered around the targetword.
</nextsent>
<nextsent>we account for all words within the sub window but use pos filter as well as short stop-word list to filter out non-content words association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems figure 1: the effect of choosing different window and sub-window sizes for the word bank.n. the best accuracy is achieved with window and sub-window size of around 450 and 50 characters respectively, while for example 50 and 25 provide very low accuracy.
</nextsent>
<nextsent>from the context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2432">
<title id=" W04-0833.xml">simple features for statistical word sense disambiguation </title>
<section> nave bayes for learning context.  </section>
<citcontext>
<prevsection>
<prevsent>various smoothing algorithms could be usedto reduce the probability of seen words and distributing them among unseen words.
</prevsent>
<prevsent>however, tuning various smoothing parameters is delicate as it involves keeping an appropriate amount of held-out data.
</prevsent>
</prevsection>
<citsent citstr=" W97-0323 ">
instead, we implemented an approximate smoothing method, which seems to perform better compared tongs (ng, 1997)<papid> W97-0323 </papid>approximate smoothing.</citsent>
<aftsection>
<nextsent>in our simple approximate smoothing the probability of seen words is not discounted to compensate for those of unseen words2.
</nextsent>
<nextsent>finding proper value to assign to unseen words was done experimen tally; for relatively large training dataset, p(an unseen word) = 1010 and for small set, 109 resulted in the highest accuracy with our 15% validation set3.
</nextsent>
<nextsent>the intuition is that, with small training set, more unseen words are likely to be seen during the testing phase, and in order to prevent the accumulating score penalty value from becoming relatively high, lower probability value is selected.
</nextsent>
<nextsent>additionally,the selection should not result in large differences in the computed scores of different senses of the target word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2434">
<title id=" W04-1107.xml">chinese chunking with another type of spec </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also find that the hardest point of chinese chunking is to identify the chunking boundary inside noun-noun sequences1.
</prevsent>
<prevsent>abney (1991) has proposed chunking as useful and relative tractable median stage that is to divide sentences into non-overlapping segments only based on superficial analysis and local information.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
(ramshaw and marcus, 1995) <papid> W95-0107 </papid>represent chunking as tagging problem and the conll2000 shared task (kim sang and buchholz, 2000) is now the standard evaluation task for chunking english.</citsent>
<aftsection>
<nextsent>their work has inspired many others to study chunking for other human languages.
</nextsent>
<nextsent>besides the chunking algorithm, spec (the detailed definitions of all chunk types) is another critical issue for automatic chunking development.
</nextsent>
<nextsent>the well-defined spec can induce the chunker to perform well.
</nextsent>
<nextsent>currently chunking specs are defined as some rules or one program to extract phrases from treebank such as (li, 2003) and (li, 2004) in order to save the cost of manual annotation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2435">
<title id=" W04-1107.xml">chinese chunking with another type of spec </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an annotated chunking corpus is built with the spec and then chunker is also constructed accordingly.
</prevsent>
<prevsent>for annotation, we adopt two-stage processing, in which text is first chunked manually and then the potential inconsistent annotations are checked semi-automatically with tool.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
for the chunker, we use hmm model and tbl (transform-based learning) (brill, 1995) <papid> J95-4004 </papid>based error correction to further improve chunking performance.</citsent>
<aftsection>
<nextsent>with our spec the overall average length of chunks arrives 1.38 tokens, in open test, the chunking measure achieves 91.13% and 95.45% if under-combining errors are not counted.
</nextsent>
<nextsent>we also find the hardest point of chinese chunking is to identify the chunking boundary inside noun-noun sequence.
</nextsent>
<nextsent>in the remainder of this paper section 2 describes some problems in chunking chinese text, section 3 discusses the reason why another type of spec is needed and proposes our chunking spec, section 4 discusses the annotation of our chunking corpus, section 5 describes chunking model, section 6 gives experiment results, section 7, 8 recall some related work and give our conclusions respectively.
</nextsent>
<nextsent>the purpose of chinese chunking is to divide sentence into syntactically correlated parts of words after word segmentation and part-of-speech (pos) tagging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2436">
<title id=" W04-1107.xml">chinese chunking with another type of spec </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>part of the sparkle project has concentrates on spec based on un-bracketed corpus of english, italian, french and german(carroll et al, 1997).
</prevsent>
<prevsent>(zhou, 2002) defines base phrase which is similar as chunk for chinese, but his annotation and experiment are on his own corpus.
</prevsent>
</prevsection>
<citsent citstr=" P03-1063 ">
for chunking algorithm, many machine learning (ml) methods have been applied and got promising results after chunking is represented as tagging problem, such as: svm (kudoh and matsumoto, 2001), memory-based (bosch and buchholz, 2002), snow (li and roth), et al. some rule-base chunking (kinyon, 2003) and combining rules with learning (park and zhang, 2003) <papid> P03-1063 </papid>are also reported.</citsent>
<aftsection>
<nextsent>for annotation, (brants, 2000) reports the inter annotator agreement of part-of-speech annotations is 98.57%, the one of structural annotations is 92.43% and some consistency measures.
</nextsent>
<nextsent>(xue et al., 2002) <papid> C02-1145 </papid>also address some issues related to building large-scale chinese corpus.</nextsent>
<nextsent>we propose solution of chinese chunking with another type of spec that is based on un-bracketed corpus rather than derived from treebank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2437">
<title id=" W04-1107.xml">chinese chunking with another type of spec </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for chunking algorithm, many machine learning (ml) methods have been applied and got promising results after chunking is represented as tagging problem, such as: svm (kudoh and matsumoto, 2001), memory-based (bosch and buchholz, 2002), snow (li and roth), et al. some rule-base chunking (kinyon, 2003) and combining rules with learning (park and zhang, 2003) <papid> P03-1063 </papid>are also reported.</prevsent>
<prevsent>for annotation, (brants, 2000) reports the inter annotator agreement of part-of-speech annotations is 98.57%, the one of structural annotations is 92.43% and some consistency measures.</prevsent>
</prevsection>
<citsent citstr=" C02-1145 ">
(xue et al., 2002) <papid> C02-1145 </papid>also address some issues related to building large-scale chinese corpus.</citsent>
<aftsection>
<nextsent>we propose solution of chinese chunking with another type of spec that is based on un-bracketed corpus rather than derived from treebank.
</nextsent>
<nextsent>through spec drafting and annotating, most significant syntactic ambiguous patterns have been studied, and those observations in turn have been described in the spec carefully.
</nextsent>
<nextsent>the proposed method of defining chunking spec helps us find proper solution for the hard problems of chunking chinese.
</nextsent>
<nextsent>the experiments show that with our spec, the overall chinese chunking f-measure achieves 91.13% and 95.45% if under-combining errors are not counted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2438">
<title id=" W04-0904.xml">ontosem and simple two multilingual world views </title>
<section> overview of ontosem.  </section>
<citcontext>
<prevsection>
<prevsent>the tmr language, which is the meta language for representing text meaning.
</prevsent>
<prevsent>a very simple example of tmr (simple because most of the sentences we process are much longer), which reflects the meaning of the sentence he asked theun to authorize the war, is as follows: request-action-69 agent human-72 theme accept-70 beneficiary organization-71 source-root-word ask time (  (find-anchor-time)) accept-70 theme war-73 theme-of request-action-69 source-root-word authorize organization-71 has-name united-nations beneficiary-of request-action-69 source-root-word un human-72 has-name colin powell agent-of request-action-69 source-root-word he ; ref.
</prevsent>
</prevsection>
<citsent citstr=" W04-0905 ">
resolution done war-73 theme-of accept-70 source-root-word war details of this approach to text processing can be found, e.g., in nirenburg et al  2004<papid> W04-0905 </papid>a,b. the ontology itself, brief ontology tutorial, and an extensive lexicon tutorial can be viewed at http://ilit.umbc.edu.</citsent>
<aftsection>
<nextsent>ontosem has been used with languages including english, spanish, chinese, arabic and persian, to varying degrees of lexical coverage (e.g., earlier, less fine-grained english and spanish lexicons contained 40k entries and were used for mt in the mikrokosmos project).
</nextsent>
<nextsent>what makes ontosem amenable to efficient cross-linguistic usage is that many of the resources are either fully language independent (the ontology, the fact repository, the tmr metalanguage) or parameterizable in well understood ways.
</nextsent>
<nextsent>here we focus on exploiting cross-linguistic similarity for lexical acquisition, but similar analysis could be applied to the ontosem analyzers.
</nextsent>
<nextsent>3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2439">
<title id=" W04-1017.xml">event based extractive summarization </title>
<section> general summarization model.  </section>
<citcontext>
<prevsection>
<prevsent>the results establish that for the majority of document sets in our test collection,events outperform tf*idf for all algorithms considered.
</prevsent>
<prevsent>furthermore, we show that this benefit is more pronounced when the selection algorithm includes steps to address potential repetition of information in the output summary.
</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
many summarization systems (e.g., (teufel and moens, 1997; mckeown et al, 1999; lin and hovy, 2000)) <papid> C00-1072 </papid>include two levels of analysis: the sentence level, where every textual unit is scored according to c1 c2 c3 c4 c5 t1 1 1 0 1 1 t2 1 0 0 1 0 t3 0 1 0 0 1 t4 1 0 1 1 1 table 1: matrix for summarization model the concepts or features it covers, and the text level,where, before being added to the final output, textual units are compared to each other on the basis of those features.</citsent>
<aftsection>
<nextsent>in section 1 we presented four-step pipeline for extractive summarization; existing summarization systems largely follow this pipeline, although they introduce different approaches for every stepin it.
</nextsent>
<nextsent>we suggest model that describes the extractive summarization task in general terms.
</nextsent>
<nextsent>consider the matrix in table 1.
</nextsent>
<nextsent>rows of this matrix represent all textual units into which the input text is divided.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2440">
<title id=" W04-1017.xml">event based extractive summarization </title>
<section> general summarization model.  </section>
<citcontext>
<prevsection>
<prevsent>for example, summary consisting only of textual unit t1 renders the same information as the summary consisting of textual units t2 and t3.
</prevsent>
<prevsent>both these summaries cover the same set of concepts, namely c1, c2 and c3.
</prevsent>
</prevsection>
<citsent citstr=" C04-1057 ">
we explore properties ofthis model in more detail in (filatova and hatzivassiloglou, 2004).<papid> C04-1057 </papid></citsent>
<aftsection>
<nextsent>before extracting summary, it is necessary to define what concepts in the input text are important and should be covered by the output text.
</nextsent>
<nextsent>there isno exact definition or even agreement between different approaches on what an important concept is. in order to use the model of section 2 one has to approximate the notion of concept?
</nextsent>
<nextsent>with some textual features.current summarization approaches use text features which give high scores to the textual units that contain important information, and low scores to those textual units which are not highly likely to contain information worth to be included in the final output.there exist approaches that deal mainly with lexical features, like tf*idf weighing of words in the input text(s), words used in the titles and section headings (luhn, 1958; edmundson, 1968), or the presence or absence of certain cue phrases like significant, important, and in conclusion (kupiec etal., 1995; teufel and moens, 1997).
</nextsent>
<nextsent>other systems exploit the co-occurrence of particular concepts (barzilay and elhadad, 1997; <papid> W97-0703 </papid>lin and hovy, 2000) <papid> C00-1072 </papid>or syntactic constraints between concepts (mckeown et al, 1999).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2441">
<title id=" W04-1017.xml">event based extractive summarization </title>
<section> associating concepts with features.  </section>
<citcontext>
<prevsection>
<prevsent>there isno exact definition or even agreement between different approaches on what an important concept is. in order to use the model of section 2 one has to approximate the notion of concept?
</prevsent>
<prevsent>with some textual features.current summarization approaches use text features which give high scores to the textual units that contain important information, and low scores to those textual units which are not highly likely to contain information worth to be included in the final output.there exist approaches that deal mainly with lexical features, like tf*idf weighing of words in the input text(s), words used in the titles and section headings (luhn, 1958; edmundson, 1968), or the presence or absence of certain cue phrases like significant, important, and in conclusion (kupiec etal., 1995; teufel and moens, 1997).
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
other systems exploit the co-occurrence of particular concepts (barzilay and elhadad, 1997; <papid> W97-0703 </papid>lin and hovy, 2000) <papid> C00-1072 </papid>or syntactic constraints between concepts (mckeown et al, 1999).</citsent>
<aftsection>
<nextsent>concepts do not have to be directly observable as text snippets they can represent abstract properties that particular text units may or may not satisfy, for example, status as first sentence in paragraph or generally position in the source text (baxendale, 1958; lin and hovy, 1997).<papid> A97-1042 </papid>some summarization systems assume that the importance of sentence is derivable from rhetorical representation of the source text (marcu, 1997).<papid> W97-0713 </papid></nextsent>
<nextsent>the matrix representation of the previous section offers way to formalize the sharing of information between textual units at the individual feature level.thus, this representation is most useful for content related concepts that should not be repeated in the summary.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2443">
<title id=" W04-1017.xml">event based extractive summarization </title>
<section> associating concepts with features.  </section>
<citcontext>
<prevsection>
<prevsent>with some textual features.current summarization approaches use text features which give high scores to the textual units that contain important information, and low scores to those textual units which are not highly likely to contain information worth to be included in the final output.there exist approaches that deal mainly with lexical features, like tf*idf weighing of words in the input text(s), words used in the titles and section headings (luhn, 1958; edmundson, 1968), or the presence or absence of certain cue phrases like significant, important, and in conclusion (kupiec etal., 1995; teufel and moens, 1997).
</prevsent>
<prevsent>other systems exploit the co-occurrence of particular concepts (barzilay and elhadad, 1997; <papid> W97-0703 </papid>lin and hovy, 2000) <papid> C00-1072 </papid>or syntactic constraints between concepts (mckeown et al, 1999).</prevsent>
</prevsection>
<citsent citstr=" A97-1042 ">
concepts do not have to be directly observable as text snippets they can represent abstract properties that particular text units may or may not satisfy, for example, status as first sentence in paragraph or generally position in the source text (baxendale, 1958; lin and hovy, 1997).<papid> A97-1042 </papid>some summarization systems assume that the importance of sentence is derivable from rhetorical representation of the source text (marcu, 1997).<papid> W97-0713 </papid></citsent>
<aftsection>
<nextsent>the matrix representation of the previous section offers way to formalize the sharing of information between textual units at the individual feature level.thus, this representation is most useful for content related concepts that should not be repeated in the summary.
</nextsent>
<nextsent>the representation can however handle independent features such as sentence position by encoding them separately for each textual unit.
</nextsent>
<nextsent>atomic events link major constituent parts of the actions described in text or collection of texts through the verbs or action nouns labeling the event itself.
</nextsent>
<nextsent>the idea behind this technique is that the major constituent parts of events (participants, locations, times) are usually realized in text as named entities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2444">
<title id=" W04-1017.xml">event based extractive summarization </title>
<section> associating concepts with features.  </section>
<citcontext>
<prevsection>
<prevsent>with some textual features.current summarization approaches use text features which give high scores to the textual units that contain important information, and low scores to those textual units which are not highly likely to contain information worth to be included in the final output.there exist approaches that deal mainly with lexical features, like tf*idf weighing of words in the input text(s), words used in the titles and section headings (luhn, 1958; edmundson, 1968), or the presence or absence of certain cue phrases like significant, important, and in conclusion (kupiec etal., 1995; teufel and moens, 1997).
</prevsent>
<prevsent>other systems exploit the co-occurrence of particular concepts (barzilay and elhadad, 1997; <papid> W97-0703 </papid>lin and hovy, 2000) <papid> C00-1072 </papid>or syntactic constraints between concepts (mckeown et al, 1999).</prevsent>
</prevsection>
<citsent citstr=" W97-0713 ">
concepts do not have to be directly observable as text snippets they can represent abstract properties that particular text units may or may not satisfy, for example, status as first sentence in paragraph or generally position in the source text (baxendale, 1958; lin and hovy, 1997).<papid> A97-1042 </papid>some summarization systems assume that the importance of sentence is derivable from rhetorical representation of the source text (marcu, 1997).<papid> W97-0713 </papid></citsent>
<aftsection>
<nextsent>the matrix representation of the previous section offers way to formalize the sharing of information between textual units at the individual feature level.thus, this representation is most useful for content related concepts that should not be repeated in the summary.
</nextsent>
<nextsent>the representation can however handle independent features such as sentence position by encoding them separately for each textual unit.
</nextsent>
<nextsent>atomic events link major constituent parts of the actions described in text or collection of texts through the verbs or action nouns labeling the event itself.
</nextsent>
<nextsent>the idea behind this technique is that the major constituent parts of events (participants, locations, times) are usually realized in text as named entities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2445">
<title id=" W04-1017.xml">event based extractive summarization </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>berkeley.edu/docfreq/.evaluation metric given the difficulties in coming up with universally accepted evaluation measure for summarization, and the fact that obtaining judgments by humans is time-consuming andlabor-intensive, we adopted an automated process for comparing system-produced summaries to ideal?
</prevsent>
<prevsent>summaries written by humans.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
the method, rouge (lin and hovy, 2003), <papid> N03-1020 </papid>is based on n-gram overlap between the system-produced and ideal summaries.</citsent>
<aftsection>
<nextsent>as such, it is recall-based measure, and it requires that the length of the summaries be controlled to allow meaningful comparisons.
</nextsent>
<nextsent>rouge can be readily applied to compare the performance of different systems on the same set of documents, assuming that ideal summaries are available for those documents.
</nextsent>
<nextsent>at the same time,rouge evaluation has not yet been tested extensively, and rouge scores are difficult to interpret as they are not absolute and not comparable across source document sets.
</nextsent>
<nextsent>50 100 200 400 events better 53.3% 63.3% 80.0% 80.0% tf*idf better 23.3% 26.7% 20.0% 20.0% equal 23.3% 10.0% 0.0% 0.0% table 2: static greedy algorithm, events versus tf*idf 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 duc document sets o ge c re events tf*idf figure 1: rouge scores for 400-word summaries for static greedy algorithm, events versus tf*idf 50 100 200 400 events better 53.3% 66.7% 86.7% 80.0% tf*idf better 23.3% 20.0% 13.3% 20.0% equal 23.3% 13.3% 0.0% 0.0% table 3: adaptive greedy algorithm, events versus tf*idfin our comparison, we used as reference summaries those created by nist assessors for the ductask of generic summarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2446">
<title id=" W04-0504.xml">a qualitative comparison of scientific and journalistic texts from the perspective of extracting definitions </title>
<section> recent related work.  </section>
<citcontext>
<prevsection>
<prevsent>the usefulness and readability of the definitions retrieved by def inder were both rated by non experts as being significantly higher than those of online dictionaries.
</prevsent>
<prevsent>however, klavans and muresan do not focus specifically on the characteristics of the source documents in their domain.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
the view of teufel and moens (2002) <papid> J02-4002 </papid>that summarization of scientific articles requires different approach from the one used in summarization of news articles may perhaps apply to qa.</citsent>
<aftsection>
<nextsent>the innovation of their work is in defining principles for content selection specifically for scientific articles.
</nextsent>
<nextsent>as an example they observe that information fusion (the comparison of results from different sources to eliminate mis-information and minimize the loss of data caused by unexpected phrasing) will be inefficient when summarizing scientific articles, because new ideas are usually the main focus of scientific writing, whereas in the news domain events are frequently repeated over short time.
</nextsent>
<nextsent>the lack of redundancy as feature of technical domains is also mentioned by moll?
</nextsent>
<nextsent>et al (2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2447">
<title id=" W04-0504.xml">a qualitative comparison of scientific and journalistic texts from the perspective of extracting definitions </title>
<section> answering definition questions related to.  </section>
<citcontext>
<prevsection>
<prevsent>extract any portions of these which.
</prevsent>
<prevsent>matched collection of syntactic patterns.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
the patterns used were similar to the ones used by hearst (1992), <papid> C92-2082 </papid>joho and sanderson (2000) and liu et al (2003) to retrieve hyponyms from an encyclopedia, descriptive phrases from news articles and definitions from web pages, respectively.</citsent>
<aftsection>
<nextsent>to evaluate the system four test collections of terms were used: 42 terms which were suggested by salmon researchers, and three collections containing 3,920, 2,000 and 1,120 terms respectively.
</nextsent>
<nextsent>the latter were extracted from database on the web called fish base (2003).
</nextsent>
<nextsent>for each collection, the output corresponding to term was inspected manually and each phrase matching pattern was judged to be either vital, okay, uncertain or wrong.
</nextsent>
<nextsent>while complete discussion of the results and methods used to obtain them can be found in gabbay (2004), the main quantitative finding of the project was that techniques adopted could achieve recall of up to 60%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2449">
<title id=" W04-2013.xml">wordnet based text document clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this work explores if and how the two following methods can improve the effectiveness of clustering.
</prevsent>
<prevsent>1http://www.vivisimo.com part-of-speech tagging.
</prevsent>
</prevsection>
<citsent citstr=" W97-0811 ">
segond et al (1997) <papid> W97-0811 </papid>observe that part-of-speech tagging (pos) solves semantic ambiguity to some extent (40% in one of their tests).</citsent>
<aftsection>
<nextsent>based on this observation, we study whether nave word sense disambiguation by pos tagging can help to improve clustering results.wordnet.
</nextsent>
<nextsent>synonymy and hypernymy can reveal hidden similarities, potentially leading to better clusters.
</nextsent>
<nextsent>wordnet,2 an ontology which models these two relations (among many others) (miller et al, 1991), is used to include synonyms and hypernyms in thedata representation and the effects on clustering quality are observed and analysed.
</nextsent>
<nextsent>the overall aim of the approach outlined above is to cluster documents by meaning, hence itis relevant to language understanding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2450">
<title id=" W04-0703.xml">event clustering on streaming news using coreference chains and event words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they introduced more semantic approach for feature selection than the approach of parts of speech.
</prevsent>
<prevsent>wong, kuo and chen (2001) employed these concepts to select informative words for headline generation, and to rank the extracted sentences in multi-document summarization (kuo, wong, lin, and chen, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P98-1012 ">
bagga and baldwin (1998) <papid> P98-1012 </papid>proposed entity based cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document.</citsent>
<aftsection>
<nextsent>azzam, humphreys, and gaizauskas (1999) proposed primitive model for text summarization using co-reference chains as well.
</nextsent>
<nextsent>silber and mccoy (2002) <papid> J02-4004 </papid>proposed text summarization model using lexical chains and showed that proper nouns and anaphora resolution is indispensable.</nextsent>
<nextsent>the two semantics-based feature selection approaches, i.e., co-reference chains and event words, are complementary in some sense.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2451">
<title id=" W04-0703.xml">event clustering on streaming news using coreference chains and event words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bagga and baldwin (1998) <papid> P98-1012 </papid>proposed entity based cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document.</prevsent>
<prevsent>azzam, humphreys, and gaizauskas (1999) proposed primitive model for text summarization using co-reference chains as well.</prevsent>
</prevsection>
<citsent citstr=" J02-4004 ">
silber and mccoy (2002) <papid> J02-4004 </papid>proposed text summarization model using lexical chains and showed that proper nouns and anaphora resolution is indispensable.</citsent>
<aftsection>
<nextsent>the two semantics-based feature selection approaches, i.e., co-reference chains and event words, are complementary in some sense.
</nextsent>
<nextsent>the former denotes equivalence classes of noun phrases, and the latter considers both nominal and verbal features, which appear across paragraphs.
</nextsent>
<nextsent>this paper will employ both co-reference chains and event words for temporal event clustering.
</nextsent>
<nextsent>an event clustering system using co-reference chains is described in section 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2452">
<title id=" W04-0703.xml">event clustering on streaming news using coreference chains and event words </title>
<section> event clustering using co-reference.  </section>
<citcontext>
<prevsection>
<prevsent>section 6 concludes the remarks.
</prevsent>
<prevsent>chains co-reference chain in document denotes an equivalence class of noun phrases.
</prevsent>
</prevsection>
<citsent citstr=" W99-0611 ">
(cardie and wagstaff, 1999) <papid> W99-0611 </papid>co-reference resolution procedure is first to find all the possible np candidates.</citsent>
<aftsection>
<nextsent>it includes word segmentation, named entity extraction, part of speech tagging, and noun phrase chunking.
</nextsent>
<nextsent>then the candidates are partitioned into equivalence classes using the attributes such as word/phrase itself, parts of speech of head nouns, named entities, positions in document, numbers (singular, plural, or unknown), pronouns, gender (female, male, or unknown), and semantics of head nouns.
</nextsent>
<nextsent>as the best f-measure of automatic co-reference resolution in english documents in muc-7 was 61.8% (muc, 1998), corpus hand-tagged with named entities, and co-reference chains are prepared and employed to examine the real effects of co-reference chains in event clustering r. headlines of news story can be regarded as its short summary.
</nextsent>
<nextsent>that is, the words in the headline represent the content of document in some sense.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2453">
<title id=" W04-2510.xml">onto logical resources and question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as any knowledge intensive application, onto logical qa systems have as intrinsic limitation related to the small scale of the underlying syntactic-semantic models of natural language.
</prevsent>
<prevsent>while limitations are well-known, we are still questioning if any improvement has occurred since the development of the first onto logical qa system lunar.
</prevsent>
</prevsection>
<citsent citstr=" P01-1037 ">
several important facts have emerged that could influence related research approaches:   growing availability of lexical knowledge bases that model and structure words: wordnet (miller 1995) and euro wordnet (vossen 1998) among others; some open-domain qa systems have proven the usefulness of these resources, e.g. wordnet in the system described in (harabagiu et al  2001).<papid> P01-1037 </papid></citsent>
<aftsection>
<nextsent>  the vision of web populated by ontologically?
</nextsent>
<nextsent>tagged documents which the semantic web initiative has promoted; in case this vision becomes reality, it will require world-wide collaborative work for building interrelated conceptualisations?
</nextsent>
<nextsent>of domain specific knowledge   the trend in building shallow, modular, and robust natural language processing systems (abney 1996, hobbs et al  1996, ait-moktar&chanod; 1997, basili&zanzotto; 2002) which is making them appealing in the context of onto logical qa systems, both for text interpretation (andreasen et al  2002) and for database access (popescu et al  2003).
</nextsent>
<nextsent>given this background, we are investigating new approach to ontology-based qa in which users ask questions in natural language to knowledge bases of facts extracted from federation of web sites and organised in topic map repositories (garshol 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2454">
<title id=" W04-1013.xml">rouge a package for automatic evaluation of summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is very expensive and difficult to conduct infrequent basis.
</prevsent>
<prevsent>therefore, how to evaluate summaries automatically has drawn lot of attention in the summarization research community in recent years.
</prevsent>
</prevsection>
<citsent citstr=" C02-1073 ">
for example, saggion et al (2002) <papid> C02-1073 </papid>proposed three content-based evaluation methods that measure similarity between summaries.</citsent>
<aftsection>
<nextsent>these methods are: cosine similarity, unit overlap (i.e. unigram or bi gram), and longest common subsequence.
</nextsent>
<nextsent>however, they did not show how the results of these automatic evaluation methods correlate to human judgments.
</nextsent>
<nextsent>following the successful application of automatic evaluation methods, such as bleu (papineni et al, 2001), in machine translation evaluation, lin and hovy (2003) <papid> N03-1020 </papid>showed that methods similar to bleu, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.</nextsent>
<nextsent>in this paper, we introduce package, rouge, for automatic evaluation of summaries and its evaluations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2456">
<title id=" W04-1013.xml">rouge a package for automatic evaluation of summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these methods are: cosine similarity, unit overlap (i.e. unigram or bi gram), and longest common subsequence.
</prevsent>
<prevsent>however, they did not show how the results of these automatic evaluation methods correlate to human judgments.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
following the successful application of automatic evaluation methods, such as bleu (papineni et al, 2001), in machine translation evaluation, lin and hovy (2003) <papid> N03-1020 </papid>showed that methods similar to bleu, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.</citsent>
<aftsection>
<nextsent>in this paper, we introduce package, rouge, for automatic evaluation of summaries and its evaluations.
</nextsent>
<nextsent>rouge stands for re call-oriented understudy for gisting evaluation.
</nextsent>
<nextsent>it includes several automatic evaluation methods that measure the similarity between summaries.
</nextsent>
<nextsent>we describe rouge-n in section 2, rouge-l in section 3, rouge-w in section 4, and rouge-s in section 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2457">
<title id=" W04-1013.xml">rouge a package for automatic evaluation of summaries </title>
<section> rouge-l: longest common sub sequence.  </section>
<citcontext>
<prevsection>
<prevsent>given two sequences and y, the longest common sub sequence (lcs) of and is common sub sequence with maximum length.
</prevsent>
<prevsent>lcs has been used in identifying cognate candidates during construction of n-best translation lexicon from parallel text.
</prevsent>
</prevsection>
<citsent citstr=" W95-0115 ">
melamed (1995) <papid> W95-0115 </papid>used the ratio (lcsr) between the length of the lcs of two words and the length of the longer word of the two words to measure the cognateness between them.</citsent>
<aftsection>
<nextsent>he used lcs as an approximate string matching algorithm.
</nextsent>
<nextsent>saggion et al (2002) <papid> C02-1073 </papid>used normalized pairwise lcs to compare similarity between two texts in automatic summarization evaluation.</nextsent>
<nextsent>3.1 sentence-level lcs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2460">
<title id=" W04-1013.xml">rouge a package for automatic evaluation of summaries </title>
<section> rouge-l: longest common sub sequence.  </section>
<citcontext>
<prevsection>
<prevsent>notice that rouge-l is 1 when = y; while rouge-l is zero when lcs(x,y) = 0, i.e. there is nothing in common between and y. measure or its equivalents has been shown to have met several theoretical criteria in measuring accuracy involving more than one factor (van rijsber gen, 1979).
</prevsent>
<prevsent>the composite factors are lcs-based recall and precision in this case.
</prevsent>
</prevsection>
<citsent citstr=" N03-2021 ">
melamed et al (2003) <papid> N03-2021 </papid>used unigram f-measure to estimate machine translation quality and showed that unigram measure was as good as bleu.</citsent>
<aftsection>
<nextsent>one advantage of using lcs is that it does notre quire consecutive matches but in-sequence matches that reflect sentence level word order as n-grams.
</nextsent>
<nextsent>the other advantage is that it automatically includes longest in-sequence common n-grams, therefore no predefined n-gram length is necessary.
</nextsent>
<nextsent>rouge-l as defined in equation 4 has the property that its value is less than or equal to the min mum of unigram f-measure of and y. unigram recall reflects the proportion of words in (refer ence summary sentence) that are also present in (candidate summary sentence); while unigram precision is the proportion of words in that are also in x. unigram recall and precision count all cooccurring words regardless their orders; while rouge-l counts only in-sequence co-occurrences.
</nextsent>
<nextsent>by only awarding credit to in-sequence unigram matches, rouge-l also captures sentence level structure in natural way.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2461">
<title id=" W04-1013.xml">rouge a package for automatic evaluation of summaries </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>we found that (1) rouge-2, rouge-l, rouge-w, and rouge-s worked well in single document summarization tasks, (2) rouge-1, rouge-l, rouge-w, rouge-su4, and rouge-su9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but rouge-1, rouge-2, rouge-s4, rouge-s9, rouge-su4, and rouge-su9 worked reasonably well when stop words were excluded from matching, (4) exclusion of stop words usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.
</prevsent>
<prevsent>in summary, we showed that the rouge package could be used effectively in automatic evaluation of summaries.
</prevsent>
</prevsection>
<citsent citstr=" P04-1077 ">
in separate study (lin and och, 2004), <papid> P04-1077 </papid>method case stem stop case stem stop case stem stop case stem stop case stem stop case stem stop r-1 0.48 0.56 0.86 0.53 0.57 0.87 0.66 0.66 0.77 0.71 0.71 0.78 0.58 0.57 0.71 0.58 0.57 0.71 r-2 0.55 0.57 0.64 0.59 0.61 0.71 0.83 0.83 0.80 0.88 0.87 0.85 0.69 0.67 0.71 0.79 0.79 0.81 r-3 0.46 0.45 0.47 0.53 0.53 0.55 0.85 0.84 0.76 0.89 0.88 0.83 0.54 0.51 0.48 0.76 0.75 0.74 r-4 0.39 0.39 0.43 0.48 0.49 0.47 0.80 0.80 0.63 0.83 0.82 0.75 0.37 0.36 0.36 0.62 0.61 0.52 r-5 0.38 0.39 0.33 0.47 0.48 0.43 0.73 0.73 0.45 0.73 0.73 0.62 0.25 0.25 0.27 0.45 0.44 0.38 r-6 0.39 0.39 0.20 0.45 0.46 0.39 0.71 0.72 0.38 0.66 0.64 0.46 0.21 0.21 0.26 0.34 0.31 0.29 r-7 0.31 0.31 0.17 0.44 0.44 0.36 0.63 0.65 0.33 0.56 0.53 0.44 0.20 0.20 0.23 0.29 0.27 0.25 r-8 0.18 0.19 0.09 0.40 0.40 0.31 0.55 0.55 0.52 0.50 0.46 0.52 0.18 0.18 0.21 0.23 0.22 0.23 r-9 0.11 0.12 0.06 0.38 0.38 0.28 0.54 0.54 0.52 0.45 0.42 0.52 0.16 0.16 0.19 0.21 0.21 0.21 r-l 0.49 0.49 0.49 0.56 0.56 0.56 0.62 0.62 0.62 0.65 0.65 0.65 0.50 0.50 0.50 0.53 0.53 0.53 r-s* 0.45 0.52 0.84 0.51 0.54 0.86 0.69 0.69 0.77 0.73 0.73 0.79 0.60 0.60 0.67 0.61 0.60 0.70 r-s4 0.46 0.50 0.71 0.54 0.57 0.78 0.79 0.80 0.79 0.84 0.85 0.82 0.63 0.64 0.70 0.73 0.73 0.78 r-s9 0.42 0.49 0.77 0.53 0.56 0.81 0.79 0.80 0.78 0.83 0.84 0.81 0.65 0.65 0.70 0.70 0.70 0.76 r-su* 0.45 0.52 0.84 0.51 0.54 0.87 0.69 0.69 0.77 0.73 0.73 0.79 0.60 0.59 0.67 0.60 0.60 0.70 r-su4 0.47 0.53 0.80 0.55 0.58 0.83 0.76 0.76 0.79 0.80 0.81 0.81 0.64 0.64 0.74 0.68 0.68 0.76 r-su9 0.44 0.50 0.80 0.53 0.57 0.84 0.77 0.78 0.78 0.81 0.82 0.81 0.65 0.65 0.72 0.68 0.68 0.75 r-w-1.2 0.52 0.52 0.52 0.60 0.60 0.60 0.67 0.67 0.67 0.69 0.69 0.69 0.53 0.53 0.53 0.58 0.58 0.58 method case stem stop case stem stop case stem stop case stem stop case stem stop case stem stop r-1 0.71 0.68 0.49 0.49 0.49 0.73 0.44 0.48 0.80 0.81 0.81 0.90 0.84 0.84 0.91 0.74 0.73 0.90 r-2 0.82 0.85 0.80 0.43 0.45 0.59 0.47 0.49 0.62 0.84 0.85 0.86 0.93 0.93 0.94 0.88 0.88 0.87 r-3 0.59 0.74 0.75 0.32 0.33 0.39 0.36 0.36 0.45 0.80 0.80 0.81 0.90 0.91 0.91 0.84 0.84 0.82 r-4 0.25 0.36 0.16 0.28 0.26 0.36 0.28 0.28 0.39 0.77 0.78 0.78 0.87 0.88 0.88 0.80 0.80 0.75 r-5 -0.25 -0.25 -0.24 0.30 0.29 0.31 0.28 0.30 0.49 0.77 0.76 0.72 0.82 0.83 0.84 0.77 0.77 0.70 r-6 0.00 0.00 0.00 0.22 0.23 0.41 0.18 0.21 -0.17 0.75 0.75 0.67 0.78 0.79 0.77 0.74 0.74 0.63 r-7 0.00 0.00 0.00 0.26 0.23 0.50 0.11 0.16 0.00 0.72 0.72 0.62 0.72 0.73 0.74 0.70 0.70 0.58 r-8 0.00 0.00 0.00 0.32 0.32 0.34 -0.11 -0.11 0.00 0.68 0.68 0.54 0.71 0.71 0.70 0.66 0.66 0.52 r-9 0.00 0.00 0.00 0.30 0.30 0.34 -0.14 -0.14 0.00 0.64 0.64 0.48 0.70 0.69 0.59 0.63 0.62 0.46 r-l 0.78 0.78 0.78 0.56 0.56 0.56 0.50 0.50 0.50 0.81 0.81 0.81 0.88 0.88 0.88 0.82 0.82 0.82 r-s* 0.83 0.82 0.69 0.46 0.45 0.74 0.46 0.49 0.80 0.80 0.80 0.90 0.84 0.85 0.93 0.75 0.74 0.89 r-s4 0.85 0.86 0.76 0.40 0.41 0.69 0.42 0.44 0.73 0.82 0.82 0.87 0.91 0.91 0.93 0.85 0.85 0.85 r-s9 0.82 0.81 0.69 0.42 0.41 0.72 0.40 0.43 0.78 0.81 0.82 0.86 0.90 0.90 0.92 0.83 0.83 0.84 r-su* 0.75 0.74 0.56 0.46 0.46 0.74 0.46 0.49 0.80 0.80 0.80 0.90 0.84 0.85 0.93 0.75 0.74 0.89 r-su4 0.76 0.75 0.58 0.45 0.45 0.72 0.44 0.46 0.78 0.82 0.83 0.89 0.90 0.90 0.93 0.84 0.84 0.88 r-su9 0.74 0.73 0.56 0.44 0.44 0.73 0.41 0.45 0.79 0.82 0.82 0.88 0.89 0.89 0.92 0.83 0.82 0.87 r-w-1.2 0.78 0.78 0.78 0.56 0.56 0.56 0.51 0.51 0.51 0.84 0.84 0.84 0.90 0.90 0.90 0.86 0.86 0.86 (a1) duc 2001 100 words multi (a2) duc 2002 100 words multi (a3) duc 2003 100 words multi 1 rff 3 refs 1 ref 2 refs 1 ref 4 refs.</citsent>
<aftsection>
<nextsent>(e2) duc02 200 (f) duc01 400(c) duc02 10 (d1) duc01 50 (d2) duc02 50 (e1) duc01 200 table 3: pearsons correlations of 17 rouge measure scores vs. human judgments for the duc 2001, 2002, and 2003 mult i-document summarization tasks rouge-l, w, and were also shown to be very effective in automatic evaluation of machine translation.
</nextsent>
<nextsent>the stability and reliability of rouge at different sample sizes was reported by the author in (lin, 2004).
</nextsent>
<nextsent>however, how to achieve high correlation with human judgments in multi-document summarization tasks as rouge already did in single document summarization tasks is still an open research topic.
</nextsent>
<nextsent>the author would like to thank the anonymous reviewers for their constructive comments, paul over at nist, u.s.a, and rouge users around the world for testing and providing useful feedback on earlier versions of the rouge evaluation package, and the darpa tides project for supporting this research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2462">
<title id=" W04-0851.xml">regularized least squares classification for word sense disambiguation </title>
<section> knowledge sources and feature.  </section>
<citcontext>
<prevsection>
<prevsent>in section 3 we briefly describe the rlsc learning algorithm and in section 4, how we applied this algorithm for word sense disambiguation and what result swe have obtained.
</prevsent>
<prevsent>finally, in section 5, we discuss some possible improvements.
</prevsent>
</prevsection>
<citsent citstr=" H93-1052 ">
space we follow the common practice (yarowsky, 1993; <papid> H93-1052 </papid>florian and yarowsky, 2002; <papid> W02-1004 </papid>lee and ng,2002) <papid> W02-1006 </papid>to represent the training instances as feature vectors.</citsent>
<aftsection>
<nextsent>this features are derived from various knowledge sources.
</nextsent>
<nextsent>we used the following knowledge sources: ? local information: ? the word form of words that appear association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems near the target word in window of size 3?
</nextsent>
<nextsent>the part-of-speech (pos) tags that appear near the target word in window of size 3 ? the lexical form of the target word ? the pos tag of the target word ? broad context information: ? the lemmas of all words that appear in the provided context of target word (stop words are removed) in the case of broad context we use the bag-of-words representation with two weighting schema.
</nextsent>
<nextsent>binary weighting for rlsc-lin and term frequency weighting1 for rlsc-comb.for stemming we used porter stem mer (porter, 1980) and for tagging we used brill tagger (brill, 1995).<papid> J95-4004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2463">
<title id=" W04-0851.xml">regularized least squares classification for word sense disambiguation </title>
<section> knowledge sources and feature.  </section>
<citcontext>
<prevsection>
<prevsent>in section 3 we briefly describe the rlsc learning algorithm and in section 4, how we applied this algorithm for word sense disambiguation and what result swe have obtained.
</prevsent>
<prevsent>finally, in section 5, we discuss some possible improvements.
</prevsent>
</prevsection>
<citsent citstr=" W02-1004 ">
space we follow the common practice (yarowsky, 1993; <papid> H93-1052 </papid>florian and yarowsky, 2002; <papid> W02-1004 </papid>lee and ng,2002) <papid> W02-1006 </papid>to represent the training instances as feature vectors.</citsent>
<aftsection>
<nextsent>this features are derived from various knowledge sources.
</nextsent>
<nextsent>we used the following knowledge sources: ? local information: ? the word form of words that appear association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems near the target word in window of size 3?
</nextsent>
<nextsent>the part-of-speech (pos) tags that appear near the target word in window of size 3 ? the lexical form of the target word ? the pos tag of the target word ? broad context information: ? the lemmas of all words that appear in the provided context of target word (stop words are removed) in the case of broad context we use the bag-of-words representation with two weighting schema.
</nextsent>
<nextsent>binary weighting for rlsc-lin and term frequency weighting1 for rlsc-comb.for stemming we used porter stem mer (porter, 1980) and for tagging we used brill tagger (brill, 1995).<papid> J95-4004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2464">
<title id=" W04-0851.xml">regularized least squares classification for word sense disambiguation </title>
<section> knowledge sources and feature.  </section>
<citcontext>
<prevsection>
<prevsent>in section 3 we briefly describe the rlsc learning algorithm and in section 4, how we applied this algorithm for word sense disambiguation and what result swe have obtained.
</prevsent>
<prevsent>finally, in section 5, we discuss some possible improvements.
</prevsent>
</prevsection>
<citsent citstr=" W02-1006 ">
space we follow the common practice (yarowsky, 1993; <papid> H93-1052 </papid>florian and yarowsky, 2002; <papid> W02-1004 </papid>lee and ng,2002) <papid> W02-1006 </papid>to represent the training instances as feature vectors.</citsent>
<aftsection>
<nextsent>this features are derived from various knowledge sources.
</nextsent>
<nextsent>we used the following knowledge sources: ? local information: ? the word form of words that appear association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems near the target word in window of size 3?
</nextsent>
<nextsent>the part-of-speech (pos) tags that appear near the target word in window of size 3 ? the lexical form of the target word ? the pos tag of the target word ? broad context information: ? the lemmas of all words that appear in the provided context of target word (stop words are removed) in the case of broad context we use the bag-of-words representation with two weighting schema.
</nextsent>
<nextsent>binary weighting for rlsc-lin and term frequency weighting1 for rlsc-comb.for stemming we used porter stem mer (porter, 1980) and for tagging we used brill tagger (brill, 1995).<papid> J95-4004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2465">
<title id=" W04-0851.xml">regularized least squares classification for word sense disambiguation </title>
<section> knowledge sources and feature.  </section>
<citcontext>
<prevsection>
<prevsent>we used the following knowledge sources: ? local information: ? the word form of words that appear association for computational linguistics for the semantic analysis of text, barcelona, spain, july 2004 senseval-3: third international workshop on the evaluation of systems near the target word in window of size 3?
</prevsent>
<prevsent>the part-of-speech (pos) tags that appear near the target word in window of size 3 ? the lexical form of the target word ? the pos tag of the target word ? broad context information: ? the lemmas of all words that appear in the provided context of target word (stop words are removed) in the case of broad context we use the bag-of-words representation with two weighting schema.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
binary weighting for rlsc-lin and term frequency weighting1 for rlsc-comb.for stemming we used porter stem mer (porter, 1980) and for tagging we used brill tagger (brill, 1995).<papid> J95-4004 </papid></citsent>
<aftsection>
<nextsent>rlsc (rifkin, 2002; poggio and smale, 2003) is learning method that obtains solutions for binary classification problems via tikhonov regularization in reproducing kernel hilbert space using the square loss.
</nextsent>
<nextsent>let = (x1, y1), . . .
</nextsent>
<nextsent>, (xn, yn) be training sample with xi ? rd and yi ? {1, 1} for all i. the hypothesis space of rlsc is the set of functions : rd ? of the form: f(x) = ? i=1 cik(x,xi) with ci ? for all and : rd ? rd ? a kernel function (a symmetric positive definite function) that measures the similarity between two instances.rlsc tries to find function from this hypothesis space that simultaneously has small empirical error and small norm in reproducing kernel hilbert space generated by kernel k. the resulting minimization problem is: min fh 1 n ? i=1 (yi ? f(xi))2 + f2k inspite of the complex mathematical tools used, the resulted learning algorithm is very 1we didnt use any kind of smoothing.
</nextsent>
<nextsent>the weight of term is simply the number of time the term appears in the context divided by the length of the context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2471">
<title id=" W04-1311.xml">some tests of an unsupervised model of language acquisition </title>
<section> related computational and linguistic.  </section>
<citcontext>
<prevsection>
<prevsent>79
</prevsent>
<prevsent>formalisms and psycho linguistic findings unlike adios, very few existing algorithms for unsupervised language acquisition use raw, unannotated corpus data (as opposed, say, to sentences converted into sequences of pos tags).
</prevsent>
</prevsection>
<citsent citstr=" W02-2012 ">
the only work described in recent review (roberts and atwell, 2002) as completely unsupervised ? the grasp model (henrichsen, 2002) ? <papid> W02-2012 </papid>does attempt to induce syntax from raw transcribed speech, yet it is not completely data-driven in that it makes prior commitment to particular theory of syntax (cate gorial grammar, complete with pre-specified setof allowed categories).</citsent>
<aftsection>
<nextsent>because of the unique nature of our chosen challenge ? finding structure in language rather than imposing it ? the following brief survey of grammar induction focuses on contrasts and comparisons to approaches that generally stop short of attempting to do what our algorithm does.
</nextsent>
<nextsent>we distinguish between approaches that are motivated computationally (local grammar and variable order markov models, and tree adjoining grammar, discussed elsewhere (edelman etal., 2004), and those whose main motivation is linguistic and cognitive psychological (cognitive and construction grammars, discussed below).local grammar and markov models.
</nextsent>
<nextsent>in capturing the regularities inherent in multiple crisscrossing paths through corpus, adios superficially resembles finite-state local grammars (gross, 1997) and variable order markov (vom)models (guyon and pereira, 1995).
</nextsent>
<nextsent>the vom approach starts by postulating maximum-n structure, which is then fitted to the data by maximizing the likelihood of the training corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZA2472">
<title id=" W03-2120.xml">palinka a highly customisable tool for discourse annotation </title>
<section> case studies.  </section>
<citcontext>
<prevsection>
<prevsent>if the annotator needs to take break during the process, this can be indicated by pressing the pause button, in this way recording the actual time required by the annotation.
</prevsent>
<prevsent>the corpus annotated for automatic summarisation is part of the computer-aided summarisation tool (cast) project.2 5.3 annotating centering.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
centering theory (ct) characterises the local coherence of text on the basis of the discourse entities in text and the way in which they are introduced (grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>ct was developed and demonstrated on simple texts.
</nextsent>
<nextsent>in order to test if the theory holds for real texts and gain insights into how the theory can be applied to them, 60 news reports and encyclopedic texts were annotated by several annotators.
</nextsent>
<nextsent>the number of annotated texts may seem small, but given the difficulty of the annotation and the fact that six versions of centering theory were marked for each text, it is impossible to produce large corpora.
</nextsent>
<nextsent>in centering theory the discourse consists of sequence of utterances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>