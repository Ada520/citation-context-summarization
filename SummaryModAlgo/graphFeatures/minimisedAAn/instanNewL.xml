<paper>
<cited id="L0">
<title id=" I05-5009.xml">evaluating contextual dependency of paraphrases using a latent variable model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, it is not easy to evaluate whether two sentences carry almost the same meaning.
</prevsent>
<prevsent>some studies have constructed and evaluated hand-made rules (takahashi et al , 2001; ohtake and yamamoto, 2001).
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
others have tried to extract paraphrases from corpora (barzilay and mckeown, 2001; <papid> P01-1008 </papid>lin and pantel, 2001), which are very useful because they enable us to construct paraphrasing rules.</citsent>
<aftsection>
<nextsent>in addition, we can construct an example-based or statistical machine translation (smt)-like paraphrasing system that utilizes paraphrasing examples.
</nextsent>
<nextsent>thus, collecting paraphrased examples must be continued to achieve high-performance paraphrasing systems.
</nextsent>
<nextsent>several methods of acquiring paraphrases have been proposed (barzilay and mckeown, 2001; <papid> P01-1008 </papid>shimohata and sumita, 2002; yamamoto, 2002).<papid> W02-1411 </papid></nextsent>
<nextsent>some use parallel corpora as resources to obtain paraphrases, which seems promising way to extract high-quality paraphrases.however, unlike translation, there is no obvious paraphrasing direction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L4">
<title id=" I05-5009.xml">evaluating contextual dependency of paraphrases using a latent variable model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, we can construct an example-based or statistical machine translation (smt)-like paraphrasing system that utilizes paraphrasing examples.
</prevsent>
<prevsent>thus, collecting paraphrased examples must be continued to achieve high-performance paraphrasing systems.
</prevsent>
</prevsection>
<citsent citstr=" W02-1411 ">
several methods of acquiring paraphrases have been proposed (barzilay and mckeown, 2001; <papid> P01-1008 </papid>shimohata and sumita, 2002; yamamoto, 2002).<papid> W02-1411 </papid></citsent>
<aftsection>
<nextsent>some use parallel corpora as resources to obtain paraphrases, which seems promising way to extract high-quality paraphrases.however, unlike translation, there is no obvious paraphrasing direction.
</nextsent>
<nextsent>given paraphrasing pair e1:e2, we have to know the paraphrasing direction to paraphrase from e1 to e2 and vice versa.
</nextsent>
<nextsent>when extracting paraphrasing pairs from corpora, whether the paraphrasing pairs are con 65textually dependent paraphrases is serious problem, and thus there is specific paraphrase direction for each pair.
</nextsent>
<nextsent>in addition, it is also important to evaluate paraphrasing pair not only when extracting but also when applying paraphrase.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L5">
<title id=" I05-3020.xml">report to bmmbased chinese word segment or with context based unknown word identifier for the second international chinese word segmentation bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the research fields of chinese natural language processing (nlp), high-performance chinese word segment or (cws) is useful preprocessing stage to produce an intermediate result for later processes, such as search engines, text mining and speech recognition, etc. the bottleneck of developing high-performance cws is to comprise of high-performance chinese uwi (lin et al 1993; tsai et al 2003).
</prevsent>
<prevsent>it is because chinese is written without any separation between words and meanwhile more than 50% words of the chinese texts in web corpus are out-of-vocabulary (tsai et al 2003).
</prevsent>
</prevsection>
<citsent citstr=" C96-1035 ">
conventionally, there are four approaches to develop cws: (1) dictionary-based approach (cheng et al 1999), especial forward and backward maximum matching (wong and chan, 1996); (<papid> C96-1035 </papid>2) linguistic approach based on syntax semantic knowledge (chen et al 2002); (3) statistical approach based on statistical language model (slm) (sproat and shih, 1990; teahan et al. 2000; <papid> J00-3004 </papid>gao et al 2003); <papid> P03-1035 </papid>and (4) hybrid approach trying to combine the benefits of diction ary-based, linguistic and statistical approaches (tsai et al 2003; ma and chen, 2003).<papid> W03-1726 </papid></citsent>
<aftsection>
<nextsent>in practice, statistical approaches are most widely used because their effective and reasonable performance.
</nextsent>
<nextsent>for cws, there are two types of word segmentation ambiguities while there are no unknown words in them: (1) overlap ambiguity(oa), take character string abc as an example.
</nextsent>
<nextsent>if its segmentation can be either ab/c or a/bc depending on different context, the abc is called an overlap ambiguity string (oas), such as ???(a general)/?(use)?
</nextsent>
<nextsent>and ??(to get)/??(for military use)?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L6">
<title id=" I05-3020.xml">report to bmmbased chinese word segment or with context based unknown word identifier for the second international chinese word segmentation bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the research fields of chinese natural language processing (nlp), high-performance chinese word segment or (cws) is useful preprocessing stage to produce an intermediate result for later processes, such as search engines, text mining and speech recognition, etc. the bottleneck of developing high-performance cws is to comprise of high-performance chinese uwi (lin et al 1993; tsai et al 2003).
</prevsent>
<prevsent>it is because chinese is written without any separation between words and meanwhile more than 50% words of the chinese texts in web corpus are out-of-vocabulary (tsai et al 2003).
</prevsent>
</prevsection>
<citsent citstr=" J00-3004 ">
conventionally, there are four approaches to develop cws: (1) dictionary-based approach (cheng et al 1999), especial forward and backward maximum matching (wong and chan, 1996); (<papid> C96-1035 </papid>2) linguistic approach based on syntax semantic knowledge (chen et al 2002); (3) statistical approach based on statistical language model (slm) (sproat and shih, 1990; teahan et al. 2000; <papid> J00-3004 </papid>gao et al 2003); <papid> P03-1035 </papid>and (4) hybrid approach trying to combine the benefits of diction ary-based, linguistic and statistical approaches (tsai et al 2003; ma and chen, 2003).<papid> W03-1726 </papid></citsent>
<aftsection>
<nextsent>in practice, statistical approaches are most widely used because their effective and reasonable performance.
</nextsent>
<nextsent>for cws, there are two types of word segmentation ambiguities while there are no unknown words in them: (1) overlap ambiguity(oa), take character string abc as an example.
</nextsent>
<nextsent>if its segmentation can be either ab/c or a/bc depending on different context, the abc is called an overlap ambiguity string (oas), such as ???(a general)/?(use)?
</nextsent>
<nextsent>and ??(to get)/??(for military use)?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L7">
<title id=" I05-3020.xml">report to bmmbased chinese word segment or with context based unknown word identifier for the second international chinese word segmentation bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the research fields of chinese natural language processing (nlp), high-performance chinese word segment or (cws) is useful preprocessing stage to produce an intermediate result for later processes, such as search engines, text mining and speech recognition, etc. the bottleneck of developing high-performance cws is to comprise of high-performance chinese uwi (lin et al 1993; tsai et al 2003).
</prevsent>
<prevsent>it is because chinese is written without any separation between words and meanwhile more than 50% words of the chinese texts in web corpus are out-of-vocabulary (tsai et al 2003).
</prevsent>
</prevsection>
<citsent citstr=" P03-1035 ">
conventionally, there are four approaches to develop cws: (1) dictionary-based approach (cheng et al 1999), especial forward and backward maximum matching (wong and chan, 1996); (<papid> C96-1035 </papid>2) linguistic approach based on syntax semantic knowledge (chen et al 2002); (3) statistical approach based on statistical language model (slm) (sproat and shih, 1990; teahan et al. 2000; <papid> J00-3004 </papid>gao et al 2003); <papid> P03-1035 </papid>and (4) hybrid approach trying to combine the benefits of diction ary-based, linguistic and statistical approaches (tsai et al 2003; ma and chen, 2003).<papid> W03-1726 </papid></citsent>
<aftsection>
<nextsent>in practice, statistical approaches are most widely used because their effective and reasonable performance.
</nextsent>
<nextsent>for cws, there are two types of word segmentation ambiguities while there are no unknown words in them: (1) overlap ambiguity(oa), take character string abc as an example.
</nextsent>
<nextsent>if its segmentation can be either ab/c or a/bc depending on different context, the abc is called an overlap ambiguity string (oas), such as ???(a general)/?(use)?
</nextsent>
<nextsent>and ??(to get)/??(for military use)?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L8">
<title id=" I05-3020.xml">report to bmmbased chinese word segment or with context based unknown word identifier for the second international chinese word segmentation bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the research fields of chinese natural language processing (nlp), high-performance chinese word segment or (cws) is useful preprocessing stage to produce an intermediate result for later processes, such as search engines, text mining and speech recognition, etc. the bottleneck of developing high-performance cws is to comprise of high-performance chinese uwi (lin et al 1993; tsai et al 2003).
</prevsent>
<prevsent>it is because chinese is written without any separation between words and meanwhile more than 50% words of the chinese texts in web corpus are out-of-vocabulary (tsai et al 2003).
</prevsent>
</prevsection>
<citsent citstr=" W03-1726 ">
conventionally, there are four approaches to develop cws: (1) dictionary-based approach (cheng et al 1999), especial forward and backward maximum matching (wong and chan, 1996); (<papid> C96-1035 </papid>2) linguistic approach based on syntax semantic knowledge (chen et al 2002); (3) statistical approach based on statistical language model (slm) (sproat and shih, 1990; teahan et al. 2000; <papid> J00-3004 </papid>gao et al 2003); <papid> P03-1035 </papid>and (4) hybrid approach trying to combine the benefits of diction ary-based, linguistic and statistical approaches (tsai et al 2003; ma and chen, 2003).<papid> W03-1726 </papid></citsent>
<aftsection>
<nextsent>in practice, statistical approaches are most widely used because their effective and reasonable performance.
</nextsent>
<nextsent>for cws, there are two types of word segmentation ambiguities while there are no unknown words in them: (1) overlap ambiguity(oa), take character string abc as an example.
</nextsent>
<nextsent>if its segmentation can be either ab/c or a/bc depending on different context, the abc is called an overlap ambiguity string (oas), such as ???(a general)/?(use)?
</nextsent>
<nextsent>and ??(to get)/??(for military use)?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L9">
<title id=" I05-3020.xml">report to bmmbased chinese word segment or with context based unknown word identifier for the second international chinese word segmentation bakeoff </title>
<section> conclusions and future directions.  </section>
<citcontext>
<prevsection>
<prevsent>(3)for cws, there are two critical and probably independent tasks: the optimization of luw-eiw tradeoff and the detection and disambiguation of oas and cas error segmentation.
</prevsent>
<prevsent>we believe the former task is more critical than the later one.
</prevsent>
</prevsection>
<citsent citstr=" I05-6003 ">
(4)we will continue to expand our cws with other linguistic knowledge (such as part-of speech information and morphology) and btm model (tsai 2005) <papid> I05-6003 </papid>to improve our bmm-based cws for attending the third international chinese word segmentation bakeoff in both closed and open testing tracks.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L10">
<title id=" I05-2033.xml">pos tagger combinations on hungarian text </title>
<section> introduction an related works.  </section>
<citcontext>
<prevsection>
<prevsent>1.1 pos tagging of hungarian texts.
</prevsent>
<prevsent>standard pos tagging methods were applied to hungarian as soon as the first annotated corpora appeared that were big enough toserve as training database for various methods.
</prevsent>
</prevsection>
<citsent citstr=" P98-1050 ">
the telri corpus (dimitrova et al, 1998) <papid> P98-1050 </papid>was the first corpus that was used for testing different pos tagging methods.</citsent>
<aftsection>
<nextsent>this corpus contains approximately 80, 000words.
</nextsent>
<nextsent>later, as the hungarian national corpus (varadi, 2002) and the manually annotated hungarian corpus (the szeged corpus)(alexin et al, 2003) <papid> E03-1012 </papid>became available, an opportunity was provided to test the results on bigger corpora (153m and 1.2m words, re spectively).in recent years several authors have published many useful pos tagging results inhungarian.</nextsent>
<nextsent>it is generally believed that, ow 191ing to the fairly free word order and the agglutinative property of the hungarian language, there are more special problems associated with hungarian than those of theindo-european languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L11">
<title id=" I05-2033.xml">pos tagger combinations on hungarian text </title>
<section> introduction an related works.  </section>
<citcontext>
<prevsection>
<prevsent>the telri corpus (dimitrova et al, 1998) <papid> P98-1050 </papid>was the first corpus that was used for testing different pos tagging methods.</prevsent>
<prevsent>this corpus contains approximately 80, 000words.</prevsent>
</prevsection>
<citsent citstr=" E03-1012 ">
later, as the hungarian national corpus (varadi, 2002) and the manually annotated hungarian corpus (the szeged corpus)(alexin et al, 2003) <papid> E03-1012 </papid>became available, an opportunity was provided to test the results on bigger corpora (153m and 1.2m words, re spectively).in recent years several authors have published many useful pos tagging results inhungarian.</citsent>
<aftsection>
<nextsent>it is generally believed that, ow 191ing to the fairly free word order and the agglutinative property of the hungarian language, there are more special problems associated with hungarian than those of theindo-european languages.
</nextsent>
<nextsent>however, the latest results are comparable to results achieved in english and other well-studied languages.fruitful approaches for hungarian pos tagging are hidden markov models, transformation based learning and rule-based learning methods.one of the most common pos tagging approaches is to build tagger based on hidden markov models (hmm).
</nextsent>
<nextsent>tufis (tufis etal., 2000) reported good results with the trigrams and tags (tnt) tagger (brants, 2000).<papid> A00-1031 </papid></nextsent>
<nextsent>a slightly better version of tnt was employed by oravecz (oravecz and dienes, 2002), andit achieved excellent results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L12">
<title id=" I05-2033.xml">pos tagger combinations on hungarian text </title>
<section> introduction an related works.  </section>
<citcontext>
<prevsection>
<prevsent>it is generally believed that, ow 191ing to the fairly free word order and the agglutinative property of the hungarian language, there are more special problems associated with hungarian than those of theindo-european languages.
</prevsent>
<prevsent>however, the latest results are comparable to results achieved in english and other well-studied languages.fruitful approaches for hungarian pos tagging are hidden markov models, transformation based learning and rule-based learning methods.one of the most common pos tagging approaches is to build tagger based on hidden markov models (hmm).
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
tufis (tufis etal., 2000) reported good results with the trigrams and tags (tnt) tagger (brants, 2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>a slightly better version of tnt was employed by oravecz (oravecz and dienes, 2002), andit achieved excellent results.
</nextsent>
<nextsent>in their paper, oravecz and dienes (oravecz and dienes,2002) argue that regardless of the rich morphology and relatively free word order, thepos tagging of hungarian with hmm methods is possible and effective once one is able to handle the data sparsity problem.
</nextsent>
<nextsent>they used modified version of tnt that was supported by an external morphological analyzer.
</nextsent>
<nextsent>in this way the trigram tagger was able to make better guesses about the unseen word sand therefore to get better results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L13">
<title id=" I05-2033.xml">pos tagger combinations on hungarian text </title>
<section> the tbl tagger.  </section>
<citcontext>
<prevsection>
<prevsent>however, in the different test sets, different combinations proved the best, so no conclusion could be drawn about the best combination.
</prevsent>
<prevsent>the combined tagger that performed best on the largest test set is shown in row 7 of table 1.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
transformation based learning (tbl) was introduced by brill (brill, 1995) <papid> J95-4004 </papid>for the taskof pos tagging.</citsent>
<aftsection>
<nextsent>brills implementation consists of two processing steps.
</nextsent>
<nextsent>in the first step, lexical tagger calculates the pos tags based on lexical information only (word forms).
</nextsent>
<nextsent>the result of the lexical tagger is used as first guess in the second run where both the word forms and the actual pos tags are applied bythe contextual tagger.
</nextsent>
<nextsent>both lexical and contextual taggers make use of the tbl concept.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L14">
<title id=" I05-2033.xml">pos tagger combinations on hungarian text </title>
<section> the tbl tagger.  </section>
<citcontext>
<prevsection>
<prevsent>the first guess of the tbl tagger is the result of the baseline tagger.
</prevsent>
<prevsent>for the second run, the contextual tagger implementation we used is based onthe fntbl learner module.
</prevsent>
</prevsection>
<citsent citstr=" N01-1006 ">
(ngai and florian, 2001) <papid> N01-1006 </papid>we used the standard parameter settings included in the fntbl package.</citsent>
<aftsection>
<nextsent>2.1 baseline tagger.
</nextsent>
<nextsent>the baseline tagger relies on an external morphological analyzer1 to get the list of possible pos tags.
</nextsent>
<nextsent>if the word occurs in the training data, the word gets its most frequent postag in the training.
</nextsent>
<nextsent>if the word does not appear in the training, but representatives of its ambiguity class (words with the same possible pos tags) are present, then the most frequent tag of all these words will be selected.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L15">
<title id=" I05-2023.xml">improvededitdistance kernel for chinese relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>usually, we can regard re as classification problem.
</prevsent>
<prevsent>all particular entity pairs are found from text and then decided whether they are relation which we need or not.at the beginning, number of manually engineered systems were developed for re problem (aone and ramos-santacruz, 2000).
</prevsent>
</prevsection>
<citsent citstr=" M98-1009 ">
the automatic learning methods (miller et al, 1998;<papid> M98-1009 </papid>soderland, 1999) are not necessary to have some one on hand with detailed knowledge of how there system works, or how to write rules for it.usually, the machine learning method represents the nlp objects as feature vectors in the feature extraction step.</citsent>
<aftsection>
<nextsent>the methods are named feature-based learning methods.
</nextsent>
<nextsent>but in many cases, data cannot be easily represented explicitly via feature vectors.
</nextsent>
<nextsent>for example, in most nlp problems, the feature-based representations produce inherently local representations of objects, for it is computationally infeasible to generate features involving long-range dependencies.
</nextsent>
<nextsent>on the other hand, finding the suitable features of aparticular problem is heuristic work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L16">
<title id=" I08-2116.xml">multi label text categorization with model combination based on f1score maximization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical classifiers such as the logistic regression model (lrm), the support vector machine (svm),and naive bayes are employed as binary classifiers (joachims, 1998).
</prevsent>
<prevsent>in text categorization, the 1 -score is often usedto evaluate classifier performance.
</prevsent>
</prevsection>
<citsent citstr=" H05-1087 ">
recently, methods for training binary classifiers to maximize the 1 -score have been proposed for svm (joachims,2005) and lrm (jansche, 2005).<papid> H05-1087 </papid></citsent>
<aftsection>
<nextsent>it was confirmed experimentally that these training methods were more effective for obtaining binary classifiers with better 1 -score performance than the minimum error rate and maximum likelihood used for training conventional classifiers, especially when there was large imbalance between positive and negative samples.
</nextsent>
<nextsent>in multi-label categorization, macro and micro-averaged 1-scores are often used to evaluate classification performance.
</nextsent>
<nextsent>therefore, we can expect to improve multi-label classification performance by using binary classifiers trained to maximize the 1 -score.
</nextsent>
<nextsent>on the other hand, classification frameworks based on classifier combination have also been studied in many previous works such as (wolpert, 1992; larkey and croft, 1996; ting and witten, 1999; ghahramani and kim, 2003; bell et al, 2005;fumera and roli, 2005), to provide better classifier systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L30">
<title id=" I08-2082.xml">a web based english proofing system for english as a second language users </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>proofing technology for native speakers of english has been focus of work for decades, and some tools like spell checkers and grammar checkers have become standard features of document processing software products.
</prevsent>
<prevsent>however, designing an english proofing system for english as second language(esl) users presents major challenge: esl writing errors vary greatly among users with different language backgrounds and proficiency levels.
</prevsent>
</prevsection>
<citsent citstr=" P06-1032 ">
recent work by brockett et al  (2006) <papid> P06-1032 </papid>utilized phrasal statistical machine translation (smt) techniques to correct esl writing errors and demonstrated that this data-intensive smt approach is very promising, but they also pointed out smt approach relies on the availability of large amount of training data.</citsent>
<aftsection>
<nextsent>the expense and difficulty of collecting large quantities of search phrase google.com live.com yahoo.com english as second language 306,000 52,407 386,000 english as second language 1,490,000 38,336,308 4,250,000 table 1: web hits for phrasal usages raw and edited esl prose pose an obstacle to this approach.
</nextsent>
<nextsent>in this work we consider the prospect of using the web, with its billions of web pages, as data source with the potential to aid esl writers.
</nextsent>
<nextsent>our research is motivated by the observation that esl users already use the web as corpus of gooden glish, often using search engines to decide whethera particular spelling, phrase, or syntactic construction is consistent with usage found on the web.
</nextsent>
<nextsent>for example, unsure whether the native-sounding phrase includes the determiner a?, user might search for both quoted strings english as second language?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L31">
<title id=" I08-2082.xml">a web based english proofing system for english as a second language users </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our findings are consistent with previous research results on esl writing errors in two respects: 1.
</prevsent>
<prevsent>esl users have significantly more problems.
</prevsent>
</prevsection>
<citsent citstr=" P98-2196 ">
with determiner usage than native speakers be cause the use and omission of definite and indefinite articles varies across different languages (schneider and mccoy, 1998)(<papid> P98-2196 </papid>lonsdale and strong-krause, 2003).</citsent>
<aftsection>
<nextsent>users, and collocational knowledge contributes to the difference between native speakers and esl learners (shei and pain, 2000): in clec, real-world chinese english learner corpus(gui and yang, 2003), about 30% of esl writing errors involve different types of collocation errors.
</nextsent>
<nextsent>in the remainder of the paper, we focus on proofing determiner usage and vnc errors.
</nextsent>
<nextsent>2 related work.
</nextsent>
<nextsent>researchers have recently proposed some successful learning-based approaches for the determiner selection task (minnen et al , 2000), <papid> W00-0708 </papid>but most of this work has aimed only at helping native english users correct typographical errors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L32">
<title id=" I08-2082.xml">a web based english proofing system for english as a second language users </title>
<section> collocation errors are common among esl.  </section>
<citcontext>
<prevsection>
<prevsent>in the remainder of the paper, we focus on proofing determiner usage and vnc errors.
</prevsent>
<prevsent>2 related work.
</prevsent>
</prevsection>
<citsent citstr=" W00-0708 ">
researchers have recently proposed some successful learning-based approaches for the determiner selection task (minnen et al , 2000), <papid> W00-0708 </papid>but most of this work has aimed only at helping native english users correct typographical errors.</citsent>
<aftsection>
<nextsent>gamon et al (2008) recently addressed the challenging task of proofing writing errors for esl users: they propose combining contextual speller techniques and language modeling for proofing several types of esl errors, and demonstrate some promising results.
</nextsent>
<nextsent>in departure from this work, our system directly uses web data for the esl error proofing task.
</nextsent>
<nextsent>there is small body of previous work on the use of online systems aimed at helping esl learners correct collocation errors.
</nextsent>
<nextsent>in shei and pains system (2000), for instance, the british national corpus (bnc) is used to extract english collocations, and an esl learner writing corpus is then used tobuild collocation error library.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L33">
<title id=" I08-1049.xml">multi view co training of transliteration model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applying this algorithm to transliteration extraction, the results show that it not only circumvents the need of data labeling, but also achieves performance close to that of supervised learning, where manual labeling is required for all training samples.
</prevsent>
<prevsent>named entities are important content words in text documents.
</prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
in many applications, such as cross language information retrieval (meng et al, 2001; virga and khudanpur, 2003) and machine translation (knight and graehl, 1998; <papid> J98-4003 </papid>chen et al, 2006), one of the fundamental tasks is to identify these words.</citsent>
<aftsection>
<nextsent>imported foreign proper names constitute good portion of such words, which are newly translated into chinese by transliteration.
</nextsent>
<nextsent>transliteration is process of translating foreign word into the native language by preserving its pronunciation in the original language, otherwise known as translation-by-sound.
</nextsent>
<nextsent>as new words emerge everyday, no lexicon is able to cover all transliterations.
</nextsent>
<nextsent>it is desirable to find ways to harvest transliterations from real world corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L37">
<title id=" I08-1049.xml">multi view co training of transliteration model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the idea is to transform both source and target words into comparable phonemes so that the phonetic similarity between two words can be measured easily.
</prevsent>
<prevsent>recently the grapheme-based approach has attracted much attention.
</prevsent>
</prevsection>
<citsent citstr=" P04-1021 ">
it was proposed by jeong et al (1999), li et al (2004) <papid> P04-1021 </papid>and many others (oh et al, 2006b), which is also known as direct orthography mapping.</citsent>
<aftsection>
<nextsent>it treats the transliteration as statistical machine translation problem under monotonic constraint.
</nextsent>
<nextsent>the idea is to obtain the bilingual ortho graphical correspondence directly to reduce the possible errors introduced in multiple conversions.
</nextsent>
<nextsent>however, the grapheme-based transliteration model has more parameters than phoneme-based one does, thus expects larger training corpus.
</nextsent>
<nextsent>most of the reported works have been focused on either phoneme- or grapheme-based approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L38">
<title id=" I08-1049.xml">multi view co training of transliteration model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the feature fusion was shown to be effective.
</prevsent>
<prevsent>however, their methods hinge on the availability of labeled bilingual corpus.
</prevsent>
</prevsection>
<citsent citstr=" P06-1010 ">
in transliteration extraction, mining translations or transliterations from the ever-growing multilingual web has become an active research topic, for example, by exploring query logs (brill et al., 2001) and parallel (nie et al, 1999) or comparable corpora (sproat et al, 2006).<papid> P06-1010 </papid></citsent>
<aftsection>
<nextsent>transliterations in such live corpus are typically unlabeled.
</nextsent>
<nextsent>for model-based transliteration extraction, recent progress in machine learning offers different options to exploit unlabeled data, that include active learning (lewis and catlett, 1994) and co-training (nigam and ghani, 2000; tr et al 2005).
</nextsent>
<nextsent>taking the prior work step forward, this paper explores new way of fusing phoneme and grapheme features through multi-view co training algorithm (blum and mitchell, 1998), which starts with small number of labeled data to bootstrap transliteration model to automatically harvest transliterations from the web.
</nextsent>
<nextsent>multiple views machine transliteration can be formulated as generative process, which takes character string in source language as input and generates character string in the target language as output.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L39">
<title id=" I08-1049.xml">multi view co training of transliteration model </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>(8) where iw is the weight of ith learner ( | )ip ew cw , which can be learnt by using development corpus.
</prevsent>
<prevsent>to validate the effectiveness of the learning framework, we conduct series of experiments in transliteration extraction on development corpus described later.
</prevsent>
</prevsection>
<citsent citstr=" P06-1142 ">
first, we repeat the experiment in (kuo et al, 2006) <papid> P06-1142 </papid>to train psm using psa and gsa feature fusion in supervised manner, which serves as the upper bound of co-training or co-em system performance.</citsent>
<aftsection>
<nextsent>we then train the psms with single view v1, v2, v3 and v4 alone in an unsupervised manner.
</nextsent>
<nextsent>the performance achieved by each view alone can be considered as the baseline for multi-view benchmarking.
</nextsent>
<nextsent>then, we run two-view co-training for different combinations of views on the same development corpus.
</nextsent>
<nextsent>we expect to see positive effects with the multi-view training.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L48">
<title id=" I05-2029.xml">multimodal question answering questions without keyboards </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 multimodal interfaces.
</prevsent>
<prevsent>a large body of work has been created on multimodal interfaces ? combining multiple modes of interaction so that the advantages of one mode offset the limitations of another.
</prevsent>
</prevsection>
<citsent citstr=" C86-1085 ">
in the specific case of combining natural language and graphics, there have been two main areas of study: interacting with graphical elements to resolve ambiguous references on the natural language side (bolt, 1980; kobsa et al, 1986); <papid> C86-1085 </papid>and generating coordinated text and graphic presentations using information from knowledge base (andr?</citsent>
<aftsection>
<nextsent>and rist (1994); towns et al (1998)).
</nextsent>
<nextsent>in addition to these two main areas, early work by tennant (1983) experimented with using predictive left-corner parser to populate dynamic menus that the user would navigate to construct queries that were guaranteed to be correct and task-relevant.
</nextsent>
<nextsent>our work contains elements from all of these categories in that we use input gestures to resolve reference ambiguity and we make use of kb to coordinate the linguistic and graphical information.
</nextsent>
<nextsent>we were also inspired by tennants work on restricting the players input to avoid parsing problems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L49">
<title id=" I05-2029.xml">multimodal question answering questions without keyboards </title>
<section> interacting with virtual photos.  </section>
<citcontext>
<prevsection>
<prevsent>in our implementation, the player interacts with the npc by clicking on an object in the photo to pull up menu of context-dependent natural language queries.
</prevsent>
<prevsent>when the player selects an item from this menu, the query is sent to the npc that the player is currently talking to?.
</prevsent>
</prevsection>
<citsent citstr=" E87-1030 ">
this menu of context sensitive queries is crucial to the interaction because pointing gesture without an accompanying description is ambiguous (schmauks, 1987) <papid> E87-1030 </papid>and it is through this menu selection that the player expresses intent and restricts the scope of the dialog.</citsent>
<aftsection>
<nextsent>there are two obvious benefits to approaching the qa interaction in this way.
</nextsent>
<nextsent>first, even though the topic is limited by the objects in the photo, the player is given control over the direction of the dialog.
</nextsent>
<nextsent>this is an improvement over the traditional scripted npc interaction where the player has little control over the dialog.
</nextsent>
<nextsent>the other benefit is that while the player is given control over the content, the player is not granted too much control since the photo metaphor limits the topic to things that are relevant to the game.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L50">
<title id=" I08-1036.xml">mining the web for relations between digital devices using a probabilistic maximum margin model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there has been spate of work on relation extraction in recent years.
</prevsent>
<prevsent>however, many papers actually address the task of role extraction: (usually two) entities are identified and the relationship is implied by the co-occurrence of these entities or by some linguistic expression (agichtein and gravano, 2000; zelenko et al, 2003).several papers propose the use of machine learning models and probabilistic models for relation extraction: nave bayes for the relation sub cellular location in the bio-medical domain (craven, 1999) or for person-affiliation and organization-location (zelenko et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" H05-1092 ">
rosario and hearst (2005) <papid> H05-1092 </papid>have used more complicated dynamic graphical model to identify interaction types between proteins and to simultaneously extract the proteins.</citsent>
<aftsection>
<nextsent>2.2 maximum margin models.
</nextsent>
<nextsent>probabilistic graphical models and different approaches to training them have received lot of attention in application to natural language processing.
</nextsent>
<nextsent>mccallum and nigam (1998) showed that nave bayes can be very accurate model for text categorization.
</nextsent>
<nextsent>since probabilistic graphical models represent joint probability distributions whereas classification focuses on the conditional probability, there has been debate regarding the objective that should be maximized in order to train these models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L51">
<title id=" I08-1036.xml">mining the web for relations between digital devices using a probabilistic maximum margin model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>since probabilistic graphical models represent joint probability distributions whereas classification focuses on the conditional probability, there has been debate regarding the objective that should be maximized in order to train these models.
</prevsent>
<prevsent>ng and jordan (2001) have compared joint likelihood 274model (nave bayes) and its discriminative counterpart (logistic regression), and they have shown that while for large number of examples logistic regression has lower error rate, nave bayes often outperforms logistic regression for smaller datasets.
</prevsent>
</prevsection>
<citsent citstr=" W02-1002 ">
however, klein and manning (2002) <papid> W02-1002 </papid>showed thatfor natural language and text processing tasks, conditional models are usually better than joint likelihood models.</citsent>
<aftsection>
<nextsent>yakhnenko et al (2005) also showed that conditional models suffer from over fitting in text and sequence structured domains.
</nextsent>
<nextsent>in recent years, the interest in learning parameters of probabilistic models by maximizing the probabilistic margin has developed.
</nextsent>
<nextsent>taskar et al (2003)have solved the problem of learning markov networks (undirected graphs) by maximizing the margin.
</nextsent>
<nextsent>their work has focused on likelihood based structured classification where the goal is to assign class to each word in the sentence or document.guo et al (2005) have proposed solution to learning parameters of the maximum margin bayesian networks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L52">
<title id=" I05-2025.xml">investigating the features that affect cue usage of nonnative speakers of english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>section 8 draws conclusion.
</prevsent>
<prevsent>almost all researches on cue phrases have been done for native speakers.
</prevsent>
</prevsection>
<citsent citstr=" C90-3018 ">
(elhadad and mckeown, 1990) <papid> C90-3018 </papid>explored the problem of cue selec tion.</citsent>
<aftsection>
<nextsent>they presented model that distinguishes small set of similar cue phrases.
</nextsent>
<nextsent>(moser and moore, 1995<papid> P95-1018 </papid>a) put forward method to identify the features that predict cue selection and place ment.</nextsent>
<nextsent>(eugenio and moore and paolucci, 1997)used c4.5 to predict cue occurrence and place ment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L53">
<title id=" I05-2025.xml">investigating the features that affect cue usage of nonnative speakers of english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(elhadad and mckeown, 1990) <papid> C90-3018 </papid>explored the problem of cue selec tion.</prevsent>
<prevsent>they presented model that distinguishes small set of similar cue phrases.</prevsent>
</prevsection>
<citsent citstr=" P95-1018 ">
(moser and moore, 1995<papid> P95-1018 </papid>a) put forward method to identify the features that predict cue selection and place ment.</citsent>
<aftsection>
<nextsent>(eugenio and moore and paolucci, 1997)used c4.5 to predict cue occurrence and placement.
</nextsent>
<nextsent>until now, the research similar to ours isthe girl system (williams, 2004) which generates texts for poor readers and good readers of native speakers.
</nextsent>
<nextsent>the author measured the differences of reading speed (especially cue phrases) between good readers and bad readers, by which they inferred how discourse level choice (e.g., cue selection) makes the difference for the two kinds of readers.
</nextsent>
<nextsent>we used two corpora (sub-bnc and cnnse) to investigate difference in cue usage between native and non-native speakers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L57">
<title id=" I05-2025.xml">investigating the features that affect cue usage of nonnative speakers of english </title>
<section> annotating two corpora.  </section>
<citcontext>
<prevsection>
<prevsent>explanation: evidence, explanation argumentative, reason 10.
</prevsent>
<prevsent>summary: summary, restatement annotation includes two stages: first, we allowed two coders to choose explanation?
</prevsent>
</prevsection>
<citsent citstr=" J93-3003 ">
relations signaled by because using (hirschberg and litman, 1993)<papid> J93-3003 </papid>s 3-way classification.</citsent>
<aftsection>
<nextsent>the word because could signal not only explanation?
</nextsent>
<nextsent>relation, but other relations.
</nextsent>
<nextsent>on the other hand, we do not consider some structures, e.g., not because ... but because?.
</nextsent>
<nextsent>thus, because could be judged as explanation?, other?, or not considered?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L70">
<title id=" I05-1017.xml">ppattachment disambiguation boosted by a gigantic volume of unambiguous examples </title>
<section> experiments and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>precision and recall for each attachment site (lex+pos+lp?
</prevsent>
<prevsent>model) class precision recall 1067/1258 (84.82%) 1067/1271 (83.95%) 1635/1839 (88.91%) 1635/1826 (89.54%) table 5.
</prevsent>
</prevsection>
<citsent citstr=" C94-2195 ">
pp-attachment accuracies of previous work method accuracy our method svm 87.25% supervised ratnaphakhi et al, 1994 me 81.6% brill and resnik, 1994 <papid> C94-2195 </papid>tbl 81.9% collins and brooks, 1995 <papid> W95-0103 </papid>back-off 84.5% zavrel et al, 1997 <papid> W97-1016 </papid>nn 84.4% stetina and nagao, 1997 <papid> W97-0109 </papid>dt 88.1% abney et al, 1999 <papid> W99-0606 </papid>boosting 84.6% vanschoenwinkel and man derick, 2003 svm 84.8% zhao and lin, 2004 nn 86.5% unsupervised ratnaparkhi, 1998 - <papid> P98-2177 </papid>81.9% pantel and lin, 2000 - <papid> P00-1014 </papid>84.3% me: maximum entropy, tbl: transformation-based learning, dt: decision tree, nn: nearest neighbor configurations (mcnemars test;   0.05).</citsent>
<aftsection>
<nextsent>lex+pos?
</nextsent>
<nextsent>model was little worse than lex?, but lex+pos+lp?
</nextsent>
<nextsent>was better than lex+lp?
</nextsent>
<nextsent>(and also pos+lp?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L71">
<title id=" I05-1017.xml">ppattachment disambiguation boosted by a gigantic volume of unambiguous examples </title>
<section> experiments and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>precision and recall for each attachment site (lex+pos+lp?
</prevsent>
<prevsent>model) class precision recall 1067/1258 (84.82%) 1067/1271 (83.95%) 1635/1839 (88.91%) 1635/1826 (89.54%) table 5.
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
pp-attachment accuracies of previous work method accuracy our method svm 87.25% supervised ratnaphakhi et al, 1994 me 81.6% brill and resnik, 1994 <papid> C94-2195 </papid>tbl 81.9% collins and brooks, 1995 <papid> W95-0103 </papid>back-off 84.5% zavrel et al, 1997 <papid> W97-1016 </papid>nn 84.4% stetina and nagao, 1997 <papid> W97-0109 </papid>dt 88.1% abney et al, 1999 <papid> W99-0606 </papid>boosting 84.6% vanschoenwinkel and man derick, 2003 svm 84.8% zhao and lin, 2004 nn 86.5% unsupervised ratnaparkhi, 1998 - <papid> P98-2177 </papid>81.9% pantel and lin, 2000 - <papid> P00-1014 </papid>84.3% me: maximum entropy, tbl: transformation-based learning, dt: decision tree, nn: nearest neighbor configurations (mcnemars test;   0.05).</citsent>
<aftsection>
<nextsent>lex+pos?
</nextsent>
<nextsent>model was little worse than lex?, but lex+pos+lp?
</nextsent>
<nextsent>was better than lex+lp?
</nextsent>
<nextsent>(and also pos+lp?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L72">
<title id=" I05-1017.xml">ppattachment disambiguation boosted by a gigantic volume of unambiguous examples </title>
<section> experiments and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>precision and recall for each attachment site (lex+pos+lp?
</prevsent>
<prevsent>model) class precision recall 1067/1258 (84.82%) 1067/1271 (83.95%) 1635/1839 (88.91%) 1635/1826 (89.54%) table 5.
</prevsent>
</prevsection>
<citsent citstr=" W97-1016 ">
pp-attachment accuracies of previous work method accuracy our method svm 87.25% supervised ratnaphakhi et al, 1994 me 81.6% brill and resnik, 1994 <papid> C94-2195 </papid>tbl 81.9% collins and brooks, 1995 <papid> W95-0103 </papid>back-off 84.5% zavrel et al, 1997 <papid> W97-1016 </papid>nn 84.4% stetina and nagao, 1997 <papid> W97-0109 </papid>dt 88.1% abney et al, 1999 <papid> W99-0606 </papid>boosting 84.6% vanschoenwinkel and man derick, 2003 svm 84.8% zhao and lin, 2004 nn 86.5% unsupervised ratnaparkhi, 1998 - <papid> P98-2177 </papid>81.9% pantel and lin, 2000 - <papid> P00-1014 </papid>84.3% me: maximum entropy, tbl: transformation-based learning, dt: decision tree, nn: nearest neighbor configurations (mcnemars test;   0.05).</citsent>
<aftsection>
<nextsent>lex+pos?
</nextsent>
<nextsent>model was little worse than lex?, but lex+pos+lp?
</nextsent>
<nextsent>was better than lex+lp?
</nextsent>
<nextsent>(and also pos+lp?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L73">
<title id=" I05-1017.xml">ppattachment disambiguation boosted by a gigantic volume of unambiguous examples </title>
<section> experiments and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>precision and recall for each attachment site (lex+pos+lp?
</prevsent>
<prevsent>model) class precision recall 1067/1258 (84.82%) 1067/1271 (83.95%) 1635/1839 (88.91%) 1635/1826 (89.54%) table 5.
</prevsent>
</prevsection>
<citsent citstr=" W97-0109 ">
pp-attachment accuracies of previous work method accuracy our method svm 87.25% supervised ratnaphakhi et al, 1994 me 81.6% brill and resnik, 1994 <papid> C94-2195 </papid>tbl 81.9% collins and brooks, 1995 <papid> W95-0103 </papid>back-off 84.5% zavrel et al, 1997 <papid> W97-1016 </papid>nn 84.4% stetina and nagao, 1997 <papid> W97-0109 </papid>dt 88.1% abney et al, 1999 <papid> W99-0606 </papid>boosting 84.6% vanschoenwinkel and man derick, 2003 svm 84.8% zhao and lin, 2004 nn 86.5% unsupervised ratnaparkhi, 1998 - <papid> P98-2177 </papid>81.9% pantel and lin, 2000 - <papid> P00-1014 </papid>84.3% me: maximum entropy, tbl: transformation-based learning, dt: decision tree, nn: nearest neighbor configurations (mcnemars test;   0.05).</citsent>
<aftsection>
<nextsent>lex+pos?
</nextsent>
<nextsent>model was little worse than lex?, but lex+pos+lp?
</nextsent>
<nextsent>was better than lex+lp?
</nextsent>
<nextsent>(and also pos+lp?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L74">
<title id=" I05-1017.xml">ppattachment disambiguation boosted by a gigantic volume of unambiguous examples </title>
<section> experiments and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>precision and recall for each attachment site (lex+pos+lp?
</prevsent>
<prevsent>model) class precision recall 1067/1258 (84.82%) 1067/1271 (83.95%) 1635/1839 (88.91%) 1635/1826 (89.54%) table 5.
</prevsent>
</prevsection>
<citsent citstr=" W99-0606 ">
pp-attachment accuracies of previous work method accuracy our method svm 87.25% supervised ratnaphakhi et al, 1994 me 81.6% brill and resnik, 1994 <papid> C94-2195 </papid>tbl 81.9% collins and brooks, 1995 <papid> W95-0103 </papid>back-off 84.5% zavrel et al, 1997 <papid> W97-1016 </papid>nn 84.4% stetina and nagao, 1997 <papid> W97-0109 </papid>dt 88.1% abney et al, 1999 <papid> W99-0606 </papid>boosting 84.6% vanschoenwinkel and man derick, 2003 svm 84.8% zhao and lin, 2004 nn 86.5% unsupervised ratnaparkhi, 1998 - <papid> P98-2177 </papid>81.9% pantel and lin, 2000 - <papid> P00-1014 </papid>84.3% me: maximum entropy, tbl: transformation-based learning, dt: decision tree, nn: nearest neighbor configurations (mcnemars test;   0.05).</citsent>
<aftsection>
<nextsent>lex+pos?
</nextsent>
<nextsent>model was little worse than lex?, but lex+pos+lp?
</nextsent>
<nextsent>was better than lex+lp?
</nextsent>
<nextsent>(and also pos+lp?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L75">
<title id=" I05-1017.xml">ppattachment disambiguation boosted by a gigantic volume of unambiguous examples </title>
<section> experiments and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>precision and recall for each attachment site (lex+pos+lp?
</prevsent>
<prevsent>model) class precision recall 1067/1258 (84.82%) 1067/1271 (83.95%) 1635/1839 (88.91%) 1635/1826 (89.54%) table 5.
</prevsent>
</prevsection>
<citsent citstr=" P98-2177 ">
pp-attachment accuracies of previous work method accuracy our method svm 87.25% supervised ratnaphakhi et al, 1994 me 81.6% brill and resnik, 1994 <papid> C94-2195 </papid>tbl 81.9% collins and brooks, 1995 <papid> W95-0103 </papid>back-off 84.5% zavrel et al, 1997 <papid> W97-1016 </papid>nn 84.4% stetina and nagao, 1997 <papid> W97-0109 </papid>dt 88.1% abney et al, 1999 <papid> W99-0606 </papid>boosting 84.6% vanschoenwinkel and man derick, 2003 svm 84.8% zhao and lin, 2004 nn 86.5% unsupervised ratnaparkhi, 1998 - <papid> P98-2177 </papid>81.9% pantel and lin, 2000 - <papid> P00-1014 </papid>84.3% me: maximum entropy, tbl: transformation-based learning, dt: decision tree, nn: nearest neighbor configurations (mcnemars test;   0.05).</citsent>
<aftsection>
<nextsent>lex+pos?
</nextsent>
<nextsent>model was little worse than lex?, but lex+pos+lp?
</nextsent>
<nextsent>was better than lex+lp?
</nextsent>
<nextsent>(and also pos+lp?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L76">
<title id=" I05-1017.xml">ppattachment disambiguation boosted by a gigantic volume of unambiguous examples </title>
<section> experiments and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>precision and recall for each attachment site (lex+pos+lp?
</prevsent>
<prevsent>model) class precision recall 1067/1258 (84.82%) 1067/1271 (83.95%) 1635/1839 (88.91%) 1635/1826 (89.54%) table 5.
</prevsent>
</prevsection>
<citsent citstr=" P00-1014 ">
pp-attachment accuracies of previous work method accuracy our method svm 87.25% supervised ratnaphakhi et al, 1994 me 81.6% brill and resnik, 1994 <papid> C94-2195 </papid>tbl 81.9% collins and brooks, 1995 <papid> W95-0103 </papid>back-off 84.5% zavrel et al, 1997 <papid> W97-1016 </papid>nn 84.4% stetina and nagao, 1997 <papid> W97-0109 </papid>dt 88.1% abney et al, 1999 <papid> W99-0606 </papid>boosting 84.6% vanschoenwinkel and man derick, 2003 svm 84.8% zhao and lin, 2004 nn 86.5% unsupervised ratnaparkhi, 1998 - <papid> P98-2177 </papid>81.9% pantel and lin, 2000 - <papid> P00-1014 </papid>84.3% me: maximum entropy, tbl: transformation-based learning, dt: decision tree, nn: nearest neighbor configurations (mcnemars test;   0.05).</citsent>
<aftsection>
<nextsent>lex+pos?
</nextsent>
<nextsent>model was little worse than lex?, but lex+pos+lp?
</nextsent>
<nextsent>was better than lex+lp?
</nextsent>
<nextsent>(and also pos+lp?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L77">
<title id=" I08-2089.xml">large and diverse language models for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the use of billions of words of lm training data has become standard in large-scale smt systems, and even trillion word lms have been demonstrated.
</prevsent>
<prevsent>since lookup of lm scores is one ofthe fundamental functions in smt decoding, efficient storage and access of the model becomes increasingly difficult.
</prevsent>
</prevsection>
<citsent citstr=" D07-1090 ">
a recent trend is to store the lm in distributed cluster of machines, which are queried via network requests (brants et al, 2007; <papid> D07-1090 </papid>emami et al, 2007).it is easier, however, to use such large lms in reranking (zhang et al, 2006).</citsent>
<aftsection>
<nextsent>since the use of clusters of machines is not always practical (or afford able) for smt applications, an alternative strategy is to find more efficient ways to store the lm in the working memory of single machine, for instance by using efficient prefix trees and fewer bits to store the lm probability (federico and bertoldi, 2006).<papid> W06-3113 </papid></nextsent>
<nextsent>also the use of lossy data structures based on bloom filters has been demonstrated to be effective for lms (talbot and osborne, 2007<papid> P07-1065 </papid>a; talbot and osborne, 2007<papid> P07-1065 </papid>b).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L78">
<title id=" I08-2089.xml">large and diverse language models for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>since lookup of lm scores is one ofthe fundamental functions in smt decoding, efficient storage and access of the model becomes increasingly difficult.
</prevsent>
<prevsent>a recent trend is to store the lm in distributed cluster of machines, which are queried via network requests (brants et al, 2007; <papid> D07-1090 </papid>emami et al, 2007).it is easier, however, to use such large lms in reranking (zhang et al, 2006).</prevsent>
</prevsection>
<citsent citstr=" W06-3113 ">
since the use of clusters of machines is not always practical (or afford able) for smt applications, an alternative strategy is to find more efficient ways to store the lm in the working memory of single machine, for instance by using efficient prefix trees and fewer bits to store the lm probability (federico and bertoldi, 2006).<papid> W06-3113 </papid></citsent>
<aftsection>
<nextsent>also the use of lossy data structures based on bloom filters has been demonstrated to be effective for lms (talbot and osborne, 2007<papid> P07-1065 </papid>a; talbot and osborne, 2007<papid> P07-1065 </papid>b).</nextsent>
<nextsent>this allows the use of much larger lms, but increases the risk of errors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L79">
<title id=" I08-2089.xml">large and diverse language models for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a recent trend is to store the lm in distributed cluster of machines, which are queried via network requests (brants et al, 2007; <papid> D07-1090 </papid>emami et al, 2007).it is easier, however, to use such large lms in reranking (zhang et al, 2006).</prevsent>
<prevsent>since the use of clusters of machines is not always practical (or afford able) for smt applications, an alternative strategy is to find more efficient ways to store the lm in the working memory of single machine, for instance by using efficient prefix trees and fewer bits to store the lm probability (federico and bertoldi, 2006).<papid> W06-3113 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1065 ">
also the use of lossy data structures based on bloom filters has been demonstrated to be effective for lms (talbot and osborne, 2007<papid> P07-1065 </papid>a; talbot and osborne, 2007<papid> P07-1065 </papid>b).</citsent>
<aftsection>
<nextsent>this allows the use of much larger lms, but increases the risk of errors.
</nextsent>
<nextsent>lm training data may be any text in the output language.
</nextsent>
<nextsent>typically, however, we are interested in building mt system for particular domain.
</nextsent>
<nextsent>if text resources come from diversity of domains, somemay be more suitable than others.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L87">
<title id=" I08-2089.xml">large and diverse language models for statistical machine translation </title>
<section> language models in decoding and.  </section>
<citcontext>
<prevsection>
<prevsent>for typical 30-word sentence, about 4% of the lm 5-grams may be potentially generated during decoding.
</prevsent>
<prevsent>for large 100word sentences, the ratio is about 15%.2 these numbers suggest that we may be able to use 510 times larger lms, if we filter the lm prior to the decoding of each sentence.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
smt decoders such as moses (koehn et al, 2007) <papid> P07-2045 </papid>may store the translation model in an efficient on-disk data structure (zens and ney, 2007), <papid> N07-1062 </papid>leaving almost the entire working memory for lm storage.</citsent>
<aftsection>
<nextsent>however, this means for 32-bit machines limit of 3 gb for the lm.
</nextsent>
<nextsent>on the other hand, we can limit the use of very large lms to re-ranking stage.
</nextsent>
<nextsent>in two-pass de 2the numbers were obtained using 5-gram lm trained on the english side of the europarl corpus (koehn, 2005), german english translation model trained on europarl, and the wmt 2006 test set (koehn and monz, 2006).<papid> W06-3114 </papid></nextsent>
<nextsent>663 french english news commentary 1.2m 1.0m europarl 37.5m 33.8m table 1: combination of small in-domain (news commentary) and large out-of-domain (europarl) training corpus (number of words).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L89">
<title id=" I08-2089.xml">large and diverse language models for statistical machine translation </title>
<section> language models in decoding and.  </section>
<citcontext>
<prevsection>
<prevsent>for typical 30-word sentence, about 4% of the lm 5-grams may be potentially generated during decoding.
</prevsent>
<prevsent>for large 100word sentences, the ratio is about 15%.2 these numbers suggest that we may be able to use 510 times larger lms, if we filter the lm prior to the decoding of each sentence.
</prevsent>
</prevsection>
<citsent citstr=" N07-1062 ">
smt decoders such as moses (koehn et al, 2007) <papid> P07-2045 </papid>may store the translation model in an efficient on-disk data structure (zens and ney, 2007), <papid> N07-1062 </papid>leaving almost the entire working memory for lm storage.</citsent>
<aftsection>
<nextsent>however, this means for 32-bit machines limit of 3 gb for the lm.
</nextsent>
<nextsent>on the other hand, we can limit the use of very large lms to re-ranking stage.
</nextsent>
<nextsent>in two-pass de 2the numbers were obtained using 5-gram lm trained on the english side of the europarl corpus (koehn, 2005), german english translation model trained on europarl, and the wmt 2006 test set (koehn and monz, 2006).<papid> W06-3114 </papid></nextsent>
<nextsent>663 french english news commentary 1.2m 1.0m europarl 37.5m 33.8m table 1: combination of small in-domain (news commentary) and large out-of-domain (europarl) training corpus (number of words).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L90">
<title id=" I08-2089.xml">large and diverse language models for statistical machine translation </title>
<section> language models in decoding and.  </section>
<citcontext>
<prevsection>
<prevsent>however, this means for 32-bit machines limit of 3 gb for the lm.
</prevsent>
<prevsent>on the other hand, we can limit the use of very large lms to re-ranking stage.
</prevsent>
</prevsection>
<citsent citstr=" W06-3114 ">
in two-pass de 2the numbers were obtained using 5-gram lm trained on the english side of the europarl corpus (koehn, 2005), german english translation model trained on europarl, and the wmt 2006 test set (koehn and monz, 2006).<papid> W06-3114 </papid></citsent>
<aftsection>
<nextsent>663 french english news commentary 1.2m 1.0m europarl 37.5m 33.8m table 1: combination of small in-domain (news commentary) and large out-of-domain (europarl) training corpus (number of words).
</nextsent>
<nextsent>coding, the initial decoder produces an n-best listof translation candidates (say, n=1000), and second pass exploits additional features, for instance very large lms.
</nextsent>
<nextsent>since the order of english words is fixed, the number of different n-grams that need to be looked up is dramatically reduced.
</nextsent>
<nextsent>however, since the n-best list is only the tip of the iceberg of possible translations, we may miss the translation that we would have found with lm integrated into the decoding process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L93">
<title id=" I08-1045.xml">a hybrid feature set based maximum entropy hindi named entity recognition </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>there are several rule based ner systems, containing mainly lexicalized 343 grammar, gazetteer lists, and list of trigger words, which are capable of providing 88%-92% f-measure accuracy for english (grishman, 1995; mcdonald, 1996; wakao et al, 1996).the main disadvantages of these rule-based techniques are that these require huge experience and grammatical knowledge of the particular language or domain and these systems are not transferable to other languages or domains.
</prevsent>
<prevsent>machine learning (ml) based techniques forner make use of large amount of ne annotated training data to acquire high level languageknowledge.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
several ml techniques have been successfully used for the ner task of which hidden markov model (bikel et al, 1997), <papid> A97-1029 </papid>maximum entropy (borthwick, 1999), conditional random field(li and mccallum, 2004) are most common.</citsent>
<aftsection>
<nextsent>combinations of different ml approaches are also used.
</nextsent>
<nextsent>srihari et al (2000) <papid> A00-1034 </papid>combines maximum entropy, hidden markov model and handcrafted rules to build an ner system.</nextsent>
<nextsent>ner systems use gazetteer lists for identifying names.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L94">
<title id=" I08-1045.xml">a hybrid feature set based maximum entropy hindi named entity recognition </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>several ml techniques have been successfully used for the ner task of which hidden markov model (bikel et al, 1997), <papid> A97-1029 </papid>maximum entropy (borthwick, 1999), conditional random field(li and mccallum, 2004) are most common.</prevsent>
<prevsent>combinations of different ml approaches are also used.</prevsent>
</prevsection>
<citsent citstr=" A00-1034 ">
srihari et al (2000) <papid> A00-1034 </papid>combines maximum entropy, hidden markov model and handcrafted rules to build an ner system.</citsent>
<aftsection>
<nextsent>ner systems use gazetteer lists for identifying names.
</nextsent>
<nextsent>both the linguistic approach (grishman,1995; wakao et al, 1996) and the ml based approach (borthwick, 1999; srihari et al, 2000) <papid> A00-1034 </papid>use gazetteer lists.</nextsent>
<nextsent>the linguistic approach uses hand-crafted rules which needs skilled linguistics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L96">
<title id=" I08-1045.xml">a hybrid feature set based maximum entropy hindi named entity recognition </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>an approach to lexical pattern learning for indian languages is described by ekbal andbandopadhyay (2007).
</prevsent>
<prevsent>they used seed data and annotated corpus to find the patterns for ner.
</prevsent>
</prevsection>
<citsent citstr=" W99-0612 ">
the ner task for hindi has been explored by cucerzan and yarowsky in their language independent ner work which used morphological and contextual evidences (cucerzan and yarowsky, 1999).<papid> W99-0612 </papid>they ran their experiment with 5 languages - romanian, english, greek, turkish and hindi.</citsent>
<aftsection>
<nextsent>among these the accuracy for hindi was the worst.
</nextsent>
<nextsent>for hindi the system achieved 41.70% f-value with avery low recall of 27.84% and about 85% precision.
</nextsent>
<nextsent>a more successful hindi ner system was developed by wei li and andrew mccallum (2004)using conditional random fields (crfs) with feature induction.
</nextsent>
<nextsent>they were able to achieve 71.50% f-value using training set of size 340k words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L97">
<title id=" I08-2127.xml">a discriminative approach to japanese abbreviation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, text retrieval systems can associate query with alternative words to find documents where the query is not obviously stated.
</prevsent>
<prevsent>abbreviations are among highly productive type of term variants, which substitutes fully expanded terms with shortened term-forms.
</prevsent>
</prevsection>
<citsent citstr=" W01-0516 ">
most previous studies aimed at establishing associations between abbreviations and their full forms in english (park and byrd, 2001; <papid> W01-0516 </papid>pakhomov, 2002; <papid> P02-1021 </papid>schwartz and hearst, 2003; adar, 2004; nadeau and turney,2005; chang and schutze, 2006; okazaki and ananiadou, 2006).<papid> P06-2083 </papid></citsent>
<aftsection>
<nextsent>although researchers have proposed various approaches to solving abbreviation recognition through methods such as deterministic algorithm, scoring function, and machine learning, these studies relyon the phenomenon specific to english abbreviations: all letters in an abbreviation appear in its full form.however, abbreviation phenomena are heavily dependent on languages.
</nextsent>
<nextsent>for example, the term one segment broadcasting is usually abbreviated as oneseg in japanese; english speakers may find this peculiar as the term is likely to be abbreviated as 1sbor osb in english.
</nextsent>
<nextsent>we show that letters do not provide useful clues for recognizing japanese abbreviations in section 2.
</nextsent>
<nextsent>elaborating on the complexity of the generative processes for japanese abbreviations, section 3 presents supervised learning approach to japanese abbreviations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L98">
<title id=" I08-2127.xml">a discriminative approach to japanese abbreviation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, text retrieval systems can associate query with alternative words to find documents where the query is not obviously stated.
</prevsent>
<prevsent>abbreviations are among highly productive type of term variants, which substitutes fully expanded terms with shortened term-forms.
</prevsent>
</prevsection>
<citsent citstr=" P02-1021 ">
most previous studies aimed at establishing associations between abbreviations and their full forms in english (park and byrd, 2001; <papid> W01-0516 </papid>pakhomov, 2002; <papid> P02-1021 </papid>schwartz and hearst, 2003; adar, 2004; nadeau and turney,2005; chang and schutze, 2006; okazaki and ananiadou, 2006).<papid> P06-2083 </papid></citsent>
<aftsection>
<nextsent>although researchers have proposed various approaches to solving abbreviation recognition through methods such as deterministic algorithm, scoring function, and machine learning, these studies relyon the phenomenon specific to english abbreviations: all letters in an abbreviation appear in its full form.however, abbreviation phenomena are heavily dependent on languages.
</nextsent>
<nextsent>for example, the term one segment broadcasting is usually abbreviated as oneseg in japanese; english speakers may find this peculiar as the term is likely to be abbreviated as 1sbor osb in english.
</nextsent>
<nextsent>we show that letters do not provide useful clues for recognizing japanese abbreviations in section 2.
</nextsent>
<nextsent>elaborating on the complexity of the generative processes for japanese abbreviations, section 3 presents supervised learning approach to japanese abbreviations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L99">
<title id=" I08-2127.xml">a discriminative approach to japanese abbreviation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, text retrieval systems can associate query with alternative words to find documents where the query is not obviously stated.
</prevsent>
<prevsent>abbreviations are among highly productive type of term variants, which substitutes fully expanded terms with shortened term-forms.
</prevsent>
</prevsection>
<citsent citstr=" P06-2083 ">
most previous studies aimed at establishing associations between abbreviations and their full forms in english (park and byrd, 2001; <papid> W01-0516 </papid>pakhomov, 2002; <papid> P02-1021 </papid>schwartz and hearst, 2003; adar, 2004; nadeau and turney,2005; chang and schutze, 2006; okazaki and ananiadou, 2006).<papid> P06-2083 </papid></citsent>
<aftsection>
<nextsent>although researchers have proposed various approaches to solving abbreviation recognition through methods such as deterministic algorithm, scoring function, and machine learning, these studies relyon the phenomenon specific to english abbreviations: all letters in an abbreviation appear in its full form.however, abbreviation phenomena are heavily dependent on languages.
</nextsent>
<nextsent>for example, the term one segment broadcasting is usually abbreviated as oneseg in japanese; english speakers may find this peculiar as the term is likely to be abbreviated as 1sbor osb in english.
</nextsent>
<nextsent>we show that letters do not provide useful clues for recognizing japanese abbreviations in section 2.
</nextsent>
<nextsent>elaborating on the complexity of the generative processes for japanese abbreviations, section 3 presents supervised learning approach to japanese abbreviations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L100">
<title id=" I08-2127.xml">a discriminative approach to japanese abbreviation extraction </title>
<section> japanese abbreviation survey.  </section>
<citcontext>
<prevsection>
<prevsent>hisamitsu and niwa (2001) compared different statistical measures (e.g., 2 test, log like 889 table 1: parenthetical expressions used in japanese newspaper articles lihood ratio) to assess the co-occurrence strength between the inner and outer phrases of parenthetical expressions (y).
</prevsent>
<prevsent>yamamoto (2002) utilized the similarity of local contexts to measure the paraphrase likelihood of two expressions based on the distributional hypothesis (harris, 1954).
</prevsent>
</prevsection>
<citsent citstr=" W06-0103 ">
chang and teng (2006) <papid> W06-0103 </papid>formalized the generative processes of chinese abbreviations with noisy channel model.</citsent>
<aftsection>
<nextsent>sasano et al (2007) designed rules about letter type sand occurrence frequency to collect lexical paraphrases used for coreference resolution.how are these approaches effective in recognizing japanese abbreviation definitions?
</nextsent>
<nextsent>as preliminary study, we examined abbreviations described in parenthetical expressions in japanese newspaper articles.
</nextsent>
<nextsent>we used the 7,887 parenthetical expressions that occurred more than eight times in japanese articles published by the mainichi newspapers and yomiuri shimbun in 19981999.
</nextsent>
<nextsent>table 1 summarizes the usages of parenthetical expressions in four groups.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L101">
<title id=" I08-2127.xml">a discriminative approach to japanese abbreviation extraction </title>
<section> a discriminative approach to </section>
<citcontext>
<prevsection>
<prevsent>(1) in this formula, dpara(x,y ) denotes the number of documents satisfying the above conditions, and d(x,y ) presents the number of documents having the parenthetical expression x(y ).
</prevsent>
<prevsent>the function pr(x, y) ranges from 0 (no abbreviation instance)to 1 (all parenthetical expressions introduce the ab breviation).
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
similarity of local contexts we regard words that have dependency relations from/to the target expression as the local contexts of the expression, applying all sentences to dependency parser (kudo and matsumoto, 2002).<papid> W02-2016 </papid></citsent>
<aftsection>
<nextsent>collecting the local context ofthe target expressions, we compute the skew divergence (lee, 2001), which is weighted version of kullback-leibler (kl) divergence, to measure the resemblance of probability distributions and q: skew?(p ||q) = kl(p ||q+ (1?
</nextsent>
<nextsent>?)p ), (2) kl(p ||q) = ? p (i) log (i) q(i) .
</nextsent>
<nextsent>(3) in these formulas, is the probability distribution function of the words in the local context for the expression , is for , and ? is skew parameter set to 0.99.
</nextsent>
<nextsent>the function skew?(p ||q) becomes close to zero if the probability distributions of local contexts for the expressions and are similar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L105">
<title id=" I08-1056.xml">cluster based query expansion for statistical question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results using the clarity (croft et al, 2001) statistical measure suggest that query ambiguity is often reduced by using clarification queries which produce focused set of docu ments.another research direction that tailors their component to question answering systems focuses on query formulation and query expansion (woods et al., 2001).
</prevsent>
<prevsent>taxonomic conceptual indexing system based on morphological, syntactic, and semantic features can be used to expand queries with inflected forms, hypernyms, and semantically related terms.in subsequent research (bilotti et al, 2004), stemming is compared to query expansion using inflectional variants.
</prevsent>
</prevsection>
<citsent citstr=" P07-1059 ">
on particular question answering controlled dataset, results show that expansion using inflectional variants produces higher recall than stemming.recently (riezler et al, 2007) <papid> P07-1059 </papid>used statistical machine translation for query expansion and took step towards bridging the lexical gap between questions and answers.</citsent>
<aftsection>
<nextsent>in (terra et al, 2005) query expansion is studied using lexical affinities with different query formulation strategies for passage retrieval.
</nextsent>
<nextsent>when evaluated on trec datasets, the affinity replacement method obtained significant improvements in precision, but did not outperform other methods in terms of recall.
</nextsent>
<nextsent>in order to explore retrieval under question answering, we employ statistical system (sqa) that achieves good factoid performance on the trecqa task: for ? 50% of the questions correct answer is in the top highest confidence answer.
</nextsent>
<nextsent>rather than manually defining complete answering strategy ? the type of question, the queries to be run, the answer extraction, and the answer merging methods ? for each type of question, sqa learns different strategies for different types of similar questions sqa takes advantage of similarity in training data (questions and answers from past trec evaluations), and performs question clustering.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L106">
<title id=" I08-1002.xml">an empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, various heuristic rules are often involved inmost existing works, and there has not been comprehensive comparison of their performance in aunified way with available large-scale gold stan dard?
</prevsent>
<prevsent>datasets, especially, multi-standard ones since bakeoff-1 1.in this paper we will propose unified framework for unsupervised segmentation of chinese text.four existing approaches to unsupervised segmenta tions or word extraction are considered as its special cases, each with its own goodness measurement to quantify word likelihood.
</prevsent>
</prevsection>
<citsent citstr=" W06-0115 ">
the output by each approach will be evaluated using benchmark data setsof bakeoff-32 (levow, 2006).<papid> W06-0115 </papid></citsent>
<aftsection>
<nextsent>note that unsupervised segmentation is different from, if not more complex than, word extraction, in that the former must carry out the segmentation task for text, for which segmentation (decoding) algorithm is indispensable, whereas the latter only acquires word candidate list as output (chang and su, 1997; zhang et al, 2000).<papid> W00-1219 </papid></nextsent>
<nextsent>we propose generalized framework to unify the existing methods for unsupervised segmentation, assuming the availability of list of word candidates each associated with goodness for how likely it is to be true word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L107">
<title id=" I08-1002.xml">an empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>datasets, especially, multi-standard ones since bakeoff-1 1.in this paper we will propose unified framework for unsupervised segmentation of chinese text.four existing approaches to unsupervised segmenta tions or word extraction are considered as its special cases, each with its own goodness measurement to quantify word likelihood.
</prevsent>
<prevsent>the output by each approach will be evaluated using benchmark data setsof bakeoff-32 (levow, 2006).<papid> W06-0115 </papid></prevsent>
</prevsection>
<citsent citstr=" W00-1219 ">
note that unsupervised segmentation is different from, if not more complex than, word extraction, in that the former must carry out the segmentation task for text, for which segmentation (decoding) algorithm is indispensable, whereas the latter only acquires word candidate list as output (chang and su, 1997; zhang et al, 2000).<papid> W00-1219 </papid></citsent>
<aftsection>
<nextsent>we propose generalized framework to unify the existing methods for unsupervised segmentation, assuming the availability of list of word candidates each associated with goodness for how likely it is to be true word.
</nextsent>
<nextsent>let = {{wi, g(wi)}i=1,...,n} be such list, where wi is word candidate and g(wi) 1first international chinese word segmentation bakeoff, at http://www.sighan.org/bakeoff2003 2the third international chinese language processing bakeoff, at http://www.sighan.org/bakeoff2006.
</nextsent>
<nextsent>9 its goodness function.
</nextsent>
<nextsent>two generalized decoding algorithms, (1) and (2), are formulated for optimal segmentation of given plain text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L108">
<title id=" I08-1002.xml">an empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework </title>
<section> goodness measurement.  </section>
<citcontext>
<prevsection>
<prevsent>et al, 2004) to produce list of such reduced sub strings for givencorpus.
</prevsent>
<prevsent>the basic idea is that if two partially overlapped n-grams have the same frequency in the input corpus, then the shorter one is discarded as redundant word candidate.
</prevsent>
</prevsection>
<citsent citstr=" P98-2206 ">
we take the logarithm of fsr3although there have been many existing works in this direction (lua and gan, 1994; chien, 1997; sun et al, 1998;<papid> P98-2206 </papid>zhang et al, 2000; <papid> W00-1219 </papid>sun et al, 2004), we have to skip the details of comparing mi due to the length limitation of this paper.</citsent>
<aftsection>
<nextsent>however, our experiments with mi provide no evidence against the conclusions in this paper.
</nextsent>
<nextsent>as the goodness for word candidate, i.e., gfsr(w) = log(p?(w)) (3) where p?(w) is ws frequency in the corpus.
</nextsent>
<nextsent>this allows the arithmetic addition in (1).
</nextsent>
<nextsent>according to zipfs law (zipf, 1949), it approximates the use of the rank of as its goodness, which would give itsome statistical significance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L111">
<title id=" I08-1002.xml">an empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>5all evaluations will be represented in terms of word f-measure if not otherwise specified.
</prevsent>
<prevsent>a standard scoring tool with this metric can be found in sighan website,http://www.sighan.org/bakeoff2003/score.
</prevsent>
</prevsection>
<citsent citstr=" A00-2032 ">
however, to compare with related work, we will also adopt boundary f-measure fb = 2rbpb/(rb + pb), where the boundary recall rb and boundary precision pb are, respectively, the proportions of the correctly recognized boundaries to all boundaries in the gold standard and segment ers output (ando and lee, 2000).<papid> A00-2032 </papid></citsent>
<aftsection>
<nextsent>table 1: bakeoff-3 corpora corpus as cityu ctb msra training(m) 8.42 2.71 0.83 2.17 test(k) 146 364 256 173 table 2: performance with decoding algorithm (1) m. good- training corpus l.a ness as cityu ctb msra fsr .400 .454 .462 .432 2 dlg/d .592 .610 .604 .603av .568 .595 .596 .577.
</nextsent>
<nextsent>be .559 .587 .592 .572 fsr .193 .251 .268 .235 7 dlg/d .331 .397 .409 .379av .399 .423 .430 .407.
</nextsent>
<nextsent>be .390 .419 .428 .403 am.l.: maximal length allowable for word candidates.
</nextsent>
<nextsent>for computation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L112">
<title id=" I08-1002.xml">an empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>it works asfollows: if g(xi..j+1)   g(xi..j) for any two overlapped sub strings xi..j and xi..j+1, then segmenting point should be located right after xi..j+1.
</prevsent>
<prevsent>this algorithm has forward and backward version.the union of the segmentation outputs by both versions is taken as the final output of the algorithm, in exactly the same way as how decoding algorithm (2) works7.
</prevsent>
</prevsection>
<citsent citstr=" P06-2056 ">
this algorithm is evaluated in (jin and tanaka-ishii, 2006) using peking university (pku)7three segmentation criteria are given in (jin and tanaka ishii, 2006), <papid> P06-2056 </papid>among which the entropy increase criterion, namely, decoding algorithm (3), proves to be the best.</citsent>
<aftsection>
<nextsent>here we would like to thank jin zhihui and prof. kumiko tanaka-ishii for presenting the details of their algorithms.
</nextsent>
<nextsent>13 table 8: performance comparison by word and boundary f-measure on pku corpus (m. l. = 6) good- decoding algorithm ness (1)/d (1) (2)/d (2) (3)/d (3) av .313 .325 .588 .373 .376 .453 av?
</nextsent>
<nextsent>.372 .372 .663 .663 .445 .445 be .309 .319 .624 .501 .376 .624 be?
</nextsent>
<nextsent>.370 .370 .676 .676 .447 .447 av .695 .700 .830 .762 .762 .728 fb av?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L113">
<title id=" I08-1002.xml">an empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>as quoted in table 9, consistency rate 8http://icl.pku.edu.cn/icl groups/corpus/dwldform1.asp 9http://ccl.pku.edu.cn:8080/ccl corpus/jsearch/index.jsp 10this is to keep consistence with (jin and tanaka-ishii, 2006), where 6 is set as the maximum n-gram length.table 9: consistency rate among bakeoff-3 segmentation standards (huang and zhao, 2007) test training corpus corpus as cityu ctb msra as 1.000 0.926 0.959 0.858 cityu 0.932 1.000 0.935 0.849 ctb 0.942 0.910 1.000 0.877 msra 0.857 0.848 0.887 1.000 beyond 84.8% is found among the four standards.
</prevsent>
<prevsent>if we do not over-expect unsupervised segmentation to achieve beyond what these standards agree with each other, it is reasonable to take this figure as the topline for evaluation.
</prevsent>
</prevsection>
<citsent citstr=" W06-0127 ">
on the other hand, zhao et al (2006) <papid> W06-0127 </papid>show that the words of 1 to 2 characters long account for 95% of all words in chinese texts, and single-character words alone for about 50%.</citsent>
<aftsection>
<nextsent>thus,we can take the result of the brute-force guess of every single character as word as baseline.
</nextsent>
<nextsent>to compare to supervised segmentation, which usually involves training using an annotated training corpus and, then, evaluation using test corpus,we carry out unsupervised segmentation incomparable manner.
</nextsent>
<nextsent>for each data track, we first extract word candidates from both the training and test corpora, all unannotated, and then evaluate the unsupervised segmentation with reference to the gold standard segmentation of the test corpus.
</nextsent>
<nextsent>there sults are presented in table 10, together with best and worst official results of the bakeoff closed test.this comparison shows that unsupervised segmentation cannot compete against supervised segmentation in terms of performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L114">
<title id=" I08-1041.xml">using rogets thesaurus for fine grained emotion recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we achieve fmeasure values that outperform the rule based baseline method for all emotion classes.
</prevsent>
<prevsent>recognizing emotions conveyed by text can provide an insight into the authors intent and sentiment, and can lead to better understanding of the texts content.
</prevsent>
</prevsection>
<citsent citstr=" H05-1073 ">
emotion recognition in text has recently attracted increased attention of the nlp community (alm et al, 2005; <papid> H05-1073 </papid>liu et al 2003; mihalcea and liu, 2006); it is also one of the tasks at semeval-20071.</citsent>
<aftsection>
<nextsent>automatic recognition of emotions can be applied in the development of affective interfaces for 1 affective text: semeval task at the 4th international work-.
</nextsent>
<nextsent>shop on semantic evaluations, 2007, prague (nlp.cs.swarthmore.edu/semeval/tasks/task14/summary.shtml).
</nextsent>
<nextsent>computer-mediated communication and human computer interaction.
</nextsent>
<nextsent>other areas that can potentially benefit from automatic emotion analysis are personality modeling and profiling (liu and maes, 2004), affective interfaces and communication systems (liu et al 2003; neviarouskaya et al, 2007a) consumer feedback analysis, affective tutoring in e-learning systems (zhang et al, 2006), and text to-speech synthesis (alm et al, 2005).<papid> H05-1073 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L117">
<title id=" I08-1041.xml">using rogets thesaurus for fine grained emotion recognition </title>
<section> approach based on machine learning.  </section>
<citcontext>
<prevsection>
<prevsent>the corpus-based features exploit the statistical characteristics of the data on the basis of the ngram distribution.
</prevsent>
<prevsent>in our experiments, we take unigrams (n=1) as features.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
unigram models have been previously shown to give good results in sentiment classification tasks (kennedy and inkpen, 2006; pang et al, 2002): <papid> W02-1011 </papid>unigram representations can capture variety of lexical combinations and distributions, including those of emotion words.</citsent>
<aftsection>
<nextsent>this is particularly important in the case of blogs, whose language is often characterized by frequent use of new words, acronyms (such as lol?), ono matopoeic words (haha?, grrr?), and slang, most of which can be captured in unigram representa this was the best summer have ever experienced.
</nextsent>
<nextsent>(happiness) dont feel like ever have that kind of privacy where can talk to god and cry and figure things out.
</nextsent>
<nextsent>(sadness) finally, got fed up.
</nextsent>
<nextsent>(disgust) cant believe she is finally here!
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L118">
<title id=" I08-1041.xml">using rogets thesaurus for fine grained emotion recognition </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>we used lists of words extracted from it for each of the six emotion categories.
</prevsent>
<prevsent>we train classifiers with unigram features for each emotion class using support vector machine (svm) for predicting the emotion category of the sentences in our corpus.
</prevsent>
</prevsection>
<citsent citstr=" W04-3253 ">
svm has been shown to be useful for text classification tasks (joachims, 1998), and has previously given good performance in sentiment classification experiments (kennedy and inkpen, 2006; mullen and collier, 2004; <papid> W04-3253 </papid>pang and lee, 2004; <papid> P04-1035 </papid>pang et al, 2002).<papid> W02-1011 </papid></citsent>
<aftsection>
<nextsent>in table 4, we report results from ten-fold cross-validation experiments conducted using the smo implementation of svm in weka (witten and frank, 2005).
</nextsent>
<nextsent>in each experiment, we represent sentence by vector indicating the number of times each feature occurs.
</nextsent>
<nextsent>in the first experiment, we use only corpus based unigram features.
</nextsent>
<nextsent>we obtain high precision values for all emotion classes (as shown in table 4), and the recall and f-measure values surpass baseline values for all classes except no-emotion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L119">
<title id=" I08-1041.xml">using rogets thesaurus for fine grained emotion recognition </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>we used lists of words extracted from it for each of the six emotion categories.
</prevsent>
<prevsent>we train classifiers with unigram features for each emotion class using support vector machine (svm) for predicting the emotion category of the sentences in our corpus.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
svm has been shown to be useful for text classification tasks (joachims, 1998), and has previously given good performance in sentiment classification experiments (kennedy and inkpen, 2006; mullen and collier, 2004; <papid> W04-3253 </papid>pang and lee, 2004; <papid> P04-1035 </papid>pang et al, 2002).<papid> W02-1011 </papid></citsent>
<aftsection>
<nextsent>in table 4, we report results from ten-fold cross-validation experiments conducted using the smo implementation of svm in weka (witten and frank, 2005).
</nextsent>
<nextsent>in each experiment, we represent sentence by vector indicating the number of times each feature occurs.
</nextsent>
<nextsent>in the first experiment, we use only corpus based unigram features.
</nextsent>
<nextsent>we obtain high precision values for all emotion classes (as shown in table 4), and the recall and f-measure values surpass baseline values for all classes except no-emotion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L121">
<title id=" I08-2112.xml">learning named entity hyponyms for question answering </title>
<section> hyponym induction.  </section>
<citcontext>
<prevsection>
<prevsent>we review several approaches to learning is-a relations.
</prevsent>
<prevsent>2.1 hearst patterns.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
the seminal work in the field of hypernym learning was done by hearst (1992).<papid> C92-2082 </papid></citsent>
<aftsection>
<nextsent>her approach was to identify discriminating lexico-syntactic patterns that suggest hypernymic relations.
</nextsent>
<nextsent>for example, x, such as y?, as in elements, such as chlorine and fluorine?.
</nextsent>
<nextsent>2.2 knowitall.
</nextsent>
<nextsent>etzioni et al  developed system, knowitall, that does not require training examples and is broadly applicable to variety of classes (2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L122">
<title id=" I08-2112.xml">learning named entity hyponyms for question answering </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>but the drop off is consistent with the fact that only one third of questions have clear need for entity knowledge.
</prevsent>
<prevsent>although there is significant body of work in automated ontology construction, few researchers have examined the relationship between their methods 803 table 8: qa performance on trec 2005 &amp; 2006 data hyponym-relevant subset (242) all questions (734) mrr % correct mrr % correct wn-alone 0.189 (-45.6%) 12.8 (-51.6%) 0.243 (-29.0%) 18.26 (-30.9%) baseline 0.348 26.4 0.342 26.4 sfd 0.405 (+16.5%) 31.0 (+17.2%) 0.362 (+5.6%) 27.9 (+5.7%) swn 0.351 (+1.0%) 26.9 (+1.6%) 0.343 (+0.3%) 26.6 (+0.5%) feat 0.373 (+7.4%) 28.9 (+9.4%) 0.351 (+2.5%) 27.3 (+3.1%)for knowledge discovery and improved question answering performance.
</prevsent>
</prevsection>
<citsent citstr=" W02-1111 ">
one notable study was conducted by mann (2002).<papid> W02-1111 </papid></citsent>
<aftsection>
<nextsent>our work differs in two ways: (1) his method for identifying hyponyms was based on single syntactic pattern, and (2) he looked at comparatively simple task ? given question and one answer sentence containing the answer, extract the correct named entity answer.
</nextsent>
<nextsent>other attempts to deal with lexical mismatch in automated qa include rescoring based on syntactic variation (cui et al , 2005) and identification of verbal paraphrases (lin and pantel, 2001).
</nextsent>
<nextsent>the main contribution of this paper is showing that large-scale, weakly-supervised hyponym learning is capable of producing improvements in an end to-end qa system.
</nextsent>
<nextsent>in contrast, previous studies have generally presented algorithmic advances and showcased sample results, but failed to demonstrate gains in realistic application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L123">
<title id=" I08-1001.xml">a lemmatization method for modern mongolian and its application to information retrieval </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ehara et al (2004) proposed morphological analysis method for mongolian, for which they manually produced rules for inflections and conca tenations.
</prevsent>
<prevsent>however, because the lemmatization methods proposed by sanduijav et al (2005) and ehara et al (2004) relyon dictionaries, these methods cannot lemmatize new words that are not in dictionaries, such as loan words and technical terms.
</prevsent>
</prevsection>
<citsent citstr=" P06-1083 ">
khaltar et al (2006)<papid> P06-1083 </papid>proposed lemmatization method for mongolian noun phrases that does not use noun dictionary.</citsent>
<aftsection>
<nextsent>their method can be used for nouns, adjectives, and numerals, because the suffixes that are concatenated with these are almost the same and the inflection types are also the same.
</nextsent>
<nextsent>however, they were not aware of the applicability of their method to adjectives and numerals.
</nextsent>
<nextsent>the method proposed by khaltar et al (2006)<papid> P06-1083 </papid>mistakenly extracts loan words with endings that are different from conventional mongolian words.</nextsent>
<nextsent>for example, if the phrase ??????????</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L175">
<title id=" I08-2134.xml">exploiting unlabeled text to extract new words of different semantic transparency for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, new word identification (nwi) is also very important be cause they represent the latest information, such as new product names.
</prevsent>
<prevsent>this paper explores methods of extracting information from both internal and external unlabeled data to augment nwi and cws.
</prevsent>
</prevsection>
<citsent citstr=" W02-1811 ">
according to (tseng and chen, 2002), <papid> W02-1811 </papid>new words can be divided into two major categories: words with high or low semantic transparency (st), which describes the correlation of semantic meanings between word and its mor phemes.</citsent>
<aftsection>
<nextsent>we designed effective strategies toward the identification of these two new word types.
</nextsent>
<nextsent>one is based on transductive learning and the other is based on association metrics.
</nextsent>
<nextsent>2.1 formulation.
</nextsent>
<nextsent>we convert the manually segmented words into tagged character sequences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L176">
<title id=" I08-2134.xml">exploiting unlabeled text to extract new words of different semantic transparency for chinese word segmentation </title>
<section> baseline n-gram features.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, we use c0 rather than xt to denote the current character.
</prevsent>
<prevsent>931
</prevsent>
</prevsection>
<citsent citstr=" W03-1728 ">
character n-gram features have proven their effectiveness in ml-based cws (xue and shen,2003).<papid> W03-1728 </papid></citsent>
<aftsection>
<nextsent>we use 4 types of unigram feature func tions: c0, c1 (next character), c1 (previouscharacter), c2 (character preceding c1).
</nextsent>
<nextsent>furthermore, 6 types of bigram features are used, and are designated here as conjunctions of the previously specified unigram features, c2c1, c1c0, c0c1, c3c1, c2c0, and c1c1.
</nextsent>
<nextsent>we mainly focus on improving new word identification (nwi) using unlabeled text.
</nextsent>
<nextsent>words with high and low stare discussed separately due to the disparity in their morphological characteristics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L177">
<title id=" I08-2134.xml">exploiting unlabeled text to extract new words of different semantic transparency for chinese word segmentation </title>
<section> new word identification.  </section>
<citcontext>
<prevsection>
<prevsent>the baseline model cannot identify ?)s xiong-tianping (a singers name) because ?)s is low st word and the morphological tendency of ) is not consistent with the recorded one.
</prevsent>
<prevsent>in english, there is similar phenomenon called multi-word expressions (mwes).
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
(choueka, 1988) regarded mwe as connected collocations: sequence of neighboring words whose exact meaning cannot be derived from the meaning or connotation of its components?, which means that mwes also have low st. as some pioneers provide mwe identification methods which are based on association metrics (am), such as likelihood ratio (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>the methods of identifying low-st words can be divided into two: filtering and merging.
</nextsent>
<nextsent>the former uses am to measure the likelihood that candidate is actually whole word that cannot be divided.
</nextsent>
<nextsent>candidates with ams lower thanthe threshold are filtered out.
</nextsent>
<nextsent>the latter strategy merges character segments in bottom-up fashion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L179">
<title id=" I08-2134.xml">exploiting unlabeled text to extract new words of different semantic transparency for chinese word segmentation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>this script provides three basic metrics: precision (p), recall (r), and f-measure (f).
</prevsent>
<prevsent>in addition, it also provides three detailed metrics: roov stands for the recall rate of the oov words.
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
riv stands for the recall rate of the ivwords, and nc stands for nchanges (insertion+deletion+substitution) (sproat and emerson, 2003).<papid> W03-1719 </papid></citsent>
<aftsection>
<nextsent>in addition, we also compare the nchange reduction rate (ncrr) because the cwss state-of-the art f-measure is over 90%.
</nextsent>
<nextsent>here, the ncrr of any system is calculated: ncrr(s) = nchangebaseline nchanges nchangebaseline 8.2 results.
</nextsent>
<nextsent>our system uses the n-gram features described in section 3 as our baseline features, denot edas n-grams.
</nextsent>
<nextsent>we then sequentially add other features and show the results in table 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L180">
<title id=" I05-6002.xml">obtaining japanese lexical units for semantic frames from berkeley framenet using a bilingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>along with propbank (kingsbury and palmer, 2002; ellsworth et al , 2004), berkeley framenet1one of the anonymous reviewers told us that it was unclear how onto logical inferences of this sort are related to bfns frame definitions.
</prevsent>
<prevsent>the question boils down to the question of definition, i.e., what kind of information we need to define semantic frames to encode, and as we will see later, this is exactly the question addressed by focal claiming that bfn frames are too coarse-grained to be used as an effective knowledge-base for onto logical inferences.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
11 (bfn) (baker et al , 1998) <papid> P98-1013 </papid>is an ongoing research project that is attempting to meet the demand for resources that encode deeper lexical semantics by providing semantic frame lexicon (sometimes called the framenet?)</citsent>
<aftsection>
<nextsent>and corpus annotated for semantic information encoded in terms of semantic frames.
</nextsent>
<nextsent>thus far, bfn has produced lexical database that currently contains more than 8,900 lexical units, more than 6,100 of which are fully annotated, in more than 625 semantic frames, exemplified in more than 135,000 annotated sentences?
</nextsent>
<nextsent>(cited from the framenet web page).
</nextsent>
<nextsent>other ongoing projects, i.e., the german framenet or salsa?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L181">
<title id=" I05-6002.xml">obtaining japanese lexical units for semantic frames from berkeley framenet using a bilingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(cited from the framenet web page).
</prevsent>
<prevsent>other ongoing projects, i.e., the german framenet or salsa?
</prevsent>
</prevsection>
<citsent citstr=" P03-1068 ">
(erk et al , 2003), <papid> P03-1068 </papid>the spanish framenet (subirats and petruck, 2003), and the japanese framenet (ohara et al , 2003), are trying to build lexical resources that are compatible with the bfn, but for japanese at least, no data has been released in usable form, except for few annotation examples for verbs of motion.in sum, no useful resource exists for frame based description/analysis of japanese.</citsent>
<aftsection>
<nextsent>this is one of the reasons that we attempted the task inthis paper, along with our efforts to assess the usefulness of the database provided by bfn.
</nextsent>
<nextsent>the anonymous reviewers of our paper pointed out that there have been some similar project sand other methodologies that have tried to translate bfn into other languages automatically, suchas biframenet (chen and fung, 2004) and romance framenet2, and that it would have been better to include the comparison against them.
</nextsent>
<nextsent>biframenet presented an automatic approach to constructing bilingual semantic network using the chinese hownet, which is chinese ontology.
</nextsent>
<nextsent>while it is an interesting approach, we have not compared their results with ours,mainly because they seem to have used different resources and had somewhat different goals, along with the space consideration.no papers are released, let al ne being available to us, related to the romance framenet project for the time being.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L182">
<title id=" I05-6002.xml">obtaining japanese lexical units for semantic frames from berkeley framenet using a bilingual corpus </title>
<section> proposed procedure.  </section>
<citcontext>
<prevsection>
<prevsent>we couldnt help putting comparison with it on hold.3 2http://ic2.epfl.ch/pallotta/rfn/ 3one of the anonymous reviewers criticized us for failing to mention romance framenet project in our paper; it is just unreasonable.
</prevsent>
<prevsent>the project was announced on june 1 on the
</prevsent>
</prevsection>
<citsent citstr=" P03-1010 ">
we used bilingual corpus (utiyama and isahara, 2003) <papid> P03-1010 </papid>to examine which semantic frames of bfn contained lus relevant to the japanese verb osou.</citsent>
<aftsection>
<nextsent>jfn, for example, used mono-lingual corpus to construct the semantic frames.
</nextsent>
<nextsent>in cases like this, the construction might be inefficient because they have to construct all semantic frames by themselves.
</nextsent>
<nextsent>but this affects on the reliability of the frames identified and described.
</nextsent>
<nextsent>this risk of arbitrary description can be reduced by using bilingual corpus, if it is of high-quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L183">
<title id=" I05-3028.xml">chinese word segmentation with multiple post processors in hitirlab </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>then we will recognize the named entities such as persons and locations.
</prevsent>
<prevsent>first, we select n3 best paths from the segment graph with dijkstra algorithm.
</prevsent>
</prevsection>
<citsent citstr=" W03-1730 ">
then for every path of the n+1 paths4 (n best paths and the atom path), we perform process of roles tagging with hmm model (zhang et al 2003).<papid> W03-1730 </papid></citsent>
<aftsection>
<nextsent>the process of it is much like that of part of speech tagging.
</nextsent>
<nextsent>then with the best role sequence of every path, we can find out all the named entities and add them to the segment graph as usual.
</nextsent>
<nextsent>take the sentence ??
</nextsent>
<nextsent>for example.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L184">
<title id=" I08-2099.xml">dependency annotation scheme for indian languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the lack of such resource has been major limiting factor in the development of good natural language tools and applications for ils.
</prevsent>
<prevsent>apart from that, rich and large-scale tree bank can be an indispensable resource for linguistic investigations.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
some notable efforts in this direction for other languages have been the penn tree bank (marcus et al., 1993) <papid> J93-2004 </papid>for english and the prague dependency bank (hajicova, 1998) for czech.</citsent>
<aftsection>
<nextsent>it is well known that context free grammar (cfg) is not well-suited for free-word order languages (shieber, 1985); instead dependency framework appears to be better suited (hudson, 1984; mel cuk, 1988, bharati et al, 1995).
</nextsent>
<nextsent>also, the dependency framework is arguably closer to semantics than the phrase structure grammar (psg) if the dependency relations are judiciously chosen.
</nextsent>
<nextsent>in recent times many research groups have been shifting to the dependency paradigm due to this reason.
</nextsent>
<nextsent>modern dependency grammar is attributed to tesnire (1959).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L185">
<title id=" I08-2099.xml">dependency annotation scheme for indian languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>out of the three levels, the analytical and tectogrammatical level are dependency based.
</prevsent>
<prevsent>the tectogrammatical level tries to capture the deep-semantics of the sentence; the annotation at this level is very rich and is linked to the other two lower levels.
</prevsent>
</prevsection>
<citsent citstr=" W04-1501 ">
other major efforts in the dependency framework are alpino (van der beek et. al, 2002) for dutch, (rambow et. al, 2002) for english, tut (bosco and lombardo, 2004) <papid> W04-1501 </papid>for italian, tiger (brants et. al, 2002) (combines dependency with psg) for german.</citsent>
<aftsection>
<nextsent>in this paper we describe an approach to annotate ils using the paninian1 model.
</nextsent>
<nextsent>the paper is arranged as follows, section 2 gives brief overview of the 1paninian theory was formulated by panini about two thousand five hundred years ago for sanskrit.
</nextsent>
<nextsent>it evolved with the contributions of grammar ians that followed.
</nextsent>
<nextsent>721 grammatical model and the motivation for following the framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L186">
<title id=" I08-2099.xml">dependency annotation scheme for indian languages </title>
<section> grammatical model.  </section>
<citcontext>
<prevsection>
<prevsent>experiments have been conducted with high performance in automatically marking intra chunk dependencies.
</prevsent>
<prevsent>using information such as karakas based on some vibhaktis (post-positions) and other information like tam (tense, aspect and modality) of the main verb seems very well suited for handling free word order languages.
</prevsent>
</prevsection>
<citsent citstr=" W04-1502 ">
other works based on this scheme like (bharati et al, 1993; bharati et al, 2002; pedersen et al, 2004) <papid> W04-1502 </papid>have shown promising results.</citsent>
<aftsection>
<nextsent>we, therefore, propose the use of dependency annotation based on the panini an model in the indian context.
</nextsent>
<nextsent>scrip tion the annotation task is planned on million word hindi corpora obtained from ciil (central institute for indian languages), mysore, india.
</nextsent>
<nextsent>it is representative corpus which contains texts from various domains like newspaper, literature, gov 722 ernment reports, etc. the present subset on which the dependency annotation is being performed has already been manually tagged and chunked.
</nextsent>
<nextsent>currently the annotation is being carried out by 2 annotators, who are graduate students with linguistic knowledge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L187">
<title id=" I05-2039.xml">the influence of data homogeneity on nlp system performance </title>
<section> a framework for corpus homogeneity </section>
<citcontext>
<prevsection>
<prevsent>however(as acknowledged in (kilgarriff and rose 98)), using ksc requires that the two corpora chosen for comparison are sufficiently similar that the most frequent lexemes in them almost perfectly overlap.
</prevsent>
<prevsent>however (liebscher 2003) showed by comparing frequency counts of different large google group 1see http://www.cs.cmu.edu/mccallum/bow . 226 corpora that it is not usually the case.
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
measuring homogeneity by counting word / lexeme frequencies introduces additional difficulties as it assumes that the word is an obvious, well-defined unit, which is not the case in the chinese (sproat and emerson 2003) <papid> W03-1719 </papid>or japanese language (matsumoto et al, 2002), for instance, where word segmentation is not trivial.</citsent>
<aftsection>
<nextsent>(denoual 2004) showed that similarity between corpora could be quantified with coefficient based on the cross-entropies of probabilistic models built upon reference data.
</nextsent>
<nextsent>the approach needed no explicit selection of features and was language independent, as it relied on character based models (as opposed to word based models) thus bypassing the word segmentation issue and making it applicable on any electronic data.
</nextsent>
<nextsent>the cross-entropy ht (a) of an n-gram model constructed on training corpus , on test corpus = {s1, .., sq} of sentences with si = {ci1..ci|si|} sentence of |si| characters is: ht (a) = i=1[ ?|si| j=1logpij ]q i=1 |si| (1) where pij = p(cij |cijn+1..cij1).we therefore define scale of similarity between two corpora on which to rank any third given one.
</nextsent>
<nextsent>two reference corpora t1 and t2 are selected by the user, and used as training sets to compute n-gram character models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L188">
<title id=" I05-2039.xml">the influence of data homogeneity on nlp system performance </title>
<section> a framework for corpus homogeneity </section>
<citcontext>
<prevsection>
<prevsent>japanese to english.
</prevsent>
<prevsent>because it is not feasible here to have humans judge the quality of many sets of translated data,we relyon an array of well known automatic evaluation measures to estimate translation quality :?
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
bleu (papineni et al 2002) <papid> P02-1040 </papid>is the geometric mean of the n-gram precis ions in the output with respect to set of reference translations.</citsent>
<aftsection>
<nextsent>it is bounded between 0 and 1, better scores indicate better translations, and it tends to be highly correlated with the fluency of outputs ; ? nist (doddington 2002) is variant of bleu based on the arithmetic mean of weighted n-gram precis ions in the output with respect to set of reference translations.
</nextsent>
<nextsent>it has lower bound of 0, no upper bound, better scores indicate better translations, andit tends to be highly correlated with the adequacy of outputs ; ? mwer (och 2003) <papid> P03-1021 </papid>or multiple word error rate is the edit distance in words between the system output and the closest reference translation in set.</nextsent>
<nextsent>it is bounded between 0and 1, and lower scores indicate better trans lations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L189">
<title id=" I05-2039.xml">the influence of data homogeneity on nlp system performance </title>
<section> a framework for corpus homogeneity </section>
<citcontext>
<prevsection>
<prevsent>bleu (papineni et al 2002) <papid> P02-1040 </papid>is the geometric mean of the n-gram precis ions in the output with respect to set of reference translations.</prevsent>
<prevsent>it is bounded between 0 and 1, better scores indicate better translations, and it tends to be highly correlated with the fluency of outputs ; ? nist (doddington 2002) is variant of bleu based on the arithmetic mean of weighted n-gram precis ions in the output with respect to set of reference translations.</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
it has lower bound of 0, no upper bound, better scores indicate better translations, andit tends to be highly correlated with the adequacy of outputs ; ? mwer (och 2003) <papid> P03-1021 </papid>or multiple word error rate is the edit distance in words between the system output and the closest reference translation in set.</citsent>
<aftsection>
<nextsent>it is bounded between 0and 1, and lower scores indicate better translations.
</nextsent>
<nextsent>figure 2 shows bleu, nist and mwer scores for increasing amounts of data from 0.5% to 100% of the btec and interpolated.
</nextsent>
<nextsent>as was expected, mt quality increases as training data increases and tends to have an asymptotic behaviour when more data is being used in training.
</nextsent>
<nextsent>here again except for the smaller amounts of data (up to 3% of the btec in bleu, up to 18% in nist and up to 2% in mwer), using the three evaluation methods, translation quality is equal or higher when using random heterogenous data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L190">
<title id=" I08-1068.xml">statistical machine translation models for personalized search </title>
<section> personalized search.  </section>
<citcontext>
<prevsection>
<prevsent>we believe short snippet usually (we considered 15) words to the left and right of the query words, similar to ashort snippet displayed by search engines can better capture the context of the query.
</prevsent>
<prevsent>indeed we experimented with different context sizes for drel.the first is using the whole document i.e., considering the query and concatenation of all the relevant documents as pair in the parallel texts extracted which is called ddocuments the second is using justa short text snippet from the document in the context of query instead of the whole document whichis called dsnippets details are described in the experiments section.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
5.1.2 learning translation model according to the standard statistical translation model (brown et al, 1993), <papid> J93-2003 </papid>we can find the optimal model m?</citsent>
<aftsection>
<nextsent>by maximizing the probability of generating queries from documents or m?
</nextsent>
<nextsent>= argmax n?
</nextsent>
<nextsent>i=1 (qi|di,m) 524 qw dw p(qw|dw,u) journal kdd 0.0176 journal conference 0.0123 journal journal 0.0176 journal sigkdd 0.0088 journal discovery 0.0211 journal mining 0.0017 journal acm 0.0088 music music 0.0375 music purchase 0.0090 music mp3 0.0090 music listen 0.0180 music mp3.com 0.0450 music free 0.0008 table 1: sample user profile to find the optimal word translation probabilities (qw|dw,m?), we can use the em algorithm.
</nextsent>
<nextsent>the details of the algorithm can be found in the literature for statistical translation models, such as (brown et al., 1993).<papid> J93-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L193">
<title id=" I08-1068.xml">statistical machine translation models for personalized search </title>
<section> personalized search.  </section>
<citcontext>
<prevsection>
<prevsent>ibm model1 (brown et al, 1993) <papid> J93-2003 </papid>is simplistic model which takes no account of the subtler aspects of language translation including the way word order tends to differ across languages.</prevsent>
<prevsent>similar to earlier work (berger and lafferty, 1999), we use ibm model1 because we believe it is more suited for irbecause the subtler aspects of language used forma chine translation can be ignored for ir.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
giza++(och and ney, 2003), <papid> J03-1002 </papid>an open source tool which implements the ibm models which we have used in our work for computing the translation probabilities.</citsent>
<aftsection>
<nextsent>a sample user profile learned is shown in table 1.
</nextsent>
<nextsent>5.2 re-ranking.
</nextsent>
<nextsent>re-ranking is phase in personalized search where the set of documents matching the query retrieved by general search engine are re-scored using the user profile and then re-ranked in descending orderof rank of the document.
</nextsent>
<nextsent>we follow similar approach in our work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L194">
<title id=" I08-2101.xml">a multi document multilingual automatic summarization system </title>
<section> text summarization approaches.  </section>
<citcontext>
<prevsection>
<prevsent>the very first developed summarization methods were of this category (edmundson and wyllys, 1961).
</prevsent>
<prevsent>scoring policy in these systems was based on different features, such as term frequency and place of the sentences.
</prevsent>
</prevsection>
<citsent citstr=" A00-2024 ">
vector space models (salton et. al., 1994), compression of sentences with automatic translation approaches (knight and marcu, 2000), hidden markov model (jing and mckeown, 2000), <papid> A00-2024 </papid>topic signatures based methods (lin and hovy, 2000, lacatusu et al , 2006) are among the most popular techniques that have been used in the summarization systems of this category.</citsent>
<aftsection>
<nextsent>the second groups of extraction-based methods, shallow understanding approaches use some information about words or textual units and their dependencies to improve the performance of extraction.
</nextsent>
<nextsent>the understanding can be induced using dependencies between words (barzilay and elhadad, 1997), <papid> W97-0703 </papid>rhetorical relations (paice and johns, 1993), events (filatova and hatzivassiloglou, 2004).<papid> W04-1017 </papid></nextsent>
<nextsent>in all of these methods, the most focused dependencies are used as measure for saliency of each textual unit.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L195">
<title id=" I08-2101.xml">a multi document multilingual automatic summarization system </title>
<section> text summarization approaches.  </section>
<citcontext>
<prevsection>
<prevsent>vector space models (salton et. al., 1994), compression of sentences with automatic translation approaches (knight and marcu, 2000), hidden markov model (jing and mckeown, 2000), <papid> A00-2024 </papid>topic signatures based methods (lin and hovy, 2000, lacatusu et al , 2006) are among the most popular techniques that have been used in the summarization systems of this category.</prevsent>
<prevsent>the second groups of extraction-based methods, shallow understanding approaches use some information about words or textual units and their dependencies to improve the performance of ex traction.</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
the understanding can be induced using dependencies between words (barzilay and elhadad, 1997), <papid> W97-0703 </papid>rhetorical relations (paice and johns, 1993), events (filatova and hatzivassiloglou, 2004).<papid> W04-1017 </papid></citsent>
<aftsection>
<nextsent>in all of these methods, the most focused dependencies are used as measure for saliency of each textual unit.
</nextsent>
<nextsent>the third group, knowledge-based approaches, uses particular domain knowledge in discriminating the important parts of input documents.
</nextsent>
<nextsent>this knowledge is usually taking some assumptions about the working domain.
</nextsent>
<nextsent>centrifuger (elhadad et al ., 2005) is good example of systems in this category, which operates in medical domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L196">
<title id=" I08-2101.xml">a multi document multilingual automatic summarization system </title>
<section> text summarization approaches.  </section>
<citcontext>
<prevsection>
<prevsent>vector space models (salton et. al., 1994), compression of sentences with automatic translation approaches (knight and marcu, 2000), hidden markov model (jing and mckeown, 2000), <papid> A00-2024 </papid>topic signatures based methods (lin and hovy, 2000, lacatusu et al , 2006) are among the most popular techniques that have been used in the summarization systems of this category.</prevsent>
<prevsent>the second groups of extraction-based methods, shallow understanding approaches use some information about words or textual units and their dependencies to improve the performance of ex traction.</prevsent>
</prevsection>
<citsent citstr=" W04-1017 ">
the understanding can be induced using dependencies between words (barzilay and elhadad, 1997), <papid> W97-0703 </papid>rhetorical relations (paice and johns, 1993), events (filatova and hatzivassiloglou, 2004).<papid> W04-1017 </papid></citsent>
<aftsection>
<nextsent>in all of these methods, the most focused dependencies are used as measure for saliency of each textual unit.
</nextsent>
<nextsent>the third group, knowledge-based approaches, uses particular domain knowledge in discriminating the important parts of input documents.
</nextsent>
<nextsent>this knowledge is usually taking some assumptions about the working domain.
</nextsent>
<nextsent>centrifuger (elhadad et al ., 2005) is good example of systems in this category, which operates in medical domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L197">
<title id=" I08-1037.xml">learning patterns from the web to translate named entities for cross language information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one key way to meet this demand is to retrieve information written in chinese by using korean queries, referred to as korean-chinese cross-language information retrieval (kcir).
</prevsent>
<prevsent>the main challenge involves translating nes because they are usually the main concepts of queries.
</prevsent>
</prevsection>
<citsent citstr=" P98-1036 ">
in (chen et al, 1998), <papid> P98-1036 </papid>the authors roman ized chinese nes and selected their english transliterations from english nes extracted from the web by comparing their phonetic similarities with chinese nes.</citsent>
<aftsection>
<nextsent>yaser al-onaizan (al-onaizan and knight, 2002) 281transliterated an ne in arabic into several candidates in english and ranked the candidates by comparing their counts in several english corpora.
</nextsent>
<nextsent>unlike the above works, whose target languages are alphabetic, in k-c translation, the target language is chinese, which uses an ideo graphic writing system.
</nextsent>
<nextsent>korean-chinese net is much more difficult than net considered in previous works because, in chinese, one syllable may map to tens or hundreds of characters.for example, if an ne written in korean comprises three syllables, there may be thousands of possible translation candidates in chinese.
</nextsent>
<nextsent>in this paper, we propose an effective hybrid net method which can help improve performance of cross-language information retrieval systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L198">
<title id=" I05-2011.xml">automatic detection of opinion bearing words and sentences </title>
<section> past computational studies.  </section>
<citcontext>
<prevsection>
<prevsent>we will no longer discuss belief, holder, or topic.
</prevsent>
<prevsent>61 (soboroff and harman, 2003) of the trec-2003 competition included task of recognizing opin ion-bearing sentences (see section 5.2).
</prevsent>
</prevsection>
<citsent citstr=" W03-2102 ">
wilson and wiebe (2003) <papid> W03-2102 </papid>developed an annotation scheme for so-called subjective sentences (opinions and other private states) as part of u.s. government-sponsored project (arda aquaint nrrc) in 2002.</citsent>
<aftsection>
<nextsent>they created corpus, mpqa, containing news articles manually annotated.
</nextsent>
<nextsent>several other approaches have been applied for learning words and phrases that signal subjectivity.
</nextsent>
<nextsent>turney (2002) <papid> P02-1053 </papid>and wiebe (2000) focused on learning adjectives and adjec tival phrases and wiebe et al (2001) focused on nouns.</nextsent>
<nextsent>riloff et al (2003) <papid> W03-0404 </papid>extracted nouns and riloff and wiebe (2003) <papid> W03-1014 </papid>extracted patterns for subjective expressions using bootstrapping process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L199">
<title id=" I05-2011.xml">automatic detection of opinion bearing words and sentences </title>
<section> past computational studies.  </section>
<citcontext>
<prevsection>
<prevsent>they created corpus, mpqa, containing news articles manually annotated.
</prevsent>
<prevsent>several other approaches have been applied for learning words and phrases that signal subjectivity.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
turney (2002) <papid> P02-1053 </papid>and wiebe (2000) focused on learning adjectives and adjec tival phrases and wiebe et al (2001) focused on nouns.</citsent>
<aftsection>
<nextsent>riloff et al (2003) <papid> W03-0404 </papid>extracted nouns and riloff and wiebe (2003) <papid> W03-1014 </papid>extracted patterns for subjective expressions using bootstrapping process.</nextsent>
<nextsent>we developed several collections of opinion bearing and non-opinion-bearing words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L200">
<title id=" I05-2011.xml">automatic detection of opinion bearing words and sentences </title>
<section> past computational studies.  </section>
<citcontext>
<prevsection>
<prevsent>several other approaches have been applied for learning words and phrases that signal subjectivity.
</prevsent>
<prevsent>turney (2002) <papid> P02-1053 </papid>and wiebe (2000) focused on learning adjectives and adjec tival phrases and wiebe et al (2001) focused on nouns.</prevsent>
</prevsection>
<citsent citstr=" W03-0404 ">
riloff et al (2003) <papid> W03-0404 </papid>extracted nouns and riloff and wiebe (2003) <papid> W03-1014 </papid>extracted patterns for subjective expressions using bootstrapping process.</citsent>
<aftsection>
<nextsent>we developed several collections of opinion bearing and non-opinion-bearing words.
</nextsent>
<nextsent>one is accurate but small; another is large but relatively inaccurate.
</nextsent>
<nextsent>we combined them to obtain more reliable list.
</nextsent>
<nextsent>we obtained an additional list from columbia university.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L201">
<title id=" I05-2011.xml">automatic detection of opinion bearing words and sentences </title>
<section> past computational studies.  </section>
<citcontext>
<prevsection>
<prevsent>several other approaches have been applied for learning words and phrases that signal subjectivity.
</prevsent>
<prevsent>turney (2002) <papid> P02-1053 </papid>and wiebe (2000) focused on learning adjectives and adjec tival phrases and wiebe et al (2001) focused on nouns.</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
riloff et al (2003) <papid> W03-0404 </papid>extracted nouns and riloff and wiebe (2003) <papid> W03-1014 </papid>extracted patterns for subjective expressions using bootstrapping process.</citsent>
<aftsection>
<nextsent>we developed several collections of opinion bearing and non-opinion-bearing words.
</nextsent>
<nextsent>one is accurate but small; another is large but relatively inaccurate.
</nextsent>
<nextsent>we combined them to obtain more reliable list.
</nextsent>
<nextsent>we obtained an additional list from columbia university.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L202">
<title id=" I05-2011.xml">automatic detection of opinion bearing words and sentences </title>
<section> data sources.  </section>
<citcontext>
<prevsection>
<prevsent>however, if we know the relative frequency of word in opinion-bearing texts compared to non-opinionbearing text, we can use the statistical information instead of lexical information.
</prevsent>
<prevsent>for this, we collected huge amount of data in order to make up for the limitations of collection 1.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
following the insight of yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>we made the basic and rough assumption that words that appear more often in newspaper editorials and letters to the editor than in non-editorial news articles could be potential opinion-bearing words (even though editorials contain sentences about factual events as well).</citsent>
<aftsection>
<nextsent>we used the trec collection to collect data, extracting and classifying all wall street journal documents from it either as editorial or non editorial based on the occurrence of the keywords letters to the editor?, letter to the editor?
</nextsent>
<nextsent>or editorial?
</nextsent>
<nextsent>present in its headline.
</nextsent>
<nextsent>this produced in total 7053 editorial documents and 166025 non-editorial documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L204">
<title id=" I08-1055.xml">corpus based question answering for why questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nazeqa, ajapanese why-qa system based on our approach, clearly outperforms baseline that uses hand-crafted patterns with mean reciprocal rank (top-5) of 0.305, making it presumably the best-performing fully implemented why-qa system.
</prevsent>
<prevsent>following the trend of non-factoid qa, we are seeing the emergence of work on why-qa; e.g., answering generic why x??
</prevsent>
</prevsection>
<citsent citstr=" E06-3005 ">
questions (verberne,2006).<papid> E06-3005 </papid></citsent>
<aftsection>
<nextsent>however, since why-qa is an inherently difficult problem, there have only been small number of fully implemented systems dedicated to solvingit.
</nextsent>
<nextsent>recent systems at ntcir-61 question answering challenge (qac-4) can handle why-questions(fukumoto et al, 2007).
</nextsent>
<nextsent>however, their performance is much lower (mori et al, 2007) than that of factoid qa systems (fukumoto et al, 2004; voorhees and dang, 2005).
</nextsent>
<nextsent>we consider that this low performance is due to the great amount of hand-crafting involved in the 1http://research.nii.ac.jp/ntcir/ntcir-ws6/ws-en.html systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L205">
<title id=" I08-1055.xml">corpus based question answering for why questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such patterns include typical cue phrases and pos-tag sequences related to causality, such as because of?
</prevsent>
<prevsent>and by reason of.?
</prevsent>
</prevsection>
<citsent citstr=" W05-0306 ">
however, as noted in (inui and okumura, 2005), <papid> W05-0306 </papid>causes are expressed in various forms, and it is difficult to cover all such expressions by hand.</citsent>
<aftsection>
<nextsent>hand-crafting is also very costly.
</nextsent>
<nextsent>some patterns may be more indicative of causes than others.
</nextsent>
<nextsent>therefore, it may be useful to assign different weights to the patterns for better answer candidate extraction, but currently this must be done by hand (mori et al, 2007).
</nextsent>
<nextsent>it is not clear whether the weights determined by hand are suitable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L210">
<title id=" I08-1055.xml">corpus based question answering for why questions </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>the actual instances of the features, the corpus, and the ranker will be presented in section 4.
</prevsent>
<prevsent>3.1 causal expression features.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
with the increasing attention paid to srl, we currently have number of corpora, such as propbank (palmer, 2005) and framenet (baker et al, 1998), <papid> P98-1013 </papid>that are tagged with semantic relations including causal relation.</citsent>
<aftsection>
<nextsent>since text spans for such relations are annotated in the corpora, we can simply collect the spans marked by causal relation as causal expressions.
</nextsent>
<nextsent>since an answer candidate that has matching expression for one of the collected causal expressions is likely to be expressing cause as well, we can make the existence of each expression feature.
</nextsent>
<nextsent>although the collected causal expressions without any modification might be used to create features, for generality, it would be better to abstract them into syntactic patterns.
</nextsent>
<nextsent>from causal expres sions/patterns automatically extracted from corpora, we can create binary features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L211">
<title id=" I08-1055.xml">corpus based question answering for why questions </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>for such cases, we can exploit existing thesauri.
</prevsent>
<prevsent>we can create feature encoding whether synonyms ofwords in the question are found in the answer candidate.
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
we could also use the value of semantic similarity and relatedness measures (pedersen et al,2004) <papid> N04-3012 </papid>or the existence of hypernym or hyponym relations as features.</citsent>
<aftsection>
<nextsent>3.3 causal relation features.
</nextsent>
<nextsent>there are semantic lexicons where semantic relation between concepts is indicated.
</nextsent>
<nextsent>for example, the edr dictionary3 shows whether causal relation holds between two concepts; e.g., between murder?
</nextsent>
<nextsent>and arrest.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L212">
<title id=" I08-1055.xml">corpus based question answering for why questions </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 100 200 300 400 500 600 700 800 900 r number of training samples top-1 top-5 top-10 top-20 figure 3: learning curve: performance changes when answering q1q100 with different sizes of training samples.
</prevsent>
<prevsent>paragraphs are used as answer candidates.
</prevsent>
</prevsection>
<citsent citstr=" C02-1053 ">
their quality itself may be to blame.furthermore, analyzing the trained ranking models allows us to calculate the weights given to the features (hirao et al, 2002).<papid> C02-1053 </papid></citsent>
<aftsection>
<nextsent>table 3 shows the weights of the top-10 features.
</nextsent>
<nextsent>we also include inthe table the weights of the synonym pair, man causal expression and cause effect pair features sothat the role of all three types of features in our approach can be shown.
</nextsent>
<nextsent>the analyzed model was the one trained with all 1,000 questions in the whyqacollection with paragraphs as answers.
</nextsent>
<nextsent>just as suggested by table 2, the question-candidate cosinesimilarity feature plays key role, followed by automatically collected causal expression features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L214">
<title id=" I05-2035.xml">rapid prototyping of scalable grammars towards modularity in extensions to a language independent core </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we illustrate the approach for several phenomena and explore the interdependence of the modules.
</prevsent>
<prevsent>manual development of precise broad-coverage grammar implementations, useful in range of natural language processing/understanding tasks, is labor-intensive undertaking, requiring many years of work by highly trained linguists.
</prevsent>
</prevsection>
<citsent citstr=" W02-1503 ">
many recent efforts toward reducing the time and level of expertise needed to produce new grammar have focused on adapting an existing grammar of another language (butt et al, 2002; <papid> W02-1503 </papid>kim et al,2003; bateman et al, ip).</citsent>
<aftsection>
<nextsent>our work on the grammar matrix?
</nextsent>
<nextsent>has pursued an alternative approach,identifying set of language-independent grammar constraints to which language-specific constraints can be added (bender et al, 2002).
</nextsent>
<nextsent>this approach has the hitherto unexploited potential to benefit from the substantial theoretical work on language typology.
</nextsent>
<nextsent>in this paper, we pre senta prototype grammar matrix customization system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L215">
<title id=" I05-2035.xml">rapid prototyping of scalable grammars towards modularity in extensions to a language independent core </title>
<section> the grammar matrix.  </section>
<citcontext>
<prevsection>
<prevsent>the prototype addresses basic word order, sentential negation, yes no questions, and small range of lexical entries.
</prevsent>
<prevsent>wide-coverage grammars representing deep linguistic analysis exist in several frameworks, including head-driven phrase structure grammar(hpsg), lexical-functional grammar, and lexicalized tree adjoining grammar.
</prevsent>
</prevsection>
<citsent citstr=" W02-1210 ">
in hpsg (p. and sag, 1994), the most extensive grammars are those of english (flickinger, 2000), german (hinrichs et al, 1997; muller and kasper, 2000; crysmann, ip), and japanese (siegel, 2000; sieg eland bender, 2002).<papid> W02-1210 </papid></citsent>
<aftsection>
<nextsent>the grammar matrix is an attempt to distill the wisdom of existing grammars and document it in form that can be used as the basis for new grammars.
</nextsent>
<nextsent>the main goals of the project are: (i) to develop in detail semantic representations and the syntax-semantics interface,consistent with other work in hpsg; (ii) to represent generalizations across linguistic objects and across languages; and (iii) to allow for very quick start-up as the matrix is applied to new languages.
</nextsent>
<nextsent>the original grammar matrix consisted of types defining the basic feature geometry, types associated with minimal recur sion semantics(e.g., (copestake et al, 2001)), <papid> P01-1019 </papid>types for lex 203 ical and syntactic rules, and configuration files for the lkb grammar development environment (copestake, 2002) and the pet system (callmeier,2000).</nextsent>
<nextsent>subsequent releases have refined the original types and developed lexical hierarchy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L216">
<title id=" I05-2035.xml">rapid prototyping of scalable grammars towards modularity in extensions to a language independent core </title>
<section> the grammar matrix.  </section>
<citcontext>
<prevsection>
<prevsent>the grammar matrix is an attempt to distill the wisdom of existing grammars and document it in form that can be used as the basis for new grammars.
</prevsent>
<prevsent>the main goals of the project are: (i) to develop in detail semantic representations and the syntax-semantics interface,consistent with other work in hpsg; (ii) to represent generalizations across linguistic objects and across languages; and (iii) to allow for very quick start-up as the matrix is applied to new languages.
</prevsent>
</prevsection>
<citsent citstr=" P01-1019 ">
the original grammar matrix consisted of types defining the basic feature geometry, types associated with minimal recur sion semantics(e.g., (copestake et al, 2001)), <papid> P01-1019 </papid>types for lex 203 ical and syntactic rules, and configuration files for the lkb grammar development environment (copestake, 2002) and the pet system (callmeier,2000).</citsent>
<aftsection>
<nextsent>subsequent releases have refined the original types and developed lexical hierarchy.
</nextsent>
<nextsent>the constraints in this core?
</nextsent>
<nextsent>matrix are intended to belanguage-independent and monotonic ally extensible in any given grammar.
</nextsent>
<nextsent>with the typology based modules presented here, we extend the constraint definitions which can be supplied to grammar developers to those that capture generalizations holding only for subsets of languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L217">
<title id=" I05-2035.xml">rapid prototyping of scalable grammars towards modularity in extensions to a language independent core </title>
<section> limits of modularity.  </section>
<citcontext>
<prevsection>
<prevsent>note that the forms are assumed to be fully inflected (modulo negation), support morphological processes awaiting future work.
</prevsent>
<prevsent>we use this information and the knowledge base to produce set of lexical types inheriting from the types defined in the core matrix and specifying appropriate language-specific constraints, and set of lexical entries.
</prevsent>
</prevsection>
<citsent citstr=" C96-2106 ">
recent computational work in hpsg has asked whether different parts of single grammar canbe abstracted into separate, independent modules, either for processing (kasper and krieger,1996; <papid> C96-2106 </papid>theofilidis et al, 1997) or grammar development (keselj, 2001).</citsent>
<aftsection>
<nextsent>our work is most similar to keseljs though we are pursuing different goals: keselj is looking to support division of labor among multiple individuals working on the same grammar and to support variants of single grammar for different domains.
</nextsent>
<nextsent>his modules each have private and public features and types, and he illustrates the approach with small-scale question answering system.
</nextsent>
<nextsent>in contrast, we are approaching this issue from the perspective of reuse of grammar code in the context of multilingual grammar engineering (a possibility suggested, but not developed, by theofilidis et al. our notion of modularity is influenced by the following constraints: (i) the questions in the customization interface must be sensible to the working linguist; (ii) the resulting starter grammars must be highly readable so that they can be extended by the grammar developer (typicallyonly one per grammar); and (iii) hpsg practice values capturing linguistic generalizations by having single types encode many different constraints and, ideally, single constraints contribute to the analysis of many different phenomena.
</nextsent>
<nextsent>even with the modest linguistic coverage of the existing system, we have found many cases of non-trivial interaction between the modules:our phrase structure rules, following hpsg practice, capture cross-categorial generalizations: ifboth verbs and adpositions follow their complements, then single complement-head rule serves for both.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L218">
<title id=" I05-5005.xml">structural variation in generated health reports </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>a summarised report based on about 1000 input events is constructed in less than 2 seconds, speed which is highly appropriate to the demands of clinical practice.
</prevsent>
<prevsent>while the various types of generated report all share the same input (i.e., the patients chronicle), and thus will have large degree of conceptual overlap, clearly there will be occassions when information that is included in some reports will not be in others.
</prevsent>
</prevsection>
<citsent citstr=" W03-1601 ">
the range of reports for any given patient at any given point in their illness thus present special class of paraphrase, with looser adherance to semantic equivalence between versions than is typically found in other paraphrase generators, for example kozlowski et al(2003), <papid> W03-1601 </papid>mckeown et al (1994), <papid> A94-1002 </papid>power, scott and bouyaad-agha (2003), rosner and stede (1994),(1996), and scott and souza (1990).</citsent>
<aftsection>
<nextsent>in this sense, our report generator is rather closer in spirit to hovys pauline system, which generates descriptions of given news events from different perspectives and with different stylistic goals (hovy, 1988).
</nextsent>
<nextsent>however, we achieve our goal with less reliance on terminological variation and more on structural variation at the discourse level.
</nextsent>
<nextsent>syntactic variation, where it does occur, is almost always simply side-effect of an earlier discourse choice.
</nextsent>
<nextsent>terminological variation is deliberately avoided to prevent false implicatures; however, we are about to introduce further class of readership, namely patients, at which stage we will make fuller use of our lexical resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L219">
<title id=" I05-5005.xml">structural variation in generated health reports </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>a summarised report based on about 1000 input events is constructed in less than 2 seconds, speed which is highly appropriate to the demands of clinical practice.
</prevsent>
<prevsent>while the various types of generated report all share the same input (i.e., the patients chronicle), and thus will have large degree of conceptual overlap, clearly there will be occassions when information that is included in some reports will not be in others.
</prevsent>
</prevsection>
<citsent citstr=" A94-1002 ">
the range of reports for any given patient at any given point in their illness thus present special class of paraphrase, with looser adherance to semantic equivalence between versions than is typically found in other paraphrase generators, for example kozlowski et al(2003), <papid> W03-1601 </papid>mckeown et al (1994), <papid> A94-1002 </papid>power, scott and bouyaad-agha (2003), rosner and stede (1994),(1996), and scott and souza (1990).</citsent>
<aftsection>
<nextsent>in this sense, our report generator is rather closer in spirit to hovys pauline system, which generates descriptions of given news events from different perspectives and with different stylistic goals (hovy, 1988).
</nextsent>
<nextsent>however, we achieve our goal with less reliance on terminological variation and more on structural variation at the discourse level.
</nextsent>
<nextsent>syntactic variation, where it does occur, is almost always simply side-effect of an earlier discourse choice.
</nextsent>
<nextsent>terminological variation is deliberately avoided to prevent false implicatures; however, we are about to introduce further class of readership, namely patients, at which stage we will make fuller use of our lexical resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L220">
<title id=" I05-3031.xml">two phase lmrrc tagging for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results show that our scheme in integrated mode performs the best in terms of accuracy, where separated mode ismore suitable under limited computational resources.
</prevsent>
<prevsent>the chinese word segmentation is non-trivial task because no explicit delimiters (like spaces in english) are used for word separation.
</prevsent>
</prevsection>
<citsent citstr=" J96-3004 ">
as thetask is an important precursor to many natural language processing systems, it receives lot of attentions in the literature for the past decade (wuand tseng, 1993; sproat et al, 1996).<papid> J96-3004 </papid></citsent>
<aftsection>
<nextsent>in this paper, we propose statistical approach based on the works of (xue and shen, 2003), <papid> W03-1728 </papid>in which the chinese word segmentation problem is first transformed into tagging problem, then the maximum entropy classifier is applied to solve theproblem.</nextsent>
<nextsent>we further improve the scheme by introducing correctional treatments after first roundtagging.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L221">
<title id=" I05-3031.xml">two phase lmrrc tagging for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the chinese word segmentation is non-trivial task because no explicit delimiters (like spaces in english) are used for word separation.
</prevsent>
<prevsent>as thetask is an important precursor to many natural language processing systems, it receives lot of attentions in the literature for the past decade (wuand tseng, 1993; sproat et al, 1996).<papid> J96-3004 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1728 ">
in this paper, we propose statistical approach based on the works of (xue and shen, 2003), <papid> W03-1728 </papid>in which the chinese word segmentation problem is first transformed into tagging problem, then the maximum entropy classifier is applied to solve theproblem.</citsent>
<aftsection>
<nextsent>we further improve the scheme by introducing correctional treatments after first roundtagging.
</nextsent>
<nextsent>two different training methods are proposed to suit our scheme.the paper is organized as follows.
</nextsent>
<nextsent>in section 2, we briefly discuss the scheme proposed by (xue and shen, 2003), <papid> W03-1728 </papid>followed by our additional works to improve the performance.</nextsent>
<nextsent>experimental and bakeoff results are presented in section 3.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L232">
<title id=" I05-3031.xml">two phase lmrrc tagging for chinese word segmentation </title>
<section> our proposed approach.  </section>
<citcontext>
<prevsection>
<prevsent>one of the difficulties in chinese word segmentation is that, chinese characters can appear in different positions within word (xue and shen, 2003), <papid> W03-1728 </papid>and lmr tagging was proposed to solve the problem.</prevsent>
<prevsent>the basic idea of lmr tagging is to assign to each character, based on its contextual information, tag which represents its relative position within the word.</prevsent>
</prevsection>
<citsent citstr=" W04-3236 ">
note that the original tag set used by (xue and shen, 2003) <papid> W03-1728 </papid>is simplified and improved by (ng and low, 2004)<papid> W04-3236 </papid>we shall then adopt and illustrate the simplified case here.</citsent>
<aftsection>
<nextsent>the tags and their meanings are summarized in table 1.
</nextsent>
<nextsent>tag l, m, and correspond to the character at the beginning, in the middle, and at the end of the word respectively.
</nextsent>
<nextsent>tag means the character is single-character?
</nextsent>
<nextsent>word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L234">
<title id=" I05-3031.xml">two phase lmrrc tagging for chinese word segmentation </title>
<section> our proposed approach.  </section>
<citcontext>
<prevsection>
<prevsent>tagging: ??
</prevsent>
<prevsent>figure 1: example of lmr tagging.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
v. j. della pietra, 1996; ratnaparkhi, 1996) <papid> W96-0213 </papid>was proposed in the original work to solve the lmrtagging problem.</citsent>
<aftsection>
<nextsent>in order to make maxent success in lmr tagging, feature templates used in capturing useful contextual information must be carefully designed.
</nextsent>
<nextsent>furthermore, it is unavoidable that invalid tag sequences will occur if we just assign the tag with the highest probability.
</nextsent>
<nextsent>inthe next subsection, we describe the feature templates and measures used to correct the tagging.
</nextsent>
<nextsent>table 1: tags used in lmr tagging scheme.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L238">
<title id=" I05-3033.xml">towards a hybrid model for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the segmenter achieved 92.8% f-score and 72.8% recall for oov words in the closed track of the peking university corpus in the second international chinese word segmentation bakeoff.
</prevsent>
<prevsent>this paper describes hybrid chinese word segmenter that participated in the closed track of the peking university corpus in the second international chinese word segmentation bake off.
</prevsent>
</prevsection>
<citsent citstr=" P05-2001 ">
this segmenter is still in its early stage of development and is being developed as part of larger chinese unknown word resolution system that performs the identification, part of speech guessing, and sense guessing of chinese unknown words (lu, 2005).<papid> P05-2001 </papid></citsent>
<aftsection>
<nextsent>the segmenter consists of two major components.
</nextsent>
<nextsent>first, tagging component tags each individual character in sentence with position-of character (poc) tag that indicates the position of the character in word.
</nextsent>
<nextsent>this could be one of the following four possibilities, i.e., the character is either monosyllabic word or is in word initial, middle, or final position.
</nextsent>
<nextsent>this component is based on the transformation-based learning (tbl) algorithm (brill, 1995), <papid> J95-4004 </papid>where simple first-order hmm tagger (charniak et al, 1993) is used to produce an initial tagging of character sequence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L240">
<title id=" I05-3033.xml">towards a hybrid model for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>first, tagging component tags each individual character in sentence with position-of character (poc) tag that indicates the position of the character in word.
</prevsent>
<prevsent>this could be one of the following four possibilities, i.e., the character is either monosyllabic word or is in word initial, middle, or final position.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
this component is based on the transformation-based learning (tbl) algorithm (brill, 1995), <papid> J95-4004 </papid>where simple first-order hmm tagger (charniak et al, 1993) is used to produce an initial tagging of character sequence.</citsent>
<aftsection>
<nextsent>second, merging component transforms the output of the tagging component, i.e., poc-tagged character sequence, into word-segmented sentence.
</nextsent>
<nextsent>whereas this process relies largely on the poc tags assigned to the individual characters, it also takes advantage of number of heuristics generalized from the training data to handle non-chinese characters, numeric type compounds, and long words.
</nextsent>
<nextsent>the approach adopted here is reminiscent of the line of research that employs the idea of character-based tagging for chinese word segmentation and/or unknown word identification (goh et al, 2003; <papid> P03-2039 </papid>xue, 2003; zhang et al, 2002).<papid> W02-1817 </papid></nextsent>
<nextsent>the notion of character-based tagging allows us to model the tendency for individual characters to combine with other characters to form words in different contexts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L241">
<title id=" I05-3033.xml">towards a hybrid model for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, merging component transforms the output of the tagging component, i.e., poc-tagged character sequence, into word-segmented sentence.
</prevsent>
<prevsent>whereas this process relies largely on the poc tags assigned to the individual characters, it also takes advantage of number of heuristics generalized from the training data to handle non-chinese characters, numeric type compounds, and long words.
</prevsent>
</prevsection>
<citsent citstr=" P03-2039 ">
the approach adopted here is reminiscent of the line of research that employs the idea of character-based tagging for chinese word segmentation and/or unknown word identification (goh et al, 2003; <papid> P03-2039 </papid>xue, 2003; zhang et al, 2002).<papid> W02-1817 </papid></citsent>
<aftsection>
<nextsent>the notion of character-based tagging allows us to model the tendency for individual characters to combine with other characters to form words in different contexts.
</nextsent>
<nextsent>this property gives the model good potential for improving the performance of chinese unknown word identification, major concern of the chinese unknown word resolution system that the seg menter is part of.
</nextsent>
<nextsent>the rest of the paper is organized as follows.
</nextsent>
<nextsent>section two describes the system architecture.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L242">
<title id=" I05-3033.xml">towards a hybrid model for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, merging component transforms the output of the tagging component, i.e., poc-tagged character sequence, into word-segmented sentence.
</prevsent>
<prevsent>whereas this process relies largely on the poc tags assigned to the individual characters, it also takes advantage of number of heuristics generalized from the training data to handle non-chinese characters, numeric type compounds, and long words.
</prevsent>
</prevsection>
<citsent citstr=" W02-1817 ">
the approach adopted here is reminiscent of the line of research that employs the idea of character-based tagging for chinese word segmentation and/or unknown word identification (goh et al, 2003; <papid> P03-2039 </papid>xue, 2003; zhang et al, 2002).<papid> W02-1817 </papid></citsent>
<aftsection>
<nextsent>the notion of character-based tagging allows us to model the tendency for individual characters to combine with other characters to form words in different contexts.
</nextsent>
<nextsent>this property gives the model good potential for improving the performance of chinese unknown word identification, major concern of the chinese unknown word resolution system that the seg menter is part of.
</nextsent>
<nextsent>the rest of the paper is organized as follows.
</nextsent>
<nextsent>section two describes the system architecture.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L249">
<title id=" I05-5007.xml">automated generalization of phrasal paraphrases from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also design three metrics to measure our generalized templates.
</prevsent>
<prevsent>the experimental results show that the representation method is reasonable and the generalized templates have higher precision and coverage.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
paraphrases are alternative ways to convey the same information (barzilay and mckeown, 2001) <papid> P01-1008 </papid>and they have been applied in many fields of natural language processing.</citsent>
<aftsection>
<nextsent>there are many previous work on paraphrase examples extraction or combining them with some applications such as information retrieval and question answering (agichtein et al, 2001; florence et al, 2003; rinaldi et al, 2003; <papid> W03-1604 </papid>tomuro, 2003; <papid> W03-1605 </papid>lin and pantel, 2001;), information extraction (shinyama et al, 2002; shinyama and sekine, 2003), <papid> W03-1609 </papid>machine translation (hiroshi et al, 2003; zhang and yamamoto, 2003), multi-document (barzilay et al, 2003).</nextsent>
<nextsent>there is also some other research about paraphrase.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L250">
<title id=" I05-5007.xml">automated generalization of phrasal paraphrases from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experimental results show that the representation method is reasonable and the generalized templates have higher precision and coverage.
</prevsent>
<prevsent>paraphrases are alternative ways to convey the same information (barzilay and mckeown, 2001) <papid> P01-1008 </papid>and they have been applied in many fields of natural language processing.</prevsent>
</prevsection>
<citsent citstr=" W03-1604 ">
there are many previous work on paraphrase examples extraction or combining them with some applications such as information retrieval and question answering (agichtein et al, 2001; florence et al, 2003; rinaldi et al, 2003; <papid> W03-1604 </papid>tomuro, 2003; <papid> W03-1605 </papid>lin and pantel, 2001;), information extraction (shinyama et al, 2002; shinyama and sekine, 2003), <papid> W03-1609 </papid>machine translation (hiroshi et al, 2003; zhang and yamamoto, 2003), multi-document (barzilay et al, 2003).</citsent>
<aftsection>
<nextsent>there is also some other research about paraphrase.
</nextsent>
<nextsent>(wu and zhou, 2003) <papid> W03-1610 </papid>just extract the synonymy collocation, such as  turn on, obj, light  and  switch on, obj, light  using both monolingual corpora and bilingual corpora to get an optimal result, but do not generalize them.</nextsent>
<nextsent>(glickman and dagan, 2003) detects verb paraphrases instances within single corpus without relying on any priori structure and in formation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L251">
<title id=" I05-5007.xml">automated generalization of phrasal paraphrases from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experimental results show that the representation method is reasonable and the generalized templates have higher precision and coverage.
</prevsent>
<prevsent>paraphrases are alternative ways to convey the same information (barzilay and mckeown, 2001) <papid> P01-1008 </papid>and they have been applied in many fields of natural language processing.</prevsent>
</prevsection>
<citsent citstr=" W03-1605 ">
there are many previous work on paraphrase examples extraction or combining them with some applications such as information retrieval and question answering (agichtein et al, 2001; florence et al, 2003; rinaldi et al, 2003; <papid> W03-1604 </papid>tomuro, 2003; <papid> W03-1605 </papid>lin and pantel, 2001;), information extraction (shinyama et al, 2002; shinyama and sekine, 2003), <papid> W03-1609 </papid>machine translation (hiroshi et al, 2003; zhang and yamamoto, 2003), multi-document (barzilay et al, 2003).</citsent>
<aftsection>
<nextsent>there is also some other research about paraphrase.
</nextsent>
<nextsent>(wu and zhou, 2003) <papid> W03-1610 </papid>just extract the synonymy collocation, such as  turn on, obj, light  and  switch on, obj, light  using both monolingual corpora and bilingual corpora to get an optimal result, but do not generalize them.</nextsent>
<nextsent>(glickman and dagan, 2003) detects verb paraphrases instances within single corpus without relying on any priori structure and in formation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L252">
<title id=" I05-5007.xml">automated generalization of phrasal paraphrases from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experimental results show that the representation method is reasonable and the generalized templates have higher precision and coverage.
</prevsent>
<prevsent>paraphrases are alternative ways to convey the same information (barzilay and mckeown, 2001) <papid> P01-1008 </papid>and they have been applied in many fields of natural language processing.</prevsent>
</prevsection>
<citsent citstr=" W03-1609 ">
there are many previous work on paraphrase examples extraction or combining them with some applications such as information retrieval and question answering (agichtein et al, 2001; florence et al, 2003; rinaldi et al, 2003; <papid> W03-1604 </papid>tomuro, 2003; <papid> W03-1605 </papid>lin and pantel, 2001;), information extraction (shinyama et al, 2002; shinyama and sekine, 2003), <papid> W03-1609 </papid>machine translation (hiroshi et al, 2003; zhang and yamamoto, 2003), multi-document (barzilay et al, 2003).</citsent>
<aftsection>
<nextsent>there is also some other research about paraphrase.
</nextsent>
<nextsent>(wu and zhou, 2003) <papid> W03-1610 </papid>just extract the synonymy collocation, such as  turn on, obj, light  and  switch on, obj, light  using both monolingual corpora and bilingual corpora to get an optimal result, but do not generalize them.</nextsent>
<nextsent>(glickman and dagan, 2003) detects verb paraphrases instances within single corpus without relying on any priori structure and in formation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L254">
<title id=" I05-5007.xml">automated generalization of phrasal paraphrases from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are many previous work on paraphrase examples extraction or combining them with some applications such as information retrieval and question answering (agichtein et al, 2001; florence et al, 2003; rinaldi et al, 2003; <papid> W03-1604 </papid>tomuro, 2003; <papid> W03-1605 </papid>lin and pantel, 2001;), information extraction (shinyama et al, 2002; shinyama and sekine, 2003), <papid> W03-1609 </papid>machine translation (hiroshi et al, 2003; zhang and yamamoto, 2003), multi-document (barzilay et al, 2003).</prevsent>
<prevsent>there is also some other research about paraphrase.</prevsent>
</prevsection>
<citsent citstr=" W03-1610 ">
(wu and zhou, 2003) <papid> W03-1610 </papid>just extract the synonymy collocation, such as  turn on, obj, light  and  switch on, obj, light  using both monolingual corpora and bilingual corpora to get an optimal result, but do not generalize them.</citsent>
<aftsection>
<nextsent>(glickman and dagan, 2003) detects verb paraphrases instances within single corpus without relying on any priori structure and information.
</nextsent>
<nextsent>generation of paraphrase examples was also investigated (barzilay and lee, 2003; <papid> N03-1003 </papid>quirk et al, 2004).<papid> W04-3219 </papid></nextsent>
<nextsent>rather than creating and storing thousands of paraphrases, paraphrase templates have strong representation capacity and can be used to generate many paraphrase examples.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L256">
<title id=" I05-5007.xml">automated generalization of phrasal paraphrases from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(wu and zhou, 2003) <papid> W03-1610 </papid>just extract the synonymy collocation, such as  turn on, obj, light  and  switch on, obj, light  using both monolingual corpora and bilingual corpora to get an optimal result, but do not generalize them.</prevsent>
<prevsent>(glickman and dagan, 2003) detects verb paraphrases instances within single corpus without relying on any priori structure and in formation.</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
generation of paraphrase examples was also investigated (barzilay and lee, 2003; <papid> N03-1003 </papid>quirk et al, 2004).<papid> W04-3219 </papid></citsent>
<aftsection>
<nextsent>rather than creating and storing thousands of paraphrases, paraphrase templates have strong representation capacity and can be used to generate many paraphrase examples.
</nextsent>
<nextsent>as (hirst, 2003) said, for each aspect of paraphrase there are two main challenges: representation of knowledge and acquisition of knowledge.
</nextsent>
<nextsent>corresponding to the problem of generalization of paraphrase templates, there are also two prob lems: the first is the representation of paraphrase templates and the second is acquisition of paraphrase templates.
</nextsent>
<nextsent>there are several methods about paraphrase templates representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L257">
<title id=" I05-5007.xml">automated generalization of phrasal paraphrases from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(wu and zhou, 2003) <papid> W03-1610 </papid>just extract the synonymy collocation, such as  turn on, obj, light  and  switch on, obj, light  using both monolingual corpora and bilingual corpora to get an optimal result, but do not generalize them.</prevsent>
<prevsent>(glickman and dagan, 2003) detects verb paraphrases instances within single corpus without relying on any priori structure and in formation.</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
generation of paraphrase examples was also investigated (barzilay and lee, 2003; <papid> N03-1003 </papid>quirk et al, 2004).<papid> W04-3219 </papid></citsent>
<aftsection>
<nextsent>rather than creating and storing thousands of paraphrases, paraphrase templates have strong representation capacity and can be used to generate many paraphrase examples.
</nextsent>
<nextsent>as (hirst, 2003) said, for each aspect of paraphrase there are two main challenges: representation of knowledge and acquisition of knowledge.
</nextsent>
<nextsent>corresponding to the problem of generalization of paraphrase templates, there are also two prob lems: the first is the representation of paraphrase templates and the second is acquisition of paraphrase templates.
</nextsent>
<nextsent>there are several methods about paraphrase templates representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L261">
<title id=" I08-1007.xml">orthographic disambiguation incorporating transliterated probability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mer problem (transliteration); our target scope is wider.
</prevsent>
<prevsent>we addressed both transliteration and character omissions/substitutions using the same framework.
</prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
transliteration (knight and graehl, 1998; <papid> J98-4003 </papid>goto et al , 2004), <papid> C04-1119 </papid>which has the goal of generating asource word (s) for japanese term (t).</citsent>
<aftsection>
<nextsent>in contrast, we employed discriminative approach, which has the goal of determining whether two terms (t1 and t2) are equivalent.
</nextsent>
<nextsent>these two goals are related.
</nextsent>
<nextsent>for example, if two terms (t1 and t2) were transliterated from the same word (s), they should be orthographic variants.
</nextsent>
<nextsent>to incorporate this information, we incorporated transliterated-probability (p (s|t1)p (s|t2)) into the svm features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L262">
<title id=" I08-1007.xml">orthographic disambiguation incorporating transliterated probability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mer problem (transliteration); our target scope is wider.
</prevsent>
<prevsent>we addressed both transliteration and character omissions/substitutions using the same framework.
</prevsent>
</prevsection>
<citsent citstr=" C04-1119 ">
transliteration (knight and graehl, 1998; <papid> J98-4003 </papid>goto et al , 2004), <papid> C04-1119 </papid>which has the goal of generating asource word (s) for japanese term (t).</citsent>
<aftsection>
<nextsent>in contrast, we employed discriminative approach, which has the goal of determining whether two terms (t1 and t2) are equivalent.
</nextsent>
<nextsent>these two goals are related.
</nextsent>
<nextsent>for example, if two terms (t1 and t2) were transliterated from the same word (s), they should be orthographic variants.
</nextsent>
<nextsent>to incorporate this information, we incorporated transliterated-probability (p (s|t1)p (s|t2)) into the svm features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L266">
<title id=" I08-1007.xml">orthographic disambiguation incorporating transliterated probability </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>for example, knight (1998) proposed probabilistic model for transliteration.
</prevsent>
<prevsent>goto et al (2004) <papid> C04-1119 </papid>proposed similar method, utilizing surrounding characters.</prevsent>
</prevsection>
<citsent citstr=" C02-1099 ">
their method is not only applicable to japanese; it has already been used for korean(oh and choi, 2002;<papid> C02-1099 </papid>oh and choi, 2005; oh and isahara, 2007),arabic(stalls and knight, 1998; sherif and kondrak, 2007), chinese(li et al , 2007), <papid> P07-1016 </papid>and per sian(karimi et al , 2007).<papid> P07-1082 </papid></citsent>
<aftsection>
<nextsent>our method uses different kind of task-setting,compared to previous methods.
</nextsent>
<nextsent>it is based on determining whether two terms within the same language are equivalent.
</nextsent>
<nextsent>it provides high levels of accuracy, which should be practical for many applications.another issue is that of how to represent transliteration phenomena.
</nextsent>
<nextsent>methods can be classified into three main types: grapheme-based (li et al ., 2004); <papid> P04-1021 </papid>phoneme-based (knight and graehl,1998); <papid> J98-4003 </papid>and combinations of both these meth ods( hybrid-model(bilac and tanaka, 2004) <papid> C04-1086 </papid>and correspondence-based model(oh and choi, 2002;<papid> C02-1099 </papid>oh and choi, 2005)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L268">
<title id=" I08-1007.xml">orthographic disambiguation incorporating transliterated probability </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>for example, knight (1998) proposed probabilistic model for transliteration.
</prevsent>
<prevsent>goto et al (2004) <papid> C04-1119 </papid>proposed similar method, utilizing surrounding characters.</prevsent>
</prevsection>
<citsent citstr=" P07-1016 ">
their method is not only applicable to japanese; it has already been used for korean(oh and choi, 2002;<papid> C02-1099 </papid>oh and choi, 2005; oh and isahara, 2007),arabic(stalls and knight, 1998; sherif and kondrak, 2007), chinese(li et al , 2007), <papid> P07-1016 </papid>and per sian(karimi et al , 2007).<papid> P07-1082 </papid></citsent>
<aftsection>
<nextsent>our method uses different kind of task-setting,compared to previous methods.
</nextsent>
<nextsent>it is based on determining whether two terms within the same language are equivalent.
</nextsent>
<nextsent>it provides high levels of accuracy, which should be practical for many applications.another issue is that of how to represent transliteration phenomena.
</nextsent>
<nextsent>methods can be classified into three main types: grapheme-based (li et al ., 2004); <papid> P04-1021 </papid>phoneme-based (knight and graehl,1998); <papid> J98-4003 </papid>and combinations of both these meth ods( hybrid-model(bilac and tanaka, 2004) <papid> C04-1086 </papid>and correspondence-based model(oh and choi, 2002;<papid> C02-1099 </papid>oh and choi, 2005)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L269">
<title id=" I08-1007.xml">orthographic disambiguation incorporating transliterated probability </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>for example, knight (1998) proposed probabilistic model for transliteration.
</prevsent>
<prevsent>goto et al (2004) <papid> C04-1119 </papid>proposed similar method, utilizing surrounding characters.</prevsent>
</prevsection>
<citsent citstr=" P07-1082 ">
their method is not only applicable to japanese; it has already been used for korean(oh and choi, 2002;<papid> C02-1099 </papid>oh and choi, 2005; oh and isahara, 2007),arabic(stalls and knight, 1998; sherif and kondrak, 2007), chinese(li et al , 2007), <papid> P07-1016 </papid>and per sian(karimi et al , 2007).<papid> P07-1082 </papid></citsent>
<aftsection>
<nextsent>our method uses different kind of task-setting,compared to previous methods.
</nextsent>
<nextsent>it is based on determining whether two terms within the same language are equivalent.
</nextsent>
<nextsent>it provides high levels of accuracy, which should be practical for many applications.another issue is that of how to represent transliteration phenomena.
</nextsent>
<nextsent>methods can be classified into three main types: grapheme-based (li et al ., 2004); <papid> P04-1021 </papid>phoneme-based (knight and graehl,1998); <papid> J98-4003 </papid>and combinations of both these meth ods( hybrid-model(bilac and tanaka, 2004) <papid> C04-1086 </papid>and correspondence-based model(oh and choi, 2002;<papid> C02-1099 </papid>oh and choi, 2005)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L271">
<title id=" I08-1007.xml">orthographic disambiguation incorporating transliterated probability </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>it is based on determining whether two terms within the same language are equivalent.
</prevsent>
<prevsent>it provides high levels of accuracy, which should be practical for many applications.another issue is that of how to represent transliteration phenomena.
</prevsent>
</prevsection>
<citsent citstr=" P04-1021 ">
methods can be classified into three main types: grapheme-based (li et al ., 2004); <papid> P04-1021 </papid>phoneme-based (knight and graehl,1998); <papid> J98-4003 </papid>and combinations of both these meth ods( hybrid-model(bilac and tanaka, 2004) <papid> C04-1086 </papid>and correspondence-based model(oh and choi, 2002;<papid> C02-1099 </papid>oh and choi, 2005)).</citsent>
<aftsection>
<nextsent>our proposed method employed grapheme-based approach.
</nextsent>
<nextsent>we selected this kind of approach because it allows us to handle not only transliteration but also character omis sions/substitutions, which we would not be able to address using phoneme-based approach (and combination approach).
</nextsent>
<nextsent>yoon et al  (2007) also proposed discriminative transliteration method, but their system was basedon determining whether target term was transliterated from source term.
</nextsent>
<nextsent>bergsma and kondrak (2007) <papid> P07-1083 </papid>and aramaki et al (2007) proposed on discriminative method for similar spelling terms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L273">
<title id=" I08-1007.xml">orthographic disambiguation incorporating transliterated probability </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>it is based on determining whether two terms within the same language are equivalent.
</prevsent>
<prevsent>it provides high levels of accuracy, which should be practical for many applications.another issue is that of how to represent transliteration phenomena.
</prevsent>
</prevsection>
<citsent citstr=" C04-1086 ">
methods can be classified into three main types: grapheme-based (li et al ., 2004); <papid> P04-1021 </papid>phoneme-based (knight and graehl,1998); <papid> J98-4003 </papid>and combinations of both these meth ods( hybrid-model(bilac and tanaka, 2004) <papid> C04-1086 </papid>and correspondence-based model(oh and choi, 2002;<papid> C02-1099 </papid>oh and choi, 2005)).</citsent>
<aftsection>
<nextsent>our proposed method employed grapheme-based approach.
</nextsent>
<nextsent>we selected this kind of approach because it allows us to handle not only transliteration but also character omis sions/substitutions, which we would not be able to address using phoneme-based approach (and combination approach).
</nextsent>
<nextsent>yoon et al  (2007) also proposed discriminative transliteration method, but their system was basedon determining whether target term was transliterated from source term.
</nextsent>
<nextsent>bergsma and kondrak (2007) <papid> P07-1083 </papid>and aramaki et al (2007) proposed on discriminative method for similar spelling terms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L275">
<title id=" I08-1007.xml">orthographic disambiguation incorporating transliterated probability </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>we selected this kind of approach because it allows us to handle not only transliteration but also character omis sions/substitutions, which we would not be able to address using phoneme-based approach (and combination approach).
</prevsent>
<prevsent>yoon et al  (2007) also proposed discriminative transliteration method, but their system was basedon determining whether target term was transliterated from source term.
</prevsent>
</prevsection>
<citsent citstr=" P07-1083 ">
bergsma and kondrak (2007) <papid> P07-1083 </papid>and aramaki et al (2007) proposed on discriminative method for similar spelling terms.</citsent>
<aftsection>
<nextsent>however, they did not deal with transliterated probability.
</nextsent>
<nextsent>masuyama et al  (2004) <papid> C04-1176 </papid>collected 178,569 japanese transliteration variants (positive examples) from large corpus.</nextsent>
<nextsent>in contrast, we collected both positive and negative examples in order to train the classifier.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L276">
<title id=" I08-1007.xml">orthographic disambiguation incorporating transliterated probability </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>bergsma and kondrak (2007) <papid> P07-1083 </papid>and aramaki et al (2007) proposed on discriminative method for similar spelling terms.</prevsent>
<prevsent>however, they did not deal with transliterated probability.</prevsent>
</prevsection>
<citsent citstr=" C04-1176 ">
masuyama et al  (2004) <papid> C04-1176 </papid>collected 178,569 japanese transliteration variants (positive examples) from large corpus.</citsent>
<aftsection>
<nextsent>in contrast, we collected both positive and negative examples in order to train the classifier.
</nextsent>
<nextsent>we developed an svm-based orthographic disambiguation classifier, incorporating transliterationprobability.
</nextsent>
<nextsent>we also developed method for collecting both positive and negative examples.
</nextsent>
<nextsent>experimental results yielded high levels of accuracy,demonstrating the feasibility of the proposed approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L279">
<title id=" I08-1072.xml">context feature selection for distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to measure the semantic relatedness of words, concept called distributional similarity has been widely used.
</prevsent>
<prevsent>distributional similarity represents the relatedness of two words by the commonality of contexts the words share, based on the distributional hypothesis (harris, 1985), which states that semantically similar words share similar contexts.
</prevsent>
</prevsection>
<citsent citstr=" W02-0908 ">
a wide range of contextual information, such as surrounding words (lowe and mcdonald, 2000; curran and moens, 2002<papid> W02-0908 </papid>a), dependency or case structure (hindle, 1990; <papid> P90-1034 </papid>ruge, 1997; lin, 1998), <papid> P98-2127 </papid>and dependency path (lin and pantel, 2001; padoand lapata, 2007), has been utilized for similarity calculation, and achieved considerable success.however, major problem which arises when adopting distributional similarity is that it easily yields huge amount of unique contexts.</citsent>
<aftsection>
<nextsent>this can lead to high dimensionality of context space, often up to the order of tens or hundreds of thousands, which makes the calculation computationally impractical.
</nextsent>
<nextsent>be cause not all of the contexts are useful, it is strongly required for the efficiency to eliminate the unwanted contexts to ease the expensive cost.
</nextsent>
<nextsent>to tackle this issue, curran and moens (2002<papid> W02-0908 </papid>b)suggest assigning an index vector of canonical attributes, i.e., small number of representative elements extracted from the original vector, to each word.</nextsent>
<nextsent>when the comparison is performed, canonical attributes of two target words are firstly consulted, and the original vectors are referred to only if the attributes have match between them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L285">
<title id=" I08-1072.xml">context feature selection for distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to measure the semantic relatedness of words, concept called distributional similarity has been widely used.
</prevsent>
<prevsent>distributional similarity represents the relatedness of two words by the commonality of contexts the words share, based on the distributional hypothesis (harris, 1985), which states that semantically similar words share similar contexts.
</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
a wide range of contextual information, such as surrounding words (lowe and mcdonald, 2000; curran and moens, 2002<papid> W02-0908 </papid>a), dependency or case structure (hindle, 1990; <papid> P90-1034 </papid>ruge, 1997; lin, 1998), <papid> P98-2127 </papid>and dependency path (lin and pantel, 2001; padoand lapata, 2007), has been utilized for similarity calculation, and achieved considerable success.however, major problem which arises when adopting distributional similarity is that it easily yields huge amount of unique contexts.</citsent>
<aftsection>
<nextsent>this can lead to high dimensionality of context space, often up to the order of tens or hundreds of thousands, which makes the calculation computationally impractical.
</nextsent>
<nextsent>be cause not all of the contexts are useful, it is strongly required for the efficiency to eliminate the unwanted contexts to ease the expensive cost.
</nextsent>
<nextsent>to tackle this issue, curran and moens (2002<papid> W02-0908 </papid>b)suggest assigning an index vector of canonical attributes, i.e., small number of representative elements extracted from the original vector, to each word.</nextsent>
<nextsent>when the comparison is performed, canonical attributes of two target words are firstly consulted, and the original vectors are referred to only if the attributes have match between them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L286">
<title id=" I08-1072.xml">context feature selection for distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to measure the semantic relatedness of words, concept called distributional similarity has been widely used.
</prevsent>
<prevsent>distributional similarity represents the relatedness of two words by the commonality of contexts the words share, based on the distributional hypothesis (harris, 1985), which states that semantically similar words share similar contexts.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
a wide range of contextual information, such as surrounding words (lowe and mcdonald, 2000; curran and moens, 2002<papid> W02-0908 </papid>a), dependency or case structure (hindle, 1990; <papid> P90-1034 </papid>ruge, 1997; lin, 1998), <papid> P98-2127 </papid>and dependency path (lin and pantel, 2001; padoand lapata, 2007), has been utilized for similarity calculation, and achieved considerable success.however, major problem which arises when adopting distributional similarity is that it easily yields huge amount of unique contexts.</citsent>
<aftsection>
<nextsent>this can lead to high dimensionality of context space, often up to the order of tens or hundreds of thousands, which makes the calculation computationally impractical.
</nextsent>
<nextsent>be cause not all of the contexts are useful, it is strongly required for the efficiency to eliminate the unwanted contexts to ease the expensive cost.
</nextsent>
<nextsent>to tackle this issue, curran and moens (2002<papid> W02-0908 </papid>b)suggest assigning an index vector of canonical attributes, i.e., small number of representative elements extracted from the original vector, to each word.</nextsent>
<nextsent>when the comparison is performed, canonical attributes of two target words are firstly consulted, and the original vectors are referred to only if the attributes have match between them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L295">
<title id=" I08-1072.xml">context feature selection for distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when the comparison is performed, canonical attributes of two target words are firstly consulted, and the original vectors are referred to only if the attributes have match between them.
</prevsent>
<prevsent>however, itis not clear whether the condition for canonical attributes they adopted, i.e., that the attributes must be the most weighted subject, direct object, or indirect object, is optimal in terms of the performance.
</prevsent>
</prevsection>
<citsent citstr=" P06-1045 ">
there are also some existing studies which paid attention to the comparison of context categories for synonym acquisition (curran and moens, 2002<papid> W02-0908 </papid>a;hagiwara et al, 2006).<papid> P06-1045 </papid></citsent>
<aftsection>
<nextsent>however, they have conducted only posteriori comparison based on performance evaluation, and we are afraid that these find 553 ings are somewhat limited to their own experimental settings which may not be applicable to completely new settings, e.g., one with new set of contexts extracted from different sources.
</nextsent>
<nextsent>therefore, general quantitative measures which can be used for reduction and selection of any kind of contexts and context categories are strongly required.
</nextsent>
<nextsent>shifting our attention from word similarity toother areas, great deal of studies on feature selection has been conducted in the literature, especially for text categorization (yang and pedersen, 1997) and gene expression classification (ding and peng, 2003).
</nextsent>
<nextsent>whereas these methods have been successful in reducing feature size while keeping classification performance, the problem of distributional similarity is radically different from that of classification, and whether the same methods are applicable and effective for automatic context selection in the similarity problem is yet to be investigated.in this paper, we firstly introduce existing quantitative methods for feature selection, namely, df, ts, mi, ig, chi2, and show how to apply them to the distributional similarity problem to measure the context importance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L297">
<title id=" I08-1072.xml">context feature selection for distributional similarity </title>
<section> synonym acquisition method.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 context extraction.
</prevsent>
<prevsent>we adopted dependency structure as the context ofwords since it is the most widely used and well performing contextual information in the past studies (ruge, 1997; lin, 1998).<papid> P98-2127 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
as the extraction of accurate and comprehensive dependency structure is in itself difficult task, the sophisticated parser rasptoolkit 2 (briscoe et al, 2006) <papid> P06-4020 </papid>was utilized to extract this kind of word relations.</citsent>
<aftsection>
<nextsent>take the following sentence for example: shipments have been relatively level since january, the commerce department noted.
</nextsent>
<nextsent>555 rasp outputs the extracted dependency structure as n-ary relations as follows, which are called grammatical relations.
</nextsent>
<nextsent>annotations regarding suffix, partof speech tags, offsets for individual words are omitted for simplicity.
</nextsent>
<nextsent>(ncsubj be shipment _) (aux be have) (xcomp _ be level) (ncmod _ be relatively) (ccomp _ level note) (ncmod _ note since) (ncsubj note department _) (det department the) (ncmod _ department commerce) (dobj since january) while the rasp outputs are n-ary relations in general, what we need here is co-occurrences ofwords and contexts, so we extract the set of cooccurrences of stemmed words and contexts by taking out the target word from the relation and replacing the slot by an asterisk ?*?: (words) - (contexts) shipment - ncsubj:be:*_ have - aux:be:* be - ncsubj:*:shipment:_ be - aux:*:have be - xcomp:_:*:level be - ncmod:_:*:relatively relatively - ncmod:_:be:* level - xcomp:_:be:* level - ccomp:_:*:note ...summing all these up produces the raw cooccurrence count n(w, c) of word and context c. 3.2 similarity calculation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L305">
<title id=" I08-1019.xml">identifying cross document relations between sentences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a document generally consists of semantic units called sentences and various relations hold between them.
</prevsent>
<prevsent>the analysis of the structure of document by identifying the relations between sentences is called discourse analysis.
</prevsent>
</prevsection>
<citsent citstr=" J00-3005 ">
the discourse structure of one document hasbeen the target of the traditional discourse analysis (marcu, 2000; <papid> J00-3005 </papid>marcu and echihabi, 2002;yokoyama et al, 2003), based on rhetorical structure theory (rst) (mann and thompson, 1987).yasunari miyabe currently works at toshiba solutions cor poration.</citsent>
<aftsection>
<nextsent>inspired by rst, radev (2000) <papid> W00-1009 </papid>proposed thecross-document structure theory (cst) for multi document analysis, such as multi-document summarization, and topic detection and tracking.</nextsent>
<nextsent>cst takes the structure of set of related documents into ac count.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L306">
<title id=" I08-1019.xml">identifying cross document relations between sentences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the analysis of the structure of document by identifying the relations between sentences is called discourse analysis.
</prevsent>
<prevsent>the discourse structure of one document hasbeen the target of the traditional discourse analysis (marcu, 2000; <papid> J00-3005 </papid>marcu and echihabi, 2002;yokoyama et al, 2003), based on rhetorical structure theory (rst) (mann and thompson, 1987).yasunari miyabe currently works at toshiba solutions cor poration.</prevsent>
</prevsection>
<citsent citstr=" W00-1009 ">
inspired by rst, radev (2000) <papid> W00-1009 </papid>proposed thecross-document structure theory (cst) for multi document analysis, such as multi-document summarization, and topic detection and tracking.</citsent>
<aftsection>
<nextsent>cst takes the structure of set of related documents into account.
</nextsent>
<nextsent>radev defined relations that hold between sentences across the documents on an event (e.g., an earthquake or traffic accident).
</nextsent>
<nextsent>radev presented taxonomy of cross-document relations, consisting of 24 types.
</nextsent>
<nextsent>in japanese, etoh et al (2005) redefined 14 cst types based on radevs taxonomy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L307">
<title id=" I08-1019.xml">identifying cross document relations between sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, we adopt coarse-to-fine approach, in which more general (coarse) class is first identified before the target fine class (eq).
</prevsent>
<prevsent>for the identification of tr pairs, we use variable noun phrases (vnps), which are defined as noun phrases representing variable with number as its value (e.g., stock prices, and population).
</prevsent>
</prevsection>
<citsent citstr=" W99-0625 ">
hatzivassiloglou et al (1999), <papid> W99-0625 </papid>hatzivassiloglou et al (2001) proposed method based on supervised machine learning to identify whether two paragraphs contain similar in formation.</citsent>
<aftsection>
<nextsent>however, we found it was difficult to accurately identify eq pairs between two sentences simply by using similarities as features.
</nextsent>
<nextsent>zhang et al.
</nextsent>
<nextsent>(2003) presented method of classifying cst relations between sentence pairs.
</nextsent>
<nextsent>however, their method used the same features for every type of cst, resulting in low recall and precision.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L308">
<title id=" I08-1019.xml">identifying cross document relations between sentences </title>
<section> positions of sentences in documents (edmund-.  </section>
<citcontext>
<prevsection>
<prevsent>the second feature is similarly defined for numeric units.
</prevsent>
<prevsent>4 experiments on identifying eq pairs.
</prevsent>
</prevsection>
<citsent citstr=" W03-0507 ">
we used the text summarization challenge (tsc) 2and 3 corpora (okumura et al, 2003) <papid> W03-0507 </papid>and the workshop on multimodal summarization for trend information (must) corpus (kato et al, 2005).</citsent>
<aftsection>
<nextsent>these two corpora contained 115 sets of related news articles (10 documents per set on average) on various events.
</nextsent>
<nextsent>a document contained 9.9 sentences on average.
</nextsent>
<nextsent>etoh et al (2005) annotated these two corpora with cst types.
</nextsent>
<nextsent>there were 471,586 pairs of sentences and 798 pairs of these had eq.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L309">
<title id=" I05-3008.xml">word meaning inducing via character ontology a survey on the semantic prediction of chinese two character words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the theoretical consideration concerning with the interaction of ontology and morpho-semantics, and an nlp experiment is performed to do semantic class prediction of unknown two-character words based on the onto logical and lexical knowledge of chinese mor phemic components of words (i.e., characters).
</prevsent>
<prevsent>the task that the semantic predictor (or classifier)performs is to automatically assign the (prede fined) semantic thesaurus classes to the unknown two-character words of chinese.
</prevsent>
</prevsection>
<citsent citstr=" C00-1026 ">
among these types of unknown words, chen and chen (2000) <papid> C00-1026 </papid>pointed out that compound words constitute the most productive type of unknown words in chinese texts.</citsent>
<aftsection>
<nextsent>however, the caveat at this point should be carefully formulated, due to the fact that there are no unequivocal opinions concerning with some basic theoretical settings in chinese morphology.
</nextsent>
<nextsent>the notion of word, morpheme and compounding are not exactly in accord with the definition common within the theoretical setting of western morphology.
</nextsent>
<nextsent>to avoid unnecessary misunderstanding, the pre-theoretical term two-character words will be mostly used instead of compound words in this paper.
</nextsent>
<nextsent>character meaning 2.1 morpho-semantic description.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L310">
<title id=" I05-3008.xml">word meaning inducing via character ontology a survey on the semantic prediction of chinese two character words </title>
<section> word meaning inducing via.  </section>
<citcontext>
<prevsection>
<prevsent>as known, bound roots?
</prevsent>
<prevsent>are the largest classes of morpheme types in chinese morphology, and they are very productive and represent lexical rather than grammatical information (packard 2000).this morphological phenomena leads many chinese linguists to view the word components (i.e.,characters) as building blocks in the semantic composition process of dis- or multisyllabic words.
</prevsent>
</prevsection>
<citsent citstr=" W04-1106 ">
in many empirical studies (tseng and chen (2002); tseng (2003); lua (1993); chen (2004)), <papid> W04-1106 </papid>this view has been confirmed repeatedly.in the semantic studies of chinese word formation, many descriptive and cognitive semantic approaches have been proposed, such as argument structure analysis (chang 1998) and theframe-based semantic analysis (chu 2004).</citsent>
<aftsection>
<nextsent>however, among these qualitative explanation theoretical models, problems often appear in the lack of predictability on the one end of spectrum, or over generation on the other.1 empirical data have1for example, in applying liebers (1992) analysis of argument structure and theta-grid in chinese v-v compounds, chang (1998) found some examples which may satisfy the semantic and syntactic constraints, but they may not be ac 56also shown that in many cases, ? e.g., the abundance of phrasal lexical units in any natural language, ? the principle of compositionality in strict sense, that is, the meaning of complex expression can be fully derivable from the meanings of its component parts, and from the schemas which sanction their combination?(taylor 2002), which is taken to be fundamental proposition in some of morpho-semantically motivated analysis, is highly questionable.this has given to the consideration of the em bedded ness of linguistic meanings within broader conceptual structures.
</nextsent>
<nextsent>in what follows, we will argue that an ontology-based approach would provide an interesting and efficient prospective toward the character-triggered morpho-semantic analysis of chinese words.
</nextsent>
<nextsent>2.2 conceptual aggregate in compounding:.
</nextsent>
<nextsent>a shift toward character ontology in prior studies, it is widely presumed that the category (be it syntactical or semantic) of word, is somehow strongly associated with that of its composing characters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L326">
<title id=" I05-2008.xml">a system to solve language tests for second grade students </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>the nlp problems would become more apparent when we degrade the target materials.
</prevsent>
<prevsent>although questions for second grade students also require world knowledge, it is expected that the questions become simpler and are resolved without tangled techniques.
</prevsent>
</prevsection>
<citsent citstr=" P99-1042 ">
hirschman et al (1999) <papid> P99-1042 </papid>and charniak et al (2000) <papid> W00-0601 </papid>proposed systems to solve reading comprehension.?</citsent>
<aftsection>
<nextsent>hirschman et al (1999) <papid> P99-1042 </papid>developed deep read,?</nextsent>
<nextsent>which is system to select sentences in the text which include answers to question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L328">
<title id=" I05-2008.xml">a system to solve language tests for second grade students </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>the nlp problems would become more apparent when we degrade the target materials.
</prevsent>
<prevsent>although questions for second grade students also require world knowledge, it is expected that the questions become simpler and are resolved without tangled techniques.
</prevsent>
</prevsection>
<citsent citstr=" W00-0601 ">
hirschman et al (1999) <papid> P99-1042 </papid>and charniak et al (2000) <papid> W00-0601 </papid>proposed systems to solve reading comprehension.?</citsent>
<aftsection>
<nextsent>hirschman et al (1999) <papid> P99-1042 </papid>developed deep read,?</nextsent>
<nextsent>which is system to select sentences in the text which include answers to question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L334">
<title id=" I08-2086.xml">effects of related term extraction in transliteration into chinese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in broad sense, the term transliteration?
</prevsent>
<prevsent>has been used to refer to two tasks.
</prevsent>
</prevsection>
<citsent citstr=" P98-2220 ">
the first task is transliteration in the strict sense, which creates new words in target language (haizhou et al, 2004;wan and verspoor, 1998; <papid> P98-2220 </papid>xu et al, 2006).</citsent>
<aftsection>
<nextsent>the second task is back-transliteration (knight and graehl,1998), <papid> J98-4003 </papid>which identifies the source word correspond 643 ing to an existing transliterated word.</nextsent>
<nextsent>both tasks require methods that model pronunciation in the source and target languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L335">
<title id=" I08-2086.xml">effects of related term extraction in transliteration into chinese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>has been used to refer to two tasks.
</prevsent>
<prevsent>the first task is transliteration in the strict sense, which creates new words in target language (haizhou et al, 2004;wan and verspoor, 1998; <papid> P98-2220 </papid>xu et al, 2006).</prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
the second task is back-transliteration (knight and graehl,1998), <papid> J98-4003 </papid>which identifies the source word correspond 643 ing to an existing transliterated word.</citsent>
<aftsection>
<nextsent>both tasks require methods that model pronunciation in the source and target languages.
</nextsent>
<nextsent>however, by definition, in back-transliteration,the word in question has already been transliterated and the meaning or impression of the source word does not have to be considered.
</nextsent>
<nextsent>thus, back transliteration is outside the scope of this paper.
</nextsent>
<nextsent>in the following, we use the term transliteration?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L337">
<title id=" I08-1038.xml">bootstrapping both product features and opinion words from chinese customer reviews with cross inducing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>popescu and etzioni (2005) has utilized statistic-based point-wise mutual information (pmi) to extract product features.
</prevsent>
<prevsent>based on the association of opinion words with product features, they take the advantage of the syntactic dependencies computed by the minipar parser (lin, 1998) to identify opinion words.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
tur 289ney (2002) <papid> P02-1053 </papid>applied specific unsupervised learning technique based on the mutual in-formation between document phrases and two seed words excellent?</citsent>
<aftsection>
<nextsent>and poor?.
</nextsent>
<nextsent>nevertheless, in previous work, identifying product features and opinion words are always considered two separate tasks.
</nextsent>
<nextsent>actually, most product features are modified by the surrounding opinion words in customer reviews, thus they are highly context dependent on each other, which is referred to as context-dependency property henceforth.
</nextsent>
<nextsent>with the co-occurrence characteristic, identifying product features and opinion words could be combined into unified process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L338">
<title id=" I08-1038.xml">bootstrapping both product features and opinion words from chinese customer reviews with cross inducing </title>
<section> the approach.  </section>
<citcontext>
<prevsection>
<prevsent>in customer reviews, features and opinion words usually co-occur frequently, features are usually modified by the surrounding opinion words.
</prevsent>
<prevsent>if the absolute value of the relative distance in sentence for feature and an opinion word is less than minimum-offset, they are considered context dependent.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
many methods have been proposed to measure the co-occurrence relation between two words such as 2 (church and mercer,1993) , mutual information (church and hanks, 1989; <papid> P89-1010 </papid>pantel and lin, 2002), t-test (church and hanks, 1989), <papid> P89-1010 </papid>and log likelihood (dunning,1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>in this paper revised formula of mutual information is used to measure the association since mutual information of low frequency word pair tends to be very high.
</nextsent>
<nextsent>table 1 gives the contingency table for two words or phrases w1 and w2, where is the number of reviews where w1 and w2 co-occur; indicates the number of reviews where w1 occurs but does not co-occur with w2; denotes the number of reviews where w2 occurs but does not co-occur with w1; is number of reviews where neither w1 nor w2 occurs; = + + + d. with the table, the revised formula of mutual information is designed to calculate the association of w1 with w2 as formula (1).
</nextsent>
<nextsent>w2 ~w2 w1 b ~w1 d table 1: contingency table 291 1 2 1 2 1 2 1 2 ( , ) ( , ) ( , ) log ( ) ( ) w rmi w freq w w w = ? log ( ) ( a )a a ?= ? + ? + (1) 2.3 identifying low-frequent features and.
</nextsent>
<nextsent>opinion words in chinese reviews, one linguistic rule noun+ ad verb* adjective+?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L340">
<title id=" I08-1038.xml">bootstrapping both product features and opinion words from chinese customer reviews with cross inducing </title>
<section> the approach.  </section>
<citcontext>
<prevsection>
<prevsent>in customer reviews, features and opinion words usually co-occur frequently, features are usually modified by the surrounding opinion words.
</prevsent>
<prevsent>if the absolute value of the relative distance in sentence for feature and an opinion word is less than minimum-offset, they are considered context dependent.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
many methods have been proposed to measure the co-occurrence relation between two words such as 2 (church and mercer,1993) , mutual information (church and hanks, 1989; <papid> P89-1010 </papid>pantel and lin, 2002), t-test (church and hanks, 1989), <papid> P89-1010 </papid>and log likelihood (dunning,1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>in this paper revised formula of mutual information is used to measure the association since mutual information of low frequency word pair tends to be very high.
</nextsent>
<nextsent>table 1 gives the contingency table for two words or phrases w1 and w2, where is the number of reviews where w1 and w2 co-occur; indicates the number of reviews where w1 occurs but does not co-occur with w2; denotes the number of reviews where w2 occurs but does not co-occur with w1; is number of reviews where neither w1 nor w2 occurs; = + + + d. with the table, the revised formula of mutual information is designed to calculate the association of w1 with w2 as formula (1).
</nextsent>
<nextsent>w2 ~w2 w1 b ~w1 d table 1: contingency table 291 1 2 1 2 1 2 1 2 ( , ) ( , ) ( , ) log ( ) ( ) w rmi w freq w w w = ? log ( ) ( a )a a ?= ? + ? + (1) 2.3 identifying low-frequent features and.
</nextsent>
<nextsent>opinion words in chinese reviews, one linguistic rule noun+ ad verb* adjective+?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L341">
<title id=" I08-1038.xml">bootstrapping both product features and opinion words from chinese customer reviews with cross inducing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on identifying opinion words, morinaga et al(2002)has utilized information gain to extract classification features with supervised method; hatzivassiloglou and wiebe (1997) used textual junctions such as fair and legitimate?
</prevsent>
<prevsent>or simplistic but well-received?
</prevsent>
</prevsection>
<citsent citstr=" W03-0404 ">
to separate similarity- and oppositely-connoted words; other methods are present in (riloff et al 2003; <papid> W03-0404 </papid>riloff and wiebe, 2003; <papid> W03-1014 </papid>gamon and aue, 2005; <papid> W05-0408 </papid>wilson et al 2006) the principal difference from previous work is that, they have considered extracting opinion words as separate work but we have combined identifying features and opinion words in unified process.</citsent>
<aftsection>
<nextsent>besides, the opinion words are identified for sentences but in their work they are identified for reviews.
</nextsent>
<nextsent>in this paper, identifying product features and opinion words are induced by each other and are combined in unified process.
</nextsent>
<nextsent>an iterative learn 294 ing strategy based on context-dependence property is proposed to learn product features and opinion words alternately, where the final feature lexicon and opinion word lexicon are identified with very few knowledge (only ten seed opinion words) and augmented by each other alternately.
</nextsent>
<nextsent>a revised formula of mutual information is used to calculate the association between each feature and opinion word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L342">
<title id=" I08-1038.xml">bootstrapping both product features and opinion words from chinese customer reviews with cross inducing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on identifying opinion words, morinaga et al(2002)has utilized information gain to extract classification features with supervised method; hatzivassiloglou and wiebe (1997) used textual junctions such as fair and legitimate?
</prevsent>
<prevsent>or simplistic but well-received?
</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
to separate similarity- and oppositely-connoted words; other methods are present in (riloff et al 2003; <papid> W03-0404 </papid>riloff and wiebe, 2003; <papid> W03-1014 </papid>gamon and aue, 2005; <papid> W05-0408 </papid>wilson et al 2006) the principal difference from previous work is that, they have considered extracting opinion words as separate work but we have combined identifying features and opinion words in unified process.</citsent>
<aftsection>
<nextsent>besides, the opinion words are identified for sentences but in their work they are identified for reviews.
</nextsent>
<nextsent>in this paper, identifying product features and opinion words are induced by each other and are combined in unified process.
</nextsent>
<nextsent>an iterative learn 294 ing strategy based on context-dependence property is proposed to learn product features and opinion words alternately, where the final feature lexicon and opinion word lexicon are identified with very few knowledge (only ten seed opinion words) and augmented by each other alternately.
</nextsent>
<nextsent>a revised formula of mutual information is used to calculate the association between each feature and opinion word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L343">
<title id=" I08-1038.xml">bootstrapping both product features and opinion words from chinese customer reviews with cross inducing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on identifying opinion words, morinaga et al(2002)has utilized information gain to extract classification features with supervised method; hatzivassiloglou and wiebe (1997) used textual junctions such as fair and legitimate?
</prevsent>
<prevsent>or simplistic but well-received?
</prevsent>
</prevsection>
<citsent citstr=" W05-0408 ">
to separate similarity- and oppositely-connoted words; other methods are present in (riloff et al 2003; <papid> W03-0404 </papid>riloff and wiebe, 2003; <papid> W03-1014 </papid>gamon and aue, 2005; <papid> W05-0408 </papid>wilson et al 2006) the principal difference from previous work is that, they have considered extracting opinion words as separate work but we have combined identifying features and opinion words in unified process.</citsent>
<aftsection>
<nextsent>besides, the opinion words are identified for sentences but in their work they are identified for reviews.
</nextsent>
<nextsent>in this paper, identifying product features and opinion words are induced by each other and are combined in unified process.
</nextsent>
<nextsent>an iterative learn 294 ing strategy based on context-dependence property is proposed to learn product features and opinion words alternately, where the final feature lexicon and opinion word lexicon are identified with very few knowledge (only ten seed opinion words) and augmented by each other alternately.
</nextsent>
<nextsent>a revised formula of mutual information is used to calculate the association between each feature and opinion word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L344">
<title id=" I08-2120.xml">mining chinese english parallel corpora from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at last, each candidate pair is examined based on multiple standards.
</prevsent>
<prevsent>we develop novel strategies for the implementation of the system, which are then proved to be rather effective by the experiments towards multilingual website.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
parallel corpora consisting of text in parallel translation plays an important role in data-driven natural language processing technologies such as statistical machine translation (brown et al, 1990) <papid> J90-2002 </papid>and cross-lingual information retrieval (landauer and littman, 1990; oard, 1997).</citsent>
<aftsection>
<nextsent>but the fact is that only few parallel corpora with high quality are publicly available such as the united nations proceedings and the canadian parliament proceedings (ldc, 1999).
</nextsent>
<nextsent>these corpora are usually small in size, specializing in narrow areas, usually with fees and licensing restrictions, or sometimes out-of-date.
</nextsent>
<nextsent>for language pairs such as chinese and english, the lack of parallel corpora is more severe.
</nextsent>
<nextsent>the lack of such kind of resource has been an obstacle in the development of the data-driven natural language processing technologies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L345">
<title id=" I08-2120.xml">mining chinese english parallel corpora from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for exam 1 http://www.spidersoft.com/webzip/default.asp 847 ple, ma and liberman (1999) measured the content similarity by the count of parallel token pairs in the text which are weak at representing the actual content of the text.
</prevsent>
<prevsent>vsm was considered for evaluating the similarity of bilingual text in (chen et al, 2004), but unfortunately the particular description of the implementation which was bit complex was not mentioned in their work, and the time complexity of their system was rather high.
</prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
besides, there are also some other types of methods for mining parallel corpora from the web such as the work in (resnik, 1998), (resnik and smith, 2003) <papid> J03-3002 </papid>and (zhang et al, 2006).</citsent>
<aftsection>
<nextsent>most of these methods are unbalanced between precision and recall or computationally too complex.
</nextsent>
<nextsent>we detail the implementation of bvsm in the pcms system in this paper.
</nextsent>
<nextsent>the experiments conducted to specific website show that pcms can achieve better overall result than relative work reported.
</nextsent>
<nextsent>the structure of the paper is as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L346">
<title id=" I08-2093.xml">automatically identifying computationally relevant typo logical features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>languages, languages that typically defy standard nlp methodologies due to the absence or paucity of relevant digital resources, such as treebanks, parallel corpora, machine readable lexicons and grammars.
</prevsent>
<prevsent>even when resources such as raw or parallel corpora exist, they often cannot be found of sufficient size to allow the use of standard machine learning methods.
</prevsent>
</prevsection>
<citsent citstr=" N06-1041 ">
in some recent grammar induction and mt work (haghighi and klein, 2006; <papid> N06-1041 </papid>quirk et al , 2005) <papid> P05-1034 </papid>it has been shown that even small amount of knowledge about language, in the form of grammar fragments, tree lets or prototypes, can go long way in helping with the induction of grammar from raw text or with alignment of parallel corpora.in this paper we present novel method for discovering knowledge about many of the worlds languages by tapping readily available language data posted to the web.</citsent>
<aftsection>
<nextsent>building upon our work on structural projections across interlinearized text (xia and lewis, 2007), <papid> N07-1057 </papid>we describe means for automatically discovering number of computationally salient typo logical features, such as the existence of particular constituents in language (e.g., the work described in this document was done while lewis was faculty at the university of washington.definite or indefinite determiners) or the canonical order of constituents (e.g., sentential word order, order of constituents in noun phrases).</nextsent>
<nextsent>this knowledge can then be used for subsequent grammar and tool developmentwork.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L347">
<title id=" I08-2093.xml">automatically identifying computationally relevant typo logical features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>languages, languages that typically defy standard nlp methodologies due to the absence or paucity of relevant digital resources, such as treebanks, parallel corpora, machine readable lexicons and grammars.
</prevsent>
<prevsent>even when resources such as raw or parallel corpora exist, they often cannot be found of sufficient size to allow the use of standard machine learning methods.
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
in some recent grammar induction and mt work (haghighi and klein, 2006; <papid> N06-1041 </papid>quirk et al , 2005) <papid> P05-1034 </papid>it has been shown that even small amount of knowledge about language, in the form of grammar fragments, tree lets or prototypes, can go long way in helping with the induction of grammar from raw text or with alignment of parallel corpora.in this paper we present novel method for discovering knowledge about many of the worlds languages by tapping readily available language data posted to the web.</citsent>
<aftsection>
<nextsent>building upon our work on structural projections across interlinearized text (xia and lewis, 2007), <papid> N07-1057 </papid>we describe means for automatically discovering number of computationally salient typo logical features, such as the existence of particular constituents in language (e.g., the work described in this document was done while lewis was faculty at the university of washington.definite or indefinite determiners) or the canonical order of constituents (e.g., sentential word order, order of constituents in noun phrases).</nextsent>
<nextsent>this knowledge can then be used for subsequent grammar and tool developmentwork.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L349">
<title id=" I08-2093.xml">automatically identifying computationally relevant typo logical features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even when resources such as raw or parallel corpora exist, they often cannot be found of sufficient size to allow the use of standard machine learning methods.
</prevsent>
<prevsent>in some recent grammar induction and mt work (haghighi and klein, 2006; <papid> N06-1041 </papid>quirk et al , 2005) <papid> P05-1034 </papid>it has been shown that even small amount of knowledge about language, in the form of grammar fragments, tree lets or prototypes, can go long way in helping with the induction of grammar from raw text or with alignment of parallel corpora.in this paper we present novel method for discovering knowledge about many of the worlds languages by tapping readily available language data posted to the web.</prevsent>
</prevsection>
<citsent citstr=" N07-1057 ">
building upon our work on structural projections across interlinearized text (xia and lewis, 2007), <papid> N07-1057 </papid>we describe means for automatically discovering number of computationally salient typo logical features, such as the existence of particular constituents in language (e.g., the work described in this document was done while lewis was faculty at the university of washington.definite or indefinite determiners) or the canonical order of constituents (e.g., sentential word order, order of constituents in noun phrases).</citsent>
<aftsection>
<nextsent>this knowledge can then be used for subsequent grammar and tool developmentwork.
</nextsent>
<nextsent>we demonstrate that given even very small sample of interlinearized data for language, it is possible to discover computationally relevant information about the language, and because of the sheer volume and diversity of inter linear text on the web, it is possible to do so for hundreds to thousands of the worlds languages.
</nextsent>
<nextsent>2.1 web-based inter linear data as resource.
</nextsent>
<nextsent>in linguistics, the practice of presenting language data in inter linear form has long history, going back at least to the time of the structuralists.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L351">
<title id=" I08-2093.xml">automatically identifying computationally relevant typo logical features </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, the trees generated, and the rules read off of them, may be incomplete or inaccurate.
</prevsent>
<prevsent>7.2 relevance to nlp.
</prevsent>
</prevsection>
<citsent citstr=" W06-0605 ">
our efforts described here were inspired by some recent work on low-density languages (yarowksy and ngai, 2001; maxwell and hughes, 2006; <papid> W06-0605 </papid>drabek and yarowsky, 2006).</citsent>
<aftsection>
<nextsent>until fairly recently, almost all nlp work was done on just dozen or so languages, with the 689vast majority of the worlds 6,000 languages being ignored.
</nextsent>
<nextsent>this is understandable, since in order to do serious nlp work, certain threshold of corpus size must be achieved.
</nextsent>
<nextsent>we provide means for generating small, richly annotated corpora for hundreds of languages using freely available data found on the web.
</nextsent>
<nextsent>these corpora can then be used to generate other electronic resources, such as annotated corpora and associated nlp tools.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L356">
<title id=" I05-3035.xml">a hybrid approach to chinese word segmentation around crfs </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>$ t k ttkk tossf osp 1 1 1 ),,,(exp 1)|( where is an arbitrary feature function over its arguments, andn is learned weight for each feature function.
</prevsent>
<prevsent>),,,( 1 tossf ttk  based on crfs model, we cast the segmentation problem as sequence tagging problem.
</prevsent>
</prevsection>
<citsent citstr=" C04-1081 ">
different from (peng et al, 2004), <papid> C04-1081 </papid>we represent the positions of hanzi (chinese character) with four different tags: for hanzi 196 that starts word, for hanzi that continues the word, for hanzi that ends the word, for hanzi that occurs as single-character word.</citsent>
<aftsection>
<nextsent>the basic segmentation is process of labeling each hanzi with tag given the features derived from its surrounding context.
</nextsent>
<nextsent>the features used in our experiment can be broken into two categories: character features and word features.
</nextsent>
<nextsent>the character features are instantiations of the following templates, similar to those described in (ng and jin, 2004), <papid> W04-3236 </papid>refers to chinese hanzi.</nextsent>
<nextsent>(a) cn (n </nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L357">
<title id=" I05-3035.xml">a hybrid approach to chinese word segmentation around crfs </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the basic segmentation is process of labeling each hanzi with tag given the features derived from its surrounding context.
</prevsent>
<prevsent>the features used in our experiment can be broken into two categories: character features and word features.
</prevsent>
</prevsection>
<citsent citstr=" W04-3236 ">
the character features are instantiations of the following templates, similar to those described in (ng and jin, 2004), <papid> W04-3236 </papid>refers to chinese hanzi.</citsent>
<aftsection>
<nextsent>(a) cn (n
</nextsent>
<nextsent>2,1,0,1,2 ) (b) cncn+1(
</nextsent>
<nextsent>2,1,0,1)
</nextsent>
<nextsent>(c) c1c1 (d) pu(c0 ) in addition to the character features, we came up with another type word context feature which was found very useful in our experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L358">
<title id=" I05-3035.xml">a hybrid approach to chinese word segmentation around crfs </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 error-driven learning.
</prevsent>
<prevsent>as method based on statistics, no matter how well crfs model is constructed, some obviously errors always occurred because of the sparseness of training data.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
for this reson, error-driven learning method (brill, 1995) <papid> J95-4004 </papid>is adopted to refine the segmentation result in this bakeoff in three steps: 1) based on crfs model, we segment the training data which has been removed all the space between words.</citsent>
<aftsection>
<nextsent>based on the comparison of the segmentation result with the original training data, the difference between them will be extracted.
</nextsent>
<nextsent>if difference occurs more than one time, an error-driven rule will be constructed.
</nextsent>
<nextsent>the rule is described as: ?? zzzzzzis the segmentation of in training data.
</nextsent>
<nextsent>we named this rule set constructed by this step crf-ruleset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L359">
<title id=" I05-3035.xml">a hybrid approach to chinese word segmentation around crfs </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>crfs segmentation model can gives good performance on oov words identification.
</prevsent>
<prevsent>but there are still some new words that have not been recognized.
</prevsent>
</prevsection>
<citsent citstr=" W03-1721 ">
so an additive new words recognizer is adopted (chen, 2003).<papid> W03-1721 </papid></citsent>
<aftsection>
<nextsent>in-word probability of each character is used for new word detection.
</nextsent>
<nextsent>the in-word probability of character is probability that the character occurs as part of word of two or more characters.
</nextsent>
<nextsent>and the in-word probability of character is trained from the training data and is calculated as follows: ( ) in word number of occurrence in wordsp number of occurrence . the consecutive single characters are combined into new word if the in-word probability of each single character is over threshold.
</nextsent>
<nextsent>obviously, the value of the threshold is the key to the performance of this new words recognizer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L361">
<title id=" I08-2109.xml">fast computing grammar driven convolution tree kernel for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in previous work, data-driven techniques, including feature-based and kernel-based learning methods, have been extensively studied for srl (carreras and ma`rquez, 2005).
</prevsent>
<prevsent>although feature-based methods are regarded asthe state-of-the-art methods and achieve much success in srl, kernel-based methods are more effective in capturing structured features than feature based methods.
</prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
in the meanwhile, the syntactic structure features hidden in parse tree have been suggested as an important feature for srl and need to be further explored in srl (gildea and palmer, 2002; <papid> P02-1031 </papid>punyakanok et al, 2005).</citsent>
<aftsection>
<nextsent>moschitti (2004) <papid> P04-1043 </papid>the work was mainly done when the author was visiting student at i2r and che et al (2006)<papid> P06-2010 </papid>are two reported work to use convolution tree kernel (tk) methods (collins and duffy, 2001) for srl and has shown promising re sults.</nextsent>
<nextsent>however, as general learning algorithm, thetk only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L362">
<title id=" I08-2109.xml">fast computing grammar driven convolution tree kernel for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although feature-based methods are regarded asthe state-of-the-art methods and achieve much success in srl, kernel-based methods are more effective in capturing structured features than feature based methods.
</prevsent>
<prevsent>in the meanwhile, the syntactic structure features hidden in parse tree have been suggested as an important feature for srl and need to be further explored in srl (gildea and palmer, 2002; <papid> P02-1031 </papid>punyakanok et al, 2005).</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
moschitti (2004) <papid> P04-1043 </papid>the work was mainly done when the author was visiting student at i2r and che et al (2006)<papid> P06-2010 </papid>are two reported work to use convolution tree kernel (tk) methods (collins and duffy, 2001) for srl and has shown promising re sults.</citsent>
<aftsection>
<nextsent>however, as general learning algorithm, thetk only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design.
</nextsent>
<nextsent>to solve the above issue, zhanget al (2007) proposed grammar-driven convolution tree kernel (gtk) for srl.
</nextsent>
<nextsent>the gtk can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes.
</nextsent>
<nextsent>experimental results show that the gtk significantly outperforms the tk (zhang et al, 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L363">
<title id=" I08-2109.xml">fast computing grammar driven convolution tree kernel for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although feature-based methods are regarded asthe state-of-the-art methods and achieve much success in srl, kernel-based methods are more effective in capturing structured features than feature based methods.
</prevsent>
<prevsent>in the meanwhile, the syntactic structure features hidden in parse tree have been suggested as an important feature for srl and need to be further explored in srl (gildea and palmer, 2002; <papid> P02-1031 </papid>punyakanok et al, 2005).</prevsent>
</prevsection>
<citsent citstr=" P06-2010 ">
moschitti (2004) <papid> P04-1043 </papid>the work was mainly done when the author was visiting student at i2r and che et al (2006)<papid> P06-2010 </papid>are two reported work to use convolution tree kernel (tk) methods (collins and duffy, 2001) for srl and has shown promising re sults.</citsent>
<aftsection>
<nextsent>however, as general learning algorithm, thetk only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design.
</nextsent>
<nextsent>to solve the above issue, zhanget al (2007) proposed grammar-driven convolution tree kernel (gtk) for srl.
</nextsent>
<nextsent>the gtk can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes.
</nextsent>
<nextsent>experimental results show that the gtk significantly outperforms the tk (zhang et al, 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L365">
<title id=" I08-2109.xml">fast computing grammar driven convolution tree kernel for semantic role labeling </title>
<section> fast computation of the gtk.  </section>
<citcontext>
<prevsection>
<prevsent>this means that the worst case complexity of the fgtk-i is o(p3|n1| ? |n2|2), where ? is the maximum branching factor of the two trees.
</prevsent>
<prevsent>3.2 fast grammar-driven convolution tree.
</prevsent>
</prevsection>
<citsent citstr=" N06-2025 ">
kernel ii (fgtk-ii) our fgtk-ii algorithm is motivated by the partial trees (pts) kernel (moschitti, 2006).<papid> N06-2025 </papid></citsent>
<aftsection>
<nextsent>the pt kernel algorithm uses the following recursive formulas to evaluate p(cn1 , cn2): p(cn1 , cn2) = |cn1 |?
</nextsent>
<nextsent>i=1 |cn2 |?
</nextsent>
<nextsent>j=1 p(cn1 [1 : i], cn2 [1 : j]) (16) where cn1 [1 : i] and cn2 [1 : j] are the child sub-sequences of cn1 and cn2 from 1 to and from 1to j, respectively.
</nextsent>
<nextsent>given two child node sequences s1a = cn1 [1 : i] and s2b = cn2 [1 : j] (a and are the last children), the pt kernel computes p(?, ?) as follows: p(s1a, s2b) = { 2??(a, b)dp(|s1|, |s2|) if = 0 else (17)where ??(a, b) is defined in eq.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L372">
<title id=" I08-3010.xml">a rule based syllable segmentation of myanmar text </title>
<section> proposal to encode seven additional myanmar characters.  </section>
<citcontext>
<prevsection>
<prevsent>poowarawan (1986) proposed dictionary-based approach to thai syllable separation.
</prevsent>
<prevsent>thai syllable segmentation was considered as the first step towards word segmentation and many of word segmentation ambiguities were resolved at the level of syllable segmentation (aroonmanakun, 2002).
</prevsent>
</prevsection>
<citsent citstr=" C04-1175 ">
thai syllable segmentation can be viewed as the problem of inserting spaces between pairs of characters in the text and the character-level ambiguity of word segmentation can be reduced by extracting syllables whose structures are more well-defined (sornil and chaiwanarom, 2004).<papid> C04-1175 </papid></citsent>
<aftsection>
<nextsent>most approaches 3 http://www.unicode.org/alloc/pipeline.html to thai word segmentation use dictionary as their basis.
</nextsent>
<nextsent>however, the segmentation accuracy depends on the quality of the dictionary used for analysis and unknown words can reduce the performance.
</nextsent>
<nextsent>theeramunkong and usanavasin (2001) <papid> H01-1057 </papid>proposed non dictionary-based approach to thai word segmentation.</nextsent>
<nextsent>a method based on decision tree models was proposed and their approach claimed to outperform some well-known diction ary-dependent techniques of word segmentation such as the maximum and the longest matching methods.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L373">
<title id=" I08-3010.xml">a rule based syllable segmentation of myanmar text </title>
<section> proposal to encode seven additional myanmar characters.  </section>
<citcontext>
<prevsection>
<prevsent>most approaches 3 http://www.unicode.org/alloc/pipeline.html to thai word segmentation use dictionary as their basis.
</prevsent>
<prevsent>however, the segmentation accuracy depends on the quality of the dictionary used for analysis and unknown words can reduce the performance.
</prevsent>
</prevsection>
<citsent citstr=" H01-1057 ">
theeramunkong and usanavasin (2001) <papid> H01-1057 </papid>proposed non dictionary-based approach to thai word segmentation.</citsent>
<aftsection>
<nextsent>a method based on decision tree models was proposed and their approach claimed to outperform some well-known diction ary-dependent techniques of word segmentation such as the maximum and the longest matching methods.
</nextsent>
<nextsent>in order to clarify the syllable structure, characters of the myanmar script are classified into twelve categories.
</nextsent>
<nextsent>each category is given name and the glyphs and unicode code points of characters belonging to each category are shown in table 1.
</nextsent>
<nextsent>the myanmar script consists of total of 75 characters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L374">
<title id=" I08-2102.xml">summarization by analogy an example based approach for news articles </title>
<section> example-based summarization.  </section>
<citcontext>
<prevsection>
<prevsent>(a meeting) and??
</prevsent>
<prevsent>(a convention) as similar.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
we use the similarity proposed by lin (1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>the method uses mutual information and dependency relationships as the phrase features.
</nextsent>
<nextsent>we extend the method to japanese by using particle as the dependency relationships.
</nextsent>
<nextsent>we link phrases as corresponding phrases, where the phrases are the top three similar to phrase in the similar instance.
</nextsent>
<nextsent>3.4 combination of the corresponding phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L375">
<title id=" I05-2007.xml">a resource based korean morphological annotation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>2 state of the art several morphological annotators of korean text are available.
</prevsent>
<prevsent>the hangul analysis module (ham1) is one of the best korean morphological analysers.
</prevsent>
</prevsection>
<citsent citstr=" P98-2167 ">
other fairly representative examples are described in (shin et al, 1995; park et al., 1998) <papid> P98-2167 </papid>and in (lee et al, 1997a; cha et al, 1998; <papid> W98-1110 </papid>lee et al, 2002).<papid> J02-1004 </papid></citsent>
<aftsection>
<nextsent>the output for each morpheme is presented in two parts: the morpheme itself, and grammatical tag.
</nextsent>
<nextsent>morphemes are usually presented in their base form if they are stems, and in they surface form if they are functional morphemes.
</nextsent>
<nextsent>tags are represented by symbols; they give the pos of stems, and grammatical information about functional morphemes.
</nextsent>
<nextsent>in output, 95% to 97% of mor pheme/tag pairs are considered correct.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L376">
<title id=" I05-2007.xml">a resource based korean morphological annotation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>2 state of the art several morphological annotators of korean text are available.
</prevsent>
<prevsent>the hangul analysis module (ham1) is one of the best korean morphological analysers.
</prevsent>
</prevsection>
<citsent citstr=" W98-1110 ">
other fairly representative examples are described in (shin et al, 1995; park et al., 1998) <papid> P98-2167 </papid>and in (lee et al, 1997a; cha et al, 1998; <papid> W98-1110 </papid>lee et al, 2002).<papid> J02-1004 </papid></citsent>
<aftsection>
<nextsent>the output for each morpheme is presented in two parts: the morpheme itself, and grammatical tag.
</nextsent>
<nextsent>morphemes are usually presented in their base form if they are stems, and in they surface form if they are functional morphemes.
</nextsent>
<nextsent>tags are represented by symbols; they give the pos of stems, and grammatical information about functional morphemes.
</nextsent>
<nextsent>in output, 95% to 97% of mor pheme/tag pairs are considered correct.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L377">
<title id=" I05-2007.xml">a resource based korean morphological annotation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>2 state of the art several morphological annotators of korean text are available.
</prevsent>
<prevsent>the hangul analysis module (ham1) is one of the best korean morphological analysers.
</prevsent>
</prevsection>
<citsent citstr=" J02-1004 ">
other fairly representative examples are described in (shin et al, 1995; park et al., 1998) <papid> P98-2167 </papid>and in (lee et al, 1997a; cha et al, 1998; <papid> W98-1110 </papid>lee et al, 2002).<papid> J02-1004 </papid></citsent>
<aftsection>
<nextsent>the output for each morpheme is presented in two parts: the morpheme itself, and grammatical tag.
</nextsent>
<nextsent>morphemes are usually presented in their base form if they are stems, and in they surface form if they are functional morphemes.
</nextsent>
<nextsent>tags are represented by symbols; they give the pos of stems, and grammatical information about functional morphemes.
</nextsent>
<nextsent>in output, 95% to 97% of mor pheme/tag pairs are considered correct.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L380">
<title id=" I05-2007.xml">a resource based korean morphological annotation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this generates all possible ways of segmenting the input word.
</prevsent>
<prevsent>the second step makes selection among the segment ations obtained and among the tags attached to the morphemes.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
the second step involves frequency-based learning from tagged corpus with statistical models such as hidden markov models, and sometimes also with error-driven learning of symbolic transformation rules (brill, 1995; <papid> J95-4004 </papid>lee et al, 1997a; lee et al, 2002).<papid> J02-1004 </papid></citsent>
<aftsection>
<nextsent>morphemes not found in the lexicon undergo special treatment that guesses at their properties.
</nextsent>
<nextsent>a recent variant of this approach (han and palmer, 2005) swaps the main two steps: first, sequence of tags is assigned to each word on the basis of statistical model; then, morphological segmentation is performed with lexicon of morphemes.
</nextsent>
<nextsent>the other approaches are less popular among searchers and language engineering companies.
</nextsent>
<nextsent>some systems are based on two-level models, such as (kim et al, 1994) <papid> C94-1087 </papid>and the klex system of han na-rae 2 .</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L384">
<title id=" I05-2007.xml">a resource based korean morphological annotation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a recent variant of this approach (han and palmer, 2005) swaps the main two steps: first, sequence of tags is assigned to each word on the basis of statistical model; then, morphological segmentation is performed with lexicon of morphemes.
</prevsent>
<prevsent>the other approaches are less popular among searchers and language engineering companies.
</prevsent>
</prevsection>
<citsent citstr=" C94-1087 ">
some systems are based on two-level models, such as (kim et al, 1994) <papid> C94-1087 </papid>and the klex system of han na-rae 2 .</citsent>
<aftsection>
<nextsent>(choi, 1999) combines lexicon of stems with lexicon of endings with the aid of connectivity table.
</nextsent>
<nextsent>1 http://nlp.kookmin.ac.kr/ham/kor/ham-intr.html 2 http://www.cis.upenn.edu/~nrh/klex.html the delimitation of morphemes is provided, but some morpheme boundaries are usually modified so that they coincide with syllable boundaries.
</nextsent>
<nextsent>for example, if two suffixes make up single syllable, like -?:-syss- which is contraction of -??:-eusi- (honorification towards sentence subject) and -?:-ss- (past), they are usually considered as one morpheme.
</nextsent>
<nextsent>such simplifications make it possible to encode morphemes on the korean syllable-based alphabet, and are compatible with syllable-based models (kang and kim, 1994).<papid> C94-1035 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L385">
<title id=" I05-2007.xml">a resource based korean morphological annotation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1 http://nlp.kookmin.ac.kr/ham/kor/ham-intr.html 2 http://www.cis.upenn.edu/~nrh/klex.html the delimitation of morphemes is provided, but some morpheme boundaries are usually modified so that they coincide with syllable boundaries.
</prevsent>
<prevsent>for example, if two suffixes make up single syllable, like -?:-syss- which is contraction of -??:-eusi- (honorification towards sentence subject) and -?:-ss- (past), they are usually considered as one morpheme.
</prevsent>
</prevsection>
<citsent citstr=" C94-1035 ">
such simplifications make it possible to encode morphemes on the korean syllable-based alphabet, and are compatible with syllable-based models (kang and kim, 1994).<papid> C94-1035 </papid></citsent>
<aftsection>
<nextsent>however, they are an approximation.
</nextsent>
<nextsent>we opted for the resource-based approach to obtain more accurate and more informative out put.
</nextsent>
<nextsent>the language resources used in annotators are corpora, rules and lexicons.
</nextsent>
<nextsent>corpus-based systems have an inherent lack of flexibility.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L386">
<title id=" I05-2007.xml">a resource based korean morphological annotation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the flexibility of resource can be defined as the ability to control its evolution.
</prevsent>
<prevsent>in order to adapt corpus-based system, one feeds new corpus into the training process, since the operation of the system is dependent on the nature of the training corpus.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
a training process with tagged corpus gives much better performance than unsupervised training (merialdo, 1994).<papid> J94-2001 </papid></citsent>
<aftsection>
<nextsent>the extension of system to input texts of new types or of new period of time involves the costly task of tagging corpus of new texts.
</nextsent>
<nextsent>another type of evolution of corpus-based system, refinement of the tag set, such as the addition of new features, involves re-tagging of existing tagged corpora, task which is seldom achieved.
</nextsent>
<nextsent>the situation is different with rules or lexicons.
</nextsent>
<nextsent>the flexibility of manually constructed and updated rule set or lexicon depends on its level of readability and of non-redundancy (see section 4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L402">
<title id=" I05-2007.xml">a resource based korean morphological annotation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>all analyses that are conform to phonotactic and grammatical in-word constraints are retained.
</prevsent>
<prevsent>however, checking these constraints does not suffice to remove all ambiguity from korean words.
</prevsent>
</prevsection>
<citsent citstr=" E95-1022 ">
a thorough removal of ambiguity requires syntactic process (voutilainen, 1995; <papid> E95-1022 </papid>laporte, 2001).</citsent>
<aftsection>
<nextsent>our system presents its output in an acyclic finite-state automaton (also called graph or lattice) of morphemes, as in lee et al (1997b), but displayed graphically.
</nextsent>
<nextsent>the output for each morpheme is presented in three parts: surface form, base form, and structured tag providing the general tag of table 1 and syntactic features.
</nextsent>
<nextsent>word separators such as spaces are also present in this automaton.
</nextsent>
<nextsent>the annotation of an evaluation sample by the system presented 67 % recall and 46 % precision.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L403">
<title id=" I08-2133.xml">a comparative study of mixture models for automatic topic segmentation of multiparty dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on lexical features, we use these models in parallel in order to generate low dimensional input representation for topic segmentation.
</prevsent>
<prevsent>our experiments demonstrate that inthis manner important information is captured from the data through less features.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
some of the earliest research related to the problem of text segmentation into thematic episodes used the word distribution as an intrinsic feature of texts (morris and hirst, 1991).<papid> J91-1002 </papid></citsent>
<aftsection>
<nextsent>the studies of (reynar, 1994; hearst, 1997; <papid> J97-1003 </papid>choi, 2000) <papid> A00-2004 </papid>continued in thisvein.</nextsent>
<nextsent>while having quite different emphasis at different levels of detail (basically from the point of view of the employed term weighting and/or the adopted inter-block similarity measure), these studies analyzed the word distribution inside the texts through the instrumentality of merely one feature, i.e. the one-dimensional inter-block similarity.more recent work use techniques from graph theory (malioutov and barzilay, 2006) <papid> P06-1004 </papid>and machine learning (galley et al , 2003; <papid> P03-1071 </papid>georgescul et al , 2006; <papid> W06-2914 </papid>purver et al , 2006) <papid> P06-1003 </papid>in order to find patterns in vocabulary use.we investigate new approaches for topic segmentation on corpora containing multi-party dialogues, which currently represents relatively less explored domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L404">
<title id=" I08-2133.xml">a comparative study of mixture models for automatic topic segmentation of multiparty dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments demonstrate that inthis manner important information is captured from the data through less features.
</prevsent>
<prevsent>some of the earliest research related to the problem of text segmentation into thematic episodes used the word distribution as an intrinsic feature of texts (morris and hirst, 1991).<papid> J91-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
the studies of (reynar, 1994; hearst, 1997; <papid> J97-1003 </papid>choi, 2000) <papid> A00-2004 </papid>continued in thisvein.</citsent>
<aftsection>
<nextsent>while having quite different emphasis at different levels of detail (basically from the point of view of the employed term weighting and/or the adopted inter-block similarity measure), these studies analyzed the word distribution inside the texts through the instrumentality of merely one feature, i.e. the one-dimensional inter-block similarity.more recent work use techniques from graph theory (malioutov and barzilay, 2006) <papid> P06-1004 </papid>and machine learning (galley et al , 2003; <papid> P03-1071 </papid>georgescul et al , 2006; <papid> W06-2914 </papid>purver et al , 2006) <papid> P06-1003 </papid>in order to find patterns in vocabulary use.we investigate new approaches for topic segmentation on corpora containing multi-party dialogues, which currently represents relatively less explored domain.</nextsent>
<nextsent>compared to other types of audio content(e.g. broadcast news recordings), meeting recordings are less structured, often exhibiting high degree of participants spontaneity and there may be overlap in finishing one topic while introducing an other.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L405">
<title id=" I08-2133.xml">a comparative study of mixture models for automatic topic segmentation of multiparty dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments demonstrate that inthis manner important information is captured from the data through less features.
</prevsent>
<prevsent>some of the earliest research related to the problem of text segmentation into thematic episodes used the word distribution as an intrinsic feature of texts (morris and hirst, 1991).<papid> J91-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" A00-2004 ">
the studies of (reynar, 1994; hearst, 1997; <papid> J97-1003 </papid>choi, 2000) <papid> A00-2004 </papid>continued in thisvein.</citsent>
<aftsection>
<nextsent>while having quite different emphasis at different levels of detail (basically from the point of view of the employed term weighting and/or the adopted inter-block similarity measure), these studies analyzed the word distribution inside the texts through the instrumentality of merely one feature, i.e. the one-dimensional inter-block similarity.more recent work use techniques from graph theory (malioutov and barzilay, 2006) <papid> P06-1004 </papid>and machine learning (galley et al , 2003; <papid> P03-1071 </papid>georgescul et al , 2006; <papid> W06-2914 </papid>purver et al , 2006) <papid> P06-1003 </papid>in order to find patterns in vocabulary use.we investigate new approaches for topic segmentation on corpora containing multi-party dialogues, which currently represents relatively less explored domain.</nextsent>
<nextsent>compared to other types of audio content(e.g. broadcast news recordings), meeting recordings are less structured, often exhibiting high degree of participants spontaneity and there may be overlap in finishing one topic while introducing an other.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L406">
<title id=" I08-2133.xml">a comparative study of mixture models for automatic topic segmentation of multiparty dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some of the earliest research related to the problem of text segmentation into thematic episodes used the word distribution as an intrinsic feature of texts (morris and hirst, 1991).<papid> J91-1002 </papid></prevsent>
<prevsent>the studies of (reynar, 1994; hearst, 1997; <papid> J97-1003 </papid>choi, 2000) <papid> A00-2004 </papid>continued in thisvein.</prevsent>
</prevsection>
<citsent citstr=" P06-1004 ">
while having quite different emphasis at different levels of detail (basically from the point of view of the employed term weighting and/or the adopted inter-block similarity measure), these studies analyzed the word distribution inside the texts through the instrumentality of merely one feature, i.e. the one-dimensional inter-block similarity.more recent work use techniques from graph theory (malioutov and barzilay, 2006) <papid> P06-1004 </papid>and machine learning (galley et al , 2003; <papid> P03-1071 </papid>georgescul et al , 2006; <papid> W06-2914 </papid>purver et al , 2006) <papid> P06-1003 </papid>in order to find patterns in vocabulary use.we investigate new approaches for topic segmentation on corpora containing multi-party dialogues, which currently represents relatively less explored domain.</citsent>
<aftsection>
<nextsent>compared to other types of audio content(e.g. broadcast news recordings), meeting recordings are less structured, often exhibiting high degree of participants spontaneity and there may be overlap in finishing one topic while introducing an other.
</nextsent>
<nextsent>moreover while ending the discussion on certain topic, there can be numerous new attempt sto introduce new topic before it becomes the focus of the dialogue.
</nextsent>
<nextsent>therefore, the task of automatic topic segmentation of meeting recordings is more difficult and requires more refined analysis.
</nextsent>
<nextsent>(gal ley et al , 2003; <papid> P03-1071 </papid>georgescul et al , 2007) dealt withthe problem of topic segmentation of multiparty dialogues by combining various features based on cue phrases, syntactic and prosodic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L407">
<title id=" I08-2133.xml">a comparative study of mixture models for automatic topic segmentation of multiparty dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some of the earliest research related to the problem of text segmentation into thematic episodes used the word distribution as an intrinsic feature of texts (morris and hirst, 1991).<papid> J91-1002 </papid></prevsent>
<prevsent>the studies of (reynar, 1994; hearst, 1997; <papid> J97-1003 </papid>choi, 2000) <papid> A00-2004 </papid>continued in thisvein.</prevsent>
</prevsection>
<citsent citstr=" P03-1071 ">
while having quite different emphasis at different levels of detail (basically from the point of view of the employed term weighting and/or the adopted inter-block similarity measure), these studies analyzed the word distribution inside the texts through the instrumentality of merely one feature, i.e. the one-dimensional inter-block similarity.more recent work use techniques from graph theory (malioutov and barzilay, 2006) <papid> P06-1004 </papid>and machine learning (galley et al , 2003; <papid> P03-1071 </papid>georgescul et al , 2006; <papid> W06-2914 </papid>purver et al , 2006) <papid> P06-1003 </papid>in order to find patterns in vocabulary use.we investigate new approaches for topic segmentation on corpora containing multi-party dialogues, which currently represents relatively less explored domain.</citsent>
<aftsection>
<nextsent>compared to other types of audio content(e.g. broadcast news recordings), meeting recordings are less structured, often exhibiting high degree of participants spontaneity and there may be overlap in finishing one topic while introducing an other.
</nextsent>
<nextsent>moreover while ending the discussion on certain topic, there can be numerous new attempt sto introduce new topic before it becomes the focus of the dialogue.
</nextsent>
<nextsent>therefore, the task of automatic topic segmentation of meeting recordings is more difficult and requires more refined analysis.
</nextsent>
<nextsent>(gal ley et al , 2003; <papid> P03-1071 </papid>georgescul et al , 2007) dealt withthe problem of topic segmentation of multiparty dialogues by combining various features based on cue phrases, syntactic and prosodic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L408">
<title id=" I08-2133.xml">a comparative study of mixture models for automatic topic segmentation of multiparty dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some of the earliest research related to the problem of text segmentation into thematic episodes used the word distribution as an intrinsic feature of texts (morris and hirst, 1991).<papid> J91-1002 </papid></prevsent>
<prevsent>the studies of (reynar, 1994; hearst, 1997; <papid> J97-1003 </papid>choi, 2000) <papid> A00-2004 </papid>continued in thisvein.</prevsent>
</prevsection>
<citsent citstr=" W06-2914 ">
while having quite different emphasis at different levels of detail (basically from the point of view of the employed term weighting and/or the adopted inter-block similarity measure), these studies analyzed the word distribution inside the texts through the instrumentality of merely one feature, i.e. the one-dimensional inter-block similarity.more recent work use techniques from graph theory (malioutov and barzilay, 2006) <papid> P06-1004 </papid>and machine learning (galley et al , 2003; <papid> P03-1071 </papid>georgescul et al , 2006; <papid> W06-2914 </papid>purver et al , 2006) <papid> P06-1003 </papid>in order to find patterns in vocabulary use.we investigate new approaches for topic segmentation on corpora containing multi-party dialogues, which currently represents relatively less explored domain.</citsent>
<aftsection>
<nextsent>compared to other types of audio content(e.g. broadcast news recordings), meeting recordings are less structured, often exhibiting high degree of participants spontaneity and there may be overlap in finishing one topic while introducing an other.
</nextsent>
<nextsent>moreover while ending the discussion on certain topic, there can be numerous new attempt sto introduce new topic before it becomes the focus of the dialogue.
</nextsent>
<nextsent>therefore, the task of automatic topic segmentation of meeting recordings is more difficult and requires more refined analysis.
</nextsent>
<nextsent>(gal ley et al , 2003; <papid> P03-1071 </papid>georgescul et al , 2007) dealt withthe problem of topic segmentation of multiparty dialogues by combining various features based on cue phrases, syntactic and prosodic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L409">
<title id=" I08-2133.xml">a comparative study of mixture models for automatic topic segmentation of multiparty dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some of the earliest research related to the problem of text segmentation into thematic episodes used the word distribution as an intrinsic feature of texts (morris and hirst, 1991).<papid> J91-1002 </papid></prevsent>
<prevsent>the studies of (reynar, 1994; hearst, 1997; <papid> J97-1003 </papid>choi, 2000) <papid> A00-2004 </papid>continued in thisvein.</prevsent>
</prevsection>
<citsent citstr=" P06-1003 ">
while having quite different emphasis at different levels of detail (basically from the point of view of the employed term weighting and/or the adopted inter-block similarity measure), these studies analyzed the word distribution inside the texts through the instrumentality of merely one feature, i.e. the one-dimensional inter-block similarity.more recent work use techniques from graph theory (malioutov and barzilay, 2006) <papid> P06-1004 </papid>and machine learning (galley et al , 2003; <papid> P03-1071 </papid>georgescul et al , 2006; <papid> W06-2914 </papid>purver et al , 2006) <papid> P06-1003 </papid>in order to find patterns in vocabulary use.we investigate new approaches for topic segmentation on corpora containing multi-party dialogues, which currently represents relatively less explored domain.</citsent>
<aftsection>
<nextsent>compared to other types of audio content(e.g. broadcast news recordings), meeting recordings are less structured, often exhibiting high degree of participants spontaneity and there may be overlap in finishing one topic while introducing an other.
</nextsent>
<nextsent>moreover while ending the discussion on certain topic, there can be numerous new attempt sto introduce new topic before it becomes the focus of the dialogue.
</nextsent>
<nextsent>therefore, the task of automatic topic segmentation of meeting recordings is more difficult and requires more refined analysis.
</nextsent>
<nextsent>(gal ley et al , 2003; <papid> P03-1071 </papid>georgescul et al , 2007) dealt withthe problem of topic segmentation of multiparty dialogues by combining various features based on cue phrases, syntactic and prosodic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L425">
<title id=" I05-2046.xml">using maximum entropy to extract biomedical named entities without dictionaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is defined as recognizing objects of particular class in plain text.
</prevsent>
<prevsent>depending on required application, ner can extract objects ranging from protein/gene names to disease/virus names.
</prevsent>
</prevsection>
<citsent citstr=" P02-1021 ">
in general, biomedical nes do not follow any nomenclature (shatkay and feldman, 2003) andcan comprise long compound words and short abbreviations (pakhomov, 2002).<papid> P02-1021 </papid></citsent>
<aftsection>
<nextsent>some nes contain various symbols and other spelling variations.
</nextsent>
<nextsent>on average, any ne of interest has five synonyms.
</nextsent>
<nextsent>biomedical ner is challenging problem.
</nextsent>
<nextsent>there are many different aspects to deal with.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L427">
<title id=" I05-2046.xml">using maximum entropy to extract biomedical named entities without dictionaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>rule-based approaches, on the other hand, are more accurate, but less portable across domains.
</prevsent>
<prevsent>therefore, we chose the machine learning approach.
</prevsent>
</prevsection>
<citsent citstr=" W02-0301 ">
various machine learning approaches such as me (kazama et al, 2002), <papid> W02-0301 </papid>svm (kazama et al, 2002; <papid> W02-0301 </papid>song et al, 2004), <papid> W04-1220 </papid>hmm (zhao, 2004) <papid> W04-1216 </papid>are applied to ner.</citsent>
<aftsection>
<nextsent>in this paper, we chose me as our framework since it is much easier to represent various features in such framework.
</nextsent>
<nextsent>in addition, me models are flexible enough to capture many correlated features, including overlapping and non-independent features.
</nextsent>
<nextsent>we can thus use multiple features with more ease than on an hmm system.
</nextsent>
<nextsent>me-based tagger, in particular, excel at solving sequence tagging problems such as pos tagging (ratnaparkhi, 1997), general english ner (borthwick, 1999), and chunking (koeling, 2000).<papid> W00-0729 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L429">
<title id=" I05-2046.xml">using maximum entropy to extract biomedical named entities without dictionaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>rule-based approaches, on the other hand, are more accurate, but less portable across domains.
</prevsent>
<prevsent>therefore, we chose the machine learning approach.
</prevsent>
</prevsection>
<citsent citstr=" W04-1220 ">
various machine learning approaches such as me (kazama et al, 2002), <papid> W02-0301 </papid>svm (kazama et al, 2002; <papid> W02-0301 </papid>song et al, 2004), <papid> W04-1220 </papid>hmm (zhao, 2004) <papid> W04-1216 </papid>are applied to ner.</citsent>
<aftsection>
<nextsent>in this paper, we chose me as our framework since it is much easier to represent various features in such framework.
</nextsent>
<nextsent>in addition, me models are flexible enough to capture many correlated features, including overlapping and non-independent features.
</nextsent>
<nextsent>we can thus use multiple features with more ease than on an hmm system.
</nextsent>
<nextsent>me-based tagger, in particular, excel at solving sequence tagging problems such as pos tagging (ratnaparkhi, 1997), general english ner (borthwick, 1999), and chunking (koeling, 2000).<papid> W00-0729 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L430">
<title id=" I05-2046.xml">using maximum entropy to extract biomedical named entities without dictionaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>rule-based approaches, on the other hand, are more accurate, but less portable across domains.
</prevsent>
<prevsent>therefore, we chose the machine learning approach.
</prevsent>
</prevsection>
<citsent citstr=" W04-1216 ">
various machine learning approaches such as me (kazama et al, 2002), <papid> W02-0301 </papid>svm (kazama et al, 2002; <papid> W02-0301 </papid>song et al, 2004), <papid> W04-1220 </papid>hmm (zhao, 2004) <papid> W04-1216 </papid>are applied to ner.</citsent>
<aftsection>
<nextsent>in this paper, we chose me as our framework since it is much easier to represent various features in such framework.
</nextsent>
<nextsent>in addition, me models are flexible enough to capture many correlated features, including overlapping and non-independent features.
</nextsent>
<nextsent>we can thus use multiple features with more ease than on an hmm system.
</nextsent>
<nextsent>me-based tagger, in particular, excel at solving sequence tagging problems such as pos tagging (ratnaparkhi, 1997), general english ner (borthwick, 1999), and chunking (koeling, 2000).<papid> W00-0729 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L431">
<title id=" I05-2046.xml">using maximum entropy to extract biomedical named entities without dictionaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, me models are flexible enough to capture many correlated features, including overlapping and non-independent features.
</prevsent>
<prevsent>we can thus use multiple features with more ease than on an hmm system.
</prevsent>
</prevsection>
<citsent citstr=" W00-0729 ">
me-based tagger, in particular, excel at solving sequence tagging problems such as pos tagging (ratnaparkhi, 1997), general english ner (borthwick, 1999), and chunking (koeling, 2000).<papid> W00-0729 </papid></citsent>
<aftsection>
<nextsent>in this paper, we describe how to construct ame-based framework that can exploit shallow linguistic information in the recognition of biomedical named entities.
</nextsent>
<nextsent>hopefully, our experience in integrating these features may prove useful for those interested in constructing machine learning based ner system.
</nextsent>
<nextsent>2.1 formulation.
</nextsent>
<nextsent>in the biomedical ner problem, we regard each word in sentence as token.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L432">
<title id=" I05-2046.xml">using maximum entropy to extract biomedical named entities without dictionaries </title>
<section> maximum entropy based tagger.  </section>
<citcontext>
<prevsection>
<prevsent>formally, we can represent this feature as follows: f(h, o) = ? ??
</prevsent>
<prevsent>1 : if w0-allcaps(h)=true and o=b-protein 0 : otherwise (1) here, w0-allcaps(h) is binary function that returns the value true if all alphabets of the current token in the history are capitalized.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
given aset of features and training corpus, the me estimation process produces model in which every feature fi has weight i. from (berger et al,1996), <papid> J96-1002 </papid>we can compute the conditional probability as: p(o|h) = 1 z(h) ? fi(h,o)i (2) z(h) = ? ? fi(h,o)i (3) the probability is given by multiplying the weights of active features (i.e., those fi(h, o) = 1).</citsent>
<aftsection>
<nextsent>the weight is estimated by procedure called generalized iterative scaling (gis) (dar roch and rat cliff, 1972).
</nextsent>
<nextsent>this method improves the estimation of weights iteratively.
</nextsent>
<nextsent>the me estimation technique guarantees that, for every feature fi, the expected value of equals the empirical expectation of in the training corpus.
</nextsent>
<nextsent>269 as noted in (borthwick, 1999), me allows users to focus on finding features that characterizes the problem while leaving feature weight assignment to the me estimation routine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L433">
<title id=" I05-2046.xml">using maximum entropy to extract biomedical named entities without dictionaries </title>
<section> linguistic features.  </section>
<citcontext>
<prevsection>
<prevsent>verbs and prepositions usually indicate annes boundaries, whereas nouns not found in the dictionary are usually good candidates for named entities.
</prevsent>
<prevsent>our experience indicates that five is also suitable window size.
</prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
the mbt pos tagger (daelemans et al, 1996) <papid> W96-0102 </papid>is used to provide pos information.</citsent>
<aftsection>
<nextsent>we trained it on genia 3.02p and achieves 97.85% accuracy.
</nextsent>
<nextsent>3.4 word shape features.
</nextsent>
<nextsent>nes in the same category may look similar (e.g.,il-2 and il-4).
</nextsent>
<nextsent>so we have come up with simple way to normalize all similar words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L434">
<title id=" I08-1074.xml">benchmarking noun compound interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the huge number of possible ncs and potentially large number of srs makes nc interpretation very difficult problem.
</prevsent>
<prevsent>in the past, much ncinterpretation work has been carried out which targets particular nlp applications such as information extraction, question-answering and machine translation.
</prevsent>
</prevsection>
<citsent citstr=" P98-1015 ">
unfortunately, much of it has not gained 1the 4th international workshop on semantic evaluation2srs used in the examples are taken from barker and szpakowicz (1998).<papid> P98-1015 </papid></citsent>
<aftsection>
<nextsent>traction in real-world applications as the accuracy of the methods has not been sufficiently high overopen-domain data.
</nextsent>
<nextsent>most prior work has been carried out under specific assumptions and with oneoff datasets, which makes it hard to analyze performance and to build hybrid methods.
</nextsent>
<nextsent>additionally, disagreement in the inventory of srs and lack of resource sharing has hampered comparative evaluation of different methods.
</nextsent>
<nextsent>the first step in nc interpretation is to define set of srs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L436">
<title id=" I08-1074.xml">benchmarking noun compound interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>additionally, disagreement in the inventory of srs and lack of resource sharing has hampered comparative evaluation of different methods.
</prevsent>
<prevsent>the first step in nc interpretation is to define set of srs.
</prevsent>
</prevsection>
<citsent citstr=" W04-2609 ">
levi (1979), for example, proposed system of 9 srs, while others have proposed classifications with 20-30 srs (finin, 1980; barker and szpakowicz, 1998; <papid> P98-1015 </papid>moldovan et al, 2004).<papid> W04-2609 </papid></citsent>
<aftsection>
<nextsent>smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement.
</nextsent>
<nextsent>additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting srs (down ing, 1977; sparckjones, 1983).
</nextsent>
<nextsent>recent attempts in the area of nc interpretation have taken two basic approaches: analogy-base interpretation (rosario, 2001; moldovan et al, 2004;<papid> W04-2609 </papid>kim and baldwin, 2005; girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (van der wende, 1994; <papid> C94-2125 </papid>lapata, 2002; <papid> J02-3004 </papid>kim and baldwin,2006; <papid> P06-2064 </papid>nakov, 2006).</nextsent>
<nextsent>most methods employ rich ontologies and ignore the context of use, supporting the claim by fan (2003) that axioms and ontologicaldistinctions are more important than detailed knowledge of specific nouns for nc interpretation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L438">
<title id=" I08-1074.xml">benchmarking noun compound interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement.
</prevsent>
<prevsent>additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting srs (down ing, 1977; sparckjones, 1983).
</prevsent>
</prevsection>
<citsent citstr=" C94-2125 ">
recent attempts in the area of nc interpretation have taken two basic approaches: analogy-base interpretation (rosario, 2001; moldovan et al, 2004;<papid> W04-2609 </papid>kim and baldwin, 2005; girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (van der wende, 1994; <papid> C94-2125 </papid>lapata, 2002; <papid> J02-3004 </papid>kim and baldwin,2006; <papid> P06-2064 </papid>nakov, 2006).</citsent>
<aftsection>
<nextsent>most methods employ rich ontologies and ignore the context of use, supporting the claim by fan (2003) that axioms and ontologicaldistinctions are more important than detailed knowledge of specific nouns for nc interpretation.
</nextsent>
<nextsent>additionally, most approaches use supervised learning, raising questions about the generality of the test and 569training datasets and the effectiveness of the algorithms in different domains (coverage of srs over the ncs is another issue).
</nextsent>
<nextsent>our aim in this paper is to compare and analyze existing nc interpretation methods over common, publicly available dataset.
</nextsent>
<nextsent>while recent research has made significant progress, bringing us one step closer to practical applicability in nlp applications, no direct comparison or analysis of the approaches has been attempted to date.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L439">
<title id=" I08-1074.xml">benchmarking noun compound interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement.
</prevsent>
<prevsent>additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting srs (down ing, 1977; sparckjones, 1983).
</prevsent>
</prevsection>
<citsent citstr=" J02-3004 ">
recent attempts in the area of nc interpretation have taken two basic approaches: analogy-base interpretation (rosario, 2001; moldovan et al, 2004;<papid> W04-2609 </papid>kim and baldwin, 2005; girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (van der wende, 1994; <papid> C94-2125 </papid>lapata, 2002; <papid> J02-3004 </papid>kim and baldwin,2006; <papid> P06-2064 </papid>nakov, 2006).</citsent>
<aftsection>
<nextsent>most methods employ rich ontologies and ignore the context of use, supporting the claim by fan (2003) that axioms and ontologicaldistinctions are more important than detailed knowledge of specific nouns for nc interpretation.
</nextsent>
<nextsent>additionally, most approaches use supervised learning, raising questions about the generality of the test and 569training datasets and the effectiveness of the algorithms in different domains (coverage of srs over the ncs is another issue).
</nextsent>
<nextsent>our aim in this paper is to compare and analyze existing nc interpretation methods over common, publicly available dataset.
</nextsent>
<nextsent>while recent research has made significant progress, bringing us one step closer to practical applicability in nlp applications, no direct comparison or analysis of the approaches has been attempted to date.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L440">
<title id=" I08-1074.xml">benchmarking noun compound interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement.
</prevsent>
<prevsent>additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting srs (down ing, 1977; sparckjones, 1983).
</prevsent>
</prevsection>
<citsent citstr=" P06-2064 ">
recent attempts in the area of nc interpretation have taken two basic approaches: analogy-base interpretation (rosario, 2001; moldovan et al, 2004;<papid> W04-2609 </papid>kim and baldwin, 2005; girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (van der wende, 1994; <papid> C94-2125 </papid>lapata, 2002; <papid> J02-3004 </papid>kim and baldwin,2006; <papid> P06-2064 </papid>nakov, 2006).</citsent>
<aftsection>
<nextsent>most methods employ rich ontologies and ignore the context of use, supporting the claim by fan (2003) that axioms and ontologicaldistinctions are more important than detailed knowledge of specific nouns for nc interpretation.
</nextsent>
<nextsent>additionally, most approaches use supervised learning, raising questions about the generality of the test and 569training datasets and the effectiveness of the algorithms in different domains (coverage of srs over the ncs is another issue).
</nextsent>
<nextsent>our aim in this paper is to compare and analyze existing nc interpretation methods over common, publicly available dataset.
</nextsent>
<nextsent>while recent research has made significant progress, bringing us one step closer to practical applicability in nlp applications, no direct comparison or analysis of the approaches has been attempted to date.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L443">
<title id=" I05-2010.xml">applying a mix word pair identifier to the chinese syllabletoword conversion problem </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(kangaroo)?, or multi-syllabic word, such as ????(mickey mouse).?
</prevsent>
<prevsent>although there are more than 13,000 distinct chinese characters (of which 5,400 are commonly used), there are only about 1,300 distinct syllables.
</prevsent>
</prevsection>
<citsent citstr=" C02-1089 ">
since the size of problem space for syllable-to-word (stw) conversion is much less than that of syllable-to character (stc) conversion, the most existing chinese input systems (hsu 1994, hsu et al 1999, tsai and hsu 2002, <papid> C02-1089 </papid>gao et al 2002, msime) are addressed on stw conversion.</citsent>
<aftsection>
<nextsent>conventionally, there are two approaches for stw conversion: (1) the linguistic approach based on syntax parsing, semantic template matching and contextual information (hsu 1994, fu et al 1996, hsu et al 1999, kuo 1995, tsai and hsu 2002); <papid> C02-1089 </papid>and (2) the statistical approach based on the n-gram models where is usually 2 or 3 (lin and tsai 1987, gu et al 1991, fu et al 1996, ho et al 1997, sproat 1990, gao et al 2002, lee 2003).</nextsent>
<nextsent>although the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, it is more user-friendly than the statistical approach on understanding why such system makes mistake (hsu 1994, tsai and hsu 2002).<papid> C02-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L447">
<title id=" I05-2010.xml">applying a mix word pair identifier to the chinese syllabletoword conversion problem </title>
<section> development of mix-wp identifier.  </section>
<citcontext>
<prevsection>
<prevsent>de5shuai1wei2 ? ?
</prevsent>
<prevsent>for the same syllables, the msime will convert them into ???[a]??[famous]?[of]??[decay]??[process].?
</prevsent>
</prevsection>
<citsent citstr=" I05-3002 ">
the detailed analysis and demonstration of our cwp identifier can be found in (tsai 2005).<papid> I05-3002 </papid></citsent>
<aftsection>
<nextsent>appendix presents case of the cwp identified results.
</nextsent>
<nextsent>to evaluate the stw performance of our mix wp identifier, the stw accuracy, the identified character ratio (icr) and the stw improvement were used (tsai 2005).<papid> I05-3002 </papid></nextsent>
<nextsent>3.1 experimental data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L451">
<title id=" I08-1066.xml">refinements in btg based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the two refinements are integrated intoa well-established btg-based chinese-toenglish smt system that is trained on large scale parallel data.
</prevsent>
<prevsent>experimental results on the nist mt-05 task show that the proposed refinements contribute significant improvement of 2% in bleu score over the baseline system.
</prevsent>
</prevsection>
<citsent citstr=" P96-1021 ">
bracket transduction grammar was proposed by wu (1995) and firstly employed in statistical machine translation in (wu, 1996).<papid> P96-1021 </papid></citsent>
<aftsection>
<nextsent>because of its good tradeoff between efficiency and expressiveness, btg restriction is widely used for reordering in smt (zens et al, 2004).<papid> C04-1030 </papid></nextsent>
<nextsent>however, btg restriction does not provide mechanism to predict final orders between two neighboring blocks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L452">
<title id=" I08-1066.xml">refinements in btg based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results on the nist mt-05 task show that the proposed refinements contribute significant improvement of 2% in bleu score over the baseline system.
</prevsent>
<prevsent>bracket transduction grammar was proposed by wu (1995) and firstly employed in statistical machine translation in (wu, 1996).<papid> P96-1021 </papid></prevsent>
</prevsection>
<citsent citstr=" C04-1030 ">
because of its good tradeoff between efficiency and expressiveness, btg restriction is widely used for reordering in smt (zens et al, 2004).<papid> C04-1030 </papid></citsent>
<aftsection>
<nextsent>however, btg restriction does not provide mechanism to predict final orders between two neighboring blocks.
</nextsent>
<nextsent>to solve this problem, xiong et al (2006)<papid> P06-1066 </papid>proposed an enhanced btg with maximum entropy (maxent) based reordering model (mebtg).</nextsent>
<nextsent>mebtg uses boundary words of bilingual phrases as features to predict their orders.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L453">
<title id=" I08-1066.xml">refinements in btg based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>because of its good tradeoff between efficiency and expressiveness, btg restriction is widely used for reordering in smt (zens et al, 2004).<papid> C04-1030 </papid></prevsent>
<prevsent>however, btg restriction does not provide mechanism to predict final orders between two neighboring blocks.</prevsent>
</prevsection>
<citsent citstr=" P06-1066 ">
to solve this problem, xiong et al (2006)<papid> P06-1066 </papid>proposed an enhanced btg with maximum entropy (maxent) based reordering model (mebtg).</citsent>
<aftsection>
<nextsent>mebtg uses boundary words of bilingual phrases as features to predict their orders.
</nextsent>
<nextsent>xiong etal.
</nextsent>
<nextsent>(2006) reported significant performance improvement on chinese-english translation tasks in two different domains when compared with both pharaoh (koehn, 2004) and the original btg using flat reordering.
</nextsent>
<nextsent>however, error analysis of the translation output of xiong et al (2006)<papid> P06-1066 </papid> reveals that boundary words predict wrong swapping, especially for long phrases although the maxent-based reordering model shows better performance than baseline reordering models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L457">
<title id=" I08-1066.xml">refinements in btg based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, error analysis of the translation output of xiong et al (2006)<papid> P06-1066 </papid> reveals that boundary words predict wrong swapping, especially for long phrases although the maxent-based reordering model shows better performance than baseline reordering models.</prevsent>
<prevsent>another big problem with btg-based smt is thehigh computational cost.</prevsent>
</prevsection>
<citsent citstr=" W05-1507 ">
huang et al (2005) <papid> W05-1507 </papid>reported that the time complexity of btg decoding with m-gram language model is o(n3+4(m1)).</citsent>
<aftsection>
<nextsent>if 4-gram language model is used (common in many current smt systems), the time complexity is as high as o(n15).
</nextsent>
<nextsent>therefore with this time complexity translating long sentences is time-consuming even with highly stringent pruning strategy.
</nextsent>
<nextsent>to speed up btg decoding, huang et al (2005) <papid> W05-1507 </papid>adapted the hook trick which changes the time complexity from o(n3+4(m1)) to o(n3+3(m1)).</nextsent>
<nextsent>however, the implementation of the hook trick with pruning is quite complicated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L461">
<title id=" I08-1066.xml">refinements in btg based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results show that both refinements improve the bleu score significantly on large-scale data.
</prevsent>
<prevsent>the above refinements can be easily implemented and integrated into baseline btg-based smt system.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
however, they are not specially designed for btg-based smt and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrase based system (koehn et al, 2007), <papid> P07-2045 </papid>syntax-based systems (chiang et al, 2005; <papid> H05-1098 </papid>marcu et al, 2006; <papid> W06-1606 </papid>liu et al., 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>the rest of the paper is organized as follows.
</nextsent>
<nextsent>in section 2, we review briefly the core elements of the baseline system.
</nextsent>
<nextsent>in section 3 we describe our proposed refinements in detail.
</nextsent>
<nextsent>section 4 presents the evaluation results on chinese-to-english translation based on these refinements as well as results obtained in the nist mt-06 evaluation exercise.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L462">
<title id=" I08-1066.xml">refinements in btg based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results show that both refinements improve the bleu score significantly on large-scale data.
</prevsent>
<prevsent>the above refinements can be easily implemented and integrated into baseline btg-based smt system.
</prevsent>
</prevsection>
<citsent citstr=" H05-1098 ">
however, they are not specially designed for btg-based smt and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrase based system (koehn et al, 2007), <papid> P07-2045 </papid>syntax-based systems (chiang et al, 2005; <papid> H05-1098 </papid>marcu et al, 2006; <papid> W06-1606 </papid>liu et al., 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>the rest of the paper is organized as follows.
</nextsent>
<nextsent>in section 2, we review briefly the core elements of the baseline system.
</nextsent>
<nextsent>in section 3 we describe our proposed refinements in detail.
</nextsent>
<nextsent>section 4 presents the evaluation results on chinese-to-english translation based on these refinements as well as results obtained in the nist mt-06 evaluation exercise.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L463">
<title id=" I08-1066.xml">refinements in btg based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results show that both refinements improve the bleu score significantly on large-scale data.
</prevsent>
<prevsent>the above refinements can be easily implemented and integrated into baseline btg-based smt system.
</prevsent>
</prevsection>
<citsent citstr=" W06-1606 ">
however, they are not specially designed for btg-based smt and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrase based system (koehn et al, 2007), <papid> P07-2045 </papid>syntax-based systems (chiang et al, 2005; <papid> H05-1098 </papid>marcu et al, 2006; <papid> W06-1606 </papid>liu et al., 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>the rest of the paper is organized as follows.
</nextsent>
<nextsent>in section 2, we review briefly the core elements of the baseline system.
</nextsent>
<nextsent>in section 3 we describe our proposed refinements in detail.
</nextsent>
<nextsent>section 4 presents the evaluation results on chinese-to-english translation based on these refinements as well as results obtained in the nist mt-06 evaluation exercise.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L464">
<title id=" I08-1066.xml">refinements in btg based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results show that both refinements improve the bleu score significantly on large-scale data.
</prevsent>
<prevsent>the above refinements can be easily implemented and integrated into baseline btg-based smt system.
</prevsent>
</prevsection>
<citsent citstr=" P06-1077 ">
however, they are not specially designed for btg-based smt and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrase based system (koehn et al, 2007), <papid> P07-2045 </papid>syntax-based systems (chiang et al, 2005; <papid> H05-1098 </papid>marcu et al, 2006; <papid> W06-1606 </papid>liu et al., 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>the rest of the paper is organized as follows.
</nextsent>
<nextsent>in section 2, we review briefly the core elements of the baseline system.
</nextsent>
<nextsent>in section 3 we describe our proposed refinements in detail.
</nextsent>
<nextsent>section 4 presents the evaluation results on chinese-to-english translation based on these refinements as well as results obtained in the nist mt-06 evaluation exercise.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L468">
<title id=" I08-1066.xml">refinements in btg based statistical machine translation </title>
<section> the baseline system.  </section>
<citcontext>
<prevsection>
<prevsent>the translation process is modeled using btg rules which are listed as follows ? [a1, a2] (1) ? a1, a2?
</prevsent>
<prevsent>(2) ? x/y (3) the lexical rule (3) is used to translate source phrase into target phrase and generate block a. thetwo rules (1) and (2) are used to merge two consecutive blocks into single larger block in straight or inverted order.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
to construct stochastic btg, we calculate rule probabilities using the log-linear model (och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>for the two merging rules (1) and (2),the assigned probability prm(a) is defined as follows prm(a) = ???
</nextsent>
<nextsent>4lmplm (a1,a2) (4) where ?, the reordering score of block a1 and a2, is calculated using the maxent-based reordering model (xiong et al, 2006) <papid> P06-1066 </papid>described in the next section, ??</nextsent>
<nextsent>is the weight of ?, and 4plm (a1,a2) is the increment of language model score of the two blocks according to their final order, lm is its weight.for the lexical rule (3), it is applied with probability prl(a) prl(a) = p(x|y)1 ? p(y|x)2 ? plex(x|y)3 plex(y|x)4 ? exp(1)5 ? exp(|y|)6 plmlm (y) (5) where p(?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L471">
<title id=" I08-1066.xml">refinements in btg based statistical machine translation </title>
<section> the baseline system.  </section>
<citcontext>
<prevsection>
<prevsent>the maxent-based reordering model is defined on two consecutive blocks a1 and a2 together with their order ? {straight, inverted} according to the maximum entropy framework.
</prevsent>
<prevsent>= p?(o|a1, a2) = exp( ? ihi(o,a1, a2))?
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
o exp( ? ihi(o,a1, a2))(6) where the functions hi ? {0, 1} are model feature sand are weights of the model features trained automatically (malouf, 2002).<papid> W02-2018 </papid>there are three steps to train maxent-based reordering model.</citsent>
<aftsection>
<nextsent>first, we need to extract reordering examples from unannotated bilingual data, then generate features from these examples and finally estimate feature weights.
</nextsent>
<nextsent>506 for extracting reordering examples, there are two points worth mentioning: 1.
</nextsent>
<nextsent>in the extraction of useful reordering examples,.
</nextsent>
<nextsent>there is no length limitation over blocks compared with extracting bilingual phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L478">
<title id=" I08-2118.xml">learning decision lists with known rules for text mining </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our experiments we also observe performance improvement over an existing decision list learning algorithm, by merely re-ordering the rules output by it.
</prevsent>
<prevsent>rule-based systems have been extensively used for several problems in text mining.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
some problems in text mining where rule-based systems have been successfully used are part of speech tagging (brill, 1992), <papid> A92-1021 </papid>named entity annotation (grishman, 1997;appelt et al, 1995), information extraction (may nard et al, 2001), question answering (riloff and thelen, 2000) <papid> W00-0603 </papid>and classification (han et al, 2003; liand yamanishi, 1999; sasaki and kita, 1998).</citsent>
<aftsection>
<nextsent>several studies have been conducted that compare the performance of rule-based systems and other machine learning techniques with mixed results.
</nextsent>
<nextsent>while there is no clear winner between the two approaches in terms of performance, the rule-based approach is clearly preferred in operational settings (borth wick, 1999; varadarajan et al, 2002).
</nextsent>
<nextsent>rule-basedsystems are human comprehensible and can be improved over time.
</nextsent>
<nextsent>therefore, it is imperative to develop methods that assist in building rule-based systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L479">
<title id=" I08-2118.xml">learning decision lists with known rules for text mining </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our experiments we also observe performance improvement over an existing decision list learning algorithm, by merely re-ordering the rules output by it.
</prevsent>
<prevsent>rule-based systems have been extensively used for several problems in text mining.
</prevsent>
</prevsection>
<citsent citstr=" W00-0603 ">
some problems in text mining where rule-based systems have been successfully used are part of speech tagging (brill, 1992), <papid> A92-1021 </papid>named entity annotation (grishman, 1997;appelt et al, 1995), information extraction (may nard et al, 2001), question answering (riloff and thelen, 2000) <papid> W00-0603 </papid>and classification (han et al, 2003; liand yamanishi, 1999; sasaki and kita, 1998).</citsent>
<aftsection>
<nextsent>several studies have been conducted that compare the performance of rule-based systems and other machine learning techniques with mixed results.
</nextsent>
<nextsent>while there is no clear winner between the two approaches in terms of performance, the rule-based approach is clearly preferred in operational settings (borth wick, 1999; varadarajan et al, 2002).
</nextsent>
<nextsent>rule-basedsystems are human comprehensible and can be improved over time.
</nextsent>
<nextsent>therefore, it is imperative to develop methods that assist in building rule-based systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L480">
<title id=" I05-3010.xml">turn taking in mandarin dialogue interactions of tone and intonation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these analyses have proposed wide range of cues to turn status, ranging from gaze and gesture in multi-modal context to prosodic cues including pitch, intensity, and duration as well as lexical and syntactic cues.
</prevsent>
<prevsent>much of this fundamental research as well as computational implementations have focused on english, language with well-studied into national sentence and discourse structure.
</prevsent>
</prevsection>
<citsent citstr=" W04-3209 ">
a substantial body of work has identified sentence-likeunits as well as fragments and repairs in conversational speech, including (ostendorf, forth coming; liu et al, 2004; <papid> W04-3209 </papid>shriberg et al, 2001) these approaches have employed lexical and prosodic cues in diverse frameworks, including hidden markov models employing decision tree sand hidden state language models, neural networks, and maximum entropy models.</citsent>
<aftsection>
<nextsent>(shriberget al, 2001) identified jump-in points and jump in words in multi-party meeting speech using prosodic and language model features at accuracies of 65 and 77% under equal priors.
</nextsent>
<nextsent>further more, (ward and tsukahara, 2000) demonstrated that back channels occurred at predictable points 72with specific prosodic characteristics in both english and japanese.in the current paper, we consider the interaction of potential prosodic into national cues related to turn-taking with the realization of lexical tone in tone language, putonghua or mandarinchinese.
</nextsent>
<nextsent>mandarin employs four canonical lexical tones distinguished by pitch height and pitch contour: high level, mid-rising, low falling-rising, and high falling.
</nextsent>
<nextsent>we explore whether prosodic features are also employed in turn-taking behavior in this language and whether the forms are comparable to those employed in languages with lexical tone.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L481">
<title id=" I05-3010.xml">turn taking in mandarin dialogue interactions of tone and intonation </title>
<section> experimental data.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 prosodic features.
</prevsent>
<prevsent>for the subsequent analysis, the conversations were divided into chunks based on theturn and overlap time-stamps.
</prevsent>
</prevsection>
<citsent citstr=" H01-1073 ">
using chinese character-to-pinyin dictionary and hand constructed mapping of pinyin sequences to arpabet phonemes, the transcribed text was then force-aligned to the corresponding audio segments using the language porting mechanism in the university of colorado sonic speech recognizer (pellom et al, 2001).<papid> H01-1073 </papid></citsent>
<aftsection>
<nextsent>the resulting alignment provided phone, syllable, and word durations as well as silence positions and durations.
</nextsent>
<nextsent>pitch and intensity values for voiced regions were computed using the functions to pitch?
</nextsent>
<nextsent>andto intensity?
</nextsent>
<nextsent>in the freely available praat acoustic analysis software package(boersma, 2001).we then computed normalized pitch and intensity values based on log-scaled z-score normalization of each conversation side.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L482">
<title id=" I05-3010.xml">turn taking in mandarin dialogue interactions of tone and intonation </title>
<section> tone and intonation.  </section>
<citcontext>
<prevsection>
<prevsent>the contrasts appear in figure 5.turning to tone contour, we find likewise little change between non-final and final contours, with the contours running parallel, but at much lower pitch.4 for illustration, mid-rising and high falling tones are shown in figure 6.
</prevsent>
<prevsent>comparable behavior has been observed at other discourse boundaries such as story boundaries in newswire speech.
</prevsent>
</prevsection>
<citsent citstr=" N04-4035 ">
(levow, 2004).<papid> N04-4035 </papid></citsent>
<aftsection>
<nextsent>and interruptions based on the salient contrasts in pitch and intensity observed above, we employ prosodic features both to identify turn boundaries and to distinguish between the start of interruptions and smooth transitions.
</nextsent>
<nextsent>we further contrast the use of prosodic features with text n-gram features.3this analysis excludes exclamatory and interject ive particles.
</nextsent>
<nextsent>4it is also true that contours do not always match their canonical forms even in non-final position.
</nextsent>
<nextsent>this variation may be attributed to combination of tonal coarticulatory effects and the presence of other turn-internal boundaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L483">
<title id=" I08-2104.xml">statistical machine translation based passage retrieval for cross lingual question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>depending on the translation techniques used for the pre-processing, the previous clqa approach can be classified into the machine translation based approach (shimizu etal., 2005; mori and kawagishi, 2005) and the dictionary based approach (isozaki et al, 2005).
</prevsent>
<prevsent>in this paper, we propose novel approach forclqa task.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
in the proposed method, the statistical machine translation (smt) (brown et al, 1993)<papid> J93-2003 </papid>is deeply incorporated into the question answering process, instead of using the smt as the preprocessing before the mono-lingual qa process as in the previous work.</citsent>
<aftsection>
<nextsent>though the proposed method can be applied to any language pairs in principle, we focus on the english-to-japanese (ej) clqa task, where question sentence is given in english and its answer is extracted from document collection in japanese.recently, language modeling approach for information retrieval has been widely studied (croft and lafferty, 2003).
</nextsent>
<nextsent>among them, statistical translation model has been applied for mono-lingual ir (berger and lafferty, 1999), cross-lingual ir (xu et al, 2001), and mono-lingual qa (murdock and croft, 2004).
</nextsent>
<nextsent>our method can be considered as that applying the translation model to cross-lingual qa.
</nextsent>
<nextsent>in the rest of this paper, section 2 summarizes the previous approach for clqa.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L485">
<title id=" I08-2104.xml">statistical machine translation based passage retrieval for cross lingual question answering </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>for the sentences of japanese side, the inflectional words were normalized to their basic forms by using japanese morphological analyzer.
</prevsent>
<prevsent>for the sentences of english side, the inflectional words were also normalized to their basic forms by using part-of-speech tagger and all the words were lowercased.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
giza++ (och and ney, 2003) <papid> J03-1002 </papid>was used for training the ibm model 4 from the normalized parallel corpus.</citsent>
<aftsection>
<nextsent>the vocabulary sizes were about 58k words for japanese side and 74k words for english side.
</nextsent>
<nextsent>the trained japanese-to-english word translation model t(e|j)was used for our proposed document retrieval (sec tion 3.1) and passage similarity calculation (section 3.2).
</nextsent>
<nextsent>4.3 compared methods.
</nextsent>
<nextsent>the proposed method was compared with the several reference methods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L486">
<title id=" I08-2104.xml">statistical machine translation based passage retrieval for cross lingual question answering </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>as the methods from previous works, three pre-translation methods were in vestigated.the first two methods translate the question by using machine translation.
</prevsent>
<prevsent>one of them used commercial off-the-shell machine translation software 2(referred to as rmt).
</prevsent>
</prevsection>
<citsent citstr=" N03-1010 ">
the other used the statistical machine translation that had been created by using the ibm model 4 obtained from the same parallel corpus and tools described in section 4.2, the tri-gram language model constructed by using the target documents of clqa1, and the existing smt decoder (germann, 2003) (<papid> N03-1010 </papid>referred to as smt).</citsent>
<aftsection>
<nextsent>the two methods, rmt and smt, differ only inthe translation methods, while their backend monolingual qa systems are common.the third method translates the question by using translation dictionary (referred to as dict).
</nextsent>
<nextsent>the cross-lingual ir system described in (fujii and ishikawa, 2001) was used for our document re trieval?
</nextsent>
<nextsent>subsystem in figure 2.
</nextsent>
<nextsent>the clir system enhances the basic translation dictionary, which has about 1,000,000 entries, with the compound words obtained by using the statistics of the target documents and with the borrowed words by using the transliteration method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L487">
<title id=" I05-3007.xml">chinese sketch engine and the extraction of grammatical collocations </title>
<section> sketch engine: new corpus-based.  </section>
<citcontext>
<prevsection>
<prevsent>the most salient grammatical information, such as grammatical functions (subject, object, adjunct etc.) is beyond the scope of the traditional corpus interface tools.
</prevsent>
<prevsent>traditional corpora relyon the human users to arrive at these kinds of generalizations.
</prevsent>
</prevsection>
<citsent citstr=" W03-1707 ">
approach to grammatical information several existing linguistically annotated corpus of chinese, e.g. penn chinese tree bank (xia et al , 2000), sinica treebank (chen et al , 2003), proposition bank (xue and palmer, 2003, <papid> W03-1707 </papid>2005) and mandarin verbnet (wu and liu, 2003), suffer from the same problem.</citsent>
<aftsection>
<nextsent>they are all extremely labor-intensive to build and typically have narrow coverage.
</nextsent>
<nextsent>in addition, since structural assignment is theory-dependent and abstract, inter-annotator consistency is difficult to achieve.
</nextsent>
<nextsent>since there is also no general consensus on the annotation scheme in chinese nlp and linguistics, building an effective interface for public use is almost impossible.
</nextsent>
<nextsent>the sketch engine offers an answer to the above issues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L488">
<title id=" I05-3007.xml">chinese sketch engine and the extraction of grammatical collocations </title>
<section> evaluation and future developments.  </section>
<citcontext>
<prevsection>
<prevsent>such information is not available in the bnc tagset and hence not used in the original sketch engine design.
</prevsent>
<prevsent>we will enrich the collocation patterns with the annotated linguistic information from the sinica corpus tagset.
</prevsent>
</prevsection>
<citsent citstr=" C90-2010 ">
in particular, we are converting icg lexical subcategorization frames (chen and huang 1990) <papid> C90-2010 </papid>to sketch engine collocation patters.</citsent>
<aftsection>
<nextsent>these icg frames, called basic patterns and adjunct patterns, have already been fully annotated lexically and tested on the sinica corpus.
</nextsent>
<nextsent>we expect their incorporation to improve chinese sketch engine results markedly.
</nextsent>
<nextsent>6.
</nextsent>
<nextsent>conclusion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L489">
<title id=" I08-1029.xml">automatic prosodic labeling with conditional random fields and rich acoustic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other contextual evidence such as relative pitch height orband energy between syllables has also been employed (levow, 2005; rosenberg and hirschberg, 2006).
</prevsent>
<prevsent>interestingly, although earlier techniques (ross and ostendorf, 1994; duster hoff et al , 1999)employed hidden markov models, they did not explicitly model these coarticulatory effects, and recent approaches have primarily employed local classifiers, such as decision trees (sun, 2002; rosenberg and hirschberg, 2006) or support vector machines (levow, 2005).another body of work on pitch accent recognition has focused on exploitation of lexical and syntactic information to predict tobi labels, for example for speech synthesis.
</prevsent>
</prevsection>
<citsent citstr=" P98-2165 ">
these approaches explored range of machine learning techniques from local classifiers such as decision trees (sun,2002) and ripper (pan and mckeown, 1998) <papid> P98-2165 </papid>to sequence models such as conditional random fields 217 (crfs)(gregory and altun, 2004) <papid> P04-1086 </papid>more recently.the systems often included features that captured local or longer range context, such as n-gram probabilities, neighboring words, or even indicators of prior mention.</citsent>
<aftsection>
<nextsent>(chen et al , 2004; rangarajan sridhar et al ., 2007) explored the integration of based prosodic and lexico-syntactic evidence in gmm-based and maximum entropy models respectively.here we explore the use of contextual acoustic and lexical models within sequence learningframework.
</nextsent>
<nextsent>we analyze the interaction of different feature types on prediction of prosodic labels using linear-chain crfs.
</nextsent>
<nextsent>we demonstrate improved recognition by integration of textual and acoustic cues, well-supported by the sequence model.
</nextsent>
<nextsent>finally we consider the joint prediction of multiple prosodic label types, finding improvement for joint modeling in the case of feature sets with lower initial performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L490">
<title id=" I08-1029.xml">automatic prosodic labeling with conditional random fields and rich acoustic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other contextual evidence such as relative pitch height orband energy between syllables has also been employed (levow, 2005; rosenberg and hirschberg, 2006).
</prevsent>
<prevsent>interestingly, although earlier techniques (ross and ostendorf, 1994; duster hoff et al , 1999)employed hidden markov models, they did not explicitly model these coarticulatory effects, and recent approaches have primarily employed local classifiers, such as decision trees (sun, 2002; rosenberg and hirschberg, 2006) or support vector machines (levow, 2005).another body of work on pitch accent recognition has focused on exploitation of lexical and syntactic information to predict tobi labels, for example for speech synthesis.
</prevsent>
</prevsection>
<citsent citstr=" P04-1086 ">
these approaches explored range of machine learning techniques from local classifiers such as decision trees (sun,2002) and ripper (pan and mckeown, 1998) <papid> P98-2165 </papid>to sequence models such as conditional random fields 217 (crfs)(gregory and altun, 2004) <papid> P04-1086 </papid>more recently.the systems often included features that captured local or longer range context, such as n-gram probabilities, neighboring words, or even indicators of prior mention.</citsent>
<aftsection>
<nextsent>(chen et al , 2004; rangarajan sridhar et al ., 2007) explored the integration of based prosodic and lexico-syntactic evidence in gmm-based and maximum entropy models respectively.here we explore the use of contextual acoustic and lexical models within sequence learningframework.
</nextsent>
<nextsent>we analyze the interaction of different feature types on prediction of prosodic labels using linear-chain crfs.
</nextsent>
<nextsent>we demonstrate improved recognition by integration of textual and acoustic cues, well-supported by the sequence model.
</nextsent>
<nextsent>finally we consider the joint prediction of multiple prosodic label types, finding improvement for joint modeling in the case of feature sets with lower initial performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L492">
<title id=" I08-1029.xml">automatic prosodic labeling with conditional random fields and rich acoustic features </title>
<section> modeling with linear-chain and.  </section>
<citcontext>
<prevsection>
<prevsent>to capture these relations between labels of different types, we also consider facto rial models.
</prevsent>
<prevsent>conditional random fields (lafferty et al , 2001) are class of graphical models which are undirected and conditionally trained.
</prevsent>
</prevsection>
<citsent citstr=" P05-1056 ">
while they can represent long term dependencies, most applications have employed first-order linear chains for language and speech processing tasks including pos tagging, sentence boundary detection (liu et al , 2005), <papid> P05-1056 </papid>and even text-oriented pitch accent prediction(gregory and altun, 2004).<papid> P04-1086 </papid></citsent>
<aftsection>
<nextsent>the models capture sequentiallabel-label relations, but unlike hmms, the conditionally trained model can more tract ably support larger text-based feature sets.
</nextsent>
<nextsent>facto rial crfs (sutton, 2006; mccallum et al , 2003) augment the linear sequence model with additional co temporal labels, so that multiple (factors) labels are predicted at each time step and dependencies between themcan be modeled.
</nextsent>
<nextsent>examples of linear-chain and fac torial crfs appear in figure 1.
</nextsent>
<nextsent>in the linear chain example, the fi items correspond to the features and the yi to labels to be predicted, for example prosodic and text features and pitch accent labels respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L495">
<title id=" I08-2090.xml">a linguistic and navigational knowledge approach to text navigation </title>
<section> navitexte: text navigation system.  </section>
<citcontext>
<prevsection>
<prevsent>another important difference is that navitexte gives the user the liberty to navigate through the text following its own interests (the system propose - the reader chooses), while the mentioned systems try to maintain user stuck to given route (the user chooses - the system propose) (hk and svensson, 1999).
</prevsent>
<prevsent>navitexte consists of sub-systems dealing with: text representation, navigational knowledge, visual representation and user interaction.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
the first one builds text representation in memory from text annotated manually or by dedicated software (cun ningham et al, 2002; <papid> P02-1022 </papid>bilhaut et al, 2003).</citsent>
<aftsection>
<nextsent>the second sub-system loads and compiles the knowledge modules.
</nextsent>
<nextsent>the result of this compilation is graph of potential navigation courses that in practice is calculated as needed and stored in optimization data structures.
</nextsent>
<nextsent>the third sub-system calculates and displays different text views and the fourth one manages the user interaction.
</nextsent>
<nextsent>the reader has the possibility to load and unload several texts and knowledge modules in the same work session.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L496">
<title id=" I05-2038.xml">syntax annotation for the genia corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while basic nlp techniques are relatively general and portable from domain to domain, customization and tuning are inevitable, especially in order to apply the techniques effectively to highly specialized literatures such as research papers and abstracts.
</prevsent>
<prevsent>as recent advances in nlp technology depend on machine learning techniques, annotated corpora from which system can acquire rules (including grammar rules, lexicon, etc.) are indispensable 220 resources for customizing general-purpose nlp tools.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
in bio-textmining, for example, training on part-of-speech (pos)-annotated genia corpus was reported to improve the accuracy of junk tagger (english pos tagger) (kazama et al., 2001) from 83.5% to 98.1% on medline abstracts (tateisi and tsujii, 2004), and the framed corpus (wermter and hahn, 2004) was used to train tnt tagger on german (brants, 2000) <papid> A00-1031 </papid>to improve its accuracy from 95.7% to 98% on clinical reports and other biomedical texts.</citsent>
<aftsection>
<nextsent>corpus annotated for syntactic structures is expected to play similar role in tuning parsers to biomedical domain, i.e., similar improvement on the performance of parsers is expected by using domain-specific treebank as resource for learning.
</nextsent>
<nextsent>for this purpose, we construct gena treebank (gtb), treebank on research abstracts in biomedical domain.
</nextsent>
<nextsent>the base text of gtb is that of the genia corpus constructed at university of tokyo (kim et al., 2003), which is collection of research abstracts selected from the search results of medline database with keywords (mesh terms) human, blood cells and transcription factors.
</nextsent>
<nextsent>in the genia corpus, the abstracts are encoded in an xml scheme where each abstract is numbered with medline uid and contains title and abstract.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L497">
<title id=" I05-2038.xml">syntax annotation for the genia corpus </title>
<section> outline of the corpus.  </section>
<citcontext>
<prevsection>
<prevsent>gtb is the addition of syntactic information to the genia corpus.
</prevsent>
<prevsent>by annotating various linguistic information on same set of text, the genia corpus will be resource not only for individual purpose such as named entity extraction or training parsers but also for integrated systems such as information extraction using deep linguistic analysis.
</prevsent>
</prevsection>
<citsent citstr=" W04-3111 ">
similar attempt of constructing integrated corpora is being done in university of pennsylvania, where corpus of medline abstracts in cyp450 and oncology domains where annotated for named entities, pos, and tree structure of sentences (kulick et al, 2004).<papid> W04-3111 </papid></citsent>
<aftsection>
<nextsent>2.1 annotation scheme.
</nextsent>
<nextsent>the annotation scheme basically follows the penn treebank ii (ptb) scheme (beis et al 1995), encoded in xml.
</nextsent>
<nextsent>a non-null constituent is marked as an element, with its syntactic category (which may be combined with its function tags indicating grammatical roles such as -sbj, -prd, and -adv) used as tags.
</nextsent>
<nextsent>a null constituent is marked as childless element whose tag corresponds to its categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L498">
<title id=" I05-2038.xml">syntax annotation for the genia corpus </title>
<section> annotation results.  </section>
<citcontext>
<prevsection>
<prevsent>the 500 abstracts with correction of these errors are made publicly available as the genia treebank beta version?
</prevsent>
<prevsent>(gtb-beta).
</prevsent>
</prevsection>
<citsent citstr=" C04-1204 ">
for further clean-up, we also tried to parse the corpus by the enju parser (miyao and tsujii 2004), <papid> C04-1204 </papid>and identify the error of the corpus by investigating into the parse errors.</citsent>
<aftsection>
<nextsent>enju is an hpsg parser that can be trained with ptb-type corpora which is reported to have 87% accuracy on wall street journal portion of penn treebank corpus.
</nextsent>
<nextsent>currently the accuracy of the parser drops down to 82% on gtb-beta, and although proper quantitative analysis is yet to be done, it was found that the mismatches between labels of the treebank and the genia pos corpus (e.g. an ing form labeled as noun in the pos corpus and as the head of verb phrase in the tree corpus) are major source of parse error.
</nextsent>
<nextsent>the correction is complicated because several errors in the genia pos corpus were found in this cleaning-up process.
</nextsent>
<nextsent>when the cleaning-up process is done, we will make the corpus publicly available as the proper release.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L500">
<title id=" I05-4002.xml">evaluation of a japanese cfg derived from a syntactically annotated corpus with respect to dependency measures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>parsing is one of the important processes for natural language processing and, in general, largescale cfg is used to parse wide variety of sentences.
</prevsent>
<prevsent>although it is difficult to build large scale cfg manually, cfg can be derived from large-scale syntactically annotated corpus.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
formany languages, large-scale syntactically annotated corpora have been built (e.g. the penn treebank (marcus et al, 1993)), <papid> J93-2004 </papid>and many parsing algorithms using cfgs have been proposed.</citsent>
<aftsection>
<nextsent>however, such syntactically annotated corpus has not been built for japanese as of yet.
</nextsent>
<nextsent>dependency analysis is preferred in order to analyze japanese sentences (dependency relation between japanese phrasal unit, called bunsetsu) (kuro hashi and nagao, 1998; uchimoto et al, 2000;kudo and matsumoto, 2002), <papid> W02-2016 </papid>and only few studies about japanese cfg have been conducted.</nextsent>
<nextsent>since many efficient parsing algorithms for cfg have been proposed, japanese cfg is necessary to apply the algorithms to japanese.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L501">
<title id=" I05-4002.xml">evaluation of a japanese cfg derived from a syntactically annotated corpus with respect to dependency measures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>formany languages, large-scale syntactically annotated corpora have been built (e.g. the penn treebank (marcus et al, 1993)), <papid> J93-2004 </papid>and many parsing algorithms using cfgs have been proposed.</prevsent>
<prevsent>however, such syntactically annotated corpus has not been built for japanese as of yet.</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
dependency analysis is preferred in order to analyze japanese sentences (dependency relation between japanese phrasal unit, called bunsetsu) (kuro hashi and nagao, 1998; uchimoto et al, 2000;kudo and matsumoto, 2002), <papid> W02-2016 </papid>and only few studies about japanese cfg have been conducted.</citsent>
<aftsection>
<nextsent>since many efficient parsing algorithms for cfg have been proposed, japanese cfg is necessary to apply the algorithms to japanese.
</nextsent>
<nextsent>we have been building large-scale japanese syntactically annotated corpus to derive japanese cfg for syntactic parsing (noro et al,2004a; noro et al, 2004b).
</nextsent>
<nextsent>according to there sult, cfg derived from the corpus can parse sentences with high accuracy and coverage.
</nextsent>
<nextsent>however, as mentioned previously, dependency analysis is usually adopted in japanese nlp, and it is difficult to compare our result with results of other dependency analysis since we evaluated ourcfg with respect to phrase structure based measure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L503">
<title id=" I05-4002.xml">evaluation of a japanese cfg derived from a syntactically annotated corpus with respect to dependency measures </title>
<section> annotation policy.  </section>
<citcontext>
<prevsection>
<prevsent>the main aspects of our policy are as follows: verb conjugation: information about verb conjugation is added to each intermediate node related to the verb (cf.
</prevsent>
<prevsent>split-vp?
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
in (klein and manning, 2003) <papid> P03-1054 </papid>and verb form?</citsent>
<aftsection>
<nextsent>in (schiehlen, 2004)).<papid> C04-1056 </papid>compound noun structure: structure ambiguity of compound noun is represented as the same structure regardless of the meaning or word-formation as shirai et al described in (shirai et al, 1995).</nextsent>
<nextsent>adnominal and adverbial phrase attachment: structure ambiguity of adnominal phrase attachment is represented as the same structure regardless of the meaning while structure ambiguity of adverbial phrase attachment is distinguished by meaning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L504">
<title id=" I05-4002.xml">evaluation of a japanese cfg derived from a syntactically annotated corpus with respect to dependency measures </title>
<section> annotation policy.  </section>
<citcontext>
<prevsection>
<prevsent>split-vp?
</prevsent>
<prevsent>in (klein and manning, 2003) <papid> P03-1054 </papid>and verb form?</prevsent>
</prevsection>
<citsent citstr=" C04-1056 ">
in (schiehlen, 2004)).<papid> C04-1056 </papid>compound noun structure: structure ambiguity of compound noun is represented as the same structure regardless of the meaning or word-formation as shirai et al described in (shirai et al, 1995).</citsent>
<aftsection>
<nextsent>adnominal and adverbial phrase attachment: structure ambiguity of adnominal phrase attachment is represented as the same structure regardless of the meaning while structure ambiguity of adverbial phrase attachment is distinguished by meaning.
</nextsent>
<nextsent>in case of phrase like watashi no chichi no hon (my fathers book)?, the structure is same whether the adnominal phrase watashi no (my)?
</nextsent>
<nextsent>attaches to the noun chichi (father)?
</nextsent>
<nextsent>or the noun hon (book)?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L505">
<title id=" I05-4002.xml">evaluation of a japanese cfg derived from a syntactically annotated corpus with respect to dependency measures </title>
<section> annotation policy.  </section>
<citcontext>
<prevsection>
<prevsent>or the verb katta (buy)?
</prevsent>
<prevsent>(it means he bought picture of sea?).
</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
conjunctive structure: conjunctive structure is not specified during syntactic parsing, instead their analysis is left for the subsequent processing (contrary to (kurohashi and na gao, 1994)).<papid> J94-4001 </papid></citsent>
<aftsection>
<nextsent>we have decided to deal with adnominal phrase attachment and adverbial phrase attachment separately in our policy since we believe that different algorithm should be used to disambiguate them.
</nextsent>
<nextsent>in the subsequent processing, we assume that adverbial phrase attachment would be disam biguated by choosing one parse tree among the results at first, and adnominal phrase attachment would be disambiguated by choosing one interpretation among all of interpretations which the parse tree represents (figure 2).
</nextsent>
<nextsent>we used the edr corpus (edr, 1994) for developing our annotation policy, and annotated8,911 sentences in the corpus and 20,190 sentences in the rwc corpus (hashida et al, 1998).
</nextsent>
<nextsent>in the following evaluation, we used the latter one.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L506">
<title id=" I05-4002.xml">evaluation of a japanese cfg derived from a syntactically annotated corpus with respect to dependency measures </title>
<section> extract dependency relations among bun-.  </section>
<citcontext>
<prevsection>
<prevsent>dependency accuracy could achieve about 95.24% for best?, which exceeds the dependency accuracy by knp and cabocha, if choosing the best result among top-100 parse results ranked by pglr model would be done correctly in the subsequent processing 4.
</prevsent>
<prevsent>from the results, we can conclude the accuracy will increase as soon as lexical and semantic information is incorporated in the subsequent processing 5 .however, segmentation accuracy is still significantly lower.
</prevsent>
</prevsection>
<citsent citstr=" W04-3224 ">
the main reasons are as follows:pos conversion error: as mentioned previously, we converted pos tags automatically since the pos system of the kyoto corpus is 4even if only top-10 parse results are considered, our cfg have possibility to outperform knp and cabocha 5in some studies, it is said that lexical information has little impact on accuracy (bikel, 2004).<papid> W04-3224 </papid></citsent>
<aftsection>
<nextsent>however, we think some lexical information is useful for disambiguation, and it is necessary to consider what kind of lexical information could improve the accuracy.
</nextsent>
<nextsent>14different from that of the rwc corpus.
</nextsent>
<nextsent>how ever, accuracy of the conversion is not high(about 80%).
</nextsent>
<nextsent>since we used only pos information and did not use any word information for parsing, the result can be easily affected by the conversion error.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L507">
<title id=" I05-5008.xml">automatic generation of paraphrases to be used as translation references in objective evaluation measures of machine translation </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>paraphrases are an important element in the evaluation of many natural language processing tasks.specifically, in the automatic evaluation of machine translation systems, the quality of translation candidates is judged against reference translations that are paraphrases in the target language.
</prevsent>
<prevsent>automatic measures like bleu (papineni et al, 2001) or nist (doddington, 2002) do so by counting sequences of words in such paraphrases.
</prevsent>
</prevsection>
<citsent citstr=" P04-1079 ">
it is expected that such reference sets contain synonymous sentences (i.e., paraphrases) that explicit possible lexical and syntactical variations in order to cope with translation variations in terms and structures (babych and hartley, 2004).<papid> P04-1079 </papid>in order to produce such reference sets, we propose method to generate paraphrases from aseed sentence where lexical and syntactical variations are handled by the use of com mutations as captured by proportional analogies whereas sequences are used to enforce fluency of expression and adequacy of meaning.</citsent>
<aftsection>
<nextsent>the linguistic resource used in the experiment presented in this paper relies on the c-star collection of utterances called basic travelers expressions1.
</nextsent>
<nextsent>this is multilingual resource of expressions from the travel and tourism domain that contains 162,318 aligned translations in several languages, among which english.
</nextsent>
<nextsent>the items are quite short as the following examples show (one line is one item in the corpus), and as the figures in table 1 show.
</nextsent>
<nextsent>1http://www.c-star.org/.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L508">
<title id=" I05-5008.xml">automatic generation of paraphrases to be used as translation references in objective evaluation measures of machine translation </title>
<section> our paraphrasing methodology.  </section>
<citcontext>
<prevsection>
<prevsent>:: id like slice of pizza, please.
</prevsent>
<prevsent>: ? = can have slice of pizzait is then legitimate to say that the produced sen tence: can have slice of pizza?
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
is paraphrase of the seed sentence (see table 4).such method alleviates the problem of creating templates from examples which would beused in an ulterior phase of generation (barzi lay and lee, 2003).<papid> N03-1003 </papid></citsent>
<aftsection>
<nextsent>here, all examples in the corpus are potential templates in their actual raw form, with the advantage that the choice of the places where com mutations may occur is left to proportional analogy.
</nextsent>
<nextsent>4.4 limitation of combinatorics by.
</nextsent>
<nextsent>contiguity constraints during paraphrase generation, spurious sentences may be produced.
</nextsent>
<nextsent>for instance, the replacement in the previous analogy, of the sentence: beer, please.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L509">
<title id=" I05-5008.xml">automatic generation of paraphrases to be used as translation references in objective evaluation measures of machine translation </title>
<section> our paraphrasing methodology.  </section>
<citcontext>
<prevsection>
<prevsent>by the following paraphrase detected during the first phase: bottle of beer, please.
</prevsent>
<prevsent>produces the unfortunate sentence: bottle of slice of pizza, please.
</prevsent>
</prevsection>
<citsent citstr=" P98-1120 ">
moreover, as no complete and valid formalisation of linguistic analogies has yet been proposed, the algorithm used (lepage, 1998) <papid> P98-1120 </papid>may deliver such unacceptable strings as: 59 43 could we have table in the corner?.</citsent>
<aftsection>
<nextsent>43 id like table in the corner.
</nextsent>
<nextsent>43 we would like table in the corner..
</nextsent>
<nextsent>28 can we have table in the corner?.
</nextsent>
<nextsent>4 wed like to sit in the corner..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L510">
<title id=" I05-5008.xml">automatic generation of paraphrases to be used as translation references in objective evaluation measures of machine translation </title>
<section> in the corner, please..  </section>
<citcontext>
<prevsection>
<prevsent>how ever, the distribution is not uniform: 60 sentences get more than 100 paraphrases.
</prevsent>
<prevsent>the maximum is reached with 529 paraphrases for the sentence sure.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
such sentence has variety of meanings depending on the context, which explains the high number of its possible paraphrases as illustrated below.3this is conform to the trend of using -sequences to assess the quality of outputs of various nlp systems like (lin and hovy, 2003) <papid> N03-1020 </papid>for summary generation, (doddington, 2002) for machine translation, etc..</citsent>
<aftsection>
<nextsent>sure.
</nextsent>
<nextsent>here you are.
</nextsent>
<nextsent>sure.
</nextsent>
<nextsent>this way, please.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L511">
<title id=" I08-1048.xml">learning a stopping criterion for active learning for word sense disambiguation and text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, creating large labeled training corpus is very expensive and time-consuming in some real-world cases such as word sense disambiguation (wsd).
</prevsent>
<prevsent>active learning is promising way to minimize the amount of human labeling effort by building an system that automatically selects the most informative unlabeled example for human annotation at each annotation cycle.
</prevsent>
</prevsection>
<citsent citstr=" P00-1016 ">
in recent years active learning has attracted lot of research interest, and has been studied in many natural language processing (nlp) tasks, such as text classification (tc) (lewis and gale, 1994; mccallum and nigam, 1998), chunking (ngai and yarowsky, 2000), <papid> P00-1016 </papid>named entity recognition (ner) (shen et al, 2004; <papid> P04-1075 </papid>tomanek et al, 2007), <papid> D07-1051 </papid>part-of-speech tagging (engelson and dagan, 1999), information extraction (thompson et al., 1999), statistical parsing (steedman et al, 2003), <papid> N03-1031 </papid>and word sense disambiguation (zhu and hovy, 2007).</citsent>
<aftsection>
<nextsent>previous studies reported that active learning can help in reducing human labeling effort.
</nextsent>
<nextsent>with selective sampling techniques such as uncertainty sampling (lewis and gale, 1994) and committee based sampling (mccallum and nigam, 1998), the size of the training data can be significantly reduced for text classification (lewis and gale, 1994; mccallum and nigam, 1998), word sense disambiguation (chen, et al 2006; <papid> N06-1016 </papid>zhu and hovy, 2007), and named entity recognition (shen et al, 2004; <papid> P04-1075 </papid>tomanek et al, 2007) <papid> D07-1051 </papid>tasks.</nextsent>
<nextsent>interestingly, deciding when to stop active learning is an issue seldom mentioned issue in these studies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L512">
<title id=" I08-1048.xml">learning a stopping criterion for active learning for word sense disambiguation and text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, creating large labeled training corpus is very expensive and time-consuming in some real-world cases such as word sense disambiguation (wsd).
</prevsent>
<prevsent>active learning is promising way to minimize the amount of human labeling effort by building an system that automatically selects the most informative unlabeled example for human annotation at each annotation cycle.
</prevsent>
</prevsection>
<citsent citstr=" P04-1075 ">
in recent years active learning has attracted lot of research interest, and has been studied in many natural language processing (nlp) tasks, such as text classification (tc) (lewis and gale, 1994; mccallum and nigam, 1998), chunking (ngai and yarowsky, 2000), <papid> P00-1016 </papid>named entity recognition (ner) (shen et al, 2004; <papid> P04-1075 </papid>tomanek et al, 2007), <papid> D07-1051 </papid>part-of-speech tagging (engelson and dagan, 1999), information extraction (thompson et al., 1999), statistical parsing (steedman et al, 2003), <papid> N03-1031 </papid>and word sense disambiguation (zhu and hovy, 2007).</citsent>
<aftsection>
<nextsent>previous studies reported that active learning can help in reducing human labeling effort.
</nextsent>
<nextsent>with selective sampling techniques such as uncertainty sampling (lewis and gale, 1994) and committee based sampling (mccallum and nigam, 1998), the size of the training data can be significantly reduced for text classification (lewis and gale, 1994; mccallum and nigam, 1998), word sense disambiguation (chen, et al 2006; <papid> N06-1016 </papid>zhu and hovy, 2007), and named entity recognition (shen et al, 2004; <papid> P04-1075 </papid>tomanek et al, 2007) <papid> D07-1051 </papid>tasks.</nextsent>
<nextsent>interestingly, deciding when to stop active learning is an issue seldom mentioned issue in these studies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L515">
<title id=" I08-1048.xml">learning a stopping criterion for active learning for word sense disambiguation and text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, creating large labeled training corpus is very expensive and time-consuming in some real-world cases such as word sense disambiguation (wsd).
</prevsent>
<prevsent>active learning is promising way to minimize the amount of human labeling effort by building an system that automatically selects the most informative unlabeled example for human annotation at each annotation cycle.
</prevsent>
</prevsection>
<citsent citstr=" D07-1051 ">
in recent years active learning has attracted lot of research interest, and has been studied in many natural language processing (nlp) tasks, such as text classification (tc) (lewis and gale, 1994; mccallum and nigam, 1998), chunking (ngai and yarowsky, 2000), <papid> P00-1016 </papid>named entity recognition (ner) (shen et al, 2004; <papid> P04-1075 </papid>tomanek et al, 2007), <papid> D07-1051 </papid>part-of-speech tagging (engelson and dagan, 1999), information extraction (thompson et al., 1999), statistical parsing (steedman et al, 2003), <papid> N03-1031 </papid>and word sense disambiguation (zhu and hovy, 2007).</citsent>
<aftsection>
<nextsent>previous studies reported that active learning can help in reducing human labeling effort.
</nextsent>
<nextsent>with selective sampling techniques such as uncertainty sampling (lewis and gale, 1994) and committee based sampling (mccallum and nigam, 1998), the size of the training data can be significantly reduced for text classification (lewis and gale, 1994; mccallum and nigam, 1998), word sense disambiguation (chen, et al 2006; <papid> N06-1016 </papid>zhu and hovy, 2007), and named entity recognition (shen et al, 2004; <papid> P04-1075 </papid>tomanek et al, 2007) <papid> D07-1051 </papid>tasks.</nextsent>
<nextsent>interestingly, deciding when to stop active learning is an issue seldom mentioned issue in these studies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L517">
<title id=" I08-1048.xml">learning a stopping criterion for active learning for word sense disambiguation and text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, creating large labeled training corpus is very expensive and time-consuming in some real-world cases such as word sense disambiguation (wsd).
</prevsent>
<prevsent>active learning is promising way to minimize the amount of human labeling effort by building an system that automatically selects the most informative unlabeled example for human annotation at each annotation cycle.
</prevsent>
</prevsection>
<citsent citstr=" N03-1031 ">
in recent years active learning has attracted lot of research interest, and has been studied in many natural language processing (nlp) tasks, such as text classification (tc) (lewis and gale, 1994; mccallum and nigam, 1998), chunking (ngai and yarowsky, 2000), <papid> P00-1016 </papid>named entity recognition (ner) (shen et al, 2004; <papid> P04-1075 </papid>tomanek et al, 2007), <papid> D07-1051 </papid>part-of-speech tagging (engelson and dagan, 1999), information extraction (thompson et al., 1999), statistical parsing (steedman et al, 2003), <papid> N03-1031 </papid>and word sense disambiguation (zhu and hovy, 2007).</citsent>
<aftsection>
<nextsent>previous studies reported that active learning can help in reducing human labeling effort.
</nextsent>
<nextsent>with selective sampling techniques such as uncertainty sampling (lewis and gale, 1994) and committee based sampling (mccallum and nigam, 1998), the size of the training data can be significantly reduced for text classification (lewis and gale, 1994; mccallum and nigam, 1998), word sense disambiguation (chen, et al 2006; <papid> N06-1016 </papid>zhu and hovy, 2007), and named entity recognition (shen et al, 2004; <papid> P04-1075 </papid>tomanek et al, 2007) <papid> D07-1051 </papid>tasks.</nextsent>
<nextsent>interestingly, deciding when to stop active learning is an issue seldom mentioned issue in these studies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L518">
<title id=" I08-1048.xml">learning a stopping criterion for active learning for word sense disambiguation and text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years active learning has attracted lot of research interest, and has been studied in many natural language processing (nlp) tasks, such as text classification (tc) (lewis and gale, 1994; mccallum and nigam, 1998), chunking (ngai and yarowsky, 2000), <papid> P00-1016 </papid>named entity recognition (ner) (shen et al, 2004; <papid> P04-1075 </papid>tomanek et al, 2007), <papid> D07-1051 </papid>part-of-speech tagging (engelson and dagan, 1999), information extraction (thompson et al., 1999), statistical parsing (steedman et al, 2003), <papid> N03-1031 </papid>and word sense disambiguation (zhu and hovy, 2007).</prevsent>
<prevsent>previous studies reported that active learning can help in reducing human labeling effort.</prevsent>
</prevsection>
<citsent citstr=" N06-1016 ">
with selective sampling techniques such as uncertainty sampling (lewis and gale, 1994) and committee based sampling (mccallum and nigam, 1998), the size of the training data can be significantly reduced for text classification (lewis and gale, 1994; mccallum and nigam, 1998), word sense disambiguation (chen, et al 2006; <papid> N06-1016 </papid>zhu and hovy, 2007), and named entity recognition (shen et al, 2004; <papid> P04-1075 </papid>tomanek et al, 2007) <papid> D07-1051 </papid>tasks.</citsent>
<aftsection>
<nextsent>interestingly, deciding when to stop active learning is an issue seldom mentioned issue in these studies.
</nextsent>
<nextsent>however, it is an important practical topic, since it obviously makes no sense to continue the active learning procedure until the whole corpus has been labeled.
</nextsent>
<nextsent>how to define an adequate stopping criterion remains an unsolved problem inactive learning.
</nextsent>
<nextsent>in principle, this is problem of estimation of classifier effectiveness (lewis and gale, 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L529">
<title id=" I08-1048.xml">learning a stopping criterion for active learning for word sense disambiguation and text classification </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 word sense disambiguation.
</prevsent>
<prevsent>the first comparison experiment is active learning for word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
we utilize maximum entropy (me) model (berger et al, 1996) <papid> J96-1002 </papid>to design the basic classifier used inactive learning for wsd.</citsent>
<aftsection>
<nextsent>the advantage of the me model is the ability to freely incorporate features from 369diverse sources into single, well-grounded statistical model.
</nextsent>
<nextsent>a publicly available me toolkit (zhang et. al., 2004) was used in our experiments.
</nextsent>
<nextsent>in order to extract the linguistic features necessary for the me model in wsd tasks, all sentences containing the target word are automatically part-of speech (pos) tagged using the brill pos tagger (brill, 1992).<papid> A92-1021 </papid></nextsent>
<nextsent>three knowledge sources are used to capture contextual information: unordered single words in topical context, pos of neighboring words with position information, and local colloca tions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L530">
<title id=" I08-1048.xml">learning a stopping criterion for active learning for word sense disambiguation and text classification </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the advantage of the me model is the ability to freely incorporate features from 369diverse sources into single, well-grounded statistical model.
</prevsent>
<prevsent>a publicly available me toolkit (zhang et. al., 2004) was used in our experiments.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
in order to extract the linguistic features necessary for the me model in wsd tasks, all sentences containing the target word are automatically part-of speech (pos) tagged using the brill pos tagger (brill, 1992).<papid> A92-1021 </papid></citsent>
<aftsection>
<nextsent>three knowledge sources are used to capture contextual information: unordered single words in topical context, pos of neighboring words with position information, and local collocations.
</nextsent>
<nextsent>these are same as the knowledge sources used in (lee and ng, 2002) <papid> W02-1006 </papid>for supervised automated wsd tasks.</nextsent>
<nextsent>the data used for comparison experiments was developed as part of the ontonotes project (hovy et al, 2006), which uses the wsj part of the penn treebank (marcus et al, 1993).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L531">
<title id=" I08-1048.xml">learning a stopping criterion for active learning for word sense disambiguation and text classification </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in order to extract the linguistic features necessary for the me model in wsd tasks, all sentences containing the target word are automatically part-of speech (pos) tagged using the brill pos tagger (brill, 1992).<papid> A92-1021 </papid></prevsent>
<prevsent>three knowledge sources are used to capture contextual information: unordered single words in topical context, pos of neighboring words with position information, and local colloca tions.</prevsent>
</prevsection>
<citsent citstr=" W02-1006 ">
these are same as the knowledge sources used in (lee and ng, 2002) <papid> W02-1006 </papid>for supervised automated wsd tasks.</citsent>
<aftsection>
<nextsent>the data used for comparison experiments was developed as part of the ontonotes project (hovy et al, 2006), which uses the wsj part of the penn treebank (marcus et al, 1993).
</nextsent>
<nextsent>the senses of noun words occurring in ontonotes are linked to the omega ontology (philpot et al, 2005).
</nextsent>
<nextsent>in ontonotes, at least two human annotators manually annotate the coarse-grained senses of selected nouns and verbs in their natural sentence context.
</nextsent>
<nextsent>in this experiment, we used several tens of thousands of annotated ontonotes examples, covering in total 421 nouns with an inter-annotator agreement rate of at least 90%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L532">
<title id=" I05-3003.xml">chinese deterministic dependency analyzer examining effects of global features and root node finder </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>we present method for improving dependency structure analysis of chinese.
</prevsent>
</prevsection>
<citsent citstr=" C04-1010 ">
our bottom-up deterministic analyzer adopt nivres algorithm (nivre and scholz, 2004).<papid> C04-1010 </papid></citsent>
<aftsection>
<nextsent>support vector machines (svms) are utilized to determine the word dependency relations.
</nextsent>
<nextsent>we find that there are two problems in our analyzer and propose two methods to solve them.
</nextsent>
<nextsent>one problem is that some operations cannot be solved only using local feature.
</nextsent>
<nextsent>we utilize the global features to solve this.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L534">
<title id=" I05-3003.xml">chinese deterministic dependency analyzer examining effects of global features and root node finder </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the experience in processing western languages cannot be guaranteed that it can apply to chinese language directly (lee, 1991).
</prevsent>
<prevsent>chinese language has many special syntactic phenomena substantially different from western languages.
</prevsent>
</prevsection>
<citsent citstr=" W00-1212 ">
discussions about such characteristics of chinese language can be found in the literature (chao 1968; li and thompson 1981; huang 1982).about the previous work of chinese dependency structure analysis, zhou proposed rule based approach (zhou, 2000).<papid> W00-1212 </papid></citsent>
<aftsection>
<nextsent>lai et al proposed span-based statistical probability approach (lai, 2001).
</nextsent>
<nextsent>ma et al proposed statistic dependency parser by using probabilistic model (ma, 2004).
</nextsent>
<nextsent>using machine learning-based approaches for dependency analysis of chinese is still limited.
</nextsent>
<nextsent>in this paper, we propose deterministic chinese syntactic structure analyzer by using global features and root node finder.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L537">
<title id=" I05-3003.xml">chinese deterministic dependency analyzer examining effects of global features and root node finder </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>17the second problem is that the top-down information isnt used in the bottom-up approach.
</prevsent>
<prevsent>we use the global features to solve the first problem and we construct svm-based root node finder in our system to supplement the top down information.
</prevsent>
</prevsection>
<citsent citstr=" C02-1145 ">
our analyzer is trained on the penn chinese treebank 5.0 (xue et al, 2002), <papid> C02-1145 </papid>which is phrase structure annotated corpus.</citsent>
<aftsection>
<nextsent>the phrase structure is converted into dependency structure according to the head rules.
</nextsent>
<nextsent>we perform experimental evaluation in several settings on this corpus.
</nextsent>
<nextsent>in the next section, we describe our deterministic dependency structure analysis algorithm.
</nextsent>
<nextsent>section 3 shows the global features and the two step process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L542">
<title id=" I05-3003.xml">chinese deterministic dependency analyzer examining effects of global features and root node finder </title>
<section> the root node finder.  </section>
<citcontext>
<prevsection>
<prevsent>the analyzer then selected the global features to analyze again and the output is the operation re duce?.
</prevsent>
<prevsent>the final result of this situation is the operation reduce?.
</prevsent>
</prevsection>
<citsent citstr=" C04-1040 ">
in isozakis work (isozaki et. al, 2004), <papid> C04-1040 </papid>they adopted root finder in their system to find the root word of the input sentence.</citsent>
<aftsection>
<nextsent>their method used the information of the root word as new feature for machine learning.
</nextsent>
<nextsent>their experiments showed that information of root word was beneficial feature.
</nextsent>
<nextsent>however, we think the information of root word can be used not only as the feature of machine learning, but also can be used to divide the sentence.
</nextsent>
<nextsent>therefore, the complexity of the sentence can be alleviated by dividing the input sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L545">
<title id=" I05-3003.xml">chinese deterministic dependency analyzer examining effects of global features and root node finder </title>
<section> paper </section>
<citcontext>
<prevsection>
<prevsent>therefore, we should adopt semantic analysis in our analyzer.
</prevsent>
<prevsent>1.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
eugene charniak, 2001.<papid> P01-1017 </papid></citsent>
<aftsection>
<nextsent>immediate-head parsing.
</nextsent>
<nextsent>for language models.
</nextsent>
<nextsent>pages 124-131, naacl 2001.
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L546">
<title id=" I08-1024.xml">a comparative study for query translation using linear combination and confidence measure </title>
<section> using confidence measures.  </section>
<citcontext>
<prevsection>
<prevsent>translation model index: an index representing the resource of translation that produced the translation candidate.
</prevsent>
<prevsent>translation probabilities: the probability of translating source word with target word.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
these probabilities are estimated with ibm model 1 (brown et al, 1993) <papid> J93-2003 </papid>on parallel corpora.</citsent>
<aftsection>
<nextsent>for translations from bilingual dictionaries, as no probability is provided, we carry out the following process to assign probability to each translation pair (e, f) in bilingual dictionary: we trained statistical translation model on parallel corpus.
</nextsent>
<nextsent>then for each translation pair (e,f) of the bilingual dictionary, we looked up the resulting translation model and extracted the probability assigned by this translation model to the translation pair in question.
</nextsent>
<nextsent>finally, the probability is normalized by the laplace smoothing method: ? = + += i istm stm bd efp efp efp 1 1)|( 1)|( )|( (12) where is the number of translations proposed by the bilingual dictionary to the word e. translation ranking: this class of features includes two features: the rank of the translation provided by each resource and the probability difference between the translation and the highest probability translation.
</nextsent>
<nextsent>reverse translation information: this includes the probability of translation of target word to source word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L547">
<title id=" I05-6004.xml">integration of a lexical type database with a linguistically interpreted corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have constructed large scale and detailed database of lexical typesin japanese from treebank that includes detailed linguistic information.the database helps treebank annotators and grammar developers to share precise knowledge about the grammatical status of words that constitute the treebank, allowing for consistent largescale tree banking and grammar development.
</prevsent>
<prevsent>in this paper, we report on the motivation and methodology of the database construction.
</prevsent>
</prevsection>
<citsent citstr=" W04-1901 ">
treebanks constructed with detailed linguistic information play an important role in various aspects of natural language processing; for example, grammatical knowledge acquisition; world knowledge acquisition (bond et al, 2004<papid> W04-1901 </papid>b); and statistical language model induction.</citsent>
<aftsection>
<nextsent>such treebanks are typically semi-automatically constructed by linguistically rich computational grammar.a detailed grammar in turn is fundamental component for precise natural language processing.
</nextsent>
<nextsent>it provides not only detailed syntactic and morphological information on linguistic expressions but also precise and usually language independent semantic structures of them.
</nextsent>
<nextsent>however, such deep linguistic treebank and grammar are often difficult to keep consistent through development cycles.
</nextsent>
<nextsent>this is both because multiple people, often in different locations, participate in development activity, and because deep linguistic treebanks and grammars are complicated by nature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L559">
<title id=" I05-6004.xml">integration of a lexical type database with a linguistically interpreted corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, it is often the case that developers lose sight of the current state of the treebank and grammar, resulting in inconsistency.
</prevsent>
<prevsent>we have constructed linguistically enriched treebank named hinoki?
</prevsent>
</prevsection>
<citsent citstr=" W02-1210 ">
(bond et al, 2004<papid> W04-1901 </papid>a), which is based on the same framework as the redwoods treebank (oepen et al, 2002) and usesthe japanese grammar jacy (siegel and bender, 2002) <papid> W02-1210 </papid>to construct the treebank.1 in the construction process, we have also encountered the problem just mentioned.</citsent>
<aftsection>
<nextsent>we are aiming to resolve this problem, which we expect many other project groups that are constructing detailed linguistic treebanks have encountered.
</nextsent>
<nextsent>our strategy is to take snapshot?
</nextsent>
<nextsent>of one important aspect of the treebank and grammar for each developmentcycle.
</nextsent>
<nextsent>to be more precise, we extract information about lexical items that are being used in tree banking from the treebank and grammar and convert it into an electronically accesible structured database (the lexical-type database).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L561">
<title id=" I05-6004.xml">integration of a lexical type database with a linguistically interpreted corpus </title>
<section> architecture of the database.  </section>
<citcontext>
<prevsection>
<prevsent>34 links to other dictionaries this information helps us to compare our grammars treatment with that of other dictionaries.
</prevsent>
<prevsent>this comparison would then facilitate understanding of lexical types and extension of the lexicon.
</prevsent>
</prevsection>
<citsent citstr=" W04-2209 ">
we currently link lexical types of our grammar to those of chasen (matsumoto et al, 2000),juman (kurohashi and nagao, 2003), alt j/e (ikehara et al, 1991) and edict (breen, 2004).<papid> W04-2209 </papid></citsent>
<aftsection>
<nextsent>for example, ga-wo-ni-case-p-lex is linked to chasens ??-???-??
</nextsent>
<nextsent>(particle-case particle-general), jumans ???
</nextsent>
<nextsent>(case particle), and alt-j/es ???-???-???????(adjunct-case particle-noun/par ticle suffix).
</nextsent>
<nextsent>figure 2 shows the document generated from the lexical type database that describes the lexical type, ga-wo-ni-p-lex.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L568">
<title id=" I05-6004.xml">integration of a lexical type database with a linguistically interpreted corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>that describes each expressions linguistic behavior, usage and examples indepth.
</prevsent>
<prevsent>notable differences between their database and ours are that their database is mostly constructed manually while ours is constructed semi-automatically, and that they target only functional expressions while we deal with all kinds of lexical types.
</prevsent>
</prevsection>
<citsent citstr=" W97-1504 ">
hyper textual grammar development (dini and mazzini, 1997) <papid> W97-1504 </papid>attempted similar task, but focused on documenting the grammar, not on linking it to dynamic treebank.</citsent>
<aftsection>
<nextsent>they suggested creating the documentation in the same file along with the grammar, in the style of literate programming.
</nextsent>
<nextsent>this is an attractive approach, especially for grammars that change constantly.
</nextsent>
<nextsent>however, we prefer the flexibility of combining different knowledge sources (the grammar, treebank and linguistic description, in addition to external re sources).
</nextsent>
<nextsent>the montage project (bender et al, 2004) aimsto develop suite of software whose primary audience is field linguists working on under documented languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L569">
<title id=" I05-6004.xml">integration of a lexical type database with a linguistically interpreted corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, their process of grammatical description and documentation looks quite similar to ours.
</prevsent>
<prevsent>the difference is that their target is under documented languages whose grammatical knowledge has so far not been made clear enough, while we target familiar language, japanese, that is well understood but whose computational implementation is so large and complex as to be difficult to fully comprehend.
</prevsent>
</prevsection>
<citsent citstr=" H94-1003 ">
another notable related work is the comlex syntax project (macleod et al, 1994).<papid> H94-1003 </papid></citsent>
<aftsection>
<nextsent>their goal is to create moderately-broad-coverage lexicon recording the syntactic features of english words for purposes of computational language analysis.
</nextsent>
<nextsent>they employed elves (elf?
</nextsent>
<nextsent>= enterer of lexicalfeatures) to create such lexicon by hand.
</nextsent>
<nextsent>naturally, the manual input task is error-prone.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L571">
<title id=" I08-2107.xml">automatic classification of english verbs using rich syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the best classifier (based on maximum entropy) yields the promising accuracy of 60.1% in classifying 204 verbs to 17 levin (1993) classes.
</prevsent>
<prevsent>we discuss the impact of this result on the state of-art, and propose avenues for future work.
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
recent research shows that it is possible, using current natural language processing (nlp) and machine learning technology, to automatically induce lexical classes from corpus data with promising accuracy (merlo and stevenson, 2001; <papid> J01-3003 </papid>korhonen et al., 2003; <papid> P03-1009 </papid>schulte im walde, 2006; joanis et al,2007).</citsent>
<aftsection>
<nextsent>this research is interesting, since lexical classifications, when tailored to the application and domain in question, can provide an effective means to deal with number of important nlp tasks (e.g. parsing, word sense disambiguation, semantic role labeling), as well as enhance performance in many applications (e.g. information extraction, question-answering, machine translation)(dorr, 1997; prescher et al, 2000; <papid> C00-2094 </papid>swier and stevenson, 2004; <papid> W04-3213 </papid>dang, 2004; shi and mihalcea, 2005).</nextsent>
<nextsent>lexical classes are useful because they capture generalizations over range of (cross-)linguisticproperties.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L574">
<title id=" I08-2107.xml">automatic classification of english verbs using rich syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the best classifier (based on maximum entropy) yields the promising accuracy of 60.1% in classifying 204 verbs to 17 levin (1993) classes.
</prevsent>
<prevsent>we discuss the impact of this result on the state of-art, and propose avenues for future work.
</prevsent>
</prevsection>
<citsent citstr=" P03-1009 ">
recent research shows that it is possible, using current natural language processing (nlp) and machine learning technology, to automatically induce lexical classes from corpus data with promising accuracy (merlo and stevenson, 2001; <papid> J01-3003 </papid>korhonen et al., 2003; <papid> P03-1009 </papid>schulte im walde, 2006; joanis et al,2007).</citsent>
<aftsection>
<nextsent>this research is interesting, since lexical classifications, when tailored to the application and domain in question, can provide an effective means to deal with number of important nlp tasks (e.g. parsing, word sense disambiguation, semantic role labeling), as well as enhance performance in many applications (e.g. information extraction, question-answering, machine translation)(dorr, 1997; prescher et al, 2000; <papid> C00-2094 </papid>swier and stevenson, 2004; <papid> W04-3213 </papid>dang, 2004; shi and mihalcea, 2005).</nextsent>
<nextsent>lexical classes are useful because they capture generalizations over range of (cross-)linguisticproperties.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L575">
<title id=" I08-2107.xml">automatic classification of english verbs using rich syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we discuss the impact of this result on the state of-art, and propose avenues for future work.
</prevsent>
<prevsent>recent research shows that it is possible, using current natural language processing (nlp) and machine learning technology, to automatically induce lexical classes from corpus data with promising accuracy (merlo and stevenson, 2001; <papid> J01-3003 </papid>korhonen et al., 2003; <papid> P03-1009 </papid>schulte im walde, 2006; joanis et al,2007).</prevsent>
</prevsection>
<citsent citstr=" C00-2094 ">
this research is interesting, since lexical classifications, when tailored to the application and domain in question, can provide an effective means to deal with number of important nlp tasks (e.g. parsing, word sense disambiguation, semantic role labeling), as well as enhance performance in many applications (e.g. information extraction, question-answering, machine translation)(dorr, 1997; prescher et al, 2000; <papid> C00-2094 </papid>swier and stevenson, 2004; <papid> W04-3213 </papid>dang, 2004; shi and mihalcea, 2005).</citsent>
<aftsection>
<nextsent>lexical classes are useful because they capture generalizations over range of (cross-)linguisticproperties.
</nextsent>
<nextsent>being defined in terms of similar meaning components and (morpho-)syntactic behaviour of words (jackendoff, 1990; levin, 1993) they generally incorporate wider range of properties than e.g. classes defined solely on semantic grounds (miller, 1990).
</nextsent>
<nextsent>they can be used to build lexical organization which effectively captures generalizations and predicts much of the syntax and semantics of new word by associating it with an appropriate class.
</nextsent>
<nextsent>this can help compensate for lack of data for individual words in nlp.large-scale exploitation of lexical classes in real world or domain-sensitive tasks has not been possible because existing manually built classifications are incomprehensive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L576">
<title id=" I08-2107.xml">automatic classification of english verbs using rich syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we discuss the impact of this result on the state of-art, and propose avenues for future work.
</prevsent>
<prevsent>recent research shows that it is possible, using current natural language processing (nlp) and machine learning technology, to automatically induce lexical classes from corpus data with promising accuracy (merlo and stevenson, 2001; <papid> J01-3003 </papid>korhonen et al., 2003; <papid> P03-1009 </papid>schulte im walde, 2006; joanis et al,2007).</prevsent>
</prevsection>
<citsent citstr=" W04-3213 ">
this research is interesting, since lexical classifications, when tailored to the application and domain in question, can provide an effective means to deal with number of important nlp tasks (e.g. parsing, word sense disambiguation, semantic role labeling), as well as enhance performance in many applications (e.g. information extraction, question-answering, machine translation)(dorr, 1997; prescher et al, 2000; <papid> C00-2094 </papid>swier and stevenson, 2004; <papid> W04-3213 </papid>dang, 2004; shi and mihalcea, 2005).</citsent>
<aftsection>
<nextsent>lexical classes are useful because they capture generalizations over range of (cross-)linguisticproperties.
</nextsent>
<nextsent>being defined in terms of similar meaning components and (morpho-)syntactic behaviour of words (jackendoff, 1990; levin, 1993) they generally incorporate wider range of properties than e.g. classes defined solely on semantic grounds (miller, 1990).
</nextsent>
<nextsent>they can be used to build lexical organization which effectively captures generalizations and predicts much of the syntax and semantics of new word by associating it with an appropriate class.
</nextsent>
<nextsent>this can help compensate for lack of data for individual words in nlp.large-scale exploitation of lexical classes in real world or domain-sensitive tasks has not been possible because existing manually built classifications are incomprehensive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L585">
<title id=" I08-2107.xml">automatic classification of english verbs using rich syntactic features </title>
<section> instrument subject alternation:.  </section>
<citcontext>
<prevsection>
<prevsent>we employed as features distributions of scfs specific to given verbs.
</prevsent>
<prevsent>we extracted them from there cent valex (korhonen et al, 2006) lexicon which provides scf frequency information for 6,397 english verbs.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
valex was acquired automatically from five large corpora and the web (using up to10,000 occurrences per verb) using the subcategorization acquisition system of briscoe and carroll(1997).<papid> A97-1052 </papid></citsent>
<aftsection>
<nextsent>the system incorporates rasp, domain independent robust statistical parser (briscoe and 770carroll, 2002), and scf classifier which identifies 163 verbal scfs.
</nextsent>
<nextsent>the basic scfs abstract over lexically-governed particles and prepositions and predicate selectional preferences.
</nextsent>
<nextsent>we used the noisy unfiltered version of valex which includes 33 scfs per verb on average1.
</nextsent>
<nextsent>some are genuine scfs but some express adjuncts (e.g. sang in the party could be scf pp).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L586">
<title id=" I05-4003.xml">corpus oriented acquisition of chinese grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>grammar development has to depend on linguistic knowledge and the characteristics of the corpus to explicate system of linguistic entities.
</prevsent>
<prevsent>however, it is expensive and time-consuming to maintain robust grammar system by manual writing.
</prevsent>
</prevsection>
<citsent citstr=" C04-1001 ">
recently some researchers (h. meng et al, 2002; s. dipper, 2004 <papid> C04-1001 </papid>and y. ding, 2004) have presented methodology to semi-automatically capture different grammar induct ions from annotated corpora within restricted domains.</citsent>
<aftsection>
<nextsent>a corpus-oriented approach (y. miyao, 2004) provides way to extract grammars automatically from an annotated corpus.
</nextsent>
<nextsent>the specific language knowledge and knowledge relations need to be constructed and oriented to different corpora and tasks (k. chen, 2004).
</nextsent>
<nextsent>the chinese treebank is useful resource for acquiring grammar rules and the context relations.
</nextsent>
<nextsent>currently there are several chinese treebanks on scale of size.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L587">
<title id=" I08-1030.xml">chinese unknown word translation by subword re segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also found that the translation quality was further improved if we applied named entity translation to translate parts of unknown words before using the subword-based translation.
</prevsent>
<prevsent>the use of phrase-based translation has led to great progress in statistical machine translation (smt).basically, the mechanism of this approach is realized by two steps:training and decoding.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
in the training phase, bilingual parallel sentences are preprocessed and aligned using alignment algorithms or tools such as giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>phrase pairs are then extracted to be phrase translation table.
</nextsent>
<nextsent>probabilities of few pre-defined features are computed and assigned to the phrase pairs.
</nextsent>
<nextsent>the final outcome of the training is translation table consisting of source phrases, target phrases, and lists of probabilities of features.
</nextsent>
<nextsent>in the decoding phase, the translation of test source sentence is made by reordering the target phrases corresponding to the source phrases, and searching for the best hypothesis that yields the highest scores defined by the search criterion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L588">
<title id=" I08-1030.xml">chinese unknown word translation by subword re segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the translation of unks need to be solved by special method.
</prevsent>
<prevsent>the translation of chinese unknown words seems more difficult than other languages because chinese language is non-inflected language.
</prevsent>
</prevsection>
<citsent citstr=" E06-1006 ">
unlike other languages (yang and kirchhoff, 2006; <papid> E06-1006 </papid>nielen andney, 2000; goldwater and mcclosky, 2005), <papid> H05-1085 </papid>chinese unk translation cannot use information from stem and inflection analysis.</citsent>
<aftsection>
<nextsent>using machine transliteration can resolve part of unk translation (knight and graehl, 1997).<papid> P97-1017 </papid></nextsent>
<nextsent>but this approach is effective for translating phonetic ally related unknown words, not for other types.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L589">
<title id=" I08-1030.xml">chinese unknown word translation by subword re segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the translation of unks need to be solved by special method.
</prevsent>
<prevsent>the translation of chinese unknown words seems more difficult than other languages because chinese language is non-inflected language.
</prevsent>
</prevsection>
<citsent citstr=" H05-1085 ">
unlike other languages (yang and kirchhoff, 2006; <papid> E06-1006 </papid>nielen andney, 2000; goldwater and mcclosky, 2005), <papid> H05-1085 </papid>chinese unk translation cannot use information from stem and inflection analysis.</citsent>
<aftsection>
<nextsent>using machine transliteration can resolve part of unk translation (knight and graehl, 1997).<papid> P97-1017 </papid></nextsent>
<nextsent>but this approach is effective for translating phonetic ally related unknown words, not for other types.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L590">
<title id=" I08-1030.xml">chinese unknown word translation by subword re segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the translation of chinese unknown words seems more difficult than other languages because chinese language is non-inflected language.
</prevsent>
<prevsent>unlike other languages (yang and kirchhoff, 2006; <papid> E06-1006 </papid>nielen andney, 2000; goldwater and mcclosky, 2005), <papid> H05-1085 </papid>chinese unk translation cannot use information from stem and inflection analysis.</prevsent>
</prevsection>
<citsent citstr=" P97-1017 ">
using machine transliteration can resolve part of unk translation (knight and graehl, 1997).<papid> P97-1017 </papid></citsent>
<aftsection>
<nextsent>but this approach is effective for translating phonetic ally related unknown words, not for other types.
</nextsent>
<nextsent>no unified approach for translating chinese unknown words has been proposed.
</nextsent>
<nextsent>in this paper we propose novel statistics-basedapproach for unknown word translation.
</nextsent>
<nextsent>this approach uses the properties of chinese word composition rules ? chinese words are composed of oneor more chinese characters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L591">
<title id=" I08-1030.xml">chinese unknown word translation by subword re segmentation </title>
<section> phrase-based statistical machine.  </section>
<citcontext>
<prevsection>
<prevsent>section 8 describes existing methods for unk translations for other languages than chinese.
</prevsent>
<prevsent>section 9 briefly summarizes the main points of this work.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
translation phrase-based smt uses framework of log-linear models (och, 2003) <papid> P03-1021 </papid>to integrate multiple features.</citsent>
<aftsection>
<nextsent>for chinese to english translation, source sentencec is translated into target sentence using probability model: p?(e|c) = exp(mi=1 fi(c, e))?
</nextsent>
<nextsent>e? exp( i=1 fi(c, e?)) ? = {m1 , } (1) where fi(c, e) is the logarithmic value of the i-th feature, and is the weight of the i-th feature.
</nextsent>
<nextsent>the candidate target sentence that maximizes p(e|c) is the solution.obviously, the performance of such model depends on the qualities of its features.
</nextsent>
<nextsent>we used the following features in this work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L592">
<title id=" I08-1030.xml">chinese unknown word translation by subword re segmentation </title>
<section> smt experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the mt04 and mt05 test data were also used as development data for cross experiments.
</prevsent>
<prevsent>we used chinese word segmentation tool, achilles, for doing word segmentation.
</prevsent>
</prevsection>
<citsent citstr=" I05-3027 ">
its word segmentation accuracy was higher than the stanford word segmenter (tseng et al, 2005) <papid> I05-3027 </papid>in our laboratory test (zhang et al, 2006).</citsent>
<aftsection>
<nextsent>the average length of sentence for the test data mt04 and mt05 after word segmentation is 37.5 by using the subword cws, and 27.9 by using the hyper word cws.
</nextsent>
<nextsent>table 6 shows statistics of unknown words inmt04 and mt05 using different word segmentation.
</nextsent>
<nextsent>obviously, character-based and subword-based cws generated much fewer unknown words, but sentences are over-segmented.
</nextsent>
<nextsent>the cws of hy perword generated many unks because of using large size of lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L593">
<title id=" I08-1030.xml">chinese unknown word translation by subword re segmentation </title>
<section> smt experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we tested the performance of each of the translation models on the test data.
</prevsent>
<prevsent>the results are shown on table 4.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the translations are evaluated in terms of bleu score (papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>this experiment was just testing the effect of the three cws.
</nextsent>
<nextsent>therefore, all the unks of the test data were not translated, simply removed from the results.
</nextsent>
<nextsent>we found the character-based cws yielded the lowest bleu scores, indicating the translation quality of this type is the worst.
</nextsent>
<nextsent>the hyper word cws achieved the best results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L597">
<title id=" I08-1030.xml">chinese unknown word translation by subword re segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>morphological analysis is effective for inflective languages but not for chinese.
</prevsent>
<prevsent>using unknown word modeling such as backoff models was proposed by (yang and kirchhoff, 2006).<papid> E06-1006 </papid>other proposed methods include paraphrasing (callison-burch et al, 2006) and transliteration (knight and graehl, 1997) <papid> P97-1017 </papid>that uses the feature of phonetic similarity.</prevsent>
</prevsection>
<citsent citstr=" E03-1076 ">
however, this approach does not work if no phonetic relationship is found.splitting compound words into translatable sub words as we did in this work have been used by (nielen and ney, 2000) and (koehn and knight,2003) <papid> E03-1076 </papid>for languages other than chinese where detailed splitting methods are proposed.</citsent>
<aftsection>
<nextsent>we used forward maximum match method to split unknown words.
</nextsent>
<nextsent>this splitting method is relatively simple but works well for chinese.
</nextsent>
<nextsent>the splitting for chinese is not as complicated as those languages with alphabet.
</nextsent>
<nextsent>we made use of the specific property of chinese language and proposed subword re-segmentation tosolve the translation of unknown words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L598">
<title id=" I05-3018.xml">combination of machine learning methods for optimum chinese word segmentation </title>
<section> overview </section>
<citcontext>
<prevsection>
<prevsent>the word classes are used as the hidden states in memm and crf-based word segmenters.
</prevsent>
<prevsent>the character classes are used as the features in character-based tagging, character-based chunking and word segmentation.
</prevsent>
</prevsection>
<citsent citstr=" W04-1109 ">
model is our previous method proposed in (gohet al, 2004<papid> W04-1109 </papid>b): first, maxent classifier is used to perform character-based tagging to identify oov words in the test data.</citsent>
<aftsection>
<nextsent>in-vocabulary (iv) word list together with the extracted oov word candidates is used in maximum matching algorithm.
</nextsent>
<nextsent>overlapping ambiguity is denoted by the different outputs from forward and backward maximum matching algorithm.
</nextsent>
<nextsent>finally, character-based tagging by maxent classifier resolves the ambiguity.section 2 describes models and c. section 3 describes model b. section 4 discusses the differences among the three models.
</nextsent>
<nextsent>models and use several modules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L600">
<title id=" I05-3018.xml">combination of machine learning methods for optimum chinese word segmentation </title>
<section> models and c.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 three oov extraction modules.
</prevsent>
<prevsent>in models and c, we use three oov extraction modules.
</prevsent>
</prevsection>
<citsent citstr=" W01-0512 ">
first and second oov extraction modules use the output of maximam entropy markov model (memm)-based word segmenter (mccallum et al, 2000) (uchimoto et al, 2001).<papid> W01-0512 </papid></citsent>
<aftsection>
<nextsent>word list is composed by the words appeared in 80% of the training data.
</nextsent>
<nextsent>the words occured only in the remaining 20% of the training data are regarded as oov words.
</nextsent>
<nextsent>all word candidates in sentence are extracted to form trellis.
</nextsent>
<nextsent>each word is assigned with word class.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L601">
<title id=" I05-3018.xml">combination of machine learning methods for optimum chinese word segmentation </title>
<section> models and c.  </section>
<citcontext>
<prevsection>
<prevsent>the output of the memm-based word segmenter is split ted character by character.
</prevsent>
<prevsent>next, character-based chunking is performed to extract oov words.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
we use two chunkers: based on svm (kudo and matsumoto, 2001) <papid> N01-1025 </papid>and crf (lafferty et al, 2001).</citsent>
<aftsection>
<nextsent>the chunker annotates bio position tags: b? stands for the first character of an oov word?; i? stands for other characters in an oov word?; o? stands for character outside an oov word?.the features used in the two chunk ers are the characters, the character classes and the information of other characters in five-character window size.
</nextsent>
<nextsent>theword sequence output by the memm-based word seg menter is converted into character sequence with bies position tags and the word classes.
</nextsent>
<nextsent>the position tags with the word classes are also introduced as the features.
</nextsent>
<nextsent>the third one is variation of the oov module in section 3 which is character-based tagging by maxent classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L602">
<title id=" I05-3018.xml">combination of machine learning methods for optimum chinese word segmentation </title>
<section> model b.  </section>
<citcontext>
<prevsection>
<prevsent>we did not use the character classes as the features.
</prevsent>
<prevsent>each character is assigned with bies position tags.
</prevsent>
</prevsection>
<citsent citstr=" W02-1815 ">
word segmentation by character based tagging is firstly introduced by (xue and converse, 2002).<papid> W02-1815 </papid></citsent>
<aftsection>
<nextsent>in encoding, we extract characters within five-character window size for each character position in the training data as the features for the classifier.
</nextsent>
<nextsent>in decoding, the bies position tag is deterministically annotated character by character in the test data.
</nextsent>
<nextsent>the 135 words that appear only in the test data are treated as oov word candidates.
</nextsent>
<nextsent>we can obtain quite high unknown word recall with this model but the precision is bit low.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L611">
<title id=" I05-3018.xml">combination of machine learning methods for optimum chinese word segmentation </title>
<section> discussions and conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>their crf-based word segmenter can compute confidence in each segment.
</prevsent>
<prevsent>the high confident segments that are not in the iv word list are regarded as oov word candidates.
</prevsent>
</prevsection>
<citsent citstr=" C04-1067 ">
(nakagawa, 2004) <papid> C04-1067 </papid>proposed integration of word and oov word position tag in trellis.</citsent>
<aftsection>
<nextsent>these three oov extraction method are different from our methods ? character-based tagging.future work will include implementation of these different sorts of oov word extraction modules.length bias problem means the tendency that the locally normalized markov model family prefers longer words.
</nextsent>
<nextsent>since choosing the longer words reduces the number of words in sentence, the state-transitions are reduced.
</nextsent>
<nextsent>the less the state-transitions, the larger the likelihood of the whole sentence.
</nextsent>
<nextsent>actually, the length bias reflects the real distribution in the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L612">
<title id=" I05-5010.xml">automatic generation of largescale paraphrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we address the problem of generating paraphrases of large chunks of texts.
</prevsent>
<prevsent>we ground our discussion through aworked example of extending an existing nlg system to accept as input source text, and to generate range of fluent semantically-equivalent alternatives, varying not only at the lexical and syntactic levels, but also in document structure and layout.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
much work on paraphrase generation has focussed on lexical variation and syntactic transformation within individual sentences (barzilay and mckeown, 2001; <papid> P01-1008 </papid>carroll et al, 1999; dras, 1999; inui and nogami, 2001; <papid> W01-0814 </papid>kozlowski et al, 2003; <papid> W03-1601 </papid>langkilde and knight, 1998; <papid> P98-1116 </papid>takahashi et al, 2001; stede, 1999).</citsent>
<aftsection>
<nextsent>our interest in this paper lies instead with variations at the level of text structuring ? the way in which propositions are grouped into units like paragraphs, sections,and bullet ted lists, and linked rhetorically by discourse connectives such as since?, nevertheless?, and however?.
</nextsent>
<nextsent>elsewhere, we have described text-structuring method in which the options for organising propositions in text are laid out as set of constraints, so that acceptable solutions can be enumerated using constraint satisfaction and evaluated using cost metric (power et al, 2003).<papid> J03-2003 </papid></nextsent>
<nextsent>in this paper we show how this method, when harnessed to system for recognising rhetorical structure in an input text, can be employed in order to produce large-scale paraphrases fulfilling purposes like improving coherence and achieving desired style of layout.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L614">
<title id=" I05-5010.xml">automatic generation of largescale paraphrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we address the problem of generating paraphrases of large chunks of texts.
</prevsent>
<prevsent>we ground our discussion through aworked example of extending an existing nlg system to accept as input source text, and to generate range of fluent semantically-equivalent alternatives, varying not only at the lexical and syntactic levels, but also in document structure and layout.
</prevsent>
</prevsection>
<citsent citstr=" W01-0814 ">
much work on paraphrase generation has focussed on lexical variation and syntactic transformation within individual sentences (barzilay and mckeown, 2001; <papid> P01-1008 </papid>carroll et al, 1999; dras, 1999; inui and nogami, 2001; <papid> W01-0814 </papid>kozlowski et al, 2003; <papid> W03-1601 </papid>langkilde and knight, 1998; <papid> P98-1116 </papid>takahashi et al, 2001; stede, 1999).</citsent>
<aftsection>
<nextsent>our interest in this paper lies instead with variations at the level of text structuring ? the way in which propositions are grouped into units like paragraphs, sections,and bullet ted lists, and linked rhetorically by discourse connectives such as since?, nevertheless?, and however?.
</nextsent>
<nextsent>elsewhere, we have described text-structuring method in which the options for organising propositions in text are laid out as set of constraints, so that acceptable solutions can be enumerated using constraint satisfaction and evaluated using cost metric (power et al, 2003).<papid> J03-2003 </papid></nextsent>
<nextsent>in this paper we show how this method, when harnessed to system for recognising rhetorical structure in an input text, can be employed in order to produce large-scale paraphrases fulfilling purposes like improving coherence and achieving desired style of layout.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L616">
<title id=" I05-5010.xml">automatic generation of largescale paraphrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we address the problem of generating paraphrases of large chunks of texts.
</prevsent>
<prevsent>we ground our discussion through aworked example of extending an existing nlg system to accept as input source text, and to generate range of fluent semantically-equivalent alternatives, varying not only at the lexical and syntactic levels, but also in document structure and layout.
</prevsent>
</prevsection>
<citsent citstr=" W03-1601 ">
much work on paraphrase generation has focussed on lexical variation and syntactic transformation within individual sentences (barzilay and mckeown, 2001; <papid> P01-1008 </papid>carroll et al, 1999; dras, 1999; inui and nogami, 2001; <papid> W01-0814 </papid>kozlowski et al, 2003; <papid> W03-1601 </papid>langkilde and knight, 1998; <papid> P98-1116 </papid>takahashi et al, 2001; stede, 1999).</citsent>
<aftsection>
<nextsent>our interest in this paper lies instead with variations at the level of text structuring ? the way in which propositions are grouped into units like paragraphs, sections,and bullet ted lists, and linked rhetorically by discourse connectives such as since?, nevertheless?, and however?.
</nextsent>
<nextsent>elsewhere, we have described text-structuring method in which the options for organising propositions in text are laid out as set of constraints, so that acceptable solutions can be enumerated using constraint satisfaction and evaluated using cost metric (power et al, 2003).<papid> J03-2003 </papid></nextsent>
<nextsent>in this paper we show how this method, when harnessed to system for recognising rhetorical structure in an input text, can be employed in order to produce large-scale paraphrases fulfilling purposes like improving coherence and achieving desired style of layout.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L618">
<title id=" I05-5010.xml">automatic generation of largescale paraphrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we address the problem of generating paraphrases of large chunks of texts.
</prevsent>
<prevsent>we ground our discussion through aworked example of extending an existing nlg system to accept as input source text, and to generate range of fluent semantically-equivalent alternatives, varying not only at the lexical and syntactic levels, but also in document structure and layout.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
much work on paraphrase generation has focussed on lexical variation and syntactic transformation within individual sentences (barzilay and mckeown, 2001; <papid> P01-1008 </papid>carroll et al, 1999; dras, 1999; inui and nogami, 2001; <papid> W01-0814 </papid>kozlowski et al, 2003; <papid> W03-1601 </papid>langkilde and knight, 1998; <papid> P98-1116 </papid>takahashi et al, 2001; stede, 1999).</citsent>
<aftsection>
<nextsent>our interest in this paper lies instead with variations at the level of text structuring ? the way in which propositions are grouped into units like paragraphs, sections,and bullet ted lists, and linked rhetorically by discourse connectives such as since?, nevertheless?, and however?.
</nextsent>
<nextsent>elsewhere, we have described text-structuring method in which the options for organising propositions in text are laid out as set of constraints, so that acceptable solutions can be enumerated using constraint satisfaction and evaluated using cost metric (power et al, 2003).<papid> J03-2003 </papid></nextsent>
<nextsent>in this paper we show how this method, when harnessed to system for recognising rhetorical structure in an input text, can be employed in order to produce large-scale paraphrases fulfilling purposes like improving coherence and achieving desired style of layout.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L620">
<title id=" I05-5010.xml">automatic generation of largescale paraphrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much work on paraphrase generation has focussed on lexical variation and syntactic transformation within individual sentences (barzilay and mckeown, 2001; <papid> P01-1008 </papid>carroll et al, 1999; dras, 1999; inui and nogami, 2001; <papid> W01-0814 </papid>kozlowski et al, 2003; <papid> W03-1601 </papid>langkilde and knight, 1998; <papid> P98-1116 </papid>takahashi et al, 2001; stede, 1999).</prevsent>
<prevsent>our interest in this paper lies instead with variations at the level of text structuring ? the way in which propositions are grouped into units like paragraphs, sections,and bullet ted lists, and linked rhetorically by discourse connectives such as since?, nevertheless?, and however?.</prevsent>
</prevsection>
<citsent citstr=" J03-2003 ">
elsewhere, we have described text-structuring method in which the options for organising propositions in text are laid out as set of constraints, so that acceptable solutions can be enumerated using constraint satisfaction and evaluated using cost metric (power et al, 2003).<papid> J03-2003 </papid></citsent>
<aftsection>
<nextsent>in this paper we show how this method, when harnessed to system for recognising rhetorical structure in an input text, can be employed in order to produce large-scale paraphrases fulfilling purposes like improving coherence and achieving desired style of layout.
</nextsent>
<nextsent>the input to our text-structuring system (icono clast) is rhetorical structure tree (mann and thompson, 1983) in which the leaves are elementary propositions, specified either as semantic formulas or as canned text.
</nextsent>
<nextsent>the following is simple example, containing one nucleus-satellite relation(reason) and one multi nuclear relation (con junction1): reason nucleus: recommend(doctors, elixir) satellite: conjunction 1: quick-results(elixir) 2: few-side-effects(elixir)ignoring variations in the wording of propositions, iconoclast generates over 20 texts realising this input (or many more if larger repertoire of discourse connectives is allowed).
</nextsent>
<nextsent>they include the following two solutions, which lie at stylistic extremes, the first compressing the message into sentence (suitable if space is at pre mium), the second laying it out more expansively in list: 1this is the rst relation, list, which we have renamed here to avoid possible confusion with the layout style of vertical lists.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L622">
<title id=" I05-5010.xml">automatic generation of largescale paraphrases </title>
<section> text structure.  </section>
<citcontext>
<prevsection>
<prevsent>here are four examples of preferences supported in iconoclast: we would assume that the first two are general, the second two specific.
</prevsent>
<prevsent>avoid single-sentence paragraphs this would penalise solution in which our example was laid out in two paragraphs, one for satellite and one for nucleus.
</prevsent>
</prevsection>
<citsent citstr=" J04-4001 ">
avoid discontinuity of reference as kibbleand power (2004) <papid> J04-4001 </papid>have shown, centering criteria can be used to penalize solutions with relatively many topic shifts.</citsent>
<aftsection>
<nextsent>avoid passivization in contexts requiring an informal, popular style, there might be astronger tendency to favour active over pas sive.?
</nextsent>
<nextsent>avoid complex sentences for some contexts we might prefer to penalize solutions in which many propositions are presented within the same sentence (e.g., solution 1).
</nextsent>
<nextsent>all these preferences are implemented through cost metric.
</nextsent>
<nextsent>to calculate the cost of solution, 74 the program first recognizes all violations, then multiplies each by weight representing its importance before summing to obtain total score.during execution, the program can either enumerate all solutions, ranking them from low cost to high, or it can simply search for the best solution using branch-and-bound optimization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L624">
<title id=" I05-5010.xml">automatic generation of largescale paraphrases </title>
<section> controlling constraints and.  </section>
<citcontext>
<prevsection>
<prevsent>the constraint might state, for example, that the proposition recommend(doctors, elixir) should appear at the beginning of the text, thus eliminating solution 2.
</prevsent>
<prevsent>or it might state that the conjunction relation between the other propositions should be realised by bullet ted list, thus eliminating solution 1.
</prevsent>
</prevsection>
<citsent citstr=" P98-2173 ">
to support constraints of this kind one would need user interface in which the user can select part of the semantic input, perhaps by clicking on the corresponding part of the text, as in wysiwym interface (power and scott, 1998); <papid> P98-2173 </papid>dialogue box would then appear allowing range of constraints specifically directed to the selected frag ment.</citsent>
<aftsection>
<nextsent>such an interface would mimic the typical interaction between human writer and human critic ? e.g., the critic might highlight paragraph and advise the writer to reformat it as list.
</nextsent>
<nextsent>we have shown that by defining text-structuring as constraint satisfaction problem, our method allows considerable flexibility and precision in controlling the generation of paraphrases (power et al, 2003).<papid> J03-2003 </papid></nextsent>
<nextsent>the question now is whether the system can be extended so that it accepts text asinput, rather than formally encoded rhetorical semantic representation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L637">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word alignment is an important preprocessing task for statistical machine translation.
</prevsent>
<prevsent>there have been many statistical word alignment methods proposed since the ibm models have been introduced.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
most existing methods treat word tokens as basic alignment units (brown et al, 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>deng and byrne, 2005), <papid> H05-1022 </papid>however, many languages have no explicit word boundary markers, such as chinese and japanese.</citsent>
<aftsection>
<nextsent>in these languages, word segmentation (chen and liu, 1992; <papid> C92-1019 </papid>chen and bai, 1998; chen and ma, 2002; <papid> C02-1049 </papid>ma and chen, 2003; <papid> W03-1705 </papid>gao et al, 2005) <papid> J05-4005 </papid>is often carried out firstly to identify words before word alignment (wu and xia, 1994).</nextsent>
<nextsent>however, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (ma et al, 2007; <papid> P07-1039 </papid>wu, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L638">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word alignment is an important preprocessing task for statistical machine translation.
</prevsent>
<prevsent>there have been many statistical word alignment methods proposed since the ibm models have been introduced.
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
most existing methods treat word tokens as basic alignment units (brown et al, 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>deng and byrne, 2005), <papid> H05-1022 </papid>however, many languages have no explicit word boundary markers, such as chinese and japanese.</citsent>
<aftsection>
<nextsent>in these languages, word segmentation (chen and liu, 1992; <papid> C92-1019 </papid>chen and bai, 1998; chen and ma, 2002; <papid> C02-1049 </papid>ma and chen, 2003; <papid> W03-1705 </papid>gao et al, 2005) <papid> J05-4005 </papid>is often carried out firstly to identify words before word alignment (wu and xia, 1994).</nextsent>
<nextsent>however, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (ma et al, 2007; <papid> P07-1039 </papid>wu, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L639">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word alignment is an important preprocessing task for statistical machine translation.
</prevsent>
<prevsent>there have been many statistical word alignment methods proposed since the ibm models have been introduced.
</prevsent>
</prevsection>
<citsent citstr=" H05-1022 ">
most existing methods treat word tokens as basic alignment units (brown et al, 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>deng and byrne, 2005), <papid> H05-1022 </papid>however, many languages have no explicit word boundary markers, such as chinese and japanese.</citsent>
<aftsection>
<nextsent>in these languages, word segmentation (chen and liu, 1992; <papid> C92-1019 </papid>chen and bai, 1998; chen and ma, 2002; <papid> C02-1049 </papid>ma and chen, 2003; <papid> W03-1705 </papid>gao et al, 2005) <papid> J05-4005 </papid>is often carried out firstly to identify words before word alignment (wu and xia, 1994).</nextsent>
<nextsent>however, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (ma et al, 2007; <papid> P07-1039 </papid>wu, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L640">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there have been many statistical word alignment methods proposed since the ibm models have been introduced.
</prevsent>
<prevsent>most existing methods treat word tokens as basic alignment units (brown et al, 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>deng and byrne, 2005), <papid> H05-1022 </papid>however, many languages have no explicit word boundary markers, such as chinese and japanese.</prevsent>
</prevsection>
<citsent citstr=" C92-1019 ">
in these languages, word segmentation (chen and liu, 1992; <papid> C92-1019 </papid>chen and bai, 1998; chen and ma, 2002; <papid> C02-1049 </papid>ma and chen, 2003; <papid> W03-1705 </papid>gao et al, 2005) <papid> J05-4005 </papid>is often carried out firstly to identify words before word alignment (wu and xia, 1994).</citsent>
<aftsection>
<nextsent>however, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (ma et al, 2007; <papid> P07-1039 </papid>wu, 1997).</nextsent>
<nextsent>for instance, chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several english words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L641">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there have been many statistical word alignment methods proposed since the ibm models have been introduced.
</prevsent>
<prevsent>most existing methods treat word tokens as basic alignment units (brown et al, 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>deng and byrne, 2005), <papid> H05-1022 </papid>however, many languages have no explicit word boundary markers, such as chinese and japanese.</prevsent>
</prevsection>
<citsent citstr=" C02-1049 ">
in these languages, word segmentation (chen and liu, 1992; <papid> C92-1019 </papid>chen and bai, 1998; chen and ma, 2002; <papid> C02-1049 </papid>ma and chen, 2003; <papid> W03-1705 </papid>gao et al, 2005) <papid> J05-4005 </papid>is often carried out firstly to identify words before word alignment (wu and xia, 1994).</citsent>
<aftsection>
<nextsent>however, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (ma et al, 2007; <papid> P07-1039 </papid>wu, 1997).</nextsent>
<nextsent>for instance, chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several english words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L642">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there have been many statistical word alignment methods proposed since the ibm models have been introduced.
</prevsent>
<prevsent>most existing methods treat word tokens as basic alignment units (brown et al, 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>deng and byrne, 2005), <papid> H05-1022 </papid>however, many languages have no explicit word boundary markers, such as chinese and japanese.</prevsent>
</prevsection>
<citsent citstr=" W03-1705 ">
in these languages, word segmentation (chen and liu, 1992; <papid> C92-1019 </papid>chen and bai, 1998; chen and ma, 2002; <papid> C02-1049 </papid>ma and chen, 2003; <papid> W03-1705 </papid>gao et al, 2005) <papid> J05-4005 </papid>is often carried out firstly to identify words before word alignment (wu and xia, 1994).</citsent>
<aftsection>
<nextsent>however, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (ma et al, 2007; <papid> P07-1039 </papid>wu, 1997).</nextsent>
<nextsent>for instance, chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several english words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L643">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there have been many statistical word alignment methods proposed since the ibm models have been introduced.
</prevsent>
<prevsent>most existing methods treat word tokens as basic alignment units (brown et al, 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>deng and byrne, 2005), <papid> H05-1022 </papid>however, many languages have no explicit word boundary markers, such as chinese and japanese.</prevsent>
</prevsection>
<citsent citstr=" J05-4005 ">
in these languages, word segmentation (chen and liu, 1992; <papid> C92-1019 </papid>chen and bai, 1998; chen and ma, 2002; <papid> C02-1049 </papid>ma and chen, 2003; <papid> W03-1705 </papid>gao et al, 2005) <papid> J05-4005 </papid>is often carried out firstly to identify words before word alignment (wu and xia, 1994).</citsent>
<aftsection>
<nextsent>however, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (ma et al, 2007; <papid> P07-1039 </papid>wu, 1997).</nextsent>
<nextsent>for instance, chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several english words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L644">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most existing methods treat word tokens as basic alignment units (brown et al, 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>deng and byrne, 2005), <papid> H05-1022 </papid>however, many languages have no explicit word boundary markers, such as chinese and japanese.</prevsent>
<prevsent>in these languages, word segmentation (chen and liu, 1992; <papid> C92-1019 </papid>chen and bai, 1998; chen and ma, 2002; <papid> C02-1049 </papid>ma and chen, 2003; <papid> W03-1705 </papid>gao et al, 2005) <papid> J05-4005 </papid>is often carried out firstly to identify words before word alignment (wu and xia, 1994).</prevsent>
</prevsection>
<citsent citstr=" P07-1039 ">
however, the differences in lexicalization may degrade word alignment performance, for different languages may realize the same concept using different numbers of words (ma et al, 2007; <papid> P07-1039 </papid>wu, 1997).</citsent>
<aftsection>
<nextsent>for instance, chinese multi-syllabic words composed of more than one meaningful morpheme which may be translated to several english words.
</nextsent>
<nextsent>for example, the chinese word ???
</nextsent>
<nextsent>is composed of two meaning units, ??
</nextsent>
<nextsent>and ?, and is translated to department of education in english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L645">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and department/?
</prevsent>
<prevsent>token pairs.
</prevsent>
</prevsection>
<citsent citstr=" P04-1066 ">
secondly, the rarely occurring compound word may cause the garbage collectors effect (moore, 2004; <papid> P04-1066 </papid>liang et al., 2006), <papid> N06-1014 </papid>aligning rare word in source language to too many words in the target language, due to the frequency imbalance with the corresponding translation words in english (lee, 2004).<papid> N04-4015 </papid></citsent>
<aftsection>
<nextsent>finally, the ibm models (moore, 2004) <papid> P04-1066 </papid>impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence.</nextsent>
<nextsent>in this case, many-to-one alignment, links phrase in the source sentence to single token in the target sentence, is not allowed, forcing most links of phrase in the source sentence to be abolished.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L646">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and department/?
</prevsent>
<prevsent>token pairs.
</prevsent>
</prevsection>
<citsent citstr=" N06-1014 ">
secondly, the rarely occurring compound word may cause the garbage collectors effect (moore, 2004; <papid> P04-1066 </papid>liang et al., 2006), <papid> N06-1014 </papid>aligning rare word in source language to too many words in the target language, due to the frequency imbalance with the corresponding translation words in english (lee, 2004).<papid> N04-4015 </papid></citsent>
<aftsection>
<nextsent>finally, the ibm models (moore, 2004) <papid> P04-1066 </papid>impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence.</nextsent>
<nextsent>in this case, many-to-one alignment, links phrase in the source sentence to single token in the target sentence, is not allowed, forcing most links of phrase in the source sentence to be abolished.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L647">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and department/?
</prevsent>
<prevsent>token pairs.
</prevsent>
</prevsection>
<citsent citstr=" N04-4015 ">
secondly, the rarely occurring compound word may cause the garbage collectors effect (moore, 2004; <papid> P04-1066 </papid>liang et al., 2006), <papid> N06-1014 </papid>aligning rare word in source language to too many words in the target language, due to the frequency imbalance with the corresponding translation words in english (lee, 2004).<papid> N04-4015 </papid></citsent>
<aftsection>
<nextsent>finally, the ibm models (moore, 2004) <papid> P04-1066 </papid>impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence.</nextsent>
<nextsent>in this case, many-to-one alignment, links phrase in the source sentence to single token in the target sentence, is not allowed, forcing most links of phrase in the source sentence to be abolished.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L649">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as in the previous example, when aligning from english to chinese, ???
</prevsent>
<prevsent>can only be linked to one of the english words, say education, because of the limitation of the ibm model.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
however for remedy, many of the current word alignment methods combine the results of both alignment directions, via intersection or 249 grow-diag-final heuristic, to improve the alignment reliability (koehn et al, 2003; <papid> N03-1017 </papid>liang et al, 2006; <papid> N06-1014 </papid>ayan et al, 2006; denero et al, 2007).</citsent>
<aftsection>
<nextsent>however the many-to-one link limitation will undermine the reliability due to the fact that some links are not allowed in one of the directions.
</nextsent>
<nextsent>in this paper, we propose two novel methods to adjust word segmentation so as to decrease the effect of lexicalization differences to improve word alignment performance.
</nextsent>
<nextsent>the main idea of our methods is to adjust chinese word segmentation according to their translation derived from parallel sentences in order to make the tokens compatible to 1-to-1 mapping between the corresponding sentences.
</nextsent>
<nextsent>the first method is based on learning set of affix rules from bilingual terminology bank, and adjusting the segmentation according to these affix rules when preprocessing the chinese part of the parallel corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L652">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>our basic idea is similar to this, but on the contrary, we try to unpack words which are translations of several words in the other language.
</prevsent>
<prevsent>since the word packing method treats the packed consecutive words as single token, as we mentioned in the previous section, it weakens the association strength of translation pairs of their morphemes while applying the ibm word alignment model.
</prevsent>
</prevsection>
<citsent citstr=" P03-1051 ">
a lot of morphological analysis methods have been proposed to improve the performance of word alignment for inflectional language (lee et al, 2003; <papid> P03-1051 </papid>lee, 2004; <papid> N04-4015 </papid>goldwater, 2005).</citsent>
<aftsection>
<nextsent>they proposed to split word into morpheme sequence of the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of morpheme).
</nextsent>
<nextsent>their experiments showed that morphological analysis can improve the quality of machine translation by reducing data sparseness and by making the tokens in two languages correspond more 1-to-1.
</nextsent>
<nextsent>however, these segmentation methods were developed from the monolingual perspective.
</nextsent>
<nextsent>the goal of word segmentation adjustment is to adjust the segmentation of chinese words such that we have as many 1-to-1 links to the english words as possible.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L656">
<title id=" I08-1033.xml">improving word alignment by adjusting chinese word segmentation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>table 2 shows the results of our analysis, the performance of impurity measure method is also slightly better than the affix rules in both recall and precision measure.
</prevsent>
<prevsent>we used the first 100,000 sentences of hongkong news parallel corpus from ldc as our training data.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
and 112 randomly selected parallel sentences were aligned manually with sure and possible tags, as described in (och and ney, 2000), <papid> P00-1056 </papid>4 in our experiment, we set 0 as the threshold..</citsent>
<aftsection>
<nextsent>254 direction recall precision f-score aer english-chinese 68.3 61.2 64.6 35.7 chinese-english 79.6 67.0 72.8 27.8 baseline intersection 59.9 92.0 72.6 26.6 english-chinese 78.2 64.6 70.8 29.8 chinese-english 80.2 68.0 73.6 27.0 affix rules intersection 69.1 92.3 79.0 20.2 english-chinese 78.1 64.9 70.9 29.7 chinese-english 81.4 70.4 75.5 25.0 impurity intersection 70.2 91.9 79.6 19.8 table 1.
</nextsent>
<nextsent>alignment results based on the standard word segmentation data.
</nextsent>
<nextsent>recall precision affix rules 82.35 66.66 impurity 84.31 67.72 table 2.
</nextsent>
<nextsent>alignment results based on the manual word segmentation data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L657">
<title id=" I05-3019.xml">unigram language model for chinese word segmentation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the main pre processors include recognizers for extracting names of people, places and organizations, and recognizer for numeric expressions.
</prevsent>
<prevsent>the proper name recog nizers are built based on the maximum entropy model, and the numeric expression recognizer is built as finite state automaton.
</prevsent>
</prevsection>
<citsent citstr=" W03-1728 ">
the conditional maximum entropy model in our implementation is based on the one described in section 2.5 in (ratnaparkhi, 1998), and features are the same as those described in (xue and shen, 2003).<papid> W03-1728 </papid></citsent>
<aftsection>
<nextsent>one of the post-processing steps is to combine single characters in the initial segmentation if each character in sequence of characters occurs in word much more frequently than as word on its own.
</nextsent>
<nextsent>the other post-processing procedure checks the segmentation of text fragment in the input text against the segmentation in the training data.
</nextsent>
<nextsent>if the segmentation produced by our system is different from the one in the training data, we will use the segmentation in the training data as the final segmentation.
</nextsent>
<nextsent>more details on the segmentation algorithm and the pre processors and post processors can be found in (chen, 2003).<papid> W03-1721 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L658">
<title id=" I05-3019.xml">unigram language model for chinese word segmentation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the other post-processing procedure checks the segmentation of text fragment in the input text against the segmentation in the training data.
</prevsent>
<prevsent>if the segmentation produced by our system is different from the one in the training data, we will use the segmentation in the training data as the final segmentation.
</prevsent>
</prevsection>
<citsent citstr=" W03-1721 ">
more details on the segmentation algorithm and the pre processors and post processors can be found in (chen, 2003).<papid> W03-1721 </papid></citsent>
<aftsection>
<nextsent>our system processes sentence independently.
</nextsent>
<nextsent>for an input sentence, the pre processors are applied to the input sentence to extract numeric expressions and proper names.
</nextsent>
<nextsent>the extracted numeric expressions and proper names are added to the segmentation dictionary, if they are not already in the dictionary.
</nextsent>
<nextsent>then the input sentence is segmented into words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L659">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>this data has been published for the purpose of encouraging research in areas relating to paraphrase and sentential synonymy and inference, and to help establish discourse on the proper construction of paraphrase corpora for training and evaluation.
</prevsent>
<prevsent>it is hoped that by releasing this corpus, we will stimulate the publication of similar corpora by others and help move the field toward adoption of shared dataset that will permit useful comparisons of results across research efforts.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
the success of statistical machine translation (smt) has sparked successful line of investigation that treats paraphrase acquisition and generation essentially as monolingual machine translation problem (e.g., barzilay &amp; lee, 2003; <papid> N03-1003 </papid>pang et al, 2003; <papid> N03-1024 </papid>quirk et al, 2004; <papid> W04-3219 </papid>finch et al, 2004).</citsent>
<aftsection>
<nextsent>however, lack of standardly-accepted corpora on which to train and evaluate models is major stumbling block to the successful application of smt models or other machine learning algorithms to paraphrase tasks.
</nextsent>
<nextsent>since paraphrase is not apparently common natural?
</nextsent>
<nextsent>task under normal circumstances people do not attempt to create extended paraphrase texts the field lacks large readily identifiable dataset comparable to, for example, the canadian hansard corpus in smt that can serve as standard against which algorithms can be trained and evaluated.
</nextsent>
<nextsent>what paraphrase data is currently available is usually too small to be viable for either training or testing, or exhibits narrow topic coverage, limiting its broad-domain applicability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L660">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>this data has been published for the purpose of encouraging research in areas relating to paraphrase and sentential synonymy and inference, and to help establish discourse on the proper construction of paraphrase corpora for training and evaluation.
</prevsent>
<prevsent>it is hoped that by releasing this corpus, we will stimulate the publication of similar corpora by others and help move the field toward adoption of shared dataset that will permit useful comparisons of results across research efforts.
</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
the success of statistical machine translation (smt) has sparked successful line of investigation that treats paraphrase acquisition and generation essentially as monolingual machine translation problem (e.g., barzilay &amp; lee, 2003; <papid> N03-1003 </papid>pang et al, 2003; <papid> N03-1024 </papid>quirk et al, 2004; <papid> W04-3219 </papid>finch et al, 2004).</citsent>
<aftsection>
<nextsent>however, lack of standardly-accepted corpora on which to train and evaluate models is major stumbling block to the successful application of smt models or other machine learning algorithms to paraphrase tasks.
</nextsent>
<nextsent>since paraphrase is not apparently common natural?
</nextsent>
<nextsent>task under normal circumstances people do not attempt to create extended paraphrase texts the field lacks large readily identifiable dataset comparable to, for example, the canadian hansard corpus in smt that can serve as standard against which algorithms can be trained and evaluated.
</nextsent>
<nextsent>what paraphrase data is currently available is usually too small to be viable for either training or testing, or exhibits narrow topic coverage, limiting its broad-domain applicability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L661">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>this data has been published for the purpose of encouraging research in areas relating to paraphrase and sentential synonymy and inference, and to help establish discourse on the proper construction of paraphrase corpora for training and evaluation.
</prevsent>
<prevsent>it is hoped that by releasing this corpus, we will stimulate the publication of similar corpora by others and help move the field toward adoption of shared dataset that will permit useful comparisons of results across research efforts.
</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
the success of statistical machine translation (smt) has sparked successful line of investigation that treats paraphrase acquisition and generation essentially as monolingual machine translation problem (e.g., barzilay &amp; lee, 2003; <papid> N03-1003 </papid>pang et al, 2003; <papid> N03-1024 </papid>quirk et al, 2004; <papid> W04-3219 </papid>finch et al, 2004).</citsent>
<aftsection>
<nextsent>however, lack of standardly-accepted corpora on which to train and evaluate models is major stumbling block to the successful application of smt models or other machine learning algorithms to paraphrase tasks.
</nextsent>
<nextsent>since paraphrase is not apparently common natural?
</nextsent>
<nextsent>task under normal circumstances people do not attempt to create extended paraphrase texts the field lacks large readily identifiable dataset comparable to, for example, the canadian hansard corpus in smt that can serve as standard against which algorithms can be trained and evaluated.
</nextsent>
<nextsent>what paraphrase data is currently available is usually too small to be viable for either training or testing, or exhibits narrow topic coverage, limiting its broad-domain applicability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L662">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>what paraphrase data is currently available is usually too small to be viable for either training or testing, or exhibits narrow topic coverage, limiting its broad-domain applicability.
</prevsent>
<prevsent>one class of paraphrase data that is relatively widely available is multiple translations of sentences in second language.
</prevsent>
</prevsection>
<citsent citstr=" C02-1056 ">
these, however, tend to be rather restricted in their domain (e.g. the atr english-chinese paraphrase corpus, which con 9 sists of translations of travel phrases (zhang &amp; yamamoto, 2002)), <papid> C02-1056 </papid>are limited to short handcrafted predicates (e.g. the atr japanese english corpus (shirai, et al, 2002)), or exhibit quality problems stemming from insufficient command of the target language by the translators of the documents in question, e.g. the linguistic data consortiums multiple-translation chinese corpus (huang et al, 2002).</citsent>
<aftsection>
<nextsent>multiple translations of novels, such as those used in (barzilay &amp; mckeown, 2001) <papid> P01-1008 </papid>provide relatively limited dataset to work with, and ? since these usually involve works that are out of copy right ? usually exhibit older styles of language that have little in common with modern language resources or application requirements.</nextsent>
<nextsent>likewise, the data made available by (barzi lay &amp; lee, 2003: <papid> N03-1003 </papid>http://www.cs.cornell.edu/ info/projects/nlp/statpar.html), while invaluable in understanding and evaluating their results, is too limited in size and domain coverage to serve as either training or test data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L663">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>one class of paraphrase data that is relatively widely available is multiple translations of sentences in second language.
</prevsent>
<prevsent>these, however, tend to be rather restricted in their domain (e.g. the atr english-chinese paraphrase corpus, which con 9 sists of translations of travel phrases (zhang &amp; yamamoto, 2002)), <papid> C02-1056 </papid>are limited to short handcrafted predicates (e.g. the atr japanese english corpus (shirai, et al, 2002)), or exhibit quality problems stemming from insufficient command of the target language by the translators of the documents in question, e.g. the linguistic data consortiums multiple-translation chinese corpus (huang et al, 2002).</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
multiple translations of novels, such as those used in (barzilay &amp; mckeown, 2001) <papid> P01-1008 </papid>provide relatively limited dataset to work with, and ? since these usually involve works that are out of copy right ? usually exhibit older styles of language that have little in common with modern language resources or application requirements.</citsent>
<aftsection>
<nextsent>likewise, the data made available by (barzi lay &amp; lee, 2003: <papid> N03-1003 </papid>http://www.cs.cornell.edu/ info/projects/nlp/statpar.html), while invaluable in understanding and evaluating their results, is too limited in size and domain coverage to serve as either training or test data.</nextsent>
<nextsent>attempting to evaluate models of paraphrase acquisition and generation under limitations can thus be an exercise in frustration.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L666">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> source data.  </section>
<citcontext>
<prevsection>
<prevsent>since the search space for identifying any two sentence pairs occurring in the wild?
</prevsent>
<prevsent>is huge, and provides far too many negative examples for humans to wade through, clustered news articles were used to constrain the initial search space to data that was likely to yield paraphrase pairs.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
the microsoft research paraphrase corpus (msrp) is distilled from database of 13,127,938 sentence pairs, extracted from 9,516,684 sentences in 32,408 news clusters collected from the world wide web over 2 year period, the methods and assumptions used in building this initial dataset are discussed in quirk et al (2004) <papid> W04-3219 </papid>and dolan et al (2004).<papid> C04-1051 </papid></citsent>
<aftsection>
<nextsent>two heuristics based on shared lexical properties and sentence position in the document were employed to construct the initial database: word-based levenshtein edit distance of 1     20; and length ratio   66%; or both sentences in the first three sentences of each file; and length ratio   50%.
</nextsent>
<nextsent>within this initial dataset we were able to automatically identify the names of both authors and copyright holders of 61,618 articles.1 limiting ourselves only to sentences found in those articles, we further narrowed the range of candidate pairs using the following criteria: the number of words in both sentences in words is 5 ? ? 40; the two sentences shared at least three words in common; the length of the shorter of the two sentences, in words, is at least 66.6% that of the longer; and the two sentences had bag-of-words lexical distance of ? 8 edits.
</nextsent>
<nextsent>this enabled us extract set of 49,375 initial candidate sentence pairs whose author was known, the purpose of these heuristics was two-fold: 1) to narrow the search space for subsequent application of the classifier algorithm and human evaluation, and 2) to ensure at least some diversity among the sentences.
</nextsent>
<nextsent>in particular, we sought to exclude the large number of sentence pairs whose differences might be attributable only to typographical errors, variance between british and american spellings, and minor editorial variations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L667">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> constructing classifier.  </section>
<citcontext>
<prevsection>
<prevsent>in the classifier we restricted the feature set to small set of feature classes.
</prevsent>
<prevsent>the main classes are given below.
</prevsent>
</prevsection>
<citsent citstr=" I05-5001 ">
more details can be found in brockett and dolan (2005).<papid> I05-5001 </papid></citsent>
<aftsection>
<nextsent>string similarity features: absolute and relative length in words, number of shared words, word-based edit distance, and bag of-words-based lexical distance.
</nextsent>
<nextsent>morphological variants: morphological variant lexicon consisting of 95,422 word pairs was created using hand-crafted stemmer.
</nextsent>
<nextsent>each pair is then treated as feature in the classifier.
</nextsent>
<nextsent>wordnet lexical mappings: 314,924 word synonyms and hypernym pairs were extracted from wordnet, (fellbaum, 1998; http://www.cogsci.princeton.edu/~wn/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L669">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>further constrains the coverage in ways whose consequences are not yet known.
</prevsent>
<prevsent>in addition, the three-shared-words heuristic further guarantees that an entire class of paraphrases in which no words are shared in common have been excluded from the data.
</prevsent>
</prevsection>
<citsent citstr=" W05-1202 ">
it has been observed that the mean lexical overlap in the corpus is relatively high 0.7 (weeds et al 2005), <papid> W05-1202 </papid>suggesting that more lexically divergent examples will be needed.</citsent>
<aftsection>
<nextsent>in these respects, as wu (2005) <papid> W05-1205 </papid>points out, the corpus is far from distributionally neu tral.</nextsent>
<nextsent>this is matter that we hope to remedy in the future, since in many ways this excluded set of pairs is the most interesting of all.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L670">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, the three-shared-words heuristic further guarantees that an entire class of paraphrases in which no words are shared in common have been excluded from the data.
</prevsent>
<prevsent>it has been observed that the mean lexical overlap in the corpus is relatively high 0.7 (weeds et al 2005), <papid> W05-1202 </papid>suggesting that more lexically divergent examples will be needed.</prevsent>
</prevsection>
<citsent citstr=" W05-1205 ">
in these respects, as wu (2005) <papid> W05-1205 </papid>points out, the corpus is far from distributionally neu tral.</citsent>
<aftsection>
<nextsent>this is matter that we hope to remedy in the future, since in many ways this excluded set of pairs is the most interesting of all.
</nextsent>
<nextsent>the above limitations, together with its relatively small size, perhaps make the mrsp inappropriate for direct use as training corpus.
</nextsent>
<nextsent>we show separately that the results of training classifier on the present corpus may be inferior to other training sets, though better than crude string or text-based heuristics (brockett &amp; dolan, 2005).<papid> I05-5001 </papid></nextsent>
<nextsent>we expect that the utility of the corpus will stem primarily from its use as tool for evaluating paraphrase recognition algorithms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L672">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>we show separately that the results of training classifier on the present corpus may be inferior to other training sets, though better than crude string or text-based heuristics (brockett &amp; dolan, 2005).<papid> I05-5001 </papid></prevsent>
<prevsent>we expect that the utility of the corpus will stem primarily from its use as tool for evaluating paraphrase recognition algorithms.</prevsent>
</prevsection>
<citsent citstr=" W05-1203 ">
it has already been applied in this way by corley &amp; mihalcea (2005) <papid> W05-1203 </papid>and wu (2005).<papid> W05-1205 </papid></citsent>
<aftsection>
<nextsent>although larger than any other non-translationbased labeled paraphrase corpus currently publicly available, msrp is tiny compared with the huge bilingual parallel corpora publicly available within the machine translation community, for example, the canadian hansa rds, the hongkong parliamentary corpus, or the united nations documents.
</nextsent>
<nextsent>it is improbable that we will ever encounter naturally occurring?
</nextsent>
<nextsent>paraphrase corpus on the scale of any of these bilingual corpora.
</nextsent>
<nextsent>moreover, whatever extraction technique is employed to identify paraphrases in other kinds of data will be apt to reflect the implicit biases of the methodology employed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L674">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> future directions.  </section>
<citcontext>
<prevsection>
<prevsent>we anticipate using multiple classifiers to reduce inherent biases in candidate corpus selection, and with better author identification to ensure proper attribution, to be able to draw on larger dataset for consideration by our judges.
</prevsent>
<prevsent>in future releases we expect to make available more information about individual evaluator judgments.
</prevsent>
</prevsection>
<citsent citstr=" W05-1209 ">
burger &amp; ferro (2005) <papid> W05-1209 </papid>have suggested that this data may allow researchers greater freedom to construct models based on the judgments of specific judges or combinations of judges, permitting more fine-grained use of the corpus.</citsent>
<aftsection>
<nextsent>one further issue that we will also be attempting to address is the need to provide better metric for corpus coverage and quality.
</nextsent>
<nextsent>until reliable metrics can be established for end-to 14 end paraphrase tasks these will probably need to be application specific the alignment error rate strategy that was successfully applied in early development of machine translation systems (och &amp; ney, 2000, <papid> P00-1056 </papid>2003) offers useful intermediate representation of the coverage and precision of corpus and extraction techniques.</nextsent>
<nextsent>though full scale reliability studies have yet to be performed, the aer technique is already finding application in other fields such as summarization (daum?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L675">
<title id=" I05-5002.xml">automatically constructing a corpus of sentential paraphrases </title>
<section> future directions.  </section>
<citcontext>
<prevsection>
<prevsent>burger &amp; ferro (2005) <papid> W05-1209 </papid>have suggested that this data may allow researchers greater freedom to construct models based on the judgments of specific judges or combinations of judges, permitting more fine-grained use of the corpus.</prevsent>
<prevsent>one further issue that we will also be attempting to address is the need to provide better metric for corpus coverage and quality.</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
until reliable metrics can be established for end-to 14 end paraphrase tasks these will probably need to be application specific the alignment error rate strategy that was successfully applied in early development of machine translation systems (och &amp; ney, 2000, <papid> P00-1056 </papid>2003) offers useful intermediate representation of the coverage and precision of corpus and extraction techniques.</citsent>
<aftsection>
<nextsent>though full scale reliability studies have yet to be performed, the aer technique is already finding application in other fields such as summarization (daum?
</nextsent>
<nextsent>&amp; marcu, forthcoming).
</nextsent>
<nextsent>we expect to be able to provide reasonably large corpus of word-aligned paraphrase sentences in the near future that we hope will serve as some sort of standard by which corpus extraction techniques can be measured and compared in uniform fashion.
</nextsent>
<nextsent>one other path that we are concurrently exploring is collection and validation of paraphrase data by volunteers on the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L677">
<title id=" I05-1008.xml">using a partially annotated corpus to build a dependency parser for japanese </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>table 4.
</prevsent>
<prevsent>comparison to related work on parsing accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
km02 = kudo and matsumoto 2002 [<papid> W02-2016 </papid>9], km00 = kudo and matsumoto 2000 [<papid> W00-1303 </papid>12], usi99 = uchimoto et al 1999 [<papid> E99-1026 </papid>4], seki00 = sekine 2000 [<papid> C00-2110 </papid>18], and sass04 = sassano 2004 [<papid> C04-1002 </papid>7].</citsent>
<aftsection>
<nextsent>algorithm/model/features acc.(%) this paper stack dependency analysis (cubic svm) w/ char.
</nextsent>
<nextsent>n-grams 89.07 stack dependency analysis (cubic svm) w/ char.
</nextsent>
<nextsent>n-grams, no pos 87.38 sass04 stack dependency analysis (cubic svm) w/ various enriched features 89.56 km02 cascaded chunking (cubic svm) w/ dynamic features 89.29 km00 backward beam search (cubic svm) 89.09 usi99 backward beam search (maxent) 87.14 seki00 deterministic finite state transducer 77.97use of partially annotated corpora.
</nextsent>
<nextsent>several papers address the use of partially annotated corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L678">
<title id=" I05-1008.xml">using a partially annotated corpus to build a dependency parser for japanese </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>table 4.
</prevsent>
<prevsent>comparison to related work on parsing accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W00-1303 ">
km02 = kudo and matsumoto 2002 [<papid> W02-2016 </papid>9], km00 = kudo and matsumoto 2000 [<papid> W00-1303 </papid>12], usi99 = uchimoto et al 1999 [<papid> E99-1026 </papid>4], seki00 = sekine 2000 [<papid> C00-2110 </papid>18], and sass04 = sassano 2004 [<papid> C04-1002 </papid>7].</citsent>
<aftsection>
<nextsent>algorithm/model/features acc.(%) this paper stack dependency analysis (cubic svm) w/ char.
</nextsent>
<nextsent>n-grams 89.07 stack dependency analysis (cubic svm) w/ char.
</nextsent>
<nextsent>n-grams, no pos 87.38 sass04 stack dependency analysis (cubic svm) w/ various enriched features 89.56 km02 cascaded chunking (cubic svm) w/ dynamic features 89.29 km00 backward beam search (cubic svm) 89.09 usi99 backward beam search (maxent) 87.14 seki00 deterministic finite state transducer 77.97use of partially annotated corpora.
</nextsent>
<nextsent>several papers address the use of partially annotated corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L679">
<title id=" I05-1008.xml">using a partially annotated corpus to build a dependency parser for japanese </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>table 4.
</prevsent>
<prevsent>comparison to related work on parsing accuracy.
</prevsent>
</prevsection>
<citsent citstr=" E99-1026 ">
km02 = kudo and matsumoto 2002 [<papid> W02-2016 </papid>9], km00 = kudo and matsumoto 2000 [<papid> W00-1303 </papid>12], usi99 = uchimoto et al 1999 [<papid> E99-1026 </papid>4], seki00 = sekine 2000 [<papid> C00-2110 </papid>18], and sass04 = sassano 2004 [<papid> C04-1002 </papid>7].</citsent>
<aftsection>
<nextsent>algorithm/model/features acc.(%) this paper stack dependency analysis (cubic svm) w/ char.
</nextsent>
<nextsent>n-grams 89.07 stack dependency analysis (cubic svm) w/ char.
</nextsent>
<nextsent>n-grams, no pos 87.38 sass04 stack dependency analysis (cubic svm) w/ various enriched features 89.56 km02 cascaded chunking (cubic svm) w/ dynamic features 89.29 km00 backward beam search (cubic svm) 89.09 usi99 backward beam search (maxent) 87.14 seki00 deterministic finite state transducer 77.97use of partially annotated corpora.
</nextsent>
<nextsent>several papers address the use of partially annotated corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L680">
<title id=" I05-1008.xml">using a partially annotated corpus to build a dependency parser for japanese </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>table 4.
</prevsent>
<prevsent>comparison to related work on parsing accuracy.
</prevsent>
</prevsection>
<citsent citstr=" C00-2110 ">
km02 = kudo and matsumoto 2002 [<papid> W02-2016 </papid>9], km00 = kudo and matsumoto 2000 [<papid> W00-1303 </papid>12], usi99 = uchimoto et al 1999 [<papid> E99-1026 </papid>4], seki00 = sekine 2000 [<papid> C00-2110 </papid>18], and sass04 = sassano 2004 [<papid> C04-1002 </papid>7].</citsent>
<aftsection>
<nextsent>algorithm/model/features acc.(%) this paper stack dependency analysis (cubic svm) w/ char.
</nextsent>
<nextsent>n-grams 89.07 stack dependency analysis (cubic svm) w/ char.
</nextsent>
<nextsent>n-grams, no pos 87.38 sass04 stack dependency analysis (cubic svm) w/ various enriched features 89.56 km02 cascaded chunking (cubic svm) w/ dynamic features 89.29 km00 backward beam search (cubic svm) 89.09 usi99 backward beam search (maxent) 87.14 seki00 deterministic finite state transducer 77.97use of partially annotated corpora.
</nextsent>
<nextsent>several papers address the use of partially annotated corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L681">
<title id=" I05-1008.xml">using a partially annotated corpus to build a dependency parser for japanese </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>table 4.
</prevsent>
<prevsent>comparison to related work on parsing accuracy.
</prevsent>
</prevsection>
<citsent citstr=" C04-1002 ">
km02 = kudo and matsumoto 2002 [<papid> W02-2016 </papid>9], km00 = kudo and matsumoto 2000 [<papid> W00-1303 </papid>12], usi99 = uchimoto et al 1999 [<papid> E99-1026 </papid>4], seki00 = sekine 2000 [<papid> C00-2110 </papid>18], and sass04 = sassano 2004 [<papid> C04-1002 </papid>7].</citsent>
<aftsection>
<nextsent>algorithm/model/features acc.(%) this paper stack dependency analysis (cubic svm) w/ char.
</nextsent>
<nextsent>n-grams 89.07 stack dependency analysis (cubic svm) w/ char.
</nextsent>
<nextsent>n-grams, no pos 87.38 sass04 stack dependency analysis (cubic svm) w/ various enriched features 89.56 km02 cascaded chunking (cubic svm) w/ dynamic features 89.29 km00 backward beam search (cubic svm) 89.09 usi99 backward beam search (maxent) 87.14 seki00 deterministic finite state transducer 77.97use of partially annotated corpora.
</nextsent>
<nextsent>several papers address the use of partially annotated corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L682">
<title id=" I08-2140.xml">global health monitor  a web based system for detecting and mapping infectious diseases </title>
<section> the system.  </section>
<citcontext>
<prevsection>
<prevsent>952 we will now describe each of the four modules in turn: * topic classification.
</prevsent>
<prevsent>this module identifies news stories with disease-related topics and retains relevant ones for later processing.
</prevsent>
</prevsection>
<citsent citstr=" W07-1003 ">
the module uses ontology-supported text classification with nave bayes as the classification algorithm and the bio caster gold standard corpus as the training dataset (doan et al, 2007).<papid> W07-1003 </papid></citsent>
<aftsection>
<nextsent>in this module, we used the rainbow toolkit.2* ner.
</nextsent>
<nextsent>disease-related news stories are automatically analyzed and tagged with nes like person, organization, disease, location.
</nextsent>
<nextsent>this module is implemented by svm classification al 2 rainbow toolkit, available at.
</nextsent>
<nextsent>http://www.cs.umass.edu/~mccallum/bow/rainbow gorithm3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L687">
<title id=" I08-1031.xml">hypothesis selection in machine transliteration a web mining approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, our proposed method based on web mining consistently outperformed systems based on simple web counts used in previous work, regardless of the language.
</prevsent>
<prevsent>machine transliteration has been great challenge for cross-lingual information retrieval and machine translation systems.
</prevsent>
</prevsection>
<citsent citstr=" C00-1061 ">
many researchers have developed machine transliteration systems that accept source language term as input and then output its transliteration in target language (al-onaizan and knight, 2002; goto et al , 2003; grefenstette et al , 2004; kang and kim, 2000; <papid> C00-1061 </papid>li et al , 2004; <papid> P04-1021 </papid>meng et al ., 2001; oh and choi, 2002; <papid> C02-1099 </papid>oh et al , 2006; qu and grefenstette, 2004).<papid> P04-1024 </papid></citsent>
<aftsection>
<nextsent>some of these have used the web to select machine-generated transliteration hypotheses and have obtained promising results (al onaizan and knight, 2002; grefenstette et al , 2004; oh et al , 2006; qu and grefenstette, 2004).<papid> P04-1024 </papid></nextsent>
<nextsent>more precisely, they used simple web counts, estimated as the number of hits (web pages) retrieved by web search engine.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L688">
<title id=" I08-1031.xml">hypothesis selection in machine transliteration a web mining approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, our proposed method based on web mining consistently outperformed systems based on simple web counts used in previous work, regardless of the language.
</prevsent>
<prevsent>machine transliteration has been great challenge for cross-lingual information retrieval and machine translation systems.
</prevsent>
</prevsection>
<citsent citstr=" P04-1021 ">
many researchers have developed machine transliteration systems that accept source language term as input and then output its transliteration in target language (al-onaizan and knight, 2002; goto et al , 2003; grefenstette et al , 2004; kang and kim, 2000; <papid> C00-1061 </papid>li et al , 2004; <papid> P04-1021 </papid>meng et al ., 2001; oh and choi, 2002; <papid> C02-1099 </papid>oh et al , 2006; qu and grefenstette, 2004).<papid> P04-1024 </papid></citsent>
<aftsection>
<nextsent>some of these have used the web to select machine-generated transliteration hypotheses and have obtained promising results (al onaizan and knight, 2002; grefenstette et al , 2004; oh et al , 2006; qu and grefenstette, 2004).<papid> P04-1024 </papid></nextsent>
<nextsent>more precisely, they used simple web counts, estimated as the number of hits (web pages) retrieved by web search engine.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L689">
<title id=" I08-1031.xml">hypothesis selection in machine transliteration a web mining approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, our proposed method based on web mining consistently outperformed systems based on simple web counts used in previous work, regardless of the language.
</prevsent>
<prevsent>machine transliteration has been great challenge for cross-lingual information retrieval and machine translation systems.
</prevsent>
</prevsection>
<citsent citstr=" C02-1099 ">
many researchers have developed machine transliteration systems that accept source language term as input and then output its transliteration in target language (al-onaizan and knight, 2002; goto et al , 2003; grefenstette et al , 2004; kang and kim, 2000; <papid> C00-1061 </papid>li et al , 2004; <papid> P04-1021 </papid>meng et al ., 2001; oh and choi, 2002; <papid> C02-1099 </papid>oh et al , 2006; qu and grefenstette, 2004).<papid> P04-1024 </papid></citsent>
<aftsection>
<nextsent>some of these have used the web to select machine-generated transliteration hypotheses and have obtained promising results (al onaizan and knight, 2002; grefenstette et al , 2004; oh et al , 2006; qu and grefenstette, 2004).<papid> P04-1024 </papid></nextsent>
<nextsent>more precisely, they used simple web counts, estimated as the number of hits (web pages) retrieved by web search engine.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L690">
<title id=" I08-1031.xml">hypothesis selection in machine transliteration a web mining approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, our proposed method based on web mining consistently outperformed systems based on simple web counts used in previous work, regardless of the language.
</prevsent>
<prevsent>machine transliteration has been great challenge for cross-lingual information retrieval and machine translation systems.
</prevsent>
</prevsection>
<citsent citstr=" P04-1024 ">
many researchers have developed machine transliteration systems that accept source language term as input and then output its transliteration in target language (al-onaizan and knight, 2002; goto et al , 2003; grefenstette et al , 2004; kang and kim, 2000; <papid> C00-1061 </papid>li et al , 2004; <papid> P04-1021 </papid>meng et al ., 2001; oh and choi, 2002; <papid> C02-1099 </papid>oh et al , 2006; qu and grefenstette, 2004).<papid> P04-1024 </papid></citsent>
<aftsection>
<nextsent>some of these have used the web to select machine-generated transliteration hypotheses and have obtained promising results (al onaizan and knight, 2002; grefenstette et al , 2004; oh et al , 2006; qu and grefenstette, 2004).<papid> P04-1024 </papid></nextsent>
<nextsent>more precisely, they used simple web counts, estimated as the number of hits (web pages) retrieved by web search engine.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L710">
<title id=" I08-1044.xml">a framework based on graphical models with logic for chinese named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the ner problem has generated much interest and great progress has been made, as evidenced by its inclusion as an understanding task to be evaluated in the the work described in this paper is substantially supported by grants from the research grant council of the hongkong special administrative region, china (project nos: cuhk 4179/03eand cuhk4193/04e) and the direct grant of the faculty of engineering, cuhk (project codes: 2050363 and 2050391).
</prevsent>
<prevsent>this work is also affiliated with the microsoft-cuhk joint laboratory for human-centric computing and interface technologies.message understanding conference (muc), the multilingual entity task (met) evaluations, and the conference on computational natural language learning (conll).
</prevsent>
</prevsection>
<citsent citstr=" W06-0124 ">
compared to european-language ner, chinese ner seems to be more difficult (yu et al, 2006).<papid> W06-0124 </papid></citsent>
<aftsection>
<nextsent>recent approaches to chinese ner are shift away from manually constructed rules or finite state patterns towards machine learning or statistical methods.
</nextsent>
<nextsent>however, rule based ner systems lack robustness and portability.
</nextsent>
<nextsent>statistical methods often suffer from the problem of data sparsity, and machine learning approaches (e.g., hidden markov models (hmms) (bikel et al, 1999; zhou and su, 2002), support vector machines (svms) (isozaki and kazawa, 2002), <papid> C02-1054 </papid>maximum entropy (maxent) (borthwick,1999; chieu and ng, 2003), transformation-based learning (tbl) (brill, 1995) <papid> J95-4004 </papid>or variants of them) might be unsatisfactory to learn linguistic information in chinese nes.</nextsent>
<nextsent>current state-of-the-art models often view chinese ner asa classification or sequence labeling problem without considering the linguistic and structural information in chinese nes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L711">
<title id=" I08-1044.xml">a framework based on graphical models with logic for chinese named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent approaches to chinese ner are shift away from manually constructed rules or finite state patterns towards machine learning or statistical methods.
</prevsent>
<prevsent>however, rule based ner systems lack robustness and portability.
</prevsent>
</prevsection>
<citsent citstr=" C02-1054 ">
statistical methods often suffer from the problem of data sparsity, and machine learning approaches (e.g., hidden markov models (hmms) (bikel et al, 1999; zhou and su, 2002), support vector machines (svms) (isozaki and kazawa, 2002), <papid> C02-1054 </papid>maximum entropy (maxent) (borthwick,1999; chieu and ng, 2003), transformation-based learning (tbl) (brill, 1995) <papid> J95-4004 </papid>or variants of them) might be unsatisfactory to learn linguistic information in chinese nes.</citsent>
<aftsection>
<nextsent>current state-of-the-art models often view chinese ner asa classification or sequence labeling problem without considering the linguistic and structural information in chinese nes.
</nextsent>
<nextsent>they assume that entities are independent, however in most cases this assumption does not hold because of the existing relationships among the entities.
</nextsent>
<nextsent>they seek to locate and identify named entities in text by sequentially classifying tokens (words or characters) as to whether or not they participate in an ne, which is sometimes prone to noise and errors.in fact, chinese nes have distinct linguistic characteristics in their composition and human beings usually use prior knowledge to recognize nes.
</nextsent>
<nextsent>for example, about 365of the highest frequently used surnames cover 99% chinese surnames (sun et al, 1995).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L712">
<title id=" I08-1044.xml">a framework based on graphical models with logic for chinese named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent approaches to chinese ner are shift away from manually constructed rules or finite state patterns towards machine learning or statistical methods.
</prevsent>
<prevsent>however, rule based ner systems lack robustness and portability.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
statistical methods often suffer from the problem of data sparsity, and machine learning approaches (e.g., hidden markov models (hmms) (bikel et al, 1999; zhou and su, 2002), support vector machines (svms) (isozaki and kazawa, 2002), <papid> C02-1054 </papid>maximum entropy (maxent) (borthwick,1999; chieu and ng, 2003), transformation-based learning (tbl) (brill, 1995) <papid> J95-4004 </papid>or variants of them) might be unsatisfactory to learn linguistic information in chinese nes.</citsent>
<aftsection>
<nextsent>current state-of-the-art models often view chinese ner asa classification or sequence labeling problem without considering the linguistic and structural information in chinese nes.
</nextsent>
<nextsent>they assume that entities are independent, however in most cases this assumption does not hold because of the existing relationships among the entities.
</nextsent>
<nextsent>they seek to locate and identify named entities in text by sequentially classifying tokens (words or characters) as to whether or not they participate in an ne, which is sometimes prone to noise and errors.in fact, chinese nes have distinct linguistic characteristics in their composition and human beings usually use prior knowledge to recognize nes.
</nextsent>
<nextsent>for example, about 365of the highest frequently used surnames cover 99% chinese surnames (sun et al, 1995).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L713">
<title id=" I08-1044.xml">a framework based on graphical models with logic for chinese named entity recognition </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>conditional random fields (crfs) (lafferty et al, 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs.
</prevsent>
<prevsent>crfs have the great flexibility to encode wide variety of arbitrary, non-independent features and to straightforwardly combine rich domain knowledge.furthermore, they are discriminatively trained, and are of ten more accurate than generative models, even with the same features.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
crfs have been successfully applied to number of real-world tasks, including np chunking (sha and pereira, 2003), <papid> N03-1028 </papid>chinese word segmentation (peng et al., 2004), <papid> C04-1081 </papid>information extraction (pinto et al, 2003; pengand mccallum, 2004), <papid> N04-1042 </papid>named entity identification (mc callum and li, 2003; settles, 2004), <papid> W04-1221 </papid>and many others.</citsent>
<aftsection>
<nextsent>1in this paper we only focus on pers, locs and orgs.
</nextsent>
<nextsent>since temporal, numerical and monetary phrases can be well identified with rule-based approaches.recently, crfs have been shown to perform exceptionally well on chinese ner shared task on the thirdsighan chinese language processing bakeoff (sighan 06) (zhou et al, 2006; chen et al, 2006<papid> W06-0130 </papid>b,a).</nextsent>
<nextsent>we follow the state-of-the-art crf models using features that have been shown to be very effective in chinese ner, namely the current character and its part-of-speech (pos) tag, several characters surrounding (both before and after) the current character and their pos tags, current word and several words surrounding the current word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L714">
<title id=" I08-1044.xml">a framework based on graphical models with logic for chinese named entity recognition </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>conditional random fields (crfs) (lafferty et al, 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs.
</prevsent>
<prevsent>crfs have the great flexibility to encode wide variety of arbitrary, non-independent features and to straightforwardly combine rich domain knowledge.furthermore, they are discriminatively trained, and are of ten more accurate than generative models, even with the same features.
</prevsent>
</prevsection>
<citsent citstr=" C04-1081 ">
crfs have been successfully applied to number of real-world tasks, including np chunking (sha and pereira, 2003), <papid> N03-1028 </papid>chinese word segmentation (peng et al., 2004), <papid> C04-1081 </papid>information extraction (pinto et al, 2003; pengand mccallum, 2004), <papid> N04-1042 </papid>named entity identification (mc callum and li, 2003; settles, 2004), <papid> W04-1221 </papid>and many others.</citsent>
<aftsection>
<nextsent>1in this paper we only focus on pers, locs and orgs.
</nextsent>
<nextsent>since temporal, numerical and monetary phrases can be well identified with rule-based approaches.recently, crfs have been shown to perform exceptionally well on chinese ner shared task on the thirdsighan chinese language processing bakeoff (sighan 06) (zhou et al, 2006; chen et al, 2006<papid> W06-0130 </papid>b,a).</nextsent>
<nextsent>we follow the state-of-the-art crf models using features that have been shown to be very effective in chinese ner, namely the current character and its part-of-speech (pos) tag, several characters surrounding (both before and after) the current character and their pos tags, current word and several words surrounding the current word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L715">
<title id=" I08-1044.xml">a framework based on graphical models with logic for chinese named entity recognition </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>conditional random fields (crfs) (lafferty et al, 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs.
</prevsent>
<prevsent>crfs have the great flexibility to encode wide variety of arbitrary, non-independent features and to straightforwardly combine rich domain knowledge.furthermore, they are discriminatively trained, and are of ten more accurate than generative models, even with the same features.
</prevsent>
</prevsection>
<citsent citstr=" N04-1042 ">
crfs have been successfully applied to number of real-world tasks, including np chunking (sha and pereira, 2003), <papid> N03-1028 </papid>chinese word segmentation (peng et al., 2004), <papid> C04-1081 </papid>information extraction (pinto et al, 2003; pengand mccallum, 2004), <papid> N04-1042 </papid>named entity identification (mc callum and li, 2003; settles, 2004), <papid> W04-1221 </papid>and many others.</citsent>
<aftsection>
<nextsent>1in this paper we only focus on pers, locs and orgs.
</nextsent>
<nextsent>since temporal, numerical and monetary phrases can be well identified with rule-based approaches.recently, crfs have been shown to perform exceptionally well on chinese ner shared task on the thirdsighan chinese language processing bakeoff (sighan 06) (zhou et al, 2006; chen et al, 2006<papid> W06-0130 </papid>b,a).</nextsent>
<nextsent>we follow the state-of-the-art crf models using features that have been shown to be very effective in chinese ner, namely the current character and its part-of-speech (pos) tag, several characters surrounding (both before and after) the current character and their pos tags, current word and several words surrounding the current word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L716">
<title id=" I08-1044.xml">a framework based on graphical models with logic for chinese named entity recognition </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>conditional random fields (crfs) (lafferty et al, 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs.
</prevsent>
<prevsent>crfs have the great flexibility to encode wide variety of arbitrary, non-independent features and to straightforwardly combine rich domain knowledge.furthermore, they are discriminatively trained, and are of ten more accurate than generative models, even with the same features.
</prevsent>
</prevsection>
<citsent citstr=" W04-1221 ">
crfs have been successfully applied to number of real-world tasks, including np chunking (sha and pereira, 2003), <papid> N03-1028 </papid>chinese word segmentation (peng et al., 2004), <papid> C04-1081 </papid>information extraction (pinto et al, 2003; pengand mccallum, 2004), <papid> N04-1042 </papid>named entity identification (mc callum and li, 2003; settles, 2004), <papid> W04-1221 </papid>and many others.</citsent>
<aftsection>
<nextsent>1in this paper we only focus on pers, locs and orgs.
</nextsent>
<nextsent>since temporal, numerical and monetary phrases can be well identified with rule-based approaches.recently, crfs have been shown to perform exceptionally well on chinese ner shared task on the thirdsighan chinese language processing bakeoff (sighan 06) (zhou et al, 2006; chen et al, 2006<papid> W06-0130 </papid>b,a).</nextsent>
<nextsent>we follow the state-of-the-art crf models using features that have been shown to be very effective in chinese ner, namely the current character and its part-of-speech (pos) tag, several characters surrounding (both before and after) the current character and their pos tags, current word and several words surrounding the current word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L717">
<title id=" I08-1044.xml">a framework based on graphical models with logic for chinese named entity recognition </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>crfs have been successfully applied to number of real-world tasks, including np chunking (sha and pereira, 2003), <papid> N03-1028 </papid>chinese word segmentation (peng et al., 2004), <papid> C04-1081 </papid>information extraction (pinto et al, 2003; pengand mccallum, 2004), <papid> N04-1042 </papid>named entity identification (mc callum and li, 2003; settles, 2004), <papid> W04-1221 </papid>and many others.</prevsent>
<prevsent>1in this paper we only focus on pers, locs and orgs.</prevsent>
</prevsection>
<citsent citstr=" W06-0130 ">
since temporal, numerical and monetary phrases can be well identified with rule-based approaches.recently, crfs have been shown to perform exceptionally well on chinese ner shared task on the thirdsighan chinese language processing bakeoff (sighan 06) (zhou et al, 2006; chen et al, 2006<papid> W06-0130 </papid>b,a).</citsent>
<aftsection>
<nextsent>we follow the state-of-the-art crf models using features that have been shown to be very effective in chinese ner, namely the current character and its part-of-speech (pos) tag, several characters surrounding (both before and after) the current character and their pos tags, current word and several words surrounding the current word.
</nextsent>
<nextsent>we also observe some important issues that significantly influence the performance as follows: window size: the primitive window size we use is 5 ( 2 characters preceding the current character and 2 following the current character).
</nextsent>
<nextsent>we extend the window size to 7 but find that it slightly hurts.
</nextsent>
<nextsent>the reason is that crfs can deal with non-independent features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L719">
<title id=" I08-1044.xml">a framework based on graphical models with logic for chinese named entity recognition </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>imply nearby per and direction words may indicate aloc or an org.
</prevsent>
<prevsent>figure 3 illustrates an example of non local dependency.we do not take the advantage of using the golden standard word segmentation and pos tagging provided inthe original corpus, since such information is hardly available in real text.
</prevsent>
</prevsection>
<citsent citstr=" W03-1709 ">
instead, we use an off-the-shelf chinese lexical analysis system, the open source ictclas (zhang et al, 2003), <papid> W03-1709 </papid>to segment and pos tag the corpus.</citsent>
<aftsection>
<nextsent>this module employs hierarchical hidden markov model (hhmm) and provides word segmentation, pos tagging (labels chinese words using set of 39 tags) and unknown word recognition.
</nextsent>
<nextsent>it performs reasonably well, with segmentation precision recently evaluated at 97.58%.
</nextsent>
<nextsent>there call of unknown words using role tagging is over 90%.
</nextsent>
<nextsent>we use one-month corpus for training and 9-day corpus for testing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L720">
<title id=" I08-1044.xml">a framework based on graphical models with logic for chinese named entity recognition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and the model fails to tag some complicated nes (e.g., nested orgs) correctly due to lack of domain adaptive techniques.
</prevsent>
<prevsent>the f-measures of locs and orgs are only 87.13 and 83.60, which show that there is still room for improving.
</prevsent>
</prevsection>
<citsent citstr=" H05-1054 ">
a method of incorporating heuristic human knowledge into statistical model was proposed in (wu et al, 2005).<papid> H05-1054 </papid></citsent>
<aftsection>
<nextsent>here chinese ner was regarded as probabilistic tagging problem and the heuristic human knowledge was used to reduce the searching space.
</nextsent>
<nextsent>however, this method assumes that pos tags are golden-standard in the training data and heuristic human knowledge is often ad hoc.
</nextsent>
<nextsent>these drawbacks make the method unstable and highly sensitive to pos errors; and when golden-standard pos tags are not available (this is often the case), it may degrade the performance.
</nextsent>
<nextsent>cohen and sarawagi (2004) proposed semi-markovmodel which combines markov ian, hmm-like extraction process and dictionary component.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L721">
<title id=" I05-3005.xml">morphological features help pos tagging of unknown words across language varieties </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the corpus we use, chinese treebank 5.0 (palmer et al, 2005), contains data from the three language varieties as well as different genres within the varieties.
</prevsent>
<prevsent>it thus provides good dataset for the impact of language variation on tagging performance.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
previous work on pos tagging of unknown words has proposed number of features based on prefixes and suffixes and spelling cues like capitalization (toutanova et al 2003, <papid> N03-1033 </papid>brants 2000, <papid> A00-1031 </papid>ratnaparkhi 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>for example, these systems followed samuelsson (1993) in using n-grams of letter sequences ending and starting each word as unknown word features.
</nextsent>
<nextsent>but these features have mainly been tested on inflectional languages like english and german, whose derivational and inflectional affixes tend to be strong indicator of word classes; brants (2000), <papid> A00-1031 </papid>for example, showed that an english word ending in the suffix -able was very likely to be an adjective.</nextsent>
<nextsent>chinese, by contrast, has more than 4000 frequent affix characters.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L724">
<title id=" I05-3005.xml">morphological features help pos tagging of unknown words across language varieties </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the corpus we use, chinese treebank 5.0 (palmer et al, 2005), contains data from the three language varieties as well as different genres within the varieties.
</prevsent>
<prevsent>it thus provides good dataset for the impact of language variation on tagging performance.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
previous work on pos tagging of unknown words has proposed number of features based on prefixes and suffixes and spelling cues like capitalization (toutanova et al 2003, <papid> N03-1033 </papid>brants 2000, <papid> A00-1031 </papid>ratnaparkhi 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>for example, these systems followed samuelsson (1993) in using n-grams of letter sequences ending and starting each word as unknown word features.
</nextsent>
<nextsent>but these features have mainly been tested on inflectional languages like english and german, whose derivational and inflectional affixes tend to be strong indicator of word classes; brants (2000), <papid> A00-1031 </papid>for example, showed that an english word ending in the suffix -able was very likely to be an adjective.</nextsent>
<nextsent>chinese, by contrast, has more than 4000 frequent affix characters.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L728">
<title id=" I05-3005.xml">morphological features help pos tagging of unknown words across language varieties </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the corpus we use, chinese treebank 5.0 (palmer et al, 2005), contains data from the three language varieties as well as different genres within the varieties.
</prevsent>
<prevsent>it thus provides good dataset for the impact of language variation on tagging performance.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
previous work on pos tagging of unknown words has proposed number of features based on prefixes and suffixes and spelling cues like capitalization (toutanova et al 2003, <papid> N03-1033 </papid>brants 2000, <papid> A00-1031 </papid>ratnaparkhi 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>for example, these systems followed samuelsson (1993) in using n-grams of letter sequences ending and starting each word as unknown word features.
</nextsent>
<nextsent>but these features have mainly been tested on inflectional languages like english and german, whose derivational and inflectional affixes tend to be strong indicator of word classes; brants (2000), <papid> A00-1031 </papid>for example, showed that an english word ending in the suffix -able was very likely to be an adjective.</nextsent>
<nextsent>chinese, by contrast, has more than 4000 frequent affix characters.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L734">
<title id=" I05-3005.xml">morphological features help pos tagging of unknown words across language varieties </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 datasets.
</prevsent>
<prevsent>to study the significance of training on different varieties of data, we created three training sets: training set contains data only from one variety, training set ii contains data from 3 varieties, and is similar in total size to training set i. training set iii also contains data from 3 varieties and has twice much data as training set i. to facilitate comparison of performance both between and within mandarin varieties, both the devset and the test set we created are composed of three varieties of data.
</prevsent>
</prevsection>
<citsent citstr=" W00-1201 ">
the xh test data we selected was identical to the test set used in previous parsing research by bikel and chiang (2000).<papid> W00-1201 </papid></citsent>
<aftsection>
<nextsent>for the remaining data, we included hksar and sm data that is similar in size to the xh test set.
</nextsent>
<nextsent>table 5 details characteristics of the datasets.
</nextsent>
<nextsent>table 5 dataset splits used.
</nextsent>
<nextsent>the unknown word tokens are with respect to training i. dataset sect ns token unknown training 26-270, 600-931 213986 - training ii 600-931, 500-527, 1001-1039 204701 - training iii 001-270, 301-527, 590-593, 600-1039, 1043-1151 485321 - devset 23839 2849 xh 001-025 7844 381 hksar 500-527 8202 1168 sm 590-593, 1001-1002 7793 1300 test set 23522 2957 xh 271-300 8008 358 hksar 528-554 7153 1020 sm 594-596, 1040-1042 8361 1579 5.2 the model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L735">
<title id=" I05-3005.xml">morphological features help pos tagging of unknown words across language varieties </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>table 5 dataset splits used.
</prevsent>
<prevsent>the unknown word tokens are with respect to training i. dataset sect ns token unknown training 26-270, 600-931 213986 - training ii 600-931, 500-527, 1001-1039 204701 - training iii 001-270, 301-527, 590-593, 600-1039, 1043-1151 485321 - devset 23839 2849 xh 001-025 7844 381 hksar 500-527 8202 1168 sm 590-593, 1001-1002 7793 1300 test set 23522 2957 xh 271-300 8008 358 hksar 528-554 7153 1020 sm 594-596, 1040-1042 8361 1579 5.2 the model.
</prevsent>
</prevsection>
<citsent citstr=" W04-3236 ">
our model builds on research into loglinear models by ng and low (2004), <papid> W04-3236 </papid>toutanova et al, (2003) <papid> N03-1033 </papid>and ratnaparkhi (1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>the first research uses independent maximum entropy classifiers, with sequence model imposing categorical valid tag sequence constraints.
</nextsent>
<nextsent>the latter two use maximum entropy markov models (memm) (mccallum et al, 2000), that use log-linear models to obtain the probabilities of state transition given an observation and the previous state, as illustrated in figure 1 (a).
</nextsent>
<nextsent>figure 1 graphical representation of transition probability calculation used in maximum entropy markov models.
</nextsent>
<nextsent>(a) the previous state and the current word are used to calculate the transition probabilities for the next state transition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L745">
<title id=" I05-3005.xml">morphological features help pos tagging of unknown words across language varieties </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we examined several tag sequence features from both left and right side of the current word.
</prevsent>
<prevsent>we use the term lexical features to refer to features derived from the identity of word, and tag sequence features refer to features derived from the tags of surrounding words.
</prevsent>
</prevsection>
<citsent citstr=" P99-1023 ">
these features have been shown to be useful in previous research on english (toutanova et al 2003, <papid> N03-1033 </papid>brants 2000, <papid> A00-1031 </papid>thede and harper 1999) <papid> P99-1023 </papid>the models9 in table 7 list the different tag sequence features used; they also use the same lexical features from the model 2rw+2lw shown in table 6.</citsent>
<aftsection>
<nextsent>theta ble shows that model lt+llt conditioning on the previous tag and the conjunction of the two previous 8 we abbreviate accuracy as a?..
</nextsent>
<nextsent>9 except where otherwise stated, during training, count cutoff of.
</nextsent>
<nextsent>3 is applied to all features found in the training set.
</nextsent>
<nextsent>if feature occurs fewer than 3 times, it is simply removed from the training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L760">
<title id=" I05-3005.xml">morphological features help pos tagging of unknown words across language varieties </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>how good are our best numbers, i.e. 93.7% on pos tagging in ctb 5.0?
</prevsent>
<prevsent>unfortunately, there are no clean direct comparisons in the literature.
</prevsent>
</prevsection>
<citsent citstr=" C02-1145 ">
the closest result in the literature is xue et al (2002), <papid> C02-1145 </papid>who retrain the ratnaparkhi (1996) <papid> W96-0213 </papid>tagger and reach accuracies of 93% using ctb-i.</citsent>
<aftsection>
<nextsent>however ctb-i contains only xh data and furthermore the data split is no longer known for this experiment (xue p.c.) so comparison is not informative.
</nextsent>
<nextsent>however, our performance on tagging when trained on training and tested on just the xh part of the test set is 94.44%, which might be more relevant comparison to xue et al (2002).<papid> C02-1145 </papid></nextsent>
<nextsent>previous research in part-of-speech tagging has resulted in taggers that perform well when the training set and test set are both drawn from the same corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L763">
<title id=" I08-3011.xml">strategies for sustainable mt for basque incremental design re usability standardization and open source </title>
<section> fourth phase: multilingual ism and general.  </section>
<citcontext>
<prevsection>
<prevsent>we conducted basque to english and spanish to basque translation experiments, evaluated on large corpus (270, 000 sentence pairs).
</prevsent>
<prevsent>some tools have been reused for this purpose: ? giza++: for word/morpheme alignment we used the giza++ statistical word alignment toolkit, and following the refined?
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
method of (och and ney, 2003), <papid> J03-1002 </papid>extracted set of high-quality word/ morpheme alignments from the original unidirectional alignment sets.</citsent>
<aftsection>
<nextsent>these along with the extracted chunk alignments were passed to the translation decoder.
</nextsent>
<nextsent>pharaoh/moses decoder: the decoder is also hybrid system which integrates ebmt and smt.
</nextsent>
<nextsent>it is capable of retrieving already translated sentences and also provides wrapper around the pharaoh smt decoder (koehn, 2004).
</nextsent>
<nextsent>matrex: the matrex (machine translation using examples) system used in our experiments is data-driven mt engine, built following an extremely modular design.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L764">
<title id=" I08-1059.xml">using contextual speller techniques and language modeling for esl error correction </title>
<section> auxiliary verb presence and choice:.  </section>
<citcontext>
<prevsection>
<prevsent>(interested in)
</prevsent>
<prevsent>my teacher does is good teacher (my teacher is...)
</prevsent>
</prevsection>
<citsent citstr=" P00-1067 ">
1 liu et al 2000 <papid> P00-1067 </papid>take similar approach, retrieving.</citsent>
<aftsection>
<nextsent>example sentences from large corpus.
</nextsent>
<nextsent>449
</nextsent>
<nextsent>i writed letter (wrote)
</nextsent>
<nextsent>this is china book (chinese book) compounds): am student of university (university student) 8.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L765">
<title id=" I08-1059.xml">using contextual speller techniques and language modeling for esl error correction </title>
<section> adjective/noun confusion:.  </section>
<citcontext>
<prevsection>
<prevsent>category which makes up about 5% of errors.
</prevsent>
<prevsent>3 related work.
</prevsent>
</prevsection>
<citsent citstr=" P06-1132 ">
models for determiner and preposition selection have mostly been investigated in the context of sentence realization and machine translation (knight and chander 1994, gamon et al 2002, bond 2005, suzuki and toutanova 2006, <papid> P06-1132 </papid>toutanova and suzuki 2007).<papid> N07-1007 </papid></citsent>
<aftsection>
<nextsent>such approaches typically relyon the fact that preposition or determiner choice is made in otherwise native-like sentences.
</nextsent>
<nextsent>turner and charniak (2007), <papid> N07-2045 </papid>for example, utilize language model based on statistical parser for penn tree bank data.</nextsent>
<nextsent>similarly, de felice and pulman (2007) utilize set of sophisticated syntactic and semantic analysis features to predict 5 common english prepositions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L766">
<title id=" I08-1059.xml">using contextual speller techniques and language modeling for esl error correction </title>
<section> adjective/noun confusion:.  </section>
<citcontext>
<prevsection>
<prevsent>category which makes up about 5% of errors.
</prevsent>
<prevsent>3 related work.
</prevsent>
</prevsection>
<citsent citstr=" N07-1007 ">
models for determiner and preposition selection have mostly been investigated in the context of sentence realization and machine translation (knight and chander 1994, gamon et al 2002, bond 2005, suzuki and toutanova 2006, <papid> P06-1132 </papid>toutanova and suzuki 2007).<papid> N07-1007 </papid></citsent>
<aftsection>
<nextsent>such approaches typically relyon the fact that preposition or determiner choice is made in otherwise native-like sentences.
</nextsent>
<nextsent>turner and charniak (2007), <papid> N07-2045 </papid>for example, utilize language model based on statistical parser for penn tree bank data.</nextsent>
<nextsent>similarly, de felice and pulman (2007) utilize set of sophisticated syntactic and semantic analysis features to predict 5 common english prepositions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L767">
<title id=" I08-1059.xml">using contextual speller techniques and language modeling for esl error correction </title>
<section> adjective/noun confusion:.  </section>
<citcontext>
<prevsection>
<prevsent>models for determiner and preposition selection have mostly been investigated in the context of sentence realization and machine translation (knight and chander 1994, gamon et al 2002, bond 2005, suzuki and toutanova 2006, <papid> P06-1132 </papid>toutanova and suzuki 2007).<papid> N07-1007 </papid></prevsent>
<prevsent>such approaches typically relyon the fact that preposition or determiner choice is made in otherwise native-like sentences.</prevsent>
</prevsection>
<citsent citstr=" N07-2045 ">
turner and charniak (2007), <papid> N07-2045 </papid>for example, utilize language model based on statistical parser for penn tree bank data.</citsent>
<aftsection>
<nextsent>similarly, de felice and pulman (2007) utilize set of sophisticated syntactic and semantic analysis features to predict 5 common english prepositions.
</nextsent>
<nextsent>obviously, this is impractical in setting where noisy non-native text is subjected to proofing.
</nextsent>
<nextsent>meanwhile, work on automated error detection on non-native text focuses primarily on detection of errors, rather than on the more difficult task of supplying viable corrections (e.g., chodorow and leacock, 2000).
</nextsent>
<nextsent>more recently, han et al (2004), han et al (2006) use maximum entropy classifier to propose article corrections in tesol essays, while izumi et al (2003) and chodorow et al (2007) <papid> W07-1604 </papid>present techniques of automatic preposition choice modeling.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L768">
<title id=" I08-1059.xml">using contextual speller techniques and language modeling for esl error correction </title>
<section> adjective/noun confusion:.  </section>
<citcontext>
<prevsection>
<prevsent>obviously, this is impractical in setting where noisy non-native text is subjected to proofing.
</prevsent>
<prevsent>meanwhile, work on automated error detection on non-native text focuses primarily on detection of errors, rather than on the more difficult task of supplying viable corrections (e.g., chodorow and leacock, 2000).
</prevsent>
</prevsection>
<citsent citstr=" W07-1604 ">
more recently, han et al (2004), han et al (2006) use maximum entropy classifier to propose article corrections in tesol essays, while izumi et al (2003) and chodorow et al (2007) <papid> W07-1604 </papid>present techniques of automatic preposition choice modeling.</citsent>
<aftsection>
<nextsent>these more recent efforts, nevertheless, do not attempt to integrate their methods into more general proofing application designed to assist non-native speakers when writing english.
</nextsent>
<nextsent>finally, yi et al (2008) designed system that uses web counts to determine correct article usage forgiven sentence, targeting esl users.
</nextsent>
<nextsent>4 system description.
</nextsent>
<nextsent>our system consists of three major components: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L771">
<title id=" I08-1059.xml">using contextual speller techniques and language modeling for esl error correction </title>
<section> adjective/noun confusion:.  </section>
<citcontext>
<prevsection>
<prevsent>currently we train decision tree classifiers with the winmine toolkit (chickering 2002).
</prevsent>
<prevsent>we also experimented with linear svms, but decision trees performed better overall and training and parameter optimization were considerably more efficient.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
before training the classifiers, we perform feature ablation by imposing count cutoff of 10, and by limiting the number of features to the top 75k features in terms of log likelihood ratio (dunning 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>we train two separate classifiers for both determiners and preposition: ? decision whether or not determiner/preposition should be present (presence/absence or pa classifier) ? decision which determiner/preposition is the most likely choice, given that determiner/preposition is present (choice or ch classifier) in the case of determiners, class values for the ch classifier are a/an and the.
</nextsent>
<nextsent>preposition choice (equivalent to the confusion set?
</nextsent>
<nextsent>of contextual speller) is limited to set of 13 prepositions that figure prominently in the errors observed in the jle corpus: about, as, at, by, for, from, in, like, of, on, since, to, with, than,  other  (for prepositions not in the list).
</nextsent>
<nextsent>the decision tree classifiers produce probability distributions over class values at their leaf nodes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L772">
<title id=" I08-1059.xml">using contextual speller techniques and language modeling for esl error correction </title>
<section> adjective/noun confusion:.  </section>
<citcontext>
<prevsection>
<prevsent>model pa ch combined accuracy 89.61% 85.97% 86.07% table 1: accuracy of the determiner pa, ch, and combined models.
</prevsent>
<prevsent>the baseline is 69.9% (choosing the most frequent class label none).
</prevsent>
</prevsection>
<citsent citstr=" W00-0708 ">
the overall accuracy of this module is state-of-the-art compared with results reported in the literature (knight and chander 1994, minnen et al 2000, <papid> W00-0708 </papid>lee 2004, <papid> N04-2006 </papid>turner and charniak 2007).<papid> N07-2045 </papid></citsent>
<aftsection>
<nextsent>turner and charniak 2007 <papid> N07-2045 </papid>obtained the best reported accuracy to date of 86.74%, using charniak language model (charniak 2001) <papid> P01-1017 </papid>based on full statistical parser on the penn tree bank.</nextsent>
<nextsent>these numbers are, of course, not directly comparable, given the different corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L773">
<title id=" I08-1059.xml">using contextual speller techniques and language modeling for esl error correction </title>
<section> adjective/noun confusion:.  </section>
<citcontext>
<prevsection>
<prevsent>model pa ch combined accuracy 89.61% 85.97% 86.07% table 1: accuracy of the determiner pa, ch, and combined models.
</prevsent>
<prevsent>the baseline is 69.9% (choosing the most frequent class label none).
</prevsent>
</prevsection>
<citsent citstr=" N04-2006 ">
the overall accuracy of this module is state-of-the-art compared with results reported in the literature (knight and chander 1994, minnen et al 2000, <papid> W00-0708 </papid>lee 2004, <papid> N04-2006 </papid>turner and charniak 2007).<papid> N07-2045 </papid></citsent>
<aftsection>
<nextsent>turner and charniak 2007 <papid> N07-2045 </papid>obtained the best reported accuracy to date of 86.74%, using charniak language model (charniak 2001) <papid> P01-1017 </papid>based on full statistical parser on the penn tree bank.</nextsent>
<nextsent>these numbers are, of course, not directly comparable, given the different corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L776">
<title id=" I08-1059.xml">using contextual speller techniques and language modeling for esl error correction </title>
<section> adjective/noun confusion:.  </section>
<citcontext>
<prevsection>
<prevsent>the baseline is 69.9% (choosing the most frequent class label none).
</prevsent>
<prevsent>the overall accuracy of this module is state-of-the-art compared with results reported in the literature (knight and chander 1994, minnen et al 2000, <papid> W00-0708 </papid>lee 2004, <papid> N04-2006 </papid>turner and charniak 2007).<papid> N07-2045 </papid></prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
turner and charniak 2007 <papid> N07-2045 </papid>obtained the best reported accuracy to date of 86.74%, using charniak language model (charniak 2001) <papid> P01-1017 </papid>based on full statistical parser on the penn tree bank.</citsent>
<aftsection>
<nextsent>these numbers are, of course, not directly comparable, given the different corpora.
</nextsent>
<nextsent>on the other hand, the distribution of determiners is similar in the ptb (as reported in minnen et al 2000) <papid> W00-0708 </papid>and in our data (table 2).</nextsent>
<nextsent>ptb reuters/encarta mix no determiner 70.0% 69.9% the 20.6% 22.2% a/an 9.4% 7.8% table 2: distribution of determiners in the penn tree bank and in our reuters/encarta data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L784">
<title id=" I05-2034.xml">probabilistic models for korean morphological analysis </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>over the past few decades, considerable number of studies have been made on korean morphological analysis.
</prevsent>
<prevsent>the early studies concentrate don the algorithmic research.
</prevsent>
</prevsection>
<citsent citstr=" C94-1035 ">
the following approaches belong to this group: longest matching algorithm, tabular parsing method using cyk algorithm (kim, 1986), dictionary based approach (kwon, 1991), two-level morphology(lee, 1992), and syllable-based approach (kang and kim, 1994).<papid> C94-1035 </papid>next, many studies have been made on improving the efficiency of the morphological an alyzers.</citsent>
<aftsection>
<nextsent>there have been studies to reduce the search space and implausible interpretations by using characteristics of korean syllables (kang, 1995; lim et al, 1995).there have been no standard tagset and annotation guideline, so researchers have developed methods with their own tagsets and guidelines.the morphological analysis and tagger evaluation contest (matec) took place in 1999.
</nextsent>
<nextsent>this isthe first trial about the objective and relative evaluation of morphological analysis.
</nextsent>
<nextsent>among the participants, some newly implemented the systems and others converted the existing systems?
</nextsent>
<nextsent>results through postprocessing steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L785">
<title id=" I08-1020.xml">experiments on semantic based clustering for cross document coreference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>refer to the same individual is difficult problem.
</prevsent>
<prevsent>automatic techniques for solving this problem are required not only for better access to information but also in natural language processing applications such as multi document summarization, question answering, and information extraction.
</prevsent>
</prevsection>
<citsent citstr=" W07-2012 ">
here, we concentrate on the web people search task (artiles etal., 2007) <papid> W07-2012 </papid>as defined in the semeval 2007 work shop: search engine user types in person name asa query.</citsent>
<aftsection>
<nextsent>instead of ranking web pages, an ideal system should organise search results in as many clusters as there are different people sharing the same name in the documents returned by the search engine.
</nextsent>
<nextsent>the input is, therefore, the results given by web search engine using person name as query.the output is number of sets, each containing documents referring to the same individual.
</nextsent>
<nextsent>the task is related to the coreference resolution problem disregarding however the linking of mentions of the target entity inside each single document.
</nextsent>
<nextsent>similarly to (bagga and baldwin, 1998; <papid> P98-1012 </papid>phan et al., 2006), we have addressed the task as document clustering problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L786">
<title id=" I08-1020.xml">experiments on semantic based clustering for cross document coreference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the input is, therefore, the results given by web search engine using person name as query.the output is number of sets, each containing documents referring to the same individual.
</prevsent>
<prevsent>the task is related to the coreference resolution problem disregarding however the linking of mentions of the target entity inside each single document.
</prevsent>
</prevsection>
<citsent citstr=" P98-1012 ">
similarly to (bagga and baldwin, 1998; <papid> P98-1012 </papid>phan et al., 2006), we have addressed the task as document clustering problem.</citsent>
<aftsection>
<nextsent>we have implemented our own clustering algorithms but relyon available extraction and summarization technology to produce document representations used as input for the clustering procedure.
</nextsent>
<nextsent>we will shown that our techniques produce not only very good results but are also very competitive when compared with semeval 2007 systems.we will also show that carefully selection of document representation is of paramount importance to achieve good performance.
</nextsent>
<nextsent>our system has similar level of performance as the best system in the recent semeval 2007 evaluation framework.
</nextsent>
<nextsent>this paper extends our previous work on this task (sag gion, 2007).<papid> W07-2063 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L787">
<title id=" I08-1020.xml">experiments on semantic based clustering for cross document coreference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we will shown that our techniques produce not only very good results but are also very competitive when compared with semeval 2007 systems.we will also show that carefully selection of document representation is of paramount importance to achieve good performance.
</prevsent>
<prevsent>our system has similar level of performance as the best system in the recent semeval 2007 evaluation framework.
</prevsent>
</prevsection>
<citsent citstr=" W07-2063 ">
this paper extends our previous work on this task (sag gion, 2007).<papid> W07-2063 </papid></citsent>
<aftsection>
<nextsent>149
</nextsent>
<nextsent>the semeval evaluation has prepared two sets of data to investigate the cross-document coreference problem: one for development and one for testing.the data consists of around 100 web files per person name, which have been frozen and so, can be used as an static corpus.
</nextsent>
<nextsent>each file in the corpus is associated with an integer number which indicates the rank at which the particular page was retrieved by the search engine.
</nextsent>
<nextsent>in addition to the files themselves, the following information was available: the page title, the url, and the snippet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L788">
<title id=" I08-1020.xml">experiments on semantic based clustering for cross document coreference </title>
<section> natural language processing.  </section>
<citcontext>
<prevsection>
<prevsent>the thresholds used for the experiments reported here are as follows:0.10 for word vectors and 0.12 for named entity vectors (see section 5 for vector representations).
</prevsent>
<prevsent>technology we relyon available extraction and summarization technology in order to linguistically process the documents for creating document representations for clustering.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
although the semeval corpus contains information other than the retrieved pages themselves, we have made no attempt to analyse or use contextual information given with the input docu ment.two tools are used: the gate system (cunningham et al, 2002) <papid> P02-1022 </papid>and summarization toolkit (sag gion, 2002; saggion and gaizauskas, 2004) which is compatible with gate.</citsent>
<aftsection>
<nextsent>the input for analysis is set of documents and person name (first name and last name).
</nextsent>
<nextsent>the documents are analysed by the default gate1 annie system which creates different types of named entity annotations.
</nextsent>
<nextsent>no adaptation of the system was carried out because we wanted to verify how far we could go using available tools.
</nextsent>
<nextsent>summarization technology was used from single document summarization modules from our summarization toolkit.the core of the toolkit is set of summarization modules which compute numeric features for each sentence in the input document, the value of the feature indicates how relevant the information in the sentence is for the feature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L790">
<title id=" I08-1020.xml">experiments on semantic based clustering for cross document coreference </title>
<section> semantic-based experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this is probably because all types of information in personal summary are somehow related to the target person.
</prevsent>
<prevsent>7.2 results per person set.
</prevsent>
</prevsection>
<citsent citstr=" W07-2041 ">
following (popescu and magnini, 2007), <papid> W07-2041 </papid>we present purity, inverse purity, and f-score results for all our configurations per category (acl, us census, wikipedia) in the test set.</citsent>
<aftsection>
<nextsent>in tables 7, 8, and 9, results are reported for full configuration set purity i.purity f-score fd+address acl 0.86 0.48 0.57 fd+address us c. 0.81 0.71 0.75 fd+address wikip.
</nextsent>
<nextsent>0.78 0.70 0.73 ps+address acl 0.96 0.38 0.50 ps+address us c. 0.94 0.61 0.72 ps+address wikip.
</nextsent>
<nextsent>0.88 0.62 0.71 fd+date acl 0.63 0.82 0.69 fd+date us c. 0.52 0.87 0.64 fd+date wikip.
</nextsent>
<nextsent>0.59 0.85 0.68 ps+date acl 0.88 0.49 0.59 ps+date us c. 0.88 0.64 0.72 ps+date wikip.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L792">
<title id=" I08-1020.xml">experiments on semantic based clustering for cross document coreference </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they have not presented an analysis of the impact of full document versus summary condition and their clustering algorithm is rather under-specified.
</prevsent>
<prevsent>here we have presented clearer picture of the influence of summary vs full document condition in the clustering process.
</prevsent>
</prevsection>
<citsent citstr=" W03-0405 ">
mann and yarowsky (mann and yarowsky, 2003)<papid> W03-0405 </papid>used semantic information extracted from documents referring to the target person in an hierarchicalagglomerative clustering algorithm.</citsent>
<aftsection>
<nextsent>semantic information here refers to factual information about person such as the date of birth, professional career or education.
</nextsent>
<nextsent>information is extracted using patterns some of them manually developed and others induced from examples.
</nextsent>
<nextsent>we differ from this approach in that our semantic information is more general and is not particularly related - although it might be - to the target person.
</nextsent>
<nextsent>phan el al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L794">
<title id=" I08-1020.xml">experiments on semantic based clustering for cross document coreference </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they use machine learning algorithm to classify sentences according to particular information types in order to automatically construct person profile.
</prevsent>
<prevsent>instead of comparing biographical information in the person profile altogether as in (mann and yarowsky, 2003)<papid> W03-0405 </papid>, they compare each type of information independently of each other, combining them only to make the final deci sion.</prevsent>
</prevsection>
<citsent citstr=" W07-2024 ">
finally, the best semeval 2007 web people search system (chen and martin, 2007) <papid> W07-2024 </papid>used techniques similar to ours: named entity recognition using off-the-shelf systems.</citsent>
<aftsection>
<nextsent>however in addition to semantic information and full document condition they also explore the use of contextual information such as the url where the document comes from.
</nextsent>
<nextsent>they show that this information is of little help.
</nextsent>
<nextsent>our improved system obtained slightly higher macro averaged f-score over their system.
</nextsent>
<nextsent>we have presented experiments on cross-document coreference of person names in the context of the first semeval 2007 web people search task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L795">
<title id=" I05-2041.xml">tree annotation tool using two phase parsing to reduce manual effort for building a treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results show that it can reduce manual effort about 24.5% as compared with tree annotation tool without segmentation because an annotations intervention related to cancellation and reconstruction remarkably decrease although it requires the annotator to segment some long sentence.
</prevsent>
<prevsent>a treebank is corpus annotated with syntactic information, and the structural analysis of each sentence is represented as bracketed tree structure.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
this kind of corpus has served as an extremely valuable resource for computational linguistics applications such as machine translation and question answering (lee et al, 1997; choi, 2001), and has also proved useful in theoretical linguistics research (marcus et al, 1993).<papid> J93-2004 </papid>however, for the purpose of building the treebank, an annotator spends lot of time and manual effort.</citsent>
<aftsection>
<nextsent>furthermore, it is too difficult to maintain the consistency of the treebank based on only the annotator (hindle, 1989; <papid> P89-1015 </papid>chang et al, 1997).therefore, we require tree annotation tool to reduce manual effort by decreasing the frequency of the human annotators?</nextsent>
<nextsent>intervention.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L796">
<title id=" I05-2041.xml">tree annotation tool using two phase parsing to reduce manual effort for building a treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a treebank is corpus annotated with syntactic information, and the structural analysis of each sentence is represented as bracketed tree structure.
</prevsent>
<prevsent>this kind of corpus has served as an extremely valuable resource for computational linguistics applications such as machine translation and question answering (lee et al, 1997; choi, 2001), and has also proved useful in theoretical linguistics research (marcus et al, 1993).<papid> J93-2004 </papid>however, for the purpose of building the treebank, an annotator spends lot of time and manual effort.</prevsent>
</prevsection>
<citsent citstr=" P89-1015 ">
furthermore, it is too difficult to maintain the consistency of the treebank based on only the annotator (hindle, 1989; <papid> P89-1015 </papid>chang et al, 1997).therefore, we require tree annotation tool to reduce manual effort by decreasing the frequency of the human annotators?</citsent>
<aftsection>
<nextsent>intervention.
</nextsent>
<nextsent>moreover, the tool can improve the annotating efficiency, and help maintain the consistency of the treebank (kwak et al, 2001; lim et al, 2004).
</nextsent>
<nextsent>in this paper, we propose tree annotation tool using parser in order to reduce manual effort for building treebank.
</nextsent>
<nextsent>fundamentally, it generates candidate syntactic structure by utilizing parser.and then, the annotator cancels the incorrect constituents in the candidate syntactic structure, and reconstructs the correct constituents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L806">
<title id=" I05-2041.xml">tree annotation tool using two phase parsing to reduce manual effort for building a treebank </title>
<section> previous works.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we propose tree annotation tool using parser for the purpose of shifting there sponsibility of extracting the reliable syntactic rules to the parser.
</prevsent>
<prevsent>it is always ready to change the parser into another parser.
</prevsent>
</prevsection>
<citsent citstr=" C90-3088 ">
however, most parsers still tend to show low performance on the long sentences (li et al, 1990; <papid> C90-3088 </papid>doi et al, 1993; <papid> E93-1060 </papid>kim et al., 2000).<papid> W00-1321 </papid></citsent>
<aftsection>
<nextsent>besides, one of the reasons to decrease the parsing performance is that the initial syntactic errors of word or phrase propagates to the whole syntactic structure.in order to prevent the initial errors from propagating without any modification of the parser, the proposed tool requires the annotator to segment asentence.
</nextsent>
<nextsent>and then, it performs two-phase parsing for the intra-structure of each segment and the inter-structure.
</nextsent>
<nextsent>the parsing methods using clause-based segmentation have been studied to improve the parsing performance and the parsing complexity (kim et al, 2000; <papid> W00-1321 </papid>lyon and dickerson, 1997; sang and dejean, 2001).</nextsent>
<nextsent>nevertheless, the clause-based segmentation can permit short sentence to be split ted into shorter segments unnecessarily although too short segments increase manual effort to build treebank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L807">
<title id=" I05-2041.xml">tree annotation tool using two phase parsing to reduce manual effort for building a treebank </title>
<section> previous works.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we propose tree annotation tool using parser for the purpose of shifting there sponsibility of extracting the reliable syntactic rules to the parser.
</prevsent>
<prevsent>it is always ready to change the parser into another parser.
</prevsent>
</prevsection>
<citsent citstr=" E93-1060 ">
however, most parsers still tend to show low performance on the long sentences (li et al, 1990; <papid> C90-3088 </papid>doi et al, 1993; <papid> E93-1060 </papid>kim et al., 2000).<papid> W00-1321 </papid></citsent>
<aftsection>
<nextsent>besides, one of the reasons to decrease the parsing performance is that the initial syntactic errors of word or phrase propagates to the whole syntactic structure.in order to prevent the initial errors from propagating without any modification of the parser, the proposed tool requires the annotator to segment asentence.
</nextsent>
<nextsent>and then, it performs two-phase parsing for the intra-structure of each segment and the inter-structure.
</nextsent>
<nextsent>the parsing methods using clause-based segmentation have been studied to improve the parsing performance and the parsing complexity (kim et al, 2000; <papid> W00-1321 </papid>lyon and dickerson, 1997; sang and dejean, 2001).</nextsent>
<nextsent>nevertheless, the clause-based segmentation can permit short sentence to be split ted into shorter segments unnecessarily although too short segments increase manual effort to build treebank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L808">
<title id=" I05-2041.xml">tree annotation tool using two phase parsing to reduce manual effort for building a treebank </title>
<section> previous works.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we propose tree annotation tool using parser for the purpose of shifting there sponsibility of extracting the reliable syntactic rules to the parser.
</prevsent>
<prevsent>it is always ready to change the parser into another parser.
</prevsent>
</prevsection>
<citsent citstr=" W00-1321 ">
however, most parsers still tend to show low performance on the long sentences (li et al, 1990; <papid> C90-3088 </papid>doi et al, 1993; <papid> E93-1060 </papid>kim et al., 2000).<papid> W00-1321 </papid></citsent>
<aftsection>
<nextsent>besides, one of the reasons to decrease the parsing performance is that the initial syntactic errors of word or phrase propagates to the whole syntactic structure.in order to prevent the initial errors from propagating without any modification of the parser, the proposed tool requires the annotator to segment asentence.
</nextsent>
<nextsent>and then, it performs two-phase parsing for the intra-structure of each segment and the inter-structure.
</nextsent>
<nextsent>the parsing methods using clause-based segmentation have been studied to improve the parsing performance and the parsing complexity (kim et al, 2000; <papid> W00-1321 </papid>lyon and dickerson, 1997; sang and dejean, 2001).</nextsent>
<nextsent>nevertheless, the clause-based segmentation can permit short sentence to be split ted into shorter segments unnecessarily although too short segments increase manual effort to build treebank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L810">
<title id=" I05-2041.xml">tree annotation tool using two phase parsing to reduce manual effort for building a treebank </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>segment length for the purpose of evaluating the parsing performance given the correct segments, we classify the constituents in the syntactic structures into the constituents in the intra-structures of segments and the constituents in the inter-structures.
</prevsent>
<prevsent>besides, we evaluate each classified constituents based on labeled precision, labeled recall, and distribution ratio.
</prevsent>
</prevsection>
<citsent citstr=" P96-1024 ">
the labeled precision (lp) indicates the ratio of correct candidate constituents from candidate constituents generated by the parser, and the labeled recall (lr) indicates the ratio of correct candidate constituents from constituents in the treebank (goodman, 1996).<papid> P96-1024 </papid></citsent>
<aftsection>
<nextsent>also,the distribution ratio (ratio) indicates the distribution ratio of constituents in the intra-structures from all of constituents in the original structure.
</nextsent>
<nextsent>table 1 shows that the distribution ratio of the constituents in the intra-structures increases according to the longer segment length while the distribution ratio of the constituents in the inter structures decreases.
</nextsent>
<nextsent>given the segment length 1,the constituents in the inter-structures of sentence are the same as the constituents of the sen table 1: parsing performance intra-structure inter-structure length lp lr ratio lp lr ratio 1 0.00 0.00 0.00 87.62 86.06 100.00 2 100.00 93.42 52.25 74.45 74.08 47.75 3 100.00 97.27 66.61 60.63 58.55 33.39 4 98.93 96.47 74.27 62.11 59.71 25.73 6 96.50 95.47 83.24 67.88 66.68 16.76 7 95.45 94.45 86.12 70.84 69.81 13.88 8 94.34 93.10 88.47 74.24 73.23 11.53 9 93.41 92.36 90.40 76.85 76.25 9.60 10 92.65 91.47 92.01 78.99 78.31 7.99 11 91.91 90.65 93.43 81.53 80.72 6.57 12 91.19 89.86 94.59 84.39 83.60 5.41 13 90.54 89.23 95.62 86.92 85.97 4.38 14 89.87 88.61 96.54 88.82 88.19 3.46 15 89.34 87.99 97.30 90.39 89.41 2.70 16 88.98 87.65 97.97 90.50 89.86 2.03 17 88.64 87.27 98.51 91.64 89.61 1.49 18 88.37 86.99 98.98 92.92 90.84 1.02 19 88.15 86.76 99.34 92.97 91.30 0.66 20 87.98 86.57 99.58 92.00 91.62 0.42 21 87.83 86.42 99.76 92.73 92.36 0.24 22 87.76 86.36 99.83 93.60 93.20 0.17 23 87.74 86.36 99.89 94.46 93.81 0.11 24 87.69 86.27 99.94 97.67 94.74 0.06 25 87.65 86.26 99.97 100.00 95.45 0.03 26 87.63 86.24 99.99 100.00 100.00 0.01 27 87.63 86.16 99.99 100.00 100.00 0.01 28 87.62 86.06 100.00 0.00 0.00 0.00 tence because there is no evaluated constituent on intra-structure of one word.
</nextsent>
<nextsent>in the same way, the constituents in the intra-structures of sentence are the same as the constituents of the sentence given the segment length 28 because all test sentences are shorter than 28 words.as described in table 1, the labeled precision and the labeled recall decrease on the intra structures according to the longer segment length because both the parsing complexity and the parsing ambiguity increase more and more.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L811">
<title id=" I05-2009.xml">analysis and modeling of manual summarization of japanese broadcast news </title>
<section> summarization model.  </section>
<citcontext>
<prevsection>
<prevsent>the supplement is often included in sum mary; this part contains different information from the other parts.
</prevsent>
<prevsent>5.3 related works and discussion.
</prevsent>
</prevsection>
<citsent citstr=" P99-1072 ">
our two-step model essentially belongs to the same category as the works of (mani et al, 1999) <papid> P99-1072 </papid>and (jing and mckeown, 2000).<papid> A00-2024 </papid></citsent>
<aftsection>
<nextsent>mani et al.
</nextsent>
<nextsent>(1999) proposed summarization system based on the draft and revision.?
</nextsent>
<nextsent>jing and mckeown (2000) <papid> A00-2024 </papid>proposed system based on extraction and cut-and-paste generation.?</nextsent>
<nextsent>our abs tractors performed the same cut-and-paste operations that jing and mckeown noted in their work, and we think that our two-step model will be reasonable starting point for our subsequent research.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L812">
<title id=" I05-2009.xml">analysis and modeling of manual summarization of japanese broadcast news </title>
<section> summarization model.  </section>
<citcontext>
<prevsection>
<prevsent>the supplement is often included in sum mary; this part contains different information from the other parts.
</prevsent>
<prevsent>5.3 related works and discussion.
</prevsent>
</prevsection>
<citsent citstr=" A00-2024 ">
our two-step model essentially belongs to the same category as the works of (mani et al, 1999) <papid> P99-1072 </papid>and (jing and mckeown, 2000).<papid> A00-2024 </papid></citsent>
<aftsection>
<nextsent>mani et al.
</nextsent>
<nextsent>(1999) proposed summarization system based on the draft and revision.?
</nextsent>
<nextsent>jing and mckeown (2000) <papid> A00-2024 </papid>proposed system based on extraction and cut-and-paste generation.?</nextsent>
<nextsent>our abs tractors performed the same cut-and-paste operations that jing and mckeown noted in their work, and we think that our two-step model will be reasonable starting point for our subsequent research.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L815">
<title id=" I08-1028.xml">rapid prototyping of robust language understanding modules for spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although its asr output can easily be transformed into concepts based on grammar rules, complicated grammars are required to understand the users utterances in various expressions.
</prevsent>
<prevsent>it takes great dealof effort to the system developer.
</prevsent>
</prevsection>
<citsent citstr=" J92-1004 ">
extracting con 210 figure 2: example of wfst for lu cepts from user utterances by keyword spotting or heuristic rules has also been proposed (seneff, 1992) <papid> J92-1004 </papid>where utterances can be transformed into concepts without major modifications to the rules.</citsent>
<aftsection>
<nextsent>however, numerous complicated rules similarly need to be manually prepared.
</nextsent>
<nextsent>unfortunately, neither method is robust against asr errors.to cope with these problems, corpus-based (su doh and tsukada, 2005; he and young, 2005) and weighted finite state transducer (wfst)-basedmethods (potamianos and kuo, 2000; wutiwiwatchai and furui, 2004) have been proposed as lu modules for spoken dialogue systems.
</nextsent>
<nextsent>since these methods extract concepts using stochastic analysis, they do not need numerous complicated rules.
</nextsent>
<nextsent>these, however, require great deal of training data to implement the module and are not suitable for constructing new domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L816">
<title id=" I08-2080.xml">japanese named entity recognition using structural natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the japanese ner system proposed in (nakano and hirai, 2004), which achieved the highest f-measure among conventional systems, introduced the bunsetsu1 feature in order to consider wider context, but considers only adjacent bunsetsus.
</prevsent>
<prevsent>*research fellow of the japan society for the promotion of science (jsps) 1bunsetsu is commonly used linguistic unit in japanese, consisting of one or more adjacent content words and zero or more following functional words.on the other hand, as for english or chinese, various ner systems have explored global information and reported their effectiveness.
</prevsent>
</prevsection>
<citsent citstr=" W02-2019 ">
in (malouf, 2002;<papid> W02-2019 </papid>chieu and ng, 2002), information about features assigned to other instances of the same token is uti lized.</citsent>
<aftsection>
<nextsent>(ji and grishman, 2005) <papid> P05-1051 </papid>uses the information obtained from coreference analysis for ner.</nextsent>
<nextsent>(mohit and hwa, 2005) <papid> P05-3015 </papid>uses syntactic features in building semi-supervised ne tagger.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L817">
<title id=" I08-2080.xml">japanese named entity recognition using structural natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>*research fellow of the japan society for the promotion of science (jsps) 1bunsetsu is commonly used linguistic unit in japanese, consisting of one or more adjacent content words and zero or more following functional words.on the other hand, as for english or chinese, various ner systems have explored global information and reported their effectiveness.
</prevsent>
<prevsent>in (malouf, 2002;<papid> W02-2019 </papid>chieu and ng, 2002), information about features assigned to other instances of the same token is uti lized.</prevsent>
</prevsection>
<citsent citstr=" P05-1051 ">
(ji and grishman, 2005) <papid> P05-1051 </papid>uses the information obtained from coreference analysis for ner.</citsent>
<aftsection>
<nextsent>(mohit and hwa, 2005) <papid> P05-3015 </papid>uses syntactic features in building semi-supervised ne tagger.</nextsent>
<nextsent>in this paper, we present japanese ner system that uses global information obtained from several structural analyses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L818">
<title id=" I08-2080.xml">japanese named entity recognition using structural natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (malouf, 2002;<papid> W02-2019 </papid>chieu and ng, 2002), information about features assigned to other instances of the same token is uti lized.</prevsent>
<prevsent>(ji and grishman, 2005) <papid> P05-1051 </papid>uses the information obtained from coreference analysis for ner.</prevsent>
</prevsection>
<citsent citstr=" P05-3015 ">
(mohit and hwa, 2005) <papid> P05-3015 </papid>uses syntactic features in building semi-supervised ne tagger.</citsent>
<aftsection>
<nextsent>in this paper, we present japanese ner system that uses global information obtained from several structural analyses.
</nextsent>
<nextsent>to be more specific, our system is based on svm, recognizes nes after syntactic, case and coreference analyses and uses information obtained from these analyses and the ner results for the previous context, integrally.
</nextsent>
<nextsent>at this point, it is true that ner results are useful for syntactic,case and coreference analyses, and thus these analyses and ner should be performed in complementary way.
</nextsent>
<nextsent>however, since we focus on ner, we recognize ne after these structural analyses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L819">
<title id=" I08-2080.xml">japanese named entity recognition using structural natural language processing </title>
<section> ner using structural information.  </section>
<citcontext>
<prevsection>
<prevsent>in order to deal with such problem, we use the information obtained from the head verb: syntactic feature and case frame feature.
</prevsent>
<prevsent>4.1 outline of our ner system.
</prevsent>
</prevsection>
<citsent citstr=" W02-1036 ">
our ner system performs the chunking process based on morpheme units because character-basedmethods do not outperform morpheme-based methods (masayuki and matsumoto, 2003) and are not suitable for considering wider context.a wide variety of trainable models have been applied to japanese ner task, including maximum entropy models (utsuro et al, 2002), <papid> W02-1036 </papid>support vector machines (nakano and hirai, 2004; yamada, 2007) and conditional random fields (fukuoka, 2006).</citsent>
<aftsection>
<nextsent>our system applies svms because, for japanese ner, svm-based systems achieved higher f-measure than the other systems.
</nextsent>
<nextsent>(isozaki and kazawa, 2003) proposed an svm-based ner system with viterbisearch, which outperforms an svm-based ner system with sequential determination, and our system basically follows this system.
</nextsent>
<nextsent>our ner system consists of the following four steps: 1.
</nextsent>
<nextsent>morphological analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L820">
<title id=" I08-2080.xml">japanese named entity recognition using structural natural language processing </title>
<section> ner using structural information.  </section>
<citcontext>
<prevsection>
<prevsent>respectively.
</prevsent>
<prevsent>4.3 syntactic, case and coreference analyses.
</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
syntactic analysis syntactic analysis is performed by using the japanese parser knp (kurohashi and nagao, 1994).<papid> J94-4001 </papid></citsent>
<aftsection>
<nextsent>knp employs some heuristic rules to determine the head of modifier.
</nextsent>
<nextsent>case analysis case analysis is performed by using the system proposed in (kawahara and kurohashi, 2002).<papid> C02-1122 </papid></nextsent>
<nextsent>this system uses japanese case frames that are automatically constructed from large corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L821">
<title id=" I08-2080.xml">japanese named entity recognition using structural natural language processing </title>
<section> ner using structural information.  </section>
<citcontext>
<prevsection>
<prevsent>syntactic analysis syntactic analysis is performed by using the japanese parser knp (kurohashi and nagao, 1994).<papid> J94-4001 </papid></prevsent>
<prevsent>knp employs some heuristic rules to determine the head of modifier.</prevsent>
</prevsection>
<citsent citstr=" C02-1122 ">
case analysis case analysis is performed by using the system proposed in (kawahara and kurohashi, 2002).<papid> C02-1122 </papid></citsent>
<aftsection>
<nextsent>this system uses japanese case frames that are automatically constructed from large corpus.
</nextsent>
<nextsent>to utilize case analysis for ner, we constructed case frames that include ne labels in advance.
</nextsent>
<nextsent>we explain details in section 4.4.2.
</nextsent>
<nextsent>the case analysis is applied to each predicate in an input sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L825">
<title id=" I08-1058.xml">multilingual text entry using automatic language detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(murthy and kumar, 2006) studied the language identification problem based on small samples in several indian languages when machine learning techniques are used.
</prevsent>
<prevsent>although they report high accuracy for the method they developed, their system handles switches between two indian languages only.
</prevsent>
</prevsection>
<citsent citstr=" P05-2023 ">
in contrast, ty peany can handle any number of languages mixed within text.(alex, 2005) <papid> P05-2023 </papid>addresses related task, called foreign inclusion detection (fid).</citsent>
<aftsection>
<nextsent>the task is to find foreign (i.e., english) inclusions, such as foreign noun compounds, within monolingual (i.e., ger man) texts.
</nextsent>
<nextsent>alex reported that the use of fid tobuild polyglot tts synthesizer was also considered (pfister and romsdorfer, 2003), (marcadet etal., 2005).
</nextsent>
<nextsent>recently, alex used fid to improve parsing accuracy (alex et al, 2007).<papid> D07-1016 </papid></nextsent>
<nextsent>while fid relies on large corpora and lexicons, our model requires only small corpora since it incorporates the transition probabilities of language switching.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L826">
<title id=" I08-1058.xml">multilingual text entry using automatic language detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task is to find foreign (i.e., english) inclusions, such as foreign noun compounds, within monolingual (i.e., ger man) texts.
</prevsent>
<prevsent>alex reported that the use of fid tobuild polyglot tts synthesizer was also considered (pfister and romsdorfer, 2003), (marcadet etal., 2005).
</prevsent>
</prevsection>
<citsent citstr=" D07-1016 ">
recently, alex used fid to improve parsing accuracy (alex et al, 2007).<papid> D07-1016 </papid></citsent>
<aftsection>
<nextsent>while fid relies on large corpora and lexicons, our model requires only small corpora since it incorporates the transition probabilities of language switching.
</nextsent>
<nextsent>also, while fid is specific to alphabetic languages, we made our method language-independent by taking into consideration the inclusion problem at the key entry level.in the following, we introduce the design of ty peany, explain its underlying model, and report on our evaluation of its effectiveness.
</nextsent>
<nextsent>figure 1 shows an example of text written in english, japanese and russian.
</nextsent>
<nextsent>the strings shown between the lines indicate the roman transcription of japanese and russian words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L827">
<title id=" I05-6011.xml">annotating honorifics denoting social ranking of referents </title>
<section> earlier studies.  </section>
<citcontext>
<prevsection>
<prevsent>the japanese honorific ation system has been studied extensively in linguistics, particularly in sociolinguistics.
</prevsent>
<prevsent>because of its importance and frequent use in the japanese language, there has been some related work in nlp; within the framework of grammar formalism, gpsg by ikeya (1983), jpsg by gunji (1987), and more recently hpsg by siegel (2000); work from view point of resolving zero pronouns in dialogues by dohsaka (1990).
</prevsent>
</prevsection>
<citsent citstr=" W02-1210 ">
of these, the most thorough work on japanese honorific ation is seen in jacy, japanese hpsg grammar (siegel 2000, siegel and bender 2002).<papid> W02-1210 </papid></citsent>
<aftsection>
<nextsent>it extends the backgr (owe ? honour) relation (pollard and sag 1994), which accounts only for subject honorifics, to accommodate the other types of honorific ation used in japanese (see section 3.1 for the types).
</nextsent>
<nextsent>the full account of the japanese honorific ation system requires syntactic and pragmatic information in many dimensions, with more input from the latter, the gathering of which is an extremely convoluted task.
</nextsent>
<nextsent>this paper builds on the basics from jacy and complements it in two ways to extend the jacy annotation presented in section 7.
</nextsent>
<nextsent>1. ranking referents in social hierarchy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L828">
<title id=" I08-2088.xml">method of selecting training data to build a compact and efficient translation model </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we then segmented the chinese words using achilles (zhang et al, 2006).
</prevsent>
<prevsent>after the segmentation, we removed all punctuation from both english and chinese corp uses and decapitalized the english corpus.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we used the preprocessed data to train the phrase-based translation model by using giza++ (och and ney, 2003) <papid> J03-1002 </papid>and the pharaoh toolkit (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>3.2.2 features we used eight features (och and ney, 2003;<papid> J03-1002 </papid>koehn et al, 2003) <papid> N03-1017 </papid>and their weights for the transla tions.</nextsent>
<nextsent>1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L829">
<title id=" I08-2088.xml">method of selecting training data to build a compact and efficient translation model </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we then segmented the chinese words using achilles (zhang et al, 2006).
</prevsent>
<prevsent>after the segmentation, we removed all punctuation from both english and chinese corp uses and decapitalized the english corpus.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
we used the preprocessed data to train the phrase-based translation model by using giza++ (och and ney, 2003) <papid> J03-1002 </papid>and the pharaoh toolkit (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>3.2.2 features we used eight features (och and ney, 2003;<papid> J03-1002 </papid>koehn et al, 2003) <papid> N03-1017 </papid>and their weights for the transla tions.</nextsent>
<nextsent>1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L832">
<title id=" I08-2088.xml">method of selecting training data to build a compact and efficient translation model </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>8.
</prevsent>
<prevsent>target language model probability (weight =.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
0.5)according to previous study, the minimum error rate training (mert) (och, 2003), <papid> P03-1021 </papid>which is the optimization of feature weights by maximizing the bleu score on the development set, can improve the performance of system.</citsent>
<aftsection>
<nextsent>however, the rangeof improvement is not stable because the mert algorithm uses random numbers while searching for the optimum weights.
</nextsent>
<nextsent>as previously mentioned, we used fixed weights instead of weights optimized by mert to remove its unstable effects and simplify the evaluation.
</nextsent>
<nextsent>3.2.3 linear interpolation of translation models the experiments used four features (feature # 1to 4 in 3.2.2) as targets for integration.
</nextsent>
<nextsent>for each feature, we applied linear interpolation by using the following formula.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L833">
<title id=" I08-2088.xml">method of selecting training data to build a compact and efficient translation model </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we used thesame language model for the entire translation experiment.
</prevsent>
<prevsent>3.3 experimental results.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
3.3.1 translation performance figure 2 and 3 plot the results of the experiments.the horizontal axis represents the weight for the out of-domain translation model, and the vertical axis 15% 16% 17% 18% 19% 20% 21% 22% 23% 24% 25% 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 weight for out-of-domain translation model l u c re 400 800 1.2 1.6 2.5 mfigure 2: results of data selection and linear interpolation (bleu)represents the automatic metric of translation quality (bleu score (papineni et al, 2002) <papid> P02-1040 </papid>in fig.</citsent>
<aftsection>
<nextsent>2, and nist score (nist, 2002) in fig.
</nextsent>
<nextsent>3).
</nextsent>
<nextsent>thick straight broken lines in the figures indicate automatic scores of baseline system.
</nextsent>
<nextsent>this base line system was trained on the in-domain and all of the out of-domain corpus (2.5m sentence pairs).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L834">
<title id=" I08-1060.xml">bilingual synonym identification with spelling variations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there have been many approaches for detecting synonyms and constructing thesauri.
</prevsent>
<prevsent>two main resources for synonym extraction are large text corpora and dictionaries.many studies extract synonyms from large monolingual corpora by using context information around target terms (croach and yang, 1992; park and choi,1996; waterman, 1996; curran, 2004).
</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
some researchers (hindle, 1990; <papid> P90-1034 </papid>grefenstette, 1994; lin, 1998) <papid> P98-2127 </papid>classify terms by similarities based on their distributional syntactic patterns.</citsent>
<aftsection>
<nextsent>these methods of ten extract not only synonyms, but also semantically related terms, such as antonyms, hyponyms and coordinate terms such as cat?
</nextsent>
<nextsent>and dog.?
</nextsent>
<nextsent>some studies make use of bilingual corpora or dictionaries to find synonyms in target language (barzilay and mckeown, 2001; <papid> P01-1008 </papid>shimohata and sumita, 2002; wu and zhou, 2003; lin et al, 2003).</nextsent>
<nextsent>lin et al (2003) chose set of synonym candidates for term by using bilingual dictionary and computing distributional similarities in the candidate setto extract synonyms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L835">
<title id=" I08-1060.xml">bilingual synonym identification with spelling variations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there have been many approaches for detecting synonyms and constructing thesauri.
</prevsent>
<prevsent>two main resources for synonym extraction are large text corpora and dictionaries.many studies extract synonyms from large monolingual corpora by using context information around target terms (croach and yang, 1992; park and choi,1996; waterman, 1996; curran, 2004).
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
some researchers (hindle, 1990; <papid> P90-1034 </papid>grefenstette, 1994; lin, 1998) <papid> P98-2127 </papid>classify terms by similarities based on their distributional syntactic patterns.</citsent>
<aftsection>
<nextsent>these methods of ten extract not only synonyms, but also semantically related terms, such as antonyms, hyponyms and coordinate terms such as cat?
</nextsent>
<nextsent>and dog.?
</nextsent>
<nextsent>some studies make use of bilingual corpora or dictionaries to find synonyms in target language (barzilay and mckeown, 2001; <papid> P01-1008 </papid>shimohata and sumita, 2002; wu and zhou, 2003; lin et al, 2003).</nextsent>
<nextsent>lin et al (2003) chose set of synonym candidates for term by using bilingual dictionary and computing distributional similarities in the candidate setto extract synonyms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L836">
<title id=" I08-1060.xml">bilingual synonym identification with spelling variations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>these methods of ten extract not only synonyms, but also semantically related terms, such as antonyms, hyponyms and coordinate terms such as cat?
</prevsent>
<prevsent>and dog.?
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
some studies make use of bilingual corpora or dictionaries to find synonyms in target language (barzilay and mckeown, 2001; <papid> P01-1008 </papid>shimohata and sumita, 2002; wu and zhou, 2003; lin et al, 2003).</citsent>
<aftsection>
<nextsent>lin et al (2003) chose set of synonym candidates for term by using bilingual dictionary and computing distributional similarities in the candidate setto extract synonyms.
</nextsent>
<nextsent>they adopt the bilingual information to exclude non-synonyms (e.g., antonyms and hyponyms) that may be used in the similar contexts.
</nextsent>
<nextsent>although they make use of bilingual dictionaries, this study aims at finding bilingual synonyms directly.in the approaches based on monolingual dictionaries, the similarities of definitions of lexical items are important clues for identifying synonyms (blon del et al, 2004; muller et al, 2006).<papid> W06-3811 </papid></nextsent>
<nextsent>for instance,blondel et al (2004) constructed an associated dictionary graph whose vertices are the terms, and whose edges from v1 to v2 represent occurrence of v2 in the definition for v1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L837">
<title id=" I08-1060.xml">bilingual synonym identification with spelling variations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>lin et al (2003) chose set of synonym candidates for term by using bilingual dictionary and computing distributional similarities in the candidate setto extract synonyms.
</prevsent>
<prevsent>they adopt the bilingual information to exclude non-synonyms (e.g., antonyms and hyponyms) that may be used in the similar contexts.
</prevsent>
</prevsection>
<citsent citstr=" W06-3811 ">
although they make use of bilingual dictionaries, this study aims at finding bilingual synonyms directly.in the approaches based on monolingual dictionaries, the similarities of definitions of lexical items are important clues for identifying synonyms (blon del et al, 2004; muller et al, 2006).<papid> W06-3811 </papid></citsent>
<aftsection>
<nextsent>for instance,blondel et al (2004) constructed an associated dictionary graph whose vertices are the terms, and whose edges from v1 to v2 represent occurrence of v2 in the definition for v1.
</nextsent>
<nextsent>they choose synonyms from the graph by collecting terms pointed to and from the same terms.another strategy for finding synonyms is to consider the terms themselves.
</nextsent>
<nextsent>we divide it into two approaches: rule-based and distance-based.
</nextsent>
<nextsent>rule-based approaches implement rules with language-specific patterns and detect variations by applying rules to terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L838">
<title id=" I08-1060.xml">bilingual synonym identification with spelling variations </title>
<section> bilingual synonyms and translation.  </section>
<citcontext>
<prevsection>
<prevsent>at first we prepare bilingual lexicon with synonymous information as training data, and generate list consisting of all bilingual lexical item figure 3: overview of our framework pairs in the bilingual lexicon.
</prevsent>
<prevsent>the presence or absence of bilingual synonymous relations is attached to each element of the list.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
then, we build classifier learned by training data, using maximum entropy model (berger et al, 1996) <papid> J96-1002 </papid>and the features related to spelling variations in table 3.</citsent>
<aftsection>
<nextsent>we apply some preprocessings for extracting some features.
</nextsent>
<nextsent>for english, we transform all terms into lower-case, and do not apply any other transformations such as tokenization by symbols.
</nextsent>
<nextsent>for japanese, we apply morphological analyzer ju man (kurohashi et al, 1994) and obtain hiraganarepresentations5 as much as possible6.
</nextsent>
<nextsent>we may require other language-specific preprocessings for applying this method to other languages.we employed binary or real-valued features described in table 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L839">
<title id=" I08-1060.xml">bilingual synonym identification with spelling variations </title>
<section> bilingual synonyms and translation.  </section>
<citcontext>
<prevsection>
<prevsent>460 h1f , h1e : agreement of the first characters whether the first characters match or not h2f , h2e : normalized edit distance 1?
</prevsent>
<prevsent>ed(w,w ?) max(|w|,|w?|) , where ed(w,w ?) is non-weighted edit distance between and w?
</prevsent>
</prevsection>
<citsent citstr=" C04-1176 ">
and |w| is the number of characters in h3f , h3e : bigram similarity |bigram(w)bigram(w ?)| max(|w|,|w?|)1 , where bigram(w) is multi set of character-based bigrams in h4f , h4e : agreement or known synonymous relation of word sub-sequences the count that sub-sequences of the target terms match as known terms or are in known synonymous relationh5f , h5e : existence of crossing bilingual lexical items for bilingual lexical items (f1, e1) and (f2, e2), whether (f1, e2) (for h5f ) or (f2, e1) (for h5e) is in the bilingual lexicon of the training set h6: acronyms whether one english term is an acronym for another (schwartz and hearst, 2003) h7: katakana variants whether one japanese term is katakana variant for another (masuyama et al, 2004) <papid> C04-1176 </papid>table 3: features used for identifying bilingual synonym pairs hif is the feature value when the terms and w?(?</citsent>
<aftsection>
<nextsent>f ) are compared in the i-th feature and so as hie . h6 is only for english and h7 is only for japanese.
</nextsent>
<nextsent>identifying monolingual synonyms in each language and then merging them according to the bilingual items (monolingual?
</nextsent>
<nextsent>method).
</nextsent>
<nextsent>we implement these two approaches and compare the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L840">
<title id=" I08-1035.xml">automatic extraction of briefing templates </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(4) [pressure entity] from [location] to [location] will move [direction] across [location] [time] in the current work we address the first problem of automatically extracting domain templates from human written reports.
</prevsent>
<prevsent>we take two-step approach to the problem; first, we cluster report sentences basedon similarity and second, we extract template(s) corresponding to each cluster by aligning the instance sin the cluster.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
we experimented with two independent, arguably complementary techniques for clustering and aligning ? predicate argument based approach that extracts more general templates containing one predicate and rouge (lin, 2004) <papid> W04-1013 </papid>based 265approach that can extract templates containing multiple verbs.</citsent>
<aftsection>
<nextsent>as we will see below, both approaches show promise.
</nextsent>
<nextsent>there has been instances of template based summarization in popular information extraction (ie) evaluations like muc (marsh &amp; perzanowski, 1998; <papid> M98-1002 </papid>onyshkevych, 1994) <papid> H94-1031 </papid>and ace (ace, 2007) where hand engineered slots were to be filled for events in text; but the focus lay on template filling rather than their creation.</nextsent>
<nextsent>(riloff, 1996) describes an interesting work on the generation of extraction patterns from untagged text, but the analysis is syntactic and the patterns do not resemble the templates that we aim to extract.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L842">
<title id=" I08-1035.xml">automatic extraction of briefing templates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we experimented with two independent, arguably complementary techniques for clustering and aligning ? predicate argument based approach that extracts more general templates containing one predicate and rouge (lin, 2004) <papid> W04-1013 </papid>based 265approach that can extract templates containing multiple verbs.</prevsent>
<prevsent>as we will see below, both approaches show promise.</prevsent>
</prevsection>
<citsent citstr=" M98-1002 ">
there has been instances of template based summarization in popular information extraction (ie) evaluations like muc (marsh &amp; perzanowski, 1998; <papid> M98-1002 </papid>onyshkevych, 1994) <papid> H94-1031 </papid>and ace (ace, 2007) where hand engineered slots were to be filled for events in text; but the focus lay on template filling rather than their creation.</citsent>
<aftsection>
<nextsent>(riloff, 1996) describes an interesting work on the generation of extraction patterns from untagged text, but the analysis is syntactic and the patterns do not resemble the templates that we aim to extract.
</nextsent>
<nextsent>(yangarber et al, 2000) describe another system called exdisco, that extracts event patterns from un-annotated text starting from seed patterns.once again, the text analysis is not deep and the patterns extracted are not sentence surface forms.(collier, 1998) proposed automatic domain template extraction for ie purposes where muc type templates for particular types of events were constructed.
</nextsent>
<nextsent>the method relies on the idea from (luhn,1958) where statistically significant words of corpus were extracted.
</nextsent>
<nextsent>based on these words, sentences containing them were chosen and aligned using subject-object-verb patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L843">
<title id=" I08-1035.xml">automatic extraction of briefing templates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we experimented with two independent, arguably complementary techniques for clustering and aligning ? predicate argument based approach that extracts more general templates containing one predicate and rouge (lin, 2004) <papid> W04-1013 </papid>based 265approach that can extract templates containing multiple verbs.</prevsent>
<prevsent>as we will see below, both approaches show promise.</prevsent>
</prevsection>
<citsent citstr=" H94-1031 ">
there has been instances of template based summarization in popular information extraction (ie) evaluations like muc (marsh &amp; perzanowski, 1998; <papid> M98-1002 </papid>onyshkevych, 1994) <papid> H94-1031 </papid>and ace (ace, 2007) where hand engineered slots were to be filled for events in text; but the focus lay on template filling rather than their creation.</citsent>
<aftsection>
<nextsent>(riloff, 1996) describes an interesting work on the generation of extraction patterns from untagged text, but the analysis is syntactic and the patterns do not resemble the templates that we aim to extract.
</nextsent>
<nextsent>(yangarber et al, 2000) describe another system called exdisco, that extracts event patterns from un-annotated text starting from seed patterns.once again, the text analysis is not deep and the patterns extracted are not sentence surface forms.(collier, 1998) proposed automatic domain template extraction for ie purposes where muc type templates for particular types of events were constructed.
</nextsent>
<nextsent>the method relies on the idea from (luhn,1958) where statistically significant words of corpus were extracted.
</nextsent>
<nextsent>based on these words, sentences containing them were chosen and aligned using subject-object-verb patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L844">
<title id=" I08-1035.xml">automatic extraction of briefing templates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>based on these words, sentences containing them were chosen and aligned using subject-object-verb patterns.
</prevsent>
<prevsent>however, this method did not look at arbitrary syntactic patterns.
</prevsent>
</prevsection>
<citsent citstr=" P06-2027 ">
(filatova et al, 2006) <papid> P06-2027 </papid>improved the paradigm by looking at the most frequent verbs occurring in corpus and aligning subtrees containing the verb, by using the syntactic parses as similarity metric.</citsent>
<aftsection>
<nextsent>however, long distance dependencies of verbs with constituents were not looked at and deep semantic analysis was not performed on the sentences to findout similar verb subcategorization frames.
</nextsent>
<nextsent>in contrast, in our predicate argument based approach welook into deeper semantic structures, and align sentences not only based on similar syntactic parses,but also based on the constituents?
</nextsent>
<nextsent>roles with respect to the main predicate.
</nextsent>
<nextsent>also, they relied on typical named entities (nes) like location, organization, person etc. and included another entity that they termed as number.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L845">
<title id=" I08-1035.xml">automatic extraction of briefing templates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>also, they relied on typical named entities (nes) like location, organization, person etc. and included another entity that they termed as number.
</prevsent>
<prevsent>however, for specific domains like weather forecasts, medical reports or student reports, more varied domain entities form slots in templates, as we observe in our data; hence,existence of module handling domain specific entities become essential for such task.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
(surdeanu et al, 2003) <papid> P03-1002 </papid>identify arguments for predicates in asentence and emphasize how semantic role information may assist in ie related tasks, but their primary focus remained on the extraction of propbank (kingsbury et al, 2002) type semantic roles.</citsent>
<aftsection>
<nextsent>to our knowledge, the rouge metric has not been used for automatic extraction of templates.
</nextsent>
<nextsent>3.1 data description.
</nextsent>
<nextsent>since our focus is on creating summary items from events or structured data rather than from text, weused corpus from the domain of weather forecasts (reiter et al, 2005).
</nextsent>
<nextsent>this is freely available parallel corpus1 consisting of weather data and human written forecasts describing them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L846">
<title id=" I08-1035.xml">automatic extraction of briefing templates </title>
<section> the data.  </section>
<citcontext>
<prevsection>
<prevsent>from this set, we selected3000 for template extraction and kept aside 262 sentences for testing.
</prevsent>
<prevsent>3.2 preprocessing.
</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
for semantic analysis, we used the assert toolkit(pradhan et al, 2004) <papid> N04-1030 </papid>that produces shallow semantic parses using the propbank conventions.</citsent>
<aftsection>
<nextsent>as aby product, it also produces syntactic parses of sentences, using the charniak parser (charniak, 2001).<papid> P01-1017 </papid></nextsent>
<nextsent>for each sentence, we maintained part-of-speech tagged (leaves of the parse tree), parsed, basenp2 tagged and semantic role tagged version.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L847">
<title id=" I08-1035.xml">automatic extraction of briefing templates </title>
<section> the data.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 preprocessing.
</prevsent>
<prevsent>for semantic analysis, we used the assert toolkit(pradhan et al, 2004) <papid> N04-1030 </papid>that produces shallow semantic parses using the propbank conventions.</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
as aby product, it also produces syntactic parses of sentences, using the charniak parser (charniak, 2001).<papid> P01-1017 </papid></citsent>
<aftsection>
<nextsent>for each sentence, we maintained part-of-speech tagged (leaves of the parse tree), parsed, basenp2 tagged and semantic role tagged version.
</nextsent>
<nextsent>the basenps were retrieved by pruning the parse tree sand not by using separate np chunker.
</nextsent>
<nextsent>the reason for having basenp tagged corpus will become clear as we go into the detail of our template extraction techniques.
</nextsent>
<nextsent>figure 1 shows typical out put from the charniak parser and figure 2 shows the same tree with nodes under the basenps pruned.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L850">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, they are available forany written languages and not only for pairs of languages involving english.
</prevsent>
<prevsent>the compilation of specialized dictionaries should take into account multiword terms (mwts) that are more precise and specific to particular scientific domain than single word terms (swts).
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
the standard approach is based on lexical context analysis and relies on the simple observation that swt or mwt and its translation tend to appear in the same lexical contexts.correct results are obtained for swts with an accuracy of about 80% for the top 10-20 proposed candidates using large comparable corpora (fung, 1998; rapp, 1999; <papid> P99-1067 </papid>chiao and zweigenbaum, 2002) <papid> C02-2020 </papid>or 60% using small comparable corpora (dejeanand gaussier, 2002).</citsent>
<aftsection>
<nextsent>in comparison, the results obtained for mwts are disappointing.
</nextsent>
<nextsent>for instance, (morin et al, 2007) <papid> P07-1084 </papid>have achieved 30% and 42% precision for the top 10 and top 20 candidates in 0.84 million-word french-japanese corpus.</nextsent>
<nextsent>these results could be explained by the low frequency ofmwts compared to swts, by the lack of parallelism between the source and the target mwt extraction systems, and by the low performance of the alignment program.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L851">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, they are available forany written languages and not only for pairs of languages involving english.
</prevsent>
<prevsent>the compilation of specialized dictionaries should take into account multiword terms (mwts) that are more precise and specific to particular scientific domain than single word terms (swts).
</prevsent>
</prevsection>
<citsent citstr=" C02-2020 ">
the standard approach is based on lexical context analysis and relies on the simple observation that swt or mwt and its translation tend to appear in the same lexical contexts.correct results are obtained for swts with an accuracy of about 80% for the top 10-20 proposed candidates using large comparable corpora (fung, 1998; rapp, 1999; <papid> P99-1067 </papid>chiao and zweigenbaum, 2002) <papid> C02-2020 </papid>or 60% using small comparable corpora (dejeanand gaussier, 2002).</citsent>
<aftsection>
<nextsent>in comparison, the results obtained for mwts are disappointing.
</nextsent>
<nextsent>for instance, (morin et al, 2007) <papid> P07-1084 </papid>have achieved 30% and 42% precision for the top 10 and top 20 candidates in 0.84 million-word french-japanese corpus.</nextsent>
<nextsent>these results could be explained by the low frequency ofmwts compared to swts, by the lack of parallelism between the source and the target mwt extraction systems, and by the low performance of the alignment program.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L852">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the standard approach is based on lexical context analysis and relies on the simple observation that swt or mwt and its translation tend to appear in the same lexical contexts.correct results are obtained for swts with an accuracy of about 80% for the top 10-20 proposed candidates using large comparable corpora (fung, 1998; rapp, 1999; <papid> P99-1067 </papid>chiao and zweigenbaum, 2002) <papid> C02-2020 </papid>or 60% using small comparable corpora (dejeanand gaussier, 2002).</prevsent>
<prevsent>in comparison, the results obtained for mwts are disappointing.</prevsent>
</prevsection>
<citsent citstr=" P07-1084 ">
for instance, (morin et al, 2007) <papid> P07-1084 </papid>have achieved 30% and 42% precision for the top 10 and top 20 candidates in 0.84 million-word french-japanese corpus.</citsent>
<aftsection>
<nextsent>these results could be explained by the low frequency ofmwts compared to swts, by the lack of parallelism between the source and the target mwt extraction systems, and by the low performance of the alignment program.
</nextsent>
<nextsent>for swts, the process is in two steps: looking in dictionary, and if no direct translation is available, starting the contextual analysis.
</nextsent>
<nextsent>looking in the dictionary gives low results for mwts: 1% compared to 30% for french and 20%for japanese swts (morin and daille, 2006).
</nextsent>
<nextsent>to extend the coverage of the bilingual dictionary, an intermediate step is added between looking in the dictionary and the contextual analysis that will propose several translation candidates to compare with the target mwts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L853">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>looking in the dictionary gives low results for mwts: 1% compared to 30% for french and 20%for japanese swts (morin and daille, 2006).
</prevsent>
<prevsent>to extend the coverage of the bilingual dictionary, an intermediate step is added between looking in the dictionary and the contextual analysis that will propose several translation candidates to compare with the target mwts.
</prevsent>
</prevsection>
<citsent citstr=" P97-1063 ">
these candidate translations are obtained thanks to compositional translation method (melamed, 1997; <papid> P97-1063 </papid>grefenstette, 1999).</citsent>
<aftsection>
<nextsent>this method reveals some limits when mwts in the source and the target languages do not share the same syntactic patterns.in this paper, we put forward an extended compo 95 sit ional method that bridges the gap between mwtsof different syntactic structures through morphological links.
</nextsent>
<nextsent>we experiment within this method of french-japanese lexical alignment, using multilingual terminology mining chain made up of two terminology extraction systems; one in each language, and an alignment program.
</nextsent>
<nextsent>the term extraction systems are publicly available and both extract mwts.
</nextsent>
<nextsent>the alignment program makes use of the direct context-vector approach (fung, 1998; rapp, 1999).<papid> P99-1067 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L856">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> multilingual terminology mining chain.  </section>
<citcontext>
<prevsection>
<prevsent>the results show an improvement of 33% inthe translation of mwts that demonstrate the efficiency of the morphologically based-method for lexical alignment.
</prevsent>
<prevsent>taking comparable corpora as input, the multilingual terminology mining chain outputs list of single- and multi-word candidate terms along with their candidate translations (see figure 1).
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
this chain performs contextual analysis that adapts the direct context-vector approach (rapp, 1995; <papid> P95-1050 </papid>fungand mckeown, 1997) for swts to mwts.</citsent>
<aftsection>
<nextsent>it consists of the following five steps: 1.
</nextsent>
<nextsent>for each language, the documents are cleaned,.
</nextsent>
<nextsent>tokenized, tagged and lemmatized.
</nextsent>
<nextsent>for french, brills pos tagger1 and the flem lemmatiser2 are used, and for japanese, chasen3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L857">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> multilingual terminology mining chain.  </section>
<citcontext>
<prevsection>
<prevsent>tokenized, tagged and lemmatized.
</prevsent>
<prevsent>for french, brills pos tagger1 and the flem lemmatiser2 are used, and for japanese, chasen3.
</prevsent>
</prevsection>
<citsent citstr=" W04-1814 ">
we then extract the mwts and their variations using the acabit terminology extraction system available for french4 (daille, 2003), english and japanese5 (takeuchi et al, 2004).<papid> W04-1814 </papid></citsent>
<aftsection>
<nextsent>(from now on, we will refer to lexical units as words, swts or mwts).
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>we collect all the lexical units in the context of.
</nextsent>
<nextsent>each lexical unit  and count their occurrence frequency in window of  words around  . for each lexical unit  of the source and the target languages, we obtain context vector 1http://www.atilf.fr/winbrill/ 2http://www.univ-nancy2.fr/pers/namer/ 3http://chasen-legacy.sourceforge.jp/ 4http://www.sciences.univ-nantes.fr/ info/perso/permanents/daille/ and release for mandriva linux.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L858">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> multilingual terminology mining chain.  </section>
<citcontext>
<prevsection>
<prevsent>we collect all the lexical units in the context of.
</prevsent>
<prevsent>each lexical unit  and count their occurrence frequency in window of  words around  . for each lexical unit  of the source and the target languages, we obtain context vector 1http://www.atilf.fr/winbrill/ 2http://www.univ-nancy2.fr/pers/namer/ 3http://chasen-legacy.sourceforge.jp/ 4http://www.sciences.univ-nantes.fr/ info/perso/permanents/daille/ and release for mandriva linux.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
5http://cl.cs.okayama-u.ac.jp/rsc/ jacabit/  which gathers the set of co-occurrence units  associated with the number of times that  and  occur together   . in order to identify specific words in the lexical context andto reduce word-frequency effects, we normalize context vectors using an association score such as mutual information (fano, 1961) or log-likelihood (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>lexical units of the source context vector.
</nextsent>
<nextsent>if the bilingual dictionary provides several translations for lexical unit, we consider all of thembut weigh the different translations by their frequency in the target language.
</nextsent>
<nextsent>4.
</nextsent>
<nextsent>for lexical unit to be translated, we com-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L860">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> multilingual terminology mining chain.  </section>
<citcontext>
<prevsection>
<prevsent>in this approach, the translation of the lexical unitsof the context vectors (step 3 of the previous approach), which depends on the coverage of the bilingual dictionary vis-a`-vis the corpus, is the most important step: the greater the number of elements translated in the context vector, the more discriminating the context vector in selecting translation sin the target language.
</prevsent>
<prevsent>since the lexical units refer to swts and mwts, the dictionary must contain many entries which occur in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" C02-1166 ">
for swts, combining general bilingual dictionary with specialized bilingual dictionary or multilingual thesaurus to translate context vectors ensures that much of their elements will be translated (chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>dejean et al, 2002).<papid> C02-1166 </papid></citsent>
<aftsection>
<nextsent>for mwt to be translated, steps 3 to 5 could be avoided thanks to compositional method that will propose several translation candidates to directly compare with the target mwts identified in step 1.
</nextsent>
<nextsent>more over, the compositional method is useful in step 3 to compensate for the bilingual dictionary when themulti-word units of the context vector are not directly translated.
</nextsent>
<nextsent>96 dictionary bilingual japanese documents french documents terminology extraction terminology extraction lexical context extraction lexical context extraction process translated terms to be translations candidate haver sting lexical alignment the web documents figure 1: architecture of the multilingual terminology mining chain
</nextsent>
<nextsent>in order to increase the coverage of the dictionary formwts that could not be directly translated, we generated possible translations by using default compositional method (melamed, 1997; <papid> P97-1063 </papid>grefenstette, 1999).for each element of the mwt found in the bilingual dictionary, we generated all the translated combinations identified by the terminology extraction system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L863">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> default compositional method.  </section>
<citcontext>
<prevsection>
<prevsent>in the above example, only one term for each element was identified by the japanese extraction system: fl ffi  . in this approach, when it is not possible to translate all parts of an mwt, or when the translated combinations are not identified by the extraction system, the mwt is 6the french word order is reversed to take into account the different constraints between french and japanese.
</prevsent>
<prevsent>not taken into account in the translation step.
</prevsent>
</prevsection>
<citsent citstr=" E06-1029 ">
chronique fatigue ! fi  flffi  ! fi ! flffi ! ! fi  flffi  ! fi # flffi # table 1: illustration of the compositional method (the underlined japanese mwt actually exists) this approach also differs from that used by(robitaille et al, 2006) <papid> E06-1029 </papid>for french-japanese trans lation.</citsent>
<aftsection>
<nextsent>they first decompose the french mwtinto combinations of shorter multi-word unit elements.
</nextsent>
<nextsent>this approach makes the direct translation of subpart of the mwt possible if it is present in the bilingual dictionary.
</nextsent>
<nextsent>for mwts of length  , (robitaille et al, 2006) <papid> E06-1029 </papid>produce allthe combinations of shorter multi-word unit elements of length less than or equal to  . for 97 example, the french mwt syndrome de fatiguechronique (chronic fatigue disorder) yields the following four combinations: i) $ syndrome de fatigue chronique % , ii) $ syndrome de fatigue %&amp;$ chronique % , iii) $ syndrome % $ fatigue chronique % and iv) $ syndrome %$ fatigue %($ chronique % . we limit ourselves to the combination of type iv) above since 90% of the french candidate terms provided by the term extraction process after clustering are only composed of two content words.</nextsent>
<nextsent>the compositional translation presents problems which have been reported by (baldwin and tanaka, 2004; <papid> W04-0404 </papid>brown et al, 1993): <papid> J93-2003 </papid>fertility swts and mwts are not translated by term of same length.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L867">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> pattern switching.  </section>
<citcontext>
<prevsection>
<prevsent>this approach makes the direct translation of subpart of the mwt possible if it is present in the bilingual dictionary.
</prevsent>
<prevsent>for mwts of length  , (robitaille et al, 2006) <papid> E06-1029 </papid>produce allthe combinations of shorter multi-word unit elements of length less than or equal to  . for 97 example, the french mwt syndrome de fatiguechronique (chronic fatigue disorder) yields the following four combinations: i) $ syndrome de fatigue chronique % , ii) $ syndrome de fatigue %&amp;$ chronique % , iii) $ syndrome % $ fatigue chronique % and iv) $ syndrome %$ fatigue %($ chronique % . we limit ourselves to the combination of type iv) above since 90% of the french candidate terms provided by the term extraction process after clustering are only composed of two content words.</prevsent>
</prevsection>
<citsent citstr=" W04-0404 ">
the compositional translation presents problems which have been reported by (baldwin and tanaka, 2004; <papid> W04-0404 </papid>brown et al, 1993): <papid> J93-2003 </papid>fertility swts and mwts are not translated by term of same length.</citsent>
<aftsection>
<nextsent>for instance, the french swt hypertension (hypertension) is translated by the japanese mwt ) * + (here the kanji) (taka) means high and the term *+ (ketsu atsu) means blood pressure).pattern switching mwts in the source and the target language do not share the same syntacticpatterns.
</nextsent>
<nextsent>for instance, the french mwt cel lule graisseuse (fat cell) of adj structure is translated by the japanese mwt , -/./0 of n structure where the french noun celluleis translated by the japanese noun .10 (sai boo - cellule - cell) and the french adjective graisseuse by the japanese noun , - (shiboo - graisse - fat).
</nextsent>
<nextsent>foreign name when proper name is part of the mwt, it is not always translated: within the french mwt syndrome de cushing (cush ing syndrome), cushing is either transliterated 2346587:9 ; = or remains unchanged cushing 9 ;= . the foreign name cushing is of course not present in the dictionary.the pattern switching problem involves the ad jective/noun and the noun/verb part-of-speech switches.
</nextsent>
<nextsent>the adjective/noun switch commonly involves relational adjective (adjr).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L868">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> pattern switching.  </section>
<citcontext>
<prevsection>
<prevsent>this approach makes the direct translation of subpart of the mwt possible if it is present in the bilingual dictionary.
</prevsent>
<prevsent>for mwts of length  , (robitaille et al, 2006) <papid> E06-1029 </papid>produce allthe combinations of shorter multi-word unit elements of length less than or equal to  . for 97 example, the french mwt syndrome de fatiguechronique (chronic fatigue disorder) yields the following four combinations: i) $ syndrome de fatigue chronique % , ii) $ syndrome de fatigue %&amp;$ chronique % , iii) $ syndrome % $ fatigue chronique % and iv) $ syndrome %$ fatigue %($ chronique % . we limit ourselves to the combination of type iv) above since 90% of the french candidate terms provided by the term extraction process after clustering are only composed of two content words.</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the compositional translation presents problems which have been reported by (baldwin and tanaka, 2004; <papid> W04-0404 </papid>brown et al, 1993): <papid> J93-2003 </papid>fertility swts and mwts are not translated by term of same length.</citsent>
<aftsection>
<nextsent>for instance, the french swt hypertension (hypertension) is translated by the japanese mwt ) * + (here the kanji) (taka) means high and the term *+ (ketsu atsu) means blood pressure).pattern switching mwts in the source and the target language do not share the same syntacticpatterns.
</nextsent>
<nextsent>for instance, the french mwt cel lule graisseuse (fat cell) of adj structure is translated by the japanese mwt , -/./0 of n structure where the french noun celluleis translated by the japanese noun .10 (sai boo - cellule - cell) and the french adjective graisseuse by the japanese noun , - (shiboo - graisse - fat).
</nextsent>
<nextsent>foreign name when proper name is part of the mwt, it is not always translated: within the french mwt syndrome de cushing (cush ing syndrome), cushing is either transliterated 2346587:9 ; = or remains unchanged cushing 9 ;= . the foreign name cushing is of course not present in the dictionary.the pattern switching problem involves the ad jective/noun and the noun/verb part-of-speech switches.
</nextsent>
<nextsent>the adjective/noun switch commonly involves relational adjective (adjr).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L869">
<title id=" I08-1013.xml">an effective compositional model for lexical alignment </title>
<section> morphologically-based compositional.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, if index glycemique (glycemic index) is french mwt, the mwu index de la glycemie (lit.
</prevsent>
<prevsent>index of the glycemia) does not appear in the french corpus.
</prevsent>
</prevsection>
<citsent citstr=" J97-3003 ">
the stripping-recoding rules could be manually encoded, mined from monolingual corpus usinga learning method such as (mikheev, 1997), <papid> J97-3003 </papid>or supplied by source terminology extraction system that handles morphological variations.</citsent>
<aftsection>
<nextsent>for such system, mwt is canonical form which merges several synonymic variations.
</nextsent>
<nextsent>for instance, the french mwt exc es pond eral (overweight) is the canonical form of the following variants: exc  es pond eral (overweight) of adj structure, exc  es de poids (overweight) of prep structure.
</nextsent>
<nextsent>it is this last method that we used for our experiment.
</nextsent>
<nextsent>in this section, we will outline the different linguistic resources used for our experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L870">
<title id=" I08-1017.xml">a new approach to automatic document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>document summarization has been rapidly evolving sub field of information retrieval (ir) since (luhn, 1958).
</prevsent>
<prevsent>a summary can be loosely defined as text that is produced from one or more texts and conveys important information of the original text(s).
</prevsent>
</prevsection>
<citsent citstr=" J02-4001 ">
usually it is no longer than half of the original text(s) or, significantly less (radev et al., 2002).<papid> J02-4001 </papid></citsent>
<aftsection>
<nextsent>recently, many evaluation competitions (like the document understanding conference duc http://duc.nist.gov?, in the style of nists trec), provided some sets of training corpus.
</nextsent>
<nextsent>it is obvious that, in the age of information explosion, document summarization will be greatly helpful to the internet users; besides, the techniques it uses can also find their applications in speech techniques and multimedia document retrieval, etc. the approach to summarizing can be categorized in many ways.
</nextsent>
<nextsent>some of them are: 1) indicative, informative and evaluative, according to func tionality; 2) single-document and multi-document, according to the amount of input documents; 3) generic and query-oriented, according to applications.
</nextsent>
<nextsent>yet the taxonomy currently widely employed is to categorize summarization into abstrac tive and extractive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L872">
<title id=" I08-1017.xml">a new approach to automatic document summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most supervised extractive methods nowadays focus on finding powerful machine learning algorithms that can properly combine these features.
</prevsent>
<prevsent>bayesian classifier was first applied to summarization by (pedersen and chen, 1995), the authors claimed that the corpus-trained feature weights were in agreement with (edmundson, 1969), which employed subjective combination of weighted features.
</prevsent>
</prevsection>
<citsent citstr=" A97-1049 ">
another usage of the nave bayesian model in summarization can be found in (aone et al., 1997).<papid> A97-1049 </papid></citsent>
<aftsection>
<nextsent>bayesian model treats each sentence individually, and misses the intrinsic connection between the sentences.
</nextsent>
<nextsent>(yeh et al, 2005) employed genetic algorithm to calculate the belief or score of each sentence belonging to the summary, but it also bears this shortcoming.
</nextsent>
<nextsent>to overcome this independence defect, (conroy and oleary, 2001) pioneered in deeming this problem as sequence labeling problem.
</nextsent>
<nextsent>the authors used hmm, which has fewer independent assumptions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L873">
<title id=" I05-1057.xml">two phase biomedical named entity recognition using a hybrid method </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>baseline2+crf is model exploiting crfs and baseline2+crf+fst is model using crfand fst as postprocessing.as shown in table 9, the crfs based 656 s. kim et al table 9.
</prevsent>
<prevsent>f-score for different methods method recall precision f-score baseline1(salientnp ) 66.21 66.34 66.27 baseline2(all) 68.51 67.58 68.04 baseline2 + fst 68.89 68.53 68.71 baseline2 + me markov 70.30 67.65 68.95 baseline2 + me markov + fst 70.61 68.40 69.49 baseline2 + crf 72.44 68.77 70.56 baseline2 + crf + fst 72.77 69.68 71.19 table 10.
</prevsent>
</prevsection>
<citsent citstr=" W04-1217 ">
comparisons with other systems system precision recall f-score zhou et. al (2004) 69.42 75.99 72.55 our system 72.77 69.68 71.19 finkel et. al (2004) <papid> W04-1217 </papid>71.62 68.56 70.06 settles (2004) <papid> W04-1221 </papid>70.0 69.0 69.5 model outperforms the me based model.</citsent>
<aftsection>
<nextsent>our system reached f-score 71.19% on the baseline2 + crf + fst model.
</nextsent>
<nextsent>table 10 shows the comparison with top-ranked systems in jnlpba 2004 shared task.
</nextsent>
<nextsent>the top-ranked systems made use of external knowledge fromgazetteers and abbreviation handling routines, which were reported to be effective.
</nextsent>
<nextsent>zhou et. al reported the usage of gazette ers and abbreviation handling improves the performance of the ner system by 4.8% in f-score [13].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L874">
<title id=" I05-1057.xml">two phase biomedical named entity recognition using a hybrid method </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>baseline2+crf is model exploiting crfs and baseline2+crf+fst is model using crfand fst as postprocessing.as shown in table 9, the crfs based 656 s. kim et al table 9.
</prevsent>
<prevsent>f-score for different methods method recall precision f-score baseline1(salientnp ) 66.21 66.34 66.27 baseline2(all) 68.51 67.58 68.04 baseline2 + fst 68.89 68.53 68.71 baseline2 + me markov 70.30 67.65 68.95 baseline2 + me markov + fst 70.61 68.40 69.49 baseline2 + crf 72.44 68.77 70.56 baseline2 + crf + fst 72.77 69.68 71.19 table 10.
</prevsent>
</prevsection>
<citsent citstr=" W04-1221 ">
comparisons with other systems system precision recall f-score zhou et. al (2004) 69.42 75.99 72.55 our system 72.77 69.68 71.19 finkel et. al (2004) <papid> W04-1217 </papid>71.62 68.56 70.06 settles (2004) <papid> W04-1221 </papid>70.0 69.0 69.5 model outperforms the me based model.</citsent>
<aftsection>
<nextsent>our system reached f-score 71.19% on the baseline2 + crf + fst model.
</nextsent>
<nextsent>table 10 shows the comparison with top-ranked systems in jnlpba 2004 shared task.
</nextsent>
<nextsent>the top-ranked systems made use of external knowledge fromgazetteers and abbreviation handling routines, which were reported to be effective.
</nextsent>
<nextsent>zhou et. al reported the usage of gazette ers and abbreviation handling improves the performance of the ner system by 4.8% in f-score [13].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L875">
<title id=" I08-1042.xml">heterogeneous automatic mt evaluation through non parametric metric combinations </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>combining different metrics into single measure of quality seems the most direct and natural way to improve over the quality of individual metrics.
</prevsent>
</prevsection>
<citsent citstr=" N07-1006 ">
recently, several approaches have been suggested (kulesza and shieber, 2004; liu and gildea, 2007; <papid> N07-1006 </papid>albrecht and hwa, 2007<papid> P07-1111 </papid>a).</citsent>
<aftsection>
<nextsent>although based on different assumptions, these approaches share the common characteristic of beingparametric.
</nextsent>
<nextsent>their models involve number of parameters whose weight must be adjusted.
</nextsent>
<nextsent>as an alternative, in this work, we study the behaviour of non-parametric schemes, in which metrics are combined without having to adjust their relative importance.
</nextsent>
<nextsent>besides, rather than limiting to the lexical dimension, we work on wideset of metrics operating at different linguistic levels (e.g., lexical, syntactic and se mantic).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L877">
<title id=" I08-1042.xml">heterogeneous automatic mt evaluation through non parametric metric combinations </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>combining different metrics into single measure of quality seems the most direct and natural way to improve over the quality of individual metrics.
</prevsent>
</prevsection>
<citsent citstr=" P07-1111 ">
recently, several approaches have been suggested (kulesza and shieber, 2004; liu and gildea, 2007; <papid> N07-1006 </papid>albrecht and hwa, 2007<papid> P07-1111 </papid>a).</citsent>
<aftsection>
<nextsent>although based on different assumptions, these approaches share the common characteristic of beingparametric.
</nextsent>
<nextsent>their models involve number of parameters whose weight must be adjusted.
</nextsent>
<nextsent>as an alternative, in this work, we study the behaviour of non-parametric schemes, in which metrics are combined without having to adjust their relative importance.
</nextsent>
<nextsent>besides, rather than limiting to the lexical dimension, we work on wideset of metrics operating at different linguistic levels (e.g., lexical, syntactic and se mantic).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L879">
<title id=" I08-1042.xml">heterogeneous automatic mt evaluation through non parametric metric combinations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there exist large number of metrics based on different similarity criteria.
</prevsent>
<prevsent>by far, the most widely used metric in recent literature is bleu (papineni et al, 2001).
</prevsent>
</prevsection>
<citsent citstr=" N03-2021 ">
other well-known metrics are wer (nieen et al, 2000), nist (doddington, 2002), gtm (melamed et al, 2003), <papid> N03-2021 </papid>rouge (lin and och, 2004<papid> P04-1077 </papid>a), meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>and ter (snover et al, 2006), just to namea few.</citsent>
<aftsection>
<nextsent>all these metrics take into account information at the lexical level1, and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (culy and riehemann, 2003).
</nextsent>
<nextsent>in order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (zhou et al, 2006; kauchak and barzilay, 2006; <papid> N06-1058 </papid>owczarzak et al, 2006).<papid> W06-3112 </papid></nextsent>
<nextsent>other authors have tried to exploit information at deeper linguistic lev els.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L880">
<title id=" I08-1042.xml">heterogeneous automatic mt evaluation through non parametric metric combinations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there exist large number of metrics based on different similarity criteria.
</prevsent>
<prevsent>by far, the most widely used metric in recent literature is bleu (papineni et al, 2001).
</prevsent>
</prevsection>
<citsent citstr=" P04-1077 ">
other well-known metrics are wer (nieen et al, 2000), nist (doddington, 2002), gtm (melamed et al, 2003), <papid> N03-2021 </papid>rouge (lin and och, 2004<papid> P04-1077 </papid>a), meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>and ter (snover et al, 2006), just to namea few.</citsent>
<aftsection>
<nextsent>all these metrics take into account information at the lexical level1, and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (culy and riehemann, 2003).
</nextsent>
<nextsent>in order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (zhou et al, 2006; kauchak and barzilay, 2006; <papid> N06-1058 </papid>owczarzak et al, 2006).<papid> W06-3112 </papid></nextsent>
<nextsent>other authors have tried to exploit information at deeper linguistic lev els.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L886">
<title id=" I08-1042.xml">heterogeneous automatic mt evaluation through non parametric metric combinations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there exist large number of metrics based on different similarity criteria.
</prevsent>
<prevsent>by far, the most widely used metric in recent literature is bleu (papineni et al, 2001).
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
other well-known metrics are wer (nieen et al, 2000), nist (doddington, 2002), gtm (melamed et al, 2003), <papid> N03-2021 </papid>rouge (lin and och, 2004<papid> P04-1077 </papid>a), meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>and ter (snover et al, 2006), just to namea few.</citsent>
<aftsection>
<nextsent>all these metrics take into account information at the lexical level1, and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (culy and riehemann, 2003).
</nextsent>
<nextsent>in order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (zhou et al, 2006; kauchak and barzilay, 2006; <papid> N06-1058 </papid>owczarzak et al, 2006).<papid> W06-3112 </papid></nextsent>
<nextsent>other authors have tried to exploit information at deeper linguistic lev els.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L887">
<title id=" I08-1042.xml">heterogeneous automatic mt evaluation through non parametric metric combinations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other well-known metrics are wer (nieen et al, 2000), nist (doddington, 2002), gtm (melamed et al, 2003), <papid> N03-2021 </papid>rouge (lin and och, 2004<papid> P04-1077 </papid>a), meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>and ter (snover et al, 2006), just to namea few.</prevsent>
<prevsent>all these metrics take into account information at the lexical level1, and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (culy and riehemann, 2003).</prevsent>
</prevsection>
<citsent citstr=" N06-1058 ">
in order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (zhou et al, 2006; kauchak and barzilay, 2006; <papid> N06-1058 </papid>owczarzak et al, 2006).<papid> W06-3112 </papid></citsent>
<aftsection>
<nextsent>other authors have tried to exploit information at deeper linguistic levels.
</nextsent>
<nextsent>for instance, we may find metrics based on full constituent parsing (liu and gildea, 2005), <papid> W05-0904 </papid>and on dependency parsing (liu and gildea, 2005; <papid> W05-0904 </papid>amigo?</nextsent>
<nextsent>et al, 2006; mehay and brew, 2007; owczarzak et al., 2007).<papid> W07-0411 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L888">
<title id=" I08-1042.xml">heterogeneous automatic mt evaluation through non parametric metric combinations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other well-known metrics are wer (nieen et al, 2000), nist (doddington, 2002), gtm (melamed et al, 2003), <papid> N03-2021 </papid>rouge (lin and och, 2004<papid> P04-1077 </papid>a), meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>and ter (snover et al, 2006), just to namea few.</prevsent>
<prevsent>all these metrics take into account information at the lexical level1, and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (culy and riehemann, 2003).</prevsent>
</prevsection>
<citsent citstr=" W06-3112 ">
in order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (zhou et al, 2006; kauchak and barzilay, 2006; <papid> N06-1058 </papid>owczarzak et al, 2006).<papid> W06-3112 </papid></citsent>
<aftsection>
<nextsent>other authors have tried to exploit information at deeper linguistic levels.
</nextsent>
<nextsent>for instance, we may find metrics based on full constituent parsing (liu and gildea, 2005), <papid> W05-0904 </papid>and on dependency parsing (liu and gildea, 2005; <papid> W05-0904 </papid>amigo?</nextsent>
<nextsent>et al, 2006; mehay and brew, 2007; owczarzak et al., 2007).<papid> W07-0411 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L889">
<title id=" I08-1042.xml">heterogeneous automatic mt evaluation through non parametric metric combinations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (zhou et al, 2006; kauchak and barzilay, 2006; <papid> N06-1058 </papid>owczarzak et al, 2006).<papid> W06-3112 </papid></prevsent>
<prevsent>other authors have tried to exploit information at deeper linguistic lev els.</prevsent>
</prevsection>
<citsent citstr=" W05-0904 ">
for instance, we may find metrics based on full constituent parsing (liu and gildea, 2005), <papid> W05-0904 </papid>and on dependency parsing (liu and gildea, 2005; <papid> W05-0904 </papid>amigo?</citsent>
<aftsection>
<nextsent>et al, 2006; mehay and brew, 2007; owczarzak et al., 2007).<papid> W07-0411 </papid></nextsent>
<nextsent>we may find also metrics at the level of shallow-semantics, e.g., over semantic roles and named entities (gimenez and ma`rquez, 2007), and at the properly semantic level, e.g., over discourse representations (gimenez, 2007).however, none of current metrics provides, in isolation, global measure of quality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L891">
<title id=" I08-1042.xml">heterogeneous automatic mt evaluation through non parametric metric combinations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other authors have tried to exploit information at deeper linguistic levels.
</prevsent>
<prevsent>for instance, we may find metrics based on full constituent parsing (liu and gildea, 2005), <papid> W05-0904 </papid>and on dependency parsing (liu and gildea, 2005; <papid> W05-0904 </papid>amigo?</prevsent>
</prevsection>
<citsent citstr=" W07-0411 ">
et al, 2006; mehay and brew, 2007; owczarzak et al., 2007).<papid> W07-0411 </papid></citsent>
<aftsection>
<nextsent>we may find also metrics at the level of shallow-semantics, e.g., over semantic roles and named entities (gimenez and ma`rquez, 2007), and at the properly semantic level, e.g., over discourse representations (gimenez, 2007).however, none of current metrics provides, in isolation, global measure of quality.
</nextsent>
<nextsent>indeed, all metrics focus on partial aspects of quality.
</nextsent>
<nextsent>the main problem of relying on partial metrics is that we may obtain biased evaluations, which may lead us to derive inaccurate conclusions.
</nextsent>
<nextsent>for instance, callison burch et al (2006) and koehn and monz (2006)<papid> W06-3114 </papid>have recently reported several problematic cases related to the automatic evaluation of systems oriented towards maximizing different quality aspects.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L892">
<title id=" I08-1042.xml">heterogeneous automatic mt evaluation through non parametric metric combinations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, all metrics focus on partial aspects of quality.
</prevsent>
<prevsent>the main problem of relying on partial metrics is that we may obtain biased evaluations, which may lead us to derive inaccurate conclusions.
</prevsent>
</prevsection>
<citsent citstr=" W06-3114 ">
for instance, callison burch et al (2006) and koehn and monz (2006)<papid> W06-3114 </papid>have recently reported several problematic cases related to the automatic evaluation of systems oriented towards maximizing different quality aspects.</citsent>
<aftsection>
<nextsent>corroborating the findings by culy and riehemann(2003), they showed that bleu over rates smt systems with respect to other types of systems, such1rouge and meteor may consider morphological variations.
</nextsent>
<nextsent>meteor may also look up for synonyms in wordnet.
</nextsent>
<nextsent>319 as rule-based, or human-aided.
</nextsent>
<nextsent>the reason is that smt systems are likelier to match the sublanguage (e.g., lexical choice and order) represented by the set of reference translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L931">
<title id=" I08-1025.xml">tsubaki an open search engine infrastructure for developing new information access methodology </title>
<section> sharing of pre-processed web pages on.  </section>
<citcontext>
<prevsection>
<prevsent>sentences in web page and their analyzed results can be obtained by looking at these tags in the standard format data corresponding to the page.
</prevsent>
<prevsent>2.2 construction of web standard format.
</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
data collection we have crawled 218 million web pages over three months, may - july in 2007, by using the shim crawler,2 and then converted these pages into web standard format data with results of japanese parser, knp (kurohashi and nagao, 1994), <papid> J94-4001 </papid>through our conversion tools.</citsent>
<aftsection>
<nextsent>note that this web page collec 2http://www.logos.t.u-tokyo.ac.jp/crawler/  xml version= 1.0  encoding= utf-8 ?   standard format url= http://www.kantei.go.jp/jp/koizumiprofile/1_sin nen.html  originalencoding= shift_jis  time= 2006-08 -14 19:48:51   text type= default    id= 1  length= 70  offset= 525    raw string ????????????????????
</nextsent>
<nextsent>????)??????? /rawstring   annotation scheme= knp    ![cdata[* 1d  ??  ??  ??  ??  ????  ?
</nextsent>
<nextsent>?  ?:??  ??:0-4  rid:1056  ??
</nextsent>
<nextsent>6 ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L932">
<title id=" I08-1051.xml">formalising multilayer corpora in owl dl  lexicon modelling querying and consistency control </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such corpora offer the possibility to empirically investigate the interactions between different levels of linguistic analysis.
</prevsent>
<prevsent>currently, the most common use of such corpora is the acquisition of statistical models that make use of the more shallow?
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
levels to predict the deeper levels of annotation (gildea and jurafsky, 2002; <papid> J02-3001 </papid>miltsakaki et al, 2005).</citsent>
<aftsection>
<nextsent>while these models fill an important need for practical applications, they fall shortof the general task of lexicon modelling, i.e., creating an abstracted and compact representation of the corpus information that lends itself to linguistically informed?
</nextsent>
<nextsent>usages such as human interpretation or integration with other knowledge sources (e.g., deep grammar resources or ontologies).
</nextsent>
<nextsent>in practice, this task faces three major problems: at the time of writing, sebastian pad?
</nextsent>
<nextsent>and dennis spohr were affiliated with saarland university, and anette frank with dfki saarbrcken and saarland university.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L933">
<title id=" I08-1051.xml">formalising multilayer corpora in owl dl  lexicon modelling querying and consistency control </title>
<section> consistency control.  </section>
<citcontext>
<prevsection>
<prevsent>on the semantic level, we extracted annotation instances (in context) for metaphorical vs. non metaphorical readings, or frames that are involved in under specification in certain sentences, but not inothers.
</prevsent>
<prevsent>while the result sets thus obtained still require manual inspection, they clearly illustrate how the detection of inconsistencies can be enhanced by declarative formalisation of the annotation scheme.
</prevsent>
</prevsection>
<citsent citstr=" E03-1068 ">
another strategy could be to concentrate on frames or lemmas exhibiting proportionally high variation in annotation (dickinson and meurers, 2003).<papid> E03-1068 </papid></citsent>
<aftsection>
<nextsent>in this paper, we have constructed description logics-based lexicon model directly from multi-layer linguistic corpus annotations.
</nextsent>
<nextsent>we have shown how such model allows for explicit data modelling, andfor flexible and fine-grained definition of various degrees of abstractions over corpus annotations.furthermore, we have demonstrated that powerful logical formalisation which integrates an underlying annotation scheme can be used to directly control consistency of the annotations using general kr techniques.
</nextsent>
<nextsent>it can also overcome limitations of current xml-based search tools by supporting queries which are able to connect multiple levels of linguistic analysis.
</nextsent>
<nextsent>these queries can be used variously as an additional means of consistency control, to derive quantitative tendencies from the data, to extract lexicon views tailored to specific purposes, and finally as general tool for linguistic research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L934">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to select reliable parses, especially for knowledge acquisition, ? and to adapt the parser to new domains.this paper proposes method for selecting reliable parses from parses output by single dependency parser.
</prevsent>
<prevsent>we do not use an ensemble method based on multiple parsers, but use only single parser, because speed and efficiency are important when processing massive volume of text.
</prevsent>
</prevsection>
<citsent citstr=" N06-1023 ">
the resulting highly reliable parses would be useful to automatically construct dictionaries and knowledge bases, such as case frames (kawahara and kurohashi, 2006).<papid> N06-1023 </papid></citsent>
<aftsection>
<nextsent>furthermore, we incorporate the reliable parses we obtained into the dependency parser to achieve domain adaptation.
</nextsent>
<nextsent>the conll 2007 shared task tackled domain adaptation of dependency parsers for the first time (nivre et al, 2007).<papid> D07-1096 </papid></nextsent>
<nextsent>sagae and tsujii applied an ensemble method to the domain adaptation track and achieved the highest score (sagae and tsujii,2007).<papid> D07-1111 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L936">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting highly reliable parses would be useful to automatically construct dictionaries and knowledge bases, such as case frames (kawahara and kurohashi, 2006).<papid> N06-1023 </papid></prevsent>
<prevsent>furthermore, we incorporate the reliable parses we obtained into the dependency parser to achieve domain adaptation.</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
the conll 2007 shared task tackled domain adaptation of dependency parsers for the first time (nivre et al, 2007).<papid> D07-1096 </papid></citsent>
<aftsection>
<nextsent>sagae and tsujii applied an ensemble method to the domain adaptation track and achieved the highest score (sagae and tsujii,2007).<papid> D07-1111 </papid></nextsent>
<nextsent>they first parsed in-domain unlabeled sentences using two parsers trained on out-of-domain labeled data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L938">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we incorporate the reliable parses we obtained into the dependency parser to achieve domain adaptation.
</prevsent>
<prevsent>the conll 2007 shared task tackled domain adaptation of dependency parsers for the first time (nivre et al, 2007).<papid> D07-1096 </papid></prevsent>
</prevsection>
<citsent citstr=" D07-1111 ">
sagae and tsujii applied an ensemble method to the domain adaptation track and achieved the highest score (sagae and tsujii,2007).<papid> D07-1111 </papid></citsent>
<aftsection>
<nextsent>they first parsed in-domain unlabeled sentences using two parsers trained on out-of-domain labeled data.
</nextsent>
<nextsent>then, they extracted identical parses that were produced by the two parsers and added them to the original (out-of-domain) training set to train domain-adapted model.
</nextsent>
<nextsent>dredze et al yielded the second highest score1 in the domain adaptation track (dredze et al, 2007).<papid> D07-1112 </papid>however, their results were obtained without adaptation.</nextsent>
<nextsent>they concluded that it is very difficult to substantially improve the target domain performance over that of state-of-the-art parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L939">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they first parsed in-domain unlabeled sentences using two parsers trained on out-of-domain labeled data.
</prevsent>
<prevsent>then, they extracted identical parses that were produced by the two parsers and added them to the original (out-of-domain) training set to train domain-adapted model.
</prevsent>
</prevsection>
<citsent citstr=" D07-1112 ">
dredze et al yielded the second highest score1 in the domain adaptation track (dredze et al, 2007).<papid> D07-1112 </papid>however, their results were obtained without adaptation.</citsent>
<aftsection>
<nextsent>they concluded that it is very difficult to substantially improve the target domain performance over that of state-of-the-art parser.
</nextsent>
<nextsent>to confirm this, we parsed the test set (chem) of the domain adaptation track by using one of the best dependency parsers, second-order mst parser (mcdonald et al, 1dredze et al achieved the second highest score on the chem test set for unlabeled dependency accuracy.
</nextsent>
<nextsent>7092006)2.
</nextsent>
<nextsent>though this parser was trained on the provided out-of-domain (penn treebank) labeled data, surprisingly, its accuracy slightly outperformed the highest score achieved by sagae and tsujii (unla beled dependency accuracy: 83.58   83.42 (sagaeand tsujii, 2007)).<papid> D07-1111 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L946">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this paper therefore focuses on unlabeled dependency parsing.
</prevsent>
<prevsent>we have already described the domain adaptation track of the conll 2007 shared task.
</prevsent>
</prevsection>
<citsent citstr=" D07-1097 ">
for the multilingual dependency parsing track, which was the other track of the shared task, nilsson et al achieved the best performance using an ensemble method(hall et al, 2007).<papid> D07-1097 </papid></citsent>
<aftsection>
<nextsent>they used method of combining several parsers?
</nextsent>
<nextsent>outputs in the framework of mst parsing (sagae and lavie, 2006).<papid> N06-2033 </papid></nextsent>
<nextsent>this method does not select parses, but considers all the output parses with weights to decide final parse of given sentence.reichart and rappoport also proposed an ensemble method to select high-quality parses from the outputs of constituency parsers (reichart and rappoport, 2007<papid> P07-1078 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L947">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for the multilingual dependency parsing track, which was the other track of the shared task, nilsson et al achieved the best performance using an ensemble method(hall et al, 2007).<papid> D07-1097 </papid></prevsent>
<prevsent>they used method of combining several parsers?</prevsent>
</prevsection>
<citsent citstr=" N06-2033 ">
outputs in the framework of mst parsing (sagae and lavie, 2006).<papid> N06-2033 </papid></citsent>
<aftsection>
<nextsent>this method does not select parses, but considers all the output parses with weights to decide final parse of given sentence.reichart and rappoport also proposed an ensemble method to select high-quality parses from the outputs of constituency parsers (reichart and rappoport, 2007<papid> P07-1078 </papid>a).</nextsent>
<nextsent>they regarded parses as being of high quality if 20 different parsers agreed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L948">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they used method of combining several parsers?
</prevsent>
<prevsent>outputs in the framework of mst parsing (sagae and lavie, 2006).<papid> N06-2033 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1078 ">
this method does not select parses, but considers all the output parses with weights to decide final parse of given sentence.reichart and rappoport also proposed an ensemble method to select high-quality parses from the outputs of constituency parsers (reichart and rappoport, 2007<papid> P07-1078 </papid>a).</citsent>
<aftsection>
<nextsent>they regarded parses as being of high quality if 20 different parsers agreed.
</nextsent>
<nextsent>they did not apply their method to domain adaptation or other applications.
</nextsent>
<nextsent>reranking methods for parsing have relation to parse selection.
</nextsent>
<nextsent>they rerank the n-best parses that are output by generative parser using lot of lexical and syntactic features (collins and koo, 2005; <papid> J05-1003 </papid>charniak and johnson, 2005).<papid> P05-1022 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L950">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they did not apply their method to domain adaptation or other applications.
</prevsent>
<prevsent>reranking methods for parsing have relation to parse selection.
</prevsent>
</prevsection>
<citsent citstr=" J05-1003 ">
they rerank the n-best parses that are output by generative parser using lot of lexical and syntactic features (collins and koo, 2005; <papid> J05-1003 </papid>charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>there are several related methods for 1-best outputs, such as revision learning (nakagawa et al, 2002) <papid> P02-1063 </papid>andtransformation-based learning (brill, 1995) for part of-speech tagging.</nextsent>
<nextsent>attardi and ciaramita proposed method of tree revision learning for dependency parsing (attardi and ciaramita, 2007).<papid> N07-1049 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L951">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they did not apply their method to domain adaptation or other applications.
</prevsent>
<prevsent>reranking methods for parsing have relation to parse selection.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
they rerank the n-best parses that are output by generative parser using lot of lexical and syntactic features (collins and koo, 2005; <papid> J05-1003 </papid>charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>there are several related methods for 1-best outputs, such as revision learning (nakagawa et al, 2002) <papid> P02-1063 </papid>andtransformation-based learning (brill, 1995) for part of-speech tagging.</nextsent>
<nextsent>attardi and ciaramita proposed method of tree revision learning for dependency parsing (attardi and ciaramita, 2007).<papid> N07-1049 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L952">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>reranking methods for parsing have relation to parse selection.
</prevsent>
<prevsent>they rerank the n-best parses that are output by generative parser using lot of lexical and syntactic features (collins and koo, 2005; <papid> J05-1003 </papid>charniak and johnson, 2005).<papid> P05-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1063 ">
there are several related methods for 1-best outputs, such as revision learning (nakagawa et al, 2002) <papid> P02-1063 </papid>andtransformation-based learning (brill, 1995) for part of-speech tagging.</citsent>
<aftsection>
<nextsent>attardi and ciaramita proposed method of tree revision learning for dependency parsing (attardi and ciaramita, 2007).<papid> N07-1049 </papid></nextsent>
<nextsent>as for the use of unlabeled data, self-trainingmethods have been successful in recent years.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L953">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they rerank the n-best parses that are output by generative parser using lot of lexical and syntactic features (collins and koo, 2005; <papid> J05-1003 </papid>charniak and johnson, 2005).<papid> P05-1022 </papid></prevsent>
<prevsent>there are several related methods for 1-best outputs, such as revision learning (nakagawa et al, 2002) <papid> P02-1063 </papid>andtransformation-based learning (brill, 1995) for part of-speech tagging.</prevsent>
</prevsection>
<citsent citstr=" N07-1049 ">
attardi and ciaramita proposed method of tree revision learning for dependency parsing (attardi and ciaramita, 2007).<papid> N07-1049 </papid></citsent>
<aftsection>
<nextsent>as for the use of unlabeled data, self-trainingmethods have been successful in recent years.
</nextsent>
<nextsent>mcclosky et al improved state-of-the-art constituency parser by 1.1% using self-training (mc 2http://sourceforge.net/projects/mstparser/ table 1: labeled and unlabeled data provided for the shared task.
</nextsent>
<nextsent>the labeled ptb data is used for training, and the labeled bio data is used for development.
</nextsent>
<nextsent>the labeled chem data is used for the final test.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L954">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the labeled chem data is used for the final test.
</prevsent>
<prevsent>name source labeled unlabeled ptb penn treebank 18,577 1,625,606 bio penn bioie 200 369,439 chem penn bioie 200 396,128 closky et al, 2006a).
</prevsent>
</prevsection>
<citsent citstr=" P06-1043 ">
they also applied self-trainingto domain adaptation of constituency parser (mc closky et al, 2006<papid> P06-1043 </papid>b).</citsent>
<aftsection>
<nextsent>their method simply adds parsed unlabeled data without selecting it to the training set.
</nextsent>
<nextsent>reichart and rappoport applied self training to domain adaptation using small set of in-domain training data (reichart and rappoport, 2007<papid> P07-1078 </papid>b).</nextsent>
<nextsent>van noord extracted bilexical preferences from adutch parsed corpus of 500m words without selection (van noord, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L962">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> the dataset.  </section>
<citcontext>
<prevsection>
<prevsent>table1 lists the dataset provided for the domain adaptation track.we pre-processed all the unlabeled sentences using conditional random fields (crfs)-based part of-speech tagger.
</prevsent>
<prevsent>this tagger is trained on the ptb training set that consists of 18,577 sentences.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
the features are the same as those in (ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>as an implementation of crfs, we usedcrf++3.
</nextsent>
<nextsent>if method of domain adaptation is applied to the tagger, the accuracy of parsing unlabeled sentences will improve (yoshida et al, 2007).
</nextsent>
<nextsent>this 3http://crfpp.sourceforge.net/ 710paper, however, does not deal with domain adaptation of tagger but focuses on that of parser.
</nextsent>
<nextsent>our approach assesses automatic parses of single parser in order to select only reliable parses fromthem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L963">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> learning reliability of parses.  </section>
<citcontext>
<prevsection>
<prevsent>bio rel dev is used for tuning the parameters of the reliability detector.
</prevsent>
<prevsent>4.1 base dependency parser.
</prevsent>
</prevsection>
<citsent citstr=" W06-2932 ">
we used the mst parser (mcdonald et al, 2006), <papid> W06-2932 </papid>which achieved top results in the conll 2006 (conll-x) shared task, as base dependencyparser.</citsent>
<aftsection>
<nextsent>to enable second-order features, the parameter order was set to 2.
</nextsent>
<nextsent>the other parameters wereset to default.
</nextsent>
<nextsent>we used ptb base train (14,862 sen tences) to train this parser.
</nextsent>
<nextsent>4.2 algorithm to detect reliable parses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L964">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> learning reliability of parses.  </section>
<citcontext>
<prevsection>
<prevsent>in this experiment, we set the accuracy threshold ? at 100%.
</prevsent>
<prevsent>as result, 736 out of2,500 examples (sentences) were judged to be posi tive.to evaluate the reliability of parses, we take advantage of the following features that can be related to the difficulty of sentences.
</prevsent>
</prevsection>
<citsent citstr=" D07-1013 ">
sentence length: the longer the sentence is, the poorer the parser performs (mcdonald and nivre, 2007).<papid> D07-1013 </papid></citsent>
<aftsection>
<nextsent>we determine sentence length by the number of words.dependency lengths: long-distance dependencies exhibit bad performance (mcdonald and nivre, 2007).<papid> D07-1013 </papid></nextsent>
<nextsent>we calculate the average of the dependency length of each word.difficulty of vocabulary: it is hard for supervised parsers to learn dependencies that include low frequency words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L968">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> learning reliability of parses.  </section>
<citcontext>
<prevsection>
<prevsent>number of commas: sentences with multiple commas are difficult to parse.
</prevsent>
<prevsent>we count the number of commas in given sentence.
</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
number of conjunctions (and/or): sentences with coordinate structures are also difficult to parse(kurohashi and nagao, 1994).<papid> J94-4001 </papid></citsent>
<aftsection>
<nextsent>we count the number of coordinate conjunctions (and/or) in given sentence.
</nextsent>
<nextsent>to apply these features to svms in practice, the numbers are binned at certain interval for each feature.
</nextsent>
<nextsent>for instance, the number of conjunctions is split into four bins: 0, 1, 2 and more than 2.
</nextsent>
<nextsent>711 table 2: example bio sentences judged as reliable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L972">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> domain adaptation of dependency.  </section>
<citcontext>
<prevsection>
<prevsent>in this table, the table 3: experimental results on chem test data.
</prevsent>
<prevsent>system accuracy ptb+unlabel (18,000 sents.)
</prevsent>
</prevsection>
<citsent citstr=" D07-1119 ">
84.12 only ptb (baseline) 83.58 1st (sagae and tsujii, 2007) <papid> D07-1111 </papid>83.42 2nd (dredze et al, 2007) <papid> D07-1112 </papid>83.38 3rd (attardi et al, 2007) <papid> D07-1119 </papid>83.08 third row lists the three highest scores of the domain adaptation track of the conll 2007 shared task.</citsent>
<aftsection>
<nextsent>the baseline parser was trained only on the ptb labeled data (as described in section 1).
</nextsent>
<nextsent>the proposed method (ptb+unlabel (18,000 sents.)) outperformed the baseline by approximately 0.5%, and also beat all the systems submitted to the domain adaptation track.
</nextsent>
<nextsent>these systems include an ensemble method (sagae and tsujii, 2007) <papid> D07-1111 </papid>and an approach of tree revision learning with selection method of only using short training sentences (shorter than 30 words) (attardi et al, 2007).<papid> D07-1119 </papid></nextsent>
<nextsent>this paper described method for detecting reliable parses out of the outputs of single dependency parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L976">
<title id=" I08-2097.xml">learning reliability of parses for domain adaptation of dependency parsing </title>
<section> discussion and conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>if greater efficiency is required, it is possible to apply pre-filter that removes long sentences (e.g., longer than 30 words), which are seldom selected by the reliability detector.
</prevsent>
<prevsent>in addition, our method does not depend on particular parser, and can be applied to other state-of-the art parsers, such as malt parser (nivre et al, 2006), which is feature-rich linear-time parser.in general, it is very difficult to improve the accuracy of the best performing systems by using unlabeled data.
</prevsent>
</prevsection>
<citsent citstr=" P05-1001 ">
there are only few successful studies, such as (ando and zhang, 2005) <papid> P05-1001 </papid>for chunking and (mcclosky et al, 2006<papid> P06-1043 </papid>a; mcclosky et al, 2006<papid> P06-1043 </papid>b) on constituency parsing.</citsent>
<aftsection>
<nextsent>we succeeded in boosting the accuracy of the second-order mst parser, which is 713 state-of-the-art dependency parser, in the conll 2007 domain adaptation task.
</nextsent>
<nextsent>this was difficult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data (dredze et al, 2007).<papid> D07-1112 </papid></nextsent>
<nextsent>the key factor in our success was the extraction of only reliable information from unlabeled data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L988">
<title id=" I08-2141.xml">a mechanism to provide language encoding support and an nlp friendly editor </title>
<section> language-encoding support.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 language-encoding identification.
</prevsent>
<prevsent>another important element of the design is alanguage-encoding identification tool that is integrated into the language-encoding support module so that if the user opens file and does not know the language or encoding of the text, the tool can automatically identify the language-encoding of the text.
</prevsent>
</prevsection>
<citsent citstr=" W06-1109 ">
the language-encoding identification tool is based on byte based n-gram models using distributional similarity measures (singh, 2006<papid> W06-1109 </papid>a).</citsent>
<aftsection>
<nextsent>this tools is computationally quite light one as the amount of 959 data required for training is very small and it hasbeen found to be one of the most accurate language encoding systems currently available.
</nextsent>
<nextsent>the user canmake it even faster by removing those language encodings which she may not be interested in.
</nextsent>
<nextsent>this will require only change in the relevant properties file.
</nextsent>
<nextsent>2.3 encoding conversion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L997">
<title id=" I08-2130.xml">combining context features by canonical belief network for chinese partofspeech tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for chinese language, word segmentation must be done before pos tagging, because, different from english sentences, there is no distinct boundary such as white space to separate different words(sun, 2001).
</prevsent>
<prevsent>also, chinese word segmentation and pos tagging can be done at the same time(ng, 2004)(wang, 2006).
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
there are two main approaches for pos tagging: rule-based and statistical algorithms(merialdo,1994).<papid> J94-2001 </papid></citsent>
<aftsection>
<nextsent>rule based pos tagging methods ex tratct rules from training corpus and use these rules to tag new sentences(brill, 1992)(<papid> A92-1021 </papid>brill, 1994).</nextsent>
<nextsent>statistic-based algorithms based on beliefnetwork(murphy, 2001) such as hidden-markovmodel(hmm)(cutting, 1992)(thede, 1999), lexicalized hmm(lee, 2000) and maximal-entropymodel(ratnaparkhi, 1996) <papid> W96-0213 </papid>use the statistical information of manually tagged corpus as background knowledge to tag new sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L998">
<title id=" I08-2130.xml">combining context features by canonical belief network for chinese partofspeech tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also, chinese word segmentation and pos tagging can be done at the same time(ng, 2004)(wang, 2006).
</prevsent>
<prevsent>there are two main approaches for pos tagging: rule-based and statistical algorithms(merialdo,1994).<papid> J94-2001 </papid></prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
rule based pos tagging methods ex tratct rules from training corpus and use these rules to tag new sentences(brill, 1992)(<papid> A92-1021 </papid>brill, 1994).</citsent>
<aftsection>
<nextsent>statistic-based algorithms based on beliefnetwork(murphy, 2001) such as hidden-markovmodel(hmm)(cutting, 1992)(thede, 1999), lexicalized hmm(lee, 2000) and maximal-entropymodel(ratnaparkhi, 1996) <papid> W96-0213 </papid>use the statistical information of manually tagged corpus as background knowledge to tag new sentences.</nextsent>
<nextsent>for example, the verb is mostly followed by noun, an adverb or nothing, so if we are sure that word is verb, we could say the word following has large probability to be noun.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L999">
<title id=" I08-2130.xml">combining context features by canonical belief network for chinese partofspeech tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are two main approaches for pos tagging: rule-based and statistical algorithms(merialdo,1994).<papid> J94-2001 </papid></prevsent>
<prevsent>rule based pos tagging methods ex tratct rules from training corpus and use these rules to tag new sentences(brill, 1992)(<papid> A92-1021 </papid>brill, 1994).</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
statistic-based algorithms based on beliefnetwork(murphy, 2001) such as hidden-markovmodel(hmm)(cutting, 1992)(thede, 1999), lexicalized hmm(lee, 2000) and maximal-entropymodel(ratnaparkhi, 1996) <papid> W96-0213 </papid>use the statistical information of manually tagged corpus as background knowledge to tag new sentences.</citsent>
<aftsection>
<nextsent>for example, the verb is mostly followed by noun, an adverb or nothing, so if we are sure that word is verb, we could say the word following has large probability to be noun.
</nextsent>
<nextsent>this could be helpful specially when has lot of possible pos tags or it is an unknown word.
</nextsent>
<nextsent>formally, this process relates to pr(noun|verb), pr(adverb|verb) and pr(nothing|verb), that canbe estimated from the training corpus.
</nextsent>
<nextsent>hmm based tagging is mainly based on such statistical information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1000">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>likewise, advances in un- and semi supervised learning techniques have made raw data more readily viable for tool development.
</prevsent>
<prevsent>still, however, such techniques often require seeds?, or pro totypes?
</prevsent>
</prevsection>
<citsent citstr=" N06-1041 ">
(c.f., (haghighi and klein, 2006)) <papid> N06-1041 </papid>which are used to prune search spaces or direct learners.</citsent>
<aftsection>
<nextsent>an important question is how to create such seeds for the hundreds to thousands of rpls.
</nextsent>
<nextsent>we describe the construction of resource that taps the large body of linguistically analyzed language data that has made its way to the web, and propose using this ? the work described in this document was done while lewis was faculty at the university of washington.resource as means to bootstrap nlp tool development.
</nextsent>
<nextsent>inter linear glossed text, or igt, semi structured data type quite common to the field of linguistics, is used to present data and analysis for alan guage and is generally embedded in scholarly linguistic documents as part of larger analysis.
</nextsent>
<nextsent>igts unique structure ? effectively each instance consists of bitext between english and some target language?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1001">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>inter linear glossed text, or igt, semi structured data type quite common to the field of linguistics, is used to present data and analysis for alan guage and is generally embedded in scholarly linguistic documents as part of larger analysis.
</prevsent>
<prevsent>igts unique structure ? effectively each instance consists of bitext between english and some target language?
</prevsent>
</prevsection>
<citsent citstr=" N01-1026 ">
can be easily enriched through alignment and projection (e.g., (yarowsky and ngai, 2001), (<papid> N01-1026 </papid>hwa et al, 2002)).<papid> P02-1050 </papid></citsent>
<aftsection>
<nextsent>the reader will note that the igt instance in example (1) consists of bitext between some target language on the first line, or the target line (in this case in welsh), and third line in english, the translation line.
</nextsent>
<nextsent>the canonical igt form, which this example is representative of, has intervening linguistic annotations and glosses on second line, the gloss line.
</nextsent>
<nextsent>because the gloss line aligns with words and morphemes on the target line, and contains glosses that are similar to words on the translation line, itcan serve as bridge between the target and translation lines; high word alignment accuracy between the three lines can be achieved without requiring parallel data or bilingual dictionaries (xia and lewis, 2007).<papid> N07-1057 </papid>furthermore, the gloss line provides additional information about the target language data, such asa variety of grammatical annotations, including verbal and tense markers (e.g., 3sg), case markers, etc., all of which can provide useful knowledge about the language.</nextsent>
<nextsent>(1) rhoddodd yr athro lyfr ir bachgen ddoe gave-3sg the teacher book to-the boy yesterday the teacher gave book to the boy yesterday?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1002">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>inter linear glossed text, or igt, semi structured data type quite common to the field of linguistics, is used to present data and analysis for alan guage and is generally embedded in scholarly linguistic documents as part of larger analysis.
</prevsent>
<prevsent>igts unique structure ? effectively each instance consists of bitext between english and some target language?
</prevsent>
</prevsection>
<citsent citstr=" P02-1050 ">
can be easily enriched through alignment and projection (e.g., (yarowsky and ngai, 2001), (<papid> N01-1026 </papid>hwa et al, 2002)).<papid> P02-1050 </papid></citsent>
<aftsection>
<nextsent>the reader will note that the igt instance in example (1) consists of bitext between some target language on the first line, or the target line (in this case in welsh), and third line in english, the translation line.
</nextsent>
<nextsent>the canonical igt form, which this example is representative of, has intervening linguistic annotations and glosses on second line, the gloss line.
</nextsent>
<nextsent>because the gloss line aligns with words and morphemes on the target line, and contains glosses that are similar to words on the translation line, itcan serve as bridge between the target and translation lines; high word alignment accuracy between the three lines can be achieved without requiring parallel data or bilingual dictionaries (xia and lewis, 2007).<papid> N07-1057 </papid>furthermore, the gloss line provides additional information about the target language data, such asa variety of grammatical annotations, including verbal and tense markers (e.g., 3sg), case markers, etc., all of which can provide useful knowledge about the language.</nextsent>
<nextsent>(1) rhoddodd yr athro lyfr ir bachgen ddoe gave-3sg the teacher book to-the boy yesterday the teacher gave book to the boy yesterday?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1003">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the reader will note that the igt instance in example (1) consists of bitext between some target language on the first line, or the target line (in this case in welsh), and third line in english, the translation line.
</prevsent>
<prevsent>the canonical igt form, which this example is representative of, has intervening linguistic annotations and glosses on second line, the gloss line.
</prevsent>
</prevsection>
<citsent citstr=" N07-1057 ">
because the gloss line aligns with words and morphemes on the target line, and contains glosses that are similar to words on the translation line, itcan serve as bridge between the target and translation lines; high word alignment accuracy between the three lines can be achieved without requiring parallel data or bilingual dictionaries (xia and lewis, 2007).<papid> N07-1057 </papid>furthermore, the gloss line provides additional information about the target language data, such asa variety of grammatical annotations, including verbal and tense markers (e.g., 3sg), case markers, etc., all of which can provide useful knowledge about the language.</citsent>
<aftsection>
<nextsent>(1) rhoddodd yr athro lyfr ir bachgen ddoe gave-3sg the teacher book to-the boy yesterday the teacher gave book to the boy yesterday?
</nextsent>
<nextsent>(bailyn, 2001) odin, the online database of inter linear text (lewis, 2006), is resource built over the past few years from data harvested from scholarly documents.
</nextsent>
<nextsent>currently, odin has over 41,581 instances of igt for 944 languages, and the number of igt instances is expected to double or triple in the near-term as new methods for collecting data are brought online.
</nextsent>
<nextsent>although the number of instances per language varies, e.g., the maximum currently is 2,891 instances (for 529 table 1: the numbers of languages in odin range of # of # of % of igt instances languages instances instances 1000-2891 10 15019 36.11 500-999 11 8111 19.50 250-499 18 6274 15.08 100-249 22 3303 7.94 50-99 38 2812 6.76 25-49 60 2089 5.02 10-24 127 1934 4.65 1-9 658 2039 4.91 japanese), and the overall number per language may appear small, it is still possible to harvest significant value from igt for targeted rpls.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1005">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> building odin.  </section>
<citcontext>
<prevsection>
<prevsent>9 the accuracy ranges from 92.9% to 99.8% depending on the article length and model parameter called profile 532 table 4: performance on the development set (the span number in the gold standard is 447) features system classification exact match partial match span num accuracy prec recall fscore prec recall fscore regex templates 269 n/a 68.40 41.16 51.40 99.26 59.73 74.58 1 130 81.50 68.46 19.91 30.85 97.69 28.41 44.02 2 405 93.28 58.27 52.80 55.40 95.56 86.58 90.85 1 + 3 180 80.26 61.67 24.83 35.40 81.11 32.66 46.57 1 + 2 420 94.42 63.09 59.28 61.13 93.81 88.14 90.88 2 + 3 339 92.68 75.81 57.49 65.39 93.21 70.69 80.40 2 + 4 456 96.91 80.92 82.55 81.73 93.64 95.53 94.57 1 + 2 + 3 370 93.39 75.14 62.20 68.05 93.51 77.40 84.70 1 + 2 + 4 444 97.00 84.68 84.11 84.40 95.95 95.30 95.62 2 + 3 + 4 431 97.79 86.77 83.67 85.19 97.68 94.18 95.90 1 + 2 + 3 + 4 431 98.00 90.02 86.80 88.38 97.22 93.74 95.44 table 5: performance on the test set (the span number in the gold standard is 843) features system classification exact match partial match span num accuracy prec recall fscore prec recall fscore regex templates 587 n/a 74.95 52.19 61.54 98.64 68.68 80.98 2 719 92.45 57.02 48.64 52.50 94.02 80.19 86.56 2 + 4 849 95.66 75.50 76.04 75.77 93.76 94.42 94.09 2 + 3 + 4 831 95.95 77.14 76.04 76.58 95.19 93.83 94.50 1 + 2 + 3 + 4 830 96.83 82.29 81.02 81.65 96.51 95.02 95.76 however, when we ran the same algorithm on theigt data, the accuracy was only 50.2%.10 in contrast, heuristic approach that predicts the language id according to the language names occurring in the document yields an accuracy of 65.6%.
</prevsent>
<prevsent>because the language name associated with an igt instance almost always appears somewhere in the document, we propose to treat the language id task as reference resolution problem, where igt instances are the mentions and the language names appearing in the document are the entities.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
a language identifier simply needs to link the mention sto the entities, allowing us to apply any good resolution algorithms such as (soon et al, 2001; <papid> J01-4004 </papid>ng, 2005; luo, 2007) <papid> N07-1010 </papid>and to provide an elegant solution to the unknown language problem.</citsent>
<aftsection>
<nextsent>more detail on this approach will be reported elsewhere.
</nextsent>
<nextsent>we see odin being used in number of different ways.
</nextsent>
<nextsent>in another study (lewis and xia, 2008), we demonstrated method for using odin to discover interesting and computationally relevant typologicalfeatures for hundreds of the worlds languages automatically.
</nextsent>
<nextsent>in this section we present two more uses length.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1006">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> building odin.  </section>
<citcontext>
<prevsection>
<prevsent>9 the accuracy ranges from 92.9% to 99.8% depending on the article length and model parameter called profile 532 table 4: performance on the development set (the span number in the gold standard is 447) features system classification exact match partial match span num accuracy prec recall fscore prec recall fscore regex templates 269 n/a 68.40 41.16 51.40 99.26 59.73 74.58 1 130 81.50 68.46 19.91 30.85 97.69 28.41 44.02 2 405 93.28 58.27 52.80 55.40 95.56 86.58 90.85 1 + 3 180 80.26 61.67 24.83 35.40 81.11 32.66 46.57 1 + 2 420 94.42 63.09 59.28 61.13 93.81 88.14 90.88 2 + 3 339 92.68 75.81 57.49 65.39 93.21 70.69 80.40 2 + 4 456 96.91 80.92 82.55 81.73 93.64 95.53 94.57 1 + 2 + 3 370 93.39 75.14 62.20 68.05 93.51 77.40 84.70 1 + 2 + 4 444 97.00 84.68 84.11 84.40 95.95 95.30 95.62 2 + 3 + 4 431 97.79 86.77 83.67 85.19 97.68 94.18 95.90 1 + 2 + 3 + 4 431 98.00 90.02 86.80 88.38 97.22 93.74 95.44 table 5: performance on the test set (the span number in the gold standard is 843) features system classification exact match partial match span num accuracy prec recall fscore prec recall fscore regex templates 587 n/a 74.95 52.19 61.54 98.64 68.68 80.98 2 719 92.45 57.02 48.64 52.50 94.02 80.19 86.56 2 + 4 849 95.66 75.50 76.04 75.77 93.76 94.42 94.09 2 + 3 + 4 831 95.95 77.14 76.04 76.58 95.19 93.83 94.50 1 + 2 + 3 + 4 830 96.83 82.29 81.02 81.65 96.51 95.02 95.76 however, when we ran the same algorithm on theigt data, the accuracy was only 50.2%.10 in contrast, heuristic approach that predicts the language id according to the language names occurring in the document yields an accuracy of 65.6%.
</prevsent>
<prevsent>because the language name associated with an igt instance almost always appears somewhere in the document, we propose to treat the language id task as reference resolution problem, where igt instances are the mentions and the language names appearing in the document are the entities.
</prevsent>
</prevsection>
<citsent citstr=" N07-1010 ">
a language identifier simply needs to link the mention sto the entities, allowing us to apply any good resolution algorithms such as (soon et al, 2001; <papid> J01-4004 </papid>ng, 2005; luo, 2007) <papid> N07-1010 </papid>and to provide an elegant solution to the unknown language problem.</citsent>
<aftsection>
<nextsent>more detail on this approach will be reported elsewhere.
</nextsent>
<nextsent>we see odin being used in number of different ways.
</nextsent>
<nextsent>in another study (lewis and xia, 2008), we demonstrated method for using odin to discover interesting and computationally relevant typologicalfeatures for hundreds of the worlds languages automatically.
</nextsent>
<nextsent>in this section we present two more uses length.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1008">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> using odin.  </section>
<citcontext>
<prevsection>
<prevsent>since the target line in igt data does not come with annotations (e.g., pos tags), it is first necessary to enrich it.
</prevsent>
<prevsent>once enriched, the data can be used as bootstrap for tools such as taggers.
</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
3.1.1 enriching igtin previous study (xia and lewis, 2007), <papid> N07-1057 </papid>we proposed three-step process to enrich igt data: (1) parse the english translation with an english parserand convert english phrase structures (ps) into dependency structures (ds) with head percolation table (magerman, 1995), (<papid> P95-1037 </papid>2) align the target line and the english translation using the gloss line, and (3) project the syntactic structures (both ps and ds) from english onto the target line.</citsent>
<aftsection>
<nextsent>for instance, given the igt example in ex (1), the enrichment algorithm will produce the word alignment in figure 1 and the syntactic structures in figure 2.
</nextsent>
<nextsent>the eache gave book o he boy yes terday rhoddodd r th ro y r ? bachgen ddoe loss i ne : r n l t o : a g t i e : a e - 3 g h t a h r o k o - h b y e t r a figure 1: aligning the target line and the english translation with the help of the gloss line 533 gave (a) projecting ds athro bachgen lyfr yr ddoeir rhoddodd np1 vp nn teacher vbd gave np2 dt np4pp nn the in np3 yesterday nn dt book nn boy dt to np nn vbd np nppp nn in+dt nn nndt rhoddodd (gave) yr(the) athro (teacher) lyfr (book) ir (to-the) bachogen (boy) ddoe (yesterday) teacher boy the book the yesterday to the (b) projecting ps figure 2: projecting syntactic structure from english to the target language we evaluated the algorithm on small set of 538 igt instances for several languages.
</nextsent>
<nextsent>on average, the accuracy of the english ds (i.e., the percentage of correct dependency links in the ds) is 93.48%; the f-score of the word alignment links between the translation and target lines is 94.03%, and the accuracy of the target ds produced by the projection algorithm is 81.45%.
</nextsent>
<nextsent>when we replace the automatically generated english ds and word alignment with the ones in the gold standard, the accuracy of target ds increases significantly, from 81.45% to 90.64%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1011">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> using odin.  </section>
<citcontext>
<prevsection>
<prevsent>from the enriched data, various kinds of information can be extracted.
</prevsent>
<prevsent>for instance, the target syntactic structures form small monolingual treebanks, from which grammars in various formalisms can be extracted (e.g., (charniak, 1996)).
</prevsent>
</prevsection>
<citsent citstr=" C00-1078 ">
the english and target syntactic structures form parallel treebanks, from which transfer rules and translation lexicon can be extracted and used for machine translation (e.g.,(meyers et al, 2000; <papid> C00-1078 </papid>menezes, 2002; xia and mccord, 2004)).<papid> C04-1073 </papid></citsent>
<aftsection>
<nextsent>there are many ways of using the enriched data to bootstrap nlp tools.
</nextsent>
<nextsent>suppose we want to build pos tagger.
</nextsent>
<nextsent>previous studies on unsupervised pos tagging can be divided into several categories according to the kind of information available to the learner.
</nextsent>
<nextsent>the first category (e.g., (kupiec, 1992; merialdo,1994; <papid> J94-2001 </papid>banko and moore, 2004; <papid> C04-1080 </papid>wang and schuurmans, 2005)) assumes there is lexicon that lists the allowable tags for each word in the text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1012">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> using odin.  </section>
<citcontext>
<prevsection>
<prevsent>from the enriched data, various kinds of information can be extracted.
</prevsent>
<prevsent>for instance, the target syntactic structures form small monolingual treebanks, from which grammars in various formalisms can be extracted (e.g., (charniak, 1996)).
</prevsent>
</prevsection>
<citsent citstr=" C04-1073 ">
the english and target syntactic structures form parallel treebanks, from which transfer rules and translation lexicon can be extracted and used for machine translation (e.g.,(meyers et al, 2000; <papid> C00-1078 </papid>menezes, 2002; xia and mccord, 2004)).<papid> C04-1073 </papid></citsent>
<aftsection>
<nextsent>there are many ways of using the enriched data to bootstrap nlp tools.
</nextsent>
<nextsent>suppose we want to build pos tagger.
</nextsent>
<nextsent>previous studies on unsupervised pos tagging can be divided into several categories according to the kind of information available to the learner.
</nextsent>
<nextsent>the first category (e.g., (kupiec, 1992; merialdo,1994; <papid> J94-2001 </papid>banko and moore, 2004; <papid> C04-1080 </papid>wang and schuurmans, 2005)) assumes there is lexicon that lists the allowable tags for each word in the text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1013">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> using odin.  </section>
<citcontext>
<prevsection>
<prevsent>suppose we want to build pos tagger.
</prevsent>
<prevsent>previous studies on unsupervised pos tagging can be divided into several categories according to the kind of information available to the learner.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
the first category (e.g., (kupiec, 1992; merialdo,1994; <papid> J94-2001 </papid>banko and moore, 2004; <papid> C04-1080 </papid>wang and schuurmans, 2005)) assumes there is lexicon that lists the allowable tags for each word in the text.</citsent>
<aftsection>
<nextsent>the common approach is to use the lexicon to initialize the emission probability in hidden markov model (hmm), and run the baum-welch algorithm (baum et al, 1970) on large amount of unlabeled data to re-estimate transition and emission probability.
</nextsent>
<nextsent>the second category uses unlabeled data only (e.g.,(schutze, 1995; clark, 2003; <papid> E03-1009 </papid>biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007)).</nextsent>
<nextsent>the idea is to cluster words based on morphological and/or distributional cues.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1014">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> using odin.  </section>
<citcontext>
<prevsection>
<prevsent>suppose we want to build pos tagger.
</prevsent>
<prevsent>previous studies on unsupervised pos tagging can be divided into several categories according to the kind of information available to the learner.
</prevsent>
</prevsection>
<citsent citstr=" C04-1080 ">
the first category (e.g., (kupiec, 1992; merialdo,1994; <papid> J94-2001 </papid>banko and moore, 2004; <papid> C04-1080 </papid>wang and schuurmans, 2005)) assumes there is lexicon that lists the allowable tags for each word in the text.</citsent>
<aftsection>
<nextsent>the common approach is to use the lexicon to initialize the emission probability in hidden markov model (hmm), and run the baum-welch algorithm (baum et al, 1970) on large amount of unlabeled data to re-estimate transition and emission probability.
</nextsent>
<nextsent>the second category uses unlabeled data only (e.g.,(schutze, 1995; clark, 2003; <papid> E03-1009 </papid>biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007)).</nextsent>
<nextsent>the idea is to cluster words based on morphological and/or distributional cues.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1015">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> using odin.  </section>
<citcontext>
<prevsection>
<prevsent>the first category (e.g., (kupiec, 1992; merialdo,1994; <papid> J94-2001 </papid>banko and moore, 2004; <papid> C04-1080 </papid>wang and schuurmans, 2005)) assumes there is lexicon that lists the allowable tags for each word in the text.</prevsent>
<prevsent>the common approach is to use the lexicon to initialize the emission probability in hidden markov model (hmm), and run the baum-welch algorithm (baum et al, 1970) on large amount of unlabeled data to re-estimate transition and emission probability.</prevsent>
</prevsection>
<citsent citstr=" E03-1009 ">
the second category uses unlabeled data only (e.g.,(schutze, 1995; clark, 2003; <papid> E03-1009 </papid>biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007)).</citsent>
<aftsection>
<nextsent>the idea is to cluster words based on morphological and/or distributional cues.
</nextsent>
<nextsent>haghighi and klein (2006) <papid> N06-1041 </papid>showed that adding small set of prototypes to the unlabeled data can improve tagging accuracy significantly.</nextsent>
<nextsent>the tagged target lines in the enriched igt datacan be incorporated in each category of work mentioned above.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1016">
<title id=" I08-1069.xml">repurposing theoretical linguistic data for tool development and search </title>
<section> using odin.  </section>
<citcontext>
<prevsection>
<prevsent>the first category (e.g., (kupiec, 1992; merialdo,1994; <papid> J94-2001 </papid>banko and moore, 2004; <papid> C04-1080 </papid>wang and schuurmans, 2005)) assumes there is lexicon that lists the allowable tags for each word in the text.</prevsent>
<prevsent>the common approach is to use the lexicon to initialize the emission probability in hidden markov model (hmm), and run the baum-welch algorithm (baum et al, 1970) on large amount of unlabeled data to re-estimate transition and emission probability.</prevsent>
</prevsection>
<citsent citstr=" P06-3002 ">
the second category uses unlabeled data only (e.g.,(schutze, 1995; clark, 2003; <papid> E03-1009 </papid>biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007)).</citsent>
<aftsection>
<nextsent>the idea is to cluster words based on morphological and/or distributional cues.
</nextsent>
<nextsent>haghighi and klein (2006) <papid> N06-1041 </papid>showed that adding small set of prototypes to the unlabeled data can improve tagging accuracy significantly.</nextsent>
<nextsent>the tagged target lines in the enriched igt datacan be incorporated in each category of work mentioned above.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1019">
<title id=" I05-4009.xml">question classification using multiple classifiers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>though qc is similar to tc in some aspects, they are clearly distinct in that ? question is usually shorter, and contains less lexicon-based information than text, which brings great trouble to qc.
</prevsent>
<prevsent>therefore to obtain higher classifying accuracy, qc has to make further analysis of sentences, namely qc has to extend interrogative sentence with syntactic and semantic knowledge, replacing or extending the vocabulary of the question with the semantic meaning of every words.
</prevsent>
</prevsection>
<citsent citstr=" C02-1042 ">
in qc, many systems apply machine-learning approaches (hovy, et al 2002; <papid> C02-1042 </papid>ittycheriah, et al 2000; zhang, et al 2003).</citsent>
<aftsection>
<nextsent>the classification is made according to the lexical, syntactic features and parts of speech.
</nextsent>
<nextsent>machine learning approach is of great adaptability, and 90.0% of classifying accuracy is obtained with svm method and tree kernel as features.
</nextsent>
<nextsent>however, there is still the problem that the classifying result is affected by the accuracy of syntactic analyzer, which need manually to determine the weights of different classifying features.
</nextsent>
<nextsent>some other systems adopting manual-rule method make qc, though may have high classifying accuracy, lack of adaptability, because regulation determination involves manual interference to solve the conflicts between regulations and to form orderly arranged rule base.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1020">
<title id=" I05-4009.xml">question classification using multiple classifiers </title>
<section> classifying method description.  </section>
<citcontext>
<prevsection>
<prevsent>an adaptation of the libsvm implementation (chang, et al 2001) is used in the following.
</prevsent>
<prevsent>four type of kernel function linear, polynomial, radial basis function, and sigmoid are provided by libsvm . 3.2 svm-tbl qc algorithm.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
tbl has been part of nlp since eric brills breakthrough paper in 1995(brill 1995), <papid> J95-4004 </papid>which has been as effective as any other approach on the part-of-speech tagging problem.</citsent>
<aftsection>
<nextsent>tbl is true machine learning technique.
</nextsent>
<nextsent>given tagged training corpus, it produces sequence of rules that serves as model of the training data.
</nextsent>
<nextsent>then, to derive the appropriate tags, each rule may be applied, in order, to each instance in an untagged corpus.
</nextsent>
<nextsent>tbl generates all of the potential rules that would make at least one tag in the training corpus correct.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1021">
<title id=" I08-3008.xml">cross language parser adaptation between related languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>both also require, in addition to the actual annotation process, substantial effort on treebank/grammar design, format specifications, tailoring of annotation guidelines etc; the latter costs are rather constant no matter how small the resulting corpus is. in this context, there is the intriguing question whether we can actually build parser without treebank (or broad-coverage formal grammar) of the particular language.
</prevsent>
<prevsent>there is some related work that addresses the issue by variety of means.
</prevsent>
</prevsection>
<citsent citstr=" W06-2912 ">
klein and manning (2004) use hybrid unsupervised approach, which combines constituency and dependency model, and achieve an unlabeled f-score of 77.6% on penn treebank wall street journal data (english), 63.9% on negra corpus (german), and 46.7% on the penn chinese tree bank.1 bod (2006) <papid> W06-2912 </papid>uses unsupervised data-oriented parsing; the input of his parser contains manually assigned gold-standard tags.</citsent>
<aftsection>
<nextsent>he reports 64.2% unlabeled f-score on wsj sentences up to 40 words long.2 hwa et al (2004) explore different approach to attacking new language.
</nextsent>
<nextsent>they train coll inss (1997) model 2 parser on the penn treebank wsj data and use it to parse the english side of parallel corpus.
</nextsent>
<nextsent>the resulting parses are converted to dependencies, the dependencies are projected to second language using automatically obtained word alignments as bridge, and the resulting dependency trees cleaned up using limited set of language-specific post-projection transformation rules.
</nextsent>
<nextsent>finally dependency parser for the target language is trained on this projected dependency treebank, and the accuracy of the parser is measured against gold standard.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1023">
<title id=" I08-1022.xml">cross language text categorization using a bilingual lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the one hand, experts who know different languages are required to organize these collections.
</prevsent>
<prevsent>on the other hand, maybe there exist large amount of labelled documents in language (e.g. english) which are in the same class structure as the un labelled documents in another language.
</prevsent>
</prevsection>
<citsent citstr=" P06-1070 ">
as result, how to ex corresponding author.ploit the existing labelled documents in some language (e.g. english) to classify the un labelled documents other than the language in multilingual scenario has attracted more and more attention (bel et al., 2003; rigutini et al, 2005; olsson et al, 2005;fortuna and shawe-taylor, 2005; li and shawe taylor, 2006; gliozzo and strapparava, 2006).<papid> P06-1070 </papid></citsent>
<aftsection>
<nextsent>werefer to this task as cross language text categorization.
</nextsent>
<nextsent>it aims to extend the existing automated text categorization system from one language to other languages without additional intervention of human experts.
</nextsent>
<nextsent>formally, given two document collections{de,df} from two different languages andre spectively, we use the labelled document collectionde in the language to deduce the labels of the document collection df in the language via an algorithm and some external bilingual resources.typically, some external bilingual lexical resources, such as machine translation system (mt),large-scale parallel corpora and multilingual ontology etc., are used to alleviate cross language text categorization.
</nextsent>
<nextsent>however, it is hard to obtain them for many language pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1024">
<title id=" I08-2126.xml">hacking wikipedia for hyponymy relation acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the remaining hyponymy relations were acquired by existing methods for extracting relations from definition sentence sand category pages.
</prevsent>
<prevsent>this means that extraction from the hierarchical layouts almost doubled the number of relations extracted.
</prevsent>
</prevsection>
<citsent citstr=" P03-1001 ">
the goal of this study has been to automatically extract large set of hyponymy relations, which play critical role in many nlp applications, such as q&asystems; (fleischman et al, 2003).<papid> P03-1001 </papid></citsent>
<aftsection>
<nextsent>in this paper, hyponymy relation is defined as relation between hy pernym and hyponym when the hyponym is (kind of) hypernym.1.
</nextsent>
<nextsent>1this is slightly modified definition of the one in (milleret al, 1990).
</nextsent>
<nextsent>linguistic literature, e.g.
</nextsent>
<nextsent>(a.cruse, 1998), distinguishes hyponymy relations, such as national university?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1025">
<title id=" I08-2126.xml">hacking wikipedia for hyponymy relation acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, we regard concept-instancecurrently, most useful sources of hyponymy relations are hand-crafted thesauri, such as wordnet (fellbaum, 1998).
</prevsent>
<prevsent>such thesauri are highly reliable,but their coverage is not large and the costs of extension and maintenance is prohibitively high.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
to reduce these costs, many methods have been proposed for automatically building thesauri (hearst, 1992; <papid> C92-2082 </papid>etzioni et al, 2005; shinzato and torisawa, 2004; <papid> N04-1010 </papid>pantel and pennacchiotti, 2006).</citsent>
<aftsection>
<nextsent>but often these methods need huge amount of documents and computational resources to obtain reasonable number ofhyponymy relations, and we still do not have thesaurus with sufficient coverage.in this paper, we attempt to extract large number of hyponymy relations without large document collection or great computational power.
</nextsent>
<nextsent>the key idea is to focus on wikipedia2, which is much more consistently organized than normal documents.
</nextsent>
<nextsent>actually, some studies have already attempted to extract hyponymy relations or semantic classifications from wikipedia.
</nextsent>
<nextsent>hyponymy relations were extracted from definition sentences (herbelot and copestake, 2006; kazama and torisawa, 2007).<papid> D07-1073 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1026">
<title id=" I08-2126.xml">hacking wikipedia for hyponymy relation acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, we regard concept-instancecurrently, most useful sources of hyponymy relations are hand-crafted thesauri, such as wordnet (fellbaum, 1998).
</prevsent>
<prevsent>such thesauri are highly reliable,but their coverage is not large and the costs of extension and maintenance is prohibitively high.
</prevsent>
</prevsection>
<citsent citstr=" N04-1010 ">
to reduce these costs, many methods have been proposed for automatically building thesauri (hearst, 1992; <papid> C92-2082 </papid>etzioni et al, 2005; shinzato and torisawa, 2004; <papid> N04-1010 </papid>pantel and pennacchiotti, 2006).</citsent>
<aftsection>
<nextsent>but often these methods need huge amount of documents and computational resources to obtain reasonable number ofhyponymy relations, and we still do not have thesaurus with sufficient coverage.in this paper, we attempt to extract large number of hyponymy relations without large document collection or great computational power.
</nextsent>
<nextsent>the key idea is to focus on wikipedia2, which is much more consistently organized than normal documents.
</nextsent>
<nextsent>actually, some studies have already attempted to extract hyponymy relations or semantic classifications from wikipedia.
</nextsent>
<nextsent>hyponymy relations were extracted from definition sentences (herbelot and copestake, 2006; kazama and torisawa, 2007).<papid> D07-1073 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1027">
<title id=" I08-2126.xml">hacking wikipedia for hyponymy relation acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the key idea is to focus on wikipedia2, which is much more consistently organized than normal documents.
</prevsent>
<prevsent>actually, some studies have already attempted to extract hyponymy relations or semantic classifications from wikipedia.
</prevsent>
</prevsection>
<citsent citstr=" D07-1073 ">
hyponymy relations were extracted from definition sentences (herbelot and copestake, 2006; kazama and torisawa, 2007).<papid> D07-1073 </papid></citsent>
<aftsection>
<nextsent>disambiguation of named entities was also attempted (bunescu and pasca, 2006).
</nextsent>
<nextsent>category pages were used to extract semantic relations (suchanek et al, 2007).
</nextsent>
<nextsent>lexical patterns for semantic relations were learned (ruiz casado et al, 2005).the difference between our work and these attempts is that we focus on the hierarchical layout of normal articles in wikipedia.
</nextsent>
<nextsent>for instance, the article titled penguin?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1029">
<title id=" I08-2126.xml">hacking wikipedia for hyponymy relation acquisition </title>
<section> alternative methods.  </section>
<citcontext>
<prevsection>
<prevsent>definition sentences in the wikipedia article were used for acquiring hyponymy relations by (kazama and torisawa, 2007) <papid> D07-1073 </papid>for named entity recognition.</prevsent>
<prevsent>their method is developed for the english version of the wikipedia and required some modifications to the japanese version.</prevsent>
</prevsection>
<citsent citstr=" C86-1105 ">
these modification was inspired by tsurumarus method (tsurumaru et al, 1986).<papid> C86-1105 </papid></citsent>
<aftsection>
<nextsent>basically, definition sentences have forms similar to hyponym word wa hypernym word no isshu de aru(hyponym is kind of hypernym)?
</nextsent>
<nextsent>in dictionaries in general, and contain hyponymy relations in them.
</nextsent>
<nextsent>in the wikipedia, such sentences usually come just after the titles of articles, so it is quite easy to recognize them.
</nextsent>
<nextsent>to extract hyponymy relations from definition sentences, we manually prepared 1,334 patterns, which are exemplified in table 4, and applied them to the first sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1030">
<title id=" I05-3027.xml">a conditional random field word segmenter for sighan bakeoff 2005 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our final system achieved f-score of 0.947 (as), 0.943 (hk), 0.950 (pk) and 0.964 (msr).
</prevsent>
<prevsent>the 2005 sighan bakeoff included four different corpora, academia sinica (as), city university of hongkong (hk), peking university (pk), and microsoft research asia (msr), each of which has its own definition of word.
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
in the 2003 sighan bakeoff (sproat &amp; emerson 2003), <papid> W03-1719 </papid>no single model performed well on all corpora included in the task.</citsent>
<aftsection>
<nextsent>rather, systems tended to do well on corpora largely drawn from set of similar mandarin varieties to the one they were originally developed for.
</nextsent>
<nextsent>across corpora, variation is seen in both the lexicons and also in the word segmentation standards.
</nextsent>
<nextsent>we concluded that, for future systems, generalization across such different mandarin varieties is crucial.
</nextsent>
<nextsent>to this end, we proposed new model using character identity, morphological and character reduplication features in conditional random field modeling framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1031">
<title id=" I05-3027.xml">a conditional random field word segmenter for sighan bakeoff 2005 </title>
<section> algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>to this end, we proposed new model using character identity, morphological and character reduplication features in conditional random field modeling framework.
</prevsent>
<prevsent>our system builds on research into conditional random field (crf), statistical sequence modeling framework first introduced by lafferty et al (2001).
</prevsent>
</prevsection>
<citsent citstr=" C04-1081 ">
work by peng et al (2004) <papid> C04-1081 </papid>first used this framework for chinese word segmentation by treating it as binary decision task, such that each character is labeled either as the beginning of word or the continuation of one.</citsent>
<aftsection>
<nextsent>gaussian priors were used to prevent over fitting and quasi-newton method was used for parameter optimization.
</nextsent>
<nextsent>the probability assigned to label sequence for particular sequence of characters by crf is given by the equation below: ( ) ( )??
</nextsent>
<nextsent>cc c cxykkxz xyp ,,exp)( 1| ??
</nextsent>
<nextsent>y is the label sequence for the sentence, is the sequence of unsegmented characters, z(x) is normalization term, fk is feature function, and indexes into characters in the sequence being labeled.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1034">
<title id=" I05-3027.xml">a conditional random field word segmenter for sighan bakeoff 2005 </title>
<section> feature engineering.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 features.
</prevsent>
<prevsent>the linguistic features used in our model fall into three categories: character identity n-grams,morphological and character reduplication features.
</prevsent>
</prevsection>
<citsent citstr=" W04-3236 ">
for each state, the character identity features (ng &amp; low 2004, <papid> W04-3236 </papid>xue &amp; shen 2003, <papid> W03-1728 </papid>goh et al 2003) <papid> P03-2039 </papid>are represented using feature functions that key off of the identity of the character in the current, proceeding and subsequent positions.</citsent>
<aftsection>
<nextsent>specifically, we used four types of unigram feature functions, designated as c0 (current charac ter), c1 (next character), c-1 (previous character), c-2 (the character two characters back).
</nextsent>
<nextsent>furthermore, four types of bi-gram features were used, and are notation ally designated here as conjunctions of the previously specified unigram features, c0c1, c-1c0, c-1c1, c-2c-1, and c2c0.
</nextsent>
<nextsent>given that unknown words are normally more than one character long, when representing the morphological features as feature functions, such feature functions keyed off the morphological information extracted from both the proceeding state and the current state.
</nextsent>
<nextsent>our morphological features are based upon the intuition regarding unknown word features given in gao et al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1035">
<title id=" I05-3027.xml">a conditional random field word segmenter for sighan bakeoff 2005 </title>
<section> feature engineering.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 features.
</prevsent>
<prevsent>the linguistic features used in our model fall into three categories: character identity n-grams,morphological and character reduplication features.
</prevsent>
</prevsection>
<citsent citstr=" W03-1728 ">
for each state, the character identity features (ng &amp; low 2004, <papid> W04-3236 </papid>xue &amp; shen 2003, <papid> W03-1728 </papid>goh et al 2003) <papid> P03-2039 </papid>are represented using feature functions that key off of the identity of the character in the current, proceeding and subsequent positions.</citsent>
<aftsection>
<nextsent>specifically, we used four types of unigram feature functions, designated as c0 (current charac ter), c1 (next character), c-1 (previous character), c-2 (the character two characters back).
</nextsent>
<nextsent>furthermore, four types of bi-gram features were used, and are notation ally designated here as conjunctions of the previously specified unigram features, c0c1, c-1c0, c-1c1, c-2c-1, and c2c0.
</nextsent>
<nextsent>given that unknown words are normally more than one character long, when representing the morphological features as feature functions, such feature functions keyed off the morphological information extracted from both the proceeding state and the current state.
</nextsent>
<nextsent>our morphological features are based upon the intuition regarding unknown word features given in gao et al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1036">
<title id=" I05-3027.xml">a conditional random field word segmenter for sighan bakeoff 2005 </title>
<section> feature engineering.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 features.
</prevsent>
<prevsent>the linguistic features used in our model fall into three categories: character identity n-grams,morphological and character reduplication features.
</prevsent>
</prevsection>
<citsent citstr=" P03-2039 ">
for each state, the character identity features (ng &amp; low 2004, <papid> W04-3236 </papid>xue &amp; shen 2003, <papid> W03-1728 </papid>goh et al 2003) <papid> P03-2039 </papid>are represented using feature functions that key off of the identity of the character in the current, proceeding and subsequent positions.</citsent>
<aftsection>
<nextsent>specifically, we used four types of unigram feature functions, designated as c0 (current charac ter), c1 (next character), c-1 (previous character), c-2 (the character two characters back).
</nextsent>
<nextsent>furthermore, four types of bi-gram features were used, and are notation ally designated here as conjunctions of the previously specified unigram features, c0c1, c-1c0, c-1c1, c-2c-1, and c2c0.
</nextsent>
<nextsent>given that unknown words are normally more than one character long, when representing the morphological features as feature functions, such feature functions keyed off the morphological information extracted from both the proceeding state and the current state.
</nextsent>
<nextsent>our morphological features are based upon the intuition regarding unknown word features given in gao et al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1037">
<title id=" I05-3027.xml">a conditional random field word segmenter for sighan bakeoff 2005 </title>
<section> feature engineering.  </section>
<citcontext>
<prevsection>
<prevsent>(2004).
</prevsent>
<prevsent>specifically, their idea was to use productive affixes and characters that only occurred independently to predict boundaries of unknown words.
</prevsent>
</prevsection>
<citsent citstr=" P04-1059 ">
to construct table containing affixes of unknown words, rather than using threshold-filtered affix tables in separate unknown word model as was done in gao et al (2004), <papid> P04-1059 </papid>we first extracted rare words from corpus and then collected the first and last characters to construct the prefix and suffix tables.</citsent>
<aftsection>
<nextsent>for the table of individual character words, we collected an individual character word table for each corpus of the characters that always occurred alone as separate word in the given corpus.
</nextsent>
<nextsent>we also collected list of bi-grams from each training corpus to distinguish known strings from unknown.
</nextsent>
<nextsent>adopting all the features together in model and using the automatically generated morphological tables prevented our system from manually over fitting the mandarin varieties we are most familiar with.
</nextsent>
<nextsent>the tables are used in the following ways: 1) c-1+c0 unknown word feature functions were created for each specific pair of characters in the bi-gram tables.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1045">
<title id=" I08-2110.xml">syn graph a flexible matching method based on synonymous expression extraction from an ordinary dictionary and a web corpus </title>
<section> extracted synonymous expressions are effec-.  </section>
<citcontext>
<prevsection>
<prevsent>relations are automatically extracted from an ordinary dictionary and web corpus.
</prevsent>
<prevsent>tively handled by syn graph data structure, which can pack the expressive divergence.
</prevsent>
</prevsection>
<citsent citstr=" C88-2098 ">
an ordinary dictionary is knowledge source to provide synonym and hypernym-hyponym relations (nakamura and nagao, 1988; <papid> C88-2098 </papid>tsurumaru et al, 1986).</citsent>
<aftsection>
<nextsent>a problem in using synonymous expressions extracted from dictionary is that some of them arenot appropriate since they are rarely used.
</nextsent>
<nextsent>for example, synonym pair suidou1 = kaikyou(strait)?
</nextsent>
<nextsent>is extracted.recently, some work has been done on corpus based paraphrase extraction (lin and pantel, 2001; barzilay and lee, 2003).<papid> N03-1003 </papid></nextsent>
<nextsent>the basic idea of their methods is that two words with similar meanings are used in similar contexts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1046">
<title id=" I08-2110.xml">syn graph a flexible matching method based on synonymous expression extraction from an ordinary dictionary and a web corpus </title>
<section> extracted synonymous expressions are effec-.  </section>
<citcontext>
<prevsection>
<prevsent>a problem in using synonymous expressions extracted from dictionary is that some of them arenot appropriate since they are rarely used.
</prevsent>
<prevsent>for example, synonym pair suidou1 = kaikyou(strait)?
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
is extracted.recently, some work has been done on corpus based paraphrase extraction (lin and pantel, 2001; barzilay and lee, 2003).<papid> N03-1003 </papid></citsent>
<aftsection>
<nextsent>the basic idea of their methods is that two words with similar meanings are used in similar contexts.
</nextsent>
<nextsent>although their methods can obtain broad-coverage paraphrases, the obtained paraphrases are not accurate enough to be utilized 1this word usually means water supply?.
</nextsent>
<nextsent>787 for achieving precise matching since they contain synonyms, near-synonyms, coordinate terms, hyper nyms, and inappropriate synonymous expressions.our approach makes the best use of an ordinary dictionary and web corpus to extract broad coverage and precise synonym and hypernymhyponym expressions.
</nextsent>
<nextsent>first, synonymous expressions are extracted from dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1047">
<title id=" I08-2110.xml">syn graph a flexible matching method based on synonymous expression extraction from an ordinary dictionary and a web corpus </title>
<section> extracted synonymous expressions are effec-.  </section>
<citcontext>
<prevsection>
<prevsent>among extracted synonymous expressions, those whose similarity is high are used for the flexible matching.
</prevsent>
<prevsent>by utilizing only synonymous expressions extracted from dictionary whose distributional similarity is high, we can exclude synonymous expressions extracted from dictionary that are rarely used, and the pair of words whose distributional similarity is high that is not actually synonymous expression (is not listed in dictionary).another point of our method is to introduce syn graph data structure.
</prevsent>
</prevsection>
<citsent citstr=" P97-1004 ">
so far, the effectiveness of handling expressive divergence has been shown for ir using thesaurus-based query expansion (voorhees, 1994; jacquemin et al, 1997).<papid> P97-1004 </papid></citsent>
<aftsection>
<nextsent>however, their methods are based on bag-of-words approach and thus does not pay attention to sentence-level synonymy with syntactic structure.
</nextsent>
<nextsent>mt requires such precise handling of synonymy, and advanced ir and qa also need it.
</nextsent>
<nextsent>to handle sentence-levelsynonymy precisely, we have to consider the combination of expressive divergence, which may cause combinatorial explosion.
</nextsent>
<nextsent>to overcome this problem, an id is assigned to each synonymous group, and then syn graph data structure is introduced to pack expressive divergence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1049">
<title id=" I08-2110.xml">syn graph a flexible matching method based on synonymous expression extraction from an ordinary dictionary and a web corpus </title>
<section> extracted synonymous expressions are effec-.  </section>
<citcontext>
<prevsection>
<prevsent>less than three phrases) 2.2 calculating the distributional similarity.
</prevsent>
<prevsent>using web corpus the similarity between pair of synonymous expressions is calculated based on distributional similarity (j.r.firth, 1957; harris, 1968) using the web corpus collected by (kawahara and kurohashi,2006).
</prevsent>
</prevsection>
<citsent citstr=" H01-1043 ">
the similarity between two predicates is defined to be one between the patterns of case examples of each predicate (kawahara and kurohashi, 2001).<papid> H01-1043 </papid></citsent>
<aftsection>
<nextsent>the similarity between two nouns are defined 2if the last word of sentence is highly general term suchas koto (thing) and tokoro (place), it is removed from the synonymous expression.
</nextsent>
<nextsent>788
</nextsent>
<nextsent>gakkou (school)
</nextsent>
<nextsent>gakue (a ca e y )  school
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1050">
<title id=" I08-2110.xml">syn graph a flexible matching method based on synonymous expression extraction from an ordinary dictionary and a web corpus </title>
<section> syn graph.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, synid is denoted by using english gloss word, surrounded by ? ?
</prevsent>
<prevsent>3.1 syn graph data structure.
</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
syn graph data structure is an acyclic directed graph, and the basis of syn graph is the dependency structure of an original sentence (in this paper,a robust parser (kurohashi and nagao, 1994) <papid> J94-4001 </papid>is always employed).</citsent>
<aftsection>
<nextsent>in the dependency structure, each node consists of one content word and zero or more function words, which is called basic node here after.
</nextsent>
<nextsent>if the content word of basic node belongs to synonymous group, new node with the synid is attached to it, and it is called syn node hereafter.
</nextsent>
<nextsent>for example, in figure 2, the shaded nodes are basic nodes and the other nodes are syn nodes5.
</nextsent>
<nextsent>then, if the expression con joining two or more 4spelling variations such as use of hiragana, katakana or kanji are handled by the morphological analyzer juman (kurohashi et al, 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1051">
<title id=" I05-2036.xml">representing semantics of texts  a non statistical approach </title>
<section> system overview.  </section>
<citcontext>
<prevsection>
<prevsent>the following sections describe in more detail the various aspects of our system, the experiments that we carried out to test the proposed algorithms and finally draw some conclusions.
</prevsent>
<prevsent>we use two-step approach for constructing conceptual graph representations of texts: firstly, by using verbnet and wordnet, we identify the semantic roles in sentence, and secondly,using these semantic roles and set of syn tactic/semantic rules we construct conceptual graph.to evaluate our algorithms we use test documents from two corpora in different domains the reuters-21578 text categorization test collec 209 tion (reuters, 1987) and the collection of aviation incident reports provided by the irish air accident investigation unit (aaiu) (air accident investigation unit, 2004).
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
all documents are parsed using eugene charniaks maximum entropy inspired parser (charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>there are number of different existing approaches for identifying semantic roles, varying from traditional parsing approaches, for example using hpsg grammars and lexical functional grammars, that strongly relyon manually developed grammars and lexicons, to data-driven approaches, for example auto slog (riloff and schmelzenbach, 1998).<papid> W98-1106 </papid></nextsent>
<nextsent>in the domain of the air traveler information system (miller et al, 1996) <papid> P96-1008 </papid>the authors apply statistical methods to compute the probability that constituent can fill in semantic slot within semantic frame.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1052">
<title id=" I05-2036.xml">representing semantics of texts  a non statistical approach </title>
<section> semantic role identification.  </section>
<citcontext>
<prevsection>
<prevsent>we use two-step approach for constructing conceptual graph representations of texts: firstly, by using verbnet and wordnet, we identify the semantic roles in sentence, and secondly,using these semantic roles and set of syn tactic/semantic rules we construct conceptual graph.to evaluate our algorithms we use test documents from two corpora in different domains the reuters-21578 text categorization test collec 209 tion (reuters, 1987) and the collection of aviation incident reports provided by the irish air accident investigation unit (aaiu) (air accident investigation unit, 2004).
</prevsent>
<prevsent>all documents are parsed using eugene charniaks maximum entropy inspired parser (charniak, 2000).<papid> A00-2018 </papid></prevsent>
</prevsection>
<citsent citstr=" W98-1106 ">
there are number of different existing approaches for identifying semantic roles, varying from traditional parsing approaches, for example using hpsg grammars and lexical functional grammars, that strongly relyon manually developed grammars and lexicons, to data-driven approaches, for example auto slog (riloff and schmelzenbach, 1998).<papid> W98-1106 </papid></citsent>
<aftsection>
<nextsent>in the domain of the air traveler information system (miller et al, 1996) <papid> P96-1008 </papid>the authors apply statistical methods to compute the probability that constituent can fill in semantic slot within semantic frame.</nextsent>
<nextsent>gildea and jurafsky (gildea and jurafsky, 2002) <papid> J02-3001 </papid>describe astatistical approach for semantic role labelling using data collected from framenet by analysing anumber of features such as phrase type, grammatical function, position in the sentence, etc. shi and mihalcea (shi and mihalcea, 2004)<papid> N04-3006 </papid>propose rule-based approach for semantic parsing using framenet and wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1053">
<title id=" I05-2036.xml">representing semantics of texts  a non statistical approach </title>
<section> semantic role identification.  </section>
<citcontext>
<prevsection>
<prevsent>all documents are parsed using eugene charniaks maximum entropy inspired parser (charniak, 2000).<papid> A00-2018 </papid></prevsent>
<prevsent>there are number of different existing approaches for identifying semantic roles, varying from traditional parsing approaches, for example using hpsg grammars and lexical functional grammars, that strongly relyon manually developed grammars and lexicons, to data-driven approaches, for example auto slog (riloff and schmelzenbach, 1998).<papid> W98-1106 </papid></prevsent>
</prevsection>
<citsent citstr=" P96-1008 ">
in the domain of the air traveler information system (miller et al, 1996) <papid> P96-1008 </papid>the authors apply statistical methods to compute the probability that constituent can fill in semantic slot within semantic frame.</citsent>
<aftsection>
<nextsent>gildea and jurafsky (gildea and jurafsky, 2002) <papid> J02-3001 </papid>describe astatistical approach for semantic role labelling using data collected from framenet by analysing anumber of features such as phrase type, grammatical function, position in the sentence, etc. shi and mihalcea (shi and mihalcea, 2004)<papid> N04-3006 </papid>propose rule-based approach for semantic parsing using framenet and wordnet.</nextsent>
<nextsent>they extract rules from the tagged data provided by framenet, which specify the realisation (order and different syntactic features) for the present semantic roles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1054">
<title id=" I05-2036.xml">representing semantics of texts  a non statistical approach </title>
<section> semantic role identification.  </section>
<citcontext>
<prevsection>
<prevsent>there are number of different existing approaches for identifying semantic roles, varying from traditional parsing approaches, for example using hpsg grammars and lexical functional grammars, that strongly relyon manually developed grammars and lexicons, to data-driven approaches, for example auto slog (riloff and schmelzenbach, 1998).<papid> W98-1106 </papid></prevsent>
<prevsent>in the domain of the air traveler information system (miller et al, 1996) <papid> P96-1008 </papid>the authors apply statistical methods to compute the probability that constituent can fill in semantic slot within semantic frame.</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
gildea and jurafsky (gildea and jurafsky, 2002) <papid> J02-3001 </papid>describe astatistical approach for semantic role labelling using data collected from framenet by analysing anumber of features such as phrase type, grammatical function, position in the sentence, etc. shi and mihalcea (shi and mihalcea, 2004)<papid> N04-3006 </papid>propose rule-based approach for semantic parsing using framenet and wordnet.</citsent>
<aftsection>
<nextsent>they extract rules from the tagged data provided by framenet, which specify the realisation (order and different syntactic features) for the present semantic roles.
</nextsent>
<nextsent>they also create feature set representation of the sentence and match it to each of the extracted rules.
</nextsent>
<nextsent>the result is the rule providing the most feature matches.
</nextsent>
<nextsent>the authors do not provide any information on how they select between different matches with the same score, or if there is any semantic check on suitability of phrase to realise semantic role (framenet does not provide any restrictions on the semantic roles similar to the selectional restrictions present in verbnet).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1055">
<title id=" I05-2036.xml">representing semantics of texts  a non statistical approach </title>
<section> semantic role identification.  </section>
<citcontext>
<prevsection>
<prevsent>there are number of different existing approaches for identifying semantic roles, varying from traditional parsing approaches, for example using hpsg grammars and lexical functional grammars, that strongly relyon manually developed grammars and lexicons, to data-driven approaches, for example auto slog (riloff and schmelzenbach, 1998).<papid> W98-1106 </papid></prevsent>
<prevsent>in the domain of the air traveler information system (miller et al, 1996) <papid> P96-1008 </papid>the authors apply statistical methods to compute the probability that constituent can fill in semantic slot within semantic frame.</prevsent>
</prevsection>
<citsent citstr=" N04-3006 ">
gildea and jurafsky (gildea and jurafsky, 2002) <papid> J02-3001 </papid>describe astatistical approach for semantic role labelling using data collected from framenet by analysing anumber of features such as phrase type, grammatical function, position in the sentence, etc. shi and mihalcea (shi and mihalcea, 2004)<papid> N04-3006 </papid>propose rule-based approach for semantic parsing using framenet and wordnet.</citsent>
<aftsection>
<nextsent>they extract rules from the tagged data provided by framenet, which specify the realisation (order and different syntactic features) for the present semantic roles.
</nextsent>
<nextsent>they also create feature set representation of the sentence and match it to each of the extracted rules.
</nextsent>
<nextsent>the result is the rule providing the most feature matches.
</nextsent>
<nextsent>the authors do not provide any information on how they select between different matches with the same score, or if there is any semantic check on suitability of phrase to realise semantic role (framenet does not provide any restrictions on the semantic roles similar to the selectional restrictions present in verbnet).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1056">
<title id=" I08-1021.xml">modeling context in scenario template creation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>from two articles are semantically similar.action clustering accordingly.
</prevsent>
<prevsent>while the target application varies, most systems that need to group text spans by similarity measures are verb-centric.
</prevsent>
</prevsection>
<citsent citstr=" P03-1029 ">
in addition to the verb, many systems expand their representation by including named entity tags (collier, 1998; yangarber et al, 2000; sudo et al,2003; <papid> P03-1029 </papid>filatova et al, 2006), <papid> P06-2027 </papid>as well as restricting matches (using constraints on subtrees (sudo et al., 2003; <papid> P03-1029 </papid>filatova et al, 2006), <papid> P06-2027 </papid>predicate argument structures (collier, 1998; riloff and schmelzenbach, 1998; yangarber et al, 2000; harabagiu and maiorano, 2002) or semantic roles).</citsent>
<aftsection>
<nextsent>given these representations, systems then cluster similar text spans.
</nextsent>
<nextsent>to our knowledge, all current systems use binary notion of similarity, in which pairs of spans are either similar or not.
</nextsent>
<nextsent>how they determine similarity is tightly coupled with their text span representation.
</nextsent>
<nextsent>one criterion used is pattern overlap: for example, (collier, 1998; harabagiu and lacatusu, 2005) judge text spans to be similar if they have similar verbs and share the same verb arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1057">
<title id=" I08-1021.xml">modeling context in scenario template creation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>from two articles are semantically similar.action clustering accordingly.
</prevsent>
<prevsent>while the target application varies, most systems that need to group text spans by similarity measures are verb-centric.
</prevsent>
</prevsection>
<citsent citstr=" P06-2027 ">
in addition to the verb, many systems expand their representation by including named entity tags (collier, 1998; yangarber et al, 2000; sudo et al,2003; <papid> P03-1029 </papid>filatova et al, 2006), <papid> P06-2027 </papid>as well as restricting matches (using constraints on subtrees (sudo et al., 2003; <papid> P03-1029 </papid>filatova et al, 2006), <papid> P06-2027 </papid>predicate argument structures (collier, 1998; riloff and schmelzenbach, 1998; yangarber et al, 2000; harabagiu and maiorano, 2002) or semantic roles).</citsent>
<aftsection>
<nextsent>given these representations, systems then cluster similar text spans.
</nextsent>
<nextsent>to our knowledge, all current systems use binary notion of similarity, in which pairs of spans are either similar or not.
</nextsent>
<nextsent>how they determine similarity is tightly coupled with their text span representation.
</nextsent>
<nextsent>one criterion used is pattern overlap: for example, (collier, 1998; harabagiu and lacatusu, 2005) judge text spans to be similar if they have similar verbs and share the same verb arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1063">
<title id=" I08-1021.xml">modeling context in scenario template creation </title>
<section> context-sensitive clustering (csc).  </section>
<citcontext>
<prevsection>
<prevsent>also, action participants are not limited to named entities, so our approach needs to process all nps.
</prevsent>
<prevsent>as both actions and actors may be realized using different words, similarity thesaurus is necessary.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
our approach to stc uses thesaurus based on corpus statistics (lin, 1998) <papid> P98-2127 </papid>for real-valuedsimilarity calculation.</citsent>
<aftsection>
<nextsent>in contrast to previous approaches, we do not threshold word similarity results; we retain their fractional values and incorporate these values holistically.
</nextsent>
<nextsent>finally, as the same action can be realized in different constructions, the semantic (not just syntactic) roles of verb arguments must be considered, lest agent and patient roles be confused.
</nextsent>
<nextsent>for these reasons, we use semantic role labeler (pradhan et al, 2004) <papid> N04-1030 </papid>to provide and delimit the text spans that contain the semantic arguments of predicate.</nextsent>
<nextsent>we term the obtained text spans as predicate argument tuples (tuples) throughout the paper.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1064">
<title id=" I08-1021.xml">modeling context in scenario template creation </title>
<section> context-sensitive clustering (csc).  </section>
<citcontext>
<prevsection>
<prevsent>in contrast to previous approaches, we do not threshold word similarity results; we retain their fractional values and incorporate these values holistically.
</prevsent>
<prevsent>finally, as the same action can be realized in different constructions, the semantic (not just syntactic) roles of verb arguments must be considered, lest agent and patient roles be confused.
</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
for these reasons, we use semantic role labeler (pradhan et al, 2004) <papid> N04-1030 </papid>to provide and delimit the text spans that contain the semantic arguments of predicate.</citsent>
<aftsection>
<nextsent>we term the obtained text spans as predicate argument tuples (tuples) throughout the paper.
</nextsent>
<nextsent>the semantic role labeler reportedly achieves an 1 measure equal to 68.7% on identification-classification of predicates and core arguments on anew swire text corpus (ldc, 2002).
</nextsent>
<nextsent>within the confines of our study, we find it is able to capture most of the tuples of interest.our approach explicitly captures contextual evidence.
</nextsent>
<nextsent>we define tuples contexts as other tu ples in the same article segment where no topic shift occurs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1065">
<title id=" I08-1021.xml">modeling context in scenario template creation </title>
<section> context-sensitive clustering (csc).  </section>
<citcontext>
<prevsection>
<prevsent>remaining tuples semantically connected with c[c.length]?
</prevsent>
<prevsent>other repeat /*e step*/for each such that   c.length dofor each such that   c.length doif == then continue; re-estimate parameters[c[i],c[j]] /*distribution parameters of edges between two clusters*/ tuplereassigned = false /*reset*/ /*m step*/for each such that   t.length do abestlikelihood = [i].likelihood; /*likelihood of being in its current cluster*/for each tuple tcontxt that contextually related with [i] dofor each cluster ccand, any candidate cluster that contextually related with tcontxt.cluster do (t [i] ? ccand) = comb(ps, pc) likelihood = log(p (t [i] ? ccand))if likelihood   abestlikelihood then abestlikelihood = likelihood [i].cluster = ccand tuplereassigned = true until tuplereassigned == false /*alignment stable*/returnduring initialization, tuples whose pairwise similarity higher than threshold ? are merged to form highly cohesive seed clusters.
</prevsent>
</prevsection>
<citsent citstr=" W06-1603 ">
to compute continuous similarity sim(ta, tb) of tuples ta and tb,we use the similarity measure described in (qiu etal., 2006), <papid> W06-1603 </papid>which linearly combines similarities between the semantic roles shared by the two tuples.some other tuples are related to these seed clusters by argument-similarity.</citsent>
<aftsection>
<nextsent>these related tuples are temporarily put into special other?
</nextsent>
<nextsent>cluster.
</nextsent>
<nextsent>the cluster membership of these related tuples, together with those currently in the seed clusters, are to be further adjusted.
</nextsent>
<nextsent>the other?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1067">
<title id=" I05-3026.xml">description of the hku chinese word segmentation system for sighan bakeoff 2005 </title>
<section> overview of the system.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we report the results of our system at this bakeoff in section 4, and give our conclusions on this work in section 5.
</prevsent>
<prevsent>in practice, our system works in two major steps as follows: the first step is process of known word segmentation, which aims to segment an input sequence of chinese characters into sequence of known words that are listed in the system dictionary.
</prevsent>
</prevsection>
<citsent citstr=" W03-1723 ">
in our current system, we apply known word bigram model to perform this task (fu and luke, 2003; <papid> W03-1723 </papid>fu and luke, 2004; fu and luke, 2005).</citsent>
<aftsection>
<nextsent>actually, known word segmentation is process of disambiguation.
</nextsent>
<nextsent>given chinese character string ncccc l21= , there are usually multiple possible segment ations of known words mwwww l21= according to the system dictionary.
</nextsent>
<nextsent>the task of known word segmentation is to find proper segmentation mwwww l21?
</nextsent>
<nextsent>= that maximizes the probability ? = ? i ii wwp 1 1 )|( , i.e. ? = ? ?= i ii ww wwpcwpw 1 1 )|(maxarg)|(maxarg?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1069">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, rich language structure is difficult to be integrated in the current smt framework.
</prevsent>
<prevsent>most of the smt approaches integrating syntactic structures are based on probabilistic tree transducers (tree to-tree model).
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
this leads to large increase in the model complexity (yamada and knight 2001; <papid> P01-1067 </papid>yamada and knight 2002; gildea 2003; <papid> P03-1011 </papid>galley et al. 2004; knight and graehl 2005; liu et al 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>however, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (wu 1997; <papid> J97-3002 </papid>chiang 2005).<papid> P05-1033 </papid></nextsent>
<nextsent>in this paper, we propose new model of smt by using structured prediction to perform tree-totree transductions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1070">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, rich language structure is difficult to be integrated in the current smt framework.
</prevsent>
<prevsent>most of the smt approaches integrating syntactic structures are based on probabilistic tree transducers (tree to-tree model).
</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
this leads to large increase in the model complexity (yamada and knight 2001; <papid> P01-1067 </papid>yamada and knight 2002; gildea 2003; <papid> P03-1011 </papid>galley et al. 2004; knight and graehl 2005; liu et al 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>however, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (wu 1997; <papid> J97-3002 </papid>chiang 2005).<papid> P05-1033 </papid></nextsent>
<nextsent>in this paper, we propose new model of smt by using structured prediction to perform tree-totree transductions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1071">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, rich language structure is difficult to be integrated in the current smt framework.
</prevsent>
<prevsent>most of the smt approaches integrating syntactic structures are based on probabilistic tree transducers (tree to-tree model).
</prevsent>
</prevsection>
<citsent citstr=" P06-1077 ">
this leads to large increase in the model complexity (yamada and knight 2001; <papid> P01-1067 </papid>yamada and knight 2002; gildea 2003; <papid> P03-1011 </papid>galley et al. 2004; knight and graehl 2005; liu et al 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>however, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (wu 1997; <papid> J97-3002 </papid>chiang 2005).<papid> P05-1033 </papid></nextsent>
<nextsent>in this paper, we propose new model of smt by using structured prediction to perform tree-totree transductions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1072">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the smt approaches integrating syntactic structures are based on probabilistic tree transducers (tree to-tree model).
</prevsent>
<prevsent>this leads to large increase in the model complexity (yamada and knight 2001; <papid> P01-1067 </papid>yamada and knight 2002; gildea 2003; <papid> P03-1011 </papid>galley et al. 2004; knight and graehl 2005; liu et al 2006).<papid> P06-1077 </papid></prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
however, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (wu 1997; <papid> J97-3002 </papid>chiang 2005).<papid> P05-1033 </papid></citsent>
<aftsection>
<nextsent>in this paper, we propose new model of smt by using structured prediction to perform tree-totree transductions.
</nextsent>
<nextsent>this model is inspired by sagae and lavie (2005), <papid> W05-1513 </papid>in which stack-based representation of monolingual parsing trees is used.</nextsent>
<nextsent>our contributions lie in the extension of this representation to bilingual parsing trees based on itgs and in the use of structured prediction method, called searn (daum?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1073">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the smt approaches integrating syntactic structures are based on probabilistic tree transducers (tree to-tree model).
</prevsent>
<prevsent>this leads to large increase in the model complexity (yamada and knight 2001; <papid> P01-1067 </papid>yamada and knight 2002; gildea 2003; <papid> P03-1011 </papid>galley et al. 2004; knight and graehl 2005; liu et al 2006).<papid> P06-1077 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
however, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (wu 1997; <papid> J97-3002 </papid>chiang 2005).<papid> P05-1033 </papid></citsent>
<aftsection>
<nextsent>in this paper, we propose new model of smt by using structured prediction to perform tree-totree transductions.
</nextsent>
<nextsent>this model is inspired by sagae and lavie (2005), <papid> W05-1513 </papid>in which stack-based representation of monolingual parsing trees is used.</nextsent>
<nextsent>our contributions lie in the extension of this representation to bilingual parsing trees based on itgs and in the use of structured prediction method, called searn (daum?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1074">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (wu 1997; <papid> J97-3002 </papid>chiang 2005).<papid> P05-1033 </papid></prevsent>
<prevsent>in this paper, we propose new model of smt by using structured prediction to perform tree-totree transductions.</prevsent>
</prevsection>
<citsent citstr=" W05-1513 ">
this model is inspired by sagae and lavie (2005), <papid> W05-1513 </papid>in which stack-based representation of monolingual parsing trees is used.</citsent>
<aftsection>
<nextsent>our contributions lie in the extension of this representation to bilingual parsing trees based on itgs and in the use of structured prediction method, called searn (daum?
</nextsent>
<nextsent>iii et al 2007), to predict parsing structures.
</nextsent>
<nextsent>furthermore, in order to facilitate the use of structured prediction method, we perform another transformation from itg-like trees to label sequence with the grouping of stack operations.
</nextsent>
<nextsent>then the structure preserving problem in translation is transferred to structured prediction one tackled by sequence labeling method such as in part-of-speech (pos) tagging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1075">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in section 5, beam search decoder with structured information is described.
</prevsent>
<prevsent>experiments are given for three european language pairs in section 6 and we conclude our paper with some discussions.
</prevsent>
</prevsection>
<citsent citstr=" P05-1069 ">
this method is similar to block-orientation modeling (tillmann and zhang 2005) <papid> P05-1069 </papid>and maximum entropy based phrase reordering model (xiong et al. 2006), <papid> P06-1066 </papid>in which local orientations (left/right) of phrase pairs (blocks) are learned via maxent clas sifiers.</citsent>
<aftsection>
<nextsent>however, we assign shift/reduce labeling of itgs taken from the shift-reduce parsing, and classifier is learned via searn.
</nextsent>
<nextsent>this paper is more elaborated by assigning detailed stack operations.
</nextsent>
<nextsent>the use of structured prediction to smt is also investigated by (liang et al 2006; <papid> P06-1096 </papid>tillmann and zhang 2006; <papid> P06-1091 </papid>watanabe et al 2007).<papid> D07-1080 </papid></nextsent>
<nextsent>in contrast, we use searn to estimate one bilingual parsing tree for each sentence pair from its word corre spondences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1076">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in section 5, beam search decoder with structured information is described.
</prevsent>
<prevsent>experiments are given for three european language pairs in section 6 and we conclude our paper with some discussions.
</prevsent>
</prevsection>
<citsent citstr=" P06-1066 ">
this method is similar to block-orientation modeling (tillmann and zhang 2005) <papid> P05-1069 </papid>and maximum entropy based phrase reordering model (xiong et al. 2006), <papid> P06-1066 </papid>in which local orientations (left/right) of phrase pairs (blocks) are learned via maxent clas sifiers.</citsent>
<aftsection>
<nextsent>however, we assign shift/reduce labeling of itgs taken from the shift-reduce parsing, and classifier is learned via searn.
</nextsent>
<nextsent>this paper is more elaborated by assigning detailed stack operations.
</nextsent>
<nextsent>the use of structured prediction to smt is also investigated by (liang et al 2006; <papid> P06-1096 </papid>tillmann and zhang 2006; <papid> P06-1091 </papid>watanabe et al 2007).<papid> D07-1080 </papid></nextsent>
<nextsent>in contrast, we use searn to estimate one bilingual parsing tree for each sentence pair from its word corre spondences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1077">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, we assign shift/reduce labeling of itgs taken from the shift-reduce parsing, and classifier is learned via searn.
</prevsent>
<prevsent>this paper is more elaborated by assigning detailed stack operations.
</prevsent>
</prevsection>
<citsent citstr=" P06-1096 ">
the use of structured prediction to smt is also investigated by (liang et al 2006; <papid> P06-1096 </papid>tillmann and zhang 2006; <papid> P06-1091 </papid>watanabe et al 2007).<papid> D07-1080 </papid></citsent>
<aftsection>
<nextsent>in contrast, we use searn to estimate one bilingual parsing tree for each sentence pair from its word correspondences.
</nextsent>
<nextsent>as consequence, the generation of target language sentences is assisted by this structured information.
</nextsent>
<nextsent>turian et al (2006) propose purely discriminative learning method for parsing and translation with tree structured models.
</nextsent>
<nextsent>the word alignments and english parse tree were fed into the genpar system (burbank et al 2005) to produce binarized tree alignments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1078">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, we assign shift/reduce labeling of itgs taken from the shift-reduce parsing, and classifier is learned via searn.
</prevsent>
<prevsent>this paper is more elaborated by assigning detailed stack operations.
</prevsent>
</prevsection>
<citsent citstr=" P06-1091 ">
the use of structured prediction to smt is also investigated by (liang et al 2006; <papid> P06-1096 </papid>tillmann and zhang 2006; <papid> P06-1091 </papid>watanabe et al 2007).<papid> D07-1080 </papid></citsent>
<aftsection>
<nextsent>in contrast, we use searn to estimate one bilingual parsing tree for each sentence pair from its word correspondences.
</nextsent>
<nextsent>as consequence, the generation of target language sentences is assisted by this structured information.
</nextsent>
<nextsent>turian et al (2006) propose purely discriminative learning method for parsing and translation with tree structured models.
</nextsent>
<nextsent>the word alignments and english parse tree were fed into the genpar system (burbank et al 2005) to produce binarized tree alignments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1079">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, we assign shift/reduce labeling of itgs taken from the shift-reduce parsing, and classifier is learned via searn.
</prevsent>
<prevsent>this paper is more elaborated by assigning detailed stack operations.
</prevsent>
</prevsection>
<citsent citstr=" D07-1080 ">
the use of structured prediction to smt is also investigated by (liang et al 2006; <papid> P06-1096 </papid>tillmann and zhang 2006; <papid> P06-1091 </papid>watanabe et al 2007).<papid> D07-1080 </papid></citsent>
<aftsection>
<nextsent>in contrast, we use searn to estimate one bilingual parsing tree for each sentence pair from its word correspondences.
</nextsent>
<nextsent>as consequence, the generation of target language sentences is assisted by this structured information.
</nextsent>
<nextsent>turian et al (2006) propose purely discriminative learning method for parsing and translation with tree structured models.
</nextsent>
<nextsent>the word alignments and english parse tree were fed into the genpar system (burbank et al 2005) to produce binarized tree alignments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1080">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> transformation.  </section>
<citcontext>
<prevsection>
<prevsent>in our method, we predict tree structures from word alignments through several transformations without involving parser and/or tree alignments.
</prevsent>
<prevsent>3.1 word alignments and itg-like tree.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
first, following koehn et al (2003), <papid> N03-1017 </papid>bilingual sentences are trained by giza++ (och and ney 2003) <papid> J03-1002 </papid>in two directions (from source to target and target to source).</citsent>
<aftsection>
<nextsent>then, two resulting alignments are recombined to form whole according to heuristic rules, e.g. grow-diag-final.
</nextsent>
<nextsent>second, based on the word alignment matrix, one unique parsing tree can be generated according to itg constraints where the left-first?
</nextsent>
<nextsent>constraint is posed.
</nextsent>
<nextsent>that is to say, we always make the leaf nodes as the right sons as possible as they can.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1081">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> transformation.  </section>
<citcontext>
<prevsection>
<prevsent>in our method, we predict tree structures from word alignments through several transformations without involving parser and/or tree alignments.
</prevsent>
<prevsent>3.1 word alignments and itg-like tree.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
first, following koehn et al (2003), <papid> N03-1017 </papid>bilingual sentences are trained by giza++ (och and ney 2003) <papid> J03-1002 </papid>in two directions (from source to target and target to source).</citsent>
<aftsection>
<nextsent>then, two resulting alignments are recombined to form whole according to heuristic rules, e.g. grow-diag-final.
</nextsent>
<nextsent>second, based on the word alignment matrix, one unique parsing tree can be generated according to itg constraints where the left-first?
</nextsent>
<nextsent>constraint is posed.
</nextsent>
<nextsent>that is to say, we always make the leaf nodes as the right sons as possible as they can.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1083">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> decoder.  </section>
<citcontext>
<prevsection>
<prevsent>instead of extracting all possible phrases from word alignments, we consider those translation pairs from the nodes of itg-like trees only.
</prevsent>
<prevsent>like pharaoh, we calculate their probability as combination of 5 constituents: phrase translation probability (in both directions), lexical translation probability (in both directions) and phrase penalty (default is set at 2.718).
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the corresponding weight is trained through minimum error rate method (och 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>parameters of this part can be calculated in advance once tree structures are generated and can be stored as phrase translation table.
</nextsent>
<nextsent>5.1 core algorithm.
</nextsent>
<nextsent>another important question is how to preserve sentence structures during decoding.
</nextsent>
<nextsent>a left-to right monotonous search procedure is needed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1084">
<title id=" I08-2087.xml">a structured prediction approach for statistical machine translation </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>finally, the performance can be further improved if we use more powerful classifier (such as svm or me) with more iterations.
</prevsent>
<prevsent>653
</prevsent>
</prevsection>
<citsent citstr=" J99-4005 ">
our method provides simple and efficient way to solve the word ordering problem partially which is np-hard (knight 1999).<papid> J99-4005 </papid></citsent>
<aftsection>
<nextsent>it is word based except for those indecomposable word sequences under itgs.
</nextsent>
<nextsent>however, it can achieve comparable results with phrase-based method (like pharaoh), while much fewer translation options are used.
</nextsent>
<nextsent>for the structure prediction process, only 3 common features are preserved and perceptron-based classifiers are chosen for the use of simplicity.
</nextsent>
<nextsent>we argue that this approach is promising when more features and more powerful classifiers are used as daum?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1085">
<title id=" I05-2020.xml">effect of domain specific corpus in compositional translation estimation for technical terms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper studies issues on compiling bilingual lexicon for technical terms.
</prevsent>
<prevsent>so far, several techniques of estimating bilingual term correspondences from parallel/comparable corpus have been studied (matsumoto and utsuro, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
for example, in the case of estimation from comparable corpora, (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999)<papid> P99-1067 </papid>proposed standard techniques of estimating bilingual term correspondences from comparable cor pora.</citsent>
<aftsection>
<nextsent>in their techniques, contextual similarity between source language term and its translation candidate is measured across the languages,and all the translation candidates are re-ranked according to the contextual similarities.
</nextsent>
<nextsent>however, collecting terms of specific domain/topic (language ) xsu (# of translations is one) compiled bilingual lexicon process data collecting corpus (language ) domain/topic specific corpus (language ) sample terms of specific domain/topic (language ) xstu , xstm ,yst estimating bilingual term correspondences language pair (s,t ) term set (language ) xtu (lang.
</nextsent>
<nextsent>t ) translation set (language ) web (language ) web (language ) existing bilingual lexicon xsm (# of translations is more than one) ys (# of translations is zero) web (language ) web (language ) looking up bilingual lexicon validating translation candidates figure 1: compilation of domain/topic specific bilingual lexicon there are limited number of parallel/comparablecorpora that are available for the purpose of estimating bilingual term correspondences.
</nextsent>
<nextsent>therefore, even if one wants to apply those existing techniques to the task of estimating bilingual term correspondences of technical terms, it is usually quite difficult to find an existing corpus for the domain of such technical terms.considering such situation, we take an approach of collecting corpus for the domain ofsuch technical terms from the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1086">
<title id=" I05-2020.xml">effect of domain specific corpus in compositional translation estimation for technical terms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper studies issues on compiling bilingual lexicon for technical terms.
</prevsent>
<prevsent>so far, several techniques of estimating bilingual term correspondences from parallel/comparable corpus have been studied (matsumoto and utsuro, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
for example, in the case of estimation from comparable corpora, (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999)<papid> P99-1067 </papid>proposed standard techniques of estimating bilingual term correspondences from comparable cor pora.</citsent>
<aftsection>
<nextsent>in their techniques, contextual similarity between source language term and its translation candidate is measured across the languages,and all the translation candidates are re-ranked according to the contextual similarities.
</nextsent>
<nextsent>however, collecting terms of specific domain/topic (language ) xsu (# of translations is one) compiled bilingual lexicon process data collecting corpus (language ) domain/topic specific corpus (language ) sample terms of specific domain/topic (language ) xstu , xstm ,yst estimating bilingual term correspondences language pair (s,t ) term set (language ) xtu (lang.
</nextsent>
<nextsent>t ) translation set (language ) web (language ) web (language ) existing bilingual lexicon xsm (# of translations is more than one) ys (# of translations is zero) web (language ) web (language ) looking up bilingual lexicon validating translation candidates figure 1: compilation of domain/topic specific bilingual lexicon there are limited number of parallel/comparablecorpora that are available for the purpose of estimating bilingual term correspondences.
</nextsent>
<nextsent>therefore, even if one wants to apply those existing techniques to the task of estimating bilingual term correspondences of technical terms, it is usually quite difficult to find an existing corpus for the domain of such technical terms.considering such situation, we take an approach of collecting corpus for the domain ofsuch technical terms from the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1087">
<title id=" I05-2020.xml">effect of domain specific corpus in compositional translation estimation for technical terms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>among those two issues, this paper focuses on the second issue of translation estimation of technical terms, and proposes method for translation estimation for technical terms using domain/topic specific corpus collected from the web.
</prevsent>
<prevsent>more specifically, the overall framework of 114 compiling bilingual lexicon from the web can be illustrated as in figure 1.
</prevsent>
</prevsection>
<citsent citstr=" P03-2020 ">
suppose that we have sample terms of specific domain/topic, technical terms to be listed as the headwords of bilingual lexicon are collected from the web by there lated term collection method of (sato and sasaki,2003).<papid> P03-2020 </papid></citsent>
<aftsection>
<nextsent>those collected technical terms can be divided into three subsets according to the number of translation candidates they have in an existing bilingual lexicon, i.e., the subset xus of terms for which the number of translations in the existing bilingual lexicon is one, the subset xms of terms for which the number of translations is more than one, and the subset ys of terms which are not found in the existing bilingual lexicon.
</nextsent>
<nextsent>(hence forth, the union xus ? xms is denoted as xs .) the translation estimation task here is to estimate translations for the terms of xms and ys . for the terms of xms , it is required to select an appropriate translation from the translation candidates found in the existing bilingual lexicon.
</nextsent>
<nextsent>forex ample, as translation of the japanese technical term ??????, which belongs to the logic circuit field, the term register?
</nextsent>
<nextsent>should be selected but not the term regista?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1088">
<title id=" I05-2020.xml">effect of domain specific corpus in compositional translation estimation for technical terms </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>as related work, (fujii and ishikawa, 2001)proposed technique of compositional estimation of bilingual term correspondences for the purpose of cross-language information retrieval.in (fujii and ishikawa, 2001), bilingual constituents lexicon is compiled from the translation pairs included in an existing bilingual lexicon in the same way as our proposed method.
</prevsent>
<prevsent>one of the major differences of the technique of (fujii and ishikawa, 2001) and the one proposed in this paper is that in (fujii and ishikawa, 2001), instead ofthe domain/topic specific corpus, they use corpus of the collection of the technical papers, each of which is published by one of the 65 japanese associations for various technical domains.
</prevsent>
</prevsection>
<citsent citstr=" C02-1011 ">
an other important difference is that in (fujii and ishikawa, 2001), they evaluate only the performance of cross-language information retrieval but not that of translation estimation.(cao and li, 2002) <papid> C02-1011 </papid>proposed method of compositional translation estimation for compounds.</citsent>
<aftsection>
<nextsent>in the proposed method of (cao and li, 2002),translation candidates of term are compositionally generated by concatenating the translation of the constituents of the term and are re-ranked by measuring contextual similarity against the source language term.
</nextsent>
<nextsent>one of the major differences of the technique of (cao and li, 2002) <papid> C02-1011 </papid>and the one proposed in this paper is that in (cao andli, 2002), they do not use the domain/topic specific corpus.</nextsent>
<nextsent>this paper proposed method of compositional translation estimation for technical terms using the domain/topic specific corpus, and through the experimental evaluation, showed that the domain/topic specific corpus contributes to improving the performance of compositional translation estimation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1092">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, supervised machine learning has be come the standard approach to obtain robust and wide-coverage nlp tools.
</prevsent>
<prevsent>but manually annotated training data is scarce and expensive resource.
</prevsent>
</prevsection>
<citsent citstr=" N01-1026 ">
annotation projection (yarowsky and ngai, 2001) <papid> N01-1026 </papid>aims at overcoming this resource bottleneck by scaling conceptually monolingual resources and tools to amultilingual level: annotations in existing monolingual corpora are transferred to different language along the word alignment to parallel corpus.</citsent>
<aftsection>
<nextsent>in this paper, we present projection framework for temporal annotations.
</nextsent>
<nextsent>the timeml specification language (pustejovsky et al, 2003a) defines an annotation scheme for time expressions (timex for ? the first author was affiliated with saarland university (saarbrucken, germany) at the time of writing.
</nextsent>
<nextsent>john [met] event mary [last night] timex . john [traf] event mary [gestern abend] timex . figure 1: annotation projection.short) and events, and there are tools for the automatic timeml annotation of english text (verha genet al, 2005).<papid> P05-3021 </papid></nextsent>
<nextsent>similar rule-based systems exist for spanish and italian (saquete et al, 2006).<papid> W06-2001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1093">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we present projection framework for temporal annotations.
</prevsent>
<prevsent>the timeml specification language (pustejovsky et al, 2003a) defines an annotation scheme for time expressions (timex for ? the first author was affiliated with saarland university (saarbrucken, germany) at the time of writing.
</prevsent>
</prevsection>
<citsent citstr=" P05-3021 ">
john [met] event mary [last night] timex . john [traf] event mary [gestern abend] timex . figure 1: annotation projection.short) and events, and there are tools for the automatic timeml annotation of english text (verha genet al, 2005).<papid> P05-3021 </papid></citsent>
<aftsection>
<nextsent>similar rule-based systems exist for spanish and italian (saquete et al, 2006).<papid> W06-2001 </papid></nextsent>
<nextsent>how ever, such resources are restricted to handful of languages.we employ the existing timeml labellers to annotate the english portion of parallel corpus, and automatically project the annotations to the word aligned german translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1094">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the timeml specification language (pustejovsky et al, 2003a) defines an annotation scheme for time expressions (timex for ? the first author was affiliated with saarland university (saarbrucken, germany) at the time of writing.
</prevsent>
<prevsent>john [met] event mary [last night] timex . john [traf] event mary [gestern abend] timex . figure 1: annotation projection.short) and events, and there are tools for the automatic timeml annotation of english text (verha genet al, 2005).<papid> P05-3021 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-2001 ">
similar rule-based systems exist for spanish and italian (saquete et al, 2006).<papid> W06-2001 </papid></citsent>
<aftsection>
<nextsent>how ever, such resources are restricted to handful of languages.we employ the existing timeml labellers to annotate the english portion of parallel corpus, and automatically project the annotations to the word aligned german translation.
</nextsent>
<nextsent>fig.
</nextsent>
<nextsent>1 shows simple example.
</nextsent>
<nextsent>the english sentence contains an event and timex annotation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1096">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to our knowledge, the present proposal is the first to apply projection algorithms to temporal annotations.
</prevsent>
<prevsent>489 cross-lingually projected information is typically noisy, due to errors in the source annotations aswell as in the word alignment.
</prevsent>
</prevsection>
<citsent citstr=" P02-1050 ">
moreover, successful projection relies on the direct correspondence assumption (dca, hwa et al (2002)) <papid> P02-1050 </papid>which demands that the annotations in the source text behomomorphous with those in its (literal) translation.</citsent>
<aftsection>
<nextsent>the dca has been found to hold, to substantial degree, for the abovementioned domains.
</nextsent>
<nextsent>the results we report here show that it can also be confirmed for temporal annotations in english and german.
</nextsent>
<nextsent>yet, we cannot preclude divergence from translational correspondence; on the contrary,it occurs routinely and to certain extent systematically (dorr, 1994).<papid> J94-4004 </papid></nextsent>
<nextsent>we employ two different techniques to filter noise.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1097">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the dca has been found to hold, to substantial degree, for the abovementioned domains.
</prevsent>
<prevsent>the results we report here show that it can also be confirmed for temporal annotations in english and german.
</prevsent>
</prevsection>
<citsent citstr=" J94-4004 ">
yet, we cannot preclude divergence from translational correspondence; on the contrary,it occurs routinely and to certain extent systematically (dorr, 1994).<papid> J94-4004 </papid></citsent>
<aftsection>
<nextsent>we employ two different techniques to filter noise.
</nextsent>
<nextsent>firstly, the projection proces sis equipped with (partly language-specific) knowledge for principled account of typical alignment errors and cross-language discrepancies in the realisation of events and timexes (section 3.2).
</nextsent>
<nextsent>secondly, we apply aggressive data engineering techniques to the noisy projections and use them to train statistical classifiers which generalise beyond the noise (sec tion 5).
</nextsent>
<nextsent>the paper is structured as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1099">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> temporal annotation.  </section>
<citcontext>
<prevsection>
<prevsent>they have been modelled and evaluated on the basis of the time bank (pustejovsky et al, 2003b), yet for the most part relyon hand-crafted rules.
</prevsent>
<prevsent>to obtain full temporal annotation, the modules are combined in cascade.
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
we are using the components for timex recognition and normalisation (mani and wilson, 2000),<papid> P00-1010 </papid>event extraction (saur??</citsent>
<aftsection>
<nextsent>et al, 2005a), and identification of modal contexts (saur??
</nextsent>
<nextsent>et al, 2006).3
</nextsent>
<nextsent>3.1 the core algorithm.
</nextsent>
<nextsent>recall that timeml represents temporal entities with event and timex3 tags which are anchored to words in the text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1100">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> informed projection.  </section>
<citcontext>
<prevsection>
<prevsent>project to non-emptysequences of words.
</prevsent>
<prevsent>the projection of the entities e, e?
</prevsent>
</prevsection>
<citsent citstr=" N03-2019 ">
themselves, however, is non-trivial task.3tarsqi also comprises component that introduces temporal links (mani et al, 2003); <papid> N03-2019 </papid>we are not using it here because the output includes the entire tlink closure.</citsent>
<aftsection>
<nextsent>although mani et al (2006) <papid> P06-1095 </papid>use the links introduced by closure to boost the amount of training data for tlink classifier, this technique is not suitable for our learning task since the closure might easily propagate errors in the automatic annotations.</nextsent>
<nextsent>a..</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1101">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> informed projection.  </section>
<citcontext>
<prevsection>
<prevsent>the projection of the entities e, e?
</prevsent>
<prevsent>themselves, however, is non-trivial task.3tarsqi also comprises component that introduces temporal links (mani et al, 2003); <papid> N03-2019 </papid>we are not using it here because the output includes the entire tlink closure.</prevsent>
</prevsection>
<citsent citstr=" P06-1095 ">
although mani et al (2006) <papid> P06-1095 </papid>use the links introduced by closure to boost the amount of training data for tlink classifier, this technique is not suitable for our learning task since the closure might easily propagate errors in the automatic annotations.</citsent>
<aftsection>
<nextsent>a..
</nextsent>
<nextsent>[ ws ]e . . .
</nextsent>
<nextsent>b. . . .
</nextsent>
<nextsent>[ ws ]e . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1103">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> informed projection.  </section>
<citcontext>
<prevsection>
<prevsent>the additional padding introduced by the convex hull (at,ch) further extends the projected extent to {1, 2, 3, 4, 5}.
</prevsent>
<prevsent>alignment selection.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
although bi-alignments are known to exhibit high precision (koehn et al, 2003),<papid> N03-1017 </papid>in the face of sparse annotations we use unidirectional alignments as fallback, as has been proposed in the context of phrase-based machine translation (koehn et al, 2003; <papid> N03-1017 </papid>tillmann, 2003).<papid> W03-1001 </papid></citsent>
<aftsection>
<nextsent>furthermore, we follow hwa et al (2005) in imposing limit on the maximum number of words that single word may align to.
</nextsent>
<nextsent>our evaluation setup consists of experiments conducted on the english-german portion of the eu roparl corpus (koehn, 2005); specifically, we work with the preprocessed and word-aligned version used in pado?
</nextsent>
<nextsent>and lapata (2006): the source-targetand target-source word alignments were automatically established by giza++ (och and ney, 2003), <papid> J03-1002 </papid>and their intersection achieves precision of 98.6%and recall of 52.9% (pado?, 2007).</nextsent>
<nextsent>the preprocessing consisted of automatic pos tagging and lemma tisation.to assess the quality of the timeml projections, we put aside and manually annotated development set of 101 and test set of 236 bisentences.4 all remaining data (approx.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1105">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> informed projection.  </section>
<citcontext>
<prevsection>
<prevsent>the additional padding introduced by the convex hull (at,ch) further extends the projected extent to {1, 2, 3, 4, 5}.
</prevsent>
<prevsent>alignment selection.
</prevsent>
</prevsection>
<citsent citstr=" W03-1001 ">
although bi-alignments are known to exhibit high precision (koehn et al, 2003),<papid> N03-1017 </papid>in the face of sparse annotations we use unidirectional alignments as fallback, as has been proposed in the context of phrase-based machine translation (koehn et al, 2003; <papid> N03-1017 </papid>tillmann, 2003).<papid> W03-1001 </papid></citsent>
<aftsection>
<nextsent>furthermore, we follow hwa et al (2005) in imposing limit on the maximum number of words that single word may align to.
</nextsent>
<nextsent>our evaluation setup consists of experiments conducted on the english-german portion of the eu roparl corpus (koehn, 2005); specifically, we work with the preprocessed and word-aligned version used in pado?
</nextsent>
<nextsent>and lapata (2006): the source-targetand target-source word alignments were automatically established by giza++ (och and ney, 2003), <papid> J03-1002 </papid>and their intersection achieves precision of 98.6%and recall of 52.9% (pado?, 2007).</nextsent>
<nextsent>the preprocessing consisted of automatic pos tagging and lemma tisation.to assess the quality of the timeml projections, we put aside and manually annotated development set of 101 and test set of 236 bisentences.4 all remaining data (approx.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1106">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we follow hwa et al (2005) in imposing limit on the maximum number of words that single word may align to.
</prevsent>
<prevsent>our evaluation setup consists of experiments conducted on the english-german portion of the eu roparl corpus (koehn, 2005); specifically, we work with the preprocessed and word-aligned version used in pado?
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
and lapata (2006): the source-targetand target-source word alignments were automatically established by giza++ (och and ney, 2003), <papid> J03-1002 </papid>and their intersection achieves precision of 98.6%and recall of 52.9% (pado?, 2007).</citsent>
<aftsection>
<nextsent>the preprocessing consisted of automatic pos tagging and lemma tisation.to assess the quality of the timeml projections, we put aside and manually annotated development set of 101 and test set of 236 bisentences.4 all remaining data (approx.
</nextsent>
<nextsent>960k bi sentences) was used for training (section 5).
</nextsent>
<nextsent>we report the weighted macro average over all possible subclasses of timexes/events, and consider only exact matches.
</nextsent>
<nextsent>the tarsqi annotations exhibit an 1 -measure of 80.56% (timex), 84.64% (events),and 43.32% (slinks) when evaluated against the english gold standard.in order to assess the usefulness of the linguistic and topological parameters presented in section 3.2, we determined the best performing combination of parameters on the development set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1111">
<title id=" I08-1064.xml">projection based acquisition of a temporal lab eller </title>
<section> robust induction.  </section>
<citcontext>
<prevsection>
<prevsent>fic ation task.
</prevsent>
<prevsent>the remaining instances6 are converted to feature vectors encoding standard lexical and grammatical features such as (lower case) lemma, pos, governing prepositions, verbal dependents, etc.7 for slink instances, we further encode the syntactic subordination path (if any) between the two events.
</prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
we trained 4 classifiers,8 with and without smoothing with artificial unknowns (collins, 2003), <papid> J03-4003 </papid>and as 1-step versus 2-step decision in which instances are first discriminated by binary classifier, so that only positive instances are passed on to be classified for subclass.</citsent>
<aftsection>
<nextsent>the performance of the various classifiers is given in table 4.
</nextsent>
<nextsent>although the overall 1 -measure does not notably differ from that achieved by direct projection, we observe drastic gain in precision, albeit at the cost of recall.
</nextsent>
<nextsent>with almost 84% and 90% precision, this is an ideal starting point for bootstrapping procedure.
</nextsent>
<nextsent>clearly, the essentially unsupervisedprojectionframework presented here does not produce stateof-the-art annotations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1112">
<title id=" I05-3001.xml">detecting segmentation errors in chinese annotated corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>especially segmentation error rate of gold standard corpus could be obtained with our approach.
</prevsent>
<prevsent>as we know particular chinese character string occurring more than once in corpus may be assigned different segmentations.
</prevsent>
</prevsection>
<citsent citstr=" W03-1727 ">
those differences are considered as segmentation inconsistencies by some researchers (wu, 2003; <papid> W03-1727 </papid>chen, 2003).<papid> W03-1721 </papid></citsent>
<aftsection>
<nextsent>segmentation consistency is also considered as one of the quality criteria of an annotated chinese corpus (sun, 1999).
</nextsent>
<nextsent>but in order to provide more clearer description of those segmentation differences we define new 1 http://www.sighan.org/bakeoff2003/ 1 term, segmentation variation, to replace the original one, segmentation inconsistency.
</nextsent>
<nextsent>our approach of spotting segmentation errors within an annotated corpus consists of two steps: (1) automatically listing the segmentation error candidates with segmentation variations found in an annotated corpus, (2) spotting segmentation errors within those candidates manually.
</nextsent>
<nextsent>the target of this approach is to count the number of error tokens in the corpus and give the segmentation error rate of the corpus, which is not given for any gold standard corpus in bakeoff1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1113">
<title id=" I05-3001.xml">detecting segmentation errors in chinese annotated corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>especially segmentation error rate of gold standard corpus could be obtained with our approach.
</prevsent>
<prevsent>as we know particular chinese character string occurring more than once in corpus may be assigned different segmentations.
</prevsent>
</prevsection>
<citsent citstr=" W03-1721 ">
those differences are considered as segmentation inconsistencies by some researchers (wu, 2003; <papid> W03-1727 </papid>chen, 2003).<papid> W03-1721 </papid></citsent>
<aftsection>
<nextsent>segmentation consistency is also considered as one of the quality criteria of an annotated chinese corpus (sun, 1999).
</nextsent>
<nextsent>but in order to provide more clearer description of those segmentation differences we define new 1 http://www.sighan.org/bakeoff2003/ 1 term, segmentation variation, to replace the original one, segmentation inconsistency.
</nextsent>
<nextsent>our approach of spotting segmentation errors within an annotated corpus consists of two steps: (1) automatically listing the segmentation error candidates with segmentation variations found in an annotated corpus, (2) spotting segmentation errors within those candidates manually.
</nextsent>
<nextsent>the target of this approach is to count the number of error tokens in the corpus and give the segmentation error rate of the corpus, which is not given for any gold standard corpus in bakeoff1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1114">
<title id=" I05-3001.xml">detecting segmentation errors in chinese annotated corpus </title>
<section> segmentation inconsistency.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 is brief conclusion.
</prevsent>
<prevsent>in the close test of bakeoff1, participants could only use training material from the training data for the particular corpus being testing on.
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
no other material was allowed (sproat and emerson, 2003).<papid> W03-1719 </papid></citsent>
<aftsection>
<nextsent>as we know that the test data should be consistent with the training data based on general definition of chinese words.
</nextsent>
<nextsent>that is if we collect all words seen in the training data and store them into lexicon, then each word in test set is either lexicon word or an oov (out of vocabulary) word (gao et al., 2003).<papid> P03-1035 </papid></nextsent>
<nextsent>in another word, if character string has been treated as one word, i.e. lexicon word, in the training data, the same occurrence should be taken in the corresponding test data unless it is cas (combination ambiguity string) and vice versa.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1116">
<title id=" I05-3001.xml">detecting segmentation errors in chinese annotated corpus </title>
<section> segmentation inconsistency.  </section>
<citcontext>
<prevsection>
<prevsent>no other material was allowed (sproat and emerson, 2003).<papid> W03-1719 </papid></prevsent>
<prevsent>as we know that the test data should be consistent with the training data based on general definition of chinese words.</prevsent>
</prevsection>
<citsent citstr=" P03-1035 ">
that is if we collect all words seen in the training data and store them into lexicon, then each word in test set is either lexicon word or an oov (out of vocabulary) word (gao et al., 2003).<papid> P03-1035 </papid></citsent>
<aftsection>
<nextsent>in another word, if character string has been treated as one word, i.e. lexicon word, in the training data, the same occurrence should be taken in the corresponding test data unless it is cas (combination ambiguity string) and vice versa.
</nextsent>
<nextsent>as we all know that cas like  ??
</nextsent>
<nextsent>[cai2-neng2]  may be segmented into one word or two words depending on different contexts.
</nextsent>
<nextsent>thus segmentation inconsistency like  ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1118">
<title id=" I05-3001.xml">detecting segmentation errors in chinese annotated corpus </title>
<section> segmentation variation.  </section>
<citcontext>
<prevsection>
<prevsent>the definitions of segmentation variation, variation instance and error instance (ei) clearly distinguish those inconsistent components, so we can count the number of segmentation errors (in tokens) exactly.
</prevsent>
<prevsent>the term variation is also used to express other annotation inconsistency in corpus by other researchers.
</prevsent>
</prevsection>
<citsent citstr=" E03-1068 ">
for example, dickinson and meurers (2003) <papid> E03-1068 </papid>used variation to describe pos (part-of-speech) inconsistency in an annotated corpus.</citsent>
<aftsection>
<nextsent>example 1: segmentation variations (bakeoff1 pk corpus): word  ??[deng3-tong2]  is segmented as  ??
</nextsent>
<nextsent>(equal) and  ??
</nextsent>
<nextsent>(et al with).
</nextsent>
<nextsent>word  ???[huang2-jin1-zhou1]  is segmented as  ???
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1121">
<title id=" I05-1049.xml">relative compositionality of multiword expressions a study of verb noun vn collocations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as an mwe.
</prevsent>
<prevsent>the classifier can also be used to rank these vectors according to their relative compositionality.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
church and hanks (1989) <papid> P89-1010 </papid>proposed measure of association called mutual information [9].</citsent>
<aftsection>
<nextsent>mutual information (mi) is the logarithm of the ratio between the probability of the two words occurring together and the product of the probability of each word occurring individually.
</nextsent>
<nextsent>the higher the mi, the more likely are the words to be associated with each other.
</nextsent>
<nextsent>the usefulness of the statistical approach suggested by church and hanks [9] is evaluated for the extraction of v-n collocations from german text corpora [7].
</nextsent>
<nextsent>several other measures like log-likelihood [10], pearsons 2 [8], z-score [8] , cubic association ratio(mi3), log-log [17], etc., have been proposed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1122">
<title id=" I05-2042.xml">toward a unified evaluation method for multiple reading support systems a reading speed based procedure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they show that the reading-speed procedure is able to evaluate the support systems, as well as, the comprehension based procedure proposed by ohguro (1993) and fuji et al (2001).
</prevsent>
<prevsent>this paper presents an evaluation method for different reading support systems such as sen tence-machine translation system (henceforth, an mt-system) and word/phrase translation system (henceforth, w/p-mt-system).
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
although, there are various manual/automatic evaluation methods for these systems, e.g., bleu (papineni et al 2002), <papid> P02-1040 </papid>these methods are basically incapable of dealing with an mt system and w/p-mt-system at the same time, as they have different output forms.</citsent>
<aftsection>
<nextsent>on the contrary, there are further methods which examine the efficacy of these systems (ohguro 1993; fuji et al 2001).
</nextsent>
<nextsent>these studies demonstrate the effectiveness of the reading support systems by comparing reading comprehension test scores between an english-only text and the one with outputs of either an mt-system (fuji et al 2001) or w/p-mt-system (ohguro 1993).
</nextsent>
<nextsent>in our evaluation method, we examined the system based not only con comprehension but also on speed, i.e., reading efficacy (alderson 2000).
</nextsent>
<nextsent>if the system supports user in an appropriate way, then the reading efficacy would in crease from the bottom line, i.e., text without any support.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1123">
<title id=" I08-1040.xml">unsupervised classification of sentiment and objectivity in chinese text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we use novel unsupervised techniques, including one-word  seed  vocabulary and iterative retraining for sentiment processing, and criterion of sentiment density  for determining the extent to which document is opinionated.the classifier achieves up to 87% f-meas ure for sentiment polarity detection.
</prevsent>
<prevsent>automatic classification of sentiment has been focus of number of recent research efforts (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
(turney, 2002; <papid> P02-1053 </papid>pang et al, 2002; <papid> W02-1011 </papid>dave at al., 2003).</citsent>
<aftsection>
<nextsent>an important potential application of such work is in business intelligence: brands and company image are valuable property, so organizations want to know how they are viewed by the media (what the  spin  is on news stories, and editorials),business analysts (as expressed in stockmarket re ports), customers (for example on product review sites) and their own employees.
</nextsent>
<nextsent>another important application is to help people find out others  views about products they have purchased (e.g. consumer electronics), services and entertainment (e.g. movies), stocks and shares (from investor bulletinboards), and so on.
</nextsent>
<nextsent>in the work reported in this paper we focus on product reviews, with the intended users of the processing being would-be customers.our approach is based on the insight that positive and negative sentiments are extreme points in continuum of sentiment, and that intermediate points in this continuum are of potential interest.
</nextsent>
<nextsent>for instance, in one scenario, someone might wantto get an idea of the types of things people are saying about particular product through reading sample of reviews covering the spectrum from highly positive, through balanced, to highly negative.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1125">
<title id=" I08-1040.xml">unsupervised classification of sentiment and objectivity in chinese text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we use novel unsupervised techniques, including one-word  seed  vocabulary and iterative retraining for sentiment processing, and criterion of sentiment density  for determining the extent to which document is opinionated.the classifier achieves up to 87% f-meas ure for sentiment polarity detection.
</prevsent>
<prevsent>automatic classification of sentiment has been focus of number of recent research efforts (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
(turney, 2002; <papid> P02-1053 </papid>pang et al, 2002; <papid> W02-1011 </papid>dave at al., 2003).</citsent>
<aftsection>
<nextsent>an important potential application of such work is in business intelligence: brands and company image are valuable property, so organizations want to know how they are viewed by the media (what the  spin  is on news stories, and editorials),business analysts (as expressed in stockmarket re ports), customers (for example on product review sites) and their own employees.
</nextsent>
<nextsent>another important application is to help people find out others  views about products they have purchased (e.g. consumer electronics), services and entertainment (e.g. movies), stocks and shares (from investor bulletinboards), and so on.
</nextsent>
<nextsent>in the work reported in this paper we focus on product reviews, with the intended users of the processing being would-be customers.our approach is based on the insight that positive and negative sentiments are extreme points in continuum of sentiment, and that intermediate points in this continuum are of potential interest.
</nextsent>
<nextsent>for instance, in one scenario, someone might wantto get an idea of the types of things people are saying about particular product through reading sample of reviews covering the spectrum from highly positive, through balanced, to highly negative.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1127">
<title id=" I08-1040.xml">unsupervised classification of sentiment and objectivity in chinese text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with the rapid growth in textual data and the emergence of new domains of knowledge it is virtually impossible to maintain corpora of tagged data that cover all ? or even most ? areas of interest.
</prevsent>
<prevsent>the cost of manual tagging also adds to the problem.
</prevsent>
</prevsection>
<citsent citstr=" P05-2008 ">
reusing the same corpus for training classifiers for new domains is also not effective: several studies report decreased accuracy in cross-domain classification (engstrm, 2004; aue &amp; gamon, 2005) similar problem has also been observed in classification of documents created over different time periods (read, 2005).<papid> P05-2008 </papid>in this paper we describe an unsupervised classification technique which is able to build its own sentiment vocabulary starting from very small seed vocabulary, using iterative retraining to enlarge the vocabulary.</citsent>
<aftsection>
<nextsent>in order to avoid problems of domain dependence, the vocabulary is built using text from the same source as the text which is to be classified.
</nextsent>
<nextsent>in this paper we work with chinese, but using very small seed vocabulary may mean that this approach would in principle need very little linguistic adjustment to be applied to different 305language.
</nextsent>
<nextsent>written chinese has some specific features, one of which is the absence of explicitly marked word boundaries, which makes word-basedprocessing problematic.
</nextsent>
<nextsent>in keeping with our unsupervised, knowledge-poor approach, we do not use any preliminary word segmentation tools or higher level grammatical analysis.the paper is structured as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1132">
<title id=" I08-1040.xml">unsupervised classification of sentiment and objectivity in chinese text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 sentiment classification.
</prevsent>
<prevsent>most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment (turney, 2002; <papid> P02-1053 </papid>pang et al, 2002; <papid> W02-1011 </papid>dave at al., 2003).</prevsent>
</prevsection>
<citsent citstr=" P05-1015 ">
however, pang &amp; lee (2005) <papid> P05-1015 </papid>describe an approach closer to ours in which they determine an author evaluation with respect to multi-point scale, similar to the  five-star  sentiment scale widely used on review sites.</citsent>
<aftsection>
<nextsent>however, authors of reviews are inconsistent in assigning fine-grainedratings and quite often star systems are not consistent between critics.
</nextsent>
<nextsent>this makes their approach very author-dependent.
</nextsent>
<nextsent>the main differences are that pang and lee use discrete classes (althoughmore than two), not continuum as in our approach, and use supervised machine learning rather than unsupervised techniques.
</nextsent>
<nextsent>a similar approach was adopted by hagedorn et al (2007), applied to news stories: they defined five classes encoding sentiment intensity and trained their classifier on manually tagged training corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1133">
<title id=" I08-1040.xml">unsupervised classification of sentiment and objectivity in chinese text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the main differences are that pang and lee use discrete classes (althoughmore than two), not continuum as in our approach, and use supervised machine learning rather than unsupervised techniques.
</prevsent>
<prevsent>a similar approach was adopted by hagedorn et al (2007), applied to news stories: they defined five classes encoding sentiment intensity and trained their classifier on manually tagged training corpus.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
they note that world knowledge is necessary for accurate classification in such open-ended domains.there has also been previous work on determining whether given text is factual or expresses opinion (yu&amp; hatzivassiloglu, 2003; pang &amp; lee, 2004); <papid> P04-1035 </papid>again this work uses binary distinction,and supervised rather than unsupervised approach es.recent work on classification of terms with respect to opinion (esuli &amp; sebastiani, 2006) uses athree-category system to characterize the opinion related properties of word meanings, assigning numerical scores to positive, negative and objectivecategories.</citsent>
<aftsection>
<nextsent>the visualization of these scores some what resembles our graphs in section 5, although we use two orthogonal scales rather than three cat egories; we are also concerned with classification of documents rather than terms.
</nextsent>
<nextsent>2.2 unsupervised classification.
</nextsent>
<nextsent>abney (2002) compares two major kinds of unsupervised approach to classification (co-training andthe yarowsky algorithm).
</nextsent>
<nextsent>as we do not use multiple classifiers our approach is quite far from co training.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1134">
<title id=" I08-1040.xml">unsupervised classification of sentiment and objectivity in chinese text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>abney (2002) compares two major kinds of unsupervised approach to classification (co-training andthe yarowsky algorithm).
</prevsent>
<prevsent>as we do not use multiple classifiers our approach is quite far from co training.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
but it is close to the paradigm described by yarowsky (1995) <papid> P95-1026 </papid>and turney (2002) <papid> P02-1053 </papid>as it also employs self-training based on relatively small seed dataset which is incrementally enlarged with un labelled samples.</citsent>
<aftsection>
<nextsent>but our approach does not usepoint-wise mutual information.
</nextsent>
<nextsent>instead we use relative frequencies of newly found features in at raining sub corpus produced by the previous iteration of the classifier.
</nextsent>
<nextsent>we also use the smallest possible seed vocabulary, containing just single word; however there are no restrictions regarding the maximum number of items in the seed vocabulary.
</nextsent>
<nextsent>3.1 seed vocabulary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1136">
<title id=" I08-3014.xml">an optimal order of factors for the computational treatment of personal anaphoric devices in urdu discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples of third person ads are ???
</prevsent>
<prevsent>([vh], [sei], [sk?], [ski], [skei], [sk??], [n], [nki], [nkei], [nk?], [nhei?]).
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
a lot of work has been done in english for the purpose of anaphora resolution and various 81 algorithms have been devised for this purpose (aone and bennette, 1996; brenan , friedman and pollard, 1987; ge, hale and charniak, 1998; grosz, aravind and weinstein, 1995; mccarthy and lehnert, 1995; lappins and leass, 1994; mitkov, 1998; <papid> P98-2143 </papid>soon, ng and lim, 1999).</citsent>
<aftsection>
<nextsent>work has also been done in south asian languages such as hindi and malay alam for the purpose of anaphora resolution (prasad and strube, 2000; sobha, 1998).
</nextsent>
<nextsent>prasad and strube (2000) worked on anaphora resolution in hindi.
</nextsent>
<nextsent>their approach relies on the discourse salience factors and is primarily inspired by the central idea of centering theory (grosz, aravind and weinstein, 1995).
</nextsent>
<nextsent>centering theory has also guided the development of pronoun resolution algorithms, such as the bfp algorithm (brenan, friedman and pollard, 1987) and the s-list algorithm developed by strube (strube, 1998).<papid> P98-2204 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1137">
<title id=" I08-3014.xml">an optimal order of factors for the computational treatment of personal anaphoric devices in urdu discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>prasad and strube (2000) worked on anaphora resolution in hindi.
</prevsent>
<prevsent>their approach relies on the discourse salience factors and is primarily inspired by the central idea of centering theory (grosz, aravind and weinstein, 1995).
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
centering theory has also guided the development of pronoun resolution algorithms, such as the bfp algorithm (brenan, friedman and pollard, 1987) and the s-list algorithm developed by strube (strube, 1998).<papid> P98-2204 </papid></citsent>
<aftsection>
<nextsent>prasad and strube (2000) applied these algorithms to the resolution of pronouns in hindi texts.
</nextsent>
<nextsent>they showed that the bfp algorithm cannot be successfully implemented for pronoun resolution in hindi.
</nextsent>
<nextsent>they argued that better results can be obtained with an algorithm that does not use the centering notions of the backward-looking center and the centering transitions for the computation of pronominal antecedents, such as the s-list algorithm (prasad and strube, 2000).
</nextsent>
<nextsent>prasad and strube used well established approaches for hindi anaphora resolution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1139">
<title id=" I05-5004.xml">a class oriented approach to building a paraphrase corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers be cause of its potential contribution to broad range of natural language applications.
</prevsent>
<prevsent>paraphrases can be viewed as monolingual translations.
</prevsent>
</prevsection>
<citsent citstr=" A00-1009 ">
from this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (mt), such as transformation algorithms (lavoie et al, 2000; <papid> A00-1009 </papid>takahashi et al, 2001), corpus-based techniques for paraphrase pattern acquisition (barzilay and mckeown, 2001; <papid> P01-1008 </papid>shinyama and sekine, 2003; <papid> W03-1609 </papid>quirk et al, 2004), <papid> W04-3219 </papid>and fluency measurements (lapata, 2001; <papid> N01-1009 </papid>fujita et al, 2004).</citsent>
<aftsection>
<nextsent>one thing the paraphrasing community is still lacking is shared collections of paraphrase examples that could be used to analyze problems underlying the tasks and to evaluate the performance of systems underdevelopment.
</nextsent>
<nextsent>to our best knowledge, the paraphrase corpus developed bydolan et al (2004) <papid> C04-1051 </papid>is one of the very few collections available for free1.</nextsent>
<nextsent>development of paraphrase corpora raises several issues: what sortsof paraphrases should be collected, where paraphrase examples can be obtained from, how the coverage and quality of the corpus can be ensured,how manual annotation cost can be effectively reduced, and how collected examples should be organized and annotated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1140">
<title id=" I05-5004.xml">a class oriented approach to building a paraphrase corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers be cause of its potential contribution to broad range of natural language applications.
</prevsent>
<prevsent>paraphrases can be viewed as monolingual translations.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
from this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (mt), such as transformation algorithms (lavoie et al, 2000; <papid> A00-1009 </papid>takahashi et al, 2001), corpus-based techniques for paraphrase pattern acquisition (barzilay and mckeown, 2001; <papid> P01-1008 </papid>shinyama and sekine, 2003; <papid> W03-1609 </papid>quirk et al, 2004), <papid> W04-3219 </papid>and fluency measurements (lapata, 2001; <papid> N01-1009 </papid>fujita et al, 2004).</citsent>
<aftsection>
<nextsent>one thing the paraphrasing community is still lacking is shared collections of paraphrase examples that could be used to analyze problems underlying the tasks and to evaluate the performance of systems underdevelopment.
</nextsent>
<nextsent>to our best knowledge, the paraphrase corpus developed bydolan et al (2004) <papid> C04-1051 </papid>is one of the very few collections available for free1.</nextsent>
<nextsent>development of paraphrase corpora raises several issues: what sortsof paraphrases should be collected, where paraphrase examples can be obtained from, how the coverage and quality of the corpus can be ensured,how manual annotation cost can be effectively reduced, and how collected examples should be organized and annotated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1141">
<title id=" I05-5004.xml">a class oriented approach to building a paraphrase corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers be cause of its potential contribution to broad range of natural language applications.
</prevsent>
<prevsent>paraphrases can be viewed as monolingual translations.
</prevsent>
</prevsection>
<citsent citstr=" W03-1609 ">
from this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (mt), such as transformation algorithms (lavoie et al, 2000; <papid> A00-1009 </papid>takahashi et al, 2001), corpus-based techniques for paraphrase pattern acquisition (barzilay and mckeown, 2001; <papid> P01-1008 </papid>shinyama and sekine, 2003; <papid> W03-1609 </papid>quirk et al, 2004), <papid> W04-3219 </papid>and fluency measurements (lapata, 2001; <papid> N01-1009 </papid>fujita et al, 2004).</citsent>
<aftsection>
<nextsent>one thing the paraphrasing community is still lacking is shared collections of paraphrase examples that could be used to analyze problems underlying the tasks and to evaluate the performance of systems underdevelopment.
</nextsent>
<nextsent>to our best knowledge, the paraphrase corpus developed bydolan et al (2004) <papid> C04-1051 </papid>is one of the very few collections available for free1.</nextsent>
<nextsent>development of paraphrase corpora raises several issues: what sortsof paraphrases should be collected, where paraphrase examples can be obtained from, how the coverage and quality of the corpus can be ensured,how manual annotation cost can be effectively reduced, and how collected examples should be organized and annotated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1143">
<title id=" I05-5004.xml">a class oriented approach to building a paraphrase corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers be cause of its potential contribution to broad range of natural language applications.
</prevsent>
<prevsent>paraphrases can be viewed as monolingual translations.
</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
from this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (mt), such as transformation algorithms (lavoie et al, 2000; <papid> A00-1009 </papid>takahashi et al, 2001), corpus-based techniques for paraphrase pattern acquisition (barzilay and mckeown, 2001; <papid> P01-1008 </papid>shinyama and sekine, 2003; <papid> W03-1609 </papid>quirk et al, 2004), <papid> W04-3219 </papid>and fluency measurements (lapata, 2001; <papid> N01-1009 </papid>fujita et al, 2004).</citsent>
<aftsection>
<nextsent>one thing the paraphrasing community is still lacking is shared collections of paraphrase examples that could be used to analyze problems underlying the tasks and to evaluate the performance of systems underdevelopment.
</nextsent>
<nextsent>to our best knowledge, the paraphrase corpus developed bydolan et al (2004) <papid> C04-1051 </papid>is one of the very few collections available for free1.</nextsent>
<nextsent>development of paraphrase corpora raises several issues: what sortsof paraphrases should be collected, where paraphrase examples can be obtained from, how the coverage and quality of the corpus can be ensured,how manual annotation cost can be effectively reduced, and how collected examples should be organized and annotated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1144">
<title id=" I05-5004.xml">a class oriented approach to building a paraphrase corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers be cause of its potential contribution to broad range of natural language applications.
</prevsent>
<prevsent>paraphrases can be viewed as monolingual translations.
</prevsent>
</prevsection>
<citsent citstr=" N01-1009 ">
from this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (mt), such as transformation algorithms (lavoie et al, 2000; <papid> A00-1009 </papid>takahashi et al, 2001), corpus-based techniques for paraphrase pattern acquisition (barzilay and mckeown, 2001; <papid> P01-1008 </papid>shinyama and sekine, 2003; <papid> W03-1609 </papid>quirk et al, 2004), <papid> W04-3219 </papid>and fluency measurements (lapata, 2001; <papid> N01-1009 </papid>fujita et al, 2004).</citsent>
<aftsection>
<nextsent>one thing the paraphrasing community is still lacking is shared collections of paraphrase examples that could be used to analyze problems underlying the tasks and to evaluate the performance of systems underdevelopment.
</nextsent>
<nextsent>to our best knowledge, the paraphrase corpus developed bydolan et al (2004) <papid> C04-1051 </papid>is one of the very few collections available for free1.</nextsent>
<nextsent>development of paraphrase corpora raises several issues: what sortsof paraphrases should be collected, where paraphrase examples can be obtained from, how the coverage and quality of the corpus can be ensured,how manual annotation cost can be effectively reduced, and how collected examples should be organized and annotated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1145">
<title id=" I05-5004.xml">a class oriented approach to building a paraphrase corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (mt), such as transformation algorithms (lavoie et al, 2000; <papid> A00-1009 </papid>takahashi et al, 2001), corpus-based techniques for paraphrase pattern acquisition (barzilay and mckeown, 2001; <papid> P01-1008 </papid>shinyama and sekine, 2003; <papid> W03-1609 </papid>quirk et al, 2004), <papid> W04-3219 </papid>and fluency measurements (lapata, 2001; <papid> N01-1009 </papid>fujita et al, 2004).</prevsent>
<prevsent>one thing the paraphrasing community is still lacking is shared collections of paraphrase examples that could be used to analyze problems underlying the tasks and to evaluate the performance of systems under development.</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
to our best knowledge, the paraphrase corpus developed bydolan et al (2004) <papid> C04-1051 </papid>is one of the very few collections available for free1.</citsent>
<aftsection>
<nextsent>development of paraphrase corpora raises several issues: what sortsof paraphrases should be collected, where paraphrase examples can be obtained from, how the coverage and quality of the corpus can be ensured,how manual annotation cost can be effectively reduced, and how collected examples should be organized and annotated.
</nextsent>
<nextsent>obviously these issues should be discussed with the purpose of each individual corpus taken into account.
</nextsent>
<nextsent>in this paper, we address the issues of building gold-standard corpus that is to be used to evaluate paraphrase generation models and report on our preliminary experiences taking japanese as target language.
</nextsent>
<nextsent>our approach is characterized by the following: ? we define set of paraphrase classes based on the syntactic features of transformation patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1146">
<title id=" I05-5004.xml">a class oriented approach to building a paraphrase corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 automatic paraphrase acquisition.
</prevsent>
<prevsent>recently, paraphrase examples have been automatically collected as source of acquiring paraphrase knowledge, such as pairs of synonymous phrases and syntactic transformation templates.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
some studies exploit topically related articles derived from multiple news sources (barzilay and lee, 2003; <papid> N03-1003 </papid>shinyama and sekine, 2003; <papid> W03-1609 </papid>quirk et al., 2004; <papid> W04-3219 </papid>dolan et al, 2004).<papid> C04-1051 </papid></citsent>
<aftsection>
<nextsent>sentence pairs thatare likely to be paraphrases are automatically collected from the parallel or comparable corpora, using such clues as overlaps of content words and named entities, syntactic similarity, and reference description, such as date of the article and positions of sentences in the articles.automatic acquisition from parallel or comparable corpora, possibly in combination with manual correction, could be more cost-efficient than manual production.
</nextsent>
<nextsent>however, it would not ensure coverage and quality, because sentence pairing algorithms virtually limit the range of obtainable paraphrases and products tend to be noisy.
</nextsent>
<nextsent>nevertheless, automatic methods are useful to discover variety of paraphrases that need further exploration.
</nextsent>
<nextsent>we hope that our approach to corpus construction, which we present below, will work complementary to those directions of research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1151">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this increases the recall of our method and in some cases, precision as well.
</prevsent>
<prevsent>word sense disambiguation (wsd) has been an active area of research over the last decade because many researches believe it will be important for applications which require, or would benefit from, some degree of semantic interpretation.
</prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
there has been considerable skepticism over whether wsd will actually improve performance of applications,but we are now starting to see improvement in performance due to wsd in cross-lingual information retrieval (clough and stevenson, 2004; vossen et al ., 2006) and machine translation (carpuat and wu,2007; <papid> D07-1007 </papid>chan et al , 2007) <papid> P07-1005 </papid>and we hope that other applications such as question-answering, text simplification and summarisation might also benefit as wsd methods improve.in addition to contextual evidence, most wsd systems exploit information on the most likely meaning of word regardless of context.</citsent>
<aftsection>
<nextsent>this is powerful back-off strategy given the skewed nature ofword sense distributions.
</nextsent>
<nextsent>for example, in the english coarse grained all words task (navigli et al ,2007) <papid> W07-2006 </papid>at the recent semeval workshop the base line of choosing the most frequent sense using the first wordnet sense attained precision and recall of 78.9%which is only few percent lower than the top scoring system which obtained 82.5%.</nextsent>
<nextsent>this finding is in line with previous results (snyder and palmer, 2004).<papid> W04-0811 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1152">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this increases the recall of our method and in some cases, precision as well.
</prevsent>
<prevsent>word sense disambiguation (wsd) has been an active area of research over the last decade because many researches believe it will be important for applications which require, or would benefit from, some degree of semantic interpretation.
</prevsent>
</prevsection>
<citsent citstr=" P07-1005 ">
there has been considerable skepticism over whether wsd will actually improve performance of applications,but we are now starting to see improvement in performance due to wsd in cross-lingual information retrieval (clough and stevenson, 2004; vossen et al ., 2006) and machine translation (carpuat and wu,2007; <papid> D07-1007 </papid>chan et al , 2007) <papid> P07-1005 </papid>and we hope that other applications such as question-answering, text simplification and summarisation might also benefit as wsd methods improve.in addition to contextual evidence, most wsd systems exploit information on the most likely meaning of word regardless of context.</citsent>
<aftsection>
<nextsent>this is powerful back-off strategy given the skewed nature ofword sense distributions.
</nextsent>
<nextsent>for example, in the english coarse grained all words task (navigli et al ,2007) <papid> W07-2006 </papid>at the recent semeval workshop the base line of choosing the most frequent sense using the first wordnet sense attained precision and recall of 78.9%which is only few percent lower than the top scoring system which obtained 82.5%.</nextsent>
<nextsent>this finding is in line with previous results (snyder and palmer, 2004).<papid> W04-0811 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1153">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there has been considerable skepticism over whether wsd will actually improve performance of applications,but we are now starting to see improvement in performance due to wsd in cross-lingual information retrieval (clough and stevenson, 2004; vossen et al ., 2006) and machine translation (carpuat and wu,2007; <papid> D07-1007 </papid>chan et al , 2007) <papid> P07-1005 </papid>and we hope that other applications such as question-answering, text simplification and summarisation might also benefit as wsd methods improve.in addition to contextual evidence, most wsd systems exploit information on the most likely meaning of word regardless of context.</prevsent>
<prevsent>this is powerful back-off strategy given the skewed nature ofword sense distributions.</prevsent>
</prevsection>
<citsent citstr=" W07-2006 ">
for example, in the english coarse grained all words task (navigli et al ,2007) <papid> W07-2006 </papid>at the recent semeval workshop the base line of choosing the most frequent sense using the first wordnet sense attained precision and recall of 78.9%which is only few percent lower than the top scoring system which obtained 82.5%.</citsent>
<aftsection>
<nextsent>this finding is in line with previous results (snyder and palmer, 2004).<papid> W04-0811 </papid></nextsent>
<nextsent>systems using first sense heuristic have relied on sense-tagged data or lexicographer judgment as to which is the predominant sense of word.however sense-tagged data is expensive and furthermore the predominant sense of word will vary depending on the domain (koeling et al , 2005; <papid> H05-1053 </papid>chan and ng, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1154">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is powerful back-off strategy given the skewed nature ofword sense distributions.
</prevsent>
<prevsent>for example, in the english coarse grained all words task (navigli et al ,2007) <papid> W07-2006 </papid>at the recent semeval workshop the base line of choosing the most frequent sense using the first wordnet sense attained precision and recall of 78.9%which is only few percent lower than the top scoring system which obtained 82.5%.</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
this finding is in line with previous results (snyder and palmer, 2004).<papid> W04-0811 </papid></citsent>
<aftsection>
<nextsent>systems using first sense heuristic have relied on sense-tagged data or lexicographer judgment as to which is the predominant sense of word.however sense-tagged data is expensive and furthermore the predominant sense of word will vary depending on the domain (koeling et al , 2005; <papid> H05-1053 </papid>chan and ng, 2007).</nextsent>
<nextsent>one direction of research following mccarthy et al .</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1155">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, in the english coarse grained all words task (navigli et al ,2007) <papid> W07-2006 </papid>at the recent semeval workshop the base line of choosing the most frequent sense using the first wordnet sense attained precision and recall of 78.9%which is only few percent lower than the top scoring system which obtained 82.5%.</prevsent>
<prevsent>this finding is in line with previous results (snyder and palmer, 2004).<papid> W04-0811 </papid></prevsent>
</prevsection>
<citsent citstr=" H05-1053 ">
systems using first sense heuristic have relied on sense-tagged data or lexicographer judgment as to which is the predominant sense of word.however sense-tagged data is expensive and furthermore the predominant sense of word will vary depending on the domain (koeling et al , 2005; <papid> H05-1053 </papid>chan and ng, 2007).</citsent>
<aftsection>
<nextsent>one direction of research following mccarthy et al .
</nextsent>
<nextsent>(2004) has been to learn the most predominant 561 sense of word automatically.
</nextsent>
<nextsent>mccarthy et al method relies on two methods of similarity.
</nextsent>
<nextsent>firstly,distributional similarity is used to estimate the predominance of sense from the number of distributionally similar words and the strength of their distributional similarity to the target word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1156">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wordnet has many semantic relations as well as glosses associated with its synsets (near synonym sets).
</prevsent>
<prevsent>while traditional dictionaries do not organise senses into synsets, they do typically have sense definitions associated withthe senses.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
mccarthy et al  (2004) <papid> P04-1036 </papid>suggest that dictionary definitions can be used with their method, however in the implementation of the measure based on dictionary definitions that they use, the dictionary definitions are extended to those of related words using the hierarchical structure of wordnet (banerjee and pedersen, 2002).</citsent>
<aftsection>
<nextsent>this extension to the original method (lesk, 1986) was proposed because there is not always sufficient overlap of the individual words for which semantic similarity is being computed.
</nextsent>
<nextsent>in this paper we refer to the original method (lesk, 1986) as lesk and the extended measure proposed by banerjee and pedersen as elesk.
</nextsent>
<nextsent>this paper investigates the potential of using the overlap of dictionary definitions with the mccarthy et al method.
</nextsent>
<nextsent>we test the method for obtaining first sense heuristic using two publicly available datasets of sense-tagged data in japanese, edr (nict, 2002) and the senseval-2 japanese dictionary task (shirai, 2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1165">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in section 3 we summarise the methods we adopt from previous work, and describe our proposal for semantic similarity method that can supplement the information from dictionary definitions with information from raw text.
</prevsent>
<prevsent>in section 4 we describe the experiments on edr and thesenseval-2 japanese dictionary task and we conclude in section 5.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
this work builds upon that of mccarthy et al  (2004) <papid> P04-1036 </papid>which acquires predominant senses for target words from large sample of text using distributional similarity (lin, 1998)<papid> P98-2127 </papid>to provide evidence for predominance.</citsent>
<aftsection>
<nextsent>the evidence from the distributional similarity is allocated to the senses using semantic similarity from wordnet (patwardhan and pedersen, 2003).
</nextsent>
<nextsent>we will describe the method more fully below in section 3.
</nextsent>
<nextsent>mccarthy et al  (2004) <papid> P04-1036 </papid>reported results for english using their automatically acquired first sense heuristic on semcor (miller et al , 1993) <papid> H93-1061 </papid>andthe senseval-2 english all words dataset (sny der and palmer, 2004).<papid> W04-0811 </papid></nextsent>
<nextsent>the results from this are promising, given that hand-labelled data is notre quired.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1171">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the evidence from the distributional similarity is allocated to the senses using semantic similarity from wordnet (patwardhan and pedersen, 2003).
</prevsent>
<prevsent>we will describe the method more fully below in section 3.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
mccarthy et al  (2004) <papid> P04-1036 </papid>reported results for english using their automatically acquired first sense heuristic on semcor (miller et al , 1993) <papid> H93-1061 </papid>andthe senseval-2 english all words dataset (sny der and palmer, 2004).<papid> W04-0811 </papid></citsent>
<aftsection>
<nextsent>the results from this are promising, given that hand-labelled data is notre quired.
</nextsent>
<nextsent>on polysemous nouns from semcor they obtained 48% wsd using their method with elesk and 46% with jcn where the random baseline was 24% and the upper-bound was 67% (derived from the semcor test data itself).
</nextsent>
<nextsent>on senseval-2 all words dataset using the jcn measure 1 they obtained 63% recall which is encouraging compared to the 1they did not apply lesk to this dataset.
</nextsent>
<nextsent>562 semcor heuristic which obtained 68% but requires hand-labelled data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1178">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to mccarthy et al  (2004) <papid> P04-1036 </papid>there are other approaches to finding predominant senses.</prevsent>
<prevsent>chan and ng (2005) use parallel data to provide estimates for sense frequency distributions to feed into supervised wsd system.</prevsent>
</prevsection>
<citsent citstr=" E06-1016 ">
mohammad and hirst (2006) <papid> E06-1016 </papid>propose an approach to acquiring predominant senses from corpora which makes useof the category information in the macquarie thesaurus (barnard, 1986).</citsent>
<aftsection>
<nextsent>lexical chains (galley and mckeown, 2003) may also provide useful first sense heuristic (brody et al , 2006) <papid> P06-1013 </papid>but are produced using wordnet relations.</nextsent>
<nextsent>we use the mccarthy et al  approach because this is applicable without aligned corpus data, semantic category and relation information and is applicable to any language assuming the minimum requirements of i) dictionary definitions associated with the sense inventory and ii) raw corpus data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1179">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>chan and ng (2005) use parallel data to provide estimates for sense frequency distributions to feed into supervised wsd system.
</prevsent>
<prevsent>mohammad and hirst (2006) <papid> E06-1016 </papid>propose an approach to acquiring predominant senses from corpora which makes useof the category information in the macquarie thesaurus (barnard, 1986).</prevsent>
</prevsection>
<citsent citstr=" P06-1013 ">
lexical chains (galley and mckeown, 2003) may also provide useful first sense heuristic (brody et al , 2006) <papid> P06-1013 </papid>but are produced using wordnet relations.</citsent>
<aftsection>
<nextsent>we use the mccarthy et al  approach because this is applicable without aligned corpus data, semantic category and relation information and is applicable to any language assuming the minimum requirements of i) dictionary definitions associated with the sense inventory and ii) raw corpus data.
</nextsent>
<nextsent>we adapt their technique to remove the reliance on hyponym links.
</nextsent>
<nextsent>we first summarise the mccarthy et al  method and the wordnet based semantic similarity functions (jcn and elesk) that they use for automatic acquisition of first sense heuristic applied to disambiguation of english wordnet datasets.
</nextsent>
<nextsent>we then describe the additional semantic similarity method that we propose for comparison with lesk and jcn.mccarthy et al  use distributional similarity thesaurus acquired from corpus data using the method of lin (1998) <papid> P98-2127 </papid>for finding the predominant sense of word where the senses are defined by wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1186">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>in the experiments, we compare the three semantic similarities, jcn, lesk and dslesk3, for use in the method to 3elesk can be used when several semantic relations such as hypnoymy and meronomy are available.
</prevsent>
<prevsent>however, we cannot directly apply elesk as it was used in (mccarthy et al , 2004) <papid> P04-1036 </papid>to find the most likely sense in the set of word senses defined in each inventory following the approach of mccarthy et al  (2004).<papid> P04-1036 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
for the thesaurus construction we used  verb, case, noun  triplets extracted from japanese newspaper articles (9 years of the mainichi shinbun (1991-1999) and 10 years of the nihon keizai shinbun (1991-2000)) and parsed by cabocha (kudo and matsumoto, 2002).<papid> W02-2016 </papid></citsent>
<aftsection>
<nextsent>this resulted in 53 million triplet instances for acquiring the distributional thesaurus.
</nextsent>
<nextsent>we adopt the similarity score proposed by lin (1998) <papid> P98-2127 </papid>as the distributional similarity score and use 50 nearest neighbours in line with mccarthy et al  for the random baseline we select one word senseat random for each word token and average the precision over 100 trials.</nextsent>
<nextsent>for contrast with supervised approach we show the performance if we use hand labelled training data for obtaining the predominant sense of the test words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1190">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we proposed an adaptation of the lesk measure of gloss-based similarity, by using the average similarity between nouns in the two glosses under comparison in bag-of-words approach without recourse to other information.
</prevsent>
<prevsent>however, it would be worthwhile exploring other information in the glosses, such as words of other pos and predicate argument relations.
</prevsent>
</prevsection>
<citsent citstr=" W07-1428 ">
we also hope to investigate applying alignment techniques introduced for entailment recognition (hickl and bensley, 2007).<papid> W07-1428 </papid>another important issue in wsd is to group fine grained word senses into clusters, making the task suitable for nlp applications (ide and wilks, 2006).</citsent>
<aftsection>
<nextsent>we believe that our gloss-based similarity dslesk might be very suitable for this task and we plan to investigate the possibility.there are other approaches we would like to explore in future.
</nextsent>
<nextsent>mihalcea (2005) <papid> H05-1052 </papid>uses dictionary definitions alongside graphical algorithms for unsupervised wsd.</nextsent>
<nextsent>whilst the results are not directly comparable to ours because we have not included contextual evidence in our models, it would be worth while exploring if unsupervised graphical models using only the definitions we have in our lexical resources can perform wsd on document and give more reliable first sense heuristics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1191">
<title id=" I08-1073.xml">gloss based semantic similarity metrics for predominant sense acquisition </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>we also hope to investigate applying alignment techniques introduced for entailment recognition (hickl and bensley, 2007).<papid> W07-1428 </papid>another important issue in wsd is to group fine grained word senses into clusters, making the task suitable for nlp applications (ide and wilks, 2006).</prevsent>
<prevsent>we believe that our gloss-based similarity dslesk might be very suitable for this task and we plan to investigate the possibility.there are other approaches we would like to explore in future.</prevsent>
</prevsection>
<citsent citstr=" H05-1052 ">
mihalcea (2005) <papid> H05-1052 </papid>uses dictionary definitions alongside graphical algorithms for unsupervised wsd.</citsent>
<aftsection>
<nextsent>whilst the results are not directly comparable to ours because we have not included contextual evidence in our models, it would be worth while exploring if unsupervised graphical models using only the definitions we have in our lexical resources can perform wsd on document and give more reliable first sense heuristics.
</nextsent>
<nextsent>acknowledgements this work was supported by the uk epsrc projectep/c537262 ranking word senses for disam biguation: models and applications?, and uk royal society dorothy hodgkin fellowship to the second author.
</nextsent>
<nextsent>we would like to thank john carroll for several useful discussions on this work.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1192">
<title id=" I05-3034.xml">chinese word segmentation based on direct maximum entropy model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to find unified approach for chinese word segmentation, the author develop chinese lexical analyzer pcws using direct maximum entropy model.
</prevsent>
<prevsent>the paper presents the general description of pcws, as well as the result and analysis of its performance at the second international chinese word segmentation bakeoff.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
och and ney(2002) <papid> P02-1038 </papid>present framework based on direct maximum entropy model to construct the machine translation system.</citsent>
<aftsection>
<nextsent>the model treats knowledge sources as feature functions, and allows the system to be extended easily by adding new feature functions.
</nextsent>
<nextsent>we think the model can be used to provide unified approach for chinese word segmentation.
</nextsent>
<nextsent>pcws is the system based on this thinking.
</nextsent>
<nextsent>pcws consists of four components: word generation, disambiguation, select the best word sequence and output the result.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1193">
<title id=" I05-3034.xml">chinese word segmentation based on direct maximum entropy model </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>in order to control the search space, all the paths will be ranked by class mode score.
</prevsent>
<prevsent>the maximum number of the candidate paths the graph cannot be larger than number threshold we give.
</prevsent>
</prevsection>
<citsent citstr=" P03-1035 ">
the class mode score we used can be written as generatescore( ) = ( | )p ( ) # #w c cthe p(c) and p(?#c) is similar to the one defined by gao et al(2003).<papid> P03-1035 </papid></citsent>
<aftsection>
<nextsent>we use the direct maximum entropy find the best word sequence.
</nextsent>
<nextsent>if ??
</nextsent>
<nextsent>is the best path we need.
</nextsent>
<nextsent>w* is the candidate set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1194">
<title id=" I05-2019.xml">ebonsai an integrated environment for annotating treebanks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ebonsai helps annotators to choose correct syntactic structure of sentence from outputs of aparser, allowing the annotators to retrieve similar sentences in the treebank for referring to their structures.
</prevsent>
<prevsent>statistical approach has been main stream of natural language processing research for the lastdecade.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
particularly, syntactically annotated corpora (treebanks), such as penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>negra corpus (skut et al, 1997)<papid> A97-1014 </papid>and edr corpus (jap, 1994), contribute to improve the performance of morpho-syntactic analysis systems.</citsent>
<aftsection>
<nextsent>it is notorious, however, that building large treebank is labor intensive and time consuming work.
</nextsent>
<nextsent>in addition, it is quite difficult to keep quality and consistency of large tree bank.
</nextsent>
<nextsent>to remedy this problem, there have been many attempts to develop software tools for annotating treebanks (plaehn and brants, 2000; bird et al., 2002).
</nextsent>
<nextsent>this paper presents an integrated environment for annotating treebanks, called ebonsai.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1195">
<title id=" I05-2019.xml">ebonsai an integrated environment for annotating treebanks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ebonsai helps annotators to choose correct syntactic structure of sentence from outputs of aparser, allowing the annotators to retrieve similar sentences in the treebank for referring to their structures.
</prevsent>
<prevsent>statistical approach has been main stream of natural language processing research for the lastdecade.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
particularly, syntactically annotated corpora (treebanks), such as penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>negra corpus (skut et al, 1997)<papid> A97-1014 </papid>and edr corpus (jap, 1994), contribute to improve the performance of morpho-syntactic analysis systems.</citsent>
<aftsection>
<nextsent>it is notorious, however, that building large treebank is labor intensive and time consuming work.
</nextsent>
<nextsent>in addition, it is quite difficult to keep quality and consistency of large tree bank.
</nextsent>
<nextsent>to remedy this problem, there have been many attempts to develop software tools for annotating treebanks (plaehn and brants, 2000; bird et al., 2002).
</nextsent>
<nextsent>this paper presents an integrated environment for annotating treebanks, called ebonsai.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1196">
<title id=" I05-4007.xml">cross lingual conversion of lexical semantic relations building parallel wordnets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wordnets, pioneered by the princeton wordnet (wn, fellbaum 1998), and greatly enriched by euro wordnet (ewn, vossen 1998), have become the standard for lexical knowledge base enriched with lexical semantic relations.
</prevsent>
<prevsent>in addition to the multilingual architecture of ewn, there are some proposals to construct the expansion for monolingual wordnets to parallel wordnet systems, such as pianta and girardi (2002).
</prevsent>
</prevsection>
<citsent citstr=" W02-1106 ">
however, the construction of multilingual wordnets eventually faces the challenge of low-density languages, which is dealt with in huang, et al (2002).<papid> W02-1106 </papid></citsent>
<aftsection>
<nextsent>low-density languages, as opposed to high-density languages, usually refer to languages that are not spoken by large number of people.
</nextsent>
<nextsent>however, there is neither direct correspondence between language population and language technology, nor an objective population number that defines density level.
</nextsent>
<nextsent>in this work, we use the availability of language resources instead to define language density.
</nextsent>
<nextsent>that is, low-density languages are languages that do not have enough language resources to support fully automated language processing, such as machine translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1200">
<title id=" I05-3009.xml">domain specific word extraction from hierarchical web documents a first step toward building lexicon trees from web corpora </title>
<section> domain specific words and lexicon.  </section>
<citcontext>
<prevsection>
<prevsent>trees as important nlp resources domain specific words (dsws) are important anchoring words?
</prevsent>
<prevsent>for natural language processing applications that involve word sense disambiguation (wsd).
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
it is appreciated that multi-sense words appearing in the same document tend to be tagged with the same word sense if they belong to the same common domain in the semantic hierarchy (yarowsky, 1995).<papid> P95-1026 </papid></citsent>
<aftsection>
<nextsent>the existence of some dsws in document will therefore be strong evidence of specific sense for words within the document.
</nextsent>
<nextsent>for instance, the existence of basketball?
</nextsent>
<nextsent>in document would strongly suggest the sport?
</nextsent>
<nextsent>sense of the word ????
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1201">
<title id=" I05-3009.xml">domain specific word extraction from hierarchical web documents a first step toward building lexicon trees from web corpora </title>
<section> conventional clustering view for.  </section>
<citcontext>
<prevsection>
<prevsent>we therefore will explore non-conventional view for constructing lexicon tree from the web hierarchy, where domain-specific word identification turns out to be key issue and the first step toward such construction process.
</prevsent>
<prevsent>an inter-domain entropy (ide) measure will be proposed for this purpose.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
constructing lexicon trees one conventional way to construct the lexicon hierarchy from web corpora is to collect the terms in all web documents and measure the degree of word association between word pairs using some well-known association metrics (church and hanks, 1989; <papid> P89-1010 </papid>smadja et al, 1996) <papid> J96-1001 </papid>as the distance measure.</citsent>
<aftsection>
<nextsent>terms of high association are then clustered bottom-up using some clustering techniques to build the hierarchy.
</nextsent>
<nextsent>the clustered hierarchy is then submitted to lexicographers to assign semantic label to each sub-cluster.
</nextsent>
<nextsent>the cost will be reduced in this way, but could still be unaffordable.
</nextsent>
<nextsent>besides, it still depends on the lexicographers to assign appropriate semantic tags to the list of highly associated words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1202">
<title id=" I05-3009.xml">domain specific word extraction from hierarchical web documents a first step toward building lexicon trees from web corpora </title>
<section> conventional clustering view for.  </section>
<citcontext>
<prevsection>
<prevsent>we therefore will explore non-conventional view for constructing lexicon tree from the web hierarchy, where domain-specific word identification turns out to be key issue and the first step toward such construction process.
</prevsent>
<prevsent>an inter-domain entropy (ide) measure will be proposed for this purpose.
</prevsent>
</prevsection>
<citsent citstr=" J96-1001 ">
constructing lexicon trees one conventional way to construct the lexicon hierarchy from web corpora is to collect the terms in all web documents and measure the degree of word association between word pairs using some well-known association metrics (church and hanks, 1989; <papid> P89-1010 </papid>smadja et al, 1996) <papid> J96-1001 </papid>as the distance measure.</citsent>
<aftsection>
<nextsent>terms of high association are then clustered bottom-up using some clustering techniques to build the hierarchy.
</nextsent>
<nextsent>the clustered hierarchy is then submitted to lexicographers to assign semantic label to each sub-cluster.
</nextsent>
<nextsent>the cost will be reduced in this way, but could still be unaffordable.
</nextsent>
<nextsent>besides, it still depends on the lexicographers to assign appropriate semantic tags to the list of highly associated words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1203">
<title id=" I08-1065.xml">acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, causal (and entailment) relation holds between the verb phrases wash something and something is clean, which reflects the commonsense notion that if someone has washed something, this object is clean as result of the washing event.
</prevsent>
<prevsent>a crucial issue ishow to obtain and maintain potentially huge collection of such event relations instances.
</prevsent>
</prevsection>
<citsent citstr=" N06-1008 ">
motivated by this background, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event mentions (typically verbs or verb phrases) (lin and pantel, 2001; inui et al , 2003; chklovski and pantel, 2005; torisawa, 2006;<papid> N06-1008 </papid>pekar, 2006; <papid> N06-1007 </papid>zanzotto et al , 2006, etc.).</citsent>
<aftsection>
<nextsent>the common idea behind them is to use small number of manually selected generic lexico-syntactic cooccurrence patterns (lsps or simply patterns).
</nextsent>
<nextsent>to verb-xand then verb-y, for example, is used to obtain temporal relations such as marry and divorce (chklovski and pantel, 2005).
</nextsent>
<nextsent>the use of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations.
</nextsent>
<nextsent>this issue has been addressed in basically two approaches, either by devising heuristic statistical scores (chklovski and pantel, 2005; torisawa, 2006;<papid> N06-1008 </papid> zanzotto et al , 2006) or training classifiers for disambiguation with heavy supervision (inui et al , 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1204">
<title id=" I08-1065.xml">acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, causal (and entailment) relation holds between the verb phrases wash something and something is clean, which reflects the commonsense notion that if someone has washed something, this object is clean as result of the washing event.
</prevsent>
<prevsent>a crucial issue ishow to obtain and maintain potentially huge collection of such event relations instances.
</prevsent>
</prevsection>
<citsent citstr=" N06-1007 ">
motivated by this background, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event mentions (typically verbs or verb phrases) (lin and pantel, 2001; inui et al , 2003; chklovski and pantel, 2005; torisawa, 2006;<papid> N06-1008 </papid>pekar, 2006; <papid> N06-1007 </papid>zanzotto et al , 2006, etc.).</citsent>
<aftsection>
<nextsent>the common idea behind them is to use small number of manually selected generic lexico-syntactic cooccurrence patterns (lsps or simply patterns).
</nextsent>
<nextsent>to verb-xand then verb-y, for example, is used to obtain temporal relations such as marry and divorce (chklovski and pantel, 2005).
</nextsent>
<nextsent>the use of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations.
</nextsent>
<nextsent>this issue has been addressed in basically two approaches, either by devising heuristic statistical scores (chklovski and pantel, 2005; torisawa, 2006;<papid> N06-1008 </papid> zanzotto et al , 2006) or training classifiers for disambiguation with heavy supervision (inui et al , 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1206">
<title id=" I08-1065.xml">acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this issue has been addressed in basically two approaches, either by devising heuristic statistical scores (chklovski and pantel, 2005; torisawa, 2006;<papid> N06-1008 </papid> zanzotto et al , 2006) or training classifiers for disambiguation with heavy supervision (inui et al , 2003).</prevsent>
<prevsent>this paper explores third way for enhancing present lsp-based methods for event relation acqui sition.</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
the basic idea is inspired by the following recent findings in relation extraction (ravichandran and hovy, 2002; <papid> P02-1006 </papid>pantel and pennacchiotti, 2006,<papid> P06-1015 </papid>etc.), which aims at extracting semantic relations between entities (as opposed to events) from texts.</citsent>
<aftsection>
<nextsent>(a) the use of generic patterns tends to be high recall but low precision, which requires an additional component for pruning.
</nextsent>
<nextsent>(b) on the other hand, there are specific patterns that are highly reliable but they are much less frequent than generic patterns and each makes only small contribution to recall.
</nextsent>
<nextsent>(c) combining few generic patters with much larger collection of reliable specific patterns boosts both pre 497cision and recall.
</nextsent>
<nextsent>such specific patterns can be acquired from very large corpus with seeds.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1207">
<title id=" I08-1065.xml">acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this issue has been addressed in basically two approaches, either by devising heuristic statistical scores (chklovski and pantel, 2005; torisawa, 2006;<papid> N06-1008 </papid> zanzotto et al , 2006) or training classifiers for disambiguation with heavy supervision (inui et al , 2003).</prevsent>
<prevsent>this paper explores third way for enhancing present lsp-based methods for event relation acqui sition.</prevsent>
</prevsection>
<citsent citstr=" P06-1015 ">
the basic idea is inspired by the following recent findings in relation extraction (ravichandran and hovy, 2002; <papid> P02-1006 </papid>pantel and pennacchiotti, 2006,<papid> P06-1015 </papid>etc.), which aims at extracting semantic relations between entities (as opposed to events) from texts.</citsent>
<aftsection>
<nextsent>(a) the use of generic patterns tends to be high recall but low precision, which requires an additional component for pruning.
</nextsent>
<nextsent>(b) on the other hand, there are specific patterns that are highly reliable but they are much less frequent than generic patterns and each makes only small contribution to recall.
</nextsent>
<nextsent>(c) combining few generic patters with much larger collection of reliable specific patterns boosts both pre 497cision and recall.
</nextsent>
<nextsent>such specific patterns can be acquired from very large corpus with seeds.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1213">
<title id=" I08-1065.xml">acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>utau::kimochiyoi (sing::feel good), hashiru::kimochiyoi (run::feel good) 4429 noun;actionwo-kiiteverb;effect?
</prevsent>
<prevsent>(to hear action?
</prevsent>
</prevsection>
<citsent citstr=" N06-1023 ">
so that effect?)setsumei-suru::nattoku-suru (explain::agree), setsumei suru::rikai-dekiru (explain::can understand)web corpus collected by kawahara and kurohashi (2006).<papid> N06-1023 </papid></citsent>
<aftsection>
<nextsent>the sentences were dependency parsed with cabocha (kudo and matsumoto, 2002), <papid> W02-2016 </papid>and cooccurrence samples of event mentions wereextracted.</nextsent>
<nextsent>event mentions with patterns whose frequency was less than 20 were discarded in order to reduce computational costs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1214">
<title id=" I08-1065.xml">acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>(to hear action?
</prevsent>
<prevsent>so that effect?)setsumei-suru::nattoku-suru (explain::agree), setsumei suru::rikai-dekiru (explain::can understand)web corpus collected by kawahara and kurohashi (2006).<papid> N06-1023 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
the sentences were dependency parsed with cabocha (kudo and matsumoto, 2002), <papid> W02-2016 </papid>and cooccurrence samples of event mentions wereextracted.</citsent>
<aftsection>
<nextsent>event mentions with patterns whose frequency was less than 20 were discarded in order to reduce computational costs.
</nextsent>
<nextsent>as result, we obtained 34m cooccurrence tokens with 11m types.
</nextsent>
<nextsent>note that among those cooccurrence samples 15m tokens(44%) with 4.8m types (43%) are those with verbal nouns, suggesting the potential impacts of using verbal nouns.
</nextsent>
<nextsent>in our experiments, we considered two of inui etal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1215">
<title id=" I08-1065.xml">acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>we are seeking way to incorporate probabilistic model of predicate-argument cooccurrences into the ranking function for relation instances.
</prevsent>
<prevsent>related to this issue, it is also crucial to devise method for controlling argument sharing patterns.
</prevsent>
</prevsection>
<citsent citstr=" P06-1079 ">
one possible approach is to employstate-of-the-art techniques for coreference and zero anaphora resolution (iida et al , 2006; <papid> P06-1079 </papid>komachi etal., 2007, etc.) in preprocessing cooccurrence sam ples.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1216">
<title id=" I08-2122.xml">towards data and goal oriented analysis tool interoperability and combinatorial comparison </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>859 uima, unstructured information management architecture (lally and ferrucci, 2004), which was originally developed by ibm and has recently become an open project in oasis and apache, provides promising framework for tool integration.
</prevsent>
<prevsent>although it has set of useful functional ities, uima only provides generic framework, thus it requires user community to develop their own platforms with set of actual software modules.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
a few attempts have already been made to establish platforms, e.g. the cmu uima component repository 1 , gate (cunningham et al, 2002) <papid> P02-1022 </papid>with its uima interoperability layer, etc. however, simply wrapping existing modules to be uima compliant does not offer complete solution.</citsent>
<aftsection>
<nextsent>most of tm/nlp tasks are composite in nature, and can only be solved by combining several modules.
</nextsent>
<nextsent>users need to test large number of combinations of tools in order to pick the most suitable combination for their specific task.
</nextsent>
<nextsent>although types and type systems are the only way to represent meanings in the uima framework, uima does not provide any specific types, except for few purely primitive types.
</nextsent>
<nextsent>in this paper, we propose way to design shar able type systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1217">
<title id=" I08-2122.xml">towards data and goal oriented analysis tool interoperability and combinatorial comparison </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>such type system allows for the automatic combinations of any uima compliant components and for the comparisons of these combinations, when the components have proper capabilities within the type system.
</prevsent>
<prevsent>we are sentence token pos token rich token protein phrase ppi genia tagger: trained on the wsj, genia and pennbioie corpora (pos).
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
uses maximum entropy (berger et al, 1996) <papid> J96-1002 </papid>classification, trained on jnlpba (kim et al, 2004) (ner).</citsent>
<aftsection>
<nextsent>trained on genia corpus (sentence splitter).
</nextsent>
<nextsent>enju: hpsg parser with predicate argument structures as well as phrase structures.
</nextsent>
<nextsent>although trained with penn treebank, it can compute accurate analyses of biomedical texts owing to its method for domain adaptation (hara et al, 2005).
</nextsent>
<nextsent>stepp tagger: based on probabilistic models, tuned to biomedical text trained by wsj, genia (kim et al, 2003) and pennbioie corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1218">
<title id=" I05-1085.xml">lexical choice via topic adaptation for paraphrasing written language to spoken language </title>
<section> style classification.  </section>
<citcontext>
<prevsection>
<prevsent>note that what is called spoken language here is not real utterance but chat like texts.
</prevsent>
<prevsent>although it is not real spoken language, it works as good substitute, as some researchers pointed out [2,11].
</prevsent>
</prevsection>
<citsent citstr=" N04-1031 ">
we follow method proposed by kaji et al(2004).<papid> N04-1031 </papid></citsent>
<aftsection>
<nextsent>their method classifies web pages into three types: (1) written language page, (2) spoken language page, and (3) ambiguous page.
</nextsent>
<nextsent>then, web pages classified into type (1) or (2) are used.
</nextsent>
<nextsent>ambiguous pages are discarded because classification precision decreases if such pages are used.
</nextsent>
<nextsent>this section summarizes their method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1222">
<title id=" I05-2027.xml">machine learning approach to augmenting news headline generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we present an approach to headline generation for single document.
</prevsent>
<prevsent>this headline generation task was added to the annual summ5arisation evaluation in the document understanding conference (duc) 2003.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
it was also included in the duc 2004 evaluation plan where summary quality was automatically judged using set of n-gram word overlap metrics called rouge (lin and hovy, 2003).<papid> N03-1020 </papid></citsent>
<aftsection>
<nextsent>eighteen research groups participated in the headline generation task at duc 2004, i.e. task 1: very short summary generation.
</nextsent>
<nextsent>the topiary system was the top performing headline system at duc 2004.
</nextsent>
<nextsent>it generated headlines by combining set of topic descriptors with compressed version of the lead sentence, e.g. kurdish turkish syria: turkey sent 10,000 troops to southeastern border.
</nextsent>
<nextsent>these topic descriptors were automatically identified using statistical approach called unsupervised topic discovery (utd) (zajic et al, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1224">
<title id=" I05-2027.xml">machine learning approach to augmenting news headline generation </title>
<section> evaluation and results.  </section>
<citcontext>
<prevsection>
<prevsent>table 1 shows the results of our headline generation experiments on the duc 2004 collection.
</prevsent>
<prevsent>seven systems in total took part in this evaluation, three topiary-style headline generation systems and four baselines: the goal of our experiments was to evaluate linguistically-motivated heuristic approaches to title generation, and establish which of our alternative techniques for padding topiary-style headlines with topic labels works best.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
since the duc 2004 evaluation, lin (2004) <papid> W04-1013 </papid>has concluded that certain rouge metrics correlate better with human judgments than others, depending on the summarisation task being evaluated, i.e. single document, headline, or multi-document summarisation.</citsent>
<aftsection>
<nextsent>in the case of headline generation, lin found that rouge-1, rouge-l and rouge-w scores worked best and so only these scores are included in table 1.
</nextsent>
<nextsent>table 1.
</nextsent>
<nextsent>rouge scores for headline generation systems as the results show the best performing topic labeling techniques are the tf and hybrid systems.
</nextsent>
<nextsent>tf system is baseline system that chooses high frequency content words as topic descriptors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1225">
<title id=" I08-1050.xml">identifying sections in scientific abstracts using conditional random fields </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude this paper in section 5.
</prevsent>
<prevsent>the previous studies regarded the task of identifying section names as text-classification problem that determines label (section name) for each sentence.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
various classifiers for text categorization, nave bayesian model (nbm) (teufel and moens, 2002; <papid> J02-4002 </papid>ruch et al, 2007), hidden markov model(hmm) (wu et al, 2006; <papid> P06-4011 </papid>lin et al, 2006), <papid> W06-3309 </papid>and support vector machines (svm) (mcknight and arini vasan, 2003; shimbo et al, 2003; ito et al, 2004; yamamoto and takagi, 2005) were applied.table 1 summarizes these approaches and perfor mances.</citsent>
<aftsection>
<nextsent>all studies target scientific abstracts except for teufel and moens (2002) <papid> J02-4002 </papid>who target scientific full papers.</nextsent>
<nextsent>field classes show the set of section names that each study assumes: background (b), objective/aim/purpose (o), method (m), result (r), conclusion (c), and introduction (i) that combines the background and objective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1226">
<title id=" I08-1050.xml">identifying sections in scientific abstracts using conditional random fields </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude this paper in section 5.
</prevsent>
<prevsent>the previous studies regarded the task of identifying section names as text-classification problem that determines label (section name) for each sentence.
</prevsent>
</prevsection>
<citsent citstr=" P06-4011 ">
various classifiers for text categorization, nave bayesian model (nbm) (teufel and moens, 2002; <papid> J02-4002 </papid>ruch et al, 2007), hidden markov model(hmm) (wu et al, 2006; <papid> P06-4011 </papid>lin et al, 2006), <papid> W06-3309 </papid>and support vector machines (svm) (mcknight and arini vasan, 2003; shimbo et al, 2003; ito et al, 2004; yamamoto and takagi, 2005) were applied.table 1 summarizes these approaches and perfor mances.</citsent>
<aftsection>
<nextsent>all studies target scientific abstracts except for teufel and moens (2002) <papid> J02-4002 </papid>who target scientific full papers.</nextsent>
<nextsent>field classes show the set of section names that each study assumes: background (b), objective/aim/purpose (o), method (m), result (r), conclusion (c), and introduction (i) that combines the background and objective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1227">
<title id=" I08-1050.xml">identifying sections in scientific abstracts using conditional random fields </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude this paper in section 5.
</prevsent>
<prevsent>the previous studies regarded the task of identifying section names as text-classification problem that determines label (section name) for each sentence.
</prevsent>
</prevsection>
<citsent citstr=" W06-3309 ">
various classifiers for text categorization, nave bayesian model (nbm) (teufel and moens, 2002; <papid> J02-4002 </papid>ruch et al, 2007), hidden markov model(hmm) (wu et al, 2006; <papid> P06-4011 </papid>lin et al, 2006), <papid> W06-3309 </papid>and support vector machines (svm) (mcknight and arini vasan, 2003; shimbo et al, 2003; ito et al, 2004; yamamoto and takagi, 2005) were applied.table 1 summarizes these approaches and perfor mances.</citsent>
<aftsection>
<nextsent>all studies target scientific abstracts except for teufel and moens (2002) <papid> J02-4002 </papid>who target scientific full papers.</nextsent>
<nextsent>field classes show the set of section names that each study assumes: background (b), objective/aim/purpose (o), method (m), result (r), conclusion (c), and introduction (i) that combines the background and objective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1234">
<title id=" I08-1050.xml">identifying sections in scientific abstracts using conditional random fields </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, we would like to model the structure of abstract sentences rather than modeling just the section label for each sentence.
</prevsent>
<prevsent>thus, the task is more suitably formalized as sequence labeling problem: given an abstract with sentences = (x1, ..., xn), determine the optimal sequence of section names = (y1, ..., yn) of all possible sequences.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
conditional random fields (crfs) have been successfully applied to various nlp tasks including part-of-speech tagging (lafferty et al, 2001) and shallow parsing (sha and pereira, 2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>crfs define conditional probability distribution p(y|x) for output and input sequences, and x, p(y|x) = 1 z?(x) exp {?
</nextsent>
<nextsent>f (y,x)} .
</nextsent>
<nextsent>(1) therein: function (y,x) denotes global feature vector for input sequence and output sequence y, (y,x) = ? f(y,x, i), (2) ranges over the input sequence, function f(y,x, i) is feature vector for input sequence and output sequence at position (based on state features and transition features), ? is vector where an element represents the weight of feature fk(y,x), and z?(x) is normalization factor, z?(x) = ? exp {?
</nextsent>
<nextsent>f (y,x)} .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1235">
<title id=" I08-1050.xml">identifying sections in scientific abstracts using conditional random fields </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>for stating the objective of study.
</prevsent>
<prevsent>we use features for sentence contents represented by: i) words, ii) word bi grams, and iii) mixture of words and word bigrams.
</prevsent>
</prevsection>
<citsent citstr=" H05-1059 ">
words are normalized into their base forms by the genia tagger (tsuruoka and tsujii, 2005), <papid> H05-1059 </papid>which is part-of-speech tagger trained for the biomedical 383 rank objective method results conclusions 1 # to be measure % ) suggest that 2 be to be perform ( may be 3 to determine =   # these 4 study be be compare ) . should be 5 this study be determine % . these result table 2: bigram features with high 2 values (?#?</citsent>
<aftsection>
<nextsent>stands for beginning of sentence).
</nextsent>
<nextsent>domain.
</nextsent>
<nextsent>we measure the co-occurrence strength (2 value) between each feature and section label.
</nextsent>
<nextsent>if feature appears selectively in specific section, the 2 value is expected to be high.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1236">
<title id=" I08-1050.xml">identifying sections in scientific abstracts using conditional random fields </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>supposing that aim and purpose are equivalent to objective.
</prevsent>
<prevsent>hence, this study assumes four sections, objective, method, results, and conclusions.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
meanwhile, it is common for np chunking tasks to represent chunk (e.g., np) with two labels, the begin (e.g., b-np) and inside (e.g., i-np) of chunk (ramshaw and marcus, 1995).<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>although none of the previous studies employed this representation, attaching b- and i- prefixes to section labels may improve classifier by associating clue phrases (e.g., to determine?)
</nextsent>
<nextsent>with the starts of sections (e.g., b-objective).
</nextsent>
<nextsent>we will compare classification performances on two sets of label repre sentations: namely, we will compare four section labels and eight labels with bi prefixes attached to section names.
</nextsent>
<nextsent>4.1 experiment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1237">
<title id=" I05-2037.xml">restoring an elided entry word in a sentence for encyclopedia qa system </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while previous work provides important insight into the abstract syntactic and semantic representations that underlie ellipsis phenomena, there has been little empirically oriented work on ellipsis.
</prevsent>
<prevsent>there are only two similar empirical experiments done for this task.
</prevsent>
</prevsection>
<citsent citstr=" J97-4002 ">
first is hardts algo rithm(hardt, 1997) <papid> J97-4002 </papid>for detecting vpe in the penn treebank.</citsent>
<aftsection>
<nextsent>it achieves precision levels of 44% and recall of 53%, giving an f-measure of 48% using simple search technique, which relies on the annotation having identified empty expressions correctly.
</nextsent>
<nextsent>second is nielsens machine learning techniques(nielsen, 2003).
</nextsent>
<nextsent>they only try to detect of elliptical verbs using four different machine learning techniques, trans formation-based learning, maximum entropy modeling, decision tree learning, memory based learning.
</nextsent>
<nextsent>it achieves precision levels of 85.14% and recall of 69.63%, giving an measure of 76.61%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1238">
<title id=" I08-1016.xml">entity driven rewrite for multi document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some improvement is also seen in content selection over extractive summarization as measured by pyramid method evaluation.
</prevsent>
<prevsent>two of the key components of effective summa riza tions are the ability to identify important points in the text and to adequately reword the original text in order to convey these points.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
automatic text summarization approaches have offered reasonablywell-performing approximations for identifiying important sentences (lin and hovy, 2002; schiffman etal., 2002; erkan and radev, 2004; mihalcea and tarau, 2004; <papid> W04-3252 </papid>daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2006) but, not surprisingly, text (re)generation has been major challange despite some work on sub-sentential modification (jing and mckeown, 2000; <papid> A00-2024 </papid>knight and marcu,2000; barzilay and mckeown, 2005).<papid> J05-3002 </papid></nextsent>
<nextsent>an additional drawback of extractive approaches is that estimates for the importance of larger text units such as sentences depend on the length of the sentence (nenkova et al, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1239">
<title id=" I08-1016.xml">entity driven rewrite for multi document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two of the key components of effective summa riza tions are the ability to identify important points in the text and to adequately reword the original text in order to convey these points.
</prevsent>
<prevsent>automatic text summarization approaches have offered reasonablywell-performing approximations for identifiying important sentences (lin and hovy, 2002; schiffman etal., 2002; erkan and radev, 2004; mihalcea and tarau, 2004; <papid> W04-3252 </papid>daume?</prevsent>
</prevsection>
<citsent citstr=" A00-2024 ">
iii and marcu, 2006) but, not surprisingly, text (re)generation has been major challange despite some work on sub-sentential modification (jing and mckeown, 2000; <papid> A00-2024 </papid>knight and marcu,2000; barzilay and mckeown, 2005).<papid> J05-3002 </papid></citsent>
<aftsection>
<nextsent>an additional drawback of extractive approaches is that estimates for the importance of larger text units such as sentences depend on the length of the sentence (nenkova et al, 2006).
</nextsent>
<nextsent>sentence simplification or compaction algorithms are driven mainly by grammaticality considerations.
</nextsent>
<nextsent>whether approaches for estimating importance can be applied to units smaller than sentences and usedin text rewrite in the summary production is question that remains unanswered.
</nextsent>
<nextsent>the option to operate on smaller units, which can be mixed and matched from the input to give novel combinations in the summary, offers several possible advantages.improve content sometimes sentences in the input can contain both information that is very appropriate to include in summary and information that should not appear in summary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1240">
<title id=" I08-1016.xml">entity driven rewrite for multi document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two of the key components of effective summa riza tions are the ability to identify important points in the text and to adequately reword the original text in order to convey these points.
</prevsent>
<prevsent>automatic text summarization approaches have offered reasonablywell-performing approximations for identifiying important sentences (lin and hovy, 2002; schiffman etal., 2002; erkan and radev, 2004; mihalcea and tarau, 2004; <papid> W04-3252 </papid>daume?</prevsent>
</prevsection>
<citsent citstr=" J05-3002 ">
iii and marcu, 2006) but, not surprisingly, text (re)generation has been major challange despite some work on sub-sentential modification (jing and mckeown, 2000; <papid> A00-2024 </papid>knight and marcu,2000; barzilay and mckeown, 2005).<papid> J05-3002 </papid></citsent>
<aftsection>
<nextsent>an additional drawback of extractive approaches is that estimates for the importance of larger text units such as sentences depend on the length of the sentence (nenkova et al, 2006).
</nextsent>
<nextsent>sentence simplification or compaction algorithms are driven mainly by grammaticality considerations.
</nextsent>
<nextsent>whether approaches for estimating importance can be applied to units smaller than sentences and usedin text rewrite in the summary production is question that remains unanswered.
</nextsent>
<nextsent>the option to operate on smaller units, which can be mixed and matched from the input to give novel combinations in the summary, offers several possible advantages.improve content sometimes sentences in the input can contain both information that is very appropriate to include in summary and information that should not appear in summary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1241">
<title id=" I08-1016.xml">entity driven rewrite for multi document summarization </title>
<section> np-rewrite enhanced frequency.  </section>
<citcontext>
<prevsection>
<prevsent>this example shows the potential power of noun phrase rewrite.
</prevsent>
<prevsent>it also suggests that context will play role in the rewrite process, since different noun phrase realizations will be most appropriate depending on what has been said in the summary up to the point at which rewrite takes place.
</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
summarizerfrequency and frequency-related measures of importance have been traditionally used in text summarization as indicators of importance (luhn, 1958;lin and hovy, 2000; <papid> C00-1072 </papid>conroy et al, 2006).<papid> P06-2020 </papid></citsent>
<aftsection>
<nextsent>notably, greedy frequency-driven approach leads to very good results in content selection (nenkova et al., 2006).
</nextsent>
<nextsent>in this approach sentence importance is measured as function of the frequency in the in put of the content words in that sentence.
</nextsent>
<nextsent>the most important sentence is selected, the weight of wordsin it are adjusted, and sentence weights are recomputed for the new weights beofre selecting the next sentence.this conceptually simple summarization approach can readily be extended to include np rewrite and allow us to examine the effect of rewrite capabilities on overall content selection and readability.the specific algorithm for frequency-driven summarization and rewrite is as follows: step 1 estimate the importance of each content word wi based on its frequency in the input ni, p(wi) = nin . step 2 for each sentence sj in the input, estimate its importance based on the words in the sentence wi ? sj : the weight of the sentence is equal to the average weight of content words appearing in it.
</nextsent>
<nextsent>weight(sj) = ? wisj p(wi) |wisj | step 3 select the sentence with the highest weight.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1242">
<title id=" I08-1016.xml">entity driven rewrite for multi document summarization </title>
<section> np-rewrite enhanced frequency.  </section>
<citcontext>
<prevsection>
<prevsent>this example shows the potential power of noun phrase rewrite.
</prevsent>
<prevsent>it also suggests that context will play role in the rewrite process, since different noun phrase realizations will be most appropriate depending on what has been said in the summary up to the point at which rewrite takes place.
</prevsent>
</prevsection>
<citsent citstr=" P06-2020 ">
summarizerfrequency and frequency-related measures of importance have been traditionally used in text summarization as indicators of importance (luhn, 1958;lin and hovy, 2000; <papid> C00-1072 </papid>conroy et al, 2006).<papid> P06-2020 </papid></citsent>
<aftsection>
<nextsent>notably, greedy frequency-driven approach leads to very good results in content selection (nenkova et al., 2006).
</nextsent>
<nextsent>in this approach sentence importance is measured as function of the frequency in the in put of the content words in that sentence.
</nextsent>
<nextsent>the most important sentence is selected, the weight of wordsin it are adjusted, and sentence weights are recomputed for the new weights beofre selecting the next sentence.this conceptually simple summarization approach can readily be extended to include np rewrite and allow us to examine the effect of rewrite capabilities on overall content selection and readability.the specific algorithm for frequency-driven summarization and rewrite is as follows: step 1 estimate the importance of each content word wi based on its frequency in the input ni, p(wi) = nin . step 2 for each sentence sj in the input, estimate its importance based on the words in the sentence wi ? sj : the weight of the sentence is equal to the average weight of content words appearing in it.
</nextsent>
<nextsent>weight(sj) = ? wisj p(wi) |wisj | step 3 select the sentence with the highest weight.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1243">
<title id=" I08-1016.xml">entity driven rewrite for multi document summarization </title>
<section> np-rewrite enhanced frequency.  </section>
<citcontext>
<prevsection>
<prevsent>this means that maximum nps can be rather complex, covering wide range of production rules in context-free grammar.
</prevsent>
<prevsent>the dependency tree definition of maximum noun phrase makes it easy to see why these are good unit for sub sentential rewrite: the subtree that has the head of the np as root contains only modifiers of the head, and by rewriting the noun phrase, the amount of information expressed about the head entity can be varied.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
in our implementation, context free grammar probabilistic parser (charniak, 2000) <papid> A00-2018 </papid>was used to parse the input.</citsent>
<aftsection>
<nextsent>the maximum noun phrases were identified by finding sequences of  np ... /np  tags in the parse such that the number of opening and closing tags is equal.
</nextsent>
<nextsent>each np identified by such tag spans was considered as candidate for rewrite.
</nextsent>
<nextsent>coreference classes coreference class crm is the class of all maximum noun phrases in the input that refer to the same entity em.
</nextsent>
<nextsent>the general problem of coreference resolution is hard, and is even more complicated for the multi-document summarization case, in which cross-document resolution needs to be performed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1244">
<title id=" I05-6010.xml">some remarks on the annotation of quantifying noun groups in treebanks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at the same time, treebanks are valuable source of datafor theoretical linguistic investigations about language use.
</prevsent>
<prevsent>the data-drivenness of this approach presents clear advantage over the traditional, idealised notion of competence grammar.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
according to skut et al (1997) <papid> A97-1014 </papid>treebanks have to meet the following requirements: 1.</citsent>
<aftsection>
<nextsent>descriptivity:  grammatical phenomena are to be described rather than explained; 2.
</nextsent>
<nextsent>theory-independence:   annotations should not be influenced by theory-specific considerations; nevertheless, different theory-specific representations should be recoverable from the annotation; 3.
</nextsent>
<nextsent>multi-stratal representations:   clear separation of different description levels; and 4.
</nextsent>
<nextsent>data-drivenness:   the annotation scheme must provide representational means for all phenomena occurring in texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1245">
<title id=" I05-6010.xml">some remarks on the annotation of quantifying noun groups in treebanks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>81the most important treebank for english nowadays is the penn treebank (cf.
</prevsent>
<prevsent>(marcus et al, 1994)).
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
many statistical taggers and parser shave been trained on it, e.g. ramshaw and marcus (1995), <papid> W95-0107 </papid>srinivas (1997) and alshawi and carter (1994).<papid> J94-4005 </papid></citsent>
<aftsection>
<nextsent>furthermore, context-free and unification-based grammars have been derived from the penn treebank (cf.
</nextsent>
<nextsent>(charniak, 1996) and (van genabith et al, 1999)).
</nextsent>
<nextsent>these parsers, trained or created by means of the treebank, can be applied for enlarging the treebank.
</nextsent>
<nextsent>for german, the first initiative in the field of treebanks was the negra corpus (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1246">
<title id=" I05-6010.xml">some remarks on the annotation of quantifying noun groups in treebanks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>81the most important treebank for english nowadays is the penn treebank (cf.
</prevsent>
<prevsent>(marcus et al, 1994)).
</prevsent>
</prevsection>
<citsent citstr=" J94-4005 ">
many statistical taggers and parser shave been trained on it, e.g. ramshaw and marcus (1995), <papid> W95-0107 </papid>srinivas (1997) and alshawi and carter (1994).<papid> J94-4005 </papid></citsent>
<aftsection>
<nextsent>furthermore, context-free and unification-based grammars have been derived from the penn treebank (cf.
</nextsent>
<nextsent>(charniak, 1996) and (van genabith et al, 1999)).
</nextsent>
<nextsent>these parsers, trained or created by means of the treebank, can be applied for enlarging the treebank.
</nextsent>
<nextsent>for german, the first initiative in the field of treebanks was the negra corpus (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1247">
<title id=" I05-6010.xml">some remarks on the annotation of quantifying noun groups in treebanks </title>
<section> using information about indications of.  </section>
<citcontext>
<prevsection>
<prevsent>nouns that could yield concrete-objectreading as well as an indication-of-quantityreading together with typical elemn following them.
</prevsent>
<prevsent>figure 2: concrete-object-reading figure 3: indication-of-quantity-reading starting from the enriched tiger we used bitpar (cf.
</prevsent>
</prevsection>
<citsent citstr=" C04-1024 ">
(schmid, 2004) <papid> C04-1024 </papid>and (schiehlen, 2004)), <papid> C04-1056 </papid>an efficient parser for treebank grammars in order to induce grammar and parse the treebank.</citsent>
<aftsection>
<nextsent>without the additional information bitpar reached an f-value of 76.17%.
</nextsent>
<nextsent>after adding the information the f-value increased to 76.27%.
</nextsent>
<nextsent>obviously, this is not tremendous improvement of performance, but looking at the absolute numbers, we get more differentiated picture:there are 11 constructions containing an indication of measurement functioning as measure complement of an adjective.
</nextsent>
<nextsent>before adding the information about the scales and the respective adjectives, only 3 constructions were rightly annotated; after having added the information, 6 constructions were rightly annotated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1248">
<title id=" I05-6010.xml">some remarks on the annotation of quantifying noun groups in treebanks </title>
<section> using information about indications of.  </section>
<citcontext>
<prevsection>
<prevsent>nouns that could yield concrete-objectreading as well as an indication-of-quantityreading together with typical elemn following them.
</prevsent>
<prevsent>figure 2: concrete-object-reading figure 3: indication-of-quantity-reading starting from the enriched tiger we used bitpar (cf.
</prevsent>
</prevsection>
<citsent citstr=" C04-1056 ">
(schmid, 2004) <papid> C04-1024 </papid>and (schiehlen, 2004)), <papid> C04-1056 </papid>an efficient parser for treebank grammars in order to induce grammar and parse the treebank.</citsent>
<aftsection>
<nextsent>without the additional information bitpar reached an f-value of 76.17%.
</nextsent>
<nextsent>after adding the information the f-value increased to 76.27%.
</nextsent>
<nextsent>obviously, this is not tremendous improvement of performance, but looking at the absolute numbers, we get more differentiated picture:there are 11 constructions containing an indication of measurement functioning as measure complement of an adjective.
</nextsent>
<nextsent>before adding the information about the scales and the respective adjectives, only 3 constructions were rightly annotated; after having added the information, 6 constructions were rightly annotated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1249">
<title id=" I05-3017.xml">the second international chinese word segmentation bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in chinese is hotly debated, so the determination of the correct division of chinese sentence into words?
</prevsent>
<prevsent>can be very complex.
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
in 2003 sighan, the special interest group for chinese language processing of the association for computational linguistics (acl) conducted the first international chinese word segmentation bakeoff (sproat and emerson, 2003).<papid> W03-1719 </papid></citsent>
<aftsection>
<nextsent>that competition was the first conducted outside of china and has become the benchmark with which researchers evaluate their segmentation systems.
</nextsent>
<nextsent>during the winter of 2004 it was decided to hold second evaluation to determine how the latest research has affected segmentation technology.
</nextsent>
<nextsent>2!
</nextsent>
<nextsent>details of the contest 2.1!
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1251">
<title id=" I05-2018.xml">detecting the count ability of english compound nouns using web based models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>count ability is the semantic property that determines whether noun can occur in singular and plural forms.
</prevsent>
<prevsent>we can obtain the information about count ability of individual nouns easily from grammar books or dictionaries.
</prevsent>
</prevsection>
<citsent citstr=" P03-1059 ">
several researchers have explored automatically learning the count ability of english nouns (bond and vatikiotis-bateson, 2002; schwartz, 2002; baldwin and bond, 2003).<papid> P03-1059 </papid></citsent>
<aftsection>
<nextsent>however, all the proposed approaches focused on learning the count ability of individual nouns.
</nextsent>
<nextsent>a compound noun is noun that is made up of two or more words.
</nextsent>
<nextsent>most compound nouns in english are formed by nouns modified by other nouns or adjectives.
</nextsent>
<nextsent>in this paper, we concentrate solely on compound nouns made up of only two words, as they account for the vast majority of compound nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1254">
<title id=" I05-2018.xml">detecting the count ability of english compound nouns using web based models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally we evaluated our detecting approach on test set and showed that our algorithm based on the simple models performed the promising results.
</prevsent>
<prevsent>querying in www adds noise to the data, we certainly lose some precision compared to supervised statistical models, but we assume that the size of the www will compensate the rough queries.
</prevsent>
</prevsection>
<citsent citstr=" J03-3005 ">
keller and lapata (2003) <papid> J03-3005 </papid>also showed the evidence of the reliability of the web counts for natural language processing.</citsent>
<aftsection>
<nextsent>in (lapata and keller, 2005), they also investigated the count ability leaning task for nouns.
</nextsent>
<nextsent>however they 1 ??????(fa yu xing tong) which is chinese compound noun means growing pains?.
</nextsent>
<nextsent>only distinguish between countable and uncountable for individual nouns.
</nextsent>
<nextsent>the best model is the determiner-noun model, which achieves 88.62% on countable and 91.53% on uncountable nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1257">
<title id=" I08-2078.xml">an online cascaded approach to biomedical named entity recognition </title>
<section> cascaded framework.  </section>
<citcontext>
<prevsection>
<prevsent>another potential advantage of dividing the ner task into two tasks is that it allows greater flexibility in choosing an appropriate set of features for eachtask.
</prevsent>
<prevsent>in fact, adding more features may not necessarily increase performance.
</prevsent>
</prevsection>
<citsent citstr=" W04-1221 ">
(settles, 2004)<papid> W04-1221 </papid>reported that system using subset of features outperformed one using full set of features.</citsent>
<aftsection>
<nextsent>we conducted our experiments on the genia corpus (kim et al, 2003) provided in the jnlpba (kimet al, 2004) shared task1.
</nextsent>
<nextsent>there are 2,000 med line abstracts in the genia corpus with named entities tagged in the iob2 format.
</nextsent>
<nextsent>there are 18,546 sentences and 492,551 words in the training set, and3,856 sentences and 101,039 words in the evaluation set.
</nextsent>
<nextsent>the line indicating the medline abstract id boundary information is not used in our experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1261">
<title id=" I08-2078.xml">an online cascaded approach to biomedical named entity recognition </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>each word is tagged with b-x?, i-x?, or o? to indicate that the word is at the beginning?
</prevsent>
<prevsent>(b) or inside?
</prevsent>
</prevsection>
<citsent citstr=" W04-1219 ">
(i) of named entity of type x, or 1http://research.nii.ac.jp/collier/ workshops/jnlpba04st.htm system f1 (zhou and su, 2004) <papid> W04-1219 </papid>72.55 online cascaded 72.16 (okanohara et al, 2006) <papid> P06-1059 </papid>71.48 (kim et al, 2005) 71.19 (finkel et al, 2004) <papid> W04-1217 </papid>70.06 (settles, 2004)<papid> W04-1221 </papid>69.80 table 1: comparisons with other systems on overall performance (in percentage).</citsent>
<aftsection>
<nextsent>outside?
</nextsent>
<nextsent>(o) of named entity.
</nextsent>
<nextsent>the named entity types are: dna, rna, cell line, cell type, and protein.
</nextsent>
<nextsent>5.1 features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1263">
<title id=" I08-2078.xml">an online cascaded approach to biomedical named entity recognition </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>each word is tagged with b-x?, i-x?, or o? to indicate that the word is at the beginning?
</prevsent>
<prevsent>(b) or inside?
</prevsent>
</prevsection>
<citsent citstr=" P06-1059 ">
(i) of named entity of type x, or 1http://research.nii.ac.jp/collier/ workshops/jnlpba04st.htm system f1 (zhou and su, 2004) <papid> W04-1219 </papid>72.55 online cascaded 72.16 (okanohara et al, 2006) <papid> P06-1059 </papid>71.48 (kim et al, 2005) 71.19 (finkel et al, 2004) <papid> W04-1217 </papid>70.06 (settles, 2004)<papid> W04-1221 </papid>69.80 table 1: comparisons with other systems on overall performance (in percentage).</citsent>
<aftsection>
<nextsent>outside?
</nextsent>
<nextsent>(o) of named entity.
</nextsent>
<nextsent>the named entity types are: dna, rna, cell line, cell type, and protein.
</nextsent>
<nextsent>5.1 features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1264">
<title id=" I08-2078.xml">an online cascaded approach to biomedical named entity recognition </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>each word is tagged with b-x?, i-x?, or o? to indicate that the word is at the beginning?
</prevsent>
<prevsent>(b) or inside?
</prevsent>
</prevsection>
<citsent citstr=" W04-1217 ">
(i) of named entity of type x, or 1http://research.nii.ac.jp/collier/ workshops/jnlpba04st.htm system f1 (zhou and su, 2004) <papid> W04-1219 </papid>72.55 online cascaded 72.16 (okanohara et al, 2006) <papid> P06-1059 </papid>71.48 (kim et al, 2005) 71.19 (finkel et al, 2004) <papid> W04-1217 </papid>70.06 (settles, 2004)<papid> W04-1221 </papid>69.80 table 1: comparisons with other systems on overall performance (in percentage).</citsent>
<aftsection>
<nextsent>outside?
</nextsent>
<nextsent>(o) of named entity.
</nextsent>
<nextsent>the named entity types are: dna, rna, cell line, cell type, and protein.
</nextsent>
<nextsent>5.1 features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1273">
<title id=" I08-2078.xml">an online cascaded approach to biomedical named entity recognition </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>these programs were developed based on the same basic framework.
</prevsent>
<prevsent>all of the experiments were run on unix machine with 2.8 ghz cpu and 16 gbram.
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
in particular, the crf trained by maximum likelihood uses the l-bfgs algorithm (liu and no cedal, 1989), which converges quickly and givesa good performance on maximum entropy models (malouf, 2002; <papid> W02-2018 </papid>sha and pereira, 2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>we compare our experimental results in several dimensions.training time: referring to table 4, the training time of the online cascaded approach is substantially shorter than that of all of the other approaches.
</nextsent>
<nextsent>in the single-phase approach, training crf by maximum likelihood (ml) using the l-bfgs algorithm is the slowest and requires around 28 hours.
</nextsent>
<nextsent>the online method greatly reduces the training time to around two hours, which is 14 times faster.
</nextsent>
<nextsent>by employing two-phase approach, the training time is further reduced to half an hour.memory requirement: table 4 shows the number of features that are required by the differentmethods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1274">
<title id=" I08-2078.xml">an online cascaded approach to biomedical named entity recognition </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>these programs were developed based on the same basic framework.
</prevsent>
<prevsent>all of the experiments were run on unix machine with 2.8 ghz cpu and 16 gbram.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
in particular, the crf trained by maximum likelihood uses the l-bfgs algorithm (liu and no cedal, 1989), which converges quickly and givesa good performance on maximum entropy models (malouf, 2002; <papid> W02-2018 </papid>sha and pereira, 2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>we compare our experimental results in several dimensions.training time: referring to table 4, the training time of the online cascaded approach is substantially shorter than that of all of the other approaches.
</nextsent>
<nextsent>in the single-phase approach, training crf by maximum likelihood (ml) using the l-bfgs algorithm is the slowest and requires around 28 hours.
</nextsent>
<nextsent>the online method greatly reduces the training time to around two hours, which is 14 times faster.
</nextsent>
<nextsent>by employing two-phase approach, the training time is further reduced to half an hour.memory requirement: table 4 shows the number of features that are required by the differentmethods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1276">
<title id=" I08-2078.xml">an online cascaded approach to biomedical named entity recognition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the online training approach used in this paper is based on the concept of margin?
</prevsent>
<prevsent>(cristianini, 2001).
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
a pioneer work in online training is the perceptron-like algorithm used in training hidden markov model (hmm) (collins, 2002).<papid> W02-1001 </papid></citsent>
<aftsection>
<nextsent>(mcdonald 5we are aware of the high f1 in (vishwanathan et al, 2006).
</nextsent>
<nextsent>we contacted the author and found that their published result may be incomplete.
</nextsent>
<nextsent>598 experiments no.
</nextsent>
<nextsent>of features training time f1 rel.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1279">
<title id=" I08-2084.xml">term extraction through unithood and termhood unification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some of them borrowed the metrics from information retrieval to evaluate how important term is within document or corpus.
</prevsent>
<prevsent>those metrics are term frequency/inverse document frequency (tf/idf), mutual information, t-score, cosine, and information gain.
</prevsent>
</prevsection>
<citsent citstr=" W02-1407 ">
there are also other works (nakagawa and mori, 2002; <papid> W02-1407 </papid>frantzi and ananiadou, 1998) that introduced better method to weigh the term candidates.</citsent>
<aftsection>
<nextsent>currently, the c/nc method (frantzi and ananiadou, 1998) is widely considered as the state-of-the-art model forte.
</nextsent>
<nextsent>although this method was first applied on english, it also performed well on other languages such as japanese (hideki mima and sophia ananiadou, 2001), slovene (pela vintar, 2004), and other domains such as medical corpus (frantzi and ananiadou, 1998), and computer science (e. milios et al 2003).
</nextsent>
<nextsent>in terminology research, term is evaluated using two types of feature: termhood1 and unithood 1 termhood refers to degree of linguistic unit.
</nextsent>
<nextsent>it considers a. term as linguistic unit representative for the document content.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1280">
<title id=" I05-2030.xml">opinion extraction using a learning based anaphora resolution technique </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous approaches to the task of mining large-scale document collection for opinions canbe classified into two groups: the document classification approach and the information extraction approach.
</prevsent>
<prevsent>in the document classification approach, researchers have been exploring techniques for classifying documents according to se mantic/sentiment orientation such as positive vs. negative (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
(dave et al , 2003; pang and lee, 2004; <papid> P04-1035 </papid>turney, 2002)).<papid> P02-1053 </papid></citsent>
<aftsection>
<nextsent>the information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g.
</nextsent>
<nextsent>(kanayama and nasukawa, 2004; hu and liu, 2004; tateishi et al , 2001)).
</nextsent>
<nextsent>the aim of this paper is to extract opinions that represent an evaluation of products together with the evidence.
</nextsent>
<nextsent>to achieve this, we consider our task from the information extraction view point.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1281">
<title id=" I05-2030.xml">opinion extraction using a learning based anaphora resolution technique </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous approaches to the task of mining large-scale document collection for opinions canbe classified into two groups: the document classification approach and the information extraction approach.
</prevsent>
<prevsent>in the document classification approach, researchers have been exploring techniques for classifying documents according to se mantic/sentiment orientation such as positive vs. negative (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
(dave et al , 2003; pang and lee, 2004; <papid> P04-1035 </papid>turney, 2002)).<papid> P02-1053 </papid></citsent>
<aftsection>
<nextsent>the information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g.
</nextsent>
<nextsent>(kanayama and nasukawa, 2004; hu and liu, 2004; tateishi et al , 2001)).
</nextsent>
<nextsent>the aim of this paper is to extract opinions that represent an evaluation of products together with the evidence.
</nextsent>
<nextsent>to achieve this, we consider our task from the information extraction view point.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1282">
<title id=" I05-2030.xml">opinion extraction using a learning based anaphora resolution technique </title>
<section> method for opinion extraction.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 existing techniques for anaphora.
</prevsent>
<prevsent>resolution corpus-based empirical approaches to anaphora resolution have been reasonably successful.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
this approach, as exemplified by (soon et al , 2001; <papid> J01-4004 </papid>iida et al , 2003; ng, 2004), <papid> P04-1020 </papid>is cost effective, while achieving better performance than the best-performing rule-based systems for the test sets of muc-6 and muc-7 2.</citsent>
<aftsection>
<nextsent>as suggested by figure 1, anaphora resolution can be decomposed into two subtasks: anaphoric ity determination and antecedent identification.anaphoricity determination is the task of judging whether given np is anaphoric or nonanaphoric.
</nextsent>
<nextsent>recent research advances have provided several important findings as follows: ? learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by centering theory (grosz et al , 1995).?<papid> J95-2003 </papid></nextsent>
<nextsent>one useful clue for anaphoricity determination is the availability of plausible candidate for the antecedent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1283">
<title id=" I05-2030.xml">opinion extraction using a learning based anaphora resolution technique </title>
<section> method for opinion extraction.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 existing techniques for anaphora.
</prevsent>
<prevsent>resolution corpus-based empirical approaches to anaphora resolution have been reasonably successful.
</prevsent>
</prevsection>
<citsent citstr=" P04-1020 ">
this approach, as exemplified by (soon et al , 2001; <papid> J01-4004 </papid>iida et al , 2003; ng, 2004), <papid> P04-1020 </papid>is cost effective, while achieving better performance than the best-performing rule-based systems for the test sets of muc-6 and muc-7 2.</citsent>
<aftsection>
<nextsent>as suggested by figure 1, anaphora resolution can be decomposed into two subtasks: anaphoric ity determination and antecedent identification.anaphoricity determination is the task of judging whether given np is anaphoric or nonanaphoric.
</nextsent>
<nextsent>recent research advances have provided several important findings as follows: ? learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by centering theory (grosz et al , 1995).?<papid> J95-2003 </papid></nextsent>
<nextsent>one useful clue for anaphoricity determination is the availability of plausible candidate for the antecedent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1284">
<title id=" I05-2030.xml">opinion extraction using a learning based anaphora resolution technique </title>
<section> method for opinion extraction.  </section>
<citcontext>
<prevsection>
<prevsent>this approach, as exemplified by (soon et al , 2001; <papid> J01-4004 </papid>iida et al , 2003; ng, 2004), <papid> P04-1020 </papid>is cost effective, while achieving better performance than the best-performing rule-based systems for the test sets of muc-6 and muc-7 2.</prevsent>
<prevsent>as suggested by figure 1, anaphora resolution can be decomposed into two subtasks: anaphoric ity determination and antecedent identification.anaphoricity determination is the task of judging whether given np is anaphoric or nonanaphoric.</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
recent research advances have provided several important findings as follows: ? learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by centering theory (grosz et al , 1995).?<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>one useful clue for anaphoricity determination is the availability of plausible candidate for the antecedent.
</nextsent>
<nextsent>if an appropriate candidate for the antecedent is found in the preceding discourse context, the np is likely to be anaphoric.
</nextsent>
<nextsent>for these reasons, an anaphora resolution model performs best if it carries out the following pro 2the 7th message understanding conference (1998): www.itl.nist.gov/iaui/894.02/related projects/muc/ interia seki??
</nextsent>
<nextsent>dezain-wa hen-desuga watashi-wa suki-desu ?????
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1286">
<title id=" I05-5001.xml">support vector machines for paraphrase identification and corpus construction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>use of this technique dramatically reduces the alignment error rate of the extracted corpora over heuristic methods based on position of the sentences in the text.
</prevsent>
<prevsent>paraphrase detection the ability to determine whether or not two formally distinct strings are similar in meaning is increasingly recognized as crucial to future applications in multiple fields including information retrieval, question answering, and summarization.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
a growing body of recent research has focused on the problems of identifying and generating paraphrases, e.g., barzilay &amp; mckeown (2001), <papid> P01-1008 </papid>lin &amp; pantel (2002), shinyama et al (2002), barzilay &amp; lee (2003), <papid> N03-1003 </papid>and pang et al (2003).<papid> N03-1024 </papid></citsent>
<aftsection>
<nextsent>one promising approach extends standard statistical machine translation (smt) techniques (e.g., brown et al, 1993; <papid> J93-2003 </papid>och &amp; ney, 2000, <papid> P00-1056 </papid>2003) to the problems of monolingual paraphrase identification and generation.</nextsent>
<nextsent>finch et al (2004) have described several mt based paraphrase systems within the context of improving machine translation output.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1287">
<title id=" I05-5001.xml">support vector machines for paraphrase identification and corpus construction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>use of this technique dramatically reduces the alignment error rate of the extracted corpora over heuristic methods based on position of the sentences in the text.
</prevsent>
<prevsent>paraphrase detection the ability to determine whether or not two formally distinct strings are similar in meaning is increasingly recognized as crucial to future applications in multiple fields including information retrieval, question answering, and summarization.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
a growing body of recent research has focused on the problems of identifying and generating paraphrases, e.g., barzilay &amp; mckeown (2001), <papid> P01-1008 </papid>lin &amp; pantel (2002), shinyama et al (2002), barzilay &amp; lee (2003), <papid> N03-1003 </papid>and pang et al (2003).<papid> N03-1024 </papid></citsent>
<aftsection>
<nextsent>one promising approach extends standard statistical machine translation (smt) techniques (e.g., brown et al, 1993; <papid> J93-2003 </papid>och &amp; ney, 2000, <papid> P00-1056 </papid>2003) to the problems of monolingual paraphrase identification and generation.</nextsent>
<nextsent>finch et al (2004) have described several mt based paraphrase systems within the context of improving machine translation output.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1288">
<title id=" I05-5001.xml">support vector machines for paraphrase identification and corpus construction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>use of this technique dramatically reduces the alignment error rate of the extracted corpora over heuristic methods based on position of the sentences in the text.
</prevsent>
<prevsent>paraphrase detection the ability to determine whether or not two formally distinct strings are similar in meaning is increasingly recognized as crucial to future applications in multiple fields including information retrieval, question answering, and summarization.
</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
a growing body of recent research has focused on the problems of identifying and generating paraphrases, e.g., barzilay &amp; mckeown (2001), <papid> P01-1008 </papid>lin &amp; pantel (2002), shinyama et al (2002), barzilay &amp; lee (2003), <papid> N03-1003 </papid>and pang et al (2003).<papid> N03-1024 </papid></citsent>
<aftsection>
<nextsent>one promising approach extends standard statistical machine translation (smt) techniques (e.g., brown et al, 1993; <papid> J93-2003 </papid>och &amp; ney, 2000, <papid> P00-1056 </papid>2003) to the problems of monolingual paraphrase identification and generation.</nextsent>
<nextsent>finch et al (2004) have described several mt based paraphrase systems within the context of improving machine translation output.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1290">
<title id=" I05-5001.xml">support vector machines for paraphrase identification and corpus construction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>paraphrase detection the ability to determine whether or not two formally distinct strings are similar in meaning is increasingly recognized as crucial to future applications in multiple fields including information retrieval, question answering, and summarization.
</prevsent>
<prevsent>a growing body of recent research has focused on the problems of identifying and generating paraphrases, e.g., barzilay &amp; mckeown (2001), <papid> P01-1008 </papid>lin &amp; pantel (2002), shinyama et al (2002), barzilay &amp; lee (2003), <papid> N03-1003 </papid>and pang et al (2003).<papid> N03-1024 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
one promising approach extends standard statistical machine translation (smt) techniques (e.g., brown et al, 1993; <papid> J93-2003 </papid>och &amp; ney, 2000, <papid> P00-1056 </papid>2003) to the problems of monolingual paraphrase identification and generation.</citsent>
<aftsection>
<nextsent>finch et al (2004) have described several mt based paraphrase systems within the context of improving machine translation output.
</nextsent>
<nextsent>quirk et al (2004) <papid> W04-3219 </papid>describe an end-to-end paraphrase identification and generation system using giza++ (och &amp; ney, 2003) <papid> J03-1002 </papid>and monotone decoder to generate information preserving paraphrases.</nextsent>
<nextsent>as with conventional smt systems, smt based paraphrase systems require extensive monolingual parallel training corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1291">
<title id=" I05-5001.xml">support vector machines for paraphrase identification and corpus construction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>paraphrase detection the ability to determine whether or not two formally distinct strings are similar in meaning is increasingly recognized as crucial to future applications in multiple fields including information retrieval, question answering, and summarization.
</prevsent>
<prevsent>a growing body of recent research has focused on the problems of identifying and generating paraphrases, e.g., barzilay &amp; mckeown (2001), <papid> P01-1008 </papid>lin &amp; pantel (2002), shinyama et al (2002), barzilay &amp; lee (2003), <papid> N03-1003 </papid>and pang et al (2003).<papid> N03-1024 </papid></prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
one promising approach extends standard statistical machine translation (smt) techniques (e.g., brown et al, 1993; <papid> J93-2003 </papid>och &amp; ney, 2000, <papid> P00-1056 </papid>2003) to the problems of monolingual paraphrase identification and generation.</citsent>
<aftsection>
<nextsent>finch et al (2004) have described several mt based paraphrase systems within the context of improving machine translation output.
</nextsent>
<nextsent>quirk et al (2004) <papid> W04-3219 </papid>describe an end-to-end paraphrase identification and generation system using giza++ (och &amp; ney, 2003) <papid> J03-1002 </papid>and monotone decoder to generate information preserving paraphrases.</nextsent>
<nextsent>as with conventional smt systems, smt based paraphrase systems require extensive monolingual parallel training corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1292">
<title id=" I05-5001.xml">support vector machines for paraphrase identification and corpus construction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one promising approach extends standard statistical machine translation (smt) techniques (e.g., brown et al, 1993; <papid> J93-2003 </papid>och &amp; ney, 2000, <papid> P00-1056 </papid>2003) to the problems of monolingual paraphrase identification and generation.</prevsent>
<prevsent>finch et al (2004) have described several mt based paraphrase systems within the context of improving machine translation output.</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
quirk et al (2004) <papid> W04-3219 </papid>describe an end-to-end paraphrase identification and generation system using giza++ (och &amp; ney, 2003) <papid> J03-1002 </papid>and monotone decoder to generate information preserving paraphrases.</citsent>
<aftsection>
<nextsent>as with conventional smt systems, smt based paraphrase systems require extensive monolingual parallel training corpora.
</nextsent>
<nextsent>however, while translation is common human activity, resulting in large corpora of human-translated bilingual sentence pairs being relatively easy to obtain across multiple domains and language pairs, this is not the case in monolingual paraphrase, where naturally-occurring parallel data are hard to come by.
</nextsent>
<nextsent>the paucity of readily available monolingual parallel training corpora poses formidable obstacle to the development of smt-based paraphrase systems.
</nextsent>
<nextsent>the present paper describes the extraction of parallel corpora from clustered news articles using annotated seed corpora and an svm classifier, demonstrating that large parallel corpora can be induced by classifier that includes morphological and synonymy features derived from both static and dynamic resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1294">
<title id=" I05-5001.xml">support vector machines for paraphrase identification and corpus construction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one promising approach extends standard statistical machine translation (smt) techniques (e.g., brown et al, 1993; <papid> J93-2003 </papid>och &amp; ney, 2000, <papid> P00-1056 </papid>2003) to the problems of monolingual paraphrase identification and generation.</prevsent>
<prevsent>finch et al (2004) have described several mt based paraphrase systems within the context of improving machine translation output.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
quirk et al (2004) <papid> W04-3219 </papid>describe an end-to-end paraphrase identification and generation system using giza++ (och &amp; ney, 2003) <papid> J03-1002 </papid>and monotone decoder to generate information preserving paraphrases.</citsent>
<aftsection>
<nextsent>as with conventional smt systems, smt based paraphrase systems require extensive monolingual parallel training corpora.
</nextsent>
<nextsent>however, while translation is common human activity, resulting in large corpora of human-translated bilingual sentence pairs being relatively easy to obtain across multiple domains and language pairs, this is not the case in monolingual paraphrase, where naturally-occurring parallel data are hard to come by.
</nextsent>
<nextsent>the paucity of readily available monolingual parallel training corpora poses formidable obstacle to the development of smt-based paraphrase systems.
</nextsent>
<nextsent>the present paper describes the extraction of parallel corpora from clustered news articles using annotated seed corpora and an svm classifier, demonstrating that large parallel corpora can be induced by classifier that includes morphological and synonymy features derived from both static and dynamic resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1300">
<title id=" I05-5001.xml">support vector machines for paraphrase identification and corpus construction </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>barzilay &amp; lee (2003) <papid> N03-1003 </papid>employ multiple sequence alignment (msa, e.g., durbin et al, 1998) to align strings extracted from closely related news articles.</prevsent>
<prevsent>although the msa approach can produce dramatic results, it is chiefly effective in extracting highly templatic data, and appears to be of limited extensibility to broad domain application (quirk et al 2004).<papid> W04-3219 </papid></prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
recent work by dolan, et al (2004) <papid> C04-1051 </papid>describes the construction of broad-domain corpora of aligned paraphrase pairs extracted from news cluster data on the world wide web using two heuristic strategies: 1) pairing sentences based on word-based edit distance heuristic; and 2) naive text-feature-based heuristic in which the first two sentences of each article in cluster are cross-matched with each other, their assumption being that the early sentences of news article will tend to summarize the whole article and are thus likely to contain the same information as other early sentences of other articles in the cluster.</citsent>
<aftsection>
<nextsent>the word-based edit distance heuristic yields pairs that are relatively clean but offer relatively minor rewrites in generation, especially when compared to the msa model of (barzilay &amp; lee, 2003).<papid> N03-1003 </papid></nextsent>
<nextsent>the text-based heuristic, on the other hand, results in noisy compara ble?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1303">
<title id=" I05-5001.xml">support vector machines for paraphrase identification and corpus construction </title>
<section> constructing classifier.  </section>
<citcontext>
<prevsection>
<prevsent>a lexicon of 314,924 word pairs of the following form created.
</prevsent>
<prevsent>only those pairs identified as occurring in either training data or the corpus to be classified were included in the final classifier.
</prevsent>
</prevsection>
<citsent citstr=" W01-1411 ">
operation|procedure operation|work word association pairs: to augment the above resources, we dynamically extracted from the l12 corpus lexicon of 13001 possibly-synonymous word pairs using log-likelihood algorithm described in moore (2001) <papid> W01-1411 </papid>for machine translation.</citsent>
<aftsection>
<nextsent>to minimize the damping effect of the overwhelming number of identical words, these were deleted from each sentence pair prior to proc essing; the algorithm was then run on the non-identical residue as if it were bilingual parallel corpus.
</nextsent>
<nextsent>to deploy this data in the svm feature set, cutoff was arbitrarily selected that yielded 13001 word pairs.
</nextsent>
<nextsent>some exemplars (not found in wordnet) include: straight|consecutive vendors|suppliersfig.
</nextsent>
<nextsent>1 shows the distribution of word pairings obtained by this method on the l12 corpus in comparison with wordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1308">
<title id=" I05-5001.xml">support vector machines for paraphrase identification and corpus construction </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>evaluation of paraphrase recognition within an smt framework is highly problematic, since no technique or dataset is standardly recognized.
</prevsent>
<prevsent>barzilay &amp; lee (2003) <papid> N03-1003 </papid>and quirk et al (2004) <papid> W04-3219 </papid>use human evaluations of end-to-end generation, but these are not very useful here, since they add an additional layer of uncertainty into the evaluation, and depend to significant extent on the quality and functionality of the decoder.</prevsent>
</prevsection>
<citsent citstr=" I05-5002 ">
dolan &amp; brockett (2005) <papid> I05-5002 </papid>report extraction precision of 67% using similar classifier, but with the explicit intention of creating corpus that contained significant number of naturally occuring paraphrase-like negative examples.</citsent>
<aftsection>
<nextsent>since our purpose in the present work is non application specific corpus construction, we apply an automated technique that is widely used for reporting intermediate results in the smt community, and is being extended in other fields such as summarization (daum?
</nextsent>
<nextsent>and marcu, forthcoming), namely word-level alignment using an off-the-shelf implementation of the smt system giza++ (och &amp; ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>below, we use alignment error rate (aer), which is indicative of how far the corpus is from providing solution under standard smt tool.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1316">
<title id=" I05-2045.xml">unsupervised feature selection for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>relation extraction is the task of finding relationships between two entities from text contents.
</prevsent>
<prevsent>there has been considerable work on supervised learning of relation patterns, using corpora which have been annotated to indicate the information tobe extracted (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W02-1010 ">
(califf and mooney, 1999; zelenko et al, 2002)).<papid> W02-1010 </papid></citsent>
<aftsection>
<nextsent>a range of extraction models have been used, including both symbolic rules and statistical rules such as hmms or kernels.these methods have been particularly successful in some specific domains.
</nextsent>
<nextsent>however, manually tagging of large amounts of training data is very time-consuming; furthermore, it is difficult for one extraction system to be ported across different domains.
</nextsent>
<nextsent>due to the limitation of supervised methods,some weakly supervised (or semi-supervised) approaches have been suggested (brin, 1998; eugene and luis, 2000; sudo et al, 2003).<papid> P03-1029 </papid></nextsent>
<nextsent>one common characteristic of these algorithms is that they need to pre-define some initial seeds for any particular relation, then bootstrap from the seeds to acquire the relation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1317">
<title id=" I05-2045.xml">unsupervised feature selection for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a range of extraction models have been used, including both symbolic rules and statistical rules such as hmms or kernels.these methods have been particularly successful in some specific domains.
</prevsent>
<prevsent>however, manually tagging of large amounts of training data is very time-consuming; furthermore, it is difficult for one extraction system to be ported across different domains.
</prevsent>
</prevsection>
<citsent citstr=" P03-1029 ">
due to the limitation of supervised methods,some weakly supervised (or semi-supervised) approaches have been suggested (brin, 1998; eugene and luis, 2000; sudo et al, 2003).<papid> P03-1029 </papid></citsent>
<aftsection>
<nextsent>one common characteristic of these algorithms is that they need to pre-define some initial seeds for any particular relation, then bootstrap from the seeds to acquire the relation.
</nextsent>
<nextsent>however, it is not easy to select representative seeds for obtaining good results.hasegawa, et al put forward an unsupervised approach for relation extraction from large text corpora (hasegawa et al, 2004).<papid> P04-1053 </papid></nextsent>
<nextsent>first, they adopted hierarchical clustering method to cluster the contexts of entity pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1318">
<title id=" I05-2045.xml">unsupervised feature selection for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>due to the limitation of supervised methods,some weakly supervised (or semi-supervised) approaches have been suggested (brin, 1998; eugene and luis, 2000; sudo et al, 2003).<papid> P03-1029 </papid></prevsent>
<prevsent>one common characteristic of these algorithms is that they need to pre-define some initial seeds for any particular relation, then bootstrap from the seeds to acquire the relation.</prevsent>
</prevsection>
<citsent citstr=" P04-1053 ">
however, it is not easy to select representative seeds for obtaining good results.hasegawa, et al put forward an unsupervised approach for relation extraction from large text corpora (hasegawa et al, 2004).<papid> P04-1053 </papid></citsent>
<aftsection>
<nextsent>first, they adopted hierarchical clustering method to cluster the contexts of entity pairs.
</nextsent>
<nextsent>second, after context clustering, they selected the most frequent words in the contexts to represent the relation that holds between the entities.
</nextsent>
<nextsent>however, the approach exists its limitation.
</nextsent>
<nextsent>firstly, the similarity threshold for the clusters, like the appropriate number of clusters, is somewhat difficult to predefined.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1319">
<title id=" I05-2045.xml">unsupervised feature selection for relation extraction </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 evaluation method for relation labelling.
</prevsent>
<prevsent>for evaluation of the relation labeling, we need to explore the relatedness between the identified labels and the pre-defined relation names.
</prevsent>
</prevsection>
<citsent citstr=" P97-1009 ">
to dothis, we use one information-content based measure (lin, 1997), <papid> P97-1009 </papid>which is provided in wordnet similarity package (pedersen et al, 2004) to evaluate the similarity between two concepts in wordnet.</citsent>
<aftsection>
<nextsent>intuitively, the relatedness between two concepts in wordnet is captured by the information content of their lowest common subsumer (lcs) and the information content of the two concepts themselves , which can be formalized as follows: relatednesslin(c1, c2) = 2ic(lcs(c1,c2))ic(c1)+ic(c2) . this measure depends upon the corpus to estimate information content.
</nextsent>
<nextsent>we carried out the experiments using the british national corpus (bnc) as the source of information content.
</nextsent>
<nextsent>3.4 experiments and results.
</nextsent>
<nextsent>for comparison of the effect of the outer and within contexts of entity pairs, we used five dif 265 table 4: automatically determined the number of relation types using different feature ranking methods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1321">
<title id=" I08-2096.xml">coverage based evaluation of parser generalizability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results indicate that the coverage typically drops several percentage points when parsers are faced with texts on genres other than newspapers.
</prevsent>
<prevsent>the fact that most of the parser evaluation resources employed consist of texts from single genre constitutes deficiency in most of the parser evaluations.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
evaluations are typically carried out on newspaper texts, i.e. on section 23 of the penn treebank (ptb) (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>a further complication is that many parsing models are trained on the same treebank.
</nextsent>
<nextsent>parsers therefore come to be applied to texts from numerous other genres untested.
</nextsent>
<nextsent>the obvious question that confronts us in these circumstances is: how well will parser that performs well on financial texts from the wall street journal generalize to other text types?
</nextsent>
<nextsent>this present paper addresses parser evaluation from the perspective of coverage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1322">
<title id=" I08-2096.xml">coverage based evaluation of parser generalizability </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>to our knowledge, this experiment is the only one reported in the literature that compares the coverage of set of parsers for english.
</prevsent>
<prevsent>the studies that critically examine the genre dependency have come to the same unsurprising conclusion that the text genre has an effect on the parsers performance.
</prevsent>
</prevsection>
<citsent citstr=" A97-1015 ">
the genre dependency of parsers is an accepted fact and has been described by, among others, sekine (1997) <papid> A97-1015 </papid>and gildea (2001).<papid> W01-0521 </papid></citsent>
<aftsection>
<nextsent>for example, clegg and shepherd (2005) have undertaken experiments on biomedical data using the genia treebank.
</nextsent>
<nextsent>laakso (2005) reports experiments on the childes corpus of transcribed speech between parents and the children.
</nextsent>
<nextsent>mazzei and lombardo (2004) report cross-training experiments in italian on newspaper and civil law texts.
</nextsent>
<nextsent>they observed dramatic drop of, most commonly, around 10-30 percentage points in the parsing coverage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1323">
<title id=" I08-2096.xml">coverage based evaluation of parser generalizability </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>to our knowledge, this experiment is the only one reported in the literature that compares the coverage of set of parsers for english.
</prevsent>
<prevsent>the studies that critically examine the genre dependency have come to the same unsurprising conclusion that the text genre has an effect on the parsers performance.
</prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
the genre dependency of parsers is an accepted fact and has been described by, among others, sekine (1997) <papid> A97-1015 </papid>and gildea (2001).<papid> W01-0521 </papid></citsent>
<aftsection>
<nextsent>for example, clegg and shepherd (2005) have undertaken experiments on biomedical data using the genia treebank.
</nextsent>
<nextsent>laakso (2005) reports experiments on the childes corpus of transcribed speech between parents and the children.
</nextsent>
<nextsent>mazzei and lombardo (2004) report cross-training experiments in italian on newspaper and civil law texts.
</nextsent>
<nextsent>they observed dramatic drop of, most commonly, around 10-30 percentage points in the parsing coverage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1324">
<title id=" I08-2096.xml">coverage based evaluation of parser generalizability </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 reasons for the coverage drop.
</prevsent>
<prevsent>genre dependency is caused by several factors.
</prevsent>
</prevsection>
<citsent citstr=" J93-2001 ">
one is that each text genre is characterized by genre-specific words (biber, 1993).<papid> J93-2001 </papid></citsent>
<aftsection>
<nextsent>another feature of genre dependency is syntactic structure distributions.
</nextsent>
<nextsent>baldwin et al (2004) have conducted one of the rare studies that offer an analysis of the main reasons for the diminished coverage.
</nextsent>
<nextsent>they experimented with an hpsg grammar that was created manually based on corpus of data extracted from informal genres such as conversations about schedules and e-mails about e-commerce.
</nextsent>
<nextsent>the grammar was used for parsing random sample of texts from several genres.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1325">
<title id=" I08-2096.xml">coverage based evaluation of parser generalizability </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we regard sentence as having been covered if app finds single nonterminal which dominates the whole sentence and if it does not contain any tags which would indicate constituents of unrecognized category.
</prevsent>
<prevsent>c&c; parser (v. 0.96, 23 november 2006) is based on ccg.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
it applies log-linear probabilistic tagging and parsing models (clark and curran, 2004).<papid> P04-1014 </papid></citsent>
<aftsection>
<nextsent>because the parser marks every output as genre description no.
</nextsent>
<nextsent>of sentences v g. le gt legislation discussions of the canadian parliament 390,042 17.2 newspaper texts from several newspapers 217,262 19.5 fiction novels from the 20th and 21st century 97,156 15.9 non-fiction non-fiction books from the 20th and 21st century 61,911 21.9 religion the bible, the koran, the book of mormon 45,459 27.1 biomedicine abstracts from biomedical journals 14,655 21.6 total 826,485 18.6 705 either parsed or failed, evaluation of failed parses is straightforward.
</nextsent>
<nextsent>fragmented parses were detected from the grammatical relations (gr) output.
</nextsent>
<nextsent>because gr representations can form cycles, an analysis was not required to have unique root.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1326">
<title id=" I08-2096.xml">coverage based evaluation of parser generalizability </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>a sentence is regarded as having been covered by minipar if single root is found for it that is connected to all the words in the sentence through path.
</prevsent>
<prevsent>the root should in addition be assigned with phrase/sentence type marker.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
stanford parser (referred in the remainder of this text as sp) (v. 1.5.1, 30 may 2006) can use both an un lexicalized and lexicalized pcfgs (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>this parser uses cyk search algorithm and can output both and ps analyses (de marneffe et al, 2006).
</nextsent>
<nextsent>we ran the experiment on the un lexicalized grammar and carried out the evaluation on the output consisting of 48 types.
</nextsent>
<nextsent>we regard sentence as having been covered by sp in way similar to that in minipar: the sentence is covered if the tree returned by the parser has single root node in which there is path to all the other nodes in the tree.
</nextsent>
<nextsent>statccg (preliminary public release, 14 january 2004) is statistical parser for ccg that was developed by julia hockenmaier (2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1327">
<title id=" I08-1054.xml">answering definition questions via temporally anchored text snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>offer little guidance as to what particular techniques could be used in order to return relevant information from large text collection.
</prevsent>
<prevsent>in fact, the same usermay choose to submit definition question or simpler exploratory query (caetano veloso), and still look for text snippets capturing relevant properties of the question concept.
</prevsent>
</prevsection>
<citsent citstr=" P06-1136 ">
various studies (chen et al,2006; <papid> P06-1136 </papid>han et al, 2006) illustrate the challenges introduced by definition questions.</citsent>
<aftsection>
<nextsent>as such questions have less irregular form than other open-domainquestions, recognizing their type is relatively easier (hildebrandt et al, 2004).<papid> N04-1007 </papid></nextsent>
<nextsent>conversely, the identification of relevant documents and the extraction of answers to definition questions are more laborious, and the impact on the architecture of qa systems is quite significant.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1328">
<title id=" I08-1054.xml">answering definition questions via temporally anchored text snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in fact, the same usermay choose to submit definition question or simpler exploratory query (caetano veloso), and still look for text snippets capturing relevant properties of the question concept.
</prevsent>
<prevsent>various studies (chen et al,2006; <papid> P06-1136 </papid>han et al, 2006) illustrate the challenges introduced by definition questions.</prevsent>
</prevsection>
<citsent citstr=" N04-1007 ">
as such questions have less irregular form than other open-domainquestions, recognizing their type is relatively easier (hildebrandt et al, 2004).<papid> N04-1007 </papid></citsent>
<aftsection>
<nextsent>conversely, the identification of relevant documents and the extraction of answers to definition questions are more laborious, and the impact on the architecture of qa systems is quite significant.
</nextsent>
<nextsent>indeed, separate, dedicated modules, or even end-to-end systems are specifically built for answering definition questions (kla vans and muresan, 2001; hildebrandt et al, 2004; <papid> N04-1007 </papid>greenwood and saggion, 2004).</nextsent>
<nextsent>the importance of definition questions among other question categories is confirmed by their inclusion among the evaluation queries from the qa track of trec evaluations (voorhees and tice, 2000).this paper investigates the impact of temporally 411 anchored text snippets derived from the web, in answering definition questions and, more generally, exploratory queries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1330">
<title id=" I08-1054.xml">answering definition questions via temporally anchored text snippets </title>
<section> temporally anchored text snippets.  </section>
<citcontext>
<prevsection>
<prevsent>all experiments relyon the unstructured text in approximately one billion documents in english from 2003 web repository snapshot of the google search.
</prevsent>
<prevsent>engine.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
pre-processing of the documents consist sin html tag removal, simplified sentence boundary detection, tokenization and part-of-speech tagging with the tnt tagger (brants, 2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>no other tools or lexical resources are employed.a sequence of sentence tokens represents potential date if it consists of: single year (four-digit numbers, e.g., 1929); or simple decade (e.g., 1930s); or month name and year (e.g., january 1929); or month name, day number and year (e.g., january15, 1929).
</nextsent>
<nextsent>dates occurring in text in any other format are ignored.
</nextsent>
<nextsent>to avoid spurious matches, such as 1929 people, potential dates are discarded if theyare immediately followed by noun or noun modifier, or immediately preceded by noun.
</nextsent>
<nextsent>to convert document sentences into few text snippets associated with dates, the overall structure of sentences is roughly approximated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1331">
<title id=" I08-1054.xml">answering definition questions via temporally anchored text snippets </title>
<section> temporally anchored text snippets.  </section>
<citcontext>
<prevsection>
<prevsent>the combination of patterns and constraints is by no means definitive or error-free.
</prevsent>
<prevsent>it is practical solution to achieve graceful degradation on large amounts of data, reduce the extraction errors, and improve the usefulness of the extracted snippets.
</prevsent>
</prevsection>
<citsent citstr=" P06-1095 ">
as such, it emphasizes robustness at web scale, without taking advantage of existing specification languages for representing events and temporal expressions occurring in text (pustejovsky et al, 2003), and forgoing the potential benefits of more complex methods that extract temporal relations from relatively clean text collections (mani et al, 2006).<papid> P06-1095 </papid></citsent>
<aftsection>
<nextsent>a concept such as particular actor, country or organization usually occurs within more than one of the extracted text snippets.
</nextsent>
<nextsent>in fact, the set of text snippets containing the concept, together with the associated dates, often represents an extract-based, simple temporal summary of the events in which the concept has been involved.
</nextsent>
<nextsent>starting from this observation, task-based evaluation of the coverage of the extracted text snippets consists in verifying to what extent they capture the condensed history of several countries.
</nextsent>
<nextsent>since any country must have been involved in some historical timeline of events,a reference timeline is readily available in an exter 412 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 am bi bu ru nd co or os jib ou ti er itr ea et hi op ia ke ny r an da se yc helle so al ia ta nz an ia ug an da ca er oo eq ua to ria g ui ne g ab on al ge ria ce ut eg yp li by m el illa or oc co su da tu ni si w es te rn ah ar co un t/p er ce nt ag total reference snippets (count) matched reference snippets (percentage) figure 1: percentage of reference snippets with corresponding extracted snippets nal resource, e.g., encyclopedia, as an excerpt covering condensed history of the country.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1332">
<title id=" I08-1054.xml">answering definition questions via temporally anchored text snippets </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>mark poet and egyptiangod?
</prevsent>
<prevsent>as correct answers respectively, thus emphasizing the genus of the question concepts anubis and william wordsworth.
</prevsent>
</prevsection>
<citsent citstr=" C02-1026 ">
this explains the strong reliance in previous work on hand-written patterns and dictionary-based techniques for detecting text fragments encoding the genus and differentia of the question concept (lin, 2002; <papid> C02-1026 </papid>xu et al, 2004).</citsent>
<aftsection>
<nextsent>414 question (rank) relevant date: associated fact q218: who was (1) 1893: first patented in 1893 by whitcomb judson, the clasp locker was notoriously unreliable whitcomb judson?
</nextsent>
<nextsent>and expensive (2) 1891: the zipper was invented by whitcomb judson q239: who is (1) february 21 1936: barbara jordan was born in houston, texas barbara jordan?
</nextsent>
<nextsent>(2) january 17 1996: barbara jordan died in austin, texas, at the age of 59 (4) 1973: barbara jordan was diagnosed with multiple sclerosis and was confined to wheelchair (5) 1976: barbara jordan became the first african-american woman to deliver keynote address at political convention (7) 1966: barbara jordan became the first black representative since 1883 to win an election to the texas legislature (8) 1972: barbara jordan was elected to the us congress q253: who is (1) 1770: william wordsworth was born in 1770 in the town of cocker mouth, england william wordsworth?
</nextsent>
<nextsent>(2) april 7 1770: william wordsworth was born (4) 1798: romanticism officially began, when william wordsworth and samuel taylor coleridge anonymously published lyrical ballads (5) 1802: william wordsworth married mary hutchinson at brompton church (7) 1795: coleridge met the poet william wordsworth (8) april 23 1850: william wordsworth died (11) 1843: william wordsworth (1770-1850) was made poet laureate of britain q346: who is (1) 1902: langston hughes was born in joplin, missouri langston hughes?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1334">
<title id=" I08-1054.xml">answering definition questions via temporally anchored text snippets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous approaches to answering definition questions from large text collections can be classified according to the kind of techniques for the extraction of answers.
</prevsent>
<prevsent>a significant body of work is oriented towards mining descriptive phrases or sentences, as opposed to other types of semantic information, for the given question concepts.
</prevsent>
</prevsection>
<citsent citstr=" C04-1093 ">
to this effect, the use of hand-written lexico-syntactic patterns and regular expressions, targeting the genus and possibly the differentia of the question concept,is widespread, whether employed for mining definitions in english (liu et al, 2003; hildebrandt et al, 2004) <papid> N04-1007 </papid>or other languages such as japanese (fujii and ishikawa, 2004), <papid> C04-1093 </papid>from local text collections (xu et al., 2004) or from the web (blair-goldensohn et al,2004; androutsopoulos and galanis, 2005).<papid> H05-1041 </papid></citsent>
<aftsection>
<nextsent>comparatively, the small set of patterns used here targets text snippets that are temporally-anchored.
</nextsent>
<nextsent>therefore the text snippets provide answers to definition answers without actually employing any specialized module for seeking specific information such as the genus of the question concept.
</nextsent>
<nextsent>several studies propose unsupervised extraction methods as an alternative to using hand-written patterns for definition questions (androutsopoulos and galanis, 2005; <papid> H05-1041 </papid>cui et al, 2007).</nextsent>
<nextsent>previous work often relies on external resources as an important or even essential guide towards the desired out put.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1335">
<title id=" I08-1054.xml">answering definition questions via temporally anchored text snippets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous approaches to answering definition questions from large text collections can be classified according to the kind of techniques for the extraction of answers.
</prevsent>
<prevsent>a significant body of work is oriented towards mining descriptive phrases or sentences, as opposed to other types of semantic information, for the given question concepts.
</prevsent>
</prevsection>
<citsent citstr=" H05-1041 ">
to this effect, the use of hand-written lexico-syntactic patterns and regular expressions, targeting the genus and possibly the differentia of the question concept,is widespread, whether employed for mining definitions in english (liu et al, 2003; hildebrandt et al, 2004) <papid> N04-1007 </papid>or other languages such as japanese (fujii and ishikawa, 2004), <papid> C04-1093 </papid>from local text collections (xu et al., 2004) or from the web (blair-goldensohn et al,2004; androutsopoulos and galanis, 2005).<papid> H05-1041 </papid></citsent>
<aftsection>
<nextsent>comparatively, the small set of patterns used here targets text snippets that are temporally-anchored.
</nextsent>
<nextsent>therefore the text snippets provide answers to definition answers without actually employing any specialized module for seeking specific information such as the genus of the question concept.
</nextsent>
<nextsent>several studies propose unsupervised extraction methods as an alternative to using hand-written patterns for definition questions (androutsopoulos and galanis, 2005; <papid> H05-1041 </papid>cui et al, 2007).</nextsent>
<nextsent>previous work often relies on external resources as an important or even essential guide towards the desired out put.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1337">
<title id=" I08-1054.xml">answering definition questions via temporally anchored text snippets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several studies propose unsupervised extraction methods as an alternative to using hand-written patterns for definition questions (androutsopoulos and galanis, 2005; <papid> H05-1041 </papid>cui et al, 2007).</prevsent>
<prevsent>previous work often relies on external resources as an important or even essential guide towards the desired out put.</prevsent>
</prevsection>
<citsent citstr=" H01-1006 ">
such resources include wordnet (prager etal., 2001) <papid> H01-1006 </papid>for finding the genus of the question con cept; large dictionaries such as merriam webster,for ready-to-use definitions (xu et al, 2004; hildebrandt et al, 2004); <papid> N04-1007 </papid>and encyclopedias, for collecting words that are likely to occur in potential definitions (fujii and ishikawa, 2004; <papid> C04-1093 </papid>xu et al, 2004).</citsent>
<aftsection>
<nextsent>in comparison, the experiments reported in this paper do not require any external lexical resource.
</nextsent>
<nextsent>416
</nextsent>
<nextsent>without specifically targeting definitions, temporally-anchored text snippets extracted from the web provide very useful answers to definition questions, as measured on standard test question sets.
</nextsent>
<nextsent>since the snippets tend to capture important events involving the question concepts, rather than phrases that describe the question concept, they can be employed as either standalone answers, or supplemental results in conjunction with answers extracted with other techniques.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1340">
<title id=" I05-5012.xml">towards statistical paraphrase generation preliminary evaluations of grammaticality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such mechanism can usefully be considered as automatically generating para phrase.2 we treat the problem as one in which new and previously unseen summary sentence is to be automatically produced given some closely related sentences extracted from source text.
</prevsent>
<prevsent>following on from (witbrock and mittal, 1999), we use and extend the viterbi algorithm (forney, 1973) for the purposes of generating non-verbatim sentences.
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
this approach treats 1these are available publically at http://www.reliefweb.com.2paraphrase here includes sentences generated in an information fusion task (barzilay et al, 1999).<papid> P99-1071 </papid></citsent>
<aftsection>
<nextsent>88 sentence generation as search problem.
</nextsent>
<nextsent>given set of words (taken from some set of sentence sto paraphrase), we search for the most likely sequence given some language model.
</nextsent>
<nextsent>intuitively, we want the generated string to be grammatical and to accurately reflect the content of the source text.
</nextsent>
<nextsent>within the viterbi search process, each time we append word to the partially generated sentence, we consider how well it attaches to dependency structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1341">
<title id=" I05-5012.xml">towards statistical paraphrase generation preliminary evaluations of grammaticality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>within the viterbi search process, each time we append word to the partially generated sentence, we consider how well it attaches to dependency structure.
</prevsent>
<prevsent>the focus of this paper is to evaluate whether or not series of iterative considerations of dependency structure results in grammatical generated sentence.
</prevsent>
</prevsection>
<citsent citstr=" W05-1628 ">
previous preliminary evaluations (wan et al, 2005) <papid> W05-1628 </papid>indicate that the generated sequences contain less fragmented text as measured by an off-the-shelf dependency parser; more fragments would indicate grammatically problematic sentence.however, while encouraging, such an evaluation says little about what the actual sentence looks like.</citsent>
<aftsection>
<nextsent>for example, such generated text might only be useful if it contains complete clauses.
</nextsent>
<nextsent>thus, in this paper, we use the precision and recall metric to measure how many generated verb arguments, as extracted from dependency relations, are correct.the remainder of this paper is structured as follows.
</nextsent>
<nextsent>section 2 provides an overview introducing our approach.
</nextsent>
<nextsent>in section 3, we briefly illustrate our algorithm with examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1343">
<title id=" I05-5012.xml">towards statistical paraphrase generation preliminary evaluations of grammaticality </title>
<section> an overview of our approach to.  </section>
<citcontext>
<prevsection>
<prevsent>mechanism is linear3 and is embedded within the viterbi algorithm, the result is an o(   ) algorithm.
</prevsent>
<prevsent>by examining surface-syntactic dependency structure at each step in the search, resulting sentences are likely to be more grammatical.
</prevsent>
</prevsection>
<citsent citstr=" P98-1035 ">
this marraige of models has been tested in other fields such as speech recognition (chelba and jelinek,1998) <papid> P98-1035 </papid>with success.</citsent>
<aftsection>
<nextsent>although it is an impoverished representation of semantics, considering dependency features in our application context mayalso serendipitously assist verisimilitude generation.
</nextsent>
<nextsent>propagating dependency structure in this section, we present an overview of themain features of our algorithm extension.
</nextsent>
<nextsent>we direct the interested reader to our technical paper (wan et al, 2005) <papid> W05-1628 </papid>for full details.</nextsent>
<nextsent>the viterbi algorithm (for comprehensive overview, see (manning and schutze, 1999)) is used to search for the best path across network of nodes, where each node represents word in the vocabulary.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1347">
<title id=" I05-5012.xml">towards statistical paraphrase generation preliminary evaluations of grammaticality </title>
<section> the extended viterbi algorithm:.  </section>
<citcontext>
<prevsection>
<prevsent>to represent the rightmost branch, we use stack data structure (referred to as the head stack) whereby older stack items correspond to nodes closer to the root of the dependency tree.the probability of the dependency-based transition is estimated as follows:   , -. 0 (fl 0 ml   9$ndo &amp;  ( ) p; qflr 7  1s   tvu:w ixi % ,:y  %5zd{5|~} * ; 9 ndo &amp;?
</prevsent>
<prevsent>0 (ff) p/ where =e\*afl=??:???\ ????
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
kflk is inspired by and closely resembles the probabilistic functions in (collins, 1996).<papid> P96-1025 </papid></citsent>
<aftsection>
<nextsent>after selecting and appending new word, we update this representation containing the governing words of the extended string that can yet be modified.
</nextsent>
<nextsent>the new path is then annotated with this updated stack.
</nextsent>
<nextsent>3.2 maintaining the head stack.
</nextsent>
<nextsent>there are three possible alternative outcomes to the head stack update mechanism.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1349">
<title id=" I05-5012.xml">towards statistical paraphrase generation preliminary evaluations of grammaticality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to prevent crossing dependencies, foodis popped off the stack before pushing to.
</prevsent>
<prevsent>appending the rest of the words is straightforward.
</prevsent>
</prevsection>
<citsent citstr=" W98-1426 ">
in recent years, there has been steady stream of research in statistical text generation (see langkilde and knight (1998), <papid> W98-1426 </papid>and bangalore and rambow (2000)).<papid> C00-1007 </papid></citsent>
<aftsection>
<nextsent>these approaches begin with representation of sentence semantics that has been produced by content planning stage.
</nextsent>
<nextsent>competing realisations of the semantic representation are ranked using an n-gram model.
</nextsent>
<nextsent>our approach differs in that we do not start with semantic representation.
</nextsent>
<nextsent>rather, we paraphrase the original text,searching for the best word sequence and dependency tree structure concurrently.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1350">
<title id=" I05-5012.xml">towards statistical paraphrase generation preliminary evaluations of grammaticality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to prevent crossing dependencies, foodis popped off the stack before pushing to.
</prevsent>
<prevsent>appending the rest of the words is straightforward.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
in recent years, there has been steady stream of research in statistical text generation (see langkilde and knight (1998), <papid> W98-1426 </papid>and bangalore and rambow (2000)).<papid> C00-1007 </papid></citsent>
<aftsection>
<nextsent>these approaches begin with representation of sentence semantics that has been produced by content planning stage.
</nextsent>
<nextsent>competing realisations of the semantic representation are ranked using an n-gram model.
</nextsent>
<nextsent>our approach differs in that we do not start with semantic representation.
</nextsent>
<nextsent>rather, we paraphrase the original text,searching for the best word sequence and dependency tree structure concurrently.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1358">
<title id=" I08-1008.xml">name origin recognition using maximum entropy model and diverse features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiments on publicly available personal name database show that the proposed approach achieves an overall accuracy of 98.44% for names written in english and 98.10% for names written in chinese, which are significantly and consistently better than those in reported work.
</prevsent>
<prevsent>many technical terms and proper names, such as personal, location and organization names, are translated from one language into another with approximate phonetic equivalents.
</prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
the phonetic translation practice is referred to as transliteration; conversely, the process of recovering word in its native language from transliteration is called as back-transliteration (zhang et al 2004; knight and graehl, 1998).<papid> J98-4003 </papid></citsent>
<aftsection>
<nextsent>for example, english name smith?
</nextsent>
<nextsent>and ????
</nextsent>
<nextsent>(pinyin 1 : shi-mi-si)?
</nextsent>
<nextsent>in 1 hanyu pinyin, or pinyin in short, is the standard romaniza-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1359">
<title id=" I08-1008.xml">name origin recognition using maximum entropy model and diverse features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>following the japanese phonetic rules.
</prevsent>
<prevsent>the name origins are equally important in back-transliteration practice.
</prevsent>
</prevsection>
<citsent citstr=" P07-1016 ">
li et al (2007) <papid> P07-1016 </papid>incorporated name origin recognition to improve the performance of personal name transliteration.</citsent>
<aftsection>
<nextsent>besides multilingual processing, the name origin also provides useful semantic information (regional and language information) for common nlp tasks, such as co-reference resolution and name entity recognition.
</nextsent>
<nextsent>unfortunately, little attention has been given to name origin recognition (nor) so far in the literature.
</nextsent>
<nextsent>in this paper, we are interested in two kinds of name origin recognition: the origin of names written in english (enor) and the origin of names written in chinese (cnor).
</nextsent>
<nextsent>for enor, the origins include english (eng), japanese (jap), chinese mandarin pinyin (man) and chinese cantonese jyutping (can).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1360">
<title id=" I08-1008.xml">name origin recognition using maximum entropy model and diverse features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for enor, the origins include english (eng), japanese (jap), chinese mandarin pinyin (man) and chinese cantonese jyutping (can).
</prevsent>
<prevsent>for cnor, they include three origins: chinese (chi, for both mandarin and cantonese), japanese and english (refer to latin scripted language).
</prevsent>
</prevsection>
<citsent citstr=" P04-1024 ">
unlike previous work (qu and grefenstette, 2004; <papid> P04-1024 </papid>li et al, 2006; li et al, 2007) <papid> P07-1016 </papid>where nor was formulated with generative model, we regard the nor task as classification problem.</citsent>
<aftsection>
<nextsent>we further propose using discriminative learning algorithm (maximum entropy model: maxent) to solve the problem.
</nextsent>
<nextsent>to draw direct comparison, we conduct experiments on the same personal name corpora as that in the previous work by li et al (2006).
</nextsent>
<nextsent>we show that the maxent method effectively incorporates diverse features and outperforms previous methods consistently across all test cases.
</nextsent>
<nextsent>the rest of the paper is organized as follows: in section 2, we review the previous work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1365">
<title id=" I08-1008.xml">name origin recognition using maximum entropy model and diverse features </title>
<section> maxent model and features.  </section>
<citcontext>
<prevsection>
<prevsent>experimental results show that the maxent model effectively incorporates diverse features to demonstrate competitive performance.
</prevsent>
<prevsent>3.1 maxent model for nor.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
the principle of maximum entropy (maxent) model is that given collection of facts, choose model consistent with all the facts, but otherwise as uniform as possible (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>maxent model is known to easily combine diverse features.
</nextsent>
<nextsent>for this reason, it has been widely adopted in many natural language processing tasks.
</nextsent>
<nextsent>the maxent model is defined as: ( , ) 1 1 ( | ) i f x j p x ? = = ?
</nextsent>
<nextsent>(3) ( , ) 1 1 1 ( | ) i kn f x j i z c ? = = = = =?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1373">
<title id=" I05-3002.xml">using word pair identifier to improve chinese input system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper presents word-pair (wp) identifier that can be used to resolve homonym/segmentation ambiguities and perform syllable-to-word (stw) conversion effectively for improving chinese input systems.
</prevsent>
<prevsent>the experiment results show the following: (1) the wp identifier is able to achieve tonal (syllables with four tones) and tone less (syllables without four tones) stw accuracies of 98.5% and 90.7%, respectively, among the identified word-pairs; (2) while applying the wp identifier, together with the microsoft input method editor 2003 and an optimized bigram model, the tonal and tone less stw improvements of the two input systems are 27.5%/18.9% and 22.1%/18.8%, respectively.
</prevsent>
</prevsection>
<citsent citstr=" C02-1089 ">
more than 100 chinese input methods have been developed in the past (becker 1985, huang 1985, gu et al  1991, chung 1993, kuo 1995, fu et al  1996, lee et al  1997, hsu et al  1999, chen et al . 2000, tsai and hsu 2002, <papid> C02-1089 </papid>gao et al  2002, lee 2003).</citsent>
<aftsection>
<nextsent>their underlying approaches can be classified into four types: (1) optical character recognition (ocr) based (chung 1993), (2) on line handwriting based (lee et al  1997), (3) speech based (fu et al  1996, chen et al  2000), and (4) keyboard based consists of phonetic and pinyin based (chang et al  1991, hsu et al  1993, hsu 1994, hsu et al  1999, kuo 1995, lua and gan 1992); arbitrary codes based [fan et al  1988]; and structure scheme based (huang 1985).
</nextsent>
<nextsent>currently, the most popular method for chinese input is phonetic and pinyin based, because chinese people are taught to write the corresponding phonetic and pinyin syllables of each chinese character and word in primary school.
</nextsent>
<nextsent>in chinese, each chinese character corresponds to at least one syllable; and each chinese word can be mono-syllabic word, such as ??
</nextsent>
<nextsent>(mouse)?, bi-syllabic word, such as ???
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1386">
<title id=" I08-1047.xml">minimally supervised learning of semantic knowledge from query logs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>extraction of lexical knowledge from large collection of text data with minimal supervision has become an active area of research in recent years.
</prevsent>
<prevsent>automatic extraction of relations by exploiting recurring patterns in text was pioneered by hearst (1992), who describes bootstrapping procedure for extracting words in the hyponym (is-a) relation, starting with three manually given lexico-syntactic patterns.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
this idea of learning with minimally supervised bootstrapping method using surface text patterns was subsequently adopted for many tasks, including relation extraction (e.g., brin, 1998; ri loff and jones, 1999; pantel and pennacchiotti, 2006) and named entity recognition (e.g., collins and singer, 1999; <papid> W99-0613 </papid>etzioni et al, 2005).</citsent>
<aftsection>
<nextsent>in this paper, we describe method of learning semantic categories of words using large collection of japanese search query logs.
</nextsent>
<nextsent>our method is based on the espresso algorithm (pantel and pennacchiotti, 2006) for extracting binary lexical relations, adapting it to work well on learning unary relations from query logs.
</nextsent>
<nextsent>the use of query data as source of knowledge extraction offers some unique advantages over using regular text.
</nextsent>
<nextsent>web search queries capture the interest of search users directly, while the distribution of the web documents do not necessarily reflect the distribution of what people search (silverstein et al, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1387">
<title id=" I08-1015.xml">lexical chains as document features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is based on the phenomenon of lexical cohesion (halliday and hasan, 1976) and works on the premise that semantically related words co-occur close together in passage more than just by chance?.
</prevsent>
<prevsent>lexical chaining is the process of identifying and grouping such words together to form chains which in turn will help in identifying and representing the topic and content of the document.
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
lexical chains have been used as an intermediate representation of text for various tasks such as au 111 tomatic text summarisation (barzilay and elhadad,1997; <papid> W97-0703 </papid>silber and mccoy, 2002), <papid> J02-4004 </papid>malapropism detection and correction (hirst and st-onge, 1997),and hypertext construction (green, 1998).<papid> W98-1213 </papid></citsent>
<aftsection>
<nextsent>an algorithm for computing lexical chains was first givenby (morris and hirst, 1991) <papid> J91-1002 </papid>using the rogets thesaurus (kirkpatrick, 1998).</nextsent>
<nextsent>since an electronic version of the rogets thesaurus was not available then, later algorithms were based on the wordnet lexical database (fellbaum, 1998).we present here two pass algorithm to compute representation of documents using lexical chains and use these lexical chains to derive feature vectors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1388">
<title id=" I08-1015.xml">lexical chains as document features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is based on the phenomenon of lexical cohesion (halliday and hasan, 1976) and works on the premise that semantically related words co-occur close together in passage more than just by chance?.
</prevsent>
<prevsent>lexical chaining is the process of identifying and grouping such words together to form chains which in turn will help in identifying and representing the topic and content of the document.
</prevsent>
</prevsection>
<citsent citstr=" J02-4004 ">
lexical chains have been used as an intermediate representation of text for various tasks such as au 111 tomatic text summarisation (barzilay and elhadad,1997; <papid> W97-0703 </papid>silber and mccoy, 2002), <papid> J02-4004 </papid>malapropism detection and correction (hirst and st-onge, 1997),and hypertext construction (green, 1998).<papid> W98-1213 </papid></citsent>
<aftsection>
<nextsent>an algorithm for computing lexical chains was first givenby (morris and hirst, 1991) <papid> J91-1002 </papid>using the rogets thesaurus (kirkpatrick, 1998).</nextsent>
<nextsent>since an electronic version of the rogets thesaurus was not available then, later algorithms were based on the wordnet lexical database (fellbaum, 1998).we present here two pass algorithm to compute representation of documents using lexical chains and use these lexical chains to derive feature vectors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1389">
<title id=" I08-1015.xml">lexical chains as document features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is based on the phenomenon of lexical cohesion (halliday and hasan, 1976) and works on the premise that semantically related words co-occur close together in passage more than just by chance?.
</prevsent>
<prevsent>lexical chaining is the process of identifying and grouping such words together to form chains which in turn will help in identifying and representing the topic and content of the document.
</prevsent>
</prevsection>
<citsent citstr=" W98-1213 ">
lexical chains have been used as an intermediate representation of text for various tasks such as au 111 tomatic text summarisation (barzilay and elhadad,1997; <papid> W97-0703 </papid>silber and mccoy, 2002), <papid> J02-4004 </papid>malapropism detection and correction (hirst and st-onge, 1997),and hypertext construction (green, 1998).<papid> W98-1213 </papid></citsent>
<aftsection>
<nextsent>an algorithm for computing lexical chains was first givenby (morris and hirst, 1991) <papid> J91-1002 </papid>using the rogets thesaurus (kirkpatrick, 1998).</nextsent>
<nextsent>since an electronic version of the rogets thesaurus was not available then, later algorithms were based on the wordnet lexical database (fellbaum, 1998).we present here two pass algorithm to compute representation of documents using lexical chains and use these lexical chains to derive feature vectors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1390">
<title id=" I08-1015.xml">lexical chains as document features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexical chaining is the process of identifying and grouping such words together to form chains which in turn will help in identifying and representing the topic and content of the document.
</prevsent>
<prevsent>lexical chains have been used as an intermediate representation of text for various tasks such as au 111 tomatic text summarisation (barzilay and elhadad,1997; <papid> W97-0703 </papid>silber and mccoy, 2002), <papid> J02-4004 </papid>malapropism detection and correction (hirst and st-onge, 1997),and hypertext construction (green, 1998).<papid> W98-1213 </papid></prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
an algorithm for computing lexical chains was first givenby (morris and hirst, 1991) <papid> J91-1002 </papid>using the rogets thesaurus (kirkpatrick, 1998).</citsent>
<aftsection>
<nextsent>since an electronic version of the rogets thesaurus was not available then, later algorithms were based on the wordnet lexical database (fellbaum, 1998).we present here two pass algorithm to compute representation of documents using lexical chains and use these lexical chains to derive feature vectors.
</nextsent>
<nextsent>these lexical chain based feature vectors are used to cluster the documents using two different algorithms - k-means and co-clustering.
</nextsent>
<nextsent>k means is well studied clustering algorithm widely used in the text domain.
</nextsent>
<nextsent>co-clustering, also known as bi-clustering (madeira and oliveira, 2004), is clustering approach which was developed in thebioinformatics domain for clustering gene expressions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1393">
<title id=" I08-1034.xml">the telling tail signals of success in electronic negotiation texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>no numerical data, however, supported this diagnosis, and there was no distinction between the prediction of successful and unsuccessful outcomes.
</prevsent>
<prevsent>when it comes to text classification, our hypothesis says that the classification of the second parts of e-negotiation texts is more accurate with respect to the outcome than the classification of the first parts.
</prevsent>
</prevsection>
<citsent citstr=" P04-3034 ">
this makes e-negotiation texts different from news group messages, newspaper articles and other documents classified by blatak et al (2004), <papid> P04-3034 </papid>where texts showe better classification accuracy on their initial parts.</citsent>
<aftsection>
<nextsent>we report the results of several sets of machine learning (ml) experiments.
</nextsent>
<nextsent>performed on varying-size text data segments, they support our hypothesis.
</nextsent>
<nextsent>we worked with collection of transcripts of negotiations conducted over the internet using theweb-based negotiation support system inspire (ker sten and zhang, 2003).
</nextsent>
<nextsent>kersten and zhang (2003)and nastase (2006) classified e-negotiation outcomes using non-textual data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1394">
<title id=" I08-1034.xml">the telling tail signals of success in electronic negotiation texts </title>
<section> background review.  </section>
<citcontext>
<prevsection>
<prevsent>this profile proposes that peoples language patterns are indicators of behavioural preferences.
</prevsent>
<prevsent>in the study of planning dialogues (chu carroll and carberry, 2000), searles theory of speech acts used through the discourse analysis also supports the fact that language carries much of peoples behaviour and emotions.
</prevsent>
</prevsection>
<citsent citstr=" P07-1102 ">
reitter and moore(2007) <papid> P07-1102 </papid>studied repetitions in task-oriented conversations.</citsent>
<aftsection>
<nextsent>they demonstrated that speakers short term ability to copy the interlocutors syntax is autonomous from the success of the task, whereas long-term adaptation varies with such success.
</nextsent>
<nextsent>we consider negotiation to be communication in which the participants want to reach an agreement relative to the splitting/sharing of resources.
</nextsent>
<nextsent>language is one of the tools used to reach the goal.
</nextsent>
<nextsent>we propose that not all messages exchanged throughout negotiation have the same effect on the negotiation outcome.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1395">
<title id=" I08-1034.xml">the telling tail signals of success in electronic negotiation texts </title>
<section> background review.  </section>
<citcontext>
<prevsection>
<prevsent>unexpected turns and moves ? typical of human behaviour ? make prediction of the negotiation outcome difficult.
</prevsent>
<prevsent>in case of electronic negotiation, the absence of the usual negotiation structure further complicates the outcome prediction.
</prevsent>
</prevsection>
<citsent citstr=" D07-1048 ">
this distinguishes e-negotiations from agent customer phone conversations studied in (takeuchi et al 2007), <papid> D07-1048 </papid>where an agent follows the call flow pre-defined by his companys policy.</citsent>
<aftsection>
<nextsent>258the longer an e-negotiation takes, the more elaborate the structure of the e-negotiation process becomes.
</nextsent>
<nextsent>simpler e-negotiation may involve an exchange of well-structured business documents such as pre-defined contract or retail transactions.
</nextsent>
<nextsent>a more complex process comprises numerous offer sand counter-offers and has high degree of uncertainty because of the possible unpredictability of negotiation moves.the next challenge stems from the limitations imposed by the use of electronic means.
</nextsent>
<nextsent>this overloads text messages with various tasks: negotiation issues themselves, introductions and closures traditional in negotiations, and even socializing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1397">
<title id=" I05-6008.xml">linguistically enriched corpora for establishing variation in support verb constructions </title>
<section> a corpus-based method to infer </section>
<citcontext>
<prevsection>
<prevsent>acorpus-based method applied on phrasal chunks was impractical.
</prevsent>
<prevsent>a lot of noise needed to be manually discarded.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
67 search, but starting point would be the approach by (briscoe and carroll, 1997).<papid> A97-1052 </papid></citsent>
<aftsection>
<nextsent>the success of the search queries is dependent on parsing accuracy.
</nextsent>
<nextsent>sometimes extracted evidence shows the specific pp we seek but misan alyzed as dependent of another verb.
</nextsent>
<nextsent>parsing accuracy introduces another shortcoming: evidence of relative clauses and pp post-nominal modifiers cannot be automatically retrieved.
</nextsent>
<nextsent>because of structural ambiguity, attachment decisions are still hard parsing problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1398">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our proposed approach achieves an unlabeled attachment score of 86.52, an absolute 1.24% improvement over the baseline system on the dataset of chinese treebank.
</prevsent>
<prevsent>in dependency parsing, we attempt to build the dependency links between words from sentence.
</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
given sufficient labeled data, there are several supervised learning methods for training high performance dependency parsers(nivre et al, 2007).<papid> D07-1096 </papid>however, current statistical dependency parsers provide worse results if the dependency length becomes longer (mcdonald and nivre, 2007).<papid> D07-1013 </papid></citsent>
<aftsection>
<nextsent>here the length of dependency from word i and word j is simply equal to |i ? j|.
</nextsent>
<nextsent>figure 1 shows the 1score1 provided by deterministic parser relative to dependency length on our testing data.
</nextsent>
<nextsent>from 1precision represents the percentage of predicted arcs of length that are correct and recall measures the percentage of gold standard arcs of length that are correctly predicted.
</nextsent>
<nextsent>f 1 = 2?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1399">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our proposed approach achieves an unlabeled attachment score of 86.52, an absolute 1.24% improvement over the baseline system on the dataset of chinese treebank.
</prevsent>
<prevsent>in dependency parsing, we attempt to build the dependency links between words from sentence.
</prevsent>
</prevsection>
<citsent citstr=" D07-1013 ">
given sufficient labeled data, there are several supervised learning methods for training high performance dependency parsers(nivre et al, 2007).<papid> D07-1096 </papid>however, current statistical dependency parsers provide worse results if the dependency length becomes longer (mcdonald and nivre, 2007).<papid> D07-1013 </papid></citsent>
<aftsection>
<nextsent>here the length of dependency from word i and word j is simply equal to |i ? j|.
</nextsent>
<nextsent>figure 1 shows the 1score1 provided by deterministic parser relative to dependency length on our testing data.
</nextsent>
<nextsent>from 1precision represents the percentage of predicted arcs of length that are correct and recall measures the percentage of gold standard arcs of length that are correctly predicted.
</nextsent>
<nextsent>f 1 = 2?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1401">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> motivation and previous work.  </section>
<citcontext>
<prevsection>
<prevsent>there are several other studies relevant to ours as described below.a simple method is self-training in which the existing model first labels unlabeled data and then the newly labeled data is then treated as hand annotated data for training new model.
</prevsent>
<prevsent>but it seems that self training is not so effective.
</prevsent>
</prevsection>
<citsent citstr=" E03-1008 ">
(steedman et al, 2003) <papid> E03-1008 </papid>reports minor improvement by using self-trainingfor syntactic parsing on small labeled data.</citsent>
<aftsection>
<nextsent>the reason may be that errors in the original model would be amplified in the new model.
</nextsent>
<nextsent>(mcclosky et al, 2006) <papid> P06-1043 </papid>presents successful instance of parsing withself-training by using re-ranker.</nextsent>
<nextsent>as figure 1 suggests, the dependency parser performs bad for parsing the words with long distances.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1402">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> motivation and previous work.  </section>
<citcontext>
<prevsection>
<prevsent>(steedman et al, 2003) <papid> E03-1008 </papid>reports minor improvement by using self-trainingfor syntactic parsing on small labeled data.</prevsent>
<prevsent>the reason may be that errors in the original model would be amplified in the new model.</prevsent>
</prevsection>
<citsent citstr=" P06-1043 ">
(mcclosky et al, 2006) <papid> P06-1043 </papid>presents successful instance of parsing withself-training by using re-ranker.</citsent>
<aftsection>
<nextsent>as figure 1 suggests, the dependency parser performs bad for parsing the words with long distances.
</nextsent>
<nextsent>in our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser.
</nextsent>
<nextsent>(smith and eisner, 2006) <papid> P06-1072 </papid>presents an approach to improve the accuracy of dependency grammar induction models by em from unlabeled data.</nextsent>
<nextsent>they obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string.the study most relevant to ours is done by (kawahara and kurohashi, 2006).<papid> N06-1023 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1403">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> motivation and previous work.  </section>
<citcontext>
<prevsection>
<prevsent>as figure 1 suggests, the dependency parser performs bad for parsing the words with long distances.
</prevsent>
<prevsent>in our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser.
</prevsent>
</prevsection>
<citsent citstr=" P06-1072 ">
(smith and eisner, 2006) <papid> P06-1072 </papid>presents an approach to improve the accuracy of dependency grammar induction models by em from unlabeled data.</citsent>
<aftsection>
<nextsent>they obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string.the study most relevant to ours is done by (kawahara and kurohashi, 2006).<papid> N06-1023 </papid></nextsent>
<nextsent>they present an integrated probabilistic model for japanese parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1404">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> motivation and previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser.
</prevsent>
<prevsent>(smith and eisner, 2006) <papid> P06-1072 </papid>presents an approach to improve the accuracy of dependency grammar induction models by em from unlabeled data.</prevsent>
</prevsection>
<citsent citstr=" N06-1023 ">
they obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string.the study most relevant to ours is done by (kawahara and kurohashi, 2006).<papid> N06-1023 </papid></citsent>
<aftsection>
<nextsent>they present an integrated probabilistic model for japanese parsing.
</nextsent>
<nextsent>they also use partial information after current parser parses the sentences.
</nextsent>
<nextsent>our work differs in that we consider general dependency relations while they only consider case frames.
</nextsent>
<nextsent>and we represent additional information as the features for learning models while they use the case frames as one component for probabilistic model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1405">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 training basic parser.
</prevsent>
<prevsent>in this paper, we implement deterministic parser based on the model described by (nivre, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W06-2933 ">
this model is simple and works very well in the shared-tasks of conll2006(nivre et al, 2006) <papid> W06-2933 </papid>and conll2007(hall et al, 2007).<papid> D07-1097 </papid></citsent>
<aftsection>
<nextsent>in fact, our approach can also be applied to other parsers, such as (ya mada and matsumoto, 2003)s parser, (mcdonald et al., 2006)<papid> W06-2932 </papid>s parser, and so on.</nextsent>
<nextsent>3.1.1 the parser the parser predicts unlabeled directed dependencies between words in sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1406">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 training basic parser.
</prevsent>
<prevsent>in this paper, we implement deterministic parser based on the model described by (nivre, 2003).
</prevsent>
</prevsection>
<citsent citstr=" D07-1097 ">
this model is simple and works very well in the shared-tasks of conll2006(nivre et al, 2006) <papid> W06-2933 </papid>and conll2007(hall et al, 2007).<papid> D07-1097 </papid></citsent>
<aftsection>
<nextsent>in fact, our approach can also be applied to other parsers, such as (ya mada and matsumoto, 2003)s parser, (mcdonald et al., 2006)<papid> W06-2932 </papid>s parser, and so on.</nextsent>
<nextsent>3.1.1 the parser the parser predicts unlabeled directed dependencies between words in sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1407">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we implement deterministic parser based on the model described by (nivre, 2003).
</prevsent>
<prevsent>this model is simple and works very well in the shared-tasks of conll2006(nivre et al, 2006) <papid> W06-2933 </papid>and conll2007(hall et al, 2007).<papid> D07-1097 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-2932 ">
in fact, our approach can also be applied to other parsers, such as (ya mada and matsumoto, 2003)s parser, (mcdonald et al., 2006)<papid> W06-2932 </papid>s parser, and so on.</citsent>
<aftsection>
<nextsent>3.1.1 the parser the parser predicts unlabeled directed dependencies between words in sentences.
</nextsent>
<nextsent>the algorithm (nivre, 2003) makes dependency parsing tree in one left-to-right pass over the input, and uses stack to store the processed tokens.
</nextsent>
<nextsent>the behaviors of the parser are defined by four elementary actions (where top is the token on top of the stack and next is the next token in the original input string): ? left-arc(la): add an arc from next to top; pop the stack.
</nextsent>
<nextsent>right-arc(ra): add an arc from top to next; push next onto the stack.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1408">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>refers to the parser 2more detailed information can be found at http://www.cis.upenn.edu/chinese/.
</prevsent>
<prevsent>3more detailed information can be found at http://www.icl.pku.edu.
</prevsent>
</prevsection>
<citsent citstr=" P06-1054 ">
4to know whether our pos tagger is good, we also tested the tnt package on the standard training and testing sets forfull parsing (wang et al, 2006).<papid> P06-1054 </papid></citsent>
<aftsection>
<nextsent>the tnt-based tagger provided 91.52% accuracy, the comparative result with (wang et al., 2006).<papid> P06-1054 </papid></nextsent>
<nextsent>with basic features and d1 features, and ?+d2?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1410">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>when we used 1% sentences of unlabeled data, the parser achieved large improvement.
</prevsent>
<prevsent>as we added more sentences, the parser obtained more benefit.table 2: the results with different numbers of unlabeled sentences sentences uas without uas with 0%(baseline) 85.28 83.79 1% 85.68 84.40 2% 85.69 84.51 5% 85.78 84.59 10% 85.97 84.62 20% 86.25 84.86 50% 86.34 84.92 100%(ours) 86.52 85.16 4.1.2 comparison of other systems finally, we compare our parser to the state of the art.
</prevsent>
</prevsection>
<citsent citstr=" W05-1516 ">
we used the same testing data as (wang et al, 2005) <papid> W05-1516 </papid>did, selecting the sentences length up to 40.</citsent>
<aftsection>
<nextsent>table 3 shows the results achieved by our method and other researchers (uas with p), where wang05 refers to (wang et al, 2005), <papid> W05-1516 </papid>wang07 refers 92 to (wang et al, 2007), and mcdonald&pereira065; refers to (mcdonald and pereira, 2006).<papid> E06-1011 </papid></nextsent>
<nextsent>from the table, we found that our parser performed best.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1412">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>as we added more sentences, the parser obtained more benefit.table 2: the results with different numbers of unlabeled sentences sentences uas without uas with 0%(baseline) 85.28 83.79 1% 85.68 84.40 2% 85.69 84.51 5% 85.78 84.59 10% 85.97 84.62 20% 86.25 84.86 50% 86.34 84.92 100%(ours) 86.52 85.16 4.1.2 comparison of other systems finally, we compare our parser to the state of the art.
</prevsent>
<prevsent>we used the same testing data as (wang et al, 2005) <papid> W05-1516 </papid>did, selecting the sentences length up to 40.</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
table 3 shows the results achieved by our method and other researchers (uas with p), where wang05 refers to (wang et al, 2005), <papid> W05-1516 </papid>wang07 refers 92 to (wang et al, 2007), and mcdonald&pereira065; refers to (mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>from the table, we found that our parser performed best.
</nextsent>
<nextsent>table 3: the results on the sentences length up to 40 uas with wang05 79.9 mcdonald&pereira06; 82.5 wang07 86.6 baseline 87.1 ours 88.4
</nextsent>
<nextsent>5.1 improvement relative to dependency length.
</nextsent>
<nextsent>we now look at the improvement relative to dependency length as figure 5 shows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1414">
<title id=" I08-1012.xml">dependency parsing with short dependency relations in unlabeled data </title>
<section> analysis.  </section>
<citcontext>
<prevsection>
<prevsent>the reason may be that shorter dependencies are often modifier of nouns such as determiners or adjectives or pronouns modifying their direct neighbors, while longer dependencies typically represent modifiers of the root or the main verb in sentence(mcdonald and nivre,2007).<papid> D07-1013 </papid></prevsent>
<prevsent>we did not provide new features for modifiers of the root.</prevsent>
</prevsection>
<citsent citstr=" N07-3002 ">
30 40 50 60 70 80 90 100 0 5 10 15 20 f1 dependency length baseline ours figure 5: improvement relative to dependency length 5(wang, 2007) <papid> N07-3002 </papid>reported this result.</citsent>
<aftsection>
<nextsent>jj nn nn jj nn nn jj nn nn nn nn nn nn nn nn nn nn nn ad vv vv ad vv vv ad vv vv jj nn cc nn jj nn cc nn jj nn cc nn figure 6: ambiguities 5.2 cases study in neighborhood.
</nextsent>
<nextsent>in chinese dependency parsing, there are many ambiguities in neighborhood, such as jj nn nn?, ad vv vv?, nn nn nn?, jj nn cc nn?.
</nextsent>
<nextsent>they have possible parsing trees as figure 6 shows.
</nextsent>
<nextsent>for these ambiguities, our approach can provide additional information for the parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1415">
<title id=" I08-2094.xml">automatic paraphrasing of japanese functional expressions using a hierarchically organized dictionary </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the latter hierarchy provides semantic equivalence classes of functional expressions and readability level for each functional expression, which are required for the above process 1.
</prevsent>
<prevsent>a few studies on paraphrasing of japanese functional expressions have been conducted.
</prevsent>
</prevsection>
<citsent citstr=" W04-0405 ">
in orderto implement automatic paraphrasing, some studies (iida et al, 2001; tsuchiya et al, 2004) use set of paraphrasing rules, and others (tanabe et al, 2001; shudo et al, 2004) <papid> W04-0405 </papid>use semantic equivalence classes.</citsent>
<aftsection>
<nextsent>all of these studies do not handle variants in asystematic way.
</nextsent>
<nextsent>in case system paraphrases functional expression into ?, it also should generate all variants of ? in potential.
</nextsent>
<nextsent>however, any proposed system does not guarantee this requirement.
</nextsent>
<nextsent>output selection of variants should be determined according to the given style specification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1417">
<title id=" I08-2094.xml">automatic paraphrasing of japanese functional expressions using a hierarchically organized dictionary </title>
<section> spelling variations 16,801.  </section>
<citcontext>
<prevsection>
<prevsent>this system consists of three modules: analysis, paraphrase generation, and ranking.
</prevsent>
<prevsent>5.1 analysis.
</prevsent>
</prevsection>
<citsent citstr=" P03-2037 ">
some methods have been proposed for detecting japanese functional expressions based on set of detection rules (tsuchiya and sato, 2003) <papid> P03-2037 </papid>and machine learning (uchimoto et al, 2003; tsuchiya et al., 2006).<papid> W06-2404 </papid></citsent>
<aftsection>
<nextsent>however, because these methods detect only limited number of functional expressions (and their variants), we cannot apply them to the analysis of phrase.
</nextsent>
<nextsent>another method is to add list of about 17,000 surface forms of functional expressions to dictionary of an existing morphological analyzer and determine connecting costs based on machine learning.
</nextsent>
<nextsent>however, it is infeasible because there is no large corpus in which all of these surface forms have been tagged.
</nextsent>
<nextsent>instead of these methods, we use different method of decomposing given phrase into sequence of content words and functional expressions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1418">
<title id=" I08-2094.xml">automatic paraphrasing of japanese functional expressions using a hierarchically organized dictionary </title>
<section> spelling variations 16,801.  </section>
<citcontext>
<prevsection>
<prevsent>this system consists of three modules: analysis, paraphrase generation, and ranking.
</prevsent>
<prevsent>5.1 analysis.
</prevsent>
</prevsection>
<citsent citstr=" W06-2404 ">
some methods have been proposed for detecting japanese functional expressions based on set of detection rules (tsuchiya and sato, 2003) <papid> P03-2037 </papid>and machine learning (uchimoto et al, 2003; tsuchiya et al., 2006).<papid> W06-2404 </papid></citsent>
<aftsection>
<nextsent>however, because these methods detect only limited number of functional expressions (and their variants), we cannot apply them to the analysis of phrase.
</nextsent>
<nextsent>another method is to add list of about 17,000 surface forms of functional expressions to dictionary of an existing morphological analyzer and determine connecting costs based on machine learning.
</nextsent>
<nextsent>however, it is infeasible because there is no large corpus in which all of these surface forms have been tagged.
</nextsent>
<nextsent>instead of these methods, we use different method of decomposing given phrase into sequence of content words and functional expressions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1419">
<title id=" I05-6003.xml">a study of applying btm model on the chinese chunk bracketing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a chunker is to divide sentences into nonoverlapping phrases by starting with finding correlated chunks of words.
</prevsent>
<prevsent>text chunking has been shown useful pre-processing step for language parsing (sang and buchholz, 2000).
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
among the chunk types, np chunking is the first to receive the attention (ramshaw and marcus, 1995), <papid> W95-0107 </papid>than other chunk types, such as vp and pp chunking (veenstra, 1999).</citsent>
<aftsection>
<nextsent>for english (sang and buchholz, 2000) and chinese (li et al, 2004) <papid> W04-1107 </papid>languages, the top 3 most frequent chunk types are np, vp and pp chunks.</nextsent>
<nextsent>meanwhile, the three chunk types cover about 80% of chunking prob lems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1420">
<title id=" I05-6003.xml">a study of applying btm model on the chinese chunk bracketing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>text chunking has been shown useful pre-processing step for language parsing (sang and buchholz, 2000).
</prevsent>
<prevsent>among the chunk types, np chunking is the first to receive the attention (ramshaw and marcus, 1995), <papid> W95-0107 </papid>than other chunk types, such as vp and pp chunking (veenstra, 1999).</prevsent>
</prevsection>
<citsent citstr=" W04-1107 ">
for english (sang and buchholz, 2000) and chinese (li et al, 2004) <papid> W04-1107 </papid>languages, the top 3 most frequent chunk types are np, vp and pp chunks.</citsent>
<aftsection>
<nextsent>meanwhile, the three chunk types cover about 80% of chunking problems.
</nextsent>
<nextsent>in many natural language processing (nlp) applications, such as information retrieval, knowledge discovery, example-based machine translation (ebmt) and text summarization, can benefit with chunks (le et al, 2003; munoz et al., 1999; oliver, 2001; zhou and su, 2003).
</nextsent>
<nextsent>as per the reports (menzel, 1995; sang and buchholz, 2000; basili and zanzotto, 2002; knutsson et al, 2003; li et al, 2004; <papid> W04-1107 </papid>xu et al, 2004; <papid> W04-1114 </papid>johnny et al, 2005), there are three important trends in the study of chinese text chunking and parsing.</nextsent>
<nextsent>these important trends are: (1) treebank-derived approaches for auto constructing useful patterns and templates from treebank (tb) as rules combined with statistical language models (slm), such as n-gram models and support vector machines (svms), etc.; (2) robust chunk ers against treebank sparseness and perfect/actual input.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1422">
<title id=" I05-6003.xml">a study of applying btm model on the chinese chunk bracketing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>meanwhile, the three chunk types cover about 80% of chunking problems.
</prevsent>
<prevsent>in many natural language processing (nlp) applications, such as information retrieval, knowledge discovery, example-based machine translation (ebmt) and text summarization, can benefit with chunks (le et al, 2003; munoz et al., 1999; oliver, 2001; zhou and su, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W04-1114 ">
as per the reports (menzel, 1995; sang and buchholz, 2000; basili and zanzotto, 2002; knutsson et al, 2003; li et al, 2004; <papid> W04-1107 </papid>xu et al, 2004; <papid> W04-1114 </papid>johnny et al, 2005), there are three important trends in the study of chinese text chunking and parsing.</citsent>
<aftsection>
<nextsent>these important trends are: (1) treebank-derived approaches for auto constructing useful patterns and templates from treebank (tb) as rules combined with statistical language models (slm), such as n-gram models and support vector machines (svms), etc.; (2) robust chunk ers against treebank sparseness and perfect/actual input.
</nextsent>
<nextsent>here the perfect input means the word-segmentation and part-of speech (pos) tags all are correct.
</nextsent>
<nextsent>the actual input means the word-segmentation and pos tags all are generated by selected segmenter and pos tagger; and (3) high performance chunk bracketing has been reported that the key issue of chinese parsing (li et al, 2004).<papid> W04-1107 </papid></nextsent>
<nextsent>to sum up these trends, one of critical issues for developing high performance chinese chunker is to find methods to achieve high performance of chunk bracketing against training size, perfect and actual input.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1427">
<title id=" I05-6003.xml">a study of applying btm model on the chinese chunk bracketing </title>
<section> development of the btm model.  </section>
<citcontext>
<prevsection>
<prevsent>a chinese treebank (ctb) is segmented, pos-tagged and fully bracketed chinese corpus with morphological, syntactic, semantic and discourse structures.
</prevsent>
<prevsent>the ckip (chinese knowledge information processing) chinese-treebank (cctb) and the penn chinese treebank (pctb) are two of most important chinese treebank resources for treebank-derived nlp tasks in chinese (ckip, 1995; xia et al, 2000; xu et al, 2000; li et al, 2004).<papid> W04-1107 </papid></prevsent>
</prevsection>
<citsent citstr=" W00-1205 ">
the brief introductions of the cctb and the pctb are given as below (table 1 is brief comparison between the cctb and the pctb): (1) cctb: the cctb is developed in traditional chinese texts (big5 encoded) taken from the academia sinica balanced corpus 3.0 (asbc3) at the academia sinica, taiwan (chen et al, 1996; chen et al, 1999; huang et al, 2000, <papid> W00-1205 </papid>chen et al, 2003; chen et al, 2004).</citsent>
<aftsection>
<nextsent>the cctb uses information-based case grammar (icg) as the language framework to express both syntactic and semantic descriptions (chen and huang, 1996).
</nextsent>
<nextsent>the structural frame of cctb is based on the head-driven principle: it means sentence or phrase is composed of core head and its arguments, or adjuncts (chen and hsieh, 2004).
</nextsent>
<nextsent>the head defines its phrasal category and relations with other constituents.
</nextsent>
<nextsent>the present version cctb2.1 (cctb version 2.1) includes 54,902 sentences (i.e. trees) and 290,144 words that are bracketed and post-edited by humans, based on the computer parsed results (ckip, 1995).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1428">
<title id=" I05-6003.xml">a study of applying btm model on the chinese chunk bracketing </title>
<section> development of the btm model.  </section>
<citcontext>
<prevsection>
<prevsent>the present version cctb2.1 (cctb version 2.1) includes 54,902 sentences (i.e. trees) and 290,144 words that are bracketed and post-edited by humans, based on the computer parsed results (ckip, 1995).
</prevsent>
<prevsent>there are 1,000 cctb trees open to the public for researchers to download on the cctb portal.
</prevsent>
</prevsection>
<citsent citstr=" W04-1116 ">
the details of supplementary principles, symbol illustrations, semantic roles, phrasal structures and applications of the cctb can be found in (cctb portal; chen et al, 2003; chen and hsieh, 2004; you and chen, 2004).<papid> W04-1116 </papid></citsent>
<aftsection>
<nextsent>table 1.
</nextsent>
<nextsent>a brief comparison between cctb2.1 and pctb4 (the number in () is the word frequency and the english word in [] is the english translation for the corresponding chinese word) cctb2.1 pctb4 developer ckip upenn content type balanced newswire corpus sources language framework icg hpsg word standard taiwan china (ckip, 1996) (liu et al,1993) pos-tagging system type hierarchical non- (5 layer) hierarchical structure frame head-driven head-driven code big5 gb no.
</nextsent>
<nextsent>of sentences 54,902 15,162 no.
</nextsent>
<nextsent>of distinct pos tags 302 47 no.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1429">
<title id=" I05-6003.xml">a study of applying btm model on the chinese chunk bracketing </title>
<section> development of the btm model.  </section>
<citcontext>
<prevsection>
<prevsent>of distinct pos tags 302 47 no.
</prevsent>
<prevsent>of words in ctb 290,144 404,156 top 3 one-char words ?(19), (212) ?(15), (080) [of] [of] ?(4), (608) ?(4), (055) [is/are] [at] ?(4), (235) ?(2), (965) [at] [is/are] top 3 two-char words ??(1), (057) ??(2), (097) [we] [china] ??(675) ??(1), (015) [a/an/one] [economy] ??(564) ??(989) [they] [business] (2) pctb: the pctb is developed in simplified chinese texts (gb encoded) taken from the newswire sources (consists of xinhua newswire, hongkong news and sino rama news magazine, taiwan) at the department of computer and information science, university of pennsylvania (upenn).
</prevsent>
</prevsection>
<citsent citstr=" C02-1145 ">
the pctb uses head-driven phrase structure grammar (hpsg) to create chinese texts with syntactic bracketing (xia et al 2000; xue et al 2002).<papid> C02-1145 </papid></citsent>
<aftsection>
<nextsent>meanwhile, the semantic annotation of pctb mainly deals with the predicate argument structure of chinese verbs in penn chinese proposition bank (xue and palmer, 2003; <papid> W03-1707 </papid>22 xue and palmer, 2005).</nextsent>
<nextsent>the present version pctb5 (pctb version 5), contains 18,782 sentences, 507,222 words, 824,983 hanzi and 890 data files.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1430">
<title id=" I05-6003.xml">a study of applying btm model on the chinese chunk bracketing </title>
<section> development of the btm model.  </section>
<citcontext>
<prevsection>
<prevsent>of words in ctb 290,144 404,156 top 3 one-char words ?(19), (212) ?(15), (080) [of] [of] ?(4), (608) ?(4), (055) [is/are] [at] ?(4), (235) ?(2), (965) [at] [is/are] top 3 two-char words ??(1), (057) ??(2), (097) [we] [china] ??(675) ??(1), (015) [a/an/one] [economy] ??(564) ??(989) [they] [business] (2) pctb: the pctb is developed in simplified chinese texts (gb encoded) taken from the newswire sources (consists of xinhua newswire, hongkong news and sino rama news magazine, taiwan) at the department of computer and information science, university of pennsylvania (upenn).
</prevsent>
<prevsent>the pctb uses head-driven phrase structure grammar (hpsg) to create chinese texts with syntactic bracketing (xia et al 2000; xue et al 2002).<papid> C02-1145 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1707 ">
meanwhile, the semantic annotation of pctb mainly deals with the predicate argument structure of chinese verbs in penn chinese proposition bank (xue and palmer, 2003; <papid> W03-1707 </papid>22 xue and palmer, 2005).</citsent>
<aftsection>
<nextsent>the present version pctb5 (pctb version 5), contains 18,782 sentences, 507,222 words, 824,983 hanzi and 890 data files.
</nextsent>
<nextsent>the pctb was created by two pass approach.
</nextsent>
<nextsent>the first pass was done by one annotator, and the resulting files were checked by second annotator (the second pass).
</nextsent>
<nextsent>the details and applications of pctb can be found in (pctb portal; xia et al 2000; chiou et al 2001; <papid> H01-1026 </papid>xue et al 2002; <papid> C02-1145 </papid>xue et al 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1431">
<title id=" I05-6003.xml">a study of applying btm model on the chinese chunk bracketing </title>
<section> development of the btm model.  </section>
<citcontext>
<prevsection>
<prevsent>the pctb was created by two pass approach.
</prevsent>
<prevsent>the first pass was done by one annotator, and the resulting files were checked by second annotator (the second pass).
</prevsent>
</prevsection>
<citsent citstr=" H01-1026 ">
the details and applications of pctb can be found in (pctb portal; xia et al 2000; chiou et al 2001; <papid> H01-1026 </papid>xue et al 2002; <papid> C02-1145 </papid>xue et al 2005).</citsent>
<aftsection>
<nextsent>overall, from table 1, the four major differences between the cctb and the pctb are content type, language framework, word standard and pos-tagging system type.
</nextsent>
<nextsent>the cctb is natural to be balanced ctb because its content is taken from the academia sinica balanced corpus (ckip, 1995).
</nextsent>
<nextsent>on the other hand, since the content type of pctb is newswire sources, it is natural to be newswire-based ctb and not balanced ctb.
</nextsent>
<nextsent>2.2 generating the btm dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1433">
<title id=" I05-6003.xml">a study of applying btm model on the chinese chunk bracketing </title>
<section> conclusion and future directions.  </section>
<citcontext>
<prevsection>
<prevsent>according to the fourth experiment results, when applying btm (0.5, 2, 45,000) model on the matching set and 4-gram model on the non-matching set, the combined system can improve the measure of 4-gram model 2.5% for perfect input and 1.0% for actual input.
</prevsent>
<prevsent>among the chunking and parsing models, cascaded markov models should be the first one to construct the parse tree layer by layer with each layers markov model.
</prevsent>
</prevsection>
<citsent citstr=" E99-1016 ">
as per (brants, 1999), <papid> E99-1016 </papid>each layers chunk bracketing of cascaded markov models is dependent because the output of lower layer is passed as input to the next higher layer.</citsent>
<aftsection>
<nextsent>on the contrast, our btm model can independently generate the chunks for top layer without the results of lower layer chunk bracketing; and (6) since the f-measures of the btm model for the matching sets of perfect and actual input both are greater than 95%, we believe our btm model can be used not only to improve the measure of existing shallow parsing or chunking systems, but also to help select valuable sentences from the non-matching set for effectively extending the cr of our btm model.
</nextsent>
<nextsent>in the future, we shall study how to combine our btm model with more conventional statistical approaches, such as bayesian networks, maximum entropy and cascaded markov models, etc. meanwhile, we will also apply our btm model to the penn english treebank ascom parative study.
</nextsent>
<nextsent>acknowledgement some work of this paper was done while the author was visitor at the institute of information science (academia sinica in taiwan).
</nextsent>
<nextsent>we thank prof. wei-lian hsu and cheng-wei shih for their kind help.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1434">
<title id=" I08-1046.xml">an eeffective methods of using web based information for relation extraction </title>
<section> introduction and motivation.  </section>
<citcontext>
<prevsection>
<prevsent>these are links between two entities mentioned in the same sentence,and further restrict our consideration to those relationships clearly supported by evidence in the scope of the same document.the aces annotators mark all mentions of relations where there is direct syntactic connection between the entities, i.e. when one entity mention modifies another one, or when two entity mentions are arguments of the same event.
</prevsent>
<prevsent>relations between entities that are implied in the text but which do not satisfy either requirement are considered to be implicit, and are marked only once.
</prevsent>
</prevsection>
<citsent citstr=" P06-1017 ">
our work sits squarely in the realm of work on regular ie done by (zelenko et al, 2003; zhou et al., 2005; chen et al, 2006).<papid> P06-1017 </papid></citsent>
<aftsection>
<nextsent>here, the corpus of interest is well defined set of texts, such as news articles, and we have to detect and classify all appearances of relations from set of given relation types in the documents.
</nextsent>
<nextsent>in line with assumptions in the related work, we assert that the differences in the markup for implicit and explicit relations does not significantly affect our performance.
</nextsent>
<nextsent>supervised learning methods have proved to be some of the most effective for regular ie.
</nextsent>
<nextsent>they do however, need large volumes of expensively annotated examples to perform robustly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1435">
<title id=" I08-1046.xml">an eeffective methods of using web based information for relation extraction </title>
<section> related work and differences.  </section>
<citcontext>
<prevsection>
<prevsent>most importantly we show how the output from such an approach can be combined with existing knowledge gleaned from supervised learning to improve the performance of relation extraction significantly.
</prevsent>
<prevsent>to our knowledge, there is no previous work that exploits the information from large raw text corpus like the web to improve supervised relation extraction.
</prevsent>
</prevsection>
<citsent citstr=" W03-1609 ">
in the spirit of the work done by (shinyama and sekine, 2003; <papid> W03-1609 </papid>bunescu and mooney, 2007), <papid> P07-1073 </papid>we are trying to collect clusters of paraphrases forgiven relation mentions.</citsent>
<aftsection>
<nextsent>briefly, since the same relation can be expressed in many ways, the information we may learn about that relation in any single sentence is very limited.
</nextsent>
<nextsent>the idea then is to alleviate this bias by collecting many paraphrases of the same relation instance into clusters when we train our system.shinyama generalizes the expressions using partof-speech information and dependency tree similarity into generic templates.
</nextsent>
<nextsent>bunescus work uses relation kernel on subsquences of words developed in (bunescu and mooney, 2005).
</nextsent>
<nextsent>we observed that both approaches suffer from low recall despite the attempts to generalize the sub sequences and templates probably because they relyon local context only.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1436">
<title id=" I08-1046.xml">an eeffective methods of using web based information for relation extraction </title>
<section> related work and differences.  </section>
<citcontext>
<prevsection>
<prevsent>most importantly we show how the output from such an approach can be combined with existing knowledge gleaned from supervised learning to improve the performance of relation extraction significantly.
</prevsent>
<prevsent>to our knowledge, there is no previous work that exploits the information from large raw text corpus like the web to improve supervised relation extraction.
</prevsent>
</prevsection>
<citsent citstr=" P07-1073 ">
in the spirit of the work done by (shinyama and sekine, 2003; <papid> W03-1609 </papid>bunescu and mooney, 2007), <papid> P07-1073 </papid>we are trying to collect clusters of paraphrases forgiven relation mentions.</citsent>
<aftsection>
<nextsent>briefly, since the same relation can be expressed in many ways, the information we may learn about that relation in any single sentence is very limited.
</nextsent>
<nextsent>the idea then is to alleviate this bias by collecting many paraphrases of the same relation instance into clusters when we train our system.shinyama generalizes the expressions using partof-speech information and dependency tree similarity into generic templates.
</nextsent>
<nextsent>bunescus work uses relation kernel on subsquences of words developed in (bunescu and mooney, 2005).
</nextsent>
<nextsent>we observed that both approaches suffer from low recall despite the attempts to generalize the sub sequences and templates probably because they relyon local context only.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1438">
<title id=" I08-1046.xml">an eeffective methods of using web based information for relation extraction </title>
<section> related work and differences.  </section>
<citcontext>
<prevsection>
<prevsent>bag-of-words or unigram representations of our paraphrase clusters are easy to compute, but information about word ordering is lost.
</prevsent>
<prevsent>hence, we settled on the use of skip-bigram representation of our relation clusters instead.
</prevsent>
</prevsection>
<citsent citstr=" C04-1072 ">
skip-bigrams (lin and och, 2004) <papid> C04-1072 </papid>are pairs ofwords in sentence order allowing for gaps in be tween.</citsent>
<aftsection>
<nextsent>longer gaps capture far-flung associations between words, while short gaps of between 1 and 4capture local structure.
</nextsent>
<nextsent>in using them, we do notre strict ourselves to context centered around the entity mentions.
</nextsent>
<nextsent>another advantage of using skip-bigramsis that we can capture some extra-sentential information since we are no longer restricted by the ability to generate dependency structures within single sentence as shinyama is. using skip-bigrams, we can assess the similarity of particular new relation mention instance against the relation clusters we collect in training.
</nextsent>
<nextsent>we can then compute likelihood that we combine with the predictions of the supervised learning algorithm for final classification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1442">
<title id=" I08-1046.xml">an eeffective methods of using web based information for relation extraction </title>
<section> overall structure.  </section>
<citcontext>
<prevsection>
<prevsent>we support the same set of features as zhou, namely: local word position features, entity type, base phrase chunking features, dependency and parse tree features as well as semantic information like country names and list of trigger words.
</prevsent>
<prevsent>in our current work, we use michael collins?
</prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
(collins, 2003) <papid> J03-4003 </papid>parser for syntactic information.sentence boundary detection, chunking, and parsing are done as preprocessing steps before we begin our learning.given sentence with the relation mention instance, the semi-supervised method goes through the following five stages: 1.</citsent>
<aftsection>
<nextsent>from the list of entitites marked in the sen-.
</nextsent>
<nextsent>tence, generate all possible pairings as candidates.
</nextsent>
<nextsent>we pick one of these candidates and proceed to the next step.
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1443">
<title id=" I08-1046.xml">an eeffective methods of using web based information for relation extraction </title>
<section> overall structure.  </section>
<citcontext>
<prevsection>
<prevsent>bill ruprecht, chief executive of sothebys, agreed that september 11 had been...
</prevsent>
<prevsent>the duke will also remain leader of sothebys germany...
</prevsent>
</prevsection>
<citsent citstr=" W06-2201 ">
3.1.2 web search (geleijnse and korst, 2006) <papid> W06-2201 </papid>use google as their search engine for extracting surface patterns from web documents.</citsent>
<aftsection>
<nextsent>we use the same procedure here to find our paraphrases.
</nextsent>
<nextsent>for each pair of arguments, we create boolean query string em1 ? em2, and submit it to google.
</nextsent>
<nextsent>the query will find documents where mentions of em1 are separated from em2 by any number of words.
</nextsent>
<nextsent>we restrict the language to english.google returns two line extract from the documents that match our boolean query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1446">
<title id=" I08-1014.xml">determining the unithood of word sequences using a probabilistic approach </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>in section 4, we summarize some findings from our evaluations.
</prevsent>
<prevsent>finally, we conclude this paper with an outlook to future work in section 5.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
some of the most common measures of unithood include pointwise mutual information (mi) (church and hanks, 1990) <papid> J90-1003 </papid>and log-likelihood ratio (dunning,1994).</citsent>
<aftsection>
<nextsent>in mutual information, the co-occurrence frequencies of the constituents of complex terms are utilised to measure their dependency.
</nextsent>
<nextsent>the mutual information for two words and is defined as: mi(a, b) = log 2 p(a, b) p(a)p(b) (1)where p(a) and p(b) are the probabilities of occurrence of and b. many measures that apply statistical techniques assuming strict normal distribution, and independence between the word occurrences (franz, 1997) <papid> P97-1024 </papid>do not fare well.</nextsent>
<nextsent>for handling extremely uncommon words or small sized corpus, log-likelihood ratio delivers the best precision (kurz and xu, 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1447">
<title id=" I08-1014.xml">determining the unithood of word sequences using a probabilistic approach </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>some of the most common measures of unithood include pointwise mutual information (mi) (church and hanks, 1990) <papid> J90-1003 </papid>and log-likelihood ratio (dunning,1994).</prevsent>
<prevsent>in mutual information, the co-occurrence frequencies of the constituents of complex terms are utilised to measure their dependency.</prevsent>
</prevsection>
<citsent citstr=" P97-1024 ">
the mutual information for two words and is defined as: mi(a, b) = log 2 p(a, b) p(a)p(b) (1)where p(a) and p(b) are the probabilities of occurrence of and b. many measures that apply statistical techniques assuming strict normal distribution, and independence between the word occurrences (franz, 1997) <papid> P97-1024 </papid>do not fare well.</citsent>
<aftsection>
<nextsent>for handling extremely uncommon words or small sized corpus, log-likelihood ratio delivers the best precision (kurz and xu, 2002).
</nextsent>
<nextsent>log-likelihood ratio attempts to quantify how much more likely one pair of words isto occur compared to the others.
</nextsent>
<nextsent>despite its potential, how to apply this statistic measure to quantify structural dependency of word sequence remains an interesting issue to explore.?
</nextsent>
<nextsent>(kit, 2002).(seretan et al, 2004) tested mutual information, log likelihood ratio and t-tests to examine the use of results from web search engines for determining the collocational strength of word pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1448">
<title id=" I08-1014.xml">determining the unithood of word sequences using a probabilistic approach </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, idr(ax, ay) = id(ax,s) id(ay ,s) . intuitively, uh(ax, ay) states that the two lexical units ax and ay can only be merged in two cases, namely, 1) if ax and ay has extremely high mutual information (i.e. higher than certain threshold mi+), or 2) if ax and ay achieve average mutual information (i.e. within the acceptable range of two thresholds mi+ and mi?)
</prevsent>
<prevsent>due to both of their extremely high independence (i.e. higher than the threshold idt ) from s.
</prevsent>
</prevsection>
<citsent citstr=" P97-1065 ">
(frantzi, 1997) <papid> P97-1065 </papid>proposed measure known as cvalue for extracting complex terms.</citsent>
<aftsection>
<nextsent>the measure 104 is based upon the claim that substring of term candidate is candidate itself given that it demonstrates adequate independence from the longer version it appears in.
</nextsent>
<nextsent>for example, e. coli food poisoning?, e. coli?
</nextsent>
<nextsent>and food poisoning?
</nextsent>
<nextsent>are accept able as valid complex term candidates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1449">
<title id=" I08-1014.xml">determining the unithood of word sequences using a probabilistic approach </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>given word sequence to be examined for unithood, the cvalue is defined as: cvalue(a) = { log 2 |a|fa if |a| = log 2 |a|(fa ? ?
</prevsent>
<prevsent>lla fl |la| ) otherwise (3) where |a| is the number of words in a, la is the set of longer term candidates that contain a, is the longest n-gram considered, fa is the frequency of occurrence of a, and /?
</prevsent>
</prevsection>
<citsent citstr=" W02-1407 ">
la. while certain researchers (kit, 2002) consider cvalue as termhood measure, others (nakagawa and mori, 2002) <papid> W02-1407 </papid>accept it as measure for unithood.</citsent>
<aftsection>
<nextsent>one can observe that longer candidates tend to gain higher weights due to the inclusion of log 2 |a| in equation 3.
</nextsent>
<nextsent>in addition, the weights computed using equation 3 are purely dependent on the frequency of a.
</nextsent>
<nextsent>unithood determination we propose probabilistically-derived measure for determining the unithood of word pairs (i.e. potential term candidates) extracted using the head driven left-right filter (wong, 2005; wong et al, 2007b) and stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>these word pairs will appear in the form of (ax, ay) ? with ax and ay located immediately next to each other (i.e. + 1 = y), or separated by preposition or coordinating conjunction and?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1450">
<title id=" I08-1014.xml">determining the unithood of word sequences using a probabilistic approach </title>
<section> a probabilistically-derived measure for </section>
<citcontext>
<prevsection>
<prevsent>one can observe that longer candidates tend to gain higher weights due to the inclusion of log 2 |a| in equation 3.
</prevsent>
<prevsent>in addition, the weights computed using equation 3 are purely dependent on the frequency of a.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
unithood determination we propose probabilistically-derived measure for determining the unithood of word pairs (i.e. potential term candidates) extracted using the head driven left-right filter (wong, 2005; wong et al, 2007b) and stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>these word pairs will appear in the form of (ax, ay) ? with ax and ay located immediately next to each other (i.e. + 1 = y), or separated by preposition or coordinating conjunction and?
</nextsent>
<nextsent>(i.e. + 2 = y).
</nextsent>
<nextsent>obviously, ax has to appear before ay in the sentence or in other words,   for all pairs where and are the word offsets produced by the stanford parser.
</nextsent>
<nextsent>the pairs in will remain as potential term candidates until their unithood have been examined.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1451">
<title id=" I08-1014.xml">determining the unithood of word sequences using a probabilistic approach </title>
<section> a probabilistically-derived measure for </section>
<citcontext>
<prevsection>
<prevsent>as we have discussed in section 2, most ofthe conventional practices employ frequency of occurrence from local corpora, and some statistical tests or information-theoretic measures to determine the coupling strength between elements in = {s, ax, ay}.
</prevsent>
<prevsent>two of the main problems associated with such approaches are:?
</prevsent>
</prevsection>
<citsent citstr=" W02-1030 ">
data sparseness is problem that is well documented by many researchers (keller et al, 2002).<papid> W02-1030 </papid></citsent>
<aftsection>
<nextsent>it is inherent to the use of local corpora that can lead to poor estimation of parameters or weights; and ? assumption of independence and normality of word distribution are two of the many problems in language modelling (franz, 1997).<papid> P97-1024 </papid></nextsent>
<nextsent>while the independence assumption reduces text to simply bag of words, the assumption of normal distribution of words will often lead to in correct conclusions during statistical tests.as general solution, we innovatively employ results from web search engines for use in probabilistic framework for measuring unithood.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1453">
<title id=" I08-1014.xml">determining the unithood of word sequences using a probabilistic approach </title>
<section> a probabilistically-derived measure for </section>
<citcontext>
<prevsection>
<prevsent>p (u |e)) (7) by substituting equation 6 in equation 7 and later, applying the multiplication rule (u? |e)p (e) = (e|u?)p (u?) to it, we will obtain: (u |e) (u? |e) = (e|u)p (u) (e|u?)p (u?)
</prevsent>
<prevsent>(8) we proceed to take the log of the odds in equation 8 (i.e. logit) to get: log (e|u) (e|u?) = log (u |e) (u? |e) ? log (u) (u?)
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
(9)while it is obvious that certain words tend to co occur more frequently than others (i.e. idioms and collocations), such phenomena are largely arbitrary (smadja, 1993).<papid> J93-1007 </papid></citsent>
<aftsection>
<nextsent>this makes the task of deciding on what constitutes an acceptable collocation difficult.
</nextsent>
<nextsent>the only way to objectively identify stable lexical units is through observations in samples of the language (e.g. text corpus) (mckeown andra dev, 2000).
</nextsent>
<nextsent>in other words, assigning the apriori probability of collocational strength without empirical evidence is both subjective and difficult.
</nextsent>
<nextsent>as such, we are left with the option to assume thatthe probability of being stable unit and not being stable unit without evidence is the same (i.e. (u) = (u?) = 0.5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1454">
<title id=" I05-4010.xml">harvesting the bitexts of the laws of hongkong from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bitexts, also referred to as parallel texts or bilingual corpora, collections of bilingual text pairs aligned at various levels of granularity, have been playing critical role in the current development of machine translation technology.
</prevsent>
<prevsent>it is such large datasets that give rise to the plausibility of empirical approaches to machine translation, most of which involve the application of variety of machine learning techniques to infer various types of translation knowledge from bitext data to facilitate automatic translation and enhance translation quality.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
large volumes of training data of this kind are indispensable for constructing statistical translation models (brown et al,1993; <papid> J93-2003 </papid>melamed, 2000), <papid> J00-2004 </papid>acquiring bilingual lexicon (gale and church, 1991<papid> P91-1023 </papid>; <papid> H91-1026 </papid>melamed, 1997), <papid> W97-0311 </papid>and building example-based machine translation (ebmt) systems (nagao, 1984; carl and way, 2003; way and gough, 2003).<papid> J03-3004 </papid></citsent>
<aftsection>
<nextsent>they also provide basis for inferring lexical connection between vocabularies in cross-languages information retrieval (davis and dunning, 1995).
</nextsent>
<nextsent>existing parallel corpora have illustrated their particular value in empirical nlp research, e.g., canadian hansard corpus (gale and church, 1991<papid> P91-1023 </papid>b), hk hansard (wu, 1994), <papid> P94-1012 </papid>intersect (salkie, 1995), enpc (ebeling, 1998), the bible parallel corpus (resnik et al, 1999) and many others.</nextsent>
<nextsent>the web is being explored not only as asuper corpus for nlp and linguistic research (kilgarriff and grefenstette, 2003) <papid> J03-3001 </papid>but also, more importantly to mt research, as treasure for mining bitexts of various language pairs (resnik, 1999; <papid> P99-1068 </papid>chen and nie, 2000; nie and cai, 2001; nie and chen, 2002; resnik and smith, 2003; <papid> J03-3002 </papid>wayand gough, 2003).<papid> J03-3004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1455">
<title id=" I05-4010.xml">harvesting the bitexts of the laws of hongkong from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bitexts, also referred to as parallel texts or bilingual corpora, collections of bilingual text pairs aligned at various levels of granularity, have been playing critical role in the current development of machine translation technology.
</prevsent>
<prevsent>it is such large datasets that give rise to the plausibility of empirical approaches to machine translation, most of which involve the application of variety of machine learning techniques to infer various types of translation knowledge from bitext data to facilitate automatic translation and enhance translation quality.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
large volumes of training data of this kind are indispensable for constructing statistical translation models (brown et al,1993; <papid> J93-2003 </papid>melamed, 2000), <papid> J00-2004 </papid>acquiring bilingual lexicon (gale and church, 1991<papid> P91-1023 </papid>; <papid> H91-1026 </papid>melamed, 1997), <papid> W97-0311 </papid>and building example-based machine translation (ebmt) systems (nagao, 1984; carl and way, 2003; way and gough, 2003).<papid> J03-3004 </papid></citsent>
<aftsection>
<nextsent>they also provide basis for inferring lexical connection between vocabularies in cross-languages information retrieval (davis and dunning, 1995).
</nextsent>
<nextsent>existing parallel corpora have illustrated their particular value in empirical nlp research, e.g., canadian hansard corpus (gale and church, 1991<papid> P91-1023 </papid>b), hk hansard (wu, 1994), <papid> P94-1012 </papid>intersect (salkie, 1995), enpc (ebeling, 1998), the bible parallel corpus (resnik et al, 1999) and many others.</nextsent>
<nextsent>the web is being explored not only as asuper corpus for nlp and linguistic research (kilgarriff and grefenstette, 2003) <papid> J03-3001 </papid>but also, more importantly to mt research, as treasure for mining bitexts of various language pairs (resnik, 1999; <papid> P99-1068 </papid>chen and nie, 2000; nie and cai, 2001; nie and chen, 2002; resnik and smith, 2003; <papid> J03-3002 </papid>wayand gough, 2003).<papid> J03-3004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1456">
<title id=" I05-4010.xml">harvesting the bitexts of the laws of hongkong from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bitexts, also referred to as parallel texts or bilingual corpora, collections of bilingual text pairs aligned at various levels of granularity, have been playing critical role in the current development of machine translation technology.
</prevsent>
<prevsent>it is such large datasets that give rise to the plausibility of empirical approaches to machine translation, most of which involve the application of variety of machine learning techniques to infer various types of translation knowledge from bitext data to facilitate automatic translation and enhance translation quality.
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
large volumes of training data of this kind are indispensable for constructing statistical translation models (brown et al,1993; <papid> J93-2003 </papid>melamed, 2000), <papid> J00-2004 </papid>acquiring bilingual lexicon (gale and church, 1991<papid> P91-1023 </papid>; <papid> H91-1026 </papid>melamed, 1997), <papid> W97-0311 </papid>and building example-based machine translation (ebmt) systems (nagao, 1984; carl and way, 2003; way and gough, 2003).<papid> J03-3004 </papid></citsent>
<aftsection>
<nextsent>they also provide basis for inferring lexical connection between vocabularies in cross-languages information retrieval (davis and dunning, 1995).
</nextsent>
<nextsent>existing parallel corpora have illustrated their particular value in empirical nlp research, e.g., canadian hansard corpus (gale and church, 1991<papid> P91-1023 </papid>b), hk hansard (wu, 1994), <papid> P94-1012 </papid>intersect (salkie, 1995), enpc (ebeling, 1998), the bible parallel corpus (resnik et al, 1999) and many others.</nextsent>
<nextsent>the web is being explored not only as asuper corpus for nlp and linguistic research (kilgarriff and grefenstette, 2003) <papid> J03-3001 </papid>but also, more importantly to mt research, as treasure for mining bitexts of various language pairs (resnik, 1999; <papid> P99-1068 </papid>chen and nie, 2000; nie and cai, 2001; nie and chen, 2002; resnik and smith, 2003; <papid> J03-3002 </papid>wayand gough, 2003).<papid> J03-3004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1458">
<title id=" I05-4010.xml">harvesting the bitexts of the laws of hongkong from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bitexts, also referred to as parallel texts or bilingual corpora, collections of bilingual text pairs aligned at various levels of granularity, have been playing critical role in the current development of machine translation technology.
</prevsent>
<prevsent>it is such large datasets that give rise to the plausibility of empirical approaches to machine translation, most of which involve the application of variety of machine learning techniques to infer various types of translation knowledge from bitext data to facilitate automatic translation and enhance translation quality.
</prevsent>
</prevsection>
<citsent citstr=" H91-1026 ">
large volumes of training data of this kind are indispensable for constructing statistical translation models (brown et al,1993; <papid> J93-2003 </papid>melamed, 2000), <papid> J00-2004 </papid>acquiring bilingual lexicon (gale and church, 1991<papid> P91-1023 </papid>; <papid> H91-1026 </papid>melamed, 1997), <papid> W97-0311 </papid>and building example-based machine translation (ebmt) systems (nagao, 1984; carl and way, 2003; way and gough, 2003).<papid> J03-3004 </papid></citsent>
<aftsection>
<nextsent>they also provide basis for inferring lexical connection between vocabularies in cross-languages information retrieval (davis and dunning, 1995).
</nextsent>
<nextsent>existing parallel corpora have illustrated their particular value in empirical nlp research, e.g., canadian hansard corpus (gale and church, 1991<papid> P91-1023 </papid>b), hk hansard (wu, 1994), <papid> P94-1012 </papid>intersect (salkie, 1995), enpc (ebeling, 1998), the bible parallel corpus (resnik et al, 1999) and many others.</nextsent>
<nextsent>the web is being explored not only as asuper corpus for nlp and linguistic research (kilgarriff and grefenstette, 2003) <papid> J03-3001 </papid>but also, more importantly to mt research, as treasure for mining bitexts of various language pairs (resnik, 1999; <papid> P99-1068 </papid>chen and nie, 2000; nie and cai, 2001; nie and chen, 2002; resnik and smith, 2003; <papid> J03-3002 </papid>wayand gough, 2003).<papid> J03-3004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1460">
<title id=" I05-4010.xml">harvesting the bitexts of the laws of hongkong from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bitexts, also referred to as parallel texts or bilingual corpora, collections of bilingual text pairs aligned at various levels of granularity, have been playing critical role in the current development of machine translation technology.
</prevsent>
<prevsent>it is such large datasets that give rise to the plausibility of empirical approaches to machine translation, most of which involve the application of variety of machine learning techniques to infer various types of translation knowledge from bitext data to facilitate automatic translation and enhance translation quality.
</prevsent>
</prevsection>
<citsent citstr=" W97-0311 ">
large volumes of training data of this kind are indispensable for constructing statistical translation models (brown et al,1993; <papid> J93-2003 </papid>melamed, 2000), <papid> J00-2004 </papid>acquiring bilingual lexicon (gale and church, 1991<papid> P91-1023 </papid>; <papid> H91-1026 </papid>melamed, 1997), <papid> W97-0311 </papid>and building example-based machine translation (ebmt) systems (nagao, 1984; carl and way, 2003; way and gough, 2003).<papid> J03-3004 </papid></citsent>
<aftsection>
<nextsent>they also provide basis for inferring lexical connection between vocabularies in cross-languages information retrieval (davis and dunning, 1995).
</nextsent>
<nextsent>existing parallel corpora have illustrated their particular value in empirical nlp research, e.g., canadian hansard corpus (gale and church, 1991<papid> P91-1023 </papid>b), hk hansard (wu, 1994), <papid> P94-1012 </papid>intersect (salkie, 1995), enpc (ebeling, 1998), the bible parallel corpus (resnik et al, 1999) and many others.</nextsent>
<nextsent>the web is being explored not only as asuper corpus for nlp and linguistic research (kilgarriff and grefenstette, 2003) <papid> J03-3001 </papid>but also, more importantly to mt research, as treasure for mining bitexts of various language pairs (resnik, 1999; <papid> P99-1068 </papid>chen and nie, 2000; nie and cai, 2001; nie and chen, 2002; resnik and smith, 2003; <papid> J03-3002 </papid>wayand gough, 2003).<papid> J03-3004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1461">
<title id=" I05-4010.xml">harvesting the bitexts of the laws of hongkong from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bitexts, also referred to as parallel texts or bilingual corpora, collections of bilingual text pairs aligned at various levels of granularity, have been playing critical role in the current development of machine translation technology.
</prevsent>
<prevsent>it is such large datasets that give rise to the plausibility of empirical approaches to machine translation, most of which involve the application of variety of machine learning techniques to infer various types of translation knowledge from bitext data to facilitate automatic translation and enhance translation quality.
</prevsent>
</prevsection>
<citsent citstr=" J03-3004 ">
large volumes of training data of this kind are indispensable for constructing statistical translation models (brown et al,1993; <papid> J93-2003 </papid>melamed, 2000), <papid> J00-2004 </papid>acquiring bilingual lexicon (gale and church, 1991<papid> P91-1023 </papid>; <papid> H91-1026 </papid>melamed, 1997), <papid> W97-0311 </papid>and building example-based machine translation (ebmt) systems (nagao, 1984; carl and way, 2003; way and gough, 2003).<papid> J03-3004 </papid></citsent>
<aftsection>
<nextsent>they also provide basis for inferring lexical connection between vocabularies in cross-languages information retrieval (davis and dunning, 1995).
</nextsent>
<nextsent>existing parallel corpora have illustrated their particular value in empirical nlp research, e.g., canadian hansard corpus (gale and church, 1991<papid> P91-1023 </papid>b), hk hansard (wu, 1994), <papid> P94-1012 </papid>intersect (salkie, 1995), enpc (ebeling, 1998), the bible parallel corpus (resnik et al, 1999) and many others.</nextsent>
<nextsent>the web is being explored not only as asuper corpus for nlp and linguistic research (kilgarriff and grefenstette, 2003) <papid> J03-3001 </papid>but also, more importantly to mt research, as treasure for mining bitexts of various language pairs (resnik, 1999; <papid> P99-1068 </papid>chen and nie, 2000; nie and cai, 2001; nie and chen, 2002; resnik and smith, 2003; <papid> J03-3002 </papid>wayand gough, 2003).<papid> J03-3004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1465">
<title id=" I05-4010.xml">harvesting the bitexts of the laws of hongkong from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>large volumes of training data of this kind are indispensable for constructing statistical translation models (brown et al,1993; <papid> J93-2003 </papid>melamed, 2000), <papid> J00-2004 </papid>acquiring bilingual lexicon (gale and church, 1991<papid> P91-1023 </papid>; <papid> H91-1026 </papid>melamed, 1997), <papid> W97-0311 </papid>and building example-based machine translation (ebmt) systems (nagao, 1984; carl and way, 2003; way and gough, 2003).<papid> J03-3004 </papid></prevsent>
<prevsent>they also provide basis for inferring lexical connection between vocabularies in cross-languages information retrieval (davis and dunning, 1995).</prevsent>
</prevsection>
<citsent citstr=" P94-1012 ">
existing parallel corpora have illustrated their particular value in empirical nlp research, e.g., canadian hansard corpus (gale and church, 1991<papid> P91-1023 </papid>b), hk hansard (wu, 1994), <papid> P94-1012 </papid>intersect (salkie, 1995), enpc (ebeling, 1998), the bible parallel corpus (resnik et al, 1999) and many others.</citsent>
<aftsection>
<nextsent>the web is being explored not only as asuper corpus for nlp and linguistic research (kilgarriff and grefenstette, 2003) <papid> J03-3001 </papid>but also, more importantly to mt research, as treasure for mining bitexts of various language pairs (resnik, 1999; <papid> P99-1068 </papid>chen and nie, 2000; nie and cai, 2001; nie and chen, 2002; resnik and smith, 2003; <papid> J03-3002 </papid>wayand gough, 2003).<papid> J03-3004 </papid></nextsent>
<nextsent>the web has been the play ground for many nlpers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1466">
<title id=" I05-4010.xml">harvesting the bitexts of the laws of hongkong from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they also provide basis for inferring lexical connection between vocabularies in cross-languages information retrieval (davis and dunning, 1995).
</prevsent>
<prevsent>existing parallel corpora have illustrated their particular value in empirical nlp research, e.g., canadian hansard corpus (gale and church, 1991<papid> P91-1023 </papid>b), hk hansard (wu, 1994), <papid> P94-1012 </papid>intersect (salkie, 1995), enpc (ebeling, 1998), the bible parallel corpus (resnik et al, 1999) and many others.</prevsent>
</prevsection>
<citsent citstr=" J03-3001 ">
the web is being explored not only as asuper corpus for nlp and linguistic research (kilgarriff and grefenstette, 2003) <papid> J03-3001 </papid>but also, more importantly to mt research, as treasure for mining bitexts of various language pairs (resnik, 1999; <papid> P99-1068 </papid>chen and nie, 2000; nie and cai, 2001; nie and chen, 2002; resnik and smith, 2003; <papid> J03-3002 </papid>wayand gough, 2003).<papid> J03-3004 </papid></citsent>
<aftsection>
<nextsent>the web has been the play ground for many nlpers.
</nextsent>
<nextsent>more and more web sites are found to have cloned their web pages in several languages, aiming at conveying information to audience in different languages.
</nextsent>
<nextsent>this gives rise to huge volume of wonderful bilingual or multi-lingual resources freely available from theweb for research.
</nextsent>
<nextsent>what we need to do is to harvest the right resources for the right applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1467">
<title id=" I05-4010.xml">harvesting the bitexts of the laws of hongkong from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they also provide basis for inferring lexical connection between vocabularies in cross-languages information retrieval (davis and dunning, 1995).
</prevsent>
<prevsent>existing parallel corpora have illustrated their particular value in empirical nlp research, e.g., canadian hansard corpus (gale and church, 1991<papid> P91-1023 </papid>b), hk hansard (wu, 1994), <papid> P94-1012 </papid>intersect (salkie, 1995), enpc (ebeling, 1998), the bible parallel corpus (resnik et al, 1999) and many others.</prevsent>
</prevsection>
<citsent citstr=" P99-1068 ">
the web is being explored not only as asuper corpus for nlp and linguistic research (kilgarriff and grefenstette, 2003) <papid> J03-3001 </papid>but also, more importantly to mt research, as treasure for mining bitexts of various language pairs (resnik, 1999; <papid> P99-1068 </papid>chen and nie, 2000; nie and cai, 2001; nie and chen, 2002; resnik and smith, 2003; <papid> J03-3002 </papid>wayand gough, 2003).<papid> J03-3004 </papid></citsent>
<aftsection>
<nextsent>the web has been the play ground for many nlpers.
</nextsent>
<nextsent>more and more web sites are found to have cloned their web pages in several languages, aiming at conveying information to audience in different languages.
</nextsent>
<nextsent>this gives rise to huge volume of wonderful bilingual or multi-lingual resources freely available from theweb for research.
</nextsent>
<nextsent>what we need to do is to harvest the right resources for the right applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1468">
<title id=" I05-4010.xml">harvesting the bitexts of the laws of hongkong from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they also provide basis for inferring lexical connection between vocabularies in cross-languages information retrieval (davis and dunning, 1995).
</prevsent>
<prevsent>existing parallel corpora have illustrated their particular value in empirical nlp research, e.g., canadian hansard corpus (gale and church, 1991<papid> P91-1023 </papid>b), hk hansard (wu, 1994), <papid> P94-1012 </papid>intersect (salkie, 1995), enpc (ebeling, 1998), the bible parallel corpus (resnik et al, 1999) and many others.</prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
the web is being explored not only as asuper corpus for nlp and linguistic research (kilgarriff and grefenstette, 2003) <papid> J03-3001 </papid>but also, more importantly to mt research, as treasure for mining bitexts of various language pairs (resnik, 1999; <papid> P99-1068 </papid>chen and nie, 2000; nie and cai, 2001; nie and chen, 2002; resnik and smith, 2003; <papid> J03-3002 </papid>wayand gough, 2003).<papid> J03-3004 </papid></citsent>
<aftsection>
<nextsent>the web has been the play ground for many nlpers.
</nextsent>
<nextsent>more and more web sites are found to have cloned their web pages in several languages, aiming at conveying information to audience in different languages.
</nextsent>
<nextsent>this gives rise to huge volume of wonderful bilingual or multi-lingual resources freely available from theweb for research.
</nextsent>
<nextsent>what we need to do is to harvest the right resources for the right applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1470">
<title id=" I05-1007.xml">parsing the penn chinese treebank with semantic knowledge </title>
<section> incorporating semantic knowledge.  </section>
<citcontext>
<prevsection>
<prevsent>n
</prevsent>
<prevsent>v-ambiguity: a. implement plans (incorrect parse) versus b. implementation plans (corpus) table 5.
</prevsent>
</prevsection>
<citsent citstr=" W00-1201 ">
previous results on ctb parsing for sentences of length at most 40 words lp lr f1 bikel and chiang 2000 <papid> W00-1201 </papid>77.2 76.2 76.7 levy and manning 2003 <papid> P03-1056 </papid>78.4 79.2 78.8 present work 80.1 78.7 79.4 bikel thesis 2004 81.2 78.0 79.6 chiang and bikel 2002 <papid> C02-1126 </papid>81.1 78.8 79.9 be parsed correctly without any external context.</citsent>
<aftsection>
<nextsent>furthermore, they argued that semantic preferences are helpful for the resolution of ambiguity between these two different structures.
</nextsent>
<nextsent>in our selection preference model, semantic preferences interweave with grammatical relations.
</nextsent>
<nextsent>these semantic dependencies impose constraints on the structure of the pattern n
</nextsent>
<nextsent>v+noun and therefore on the pos tag of n
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1471">
<title id=" I05-1007.xml">parsing the penn chinese treebank with semantic knowledge </title>
<section> incorporating semantic knowledge.  </section>
<citcontext>
<prevsection>
<prevsent>n
</prevsent>
<prevsent>v-ambiguity: a. implement plans (incorrect parse) versus b. implementation plans (corpus) table 5.
</prevsent>
</prevsection>
<citsent citstr=" P03-1056 ">
previous results on ctb parsing for sentences of length at most 40 words lp lr f1 bikel and chiang 2000 <papid> W00-1201 </papid>77.2 76.2 76.7 levy and manning 2003 <papid> P03-1056 </papid>78.4 79.2 78.8 present work 80.1 78.7 79.4 bikel thesis 2004 81.2 78.0 79.6 chiang and bikel 2002 <papid> C02-1126 </papid>81.1 78.8 79.9 be parsed correctly without any external context.</citsent>
<aftsection>
<nextsent>furthermore, they argued that semantic preferences are helpful for the resolution of ambiguity between these two different structures.
</nextsent>
<nextsent>in our selection preference model, semantic preferences interweave with grammatical relations.
</nextsent>
<nextsent>these semantic dependencies impose constraints on the structure of the pattern n
</nextsent>
<nextsent>v+noun and therefore on the pos tag of n
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1472">
<title id=" I05-1007.xml">parsing the penn chinese treebank with semantic knowledge </title>
<section> incorporating semantic knowledge.  </section>
<citcontext>
<prevsection>
<prevsent>n
</prevsent>
<prevsent>v-ambiguity: a. implement plans (incorrect parse) versus b. implementation plans (corpus) table 5.
</prevsent>
</prevsection>
<citsent citstr=" C02-1126 ">
previous results on ctb parsing for sentences of length at most 40 words lp lr f1 bikel and chiang 2000 <papid> W00-1201 </papid>77.2 76.2 76.7 levy and manning 2003 <papid> P03-1056 </papid>78.4 79.2 78.8 present work 80.1 78.7 79.4 bikel thesis 2004 81.2 78.0 79.6 chiang and bikel 2002 <papid> C02-1126 </papid>81.1 78.8 79.9 be parsed correctly without any external context.</citsent>
<aftsection>
<nextsent>furthermore, they argued that semantic preferences are helpful for the resolution of ambiguity between these two different structures.
</nextsent>
<nextsent>in our selection preference model, semantic preferences interweave with grammatical relations.
</nextsent>
<nextsent>these semantic dependencies impose constraints on the structure of the pattern n
</nextsent>
<nextsent>v+noun and therefore on the pos tag of n
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1473">
<title id=" I08-1062.xml">japanese spanish thesaurus construction using english as a pivot </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our approach differs in that we disambiguate the wikipedia category tree using wordnet hyper /hyponym tree.
</prevsent>
<prevsent>we compare our approach to m. ruiz et al, (2005) using it as the baseline in section 7.
</prevsent>
</prevsection>
<citsent citstr=" P98-2245 ">
oi yee kwong (1998) <papid> P98-2245 </papid>integrates different resources to construct thesaurus by using wordnet as pivot to fill gaps between thesaurus and dic tionary.</citsent>
<aftsection>
<nextsent>strube and ponzetto (2006) present some experiments using wikipedia for the computing semantic relatedness of words (a measure of degree to which two concepts are related in taxonomy measured using all semantic relations), and compare the results with wordnet.
</nextsent>
<nextsent>they also integrate google hits, in addition to wikipedia and wordnet based measures.
</nextsent>
<nextsent>first we extract from wikipedia all the aligned links i.e. wikipedia article titles.
</nextsent>
<nextsent>we map these on to wordnet to determine if word has more than one sense (polysemous) and extract the ambiguous articles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1474">
<title id=" I05-3029.xml">maximal match chinese segmentation augmented by resources generated from a very large dictionary for postprocessing </title>
<section> results and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>will override the other two.
</prevsent>
<prevsent>table 3 shows that the number of correct replacements from fmm is 399 in the as test corpus, combining the gain from the reshuffling * the reported figures differ from those computed on our platform, probably due to system differences.
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
the official scorer program is publicly available and described in (sproat and emerson, 2003).<papid> W03-1719 </papid></citsent>
<aftsection>
<nextsent>of 5-character strings, the total is 408.
</nextsent>
<nextsent>since the default choice is the bmm segmented texts, the sum 408 is the total gain from this bmm/fmm comparison, while 77 correct segmented texts have been mis-replaced, the gain/loss ratio is 5.30.
</nextsent>
<nextsent>this means that our system only loses 1 correct segmentation in exchange of gaining 5.3 correct ones.
</nextsent>
<nextsent>likewise in the case of the pk test corpus in table 4, the gain/loss ratio is 4.67.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1475">
<title id=" I08-1003.xml">a hybrid approach to the induction of underlying morphology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are basic maximum priori(map) approaches that try to maximize the probability of the lexicon against linguistically motivated priors (deligne and bimbot, 1997; snover and brent, 2001; creutz and lagus, 2005).
</prevsent>
<prevsent>an alternative tomap, mdl approaches use their own set of priors motivated by complexity theory.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
these studies attempt to minimize lexicon complexity (bit-length in crude mdl) while simultaneously minimizing the complexity (by maximizing the probability) of the corpus given the lexicon (de marcken, 1996; goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2002).<papid> W02-0603 </papid></citsent>
<aftsection>
<nextsent>many of the approaches mentioned above utilize simplistic unigram model of morphology to produce the segmentation of the corpus given the lexicon.
</nextsent>
<nextsent>sub strings in the lexicon are proposed as morphswithin word based on frequency alone, independently of phrase-, word- and morph-surroundings (de marcken, 1996; peng and schuurmans, 2001; creutz and lagus, 2002).<papid> W02-0603 </papid></nextsent>
<nextsent>there are many approaches, however, which further constrain the segmentation procedure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1476">
<title id=" I08-1003.xml">a hybrid approach to the induction of underlying morphology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are basic maximum priori(map) approaches that try to maximize the probability of the lexicon against linguistically motivated priors (deligne and bimbot, 1997; snover and brent, 2001; creutz and lagus, 2005).
</prevsent>
<prevsent>an alternative tomap, mdl approaches use their own set of priors motivated by complexity theory.
</prevsent>
</prevsection>
<citsent citstr=" W02-0603 ">
these studies attempt to minimize lexicon complexity (bit-length in crude mdl) while simultaneously minimizing the complexity (by maximizing the probability) of the corpus given the lexicon (de marcken, 1996; goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2002).<papid> W02-0603 </papid></citsent>
<aftsection>
<nextsent>many of the approaches mentioned above utilize simplistic unigram model of morphology to produce the segmentation of the corpus given the lexicon.
</nextsent>
<nextsent>sub strings in the lexicon are proposed as morphswithin word based on frequency alone, independently of phrase-, word- and morph-surroundings (de marcken, 1996; peng and schuurmans, 2001; creutz and lagus, 2002).<papid> W02-0603 </papid></nextsent>
<nextsent>there are many approaches, however, which further constrain the segmentation procedure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1480">
<title id=" I08-1003.xml">a hybrid approach to the induction of underlying morphology </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>this improvement can be quite large.
</prevsent>
<prevsent>on the morpheme-consistency measure used in the last morpho challenge, we observed an improvement of the turkish segmentation of over66% against the baseline, and 48% against the top of-the-line unsupervised approach.work in progress includes error analysis of there sults to more closely examine the contribution ofeach rule, as well as developing rule sets for additional languages.
</prevsent>
</prevsection>
<citsent citstr=" P07-1116 ">
this will help highlight various aspects of the most beneficial rules.there has been recent work on discovering allo morphic phenomena automatically (dasgupta and ng, 2007; demberg, 2007).<papid> P07-1116 </papid></citsent>
<aftsection>
<nextsent>it is hoped that our work can inform these approaches, if only by showing what variation is possible, and what is relevant to particular languages.
</nextsent>
<nextsent>for example, variation in inflectional suffixes, driven by vowel harmony and other phenomena, should be captured for language like turkish.future work involves attempting to learn broad coverage underlying morphology without the hand coded element of the current work.
</nextsent>
<nextsent>this might involve employing aspects of the most beneficial rules as variable features in rule-templates.
</nextsent>
<nextsent>it is hoped that we can start to derive underlying morphemes through processes (rules, constraints, etc) suggested by these templates, and possibly learn instantiations of templates from seed corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1481">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the best configuration uses the syntactically-constrained graph, selectional preferences computed from the corpus and page rank tie-breaking algorithm.
</prevsent>
<prevsent>we especially note good performance when disambiguating verbs with grammatically constrained links.
</prevsent>
</prevsection>
<citsent citstr=" P07-1005 ">
it has long been believed that being able to detect the correct sense of word in given context ? performing word sense disambiguation (wsd) ? will lead to improved performance of systems tackling high end applications such as machine translation (chan et al, 2007) <papid> P07-1005 </papid>and summarization(elhadad et al., 1997).<papid> J97-2001 </papid></citsent>
<aftsection>
<nextsent>in order for wsd methods to be useful,they must be robust, portable, scalable, and therefore preferably not reliant on manually tagged data.
</nextsent>
<nextsent>these desiderata have lead to an increased interest in developing unsupervised wsd methods, flexible relative to the word sense inventory, and which disambiguate all open-class words in given context as opposed to selected few.
</nextsent>
<nextsent>particularly appropriate from this point of view are graph-based methods (navigli and lapata, 2007), which map the open-class words in given context onto highly interconnected graph.
</nextsent>
<nextsent>each node in this graph represents word sense, and weighted edges will connect every pair of senses (corresponding to different words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1482">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the best configuration uses the syntactically-constrained graph, selectional preferences computed from the corpus and page rank tie-breaking algorithm.
</prevsent>
<prevsent>we especially note good performance when disambiguating verbs with grammatically constrained links.
</prevsent>
</prevsection>
<citsent citstr=" J97-2001 ">
it has long been believed that being able to detect the correct sense of word in given context ? performing word sense disambiguation (wsd) ? will lead to improved performance of systems tackling high end applications such as machine translation (chan et al, 2007) <papid> P07-1005 </papid>and summarization(elhadad et al., 1997).<papid> J97-2001 </papid></citsent>
<aftsection>
<nextsent>in order for wsd methods to be useful,they must be robust, portable, scalable, and therefore preferably not reliant on manually tagged data.
</nextsent>
<nextsent>these desiderata have lead to an increased interest in developing unsupervised wsd methods, flexible relative to the word sense inventory, and which disambiguate all open-class words in given context as opposed to selected few.
</nextsent>
<nextsent>particularly appropriate from this point of view are graph-based methods (navigli and lapata, 2007), which map the open-class words in given context onto highly interconnected graph.
</nextsent>
<nextsent>each node in this graph represents word sense, and weighted edges will connect every pair of senses (corresponding to different words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1483">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we study the impact on disambiguation performance when connections are restricted to pairs ofword senses corresponding to words that are grammatically linked in the considered context.
</prevsent>
<prevsent>the benefits of using grammatical information for automatic wsd were first explored by yarowsky(1995) and resnik (1996), in unsupervised approaches to disambiguating single words in context.sussna (1993) presents first approach to disambiguating together words within context.
</prevsent>
</prevsection>
<citsent citstr=" W98-0701 ">
the focus is on nouns, and the sense combination that minimizes the overall distance in the wordnet nouns network is chosen.stetina et al (1998) <papid> W98-0701 </papid>present the first approach, supervised, to disambiguating all words in sentence with sense association (or selectional) preferences computed from sense-tagged corpus.</citsent>
<aftsection>
<nextsent>an untagged grammatically linked word pair will have associated matrix of sense combination scores, based on the analyzed sense-tagged corpus, and similarities between the current words and those in tagged pairs with the same grammatical relation.
</nextsent>
<nextsent>once such matrices are computed for all grammatically related word pairs, the sense preferences are propagated from the bottom of the parse tree towards the top,and the sense selection starts from the top and propagates downward.mccarthy and carroll (2003) <papid> J03-4004 </papid>also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes.</nextsent>
<nextsent>in an approach inspired by the works of li and abe (1998) <papid> J98-2002 </papid>and clark and weir (2002), <papid> J02-2003 </papid>mccarthy and carroll use grammatically connected words from corpus to induce distribution of senses over subtrees in the wordnet hierarchy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1484">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the focus is on nouns, and the sense combination that minimizes the overall distance in the wordnet nouns network is chosen.stetina et al (1998) <papid> W98-0701 </papid>present the first approach, supervised, to disambiguating all words in sentence with sense association (or selectional) preferences computed from sense-tagged corpus.</prevsent>
<prevsent>an untagged grammatically linked word pair will have associated matrix of sense combination scores, based on the analyzed sense-tagged corpus, and similarities between the current words and those in tagged pairs with the same grammatical relation.</prevsent>
</prevsection>
<citsent citstr=" J03-4004 ">
once such matrices are computed for all grammatically related word pairs, the sense preferences are propagated from the bottom of the parse tree towards the top,and the sense selection starts from the top and propagates downward.mccarthy and carroll (2003) <papid> J03-4004 </papid>also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes.</citsent>
<aftsection>
<nextsent>in an approach inspired by the works of li and abe (1998) <papid> J98-2002 </papid>and clark and weir (2002), <papid> J02-2003 </papid>mccarthy and carroll use grammatically connected words from corpus to induce distribution of senses over subtrees in the wordnet hierarchy.</nextsent>
<nextsent>mccarthy et al (2004) <papid> P04-1036 </papid>use corpus and word similarities to induce ranking of word senses from an untagged corpus to be used in wsd.we build upon this previous research, and propose an unsupervised wsd method in which senses for two grammatically related words in the sentence will be connected through directed edges.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1485">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an untagged grammatically linked word pair will have associated matrix of sense combination scores, based on the analyzed sense-tagged corpus, and similarities between the current words and those in tagged pairs with the same grammatical relation.
</prevsent>
<prevsent>once such matrices are computed for all grammatically related word pairs, the sense preferences are propagated from the bottom of the parse tree towards the top,and the sense selection starts from the top and propagates downward.mccarthy and carroll (2003) <papid> J03-4004 </papid>also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes.</prevsent>
</prevsection>
<citsent citstr=" J98-2002 ">
in an approach inspired by the works of li and abe (1998) <papid> J98-2002 </papid>and clark and weir (2002), <papid> J02-2003 </papid>mccarthy and carroll use grammatically connected words from corpus to induce distribution of senses over subtrees in the wordnet hierarchy.</citsent>
<aftsection>
<nextsent>mccarthy et al (2004) <papid> P04-1036 </papid>use corpus and word similarities to induce ranking of word senses from an untagged corpus to be used in wsd.we build upon this previous research, and propose an unsupervised wsd method in which senses for two grammatically related words in the sentence will be connected through directed edges.</nextsent>
<nextsent>we experiment with graph edge weights computed using wordnet, and weights computed using grammatical collocation information from corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1486">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an untagged grammatically linked word pair will have associated matrix of sense combination scores, based on the analyzed sense-tagged corpus, and similarities between the current words and those in tagged pairs with the same grammatical relation.
</prevsent>
<prevsent>once such matrices are computed for all grammatically related word pairs, the sense preferences are propagated from the bottom of the parse tree towards the top,and the sense selection starts from the top and propagates downward.mccarthy and carroll (2003) <papid> J03-4004 </papid>also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes.</prevsent>
</prevsection>
<citsent citstr=" J02-2003 ">
in an approach inspired by the works of li and abe (1998) <papid> J98-2002 </papid>and clark and weir (2002), <papid> J02-2003 </papid>mccarthy and carroll use grammatically connected words from corpus to induce distribution of senses over subtrees in the wordnet hierarchy.</citsent>
<aftsection>
<nextsent>mccarthy et al (2004) <papid> P04-1036 </papid>use corpus and word similarities to induce ranking of word senses from an untagged corpus to be used in wsd.we build upon this previous research, and propose an unsupervised wsd method in which senses for two grammatically related words in the sentence will be connected through directed edges.</nextsent>
<nextsent>we experiment with graph edge weights computed using wordnet, and weights computed using grammatical collocation information from corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1487">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>once such matrices are computed for all grammatically related word pairs, the sense preferences are propagated from the bottom of the parse tree towards the top,and the sense selection starts from the top and propagates downward.mccarthy and carroll (2003) <papid> J03-4004 </papid>also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes.</prevsent>
<prevsent>in an approach inspired by the works of li and abe (1998) <papid> J98-2002 </papid>and clark and weir (2002), <papid> J02-2003 </papid>mccarthy and carroll use grammatically connected words from corpus to induce distribution of senses over subtrees in the wordnet hierarchy.</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
mccarthy et al (2004) <papid> P04-1036 </papid>use corpus and word similarities to induce ranking of word senses from an untagged corpus to be used in wsd.we build upon this previous research, and propose an unsupervised wsd method in which senses for two grammatically related words in the sentence will be connected through directed edges.</citsent>
<aftsection>
<nextsent>we experiment with graph edge weights computed using wordnet, and weights computed using grammatical collocation information from corpus.
</nextsent>
<nextsent>these 757 weights are used to induce an initial scoring of the graph vertices, starting from the leaves and propagating upwards.
</nextsent>
<nextsent>the disambiguation process starts with choosing sense for the head of the sentence,and moves towards the leaves, propagating downward the chosen senses at each step, and using the edge weights and vertex scores to guide the sense selection process.
</nextsent>
<nextsent>we investigate two issues: (i) whether using in disambiguation only syntactically connected words leads to results on par with, or better than, using allword-sense combinations, (ii) whether sense association strength induced from sense-unlabeled corpus can rival relatedness measures induced from lexical resource - in our case, wordnet.we evaluate this approach on the senseval 2(palmer et al, 2001) and senseval-3(snyder and palmer, 2004) english all-words test data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1489">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> disambiguation algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>selectional preferences are estimated using grammatical collocation information from the british national corpus (bnc), obtained with the word sketch engine (wse) (kilgarriff et al, 2004).
</prevsent>
<prevsent>2.1 extracting grammatical relation.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
information we parse the senseval test data using the stanford parser(klein and manning, 2003) <papid> P03-1054 </papid>generating the output in dependency relation format (de marneffe et al, 2006).</citsent>
<aftsection>
<nextsent>edges that do not connect open-class words are filtered out, words are lemmatized, and we reintroduce the copula (it is bypassed as predicate) because the verb be must be disambiguated as well.to estimate selectional preferences from sense untagged corpus, for each grammatically related pairof words in sentence we extract evidence consist dependency relation wse relation nsubj(verb,noun) subject(verb,noun) subject of(noun,verb) dobj(verb,noun) object(verb,noun) object of(noun,verb) amod(noun,adj) modifier(noun,adj) modifies(adj,noun) nn(noun1,noun2) modifier(noun1,noun2)modifies(noun2,noun1)prep of(verb,noun) pp of(verb,noun) pp-obj of(noun,verb) table 1: mapping of grammatical relations from the stanford parser onto the wse relation set ? sample.
</nextsent>
<nextsent>ing of pairs with the same grammatical relation and either the same head or dependent, using the word sketch engine.
</nextsent>
<nextsent>to obtain such pairs we map the grammatical relations used by the stanford parser onto the set of grammatical relations used by the wse.
</nextsent>
<nextsent>table 1 shows sample of this mapping.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1490">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> disambiguation algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>sum of these relatedness scores: as w2 w1|gr = ? wxgcgrw1 ? sjwxswx rel(siw2 , j wx) where swx is the set of senses for word wx.
</prevsent>
<prevsent>if this value is 0, then as w2 w1|gr = 1 nw2 , where nw2 is the number of senses of w2.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
rel(siw2 , sjwx) can be computed as similarity or relatedness measure (budanitsky and hirst, 2006).<papid> J06-1003 </papid></citsent>
<aftsection>
<nextsent>because the sense inventory for the senseval data comes from wordnet and we work at the sense level, we use relatedness measures based on wordnet, as opposed to corpus-based ones.
</nextsent>
<nextsent>in the experiments presented further in the paper, we have used relatedness measure based on hypernym and hyponym information, in the following manner: rel(siw2 , j wx) = ? ?
</nextsent>
<nextsent>1 : siw2 is hypernym of sjwx 1 : siw2 is hyponym of sjwxand path length(siw2 , sjwx) ? 2 1 : siw2 similar to/antonym of sjwx0 : otherwise in other words, if the sense siw2 of w2 is hy pernym of the sense sjwx or close hyponym (dis-tance at most 2) or connected through similar to/antonym of relation, we consider the two senses related and relatedness gets score of 1.
</nextsent>
<nextsent>otherwise, we consider the two senses unrelated.the motivation for using this relatedness measure is that it allows fast computations ? essential when dealing with large amount of information from corpus ? and it clusters closely related senses based on wordnets hypernym/hyponym relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1491">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> disambiguation algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>because at this point it is not determined to whichof wxs senses the selectional preference is due, allof wxs senses will have the same selectional preference to sense of wy: as wy siwx |gr = as wy wx|gr, for all senses siwx of wx.
</prevsent>
<prevsent>sense-selectional preferences based on lexical resource when using the lexical resource, because we have pairs that connect words under different parts of speech, we opt for lesk-based measure (banerjee and pedersen, 2003).
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
relatedness scores are computed for each pair of senses of the grammatically linked pair of words (w1, w2, gr), using the wordnet-similarity-1.03 package and the lesk 759option (pedersen et al, 2004).<papid> N04-3012 </papid></citsent>
<aftsection>
<nextsent>to maintain the notation from above, we denote by asiwx sjwythe lesk relatedness score between sense of wx and sense of wy. these scores are symmetric: as wx sjwy = as wy siwx , and independent of grammatical relations gr.
</nextsent>
<nextsent>2.3 the sense-enhanced dependency tree.
</nextsent>
<nextsent>after computing the sense association strength scores for w1 and w2 in grammatical relation grin the sentence, we expand the edge (wx, wy, gr)from the dependency tree to the two sets of directed edges: {(siwx ? sjwy , gr)|i = 1, n; = 1, m}, {(sjwy ? siwx , gr1)|i = 1, n; = 1, m}.
</nextsent>
<nextsent>the weight of an edge (siwx ? sjwy , gr) is as wy siwx |gr . figure 2 shows one sense-enhanced edge..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1492">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> disambiguation algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>score(sjwy ) = score(s wy ) + sjwy siwx |gr1 where (wx, wy, gr) ((wy, wx, gr1)) is in the current sentence.because of data sparseness, there may be not enough evidence in the corpus to produce clear winner, and several senses are tied.
</prevsent>
<prevsent>all senses arethen kept, and disambiguation proceeds further.
</prevsent>
</prevsection>
<citsent citstr=" H05-1052 ">
if more than one word has multiple senses left after the top-down traversal of the tree, we use two meth ods: random choosing from the tied senses or the sequence labeling method described in (mihalcea,2005).<papid> H05-1052 </papid></citsent>
<aftsection>
<nextsent>the graphs vertices are the senses that remain to be disambiguated, and its edges connect every pair of these senses (provided that they correspond to different words).
</nextsent>
<nextsent>the score of each vertex is initially set to 1, and the edge weights are lesk similarity scores.
</nextsent>
<nextsent>the vertices are scored using apage rank algorithm, in which the rank at every iteration step is computed with the formula: wp (a) = (1?
</nextsent>
<nextsent>d) + ? bin(a) wba ? cout(b) wbc wp (b) where: a, b, are vertices in the graph; wp (a) is the weighted page rank score of node a; is the probability that there will be jump from given vertex to another in the graph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1497">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>many sense selection preferences are 0.
</prevsent>
<prevsent>in order to improve this approach, we will look into more flexible methods for computing dependency pair similarities (without fixing one ofthe vertices as we did in this paper).
</prevsent>
</prevsection>
<citsent citstr=" P07-1028 ">
previous research in inducing sense rankings from an untagged corpus (mccarthy et al, 2004), <papid> P04-1036 </papid>and inducing selectional preferences at the word level (for other appli cations) (erk, 2007) <papid> P07-1028 </papid>will provide the starting point for research in this direction.</citsent>
<aftsection>
<nextsent>the most similar approach to the one we describe,that has been tested on senseval-2, is the one described in (mccarthy and carroll, 2003).<papid> J03-4004 </papid></nextsent>
<nextsent>the best results reported are 51.1% precision and 23.2% recall.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1503">
<title id=" I08-2105.xml">unsupervised all words word sense disambiguation with grammatical dependencies </title>
<section> comparison with related work.  </section>
<citcontext>
<prevsection>
<prevsent>mccarthy et al (2004) <papid> P04-1036 </papid>report disambiguation precision of 53.0% and recall of 49.0% on the senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in corpus.</prevsent>
<prevsent>mihalcea (2005) <papid> H05-1052 </papid>reports the highest results on thesenseval-2 data obtained with graph-based algorithm ? 54.2% precision and recall.</prevsent>
</prevsection>
<citsent citstr=" W04-0856 ">
the results obtained with page rank algorithm applied to sense graph built from words within context of given 761size are also the highest for completely unsupervised wsd6 system in senseval-2.the best result obtained by an unsupervised system on the senseval-3 data is reported by strapparava et al (2004) ? <papid> W04-0856 </papid>58.3%.</citsent>
<aftsection>
<nextsent>this implementation uses wordnet-domains, version of wordnet enhanced with domain information (e.g. economy, geography).
</nextsent>
<nextsent>the domain of given text is automatically detected, and this information will constrain the possible senses of words in the given text.
</nextsent>
<nextsent>for senseval 3 data, using graph method with the key player problem to measure vertex relevance, navigli and lapata (2007) report very close results to (strapparava et al, 2004) <papid> W04-0856 </papid>on nouns and adjectives, and lower scores for verbs (f1-scores: 61.9% for nouns, 62.8% for adjectives, 36.1% for verbs compared with 62.2% for nouns, 66.9% for adjectives,50.4% for verbs).</nextsent>
<nextsent>mihalcea (2005) <papid> H05-1052 </papid>reports an over all score of 52.2% for this data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1507">
<title id=" I05-6001.xml">the tiger 700 rmrs bank rmrs construction from dependencies </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>we present treebank conversion method by which we construct anrmrs bank for hpsg parser evaluation from the tiger dependencybank.
</prevsent>
</prevsection>
<citsent citstr=" P01-1019 ">
our method effectively performs automatic rmrs semantics construction from functional dependencies, following the semantic algebra of copestake et al (2001).<papid> P01-1019 </papid></citsent>
<aftsection>
<nextsent>we present the semantics construction mechanism, and focus on some special phenomena.
</nextsent>
<nextsent>automatic conversion is followed by manual validation.
</nextsent>
<nextsent>first evaluation results yield high precision of the automatic semantics construction rules.
</nextsent>
<nextsent>treebanks are underdevelopment for many languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1509">
<title id=" I05-6001.xml">the tiger 700 rmrs bank rmrs construction from dependencies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent joint initiative, the tiger project provides dependency-based treebank representations for german, on the basis of the tiger treebank (brants et al, 2002).
</prevsent>
<prevsent>forst (2003) applied treebank conversion methods to the tiger treebank, to derive an f-structure bank for stochastic training and evaluation of german lfg parser.
</prevsent>
</prevsection>
<citsent citstr=" W04-1905 ">
a more theory-neutral dependency representation is currently derived from this tiger-lfg tree bank for cross-framework parser evaluation(forst et al, 2004).<papid> W04-1905 </papid></citsent>
<aftsection>
<nextsent>however, while penn treebank style grammars and lfg analyses are relatively close to dependency representations, the output of hpsg parsing is difficult to match against such structures.
</nextsent>
<nextsent>hpsg analyses do not come with an explicit representation of functional structure, but directly encode semantic structures, in terms of (robust) minimal recur sion semantics (hence forth (r)mrs.1 this leaves gap to be bridged in terms of the encoding of arguments vs. adjuncts, the representation of special constructions like relative clauses, and not least, the representation of quantifiers and their (underspecified) scoping relations.
</nextsent>
<nextsent>in order to bridge this gap, we construct an rmrs treebank?
</nextsent>
<nextsent>from subset of the tiger dependency bank (forst et al, 2004), <papid> W04-1905 </papid>which can serve as gold standard for hpsg parsing for evaluation, and for training of stochastic hpsg grammar models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1537">
<title id=" I05-6001.xml">the tiger 700 rmrs bank rmrs construction from dependencies </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>by applying this method to the tiger dependency bank, we construct an rmrsbank that allows cross-framework parser evaluation for german.
</prevsent>
<prevsent>our method for rmrs construction can be transposed to dependency banks for other languages, such as the parc 700 dependency bank for english (king et. al., 2003).
</prevsent>
</prevsection>
<citsent citstr=" C02-1013 ">
the choice of rmrs also ensures that the semantic bank can be used for comparative evaluation of hpsg grammars withlow-level parsers that output partial semantics in terms of rmrs, such as the rasp parser of carroll and briscoe (2002).<papid> C02-1013 </papid>while the formalism of (r)mrs has its origins in hpsg, we have shown that rmrs semantics construction can be carried over to dependency-based frameworks like lfg.</citsent>
<aftsection>
<nextsent>in future research, we will investigate how the semantic algebra of copestake et al (2001) <papid> P01-1019 </papid>9 compares to glue semantics (dalrymple, 1999).</nextsent>
<nextsent>our construction rules may in fact be modified and extended to yield semantics construction along the lines of glue semantics, with hooks as resources and rels, hconsand ing sets as meaning language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1543">
<title id=" I08-2079.xml">automatic rule acquisition for chinese intra chunk relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many computational linguists regard chunking as shallow parsing technique.
</prevsent>
<prevsent>due to its efficiency and robustness on non-restricted texts, it has become an interesting alternative to full parsing in many nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
on the base of the chunk scheme proposed by abney (1991) and the bio tagging system proposed in ramshaw and marcus(1995), <papid> W95-0107 </papid>many machine learning techniques are used to deal with the problem.</citsent>
<aftsection>
<nextsent>however, almost all the chunking systems focus on the recognition of non-overlapping cores of chunks till now, none of them care about the internal structure analysis of chunks.
</nextsent>
<nextsent>in our opinion, the internal structure of chunk, including its head and the dependency relation between head and other components, plays an important role for semantic content understanding for the chunk.
</nextsent>
<nextsent>they are especially useful for the languages with few morphological inflections, such as the chinese language.
</nextsent>
<nextsent>therefore, we design multiword chunking task to recognize different multiword chunks (mwcs) with the detailed descriptions of external function and internal structure in real texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1544">
<title id=" I05-2022.xml">hmm based chunker for hindi </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>chunking has been studied for english and other languages, though not very extensively.
</prevsent>
<prevsent>the earliest work on chunking based on machine learning goes to (church k, 1988) for english.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
(ramshaw and marcus, 1995) <papid> W95-0107 </papid>used transformation based learning using large annotated corpus for en glish.</citsent>
<aftsection>
<nextsent>(skut and brants, 1998) modified churchs approach, and used standard hmm based tagging methods to model the chunking process.
</nextsent>
<nextsent>(zhou,et al., 2000) <papid> W00-0737 </papid>continued using the same methods, and achieved an accuracy of 91.99% precision and 92.25% recall using contextual lexicon.(kudo and matsumoto, 2001) <papid> N01-1025 </papid>use support vec 126 tor machines for chunking with 93.48% accuracy for english.</nextsent>
<nextsent>(veenstra and bosch, 2000) <papid> W00-0735 </papid>use memory based phrase chunking with accuracy of 91.05% precision and 92.03% recall for english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1545">
<title id=" I05-2022.xml">hmm based chunker for hindi </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(ramshaw and marcus, 1995) <papid> W95-0107 </papid>used transformation based learning using large annotated corpus for en glish.</prevsent>
<prevsent>(skut and brants, 1998) modified churchs approach, and used standard hmm based tagging methods to model the chunking process.</prevsent>
</prevsection>
<citsent citstr=" W00-0737 ">
(zhou,et al., 2000) <papid> W00-0737 </papid>continued using the same methods, and achieved an accuracy of 91.99% precision and 92.25% recall using contextual lexicon.(kudo and matsumoto, 2001) <papid> N01-1025 </papid>use support vec 126 tor machines for chunking with 93.48% accuracy for english.</citsent>
<aftsection>
<nextsent>(veenstra and bosch, 2000) <papid> W00-0735 </papid>use memory based phrase chunking with accuracy of 91.05% precision and 92.03% recall for english.</nextsent>
<nextsent>(osborne, 2000) <papid> W00-0731 </papid>experimented with various sets of features for the purpose of shallow parsing.in this work, we have used hmm based chunking.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1546">
<title id=" I05-2022.xml">hmm based chunker for hindi </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(ramshaw and marcus, 1995) <papid> W95-0107 </papid>used transformation based learning using large annotated corpus for en glish.</prevsent>
<prevsent>(skut and brants, 1998) modified churchs approach, and used standard hmm based tagging methods to model the chunking process.</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
(zhou,et al., 2000) <papid> W00-0737 </papid>continued using the same methods, and achieved an accuracy of 91.99% precision and 92.25% recall using contextual lexicon.(kudo and matsumoto, 2001) <papid> N01-1025 </papid>use support vec 126 tor machines for chunking with 93.48% accuracy for english.</citsent>
<aftsection>
<nextsent>(veenstra and bosch, 2000) <papid> W00-0735 </papid>use memory based phrase chunking with accuracy of 91.05% precision and 92.03% recall for english.</nextsent>
<nextsent>(osborne, 2000) <papid> W00-0731 </papid>experimented with various sets of features for the purpose of shallow parsing.in this work, we have used hmm based chunking.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1547">
<title id=" I05-2022.xml">hmm based chunker for hindi </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(skut and brants, 1998) modified churchs approach, and used standard hmm based tagging methods to model the chunking process.
</prevsent>
<prevsent>(zhou,et al., 2000) <papid> W00-0737 </papid>continued using the same methods, and achieved an accuracy of 91.99% precision and 92.25% recall using contextual lexicon.(kudo and matsumoto, 2001) <papid> N01-1025 </papid>use support vec 126 tor machines for chunking with 93.48% accuracy for english.</prevsent>
</prevsection>
<citsent citstr=" W00-0735 ">
(veenstra and bosch, 2000) <papid> W00-0735 </papid>use memory based phrase chunking with accuracy of 91.05% precision and 92.03% recall for english.</citsent>
<aftsection>
<nextsent>(osborne, 2000) <papid> W00-0731 </papid>experimented with various sets of features for the purpose of shallow parsing.in this work, we have used hmm based chunking.</nextsent>
<nextsent>we report on number of experiments showing the effect of different encoding methods on accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1548">
<title id=" I05-2022.xml">hmm based chunker for hindi </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(zhou,et al., 2000) <papid> W00-0737 </papid>continued using the same methods, and achieved an accuracy of 91.99% precision and 92.25% recall using contextual lexicon.(kudo and matsumoto, 2001) <papid> N01-1025 </papid>use support vec 126 tor machines for chunking with 93.48% accuracy for english.</prevsent>
<prevsent>(veenstra and bosch, 2000) <papid> W00-0735 </papid>use memory based phrase chunking with accuracy of 91.05% precision and 92.03% recall for english.</prevsent>
</prevsection>
<citsent citstr=" W00-0731 ">
(osborne, 2000) <papid> W00-0731 </papid>experimented with various sets of features for the purpose of shallow parsing.in this work, we have used hmm based chunking.</citsent>
<aftsection>
<nextsent>we report on number of experiments showing the effect of different encoding methods on accuracy.
</nextsent>
<nextsent>different encodings of the input show the effect of including either words only, pos tags only, or combination thereof, in training.
</nextsent>
<nextsent>their effect on transition probabilities is also studied.
</nextsent>
<nextsent>we do not use any externally supplied lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1550">
<title id=" I05-2022.xml">hmm based chunker for hindi </title>
<section> initial experiments.  </section>
<citcontext>
<prevsection>
<prevsent>, ci+k1) (2)where the probabilities in the first term are emission probabilities and in the second term are transition probabilities.
</prevsent>
<prevsent>the optimal sequence ofchunk tags can be found using the viterbi algorithm.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
for training and testing of hmm we have used the tnt system (brants, 2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>since tnt is implemented up to tri-gram model, we use second order hmm (k = 2) in our study.
</nextsent>
<nextsent>before discussing the possible chunk sets and the token sets, we consider an example below.
</nextsent>
<nextsent>(( sher )) (( hiran ke piche )) lion deer of behind nn nn prep prep strt strt cnt cnt (( jangal mem )) (( bhaga . )) forest in ran . nn prep vb sym strt cnt strt cnt in this example, the chunk tags considered are strt and cnt where strt indicates that the new chunk starts at the token which is assigned this tag and cnt indicated that the token whichis assigned this tag is inside the chunk.
</nextsent>
<nextsent>we refer to this as 2-tag scheme.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1551">
<title id=" I05-6007.xml">syntactic identification of attribution in the rst treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there has been growing interest in recent years in discourse structure.
</prevsent>
<prevsent>a prominent example of this is the rst treebank (carlson et al, 2002),which imposes hierarchical structures on multi sentence discourses.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
since the texts in the rsttreebank are taken from the syntactically annotated penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>it is natural to ask what the relation is between the discourse structures in the rst treebank and the syntactic structures of the penn treebank.</citsent>
<aftsection>
<nextsent>in our view, the most natural relationship would be that discourse structures always relatewell-formed syntactic expressions, typically sentences.
</nextsent>
<nextsent>discourse trees would then be seen as elaborations of syntactic trees, adding relations between sentential nodes that are not linked by syntactic relations.
</nextsent>
<nextsent>this would allow discourse structures and syntactic structures to coexist in combined hierarchical structure.
</nextsent>
<nextsent>surprisingly, this is not what we have found in examining the syntax-discourse relation in therst treebank.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1552">
<title id=" I05-6007.xml">syntactic identification of attribution in the rst treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our hypothesis is that the sub sentential relations in the rst treebank arein fact redundant; if this is true it should be possible to automatically infer these relations based solely on penn treebank syntactic information.
</prevsent>
<prevsent>in this paper, we present the results of an initial study that strongly supports our hypothesis.
</prevsent>
</prevsection>
<citsent citstr=" W99-0307 ">
we examine the attribution relation, which is of particular interest for the following reasons: it appears quite frequently in the rst tree bank (15% of all relations, according to marcu et al (1999))  <papid> W99-0307 </papid>it always appears within, rather than across, sentence boundaries it conflicts with penn treebank syntax, always relating text spans that do not correspond to nodes in the syntax treewe describe system that identifies attributions by simple, clearly defined syntactic features.</citsent>
<aftsection>
<nextsent>1in the training portion of the rst treebank, we found 17213 elementary discourse units (edus).
</nextsent>
<nextsent>of these only 6068 occurred at sentence boundaries.
</nextsent>
<nextsent>57 this system identifies rst attributions within precision and recall over 90%.
</nextsent>
<nextsent>in our view, this strongly supports the view that attribution is in fact syntactic relation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1553">
<title id=" I05-6007.xml">syntactic identification of attribution in the rst treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>57 this system identifies rst attributions within precision and recall over 90%.
</prevsent>
<prevsent>in our view, this strongly supports the view that attribution is in fact syntactic relation.
</prevsent>
</prevsection>
<citsent citstr=" N03-1030 ">
the system performs dramatically better than the results reported in(soricut and marcu, 2003) <papid> N03-1030 </papid>for automatic identification of such relations, where the precision and recall were reported at below .76.</citsent>
<aftsection>
<nextsent>furthermore, human annotator agreement reported in the rst treebank project is also well below our results,with reported f-scores no higher than .77.
</nextsent>
<nextsent>(sori cut and marcu, 2003) <papid> N03-1030 </papid>in what follows, we first describe attributions as they are understood in the rst treebankproject.</nextsent>
<nextsent>next we present the attribution identification procedure, followed by presentation of results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1563">
<title id=" I05-6007.xml">syntactic identification of attribution in the rst treebank </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>our approach to identifying attributions is direct implementation of the description in the rst treebank tagging manual.
</prevsent>
<prevsent>for inter-sentential relations such as contrast or explanation evidence, the situation is quite different.
</prevsent>
</prevsection>
<citsent citstr=" P02-1047 ">
syntactic criteria are relevant, but clearly not decisive, as can be observed in (marcu and echihabi, 2002).<papid> P02-1047 </papid></citsent>
<aftsection>
<nextsent>finally, the elimination of intra sentential relations like attribution would appear to be more in line with the original vision behindrst; for example, according to mann &amp; thompson (1988), the basic unit for rst relations is the clause.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1564">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>senseval focuses on the evaluation of standalone, generic wsd models, even though many application-specific systemsmachinetranslation, information retrieval, and so onall perform wsd either explicitly or implicitly.since the senseval models have been built and optimized specifically to address the wsd problems, they typically use richer disambiguating information than smt systems.
</prevsent>
<prevsent>this, however, raises the question of whether the sophisticated wsd models are in fact needed in practice.in many machine translation architectures, in particular most current statistical machine translation (smt) models, the wsd problem is typically not explicitlyaddressed.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
however, recent progress in machine translation and the continuous improvement on evaluation metrics such as bleu (papineni et al, 2002) <papid> P02-1040 </papid>suggest that smt systems are already very good at choosing correct word translations.</citsent>
<aftsection>
<nextsent>bleu score with low ordern-grams can be seen as an evaluation of the translation adequacy, which suggests that as smt systems achieve higher bleu score, their ability to disambiguate word translations improves.in other work, we have been conducting comparative studies testing whether state-of-the-art wsd mod 120 els can improve smt translation quality (carpuat and wu, 2005).<papid> P05-1048 </papid></nextsent>
<nextsent>using state-of-the-art chinese word sense disambiguation model to choose translation candidates for typical ibm statistical mt system, we found that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1565">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this, however, raises the question of whether the sophisticated wsd models are in fact needed in practice.in many machine translation architectures, in particular most current statistical machine translation (smt) models, the wsd problem is typically not explicitlyaddressed.
</prevsent>
<prevsent>however, recent progress in machine translation and the continuous improvement on evaluation metrics such as bleu (papineni et al, 2002) <papid> P02-1040 </papid>suggest that smt systems are already very good at choosing correct word translations.</prevsent>
</prevsection>
<citsent citstr=" P05-1048 ">
bleu score with low ordern-grams can be seen as an evaluation of the translation adequacy, which suggests that as smt systems achieve higher bleu score, their ability to disambiguate word translations improves.in other work, we have been conducting comparative studies testing whether state-of-the-art wsd mod 120 els can improve smt translation quality (carpuat and wu, 2005).<papid> P05-1048 </papid></citsent>
<aftsection>
<nextsent>using state-of-the-art chinese word sense disambiguation model to choose translation candidates for typical ibm statistical mt system, we found that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone.
</nextsent>
<nextsent>the surprising difficulty of this challenge might suggest that smt models are sufficiently strong at word level disambiguation on their own, and has recently encouraged speculation that smt performs wsd as well as the dedicated wsd models.the studies described in this paper are aimed at directly testing this increasingly common speculation.the comparison of smt and wsd strengths is not ob vious; there are strong arguments in support of boththe wsd and the smt models.
</nextsent>
<nextsent>a controlled empirical comparison is therefore needed to better assess the strengths and weaknesses of each type of model on the wsd task.
</nextsent>
<nextsent>we therefore propose to evaluate statistical machine translation models on wsd task, in terms of standard wsd accuracy metrics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1567">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> the smt system.  </section>
<citcontext>
<prevsection>
<prevsent>to build representative baseline smt system, we restricted ourselves to making use of freely available tools.
</prevsent>
<prevsent>3.1 alignment model.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the alignment model was trained with giza++ (och and ney, 2003), <papid> J03-1002 </papid>which implements the most typical ibm and hmm alignment models.</citsent>
<aftsection>
<nextsent>translation quality could be improved using more advanced hybrid phrasal or tree models, but this would interfere with the questions being investigated here.
</nextsent>
<nextsent>the alignment model used is ibm-4, as required by our decoder.
</nextsent>
<nextsent>the training scheme is ibm-1, hmm, ibm-3 and ibm-4, as specified in (och and ney, 2003).<papid> J03-1002 </papid>the training corpus consists of about 1 million sentences from the united nations chinese-english parallel corpus from ldc.</nextsent>
<nextsent>this corpus was automatically sentence-aligned, so the training data does not require as much manual annotation as for the wsd model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1569">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> the smt system.  </section>
<citcontext>
<prevsection>
<prevsent>the language model is also trained using publicly available software, the cmu-cambridge statistical language modeling toolkit (clarkson and rosenfeld, 1997).
</prevsent>
<prevsent>3.3 decoding.
</prevsent>
</prevsection>
<citsent citstr=" N03-1010 ">
the isi rewrite decoder (germann, 2003), <papid> N03-1010 </papid>which implements an efficient greedy decoding algorithm, is used to translate the chinese sentences, using the alignment model and language model previously de scribed.</citsent>
<aftsection>
<nextsent>notice that very little contextual information is available to the ibm smt models.
</nextsent>
<nextsent>lexical choice during decoding essentially depends on the translation probabilities learned for the target word, and on the english language model scores.
</nextsent>
<nextsent>the wsd system used here is based on the model that achieved the best performance on the senseval-3 chinese lexical sample task, outperforming other systems by large margin (carpuat et al, 2004).<papid> W04-0822 </papid></nextsent>
<nextsent>the model consists of an ensemble of four highly accurate classifiers combined by majority vote: naive bayes classifier, maximum entropy model (jaynes, 1978), boosting model (freund and schapire, 1997), and kernel pca-based model (wu et al, 2004),<papid> P04-1081 </papid>which has the advantage of having signficantly different bias.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1570">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> the wsd system.  </section>
<citcontext>
<prevsection>
<prevsent>notice that very little contextual information is available to the ibm smt models.
</prevsent>
<prevsent>lexical choice during decoding essentially depends on the translation probabilities learned for the target word, and on the english language model scores.
</prevsent>
</prevsection>
<citsent citstr=" W04-0822 ">
the wsd system used here is based on the model that achieved the best performance on the senseval-3 chinese lexical sample task, outperforming other systems by large margin (carpuat et al, 2004).<papid> W04-0822 </papid></citsent>
<aftsection>
<nextsent>the model consists of an ensemble of four highly accurate classifiers combined by majority vote: naive bayes classifier, maximum entropy model (jaynes, 1978), boosting model (freund and schapire, 1997), and kernel pca-based model (wu et al, 2004),<papid> P04-1081 </papid>which has the advantage of having signficantly different bias.</nextsent>
<nextsent>all these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1571">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> the wsd system.  </section>
<citcontext>
<prevsection>
<prevsent>lexical choice during decoding essentially depends on the translation probabilities learned for the target word, and on the english language model scores.
</prevsent>
<prevsent>the wsd system used here is based on the model that achieved the best performance on the senseval-3 chinese lexical sample task, outperforming other systems by large margin (carpuat et al, 2004).<papid> W04-0822 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1081 ">
the model consists of an ensemble of four highly accurate classifiers combined by majority vote: naive bayes classifier, maximum entropy model (jaynes, 1978), boosting model (freund and schapire, 1997), and kernel pca-based model (wu et al, 2004),<papid> P04-1081 </papid>which has the advantage of having signficantly different bias.</citsent>
<aftsection>
<nextsent>all these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant.
</nextsent>
<nextsent>moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent.
</nextsent>
<nextsent>the feature set used consists of position-sensitive, syntactic, and local collocational features, as described by yarowsky and florian (2002).
</nextsent>
<nextsent>5.1 senseval-3 chinese lexical sample task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1572">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> experimental method.  </section>
<citcontext>
<prevsection>
<prevsent>in order to evaluate the predictions of the smt model just like any wsd model, we need to map the english translations to hownet senses.
</prevsent>
<prevsent>this mapping is done using hownet, which provides english glosses for each of the senses of every chinese word.
</prevsent>
</prevsection>
<citsent citstr=" W04-0802 ">
note that senseval-3 also defined translation or multilingual lexical sample task (chklovski et al, 2004), <papid> W04-0802 </papid>which is just like the english lexical sample task, except that the wsd systems are expected to predict hindi translations instead of wordnet senses.</citsent>
<aftsection>
<nextsent>this translation task might seem to be more natural evaluation framework for smt than the monolingual chinese lexical sample task.
</nextsent>
<nextsent>however, in practice,there is very little data available to train an englishto-hindi smt model, which would significantly hinder its performance and bias the study in favor of the dedicated wsd models.
</nextsent>
<nextsent>5.2 allowing the smt model to exploit the.
</nextsent>
<nextsent>senseval data comparing the senseval wsd models with regular smt model is not entirely fair, since, unlike the smt model, the dedicated wsd models are trained and evaluated on similar data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1574">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>6.4 dedicated unsupervised wsd model also outperforms smtone might speculate that the difference in performance obtained with smt vs. wsd models can be explained by the fact that we are essentially comparing unsupervised models with fully supervised models.
</prevsent>
<prevsent>to address this we can again take advantage of the senseval framework, and compare the performance of our smt system with other published results on the same dataset.
</prevsent>
</prevsection>
<citsent citstr=" W04-0821 ">
the system described in (cabezas etal., 2004) <papid> W04-0821 </papid>is of particular interest as it uses an unsupervised approach.</citsent>
<aftsection>
<nextsent>an unsupervised chinese-english bilingual wsd model is learned from automatically word-aligned parallel corpora.
</nextsent>
<nextsent>in order to use this bilingual model, the chinese lexical sample task is artificially converted into bilingual task, by automatically translating the chinese test sentences into english, using an alignment-template based smt system.
</nextsent>
<nextsent>this unsupervised, but dedicated, wsd model yields an accuracy of 44.5%, thus outperforming allthe smt model variations.
</nextsent>
<nextsent>it yields 35% relative improvement over the best smt model, which remains relatively little compared to the best supervised dedicated wsd system, which doubles the accuracy score of the smt model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1576">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, wer does not isolate wsd performance since it also encompasses many other types of errors.
</prevsent>
<prevsent>also, since the choice ofa translation for particular word affects the translation of other words in the sentence, the effect of wsd performance on wer is unclear.
</prevsent>
</prevsection>
<citsent citstr=" W04-0847 ">
in contrast, the senseval accuracy metric counts each incorrect translation choice only once.apart from the voted wsd system described in section 4, and the unsupervised system (cabezas et al,2004) <papid> W04-0821 </papid>mentioned in section 6.4, systems built and optimized for the senseval-3 chinese lexical sample task, include niu et al (2004).<papid> W04-0847 </papid></citsent>
<aftsection>
<nextsent>many of the senseval-type 124wsd system are not language specific and the presentation of the results in the english lexical sample task (midhalcea et al, 2004), english-hindi multilingual task (chklovski et al, 2004), <papid> W04-0802 </papid>or any of the lexical sample tasks defined in other languages, give good overview of the variety of approaches to wsd.most previous work on multilingual wsd has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full mt systems) to help wsd.</nextsent>
<nextsent>for instance, ng et al (2003) <papid> P03-1058 </papid>showed that it is possible to use word aligned parallel corpora to train accurate supervised wsd models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1579">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, the senseval accuracy metric counts each incorrect translation choice only once.apart from the voted wsd system described in section 4, and the unsupervised system (cabezas et al,2004) <papid> W04-0821 </papid>mentioned in section 6.4, systems built and optimized for the senseval-3 chinese lexical sample task, include niu et al (2004).<papid> W04-0847 </papid></prevsent>
<prevsent>many of the senseval-type 124wsd system are not language specific and the presentation of the results in the english lexical sample task (midhalcea et al, 2004), english-hindi multilingual task (chklovski et al, 2004), <papid> W04-0802 </papid>or any of the lexical sample tasks defined in other languages, give good overview of the variety of approaches to wsd.most previous work on multilingual wsd has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full mt systems) to help wsd.</prevsent>
</prevsection>
<citsent citstr=" P03-1058 ">
for instance, ng et al (2003) <papid> P03-1058 </papid>showed that it is possible to use word aligned parallel corpora to train accurate supervised wsd models.</citsent>
<aftsection>
<nextsent>other work includes li and li (2002) <papid> P02-1044 </papid>who propose bilingual bootstrapping method to learn translation disambiguation wsd model, and diab (2004) <papid> P04-1039 </papid>who exploited large amounts of automatically generated noisy parallel data to learn wsd models in an unsupervised bootstrapping scheme.</nextsent>
<nextsent>in all this work, the goal is to achieve accurate wsd with minimum amounts of annotated data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1580">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>many of the senseval-type 124wsd system are not language specific and the presentation of the results in the english lexical sample task (midhalcea et al, 2004), english-hindi multilingual task (chklovski et al, 2004), <papid> W04-0802 </papid>or any of the lexical sample tasks defined in other languages, give good overview of the variety of approaches to wsd.most previous work on multilingual wsd has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full mt systems) to help wsd.</prevsent>
<prevsent>for instance, ng et al (2003) <papid> P03-1058 </papid>showed that it is possible to use word aligned parallel corpora to train accurate supervised wsd models.</prevsent>
</prevsection>
<citsent citstr=" P02-1044 ">
other work includes li and li (2002) <papid> P02-1044 </papid>who propose bilingual bootstrapping method to learn translation disambiguation wsd model, and diab (2004) <papid> P04-1039 </papid>who exploited large amounts of automatically generated noisy parallel data to learn wsd models in an unsupervised bootstrapping scheme.</citsent>
<aftsection>
<nextsent>in all this work, the goal is to achieve accurate wsd with minimum amounts of annotated data.
</nextsent>
<nextsent>again, this differs from our objective which is to directly evaluate an smt model as wsd model.
</nextsent>
<nextsent>we presented empirical results casting doubt on the increasingly common assumption that smt models are very good at wsd, even though they do not explicitly address wsd as an independent task.
</nextsent>
<nextsent>using the senseval-3 chinese lexical sample task as testbed, we directly compared the performance of typical chinese-to-english smt model, built fromoff-the-shelf tool kits, with that of state-of-the-art senseval models and found that the smt model does not achieve the same high accuracies as any the dedicated wsd models considered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1581">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>many of the senseval-type 124wsd system are not language specific and the presentation of the results in the english lexical sample task (midhalcea et al, 2004), english-hindi multilingual task (chklovski et al, 2004), <papid> W04-0802 </papid>or any of the lexical sample tasks defined in other languages, give good overview of the variety of approaches to wsd.most previous work on multilingual wsd has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full mt systems) to help wsd.</prevsent>
<prevsent>for instance, ng et al (2003) <papid> P03-1058 </papid>showed that it is possible to use word aligned parallel corpora to train accurate supervised wsd models.</prevsent>
</prevsection>
<citsent citstr=" P04-1039 ">
other work includes li and li (2002) <papid> P02-1044 </papid>who propose bilingual bootstrapping method to learn translation disambiguation wsd model, and diab (2004) <papid> P04-1039 </papid>who exploited large amounts of automatically generated noisy parallel data to learn wsd models in an unsupervised bootstrapping scheme.</citsent>
<aftsection>
<nextsent>in all this work, the goal is to achieve accurate wsd with minimum amounts of annotated data.
</nextsent>
<nextsent>again, this differs from our objective which is to directly evaluate an smt model as wsd model.
</nextsent>
<nextsent>we presented empirical results casting doubt on the increasingly common assumption that smt models are very good at wsd, even though they do not explicitly address wsd as an independent task.
</nextsent>
<nextsent>using the senseval-3 chinese lexical sample task as testbed, we directly compared the performance of typical chinese-to-english smt model, built fromoff-the-shelf tool kits, with that of state-of-the-art senseval models and found that the smt model does not achieve the same high accuracies as any the dedicated wsd models considered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1584">
<title id=" I05-2021.xml">evaluating the word sense disambiguation performance of statistical machine translation </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>taken together, these results suggest that another smt formulation might be needed.
</prevsent>
<prevsent>in particular, more grammatically structured statistical mt models that are better equipped to handle long distance dependencies, such as the itg based grammatical channel?
</prevsent>
</prevsection>
<citsent citstr=" P98-2230 ">
translation model (wu and wong, 1998), <papid> P98-2230 </papid>might make better use of the wsd predictions.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1585">
<title id=" I05-2040.xml">transformation based chinese entity detection and tracking </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J95-4004 ">
this paper proposes unified transformation based learning (tbl, brill, 1995) <papid> J95-4004 </papid>framework for chinese entity detection and tracking (edt).</citsent>
<aftsection>
<nextsent>it consists of two sub models: mention detection model and an entity tracking/coreference model.
</nextsent>
<nextsent>the first sub-model is used to adapt existing chinese word segmentation and named entity (ne) recognition results to specific edt standard to find all the mentions.
</nextsent>
<nextsent>the second sub-model is used to find the coreference relation between the mentions.
</nextsent>
<nextsent>in addition, feedback technique is proposed to further improve the performance of the system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1587">
<title id=" I05-2040.xml">transformation based chinese entity detection and tracking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the ace project, there are five types of entities defined in edt: person (per), geography political entity (gpe), organization (org), location (loc), and facility (fac).
</prevsent>
<prevsent>many traditional coreference techniques can be extended to edt for entity tracking.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
early work on pronoun anaphora resolution usually uses rule-based methods (e.g. hobbs 1976; ge et al , 1998; <papid> W98-1119 </papid>mitkov, 1998), <papid> P98-2143 </papid>which try to mine the cues of the relation between the pronouns and its antecedents.</citsent>
<aftsection>
<nextsent>recent research (soon et al , 2001; <papid> J01-4004 </papid>yang et al , 2003; <papid> P03-1023 </papid>ng and cardie, 2002; ittycherah et al , 2003; luo et al , 2004) <papid> P04-1018 </papid>focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases, including name, nominal and pronoun phrase.</nextsent>
<nextsent>one common approach applied by them is to first train binary statistical model to measure how likely pair of 232 mentions corefer; and then followed by greedy procedure to group the mentions into entities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1588">
<title id=" I05-2040.xml">transformation based chinese entity detection and tracking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the ace project, there are five types of entities defined in edt: person (per), geography political entity (gpe), organization (org), location (loc), and facility (fac).
</prevsent>
<prevsent>many traditional coreference techniques can be extended to edt for entity tracking.
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
early work on pronoun anaphora resolution usually uses rule-based methods (e.g. hobbs 1976; ge et al , 1998; <papid> W98-1119 </papid>mitkov, 1998), <papid> P98-2143 </papid>which try to mine the cues of the relation between the pronouns and its antecedents.</citsent>
<aftsection>
<nextsent>recent research (soon et al , 2001; <papid> J01-4004 </papid>yang et al , 2003; <papid> P03-1023 </papid>ng and cardie, 2002; ittycherah et al , 2003; luo et al , 2004) <papid> P04-1018 </papid>focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases, including name, nominal and pronoun phrase.</nextsent>
<nextsent>one common approach applied by them is to first train binary statistical model to measure how likely pair of 232 mentions corefer; and then followed by greedy procedure to group the mentions into entities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1589">
<title id=" I05-2040.xml">transformation based chinese entity detection and tracking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many traditional coreference techniques can be extended to edt for entity tracking.
</prevsent>
<prevsent>early work on pronoun anaphora resolution usually uses rule-based methods (e.g. hobbs 1976; ge et al , 1998; <papid> W98-1119 </papid>mitkov, 1998), <papid> P98-2143 </papid>which try to mine the cues of the relation between the pronouns and its antecedents.</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
recent research (soon et al , 2001; <papid> J01-4004 </papid>yang et al , 2003; <papid> P03-1023 </papid>ng and cardie, 2002; ittycherah et al , 2003; luo et al , 2004) <papid> P04-1018 </papid>focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases, including name, nominal and pronoun phrase.</citsent>
<aftsection>
<nextsent>one common approach applied by them is to first train binary statistical model to measure how likely pair of 232 mentions corefer; and then followed by greedy procedure to group the mentions into entities.
</nextsent>
<nextsent>mention detection is to find all the named entity, noun or noun phrase, pronoun or pronoun phrase.
</nextsent>
<nextsent>therefore, it needs named entity recognition, but not only.
</nextsent>
<nextsent>though the detection of entity mentions is an essential problem for edt/coreference, there has been relatively less previous research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1590">
<title id=" I05-2040.xml">transformation based chinese entity detection and tracking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many traditional coreference techniques can be extended to edt for entity tracking.
</prevsent>
<prevsent>early work on pronoun anaphora resolution usually uses rule-based methods (e.g. hobbs 1976; ge et al , 1998; <papid> W98-1119 </papid>mitkov, 1998), <papid> P98-2143 </papid>which try to mine the cues of the relation between the pronouns and its antecedents.</prevsent>
</prevsection>
<citsent citstr=" P03-1023 ">
recent research (soon et al , 2001; <papid> J01-4004 </papid>yang et al , 2003; <papid> P03-1023 </papid>ng and cardie, 2002; ittycherah et al , 2003; luo et al , 2004) <papid> P04-1018 </papid>focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases, including name, nominal and pronoun phrase.</citsent>
<aftsection>
<nextsent>one common approach applied by them is to first train binary statistical model to measure how likely pair of 232 mentions corefer; and then followed by greedy procedure to group the mentions into entities.
</nextsent>
<nextsent>mention detection is to find all the named entity, noun or noun phrase, pronoun or pronoun phrase.
</nextsent>
<nextsent>therefore, it needs named entity recognition, but not only.
</nextsent>
<nextsent>though the detection of entity mentions is an essential problem for edt/coreference, there has been relatively less previous research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1591">
<title id=" I05-2040.xml">transformation based chinese entity detection and tracking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many traditional coreference techniques can be extended to edt for entity tracking.
</prevsent>
<prevsent>early work on pronoun anaphora resolution usually uses rule-based methods (e.g. hobbs 1976; ge et al , 1998; <papid> W98-1119 </papid>mitkov, 1998), <papid> P98-2143 </papid>which try to mine the cues of the relation between the pronouns and its antecedents.</prevsent>
</prevsection>
<citsent citstr=" P04-1018 ">
recent research (soon et al , 2001; <papid> J01-4004 </papid>yang et al , 2003; <papid> P03-1023 </papid>ng and cardie, 2002; ittycherah et al , 2003; luo et al , 2004) <papid> P04-1018 </papid>focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases, including name, nominal and pronoun phrase.</citsent>
<aftsection>
<nextsent>one common approach applied by them is to first train binary statistical model to measure how likely pair of 232 mentions corefer; and then followed by greedy procedure to group the mentions into entities.
</nextsent>
<nextsent>mention detection is to find all the named entity, noun or noun phrase, pronoun or pronoun phrase.
</nextsent>
<nextsent>therefore, it needs named entity recognition, but not only.
</nextsent>
<nextsent>though the detection of entity mentions is an essential problem for edt/coreference, there has been relatively less previous research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1592">
<title id=" I05-2040.xml">transformation based chinese entity detection and tracking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>though the detection of entity mentions is an essential problem for edt/coreference, there has been relatively less previous research.
</prevsent>
<prevsent>ng and cardie (2002) shows that improving the recall of noun phrase identification can improve the performance of coreference system.
</prevsent>
</prevsection>
<citsent citstr=" N04-1001 ">
florian et al  (2004) <papid> N04-1001 </papid>formulate the mention detection problem as charac ter-based classification problem.</citsent>
<aftsection>
<nextsent>they assign for each character in the text label, indicating whether it is the start of specific mention, inside specific mention, or outside of any mention.
</nextsent>
<nextsent>in this paper, we propose unified edt model based on the transformation based learning (tbl, brill, 1995) <papid> J95-4004 </papid>framework for chi nese.</nextsent>
<nextsent>the model consists of two sub models: mention detection model and coreference model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1595">
<title id=" I05-2040.xml">transformation based chinese entity detection and tracking </title>
<section> the unified system framework.  </section>
<citcontext>
<prevsection>
<prevsent>the experimental results on the ace chinese edt corpus are shown in section 7.
</prevsent>
<prevsent>our chinese edt system consists of two components, mention detection module and coreference module besides feedback technique between them as illustrated in figure 1.
</prevsent>
</prevsection>
<citsent citstr=" P03-1035 ">
msrseg (gao et al , 2003; <papid> P03-1035 </papid>gao et al ), microsoft research asias chinese word segmentation system that is integrated with named entity recognition, is used to segment chinese words.</citsent>
<aftsection>
<nextsent>however msrseg cant well match the standard of ace edt evaluation for either types or boundaries.
</nextsent>
<nextsent>the difference of the standard of named entity between msrseg and ace cause more than half of the errors for name mention detection.
</nextsent>
<nextsent>in order to overcome these problems, we integrate segmentation adapter to mention detection model.
</nextsent>
<nextsent>the edt system is unified system that uses the tbl scheme.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1596">
<title id=" I05-2040.xml">transformation based chinese entity detection and tracking </title>
<section> word segmentation adaptation.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1.
</prevsent>
<prevsent>entity detection and tracking system flow.
</prevsent>
</prevsection>
<citsent citstr=" P04-1059 ">
the method of applying tbl to adapt the chinese word segmentation standard has been described in gao et al  (2004).<papid> P04-1059 </papid></citsent>
<aftsection>
<nextsent>our approach is slightly different for not have correctly segmented corpus according to ace standard.
</nextsent>
<nextsent>from the un-segmented ace edt corpus, we can only obtain mention boundary information.
</nextsent>
<nextsent>so the adapting objective is to detect the mention boundary instead of all words in text, correctly.
</nextsent>
<nextsent>in the corpus, very few mentions?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1599">
<title id=" I05-2040.xml">transformation based chinese entity detection and tracking </title>
<section> feedback.  </section>
<citcontext>
<prevsection>
<prevsent>an isolated mention is more likely not to be mention.
</prevsent>
<prevsent>the third is to pick up lost mention according to its multi-appearance in the discourse.
</prevsent>
</prevsection>
<citsent citstr=" W04-0705 ">
in fact, [ji and crishman, 2004] <papid> W04-0705 </papid>has used five hubristic rules based on coreference results to improve the name recognition result.</citsent>
<aftsection>
<nextsent>while in this section we will present an automatic method.
</nextsent>
<nextsent>the feedback technique is employed by using entity features in mention detection model.
</nextsent>
<nextsent>in our model, the transformation templates refer to the number of mentions in the entity, the single character feature, the entity type feature, the mention type feature and mention string, as listed follows.
</nextsent>
<nextsent>sdd: its possible values are the combination of the mention type and entity type of the mention string in discourse: per, gpe, org, loc, fac, nper (nominal per), ngpe, norg, nloc, nfac, pper (prounoun per), pgpe, porg, ploc, and pfac.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1602">
<title id=" I08-1063.xml">automatic identification of rhetorical roles using conditional random fields for legal document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with the availability of large number of colossal legal documents in electronic format, there is rising need for effective information retrieval tools to assist in organizing, processing and retrieving this information and presenting them in suitable user-friendly format.
</prevsent>
<prevsent>to that end, text summarization is an important step for many of these larger information management goals.
</prevsent>
</prevsection>
<citsent citstr=" W04-1006 ">
in recent years, much attention has been focused on the problem of understanding the structure and textual units in legal judgments (farzindar &amp; lapalme, 2004).<papid> W04-1006 </papid></citsent>
<aftsection>
<nextsent>in this case, performing automatic segmentation of document to understand the rhetorical roles turns out to be an important research issue.
</nextsent>
<nextsent>for instance, farzindar (2004) proposed text summarization method to manipulate factual and heuristic knowledge from legal documents.
</nextsent>
<nextsent>hachey and grover (2005) explored machine learning approach to rhetorical status classification by performing fact extraction and sentence extraction for automatic summarization of texts in the legal domain.
</nextsent>
<nextsent>they formalized the problem to extract most important units based on the identification of thematic structure of the document and determination of argumentative roles of the textual units in the judgment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1603">
<title id=" I08-1063.xml">automatic identification of rhetorical roles using conditional random fields for legal document summarization </title>
<section> text segmentation algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>in this study, we encoded this information and generated automatically explicit linguistic features.
</prevsent>
<prevsent>feature functions for the rules are set to 1 if they match words/phrases in the input sequence exactly.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
named entity recognition - this type of recognition is not considered fully in summarizing scientific articles (teufel &amp; moens, 2002).<papid> J02-4002 </papid></citsent>
<aftsection>
<nextsent>but in our work, we included few named entities like supreme court, lower court etc., and generate binary-valued entity type features which take the value 0 or 1 indicating the presence or absence of particular entity type in the sentences.
</nextsent>
<nextsent>local features and layout features - one of the main advantages of crfs is that they easily afford the use of arbitrary features of the input.
</nextsent>
<nextsent>one can encode abbreviated features; layout features such as position of paragraph beginning, as well as the sentences appearing with quotes, all in one framework.
</nextsent>
<nextsent>k (lt-1, lt. s, t) in eq.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1604">
<title id=" I08-1063.xml">automatic identification of rhetorical roles using conditional random fields for legal document summarization </title>
<section> legal document summarization.  </section>
<citcontext>
<prevsection>
<prevsent>the sentences with labels identified during crf implementation can be used with the term distribution model to give more significance to some of the sentences with specific roles.
</prevsent>
<prevsent>moreover, the structure details available in this stage are useful in improving the coherency and readability among the sentences present in the summary.
</prevsent>
</prevsection>
<citsent citstr=" J02-4001 ">
extraction of sentences in the generation of summary at different percentage levels of text is one of the widely used methods in document summarization (radev et al, 2002).<papid> J02-4001 </papid></citsent>
<aftsection>
<nextsent>for the legal domain, generating summary from the original judgment is complex problem.
</nextsent>
<nextsent>our approach to produce the summary is extraction-based method which identifies important elements present in legal judgment.
</nextsent>
<nextsent>the identification of the document structure using crf-model categorizes the key ideas from the details of legal judgment.
</nextsent>
<nextsent>the genre structure has been applied to final summary to improve the readability and coherence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1605">
<title id=" I08-1063.xml">automatic identification of rhetorical roles using conditional random fields for legal document summarization </title>
<section> legal document summarization.  </section>
<citcontext>
<prevsection>
<prevsent>intrinsic measure shows the presence of source contents in the summary.
</prevsent>
<prevsent>f-measure and map are two standard intrinsic measures used for the evaluation of our system-generated summary.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
we have also used rouge evaluation approach (lin, 2004) <papid> W04-1013 </papid>which is based on n-gram co-occurrences between machine summaries and ideal human summaries.</citsent>
<aftsection>
<nextsent>3.1 applying term distribution model.
</nextsent>
<nextsent>legal documents segmented text with labels (crf imple mentation)pre processing term distribution model summary with ratio &amp; final decision figure 3.
</nextsent>
<nextsent>architectural view of summarization system.
</nextsent>
<nextsent>3.2 evaluation of summary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1606">
<title id=" I05-3021.xml">an example based chinese word segmentation system for cwsb2 </title>
<section> example-based segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>146
</prevsent>
<prevsent>how to utilize as much information as possible from the training corpus to adapt segmentation system towards segmentation standard has been critical issue.
</prevsent>
</prevsection>
<citsent citstr=" P04-1059 ">
kit et al (2002) and kit et al (2003) attempt to integrate case-based learning with statistical models (e.g., n-gram) by extracting transformation rules from the training corpus for disambiguation via error correc tion; gao et al (2004) <papid> P04-1059 </papid>adopt similar strategy for adaptive segmentation, with transformation templates (instead of case-based rules) to modify word boundaries (instead of individual words).</citsent>
<aftsection>
<nextsent>the basic idea of example-based segmentation is very simple: existing pre-segmented strings intraining corpus provide reliable examples for segmenting similar strings in input texts.
</nextsent>
<nextsent>in contrast to dictionary checking for locating possible wordsin an input sentence to facilitate later segmentation operations, pre-segmented examples give exact segmentation to copy.the example-based segmentation can be implemented in the following steps.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>find all exemplar pre-segmented fragments,.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1607">
<title id=" I05-2024.xml">information retrieval capable of visualization and high precision </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in computer experiments, two-dimensional documentary maps in which queries and documents were mapped in topological order according to their similarities were created.the ranking of the results retrieved using the maps was better than that ofthe results obtained using conventional tfidf method.
</prevsent>
<prevsent>furthermore, the precision of the proposed method wasmuch higher than that of the conventional tfidf method when the process was focused on retrieving highly relevant documents, suggesting that the proposed method might be especially suited to information retrieval tasks inwhich precision is more critical than recall.
</prevsent>
</prevsection>
<citsent citstr=" P96-1003 ">
information retrieval (ir) has been studied sincean earlier stage [e.g., (menzel, 1966)] and several kinds of basic retrieval models have been proposed (salton and buckley, 1988) and number of improved ir systems based on these models have been developed by adopting various nlp techniques [e.g., (evans and zhai, 1996; <papid> P96-1003 </papid>mitra et al, 1997; mandara, et al, 1998; murata, et al., 2000)].</citsent>
<aftsection>
<nextsent>however, an epoch-making technique that surpasses the tfidf weighted vector space model, the main approach to ir at present, has notyet been invented and ir is still relatively imprecise.
</nextsent>
<nextsent>there are also challenges presenting large number of retrieval results to users in visual and intelligible form.
</nextsent>
<nextsent>our aim is to develop high-precision, visual ir system that consists of two phases.
</nextsent>
<nextsent>the first phase is carried out using conventional ir techniques in which large number of related documents are gathered from newspapers or web sites in response to query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1608">
<title id=" I08-3012.xml">design of a rule based stemmer for natural language text in bengali </title>
<section> design.  </section>
<citcontext>
<prevsection>
<prevsent>hence, if the system is pos aware, it will be able to generate more accurate result.
</prevsent>
<prevsent>this can be achieved by sending pos tagged text to the stemmer system, which will apply pos specific rules to discover stems.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
this proposition is quite viable as statistical pos taggers like tnt (brants, 2000) <papid> A00-1031 </papid>are available.</citsent>
<aftsection>
<nextsent>the context of the proposed system is provided below: figure 1: context of proposed stemmer 4.2 inflection rule observations.
</nextsent>
<nextsent>to discover the rules, we took the help of the seminal work by chatterji (1939).
</nextsent>
<nextsent>for this work we limited our study within traditional and standard colloquial styles (dialects) of bengali.
</nextsent>
<nextsent>for each of the poss, we prepared the list of applicable inflections considering these dialects only.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1609">
<title id=" I08-1006.xml">story link detection based on dynamic information extending </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it can be classified into two categories:vector-based methods and probabilistic-based methods.
</prevsent>
<prevsent>the vector space model is widely used in ir andtext classification research.
</prevsent>
</prevsection>
<citsent citstr=" N04-1040 ">
cosine similarity between document vectors with tfidf term weighting (connell et al, 2004) (chen et al, 2004) (<papid> N04-1040 </papid>allan et al., 2003) is also one of the best technologies for link detection.</citsent>
<aftsection>
<nextsent>we have examined number of similarity measures in story link detection, including cosine,hellinger and tanimoto, and found that cosine similarity produced outstanding results.
</nextsent>
<nextsent>furthermore, (allan et al, 2000) also confirms this conclusion among cosine, weighted sum, language modeling and kullback-leibler divergence in its story link detection research.
</nextsent>
<nextsent>probabilistic-based method has been proven to be very effective in several ir applications.
</nextsent>
<nextsent>one of its attractive features is that it is firmly rooted in the theory of probability, thereby allowing the researcher to explore more sophisticated models guided by the theoretical framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1610">
<title id=" I08-1006.xml">story link detection based on dynamic information extending </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>probabilistic-based method has been proven to be very effective in several ir applications.
</prevsent>
<prevsent>one of its attractive features is that it is firmly rooted in the theory of probability, thereby allowing the researcher to explore more sophisticated models guided by the theoretical framework.
</prevsent>
</prevsection>
<citsent citstr=" N03-3001 ">
(nallapati and allan, 2002)(lavrenko et al, 2002) (nallapati, 2003) <papid> N03-3001 </papid>all apply probability models (language model or relevance model) for story link detection.</citsent>
<aftsection>
<nextsent>and the experiment results indicate that the performances are comparable with those using traditional vector space models, if not better.
</nextsent>
<nextsent>on the basis of vector-based methods, this paper represents method of dynamic information extending to improve the performance of story link detection.
</nextsent>
<nextsent>it makes use of the previous latest topically related story to extend the vector model of current being processed story.
</nextsent>
<nextsent>new dynamic models are generated for computing the similarity between two stories in current pair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1611">
<title id=" I05-2004.xml">a language independent algorithm for single and multiple document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(hirao et al, 2002), (teufel and moens, 1997).
</prevsent>
<prevsent>how ever, the price paid for the high performance ofsuch supervised algorithms is their inability to easily adapt to new languages or domains, as new training data are required for each new data type.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
in this paper, we show that method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (mihalcea and tarau, 2004) <papid> W04-3252 </papid>can be applied to the summarization of documents in different languages without any requirements for additional data.</citsent>
<aftsection>
<nextsent>additionally, wealso show that layered application of this single document summarization method can result into an efficient multi-document summarization tool.earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (mihalcea and tarau, 2004) <papid> W04-3252 </papid>and (erkanand radev, 2004), <papid> W04-3247 </papid>were either limited to single document english summarization, or they were applied to english multi-document summarization,but in conjunction with other extractive summarization techniques that did not allow for clear evaluation of the impact of the graph algorithms alone.</nextsent>
<nextsent>in this paper, we show that method exclusively basedon graph-based algorithms can be successfully applied to the summarization of single and multiple documents in any language, and show that there sults are competitive with those of state-of-the-art summarization systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1615">
<title id=" I05-2004.xml">a language independent algorithm for single and multiple document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, the price paid for the high performance ofsuch supervised algorithms is their inability to easily adapt to new languages or domains, as new training data are required for each new data type.
</prevsent>
<prevsent>in this paper, we show that method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (mihalcea and tarau, 2004) <papid> W04-3252 </papid>can be applied to the summarization of documents in different languages without any requirements for additional data.</prevsent>
</prevsection>
<citsent citstr=" W04-3247 ">
additionally, wealso show that layered application of this single document summarization method can result into an efficient multi-document summarization tool.earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (mihalcea and tarau, 2004) <papid> W04-3252 </papid>and (erkanand radev, 2004), <papid> W04-3247 </papid>were either limited to single document english summarization, or they were applied to english multi-document summarization,but in conjunction with other extractive summarization techniques that did not allow for clear evaluation of the impact of the graph algorithms alone.</citsent>
<aftsection>
<nextsent>in this paper, we show that method exclusively basedon graph-based algorithms can be successfully applied to the summarization of single and multiple documents in any language, and show that there sults are competitive with those of state-of-the-art summarization systems.
</nextsent>
<nextsent>the paper is organized as follows.
</nextsent>
<nextsent>section 2 briefly overviews two iterative graph-based ranking algorithms, and shows how these algorithms can be applied to single and multiple document summarization.
</nextsent>
<nextsent>section 3 describes the datasets used in the summarization experiments and the evaluation methodology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1618">
<title id=" I05-2004.xml">a language independent algorithm for single and multiple document summarization </title>
<section> iterative graph-based algorithms for.  </section>
<citcontext>
<prevsection>
<prevsent>experimental results are presented in section 4, followed by discussions, pointers to related work, and conclusions.
</prevsent>
<prevsent>extractive summarization in this section, we shortly describe two graph-based ranking algorithms and their application to the task of extractive summarization.
</prevsent>
</prevsection>
<citsent citstr=" C04-1162 ">
ranking algorithms, such as klein bergs hits algorithm (kleinberg, 1999) or googles page rank (brin and page, 1998), have been traditionally and successfully used inweb-link analysis (brin and page, 1998), social networks, and more recently in text processing applications (mihalcea and tarau, 2004), (<papid> W04-3252 </papid>mihalcea et al., 2004), (<papid> C04-1162 </papid>erkan and radev, 2004).<papid> W04-3247 </papid></citsent>
<aftsection>
<nextsent>in short, agraph-based ranking algorithm is way of deciding on the importance of vertex within graph, by taking into account global information recursively computed from the entire graph, rather than relying 19only on local vertex-specific information.
</nextsent>
<nextsent>the basic idea implemented by the ranking model is that of voting?
</nextsent>
<nextsent>or recommendation?.
</nextsent>
<nextsent>when one vertex links to another one, it is basically casting vote for that other vertex.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1620">
<title id=" I05-2004.xml">a language independent algorithm for single and multiple document summarization </title>
<section> materials and evaluation methodology.  </section>
<citcontext>
<prevsection>
<prevsent>the documents were selected to cover variety of domains (e.g. world, politics, foreign affairs, editorials), and manual summaries were produced by an expert in brazilian portuguese.
</prevsent>
<prevsent>unlike the summaries produced for the english duc documents ? which had length requirement of approximately 100 words, the length of the summaries in the temario dataset is constrained relative to the length of the corre 21 spon ding documents, i.e. summary has to account for about 25-30% of the original document.
</prevsent>
</prevsection>
<citsent citstr=" W03-0510 ">
consequently, the automatic summaries generated for the documents in this collection are not restricted to 100 words, as in the english experiments, but are required to have length comparable to the corresponding manual summaries, to ensure fair evalu ation.for evaluation, we are using the rouge evaluation toolkit1, which is method based on ngram statistics, found to be highly correlated with human evaluations (lin and hovy, 2003<papid> W03-0510 </papid>a).</citsent>
<aftsection>
<nextsent>the evaluation is done using the ngram(1), using the ngram(1) setting of rouge, which was found to have the highest correlation with human judgments, at confidence level of 95%.
</nextsent>
<nextsent>the extractive summarization algorithm is evaluated in the context of: (1) single-document summarization task, where summary is generated foreach of the 567 english news articles provided during the document understanding evaluations 2002 (duc, 2002), and for each of the 100 portuguese documents in the temario data set; and (2) multi document summarization task, where summary is generated for each of the 59 document clusters in the duc 2002 data.
</nextsent>
<nextsent>since document clusters and multi-document summaries are not available for the portuguese documents, multi-document summarization evaluation could not be conducted on this dataset.
</nextsent>
<nextsent>note however that the multi-document summarization tool is based on the single-document summarization method (see figure 2), and thus high performance in single-document summarization is expected to result into similar level of performance in multi-document summarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1627">
<title id=" I05-2004.xml">a language independent algorithm for single and multiple document summarization </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>extractive summarization is considered an important first step for more sophisticated automatic text summarization.
</prevsent>
<prevsent>as consequence, there is large body of work on algorithms for extractive summarization undertaken as part of the duc evaluation exercises (http://www-nlpir.nist.gov/projects/duc/).
</prevsent>
</prevsection>
<citsent citstr=" P04-1049 ">
previous approaches include supervised learning(hirao et al, 2002), (teufel and moens, 1997), vector ial similarity computed between an initial abstract and sentences in the given document, intra document similarities (salton et al, 1997), or graph algorithms (mihalcea and tarau, 2004), (<papid> W04-3252 </papid>erkan and radev, 2004), (<papid> W04-3247 </papid>wolf and gibson, 2004).<papid> P04-1049 </papid></citsent>
<aftsection>
<nextsent>it is also notable the study reported in (lin and hovy, 2003<papid> W03-0510 </papid>b)discussing the usefulness and limitations of automatic sentence extraction for text summarization, 23 single document meta?</nextsent>
<nextsent>summarization algorithm summarization algo.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1632">
<title id=" I08-1053.xml">translating compounds by learning component gloss translation models via multiple languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>across multiple languages.
</prevsent>
<prevsent>performance is improved by adding component-sequence and learned morphology models along with context similarity from monolingual text and optional combination with traditional bilingual-text based translation discovery.
</prevsent>
</prevsection>
<citsent citstr=" C92-4201 ">
compound words such as lighthouse and fireplace are words that are composed of two or more component words and are often challenge for machine translation due to their potentially complex compounding behavior and ambiguous interpretations (rackow et al, 1992).<papid> C92-4201 </papid></citsent>
<aftsection>
<nextsent>for many languages, such words form significant portion of the lexicon and the compounding process is further complicated by diverse morphological processes (levi, 1978) and the properties of different compound sequences such as noun-noun, adj-adj, adj-noun, verb-verb, etc. compounds also tend to have high type frequency but low token frequency which makes their translation difficult to learn using corpus-based algorithms (tanaka and baldwin, 2003).<papid> W03-1803 </papid></nextsent>
<nextsent>furthermore, most ofthe literature on compound translation has been restricted to few languages dealing with compounding phenomena specific to the language in question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1633">
<title id=" I08-1053.xml">translating compounds by learning component gloss translation models via multiple languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>performance is improved by adding component-sequence and learned morphology models along with context similarity from monolingual text and optional combination with traditional bilingual-text based translation discovery.
</prevsent>
<prevsent>compound words such as lighthouse and fireplace are words that are composed of two or more component words and are often challenge for machine translation due to their potentially complex compounding behavior and ambiguous interpretations (rackow et al, 1992).<papid> C92-4201 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1803 ">
for many languages, such words form significant portion of the lexicon and the compounding process is further complicated by diverse morphological processes (levi, 1978) and the properties of different compound sequences such as noun-noun, adj-adj, adj-noun, verb-verb, etc. compounds also tend to have high type frequency but low token frequency which makes their translation difficult to learn using corpus-based algorithms (tanaka and baldwin, 2003).<papid> W03-1803 </papid></citsent>
<aftsection>
<nextsent>furthermore, most ofthe literature on compound translation has been restricted to few languages dealing with compounding phenomena specific to the language in question.
</nextsent>
<nextsent>compound splitting english gloss translation input: distilled glosses from german-english dictionary krankenhaus kranken-haus sick-house hospital regenschirm regen-schirm rain-guard umbrella worterbuch worter-buch words-book dictionary eisen bahn eisen-bahn iron-path railroad input: distilled glosses from swedish-english dictionary sjukhus sjhu-khus sick-house hospital jarnvag jarn-vag iron-path railway ordbok ord-bok words-book dictionary goal: to translate new albanian compounds hekurudhe?
</nextsent>
<nextsent>hekur-udhe?
</nextsent>
<nextsent>iron-path ???
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1634">
<title id=" I08-1053.xml">translating compounds by learning component gloss translation models via multiple languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>403
</prevsent>
<prevsent>the compound-translation literature typically deals with these steps: 1) compound splitting, 2) translation candidate generation and 3) translation candidate scoring.
</prevsent>
</prevsection>
<citsent citstr=" E03-1076 ">
compound splitting is generally done using translation lexicon lookup and allowing for different splitting options based on corpus frequency (zhang et al, 2000; koehn and knight, 2003).<papid> E03-1076 </papid></citsent>
<aftsection>
<nextsent>translation candidate generation is an important phase and this is where our work differs significantly from the previous literature.
</nextsent>
<nextsent>most of the previous work has been focused on generating compositional translation candidates, that is, the translation candidates of the compound words are lexi cally composed of the component word translations.
</nextsent>
<nextsent>this has been done by either just concatenating the translations of component words to form candidate (grefenstette, 1999; cao and li, 2002), or using syntactic templates such as e2 in e1?, e1 ofe2?
</nextsent>
<nextsent>to form translation candidates from the translation of the component words e2 and e1 (baldwinand tanaka, 2004), <papid> W04-0404 </papid>or using synsets of the component word translations to include synonyms in the compositional candidates (navigli et al, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1635">
<title id=" I08-1053.xml">translating compounds by learning component gloss translation models via multiple languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most of the previous work has been focused on generating compositional translation candidates, that is, the translation candidates of the compound words are lexi cally composed of the component word translations.
</prevsent>
<prevsent>this has been done by either just concatenating the translations of component words to form candidate (grefenstette, 1999; cao and li, 2002), or using syntactic templates such as e2 in e1?, e1 ofe2?
</prevsent>
</prevsection>
<citsent citstr=" W04-0404 ">
to form translation candidates from the translation of the component words e2 and e1 (baldwinand tanaka, 2004), <papid> W04-0404 </papid>or using synsets of the component word translations to include synonyms in the compositional candidates (navigli et al, 2003).</citsent>
<aftsection>
<nextsent>the above class of work in compositional-candidate generation fails to translate compounds such as krankenhaus (hospital) whose component word translations are kranken (sick) and haus (hospital), and composing sick and house in any order will not result in the correct translation (hospital).
</nextsent>
<nextsent>another problem with using fixed syntactic templates is that they are restricted to the specific patterns occurring in the target language.
</nextsent>
<nextsent>we show how one can use the gloss patterns of compounds in multiple other languages to hypothesize translation candidates that are not lexically compositional.
</nextsent>
<nextsent>our approach to compound word translation is illustrated in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1637">
<title id=" I08-1053.xml">translating compounds by learning component gloss translation models via multiple languages </title>
<section> comparison of different compound.  </section>
<citcontext>
<prevsection>
<prevsent>because evaluation dictionaries are often missing acceptable translations (e.g. railroad rather than railway), and any deviation from exact-match is scored as incorrect, these measures will be lower bound on acceptable translation accuracy.
</prevsent>
<prevsent>also, target language models can often select effectively among such hypothesis lists in context.
</prevsent>
</prevsection>
<citsent citstr=" C02-1011 ">
translation models 6.1 simple model using literal english gloss concatenation as the translation our baseline model is simple gloss concatenation model for generating compositional translation candidates on the lines of grefenstette (1999) and caoand li (2002).<papid> C02-1011 </papid></citsent>
<aftsection>
<nextsent>we take the translations of the individual component-words (e.g. for the compound word hekurudhe?, they would be hekur (iron) and 405udhe?
</nextsent>
<nextsent>(path)) and hypothesizes three translation candidate variants: iron path?, iron-path?
</nextsent>
<nextsent>and iron path?.
</nextsent>
<nextsent>a test instance is scored as correct if anyof these translation candidates occur in the translations of hekurudhe?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1640">
<title id=" I08-1053.xml">translating compounds by learning component gloss translation models via multiple languages </title>
<section> comparison of different compound.  </section>
<citcontext>
<prevsection>
<prevsent>albanian 3272(+1.33%) .214(-1.38%) .407(-0.49%) bulgarian 7211(+5.95%) .258(+1.18%) .443(+0.23%) german 13372(+17.86%) .200(+0.50%) .391(+0.77%) swedish 15094(+16.38%) .190(+0.53%) .363(+0.55%) avg10 10273(+6.98%) .194(+0.52%) .363(+0.28%) table 6: performance for increasing coverage by including compounding morphology.
</prevsent>
<prevsent>the percentages in parentheses are relative improvements from the performance in table 4 . their context similarity.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
this can be accomplished as in rapp (1999) <papid> P99-1067 </papid>and schafer and yarowsky (2002) <papid> W02-2026 </papid>by creating bag-of-words context vectors around boththe source and target language words and then projecting the source vectors into the (english) target space via the current small translation dictionary.</citsent>
<aftsection>
<nextsent>once in the same language space, source words and their translation hypotheses are compared via cosine similarity using their surrounding context vectors.
</nextsent>
<nextsent>we performed this experiment for german and swedish and report average accuracies with and without this addition in table 7.
</nextsent>
<nextsent>for monolingual corpora, we used the german and swedish side ofthe europarl corpus (koehn, 2005) consisting of approximately 15 million and 21 million words respectively.
</nextsent>
<nextsent>we were able to project context vectors for an average of 4224.5 words in the two languages among all the possible compound words detected in section 6.4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1641">
<title id=" I08-1053.xml">translating compounds by learning component gloss translation models via multiple languages </title>
<section> comparison of different compound.  </section>
<citcontext>
<prevsection>
<prevsent>albanian 3272(+1.33%) .214(-1.38%) .407(-0.49%) bulgarian 7211(+5.95%) .258(+1.18%) .443(+0.23%) german 13372(+17.86%) .200(+0.50%) .391(+0.77%) swedish 15094(+16.38%) .190(+0.53%) .363(+0.55%) avg10 10273(+6.98%) .194(+0.52%) .363(+0.28%) table 6: performance for increasing coverage by including compounding morphology.
</prevsent>
<prevsent>the percentages in parentheses are relative improvements from the performance in table 4 . their context similarity.
</prevsent>
</prevsection>
<citsent citstr=" W02-2026 ">
this can be accomplished as in rapp (1999) <papid> P99-1067 </papid>and schafer and yarowsky (2002) <papid> W02-2026 </papid>by creating bag-of-words context vectors around boththe source and target language words and then projecting the source vectors into the (english) target space via the current small translation dictionary.</citsent>
<aftsection>
<nextsent>once in the same language space, source words and their translation hypotheses are compared via cosine similarity using their surrounding context vectors.
</nextsent>
<nextsent>we performed this experiment for german and swedish and report average accuracies with and without this addition in table 7.
</nextsent>
<nextsent>for monolingual corpora, we used the german and swedish side ofthe europarl corpus (koehn, 2005) consisting of approximately 15 million and 21 million words respectively.
</nextsent>
<nextsent>we were able to project context vectors for an average of 4224.5 words in the two languages among all the possible compound words detected in section 6.4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1642">
<title id=" I08-1053.xml">translating compounds by learning component gloss translation models via multiple languages </title>
<section> comparison of different compound.  </section>
<citcontext>
<prevsection>
<prevsent>6.6 using phrase-tables if parallel corpus is. available all previous results in this paper have been for translation lexicon discovery without the need for parallel bilingual text (bitext), which is often in limited supply for lower-resource languages.
</prevsent>
<prevsent>however, itis useful to assess how this translation lexicon dis 407 covery work compares with traditional bitext-based lexicon induction (and how well the approaches canbe combined).
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
for this purpose, we used phrase tables learned by the standard statistical mt toolkit moses (koehn et al, 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>we tested the phrase table accuracy on two languages, one for which we had lot of parallel data available (german-english europarl corpus with approx.
</nextsent>
<nextsent>15 million words) and one for which we had relatively little parallel data(czech-english news-commentary corpus with approx.
</nextsent>
<nextsent>1 million words).
</nextsent>
<nextsent>this was done to see howthe amount of parallel data available affects the accuracy and coverage of compound translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1643">
<title id=" I05-3025.xml">a maximum entropy approach to chinese word segmentation </title>
<section> chinese word segmenter.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W04-3236 ">
the chinese word segmenter we built is similar to the maximum entropy word segmenter we employed in our previous work (ng and low, 2004).<papid> W04-3236 </papid></citsent>
<aftsection>
<nextsent>our word segmenter uses maximum entropy framework (ratnaparkhi, 1998; xue and shen,2003) <papid> W03-1728 </papid>and is trained on manually segmented sen tences.</nextsent>
<nextsent>it classifies each chinese character given the features derived from its surrounding context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1644">
<title id=" I05-3025.xml">a maximum entropy approach to chinese word segmentation </title>
<section> chinese word segmenter.  </section>
<citcontext>
<prevsection>

<prevsent>the chinese word segmenter we built is similar to the maximum entropy word segmenter we employed in our previous work (ng and low, 2004).<papid> W04-3236 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1728 ">
our word segmenter uses maximum entropy framework (ratnaparkhi, 1998; xue and shen,2003) <papid> W03-1728 </papid>and is trained on manually segmented sen tences.</citsent>
<aftsection>
<nextsent>it classifies each chinese character given the features derived from its surrounding context.
</nextsent>
<nextsent>each chinese character can be assigned one of four possible boundary tags: for character that occurs as single-character word, for character that begins multi-character (i.e., two or more characters) word, for character that ends multi-character word, and for character that is neither the first nor last in multi-character word.
</nextsent>
<nextsent>our implementation used the opennlp maximum entropy package v2.1.0 from sourceforge.1 1.1 basic features.
</nextsent>
<nextsent>the basic features of our word segmenter are similar to our previous work (ng and low, 2004): (<papid> W04-3236 </papid>a) cn(n = 2,1, 0, 1, 2) (b) cncn+1(n = 2,1, 0, 1) (c) c1c1 (d) pu(c0) (e) (c2)t (c1)t (c0)t (c1)t (c2) in the above feature templates, refers to chinese character.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1646">
<title id=" I05-2015.xml">building an annotated japanese chinese parallel corpus a8c a part of nict multilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>parallel corpora between european languages are well developed and are available through the linguistic data consortium (ldc).
</prevsent>
<prevsent>however, parallel corpora between european languages and asian languages are less developed, and parallel corpora between two asian languages are even less developed.
</prevsent>
</prevsection>
<citsent citstr=" W04-2208 ">
the national institute of information and communications technology therefore started project to build multilingual parallel corpora in 2002 (uchimoto et al, 2004).<papid> W04-2208 </papid></citsent>
<aftsection>
<nextsent>the project focuses on asian language pairs and annotation of detailed information, including syntactic structure and alignment at word and phrase levels.
</nextsent>
<nextsent>we call the corpus the nict multilingual corpora.
</nextsent>
<nextsent>the corpus will be open to the public in the near future.
</nextsent>
<nextsent>corpora at present, japanese-english parallel corpus and japanese-chinese parallel corpus are under construction following systematic specifications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1647">
<title id=" I05-2015.xml">building an annotated japanese chinese parallel corpus a8c a part of nict multilingual corpora </title>
<section> morphological information annotation.  </section>
<citcontext>
<prevsection>
<prevsent>the achievements include grammatical knowledge base of contemporary chinese, an automatic morphological analyser, and an annotated peoples daily corpus.
</prevsent>
<prevsent>since the definition and tagset are widely used in chinese language processing, we also took the criteria as the basis of our guidelines.
</prevsent>
</prevsection>
<citsent citstr=" C94-2209 ">
a morphological analyzer developed by peking university (zhou and yu, 1994) <papid> C94-2209 </papid>was applied for automatic annotation of the chinese sentences and then the automatically tagged sentences were revised by humans.</citsent>
<aftsection>
<nextsent>an annotated sentence is illustrated in figure 3, which is the chinese sentence in ex.
</nextsent>
<nextsent>1 in section 2.
</nextsent>
<nextsent>s-id: 950104141-008 ??/r ??/j ??/n ?/d ?/v ??/m ?/q ??/m ?/u ???/n ?/w ??/r ??/d ?/p ??/v ??/n ?/u ??/n ?/d ??/v ?/w figure 3 an annotated chinese sentence 4.3 tool for manual revision.
</nextsent>
<nextsent>we developed tool to assist annotators in revision.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1648">
<title id=" I05-2015.xml">building an annotated japanese chinese parallel corpus a8c a part of nict multilingual corpora </title>
<section> annotation of word alignment.  </section>
<citcontext>
<prevsection>
<prevsent>the contents of any column can be sorted by clicking the top line of the column.
</prevsent>
<prevsent>since automatic word alignment techniques cannot reach as high level as the morphological analyses, we adopt practical method of using multiple aligners.
</prevsent>
</prevsection>
<citsent citstr=" J97-2004 ">
one aligner is lexical knowledge-based approach, which was implemented by us based on the work of ker (ker and chang, 1997).<papid> J97-2004 </papid></citsent>
<aftsection>
<nextsent>another aligner is the well-known giza++ toolkit, which is statistics-based approach.
</nextsent>
<nextsent>for giza++, two directions were adopted: the chinese sentences were used as source sentences and the japanese sentences as target sentences, and vice versa.
</nextsent>
<nextsent>the results produced by the lexical knowledge based aligner, c?
</nextsent>
<nextsent>j of giza++, and jc of giza++ were selected in majority decision.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1649">
<title id=" I08-2100.xml">nonfactoid japanese question answering through passage retrieval that is weighted based on types of answers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we believe question-answering systems will becomea more convenient alternative to other systems designed for information retrieval and basic component of future artificial intelligence systems.
</prevsent>
<prevsent>numerous researchers have recently been attracted to this important topic.
</prevsent>
</prevsection>
<citsent citstr=" P02-1054 ">
these researchers have produced many interesting studies on question-answering systems (kupiec, 1993; ittycheriah et al, 2001; clarke et al, 2001; dumis et al, 2002; magnini et al, 2002; <papid> P02-1054 </papid>moldovan et al, 2003).</citsent>
<aftsection>
<nextsent>evaluation conferences and contests on question-answering systems have also been held.
</nextsent>
<nextsent>in particular, the u.s.a. has held the text retrieval conferences (trec) (trec-10 committee, 2001), and japan has hosted the question answering challenges (qac) (national institute of informatics, 2002) at ntcir (nii test collection for ir systems ) 3.
</nextsent>
<nextsent>these conferences and contest shave aimed at improving question-answering systems.
</nextsent>
<nextsent>the researchers who participate in these createquestion-answering systems that they then use to answer the same questions, and each systems performance is then evaluated to yield possible improvements.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1650">
<title id=" I08-2100.xml">nonfactoid japanese question answering through passage retrieval that is weighted based on types of answers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, when the question was why are people opposed to the private information protection law??
</prevsent>
<prevsent>the system retrieved sentences based on terms appearing in the question and output an answer using the retrieved sentences.
</prevsent>
</prevsection>
<citsent citstr=" N04-1008 ">
numerous studies have addressed issues that are involved in the answering of non-factoid questions (berger et al, 2000; blair-goldensohn et al, 2003; 727 xu et al, 2003; soricut and brill, 2004; <papid> N04-1008 </papid>han et al, 2005; morooka and fukumoto, 2006; maehara et al., 2006; asada, 2006).we constructed system for answering non factoid japanese questions for qac-4.</citsent>
<aftsection>
<nextsent>we used methods of passage retrieval for the system.
</nextsent>
<nextsent>we extracted paragraphs based on terms from an input question and output them as the preferred answers.we classified the non-factoid questions into six categories.
</nextsent>
<nextsent>we used particular method for each category.
</nextsent>
<nextsent>for example, we increased the scores of paragraphs including the word reason?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1651">
<title id=" I08-1043.xml">paraphrasing depending on bilingual context toward generalization of translation knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>through experiments with parallel corpora of korean and english language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for korean and english, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.
</prevsent>
<prevsent>approaches based on bilingual corpora are promising for the automatic acquisition of translation knowledge.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
phrase-based smt(statistical machine translation) models have advanced the state of the art in machine translation by expanding the basic unit of translation from words to phrases, which allows the local reordering of words and translation of multi-word expressions(chiang, 2007) (koehn et al, 2003) (och and ney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>however phrase-based smt techniques suffer from data sparseness problems, that is; unreliable translation probabilities of low frequency phrases and low coverage in that many phrases encountered at run-time are not observed in the training data.an alternative for these problems is to utilize paraphrases.
</nextsent>
<nextsent>an unknown phrase can be replaced with its paraphrase that is already known.
</nextsent>
<nextsent>moreover, we can smooth the phrase translation probability using the class of paraphrases.
</nextsent>
<nextsent>on the other hand, ebmt or pbmt systems might translate given sentence fast and robustly geared by sentence translation patterns or generalized transfer rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1652">
<title id=" I08-1043.xml">paraphrasing depending on bilingual context toward generalization of translation knowledge </title>
<section> extracting translation patterns.  </section>
<citcontext>
<prevsection>
<prevsent>word/phrase alignment given an input sentence pair, source language sentence is dependency parsed in base phrase lev eland target language sentence is chunked by shallow parser.
</prevsent>
<prevsent>during the dependency parsing and the chunking, each sentence is also segmented into morphemes and we regard morpheme as word.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
we make word alignments through the learning of ibm models by using the giza++ toolkit(och and ney, 2000): <papid> P00-1056 </papid>we learn the translation model toward ibm model 4, initiating translation iterations from ibm model 1 with intermediate hmm model iterations.</citsent>
<aftsection>
<nextsent>for improving the word alignment, we use the word-classes that are trained from monolingual corpus using the srilm toolkit(stolcke, 2002).
</nextsent>
<nextsent>then, we do phrase alignments based on the word alignments, which are consistent with the base phrase boundaries as well as the word alignments as (hwang et al, 2007) did.
</nextsent>
<nextsent>a phrase is defined as aword sequence that is covered by base phrase sequence, not by single sub-tree in syntactic parse tree.
</nextsent>
<nextsent>after the word and the phrase alignments, we obtain bilingual dependency parses by sharing the dependency relations of monolingual dependency parser among the aligned phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1653">
<title id=" I08-1043.xml">paraphrasing depending on bilingual context toward generalization of translation knowledge </title>
<section> extracting translation patterns.  </section>
<citcontext>
<prevsection>
<prevsent>a phrase is defined as aword sequence that is covered by base phrase sequence, not by single sub-tree in syntactic parse tree.
</prevsent>
<prevsent>after the word and the phrase alignments, we obtain bilingual dependency parses by sharing the dependency relations of monolingual dependency parser among the aligned phrases.
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
the bilingual dependency parsing is similar to the technique of bilingual parsing in word level described in (hwa et al, 2005)(quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>our bilingual parsing in aphrase level has an advantage of being capable of reducing not only the parsing complexity but also the errors caused by structural differences between two languages, such like korean and english pairs1.for bilingual parsing between korean and english, we use korean dependency parse on the 1since we regard that phrase in source language sentence is aligned with target phrase if at least one word in source phrase is aligned with the words in target phrase, we robustly project the source phrases onto the target phrases.
</nextsent>
<nextsent>328 figure 1: illustration of acquiring bilingual dependency relations source language side as pivot.
</nextsent>
<nextsent>figure 1 shows an illustration of bilingual dependency parsing between korean and english based on the word/phrase alignments.
</nextsent>
<nextsent>the dependency structure induced on the target language side is in some sense isomorphic to the structure of the source language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1654">
<title id=" I05-3006.xml">product named entity recognition based on hierarchical hidden markov model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the method is similar to token matching and has limitation for product ner applications.
</prevsent>
<prevsent>[bick et al 2004] recognized named entities including product names based on constraint grammar based parser fordanish.
</prevsent>
</prevsection>
<citsent citstr=" P03-1043 ">
this rule-based approach is highly dependent on the performance of danish parser and suffers from its weakness in system portability.[c. niu et al 2003] <papid> P03-1043 </papid>presented bootstrapping approach for english named entity recognition using successive learners of parsing-based decision 40 system statistical model linguistic feature combi native points [zhang et al 2003] hmm semantic role, tokens pattern rules [sun et al 2002] <papid> C02-1012 </papid>class-based lm word form, ne category cue words list [tsai et al 2004] me model tokens knowledge representation table 1: comparison between several chinese ner systems1 list and hmm, and promising experiment results(f-measure: 69.8%) on product ne (corresponding to our pro) were obtained.</citsent>
<aftsection>
<nextsent>its main advantage lies in that manual annotation of sizable training corpus can be avoided, but it suffers fromtwo problems, one is that it is difficult to find sufficient concept-based seeds needed in bootstrapping for the coverage of the variations of pro subcategories, another it is highly dependent on parser performance as well.
</nextsent>
<nextsent>research on product ner is still at its early stage, especially in chinese free text collections.
</nextsent>
<nextsent>however, considerable amount of workhas been done in the last decade on the general ner task and biological ner task.
</nextsent>
<nextsent>the typical machine learning approaches for english ne are transformation-based learning[aberdeen et al 1995], <papid> M95-1012 </papid>hidden markov model[bikel et al. 1997], <papid> A97-1029 </papid>maximum entropy model[borthwick, 1999], support vector machine learning[eunji yi et al 2004], unsupervised model[collins et al 1999]and etc. for chinese ner, the prevailing methodology applied recently also lie in machine learning combining other knowledge base or heuristic rules,which can be compared on the whole in three aspects showed in table 1.in short, the trend inner is to adopt statistical framework which try to exploit some knowledge base as well as different level of text features within and outside nes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1655">
<title id=" I05-3006.xml">product named entity recognition based on hierarchical hidden markov model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the method is similar to token matching and has limitation for product ner applications.
</prevsent>
<prevsent>[bick et al 2004] recognized named entities including product names based on constraint grammar based parser fordanish.
</prevsent>
</prevsection>
<citsent citstr=" C02-1012 ">
this rule-based approach is highly dependent on the performance of danish parser and suffers from its weakness in system portability.[c. niu et al 2003] <papid> P03-1043 </papid>presented bootstrapping approach for english named entity recognition using successive learners of parsing-based decision 40 system statistical model linguistic feature combi native points [zhang et al 2003] hmm semantic role, tokens pattern rules [sun et al 2002] <papid> C02-1012 </papid>class-based lm word form, ne category cue words list [tsai et al 2004] me model tokens knowledge representation table 1: comparison between several chinese ner systems1 list and hmm, and promising experiment results(f-measure: 69.8%) on product ne (corresponding to our pro) were obtained.</citsent>
<aftsection>
<nextsent>its main advantage lies in that manual annotation of sizable training corpus can be avoided, but it suffers fromtwo problems, one is that it is difficult to find sufficient concept-based seeds needed in bootstrapping for the coverage of the variations of pro subcategories, another it is highly dependent on parser performance as well.
</nextsent>
<nextsent>research on product ner is still at its early stage, especially in chinese free text collections.
</nextsent>
<nextsent>however, considerable amount of workhas been done in the last decade on the general ner task and biological ner task.
</nextsent>
<nextsent>the typical machine learning approaches for english ne are transformation-based learning[aberdeen et al 1995], <papid> M95-1012 </papid>hidden markov model[bikel et al. 1997], <papid> A97-1029 </papid>maximum entropy model[borthwick, 1999], support vector machine learning[eunji yi et al 2004], unsupervised model[collins et al 1999]and etc. for chinese ner, the prevailing methodology applied recently also lie in machine learning combining other knowledge base or heuristic rules,which can be compared on the whole in three aspects showed in table 1.in short, the trend inner is to adopt statistical framework which try to exploit some knowledge base as well as different level of text features within and outside nes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1656">
<title id=" I05-3006.xml">product named entity recognition based on hierarchical hidden markov model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>research on product ner is still at its early stage, especially in chinese free text collections.
</prevsent>
<prevsent>however, considerable amount of workhas been done in the last decade on the general ner task and biological ner task.
</prevsent>
</prevsection>
<citsent citstr=" M95-1012 ">
the typical machine learning approaches for english ne are transformation-based learning[aberdeen et al 1995], <papid> M95-1012 </papid>hidden markov model[bikel et al. 1997], <papid> A97-1029 </papid>maximum entropy model[borthwick, 1999], support vector machine learning[eunji yi et al 2004], unsupervised model[collins et al 1999]and etc. for chinese ner, the prevailing methodology applied recently also lie in machine learning combining other knowledge base or heuristic rules,which can be compared on the whole in three aspects showed in table 1.in short, the trend inner is to adopt statistical framework which try to exploit some knowledge base as well as different level of text features within and outside nes.</citsent>
<aftsection>
<nextsent>further those ideas, we present hybrid approach based on hhmm [s.fine et al 1998] which will be described in detail.
</nextsent>
<nextsent>3.1 task definition.
</nextsent>
<nextsent>3.1.1 definition of product named entity in our study, only three kinds of product named entities are considered, namely 1note: lm(language model); me(maximum entropy).
</nextsent>
<nextsent>brand name(bra), product type(typ), productname(pro), and bra and typ are often embedded in pro.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1657">
<title id=" I05-3006.xml">product named entity recognition based on hierarchical hidden markov model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>research on product ner is still at its early stage, especially in chinese free text collections.
</prevsent>
<prevsent>however, considerable amount of workhas been done in the last decade on the general ner task and biological ner task.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
the typical machine learning approaches for english ne are transformation-based learning[aberdeen et al 1995], <papid> M95-1012 </papid>hidden markov model[bikel et al. 1997], <papid> A97-1029 </papid>maximum entropy model[borthwick, 1999], support vector machine learning[eunji yi et al 2004], unsupervised model[collins et al 1999]and etc. for chinese ner, the prevailing methodology applied recently also lie in machine learning combining other knowledge base or heuristic rules,which can be compared on the whole in three aspects showed in table 1.in short, the trend inner is to adopt statistical framework which try to exploit some knowledge base as well as different level of text features within and outside nes.</citsent>
<aftsection>
<nextsent>further those ideas, we present hybrid approach based on hhmm [s.fine et al 1998] which will be described in detail.
</nextsent>
<nextsent>3.1 task definition.
</nextsent>
<nextsent>3.1.1 definition of product named entity in our study, only three kinds of product named entities are considered, namely 1note: lm(language model); me(maximum entropy).
</nextsent>
<nextsent>brand name(bra), product type(typ), productname(pro), and bra and typ are often embedded in pro.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1658">
<title id=" I05-3004.xml">chinese classifier assignment using svms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>many asian languages (e.g. chinese, korean,japanese and thai) have numeral classifier systems.
</prevsent>
<prevsent>previous work on noun-classifier matching has been done in these languages.
</prevsent>
</prevsection>
<citsent citstr=" C94-1091 ">
(sorn lertlamvanich et al, 1994) <papid> C94-1091 </papid>present an algorithm for selecting an appropriate classifier for nounin thai.</citsent>
<aftsection>
<nextsent>the general idea is to extract noun classifier collocations from corpus, and output alist of noun-classifier pairs with frequency information.
</nextsent>
<nextsent>during noun phrase generation, the most frequently co-occurring classifier forgiven noun is selected.
</nextsent>
<nextsent>however, no evaluation is reported for this algorithm.
</nextsent>
<nextsent>the algorithm described in (paik and bond, 2001) generates japanese and korean numeral 25classifiers using semantic classes from an ontology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1659">
<title id=" I05-3004.xml">chinese classifier assignment using svms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the classifier assignment accuracy is 81%for japanese classifiers and 62% for korean classifiers.
</prevsent>
<prevsent>however, the evaluation set contains only90 noun phrases, which is pretty small.
</prevsent>
</prevsection>
<citsent citstr=" C02-1017 ">
furthermore, it is hard work to attach classifiers to an ontology by hand, and with this approach it is hardto deal with cases like the cattle example mentioned earlier.(paul et al, 2002) <papid> C02-1017 </papid>present method forex tracting classifier information from bilingual(japanese-english) corpus based on phrasal correspondences in the sentential context.</citsent>
<aftsection>
<nextsent>bilingual sentence pairs are compared to find noun classifier collocations.
</nextsent>
<nextsent>the evaluation was done by human.
</nextsent>
<nextsent>the precision is high (84.2%) but the recall is only about 40% because the algorithm does not give output for half of the nouns.
</nextsent>
<nextsent>in contrast to these algorithms, our approach: is based on large data set; uses machine learning; and does not require the attachment of classifiers to an ontology by hand.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1660">
<title id=" I05-3004.xml">chinese classifier assignment using svms </title>
<section> data and resources.  </section>
<citcontext>
<prevsection>
<prevsent>we chose libsvm (chang and lin,2001), which is an efficient multi-class implementation.
</prevsent>
<prevsent>libsvm uses the one-against-oneapproach in which k(k? 1)/2 classifiers are constructed and each one trains on data from two different classes (hsu and lin, 2002).
</prevsent>
</prevsection>
<citsent citstr=" C02-1145 ">
we use the penn chinese treebank (xue et al, 2002) <papid> C02-1145 </papid>as our corpus and the ontology/lexiconhownet (dong and dong, 2000) to get onto logical features for nouns.</citsent>
<aftsection>
<nextsent>we train svms on different feature sets to see which set(s) of features are important for noun-classifier matching.
</nextsent>
<nextsent>4.1 penn chinese treebank.
</nextsent>
<nextsent>the penn chinese treebank is 500,000 word chinese corpus annotated with both part-of speech (pos) tags and syntactic brackets.
</nextsent>
<nextsent>we automatically extract noun phrases that contain classifiers from the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1663">
<title id=" I08-1009.xml">a more discerning and adaptable multilingual transliteration mechanism for indian languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(sinha, 2001) had used hindi transliteration used to handle unknowns in mt. naukri (a popular domain name) 722,000 nokri (domain name) 19,800 naukari 10,500 naukary (domain name) 5,490 nokari 665 naukarii 133 nauka ree 102 table 1: variations of hindi word nokari (job).
</prevsent>
<prevsent>the numbers are pages returned when searching on google.
</prevsent>
</prevsection>
<citsent citstr=" W05-0808 ">
1www.google.co.in/press/pressrel/news transliteration.html 2www.quillpad.com aswani et. al (aswani and gaizauskas, 2005) <papid> W05-0808 </papid>have used transliteration similarity mechanism toalign english-hindi parallel texts.</citsent>
<aftsection>
<nextsent>they used character based direct correspondences between hindi and english to produce possible transliterations.then they apply edit distance based similarity to select the most probable transliteration in the english text.
</nextsent>
<nextsent>however, such method can only be appropriate for aligning parallel texts as the number of possible candidates is quite small.the paper is structured as follows.
</nextsent>
<nextsent>in section2, we discuss the problem of high degree of variation in indian words, especially when written in latin script.
</nextsent>
<nextsent>in section-3, we explain the idea ofusing information about the word origin for improving transliteration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1664">
<title id=" I08-1009.xml">a more discerning and adaptable multilingual transliteration mechanism for indian languages </title>
<section> word origin and two ways of.  </section>
<citcontext>
<prevsection>
<prevsent>at present the situation is that most indians use indian scripts while writing in ils, but use the latin script when communicatingonline.
</prevsent>
<prevsent>ils are rarely used for official communication, except in government offices in some states.
</prevsent>
</prevsection>
<citsent citstr=" C02-1099 ">
transliteration previous work for other languages has shown that word origin plays part in how the word should be transliterated(oh and choi, 2002; <papid> C02-1099 </papid>may et al, 2004).</citsent>
<aftsection>
<nextsent>llitjos and black (llitjos and black, 2001) had shown that the knowledge of language origin can substantially improve pronunciation generationaccuracy.
</nextsent>
<nextsent>this information has been used to get better results (oh and choi, 2002).<papid> C02-1099 </papid></nextsent>
<nextsent>they first checked whether the word origin is greek or not before selecting one of the two methods for transliteration.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1666">
<title id=" I08-1009.xml">a more discerning and adaptable multilingual transliteration mechanism for indian languages </title>
<section> disambiguating word origin.  </section>
<citcontext>
<prevsection>
<prevsent>two different methods are used for transliterating, as explained later.
</prevsent>
<prevsent>previously (llitjos and black, 2001) used probabilities of all trigrams to belong to particular language as an measure to disambiguate word origins.
</prevsent>
</prevsection>
<citsent citstr=" W06-1109 ">
weuse more sophisticated method that has been successfully used for language and encoding identification (singh, 2006<papid> W06-1109 </papid>a).</citsent>
<aftsection>
<nextsent>we first prepare letter based 5-gram models from the lists of two kinds of words (indian and foreign).
</nextsent>
<nextsent>then we combine n-grams of all orders and rank them according to their probability in descending order.
</nextsent>
<nextsent>only the top n-grams are retained and therest are pruned.
</nextsent>
<nextsent>now we have two probability distributions which can be compared by measure of distributional similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1675">
<title id=" I08-3007.xml">joint grammar development by linguists and computer scientists </title>
<section> problems for grammar development.  </section>
<citcontext>
<prevsection>
<prevsent>transducer, that is, program which functions to both parse and generate inflected words.
</prevsent>
<prevsent>however, because it is more familiar, in this paper we will frequently use the term parser.?
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
while there has been considerable research into automatically deriving morphological parser from corpus (see for example creutz and lagus, 2007; goldsmith, 2001; <papid> J01-2001 </papid>goldsmith and hu, 2004; and the papers in maxwell, 2002), the results are still far from producing reliable, wide-coverage parsers.</citsent>
<aftsection>
<nextsent>hence most morphological parsers are still built by hand.
</nextsent>
<nextsent>this paper focuses on practical aspects of how such parsers can best be built, and presents model for collaborative development.
</nextsent>
<nextsent>hand-built parsers suffer from at least two drawbacks, which we will call the expertise problem?
</nextsent>
<nextsent>and the half-life problem.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1676">
<title id=" I08-3007.xml">joint grammar development by linguists and computer scientists </title>
<section> problems for grammar development.  </section>
<citcontext>
<prevsection>
<prevsent>the difficulty is compounded when the number of speakers of the language is small.
</prevsent>
<prevsent>we therefore believe that for many languages of the world, for the near future, the way to develop computational tools in general, and morphological parsers in particular, lies in teamwork.
</prevsent>
</prevsection>
<citsent citstr=" J01-1003 ">
an example of the team approach was the boas project (oflazer et al, 2001).<papid> J01-1003 </papid></citsent>
<aftsection>
<nextsent>a boas team consisted of two peoplea language informant?
</nextsent>
<nextsent>and programmer plus computer program which interviewed the informant and created the grammar rules.
</nextsent>
<nextsent>the computer program is described as linguist in box?
</nextsent>
<nextsent>(oflazer et al, 61).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1677">
<title id=" I08-3015.xml">morphology driven mani puri pos tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the majority of the roots found in the language are bound and the affixes are the determining factor of the class of the words in the language.
</prevsent>
<prevsent>classification of words using the role of affix helps to implement the tagger for resource poor language like mani puri with high performance.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
there are many pos taggers developed using different techniques for many major languages such as transformation-based error-driven learning (brill, 1995), <papid> J95-4004 </papid>decision trees (black et al, 1992), <papid> H92-1023 </papid>markov model (cutting et al, 1992), <papid> A92-1018 </papid>maximum entropy methods (ratnaparkhi, 1996) etc for english.</citsent>
<aftsection>
<nextsent>decision trees are used to estimate marginal probabilities in maximum entropy model for predicting the parts-of-speech of word given the context in which it appears (black et al, 1992).<papid> H92-1023 </papid></nextsent>
<nextsent>the rules in rule-based system are usually difficult to construct and typically are not very robust (brill, 1992).<papid> A92-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1678">
<title id=" I08-3015.xml">morphology driven mani puri pos tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the majority of the roots found in the language are bound and the affixes are the determining factor of the class of the words in the language.
</prevsent>
<prevsent>classification of words using the role of affix helps to implement the tagger for resource poor language like mani puri with high performance.
</prevsent>
</prevsection>
<citsent citstr=" H92-1023 ">
there are many pos taggers developed using different techniques for many major languages such as transformation-based error-driven learning (brill, 1995), <papid> J95-4004 </papid>decision trees (black et al, 1992), <papid> H92-1023 </papid>markov model (cutting et al, 1992), <papid> A92-1018 </papid>maximum entropy methods (ratnaparkhi, 1996) etc for english.</citsent>
<aftsection>
<nextsent>decision trees are used to estimate marginal probabilities in maximum entropy model for predicting the parts-of-speech of word given the context in which it appears (black et al, 1992).<papid> H92-1023 </papid></nextsent>
<nextsent>the rules in rule-based system are usually difficult to construct and typically are not very robust (brill, 1992).<papid> A92-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1679">
<title id=" I08-3015.xml">morphology driven mani puri pos tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the majority of the roots found in the language are bound and the affixes are the determining factor of the class of the words in the language.
</prevsent>
<prevsent>classification of words using the role of affix helps to implement the tagger for resource poor language like mani puri with high performance.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
there are many pos taggers developed using different techniques for many major languages such as transformation-based error-driven learning (brill, 1995), <papid> J95-4004 </papid>decision trees (black et al, 1992), <papid> H92-1023 </papid>markov model (cutting et al, 1992), <papid> A92-1018 </papid>maximum entropy methods (ratnaparkhi, 1996) etc for english.</citsent>
<aftsection>
<nextsent>decision trees are used to estimate marginal probabilities in maximum entropy model for predicting the parts-of-speech of word given the context in which it appears (black et al, 1992).<papid> H92-1023 </papid></nextsent>
<nextsent>the rules in rule-based system are usually difficult to construct and typically are not very robust (brill, 1992).<papid> A92-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1681">
<title id=" I08-3015.xml">morphology driven mani puri pos tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are many pos taggers developed using different techniques for many major languages such as transformation-based error-driven learning (brill, 1995), <papid> J95-4004 </papid>decision trees (black et al, 1992), <papid> H92-1023 </papid>markov model (cutting et al, 1992), <papid> A92-1018 </papid>maximum entropy methods (ratnaparkhi, 1996) etc for english.</prevsent>
<prevsent>decision trees are used to estimate marginal probabilities in maximum entropy model for predicting the parts-of-speech of word given the context in which it appears (black et al, 1992).<papid> H92-1023 </papid></prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
the rules in rule-based system are usually difficult to construct and typically are not very robust (brill, 1992).<papid> A92-1021 </papid></citsent>
<aftsection>
<nextsent>large tables of statistics are not needed for the rule-based tagger.
</nextsent>
<nextsent>in stochastic tagger, tens of thousands of lines of statistical information are needed to capture the contextual information (brill, 1992).<papid> A92-1021 </papid></nextsent>
<nextsent>for tagger to function as practical component in language processing system, tagger must be robust, efficient, accurate, tunable and reusable (cutting, 1992).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1685">
<title id=" I08-3015.xml">morphology driven mani puri pos tagger </title>
<section> previous work on mani puri pos tagger.  </section>
<citcontext>
<prevsection>
<prevsent>in stochastic tagger, tens of thousands of lines of statistical information are needed to capture the contextual information (brill, 1992).<papid> A92-1021 </papid></prevsent>
<prevsent>for tagger to function as practical component in language processing system, tagger must be robust, efficient, accurate, tunable and reusable (cutting, 1992).</prevsent>
</prevsection>
<citsent citstr=" A94-1024 ">
morphology based pos tagging of some languages like turkish (oflazer and kuruoz, 1994), <papid> A94-1024 </papid>czech (hajic, et al, 2001) has been tried out using combination of hand-crafted rules and statistical learning.</citsent>
<aftsection>
<nextsent>a marathi rule based pos tagger used technique called srr (suffix replacement rule) (burange et al, 2006) with considerable accuracy.
</nextsent>
<nextsent>a pos tagger for hindi overcomes the handicap of annotated corpora scarcity by exploiting the rich morphology of the language (singh et al, 2006).<papid> P06-2100 </papid></nextsent>
<nextsent>to the best of our knowledge, there is no record available of work done on mani puri pos tagger.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1686">
<title id=" I08-3015.xml">morphology driven mani puri pos tagger </title>
<section> previous work on mani puri pos tagger.  </section>
<citcontext>
<prevsection>
<prevsent>morphology based pos tagging of some languages like turkish (oflazer and kuruoz, 1994), <papid> A94-1024 </papid>czech (hajic, et al, 2001) has been tried out using combination of hand-crafted rules and statistical learning.</prevsent>
<prevsent>a marathi rule based pos tagger used technique called srr (suffix replacement rule) (burange et al, 2006) with considerable accuracy.</prevsent>
</prevsection>
<citsent citstr=" P06-2100 ">
a pos tagger for hindi overcomes the handicap of annotated corpora scarcity by exploiting the rich morphology of the language (singh et al, 2006).<papid> P06-2100 </papid></citsent>
<aftsection>
<nextsent>to the best of our knowledge, there is no record available of work done on mani puri pos tagger.
</nextsent>
<nextsent>a related work of word class and sentence type identification in mani puri morphological analyzer 91 is found in (thoudam and bandyopadhyay, 2006) where the classification of few word categories and sentence type identification are discussed based on affix rules.
</nextsent>
<nextsent>there are free and bound roots in manipuri.
</nextsent>
<nextsent>all the verb roots are bound roots.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1687">
<title id=" I08-2131.xml">language independent text correction using finite state automata </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the proposed approach is language independent and requires only dictionary and text datafor building language model.
</prevsent>
<prevsent>the approach have been tested on both arabic and english text and achieved accuracy of 89%.
</prevsent>
</prevsection>
<citsent citstr=" J95-2004 ">
the problem of detecting and correcting misspelled words in text has received great attention due toits importance in several applications like text editing systems, optical character recognition systems, and morphological analysis and tagging (roche and schabes, 1995).<papid> J95-2004 </papid></citsent>
<aftsection>
<nextsent>other applications, like machine translation and information extraction, operate on text that might have spelling errors.
</nextsent>
<nextsent>the automatic detection, and correction of spelling erros should be of great help to those applications.the problem of detecting and correcting misspelled words in text is usually solved by checking whether word already exists in the dictionary ornot.
</nextsent>
<nextsent>if not, we try to extract words from the dictionary that are most similar to the word in question.now with the university of michigan ann arbor, has sanam@umich.edu those words are reported as candidate corrections for the misspelled word.similarity between the misspelled word and dictionary words is measured by the levenshtein edit distance (levenshtein, 1966; wagner and m.fisher,1974).
</nextsent>
<nextsent>the levenshtein edit distance is usually calculated using dynamic programming technique with quadratic time complexity (wagner andm.fisher, 1974).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1688">
<title id=" I08-2131.xml">language independent text correction using finite state automata </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the input word is compared to words in classes that have similar features only.
</prevsent>
<prevsent>in addition to the techniques discussed above, other techniques from finite state automata have been recently proposed.
</prevsent>
</prevsection>
<citsent citstr=" J96-1003 ">
(oflazer, 1996) <papid> J96-1003 </papid>suggested method where all words in dictionary are treated as regular language over an alphabet of letters.</citsent>
<aftsection>
<nextsent>all the words are represented by finite state machineautomaton.
</nextsent>
<nextsent>for each garbled input word, an exhaustive traversal of the dictionary automaton is initiated using variant of wagner-fisher algorithm (wag ner and m.fisher, 1974) to control the traversal ofthe dictionary.
</nextsent>
<nextsent>in this approach levenshtein distance is calculated several times during the traversal.
</nextsent>
<nextsent>the method carefully traverses the dictionary such that the inspection of most of the dictionary states is avoided.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1689">
<title id=" I05-3023.xml">perceptron learning for chinese word segmentation </title>
<section> character based chinese word.  </section>
<citcontext>
<prevsection>
<prevsent>we explored simple and effective learning algorithm, the perceptron with uneven margins (paum) for chinese word segmentation task.for the open task, we only used the minimal external information ? the utf-8 code knowledge to distinguish latin characters and arabic numbers from other characters, justified by the fact that the english text requires no segmentation since they has been segmented already, and another fact that any arabic number in one particular context should have the same segmentation.
</prevsent>
<prevsent>segmentation we adopted the character based methodology for chinese word segmentation, in which every character in sentence was checked one by one tosee if it was word on its own or it was beginning, middle, or end character of multi-characterword.
</prevsent>
</prevsection>
<citsent citstr=" W03-1721 ">
in contrast, another commonly used strategy, the word based methodology segments chinese sentence into the words in pre-defined word list possibly with probability information about each word, according to some maximum probability criteria ( see e.g. chen (2003)).<papid> W03-1721 </papid></citsent>
<aftsection>
<nextsent>the performance of word based segmentation is dependent upon the quality of word list used, while the character based method does not need any word list ? it segments sentence only based on the characters in the sentence.using character based methodology, we transform the word segmentation problem into four binary classification problems, corresponding to single-character word, the beginning, middle andend character of multi-character word, respectively.
</nextsent>
<nextsent>for each of the four classes classifier was learnt from training set using the one vs. all others paradigm, in which every character in the training data belonging to the class considered was regarded as positive example and all other characters were negative examples.
</nextsent>
<nextsent>after learning, we applied the four classifiers toeach character in test text and assigned the character the class which classifier had the maximal output among the four.
</nextsent>
<nextsent>this kind of strategy has been widely used in the applications of machine learning to named entity recognition and has also 154 been used in chinese word segmentation (xue and shen, 2003).<papid> W03-1728 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1690">
<title id=" I05-3023.xml">perceptron learning for chinese word segmentation </title>
<section> character based chinese word.  </section>
<citcontext>
<prevsection>
<prevsent>for each of the four classes classifier was learnt from training set using the one vs. all others paradigm, in which every character in the training data belonging to the class considered was regarded as positive example and all other characters were negative examples.
</prevsent>
<prevsent>after learning, we applied the four classifiers toeach character in test text and assigned the character the class which classifier had the maximal output among the four.
</prevsent>
</prevsection>
<citsent citstr=" W03-1728 ">
this kind of strategy has been widely used in the applications of machine learning to named entity recognition and has also 154 been used in chinese word segmentation (xue and shen, 2003).<papid> W03-1728 </papid></citsent>
<aftsection>
<nextsent>finally word delimiter (often blank space, depending on particular corpus) was added to the right of one character if it was not the last character of sentence and it was predicted as end character of word or as single character word.
</nextsent>
<nextsent>perceptron is simple and effective learning algorithm.
</nextsent>
<nextsent>for binary classification problem, it checks the training examples one by one by predicting their labels.
</nextsent>
<nextsent>if the prediction is correct, the example is passed; otherwise, the example is used to correct the model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1691">
<title id=" I05-3023.xml">perceptron learning for chinese word segmentation </title>
<section> learning algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>into the update rules for the positive and negative examples, respectively.
</prevsent>
<prevsent>two margin parameters allow the paum to handle imbalanced datasets better than both the standard perceptron and the margin perceptron.
</prevsent>
</prevsection>
<citsent citstr=" W05-0610 ">
paumhas been successfully used for document classification and information extraction (li et al, 2005).<papid> W05-0610 </papid>we used the paum algorithm to train classifier for each of four classes for chinese word segmentation.</citsent>
<aftsection>
<nextsent>for one test example, the output of the perceptron classifier before thresholding was used for comparison among the four classifiers.the important parameters of the learning algorithm are the uneven margins parameters ?+ and ??.
</nextsent>
<nextsent>in all our experiments ?+ = 20 and ??
</nextsent>
<nextsent>= 1 were used.
</nextsent>
<nextsent>table 1 presents the results for each of the four classification problems, obtained from 4-fold cross-validation on training set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1693">
<title id=" I05-3023.xml">perceptron learning for chinese word segmentation </title>
<section> features for each character.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, this kind of semi-quadratic kernel was used in our submissions.
</prevsent>
<prevsent>table 3: comparisons between different kernels for perceptron: f1 (%) averaged over 4-fold cross-validation on three training sets.
</prevsent>
</prevsection>
<citsent citstr=" W03-0422 ">
linear quadratic semi-quadratic cityu 81.30 94.78 95.13 msr 79.80 94.78 94.93 pku 82.33 94.80 95.05actually it has been noted that quadratic kernel for perceptron, as well as for svm, performed better than linear kernel for information extraction and other nlp tasks (see e.g.carreras et al (2003)).<papid> W03-0422 </papid></citsent>
<aftsection>
<nextsent>however, quadratic kernel was usually implemented in dual form for perceptron and it took very long time for training.
</nextsent>
<nextsent>we implemented the quadratic kernel for perceptron in primal form by encoding the linea rand quadratic features into feature vector explicitly.
</nextsent>
<nextsent>actually our implementation performed even slightly better than the perceptron with quadratic kernel as we used only part of quadratic features, and it was still as efficient as the perceptron with linear kernel.
</nextsent>
<nextsent>while closed test required the participants only to use the information presented in training material, open test allowed to use any external information or resources besides the training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1694">
<title id=" I05-1023.xml">inversion transduction grammar constraints for mining parallel sentences from quasi comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but while loose translations by themselves already have numerous applications, truly parallel sentence translations provide invaluable types of information for the aforementioned types of mining and induction, which cannot easily be obtained from merely loose translations or comparable sentence pairs.
</prevsent>
<prevsent>in particular, truly parallel bi-sentences are especially useful for extracting more precise syntactic and semantic relations within word sequences.
</prevsent>
</prevsection>
<citsent citstr=" P95-1033 ">
we present new method that exploits novel application of inversion transduction grammar or itg expressiveness constraints (wu 1995 [<papid> P95-1033 </papid>1], wu 1997 [<papid> J97-3002 </papid>2]) for mining monolingual data to obtain tight sentence translation pairs, yielding accuracy significantly higher than previous known methods.</citsent>
<aftsection>
<nextsent>we focus here on very non-parallel quasi-comparable monolingual corpora, which are available in far larger quantities but are significantly more difficult to mine than either noisy parallel corpora or comparable corpora.
</nextsent>
<nextsent>the majority of previous work has concerned noisy parallel corpora (sometimes imprecisely also called comparable corpora?), which contain non-aligned sentences that are nevertheless mostly bilingual translations of the same document.
</nextsent>
<nextsent>more recent work has examined comparable corpora, which contain non-sentence-aligned, non-translated bilingual documents that are topic-aligned.
</nextsent>
<nextsent>still relatively few methods attempt tomine quasi-comparable corpora, which contain far more heterogeneous, very non parallel bilingual documents that could be either on the same topic (in-topic) or not (off-topic).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1695">
<title id=" I05-1023.xml">inversion transduction grammar constraints for mining parallel sentences from quasi comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but while loose translations by themselves already have numerous applications, truly parallel sentence translations provide invaluable types of information for the aforementioned types of mining and induction, which cannot easily be obtained from merely loose translations or comparable sentence pairs.
</prevsent>
<prevsent>in particular, truly parallel bi-sentences are especially useful for extracting more precise syntactic and semantic relations within word sequences.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
we present new method that exploits novel application of inversion transduction grammar or itg expressiveness constraints (wu 1995 [<papid> P95-1033 </papid>1], wu 1997 [<papid> J97-3002 </papid>2]) for mining monolingual data to obtain tight sentence translation pairs, yielding accuracy significantly higher than previous known methods.</citsent>
<aftsection>
<nextsent>we focus here on very non-parallel quasi-comparable monolingual corpora, which are available in far larger quantities but are significantly more difficult to mine than either noisy parallel corpora or comparable corpora.
</nextsent>
<nextsent>the majority of previous work has concerned noisy parallel corpora (sometimes imprecisely also called comparable corpora?), which contain non-aligned sentences that are nevertheless mostly bilingual translations of the same document.
</nextsent>
<nextsent>more recent work has examined comparable corpora, which contain non-sentence-aligned, non-translated bilingual documents that are topic-aligned.
</nextsent>
<nextsent>still relatively few methods attempt tomine quasi-comparable corpora, which contain far more heterogeneous, very non parallel bilingual documents that could be either on the same topic (in-topic) or not (off-topic).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1696">
<title id=" I05-1023.xml">inversion transduction grammar constraints for mining parallel sentences from quasi comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>conversely, itgs should do well at rejecting candidates where (1) too many words in one sentence find no correspondence in the other, (2) frames do not nest in similar ways in the candidate sentence pair, or (3) too many arguments must be transposed to achieve an alignment all of which would suggest that the sentences probably express different ideas.
</prevsent>
<prevsent>various forms of empirical confirmation for the itg hypothesis have emerged recently, which quantitatively support the qualitative cross-linguistic characteristics just described across variety of language pairs and tasks.
</prevsent>
</prevsection>
<citsent citstr=" P03-1019 ">
zens and ney (2003) [<papid> P03-1019 </papid>3] show that itg constraints yield significantly better alignment coverage than the constraints used in ibm statistical machine translation models on both german-english (verbmobil corpus) and french-english (canadian hansa rds corpus).</citsent>
<aftsection>
<nextsent>zhang and gildea (2004) [<papid> C04-1060 </papid>4] found that unsupervised alignment using bracketing itgs produces significantly lower chinese-english alignment error rates than syntactically supervised tree-to-string model [5].</nextsent>
<nextsent>zhang and gildea (2005) [<papid> P05-1059 </papid>6] show that lexicalized itgs can further improve alignment accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1697">
<title id=" I05-1023.xml">inversion transduction grammar constraints for mining parallel sentences from quasi comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>various forms of empirical confirmation for the itg hypothesis have emerged recently, which quantitatively support the qualitative cross-linguistic characteristics just described across variety of language pairs and tasks.
</prevsent>
<prevsent>zens and ney (2003) [<papid> P03-1019 </papid>3] show that itg constraints yield significantly better alignment coverage than the constraints used in ibm statistical machine translation models on both german-english (verbmobil corpus) and french-english (canadian hansa rds corpus).</prevsent>
</prevsection>
<citsent citstr=" C04-1060 ">
zhang and gildea (2004) [<papid> C04-1060 </papid>4] found that unsupervised alignment using bracketing itgs produces significantly lower chinese-english alignment error rates than syntactically supervised tree-to-string model [5].</citsent>
<aftsection>
<nextsent>zhang and gildea (2005) [<papid> P05-1059 </papid>6] show that lexicalized itgs can further improve alignment accuracy.</nextsent>
<nextsent>with regard to translation rather than alignment accuracy, zens et al  (2004) [<papid> C04-1030 </papid>7] show that decoding under itg constraints yields significantly lower word error rates and bleu scores than the ibm constraints.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1698">
<title id=" I05-1023.xml">inversion transduction grammar constraints for mining parallel sentences from quasi comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>zens and ney (2003) [<papid> P03-1019 </papid>3] show that itg constraints yield significantly better alignment coverage than the constraints used in ibm statistical machine translation models on both german-english (verbmobil corpus) and french-english (canadian hansa rds corpus).</prevsent>
<prevsent>zhang and gildea (2004) [<papid> C04-1060 </papid>4] found that unsupervised alignment using bracketing itgs produces significantly lower chinese-english alignment error rates than syntactically supervised tree-to-string model [5].</prevsent>
</prevsection>
<citsent citstr=" P05-1059 ">
zhang and gildea (2005) [<papid> P05-1059 </papid>6] show that lexicalized itgs can further improve alignment accuracy.</citsent>
<aftsection>
<nextsent>with regard to translation rather than alignment accuracy, zens et al  (2004) [<papid> C04-1030 </papid>7] show that decoding under itg constraints yields significantly lower word error rates and bleu scores than the ibm constraints.</nextsent>
<nextsent>chiang (2005) [<papid> P05-1033 </papid>8] obtains significant bleu score improvements via unsupervised induction of hierarchical phrasal bracketing itgs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1699">
<title id=" I05-1023.xml">inversion transduction grammar constraints for mining parallel sentences from quasi comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>zhang and gildea (2004) [<papid> C04-1060 </papid>4] found that unsupervised alignment using bracketing itgs produces significantly lower chinese-english alignment error rates than syntactically supervised tree-to-string model [5].</prevsent>
<prevsent>zhang and gildea (2005) [<papid> P05-1059 </papid>6] show that lexicalized itgs can further improve alignment accuracy.</prevsent>
</prevsection>
<citsent citstr=" C04-1030 ">
with regard to translation rather than alignment accuracy, zens et al  (2004) [<papid> C04-1030 </papid>7] show that decoding under itg constraints yields significantly lower word error rates and bleu scores than the ibm constraints.</citsent>
<aftsection>
<nextsent>chiang (2005) [<papid> P05-1033 </papid>8] obtains significant bleu score improvements via unsupervised induction of hierarchical phrasal bracketing itgs.</nextsent>
<nextsent>such results partly motivate the work we discuss here.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1700">
<title id=" I05-1023.xml">inversion transduction grammar constraints for mining parallel sentences from quasi comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>zhang and gildea (2005) [<papid> P05-1059 </papid>6] show that lexicalized itgs can further improve alignment accuracy.</prevsent>
<prevsent>with regard to translation rather than alignment accuracy, zens et al  (2004) [<papid> C04-1030 </papid>7] show that decoding under itg constraints yields significantly lower word error rates and bleu scores than the ibm constraints.</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
chiang (2005) [<papid> P05-1033 </papid>8] obtains significant bleu score improvements via unsupervised induction of hierarchical phrasal bracketing itgs.</citsent>
<aftsection>
<nextsent>such results partly motivate the work we discuss here.
</nextsent>
<nextsent>we will begin by surveying recent related work and reviewing the formal properties of itgs.
</nextsent>
<nextsent>subsequently we describe the architecture of our new method, which relies on multiple stages so as to balance efficiency and accuracy considerations.
</nextsent>
<nextsent>finally we discuss experimental results on quasi-comparable corpus of chinese and english from the topic detection task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1701">
<title id=" I05-1023.xml">inversion transduction grammar constraints for mining parallel sentences from quasi comparable corpora </title>
<section> recent approaches to mining non-parallel corpora.  </section>
<citcontext>
<prevsection>
<prevsent>subsequently we describe the architecture of our new method, which relies on multiple stages so as to balance efficiency and accuracy considerations.
</prevsent>
<prevsent>finally we discuss experimental results on quasi-comparable corpus of chinese and english from the topic detection task.
</prevsent>
</prevsection>
<citsent citstr=" W04-3208 ">
recent work (fung and cheung 2004 [<papid> W04-3208 </papid>9]; munteanu et al  2004 [<papid> N04-1034 </papid>10]; zhao and vogel 2002 [11]) on extracting bi-sentences from comparable corpora is largely based on finding on-topic documents first through similarity matching and time alignment.</citsent>
<aftsection>
<nextsent>however, zhao and vogel used corpus of chinese and english versions of news stories from the xinhua newsagency, with roughly similar sentence order 260 d. wu and p. fungof content?.
</nextsent>
<nextsent>this corpus can be more accurately described as noisy parallel corpus.
</nextsent>
<nextsent>munteanu et al  used comparable corpora of news articles published within the same 5-day window.
</nextsent>
<nextsent>in both cases, the corpora contain documents on the same matching topics; unlike our present objective of mining quasi-comparable corpora, these other methods assume corpora of on-topic documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1702">
<title id=" I05-1023.xml">inversion transduction grammar constraints for mining parallel sentences from quasi comparable corpora </title>
<section> recent approaches to mining non-parallel corpora.  </section>
<citcontext>
<prevsection>
<prevsent>subsequently we describe the architecture of our new method, which relies on multiple stages so as to balance efficiency and accuracy considerations.
</prevsent>
<prevsent>finally we discuss experimental results on quasi-comparable corpus of chinese and english from the topic detection task.
</prevsent>
</prevsection>
<citsent citstr=" N04-1034 ">
recent work (fung and cheung 2004 [<papid> W04-3208 </papid>9]; munteanu et al  2004 [<papid> N04-1034 </papid>10]; zhao and vogel 2002 [11]) on extracting bi-sentences from comparable corpora is largely based on finding on-topic documents first through similarity matching and time alignment.</citsent>
<aftsection>
<nextsent>however, zhao and vogel used corpus of chinese and english versions of news stories from the xinhua newsagency, with roughly similar sentence order 260 d. wu and p. fungof content?.
</nextsent>
<nextsent>this corpus can be more accurately described as noisy parallel corpus.
</nextsent>
<nextsent>munteanu et al  used comparable corpora of news articles published within the same 5-day window.
</nextsent>
<nextsent>in both cases, the corpora contain documents on the same matching topics; unlike our present objective of mining quasi-comparable corpora, these other methods assume corpora of on-topic documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1704">
<title id=" I05-1023.xml">inversion transduction grammar constraints for mining parallel sentences from quasi comparable corpora </title>
<section> candidate generation.  </section>
<citcontext>
<prevsection>
<prevsent>the documents are word segmented with the linguistic data consortium (ldc) chinese-english dictionary 2.0.
</prevsent>
<prevsent>the chinese document is then glossed using all the dictionary entries.
</prevsent>
</prevsection>
<citsent citstr=" P99-1043 ">
when chinese word has multiple possible translations in english, it is disambiguated using an extension of fung et al (1999) <papid> P99-1043 </papid>method [13].initial document matching.</citsent>
<aftsection>
<nextsent>the aim of this step is to roughly match the chinese english documents pairs that are on-topic, in order to extract parallel sentences from them.
</nextsent>
<nextsent>following previous work, cosine similarity between document vectors is used to judge whether bilingual document pair is on-topic or off-topic.
</nextsent>
<nextsent>both the glossed chinese document and english are represented in word vectors, with term weights.
</nextsent>
<nextsent>pair-wise similarities are calculated for all possible chinese-english document pairs, and bilingual documents with similarities above certain threshold are considered to be comparable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1706">
<title id=" I08-2119.xml">a reexamination of dependency path kernels for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, inthe sentence the funeral was scheduled for thursday in paris at the saint-germain-des-pres church?, the organization saint-germain-des-pres church islocated-in?
</prevsent>
<prevsent>gpe paris.
</prevsent>
</prevsection>
<citsent citstr=" H05-1091 ">
the second type of collection that has been widely studied is biomedical literature (bunescu and mooney, 2005<papid> H05-1091 </papid>b; giuliano et al,2006; <papid> E06-1051 </papid>mcdonald et al, 2005<papid> P05-1061 </papid>b), promoted by evaluation programs such as bio creative and jnlpba 2004.</citsent>
<aftsection>
<nextsent>in this particular domain, studies often focus on specific entities such as genes and proteins.
</nextsent>
<nextsent>andthe kinds of relations to extract are usually gene-to protein interactions.
</nextsent>
<nextsent>the predominant approach to relation extraction treats the task as multi-class classification problem, in which different relation types form different output classes.
</nextsent>
<nextsent>early work employed diverse range of features in linear classifier (commonlyreferred to as feature-based?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1715">
<title id=" I08-2119.xml">a reexamination of dependency path kernels for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, inthe sentence the funeral was scheduled for thursday in paris at the saint-germain-des-pres church?, the organization saint-germain-des-pres church islocated-in?
</prevsent>
<prevsent>gpe paris.
</prevsent>
</prevsection>
<citsent citstr=" E06-1051 ">
the second type of collection that has been widely studied is biomedical literature (bunescu and mooney, 2005<papid> H05-1091 </papid>b; giuliano et al,2006; <papid> E06-1051 </papid>mcdonald et al, 2005<papid> P05-1061 </papid>b), promoted by evaluation programs such as bio creative and jnlpba 2004.</citsent>
<aftsection>
<nextsent>in this particular domain, studies often focus on specific entities such as genes and proteins.
</nextsent>
<nextsent>andthe kinds of relations to extract are usually gene-to protein interactions.
</nextsent>
<nextsent>the predominant approach to relation extraction treats the task as multi-class classification problem, in which different relation types form different output classes.
</nextsent>
<nextsent>early work employed diverse range of features in linear classifier (commonlyreferred to as feature-based?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1716">
<title id=" I08-2119.xml">a reexamination of dependency path kernels for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, inthe sentence the funeral was scheduled for thursday in paris at the saint-germain-des-pres church?, the organization saint-germain-des-pres church islocated-in?
</prevsent>
<prevsent>gpe paris.
</prevsent>
</prevsection>
<citsent citstr=" P05-1061 ">
the second type of collection that has been widely studied is biomedical literature (bunescu and mooney, 2005<papid> H05-1091 </papid>b; giuliano et al,2006; <papid> E06-1051 </papid>mcdonald et al, 2005<papid> P05-1061 </papid>b), promoted by evaluation programs such as bio creative and jnlpba 2004.</citsent>
<aftsection>
<nextsent>in this particular domain, studies often focus on specific entities such as genes and proteins.
</nextsent>
<nextsent>andthe kinds of relations to extract are usually gene-to protein interactions.
</nextsent>
<nextsent>the predominant approach to relation extraction treats the task as multi-class classification problem, in which different relation types form different output classes.
</nextsent>
<nextsent>early work employed diverse range of features in linear classifier (commonlyreferred to as feature-based?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1720">
<title id=" I08-2119.xml">a reexamination of dependency path kernels for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the predominant approach to relation extraction treats the task as multi-class classification problem, in which different relation types form different output classes.
</prevsent>
<prevsent>early work employed diverse range of features in linear classifier (commonlyreferred to as feature-based?
</prevsent>
</prevsection>
<citsent citstr=" N07-1015 ">
approaches), including lexical features, syntactic parse features, dependency features and semantic features (jiang and zhai, 2007; <papid> N07-1015 </papid>kambhatla, 2004; <papid> P04-3022 </papid>zhou et al, 2005).</citsent>
<aftsection>
<nextsent>these approaches were hindered by drawbacks suchas limited feature space and excessive feature engineering.
</nextsent>
<nextsent>kernel methods (cortes and vapnik, 1995; cristianini and shawe-taylor, 2000) on the other hand can explore much larger feature space very efficiently.
</nextsent>
<nextsent>recent studies on relation extraction have shown that by combining kernels withsupport-vector machines (svm), one can obtain results superior to feature-based methods (bunescu 841 and mooney, 2005b; bunescu and mooney, 2005<papid> H05-1091 </papid>a; culotta and sorensen, 2004; <papid> P04-1054 </papid>cumby and roth, 2003; zelenko et al, 2003; zhang et al, 2006<papid> P06-1104 </papid>a; zhang et al, 2006<papid> P06-1104 </papid>b; zhao and grishman, 2005).<papid> P05-1052 </papid></nextsent>
<nextsent>despite the large number of recently proposed kernels and their reported success, there lacks clear understanding of their relative strength and weakness.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1722">
<title id=" I08-2119.xml">a reexamination of dependency path kernels for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the predominant approach to relation extraction treats the task as multi-class classification problem, in which different relation types form different output classes.
</prevsent>
<prevsent>early work employed diverse range of features in linear classifier (commonlyreferred to as feature-based?
</prevsent>
</prevsection>
<citsent citstr=" P04-3022 ">
approaches), including lexical features, syntactic parse features, dependency features and semantic features (jiang and zhai, 2007; <papid> N07-1015 </papid>kambhatla, 2004; <papid> P04-3022 </papid>zhou et al, 2005).</citsent>
<aftsection>
<nextsent>these approaches were hindered by drawbacks suchas limited feature space and excessive feature engineering.
</nextsent>
<nextsent>kernel methods (cortes and vapnik, 1995; cristianini and shawe-taylor, 2000) on the other hand can explore much larger feature space very efficiently.
</nextsent>
<nextsent>recent studies on relation extraction have shown that by combining kernels withsupport-vector machines (svm), one can obtain results superior to feature-based methods (bunescu 841 and mooney, 2005b; bunescu and mooney, 2005<papid> H05-1091 </papid>a; culotta and sorensen, 2004; <papid> P04-1054 </papid>cumby and roth, 2003; zelenko et al, 2003; zhang et al, 2006<papid> P06-1104 </papid>a; zhang et al, 2006<papid> P06-1104 </papid>b; zhao and grishman, 2005).<papid> P05-1052 </papid></nextsent>
<nextsent>despite the large number of recently proposed kernels and their reported success, there lacks clear understanding of their relative strength and weakness.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1733">
<title id=" I08-2119.xml">a reexamination of dependency path kernels for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these approaches were hindered by drawbacks suchas limited feature space and excessive feature engineering.
</prevsent>
<prevsent>kernel methods (cortes and vapnik, 1995; cristianini and shawe-taylor, 2000) on the other hand can explore much larger feature space very efficiently.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
recent studies on relation extraction have shown that by combining kernels withsupport-vector machines (svm), one can obtain results superior to feature-based methods (bunescu 841 and mooney, 2005b; bunescu and mooney, 2005<papid> H05-1091 </papid>a; culotta and sorensen, 2004; <papid> P04-1054 </papid>cumby and roth, 2003; zelenko et al, 2003; zhang et al, 2006<papid> P06-1104 </papid>a; zhang et al, 2006<papid> P06-1104 </papid>b; zhao and grishman, 2005).<papid> P05-1052 </papid></citsent>
<aftsection>
<nextsent>despite the large number of recently proposed kernels and their reported success, there lacks clear understanding of their relative strength and weakness.
</nextsent>
<nextsent>in this study, we provide systematic comparison and analysis of three such kernels ? sub sequence kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>b), dependency tree kernel (culotta and sorensen, 2004) <papid> P04-1054 </papid>and dependency path kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>a).</nextsent>
<nextsent>we replicated these kernels and conducted experiments on the standard ace 2003 newswire text evaluation set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1734">
<title id=" I08-2119.xml">a reexamination of dependency path kernels for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these approaches were hindered by drawbacks suchas limited feature space and excessive feature engineering.
</prevsent>
<prevsent>kernel methods (cortes and vapnik, 1995; cristianini and shawe-taylor, 2000) on the other hand can explore much larger feature space very efficiently.
</prevsent>
</prevsection>
<citsent citstr=" P06-1104 ">
recent studies on relation extraction have shown that by combining kernels withsupport-vector machines (svm), one can obtain results superior to feature-based methods (bunescu 841 and mooney, 2005b; bunescu and mooney, 2005<papid> H05-1091 </papid>a; culotta and sorensen, 2004; <papid> P04-1054 </papid>cumby and roth, 2003; zelenko et al, 2003; zhang et al, 2006<papid> P06-1104 </papid>a; zhang et al, 2006<papid> P06-1104 </papid>b; zhao and grishman, 2005).<papid> P05-1052 </papid></citsent>
<aftsection>
<nextsent>despite the large number of recently proposed kernels and their reported success, there lacks clear understanding of their relative strength and weakness.
</nextsent>
<nextsent>in this study, we provide systematic comparison and analysis of three such kernels ? sub sequence kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>b), dependency tree kernel (culotta and sorensen, 2004) <papid> P04-1054 </papid>and dependency path kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>a).</nextsent>
<nextsent>we replicated these kernels and conducted experiments on the standard ace 2003 newswire text evaluation set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1754">
<title id=" I08-2119.xml">a reexamination of dependency path kernels for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these approaches were hindered by drawbacks suchas limited feature space and excessive feature engineering.
</prevsent>
<prevsent>kernel methods (cortes and vapnik, 1995; cristianini and shawe-taylor, 2000) on the other hand can explore much larger feature space very efficiently.
</prevsent>
</prevsection>
<citsent citstr=" P05-1052 ">
recent studies on relation extraction have shown that by combining kernels withsupport-vector machines (svm), one can obtain results superior to feature-based methods (bunescu 841 and mooney, 2005b; bunescu and mooney, 2005<papid> H05-1091 </papid>a; culotta and sorensen, 2004; <papid> P04-1054 </papid>cumby and roth, 2003; zelenko et al, 2003; zhang et al, 2006<papid> P06-1104 </papid>a; zhang et al, 2006<papid> P06-1104 </papid>b; zhao and grishman, 2005).<papid> P05-1052 </papid></citsent>
<aftsection>
<nextsent>despite the large number of recently proposed kernels and their reported success, there lacks clear understanding of their relative strength and weakness.
</nextsent>
<nextsent>in this study, we provide systematic comparison and analysis of three such kernels ? sub sequence kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>b), dependency tree kernel (culotta and sorensen, 2004) <papid> P04-1054 </papid>and dependency path kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>a).</nextsent>
<nextsent>we replicated these kernels and conducted experiments on the standard ace 2003 newswire text evaluation set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1884">
<title id=" I05-6006.xml">the syntactically annotated ice corpus and the automatic induction of a formal grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>test set 1 test set 2 test set 3 test set 4 # % # % # % # % tree 1000 1000 1000 1000 node 44127 47485 43974 41998 correct 38008 86.1 40665 85.6 37844 86.1 36319 86.5 subs 4302 9.7 4879 10.3 4368 9.9 4052 9.6 del 1781 4.0 1941 4.1 1762 4.0 1627 3.9 ins 3148 3613 3015 2868 prec 83.6 82.7 83.7 83.9 overall 79.0 78.0 79.2 79.6 table 3: combined accuracy 4.5 discussion.
</prevsent>
<prevsent>although the scores for the grammar and the parser look both encouraging and promising, it is difficult to draw straightforward comparisons with other systems.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
charniak (2000) <papid> A00-2018 </papid>reports maximum entropy inspired parser that scored 90.1% average precision/recall when trained and tested with sentences from the wall street journal corpus (wsj).</citsent>
<aftsection>
<nextsent>while the difference in precision/recall between the two parsers may indicate the difference in terms of performance between the two parsing approaches, there nevertheless remain two issues to be investigated.
</nextsent>
<nextsent>firstly, there is the issue of how text types may influence the performance of the grammar and indeed the parsing system as whole.
</nextsent>
<nextsent>charniak (2000) <papid> A00-2018 </papid>uses wsj as both training and testing data and it is reasonable to expect fairly good overlap in terms of lexical co-occurrences and linguistic structures and hence good performance scores.</nextsent>
<nextsent>indeed, gildea (2001) <papid> W01-0521 </papid>suggests that the standard wsj task seems to be simplified by its homogenous style.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1888">
<title id=" I05-6006.xml">the syntactically annotated ice corpus and the automatic induction of a formal grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>firstly, there is the issue of how text types may influence the performance of the grammar and indeed the parsing system as whole.
</prevsent>
<prevsent>charniak (2000) <papid> A00-2018 </papid>uses wsj as both training and testing data and it is reasonable to expect fairly good overlap in terms of lexical co-occurrences and linguistic structures and hence good performance scores.</prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
indeed, gildea (2001) <papid> W01-0521 </papid>suggests that the standard wsj task seems to be simplified by its homogenous style.</citsent>
<aftsection>
<nextsent>it is thus yet 55 to be verified how well the same system will perform when trained and tested on more balanced?
</nextsent>
<nextsent>corpus such as ice.
</nextsent>
<nextsent>secondly, it is not clear what the performance will be for charniaks parsing model when dealing with much more complex grammar such as ice,which has almost three times as many nonterminal parsing symbols.
</nextsent>
<nextsent>the performance of the survey parser is very close to that of an un lexicalised pcfg parser reported in klain and manning (2003) but again wsj was used for training and testing and it is not clear how well their system will scale up to typo logically more varied corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1889">
<title id=" I05-5003.xml">using machine translation evaluation techniques to determine sentence level semantic equivalence </title>
<section> mt evaluation methods.  </section>
<citcontext>
<prevsection>
<prevsent>thus the set of references is given by: = {r1, r2, ..., ri}.
</prevsent>
<prevsent>2.1 wer.
</prevsent>
</prevsection>
<citsent citstr=" C92-2067 ">
word error rate (wer) (su et al, 1992) <papid> C92-2067 </papid>is measure of the number of edit operations required to transform one sentence into another, defined as: wer(si, ri) = i(si, ri) + d(si, ri) + s(si, ri) |ri| where i(si, ri), d(si, ri) and s(si, ri) are the number of insertions, deletions and substitutions respectively.</citsent>
<aftsection>
<nextsent>2.2 per.
</nextsent>
<nextsent>position-independent word error rate (per) (till mann et al, 1997) is similar tower except that word order is not taken into account, both sentences are treated as bags of words: per(si, ri) = max[diff(si, ri), diff(ri, si)] |ri|where diff(si, ri) is the number of words observed only in si.
</nextsent>
<nextsent>2.3 bleu.
</nextsent>
<nextsent>the bleu score (papineni et al, 2001) is based on the geometric mean of n-gram precision.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1890">
<title id=" I05-2014.xml">bleu in characters towards automatic mt evaluation in languages without word delimiters </title>
<section> overview.  </section>
<citcontext>
<prevsection>
<prevsent>the consequence of this state of affairs is that evaluation campaigns of english to japanese or english to chinese machine translation systems for instance,are not, to our knowledge, widely seen or reported.
</prevsent>
<prevsent>2.1 the word segmentation problem.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
as statistical machine translation systems basically relyon the notion of words through their lexicon models (brown et al, 1993), <papid> J93-2003 </papid>they are usually capable of out putting sentences already segmented into words when they translate into languages like chinese or japanese.</citsent>
<aftsection>
<nextsent>but this is not necessarily the case with commercial systems.
</nextsent>
<nextsent>for instance, systran4 does not output segmented texts when it translates into chinese or japanese.
</nextsent>
<nextsent>as such, comparing systems that translate into languages where words are not an immediate given in unprocessed texts, is still hindered by the human evaluation bottleneck.
</nextsent>
<nextsent>to compare the performance of different systems, segmentation has to be performed beforehand.1http://www.nist.gov/speech/tests/mt /doc/mt04 evalplan.v2.1.pdf2http://www.nist.gov/speech/tests/mt /mt tides01 knight.pdf3http://www.slt.atr.jp/iwslt2004 /archives/000619.html4http://www.systranbox.com/systran /box.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1891">
<title id=" I05-2014.xml">bleu in characters towards automatic mt evaluation in languages without word delimiters </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>|s| is the length of sentence in words.
</prevsent>
<prevsent>using consistent notation, we note as |s|w the number of occurrences of the (sub)string in the sentence s, so that |s|w1...wn is the number of occurrences of the word n-gram w1 . . .
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
wn in the sentence s. with the previous notations, modified n-gram precision for the order is the ratio of two sums7: pn = ? w1...wnc min ( |c|w1...wn , maxr ( |r|w1...wn )) ? w1...wnc |c|w1...wn ? the numerator gives the number of n-grams of the candidate appearing in the references,6rouge (lin and hovy, 2003) <papid> N03-1020 </papid>would be representative of measures with the focus on recall.</citsent>
<aftsection>
<nextsent>7we limit ourselves to the cases where one candidate or one reference is one sentence.
</nextsent>
<nextsent>80limited to the maximal number of occurrences of the n-gram considered in single reference8;?
</nextsent>
<nextsent>the denominator gives the total number of grams in the candidate.we leave the basic definition of bleu untouched.
</nextsent>
<nextsent>the previous formulae can be applied to character n-grams instead of word n-grams.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1892">
<title id=" I08-1039.xml">learning to shift the polarity of words for sentiment classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we present our experiments and results in section 5.
</prevsent>
<prevsent>finally in section 6, we conclude our work and mention possible future work.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
supervised machine learning methods including support vector machines (svm) are often used in sentiment analysis and shown to be very promising (pang et al, 2002; <papid> W02-1011 </papid>matsumoto et al, 2005; kudo and matsumoto, 2004; <papid> W04-3239 </papid>mullen and collier, 2004; <papid> W04-3253 </papid>gamon, 2004).<papid> C04-1121 </papid></citsent>
<aftsection>
<nextsent>one of the advantages of these methods is that wide variety of features such as dependency trees and sequences of words can easily be incorporated (matsumoto et al, 2005; kudo and matsumoto, 2004; <papid> W04-3239 </papid>pang et al, 2002).<papid> W02-1011 </papid></nextsent>
<nextsent>our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is resource usually at hand.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1893">
<title id=" I08-1039.xml">learning to shift the polarity of words for sentiment classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we present our experiments and results in section 5.
</prevsent>
<prevsent>finally in section 6, we conclude our work and mention possible future work.
</prevsent>
</prevsection>
<citsent citstr=" W04-3239 ">
supervised machine learning methods including support vector machines (svm) are often used in sentiment analysis and shown to be very promising (pang et al, 2002; <papid> W02-1011 </papid>matsumoto et al, 2005; kudo and matsumoto, 2004; <papid> W04-3239 </papid>mullen and collier, 2004; <papid> W04-3253 </papid>gamon, 2004).<papid> C04-1121 </papid></citsent>
<aftsection>
<nextsent>one of the advantages of these methods is that wide variety of features such as dependency trees and sequences of words can easily be incorporated (matsumoto et al, 2005; kudo and matsumoto, 2004; <papid> W04-3239 </papid>pang et al, 2002).<papid> W02-1011 </papid></nextsent>
<nextsent>our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is resource usually at hand.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1894">
<title id=" I08-1039.xml">learning to shift the polarity of words for sentiment classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we present our experiments and results in section 5.
</prevsent>
<prevsent>finally in section 6, we conclude our work and mention possible future work.
</prevsent>
</prevsection>
<citsent citstr=" W04-3253 ">
supervised machine learning methods including support vector machines (svm) are often used in sentiment analysis and shown to be very promising (pang et al, 2002; <papid> W02-1011 </papid>matsumoto et al, 2005; kudo and matsumoto, 2004; <papid> W04-3239 </papid>mullen and collier, 2004; <papid> W04-3253 </papid>gamon, 2004).<papid> C04-1121 </papid></citsent>
<aftsection>
<nextsent>one of the advantages of these methods is that wide variety of features such as dependency trees and sequences of words can easily be incorporated (matsumoto et al, 2005; kudo and matsumoto, 2004; <papid> W04-3239 </papid>pang et al, 2002).<papid> W02-1011 </papid></nextsent>
<nextsent>our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is resource usually at hand.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1895">
<title id=" I08-1039.xml">learning to shift the polarity of words for sentiment classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we present our experiments and results in section 5.
</prevsent>
<prevsent>finally in section 6, we conclude our work and mention possible future work.
</prevsent>
</prevsection>
<citsent citstr=" C04-1121 ">
supervised machine learning methods including support vector machines (svm) are often used in sentiment analysis and shown to be very promising (pang et al, 2002; <papid> W02-1011 </papid>matsumoto et al, 2005; kudo and matsumoto, 2004; <papid> W04-3239 </papid>mullen and collier, 2004; <papid> W04-3253 </papid>gamon, 2004).<papid> C04-1121 </papid></citsent>
<aftsection>
<nextsent>one of the advantages of these methods is that wide variety of features such as dependency trees and sequences of words can easily be incorporated (matsumoto et al, 2005; kudo and matsumoto, 2004; <papid> W04-3239 </papid>pang et al, 2002).<papid> W02-1011 </papid></nextsent>
<nextsent>our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is resource usually at hand.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1898">
<title id=" I08-1039.xml">learning to shift the polarity of words for sentiment classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>thusour work is an instantiation of the idea to use resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level).there have been some pieces of work which focus on multiple levels in text.
</prevsent>
<prevsent>mao and lebanon(2006) proposed method that captures local sentiment flow in documents using iso tonic conditional random fields.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
pang and lee (2004) <papid> P04-1035 </papid>proposed to eliminate objective sentences before the sentiment classification of documents.</citsent>
<aftsection>
<nextsent>mcdonald et al (2007)proposed model for classifying sentences and documents simultaneously.
</nextsent>
<nextsent>they experimented with joint classification of subjectivity for sentence-level, and sentiment for document-level, and reported that their model obtained higher accuracy than the standard document classification model.
</nextsent>
<nextsent>although these pieces of work aim to predict not sentence-level but document-level sentiments, their concepts are similar to ours.
</nextsent>
<nextsent>however, all the above methods require annotated corpora for all levels,such as both subjectivity for sentences and sentiments for documents, which are fairly expensive toobtain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1899">
<title id=" I08-1039.xml">learning to shift the polarity of words for sentiment classification </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we can use high degree polynomialkernels as both which is kernel between substructures, i.e. sentiment words, of sentences, and ? which is kernel between sentences to make the classifiers take into consideration the combination of features.
</prevsent>
<prevsent>5.1 datasets.
</prevsent>
</prevsection>
<citsent citstr=" P05-1015 ">
we used two datasets, customer reviews 1 (hu and liu, 2004) and movie reviews 2 (pang and lee, 2005) <papid> P05-1015 </papid>to evaluate sentiment classification of sentences.</citsent>
<aftsection>
<nextsent>both of these two datasets are often used for evaluation in sentiment analysis researches.
</nextsent>
<nextsent>the number of examples and other statistics of the datasets are shown in table 1.
</nextsent>
<nextsent>our method cannot be applied to sentences which contain no sentiment words.
</nextsent>
<nextsent>we therefore eliminated such sentences from the datasets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1900">
<title id=" I05-3013.xml">nil is not nothing recognition of chinese network informal language expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>processing nil text requires unconventional linguistic knowledge and techniques.
</prevsent>
<prevsent>unfortunately, developed to handle formal language text, the existing natural language processing (nlp) approaches exhibit less effectiveness in dealing with nil text.
</prevsent>
</prevsection>
<citsent citstr=" W03-1730 ">
for example, we use ictclas (zhang et al, 2003) <papid> W03-1730 </papid>tool to process sentence ???</citsent>
<aftsection>
<nextsent>???????(is he going to attend meeting?)?.
</nextsent>
<nextsent>the word segmentation result is ??|?|?|?|?|??|?|??.
</nextsent>
<nextsent>in this sentence , ??
</nextsent>
<nextsent>(xi4 ba1 xi4)?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1901">
<title id=" I05-3013.xml">nil is not nothing recognition of chinese network informal language expressions </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>it typically aims to recognize names for person, organization, location, and expressions of number, time and currency.
</prevsent>
<prevsent>the objective is achieved by employing either handcrafted knowledge or supervised learning techniques.
</prevsent>
</prevsection>
<citsent citstr=" W98-1120 ">
the latter is currently dominating inner amongst which the most popular methods are decision tree (sekine et al, 1998; <papid> W98-1120 </papid>pailouras et al., 2000), hidden markov model (zhang et al, 2003; <papid> W03-1730 </papid>zhao, 2004), <papid> W04-1216 </papid>maximum entropy (chieu and ng, 2002; bender et al, 2003), <papid> W03-0420 </papid>and support vector machines (isozaki and kazawa, 2002; <papid> C02-1054 </papid>takeuchi and collier, 2002; <papid> W02-2029 </papid>mayfield, 2003).</citsent>
<aftsection>
<nextsent>from the linguistic perspective, nil expressions are rather different from named entities in nature.
</nextsent>
<nextsent>firstly, named entity is typically noun or noun phrase (np), but nil expression can be any kind, e.g. number 94?
</nextsent>
<nextsent>in nil represents ????
</nextsent>
<nextsent>which is verb meaning exactly be?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1903">
<title id=" I05-3013.xml">nil is not nothing recognition of chinese network informal language expressions </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>it typically aims to recognize names for person, organization, location, and expressions of number, time and currency.
</prevsent>
<prevsent>the objective is achieved by employing either handcrafted knowledge or supervised learning techniques.
</prevsent>
</prevsection>
<citsent citstr=" W04-1216 ">
the latter is currently dominating inner amongst which the most popular methods are decision tree (sekine et al, 1998; <papid> W98-1120 </papid>pailouras et al., 2000), hidden markov model (zhang et al, 2003; <papid> W03-1730 </papid>zhao, 2004), <papid> W04-1216 </papid>maximum entropy (chieu and ng, 2002; bender et al, 2003), <papid> W03-0420 </papid>and support vector machines (isozaki and kazawa, 2002; <papid> C02-1054 </papid>takeuchi and collier, 2002; <papid> W02-2029 </papid>mayfield, 2003).</citsent>
<aftsection>
<nextsent>from the linguistic perspective, nil expressions are rather different from named entities in nature.
</nextsent>
<nextsent>firstly, named entity is typically noun or noun phrase (np), but nil expression can be any kind, e.g. number 94?
</nextsent>
<nextsent>in nil represents ????
</nextsent>
<nextsent>which is verb meaning exactly be?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1904">
<title id=" I05-3013.xml">nil is not nothing recognition of chinese network informal language expressions </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>it typically aims to recognize names for person, organization, location, and expressions of number, time and currency.
</prevsent>
<prevsent>the objective is achieved by employing either handcrafted knowledge or supervised learning techniques.
</prevsent>
</prevsection>
<citsent citstr=" W03-0420 ">
the latter is currently dominating inner amongst which the most popular methods are decision tree (sekine et al, 1998; <papid> W98-1120 </papid>pailouras et al., 2000), hidden markov model (zhang et al, 2003; <papid> W03-1730 </papid>zhao, 2004), <papid> W04-1216 </papid>maximum entropy (chieu and ng, 2002; bender et al, 2003), <papid> W03-0420 </papid>and support vector machines (isozaki and kazawa, 2002; <papid> C02-1054 </papid>takeuchi and collier, 2002; <papid> W02-2029 </papid>mayfield, 2003).</citsent>
<aftsection>
<nextsent>from the linguistic perspective, nil expressions are rather different from named entities in nature.
</nextsent>
<nextsent>firstly, named entity is typically noun or noun phrase (np), but nil expression can be any kind, e.g. number 94?
</nextsent>
<nextsent>in nil represents ????
</nextsent>
<nextsent>which is verb meaning exactly be?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1905">
<title id=" I05-3013.xml">nil is not nothing recognition of chinese network informal language expressions </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>it typically aims to recognize names for person, organization, location, and expressions of number, time and currency.
</prevsent>
<prevsent>the objective is achieved by employing either handcrafted knowledge or supervised learning techniques.
</prevsent>
</prevsection>
<citsent citstr=" C02-1054 ">
the latter is currently dominating inner amongst which the most popular methods are decision tree (sekine et al, 1998; <papid> W98-1120 </papid>pailouras et al., 2000), hidden markov model (zhang et al, 2003; <papid> W03-1730 </papid>zhao, 2004), <papid> W04-1216 </papid>maximum entropy (chieu and ng, 2002; bender et al, 2003), <papid> W03-0420 </papid>and support vector machines (isozaki and kazawa, 2002; <papid> C02-1054 </papid>takeuchi and collier, 2002; <papid> W02-2029 </papid>mayfield, 2003).</citsent>
<aftsection>
<nextsent>from the linguistic perspective, nil expressions are rather different from named entities in nature.
</nextsent>
<nextsent>firstly, named entity is typically noun or noun phrase (np), but nil expression can be any kind, e.g. number 94?
</nextsent>
<nextsent>in nil represents ????
</nextsent>
<nextsent>which is verb meaning exactly be?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1906">
<title id=" I05-3013.xml">nil is not nothing recognition of chinese network informal language expressions </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>it typically aims to recognize names for person, organization, location, and expressions of number, time and currency.
</prevsent>
<prevsent>the objective is achieved by employing either handcrafted knowledge or supervised learning techniques.
</prevsent>
</prevsection>
<citsent citstr=" W02-2029 ">
the latter is currently dominating inner amongst which the most popular methods are decision tree (sekine et al, 1998; <papid> W98-1120 </papid>pailouras et al., 2000), hidden markov model (zhang et al, 2003; <papid> W03-1730 </papid>zhao, 2004), <papid> W04-1216 </papid>maximum entropy (chieu and ng, 2002; bender et al, 2003), <papid> W03-0420 </papid>and support vector machines (isozaki and kazawa, 2002; <papid> C02-1054 </papid>takeuchi and collier, 2002; <papid> W02-2029 </papid>mayfield, 2003).</citsent>
<aftsection>
<nextsent>from the linguistic perspective, nil expressions are rather different from named entities in nature.
</nextsent>
<nextsent>firstly, named entity is typically noun or noun phrase (np), but nil expression can be any kind, e.g. number 94?
</nextsent>
<nextsent>in nil represents ????
</nextsent>
<nextsent>which is verb meaning exactly be?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1907">
<title id=" I05-3013.xml">nil is not nothing recognition of chinese network informal language expressions </title>
<section> niler system.  </section>
<citcontext>
<prevsection>
<prevsent>(he has been working for eight hours.)?
</prevsent>
<prevsent>is not recognized as nil expression.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
5.2.2 method ii: support vector machines support vector machines (svm) method produces high performance in many classification tasks (joachims, 1998; kudo and matsumoto, 2001).<papid> N01-1025 </papid></citsent>
<aftsection>
<nextsent>as svm can handle large numbers of features efficiently, we employ svm classification method to nil expression recognition.
</nextsent>
<nextsent>suppose we have set of training data for two-class classification problem {(x1,y1), (x2,y2),?,(xn, yn)}, where ),...2,1( nirx di ? is feature vector of the i-th order sample in the training set and }1,1{ iy is the label for the sample.
</nextsent>
<nextsent>the goal of svm is to find decision function that accurately predicts for unseen x. non-linear svm classifier gives decision function ))(()( xgsignxf for an input vector x, where ?  i ii bzxkxg 1 ),()( the szi are so-called support vectors, and represents the training samples.
</nextsent>
<nextsent>iy and are parameters for svm motel.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1908">
<title id=" I08-1071.xml">augmenting wikipedia with named entity tags </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper presents successful study on enriching the wikipedia data with named entity tags.
</prevsent>
<prevsent>such tags could be employed by disambiguation systems such as bunescu and pa
</prevsent>
</prevsection>
<citsent citstr=" D07-1074 ">
ca (2006) and cucerzan (2007), <papid> D07-1074 </papid>in mining relationships between named entities, or in extracting useful facet terms from news articles (e.g., dakka and ipeirotis, 2008).</citsent>
<aftsection>
<nextsent>in this work, we classify the wikipedia pages into categories similar to those used in the conll shared tasks (tjong kim sang, 2002; tjong kim sang and de meulder, 2003) and ace (doddington et al, 2004).
</nextsent>
<nextsent>to the best of our knowledge, this is the first attempt to perform such classification on the english language version of the collection.
</nextsent>
<nextsent>1 although the task settings are different, the results we obtained are comparable with those previously reported in document classification tasks.
</nextsent>
<nextsent>we examined the wikipedia pages to extract several feature groups for our classification task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1910">
<title id=" I08-1071.xml">augmenting wikipedia with named entity tags </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we examined the wikipedia pages to extract several feature groups for our classification task.
</prevsent>
<prevsent>we also observed that each entity/concept has at least two pseudo-independent views (page-based features and link-based features), which allow the use co-training method to boost the performance of classifiers trained separately on each view.
</prevsent>
</prevsection>
<citsent citstr=" D07-1068 ">
the classifier that achieved the best accuracy on out test set was applied then to all wikipedia pages and its classifications are provided to the academic community for use in future studies through web service.2 1 watanabe et al (2007) <papid> D07-1068 </papid>have reported recently experiments on categorizing named entities in the japanese version of wikipedia using graph-based approach.</citsent>
<aftsection>
<nextsent>2 the web service is available at wikinet.stern.nyu.edu.
</nextsent>
<nextsent>545
</nextsent>
<nextsent>this study is related to the area of named entity recognition, which has supported extensive evaluations (conll and ace).
</nextsent>
<nextsent>since the introduction of this task in muc-6 (grishman and sundheim, 1996), <papid> C96-1079 </papid>numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as cucerzan and yarowsky (2002) <papid> W02-2007 </papid>and klein et al (2003) <papid> W03-0428 </papid>to complex models making use of various lexical, syntactic, morphological, and ortho graphical information, such as wacholder et al (1997), <papid> A97-1030 </papid>fleischman and hovy (2002), <papid> C02-1130 </papid>and florian et al (2003).<papid> W03-0425 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1911">
<title id=" I08-1071.xml">augmenting wikipedia with named entity tags </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>545
</prevsent>
<prevsent>this study is related to the area of named entity recognition, which has supported extensive evaluations (conll and ace).
</prevsent>
</prevsection>
<citsent citstr=" C96-1079 ">
since the introduction of this task in muc-6 (grishman and sundheim, 1996), <papid> C96-1079 </papid>numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as cucerzan and yarowsky (2002) <papid> W02-2007 </papid>and klein et al (2003) <papid> W03-0428 </papid>to complex models making use of various lexical, syntactic, morphological, and ortho graphical information, such as wacholder et al (1997), <papid> A97-1030 </papid>fleischman and hovy (2002), <papid> C02-1130 </papid>and florian et al (2003).<papid> W03-0425 </papid></citsent>
<aftsection>
<nextsent>while the task we address is not the conventional named entity recognition but rather document classification, our classes are derived from the labels traditionally employed in named entity recognition, following the conll and ace guidelines, as described in section 3.
</nextsent>
<nextsent>the areas of text categorization and document classification have also been extensively researched over time.
</nextsent>
<nextsent>these task have the goal of assigning to each document in collection one or several labels from given set, such as news groups (lang, 1995), reuters (reuters, 1997), ya hoo!
</nextsent>
<nextsent>(mladenic, 1998), open directory project (chakrabarti et al, 2002), and hoovers online (yang et al, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1912">
<title id=" I08-1071.xml">augmenting wikipedia with named entity tags </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>545
</prevsent>
<prevsent>this study is related to the area of named entity recognition, which has supported extensive evaluations (conll and ace).
</prevsent>
</prevsection>
<citsent citstr=" W02-2007 ">
since the introduction of this task in muc-6 (grishman and sundheim, 1996), <papid> C96-1079 </papid>numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as cucerzan and yarowsky (2002) <papid> W02-2007 </papid>and klein et al (2003) <papid> W03-0428 </papid>to complex models making use of various lexical, syntactic, morphological, and ortho graphical information, such as wacholder et al (1997), <papid> A97-1030 </papid>fleischman and hovy (2002), <papid> C02-1130 </papid>and florian et al (2003).<papid> W03-0425 </papid></citsent>
<aftsection>
<nextsent>while the task we address is not the conventional named entity recognition but rather document classification, our classes are derived from the labels traditionally employed in named entity recognition, following the conll and ace guidelines, as described in section 3.
</nextsent>
<nextsent>the areas of text categorization and document classification have also been extensively researched over time.
</nextsent>
<nextsent>these task have the goal of assigning to each document in collection one or several labels from given set, such as news groups (lang, 1995), reuters (reuters, 1997), ya hoo!
</nextsent>
<nextsent>(mladenic, 1998), open directory project (chakrabarti et al, 2002), and hoovers online (yang et al, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1913">
<title id=" I08-1071.xml">augmenting wikipedia with named entity tags </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>545
</prevsent>
<prevsent>this study is related to the area of named entity recognition, which has supported extensive evaluations (conll and ace).
</prevsent>
</prevsection>
<citsent citstr=" W03-0428 ">
since the introduction of this task in muc-6 (grishman and sundheim, 1996), <papid> C96-1079 </papid>numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as cucerzan and yarowsky (2002) <papid> W02-2007 </papid>and klein et al (2003) <papid> W03-0428 </papid>to complex models making use of various lexical, syntactic, morphological, and ortho graphical information, such as wacholder et al (1997), <papid> A97-1030 </papid>fleischman and hovy (2002), <papid> C02-1130 </papid>and florian et al (2003).<papid> W03-0425 </papid></citsent>
<aftsection>
<nextsent>while the task we address is not the conventional named entity recognition but rather document classification, our classes are derived from the labels traditionally employed in named entity recognition, following the conll and ace guidelines, as described in section 3.
</nextsent>
<nextsent>the areas of text categorization and document classification have also been extensively researched over time.
</nextsent>
<nextsent>these task have the goal of assigning to each document in collection one or several labels from given set, such as news groups (lang, 1995), reuters (reuters, 1997), ya hoo!
</nextsent>
<nextsent>(mladenic, 1998), open directory project (chakrabarti et al, 2002), and hoovers online (yang et al, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1914">
<title id=" I08-1071.xml">augmenting wikipedia with named entity tags </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>545
</prevsent>
<prevsent>this study is related to the area of named entity recognition, which has supported extensive evaluations (conll and ace).
</prevsent>
</prevsection>
<citsent citstr=" A97-1030 ">
since the introduction of this task in muc-6 (grishman and sundheim, 1996), <papid> C96-1079 </papid>numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as cucerzan and yarowsky (2002) <papid> W02-2007 </papid>and klein et al (2003) <papid> W03-0428 </papid>to complex models making use of various lexical, syntactic, morphological, and ortho graphical information, such as wacholder et al (1997), <papid> A97-1030 </papid>fleischman and hovy (2002), <papid> C02-1130 </papid>and florian et al (2003).<papid> W03-0425 </papid></citsent>
<aftsection>
<nextsent>while the task we address is not the conventional named entity recognition but rather document classification, our classes are derived from the labels traditionally employed in named entity recognition, following the conll and ace guidelines, as described in section 3.
</nextsent>
<nextsent>the areas of text categorization and document classification have also been extensively researched over time.
</nextsent>
<nextsent>these task have the goal of assigning to each document in collection one or several labels from given set, such as news groups (lang, 1995), reuters (reuters, 1997), ya hoo!
</nextsent>
<nextsent>(mladenic, 1998), open directory project (chakrabarti et al, 2002), and hoovers online (yang et al, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1915">
<title id=" I08-1071.xml">augmenting wikipedia with named entity tags </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>545
</prevsent>
<prevsent>this study is related to the area of named entity recognition, which has supported extensive evaluations (conll and ace).
</prevsent>
</prevsection>
<citsent citstr=" C02-1130 ">
since the introduction of this task in muc-6 (grishman and sundheim, 1996), <papid> C96-1079 </papid>numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as cucerzan and yarowsky (2002) <papid> W02-2007 </papid>and klein et al (2003) <papid> W03-0428 </papid>to complex models making use of various lexical, syntactic, morphological, and ortho graphical information, such as wacholder et al (1997), <papid> A97-1030 </papid>fleischman and hovy (2002), <papid> C02-1130 </papid>and florian et al (2003).<papid> W03-0425 </papid></citsent>
<aftsection>
<nextsent>while the task we address is not the conventional named entity recognition but rather document classification, our classes are derived from the labels traditionally employed in named entity recognition, following the conll and ace guidelines, as described in section 3.
</nextsent>
<nextsent>the areas of text categorization and document classification have also been extensively researched over time.
</nextsent>
<nextsent>these task have the goal of assigning to each document in collection one or several labels from given set, such as news groups (lang, 1995), reuters (reuters, 1997), ya hoo!
</nextsent>
<nextsent>(mladenic, 1998), open directory project (chakrabarti et al, 2002), and hoovers online (yang et al, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1916">
<title id=" I08-1071.xml">augmenting wikipedia with named entity tags </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>545
</prevsent>
<prevsent>this study is related to the area of named entity recognition, which has supported extensive evaluations (conll and ace).
</prevsent>
</prevsection>
<citsent citstr=" W03-0425 ">
since the introduction of this task in muc-6 (grishman and sundheim, 1996), <papid> C96-1079 </papid>numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as cucerzan and yarowsky (2002) <papid> W02-2007 </papid>and klein et al (2003) <papid> W03-0428 </papid>to complex models making use of various lexical, syntactic, morphological, and ortho graphical information, such as wacholder et al (1997), <papid> A97-1030 </papid>fleischman and hovy (2002), <papid> C02-1130 </papid>and florian et al (2003).<papid> W03-0425 </papid></citsent>
<aftsection>
<nextsent>while the task we address is not the conventional named entity recognition but rather document classification, our classes are derived from the labels traditionally employed in named entity recognition, following the conll and ace guidelines, as described in section 3.
</nextsent>
<nextsent>the areas of text categorization and document classification have also been extensively researched over time.
</nextsent>
<nextsent>these task have the goal of assigning to each document in collection one or several labels from given set, such as news groups (lang, 1995), reuters (reuters, 1997), ya hoo!
</nextsent>
<nextsent>(mladenic, 1998), open directory project (chakrabarti et al, 2002), and hoovers online (yang et al, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1919">
<title id=" I08-2108.xml">mrdbased word sense disambiguation further extending lesk </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, we will make clear when hand-tagged sense information is being used.unsupervised methods relyon different knowledge sources to build their models.
</prevsent>
<prevsent>primarily the following types of lexical resources have been used for wsd: mrds, lexical ontologies, and untagged corpora (monolingual corpora, second language corpora, and parallel corpora).
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
although early approaches focused on exploiting single resource (lesk, 1986), recent trends show the benefits of combining different knowledge sources, suchas hierarchical relations from an ontology and untagged corpora (mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>in this summary, we will focus on few representative systems that make use of different resources, noting that this is an area of very active research which we cannot do true justice to within the confines of this paper.
</nextsent>
<nextsent>the lesk method (lesk, 1986) is an mrd-based system that relies on counting the overlap between the words in the target context and the dictionary definitions of the senses.
</nextsent>
<nextsent>inspite of its simplicity,it has been shown to be hard baseline for unsupervised methods in senseval, and it is applicable toall-words with minimal effort.
</nextsent>
<nextsent>banerjee and pedersen (2002) extended the lesk method for wordnet based wsd tasks, to include hierarchical data fromthe wordnet ontology (fellbaum, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1921">
<title id=" I08-2108.xml">mrdbased word sense disambiguation further extending lesk </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they observed that the hierarchical relations significantly enhance the basic model.
</prevsent>
<prevsent>both these methods willbe described extensively in section 3.1, as our approach is based on them.
</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
other notable unsupervised and semi-supervised approaches are those of mccarthy et al (2004), <papid> P04-1036 </papid>who combine onto logical relations and untagged corpora to automatically rank word senses in relation to corpus, and leacock et al (1998) <papid> J98-1006 </papid>who use untagged data to build sense-tagged data automatically based on monosemous words.</citsent>
<aftsection>
<nextsent>parallel corpora have also been used to avoid the need for hand-tagged data, e.g. by chan and ng (2005).
</nextsent>
<nextsent>as background to our work, we first describe the basic and extended lesk algorithms that form the coreof our approach.
</nextsent>
<nextsent>then we present the lexeed lexical resource we have used in our experiments, and finally we outline aspects of japanese relevant for this work.
</nextsent>
<nextsent>3.1 basic and extended lesk.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1922">
<title id=" I08-2108.xml">mrdbased word sense disambiguation further extending lesk </title>
<section> proposed extensions.  </section>
<citcontext>
<prevsection>
<prevsent>token isation is particularly important in japanese because it is non-segmenting language with lo go graphic orthography (kanji).
</prevsent>
<prevsent>as such, we can chose to either word token ise via word splitter such as chasen, or character tokenise.
</prevsent>
</prevsection>
<citsent citstr=" P01-1004 ">
character and word token isation have been compared in the context of japanese information retrieval (fujii and croft, 1993) and translation retrieval (baldwin, 2001), <papid> P01-1004 </papid>and in both cases, characters have been found to be the superior representation overall.</citsent>
<aftsection>
<nextsent>orthogonal to the question of whether to tokeniseinto words or characters, we adopt an n-gram segment representation, in the form of simple unigrams and simple bigrams.
</nextsent>
<nextsent>in the case of word token isa tion and simple bigrams, e.g., example (1) would be represented as {??????
</nextsent>
<nextsent>4.3 extended glosses.
</nextsent>
<nextsent>the main direction in which banerjee and pedersen (2002) successfully extended the lesk algorithm was in including hierarchically-adjacent glosses (i.e. hyponyms and hypernyms).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1924">
<title id=" I08-1067.xml">simple syntactic and morphological processing can help english hindi statistical machine translation </title>
<section> incorporating morphological information by.  </section>
<citcontext>
<prevsection>
<prevsent>2 related work.
</prevsent>
<prevsent>statistical translation models have evolved from the word-based models originally proposed by brownet al (1990) to syntax-based and phrase-based techniques.
</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
the beginnings of phrase-based translation can be seen in the alignment template model introduced by och et al (1999).<papid> W99-0604 </papid></citsent>
<aftsection>
<nextsent>a joint probability model for phrase translation was proposed by marcu and wong (2002).<papid> W02-1018 </papid></nextsent>
<nextsent>koehn et al (2003) <papid> N03-1017 </papid>propose certain heuristics to extract phrases that are consistent with bidirectional word-alignments generated by the ibm models (brown et al, 1990).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1925">
<title id=" I08-1067.xml">simple syntactic and morphological processing can help english hindi statistical machine translation </title>
<section> incorporating morphological information by.  </section>
<citcontext>
<prevsection>
<prevsent>statistical translation models have evolved from the word-based models originally proposed by brownet al (1990) to syntax-based and phrase-based techniques.
</prevsent>
<prevsent>the beginnings of phrase-based translation can be seen in the alignment template model introduced by och et al (1999).<papid> W99-0604 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
a joint probability model for phrase translation was proposed by marcu and wong (2002).<papid> W02-1018 </papid></citsent>
<aftsection>
<nextsent>koehn et al (2003) <papid> N03-1017 </papid>propose certain heuristics to extract phrases that are consistent with bidirectional word-alignments generated by the ibm models (brown et al, 1990).</nextsent>
<nextsent>phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases, the joint model, and ibm model 4 (koehn et al, 2003).<papid> N03-1017 </papid>syntax-based models use parse-tree representations of the sentences in the training data to learn,among other things, tree transformation probabili ties.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1926">
<title id=" I08-1067.xml">simple syntactic and morphological processing can help english hindi statistical machine translation </title>
<section> incorporating morphological information by.  </section>
<citcontext>
<prevsection>
<prevsent>the beginnings of phrase-based translation can be seen in the alignment template model introduced by och et al (1999).<papid> W99-0604 </papid></prevsent>
<prevsent>a joint probability model for phrase translation was proposed by marcu and wong (2002).<papid> W02-1018 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
koehn et al (2003) <papid> N03-1017 </papid>propose certain heuristics to extract phrases that are consistent with bidirectional word-alignments generated by the ibm models (brown et al, 1990).</citsent>
<aftsection>
<nextsent>phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases, the joint model, and ibm model 4 (koehn et al, 2003).<papid> N03-1017 </papid>syntax-based models use parse-tree representations of the sentences in the training data to learn,among other things, tree transformation probabili ties.</nextsent>
<nextsent>these methods require parser for the target language and, in some cases, the source language too.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1928">
<title id=" I08-1067.xml">simple syntactic and morphological processing can help english hindi statistical machine translation </title>
<section> incorporating morphological information by.  </section>
<citcontext>
<prevsection>
<prevsent>phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases, the joint model, and ibm model 4 (koehn et al, 2003).<papid> N03-1017 </papid>syntax-based models use parse-tree representations of the sentences in the training data to learn,among other things, tree transformation probabili ties.</prevsent>
<prevsent>these methods require parser for the target language and, in some cases, the source language too.</prevsent>
</prevsection>
<citsent citstr=" N04-1014 ">
yamada and knight (2001) propose model that transforms target language parse trees to source language strings by applying reordering, insertion, and translation operations at each node of the tree.graehl and knight (2004) <papid> N04-1014 </papid>and melamed (2004), <papid> P04-1083 </papid>propose methods based on tree-to-tree mappings.</citsent>
<aftsection>
<nextsent>imamura et al (2005) present similar method that achieves significant improvements over phrase based baseline model for japanese-english translation.
</nextsent>
<nextsent>recently, various preprocessing approaches have been proposed for handling syntax within smt.these algorithms attempt to reconcile the word order differences between the source and target language sentences by reordering the source language data prior to the smt training and decoding cycles.
</nextsent>
<nextsent>nieen and ney (2004) propose some restructuring steps for german-english smt.
</nextsent>
<nextsent>popovic andney (2006) report the use of simple local transformation rules for spanish-english and serbian english translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1929">
<title id=" I08-1067.xml">simple syntactic and morphological processing can help english hindi statistical machine translation </title>
<section> incorporating morphological information by.  </section>
<citcontext>
<prevsection>
<prevsent>phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases, the joint model, and ibm model 4 (koehn et al, 2003).<papid> N03-1017 </papid>syntax-based models use parse-tree representations of the sentences in the training data to learn,among other things, tree transformation probabili ties.</prevsent>
<prevsent>these methods require parser for the target language and, in some cases, the source language too.</prevsent>
</prevsection>
<citsent citstr=" P04-1083 ">
yamada and knight (2001) propose model that transforms target language parse trees to source language strings by applying reordering, insertion, and translation operations at each node of the tree.graehl and knight (2004) <papid> N04-1014 </papid>and melamed (2004), <papid> P04-1083 </papid>propose methods based on tree-to-tree mappings.</citsent>
<aftsection>
<nextsent>imamura et al (2005) present similar method that achieves significant improvements over phrase based baseline model for japanese-english translation.
</nextsent>
<nextsent>recently, various preprocessing approaches have been proposed for handling syntax within smt.these algorithms attempt to reconcile the word order differences between the source and target language sentences by reordering the source language data prior to the smt training and decoding cycles.
</nextsent>
<nextsent>nieen and ney (2004) propose some restructuring steps for german-english smt.
</nextsent>
<nextsent>popovic andney (2006) report the use of simple local transformation rules for spanish-english and serbian english translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1930">
<title id=" I08-1067.xml">simple syntactic and morphological processing can help english hindi statistical machine translation </title>
<section> incorporating morphological information by.  </section>
<citcontext>
<prevsection>
<prevsent>collins et al (2006) propose german clause restructuring to improve german english smt.
</prevsent>
<prevsent>the use of morphological information for smt has been reported in (nieen and ney, 2004) and(popovic and ney, 2006).
</prevsent>
</prevsection>
<citsent citstr=" D07-1091 ">
the detailed experiments by nieen and ney (2004) show that the use of morpho-syntactic information drastically reduces the need for bilingual training data.recent work by koehn and hoang (2007) <papid> D07-1091 </papid>pro 514poses factored translation models that combine feature functions to handle syntactic, morphological, and other linguistic information in log-linear model.our work uses preprocessing approach for incorporating syntactic information within phrase based smt system.</citsent>
<aftsection>
<nextsent>for incorporating morphology, we use simple suffix removal program for hindi and morphological analyzer for english.
</nextsent>
<nextsent>these aspects are described in detail in the next section.
</nextsent>
<nextsent>for english-hindi smt 3.1 phrase-based smt: the baseline.
</nextsent>
<nextsent>given source sentence , smt chooses as its translation e?, which is the sentence with the highest prob ability: e?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1933">
<title id=" I08-1067.xml">simple syntactic and morphological processing can help english hindi statistical machine translation </title>
<section> syntactic &amp; morphological information.  </section>
<citcontext>
<prevsection>
<prevsent>given the set of collected phrase pairs, the phrase translation probability is calculated by relative frequency: ?(f |e) = count(f, e)?
</prevsent>
<prevsent>f count(f, e) lexical weighting, which measures how well words within phrase pairs translate to each other, validates the phrase translation, and addresses the problem of data sparsity.the language model p(e) used in our baseline system is trigram model with modified kneser-ney smoothing (chen and goodman, 1998).
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the weights for the various components of the model (phrase translation model, language model, distortion model etc.) are set by minimum error rate training (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>3.2 syntactic information.
</nextsent>
<nextsent>as mentioned in section 2, phrase-based models have emerged as the most successful method for smt.
</nextsent>
<nextsent>these models, however, do not handle syntax in natural way.
</nextsent>
<nextsent>reordering of phrases during translation is typically managed by distortion models, which have proved not entirely satisfactory (collins et al, 2006), especially for language pairs that differa lot in terms of word-order.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1934">
<title id=" I08-1004.xml">context sensitive convolution tree kernel for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much research work has been done in this direction.
</prevsent>
<prevsent>prior researches apply feature-based methods to select and define set of flat features, which can be mined from the parse trees, to represent particular structured information in the parse tree, such as the grammatical role (e.g. subject or object), according to the particular application.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
indeed, such feature-based methods have been widely applied in parsing (collins 1999; charniak 2001), <papid> P01-1017 </papid>semantic role labeling (pradhan et al 2005), semantic relation extraction (zhou et al 2005) <papid> P05-1053 </papid>and co-reference resolution (lapin and leass 1994; aone and bennett 1995; mitkov 1998; <papid> P98-2143 </papid>yang et al 2004; luo and zitouni 2005; bergsma and lin 2006).<papid> P06-1005 </papid></citsent>
<aftsection>
<nextsent>the major problem with feature-based methods on exploring structured information is that they may fail to well capture complex structured information, which is critical for further performance improvement.
</nextsent>
<nextsent>the current trend is to explore kernel-based methods (haussler, 1999) which can implicitly explore features in high dimensional space by employing kernel to calculate the similarity between two objects directly.
</nextsent>
<nextsent>in particular, the ker nel-based methods could be very effective at reducing the burden of feature engineering for structured objects in nlp, e.g. the parse tree structure in coreference resolution.
</nextsent>
<nextsent>during recent years, various tree kernels, such as the convolution tree kernel (collins and duffy 2001), the shallow parse tree kernel (zelenko et al 2003) and the dependency tree kernel (culota and sorensen 2004), have been proposed in the literature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1935">
<title id=" I08-1004.xml">context sensitive convolution tree kernel for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much research work has been done in this direction.
</prevsent>
<prevsent>prior researches apply feature-based methods to select and define set of flat features, which can be mined from the parse trees, to represent particular structured information in the parse tree, such as the grammatical role (e.g. subject or object), according to the particular application.
</prevsent>
</prevsection>
<citsent citstr=" P05-1053 ">
indeed, such feature-based methods have been widely applied in parsing (collins 1999; charniak 2001), <papid> P01-1017 </papid>semantic role labeling (pradhan et al 2005), semantic relation extraction (zhou et al 2005) <papid> P05-1053 </papid>and co-reference resolution (lapin and leass 1994; aone and bennett 1995; mitkov 1998; <papid> P98-2143 </papid>yang et al 2004; luo and zitouni 2005; bergsma and lin 2006).<papid> P06-1005 </papid></citsent>
<aftsection>
<nextsent>the major problem with feature-based methods on exploring structured information is that they may fail to well capture complex structured information, which is critical for further performance improvement.
</nextsent>
<nextsent>the current trend is to explore kernel-based methods (haussler, 1999) which can implicitly explore features in high dimensional space by employing kernel to calculate the similarity between two objects directly.
</nextsent>
<nextsent>in particular, the ker nel-based methods could be very effective at reducing the burden of feature engineering for structured objects in nlp, e.g. the parse tree structure in coreference resolution.
</nextsent>
<nextsent>during recent years, various tree kernels, such as the convolution tree kernel (collins and duffy 2001), the shallow parse tree kernel (zelenko et al 2003) and the dependency tree kernel (culota and sorensen 2004), have been proposed in the literature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1936">
<title id=" I08-1004.xml">context sensitive convolution tree kernel for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much research work has been done in this direction.
</prevsent>
<prevsent>prior researches apply feature-based methods to select and define set of flat features, which can be mined from the parse trees, to represent particular structured information in the parse tree, such as the grammatical role (e.g. subject or object), according to the particular application.
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
indeed, such feature-based methods have been widely applied in parsing (collins 1999; charniak 2001), <papid> P01-1017 </papid>semantic role labeling (pradhan et al 2005), semantic relation extraction (zhou et al 2005) <papid> P05-1053 </papid>and co-reference resolution (lapin and leass 1994; aone and bennett 1995; mitkov 1998; <papid> P98-2143 </papid>yang et al 2004; luo and zitouni 2005; bergsma and lin 2006).<papid> P06-1005 </papid></citsent>
<aftsection>
<nextsent>the major problem with feature-based methods on exploring structured information is that they may fail to well capture complex structured information, which is critical for further performance improvement.
</nextsent>
<nextsent>the current trend is to explore kernel-based methods (haussler, 1999) which can implicitly explore features in high dimensional space by employing kernel to calculate the similarity between two objects directly.
</nextsent>
<nextsent>in particular, the ker nel-based methods could be very effective at reducing the burden of feature engineering for structured objects in nlp, e.g. the parse tree structure in coreference resolution.
</nextsent>
<nextsent>during recent years, various tree kernels, such as the convolution tree kernel (collins and duffy 2001), the shallow parse tree kernel (zelenko et al 2003) and the dependency tree kernel (culota and sorensen 2004), have been proposed in the literature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1937">
<title id=" I08-1004.xml">context sensitive convolution tree kernel for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much research work has been done in this direction.
</prevsent>
<prevsent>prior researches apply feature-based methods to select and define set of flat features, which can be mined from the parse trees, to represent particular structured information in the parse tree, such as the grammatical role (e.g. subject or object), according to the particular application.
</prevsent>
</prevsection>
<citsent citstr=" P06-1005 ">
indeed, such feature-based methods have been widely applied in parsing (collins 1999; charniak 2001), <papid> P01-1017 </papid>semantic role labeling (pradhan et al 2005), semantic relation extraction (zhou et al 2005) <papid> P05-1053 </papid>and co-reference resolution (lapin and leass 1994; aone and bennett 1995; mitkov 1998; <papid> P98-2143 </papid>yang et al 2004; luo and zitouni 2005; bergsma and lin 2006).<papid> P06-1005 </papid></citsent>
<aftsection>
<nextsent>the major problem with feature-based methods on exploring structured information is that they may fail to well capture complex structured information, which is critical for further performance improvement.
</nextsent>
<nextsent>the current trend is to explore kernel-based methods (haussler, 1999) which can implicitly explore features in high dimensional space by employing kernel to calculate the similarity between two objects directly.
</nextsent>
<nextsent>in particular, the ker nel-based methods could be very effective at reducing the burden of feature engineering for structured objects in nlp, e.g. the parse tree structure in coreference resolution.
</nextsent>
<nextsent>during recent years, various tree kernels, such as the convolution tree kernel (collins and duffy 2001), the shallow parse tree kernel (zelenko et al 2003) and the dependency tree kernel (culota and sorensen 2004), have been proposed in the literature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1938">
<title id=" I08-1004.xml">context sensitive convolution tree kernel for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, the ker nel-based methods could be very effective at reducing the burden of feature engineering for structured objects in nlp, e.g. the parse tree structure in coreference resolution.
</prevsent>
<prevsent>during recent years, various tree kernels, such as the convolution tree kernel (collins and duffy 2001), the shallow parse tree kernel (zelenko et al 2003) and the dependency tree kernel (culota and sorensen 2004), have been proposed in the literature.
</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
among previous tree kernels, the convolution tree kernel represents the state-of-the-art and have been successfully applied by collins and duffy (2002) on parsing, moschitti (2004) <papid> P04-1043 </papid>on semantic role labeling, zhang et al  (2006) <papid> P06-1104 </papid>on semantic relation extraction and yang et al (2006) <papid> P06-1006 </papid>on pronoun resolution.</citsent>
<aftsection>
<nextsent>however, there exist two problems in collins and duffys kernel.
</nextsent>
<nextsent>the first is that the sub-trees enumerated in the tree kernel are context-free.
</nextsent>
<nextsent>that is, each sub-tree enumerated in the tree kernel does not consider the context information outside the sub-tree.
</nextsent>
<nextsent>the second is how to decide proper tree span in the tree kernel computation according to the particular application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1939">
<title id=" I08-1004.xml">context sensitive convolution tree kernel for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, the ker nel-based methods could be very effective at reducing the burden of feature engineering for structured objects in nlp, e.g. the parse tree structure in coreference resolution.
</prevsent>
<prevsent>during recent years, various tree kernels, such as the convolution tree kernel (collins and duffy 2001), the shallow parse tree kernel (zelenko et al 2003) and the dependency tree kernel (culota and sorensen 2004), have been proposed in the literature.
</prevsent>
</prevsection>
<citsent citstr=" P06-1104 ">
among previous tree kernels, the convolution tree kernel represents the state-of-the-art and have been successfully applied by collins and duffy (2002) on parsing, moschitti (2004) <papid> P04-1043 </papid>on semantic role labeling, zhang et al  (2006) <papid> P06-1104 </papid>on semantic relation extraction and yang et al (2006) <papid> P06-1006 </papid>on pronoun resolution.</citsent>
<aftsection>
<nextsent>however, there exist two problems in collins and duffys kernel.
</nextsent>
<nextsent>the first is that the sub-trees enumerated in the tree kernel are context-free.
</nextsent>
<nextsent>that is, each sub-tree enumerated in the tree kernel does not consider the context information outside the sub-tree.
</nextsent>
<nextsent>the second is how to decide proper tree span in the tree kernel computation according to the particular application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1940">
<title id=" I08-1004.xml">context sensitive convolution tree kernel for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, the ker nel-based methods could be very effective at reducing the burden of feature engineering for structured objects in nlp, e.g. the parse tree structure in coreference resolution.
</prevsent>
<prevsent>during recent years, various tree kernels, such as the convolution tree kernel (collins and duffy 2001), the shallow parse tree kernel (zelenko et al 2003) and the dependency tree kernel (culota and sorensen 2004), have been proposed in the literature.
</prevsent>
</prevsection>
<citsent citstr=" P06-1006 ">
among previous tree kernels, the convolution tree kernel represents the state-of-the-art and have been successfully applied by collins and duffy (2002) on parsing, moschitti (2004) <papid> P04-1043 </papid>on semantic role labeling, zhang et al  (2006) <papid> P06-1104 </papid>on semantic relation extraction and yang et al (2006) <papid> P06-1006 </papid>on pronoun resolution.</citsent>
<aftsection>
<nextsent>however, there exist two problems in collins and duffys kernel.
</nextsent>
<nextsent>the first is that the sub-trees enumerated in the tree kernel are context-free.
</nextsent>
<nextsent>that is, each sub-tree enumerated in the tree kernel does not consider the context information outside the sub-tree.
</nextsent>
<nextsent>the second is how to decide proper tree span in the tree kernel computation according to the particular application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1952">
<title id=" I08-1004.xml">context sensitive convolution tree kernel for pronoun resolution </title>
<section> context sensitive convolution tree.  </section>
<citcontext>
<prevsection>
<prevsent>is not attached since the former is compatible with the anaphor and the latter is not compatible with the anaphor.
</prevsent>
<prevsent>in this way, the competition between the considered candidate and other compatible candidates can be included in the tree span.
</prevsent>
</prevsection>
<citsent citstr=" P03-1023 ">
in some sense, this is natural extension of the twin-candidate learning approach proposed in yang et al  (2003), <papid> P03-1023 </papid>which explicitly models the competition between two antecedent candidates.</citsent>
<aftsection>
<nextsent>3) for each node in the tree span, attaching the path from the node to the predicate terminal node if it is predicate-headed node.
</nextsent>
<nextsent>as shown in figure 1(c), said?
</nextsent>
<nextsent>and bit?
</nextsent>
<nextsent>are attached.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1955">
<title id=" I08-1004.xml">context sensitive convolution tree kernel for pronoun resolution </title>
<section> experimentation.  </section>
<citcontext>
<prevsection>
<prevsent>this paper focuses on the third-person pronoun resolution and, in all our experiments, uses the ace 2003 corpus for evaluation.
</prevsent>
<prevsent>this ace corpus contains ~3.9k pronouns in the training data and ~1.0k pronouns in the test data.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
similar to soon et al (2001), <papid> J01-4004 </papid>an input raw text is first preprocessed automatically by pipeline of nlp components, including sentence boundary detection, pos tagging, named entity recognition and phrase chunking, and then training or test instance is formed by pronoun and one of its antecedent candidates.</citsent>
<aftsection>
<nextsent>during training, for each ana phor encountered, positive instance is created by pairing the anaphor and its closest antecedent while set of negative instances is formed by pairing the anaphor with each of the non-coreferential candidates.
</nextsent>
<nextsent>based on the training instances, binary classifier is generated using particular learning algorithm.
</nextsent>
<nextsent>in this paper, we use svm light deleveloped by joachims (1998).
</nextsent>
<nextsent>during resolution, an anaphor is first paired in turn with each preceding antecedent candidate to form test instance, which is presented to classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1958">
<title id=" I05-3016.xml">resolving pronominal references in chinese with the hobbs algorithm </title>
<section> the hobbs algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>structurally, there are many sentences in thectb that consist of just sequence of parallel independent clauses, separated by commas or semi colons.
</prevsent>
<prevsent>these multi-clause sentences were treated as single sentences from the point of view of the algorithm.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the implementation of the algorithm is one that has core of code that can run on either the penn treebank (marcus et al , 1993) <papid> J93-2004 </papid>or on the chinese treebank.</citsent>
<aftsection>
<nextsent>the only differences between the two executables are in the tables for the part-of-speech tags and the syntactic phrase labels (e.g., pn vs. prn for pronouns and ip vs. for clauses), and in separate np head-finding routines (not used in the current study).
</nextsent>
<nextsent>despite the svo similarity between chinese and english, we were interested to see if there 118might be differences in the success of the algorithm due to structural differences between the languages that might require adapting its steps to chinese.
</nextsent>
<nextsent>the most obvious place to look was in the placement of modifiers relative to the head noun in an np.
</nextsent>
<nextsent>although unplanned, it turned out that the policy of annotating complex nps at the parent level rather than at the head noun level actually made this moot point because of the top down nature of the tree traversal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1959">
<title id=" I05-3016.xml">resolving pronominal references in chinese with the hobbs algorithm </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>4of the 31 anaphors marked amb, in only eight cases (25.8%) did the algorithm pick an antecedent that was one of the choices given by the annotators.
</prevsent>
<prevsent>all eight were *pro*.
</prevsent>
</prevsection>
<citsent citstr=" J02-3003 ">
119 table 1: distribution of all anaphors indexed amb cat, no amb total count % count % count % counts % overt 176 53.2% 2 6.5% 29 24.8% 207 43.2% *pro* 155 46.8% 29 93.5% 88 75.2% 272 56.8% 331 69.1% 31 6.5% 117 24.4% 479 table 2: counts by syntactic level for all anaphors correct wrong cat(incl amb) m2 m m2 m m2 total overt 47 8 45 14 5 57 10 2 19 207 *pro* 11 17 48 4 10 65 15 21 81 272 58 25 93 18 15 122 25 23 100 479these data are consistent with the observations made by miltsakaki in her 2002 paper (milt sakaki, 2002).<papid> J02-3003 </papid></citsent>
<aftsection>
<nextsent>taking main clause and all its dependent clauses as unit, she found that there were different mechanisms needed to account for (1) topic continuity from unit to unit (inter sentential), and (2) focusing preferences within unit (intra-sentential).
</nextsent>
<nextsent>topic continuity was best modeled structurally but the semantics and prag matics of verbs and connectives were prominent within unit.
</nextsent>
<nextsent>since inter-sentential anaphoric links relate to topic continuity, structural rules work best for resolution at the matrix level, while anaphors in subordinate clauses are subject to the seman tic/pragmatic constraints of the predicates and connectives.
</nextsent>
<nextsent>in our results the anaphors that are subjects of matrix clauses tend to resolve inter-sententially(that is, step 4 of the algorithm is the resolving condition), while the anaphors insubordinate constructions are more likely to have intra sentential antecedents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1960">
<title id=" I08-2124.xml">towards automated semantic analysis on biomedical research articles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, human annotations are either too expensive or not always available and this has become bottleneck to developing supervised ie methods to new domains.
</prevsent>
<prevsent>fortunately, active learning systems design strategies to select the most informative training examples.
</prevsent>
</prevsection>
<citsent citstr=" P04-1075 ">
this process can achieve certain levels of performance faster and reduce human annotation (e.g., thompson et al , 1999; shen et al , 2004).<papid> P04-1075 </papid></citsent>
<aftsection>
<nextsent>in this paper, we present an empirical study on adapting crf model to conduct semantic analysis on biomedical research literature.
</nextsent>
<nextsent>we integrate an uncertainty-based active learning framework with the crf model to dynamically select the most informative training examples and reduce human annotation cost.
</nextsent>
<nextsent>a systematic study with exhaustive experimental evaluations shows that it can achieve satisfactory performance on biomedical data while requiring less human annotation.
</nextsent>
<nextsent>unlike direct estimation on target individuals in traditional active learning, we use two heuristic certainty scores, peer comparison certainty and set comparison certainty, to indirectly estimate sequences labeling quality in crf models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1961">
<title id=" I08-2124.xml">towards automated semantic analysis on biomedical research articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>knowledge base management systems allow individual users to construct personalized repositories of knowledge statements based on their own interaction with the research literature (stephan et al , 2001; burns and cheng, 2006).
</prevsent>
<prevsent>but this process of data entry and cur ation is manual.
</prevsent>
</prevsection>
<citsent citstr=" P06-1059 ">
current approaches on biomedical text mining (e.g., srinivas et al , 2005; okanohara et al , 2006) <papid> P06-1059 </papid>tend to address the tasks of named entity recognition or relation extraction, and our goal is more complex: to extract computational representations of the minimum information in given experiment type.</citsent>
<aftsection>
<nextsent>pattern-based ie approaches employ seed data to learn useful patterns to pinpoint required fields values (e.g. ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>feng et al , 2006).</nextsent>
<nextsent>however, this only works if the data corpus is rich enough to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1962">
<title id=" I08-2124.xml">towards automated semantic analysis on biomedical research articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>but this process of data entry and cur ation is manual.
</prevsent>
<prevsent>current approaches on biomedical text mining (e.g., srinivas et al , 2005; okanohara et al , 2006) <papid> P06-1059 </papid>tend to address the tasks of named entity recognition or relation extraction, and our goal is more complex: to extract computational representations of the minimum information in given experiment type.</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
pattern-based ie approaches employ seed data to learn useful patterns to pinpoint required fields values (e.g. ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>feng et al , 2006).</citsent>
<aftsection>
<nextsent>however, this only works if the data corpus is rich enough to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem.
</nextsent>
<nextsent>within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint.
</nextsent>
<nextsent>871 the crf model (lafferty et al , 2001) provides compact way to integrate different types of features for sequential labeling problems.
</nextsent>
<nextsent>reported work includes improved model variants (e.g., jiao et al , 2006) <papid> P06-1027 </papid>and applications such as web data extraction (pinto et al , 2003), scientific citation extraction (peng and mccallum, 2004), <papid> N04-1042 </papid>word alignment (blunsom and cohn, 2006), <papid> P06-1009 </papid>and discourse level chunking (feng et al , 2007).<papid> D07-1088 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1963">
<title id=" I08-2124.xml">towards automated semantic analysis on biomedical research articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>but this process of data entry and cur ation is manual.
</prevsent>
<prevsent>current approaches on biomedical text mining (e.g., srinivas et al , 2005; okanohara et al , 2006) <papid> P06-1059 </papid>tend to address the tasks of named entity recognition or relation extraction, and our goal is more complex: to extract computational representations of the minimum information in given experiment type.</prevsent>
</prevsection>
<citsent citstr=" P05-1060 ">
pattern-based ie approaches employ seed data to learn useful patterns to pinpoint required fields values (e.g. ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>feng et al , 2006).</citsent>
<aftsection>
<nextsent>however, this only works if the data corpus is rich enough to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem.
</nextsent>
<nextsent>within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint.
</nextsent>
<nextsent>871 the crf model (lafferty et al , 2001) provides compact way to integrate different types of features for sequential labeling problems.
</nextsent>
<nextsent>reported work includes improved model variants (e.g., jiao et al , 2006) <papid> P06-1027 </papid>and applications such as web data extraction (pinto et al , 2003), scientific citation extraction (peng and mccallum, 2004), <papid> N04-1042 </papid>word alignment (blunsom and cohn, 2006), <papid> P06-1009 </papid>and discourse level chunking (feng et al , 2007).<papid> D07-1088 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1964">
<title id=" I08-2124.xml">towards automated semantic analysis on biomedical research articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint.
</prevsent>
<prevsent>871 the crf model (lafferty et al , 2001) provides compact way to integrate different types of features for sequential labeling problems.
</prevsent>
</prevsection>
<citsent citstr=" P06-1027 ">
reported work includes improved model variants (e.g., jiao et al , 2006) <papid> P06-1027 </papid>and applications such as web data extraction (pinto et al , 2003), scientific citation extraction (peng and mccallum, 2004), <papid> N04-1042 </papid>word alignment (blunsom and cohn, 2006), <papid> P06-1009 </papid>and discourse level chunking (feng et al , 2007).<papid> D07-1088 </papid></citsent>
<aftsection>
<nextsent>pool-based active learning was first successfully applied to language processing on text classification (lewis and gale, 1994; mccallum and nigam, 1998; tong and koller, 2000).
</nextsent>
<nextsent>it was also gradually applied to nlp tasks, such as information extraction (thompson et al , 1999); semantic parsing (thompson et al , 1999); statistical parsing (tang et al , 2002); <papid> P02-1016 </papid>ner (shen et al , 2004); <papid> P04-1075 </papid>and word sense disambiguation (chen et al , 2006).</nextsent>
<nextsent>in this paper, we use crf models to perform more complex task on the primary tte experimental results and adapt it to process new biomedical data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1965">
<title id=" I08-2124.xml">towards automated semantic analysis on biomedical research articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint.
</prevsent>
<prevsent>871 the crf model (lafferty et al , 2001) provides compact way to integrate different types of features for sequential labeling problems.
</prevsent>
</prevsection>
<citsent citstr=" N04-1042 ">
reported work includes improved model variants (e.g., jiao et al , 2006) <papid> P06-1027 </papid>and applications such as web data extraction (pinto et al , 2003), scientific citation extraction (peng and mccallum, 2004), <papid> N04-1042 </papid>word alignment (blunsom and cohn, 2006), <papid> P06-1009 </papid>and discourse level chunking (feng et al , 2007).<papid> D07-1088 </papid></citsent>
<aftsection>
<nextsent>pool-based active learning was first successfully applied to language processing on text classification (lewis and gale, 1994; mccallum and nigam, 1998; tong and koller, 2000).
</nextsent>
<nextsent>it was also gradually applied to nlp tasks, such as information extraction (thompson et al , 1999); semantic parsing (thompson et al , 1999); statistical parsing (tang et al , 2002); <papid> P02-1016 </papid>ner (shen et al , 2004); <papid> P04-1075 </papid>and word sense disambiguation (chen et al , 2006).</nextsent>
<nextsent>in this paper, we use crf models to perform more complex task on the primary tte experimental results and adapt it to process new biomedical data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1966">
<title id=" I08-2124.xml">towards automated semantic analysis on biomedical research articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint.
</prevsent>
<prevsent>871 the crf model (lafferty et al , 2001) provides compact way to integrate different types of features for sequential labeling problems.
</prevsent>
</prevsection>
<citsent citstr=" P06-1009 ">
reported work includes improved model variants (e.g., jiao et al , 2006) <papid> P06-1027 </papid>and applications such as web data extraction (pinto et al , 2003), scientific citation extraction (peng and mccallum, 2004), <papid> N04-1042 </papid>word alignment (blunsom and cohn, 2006), <papid> P06-1009 </papid>and discourse level chunking (feng et al , 2007).<papid> D07-1088 </papid></citsent>
<aftsection>
<nextsent>pool-based active learning was first successfully applied to language processing on text classification (lewis and gale, 1994; mccallum and nigam, 1998; tong and koller, 2000).
</nextsent>
<nextsent>it was also gradually applied to nlp tasks, such as information extraction (thompson et al , 1999); semantic parsing (thompson et al , 1999); statistical parsing (tang et al , 2002); <papid> P02-1016 </papid>ner (shen et al , 2004); <papid> P04-1075 </papid>and word sense disambiguation (chen et al , 2006).</nextsent>
<nextsent>in this paper, we use crf models to perform more complex task on the primary tte experimental results and adapt it to process new biomedical data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1967">
<title id=" I08-2124.xml">towards automated semantic analysis on biomedical research articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint.
</prevsent>
<prevsent>871 the crf model (lafferty et al , 2001) provides compact way to integrate different types of features for sequential labeling problems.
</prevsent>
</prevsection>
<citsent citstr=" D07-1088 ">
reported work includes improved model variants (e.g., jiao et al , 2006) <papid> P06-1027 </papid>and applications such as web data extraction (pinto et al , 2003), scientific citation extraction (peng and mccallum, 2004), <papid> N04-1042 </papid>word alignment (blunsom and cohn, 2006), <papid> P06-1009 </papid>and discourse level chunking (feng et al , 2007).<papid> D07-1088 </papid></citsent>
<aftsection>
<nextsent>pool-based active learning was first successfully applied to language processing on text classification (lewis and gale, 1994; mccallum and nigam, 1998; tong and koller, 2000).
</nextsent>
<nextsent>it was also gradually applied to nlp tasks, such as information extraction (thompson et al , 1999); semantic parsing (thompson et al , 1999); statistical parsing (tang et al , 2002); <papid> P02-1016 </papid>ner (shen et al , 2004); <papid> P04-1075 </papid>and word sense disambiguation (chen et al , 2006).</nextsent>
<nextsent>in this paper, we use crf models to perform more complex task on the primary tte experimental results and adapt it to process new biomedical data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1968">
<title id=" I08-2124.xml">towards automated semantic analysis on biomedical research articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>reported work includes improved model variants (e.g., jiao et al , 2006) <papid> P06-1027 </papid>and applications such as web data extraction (pinto et al , 2003), scientific citation extraction (peng and mccallum, 2004), <papid> N04-1042 </papid>word alignment (blunsom and cohn, 2006), <papid> P06-1009 </papid>and discourse level chunking (feng et al , 2007).<papid> D07-1088 </papid></prevsent>
<prevsent>pool-based active learning was first successfully applied to language processing on text classification (lewis and gale, 1994; mccallum and nigam, 1998; tong and koller, 2000).</prevsent>
</prevsection>
<citsent citstr=" P02-1016 ">
it was also gradually applied to nlp tasks, such as information extraction (thompson et al , 1999); semantic parsing (thompson et al , 1999); statistical parsing (tang et al , 2002); <papid> P02-1016 </papid>ner (shen et al , 2004); <papid> P04-1075 </papid>and word sense disambiguation (chen et al , 2006).</citsent>
<aftsection>
<nextsent>in this paper, we use crf models to perform more complex task on the primary tte experimental results and adapt it to process new biomedical data.
</nextsent>
<nextsent>3.1 what knowledge is of interest?.
</nextsent>
<nextsent>the goal of tte is to chart the inter connectivity of the brain by injecting tracer chemicals into region of the brain and then identifying corresponding labeled regions where the tracer is transported to.
</nextsent>
<nextsent>a typical tte paper may report experiments about one or many labeled regions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1970">
<title id=" I08-2124.xml">towards automated semantic analysis on biomedical research articles </title>
<section> uncertainty-based active learning.  </section>
<citcontext>
<prevsection>
<prevsent>unlike traditional classification tasks, where disagreement or uncertainty is easy to obtain on target individuals, information extraction tasks in our problem take whole sequence of tokens that might include several slots as processing units.
</prevsent>
<prevsent>we therefore need to make decisions on whether full sequence should be returned for labeling.
</prevsent>
</prevsection>
<citsent citstr=" N04-4028 ">
estimations on confidence for single segments in the crf model have been proposed by (culotta and mccallum, 2004; <papid> N04-4028 </papid>kristjannson et al , 2004).</citsent>
<aftsection>
<nextsent>however as every processing unit in the dataset is at the sentence level and we make decisions at the sentence level to train better sequential labeling models, we define heuristic scores at the sentence level.
</nextsent>
<nextsent>symons et al  (2006) presents multi-criterion for active learning with crf models, but our motivation is from different perspective.
</nextsent>
<nextsent>the labeling result for every sentence corresponds to decoding path in the state transition network.
</nextsent>
<nextsent>inspired by the decoding and re-ranking approaches in statistical machine translation, we use two heuristic scores to measure the degree of correctness of the top labeling path, namely, peer comparison certainty and set comparison certainty.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1971">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, system which generates paraphrases forgiven expressions is useful for text-transcoding tasks, such as machine translation and summarization, as well as beneficial to human, for instance, in text-to-speech, text simplification, and writing assistance.
</prevsent>
<prevsent>paraphrase phenomena can roughly be divided into two groups according to their compositionality.examples in (1) exhibit degree of compositional ity, while each example in (2) is composed of totally different lexical items.
</prevsent>
</prevsection>
<citsent citstr=" W07-1425 ">
(1) a. be in our favor ? be favorable for us b. show sharp decrease ? decrease sharply (fujita et al, 2007) <papid> W07-1425 </papid>2) a. burst into tears ? cried b. comfort ? console (barzilay and mckeown, 2001) <papid> P01-1008 </papid>number of studies have been carried out onboth compositional (morpho-syntactic) and non compositional (lexical and idiomatic) paraphrases (see section 2).</citsent>
<aftsection>
<nextsent>in most research, paraphrases have been represented with the similar templates, such as shown in (3) and (4).
</nextsent>
<nextsent>(3) a. 1 n 2 ? 1 v -ing of 2 b. 1 n 2 ? 2 be -en by 1 (harris, 1957) (4) a. wrote ? is the author of b. solves ? deals with (lin and pantel, 2001) the weakness of these templates is that they should be applied only in some contexts.
</nextsent>
<nextsent>in other words, the lack of applicability conditions for slot fillers may lead incorrect paraphrases.
</nextsent>
<nextsent>one wayto specify the applicability condition is to enumerate correct slot fillers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1973">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, system which generates paraphrases forgiven expressions is useful for text-transcoding tasks, such as machine translation and summarization, as well as beneficial to human, for instance, in text-to-speech, text simplification, and writing assistance.
</prevsent>
<prevsent>paraphrase phenomena can roughly be divided into two groups according to their compositionality.examples in (1) exhibit degree of compositional ity, while each example in (2) is composed of totally different lexical items.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
(1) a. be in our favor ? be favorable for us b. show sharp decrease ? decrease sharply (fujita et al, 2007) <papid> W07-1425 </papid>2) a. burst into tears ? cried b. comfort ? console (barzilay and mckeown, 2001) <papid> P01-1008 </papid>number of studies have been carried out onboth compositional (morpho-syntactic) and non compositional (lexical and idiomatic) paraphrases (see section 2).</citsent>
<aftsection>
<nextsent>in most research, paraphrases have been represented with the similar templates, such as shown in (3) and (4).
</nextsent>
<nextsent>(3) a. 1 n 2 ? 1 v -ing of 2 b. 1 n 2 ? 2 be -en by 1 (harris, 1957) (4) a. wrote ? is the author of b. solves ? deals with (lin and pantel, 2001) the weakness of these templates is that they should be applied only in some contexts.
</nextsent>
<nextsent>in other words, the lack of applicability conditions for slot fillers may lead incorrect paraphrases.
</nextsent>
<nextsent>one wayto specify the applicability condition is to enumerate correct slot fillers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1974">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in other words, the lack of applicability conditions for slot fillers may lead incorrect paraphrases.
</prevsent>
<prevsent>one wayto specify the applicability condition is to enumerate correct slot fillers.
</prevsent>
</prevsection>
<citsent citstr=" N07-1071 ">
for example, pantel et al(2007) <papid> N07-1071 </papid>have harvested instances for the given paraphrase templates based on the co-occurrence statistics of slot fillers and lexicalized part of templates (e.g. deal with?</citsent>
<aftsection>
<nextsent>in (4b)).
</nextsent>
<nextsent>yet, there is no method which assesses semantic equivalence and syntactic substitutability of resultant pairs of expressions.
</nextsent>
<nextsent>537 in this paper, we propose method of directly computing semantic equivalence and syntactic substitutability, i.e., paraphrasability, particularly focusing on automatically generated compositional paraphrases (henceforth, syntactic variants) of predicatephrases.
</nextsent>
<nextsent>while previous studies have mainly targeted at words or canned phrases, we treat predicate phrases having bit more complex structures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1975">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>meaning-text theory (mtt) is framework which takes into account several types of lexical dependencies in handling paraphrases (melcuk and polgue`re, 1987).
</prevsent>
<prevsent>a bottleneck of mtt is that huge amount of lexical knowledge is required to represent various relationships between lexical items.
</prevsent>
</prevsection>
<citsent citstr=" P99-1044 ">
jacquemin (1999) <papid> P99-1044 </papid>has represented thesyntagmatic and paradigmatic correspondences between paraphrases with context-free transformation rules and morphological and/or semantic relations between lexical items, targeting at syntactic variants of technical terms that are typically noun phrases consisting of more than one word.</citsent>
<aftsection>
<nextsent>we have proposed framework of generating syntactic variants of predicate phrases (fujita et al, 2007).<papid> W07-1425 </papid></nextsent>
<nextsent>following the previous work, we have been developing three sorts of resources for japanese.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1977">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>among number of models based on this hypothesis, two algorithms are referred to as the state-of-the-art.
</prevsent>
<prevsent>dirt (lin and pantel, 2001) collects paraphrase rules consisting of pair of paths between two nominal slots based on point-wise mutual information.
</prevsent>
</prevsection>
<citsent citstr=" W04-3206 ">
tease (szpektor et al, 2004) <papid> W04-3206 </papid>discovers binary relation templates from the web basedon sets of representative entities forgiven binary relation templates.</citsent>
<aftsection>
<nextsent>these systems often output directional rules such as exemplified in (5).
</nextsent>
<nextsent>(5) a. is charged by ? announced the arrest of b. prevent ? lower the risk of they are actually called inference/entailment rules,and paraphrase is defined as bidirectional infer ence/entailment relation1.
</nextsent>
<nextsent>while the similarity scorein dirt is symmetric forgiven pair of paths, the algorithm of tease considers the direction.
</nextsent>
<nextsent>the other utilizes sort of parallel texts, such as multiple translation of the same text (barzilay and mckeown, 2001; <papid> P01-1008 </papid>pang et al, 2003), <papid> N03-1024 </papid>corresponding articles from multiple news sources (barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al, 2004), <papid> C04-1051 </papid>and bilingual corpus (wu and zhou, 2003; bannard and callison-burch,2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1979">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(5) a. is charged by ? announced the arrest of b. prevent ? lower the risk of they are actually called inference/entailment rules,and paraphrase is defined as bidirectional infer ence/entailment relation1.
</prevsent>
<prevsent>while the similarity scorein dirt is symmetric forgiven pair of paths, the algorithm of tease considers the direction.
</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
the other utilizes sort of parallel texts, such as multiple translation of the same text (barzilay and mckeown, 2001; <papid> P01-1008 </papid>pang et al, 2003), <papid> N03-1024 </papid>corresponding articles from multiple news sources (barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al, 2004), <papid> C04-1051 </papid>and bilingual corpus (wu and zhou, 2003; bannard and callison-burch,2005).</citsent>
<aftsection>
<nextsent>this approach is, however, limited by the difficulty of obtaining parallel/comparable corpora.
</nextsent>
<nextsent>2.3 acquiring paraphrase instances.
</nextsent>
<nextsent>as reviewed in section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified.
</nextsent>
<nextsent>to avoid the drawback,several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (sekine, 2005; <papid> I05-5011 </papid>torisawa, 2006).<papid> N06-1008 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1980">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(5) a. is charged by ? announced the arrest of b. prevent ? lower the risk of they are actually called inference/entailment rules,and paraphrase is defined as bidirectional infer ence/entailment relation1.
</prevsent>
<prevsent>while the similarity scorein dirt is symmetric forgiven pair of paths, the algorithm of tease considers the direction.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
the other utilizes sort of parallel texts, such as multiple translation of the same text (barzilay and mckeown, 2001; <papid> P01-1008 </papid>pang et al, 2003), <papid> N03-1024 </papid>corresponding articles from multiple news sources (barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al, 2004), <papid> C04-1051 </papid>and bilingual corpus (wu and zhou, 2003; bannard and callison-burch,2005).</citsent>
<aftsection>
<nextsent>this approach is, however, limited by the difficulty of obtaining parallel/comparable corpora.
</nextsent>
<nextsent>2.3 acquiring paraphrase instances.
</nextsent>
<nextsent>as reviewed in section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified.
</nextsent>
<nextsent>to avoid the drawback,several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (sekine, 2005; <papid> I05-5011 </papid>torisawa, 2006).<papid> N06-1008 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1981">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(5) a. is charged by ? announced the arrest of b. prevent ? lower the risk of they are actually called inference/entailment rules,and paraphrase is defined as bidirectional infer ence/entailment relation1.
</prevsent>
<prevsent>while the similarity scorein dirt is symmetric forgiven pair of paths, the algorithm of tease considers the direction.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
the other utilizes sort of parallel texts, such as multiple translation of the same text (barzilay and mckeown, 2001; <papid> P01-1008 </papid>pang et al, 2003), <papid> N03-1024 </papid>corresponding articles from multiple news sources (barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al, 2004), <papid> C04-1051 </papid>and bilingual corpus (wu and zhou, 2003; bannard and callison-burch,2005).</citsent>
<aftsection>
<nextsent>this approach is, however, limited by the difficulty of obtaining parallel/comparable corpora.
</nextsent>
<nextsent>2.3 acquiring paraphrase instances.
</nextsent>
<nextsent>as reviewed in section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified.
</nextsent>
<nextsent>to avoid the drawback,several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (sekine, 2005; <papid> I05-5011 </papid>torisawa, 2006).<papid> N06-1008 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1982">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 acquiring paraphrase instances.
</prevsent>
<prevsent>as reviewed in section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified.
</prevsent>
</prevsection>
<citsent citstr=" I05-5011 ">
to avoid the drawback,several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (sekine, 2005; <papid> I05-5011 </papid>torisawa, 2006).<papid> N06-1008 </papid></citsent>
<aftsection>
<nextsent>although these clues restrict phenomena to those appearing in particular domain or those describing coordinated events, they have enabled us to collect 1see http://nlp.cs.nyu.edu/wtep/ 538paraphrases accurately.
</nextsent>
<nextsent>the notion of inferential selectional preference (isp) has been introduced by pantel et al (2007).<papid> N07-1071 </papid></nextsent>
<nextsent>isp can capture more general phenomena than above two; however, it lacks abilities to distinguish antonym relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1983">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 acquiring paraphrase instances.
</prevsent>
<prevsent>as reviewed in section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified.
</prevsent>
</prevsection>
<citsent citstr=" N06-1008 ">
to avoid the drawback,several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (sekine, 2005; <papid> I05-5011 </papid>torisawa, 2006).<papid> N06-1008 </papid></citsent>
<aftsection>
<nextsent>although these clues restrict phenomena to those appearing in particular domain or those describing coordinated events, they have enabled us to collect 1see http://nlp.cs.nyu.edu/wtep/ 538paraphrases accurately.
</nextsent>
<nextsent>the notion of inferential selectional preference (isp) has been introduced by pantel et al (2007).<papid> N07-1071 </papid></nextsent>
<nextsent>isp can capture more general phenomena than above two; however, it lacks abilities to distinguish antonym relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1985">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>semantic equivalence between given pair of expressions has so far been estimated under the distributional hypothesis (harris, 1968).
</prevsent>
<prevsent>geffet and dagan(2005) have extended it to the distributional inclusion hypothesis for recognizing the direction of lexical entailment.
</prevsent>
</prevsection>
<citsent citstr=" W05-1202 ">
weeds et al (2005), <papid> W05-1202 </papid>on the other hand, have pointed out the limitations of lexical similarity and syntactic transformation, and have proposed to directly compute the distributional similarity of pair of sub-parses based on the distributions of their modifiers and parents.</citsent>
<aftsection>
<nextsent>we think it is worth examining if the web can be used as the source for extracting features of phrases.
</nextsent>
<nextsent>predicate phrases using web snippets we define the concept of paraphrasability as follows: grammatical phrase is paraphrasablewith another phrase t, iff satisfies the following three: ? is grammatical ? holds if holds ? is substitutable for in some context most previous studies on acquiring paraphrase rules have evaluated resultant pairs from only the second viewpoint, i.e., semantic equivalence.
</nextsent>
<nextsent>additionally,we assume that one of pair (t) of syntactic variants is automatically generated from the other (s).
</nextsent>
<nextsent>thus, grammaticality of should also be assessed.we also take into account the syntactic substitutability, because head-words of syntactic variants sometimes have different syntactic categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1988">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> computing paraphrasability between.  </section>
<citcontext>
<prevsection>
<prevsent>w(x, f) stands for the weight (frequency in our experiment) of in fx.
</prevsent>
<prevsent>while parlin is symmetric, it has been argued that it is important to determine the direction of paraphrase.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
as an asymmetric measure, we examine ? skew divergence defined by the following equation (lee, 1999): <papid> P99-1004 </papid>dskew(t, s) = (pspt + (1 ? ?)ps) ,where px denotes probability distribution estimated6 from feature set fx.</citsent>
<aftsection>
<nextsent>how well pt approximates ps is calculated based on the kl divergence,d. the parameter ? is set to 0.99, following tradition, because the optimization of ? is difficult.
</nextsent>
<nextsent>totake consistent measurements, we define the para phrasability score par skew as follows: par skew (st) = exp (dskew(t, s)) .6we estimate them simply using maximum likelihood estimation, i.e., x (f) = w(x, f)/ f ? fx w(x, ?).
</nextsent>
<nextsent>540 table 1: # of sampled source phrases and automatically generated syntactic variants.
</nextsent>
<nextsent>phrase type # of tokens # of types th types cov.(%) output ave. : : 20,200,041 4,323,756 1,000 1,014 10.7 1,536 (489) 3.1 1 : 2 : : 3,796,351 2,013,682 107 1,005 6.3 88,040 (966) 91.1 : : 1 : 2 325,964 213,923 15 1,022 12.9 75,344 (982) 76.7 : : adv : 1,209,265 923,475 21 1,097 3.9 8,281 (523) 15.7 adj : : : 378,617 233,952 20 1,049 14.1 128 (50) 2.6 : : adj 788,038 203,845 86 1,003 31.4 3,212 (992) 3.2 total 26,698,276 7,912,633 6,190 176,541 (4), 541 (002) 44.1 table 2: # of syntactic variants whose paraphrasability scores are computed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1990">
<title id=" I08-1070.xml">computing paraphrasability of syntactic variants using web snippets </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>ev.ling: this compares paraphrasability of each phrase type using the 20-best pairs for each of 6 phrase type and 14 web-based models.
</prevsent>
<prevsent>4.3 criteria of paraphrasability.
</prevsent>
</prevsection>
<citsent citstr=" P07-1058 ">
to assess by human the paraphrasability discussed in section 3, we designed the following four questions based on (szpektor et al, 2007): <papid> P07-1058 </papid>qsc: is a correct phrase in japanese?</citsent>
<aftsection>
<nextsent>qtc: is a correct phrase in japanese?
</nextsent>
<nextsent>qs2t: does hold if holds and can substituted for in some context?
</nextsent>
<nextsent>qt2s: does hold if holds and can substituted for in some context?
</nextsent>
<nextsent>541
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1992">
<title id=" I08-1057.xml">a semantic feature for relation recognition using a web based corpus </title>
<section> introduction.  </section>
<citcontext>
<prevsection>

<prevsent>relation recognition is challenging task because finding appropriate features to represent the relationship between two entities is difficult and limited by the scarcity of annotated corpora.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
prior works on relation recognition have focused on syntactic features, e.g., parsing trees (culotta and sorensen,2004; <papid> P04-1054 </papid>zelenko et al , 2003), and on lexical features, e.g., part-of-speech (pos) features.</citsent>
<aftsection>
<nextsent>these approaches show that syntactic features and lexical features outperform bag-of-words (bow) on existing annotated corpora such as the rdc corpus of the ace project.
</nextsent>
<nextsent>the superior performance achieved by syntactic and lexical features is due to their ability to capture the grammatical relations between two entities and the characteristics of the entities.
</nextsent>
<nextsent>for example, (culotta and sorensen, 2004) <papid> P04-1054 </papid>add hyper nyms of entities to features derived from wordnet.</nextsent>
<nextsent>however, neither syntactic nor lexical features can capture the interaction between two entities at the semantic level.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L1997">
<title id=" I08-1057.xml">a semantic feature for relation recognition using a web based corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(zelenko et al , 2003) showed how to extract relations by computing the kernel functions between the kernels of shallow parse trees.
</prevsent>
<prevsent>the kernels are defined over shallow parse representation of the textand used in conjunction with support vector machine (svm) learning algorithm to extract person affiliation and organization-location relations.
</prevsent>
</prevsection>
<citsent citstr=" P04-3022 ">
(culotta and sorensen, 2004) <papid> P04-1054 </papid>extended this work to estimate kernel functions between augmented dependency trees, while (kambhatla, 2004)<papid> P04-3022 </papid>combined lexical features, syntactic features, and semantic features in maximum entropy model.</citsent>
<aftsection>
<nextsent>however, the semantic features discussed in (kambhatla, 2004)<papid> P04-3022 </papid>still focus on the word level instead of the conceptual level.lda is an aspect model that represents documents as set of topics instead of bag-of-words.</nextsent>
<nextsent>latent semantic indexing (lsi) (deerwester et al , 1990) and probabilistic latent semantic indexing (plsi) (hofmann, 1999) are also aspect models and have been widely used in the field of information retrieval.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="L2001">
<title id=" I08-1018.xml">generic text summarization using probabilistic latent semantic indexing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the query-oriented summary presents text that contains information relevant to the given query, and the generic summarization method presents the summary that gives overall sense of the document (goldstein et al 1998).
</prevsent>
<prevsent>in this paper, we will focus on extract-based generic single-document summarization.
</prevsent>
</prevsection>
<citsent citstr=" W04-3247 ">
in the recent years graph based techinques have become very popular in automatic text summa riza 133 tion (erkan and radev, 2004), (<papid> W04-3247 </papid>mihalcea, 2005).</citsent>
<aftsection>
<nextsent>these techniques view each sentence as node of graph and the similarities between each sentences as the links between those sentences.
</nextsent>
<nextsent>generally the links are retained only if the similarity values between the sentences exceed pre-determined threshold value; the links are discarded otherwise.
</nextsent>
<nextsent>the sentences are then ranked using some graph ranking algorithms such as hits (kleinberg, 1998) or page rank (brin and page, 1998) etc. however the graph ranking algorithms tend to give the highest ranking to the sentences related to one central topic in the document.
</nextsent>
<nextsent>so if document contains several topics, these algorithms will only choose one central topic and rank the sentences related to those topic higher than any other topics, ignoring the importance of other topics present.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>