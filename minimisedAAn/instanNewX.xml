<paper>
<cited id="X0">
<title id=" S12-1096.xml">stanford probabilistic edit distance metrics for sts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a fsm defines language by accepting astring of input tokens in the language, and rejecting those that are not.
</prevsent>
<prevsent>a probabilistic fsm defines the probability that string is in language, extending on the concept of fsm.
</prevsent>
</prevsection>
<citsent citstr=" P02-1001 ">
commonly used models such as hmms, n-gram models, markov chains and probabilistic finite state transducers all fall in the broad family of pfsms (knight and al-onaizan,1998; eisner, 2002; <papid> P02-1001 </papid>kumar and byrne, 2003; <papid> N03-1019 </papid>vidal et al, 2005).</citsent>
<aftsection>
<nextsent>unlike all the other applications of fsms where tokens in the language are words, in our language tokens are edit operations.
</nextsent>
<nextsent>a string of tokens that our fsm accepts is an edit sequence that transforms one side of the sentence pair (denoted as s1) into the other side (s2).
</nextsent>
<nextsent>our pfsm has unique start and stop state, andone state per edit operation (i.e., insert, delete, sub stitution).
</nextsent>
<nextsent>the probability of an edit sequence is generated by the model is the product of the state transition probabilities in the pfsm, formally de 648 figure 1: this diagram illustrates an example sentence pair from the statistical machine translation subtask of sts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1">
<title id=" S12-1096.xml">stanford probabilistic edit distance metrics for sts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a fsm defines language by accepting astring of input tokens in the language, and rejecting those that are not.
</prevsent>
<prevsent>a probabilistic fsm defines the probability that string is in language, extending on the concept of fsm.
</prevsent>
</prevsection>
<citsent citstr=" N03-1019 ">
commonly used models such as hmms, n-gram models, markov chains and probabilistic finite state transducers all fall in the broad family of pfsms (knight and al-onaizan,1998; eisner, 2002; <papid> P02-1001 </papid>kumar and byrne, 2003; <papid> N03-1019 </papid>vidal et al, 2005).</citsent>
<aftsection>
<nextsent>unlike all the other applications of fsms where tokens in the language are words, in our language tokens are edit operations.
</nextsent>
<nextsent>a string of tokens that our fsm accepts is an edit sequence that transforms one side of the sentence pair (denoted as s1) into the other side (s2).
</nextsent>
<nextsent>our pfsm has unique start and stop state, andone state per edit operation (i.e., insert, delete, sub stitution).
</nextsent>
<nextsent>the probability of an edit sequence is generated by the model is the product of the state transition probabilities in the pfsm, formally de 648 figure 1: this diagram illustrates an example sentence pair from the statistical machine translation subtask of sts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2">
<title id=" S12-1096.xml">stanford probabilistic edit distance metrics for sts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>among the three official runs submitted to the shared task (ppdaall, pfsmindi and en tailment), pfsmindi performs the best, placed at 38th place among 89 runs.
</prevsent>
<prevsent>since our metrics were originally designed for statistical machine translation (mt) evaluation, we found that on the unseensmtnews test set, which consists of news conversation sentence pairs from the mt domain, our ppda model placed at much higher position (13 among 89 runs).
</prevsent>
</prevsection>
<citsent citstr=" W12-3107 ">
in comparison to results on mt evaluation task (wang and manning, 2012), <papid> W12-3107 </papid>we found that the ppda and pfsm models work less well on sts.</citsent>
<aftsection>
<nextsent>whereas in mt evaluation it is common to have access to thousands of training examples, there is an order of magnitude less available training datain sts.
</nextsent>
<nextsent>therefore, learning hundreds of feature parameters in our models from such few examples are likely to be ill-posed.
</nextsent>
<nextsent>overall, the rte system did not perform as well as the regression based models except for msrvid domain , which has the shortest overall sentence length.
</nextsent>
<nextsent>our qualitative evaluation suggests that msrvid domain seems to exhibit the least degree of lexical divergence between the sentence pairs, thus making this task easier than other domains (the median score of all 89 official systems for msrvidis 0.7538, while the median for msrpar and sm teuroparl is 0.5128 and 0.4437, respectively).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X3">
<title id=" S12-1089.xml">saarland vector based models of semantic textual similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, recent research has shown that, at least to some extent, these models can be generalized to capture similarity beyond the (isolated) word level, either as lexical meaning modulated by context, or as vector ial meaning representations for phrases and sentences.
</prevsent>
<prevsent>in this paper we evaluate the use of some of these models for the semantic textual similarity (sts) task, which measures the degree of semantic equivalence between two sentences.
</prevsent>
</prevsection>
<citsent citstr=" P08-1028 ">
in recent work mitchell and lapata (2008) <papid> P08-1028 </papid>has drawn the attention to the question of building vector ial meaning representations for sentences by combining individual word vectors.</citsent>
<aftsection>
<nextsent>they propose family of simple compositional?
</nextsent>
<nextsent>models that compute vector for phrase or sentence by combining vectors of the constituent words, using different operations such as vector addition or component-wise multiplication.
</nextsent>
<nextsent>more refined models have been proposed recently by baroni and zamparelli (2010) <papid> D10-1115 </papid>and grefenstette and sadrzadeh (2011).<papid> D11-1129 </papid>thater et al (2011) and others take slightly different perspective on the problem: instead of computing vector representation for complete phrase or sentence, they focus on the problem of disam biguating?</nextsent>
<nextsent>the vector representation of target word based on distributional information about the words in the targets context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X5">
<title id=" S12-1089.xml">saarland vector based models of semantic textual similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they propose family of simple compositional?
</prevsent>
<prevsent>models that compute vector for phrase or sentence by combining vectors of the constituent words, using different operations such as vector addition or component-wise multiplication.
</prevsent>
</prevsection>
<citsent citstr=" D10-1115 ">
more refined models have been proposed recently by baroni and zamparelli (2010) <papid> D10-1115 </papid>and grefenstette and sadrzadeh (2011).<papid> D11-1129 </papid>thater et al (2011) and others take slightly different perspective on the problem: instead of computing vector representation for complete phrase or sentence, they focus on the problem of disam biguating?</citsent>
<aftsection>
<nextsent>the vector representation of target word based on distributional information about the words in the targets context.
</nextsent>
<nextsent>while this approach is not compositional?
</nextsent>
<nextsent>in the sense described above, it still captures some meaning of the complete phrase in which target word occurs.
</nextsent>
<nextsent>in this paper, we report on the system we used in the semeval 2012 sentence textual similarity shared task and describe an approach that uses combination of few simple vector-based components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X6">
<title id=" S12-1089.xml">saarland vector based models of semantic textual similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they propose family of simple compositional?
</prevsent>
<prevsent>models that compute vector for phrase or sentence by combining vectors of the constituent words, using different operations such as vector addition or component-wise multiplication.
</prevsent>
</prevsection>
<citsent citstr=" D11-1129 ">
more refined models have been proposed recently by baroni and zamparelli (2010) <papid> D10-1115 </papid>and grefenstette and sadrzadeh (2011).<papid> D11-1129 </papid>thater et al (2011) and others take slightly different perspective on the problem: instead of computing vector representation for complete phrase or sentence, they focus on the problem of disam biguating?</citsent>
<aftsection>
<nextsent>the vector representation of target word based on distributional information about the words in the targets context.
</nextsent>
<nextsent>while this approach is not compositional?
</nextsent>
<nextsent>in the sense described above, it still captures some meaning of the complete phrase in which target word occurs.
</nextsent>
<nextsent>in this paper, we report on the system we used in the semeval 2012 sentence textual similarity shared task and describe an approach that uses combination of few simple vector-based components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X9">
<title id=" S12-1089.xml">saarland vector based models of semantic textual similarity </title>
<section> systems for sentence similarity.  </section>
<citcontext>
<prevsection>
<prevsent>this simplified version performs better on the training dataset.
</prevsent>
<prevsent>furthermore, the simplified model has been shown to be equivalent to the models of erk and pado?
</prevsent>
</prevsection>
<citsent citstr=" P10-1097 ">
(2008) and thater et al (2010) <papid> P10-1097 </papid>by dinu and thater (2012), so the results reported below carry over directly to these other models as well.</citsent>
<aftsection>
<nextsent>2.2 vector composition and alignment.
</nextsent>
<nextsent>the two vector space models sketched above represent the meaning of words, and thus cannot be applied 604 directly to model similarity of phrases or sentences.
</nextsent>
<nextsent>one obvious and straightforward way to extend these models to the sentence level is to follow mitchell and lapata (2008) <papid> P08-1028 </papid>and represent sentences by vectors obtained by summing over the individual vectors ofthe constituent words.</nextsent>
<nextsent>these compositional?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X13">
<title id=" S12-1086.xml">weiwei a simple unsupervised latent semantics based approach for sentence similarity </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" S12-1051 ">
the semantic textual similarity (sts) shared task (agirre et al , 2012) <papid> S12-1051 </papid>computes the degree of semantic equivalence between two sen tences.1 we show that simple unsupervised latent semantics based approach, weighted textual matrix factor ization that only exploits bag-of-words features, can outperform most systems for this task.</citsent>
<aftsection>
<nextsent>the key to the approach is to carefully handle missing words that arenot in the sentence, and thus rendering it superior to latent semantic analysis (lsa) and latent dirichlet allocation (lda).
</nextsent>
<nextsent>our system ranks 20 out of 89 systems according tothe official evaluation metric for the task, pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers.
</nextsent>
<nextsent>identifying the degree of semantic similarity [ss]between two sentences is helpful for many nlp topics.
</nextsent>
<nextsent>in machine translation (kauchak and barzilay, 2006) <papid> N06-1058 </papid>and text summarization (zhou et al , 2006), <papid> N06-1057 </papid>results are automatically evaluated based on sentence comparison.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X14">
<title id=" S12-1086.xml">weiwei a simple unsupervised latent semantics based approach for sentence similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our system ranks 20 out of 89 systems according tothe official evaluation metric for the task, pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers.
</prevsent>
<prevsent>identifying the degree of semantic similarity [ss]between two sentences is helpful for many nlp topics.
</prevsent>
</prevsection>
<citsent citstr=" N06-1058 ">
in machine translation (kauchak and barzilay, 2006) <papid> N06-1058 </papid>and text summarization (zhou et al , 2006), <papid> N06-1057 </papid>results are automatically evaluated based on sentence comparison.</citsent>
<aftsection>
<nextsent>in text coherence detection(lapata and barzilay, 2005), sentences are linked together by similar or related words.
</nextsent>
<nextsent>for word sense disambiguation, researchers (banerjee and pedersen, 2003; guo and diab, 2012<papid> P12-2028 </papid>a) construct sense similarity measure from the sentence similarity of the sense definitions.</nextsent>
<nextsent>almost all ss approaches decompose the task into word pairwise similarity problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X15">
<title id=" S12-1086.xml">weiwei a simple unsupervised latent semantics based approach for sentence similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our system ranks 20 out of 89 systems according tothe official evaluation metric for the task, pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers.
</prevsent>
<prevsent>identifying the degree of semantic similarity [ss]between two sentences is helpful for many nlp topics.
</prevsent>
</prevsection>
<citsent citstr=" N06-1057 ">
in machine translation (kauchak and barzilay, 2006) <papid> N06-1058 </papid>and text summarization (zhou et al , 2006), <papid> N06-1057 </papid>results are automatically evaluated based on sentence comparison.</citsent>
<aftsection>
<nextsent>in text coherence detection(lapata and barzilay, 2005), sentences are linked together by similar or related words.
</nextsent>
<nextsent>for word sense disambiguation, researchers (banerjee and pedersen, 2003; guo and diab, 2012<papid> P12-2028 </papid>a) construct sense similarity measure from the sentence similarity of the sense definitions.</nextsent>
<nextsent>almost all ss approaches decompose the task into word pairwise similarity problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X16">
<title id=" S12-1086.xml">weiwei a simple unsupervised latent semantics based approach for sentence similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in machine translation (kauchak and barzilay, 2006) <papid> N06-1058 </papid>and text summarization (zhou et al , 2006), <papid> N06-1057 </papid>results are automatically evaluated based on sentence comparison.</prevsent>
<prevsent>in text coherence detection(lapata and barzilay, 2005), sentences are linked together by similar or related words.</prevsent>
</prevsection>
<citsent citstr=" P12-2028 ">
for word sense disambiguation, researchers (banerjee and pedersen, 2003; guo and diab, 2012<papid> P12-2028 </papid>a) construct sense similarity measure from the sentence similarity of the sense definitions.</citsent>
<aftsection>
<nextsent>almost all ss approaches decompose the task into word pairwise similarity problems.
</nextsent>
<nextsent>for example, is1mona diab, co-author of this paper, is one of the task orga nizerslam and inkpen (2008) create matrix for each sentence pair, where columns are the words in the first sentence and rows are the words in the second sentence, and each cell stores the distributional similarity of the two words.
</nextsent>
<nextsent>then they create an alignment between words in two sentences, and sentence similarity is calculated based on the sum of the similarity of aligned word pairs.
</nextsent>
<nextsent>there are two disadvantages with word similarity based approaches: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X32">
<title id=" S12-1086.xml">weiwei a simple unsupervised latent semantics based approach for sentence similarity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>also we do not extract additional features such as multi words expression or syntax from sentences ? all we use is bag-of-words feature.
</prevsent>
<prevsent>almost all current ss methods work in the high dimensional word space, and rely heavily on word/sense similarity measures.
</prevsent>
</prevsection>
<citsent citstr=" C10-2048 ">
the word/sense similarity measure is either knowledge based (li etal., 2006; feng et al , 2008; ho et al , 2010; <papid> C10-2048 </papid>tsatsaronis et al , 2010), corpus-based (islam and inkpen, 2008) or hybrid (mihalcea et al , 2006).</citsent>
<aftsection>
<nextsent>almost all of them are evaluated on dataset introduced in (li et al , 2006).
</nextsent>
<nextsent>the li06 dataset consists of 65 pairs of noun definitions selected from the collin cobuild dictionary.
</nextsent>
<nextsent>a subset of 30 pairs is further selected by li06 to render the similarity scores evenly distributed.
</nextsent>
<nextsent>our approach has outperformed most of the previous methods on li06 achieving the second best pearsons correlation and the best spearman correlation (guo and diab, 2012<papid> P12-2028 </papid>b).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X41">
<title id=" S12-1086.xml">weiwei a simple unsupervised latent semantics based approach for sentence similarity </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 setting.
</prevsent>
<prevsent>sts data: the sentence pair data in the ststask is collected from five sources: 1.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
msr paraphrase corpus (dolan et al , 2004), <papid> C04-1051 </papid>2.</citsent>
<aftsection>
<nextsent>msr video data (chen and dolan, 2011), <papid> P11-1020 </papid>3.</nextsent>
<nextsent>smt europarl data, 2http://en.wiktionary.org/wiki/wiktionary:main page 3http://nlp.stanford.edu/software/tagger.shtml 4http://wn-similarity.sourceforge.net, wordnet::querydata 588 models msrpar msrvid smt-eur on-wn smt-news lda 0.274 0.7682 0.452 0.619 0.366 wtmf 0.411(67/89) 0.835(11/89) 0.513(10/89) 0.727(1/89) 0.438(28/89) table 2: performance of lda and wtmf on each individual test set of task 6 sts data all allnrm mean 0.695(20/89) 0.830(10/89) 0.608(19/89) table 3: performance of wtmf on all test sets 4.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X42">
<title id=" S12-1086.xml">weiwei a simple unsupervised latent semantics based approach for sentence similarity </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>sts data: the sentence pair data in the ststask is collected from five sources: 1.
</prevsent>
<prevsent>msr paraphrase corpus (dolan et al , 2004), <papid> C04-1051 </papid>2.</prevsent>
</prevsection>
<citsent citstr=" P11-1020 ">
msr video data (chen and dolan, 2011), <papid> P11-1020 </papid>3.</citsent>
<aftsection>
<nextsent>smt europarl data, 2http://en.wiktionary.org/wiki/wiktionary:main page 3http://nlp.stanford.edu/software/tagger.shtml 4http://wn-similarity.sourceforge.net, wordnet::querydata 588 models msrpar msrvid smt-eur on-wn smt-news lda 0.274 0.7682 0.452 0.619 0.366 wtmf 0.411(67/89) 0.835(11/89) 0.513(10/89) 0.727(1/89) 0.438(28/89) table 2: performance of lda and wtmf on each individual test set of task 6 sts data all allnrm mean 0.695(20/89) 0.830(10/89) 0.608(19/89) table 3: performance of wtmf on all test sets 4.
</nextsent>
<nextsent>ontonotes-wordnet data (hovy et al , 2006), <papid> N06-2015 </papid>5..</nextsent>
<nextsent>smt news data.evaluation metrics: since the systems are required to assigned similarity score to each sentence pair, pearsons correlation is used to measure the performance of systems on each of the 5 data sets.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X43">
<title id=" S12-1086.xml">weiwei a simple unsupervised latent semantics based approach for sentence similarity </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>msr video data (chen and dolan, 2011), <papid> P11-1020 </papid>3.</prevsent>
<prevsent>smt europarl data, 2http://en.wiktionary.org/wiki/wiktionary:main page 3http://nlp.stanford.edu/software/tagger.shtml 4http://wn-similarity.sourceforge.net, wordnet::querydata 588 models msrpar msrvid smt-eur on-wn smt-news lda 0.274 0.7682 0.452 0.619 0.366 wtmf 0.411(67/89) 0.835(11/89) 0.513(10/89) 0.727(1/89) 0.438(28/89) table 2: performance of lda and wtmf on each individual test set of task 6 sts data all allnrm mean 0.695(20/89) 0.830(10/89) 0.608(19/89) table 3: performance of wtmf on all test sets 4.</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
ontonotes-wordnet data (hovy et al , 2006), <papid> N06-2015 </papid>5..</citsent>
<aftsection>
<nextsent>smt news data.evaluation metrics: since the systems are required to assigned similarity score to each sentence pair, pearsons correlation is used to measure the performance of systems on each of the 5 datasets.
</nextsent>
<nextsent>however, measuring the overall performance on the concatenation of 5 datasets is rarely discussed in previous work.
</nextsent>
<nextsent>accordingly the organizers of ststask provide three evaluation metrics: 1.
</nextsent>
<nextsent>all: pearson correlation with the gold standard for the combined 5 datasets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X52">
<title id=" S12-1063.xml">utdhlt cop acetic system for choosing plausible alternatives </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>given question, such as question 15 (as shown in figure 1), our system selects the most plausible alternative by using the output of an svm classifier, trained on the 500 provided development questions and tested on the 500 provided test questions.
</prevsent>
<prevsent>the classifier operates with features describing information extracted from the processing of the questions premise and alternatives.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
as illustrated by figure 2,the preprocessing involves part of speech (pos) tagging, and syntactic dependency parsing provided by the stanford parser (klein and manning, 2003;<papid> P03-1054 </papid>toutanova et al, 2003), <papid> N03-1033 </papid>multi-word expression detection using wikipedia, automatic timeml annotation using tarsqi (verhagen et al, 2005; <papid> P05-3021 </papid>pustejovsky et al, 2003), and brown clustering as provided in (turian, 2010).the architecture of the cop acetic system is divided into offline (independent of any question) and online (question dependent) processing.</citsent>
<aftsection>
<nextsent>the online aspect of our system inspects each question using an svm and selects the most likely alternative.
</nextsent>
<nextsent>our systems offline functions focus on pre-processing resources so that they may be used by components of the online aspect of our system.
</nextsent>
<nextsent>in the next section, we describe the offline processing upon which our system is built, and in the following section, the online manner in which we evaluate each question.
</nextsent>
<nextsent>2.1 offline processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X53">
<title id=" S12-1063.xml">utdhlt cop acetic system for choosing plausible alternatives </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>given question, such as question 15 (as shown in figure 1), our system selects the most plausible alternative by using the output of an svm classifier, trained on the 500 provided development questions and tested on the 500 provided test questions.
</prevsent>
<prevsent>the classifier operates with features describing information extracted from the processing of the questions premise and alternatives.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
as illustrated by figure 2,the preprocessing involves part of speech (pos) tagging, and syntactic dependency parsing provided by the stanford parser (klein and manning, 2003;<papid> P03-1054 </papid>toutanova et al, 2003), <papid> N03-1033 </papid>multi-word expression detection using wikipedia, automatic timeml annotation using tarsqi (verhagen et al, 2005; <papid> P05-3021 </papid>pustejovsky et al, 2003), and brown clustering as provided in (turian, 2010).the architecture of the cop acetic system is divided into offline (independent of any question) and online (question dependent) processing.</citsent>
<aftsection>
<nextsent>the online aspect of our system inspects each question using an svm and selects the most likely alternative.
</nextsent>
<nextsent>our systems offline functions focus on pre-processing resources so that they may be used by components of the online aspect of our system.
</nextsent>
<nextsent>in the next section, we describe the offline processing upon which our system is built, and in the following section, the online manner in which we evaluate each question.
</nextsent>
<nextsent>2.1 offline processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X54">
<title id=" S12-1063.xml">utdhlt cop acetic system for choosing plausible alternatives </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>given question, such as question 15 (as shown in figure 1), our system selects the most plausible alternative by using the output of an svm classifier, trained on the 500 provided development questions and tested on the 500 provided test questions.
</prevsent>
<prevsent>the classifier operates with features describing information extracted from the processing of the questions premise and alternatives.
</prevsent>
</prevsection>
<citsent citstr=" P05-3021 ">
as illustrated by figure 2,the preprocessing involves part of speech (pos) tagging, and syntactic dependency parsing provided by the stanford parser (klein and manning, 2003;<papid> P03-1054 </papid>toutanova et al, 2003), <papid> N03-1033 </papid>multi-word expression detection using wikipedia, automatic timeml annotation using tarsqi (verhagen et al, 2005; <papid> P05-3021 </papid>pustejovsky et al, 2003), and brown clustering as provided in (turian, 2010).the architecture of the cop acetic system is divided into offline (independent of any question) and online (question dependent) processing.</citsent>
<aftsection>
<nextsent>the online aspect of our system inspects each question using an svm and selects the most likely alternative.
</nextsent>
<nextsent>our systems offline functions focus on pre-processing resources so that they may be used by components of the online aspect of our system.
</nextsent>
<nextsent>in the next section, we describe the offline processing upon which our system is built, and in the following section, the online manner in which we evaluate each question.
</nextsent>
<nextsent>2.1 offline processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X56">
<title id=" S12-1063.xml">utdhlt cop acetic system for choosing plausible alternatives </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>then, we represented our dependency structures using in the sparql4query language and extracted cause and effect pairs by issuing sparql queries against therdf3x database.
</prevsent>
<prevsent>we used sparql and rdf representations because they allowed us to easily represent and reason over graphical structures, such as those of our dependency trees.
</prevsent>
</prevsection>
<citsent citstr=" P08-2045 ">
it has been shown that causality often manifests asa temporal relation (bethard, 2008; bethard and martin, 2008).<papid> P08-2045 </papid></citsent>
<aftsection>
<nextsent>the questions presented in this task are no exception: many of the alternative-premise pairs necessitate temporal understanding.
</nextsent>
<nextsent>for example, consider question 63 provided in figure 4.
</nextsent>
<nextsent>question 63 (find the effect) premise: the man removed his coat.
</nextsent>
<nextsent>alternative 1: he entered the house.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X61">
<title id=" S12-1063.xml">utdhlt cop acetic system for choosing plausible alternatives </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>our first system uses only the bigram pmi feature and is denoted as bigram pmi.
</prevsent>
<prevsent>our second system uses all four features and is denoted as svm combined.
</prevsent>
</prevsection>
<citsent citstr=" S12-1052 ">
the accuracy of our two systems on the 500 provided test questions is provided in table 1 (gordon et al,2012).<papid> S12-1052 </papid></citsent>
<aftsection>
<nextsent>on this task, accuracy is defined as the quotient of dividing the number of questions for which the correct alternative was chosen by the number of questions.
</nextsent>
<nextsent>although multiple groups registered, ours were the only submitted results.
</nextsent>
<nextsent>note that the difference in performance between our two systems is not statistically significant (p = 0.411) (gordon et al, 2012).<papid> S12-1052 </papid></nextsent>
<nextsent>team id system id score utdhlt bigram pmi 0.618 utdhlt svm combined 0.634 table 1: accuracy of submitted systems the primary hindrance to our approach is in combining each feature ? that is, determining the confidence of each features judgement.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X63">
<title id=" P98-2178.xml">jabot a multilingual java based intelligent agent for web sites </title>
<section> jabot  - the  design.  </section>
<citcontext>
<prevsection>
<prevsent>now that the requirements have been presented, the resulting design is described.
</prevsent>
<prevsent>jabot is domain- specific in the sense that it can only operate on the web site for which it was configured.
</prevsent>
</prevsection>
<citsent citstr=" C90-2006 ">
this is useful from practical functional perspective because it limits both the conceptual nd linguistic diversity which needs to be processed (so far this approach has produced the best results in computational inguistic applications \[boitet, 1990\]).<papid> C90-2006 </papid></citsent>
<aftsection>
<nextsent>in other words, users of jabot will be formulating questions which attempt locate information that is likely to be contained on the web site, and not the full range of questions that they might like to ask human expert on the subject.
</nextsent>
<nextsent>for example, if jabot were placed on the web site of university department, users would be enquiring about subject contents, tutorial hours, exam dates, etc., and not attempting to ask which of subjects and is easier or more relevant for their careers.
</nextsent>
<nextsent>as can be seen in the diagram below, jabot has three modules: natural language interface, search engine and an interactive list of references to the web pages on the site at which it is operating.
</nextsent>
<nextsent>at start up time, two data files are loaded, namely, file of linguistic units with little or no semantic relevance in the context of web site information retrieval, and lexical semantic map of the particular web site.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X64">
<title id=" P99-1026.xml">understanding unsegmented user utterances in real time spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>considering the current level of speech recognition technology, system-initiative dialogue systems, which prohibit users from speaking unrestrictedly, are preferred (walker et al, 1998).
</prevsent>
<prevsent>nevertheless, we are still pursuing techniques for understanding unrestricted user utterances because, if the accuracy of under-standing can be improved, systems that allow users to speak freely could be developed and these would be more useful than systems that do not.
</prevsent>
</prevsection>
<citsent citstr=" P96-1009 ">
* current address: i laboratories, 1-1 hikarino-oka, yoko-suka 239-0847, japan most previous poken dialogue systems (e.g. sys-tems by allen et al (1996), <papid> P96-1009 </papid>zue et al (1994) and peckham (1993)) assume that the user makes one utterance unit in each speech push-to-talk method is used.</citsent>
<aftsection>
<nextsent>unit we mean phrase from representation is derived, and sentence in written language.
</nextsent>
<nextsent>act in this paper to mean interval, unless the here, by utterance which speech act it corresponds to we also use speech command that up-dates the hearer belief state about the speaker intention and the context of the dialogue.
</nextsent>
<nextsent>in this paper, system using this assumption is called an interval-based system.
</nextsent>
<nextsent>the above assumption longer holds when no restrictions are placed on the way the user speaks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X65">
<title id=" P99-1026.xml">understanding unsegmented user utterances in real time spoken dialogue systems </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>we therefore need way to reconcile real-time understanding and analysis without boundary clues.
</prevsent>
<prevsent>several techniques have been proposed to segment user utterances prior to parsing.
</prevsent>
</prevsection>
<citsent citstr=" P97-1033 ">
they use into- nation (wang and hirschberg, 1992; traum and heeman, 1997; heeman and allen, 1997) <papid> P97-1033 </papid>and prob-abilistic language models (stolcke et al, 1998; ramaswamy and kleindienst, 1998; cettolo and fala vigna, 1998).</citsent>
<aftsection>
<nextsent>since these methods are not perfect, the resulting segments do not always cor-respond to utterances and might not be parsable because of speech recognition errors.
</nextsent>
<nextsent>in addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in straightfor-ward way.
</nextsent>
<nextsent>some methods use keyword detection (rose, 1995; hata zaki et al, 1994; seto et al, 1994) and key-phrase detection (aust et al, 1995; kawahara et al, 1996) to understand speech mainly because the speech recognition score is not high enough.
</nextsent>
<nextsent>the lack of the full use of syntax in these ap-proaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X66">
<title id=" P99-1026.xml">understanding unsegmented user utterances in real time spoken dialogue systems </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>some methods use keyword detection (rose, 1995; hata zaki et al, 1994; seto et al, 1994) and key-phrase detection (aust et al, 1995; kawahara et al, 1996) to understand speech mainly because the speech recognition score is not high enough.
</prevsent>
<prevsent>the lack of the full use of syntax in these ap-proaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer.
</prevsent>
</prevsection>
<citsent citstr=" P98-2237 ">
zechner and waibel (1998) <papid> P98-2237 </papid>and worm (1998) <papid> P98-2229 </papid>proposed understanding utterances by combining partial parses.</citsent>
<aftsection>
<nextsent>their methods, however, cannot syntactically analyze phrases across pauses since they use speech intervals as input units.
</nextsent>
<nextsent>al-though lavie et al (1997) proposed segmentation method that combines egmentation prior to parsing and segmentation during parsing, but it suffers from the same problem.
</nextsent>
<nextsent>in the parser proposed by core and schubert (1997), utterances interrupted by the other dialogue participant are analyzed based on recta-rules.
</nextsent>
<nextsent>it is unclear, however, how this parser can be incorpo- 201 rated into real-time dialogue system; it seems that it cannot output analysis results without boundary clues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X67">
<title id=" P99-1026.xml">understanding unsegmented user utterances in real time spoken dialogue systems </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>some methods use keyword detection (rose, 1995; hata zaki et al, 1994; seto et al, 1994) and key-phrase detection (aust et al, 1995; kawahara et al, 1996) to understand speech mainly because the speech recognition score is not high enough.
</prevsent>
<prevsent>the lack of the full use of syntax in these ap-proaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer.
</prevsent>
</prevsection>
<citsent citstr=" P98-2229 ">
zechner and waibel (1998) <papid> P98-2237 </papid>and worm (1998) <papid> P98-2229 </papid>proposed understanding utterances by combining partial parses.</citsent>
<aftsection>
<nextsent>their methods, however, cannot syntactically analyze phrases across pauses since they use speech intervals as input units.
</nextsent>
<nextsent>al-though lavie et al (1997) proposed segmentation method that combines egmentation prior to parsing and segmentation during parsing, but it suffers from the same problem.
</nextsent>
<nextsent>in the parser proposed by core and schubert (1997), utterances interrupted by the other dialogue participant are analyzed based on recta-rules.
</nextsent>
<nextsent>it is unclear, however, how this parser can be incorpo- 201 rated into real-time dialogue system; it seems that it cannot output analysis results without boundary clues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X70">
<title id=" P99-1026.xml">understanding unsegmented user utterances in real time spoken dialogue systems </title>
<section> incremental significant-utterance-.  </section>
<citcontext>
<prevsection>
<prevsent>sus are identified in the process of understanding.
</prevsent>
<prevsent>unlike ordinary parsers, the understanding mod-ule does not try to determine whether the whole input forms an su or not, but instead determines where sus are.
</prevsent>
</prevsection>
<citsent citstr=" A92-1027 ">
although this can be considered kind of partial parsing technique (mcdonald, 1992; <papid> A92-1027 </papid>lavie, 1996; abney, 1996), the sus obtained by isss are not always sub sentential phrases; they are sometimes full sentences.</citsent>
<aftsection>
<nextsent>for one discourse, multiple significant-utterance sequences can be considered.
</nextsent>
<nextsent> wednesday next week  above illustrates this well.
</nextsent>
<nextsent>let us assume that the parser finds two sus,  wednesday  and  wednesday next week .
</nextsent>
<nextsent>then three significant- utterance sequences are possible: one consisting of  wednesday , one consisting of  wednesday next 202 week , and one consisting of no sus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X72">
<title id=" S10-1078.xml">kcdc word sense induction by using grammatical dependencies and sentence phrase structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of the semeval-2 word sense induction and discrimination task, see manandhar et al.
</prevsent>
<prevsent>(2010), is to identify the senses of ambiguous nouns and verbs in an unsupervised manner and to label unseen instances of these words with one of the induced senses.
</prevsent>
</prevsection>
<citsent citstr=" W07-2037 ">
the most common approach towards this task is to apply clustering or graph partitioning algorithms on representation of the words that surround an ambiguous target word, see for example niu et al (2007) <papid> W07-2037 </papid>and pedersen (2007).<papid> W07-2087 </papid>we followed this approach by employing clustering algorithm to detect the individual senses, but focused on generating feature sets different to the mainstream approach.</citsent>
<aftsection>
<nextsent>our feature sets utilize the output of linguistic processing pipeline that captures the syntax and semantics of sentence parts closely related with the target word.
</nextsent>
<nextsent>the base of our system is to apply parser on the sentence in which the target word occurs.
</nextsent>
<nextsent>contextual information, for example the sentences surrounding the target sentence, are currently not exploited by our system.
</nextsent>
<nextsent>to analyze the sentences we applied the stanford parser (version1.6.2), which is based on lexicalized probabilistic context free grammars, see klein and manning (2003).<papid> P03-1054 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X73">
<title id=" S10-1078.xml">kcdc word sense induction by using grammatical dependencies and sentence phrase structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of the semeval-2 word sense induction and discrimination task, see manandhar et al.
</prevsent>
<prevsent>(2010), is to identify the senses of ambiguous nouns and verbs in an unsupervised manner and to label unseen instances of these words with one of the induced senses.
</prevsent>
</prevsection>
<citsent citstr=" W07-2087 ">
the most common approach towards this task is to apply clustering or graph partitioning algorithms on representation of the words that surround an ambiguous target word, see for example niu et al (2007) <papid> W07-2037 </papid>and pedersen (2007).<papid> W07-2087 </papid>we followed this approach by employing clustering algorithm to detect the individual senses, but focused on generating feature sets different to the mainstream approach.</citsent>
<aftsection>
<nextsent>our feature sets utilize the output of linguistic processing pipeline that captures the syntax and semantics of sentence parts closely related with the target word.
</nextsent>
<nextsent>the base of our system is to apply parser on the sentence in which the target word occurs.
</nextsent>
<nextsent>contextual information, for example the sentences surrounding the target sentence, are currently not exploited by our system.
</nextsent>
<nextsent>to analyze the sentences we applied the stanford parser (version1.6.2), which is based on lexicalized probabilistic context free grammars, see klein and manning (2003).<papid> P03-1054 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X74">
<title id=" S10-1078.xml">kcdc word sense induction by using grammatical dependencies and sentence phrase structure </title>
<section> system overview.  </section>
<citcontext>
<prevsection>
<prevsent>the base of our system is to apply parser on the sentence in which the target word occurs.
</prevsent>
<prevsent>contextual information, for example the sentences surrounding the target sentence, are currently not exploited by our system.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
to analyze the sentences we applied the stanford parser (version1.6.2), which is based on lexicalized probabilistic context free grammars, see klein and manning (2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>this open-source parser not only extracts the phrase structure of given sentence, butalso provides list of so called grammatical relations (typed dependencies), see de marneffe et al (2006).
</nextsent>
<nextsent>these relations reflect the dependencies between the words within the sentence, for example the relationship between the verb and the subject.
</nextsent>
<nextsent>see chen et al (2009) <papid> N09-1004 </papid>for an application of grammatical dependencies for word sense disam biguation.</nextsent>
<nextsent>2.1 feature extraction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X75">
<title id=" S10-1078.xml">kcdc word sense induction by using grammatical dependencies and sentence phrase structure </title>
<section> system overview.  </section>
<citcontext>
<prevsection>
<prevsent>this open-source parser not only extracts the phrase structure of given sentence, butalso provides list of so called grammatical relations (typed dependencies), see de marneffe et al (2006).
</prevsent>
<prevsent>these relations reflect the dependencies between the words within the sentence, for example the relationship between the verb and the subject.
</prevsent>
</prevsection>
<citsent citstr=" N09-1004 ">
see chen et al (2009) <papid> N09-1004 </papid>for an application of grammatical dependencies for word sense disam biguation.</citsent>
<aftsection>
<nextsent>2.1 feature extraction.
</nextsent>
<nextsent>the phrase structure and the grammatical dependencies are sources for the feature extraction stage.to illustrate the result of the parser and feature extraction stages we use an example sentence, where the target word is the verb file?: afterward , watched as butt-ton of good , but misguided people filed out of the theater , and immediately lit up smoke . 2.1.1 grammatical dependency features the stanford parser provides 55 different grammatical dependency types.
</nextsent>
<nextsent>figure 2 depicts the list of the grammatical dependencies identified by the stanford parser for the example sentence.
</nextsent>
<nextsent>only limited subset of these dependencies are selected to build the grammatical feature set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X76">
<title id=" S10-1014.xml">semeval2010 task 18 disambiguating sentiment ambiguous adjectives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>has positive orientation in the first case but negative orientation in the second case.
</prevsent>
<prevsent>turney and littman (2003) claimed that sentiment ambiguous words could not be avoided easily in real-world application in the future research.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
but unfortunately, sentiment ambiguous words are discarded by most research concerning sentiment analysis (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>turney and littman, 2003; kim and hovy, 2004).<papid> C04-1200 </papid></citsent>
<aftsection>
<nextsent>the exception work is ding et al (2008).
</nextsent>
<nextsent>they call these words as context dependant opinions and propose holistic lexicon-based approach to solve this problem.
</nextsent>
<nextsent>the language they deal with is english.
</nextsent>
<nextsent>the disambiguation of sentiment ambiguous words can also be considered as problem of phrase-level sentiment analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X77">
<title id=" S10-1014.xml">semeval2010 task 18 disambiguating sentiment ambiguous adjectives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>has positive orientation in the first case but negative orientation in the second case.
</prevsent>
<prevsent>turney and littman (2003) claimed that sentiment ambiguous words could not be avoided easily in real-world application in the future research.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
but unfortunately, sentiment ambiguous words are discarded by most research concerning sentiment analysis (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>turney and littman, 2003; kim and hovy, 2004).<papid> C04-1200 </papid></citsent>
<aftsection>
<nextsent>the exception work is ding et al (2008).
</nextsent>
<nextsent>they call these words as context dependant opinions and propose holistic lexicon-based approach to solve this problem.
</nextsent>
<nextsent>the language they deal with is english.
</nextsent>
<nextsent>the disambiguation of sentiment ambiguous words can also be considered as problem of phrase-level sentiment analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X78">
<title id=" S10-1014.xml">semeval2010 task 18 disambiguating sentiment ambiguous adjectives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the language they deal with is english.
</prevsent>
<prevsent>the disambiguation of sentiment ambiguous words can also be considered as problem of phrase-level sentiment analysis.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
wilson et al (2005) <papid> H05-1044 </papid>present two-step process to recognize contextual polarity that employs machine learning and variety of features.</citsent>
<aftsection>
<nextsent>takamura et al (2006), <papid> E06-1026 </papid>takamura et al (2007) <papid> N07-1037 </papid>propose latent variable model and lexical network to determine so of phrases, focusing on noun+adjective?</nextsent>
<nextsent>pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X79">
<title id=" S10-1014.xml">semeval2010 task 18 disambiguating sentiment ambiguous adjectives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the disambiguation of sentiment ambiguous words can also be considered as problem of phrase-level sentiment analysis.
</prevsent>
<prevsent>wilson et al (2005) <papid> H05-1044 </papid>present two-step process to recognize contextual polarity that employs machine learning and variety of features.</prevsent>
</prevsection>
<citsent citstr=" E06-1026 ">
takamura et al (2006), <papid> E06-1026 </papid>takamura et al (2007) <papid> N07-1037 </papid>propose latent variable model and lexical network to determine so of phrases, focusing on noun+adjective?</citsent>
<aftsection>
<nextsent>pairs.
</nextsent>
<nextsent>their experimental results suggest that the classification of pairs containing ambiguous adjectives is much harder than those with unambiguous adjectives.
</nextsent>
<nextsent>the task 18 at semeval 2010 provides benchmark dataset to encourage studies on this problem.
</nextsent>
<nextsent>this paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X80">
<title id=" S10-1014.xml">semeval2010 task 18 disambiguating sentiment ambiguous adjectives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the disambiguation of sentiment ambiguous words can also be considered as problem of phrase-level sentiment analysis.
</prevsent>
<prevsent>wilson et al (2005) <papid> H05-1044 </papid>present two-step process to recognize contextual polarity that employs machine learning and variety of features.</prevsent>
</prevsection>
<citsent citstr=" N07-1037 ">
takamura et al (2006), <papid> E06-1026 </papid>takamura et al (2007) <papid> N07-1037 </papid>propose latent variable model and lexical network to determine so of phrases, focusing on noun+adjective?</citsent>
<aftsection>
<nextsent>pairs.
</nextsent>
<nextsent>their experimental results suggest that the classification of pairs containing ambiguous adjectives is much harder than those with unambiguous adjectives.
</nextsent>
<nextsent>the task 18 at semeval 2010 provides benchmark dataset to encourage studies on this problem.
</nextsent>
<nextsent>this paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X81">
<title id=" S10-1099.xml">opal applying opinion mining techniques for the disambiguation of sentiment ambiguous adjectives in semeval2 task 18 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, we present the results we obtained in the evaluation and our plans for future work.
</prevsent>
<prevsent>444 3 state of the art.
</prevsent>
</prevsection>
<citsent citstr=" J94-2004 ">
subjectivity analysis is defined by (wiebe, 1994) <papid> J94-2004 </papid>as the linguistic expression of somebodys opinions, sentiments, emotions, evaluations, beliefs and speculations?.</citsent>
<aftsection>
<nextsent>sentiment analysis, on the other hand, is defined as the task of extracting, from text, the opinion expressed on an object (product, person, topic etc.) and classifying it as positive, negative or neutral.
</nextsent>
<nextsent>the task of sentiment analysis, considered step further to subjectivity analysis, is more complex than the latter, because it involves an extra step: the classification of the retrieved opinion words according to their polarity.
</nextsent>
<nextsent>there are series of techniques that were used to obtain lexicons of subjective words ? e.g. the opinion finder lexicon (wilson et al, 2005) <papid> H05-1044 </papid>and opinion words with associated polarity.</nextsent>
<nextsent>(hu and liu, 2004) start with set of seed adjectives (good?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X82">
<title id=" S10-1099.xml">opal applying opinion mining techniques for the disambiguation of sentiment ambiguous adjectives in semeval2 task 18 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sentiment analysis, on the other hand, is defined as the task of extracting, from text, the opinion expressed on an object (product, person, topic etc.) and classifying it as positive, negative or neutral.
</prevsent>
<prevsent>the task of sentiment analysis, considered step further to subjectivity analysis, is more complex than the latter, because it involves an extra step: the classification of the retrieved opinion words according to their polarity.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
there are series of techniques that were used to obtain lexicons of subjective words ? e.g. the opinion finder lexicon (wilson et al, 2005) <papid> H05-1044 </papid>and opinion words with associated polarity.</citsent>
<aftsection>
<nextsent>(hu and liu, 2004) start with set of seed adjectives (good?
</nextsent>
<nextsent>and bad?)
</nextsent>
<nextsent>and apply synonymy and antonymy relations in wordnet.
</nextsent>
<nextsent>a similar approach was used in building wordnet affect (strapparava and valitutti, 2004), starting from larger set of seed affective words, classified according to the six basic categories of emotion (joy, sadness, fear, surprise, anger and disgust) and expanding the lexicon using paths in wordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X83">
<title id=" S10-1099.xml">opal applying opinion mining techniques for the disambiguation of sentiment ambiguous adjectives in semeval2 task 18 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>micrownop (cerini et al, 2007), another lexicon containing opinion words with their associated polarity, was built on the basis of set of terms extracted from the general inquirer lexicon and subsequently adding all the synsets in wordnet where these words appear.
</prevsent>
<prevsent>other methods built sentiment lexicons using the local context of words.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
(pang et al, 2002) <papid> W02-1011 </papid>built lexicon of sentiment words with associated polarity value, starting with set of classified seed adjectives and using conjunctions (and?)</citsent>
<aftsection>
<nextsent>dis junctions (or?, but?)
</nextsent>
<nextsent>to deduce orientation of new words in corpus.
</nextsent>
<nextsent>(turney, 2002) <papid> P02-1053 </papid>classifies words according to their polarity on the basis of the idea that terms with similar orientation tend to co occur in documents.</nextsent>
<nextsent>thus, the author computes the pointwise mutual information score between seed words and new words on the basis of the number of alta vista hits returned when querying the seed word and the word to be classified with the near?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X84">
<title id=" S10-1099.xml">opal applying opinion mining techniques for the disambiguation of sentiment ambiguous adjectives in semeval2 task 18 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dis junctions (or?, but?)
</prevsent>
<prevsent>to deduce orientation of new words in corpus.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
(turney, 2002) <papid> P02-1053 </papid>classifies words according to their polarity on the basis of the idea that terms with similar orientation tend to co occur in documents.</citsent>
<aftsection>
<nextsent>thus, the author computes the pointwise mutual information score between seed words and new words on the basis of the number of alta vista hits returned when querying the seed word and the word to be classified with the near?
</nextsent>
<nextsent>operator.
</nextsent>
<nextsent>in our work in (balahur and montoyo, 2008a), we compute the polarity of new words using polarity anchors?
</nextsent>
<nextsent>(words whose polarity is known beforehand) and normalized google distance (cilibrasi and vitanyi, 2006) scores.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X85">
<title id=" S10-1099.xml">opal applying opinion mining techniques for the disambiguation of sentiment ambiguous adjectives in semeval2 task 18 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our work in (balahur and montoyo, 2008a), we compute the polarity of new words using polarity anchors?
</prevsent>
<prevsent>(words whose polarity is known beforehand) and normalized google distance (cilibrasi and vitanyi, 2006) scores.
</prevsent>
</prevsection>
<citsent citstr=" H05-1043 ">
another approach that uses the polarity of the local context for computing word polarity is (popescu and etzioni, 2005), <papid> H05-1043 </papid>who use weighting function of the words around the context to be classified.</citsent>
<aftsection>
<nextsent>4 the opal system at semeval 2010.
</nextsent>
<nextsent>task 18 in the semeval 2010 task 18, the participants were given set of contexts in chinese, in which 14 dynamic sentiment ambiguous adjectives are selected.
</nextsent>
<nextsent>they are: ?|big, ?|small, ?|many, ? |few, ?|high, ?|low, ?|thick, ?|thin, ?|deep, ?|shallow, ?|heavy, ? |light, ??|huge, ??
</nextsent>
<nextsent>|grave.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X86">
<title id=" S10-1047.xml">fbkirst semantic relation extraction using cyc </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cyc hasa good coverage of common nouns, making it interesting for our task.
</prevsent>
<prevsent>the full version of cyc is freely available to the research community as re searchcyc.
</prevsent>
</prevsection>
<citsent citstr=" W07-2028 ">
4we approached the task using the system introduced by giuliano et al (2007) <papid> W07-2028 </papid>as basis.</citsent>
<aftsection>
<nextsent>they exploited two information sources: the whole sentence where the relation appears, and wordnet synonymy and hyperonymy information.
</nextsent>
<nextsent>in this paper, we (i) investigate usage of cyc as source of semantic knowledge and (ii) linguistic information, which give useful clues to semantic relation extraction.
</nextsent>
<nextsent>from cyc, we obtain information about super-classes (in the cyc terminology generalizations) of the classes which correspond to nominals in sentence.
</nextsent>
<nextsent>the sentence itself provides linguistic information, such as local contexts of entities, bag of verbs and distance between nominals in the context.the different sources of information are represented by kernel functions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X88">
<title id=" P98-2241.xml">a preliminary model of centering in dialog </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J95-2003 ">
the centering framework (grosz et al, 1995) <papid> J95-2003 </papid>is one of the most influential computational linguistics the-ories relating local focus to the form chosen for re-ferring expressions.</citsent>
<aftsection>
<nextsent>a number of studies have de-veloped refinements and extensions of the theory (eg.
</nextsent>
<nextsent>brennan et al, 1987; <papid> P87-1022 </papid>kameyama, 1986; <papid> P86-1031 </papid>strube and hahn, 1996; <papid> P96-1036 </papid>walker et al, 1998), but few have attempted to extend the model to multi-party dis-course (cf.</nextsent>
<nextsent>brennan, 1998; walker, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X91">
<title id=" P98-2241.xml">a preliminary model of centering in dialog </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the centering framework (grosz et al, 1995) <papid> J95-2003 </papid>is one of the most influential computational linguistics the-ories relating local focus to the form chosen for re-ferring expressions.</prevsent>
<prevsent>a number of studies have de-veloped refinements and extensions of the theory (eg.</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
brennan et al, 1987; <papid> P87-1022 </papid>kameyama, 1986; <papid> P86-1031 </papid>strube and hahn, 1996; <papid> P96-1036 </papid>walker et al, 1998), but few have attempted to extend the model to multi-party dis-course (cf.</citsent>
<aftsection>
<nextsent>brennan, 1998; walker, 1998).
</nextsent>
<nextsent>for dialog systems, the benefits of using cen-tering theory include improved reference resolution and generation of more coherent referring expres-sions.
</nextsent>
<nextsent>however, it is not at all clear how to adapt the theory for multi-party discourse.
</nextsent>
<nextsent>this paper ex-amines some of the issues involved in adapting the theory, then describes the results of applying three alternative models to corpus of 2-person dialogs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X92">
<title id=" P98-2241.xml">a preliminary model of centering in dialog </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the centering framework (grosz et al, 1995) <papid> J95-2003 </papid>is one of the most influential computational linguistics the-ories relating local focus to the form chosen for re-ferring expressions.</prevsent>
<prevsent>a number of studies have de-veloped refinements and extensions of the theory (eg.</prevsent>
</prevsection>
<citsent citstr=" P86-1031 ">
brennan et al, 1987; <papid> P87-1022 </papid>kameyama, 1986; <papid> P86-1031 </papid>strube and hahn, 1996; <papid> P96-1036 </papid>walker et al, 1998), but few have attempted to extend the model to multi-party dis-course (cf.</citsent>
<aftsection>
<nextsent>brennan, 1998; walker, 1998).
</nextsent>
<nextsent>for dialog systems, the benefits of using cen-tering theory include improved reference resolution and generation of more coherent referring expres-sions.
</nextsent>
<nextsent>however, it is not at all clear how to adapt the theory for multi-party discourse.
</nextsent>
<nextsent>this paper ex-amines some of the issues involved in adapting the theory, then describes the results of applying three alternative models to corpus of 2-person dialogs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X93">
<title id=" P98-2241.xml">a preliminary model of centering in dialog </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the centering framework (grosz et al, 1995) <papid> J95-2003 </papid>is one of the most influential computational linguistics the-ories relating local focus to the form chosen for re-ferring expressions.</prevsent>
<prevsent>a number of studies have de-veloped refinements and extensions of the theory (eg.</prevsent>
</prevsection>
<citsent citstr=" P96-1036 ">
brennan et al, 1987; <papid> P87-1022 </papid>kameyama, 1986; <papid> P86-1031 </papid>strube and hahn, 1996; <papid> P96-1036 </papid>walker et al, 1998), but few have attempted to extend the model to multi-party dis-course (cf.</citsent>
<aftsection>
<nextsent>brennan, 1998; walker, 1998).
</nextsent>
<nextsent>for dialog systems, the benefits of using cen-tering theory include improved reference resolution and generation of more coherent referring expres-sions.
</nextsent>
<nextsent>however, it is not at all clear how to adapt the theory for multi-party discourse.
</nextsent>
<nextsent>this paper ex-amines some of the issues involved in adapting the theory, then describes the results of applying three alternative models to corpus of 2-person dialogs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X101">
<title id=" S10-1089.xml">tree match a fully unsupervised wsd system using dependency knowledge on a specific domain </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in these approaches, first graph is built with senses as nodes and relations among words/senses (e.g., synonymy, antonymy) as edges, and the relations are usually acquired from lkb (e.g., wordnet).
</prevsent>
<prevsent>then ranking algorithm is conducted over the graph, and senses ranked the highest are assigned to the corresponding words.
</prevsent>
</prevsection>
<citsent citstr=" H05-1052 ">
different relations and ranking algorithms were experimented with these methods, such as texrank algorithm (mihalcea, 2005), <papid> H05-1052 </papid>personalized page rank algorithm (agirre, 2009), two-stage searching algorithm (navigli, 2007), structural semantic interconnections algorithm (navigli, 2005), centrality algorithms (sinha, 2009).</citsent>
<aftsection>
<nextsent>supervised methods.
</nextsent>
<nextsent>a supervised method includes training phase and testing phase.
</nextsent>
<nextsent>in the training phase, sense-annotated training corpus is required, from which syntactic and semantic features are extracted to build classifier using machine learning techniques, such as support vector machine (novisch, 2007).
</nextsent>
<nextsent>in the following testing phase, the classifier picks the best sense for word based on its surrounding words (mihalcea, 2002).<papid> C02-1039 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X102">
<title id=" S10-1089.xml">tree match a fully unsupervised wsd system using dependency knowledge on a specific domain </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a supervised method includes training phase and testing phase.
</prevsent>
<prevsent>in the training phase, sense-annotated training corpus is required, from which syntactic and semantic features are extracted to build classifier using machine learning techniques, such as support vector machine (novisch, 2007).
</prevsent>
</prevsection>
<citsent citstr=" C02-1039 ">
in the following testing phase, the classifier picks the best sense for word based on its surrounding words (mihalcea, 2002).<papid> C02-1039 </papid></citsent>
<aftsection>
<nextsent>currently supervised methods achieved the best disambiguation quality (about 80% in precision and recall for coarse-grained wsd in the most recent wsd evaluation conference semeval 2007 (novisch, 2007).
</nextsent>
<nextsent>nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is impractical to manually annotate huge number of words existing in natural language.
</nextsent>
<nextsent>semi-supervised methods.
</nextsent>
<nextsent>to overcome the knowledge acquisition bottleneck suffered in supervised methods, semi-supervised methods make use of small annotated corpus as seed data in bootstrapping process (hearst, 1991) (yarowsky, 1995).<papid> P95-1026 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X103">
<title id=" S10-1089.xml">tree match a fully unsupervised wsd system using dependency knowledge on a specific domain </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is impractical to manually annotate huge number of words existing in natural language.
</prevsent>
<prevsent>semi-supervised methods.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
to overcome the knowledge acquisition bottleneck suffered in supervised methods, semi-supervised methods make use of small annotated corpus as seed data in bootstrapping process (hearst, 1991) (yarowsky, 1995).<papid> P95-1026 </papid></citsent>
<aftsection>
<nextsent>a system precision recall 1sense 0.505 0.505 treematch-1 0.506 0.493 treematch-2 0.504 0.491 treematch-3 0.492 0.479 random 0.23 0.23 399 word-aligned bilingual corpus can also serve as seed data (zhong, 2009).
</nextsent>
<nextsent>unsupervised methods.
</nextsent>
<nextsent>these methods acquire knowledge from unannotated raw text, and induce senses using similarity measures (lin, 1997).<papid> P97-1009 </papid></nextsent>
<nextsent>unsupervised methods overcome the problem of knowledge acquisition bottleneck, but none of existing methods can outperform the most frequent sense baseline, which makes them not useful at all in practice.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X104">
<title id=" S10-1089.xml">tree match a fully unsupervised wsd system using dependency knowledge on a specific domain </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a system precision recall 1sense 0.505 0.505 treematch-1 0.506 0.493 treematch-2 0.504 0.491 treematch-3 0.492 0.479 random 0.23 0.23 399 word-aligned bilingual corpus can also serve as seed data (zhong, 2009).
</prevsent>
<prevsent>unsupervised methods.
</prevsent>
</prevsection>
<citsent citstr=" P97-1009 ">
these methods acquire knowledge from unannotated raw text, and induce senses using similarity measures (lin, 1997).<papid> P97-1009 </papid></citsent>
<aftsection>
<nextsent>unsupervised methods overcome the problem of knowledge acquisition bottleneck, but none of existing methods can outperform the most frequent sense baseline, which makes them not useful at all in practice.
</nextsent>
<nextsent>the best unsupervised systems only achieved about 70% in precision and 50% in recall in the semeval 2007 (navigli, 2007).
</nextsent>
<nextsent>one recent study utilized automatically acquired dependency knowledge and achieved 73% in precision and recall (chen, 2009), which is still below the most-frequent-sense baseline (78.89% in precision and recall in the semeval 2007 task 07).
</nextsent>
<nextsent>additionally there exist some meta disambiguation?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X105">
<title id=" S12-1007.xml">could you make me a favour and do coffee please implications for automatic error correction in english and dutch </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other errors can be explained by non-native speakers inability to distinguish between words because there exists only one corresponding word in their native language.
</prevsent>
<prevsent>for example, portuguese and spanish speakers have difficulties to differentiate between te doen (to do) and te maken (to make), and turkish between kunnen (can), weten (to know) and kennen (to know) indutch (coenen et al, 1979).
</prevsent>
</prevsection>
<citsent citstr=" D10-1094 ">
adopting terminology from golding and roth (1999) and rozovskaya and roth (2010), <papid> D10-1094 </papid>do/make and kunnen/kennen/weten form two confusion sets.</citsent>
<aftsection>
<nextsent>however, unlike the case of kunnen/kennen/weten, where the correct choice is often determined by syntactic context 1, the choice between to make and to do can be motivated by semantic factors.
</nextsent>
<nextsent>it has been argued in the literature that the correct use of these verbs depends on what is being expressed: to do is used to refer to daily routines and activities, while to make is used to describe constructing or creating something.
</nextsent>
<nextsent>since word choice errors have different nature, we hypothesize that there may exist no uniform approach to correct them.
</nextsent>
<nextsent>state-of-the-art spell-checkers are able to detect spelling and agreement errors but fail to find words used incorrectly, e.g. to distinguish to make from to do.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X106">
<title id=" S12-1007.xml">could you make me a favour and do coffee please implications for automatic error correction in english and dutch </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>state-of-the-art spell-checkers are able to detect spelling and agreement errors but fail to find words used incorrectly, e.g. to distinguish to make from to do.
</prevsent>
<prevsent>motivated by the implications that the correct prediction of two verbs of interest may have for automatic error correction, we model the problem of choosing the correct verb in similar vein to selectional preferences.
</prevsent>
</prevsection>
<citsent citstr=" P09-2019 ">
the latter has been considered for variety of applications, e. g. semantic role labeling (zapirain et al, 2009).<papid> P09-2019 </papid></citsent>
<aftsection>
<nextsent>words such as be ordo have been often excluded from consideration be cause they are highly polysemous and do not select strongly for their arguments?
</nextsent>
<nextsent>(mccarthy and car roll, 2003).<papid> J03-4004 </papid></nextsent>
<nextsent>in this paper, we study whether semantic classes of arguments may be used to determine the correct predicate (e.g., to make or to do) and consider the following research questions: 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X107">
<title id=" S12-1007.xml">could you make me a favour and do coffee please implications for automatic error correction in english and dutch </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the latter has been considered for variety of applications, e. g. semantic role labeling (zapirain et al, 2009).<papid> P09-2019 </papid></prevsent>
<prevsent>words such as be ordo have been often excluded from consideration be cause they are highly polysemous and do not select strongly for their arguments?</prevsent>
</prevsection>
<citsent citstr=" J03-4004 ">
(mccarthy and car roll, 2003).<papid> J03-4004 </papid></citsent>
<aftsection>
<nextsent>in this paper, we study whether semantic classes of arguments may be used to determine the correct predicate (e.g., to make or to do) and consider the following research questions: 1.
</nextsent>
<nextsent>can information on semantic classes of direct.
</nextsent>
<nextsent>1kunnen is modal verb followed by the main verb, kennen takes direct object as in, e.g., to know somebody, and weten is often followed by clause (as in know that).
</nextsent>
<nextsent>49 objects potentially help to correct verb choice errors?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X109">
<title id=" S12-1007.xml">could you make me a favour and do coffee please implications for automatic error correction in english and dutch </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude in section 3.
</prevsent>
<prevsent>we re-examine several approaches to selectional preferences in the context of error correction.
</prevsent>
</prevsection>
<citsent citstr=" P07-1028 ">
existing methods fall into one of two categories, either those relying on information from wordnet (mc carthy and carroll, 2003), <papid> J03-4004 </papid>or data-driven (erk, 2007; <papid> P07-1028 </papid>schulte im walde, 2010; pado et al, 2007).</citsent>
<aftsection>
<nextsent>for the purpose of our study, we focus on the latter.
</nextsent>
<nextsent>2.1 methods.
</nextsent>
<nextsent>for each verb in question, we have frequency based ranking list of nouns co-occurring with it (verb-object pairs) which we use for the first two methods.
</nextsent>
<nextsent>latent semantic clustering (lsc) rooth et al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X110">
<title id=" S12-1007.xml">could you make me a favour and do coffee please implications for automatic error correction in english and dutch </title>
<section> andrew made [his mum]dobj happy..  </section>
<citcontext>
<prevsection>
<prevsent>2.
</prevsent>
<prevsent>andrew made [his mum]iobj [a cake]dobj ..
</prevsent>
</prevsection>
<citsent citstr=" C04-1010 ">
for english, we use one of the largest corpora available, the pukwac (over 2 billion words, 30gb) (baroni et al, 2009), which has been parsed by malt parser (nivre and scholz, 2004).<papid> C04-1010 </papid></citsent>
<aftsection>
<nextsent>we extract all sentences with to do or to make (based on lemmata).
</nextsent>
<nextsent>the verb to make occurs in 2,13% of sentences, and the verb to do in 3,27% of sentences inthe pukwac corpus.
</nextsent>
<nextsent>next, we exclude from consideration phrasal mono-transitives and select sentences where verb complements are nouns (table 1).
</nextsent>
<nextsent>for experiments in dutch, we use the wikipedia dump of 2010?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X111">
<title id=" S12-1007.xml">could you make me a favour and do coffee please implications for automatic error correction in english and dutch </title>
<section> andrew made [his mum]dobj happy..  </section>
<citcontext>
<prevsection>
<prevsent>similarly to the english dataset, phrasal mono transit ives are filtered out.
</prevsent>
<prevsent>finally, the sentences that contain either to make or to do from wiki01 upto wiki07 (19,847 sentences in total) have been selected for training and wiki08 (1,769 sentences in total) for testing.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
to be able to compare our results against the performance on english data, we sample subset from pukwac which is of the same size as dutch dataset and is referred to as en (sm).to measure distributional similarity for the nearest neighbour method, we use first-order and second-order similarity based on lins information theoretic measure (lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>for both languages, similarity scores have been derived given subset of wikipedia (276 million tokens for english and 114 million tokens for dutch) using the disco api (kolb, 2009).
</nextsent>
<nextsent>2.3 results.
</nextsent>
<nextsent>table 2 and table 3 summarize our results.
</nextsent>
<nextsent>when referring to similarity-based methods, the symbols (f)and (s) indicate first-order and second-order similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X112">
<title id=" P99-1035.xml">inside outside estimation of a lexicalized pcfg for german </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the precision measure of the first lex-icalized model falls below that of the unlexi- calized random model (74%), only recovering through lexicalized training to equalize the pre-cision measure of the random model (75.6%).
</prevsent>
<prevsent>this indicates that some degree of un lexicalized initialization is necessary, if good lexica\]ized model is to be obtained.
</prevsent>
</prevsection>
<citsent citstr=" W98-1117 ">
(skut and brants, 1998) <papid> W98-1117 </papid>report 84.4% recall and 84.2% for np and pp chunking without case labels.</citsent>
<aftsection>
<nextsent>while these are numbers for simpler problem and are slightly below ours, they are figures for an experiment on unrestricted sen-tences.
</nextsent>
<nextsent>a genuine comparison has to await ex-tension of our model to free text.
</nextsent>
<nextsent>6.3 verb frame evaluation.
</nextsent>
<nextsent>figure 9 gives results for verb frame recogni-tion under the same training conditions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X113">
<title id=" S10-1021.xml">bart a multilingual anaphora resolution system </title>
<section> language-specific issues.  </section>
<citcontext>
<prevsection>
<prevsent>for english and german, we use the parsing pipeline and mention factory to extract mentions.
</prevsent>
<prevsent>the parse trees are used to identify minimal and maximal noun projections, as well as additional features such as number, gender, and semantic class.
</prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
for english, we use parses from state-of-the-art constituent parser (petrov et al, 2006) <papid> P06-1055 </papid>and extract all base noun phrases as mentions.</citsent>
<aftsection>
<nextsent>for german, the semeval dependency tree is transformed to aconstituent representation and minimal and maximal phrases are extracted for all nominal elements (pronouns, common nouns, names), except when the noun phrase is in non-referring syntactic position (for example, expletive es?, predicates in copula constructions).for italian, we use the emd pipeline and mention factory.
</nextsent>
<nextsent>the typhoon (zanoli et al, 2009) and demention (biggio et al, 2009) systems were used to recognize mentions in the test set.
</nextsent>
<nextsent>for each mention, its head and extension were considered.the extension was learned by using the mention annotation provided in the training set (13th column)whereas the head annotation was learned by exploiting the information produced by malt parser (nivre et al, 2007).
</nextsent>
<nextsent>in addition to the features extracted from the training set, such as prefixes and suffixes(1-4 characters) and orthographic information (capitalization and hyphenation), number of features extracted by using external resources were used: mentions recognized by textpro (http://textpro.fbk.eu), gazette ers of generic proper nouns extracted fromthe italian phone-book and wikipedia, and other features derived from wordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X114">
<title id=" S12-1026.xml">unsupervised induction of a syntax semantics lexicon using iterative refinement </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other verbs that show this pattern include break or melt.
</prevsent>
<prevsent>much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (burzio, 1986; levin, 1993).
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the last decades have seen surge inactivity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (baker et al, 1998; <papid> P98-1013 </papid>palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (la pata and brew, 1999; <papid> W99-0632 </papid>merlo and stevenson, 2001).<papid> J01-3003 </papid>more recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite).in this paper, we explore learning syntax semantics mappings for verbs from unannotated corpora.</nextsent>
<nextsent>we are specifically interested in learning link ings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X115">
<title id=" S12-1026.xml">unsupervised induction of a syntax semantics lexicon using iterative refinement </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other verbs that show this pattern include break or melt.
</prevsent>
<prevsent>much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (burzio, 1986; levin, 1993).
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
the last decades have seen surge inactivity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (baker et al, 1998; <papid> P98-1013 </papid>palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (la pata and brew, 1999; <papid> W99-0632 </papid>merlo and stevenson, 2001).<papid> J01-3003 </papid>more recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite).in this paper, we explore learning syntax semantics mappings for verbs from unannotated corpora.</nextsent>
<nextsent>we are specifically interested in learning link ings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X117">
<title id=" S12-1026.xml">unsupervised induction of a syntax semantics lexicon using iterative refinement </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (burzio, 1986; levin, 1993).
</prevsent>
<prevsent>the last decades have seen surge inactivity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (baker et al, 1998; <papid> P98-1013 </papid>palmer et al, 2005).<papid> J05-1004 </papid></prevsent>
</prevsection>
<citsent citstr=" W99-0632 ">
initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (la pata and brew, 1999; <papid> W99-0632 </papid>merlo and stevenson, 2001).<papid> J01-3003 </papid>more recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite).in this paper, we explore learning syntax semantics mappings for verbs from unannotated corpora.</citsent>
<aftsection>
<nextsent>we are specifically interested in learning linkings.
</nextsent>
<nextsent>a linking is mapping for one verb from its syntactic arguments and adjuncts to all of its semantic roles, so that individual semantic roles are not modeled independently of one another and so that we can exploit the relation between different mappings for the same verb (as in (1) above), or between mappings for different verbs.
</nextsent>
<nextsent>we therefore follow grenager and manning (2006) <papid> W06-1601 </papid>in treating linkings as first-class objects; however, we differ from their work in two important respects.</nextsent>
<nextsent>first,we use semantic clustering of headwords of arguments in an approach that resembles topic modeling, rather than directly modeling the subcategorization of verbs with distribution over words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X118">
<title id=" S12-1026.xml">unsupervised induction of a syntax semantics lexicon using iterative refinement </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (burzio, 1986; levin, 1993).
</prevsent>
<prevsent>the last decades have seen surge inactivity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (baker et al, 1998; <papid> P98-1013 </papid>palmer et al, 2005).<papid> J05-1004 </papid></prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (la pata and brew, 1999; <papid> W99-0632 </papid>merlo and stevenson, 2001).<papid> J01-3003 </papid>more recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite).in this paper, we explore learning syntax semantics mappings for verbs from unannotated corpora.</citsent>
<aftsection>
<nextsent>we are specifically interested in learning linkings.
</nextsent>
<nextsent>a linking is mapping for one verb from its syntactic arguments and adjuncts to all of its semantic roles, so that individual semantic roles are not modeled independently of one another and so that we can exploit the relation between different mappings for the same verb (as in (1) above), or between mappings for different verbs.
</nextsent>
<nextsent>we therefore follow grenager and manning (2006) <papid> W06-1601 </papid>in treating linkings as first-class objects; however, we differ from their work in two important respects.</nextsent>
<nextsent>first,we use semantic clustering of headwords of arguments in an approach that resembles topic modeling, rather than directly modeling the subcategorization of verbs with distribution over words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X119">
<title id=" S12-1026.xml">unsupervised induction of a syntax semantics lexicon using iterative refinement </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we are specifically interested in learning linkings.
</prevsent>
<prevsent>a linking is mapping for one verb from its syntactic arguments and adjuncts to all of its semantic roles, so that individual semantic roles are not modeled independently of one another and so that we can exploit the relation between different mappings for the same verb (as in (1) above), or between mappings for different verbs.
</prevsent>
</prevsection>
<citsent citstr=" W06-1601 ">
we therefore follow grenager and manning (2006) <papid> W06-1601 </papid>in treating linkings as first-class objects; however, we differ from their work in two important respects.</citsent>
<aftsection>
<nextsent>first,we use semantic clustering of headwords of arguments in an approach that resembles topic modeling, rather than directly modeling the subcategorization of verbs with distribution over words.
</nextsent>
<nextsent>second and most importantly, we do not make any assumptions about the linkings, as do grenager and manning (2006).<papid> W06-1601 </papid></nextsent>
<nextsent>they list small set of rules from which they derive all linkings possible in their model; in contrast, we are able to learn any linking observed in the data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X128">
<title id=" S12-1026.xml">unsupervised induction of a syntax semantics lexicon using iterative refinement </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on these, we achieve a20% f-measure error reduction over high syntactic baseline (which maps each syntactic relation to single semantic argument).
</prevsent>
<prevsent>as mentioned above, our approach is most similar to that of grenager and manning (2006).<papid> W06-1601 </papid></prevsent>
</prevsection>
<citsent citstr=" P11-1112 ">
however, since their model uses hand-crafted rules, they are able to predict and evaluate against actual propbank role labels, whereas our approach has to be evaluated in terms of clustering quality.the problem of unsupervised semantic role labeling has recently attracted some attention (lang and lapata, 2011<papid> P11-1112 </papid>a; lang and lapata, 2011<papid> P11-1112 </papid>b; titov and klementiev, 2012).<papid> E12-1003 </papid></citsent>
<aftsection>
<nextsent>while the present paper shares the general aim of inducing semantic role clusters in an unsupervised way, it differs in treating syntax-semantics linkings explicitly and modeling predicate-specific distributions over them.abend et al (2009) <papid> P09-1004 </papid>address the problem of unsupervised argument recognition, which we do not address in the present paper.</nextsent>
<nextsent>for the purpose of building complete unsupervised semantic parser, method such as theirs would be complementary to our work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X136">
<title id=" S12-1026.xml">unsupervised induction of a syntax semantics lexicon using iterative refinement </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on these, we achieve a20% f-measure error reduction over high syntactic baseline (which maps each syntactic relation to single semantic argument).
</prevsent>
<prevsent>as mentioned above, our approach is most similar to that of grenager and manning (2006).<papid> W06-1601 </papid></prevsent>
</prevsection>
<citsent citstr=" E12-1003 ">
however, since their model uses hand-crafted rules, they are able to predict and evaluate against actual propbank role labels, whereas our approach has to be evaluated in terms of clustering quality.the problem of unsupervised semantic role labeling has recently attracted some attention (lang and lapata, 2011<papid> P11-1112 </papid>a; lang and lapata, 2011<papid> P11-1112 </papid>b; titov and klementiev, 2012).<papid> E12-1003 </papid></citsent>
<aftsection>
<nextsent>while the present paper shares the general aim of inducing semantic role clusters in an unsupervised way, it differs in treating syntax-semantics linkings explicitly and modeling predicate-specific distributions over them.abend et al (2009) <papid> P09-1004 </papid>address the problem of unsupervised argument recognition, which we do not address in the present paper.</nextsent>
<nextsent>for the purpose of building complete unsupervised semantic parser, method such as theirs would be complementary to our work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X137">
<title id=" S12-1026.xml">unsupervised induction of a syntax semantics lexicon using iterative refinement </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as mentioned above, our approach is most similar to that of grenager and manning (2006).<papid> W06-1601 </papid></prevsent>
<prevsent>however, since their model uses hand-crafted rules, they are able to predict and evaluate against actual propbank role labels, whereas our approach has to be evaluated in terms of clustering quality.the problem of unsupervised semantic role labeling has recently attracted some attention (lang and lapata, 2011<papid> P11-1112 </papid>a; lang and lapata, 2011<papid> P11-1112 </papid>b; titov and klementiev, 2012).<papid> E12-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" P09-1004 ">
while the present paper shares the general aim of inducing semantic role clusters in an unsupervised way, it differs in treating syntax-semantics linkings explicitly and modeling predicate-specific distributions over them.abend et al (2009) <papid> P09-1004 </papid>address the problem of unsupervised argument recognition, which we do not address in the present paper.</citsent>
<aftsection>
<nextsent>for the purpose of building complete unsupervised semantic parser, method such as theirs would be complementary to our work.
</nextsent>
<nextsent>in this section, we decribe model that generates arguments forgiven predicate instance.
</nextsent>
<nextsent>specifically, this generative model describes the probability of given set of argument headwords and associated syntactic functions in terms of underlying semantic roles, which are modelled as latent variables.
</nextsent>
<nextsent>the semantic role labeling task is therefore framed as the induction of these latent variables from the observed data, which we assume to be preprocessed by syntactic parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X139">
<title id=" S12-1026.xml">unsupervised induction of a syntax semantics lexicon using iterative refinement </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>to predict semantic roles forgiven predicate and argument set, we maximize (l, w, y|p, s).
</prevsent>
<prevsent>if the 184 space of permutations is too large for exhaustive enumeration, we apply similar beam search procedure as the one employed in training to approximately maximize (w, y|p, s, l) for each value of l. for efficiency, we do not marginalize over l. this has the potential of reducing prediction quality, as we do not predict the most likely role assignment,but rather the most likely combination of role assignment and latent linking.in all experiments we averaged over 10 consecutive samples of the latent distributions, at the end of the sampling process (i.e., when convergence has been reached).
</prevsent>
</prevsection>
<citsent citstr=" W08-2121 ">
we train and evaluate our linking model on the dataset produced for the conll-08 shared task onjoint parsing of syntactic and semantic dependencies (surdeanu et al, 2008), <papid> W08-2121 </papid>which is based on the propbank corpus (palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>this dataset includes part-of-speech tags, lemmatized tokens,and syntactic dependencies, which have been converted from the manual syntactic annotation of the underlying penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
<nextsent>4.1 data set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X142">
<title id=" S12-1026.xml">unsupervised induction of a syntax semantics lexicon using iterative refinement </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>if the 184 space of permutations is too large for exhaustive enumeration, we apply similar beam search procedure as the one employed in training to approximately maximize (w, y|p, s, l) for each value of l. for efficiency, we do not marginalize over l. this has the potential of reducing prediction quality, as we do not predict the most likely role assignment,but rather the most likely combination of role assignment and latent linking.in all experiments we averaged over 10 consecutive samples of the latent distributions, at the end of the sampling process (i.e., when convergence has been reached).
</prevsent>
<prevsent>we train and evaluate our linking model on the dataset produced for the conll-08 shared task onjoint parsing of syntactic and semantic dependencies (surdeanu et al, 2008), <papid> W08-2121 </papid>which is based on the propbank corpus (palmer et al, 2005).<papid> J05-1004 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
this dataset includes part-of-speech tags, lemmatized tokens,and syntactic dependencies, which have been converted from the manual syntactic annotation of the underlying penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>4.1 dataset.
</nextsent>
<nextsent>as input to our model, we decided not to use the syntactic representation in the conll-08 dataset, but instead to relyon stanford dependencies (de marneffe et al, 2006), which seem to facilitate semantic analysis.
</nextsent>
<nextsent>we thus used the stanford parser2 to convert the underlying phrase structure trees of the penn tree bank into stanford dependencies.
</nextsent>
<nextsent>in the resulting dependency analyses, the syntactic head word of semantic role may differ from the syntactic head according to the provided syntax.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X162">
<title id=" S12-1053.xml">semeval2012 task 8 cross lingual textual entailment for content synchronization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>had to be identified.
</prevsent>
<prevsent>we report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams,92 runs), the approaches adopted and there sults achieved.
</prevsent>
</prevsection>
<citsent citstr=" N10-1045 ">
the cross-lingual textual entailment task (mehdad etal., 2010) <papid> N10-1045 </papid>addresses textual entailment (te) recognition (dagan and glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchro nization.</citsent>
<aftsection>
<nextsent>cross-linguality represents dimension of the tere cognition problem that has been so far only partially investigated.
</nextsent>
<nextsent>the great potential for integrating monolingual te recognition components intonlp architectures has been reported in several areas, including question answering, information retrieval, information extraction, and document summarization.
</nextsent>
<nextsent>however, mainly due to the absence of cross-lingual textual entailment (clte) recognition components, similar improvements have not been achieved yet in any cross-lingual application.
</nextsent>
<nextsent>the clte task aims at prompting research to fill thisgap.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X163">
<title id=" S12-1053.xml">semeval2012 task 8 cross lingual textual entailment for content synchronization </title>
<section> dataset description.  </section>
<citcontext>
<prevsection>
<prevsent>the datasets are released in the xml format shown in figure 1.
</prevsent>
<prevsent>3.1 data collection and annotation.
</prevsent>
</prevsection>
<citsent citstr=" D11-1062 ">
the dataset was created following the crowdsourcing methodology proposed in (negri et al, 2011), <papid> D11-1062 </papid>which consists of the following steps: 1.</citsent>
<aftsection>
<nextsent>first, english sentences were manually ex-.
</nextsent>
<nextsent>tracted from copyright-free sources (wikipediaand wikinews).
</nextsent>
<nextsent>the selected sentences represent one of the elements (t1) of each entailment pair; 2.
</nextsent>
<nextsent>next, each t1 was modified through crowd-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X164">
<title id=" S12-1053.xml">semeval2012 task 8 cross lingual textual entailment for content synchronization </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>such judgments are finally composed into single one for each pair.
</prevsent>
<prevsent>in the other runs, the same features are used for multi-class classification.
</prevsent>
</prevsection>
<citsent citstr=" S12-1107 ">
dirrelcond3 [cross lingual, compositional] (perini, 2012) <papid> S12-1107 </papid>uses bilingual dictionaries (freedict5 and wordreference6) to translate content words intoenglish.</citsent>
<aftsection>
<nextsent>then, entailment decisions are taken combining directional relatedness scores between words in both directions (perini, 2011).fbk [cross lingual, compositional &amp; multi class] (mehdad et al, 2012<papid> W12-3122 </papid>a) uses cross-lingualmatching features extracted from lexical phrase tables, semantic phrase tables, and dependency relations (mehdad et al, 2011; <papid> P11-1134 </papid>mehdad et al, 2012<papid> W12-3122 </papid>b; mehdad et al, 2012<papid> W12-3122 </papid>c).</nextsent>
<nextsent>the features are used for multi-class and binary classification using svms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X165">
<title id=" S12-1053.xml">semeval2012 task 8 cross lingual textual entailment for content synchronization </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>in the other runs, the same features are used for multi-class classification.
</prevsent>
<prevsent>dirrelcond3 [cross lingual, compositional] (perini, 2012) <papid> S12-1107 </papid>uses bilingual dictionaries (freedict5 and wordreference6) to translate content words intoenglish.</prevsent>
</prevsection>
<citsent citstr=" W12-3122 ">
then, entailment decisions are taken combining directional relatedness scores between words in both directions (perini, 2011).fbk [cross lingual, compositional &amp; multi class] (mehdad et al, 2012<papid> W12-3122 </papid>a) uses cross-lingualmatching features extracted from lexical phrase tables, semantic phrase tables, and dependency relations (mehdad et al, 2011; <papid> P11-1134 </papid>mehdad et al, 2012<papid> W12-3122 </papid>b; mehdad et al, 2012<papid> W12-3122 </papid>c).</citsent>
<aftsection>
<nextsent>the features are used for multi-class and binary classification using svms.
</nextsent>
<nextsent>hdu [hybrid, compositional] (waschle and fendrich, 2012) uses combination of binary classifiers for each entailment direction.
</nextsent>
<nextsent>the classifiers use both monolingual alignment features based on meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>alignments (translations obtained from google translate), and cross-lingual alignment features based on giza++ (och and ney, 2000) (<papid> P00-1056 </papid>word alignments learned on europarl).</nextsent>
<nextsent>ict [pivoting, compositional] (meng et al, 2012) <papid> S12-1108 </papid>adopts pivoting method (using google translate and an in-house hierarchical mt system), and the open source edits system (kouylekov and negri, 2010) to calculate similarity scores between monolingual english pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X171">
<title id=" S12-1053.xml">semeval2012 task 8 cross lingual textual entailment for content synchronization </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>in the other runs, the same features are used for multi-class classification.
</prevsent>
<prevsent>dirrelcond3 [cross lingual, compositional] (perini, 2012) <papid> S12-1107 </papid>uses bilingual dictionaries (freedict5 and wordreference6) to translate content words intoenglish.</prevsent>
</prevsection>
<citsent citstr=" P11-1134 ">
then, entailment decisions are taken combining directional relatedness scores between words in both directions (perini, 2011).fbk [cross lingual, compositional &amp; multi class] (mehdad et al, 2012<papid> W12-3122 </papid>a) uses cross-lingualmatching features extracted from lexical phrase tables, semantic phrase tables, and dependency relations (mehdad et al, 2011; <papid> P11-1134 </papid>mehdad et al, 2012<papid> W12-3122 </papid>b; mehdad et al, 2012<papid> W12-3122 </papid>c).</citsent>
<aftsection>
<nextsent>the features are used for multi-class and binary classification using svms.
</nextsent>
<nextsent>hdu [hybrid, compositional] (waschle and fendrich, 2012) uses combination of binary classifiers for each entailment direction.
</nextsent>
<nextsent>the classifiers use both monolingual alignment features based on meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>alignments (translations obtained from google translate), and cross-lingual alignment features based on giza++ (och and ney, 2000) (<papid> P00-1056 </papid>word alignments learned on europarl).</nextsent>
<nextsent>ict [pivoting, compositional] (meng et al, 2012) <papid> S12-1108 </papid>adopts pivoting method (using google translate and an in-house hierarchical mt system), and the open source edits system (kouylekov and negri, 2010) to calculate similarity scores between monolingual english pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X184">
<title id=" S12-1053.xml">semeval2012 task 8 cross lingual textual entailment for content synchronization </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>the features are used for multi-class and binary classification using svms.
</prevsent>
<prevsent>hdu [hybrid, compositional] (waschle and fendrich, 2012) uses combination of binary classifiers for each entailment direction.
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
the classifiers use both monolingual alignment features based on meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>alignments (translations obtained from google translate), and cross-lingual alignment features based on giza++ (och and ney, 2000) (<papid> P00-1056 </papid>word alignments learned on europarl).</citsent>
<aftsection>
<nextsent>ict [pivoting, compositional] (meng et al, 2012) <papid> S12-1108 </papid>adopts pivoting method (using google translate and an in-house hierarchical mt system), and the open source edits system (kouylekov and negri, 2010) to calculate similarity scores between monolingual english pairs.</nextsent>
<nextsent>separate unidirectional entailment judgments obtained from binary classifier are combined to return one of the four valid clte judgments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X185">
<title id=" S12-1053.xml">semeval2012 task 8 cross lingual textual entailment for content synchronization </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>the features are used for multi-class and binary classification using svms.
</prevsent>
<prevsent>hdu [hybrid, compositional] (waschle and fendrich, 2012) uses combination of binary classifiers for each entailment direction.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
the classifiers use both monolingual alignment features based on meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>alignments (translations obtained from google translate), and cross-lingual alignment features based on giza++ (och and ney, 2000) (<papid> P00-1056 </papid>word alignments learned on europarl).</citsent>
<aftsection>
<nextsent>ict [pivoting, compositional] (meng et al, 2012) <papid> S12-1108 </papid>adopts pivoting method (using google translate and an in-house hierarchical mt system), and the open source edits system (kouylekov and negri, 2010) to calculate similarity scores between monolingual english pairs.</nextsent>
<nextsent>separate unidirectional entailment judgments obtained from binary classifier are combined to return one of the four valid clte judgments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X186">
<title id=" S12-1053.xml">semeval2012 task 8 cross lingual textual entailment for content synchronization </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>hdu [hybrid, compositional] (waschle and fendrich, 2012) uses combination of binary classifiers for each entailment direction.
</prevsent>
<prevsent>the classifiers use both monolingual alignment features based on meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>alignments (translations obtained from google translate), and cross-lingual alignment features based on giza++ (och and ney, 2000) (<papid> P00-1056 </papid>word alignments learned on europarl).</prevsent>
</prevsection>
<citsent citstr=" S12-1108 ">
ict [pivoting, compositional] (meng et al, 2012) <papid> S12-1108 </papid>adopts pivoting method (using google translate and an in-house hierarchical mt system), and the open source edits system (kouylekov and negri, 2010) to calculate similarity scores between monolingual english pairs.</citsent>
<aftsection>
<nextsent>separate unidirectional entailment judgments obtained from binary classifier are combined to return one of the four valid clte judgments.
</nextsent>
<nextsent>5http://www.freedict.com/ 6http://www.wordreference.com/ 404 sp-en forward backward no entailment bidirectional system name r f1 r f1 r f1 r f1 buap spa-eng run2 0,337 0,664 0,447 0,406 0,568 0,473 0,333 0,088 0,139 0,391 0,144 0,211 celi spa-eng run2 0,324 0,368 0,345 0,411 0,368 0,388 0,306 0,296 0,301 0,312 0,312 0,312 dirrelcond3 spa-eng run4 0,358 0,608 0,451 0,444 0,448 0,446 0,286 0,032 0,058 0,243 0,288 0,264 fbk spa-eng run3 0,515 0,704 0,595 0,546 0,568 0,557 0,447 0,304 0,362 0,482 0,440 0,460 hdu spa-eng run2 0,607 0,656 0,631 0,677 0,704 0,690 0,602 0,592 0,597 0,643 0,576 0,608 ict spa-eng run1 0,750 0,240 0,364 0,440 0,472 0,456 0,395 0,560 0,464 0,436 0,520 0,474 ju-cse-nlp spa-eng run1 0,211 0,288 0,243 0,272 0,296 0,284 0,354 0,232 0,280 0,315 0,280 0,297 sagan spa-eng run3 0,225 0,200 0,212 0,269 0,224 0,245 0,418 0,448 0,432 0,424 0,512 0,464 soft card spa-eng run1 0,602 0,616 0,609 0,650 0,624 0,637 0,471 0,448 0,459 0,489 0,520 0,504 ualacant spa-eng run1 late 0,689 0,568 0,623 0,645 0,728 0,684 0,507 0,544 0,525 0,566 0,552 0,559 avg.
</nextsent>
<nextsent>0,462 0,491 0,452 0,476 0,5 0,486 0,412 0,354 0,362 0,43 0,414 0,415 it-en forward backward no entailment bidirectional system name r f1 r f1 r f1 r f1 buap ita-eng run2 0,324 0,456 0,379 0,327 0,672 0,440 0,538 0,056 0,101 0,444 0,192 0,268 celi ita-eng run2 0,349 0,360 0,354 0,455 0,36 0,402 0,294 0,320 0,307 0,287 0,312 0,299 dirrelcond3 ita-eng run3 0,323 0,488 0,389 0,480 0,288 0,360 0,331 0,368 0,348 0,268 0,208 0,234 hdu ita-eng run2 0,564 0,600 0,581 0,628 0,648 0,638 0,551 0,520 0,535 0,500 0,480 0,490 ict ita-eng run1 0,661 0,296 0,409 0,554 0,368 0,442 0,427 0,448 0,438 0,383 0,704 0,496 ju-cse-nlp ita-eng run2 0,240 0,280 0,258 0,339 0,480 0,397 0,412 0,280 0,333 0,359 0,264 0,304 sagan ita-eng run3 0,306 0,296 0,301 0,252 0,216 0,233 0,395 0,512 0,446 0,455 0,400 0,426 soft card ita-eng run1 0,602 0,616 0,609 0,617 0,696 0,654 0,560 0,448 0,498 0,481 0,504 0,492 avg.
</nextsent>
<nextsent>0,421 0,424 0,410 0,457 0,466 0,446 0,439 0,369 0,376 0,397 0,383 0,376 fr-en forward backward no entailment bidirectional system name r f1 r f1 r f1 r f1 buap fra-eng run2 0,447 0,272 0,338 0,291 0,760 0,420 0,250 0,016 0,030 0,449 0,320 0,374 celi fra-eng run2 0,316 0,296 0,306 0,378 0,360 0,369 0,270 0,296 0,282 0,244 0,248 0,246 dirrelcond3 fra-eng run3 0,393 0,576 0,468 0,441 0,512 0,474 0,387 0,232 0,290 0,278 0,216 0,243 hdu fra-eng run2 0,564 0,672 0,613 0,582 0,736 0,650 0,676 0,384 0,490 0,500 0,488 0,494 ict fra-eng run1 0,750 0,192 0,306 0,517 0,496 0,506 0,385 0,656 0,485 0,444 0,480 0,462 ju-cse-nlp fra-eng run3 0,215 0,208 0,211 0,289 0,296 0,292 0,341 0,496 0,404 0,333 0,184 0,237 sagan fra-eng run1 0,244 0,168 0,199 0,297 0,344 0,319 0,394 0,568 0,466 0,427 0,304 0,355 soft card fra-eng run1 0,551 0,608 0,578 0,649 0,696 0,672 0,560 0,488 0,521 0,513 0,488 0,500 avg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X187">
<title id=" S12-1053.xml">semeval2012 task 8 cross lingual textual entailment for content synchronization </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>0,435 0,374 0,377 0,431 0,525 0,463 0,408 0,392 0,371 0,399 0,341 0,364 de-en forward backward no entailment bidirectional system name r f1 r f1 r f1 r f1 buap deu-eng run1 0,395 0,120 0,184 0,248 0,224 0,235 0,344 0,688 0,459 0,364 0,288 0,321 celi deu-eng run2 0,347 0,416 0,378 0,402 0,392 0,397 0,339 0,312 0,325 0,319 0,288 0,303 dirrelcond3 deu-eng run4 0,429 0,312 0,361 0,408 0,552 0,469 0,367 0,320 0,342 0,298 0,312 0,305 hdu deu-eng run1 0,559 0,528 0,543 0,600 0,696 0,644 0,540 0,488 0,513 0,524 0,520 0,522 ict deu-eng run1 0,718 0,224 0,341 0,493 0,552 0,521 0,390 0,512 0,443 0,439 0,552 0,489 ju-cse-nlp deu-eng run2 0,182 0,048 0,076 0,307 0,496 0,379 0,315 0,560 0,403 0,233 0,080 0,119 sagan deu-eng run1 0,250 0,168 0,201 0,239 0,256 0,247 0,405 0,600 0,484 0,443 0,344 0,387 soft card deu-eng run1 0,568 0,568 0,568 0,611 0,640 0,625 0,521 0,488 0,504 0,496 0,504 0,500 avg.
</prevsent>
<prevsent>0,431 0,298 0,332 0,414 0,476 0,440 0,403 0,496 0,434 0,390 0,361 0,368 table 4: precision, recall and f1 scores, calculated for each teams best run for all the language combinations.
</prevsent>
</prevsection>
<citsent citstr=" S12-1103 ">
ju-cse-nlp [pivoting, compositional] (neogiet al, 2012) <papid> S12-1103 </papid>uses microsoft bing translator7 to produce monolingual english pairs.</citsent>
<aftsection>
<nextsent>separate lexical mapping scores are calculated (from t1 to t2 andvice-versa) considering different types of information and similarity metrics.
</nextsent>
<nextsent>binary entailment de 7http://www.microsofttranslator.com/ cisions are then heuristic ally combined into single decisions.sagan [pivoting, multi-class] (castillo and cardenas, 2012) <papid> S12-1109 </papid>adopts pivoting method using google translate, and trains monolingual system based ona svm multi-class classifier.</nextsent>
<nextsent>a clte corpus derived from the rte-3 dataset is also used as source of additional training material.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X188">
<title id=" S12-1053.xml">semeval2012 task 8 cross lingual textual entailment for content synchronization </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>ju-cse-nlp [pivoting, compositional] (neogiet al, 2012) <papid> S12-1103 </papid>uses microsoft bing translator7 to produce monolingual english pairs.</prevsent>
<prevsent>separate lexical mapping scores are calculated (from t1 to t2 andvice-versa) considering different types of information and similarity metrics.</prevsent>
</prevsection>
<citsent citstr=" S12-1109 ">
binary entailment de 7http://www.microsofttranslator.com/ cisions are then heuristic ally combined into single decisions.sagan [pivoting, multi-class] (castillo and cardenas, 2012) <papid> S12-1109 </papid>adopts pivoting method using google translate, and trains monolingual system based ona svm multi-class classifier.</citsent>
<aftsection>
<nextsent>a clte corpus derived from the rte-3 dataset is also used as source of additional training material.
</nextsent>
<nextsent>405 soft card [pivoting, multi-class] (jimenez et al,2012) <papid> S12-1102 </papid>after automatic translation with google translate, uses svms to learn entailment decisions based on information about the cardinality of: t1, t2, their intersection and their union.</nextsent>
<nextsent>cardinal ities are computed in different ways, considering tokens in t1 and t2, their idf, and their similarity (computed with edit-distance) ualacant [pivoting, multi-class] (espla`-gomis et al, 2012) exploits translations obtained from google translate, microsoft bing translator, and the apertium open-source mt platform (forcada et al, 2011).8 then, multi-class svm classifier is used to take entailment decisions using information about overlapping sub-segments as features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X189">
<title id=" S12-1053.xml">semeval2012 task 8 cross lingual textual entailment for content synchronization </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>binary entailment de 7http://www.microsofttranslator.com/ cisions are then heuristic ally combined into single decisions.sagan [pivoting, multi-class] (castillo and cardenas, 2012) <papid> S12-1109 </papid>adopts pivoting method using google translate, and trains monolingual system based ona svm multi-class classifier.</prevsent>
<prevsent>a clte corpus derived from the rte-3 dataset is also used as source of additional training material.</prevsent>
</prevsection>
<citsent citstr=" S12-1102 ">
405 soft card [pivoting, multi-class] (jimenez et al,2012) <papid> S12-1102 </papid>after automatic translation with google translate, uses svms to learn entailment decisions based on information about the cardinality of: t1, t2, their intersection and their union.</citsent>
<aftsection>
<nextsent>cardinal ities are computed in different ways, considering tokens in t1 and t2, their idf, and their similarity (computed with edit-distance) ualacant [pivoting, multi-class] (espla`-gomis et al, 2012) exploits translations obtained from google translate, microsoft bing translator, and the apertium open-source mt platform (forcada et al, 2011).8 then, multi-class svm classifier is used to take entailment decisions using information about overlapping sub-segments as features.
</nextsent>
<nextsent>despite the novelty of the problem and the difficulty to capture multi-directional entailment relations across languages, the first round of the cross lingual textual entailment for content synchronization task organized within semeval-2012 was successful experience.
</nextsent>
<nextsent>this year new interesting challenge has been proposed, benchmark for four language combinations has been released, baseline results have been proposed for comparison, and monolingual english dataset has been produced as by-product which can be useful for monolingual te research.
</nextsent>
<nextsent>the interest shown by participants was encouraging: 10 teams submitted total of 92 runs for all the language pairs proposed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X190">
<title id=" S10-1061.xml">mars a specialized rte system for parser evaluation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, hpsg parsing is filter for ungrammatical hypotheses.
</prevsent>
<prevsent>tokenization and pos tagging we use the penn treebank style tokenization throughout the various processing stages.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
tnt, an hmm-basedpos tagger trained with wall street journal sections of the ptb, was used to automatically predict the part-of-speech of each token in the texts and hypotheses.dependency parsing for obtaining the syntactic dependencies, we use two dependency parsers,mstparser (mcdonald et al, 2005) <papid> H05-1066 </papid>and malt parser (nivre et al, 2007).</citsent>
<aftsection>
<nextsent>mst parser is graph based dependency parser where the best parse tree is acquired by searching for spanning tree 272 dependency path extraction feature-based classification preprocessing hpsg parsing dependency parsing semantic role labeling h dependency triple extraction path extraction feature extraction svm-based classification yes/no no figure 1: workflow of the system which maximize the score on an either partially or fully connected dependency graph.
</nextsent>
<nextsent>malt parser is transition-based incremental dependency parser, which is language-independent and data-driven.
</nextsent>
<nextsent>it contains deterministic algorithm, which can be viewed as variant of the basic shift-reduce algorithm.
</nextsent>
<nextsent>both parsers can achieve state-of-the-artperformance and figure 2 shows the resulting syntactic dependency trees of the following t-h pair, id: 2036; entailment: yes t: devotees of the market question the value of the work national service would perform.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X191">
<title id=" S10-1061.xml">mars a specialized rte system for parser evaluation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>while such dependencies capture interesting syntactic relations, when compared to the parsing systems with deeper representations, the contained information is not as detailed.
</prevsent>
<prevsent>to compensate for this, we used shallow semantic parser to predict the semantic role relations in the and of entailment pairs.
</prevsent>
</prevsection>
<citsent citstr=" W08-2126 ">
the shallow semantic parser was also trained with conll 2008 shared task dataset, with semantic roles extracted from the propbank and nombank annotations (zhang et al, 2008).<papid> W08-2126 </papid></citsent>
<aftsection>
<nextsent>figure 3 shows the resulting semantic dependency graphs of the t-h pair.hpsg parsing we employ the english resource grammar (flickinger, 2000), hand written linguistic grammar in the framework of hpsg, and the pet hpsg parser (callmeier,2001) to check the grammaticality of each hypothesis sentence.
</nextsent>
<nextsent>as the hypotheses in this pete shared task were automatically generated,some ungrammatical hypotheses occur in non entailment pairs.
</nextsent>
<nextsent>the grammaticality checking allows us to quickly identify these instances.
</nextsent>
<nextsent>2.2 dependency path extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X193">
<title id=" S12-1043.xml">umichigan a conditional random field model for resolving the scope of negation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>(sbar(s(np* - ? - - - god speed god speed nnp * - god speed - - - ? ?
</prevsent>
<prevsent>*) - ? - - - had have vbd (vp* - had - - had it it prp (advp* - it - - it not not rb *) - not - not - been be vbn (vp* - been - - been so so rb (advp*)))))))) - so - - so . . .
</prevsent>
</prevsection>
<citsent citstr=" W09-1304 ">
*) - - - - - table 1: example sentence annotated for negation following sem shared task 2012 format 2008a; morante and daelemans, 2009; <papid> W09-1304 </papid>agarwal and yu, 2010; morante, 2010; read et al, 2011), most lyon clinical reports.</citsent>
<aftsection>
<nextsent>the reason is that most nlp research in the biomedical domain is interested in automatically extracting factual relations and pieces of information from unstructured data.
</nextsent>
<nextsent>negation detection is important here because information that falls in the scope of negation cue cannot be treated as facts.chapman et al (2001) proposed rule-based algorithm called negex for determining whether afinding or disease mentioned within narrative medical reports is present or absent.
</nextsent>
<nextsent>the algorithm uses regular-expression-based rules.
</nextsent>
<nextsent>mutalik et al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X194">
<title id=" S12-1043.xml">umichigan a conditional random field model for resolving the scope of negation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>morante (2008b) proposed supervised approach for detecting negation cues and their scopes in biomedical text.
</prevsent>
<prevsent>their system consists of twomemory-based engines, one that decides if the tokens in sentence are negation signals, and another one that finds the full scope of these negation signals.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
negation has been also studied in the context of sentiment analysis (wilson et al, 2005; <papid> H05-1044 </papid>jia et al, 2009; councill et al, 2010; <papid> W10-3110 </papid>heerschop et al, 2011;hogenboom et al, 2011).</citsent>
<aftsection>
<nextsent>wiegand et al (2010) <papid> W10-3111 </papid>surveyed the recent work on negation scope detection for sentiment analysis.</nextsent>
<nextsent>wilson et al (2005) <papid> H05-1044 </papid>studied the contextual features that affect text polarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X195">
<title id=" S12-1043.xml">umichigan a conditional random field model for resolving the scope of negation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>morante (2008b) proposed supervised approach for detecting negation cues and their scopes in biomedical text.
</prevsent>
<prevsent>their system consists of twomemory-based engines, one that decides if the tokens in sentence are negation signals, and another one that finds the full scope of these negation signals.
</prevsent>
</prevsection>
<citsent citstr=" W10-3110 ">
negation has been also studied in the context of sentiment analysis (wilson et al, 2005; <papid> H05-1044 </papid>jia et al, 2009; councill et al, 2010; <papid> W10-3110 </papid>heerschop et al, 2011;hogenboom et al, 2011).</citsent>
<aftsection>
<nextsent>wiegand et al (2010) <papid> W10-3111 </papid>surveyed the recent work on negation scope detection for sentiment analysis.</nextsent>
<nextsent>wilson et al (2005) <papid> H05-1044 </papid>studied the contextual features that affect text polarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X196">
<title id=" S12-1043.xml">umichigan a conditional random field model for resolving the scope of negation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>their system consists of twomemory-based engines, one that decides if the tokens in sentence are negation signals, and another one that finds the full scope of these negation signals.
</prevsent>
<prevsent>negation has been also studied in the context of sentiment analysis (wilson et al, 2005; <papid> H05-1044 </papid>jia et al, 2009; councill et al, 2010; <papid> W10-3110 </papid>heerschop et al, 2011;hogenboom et al, 2011).</prevsent>
</prevsection>
<citsent citstr=" W10-3111 ">
wiegand et al (2010) <papid> W10-3111 </papid>surveyed the recent work on negation scope detection for sentiment analysis.</citsent>
<aftsection>
<nextsent>wilson et al (2005) <papid> H05-1044 </papid>studied the contextual features that affect text polarity.</nextsent>
<nextsent>they used machine learning approach in which negation is encoded using several features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X198">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given input in the form of natural language, text-to-text generation system produces natural language output that is subject to set ofconstraints.
</prevsent>
<prevsent>compression systems, for instance, produce shorter sentences.
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
paraphrases, i.e. differing textual realizations of the same meaning, are acrucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (barzilay etal., 1999; <papid> P99-1071 </papid>barzilay, 2003), query expansion (anick and tipirneni, 1999; riezler et al, 2007), <papid> P07-1059 </papid>question answering (mckeown, 1979; <papid> P79-1016 </papid>ravichandran andhovy, 2002), <papid> P02-1006 </papid>sentence compression (cohn and la pata, 2008; <papid> C08-1018 </papid>zhao et al, 2009), <papid> P09-1094 </papid>and simplification (wubben et al, 2012).<papid> P12-1107 </papid></citsent>
<aftsection>
<nextsent>paraphrase collections for text-to-text generation have been extracted from variety of different corpora.
</nextsent>
<nextsent>several approaches relyon bilingual parallel data (bannard and callison-burch, 2005; zhao et al, 2008; <papid> P08-1089 </papid>callison-burch, 2008; ganitkevitch etal., 2011), <papid> D11-1108 </papid>while others leverage distributional methods on monolingual text corpora (lin and pantel,2001; bhagat and ravichandran, 2008).<papid> P08-1077 </papid></nextsent>
<nextsent>so far, how ever, only preliminary studies have been undertaken to combine the information from these two sources (chan et al, 2011).<papid> W11-2504 </papid>in this paper, we describe an extension of ganitkevitch et al (2011)<papid> D11-1108 </papid>s bilingual data-based approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X199">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given input in the form of natural language, text-to-text generation system produces natural language output that is subject to set ofconstraints.
</prevsent>
<prevsent>compression systems, for instance, produce shorter sentences.
</prevsent>
</prevsection>
<citsent citstr=" P07-1059 ">
paraphrases, i.e. differing textual realizations of the same meaning, are acrucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (barzilay etal., 1999; <papid> P99-1071 </papid>barzilay, 2003), query expansion (anick and tipirneni, 1999; riezler et al, 2007), <papid> P07-1059 </papid>question answering (mckeown, 1979; <papid> P79-1016 </papid>ravichandran andhovy, 2002), <papid> P02-1006 </papid>sentence compression (cohn and la pata, 2008; <papid> C08-1018 </papid>zhao et al, 2009), <papid> P09-1094 </papid>and simplification (wubben et al, 2012).<papid> P12-1107 </papid></citsent>
<aftsection>
<nextsent>paraphrase collections for text-to-text generation have been extracted from variety of different corpora.
</nextsent>
<nextsent>several approaches relyon bilingual parallel data (bannard and callison-burch, 2005; zhao et al, 2008; <papid> P08-1089 </papid>callison-burch, 2008; ganitkevitch etal., 2011), <papid> D11-1108 </papid>while others leverage distributional methods on monolingual text corpora (lin and pantel,2001; bhagat and ravichandran, 2008).<papid> P08-1077 </papid></nextsent>
<nextsent>so far, how ever, only preliminary studies have been undertaken to combine the information from these two sources (chan et al, 2011).<papid> W11-2504 </papid>in this paper, we describe an extension of ganitkevitch et al (2011)<papid> D11-1108 </papid>s bilingual data-based approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X200">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given input in the form of natural language, text-to-text generation system produces natural language output that is subject to set ofconstraints.
</prevsent>
<prevsent>compression systems, for instance, produce shorter sentences.
</prevsent>
</prevsection>
<citsent citstr=" P79-1016 ">
paraphrases, i.e. differing textual realizations of the same meaning, are acrucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (barzilay etal., 1999; <papid> P99-1071 </papid>barzilay, 2003), query expansion (anick and tipirneni, 1999; riezler et al, 2007), <papid> P07-1059 </papid>question answering (mckeown, 1979; <papid> P79-1016 </papid>ravichandran andhovy, 2002), <papid> P02-1006 </papid>sentence compression (cohn and la pata, 2008; <papid> C08-1018 </papid>zhao et al, 2009), <papid> P09-1094 </papid>and simplification (wubben et al, 2012).<papid> P12-1107 </papid></citsent>
<aftsection>
<nextsent>paraphrase collections for text-to-text generation have been extracted from variety of different corpora.
</nextsent>
<nextsent>several approaches relyon bilingual parallel data (bannard and callison-burch, 2005; zhao et al, 2008; <papid> P08-1089 </papid>callison-burch, 2008; ganitkevitch etal., 2011), <papid> D11-1108 </papid>while others leverage distributional methods on monolingual text corpora (lin and pantel,2001; bhagat and ravichandran, 2008).<papid> P08-1077 </papid></nextsent>
<nextsent>so far, how ever, only preliminary studies have been undertaken to combine the information from these two sources (chan et al, 2011).<papid> W11-2504 </papid>in this paper, we describe an extension of ganitkevitch et al (2011)<papid> D11-1108 </papid>s bilingual data-based approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X201">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given input in the form of natural language, text-to-text generation system produces natural language output that is subject to set ofconstraints.
</prevsent>
<prevsent>compression systems, for instance, produce shorter sentences.
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
paraphrases, i.e. differing textual realizations of the same meaning, are acrucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (barzilay etal., 1999; <papid> P99-1071 </papid>barzilay, 2003), query expansion (anick and tipirneni, 1999; riezler et al, 2007), <papid> P07-1059 </papid>question answering (mckeown, 1979; <papid> P79-1016 </papid>ravichandran andhovy, 2002), <papid> P02-1006 </papid>sentence compression (cohn and la pata, 2008; <papid> C08-1018 </papid>zhao et al, 2009), <papid> P09-1094 </papid>and simplification (wubben et al, 2012).<papid> P12-1107 </papid></citsent>
<aftsection>
<nextsent>paraphrase collections for text-to-text generation have been extracted from variety of different corpora.
</nextsent>
<nextsent>several approaches relyon bilingual parallel data (bannard and callison-burch, 2005; zhao et al, 2008; <papid> P08-1089 </papid>callison-burch, 2008; ganitkevitch etal., 2011), <papid> D11-1108 </papid>while others leverage distributional methods on monolingual text corpora (lin and pantel,2001; bhagat and ravichandran, 2008).<papid> P08-1077 </papid></nextsent>
<nextsent>so far, how ever, only preliminary studies have been undertaken to combine the information from these two sources (chan et al, 2011).<papid> W11-2504 </papid>in this paper, we describe an extension of ganitkevitch et al (2011)<papid> D11-1108 </papid>s bilingual data-based approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X202">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given input in the form of natural language, text-to-text generation system produces natural language output that is subject to set ofconstraints.
</prevsent>
<prevsent>compression systems, for instance, produce shorter sentences.
</prevsent>
</prevsection>
<citsent citstr=" C08-1018 ">
paraphrases, i.e. differing textual realizations of the same meaning, are acrucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (barzilay etal., 1999; <papid> P99-1071 </papid>barzilay, 2003), query expansion (anick and tipirneni, 1999; riezler et al, 2007), <papid> P07-1059 </papid>question answering (mckeown, 1979; <papid> P79-1016 </papid>ravichandran andhovy, 2002), <papid> P02-1006 </papid>sentence compression (cohn and la pata, 2008; <papid> C08-1018 </papid>zhao et al, 2009), <papid> P09-1094 </papid>and simplification (wubben et al, 2012).<papid> P12-1107 </papid></citsent>
<aftsection>
<nextsent>paraphrase collections for text-to-text generation have been extracted from variety of different corpora.
</nextsent>
<nextsent>several approaches relyon bilingual parallel data (bannard and callison-burch, 2005; zhao et al, 2008; <papid> P08-1089 </papid>callison-burch, 2008; ganitkevitch etal., 2011), <papid> D11-1108 </papid>while others leverage distributional methods on monolingual text corpora (lin and pantel,2001; bhagat and ravichandran, 2008).<papid> P08-1077 </papid></nextsent>
<nextsent>so far, how ever, only preliminary studies have been undertaken to combine the information from these two sources (chan et al, 2011).<papid> W11-2504 </papid>in this paper, we describe an extension of ganitkevitch et al (2011)<papid> D11-1108 </papid>s bilingual data-based approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X203">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given input in the form of natural language, text-to-text generation system produces natural language output that is subject to set ofconstraints.
</prevsent>
<prevsent>compression systems, for instance, produce shorter sentences.
</prevsent>
</prevsection>
<citsent citstr=" P09-1094 ">
paraphrases, i.e. differing textual realizations of the same meaning, are acrucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (barzilay etal., 1999; <papid> P99-1071 </papid>barzilay, 2003), query expansion (anick and tipirneni, 1999; riezler et al, 2007), <papid> P07-1059 </papid>question answering (mckeown, 1979; <papid> P79-1016 </papid>ravichandran andhovy, 2002), <papid> P02-1006 </papid>sentence compression (cohn and la pata, 2008; <papid> C08-1018 </papid>zhao et al, 2009), <papid> P09-1094 </papid>and simplification (wubben et al, 2012).<papid> P12-1107 </papid></citsent>
<aftsection>
<nextsent>paraphrase collections for text-to-text generation have been extracted from variety of different corpora.
</nextsent>
<nextsent>several approaches relyon bilingual parallel data (bannard and callison-burch, 2005; zhao et al, 2008; <papid> P08-1089 </papid>callison-burch, 2008; ganitkevitch etal., 2011), <papid> D11-1108 </papid>while others leverage distributional methods on monolingual text corpora (lin and pantel,2001; bhagat and ravichandran, 2008).<papid> P08-1077 </papid></nextsent>
<nextsent>so far, how ever, only preliminary studies have been undertaken to combine the information from these two sources (chan et al, 2011).<papid> W11-2504 </papid>in this paper, we describe an extension of ganitkevitch et al (2011)<papid> D11-1108 </papid>s bilingual data-based approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X204">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given input in the form of natural language, text-to-text generation system produces natural language output that is subject to set ofconstraints.
</prevsent>
<prevsent>compression systems, for instance, produce shorter sentences.
</prevsent>
</prevsection>
<citsent citstr=" P12-1107 ">
paraphrases, i.e. differing textual realizations of the same meaning, are acrucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (barzilay etal., 1999; <papid> P99-1071 </papid>barzilay, 2003), query expansion (anick and tipirneni, 1999; riezler et al, 2007), <papid> P07-1059 </papid>question answering (mckeown, 1979; <papid> P79-1016 </papid>ravichandran andhovy, 2002), <papid> P02-1006 </papid>sentence compression (cohn and la pata, 2008; <papid> C08-1018 </papid>zhao et al, 2009), <papid> P09-1094 </papid>and simplification (wubben et al, 2012).<papid> P12-1107 </papid></citsent>
<aftsection>
<nextsent>paraphrase collections for text-to-text generation have been extracted from variety of different corpora.
</nextsent>
<nextsent>several approaches relyon bilingual parallel data (bannard and callison-burch, 2005; zhao et al, 2008; <papid> P08-1089 </papid>callison-burch, 2008; ganitkevitch etal., 2011), <papid> D11-1108 </papid>while others leverage distributional methods on monolingual text corpora (lin and pantel,2001; bhagat and ravichandran, 2008).<papid> P08-1077 </papid></nextsent>
<nextsent>so far, how ever, only preliminary studies have been undertaken to combine the information from these two sources (chan et al, 2011).<papid> W11-2504 </papid>in this paper, we describe an extension of ganitkevitch et al (2011)<papid> D11-1108 </papid>s bilingual data-based approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X205">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>paraphrases, i.e. differing textual realizations of the same meaning, are acrucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (barzilay etal., 1999; <papid> P99-1071 </papid>barzilay, 2003), query expansion (anick and tipirneni, 1999; riezler et al, 2007), <papid> P07-1059 </papid>question answering (mckeown, 1979; <papid> P79-1016 </papid>ravichandran andhovy, 2002), <papid> P02-1006 </papid>sentence compression (cohn and la pata, 2008; <papid> C08-1018 </papid>zhao et al, 2009), <papid> P09-1094 </papid>and simplification (wubben et al, 2012).<papid> P12-1107 </papid></prevsent>
<prevsent>paraphrase collections for text-to-text generation have been extracted from variety of different corpora.</prevsent>
</prevsection>
<citsent citstr=" P08-1089 ">
several approaches relyon bilingual parallel data (bannard and callison-burch, 2005; zhao et al, 2008; <papid> P08-1089 </papid>callison-burch, 2008; ganitkevitch etal., 2011), <papid> D11-1108 </papid>while others leverage distributional methods on monolingual text corpora (lin and pantel,2001; bhagat and ravichandran, 2008).<papid> P08-1077 </papid></citsent>
<aftsection>
<nextsent>so far, how ever, only preliminary studies have been undertaken to combine the information from these two sources (chan et al, 2011).<papid> W11-2504 </papid>in this paper, we describe an extension of ganitkevitch et al (2011)<papid> D11-1108 </papid>s bilingual data-based approach.</nextsent>
<nextsent>we augment the bilingually-sourced paraphrases using features based on monolingual distributional similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X206">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>paraphrases, i.e. differing textual realizations of the same meaning, are acrucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (barzilay etal., 1999; <papid> P99-1071 </papid>barzilay, 2003), query expansion (anick and tipirneni, 1999; riezler et al, 2007), <papid> P07-1059 </papid>question answering (mckeown, 1979; <papid> P79-1016 </papid>ravichandran andhovy, 2002), <papid> P02-1006 </papid>sentence compression (cohn and la pata, 2008; <papid> C08-1018 </papid>zhao et al, 2009), <papid> P09-1094 </papid>and simplification (wubben et al, 2012).<papid> P12-1107 </papid></prevsent>
<prevsent>paraphrase collections for text-to-text generation have been extracted from variety of different corpora.</prevsent>
</prevsection>
<citsent citstr=" D11-1108 ">
several approaches relyon bilingual parallel data (bannard and callison-burch, 2005; zhao et al, 2008; <papid> P08-1089 </papid>callison-burch, 2008; ganitkevitch etal., 2011), <papid> D11-1108 </papid>while others leverage distributional methods on monolingual text corpora (lin and pantel,2001; bhagat and ravichandran, 2008).<papid> P08-1077 </papid></citsent>
<aftsection>
<nextsent>so far, how ever, only preliminary studies have been undertaken to combine the information from these two sources (chan et al, 2011).<papid> W11-2504 </papid>in this paper, we describe an extension of ganitkevitch et al (2011)<papid> D11-1108 </papid>s bilingual data-based approach.</nextsent>
<nextsent>we augment the bilingually-sourced paraphrases using features based on monolingual distributional similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X207">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>paraphrases, i.e. differing textual realizations of the same meaning, are acrucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (barzilay etal., 1999; <papid> P99-1071 </papid>barzilay, 2003), query expansion (anick and tipirneni, 1999; riezler et al, 2007), <papid> P07-1059 </papid>question answering (mckeown, 1979; <papid> P79-1016 </papid>ravichandran andhovy, 2002), <papid> P02-1006 </papid>sentence compression (cohn and la pata, 2008; <papid> C08-1018 </papid>zhao et al, 2009), <papid> P09-1094 </papid>and simplification (wubben et al, 2012).<papid> P12-1107 </papid></prevsent>
<prevsent>paraphrase collections for text-to-text generation have been extracted from variety of different corpora.</prevsent>
</prevsection>
<citsent citstr=" P08-1077 ">
several approaches relyon bilingual parallel data (bannard and callison-burch, 2005; zhao et al, 2008; <papid> P08-1089 </papid>callison-burch, 2008; ganitkevitch etal., 2011), <papid> D11-1108 </papid>while others leverage distributional methods on monolingual text corpora (lin and pantel,2001; bhagat and ravichandran, 2008).<papid> P08-1077 </papid></citsent>
<aftsection>
<nextsent>so far, how ever, only preliminary studies have been undertaken to combine the information from these two sources (chan et al, 2011).<papid> W11-2504 </papid>in this paper, we describe an extension of ganitkevitch et al (2011)<papid> D11-1108 </papid>s bilingual data-based approach.</nextsent>
<nextsent>we augment the bilingually-sourced paraphrases using features based on monolingual distributional similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X208">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>paraphrase collections for text-to-text generation have been extracted from variety of different corpora.
</prevsent>
<prevsent>several approaches relyon bilingual parallel data (bannard and callison-burch, 2005; zhao et al, 2008; <papid> P08-1089 </papid>callison-burch, 2008; ganitkevitch etal., 2011), <papid> D11-1108 </papid>while others leverage distributional methods on monolingual text corpora (lin and pantel,2001; bhagat and ravichandran, 2008).<papid> P08-1077 </papid></prevsent>
</prevsection>
<citsent citstr=" W11-2504 ">
so far, how ever, only preliminary studies have been undertaken to combine the information from these two sources (chan et al, 2011).<papid> W11-2504 </papid>in this paper, we describe an extension of ganitkevitch et al (2011)<papid> D11-1108 </papid>s bilingual data-based approach.</citsent>
<aftsection>
<nextsent>we augment the bilingually-sourced paraphrases using features based on monolingual distributional similarity.
</nextsent>
<nextsent>more specifically: ? we show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data.?
</nextsent>
<nextsent>we define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim(one jj instance of np , jj case of np).
</nextsent>
<nextsent>this generalizes over distributional similarity for contiguous phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X211">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in section 2.1 we outline pivot-based paraphrase extraction from bilingual data, while the contextual features used to determine closeness in meaning in monolingual approaches is described in section 2.2.
</prevsent>
<prevsent>2.1 paraphrase extraction via pivoting.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
following ganitkevitch et al (2011)<papid> D11-1108 </papid>, we formulate our paraphrases as syntactically annotated synchronous context-free grammar (scfg) (aho and ullman, 1972; chiang, 2005).<papid> P05-1033 </papid></citsent>
<aftsection>
<nextsent>an scfg rule has the form: = ? f, e,?, ~??,where the left-hand side of the rule, c, is nonterminal and the right-hand sides and are strings of terminal and nonterminal symbols.
</nextsent>
<nextsent>there is aone-to-one correspondency between the nonterminals in and e: each nonterminal symbol in hasto also appear in e. the function ? captures this bi jective mapping between the nonterminals.
</nextsent>
<nextsent>drawing on machine translation terminology, we refer to as the source and as the target side of the rule.each rule is annotated with feature vector of feature functions ~?
</nextsent>
<nextsent>= {1...n} that, using corresponding weight vector ~?, are combined in loglinear model to compute the cost of applying r: cost(r) = ? n? i=1 logi.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X212">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, we estimate the conditional paraphrase probability p(e2|e1) by marginalizing over all shared foreign-language translations : p(e2|e1) = ? p(e2, |e1) (2) = ? p(e2|f, e1)p(f |e1) (3) ? ?
</prevsent>
<prevsent>f p(e2|f)p(f |e1).
</prevsent>
</prevsection>
<citsent citstr=" N12-1078 ">
(4) 1see yao et al (2012) <papid> N12-1078 </papid>for an analysis of this assumption.</citsent>
<aftsection>
<nextsent>257 twelve cartoons insulting the prophet mohammad cd nns jj dt nnp np np vp np dt+nnp 12 the prophet mohammad cd nns jj dt nnp np np vp np dt+nnp cartoons offensive of the that are to figure 3: an example of synchronous paraphras tic derivation, here sentence compression.
</nextsent>
<nextsent>shaded words are deleted in the indicated rule applications.
</nextsent>
<nextsent>figure 2 illustrates syntax-constrained pivoting and feature aggregation over multiple foreign language translations for paraphrase pattern.
</nextsent>
<nextsent>after the scfg has been extracted, it can be used within standard machine translation machinery, such as the joshua decoder (ganitkevitch et al, 2012).<papid> W12-3134 </papid>figure 3 shows an example for synchronous para phrastic derivation produced as result of applying our paraphrase grammar in the decoding process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X213">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>shaded words are deleted in the indicated rule applications.
</prevsent>
<prevsent>figure 2 illustrates syntax-constrained pivoting and feature aggregation over multiple foreign language translations for paraphrase pattern.
</prevsent>
</prevsection>
<citsent citstr=" W12-3134 ">
after the scfg has been extracted, it can be used within standard machine translation machinery, such as the joshua decoder (ganitkevitch et al, 2012).<papid> W12-3134 </papid>figure 3 shows an example for synchronous para phrastic derivation produced as result of applying our paraphrase grammar in the decoding process.</citsent>
<aftsection>
<nextsent>the approach outlined relies on aligned bilingual texts to identify phrases and patterns that are equivalent in meaning.
</nextsent>
<nextsent>when extracting paraphrases from monolingual text, we have to relyon an entirely different set of semantic cues and features.
</nextsent>
<nextsent>2.2 monolingual distributional similarity.
</nextsent>
<nextsent>methods based on monolingual text corpora measure the similarity of phrases based on contextual features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X219">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 baseline paraphrase grammar.
</prevsent>
<prevsent>we extract our paraphrase grammar from thefrenchenglish portion of the europarl corpus (ver sion 5) (koehn, 2005).
</prevsent>
</prevsection>
<citsent citstr=" N06-1014 ">
the berkeley aligner (liang et al, 2006) <papid> N06-1014 </papid>and the berkeley parser (petrov and klein, 2007) <papid> N07-1051 </papid>are used to align the bitext and parse the english side, respectively.</citsent>
<aftsection>
<nextsent>the paraphrase grammar is produced using the hadoop-based thrax 259 the long-term achieve25 goals 23 plans 97 investment 10 confirmed64 revise43 left right the long-term the long-term the long-term the long-term the long-term . . .
</nextsent>
<nextsent>l-achieve = 25 l-confirmed = 64 l-revise = 43 ? r-goals = 23 r-plans = 97 r-investment = 10 ? the long-term ? =~signgram figure 5: an example of the n-gram feature extraction on an n-gram corpus.
</nextsent>
<nextsent>here, the long-term?
</nextsent>
<nextsent>is seen preceded by revise?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X220">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 baseline paraphrase grammar.
</prevsent>
<prevsent>we extract our paraphrase grammar from thefrenchenglish portion of the europarl corpus (ver sion 5) (koehn, 2005).
</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
the berkeley aligner (liang et al, 2006) <papid> N06-1014 </papid>and the berkeley parser (petrov and klein, 2007) <papid> N07-1051 </papid>are used to align the bitext and parse the english side, respectively.</citsent>
<aftsection>
<nextsent>the paraphrase grammar is produced using the hadoop-based thrax 259 the long-term achieve25 goals 23 plans 97 investment 10 confirmed64 revise43 left right the long-term the long-term the long-term the long-term the long-term . . .
</nextsent>
<nextsent>l-achieve = 25 l-confirmed = 64 l-revise = 43 ? r-goals = 23 r-plans = 97 r-investment = 10 ? the long-term ? =~signgram figure 5: an example of the n-gram feature extraction on an n-gram corpus.
</nextsent>
<nextsent>here, the long-term?
</nextsent>
<nextsent>is seen preceded by revise?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X224">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>we extend the feature-set used in ganitkevitch etal.
</prevsent>
<prevsent>(2011) with number of features that aim to better describe rules compressive power: on top of the word count features wcountsrc and wcount tgt and the word count difference feature wcountdiff , we add character based count and difference featuresccountsrc , ccount tgt , and ccountdiff , as well as log compression ratio features wordcr = log wcount tgtwcountsrc and the analogously defined charcr = log ccount tgtccountsrc . for model tuning and decoding we used the joshua machine translation system (ganitkevitch et al., 2012).<papid> W12-3134 </papid></prevsent>
</prevsection>
<citsent citstr=" D11-1125 ">
the model weights are estimated using an implementation of the pro tuning algorithm (hop kins and may, 2011), <papid> D11-1125 </papid>with precis as our objective function (ganitkevitch et al, 2011).<papid> D11-1108 </papid></citsent>
<aftsection>
<nextsent>the language model used in our paraphraser and the clarke andlapata (2008) baseline system is kneser-ney discounted 5-gram model estimated on the gigaword corpus using the srilm toolkit (stolcke, 2002).
</nextsent>
<nextsent>long-term investment holding on to det amod the jj nn vbg in to dt np pp vp ? ?
</nextsent>
<nextsent>the long-term ? =~sigsyntax ? dep-det-r-investment pos-l-to pos-r-nn lex-r-investment lex-l-to dep-amod-r-investment syn-gov-np syn-miss-l-nn lex-l-on-to pos-l-in-to dep-det-r-nn dep-amod-r-nnfigure 6: an example of the syntactic feature set.
</nextsent>
<nextsent>the phrase the long-term?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X227">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>4.3.3 locality sensitive hashing collecting distributional signatures for large number of phrases quickly leads to unmanageablylarge datasets.
</prevsent>
<prevsent>storing the syntax models 12 million signatures in compressed readable format, for instance, requires over 20gb of disk space.
</prevsent>
</prevsection>
<citsent citstr=" P05-1077 ">
like ravichandran et al (2005) <papid> P05-1077 </papid>and bhagat and ravichandran (2008), <papid> P08-1077 </papid>we relyon locality sensitive hashing (lsh) to make the use of these large collections practical.in order to avoid explicitly computing the feature vectors, which can be memory intensive for frequent phrases, we chose the online lsh variant described by van durme and lall (2010), as implemented in the jerboa toolkit (van durme, 2012).</citsent>
<aftsection>
<nextsent>this method, based on the earlier work of indyk and motwani (1998) and chari kar (2002), approximates the cosine similarity between two feature vectors based on the hamming distance in dimensionality reduced bitwise representation.
</nextsent>
<nextsent>two feature vectors u, each of dimension are first projected through db random matrix populated with draws from (0), with draws from (1).
</nextsent>
<nextsent>we then convert the resulting dimensional vectors into bit-vectors by setting eachbit of the signature conditioned on whether the corresponding projected value is less than 0.
</nextsent>
<nextsent>now,given the bit signatures h(~u) and h(~v), we can approximate the cosine similarity of and as: sim ?(u, v) = cos (d(h(~u), h(~v)) pi ) ,where d(?, ?) is the hamming distance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X229">
<title id=" S12-1034.xml">monolingual distributional similarity for texttotext generation </title>
<section> evaluation results.  </section>
<citcontext>
<prevsection>
<prevsent>meaning retention.
</prevsent>
<prevsent>moving to distributional similarity estimated on the syntactic feature-set yields additional improvement, despite the models lower coverage.
</prevsent>
</prevsection>
<citsent citstr=" W11-1610 ">
it is known that human evaluation scores correlate linearly with the compression ratio produced by sentence compression system (napoles et al, 2011).<papid> W11-1610 </papid>thus, to ensure fairness in our comparisons, we produce pairwise comparison breakdown that only takes into account compress ions of almost identical length.2 figure 7 shows the results of this analysis, detailing the number of wins and ties in the human judgements.</citsent>
<aftsection>
<nextsent>we note that the gains in meaning retention over both the baseline and the ilp system are still present in the pairwise breakdown.
</nextsent>
<nextsent>the gains over the paraphrasing baseline, as well as the improvement in meaning over ilp are statistically significant at   0.05 (using the sign test).
</nextsent>
<nextsent>we can observe that there is substantial overlap between the baseline paraphraser and the n-grammodel, while the syntax model appears to yield noticeably different output far more often.
</nextsent>
<nextsent>table 2 shows two example sentences drawn from our test set and the compress ions produced by the different systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X230">
<title id=" S10-1100.xml">hitszcityu combine collocation context words and neighboring sentence sentiment in sentiment adjectives disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hatzivassiloglou and mckeown (1997) predicated the polarity of adjectives by using the pairs of adjectives linked by consecutive or negation conjunctions.
</prevsent>
<prevsent>turney and littman (2003) determined the polarity of sentiment words by estimating the point-wise mutual information between sentiment words and set of seed words with strong polarity.
</prevsent>
</prevsection>
<citsent citstr=" E06-1027 ">
andreevskaia and bergler (2006) <papid> E06-1027 </papid>used sentiment tag extraction program to extract sentiment bearing adjectives from wordnet.</citsent>
<aftsection>
<nextsent>esuli and seb asian (2006) studied the context-dependent sentiment words in wordnet but ignored the instances in real context.
</nextsent>
<nextsent>wu et al (2008) applied collocation plus svm classifier in chinese sentiment adjectives disambiguation.
</nextsent>
<nextsent>xu et al (2008) proposed semi-supervised learning algorithm to learn new sentiment word and their context dependent characteristics.
</nextsent>
<nextsent>semeval-2 task 18 is designed to provide common framework and dataset for evaluating the disambiguation techniques for chinese sentiment adjectives.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X231">
<title id=" S12-1061.xml">soft cardinality a parameterized similarity function for text comparison </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>particularly in ai, similarity measures play an important role in the construction of intelligent systems that are required to exhibit behavior similar to humans.
</prevsent>
<prevsent>for instance, in the field of natural language processing, text similarity functions provide estimates of the human similarity judgments related to language.
</prevsent>
</prevsection>
<citsent citstr=" W05-1203 ">
in this paper, we combine elements from the perspective of cognitive psychology and computer science to propose model for building similarity functions suitable for the task of semantic text sim ilarity.we identify four main families of text similarity functions: i) resemblance coefficients based on sets (e.g. jac cards (1901) and dices (1945) coefficients) ii) functions in metric spaces (e.g. cosine tf-idf similarity (salton et al., 1975)); iii) the edit distance family of measures (e.g. leven stein (1966) distance, lcs (hirschberg, 1977));and iv) hybrid approaches ((monge and elkan, 1996; cohen et al, 2003; corley and mihalcea, 2005; <papid> W05-1203 </papid>jimenez et al., 2010)).</citsent>
<aftsection>
<nextsent>all of these measures use subdivision of the texts in different granularity levels, such as q-grams of words, words, q-grams of characters, syllables, and characters.
</nextsent>
<nextsent>among hybrid approaches, monge-elkans measure and soft cardinality methods are recursive and can be used to build similarity functions at any arbitrary range of granularity.
</nextsent>
<nextsent>for instance, it is possible to construct similarity function to compare sentences based on function that compares words, which in turn can be constructed based on function that compares bigrams of characters.
</nextsent>
<nextsent>furthermore, hybrid approaches can integrate similarity functions that are not based on the representation of the surface of text, such as semantic relatedness measures (pedersen et al, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X232">
<title id=" S12-1061.xml">soft cardinality a parameterized similarity function for text comparison </title>
<section> experimental setup and results.  </section>
<citcontext>
<prevsection>
<prevsent>each dataset consist of set of pairs of text annotated with human similarity judgments on scale of 0 to 5.
</prevsent>
<prevsent>each similarity judgment is the average of the judgments provided by 5 human judges.
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
for comprehensible description of the task see(agirre et al, 2012).<papid> S12-1051 </papid></citsent>
<aftsection>
<nextsent>for the experiments, all datasets were pre-processedby converting to lowercase characters, english stop words removal and stemming using porter stemmer(porter, 1980).
</nextsent>
<nextsent>the performance measure used for all experiments was the pearson correlation r. 451 4.1 model parameters.
</nextsent>
<nextsent>in order to make an initial exploration of the parameters in table 1, we set q1 = 2 (i.e. bigrams) and used wai = idf(ai).
</nextsent>
<nextsent>for other parameters, we started with all the non-effect values, i.e. ? = 0.5, bias = 0, = 1, sim = 0.5 and biassim = 0.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X233">
<title id=" P98-2184.xml">how verb subcategorization frequencies are affected by corpus choice </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P96-1025 ">
the probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and super taggers (charniak 1995, collins 1996/<papid> P96-1025 </papid>1997, joshi and srinivas 1994, <papid> C94-1024 </papid>kim, srinivas, and true swell 1997, stolcke et al 1997), and in psychological theories of language processing (clifton et al 1984, ferfeira &amp; mcclure 1997, gamsey et al 1997, jurafsky 1996, macdonald 1994, mitchell &amp; holmes 1985, tanenhaus et al 1990, true swell et al 1993).</citsent>
<aftsection>
<nextsent>these probabilities are computed in very different ways by the two sets of researchers.
</nextsent>
<nextsent>psychological studies use methods such as sentence completion and sentence production for collecting verb argument structure probabilities.
</nextsent>
<nextsent>in sentence completion, subjects are asked to complete sentence fragment.
</nextsent>
<nextsent>garnsey at al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X234">
<title id=" P98-2184.xml">how verb subcategorization frequencies are affected by corpus choice </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" C94-1024 ">
the probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and super taggers (charniak 1995, collins 1996/<papid> P96-1025 </papid>1997, joshi and srinivas 1994, <papid> C94-1024 </papid>kim, srinivas, and true swell 1997, stolcke et al 1997), and in psychological theories of language processing (clifton et al 1984, ferfeira &amp; mcclure 1997, gamsey et al 1997, jurafsky 1996, macdonald 1994, mitchell &amp; holmes 1985, tanenhaus et al 1990, true swell et al 1993).</citsent>
<aftsection>
<nextsent>these probabilities are computed in very different ways by the two sets of researchers.
</nextsent>
<nextsent>psychological studies use methods such as sentence completion and sentence production for collecting verb argument structure probabilities.
</nextsent>
<nextsent>in sentence completion, subjects are asked to complete sentence fragment.
</nextsent>
<nextsent>garnsey at al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X235">
<title id=" P98-2184.xml">how verb subcategorization frequencies are affected by corpus choice </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>(1984).
</prevsent>
<prevsent>an alternative to these psychological methods is to use corpus data.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
this can be done automatically with un parsed corpora (briscoe and carroll 1997, <papid> A97-1052 </papid>manning 1993, <papid> P93-1032 </papid>ushioda et al 1993), <papid> W93-0109 </papid>from parsed corpora such as marcus et al (1993) <papid> J93-2004 </papid>treebank (merlo 1994, framis 1994) or manually as was done for comlex (macleod and grishman 1994).</citsent>
<aftsection>
<nextsent>the advantage of any of these corpus methods is the much greater amount of data that can be used, and the much more natural contexts.
</nextsent>
<nextsent>this seems to make it preferable to data generated in psychological studies.
</nextsent>
<nextsent>recent studies (merlo 1994, gibson et al 1996) have found differences between corpus frequencies and experimental measures.
</nextsent>
<nextsent>this suggests that corpus-based frequencies and experiment-based frequencies may not be interchangeable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X236">
<title id=" P98-2184.xml">how verb subcategorization frequencies are affected by corpus choice </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>(1984).
</prevsent>
<prevsent>an alternative to these psychological methods is to use corpus data.
</prevsent>
</prevsection>
<citsent citstr=" P93-1032 ">
this can be done automatically with un parsed corpora (briscoe and carroll 1997, <papid> A97-1052 </papid>manning 1993, <papid> P93-1032 </papid>ushioda et al 1993), <papid> W93-0109 </papid>from parsed corpora such as marcus et al (1993) <papid> J93-2004 </papid>treebank (merlo 1994, framis 1994) or manually as was done for comlex (macleod and grishman 1994).</citsent>
<aftsection>
<nextsent>the advantage of any of these corpus methods is the much greater amount of data that can be used, and the much more natural contexts.
</nextsent>
<nextsent>this seems to make it preferable to data generated in psychological studies.
</nextsent>
<nextsent>recent studies (merlo 1994, gibson et al 1996) have found differences between corpus frequencies and experimental measures.
</nextsent>
<nextsent>this suggests that corpus-based frequencies and experiment-based frequencies may not be interchangeable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X237">
<title id=" P98-2184.xml">how verb subcategorization frequencies are affected by corpus choice </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>(1984).
</prevsent>
<prevsent>an alternative to these psychological methods is to use corpus data.
</prevsent>
</prevsection>
<citsent citstr=" W93-0109 ">
this can be done automatically with un parsed corpora (briscoe and carroll 1997, <papid> A97-1052 </papid>manning 1993, <papid> P93-1032 </papid>ushioda et al 1993), <papid> W93-0109 </papid>from parsed corpora such as marcus et al (1993) <papid> J93-2004 </papid>treebank (merlo 1994, framis 1994) or manually as was done for comlex (macleod and grishman 1994).</citsent>
<aftsection>
<nextsent>the advantage of any of these corpus methods is the much greater amount of data that can be used, and the much more natural contexts.
</nextsent>
<nextsent>this seems to make it preferable to data generated in psychological studies.
</nextsent>
<nextsent>recent studies (merlo 1994, gibson et al 1996) have found differences between corpus frequencies and experimental measures.
</nextsent>
<nextsent>this suggests that corpus-based frequencies and experiment-based frequencies may not be interchangeable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X238">
<title id=" P98-2184.xml">how verb subcategorization frequencies are affected by corpus choice </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>(1984).
</prevsent>
<prevsent>an alternative to these psychological methods is to use corpus data.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
this can be done automatically with un parsed corpora (briscoe and carroll 1997, <papid> A97-1052 </papid>manning 1993, <papid> P93-1032 </papid>ushioda et al 1993), <papid> W93-0109 </papid>from parsed corpora such as marcus et al (1993) <papid> J93-2004 </papid>treebank (merlo 1994, framis 1994) or manually as was done for comlex (macleod and grishman 1994).</citsent>
<aftsection>
<nextsent>the advantage of any of these corpus methods is the much greater amount of data that can be used, and the much more natural contexts.
</nextsent>
<nextsent>this seems to make it preferable to data generated in psychological studies.
</nextsent>
<nextsent>recent studies (merlo 1994, gibson et al 1996) have found differences between corpus frequencies and experimental measures.
</nextsent>
<nextsent>this suggests that corpus-based frequencies and experiment-based frequencies may not be interchangeable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X241">
<title id=" P98-2184.xml">how verb subcategorization frequencies are affected by corpus choice </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>27% of the examples of these verbs were mis-classified, always by failing to find quote-type argument of the verb.
</prevsent>
<prevsent>using separate search strings for these verbs would greatly improve the accuracy of these searches.
</prevsent>
</prevsection>
<citsent citstr=" P98-1071 ">
our eventual goal is to develop set of regular expressions that work on fiat tagged corpora instead of treebank parsed structures to allow us to gather information from larger corpora than have been done by the treebank project (see manning 1993 <papid> P93-1032 </papid>and gahl 1998).<papid> P98-1071 </papid></citsent>
<aftsection>
<nextsent>we find that there are significant differences between the verb subcategorization frequencies generated through experimental methods and corpus methods, and between the frequencies found in different corpora.
</nextsent>
<nextsent>we have identified two distinct sources for these differences.
</nextsent>
<nextsent>discourse influences are caused by the changes in the ways language is used in different discourse types and are to some extent predictable from the discourse type of the corpus in question.
</nextsent>
<nextsent>semantic influences are based on the semantic ontext of the discourse.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X242">
<title id=" S12-1095.xml">deriupm pushing corpus based relatedness to similarity shared task system description </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one plausible limitation of existing methods for sentence similarity is their adaptation from long text (e.g. documents) similarity methods, where word co-occurrence plays significant role.
</prevsent>
<prevsent>however,sentences are too short, thats why taking syntactic role of each word with its narrow semantic meaning into account, can be highly relevant to reflect the semantic equivalence of two sentences.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
these narrow semantics can be reflected from any existing large lexicons [(wu and palmer, 1994) <papid> P94-1019 </papid>and (lin, 1998)]; nevertheless, these lexicons can not provide the semantics of words which are out of lexicon (e.g. guy) or multiword expressions.</citsent>
<aftsection>
<nextsent>these semantics can be represented by large distributed semantic space such as wikipedia and similarity can be reflected by relatedness of these extracted semantics.
</nextsent>
<nextsent>however, relatedness covers broader space than similarity, which forced us to tune the wikipedia based relatedness with lexical structure (e.g. wordnet) based similarities driven by linguistic syntactic structure, in reflecting more sophisticated similarity of two given sentences.
</nextsent>
<nextsent>in this work, we present sentence similarity using esa and syntactic similarities.
</nextsent>
<nextsent>the rest of this paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X243">
<title id=" S12-1095.xml">deriupm pushing corpus based relatedness to similarity shared task system description </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>(oliva et. al, 2011) present similarity measure for sentences and short text that takes syntactic information, such as morphology and parsing tree, into account and calculate similarities between words with same syntactic role, by using wordnet.
</prevsent>
<prevsent>our work takes inspiration from existing approaches that exploit combination of wikipedia based relatedness with lexical structure based similarities driven by linguistic syntactic structure.
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
we implemented two approaches for the sts task [(agirre et. al, 2012)].<papid> S12-1051 </papid></citsent>
<aftsection>
<nextsent>first approach is fusion of corpus-based semantic relatedness and knowledge-based semantic similarity measures.
</nextsent>
<nextsent>the core of this combination is the corpus-based measure because the combination includes the corpus-based semantic relatedness score over the whole sentences and the knowledge-based semantic similarity scores for the words falling under the same syntactic roles in both the sentences.
</nextsent>
<nextsent>machine learning models are trained by taking all these scores as different features.
</nextsent>
<nextsent>for the submission, we used linear regression and bagging models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X244">
<title id=" P99-1006.xml">discourse relations a structural and presuppositional account using lexicalised tag </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>research on discourse structure has, by and large, attempted to associate all meaningful relations between propositions with structural connections between discourse clauses (syntactic clauses or structures composed of them).
</prevsent>
<prevsent>recognising that this could mean multiple structural connections between clauses, rhetorical structure theory (mann and thompson, 1988) simply stipulates that only single relation may hold.
</prevsent>
</prevsection>
<citsent citstr=" J92-4007 ">
moore and pollack (1992) <papid> J92-4007 </papid>argue that both informational (semantic) and inten-tional relations can hold between clauses imultan- eously and independently.</citsent>
<aftsection>
<nextsent>this suggests that factor-ing the two kinds of relations might lead to pair of structures, each still with no more than single structural connection between any two clauses.
</nextsent>
<nextsent>but examples of multiple semantic relations are easy to find (webber et al, 1999).
</nextsent>
<nextsent>having struc-ture account for all of them leads to the complex-ities shown in figure 1, including the crossing de-pendencies shown in fig.
</nextsent>
<nextsent>l c. these structures are no longer trees, making it difficult to define com-positional semantics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X245">
<title id=" P99-1006.xml">discourse relations a structural and presuppositional account using lexicalised tag </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the idea of combining compositional semantics with defeasible inference is not new.
</prevsent>
<prevsent>neither is the idea of taking certain lexical items as anaphoric ally presupposing an eventuality or set of eventualities: it is implicit in all work on the anaphoric nature of tense (cf.
</prevsent>
</prevsection>
<citsent citstr=" J88-2006 ">
partee (1984), webber (1988), <papid> J88-2006 </papid>inter alia) and modality (stone, 1999).</citsent>
<aftsection>
<nextsent>what is new is the way we enable anaphoric pre supposition to contribute to semantic relations and modal operators, in way 41 ci ci (a) r1 2 i i k ci i k m (b) (c) figure 1: multiple semantic links (r ) between discourse clauses (ci) : (a) back to the same discourse clause; (b) back to different discourse clauses; (c) back to different discourse clauses, with crossing dependencies.
</nextsent>
<nextsent>that does not lead to the violations of tree structure mentioned earlier.t we discuss these differences in more detail in section 2, after describing the lexicalised frame-work that facilitates the derivation of discourse se-mantics from structure, inference and anaphoric presuppositions.
</nextsent>
<nextsent>sections 3 and 4 then present more detailed semantic analyses of the connectives forex- ample and otherwise.
</nextsent>
<nextsent>finally, in section 5, we sum-marize our arguments for the approach and suggest program of future work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X246">
<title id=" P99-1006.xml">discourse relations a structural and presuppositional account using lexicalised tag </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>sections 3 and 4 then present more detailed semantic analyses of the connectives forex- ample and otherwise.
</prevsent>
<prevsent>finally, in section 5, we sum-marize our arguments for the approach and suggest program of future work.
</prevsent>
</prevsection>
<citsent citstr=" P97-1012 ">
in previous papers (cristea and webber, 1997; <papid> P97-1012 </papid>webber and joshi, 1998; <papid> W98-0315 </papid>webber et al, 1999), we have argued for using the more complex structures (elementary trees) of lexicalized tree-adjoining grammar (ltag) and its operations (adjoining and substitution) to associate structure and semantics with sequence of discourse clauses.</citsent>
<aftsection>
<nextsent>2 here we briefly review how it works.
</nextsent>
<nextsent>in lexicalized tag, each elementary tree has at least one anchor.
</nextsent>
<nextsent>in the case of discourse, the an-chor for an elementary tree may be lexical item, punctuation or feature structure that is lexically null.
</nextsent>
<nextsent>the semantic ontribution of lexical anchor includes both what it presupposes and what it as-serts (stone and doran, 1997; <papid> P97-1026 </papid>stone, 1998; stone and webber, 1998).<papid> W98-1419 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X247">
<title id=" P99-1006.xml">discourse relations a structural and presuppositional account using lexicalised tag </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>sections 3 and 4 then present more detailed semantic analyses of the connectives forex- ample and otherwise.
</prevsent>
<prevsent>finally, in section 5, we sum-marize our arguments for the approach and suggest program of future work.
</prevsent>
</prevsection>
<citsent citstr=" W98-0315 ">
in previous papers (cristea and webber, 1997; <papid> P97-1012 </papid>webber and joshi, 1998; <papid> W98-0315 </papid>webber et al, 1999), we have argued for using the more complex structures (elementary trees) of lexicalized tree-adjoining grammar (ltag) and its operations (adjoining and substitution) to associate structure and semantics with sequence of discourse clauses.</citsent>
<aftsection>
<nextsent>2 here we briefly review how it works.
</nextsent>
<nextsent>in lexicalized tag, each elementary tree has at least one anchor.
</nextsent>
<nextsent>in the case of discourse, the an-chor for an elementary tree may be lexical item, punctuation or feature structure that is lexically null.
</nextsent>
<nextsent>the semantic ontribution of lexical anchor includes both what it presupposes and what it as-serts (stone and doran, 1997; <papid> P97-1026 </papid>stone, 1998; stone and webber, 1998).<papid> W98-1419 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X250">
<title id=" P99-1006.xml">discourse relations a structural and presuppositional account using lexicalised tag </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>in lexicalized tag, each elementary tree has at least one anchor.
</prevsent>
<prevsent>in the case of discourse, the an-chor for an elementary tree may be lexical item, punctuation or feature structure that is lexically null.
</prevsent>
</prevsection>
<citsent citstr=" P97-1026 ">
the semantic ontribution of lexical anchor includes both what it presupposes and what it as-serts (stone and doran, 1997; <papid> P97-1026 </papid>stone, 1998; stone and webber, 1998).<papid> W98-1419 </papid></citsent>
<aftsection>
<nextsent>a feature structure anchor will either unify with lexical item with compatible fea-tures (knott and mellish, 1996), yielding the previ-ous case, or have an empty realisation, though one 1one may still need to admit structures having both link back and link forward to different clauses (gardent, 1997).
</nextsent>
<nextsent>but similar situation can occur within the clause, with rel-ative clause dependencies - from the verb back to the relative pronoun and forward to trace - so the possibility is not unmo-tivated from the perspective of syntax.
</nextsent>
<nextsent>2we take this to be only the most basic level of discourse structure, producing what are essentially extended descriptions of situations/events.
</nextsent>
<nextsent>discourse may be further structured with respect speaker intentions, genre-specific presentations, etc. that maintains its semantic features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X251">
<title id=" P99-1006.xml">discourse relations a structural and presuppositional account using lexicalised tag </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>in lexicalized tag, each elementary tree has at least one anchor.
</prevsent>
<prevsent>in the case of discourse, the an-chor for an elementary tree may be lexical item, punctuation or feature structure that is lexically null.
</prevsent>
</prevsection>
<citsent citstr=" W98-1419 ">
the semantic ontribution of lexical anchor includes both what it presupposes and what it as-serts (stone and doran, 1997; <papid> P97-1026 </papid>stone, 1998; stone and webber, 1998).<papid> W98-1419 </papid></citsent>
<aftsection>
<nextsent>a feature structure anchor will either unify with lexical item with compatible fea-tures (knott and mellish, 1996), yielding the previ-ous case, or have an empty realisation, though one 1one may still need to admit structures having both link back and link forward to different clauses (gardent, 1997).
</nextsent>
<nextsent>but similar situation can occur within the clause, with rel-ative clause dependencies - from the verb back to the relative pronoun and forward to trace - so the possibility is not unmo-tivated from the perspective of syntax.
</nextsent>
<nextsent>2we take this to be only the most basic level of discourse structure, producing what are essentially extended descriptions of situations/events.
</nextsent>
<nextsent>discourse may be further structured with respect speaker intentions, genre-specific presentations, etc. that maintains its semantic features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X258">
<title id=" P99-1006.xml">discourse relations a structural and presuppositional account using lexicalised tag </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>thus factored approach as better chance of providing cross-linguistic account of discourse than one that relies on single premise.
</prevsent>
<prevsent>clearly, more remains to be done.
</prevsent>
</prevsection>
<citsent citstr=" W98-0304 ">
first, the ap-proach demands precise semantics for connect-ives, as in the work of grote (1998), <papid> W98-0304 </papid>grote et al (1997), jayez and rossari (1998) and lagerwerf (1998).</citsent>
<aftsection>
<nextsent>secondly, the approach demands an understand-ing of the attentional characteristics of presupposi- tions.
</nextsent>
<nextsent>in particular, preliminary study seems to sug-gest that p-bearing elements differ in what source can license them, where this source can be located, and what can act as dis tractors for this source.
</nextsent>
<nextsent>in fact, these differences seem to resemble the range of differences in the information status (prince, 1981; prince, 1992) or familiarity (gundel et al, 1993) of referential nps.
</nextsent>
<nextsent>consider, for example: (11 ) got in my old volvo and set off to drive cross- country and see as many different mountain ranges as possible.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X259">
<title id=" S12-1093.xml">fcc three approaches for semantic textual similarity </title>
<section> experimentation setup.  </section>
<citcontext>
<prevsection>
<prevsent>(3) (mihalcea et al, 2006).
</prevsent>
<prevsent>similarity(s1, s2) = 12 ( ? w?{s1} (maxsim(w,s2)idf(w)) ? w?{s1} idf(w) + ? w?{s2} (maxsim(w,s1)idf(w)) ? w?{s2} idf(w) ) (3) where idf(w) is the inverse document frequency of the word w, and maxsim(w, s2) is the maximum lexical similarity between the word in sentence s2 and all the words in sentence s2 calculated by means of the eq.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
(4) reported by (wu and palmer, 1994).<papid> P94-1019 </papid></citsent>
<aftsection>
<nextsent>the sentence terms are assumed to be concepts, lcs is the depth of the least common subsumer, and the equation is calculated using the nltk libraries1.
</nextsent>
<nextsent>simwup = 2 ? depth(lcs) depth(concept1) + depth(concept2) (4) 2.3 approach buap-run-3: random.
</nextsent>
<nextsent>indexing and bag of concepts the vector space model (vsm) for document representation supporting search is probably the most well-known ir model.
</nextsent>
<nextsent>the vsm assumes that term vectors are pair-wise orthogonal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X260">
<title id=" S12-1093.xml">fcc three approaches for semantic textual similarity </title>
<section> experimentation setup.  </section>
<citcontext>
<prevsection>
<prevsent>as an alternative, there is vector space methodology called random indexing (ri) (sahlgren, 2005), which presents an efficient, scalable, and incremental method for building context vectors.
</prevsent>
<prevsent>its computational complexity is (nr) where is as previously described and is the vector dimension.
</prevsent>
</prevsection>
<citsent citstr=" C04-1070 ">
particularly, we apply ri to capture the inherent semantic structure using bag of concepts representation (boc) as proposed by sahlgren and coster (sahlgren and coster, 2004), <papid> C04-1070 </papid>where the meaning of term is considered as the sum of contexts in which it occurs.</citsent>
<aftsection>
<nextsent>1http://www.nltk.org/ 632 2.3.1 random indexing random indexing (ri) is vector space methodology that accumulates context vectors for words based on co-occurrence data.
</nextsent>
<nextsent>the technique can be described as: ? first unique random representation known asindex vector is assigned to each context (docu ment).
</nextsent>
<nextsent>index vectors are binary vectors with small number of non-zero elements, which are either +1 or -1, with equal amounts of both.
</nextsent>
<nextsent>for example, if the index vectors have twentynon-zero elements in 1024-dimensional vector space, they have ten +1s and ten -1s.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X264">
<title id=" S12-1042.xml">uio 2 sequence labeling negation using dependency features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for our system these two scope level scores are identical and so are not duplicated in our reporting.
</prevsent>
<prevsent>additionally we chose not to optimize for the scope tokens measure, and hence this is also not reported as development result.note also that the official evaluation actually includes two different variants of the metrics mentioned above; set of primary measures with precision computed as p=tp/(tp+fp) and set of measures where precision is rather computed asp=tp/sys, where sys is the total number of predictions made by the system.
</prevsent>
</prevsection>
<citsent citstr=" S12-1041 ">
the reason why sys is not identical with tp+fp is that partial matches are 1note that the cue classifier applied in the current paper is the same as that used in the other shared task submission from the university of oslo (read et al, 2012), <papid> S12-1041 </papid>and the two system descriptions will therefore have much overlap on this particular point.</citsent>
<aftsection>
<nextsent>for all other components the architectures of the two system are completely different, however.
</nextsent>
<nextsent>only counted as fns (and not fps) in order to avoid double penalties.
</nextsent>
<nextsent>we do not report the measures for development testing as they were introduced for the final evaluation and hence were not considered in our system optimization.
</nextsent>
<nextsent>we note though, that therelative-ranking of participating systems for the primary and measures is identical, and that the correlation between the paired lists of scores is nearly perfect (r=0.997).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X265">
<title id=" S12-1042.xml">uio 2 sequence labeling negation using dependency features </title>
<section> cue detection.  </section>
<citcontext>
<prevsection>
<prevsent>identification of negation cues is based on the light weight classification scheme presented by velldal et al.
</prevsent>
<prevsent>(2012).
</prevsent>
</prevsection>
<citsent citstr=" J12-2005 ">
by treating the set of cue words as closed class, velldal et al (2012) <papid> J12-2005 </papid>showed that one could greatly reduce the number of examples presented to the learner, and correspondingly the number of features, while at the same time improvingperformance.</citsent>
<aftsection>
<nextsent>this means that the classifier only attempts to disambiguate?
</nextsent>
<nextsent>known cue words while ignoring any words not observed as cues in the training data.
</nextsent>
<nextsent>the classifier applied in the current submission is extended to also handle affixal negation cues, such as the prefix cue in impatience, the infix in carelessness, and the suffix of colourless.
</nextsent>
<nextsent>the typesof negation affixes observed in cdtd are; the prefixes un, dis, ir, im, and in; the infix less (we internally treat this as the suffixes lessly and lessness); and the suffix less.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X266">
<title id=" S12-1042.xml">uio 2 sequence labeling negation using dependency features </title>
<section> scope and event resolution.  </section>
<citcontext>
<prevsection>
<prevsent>among the fns, two are due to mwcs not covered by our heuristics (e.g.,no more), while the remaining errors concern affixes, including one in an interesting context of double negation; not dissatisfied.
</prevsent>
<prevsent>in this work, we model negation scope resolution as special instance of the classical iob (inside, outside, begin) sequence labeling problem, where negation cues are labeled to be sequence starters and scopes and events as two different kinds of chunks.
</prevsent>
</prevsection>
<citsent citstr=" W09-1105 ">
crfs allow the computation of p(x|y), wherex isa sequence of labels andy is sequence of observations, and have already been shown to be efficient in similar, albeit less involved, tasks of negation scope resolution (morante and daelemans, 2009; <papid> W09-1105 </papid>councill et al, 2010).<papid> W10-3110 </papid></citsent>
<aftsection>
<nextsent>we employ the crf implementation in the wapiti toolkit, using default settings (lavergne et al, 2010).<papid> P10-1052 </papid></nextsent>
<nextsent>a number of features were used to create the models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X267">
<title id=" S12-1042.xml">uio 2 sequence labeling negation using dependency features </title>
<section> scope and event resolution.  </section>
<citcontext>
<prevsection>
<prevsent>among the fns, two are due to mwcs not covered by our heuristics (e.g.,no more), while the remaining errors concern affixes, including one in an interesting context of double negation; not dissatisfied.
</prevsent>
<prevsent>in this work, we model negation scope resolution as special instance of the classical iob (inside, outside, begin) sequence labeling problem, where negation cues are labeled to be sequence starters and scopes and events as two different kinds of chunks.
</prevsent>
</prevsection>
<citsent citstr=" W10-3110 ">
crfs allow the computation of p(x|y), wherex isa sequence of labels andy is sequence of observations, and have already been shown to be efficient in similar, albeit less involved, tasks of negation scope resolution (morante and daelemans, 2009; <papid> W09-1105 </papid>councill et al, 2010).<papid> W10-3110 </papid></citsent>
<aftsection>
<nextsent>we employ the crf implementation in the wapiti toolkit, using default settings (lavergne et al, 2010).<papid> P10-1052 </papid></nextsent>
<nextsent>a number of features were used to create the models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X268">
<title id=" S12-1042.xml">uio 2 sequence labeling negation using dependency features </title>
<section> scope and event resolution.  </section>
<citcontext>
<prevsection>
<prevsent>in this work, we model negation scope resolution as special instance of the classical iob (inside, outside, begin) sequence labeling problem, where negation cues are labeled to be sequence starters and scopes and events as two different kinds of chunks.
</prevsent>
<prevsent>crfs allow the computation of p(x|y), wherex isa sequence of labels andy is sequence of observations, and have already been shown to be efficient in similar, albeit less involved, tasks of negation scope resolution (morante and daelemans, 2009; <papid> W09-1105 </papid>councill et al, 2010).<papid> W10-3110 </papid></prevsent>
</prevsection>
<citsent citstr=" P10-1052 ">
we employ the crf implementation in the wapiti toolkit, using default settings (lavergne et al, 2010).<papid> P10-1052 </papid></citsent>
<aftsection>
<nextsent>a number of features were used to create the models.
</nextsent>
<nextsent>in addition to the information provided for each token in the cd corpus (lemma, part of speech and constituent), we extracted both left and right token distance to the closest negation cue.
</nextsent>
<nextsent>features were expanded to include forward and backward bigrams and trigrams on both token and pos level, as well as lexicalized pos unigrams andbigrams2.
</nextsent>
<nextsent>table 2 presents complete list of features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X269">
<title id=" S12-1042.xml">uio 2 sequence labeling negation using dependency features </title>
<section> scope and event resolution.  </section>
<citcontext>
<prevsection>
<prevsent>the latter feature proved to be more effective than the former when not used together; using them in conjunction seemed to confuse the model, thus the final model utilizes only bd.
</prevsent>
<prevsent>we furthermore use the dependency graph path (dgp) as feature.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
this feature was inspired by the parse tree path feature presented in gildea and jurafsky (2002) <papid> J02-3001 </papid>in the context of semantic role labeling.</citsent>
<aftsection>
<nextsent>it represents thepath traversed from each token to the cue, encoding both the dependency relations and the direction of the arc that is traversed: for instance, the relation between our and no in figure 1 is described as  poss  dobj  nsubj  det. like councill et al.
</nextsent>
<nextsent>(2010), we also encode the pos of the first and second order syntactic head of each token.
</nextsent>
<nextsent>for the token no in figure 1, for instance, we record the pos of one and escaped, respectively.
</nextsent>
<nextsent>3.2 model-internal representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X271">
<title id=" S10-1037.xml">buap an unsupervised approach to automatic key phrase extraction from scientific articles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the obtained results were also compared against three different base lines: one unsupervised (tf-idf based) and two supervised (nave bayes and maximum entropy).
</prevsent>
<prevsent>the task of automatic key phrase extraction has been studied for several years.
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
firstly, as semantic meta data useful for tasks such as summarization (barzilay and elhadad, 1997; <papid> W97-0703 </papid>lawrie et al, 2001;davanzo and magnini, 2005), but later recognizing the impact that good keyphraseswould have on the quality of various natural language processing (nlp) applications (frank et al, 1999; witten et al, 1999; turney, 1999; barker and corrnacchia, 2000; medelyan and witten, 2008).</citsent>
<aftsection>
<nextsent>thus, the selection of important, topical phrases from within the body of document may be used in order to improve the performance of systems dealing with different nlp problems such as, clustering, question-answering, named entity recognition, information retrieval, etc. in general, key phrase may be considered as sequence of one or more words that capture the main topic of the document, as that key phrase is expected to represent one of the key ideas expressed by the document author.
</nextsent>
<nextsent>following the previously mentioned hypothesis, we may take advantage of two different techniques of text analysis: maximal frequent sequences to extract sequence of one or more words from given text, and page ranking, expecting to extract those word sequences that represent the key ideas of the author.
</nextsent>
<nextsent>the interest on extracting high quality key phrases from raw text has motivated forums, such as semeval, where different systems may evaluate their performances.
</nextsent>
<nextsent>the purpose of semeval is to evaluate semantic analysis systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X272">
<title id=" S10-1037.xml">buap an unsupervised approach to automatic key phrase extraction from scientific articles </title>
<section> description of the approach.  </section>
<citcontext>
<prevsection>
<prevsent>jin(v ) 1 |out(v )| s(v ) (1)where is damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from given vertex to another random vertex in the graph.
</prevsent>
<prevsent>this factor is usually set to 0.85 (brin and page, 1998).
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
there are some other propossals, like the one presented in (mihalcea and tarau, 2004), <papid> W04-3252 </papid>where text ranking algorithm is presented.</citsent>
<aftsection>
<nextsent>the authors consider weighted version of page rank and present some applications to nlp using unigrams.
</nextsent>
<nextsent>they also construct multi-word terms by exploring the conections among ranked words in the graph.
</nextsent>
<nextsent>our algorithm differs from text ranking in that we use mfs for feeding the page ranking algorithm.
</nextsent>
<nextsent>2.3 algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X273">
<title id=" S12-1008.xml">detecting text reuse with modified and weighted ngrams </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>text reuse is the process of creating new docu ment(s) using text from existing document(s).
</prevsent>
<prevsent>text reuse is standard practice in some situations, such as journalism.
</prevsent>
</prevsection>
<citsent citstr=" P02-1020 ">
applications of automatic detection of text reuse include the removal of (near-)duplicates from search results (hoad and zobel, 2003; seo and croft, 2008), identification of text reuse in journalism (clough et al, 2002) <papid> P02-1020 </papid>and identification of plagiarism (potthast et al, 2011).</citsent>
<aftsection>
<nextsent>text reuse is more difficult to detect when the original text has been altered.
</nextsent>
<nextsent>we propose an approach to the identification of text reuse which is intended to identify reuse in such cases.
</nextsent>
<nextsent>the approach is based on comparison of word n-grams, popular approach to detecting text reuse.
</nextsent>
<nextsent>however, we also account for synonym replacement and word deletion, two common text editing operations (bell,1991).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X277">
<title id=" S12-1008.xml">detecting text reuse with modified and weighted ngrams </title>
<section> determining text reuse with n-gram.  </section>
<citcontext>
<prevsection>
<prevsent>thesen-grams are then compared with the original document to determine the overlap.
</prevsent>
<prevsent>however, the techniques in section 3.2 generate large number of modified n-grams which means that the number of n-grams that overlap with document can be greater than the total number of n-grams in b, leading to similarity scores greater than 1.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
to avoid thisthe n-gram overlap counts are constrained in similar way that they are clipped in bleu and rouge (papineni et al, 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>for each n-gram in b, set of modified n-grams,mod(ngram), is created.1 the count for an individual n-gram in b, exp count(ngram,b), can be computed as the number of times any n-gram in mod(ngram) occurs in a, see equation 2.
</nextsent>
<nextsent>? ngram?
</nextsent>
<nextsent>mod(ngram) count(ngram?, a) (2) however, the contribution of this count to the text reuse score has to be bounded to ensure that the combined count of the modified n-grams appearing ina does not exceed the number of times the original n-gram occurs in b. consequently the text reuse score, scoren(a,b), is computed using equation 3.
</nextsent>
<nextsent>? ngram min(exp count(ngram,a), count(ngram,b)) ? ngramb count(ngram,b) (3) 3.4 weighting n-grams.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X278">
<title id=" S12-1008.xml">detecting text reuse with modified and weighted ngrams </title>
<section> determining text reuse with n-gram.  </section>
<citcontext>
<prevsection>
<prevsent>thesen-grams are then compared with the original document to determine the overlap.
</prevsent>
<prevsent>however, the techniques in section 3.2 generate large number of modified n-grams which means that the number of n-grams that overlap with document can be greater than the total number of n-grams in b, leading to similarity scores greater than 1.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
to avoid thisthe n-gram overlap counts are constrained in similar way that they are clipped in bleu and rouge (papineni et al, 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>for each n-gram in b, set of modified n-grams,mod(ngram), is created.1 the count for an individual n-gram in b, exp count(ngram,b), can be computed as the number of times any n-gram in mod(ngram) occurs in a, see equation 2.
</nextsent>
<nextsent>? ngram?
</nextsent>
<nextsent>mod(ngram) count(ngram?, a) (2) however, the contribution of this count to the text reuse score has to be bounded to ensure that the combined count of the modified n-grams appearing ina does not exceed the number of times the original n-gram occurs in b. consequently the text reuse score, scoren(a,b), is computed using equation 3.
</nextsent>
<nextsent>? ngram min(exp count(ngram,a), count(ngram,b)) ? ngramb count(ngram,b) (3) 3.4 weighting n-grams.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X284">
<title id=" S12-1070.xml">duluth  measuring degrees of relational similarity with the gloss vector measure of semantic relatedness </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these systems were unsupervised and relied on variations of the gloss vector measure found in the freely available software package wordnet::similarity.
</prevsent>
<prevsent>this method was moderately successful for the class-inclusion,similar, contrast, and non-attribute categories of semantic relations, but mimicked random baseline for the other six categories.
</prevsent>
</prevsection>
<citsent citstr=" S12-1047 ">
this paper describes the duluth systems that participated in task 2 of semeval2012, measuring the degree of relational similarity (jurgens et al, 2012).<papid> S12-1047 </papid></citsent>
<aftsection>
<nextsent>the goal of the task was to rank sets of word pairs according to the degree to which they represented an underlying category of semantic relation.
</nextsent>
<nextsent>a highly ranked pair would be considered good or prototypical example of the relation.
</nextsent>
<nextsent>for example, given the relation functions as an the pair weapon:knife (x:y) would likely be considered more representative of that relation than would be tool:spoon.
</nextsent>
<nextsent>the task included word pairs from 10 different categories of relational similarity, each with number of subcategories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X285">
<title id=" S12-1070.xml">duluth  measuring degrees of relational similarity with the gloss vector measure of semantic relatedness </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in total the evaluation data consisted of 69 files, each containing set of approximately 40 word pairs.
</prevsent>
<prevsent>while training examples were also provided, these were not used by the duluth systems.
</prevsent>
</prevsection>
<citsent citstr=" W06-2501 ">
the system generated rankings were compared with gold standard data created via amazon mechanical turk.the duluth systems relied on the gloss vector measure of semantic relatedness (patwardhanand pedersen, 2006) <papid> W06-2501 </papid>as implemented in wordnet::similarity (pedersen et al, 2004)<papid> N04-3012 </papid>1.</citsent>
<aftsection>
<nextsent>this quantifies the degree of semantic relatedness between twoword senses.
</nextsent>
<nextsent>it does not, however, discover or indicate the nature of the relation between the words.
</nextsent>
<nextsent>when given two words as input (as was the case inthis task), it measures the relatedness of all possible combinations of word senses associated with this pair and reports the highest resulting score.
</nextsent>
<nextsent>note that throughout this paper we use word and word sense somewhat interchangeably.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X286">
<title id=" S12-1070.xml">duluth  measuring degrees of relational similarity with the gloss vector measure of semantic relatedness </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in total the evaluation data consisted of 69 files, each containing set of approximately 40 word pairs.
</prevsent>
<prevsent>while training examples were also provided, these were not used by the duluth systems.
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
the system generated rankings were compared with gold standard data created via amazon mechanical turk.the duluth systems relied on the gloss vector measure of semantic relatedness (patwardhanand pedersen, 2006) <papid> W06-2501 </papid>as implemented in wordnet::similarity (pedersen et al, 2004)<papid> N04-3012 </papid>1.</citsent>
<aftsection>
<nextsent>this quantifies the degree of semantic relatedness between twoword senses.
</nextsent>
<nextsent>it does not, however, discover or indicate the nature of the relation between the words.
</nextsent>
<nextsent>when given two words as input (as was the case inthis task), it measures the relatedness of all possible combinations of word senses associated with this pair and reports the highest resulting score.
</nextsent>
<nextsent>note that throughout this paper we use word and word sense somewhat interchangeably.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X287">
<title id=" S12-1070.xml">duluth  measuring degrees of relational similarity with the gloss vector measure of semantic relatedness </title>
<section> semantic relatedness.  </section>
<citcontext>
<prevsection>
<prevsent>1wn-similarity.sourceforge.net 497
</prevsent>
<prevsent>semantic relatedness is more general notion than semantic similarity.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
we follow (budanitsky and hirst, 2006) <papid> J06-1003 </papid>and limit semantic similarity to those measures based on distances and perhaps depths ina hierarchy made up of isa relations.</citsent>
<aftsection>
<nextsent>for example, car and motorcycle are similar in that they are connected via an isa relation with vehicle.
</nextsent>
<nextsent>semantic similarity is most often applied to nouns, but can also be used with verbs.
</nextsent>
<nextsent>two word senses can be related in many ways,including similarity.
</nextsent>
<nextsent>car and furnace might be considered related because they are both made of steel, and firefighter and hose might be considered related because one uses the other, but neither pair is likely to be considered similar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X288">
<title id=" P98-2219.xml">learning optimal dialogue strategies a case study of a spoken dialogue agent for email </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>our method is based on combina-tion of learning algorithms and empirical evaluation techniques.
</prevsent>
<prevsent>the learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and q-learning.
</prevsent>
</prevsection>
<citsent citstr=" P97-1035 ">
the empirical component uses the paradise evalua-tion framework (walker et al, 1997) <papid> P97-1035 </papid>to identify the important performance factors and to provide the performance function needed by the learning algo- rithm.</citsent>
<aftsection>
<nextsent>we illustrate our method with dialogue agent named elvis (email voice interactive sys- tem), that supports access to email over the phone.
</nextsent>
<nextsent>we show how elvis can learn to choose among alternate strategies for agent initiative, for reading messages, and for summarizing email folders.
</nextsent>
<nextsent>this paper describes novel method by which dia-logue agent can learn to choose an optimal dialogue strategy.
</nextsent>
<nextsent>the main problem for dialogue agents is deciding what information to communicate a hearer and how and when to communicate it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X304">
<title id=" S10-1054.xml">uba using automatic translation and wikipedia for cross lingual lexical substitution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of the cross-lingual lexical substitution (clls) task is to substitute word in language s , which occurs in particular context, by providing the best substitutions in different language t . in semeval-2010 the source lan-.
</prevsent>
<prevsent>guage s is english, while the target language t is spanish.
</prevsent>
</prevsection>
<citsent citstr=" W07-2009 ">
clearly, this task is related to lexical substitution (ls) (mccarthy and navigli, 2007) <papid> W07-2009 </papid>which consists in selecting an alternative word forgiven one in particular context by preserving its meaning.</citsent>
<aftsection>
<nextsent>the main difference between the ls task and the clls one is that in ls source and target languages are the same.
</nextsent>
<nextsent>clls is not easy task since neither list of candidate words nor aspecific parallel corpus are supplied by the organizers.
</nextsent>
<nextsent>however, this opens the possibility of using several knowledge sources, instead of single one fixed by the task organizers.
</nextsent>
<nextsent>therefore, the system must identify set of candidate words in t and then select only those words which fit thecontext.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X305">
<title id=" S10-1054.xml">uba using automatic translation and wikipedia for cross lingual lexical substitution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, the system must identify set of candidate words in t and then select only those words which fit thecontext.
</prevsent>
<prevsent>from another point of view, the cross lingual nature of the task allows to exploit automatic machine translation methods, hence the goal is to find as many good translations as possible for the given target word.
</prevsent>
</prevsection>
<citsent citstr=" W09-2412 ">
a thorough description of the task can be found in (mihalcea et al, 2010; sinha et al, 2009).<papid> W09-2412 </papid>to easily understand the task, an example fol lows.</citsent>
<aftsection>
<nextsent>consider the sentence: during the siege, george robertson had appointed shuja-ul-mulk , who was bright boy only 12 years old and the youngest surviving son of aman-ul-mulk, as the ruler of chitral.
</nextsent>
<nextsent>in the previous sentence the target word is bright?.
</nextsent>
<nextsent>taking into account the meaning of the word bright?
</nextsent>
<nextsent>in this particular context, the best substitutions in spanish are: inteligente?, brillante?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X307">
<title id=" S10-1072.xml">kul recognition and normalization of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the best results obtained by the system are 0.85 and 0.84 for precision and recall respectively for recognition of temporal ex pressions; the accuracy values of 0.91 and 0.55 were obtained for the feature values type and val respectively.
</prevsent>
<prevsent>recognition of temporal expressions1 is task of proper identification of phrases with temporal semantics in running text.
</prevsent>
</prevsection>
<citsent citstr=" W07-2014 ">
after several evaluation campaigns targeted at temporal processing of text, such as muc, ace tern and tempev al-1 (verhagen et al, 2007), <papid> W07-2014 </papid>the recognition and normalization task has been again newly reintroduced in tempeval-2 (pustejovsky &amp; verhagen, 2009).<papid> W09-2418 </papid></citsent>
<aftsection>
<nextsent>the task is defined as follows: determine the extent of the time expressions; in addition, determine the value of the features type for the type of the temporal expression and its temporal value val.
</nextsent>
<nextsent>in this paper we describe the kul system that has participated in this task.
</nextsent>
<nextsent>1 temporal expressions are sometimes referenced as time expressions and timexes.
</nextsent>
<nextsent>architecturally, the system employs pipe lined information processing chain and implements number of machine learning classifiers for extracting the necessary information for the temporal value estimation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X308">
<title id=" S10-1072.xml">kul recognition and normalization of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the best results obtained by the system are 0.85 and 0.84 for precision and recall respectively for recognition of temporal ex pressions; the accuracy values of 0.91 and 0.55 were obtained for the feature values type and val respectively.
</prevsent>
<prevsent>recognition of temporal expressions1 is task of proper identification of phrases with temporal semantics in running text.
</prevsent>
</prevsection>
<citsent citstr=" W09-2418 ">
after several evaluation campaigns targeted at temporal processing of text, such as muc, ace tern and tempev al-1 (verhagen et al, 2007), <papid> W07-2014 </papid>the recognition and normalization task has been again newly reintroduced in tempeval-2 (pustejovsky &amp; verhagen, 2009).<papid> W09-2418 </papid></citsent>
<aftsection>
<nextsent>the task is defined as follows: determine the extent of the time expressions; in addition, determine the value of the features type for the type of the temporal expression and its temporal value val.
</nextsent>
<nextsent>in this paper we describe the kul system that has participated in this task.
</nextsent>
<nextsent>1 temporal expressions are sometimes referenced as time expressions and timexes.
</nextsent>
<nextsent>architecturally, the system employs pipe lined information processing chain and implements number of machine learning classifiers for extracting the necessary information for the temporal value estimation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X309">
<title id=" S10-1072.xml">kul recognition and normalization of temporal expressions </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>the candidates, whose parse and annotation extents aligned, are taken as positive examples and the rest is considered as negative.
</prevsent>
<prevsent>feature design: to produce feature-vector we use most valuable features extracted for phrase-candidate.
</prevsent>
</prevsection>
<citsent citstr=" N07-1053 ">
after number of experiments the following features were selected: ? last token in the phrase, most probable token to be temporal trigger; ? lemma of the last phrasal token; ? part-of-speech of the last phrasal token; ? character pattern of the last phrasal token as introduced in (ahn et al, 2007); ? <papid> N07-1053 </papid>neighbor poss. the concatenated part of-speech tags of the last phrasal token and its preceding token; ? character pattern of the entire phrase; ? phrase surface.</citsent>
<aftsection>
<nextsent>a concatenated string of sub-parse types for the phrase; ? boolean feature indicating nested complex phrasal parses, such as noun verb, adverbial, adjective or prepositional phrase; ? depth of the phrase.
</nextsent>
<nextsent>the number of the nested sub-parses to the deepest pre terminal sub-parse.
</nextsent>
<nextsent>all the features are considered as boolean.
</nextsent>
<nextsent>classification: once the classifiers are trained they can be used for recognition of temporal expressions on test sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X310">
<title id=" S10-1072.xml">kul recognition and normalization of temporal expressions </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>but grammatically wrong by nave replacement from gazetteer: ? on week said*?
</prevsent>
<prevsent>on day said*?
</prevsent>
</prevsection>
<citsent citstr=" D09-1003 ">
on month said* ? in order to find these words, which are legitimate at certain position in certain context we use the latent word language model (lwlm) (deschacht &amp; moens, 2009) <papid> D09-1003 </papid>with hidden markov model approach for estimating the latent word parameters.</citsent>
<aftsection>
<nextsent>complementary, we use wordnet (miller, 1995) as source that can provide most complete set of words similar to the given one.
</nextsent>
<nextsent>one should note that the use of wordnet is not straight-forward.
</nextsent>
<nextsent>due to the polysemy, the word sense disambiguation (wsd) problem has to be solved.
</nextsent>
<nextsent>our system uses latent words obtained by the lwlm and chooses the synset with the high 326 est overlap between wordnet synonyms and coordinate terms, and the latent words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X311">
<title id=" S12-1069.xml">untsimprank systems for lexical simplification ranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(carroll et al, 1998)seem to have pioneered some methodology and evaluation metrics in this field.
</prevsent>
<prevsent>yatskar et. al.
</prevsent>
</prevsection>
<citsent citstr=" N10-1056 ">
(yatskar et al., 2010) <papid> N10-1056 </papid>use an unsupervised learning method and meta data from the simple english wikipedia.</citsent>
<aftsection>
<nextsent>the data (trial and test, no training) have been adopted from the original lexical substitution task (mccarthy et al, 2007).
</nextsent>
<nextsent>the trial set has 300 examples, each with context, target word, anda set of substitutions.
</nextsent>
<nextsent>the test set has 1710 examples.
</nextsent>
<nextsent>the organizers provide scorer for the task, the trial gold standard rankings, and three baselines.the data is provided in xml format, with tags identifying the lemmas, parts of speech, instances, contexts and headwords.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X313">
<title id=" S12-1055.xml">utd determining relational similarity using lexical patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, word pairs are ranked based on model predicting the probability that they belong to the relation of interest.
</prevsent>
<prevsent>this approach achieved the best results on the semeval 2012 task 2, obtaining spearman correlation of 0.229 and an accuracy on reproducing human answers to maxdiff questions of 39.4%.
</prevsent>
</prevsection>
<citsent citstr=" P04-1055 ">
considerable prior research has examined and elaborated upon wide variety of semantic relations between concepts along with techniques for automatically discovering pairs of concepts for which relation holds (bejar et al, 1991; stephens and chen, 1996; rosario and hearst, 2004; <papid> P04-1055 </papid>khoo andna, 2006; girju et al, 2009).</citsent>
<aftsection>
<nextsent>however, most previous work has considered membership assignment for semantic relation as binary property.
</nextsent>
<nextsent>in this paper we discuss an approach which assigns degree of membership to pair of concepts forgiven relation.
</nextsent>
<nextsent>for example, for the semantic relation class-inclusion (taxonomic), the concept pairs weapon:spear and bird:robin are stronger members consider the following word pairs: millionaire:money, author:copyright, robin:nest.
</nextsent>
<nextsent>these x:y pairs share relation r y?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X314">
<title id=" S12-1055.xml">utd determining relational similarity using lexical patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>which of the above numbered word pairs is the least illustrative example of the same relation r yfigure 1: example phase 2 maxdiff question for there lation 2h part-whole: creature:possession.
</prevsent>
<prevsent>of the relationship than hair:brown, because brown may describe many things other than hair, and brown is also used much less frequently as noun than the words in the first two word pairs.
</prevsent>
</prevsection>
<citsent citstr=" S12-1047 ">
task 2 of semeval 2012 (jurgens et al, 2012) <papid> S12-1047 </papid>was designed to evaluate the effectiveness of automatic approaches for determining the similarity of pair of concepts to specific semantic relation.</citsent>
<aftsection>
<nextsent>the task focused on 79 semantic relations from bejar et al (1991) which broadly fall into the ten categories enumerated in table 1.
</nextsent>
<nextsent>the data for the task was collected in two phases using amazon mechanical turk 1.
</nextsent>
<nextsent>during phase 1, turkers were asked to provide pairs of words which fit relation template, such as pos sesses/owns/has y?.
</nextsent>
<nextsent>turkers provided word pairs such as expert:experience, mall:shops, letters:words, and doctor:degree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X315">
<title id=" S12-1055.xml">utd determining relational similarity using lexical patterns </title>
<section> approach for determining relational.  </section>
<citcontext>
<prevsection>
<prevsent>when turkers select pair of words they are performing semantic inference thatwe wanted to also perform in computational manner.
</prevsent>
<prevsent>in this paper we present method for automatically ranking word pairs according to their relatedness to given semantic relation.
</prevsent>
</prevsection>
<citsent citstr=" C08-1114 ">
similarity in the vein of previous methods for determining relational similarity (turney, 2011; turney, 2008<papid> C08-1114 </papid>a;turney, 2008<papid> C08-1114 </papid>b; turney, 2005), we propose two approaches using patterns generated from the contexts in which the word pairs occur.</citsent>
<aftsection>
<nextsent>our corpus consists of 8.4 million documents from gigaword (parker and consortium, 2009) and over 4 million articles from wikipedia.
</nextsent>
<nextsent>for each word pair,  w1 ,  w2  provided by turkers in phase 1, as well as the three relation examples, we collected all contexts which matched the schema: ? [0 or more non-content words]  w1  [0 to 7 words]  w2  [0 or more non-content words]?
</nextsent>
<nextsent>we also include those contexts where w1 and w2 are swapped.
</nextsent>
<nextsent>the window size of seven words was determined based on experiments on the training set of ten relations provided by the task organizers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X319">
<title id=" S12-1058.xml">zhijun wu chinese semantic dependency parsing with third order features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>conll shared tasks for joint parsing for syntactic and semantic dependencies both in the year 2008 and 2009, cf.
</prevsent>
<prevsent>(surdeanu et al, 2008; haji?
</prevsent>
</prevsection>
<citsent citstr=" W09-1210 ">
et al, 2009; bohnet, 2009).<papid> W09-1210 </papid></citsent>
<aftsection>
<nextsent>same shared tasks in semeval-2007 (sameer s., 2007).
</nextsent>
<nextsent>the srl is traditionally implemented as two subtasks, argument identification and classification.
</nextsent>
<nextsent>however, there are some problems for the semantic representation method used by the semantic role labeling.
</nextsent>
<nextsent>for example, the srl only considers the predicate-argument relations and ignores the relations between noun and its modifier, the meaning of semantic roles is related with special predicates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X320">
<title id=" S12-1058.xml">zhijun wu chinese semantic dependency parsing with third order features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>table 1: an example position dependency although semantic relations are different from syntactic relations, yet they are identical in the dependency tree.
</prevsent>
<prevsent>that means the methods used in syntactic dependency parsing can also be applied in sdp.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
two main approaches to syntactic dependency paring are maximum spanning tree (mst) based dependency parsing and transition based dependency parsing (eisner, 1996; <papid> C96-1058 </papid>nivre et al, 2004; <papid> W04-2407 </papid>mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>the main idea of 430 mst parser is to take dependency parsing as problem of searching maximum spanning tree (mst) in directed graph (dependency tree).
</nextsent>
<nextsent>we see mst parser better chance to improve the parsing speed and mst parser provides the stateof-the-art performance for both projective and non projective treebanks.
</nextsent>
<nextsent>for the reasons above, we choose mst parser as our semeval-2012 shared task participating system basic framework.
</nextsent>
<nextsent>our parser is based on the projective mst parser using all the features described by (mcdonald et al., 2006) as well as some third-order features described in the following sections.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X321">
<title id=" S12-1058.xml">zhijun wu chinese semantic dependency parsing with third order features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>table 1: an example position dependency although semantic relations are different from syntactic relations, yet they are identical in the dependency tree.
</prevsent>
<prevsent>that means the methods used in syntactic dependency parsing can also be applied in sdp.
</prevsent>
</prevsection>
<citsent citstr=" W04-2407 ">
two main approaches to syntactic dependency paring are maximum spanning tree (mst) based dependency parsing and transition based dependency parsing (eisner, 1996; <papid> C96-1058 </papid>nivre et al, 2004; <papid> W04-2407 </papid>mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>the main idea of 430 mst parser is to take dependency parsing as problem of searching maximum spanning tree (mst) in directed graph (dependency tree).
</nextsent>
<nextsent>we see mst parser better chance to improve the parsing speed and mst parser provides the stateof-the-art performance for both projective and non projective treebanks.
</nextsent>
<nextsent>for the reasons above, we choose mst parser as our semeval-2012 shared task participating system basic framework.
</nextsent>
<nextsent>our parser is based on the projective mst parser using all the features described by (mcdonald et al., 2006) as well as some third-order features described in the following sections.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X322">
<title id=" S12-1058.xml">zhijun wu chinese semantic dependency parsing with third order features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>table 1: an example position dependency although semantic relations are different from syntactic relations, yet they are identical in the dependency tree.
</prevsent>
<prevsent>that means the methods used in syntactic dependency parsing can also be applied in sdp.
</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
two main approaches to syntactic dependency paring are maximum spanning tree (mst) based dependency parsing and transition based dependency parsing (eisner, 1996; <papid> C96-1058 </papid>nivre et al, 2004; <papid> W04-2407 </papid>mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>the main idea of 430 mst parser is to take dependency parsing as problem of searching maximum spanning tree (mst) in directed graph (dependency tree).
</nextsent>
<nextsent>we see mst parser better chance to improve the parsing speed and mst parser provides the stateof-the-art performance for both projective and non projective treebanks.
</nextsent>
<nextsent>for the reasons above, we choose mst parser as our semeval-2012 shared task participating system basic framework.
</nextsent>
<nextsent>our parser is based on the projective mst parser using all the features described by (mcdonald et al., 2006) as well as some third-order features described in the following sections.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X323">
<title id=" S12-1058.xml">zhijun wu chinese semantic dependency parsing with third order features </title>
<section> semantic dependency parsers.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 gives our conclusion and future work.
</prevsent>
<prevsent>3.1 first-order model.
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
dependency tree parsing as the search for the maximum spanning tree in directed graph was proposed by mcdonald et al (2005<papid> P05-1012 </papid>c).</citsent>
<aftsection>
<nextsent>this formulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the eisner algorithm (eisner, 1996) <papid> C96-1058 </papid>and the chu-liu-edmonds algorithm (chu and liu, 1965; edmonds, 1967) respectively.</nextsent>
<nextsent>the formulation works by defining in mcdonald et al(2005<papid> P05-1012 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X329">
<title id=" S12-1058.xml">zhijun wu chinese semantic dependency parsing with third order features </title>
<section> semantic dependency parsers.  </section>
<citcontext>
<prevsection>
<prevsent>we divide the dependency distance into six parts which are 1 if   1, 2 if   2, ? , 5 if   5, 10 if   10.
</prevsent>
<prevsent>3.3 third-order features.
</prevsent>
</prevsection>
<citsent citstr=" P10-1001 ">
the order of parsing is defined according to the number of dependencies it contains (koo and collins, 2010).<papid> P10-1001 </papid></citsent>
<aftsection>
<nextsent>collins classifies the third-order as two models, model 1 is all grand-siblings, and model 2 is grand-siblings and tri-siblings.
</nextsent>
<nextsent>a grand-sibling is 4-tuple of indices (g, h, m, s) where is grand father.
</nextsent>
<nextsent>(h, m, s) is sibling part and (g, h, m) is grandchild part as well as (g, h, s).
</nextsent>
<nextsent>a tri-sibling part is also 4-tuple of indices (h, m, s, t).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X330">
<title id=" S12-1084.xml">tiantianzhu7system description of semantic textual similarity sts in the semeval2012 task 6 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the knowledge based method computes sentence similarity basedon the semantic information collected from knowledge bases.
</prevsent>
<prevsent>with the aid of number of successful computational linguistic projects, many semantic knowledge bases are readily available, forex ample, wordnet, spatial date transfer standard, gene ontology, etc. among them, the most widely used one is wordnet, which is organized by meanings and developed at princeton university.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
several methods computed word similarity by using wordnet, such as the lesk method in (banerjee and pedersen, 2003), the lch method in (leacock and chodorow, 1998)and the wup method in (wu and palmer, 1994).<papid> P94-1019 </papid></citsent>
<aftsection>
<nextsent>generally, although the knowledge based methods heavily depend on the knowledge bases, they performed much better than the corpus based methods in most cases.
</nextsent>
<nextsent>therefore, in our sts system, we use knowledge-based method to compute word similarity.the rest of this paper is organized as follows.
</nextsent>
<nextsent>section 2 describes our system.
</nextsent>
<nextsent>section 3 presents the results of our system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X332">
<title id=" S10-1004.xml">semeval2010 task 5  automatic key phrase extraction from scientific articles </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>key phrases 1are words that capture the main topics of document.
</prevsent>
<prevsent>as they represent these key ideas, extracting high-quality key phrases can benefit various natural language processing (nlp) applications such as summarization, information retrieval and question-answering.
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
in summarization, key phrases can be used as form of semantic meta data (barzilay and elhadad, 1997; <papid> W97-0703 </papid>lawrie et al, 2001; davanzo and magnini,2005).</citsent>
<aftsection>
<nextsent>in search engines, key phrases can supplement full-text indexing and assist users in formulating queries.
</nextsent>
<nextsent>recently, resurgence of interest in key phrase extraction has led to the development of several new systems and techniques for the task (frank et al, 1999; witten et al, 1999; turney, 1999; hulth, 2003; <papid> W03-1028 </papid>turney, 2003; park et al, 2004;barker and corrnacchia, 2000; hulth, 2004; <papid> N04-4005 </papid>matsuo and ishizuka, 2004; mihalcea and tarau, 2004; medelyan and witten, 2006; nguyen and kan, 2007; wan and xiao, 2008; liu et al, 2009; medelyan, 2009; nguyen and phan, 2009).</nextsent>
<nextsent>these 1 we use keyphrase?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X333">
<title id=" S10-1004.xml">semeval2010 task 5  automatic key phrase extraction from scientific articles </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>in summarization, key phrases can be used as form of semantic meta data (barzilay and elhadad, 1997; <papid> W97-0703 </papid>lawrie et al, 2001; davanzo and magnini,2005).</prevsent>
<prevsent>in search engines, key phrases can supplement full-text indexing and assist users in formulating queries.</prevsent>
</prevsection>
<citsent citstr=" W03-1028 ">
recently, resurgence of interest in key phrase extraction has led to the development of several new systems and techniques for the task (frank et al, 1999; witten et al, 1999; turney, 1999; hulth, 2003; <papid> W03-1028 </papid>turney, 2003; park et al, 2004;barker and corrnacchia, 2000; hulth, 2004; <papid> N04-4005 </papid>matsuo and ishizuka, 2004; mihalcea and tarau, 2004; medelyan and witten, 2006; nguyen and kan, 2007; wan and xiao, 2008; liu et al, 2009; medelyan, 2009; nguyen and phan, 2009).</citsent>
<aftsection>
<nextsent>these 1 we use keyphrase?
</nextsent>
<nextsent>and keywords?
</nextsent>
<nextsent>interchangeably to refer to both single words and phrases.
</nextsent>
<nextsent>min-yen kans work was funded by national research foundation grant interactive media search?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X334">
<title id=" S10-1004.xml">semeval2010 task 5  automatic key phrase extraction from scientific articles </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>in summarization, key phrases can be used as form of semantic meta data (barzilay and elhadad, 1997; <papid> W97-0703 </papid>lawrie et al, 2001; davanzo and magnini,2005).</prevsent>
<prevsent>in search engines, key phrases can supplement full-text indexing and assist users in formulating queries.</prevsent>
</prevsection>
<citsent citstr=" N04-4005 ">
recently, resurgence of interest in key phrase extraction has led to the development of several new systems and techniques for the task (frank et al, 1999; witten et al, 1999; turney, 1999; hulth, 2003; <papid> W03-1028 </papid>turney, 2003; park et al, 2004;barker and corrnacchia, 2000; hulth, 2004; <papid> N04-4005 </papid>matsuo and ishizuka, 2004; mihalcea and tarau, 2004; medelyan and witten, 2006; nguyen and kan, 2007; wan and xiao, 2008; liu et al, 2009; medelyan, 2009; nguyen and phan, 2009).</citsent>
<aftsection>
<nextsent>these 1 we use keyphrase?
</nextsent>
<nextsent>and keywords?
</nextsent>
<nextsent>interchangeably to refer to both single words and phrases.
</nextsent>
<nextsent>min-yen kans work was funded by national research foundation grant interactive media search?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X336">
<title id=" S12-1037.xml">ucmi a rule based syntactic approach for resolving the scope of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in information extraction, for instance, it is obviously important to distinguish negated information from affirmative one (kim and park, 2006).
</prevsent>
<prevsent>it may also improve automatic indexing (mutalik etal., 2001).
</prevsent>
</prevsection>
<citsent citstr=" W10-3111 ">
in sentiment analysis, detecting and dealing with negation is critical, as it may change the polarity of text (wiegand et al, 2010).<papid> W10-3111 </papid></citsent>
<aftsection>
<nextsent>how ever, research on negation has mainly focused on the biomedical domain, and addressed the problem of detecting if medical term is negated or not (chap manet al, 2001), or the scope of different negation signals (morante et al, 2008).<papid> D08-1075 </papid>during the last years, the importance of processing negation is gaining recognition by the nlp research community, as evidenced by the success of several initiatives such as the negation and speculation in natural language processing workshop (nesp-nlp 2010)1 or the conll-2010 shared task2, which aimed at identifying hedges and their scope in natural language texts.</nextsent>
<nextsent>inspite of this, most of the approaches proposed so far deal with negation in superficial manner.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X337">
<title id=" S12-1037.xml">ucmi a rule based syntactic approach for resolving the scope of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it may also improve automatic indexing (mutalik etal., 2001).
</prevsent>
<prevsent>in sentiment analysis, detecting and dealing with negation is critical, as it may change the polarity of text (wiegand et al, 2010).<papid> W10-3111 </papid></prevsent>
</prevsection>
<citsent citstr=" D08-1075 ">
how ever, research on negation has mainly focused on the biomedical domain, and addressed the problem of detecting if medical term is negated or not (chap manet al, 2001), or the scope of different negation signals (morante et al, 2008).<papid> D08-1075 </papid>during the last years, the importance of processing negation is gaining recognition by the nlp research community, as evidenced by the success of several initiatives such as the negation and speculation in natural language processing workshop (nesp-nlp 2010)1 or the conll-2010 shared task2, which aimed at identifying hedges and their scope in natural language texts.</citsent>
<aftsection>
<nextsent>inspite of this, most of the approaches proposed so far deal with negation in superficial manner.
</nextsent>
<nextsent>this paper describes our contribution to the *sem shared task 2012 on resolving the scope and focus of negation.
</nextsent>
<nextsent>as its name suggests, thetask aims at detecting the scope and focus of negation, as means of encouraging research in negation processing.
</nextsent>
<nextsent>in particular, we participate in task 1: scope detection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X338">
<title id=" S12-1037.xml">ucmi a rule based syntactic approach for resolving the scope of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for each negation in the text, the negation cue must be detected, and its scope marked.
</prevsent>
<prevsent>moreover, the event or property that is negated must be recognized.
</prevsent>
</prevsection>
<citsent citstr=" S12-1035 ">
a comprehensive description of the task may be found in (morante and blanco, 2012).<papid> S12-1035 </papid></citsent>
<aftsection>
<nextsent>for the sake of clarity, it is important to define what the organization of the task understands by negation cue, scope of negation and negated event.
</nextsent>
<nextsent>the words that express negation are called negation cues.
</nextsent>
<nextsent>not and no are common examples of suchcues.
</nextsent>
<nextsent>scope is defined as the part of the meaning that is negated, and encloses all negated concepts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X339">
<title id=" S12-1037.xml">ucmi a rule based syntactic approach for resolving the scope of negation </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>words with implicit negation cues unpleasant unnatural dislike impatient fearless hopeless illegal ...have into account these negation cues when analyzing opinionated texts because these words themselves usually appear in affective lexicons with their corresponding polarity values (i.e., impatient, for instance, appears in sentiwordnet with negative polarity value).
</prevsent>
<prevsent>in order to detect negation cues, we use list of predefined negation signals, along with an automatic method for detecting new ones.
</prevsent>
</prevsection>
<citsent citstr=" W10-3110 ">
the list has been extracted from different previous works (councill et al., 2010; <papid> W10-3110 </papid>morante, 2010).</citsent>
<aftsection>
<nextsent>this list also includes the most frequent contracted forms (e.g., dont, didnt, etc.).
</nextsent>
<nextsent>the automated method, in turn, is intended for discovering in text new affixal negation cues.
</nextsent>
<nextsent>tothis end, we first find in the text all words with prefixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suffix -less that present the appropriate part of speech.
</nextsent>
<nextsent>since not all words with such affixes are negation cues, we use semantic information from wordnet concepts and relations to decide.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X340">
<title id=" S12-1037.xml">ucmi a rule based syntactic approach for resolving the scope of negation </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>tothis end, we first find in the text all words with prefixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suffix -less that present the appropriate part of speech.
</prevsent>
<prevsent>since not all words with such affixes are negation cues, we use semantic information from wordnet concepts and relations to decide.
</prevsent>
</prevsection>
<citsent citstr=" P05-3019 ">
in this way, we retrieve from wordnet the synset that correspond toeach word, using wordnet::senserelate (patwardhan et al, 2005) <papid> P05-3019 </papid>to correctly disambiguate the meaning of the word according to its context, along withall its antonym synsets.</citsent>
<aftsection>
<nextsent>we next check if, after removing the affix, the word exists in wordnet and belongs to any of the antonym synsets.
</nextsent>
<nextsent>if so, we consider the original word to be negation cue (i.e., the word without the affix has the opposite meaning than the lexical item with the affix).table 1 presents some examples of explicit negation cues and words with implicit negation cues.
</nextsent>
<nextsent>for space reasons, not all cues are shown.
</nextsent>
<nextsent>we also consider common spelling errors such as the omission of apostrophes (e.g., isnt or nt).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X342">
<title id=" S12-1044.xml">uwashington negation resolution using machine learning methods </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the one this system was developed for is the identification of three features of negation: the cue, the scope, and the factual negated event (if any).
</prevsent>
<prevsent>the other task is concerned with the focus of negation.
</prevsent>
</prevsection>
<citsent citstr=" S12-1035 ">
detailed description of both subtasks, including definition of the relevant concepts and terminology (negation, cue, scope, event, and focus) appears in this volume (morante and blanco, 2012).<papid> S12-1035 </papid></citsent>
<aftsection>
<nextsent>roser morante and eduardo blanco describe the corpora provided to partici?
</nextsent>
<nextsent>pants with numbers and examples, methods used used to process the data, and briefly describes each participant and analyzes the overall results.
</nextsent>
<nextsent>annotation of the corpus was undertaken at the university of antwerp and was performed on sev?
</nextsent>
<nextsent>eral sherlock holmes works of fiction written by sir arthur conan doyle.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X343">
<title id=" S12-1044.xml">uwashington negation resolution using machine learning methods </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ever, that system only recognizes whole word as cue and does not recognize nor generalize nega?
</prevsent>
<prevsent>tion cues which are affixes.
</prevsent>
</prevsection>
<citsent citstr=" W10-3002 ">
there are also systems that use crf sequence taggers for detection of hedge scopes (tang et al  2010, <papid> W10-3002 </papid>zhao et al  2010).<papid> W10-3014 </papid></citsent>
<aftsection>
<nextsent>morante and daelemans describe method for im?
</nextsent>
<nextsent>proving resolution of the scope of negation by combining igtree, crf, and support vector ma?
</nextsent>
<nextsent>chines (svm) (morante and daelemans, 2009).<papid> W09-1105 </papid></nextsent>
<nextsent>this system is implemented as three stage cas?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X344">
<title id=" S12-1044.xml">uwashington negation resolution using machine learning methods </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ever, that system only recognizes whole word as cue and does not recognize nor generalize nega?
</prevsent>
<prevsent>tion cues which are affixes.
</prevsent>
</prevsection>
<citsent citstr=" W10-3014 ">
there are also systems that use crf sequence taggers for detection of hedge scopes (tang et al  2010, <papid> W10-3002 </papid>zhao et al  2010).<papid> W10-3014 </papid></citsent>
<aftsection>
<nextsent>morante and daelemans describe method for im?
</nextsent>
<nextsent>proving resolution of the scope of negation by combining igtree, crf, and support vector ma?
</nextsent>
<nextsent>chines (svm) (morante and daelemans, 2009).<papid> W09-1105 </papid></nextsent>
<nextsent>this system is implemented as three stage cas?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X345">
<title id=" S12-1044.xml">uwashington negation resolution using machine learning methods </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>morante and daelemans describe method for im?
</prevsent>
<prevsent>proving resolution of the scope of negation by combining igtree, crf, and support vector ma?
</prevsent>
</prevsection>
<citsent citstr=" W09-1105 ">
chines (svm) (morante and daelemans, 2009).<papid> W09-1105 </papid></citsent>
<aftsection>
<nextsent>this system is implemented as three stage cas?
</nextsent>
<nextsent>cade with the output from each of the first two stages included as input to the subsequent stage.
</nextsent>
<nextsent>the stages are ordered as cue detection, scope de?
</nextsent>
<nextsent>tection, and finally negated event detection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X346">
<title id=" S10-1088.xml">ucfws domain word sense disambiguation using web selectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>results indicated that the system performs relatively the same with domain predominant sense information as without, scoring well above random baseline, but still 5 percentage points below results of using the first sense.
</prevsent>
<prevsent>we explore the use of the web selectors word sense disambiguation system for disambiguating nouns and verbs of domain text.
</prevsent>
</prevsection>
<citsent citstr=" W08-2114 ">
our method to acquire selectors from the web for wsd was first described in (schwartz and gomez, 2008).<papid> W08-2114 </papid></citsent>
<aftsection>
<nextsent>the system is extended for the all-words domain task by including part of speech tags from the stanford parser (klein and manning, 2003).
</nextsent>
<nextsent>additionally, adomain adaptation technique of using domain predominant senses (koeling et al, 2005) <papid> H05-1053 </papid>is explored, but our primary goal is concerned with evaluating the performance of the existing web selectors system on domain text.</nextsent>
<nextsent>in previous studies, the web selectors system was applied to text of general domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X347">
<title id=" S10-1088.xml">ucfws domain word sense disambiguation using web selectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our method to acquire selectors from the web for wsd was first described in (schwartz and gomez, 2008).<papid> W08-2114 </papid></prevsent>
<prevsent>the system is extended for the all-words domain task by including part of speech tags from the stanford parser (klein and manning, 2003).</prevsent>
</prevsection>
<citsent citstr=" H05-1053 ">
additionally, adomain adaptation technique of using domain predominant senses (koeling et al, 2005) <papid> H05-1053 </papid>is explored, but our primary goal is concerned with evaluating the performance of the existing web selectors system on domain text.</citsent>
<aftsection>
<nextsent>in previous studies, the web selectors system was applied to text of general domain.
</nextsent>
<nextsent>however, the system was not directly tuned for the general domain.
</nextsent>
<nextsent>the system may perform just as strong for domain wsd since the selectors, which are thecore of disambiguation, can come from any do main present on the web.
</nextsent>
<nextsent>in this paper, we study the application of the web selectors wsd algorithm to an all-words task on specific domain, the semeval 2010: task 17 (agirre et al, 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X348">
<title id=" S10-1088.xml">ucfws domain word sense disambiguation using web selectors </title>
<section> web selectors.  </section>
<citcontext>
<prevsection>
<prevsent>the system may perform just as strong for domain wsd since the selectors, which are thecore of disambiguation, can come from any do main present on the web.
</prevsent>
<prevsent>in this paper, we study the application of the web selectors wsd algorithm to an all-words task on specific domain, the semeval 2010: task 17 (agirre et al, 2010).
</prevsent>
</prevsection>
<citsent citstr=" P97-1009 ">
selectors are words which take the place of given target word within its local context (lin, 1997).<papid> P97-1009 </papid></citsent>
<aftsection>
<nextsent>in the case of acquiring selectors from the web, we search with the text of local context (schwartz and gomez, 2008).<papid> W08-2114 </papid></nextsent>
<nextsent>for example, if one was searching for selectors of channel?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X350">
<title id=" S10-1088.xml">ucfws domain word sense disambiguation using web selectors </title>
<section> web selectors.  </section>
<citcontext>
<prevsection>
<prevsent>referring to our previous example, the senses of channel?
</prevsent>
<prevsent>are compared to its own (target) selectors via similarity measures, while relatedness measures are used for the context se lectors: noun selectors of navigation?, shifts?, north?, south?, and banks?; the verb selectors of 392 figure 1: the overall process undertaken to disambiguate word using web selectors.undergoes?; plus the adjective selectors of ma jor?.
</prevsent>
</prevsection>
<citsent citstr=" W09-2405 ">
adverbs, proper nouns, and pronouns are not present in the sentence, and so no selectors from those parts of speech are considered.for this study, we implemented the web selectors system that was presented in (schwartz and gomez, 2009).<papid> W09-2405 </papid></citsent>
<aftsection>
<nextsent>this generalized version of the system may annotate verbs in addition to nouns,and it includes the previously unused context selectors of adverbs.
</nextsent>
<nextsent>we used the path-based similarity measure of (jiang and conrath, 1997) for target selectors, and the gloss-based relatedness measure of (banerjee and pedersen, 2002) for context selectors.
</nextsent>
<nextsent>the incorporation of part of speech tagger wasa necessary addition to the existing system.
</nextsent>
<nextsent>previous evaluations of web selectors relied on the testing corpus to provide part of speech (pos) tags for content words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X352">
<title id=" S10-1088.xml">ucfws domain word sense disambiguation using web selectors </title>
<section> web selectors.  </section>
<citcontext>
<prevsection>
<prevsent>selectors themselves can be from any domain.
</prevsent>
<prevsent>however, sense tagged data may be used indirectly within the system.
</prevsent>
</prevsection>
<citsent citstr=" H94-1046 ">
first, the similarity and relatedness measures used in the system may relyon semcor data (miller etal., 1994).<papid> H94-1046 </papid></citsent>
<aftsection>
<nextsent>also, the system breaks ties by choosing the most frequent sense according to wordnet frequency data (based on semcor).
</nextsent>
<nextsent>these two aspects of the system can be seen as tuned to the general domain, and thus, they are likely aspects of the system for adaptation to specific domain.
</nextsent>
<nextsent>for this work, we focused on domain-adaptingthe tie breaker aspect of the web selectors system.
</nextsent>
<nextsent>the system defines tie occurring when multiple sense choices are scored within 5% of the top sense choice.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X355">
<title id=" S10-1088.xml">ucfws domain word sense disambiguation using web selectors </title>
<section> web selectors.  </section>
<citcontext>
<prevsection>
<prevsent>because sense tagged do main data is not typically available, koeling et al (2005) <papid> H05-1053 </papid>presented the idea of estimating the most frequent sense of domain by calculating sense prevalence scores from unannotated domain text.several steps are taken to calculate the prevalence scores.</prevsent>
<prevsent>first, dependency database is created, listing the frequencies that each dependency relationship appears.</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
in our case, we used the stanford parser (klein and manning, 2003) on the background data provided by the task organizers.from the dependency database, thesaurus is created based on the method of (lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>in our approach, we considered the following relationships from the dependency database: subject (agent, csubj, subjpass, nsubj, nsubjpass, xsubj) direct object (dobj) indirect object (iobj) 393 adjective modifier (amod) noun modifier (nn)prepositional modifier (any preposition, excluding prep of and prep for) (typed dependency names listed in parenthesis) finally, prevalence score is calculated for each sense of noun or verb by finding the similarity between it and the top 50 most similar words according to the automatically created thesaurus.
</nextsent>
<nextsent>as koeling et al did, we use the similarity measure of (jiang and conrath, 1997).
</nextsent>
<nextsent>the results of our system are given in table 1.
</nextsent>
<nextsent>the first set of results (ws) was standard run of the system without any domain adaptation, while the second set (ws dom ) was from run including the domain prevalence scores in order to break ties.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X360">
<title id=" P98-2154.xml">translating a unification grammar with dis junctions into logical constraints </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the objective of our research isto build natural language understanding system that is based on unification.
</prevsent>
<prevsent>the reason we have chosen unification-based approach is that it enables us to describe grammar declaratively, making the development and amendment of grammar easy.
</prevsent>
</prevsection>
<citsent citstr=" P84-1075 ">
analysis systems that are based on unification gram-mars can be classified into two groups from the viewpoint of the ways feature structures are represented: (a) those using labeled, directed graphs (shieber, 1984) <papid> P84-1075 </papid>and (b) those using first-order terms (pereira nd warren, 1980; matsumoto et al, 1983; tokunaga et al, 1991).</citsent>
<aftsection>
<nextsent>in addition to internal representation, grammar for-malisms can be classified into two groups, (i) those that describe feature structures with path equations and lists of pairs of labels and values (mukai and yasukawa, 1985; ai t-kaci, 1986; tsuda, 1994), and (ii) those that describe feature structures with first-order terms (pereira and warren, 1980; matsumoto et al, 1983; tokunaga et * presently with japan advanced institute of science and technology.
</nextsent>
<nextsent>al., 1991).
</nextsent>
<nextsent>since formalisms (i) are used in the family of the patr parsing systems (shieber, 1984), <papid> P84-1075 </papid>hereafter they will be called patr-iike formalisms.</nextsent>
<nextsent>most of the previous systems are either ones that generate representation (a) from formalisms (i) or ones that generate representation (b) from formalisms (ii).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X362">
<title id=" P98-2154.xml">translating a unification grammar with dis junctions into logical constraints </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>formalism (i) is better because term-based formalism is problematic in that readers need to memorize the correspondence between arguments and features and it is not easy to add new features or delete features (gazdar and mellish, 1989).
</prevsent>
<prevsent>therefore, it is effective to translate formalism (i) into representation (b).
</prevsent>
</prevsection>
<citsent citstr=" E95-1025 ">
previous translation methods 2 (covington, 1989; hirsh, 1988; schster, 1993; erbach, 1995) <papid> E95-1025 </papid>are problematic that they cannot deal with dis-junctive feature descriptions, which reduce redundancies in grammar.</citsent>
<aftsection>
<nextsent>moreover, incorporating disjunctive infor-mation into internal representation makes parsing more efficient (kasper, 1987; <papid> P87-1033 </papid>eisele and dsrre, 1988; <papid> P88-1035 </papid>maxwell and kaplan, 1991; hasida, 1986).<papid> C86-1018 </papid></nextsent>
<nextsent>this paper presents method for translating rammar formalism with disjunctive information based on path equations and lists of pairs of labels and values into term- since unspecified features are represented by variables in term unification, when most of the features are unspecified, it is inefficient to represent feature structures by terms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X363">
<title id=" P98-2154.xml">translating a unification grammar with dis junctions into logical constraints </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, it is effective to translate formalism (i) into representation (b).
</prevsent>
<prevsent>previous translation methods 2 (covington, 1989; hirsh, 1988; schster, 1993; erbach, 1995) <papid> E95-1025 </papid>are problematic that they cannot deal with dis-junctive feature descriptions, which reduce redundancies in grammar.</prevsent>
</prevsection>
<citsent citstr=" P87-1033 ">
moreover, incorporating disjunctive infor-mation into internal representation makes parsing more efficient (kasper, 1987; <papid> P87-1033 </papid>eisele and dsrre, 1988; <papid> P88-1035 </papid>maxwell and kaplan, 1991; hasida, 1986).<papid> C86-1018 </papid></citsent>
<aftsection>
<nextsent>this paper presents method for translating rammar formalism with disjunctive information based on path equations and lists of pairs of labels and values into term- since unspecified features are represented by variables in term unification, when most of the features are unspecified, it is inefficient to represent feature structures by terms.
</nextsent>
<nextsent>in current linguistic theories such as hpsg (pollard and sag, 1994), however, thanks to the type specifications, the number of features that feature structure can have is reduced, so it does not cause as much trouble.
</nextsent>
<nextsent>2methods that generate presentation (b)after generating represen-tation (a) are included.
</nextsent>
<nextsent>934 based representations, without expanding disjunctions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X365">
<title id=" P98-2154.xml">translating a unification grammar with dis junctions into logical constraints </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, it is effective to translate formalism (i) into representation (b).
</prevsent>
<prevsent>previous translation methods 2 (covington, 1989; hirsh, 1988; schster, 1993; erbach, 1995) <papid> E95-1025 </papid>are problematic that they cannot deal with dis-junctive feature descriptions, which reduce redundancies in grammar.</prevsent>
</prevsection>
<citsent citstr=" P88-1035 ">
moreover, incorporating disjunctive infor-mation into internal representation makes parsing more efficient (kasper, 1987; <papid> P87-1033 </papid>eisele and dsrre, 1988; <papid> P88-1035 </papid>maxwell and kaplan, 1991; hasida, 1986).<papid> C86-1018 </papid></citsent>
<aftsection>
<nextsent>this paper presents method for translating rammar formalism with disjunctive information based on path equations and lists of pairs of labels and values into term- since unspecified features are represented by variables in term unification, when most of the features are unspecified, it is inefficient to represent feature structures by terms.
</nextsent>
<nextsent>in current linguistic theories such as hpsg (pollard and sag, 1994), however, thanks to the type specifications, the number of features that feature structure can have is reduced, so it does not cause as much trouble.
</nextsent>
<nextsent>2methods that generate presentation (b)after generating represen-tation (a) are included.
</nextsent>
<nextsent>934 based representations, without expanding disjunctions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X366">
<title id=" P98-2154.xml">translating a unification grammar with dis junctions into logical constraints </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, it is effective to translate formalism (i) into representation (b).
</prevsent>
<prevsent>previous translation methods 2 (covington, 1989; hirsh, 1988; schster, 1993; erbach, 1995) <papid> E95-1025 </papid>are problematic that they cannot deal with dis-junctive feature descriptions, which reduce redundancies in grammar.</prevsent>
</prevsection>
<citsent citstr=" C86-1018 ">
moreover, incorporating disjunctive infor-mation into internal representation makes parsing more efficient (kasper, 1987; <papid> P87-1033 </papid>eisele and dsrre, 1988; <papid> P88-1035 </papid>maxwell and kaplan, 1991; hasida, 1986).<papid> C86-1018 </papid></citsent>
<aftsection>
<nextsent>this paper presents method for translating rammar formalism with disjunctive information based on path equations and lists of pairs of labels and values into term- since unspecified features are represented by variables in term unification, when most of the features are unspecified, it is inefficient to represent feature structures by terms.
</nextsent>
<nextsent>in current linguistic theories such as hpsg (pollard and sag, 1994), however, thanks to the type specifications, the number of features that feature structure can have is reduced, so it does not cause as much trouble.
</nextsent>
<nextsent>2methods that generate presentation (b)after generating represen-tation (a) are included.
</nextsent>
<nextsent>934 based representations, without expanding disjunctions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X368">
<title id=" P98-2154.xml">translating a unification grammar with dis junctions into logical constraints </title>
<section> logical-constraint-based grammar.  </section>
<citcontext>
<prevsection>
<prevsent>pos \] (8) sign agrsubj signagr \[ per 3rd sing \] \[ agr 3rd 3since there is no limitation on the number of arguments of macro, named is junctions can be described.
</prevsent>
<prevsent>4horn clauses are described in different notation from dec-10 prolog so as to indicate xplicitly that he bodies can be recognized as constraints.
</prevsent>
</prevsection>
<citsent citstr=" C90-3052 ">
935 (9) sign(v, agr( sing, 3rd), sign(_, agr( sing, 3rd), _)) feature structure (8) is o ped feature structure used in typed unification grammars (emele and zajac, 1990).<papid> C90-3052 </papid></citsent>
<aftsection>
<nextsent>the set of features that feature structure can have is specified according to types.
</nextsent>
<nextsent>in this paper, we do not consider type hierarchies.
</nextsent>
<nextsent>symbol  _  in (9) is an anonymous variable.
</nextsent>
<nextsent>the arguments of function symbol sign correspond to pos feature, agr feature, and subj feature values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X375">
<title id=" P98-2154.xml">translating a unification grammar with dis junctions into logical constraints </title>
<section> logical-constraint-based grammar.  </section>
<citcontext>
<prevsection>
<prevsent>the efficiency of another unification using the result-ing constraint depends on which form of constraint the transformation process has returned.
</prevsent>
<prevsent>obtaining compact constraints corresponds to avoiding unnecessary expan-sions of dis junctions in graph unification (kasper, 1987; <papid> P87-1033 </papid>eisele and dsrre, 1988).<papid> P88-1035 </papid></prevsent>
</prevsection>
<citsent citstr=" P91-1040 ">
some constraint transformation methods whose resulting constraints are compact have been proposed (hasida, 1986; <papid> C86-1018 </papid>nakano, 1991).<papid> P91-1040 </papid></citsent>
<aftsection>
<nextsent>by using these algorithms, we can efficiently analyze using lcgr.
</nextsent>
<nextsent>3.3 grammar representation.
</nextsent>
<nextsent>lcgr consists of set of phrase structure rules, set of lexical items, and database.
</nextsent>
<nextsent>each phrase structure role is tri plate ( --, ~, /, where is variable, ~ is list of variables, and is constraint on and variables in ~.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X378">
<title id=" S10-1079.xml">uoy graphs of unambiguous vertices for word sense induction and disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there exists significant evidence that word sense disambiguation is important for variety of natural language processing tasks: machine translation, information retrieval, grammatical analysis,speech and text processing (veronis, 2004).
</prevsent>
<prevsent>how ever, the fixed-list?
</prevsent>
</prevsection>
<citsent citstr=" W06-1669 ">
of senses paradigm, where the senses of target word is closed list of definitions coming from standard dictionary (agirreet al, 2006), <papid> W06-1669 </papid>was long ago abandoned.</citsent>
<aftsection>
<nextsent>the reason is that sense lists, such as wordnet (miller,1995), miss many senses, especially domain specific ones (pantel and lin, 2002).
</nextsent>
<nextsent>the missing concepts are not recognised.
</nextsent>
<nextsent>moreover, senses cannot be easily related to their use in context.
</nextsent>
<nextsent>word sense induction methods can be divided into vector-space models and graph based ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X379">
<title id=" S10-1079.xml">uoy graphs of unambiguous vertices for word sense induction and disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, senses cannot be easily related to their use in context.
</prevsent>
<prevsent>word sense induction methods can be divided into vector-space models and graph based ones.
</prevsent>
</prevsection>
<citsent citstr=" W06-1203 ">
in vector-space model, each context of target word is represented as feature vector, e.g. frequency of cooccurring words (katz and giesbrecht, 2006).<papid> W06-1203 </papid></citsent>
<aftsection>
<nextsent>context vectors are clustered and the resulting clusters represent the induced senses.recently, graph-based methods have been employed for word sense induction (agirre and soroa, 2007).<papid> W07-2002 </papid></nextsent>
<nextsent>typically, graph-based methods represent each context word of the target word as vertex.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X380">
<title id=" S10-1079.xml">uoy graphs of unambiguous vertices for word sense induction and disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense induction methods can be divided into vector-space models and graph based ones.
</prevsent>
<prevsent>in vector-space model, each context of target word is represented as feature vector, e.g. frequency of cooccurring words (katz and giesbrecht, 2006).<papid> W06-1203 </papid></prevsent>
</prevsection>
<citsent citstr=" W07-2002 ">
context vectors are clustered and the resulting clusters represent the induced senses.recently, graph-based methods have been employed for word sense induction (agirre and soroa, 2007).<papid> W07-2002 </papid></citsent>
<aftsection>
<nextsent>typically, graph-based methods represent each context word of the target word as vertex.
</nextsent>
<nextsent>two vertices are connected via an edge if they cooccur in one or more instances.
</nextsent>
<nextsent>oncethe cooccurrence graph has been constructed, different graph clustering algorithms are applied to partition the graph.
</nextsent>
<nextsent>each cluster (partition) consists of set of words that are semantically related to the particular sense (veronis, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X382">
<title id=" S10-1079.xml">uoy graphs of unambiguous vertices for word sense induction and disambiguation </title>
<section> word sense induction.  </section>
<citcontext>
<prevsection>
<prevsent>2 www-tsujii.is.s.u-tokyo.ac.jp/genia/tagger figure 1: an example showing how the proposed word sense induction system works.
</prevsent>
<prevsent>nouns that occur infrequently in the reference corpus are removed (parameter 1).
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
then, log likelihood ratio (ll) (dunning, 1993) <papid> J93-1003 </papid>is employed to compare the distribution of each noun to its distribution in reference corpus.</citsent>
<aftsection>
<nextsent>the null hypothesis is that the two distributions are similar.
</nextsent>
<nextsent>if this is true, ll is small value and the corresponding noun is removed (parameter 2 ).
</nextsent>
<nextsent>we also filter out nouns that are more indicative in the reference corpus than in the target word corpus;i.e. the nouns whose relative frequency in the former is larger than in the latter.
</nextsent>
<nextsent>at the end of this stage, each snippet is list of lemmatised nouns contextually related to the target word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X383">
<title id=" S10-1079.xml">uoy graphs of unambiguous vertices for word sense induction and disambiguation </title>
<section> word sense induction.  </section>
<citcontext>
<prevsection>
<prevsent>low weight edges are filtered out (parameter 3 ).
</prevsent>
<prevsent>2.3 clustering the graph.
</prevsent>
</prevsection>
<citsent citstr=" W06-3812 ">
chinese whispers (cw) (biemann, 2006) <papid> W06-3812 </papid>was used to cluster the graph.</citsent>
<aftsection>
<nextsent>cw is random ised graph-clustering algorithm, time-linear to the number of edges.
</nextsent>
<nextsent>the number of clusters it produces is automatically inferred.
</nextsent>
<nextsent>evaluation has shown that cw suits well in sense induction applications, where class distributions are often highly skewed.
</nextsent>
<nextsent>in our experiments, cw produced less clusters using constant mutation rate (5%).to further reduce the number of induced clusters, we applied post-processing stage, which exploits the one sense per collocation property (yarowsky, 1995).<papid> P95-1026 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X384">
<title id=" S10-1079.xml">uoy graphs of unambiguous vertices for word sense induction and disambiguation </title>
<section> word sense induction.  </section>
<citcontext>
<prevsection>
<prevsent>the number of clusters it produces is automatically inferred.
</prevsent>
<prevsent>evaluation has shown that cw suits well in sense induction applications, where class distributions are often highly skewed.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
in our experiments, cw produced less clusters using constant mutation rate (5%).to further reduce the number of induced clusters, we applied post-processing stage, which exploits the one sense per collocation property (yarowsky, 1995).<papid> P95-1026 </papid></citsent>
<aftsection>
<nextsent>for each cluster i, wegener ated the set i of all snippets that contain at least one vertex content of i . then, any clusters l. and b were merged if a ? b or a ? b .
</nextsent>
<nextsent>the induced senses are used to sense-tag each test instance of the target word (snippet).
</nextsent>
<nextsent>given snippet, each induced cluster is assigned score equal to the number of its vertex contents (single or pairs of words) occurring in the snippet.
</nextsent>
<nextsent>the instance is assigned to the sense with the highest score or with equal weights to all highest scoring senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X385">
<title id=" P98-2204.xml">never look back an alternative to centering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the model is the basis for an algorithm which operates incrementally, word by word.
</prevsent>
<prevsent>i propose model for determining the heater at- ten tional state in understanding discourse.
</prevsent>
</prevsection>
<citsent citstr=" P83-1007 ">
my pro-posal is inspired by the centering model (grosz et al, 1983; <papid> P83-1007 </papid>1995) and draws on the conclusions of strube &amp; hahn (1996) <papid> P96-1036 </papid>approach for the ranking of the forward-looking center list for german.</citsent>
<aftsection>
<nextsent>their approach as been proven as the point of departure for new model which is valid for english as well.
</nextsent>
<nextsent>the use of the centering transitions in brennan et al (1987) <papid> P87-1022 </papid>algorithm prevents it from being ap-plied incrementally (cf.</nextsent>
<nextsent>kehler (1997)).<papid> J97-3006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X387">
<title id=" P98-2204.xml">never look back an alternative to centering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the model is the basis for an algorithm which operates incrementally, word by word.
</prevsent>
<prevsent>i propose model for determining the heater at- ten tional state in understanding discourse.
</prevsent>
</prevsection>
<citsent citstr=" P96-1036 ">
my pro-posal is inspired by the centering model (grosz et al, 1983; <papid> P83-1007 </papid>1995) and draws on the conclusions of strube &amp; hahn (1996) <papid> P96-1036 </papid>approach for the ranking of the forward-looking center list for german.</citsent>
<aftsection>
<nextsent>their approach as been proven as the point of departure for new model which is valid for english as well.
</nextsent>
<nextsent>the use of the centering transitions in brennan et al (1987) <papid> P87-1022 </papid>algorithm prevents it from being ap-plied incrementally (cf.</nextsent>
<nextsent>kehler (1997)).<papid> J97-3006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X390">
<title id=" P98-2204.xml">never look back an alternative to centering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>my pro-posal is inspired by the centering model (grosz et al, 1983; <papid> P83-1007 </papid>1995) and draws on the conclusions of strube &amp; hahn (1996) <papid> P96-1036 </papid>approach for the ranking of the forward-looking center list for german.</prevsent>
<prevsent>their approach as been proven as the point of departure for new model which is valid for english as well.</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
the use of the centering transitions in brennan et al (1987) <papid> P87-1022 </papid>algorithm prevents it from being ap-plied incrementally (cf.</citsent>
<aftsection>
<nextsent>kehler (1997)).<papid> J97-3006 </papid></nextsent>
<nextsent>in my ap-proach, propose to replace the functions of the backward-looking center and the centering transi-tions by the order among the elements of the list of salient discourse ntities (s-list).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X393">
<title id=" P98-2204.xml">never look back an alternative to centering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>their approach as been proven as the point of departure for new model which is valid for english as well.
</prevsent>
<prevsent>the use of the centering transitions in brennan et al (1987) <papid> P87-1022 </papid>algorithm prevents it from being ap-plied incrementally (cf.</prevsent>
</prevsection>
<citsent citstr=" J97-3006 ">
kehler (1997)).<papid> J97-3006 </papid></citsent>
<aftsection>
<nextsent>in my ap-proach, propose to replace the functions of the backward-looking center and the centering transi-tions by the order among the elements of the list of salient discourse ntities (s-list).
</nextsent>
<nextsent>the s-list rank-ing criteria define preference for hearer-old over hearer-new discourse ntities (prince, 1981) gener-alizing strube &amp; hahn (1996) <papid> P96-1036 </papid>approach.</nextsent>
<nextsent>because of these ranking criteria, can account for the dif-ference in salience between definite nps (mostly hearer-old) and indefinite nps (mostly hearer-new).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X402">
<title id=" P98-2204.xml">never look back an alternative to centering </title>
<section> a look back: centering  </section>
<citcontext>
<prevsection>
<prevsent>the most highly ranked el-ement of cf(ui) that is realized in ui+x (i.e., is associated with an expression that has valid inter-pretation in the underlying semantic representation) is the cb(ui+l).
</prevsent>
<prevsent>therefore, the ranking on the cf plays crucial role in the model.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
grosz et al (1995) <papid> J95-2003 </papid>and brennan et al (1987) <papid> P87-1022 </papid>use grammatical relations to rank the cf (i.e., subj -.  obj -  ...)</citsent>
<aftsection>
<nextsent>but state that other factors might also play role.
</nextsent>
<nextsent>1251 cb(ui) = cp(vi) cb(ui) y?
</nextsent>
<nextsent>cp(t:i) for their centering algorithm, brennan et al (1987, <papid> P87-1022 </papid>henceforth bfp-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf.</nextsent>
<nextsent>table 1 taken from walker et al (1994)).<papid> J94-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X407">
<title id=" P98-2204.xml">never look back an alternative to centering </title>
<section> a look back: centering  </section>
<citcontext>
<prevsection>
<prevsent>1251 cb(ui) = cp(vi) cb(ui) y?
</prevsent>
<prevsent>cp(t:i) for their centering algorithm, brennan et al (1987, <papid> P87-1022 </papid>henceforth bfp-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf.</prevsent>
</prevsection>
<citsent citstr=" J94-2003 ">
table 1 taken from walker et al (1994)).<papid> J94-2003 </papid></citsent>
<aftsection>
<nextsent>cb(ui) = cb(ui-1) cb(ui) or no cb(ui-1) cb(vi-1) continue smooth-shift retain rough-shift table 1: transition types brennan et al (1987) <papid> P87-1022 </papid>modify the second of two rules on center movement and realization which were defined by grosz et al (1983), <papid> P83-1007 </papid>grosz et al (1995): <papid> J95-2003 </papid>rule 1: if some element of cf(ui-1) is realized as pronoun in ui, then so is cb(ui).</nextsent>
<nextsent>rule 2  transition states are ordered.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X419">
<title id=" P98-2204.xml">never look back an alternative to centering </title>
<section> some empirical dat:i.  </section>
<citcontext>
<prevsection>
<prevsent>in the first experiment, compare my algorithm with the bfp-algorithm which was in second experi-ment extended by the constraints for complex sen-tences as described by kameyama (1998).
</prevsent>
<prevsent>method.
</prevsent>
</prevsection>
<citsent citstr=" P89-1031 ">
i use the following guidelines for the hand-simulated analysis (walker, 1989).<papid> P89-1031 </papid></citsent>
<aftsection>
<nextsent>i do not as-sume any world knowledge as part of the anaphora resolution process.
</nextsent>
<nextsent>only agreement criteria, bind-ing and sortal constraints are applied.
</nextsent>
<nextsent>i do not ac-count for false positives and error chains.
</nextsent>
<nextsent>following walker (1989), <papid> P89-1031 </papid>segment is defined as paragraph unless its first sentence has pronoun in subject po-sition or pronoun where none of the preceding sentence-internal oun phrases matches its syntactic features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X421">
<title id=" P98-2204.xml">never look back an alternative to centering </title>
<section> comparison to related approaches.  </section>
<citcontext>
<prevsection>
<prevsent>the com-plete short stories of ernest hemingway.
</prevsent>
<prevsent>new york: charles scribner sons, 1987, p.60.
</prevsent>
</prevsection>
<citsent citstr=" J94-2006 ">
1256 the focus model (sidner, 1983; suri &amp; mccoy, 1994) <papid> J94-2006 </papid>accounts for evoked is course entities explic-itly because it uses the discourse focus, which is de-termined by successful anaphora resolution.</citsent>
<aftsection>
<nextsent>in-cremental processing is not topic of these papers.
</nextsent>
<nextsent>even models which use salience measures forde- termining the antecedents of pronoun use the con-cept of evoked discourse entities.
</nextsent>
<nextsent>haji~ov~i et al (1992) assign the highest value to an evoked dis-course entity.
</nextsent>
<nextsent>also lappin &amp; leass (1994), <papid> J94-4002 </papid>who give the subject of the current sentence the high-est weight, have an implicit notion of evokedness.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X422">
<title id=" P98-2204.xml">never look back an alternative to centering </title>
<section> comparison to related approaches.  </section>
<citcontext>
<prevsection>
<prevsent>even models which use salience measures forde- termining the antecedents of pronoun use the con-cept of evoked discourse entities.
</prevsent>
<prevsent>haji~ov~i et al (1992) assign the highest value to an evoked dis-course entity.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
also lappin &amp; leass (1994), <papid> J94-4002 </papid>who give the subject of the current sentence the high-est weight, have an implicit notion of evokedness.</citsent>
<aftsection>
<nextsent>the salience weight degrades from one sentence to another by factor of two which implies that re-peatedly mentioned discourse entity gets higher weight than brand-new subject.
</nextsent>
<nextsent>in this paper, proposed model for determining the hearer attentional state which is based on the distinction between hearer-old and hearer-new dis-course entities.
</nextsent>
<nextsent>i showed that my model, though it omits the backward-looking center and the cen-tering transitions, does not lose any of the predic-tive power of the centering model with respect to anaphora resolution.
</nextsent>
<nextsent>in contrast to the centering model, my model includes treatment for intra- sentential anaphora and is sufficiently well specified to be applied to real texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X424">
<title id=" S12-1091.xml">sriubc simple similarity features for semantic textual similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this system underperformed, primarily due to errors in the dataset predictor.
</prevsent>
<prevsent>previous semantic similarity tasks, such as paraphrase identification or recognizing textual entailment, have focused on performing binary decisions.these problems are usually framed in terms of identifying whether pair of texts exhibit the needed similarity or entailment relationship or not.
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
in many cases, such as producing ranking over similarity scores, soft measure of similarity between pair of texts would be more desirable.we contributed three systems for the 2012 semantic textual similarity (sts) task (agirre et al, 2012).<papid> S12-1051 </papid></citsent>
<aftsection>
<nextsent>these are: 1.
</nextsent>
<nextsent>system 1, which used combination of seman-.
</nextsent>
<nextsent>tic similarity, lexical similarity, and precision focused part-of-speech (pos) features.
</nextsent>
<nextsent>1, with the addition of skip-bigram features derived from the rouge-s (lin, 2004) <papid> W04-1013 </papid>measure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X425">
<title id=" S12-1091.xml">sriubc simple similarity features for semantic textual similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>system 1, which used combination of seman-.
</prevsent>
<prevsent>tic similarity, lexical similarity, and precision focused part-of-speech (pos) features.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
1, with the addition of skip-bigram features derived from the rouge-s (lin, 2004) <papid> W04-1013 </papid>measure.</citsent>
<aftsection>
<nextsent>pos variants of skip-bigrams were incorporated as well.
</nextsent>
<nextsent>3.
</nextsent>
<nextsent>system 3, used the features from above to first.
</nextsent>
<nextsent>classify the dataset the pair was drawn from, and then applied regress ors trained for that dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X426">
<title id=" S12-1091.xml">sriubc simple similarity features for semantic textual similarity </title>
<section> cosine-based lexical similarity measure..  </section>
<citcontext>
<prevsection>
<prevsent>wordnet is lexical semantic resource that describes typed relationships between synsets, semantic categories word may belong to.
</prevsent>
<prevsent>similarity scoring methods identify the synsets associated with pair of words, and then use this relationship graph to generate score.
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
the first set of scorers were generated from the leacock-chodorow, lin, and wu-palmer measures from the wordnet similarity package (pedersen et al., 2004).<papid> N04-3012 </papid></citsent>
<aftsection>
<nextsent>for each of these measures, we averaged across all of the possible synsets between given pair of words.another scorer we used was personalized pager ank (ppr) (agirre et al, 2010), topic sensitive variant of the page rank algorithm (page et al, 1999) that uses random walk process to identify the significant nodes of graph given its link structure.
</nextsent>
<nextsent>we first derived graph from wordnet, treating synsets as the vertices and the relationships between synsets as the edges.
</nextsent>
<nextsent>to obtain signature forgiven word, we apply topic sensitive pagerank(haveliwala, 2002) over g, using the synsets associated with the word as the initial distribution.
</nextsent>
<nextsent>at convergence, we convert the stationary distribution into vector.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X427">
<title id=" S12-1091.xml">sriubc simple similarity features for semantic textual similarity </title>
<section> bleu features.  </section>
<citcontext>
<prevsection>
<prevsent>another scorer we used was the cosine similarity over the lemmas found in the sentences in pair.for generating the vectors used in the cosine similarity computation, we used the term frequency of the lemmas.
</prevsent>
<prevsent>bleu is measure developed to automatically assess how closely sentences generated by machine translation systems match reference human generated texts.
</prevsent>
</prevsection>
<citsent citstr=" I05-5003 ">
bleu is directional measurement, and works on the assumption that the more lexically similar system generated sentence is to reference sentence, human generated translation, the better the system sentence is. this can also be seen as stand in for the semantic similarity of the pairs, as was shown when bleu was applied to the paraphrase identification identification problem in (finch et al, 2005).<papid> I05-5003 </papid></citsent>
<aftsection>
<nextsent>the bleu score forgiven system sentence and reference sentence of order is computed using formula 2.
</nextsent>
<nextsent>bleu(sys, ref) = ? exp n?
</nextsent>
<nextsent>n=1 1 log(pn) (2) 619 is brevity penalty used to prevent degeneratetranslations.
</nextsent>
<nextsent>given this has little bearing on our experiments, we set its value to 1 for our experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X429">
<title id=" S12-1091.xml">sriubc simple similarity features for semantic textual similarity </title>
<section> bleu features.  </section>
<citcontext>
<prevsection>
<prevsent>n=1 1 log(pn) (2) 619 is brevity penalty used to prevent degeneratetranslations.
</prevsent>
<prevsent>given this has little bearing on our experiments, we set its value to 1 for our experiments.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
following (papineni et al, 2002), <papid> P02-1040 </papid>we give each ordern equal weight in the geometric mean.</citsent>
<aftsection>
<nextsent>the probability of an order n-gram from the system sentence being found in the reference, pn, is given in formula 3.
</nextsent>
<nextsent>pn = ? ngramsys countsysref (ngram) ? ngramsys countsys(ngram) (3)countsys(ngram) is frequency of occurrence for the given n-gram in the system sentence.
</nextsent>
<nextsent>the numerator term is computed as countsysref (ngram) = min(countsys(ngram), count ref (ngram)) where count ref (ngram) is the frequency of occurrence of that n-gram in the reference sentence.
</nextsent>
<nextsent>this is equivalent to having each n-gram have 1-1 mapping with matching n-gram in the reference (if any), and counting the number of mappings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X433">
<title id=" S12-1091.xml">sriubc simple similarity features for semantic textual similarity </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>for our experiments, we only considered split-bigrampos features of up to distance 3.
</prevsent>
<prevsent>in our initial experiments we found split-bigram pos features helped only in the case of shorter sentence pairs, so we only generated features if both the sentences in given pair contained ten tokens or less.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
for all three systems, we used the stanfordcorenlp (toutanova et al, 2003) <papid> N03-1033 </papid>package to perform lemmatization and pos tagging of the in put sentences.</citsent>
<aftsection>
<nextsent>for regress ors, we used libsvms(chang and lin, 2011) support vector regression capability, using radial basis kernels.
</nextsent>
<nextsent>based off of tuning on the training set, we set ? = 1 and the default dataset mean std.dev msrpar 3.322 0.9294 msrvid 2.135 1.595 smteur 4.307 0.7114 table 1: means and standard deviations of similarity scores for each of the training datasets.
</nextsent>
<nextsent>slack value.from previous experience with paraphrase identification over the msr paraphrase corpus, we retained stop words in all of our experiments.
</nextsent>
<nextsent>as the sts training data was broken into three separate datasets, each with their own distinct statistics, we developed three regress ors trained individually on each of these datasets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X434">
<title id=" S12-1091.xml">sriubc simple similarity features for semantic textual similarity </title>
<section> dealing with surprise data.  </section>
<citcontext>
<prevsection>
<prevsent>the approach taken by system 3 was to consider the feature vectors themselves as capturing information about which dataset they were drawn from, and to use classifier to predict that dataset.
</prevsent>
<prevsent>we then emit the score from the regress or trained on just that matching dataset.
</prevsent>
</prevsection>
<citsent citstr=" N03-5008 ">
we used the stanford classifiers (manning and klein, 2003) <papid> N03-5008 </papid>multinomial logistic regression as our dataset predictor, using the feature vectors from system 2.</citsent>
<aftsection>
<nextsent>five-fold cross validation over the training data showed the dataset predictor to have an overall accuracy of 91.75%.
</nextsent>
<nextsent>in order to assess performance over the known datasets at test time, system 3 also applied the same strategy for the msrpar, msrvid, and smteuroparl test sets.
</nextsent>
<nextsent>621 sys all allnorm mean msrpar msrvid smteur onwn smtnews 1 0.7513 / 11 0.8017 / 40 0.5997 / 22 0.6084 0.7458 0.4688 0.6315 0.3994 2 0.7562 / 10 0.8111 / 24 0.5858 / 33 0.6050 0.7939 0.4294 0.5871 0.3366 3 0.6876 / 21 0.7812 / 54 0.4668 / 68 0.4791 0.7901 0.2159 0.3843 0.2801 table 2: pearson correlation of described systems against test data, by dataset.
</nextsent>
<nextsent>overall measures are all indicates the combined pearson, allnorm the normalized variant, and mean the macro average of pearson correlations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X435">
<title id=" S12-1091.xml">sriubc simple similarity features for semantic textual similarity </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>future avenues of investigation would dataset prec rec f1 msrpar 0.8901 0.8853 0.8877 msrvid 0.9775 0.9827 0.9801 smteur 0.8842 0.8842 0.8842 table 4: results on classifying pairs by source dataset, using five-fold cross validation over training data.
</prevsent>
<prevsent>be to include the use of syntactic information, inorder to obtain better predicate-argument information.
</prevsent>
</prevsection>
<citsent citstr=" P09-1053 ">
syntactic information has proven useful for the paraphrase identification task over msrpar, as demonstrated in studies such as (das and smith, 2009) <papid> P09-1053 </papid>and (socher et al, 2011).</citsent>
<aftsection>
<nextsent>furthermore, qualitative assessment of the pairs across different datasets showed relatively significant differences,which would strengthen the argument for developing features and methods specific to each dataset.another improvement would be to develop better dataset predictor for system 3.
</nextsent>
<nextsent>also recognizing there may be ways to normalize and rescale scores across datasets so the regression models used do nothave to account for differing means and standard deviations.
</nextsent>
<nextsent>finally, there are other bodies of source data that may be adapted for use with the sts task, such as the paraphrasing pairs of the recognizing textual entailment challenges, human generated reference translations for machine translation evaluation, and human generated summaries used for summarization evaluations.
</nextsent>
<nextsent>although these are gold decisions, at the very least they could provide source of high similarity pairs, from which one could manufacture lower scoring variants.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X436">
<title id=" P98-2179.xml">generating the structure of argument </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these structures are found with great frequency in natural argument, and cannot, therefore, be ignored.
</prevsent>
<prevsent>parallel structures form the very basis of argument, since only the most trivial will involve lines of reasoning in which single premise supports single conclusion.
</prevsent>
</prevsection>
<citsent citstr=" J87-1002 ">
multiple sub arguments conjoined to support 1091 conclusion are the norm (see for example, cohen (1987), <papid> J87-1002 </papid>reed and long, 1997b), and these necessarily form parallel structures.</citsent>
<aftsection>
<nextsent>another shortcoming is highlighted by dissonance between rst and argument analysis (see eemeren et al (1996) for review).
</nextsent>
<nextsent>a given text may be amenable to multiple rst analyses - not just as result of ambiguity, but because there are, at fundamental level,  multiple compatible analyses .
</nextsent>
<nextsent>this contrasts with the view in argumentation theory, where one argument has single, unequivocal structure.
</nextsent>
<nextsent>there may, of course, be practical problems in identifying this structure, and two analysts may disagree on the most appropriate analysis (and indeed this latter has close parallel in rst, since different analysts are at liberty to make different  plausibility judgements  as to the aims of the speaker).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X437">
<title id=" P98-2179.xml">generating the structure of argument </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although there is much debate over the number and range of rhetorical relations (e.g. hovy, 1993) there seems to be no way of dealing with the idea of argumentative support.
</prevsent>
<prevsent>in the first place, as snoeck-henkemans (1997) points out, motivation, evidence, justification, cause, solution hood and other relations could all be used argumentatively (as well, of course, as being applicable in non-argumentative situations).
</prevsent>
</prevsection>
<citsent citstr=" C92-2096 ">
elhadad (1992) <papid> C92-2096 </papid>draws similar conclusion (though his list of potentially argumentative lations is somewhat shorter).</citsent>
<aftsection>
<nextsent>thus it is impossible to identify an argumentative lation on the basis of rst alone.
</nextsent>
<nextsent>secondly, rst offers no way of capturing higher level organisational units, such as modus ponens, modus tollens, and so on.
</nextsent>
<nextsent>for although their structure (or at least the structure of any one instance) can be represented in rst - and, given marcu (1996) elegant extensions, even their hierarchical use in larger units - adopting this approach necessitates lower level view.
</nextsent>
<nextsent>it becomes impossible to represent and employ modus tollens sub argument supporting the antecedent of modus ponens - rather, the situation can only be characterised as supporting through one of the potentially argumentative rst relations q, and showing that -q, so -p, and -p then supporting through one of the potentially argumentative rst relations r, therefore r. apart from being obviously cumbersome, the representation has lost the abstract structure of the argument altogether, and is not generali sable and comparable to other similar argument structures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X438">
<title id=" P98-2179.xml">generating the structure of argument </title>
<section> abstraction-based planning.  </section>
<citcontext>
<prevsection>
<prevsent>the operators employed by abnlp utilise highly parsimonious set of intentional goals.
</prevsent>
<prevsent>belief goals are used to build the content of an argument (as in much other nlg work); saliency goals to express the intention to convey information to the hearer (following notion of saliency similar to that proposed in walker, 1996); and topic manipulation goals to control the focus of attention through the discourse.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
the roles of these goals and their interrelationships are explored in relation to the information- intention-attention model of grosz and sidner (1986) <papid> J86-3001 </papid>in more detail in reed and long (1997a).</citsent>
<aftsection>
<nextsent>the choice of operators implemented in the rhetorica system has been influenced by number of factors.
</nextsent>
<nextsent>the rules of inference are clear candidates for operationalisation: moves such as modus ponens are clearly vital components of any argument - though, as noted in grosz and sidner (1986), <papid> J86-3001 </papid>p201, it is inappropriate oview the implication step as one of conventional material implication.</nextsent>
<nextsent>the relationship rather one of support - the hearer must be brought to believe that (given the current context and domain of discourse) the first proposition warrants, in part, concluding the second.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X441">
<title id=" P98-2251.xml">predicting partof speech information about unknown words using statistical methods </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>part-of-speech tagging involves selecting the most likely sequence of syntactic ategories for the words in sentence.
</prevsent>
<prevsent>these syntactic at- egories, or tags, generally consist of parts of speech, often with feature information included.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
an example set of tags can be found in the penn treebank project (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>part-of- speech tagging is useful for speeding up parsing systems, and allowing the use of partial parsing.
</nextsent>
<nextsent>many current systems make use of hid-den markov model (hmm) for part-of-speech tagging.
</nextsent>
<nextsent>other methods include rule-based systems (brill, 1995), <papid> J95-4004 </papid>maximum entropy mod-els (ratnaparkhi, 1996), <papid> W96-0213 </papid>and memory-based models (daelemans et al, 1996).<papid> W96-0102 </papid></nextsent>
<nextsent>in an hmm tagger the markov assumption is made so that the current word depends only on the current tag, and the current tag depends only on ad-jacent tags.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X442">
<title id=" P98-2251.xml">predicting partof speech information about unknown words using statistical methods </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>part-of- speech tagging is useful for speeding up parsing systems, and allowing the use of partial parsing.
</prevsent>
<prevsent>many current systems make use of hid-den markov model (hmm) for part-of-speech tagging.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
other methods include rule-based systems (brill, 1995), <papid> J95-4004 </papid>maximum entropy mod-els (ratnaparkhi, 1996), <papid> W96-0213 </papid>and memory-based models (daelemans et al, 1996).<papid> W96-0102 </papid></citsent>
<aftsection>
<nextsent>in an hmm tagger the markov assumption is made so that the current word depends only on the current tag, and the current tag depends only on ad-jacent tags.
</nextsent>
<nextsent>charniak (charniak et al, 1993) gives thorough explanation of the equations for an hmm model, and kupiec (kupiec, 1992) describes an hmm tagging system in detail.
</nextsent>
<nextsent>one important area of research in part-of- speech tagging is how to handle unknown words.
</nextsent>
<nextsent>if word is not in the lexicon, then the lexical probabilities must be provided from some other source.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X443">
<title id=" P98-2251.xml">predicting partof speech information about unknown words using statistical methods </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>part-of- speech tagging is useful for speeding up parsing systems, and allowing the use of partial parsing.
</prevsent>
<prevsent>many current systems make use of hid-den markov model (hmm) for part-of-speech tagging.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
other methods include rule-based systems (brill, 1995), <papid> J95-4004 </papid>maximum entropy mod-els (ratnaparkhi, 1996), <papid> W96-0213 </papid>and memory-based models (daelemans et al, 1996).<papid> W96-0102 </papid></citsent>
<aftsection>
<nextsent>in an hmm tagger the markov assumption is made so that the current word depends only on the current tag, and the current tag depends only on ad-jacent tags.
</nextsent>
<nextsent>charniak (charniak et al, 1993) gives thorough explanation of the equations for an hmm model, and kupiec (kupiec, 1992) describes an hmm tagging system in detail.
</nextsent>
<nextsent>one important area of research in part-of- speech tagging is how to handle unknown words.
</nextsent>
<nextsent>if word is not in the lexicon, then the lexical probabilities must be provided from some other source.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X444">
<title id=" P98-2251.xml">predicting partof speech information about unknown words using statistical methods </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>part-of- speech tagging is useful for speeding up parsing systems, and allowing the use of partial parsing.
</prevsent>
<prevsent>many current systems make use of hid-den markov model (hmm) for part-of-speech tagging.
</prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
other methods include rule-based systems (brill, 1995), <papid> J95-4004 </papid>maximum entropy mod-els (ratnaparkhi, 1996), <papid> W96-0213 </papid>and memory-based models (daelemans et al, 1996).<papid> W96-0102 </papid></citsent>
<aftsection>
<nextsent>in an hmm tagger the markov assumption is made so that the current word depends only on the current tag, and the current tag depends only on ad-jacent tags.
</nextsent>
<nextsent>charniak (charniak et al, 1993) gives thorough explanation of the equations for an hmm model, and kupiec (kupiec, 1992) describes an hmm tagging system in detail.
</nextsent>
<nextsent>one important area of research in part-of- speech tagging is how to handle unknown words.
</nextsent>
<nextsent>if word is not in the lexicon, then the lexical probabilities must be provided from some other source.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X445">
<title id=" P98-2251.xml">predicting partof speech information about unknown words using statistical methods </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>if word is not in the lexicon, then the lexical probabilities must be provided from some other source.
</prevsent>
<prevsent>one common approach is to use affixa- tion rules to  learn  the probabilities for words based on their suffixes or prefixes.
</prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
weischedel group (weischedel et al, 1993) <papid> J93-2006 </papid>examines un-known words in the context of part-of-speech tagging.</citsent>
<aftsection>
<nextsent>their method creates probability dis-tribution for an unknown word based on certain features: word endings, hyphenation, and capi-talization.
</nextsent>
<nextsent>the features to be used are chosen by hand for the system.
</nextsent>
<nextsent>mikheev (mikheev, 1996; <papid> P96-1043 </papid>mikheev, 1997) <papid> J97-3003 </papid>uses general purpose lexicon to learn affix and word ending information to be used in tagging unknown words.</nextsent>
<nextsent>his work re-turns set of possible tags for unknown words, with no probabilities attached, relying on the tagger to disambiguate hem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X446">
<title id=" P98-2251.xml">predicting partof speech information about unknown words using statistical methods </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>their method creates probability dis-tribution for an unknown word based on certain features: word endings, hyphenation, and capi-talization.
</prevsent>
<prevsent>the features to be used are chosen by hand for the system.
</prevsent>
</prevsection>
<citsent citstr=" P96-1043 ">
mikheev (mikheev, 1996; <papid> P96-1043 </papid>mikheev, 1997) <papid> J97-3003 </papid>uses general purpose lexicon to learn affix and word ending information to be used in tagging unknown words.</citsent>
<aftsection>
<nextsent>his work re-turns set of possible tags for unknown words, with no probabilities attached, relying on the tagger to disambiguate hem.
</nextsent>
<nextsent>this work investigates the possibility of au-tomatically creating probability distribution over all tags for an unknown word, instead of simple set of tags.
</nextsent>
<nextsent>this can be done by creat-ing probabilistic lexicon from large tagged corpus (in this case, the brown corpus), and us-ing that data to estimate distributions for words with given  prefix  or  suffix .
</nextsent>
<nextsent>prefix and suffix indicate sub strings that come at the be-ginning and end of word respectively, and are not necessarily morphologically meaningful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X447">
<title id=" P98-2251.xml">predicting partof speech information about unknown words using statistical methods </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>their method creates probability dis-tribution for an unknown word based on certain features: word endings, hyphenation, and capi-talization.
</prevsent>
<prevsent>the features to be used are chosen by hand for the system.
</prevsent>
</prevsection>
<citsent citstr=" J97-3003 ">
mikheev (mikheev, 1996; <papid> P96-1043 </papid>mikheev, 1997) <papid> J97-3003 </papid>uses general purpose lexicon to learn affix and word ending information to be used in tagging unknown words.</citsent>
<aftsection>
<nextsent>his work re-turns set of possible tags for unknown words, with no probabilities attached, relying on the tagger to disambiguate hem.
</nextsent>
<nextsent>this work investigates the possibility of au-tomatically creating probability distribution over all tags for an unknown word, instead of simple set of tags.
</nextsent>
<nextsent>this can be done by creat-ing probabilistic lexicon from large tagged corpus (in this case, the brown corpus), and us-ing that data to estimate distributions for words with given  prefix  or  suffix .
</nextsent>
<nextsent>prefix and suffix indicate sub strings that come at the be-ginning and end of word respectively, and are not necessarily morphologically meaningful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X448">
<title id=" S10-1028.xml">owns cross lingual word sense disambiguation using weighted overlap counts and wordnet based similarity measures </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in section 3 we describe our approach.
</prevsent>
<prevsent>in section 4 we present the results followed by conclusion in section 5.
</prevsent>
</prevsection>
<citsent citstr=" C96-1005 ">
knowledge based approaches to wsd such as lesks algorithm (lesk, 1986), walkers algorithm (walker and amsler, 1986), conceptual density(agirre and rigau, 1996) <papid> C96-1005 </papid>and random walk algorithm (mihalcea, 2005) <papid> H05-1052 </papid>are fundamentally overlap based algorithms which suffer from data sparsity.</citsent>
<aftsection>
<nextsent>while these approaches do well in cases where there is surface match (i.e., exact word match) between two occurrences of the target word (say, training and test sentence) they fail in cases where their is semantic match between two occurrences of the target word even though there is no surface match between them.
</nextsent>
<nextsent>the main reason for this failure is that these approaches do not take into account semantic generalizations (e.g., train is vehicle).
</nextsent>
<nextsent>on the other hand, wsd approaches which use wordnet based semantic similarity measures (patwardhan et al, 2003) account for such semantic generalizations and can be used in conjunction with overlap based approaches.
</nextsent>
<nextsent>we therefore propose scoring function which combines the strength of overlap based approaches ? frequently co-occurring words indeed provide strong clues ? with semantic generalizations using wordnet based similarity measures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X449">
<title id=" S10-1028.xml">owns cross lingual word sense disambiguation using weighted overlap counts and wordnet based similarity measures </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in section 3 we describe our approach.
</prevsent>
<prevsent>in section 4 we present the results followed by conclusion in section 5.
</prevsent>
</prevsection>
<citsent citstr=" H05-1052 ">
knowledge based approaches to wsd such as lesks algorithm (lesk, 1986), walkers algorithm (walker and amsler, 1986), conceptual density(agirre and rigau, 1996) <papid> C96-1005 </papid>and random walk algorithm (mihalcea, 2005) <papid> H05-1052 </papid>are fundamentally overlap based algorithms which suffer from data sparsity.</citsent>
<aftsection>
<nextsent>while these approaches do well in cases where there is surface match (i.e., exact word match) between two occurrences of the target word (say, training and test sentence) they fail in cases where their is semantic match between two occurrences of the target word even though there is no surface match between them.
</nextsent>
<nextsent>the main reason for this failure is that these approaches do not take into account semantic generalizations (e.g., train is vehicle).
</nextsent>
<nextsent>on the other hand, wsd approaches which use wordnet based semantic similarity measures (patwardhan et al, 2003) account for such semantic generalizations and can be used in conjunction with overlap based approaches.
</nextsent>
<nextsent>we therefore propose scoring function which combines the strength of overlap based approaches ? frequently co-occurring words indeed provide strong clues ? with semantic generalizations using wordnet based similarity measures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X450">
<title id=" S10-1028.xml">owns cross lingual word sense disambiguation using weighted overlap counts and wordnet based similarity measures </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, wsd approaches which use wordnet based semantic similarity measures (patwardhan et al, 2003) account for such semantic generalizations and can be used in conjunction with overlap based approaches.
</prevsent>
<prevsent>we therefore propose scoring function which combines the strength of overlap based approaches ? frequently co-occurring words indeed provide strong clues ? with semantic generalizations using wordnet based similarity measures.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
the disambiguation is then done using k-nn (ng and lee, 1996) <papid> P96-1006 </papid>where the nearest neighbors of the test sentence are identified using this scoring function.</citsent>
<aftsection>
<nextsent>once the nearest neighbors have been identified, the best translation is found by taking majority vote over the translations of these nearest neighbors.
</nextsent>
<nextsent>in this section we explain our approach for cross language word sense disambiguation.
</nextsent>
<nextsent>the main emphasis is on disambiguation i.e. finding english sentences from the training data which are closely related to the test sentence.
</nextsent>
<nextsent>3.1 motivating examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X451">
<title id=" S10-1028.xml">owns cross lingual word sense disambiguation using weighted overlap counts and wordnet based similarity measures </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>in other words, we placed higher beton overlap(s 1 , 2 ) than on semantic sim(s 1 , 2 ) as we found the former to be more reliable.
</prevsent>
<prevsent>3.3 finding translations of the target word.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
we used giza++2 (och and ney, 2003), freely available implementation of the ibm alignment models (brown et al, 1993) <papid> J93-2003 </papid>to get word level alignments for the sentences in the english-french 2http://sourceforge.net/projects/giza/portion of the europarl corpus.</citsent>
<aftsection>
<nextsent>under this alignment, each word in the source sentence is aligned to zero or more words in the corresponding target sentence.
</nextsent>
<nextsent>once the nearest neighbors for test sentence are identified using the similarity score described earlier, we use the word alignment models to find the french translation of the target word in the top-k nearest training sentences.
</nextsent>
<nextsent>these translations are then ranked according to the number of times they appear in these top-k nearest neighbors.
</nextsent>
<nextsent>the top-5 most frequent translations are then returned as the output.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X452">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the semeval test set.
</prevsent>
<prevsent>recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task.
</prevsent>
</prevsection>
<citsent citstr=" C08-1107 ">
algorithms for computing semantic textual similarity (sts) are relevant for variety of applications, including information extraction (szpektor and dagan, 2008), <papid> C08-1107 </papid>question answering (harabagiu and hickl, 2006)<papid> P06-1114 </papid>and machine translation (mirkin et al, 2009).<papid> P09-1089 </papid></citsent>
<aftsection>
<nextsent>word or term-level sts (a special case of sentence levelsts) has also been successfully applied to the problem of grammar induction (meng and siu, 2002) and affective text categorization (malandrakis et al, 2011).
</nextsent>
<nextsent>in this work, we built on previous research on word-level semantic similarity estimation to design and implement system for sentence-level sts for task6 of the semeval12 campaign.semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (agirreand edmonds, 2007).
</nextsent>
<nextsent>metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (miller, 1990; budanitsky and hirst, 2006) <papid> J06-1003 </papid>and (ii) corpus-based metrics (baroni and lenci, 2010; <papid> J10-4006 </papid>iosif and potamianos, 2010).</nextsent>
<nextsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible inter pre tations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X453">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the semeval test set.
</prevsent>
<prevsent>recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task.
</prevsent>
</prevsection>
<citsent citstr=" P06-1114 ">
algorithms for computing semantic textual similarity (sts) are relevant for variety of applications, including information extraction (szpektor and dagan, 2008), <papid> C08-1107 </papid>question answering (harabagiu and hickl, 2006)<papid> P06-1114 </papid>and machine translation (mirkin et al, 2009).<papid> P09-1089 </papid></citsent>
<aftsection>
<nextsent>word or term-level sts (a special case of sentence levelsts) has also been successfully applied to the problem of grammar induction (meng and siu, 2002) and affective text categorization (malandrakis et al, 2011).
</nextsent>
<nextsent>in this work, we built on previous research on word-level semantic similarity estimation to design and implement system for sentence-level sts for task6 of the semeval12 campaign.semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (agirreand edmonds, 2007).
</nextsent>
<nextsent>metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (miller, 1990; budanitsky and hirst, 2006) <papid> J06-1003 </papid>and (ii) corpus-based metrics (baroni and lenci, 2010; <papid> J10-4006 </papid>iosif and potamianos, 2010).</nextsent>
<nextsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible inter pre tations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X454">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the semeval test set.
</prevsent>
<prevsent>recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task.
</prevsent>
</prevsection>
<citsent citstr=" P09-1089 ">
algorithms for computing semantic textual similarity (sts) are relevant for variety of applications, including information extraction (szpektor and dagan, 2008), <papid> C08-1107 </papid>question answering (harabagiu and hickl, 2006)<papid> P06-1114 </papid>and machine translation (mirkin et al, 2009).<papid> P09-1089 </papid></citsent>
<aftsection>
<nextsent>word or term-level sts (a special case of sentence levelsts) has also been successfully applied to the problem of grammar induction (meng and siu, 2002) and affective text categorization (malandrakis et al, 2011).
</nextsent>
<nextsent>in this work, we built on previous research on word-level semantic similarity estimation to design and implement system for sentence-level sts for task6 of the semeval12 campaign.semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (agirreand edmonds, 2007).
</nextsent>
<nextsent>metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (miller, 1990; budanitsky and hirst, 2006) <papid> J06-1003 </papid>and (ii) corpus-based metrics (baroni and lenci, 2010; <papid> J10-4006 </papid>iosif and potamianos, 2010).</nextsent>
<nextsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible inter pre tations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X455">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word or term-level sts (a special case of sentence levelsts) has also been successfully applied to the problem of grammar induction (meng and siu, 2002) and affective text categorization (malandrakis et al, 2011).
</prevsent>
<prevsent>in this work, we built on previous research on word-level semantic similarity estimation to design and implement system for sentence-level sts for task6 of the semeval12 campaign.semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (agirreand edmonds, 2007).
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (miller, 1990; budanitsky and hirst, 2006) <papid> J06-1003 </papid>and (ii) corpus-based metrics (baroni and lenci, 2010; <papid> J10-4006 </papid>iosif and potamianos, 2010).</citsent>
<aftsection>
<nextsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible interpretations.
</nextsent>
<nextsent>sts is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (madnani and dorr, 2010) <papid> J10-3003 </papid>and textual entailment, which is directional and based on relations between semantics (dagan et al, 2006).related methods incorporate measurements of similarity at various levels: lexical (malakasiotis and androutsopoulos, 2007), <papid> W07-1407 </papid>syntactic (malakasiotis, 2009; <papid> P09-3004 </papid>zanzotto et al, 2009), and semantic (rinaldi et al, 2003; <papid> W03-1604 </papid>bos and markert, 2005).<papid> H05-1079 </papid></nextsent>
<nextsent>measures from machine translation evaluation are often used to evaluate lexical level approaches (finch et al, 2005; <papid> I05-5003 </papid>perez and alfonseca, 2005), including bleu(papineni et al, 2002), <papid> P02-1040 </papid>metric based on word gram hit rates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X456">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word or term-level sts (a special case of sentence levelsts) has also been successfully applied to the problem of grammar induction (meng and siu, 2002) and affective text categorization (malandrakis et al, 2011).
</prevsent>
<prevsent>in this work, we built on previous research on word-level semantic similarity estimation to design and implement system for sentence-level sts for task6 of the semeval12 campaign.semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (agirreand edmonds, 2007).
</prevsent>
</prevsection>
<citsent citstr=" J10-4006 ">
metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (miller, 1990; budanitsky and hirst, 2006) <papid> J06-1003 </papid>and (ii) corpus-based metrics (baroni and lenci, 2010; <papid> J10-4006 </papid>iosif and potamianos, 2010).</citsent>
<aftsection>
<nextsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible interpretations.
</nextsent>
<nextsent>sts is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (madnani and dorr, 2010) <papid> J10-3003 </papid>and textual entailment, which is directional and based on relations between semantics (dagan et al, 2006).related methods incorporate measurements of similarity at various levels: lexical (malakasiotis and androutsopoulos, 2007), <papid> W07-1407 </papid>syntactic (malakasiotis, 2009; <papid> P09-3004 </papid>zanzotto et al, 2009), and semantic (rinaldi et al, 2003; <papid> W03-1604 </papid>bos and markert, 2005).<papid> H05-1079 </papid></nextsent>
<nextsent>measures from machine translation evaluation are often used to evaluate lexical level approaches (finch et al, 2005; <papid> I05-5003 </papid>perez and alfonseca, 2005), including bleu(papineni et al, 2002), <papid> P02-1040 </papid>metric based on word gram hit rates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X457">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (miller, 1990; budanitsky and hirst, 2006) <papid> J06-1003 </papid>and (ii) corpus-based metrics (baroni and lenci, 2010; <papid> J10-4006 </papid>iosif and potamianos, 2010).</prevsent>
<prevsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible inter pre tations.</prevsent>
</prevsection>
<citsent citstr=" J10-3003 ">
sts is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (madnani and dorr, 2010) <papid> J10-3003 </papid>and textual entailment, which is directional and based on relations between semantics (dagan et al, 2006).related methods incorporate measurements of similarity at various levels: lexical (malakasiotis and androutsopoulos, 2007), <papid> W07-1407 </papid>syntactic (malakasiotis, 2009; <papid> P09-3004 </papid>zanzotto et al, 2009), and semantic (rinaldi et al, 2003; <papid> W03-1604 </papid>bos and markert, 2005).<papid> H05-1079 </papid></citsent>
<aftsection>
<nextsent>measures from machine translation evaluation are often used to evaluate lexical level approaches (finch et al, 2005; <papid> I05-5003 </papid>perez and alfonseca, 2005), including bleu(papineni et al, 2002), <papid> P02-1040 </papid>metric based on word gram hit rates.</nextsent>
<nextsent>motivated by bleu, we use n-gram hit rates and word-level semantic similarity scores as features in 565 linear regression model to estimate sentence level semantic similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X458">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (miller, 1990; budanitsky and hirst, 2006) <papid> J06-1003 </papid>and (ii) corpus-based metrics (baroni and lenci, 2010; <papid> J10-4006 </papid>iosif and potamianos, 2010).</prevsent>
<prevsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible inter pre tations.</prevsent>
</prevsection>
<citsent citstr=" W07-1407 ">
sts is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (madnani and dorr, 2010) <papid> J10-3003 </papid>and textual entailment, which is directional and based on relations between semantics (dagan et al, 2006).related methods incorporate measurements of similarity at various levels: lexical (malakasiotis and androutsopoulos, 2007), <papid> W07-1407 </papid>syntactic (malakasiotis, 2009; <papid> P09-3004 </papid>zanzotto et al, 2009), and semantic (rinaldi et al, 2003; <papid> W03-1604 </papid>bos and markert, 2005).<papid> H05-1079 </papid></citsent>
<aftsection>
<nextsent>measures from machine translation evaluation are often used to evaluate lexical level approaches (finch et al, 2005; <papid> I05-5003 </papid>perez and alfonseca, 2005), including bleu(papineni et al, 2002), <papid> P02-1040 </papid>metric based on word gram hit rates.</nextsent>
<nextsent>motivated by bleu, we use n-gram hit rates and word-level semantic similarity scores as features in 565 linear regression model to estimate sentence level semantic similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X459">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (miller, 1990; budanitsky and hirst, 2006) <papid> J06-1003 </papid>and (ii) corpus-based metrics (baroni and lenci, 2010; <papid> J10-4006 </papid>iosif and potamianos, 2010).</prevsent>
<prevsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible inter pre tations.</prevsent>
</prevsection>
<citsent citstr=" P09-3004 ">
sts is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (madnani and dorr, 2010) <papid> J10-3003 </papid>and textual entailment, which is directional and based on relations between semantics (dagan et al, 2006).related methods incorporate measurements of similarity at various levels: lexical (malakasiotis and androutsopoulos, 2007), <papid> W07-1407 </papid>syntactic (malakasiotis, 2009; <papid> P09-3004 </papid>zanzotto et al, 2009), and semantic (rinaldi et al, 2003; <papid> W03-1604 </papid>bos and markert, 2005).<papid> H05-1079 </papid></citsent>
<aftsection>
<nextsent>measures from machine translation evaluation are often used to evaluate lexical level approaches (finch et al, 2005; <papid> I05-5003 </papid>perez and alfonseca, 2005), including bleu(papineni et al, 2002), <papid> P02-1040 </papid>metric based on word gram hit rates.</nextsent>
<nextsent>motivated by bleu, we use n-gram hit rates and word-level semantic similarity scores as features in 565 linear regression model to estimate sentence level semantic similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X460">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (miller, 1990; budanitsky and hirst, 2006) <papid> J06-1003 </papid>and (ii) corpus-based metrics (baroni and lenci, 2010; <papid> J10-4006 </papid>iosif and potamianos, 2010).</prevsent>
<prevsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible inter pre tations.</prevsent>
</prevsection>
<citsent citstr=" W03-1604 ">
sts is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (madnani and dorr, 2010) <papid> J10-3003 </papid>and textual entailment, which is directional and based on relations between semantics (dagan et al, 2006).related methods incorporate measurements of similarity at various levels: lexical (malakasiotis and androutsopoulos, 2007), <papid> W07-1407 </papid>syntactic (malakasiotis, 2009; <papid> P09-3004 </papid>zanzotto et al, 2009), and semantic (rinaldi et al, 2003; <papid> W03-1604 </papid>bos and markert, 2005).<papid> H05-1079 </papid></citsent>
<aftsection>
<nextsent>measures from machine translation evaluation are often used to evaluate lexical level approaches (finch et al, 2005; <papid> I05-5003 </papid>perez and alfonseca, 2005), including bleu(papineni et al, 2002), <papid> P02-1040 </papid>metric based on word gram hit rates.</nextsent>
<nextsent>motivated by bleu, we use n-gram hit rates and word-level semantic similarity scores as features in 565 linear regression model to estimate sentence level semantic similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X461">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (miller, 1990; budanitsky and hirst, 2006) <papid> J06-1003 </papid>and (ii) corpus-based metrics (baroni and lenci, 2010; <papid> J10-4006 </papid>iosif and potamianos, 2010).</prevsent>
<prevsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible inter pre tations.</prevsent>
</prevsection>
<citsent citstr=" H05-1079 ">
sts is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (madnani and dorr, 2010) <papid> J10-3003 </papid>and textual entailment, which is directional and based on relations between semantics (dagan et al, 2006).related methods incorporate measurements of similarity at various levels: lexical (malakasiotis and androutsopoulos, 2007), <papid> W07-1407 </papid>syntactic (malakasiotis, 2009; <papid> P09-3004 </papid>zanzotto et al, 2009), and semantic (rinaldi et al, 2003; <papid> W03-1604 </papid>bos and markert, 2005).<papid> H05-1079 </papid></citsent>
<aftsection>
<nextsent>measures from machine translation evaluation are often used to evaluate lexical level approaches (finch et al, 2005; <papid> I05-5003 </papid>perez and alfonseca, 2005), including bleu(papineni et al, 2002), <papid> P02-1040 </papid>metric based on word gram hit rates.</nextsent>
<nextsent>motivated by bleu, we use n-gram hit rates and word-level semantic similarity scores as features in 565 linear regression model to estimate sentence level semantic similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X462">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible interpretations.
</prevsent>
<prevsent>sts is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (madnani and dorr, 2010) <papid> J10-3003 </papid>and textual entailment, which is directional and based on relations between semantics (dagan et al, 2006).related methods incorporate measurements of similarity at various levels: lexical (malakasiotis and androutsopoulos, 2007), <papid> W07-1407 </papid>syntactic (malakasiotis, 2009; <papid> P09-3004 </papid>zanzotto et al, 2009), and semantic (rinaldi et al, 2003; <papid> W03-1604 </papid>bos and markert, 2005).<papid> H05-1079 </papid></prevsent>
</prevsection>
<citsent citstr=" I05-5003 ">
measures from machine translation evaluation are often used to evaluate lexical level approaches (finch et al, 2005; <papid> I05-5003 </papid>perez and alfonseca, 2005), including bleu(papineni et al, 2002), <papid> P02-1040 </papid>metric based on word gram hit rates.</citsent>
<aftsection>
<nextsent>motivated by bleu, we use n-gram hit rates and word-level semantic similarity scores as features in 565 linear regression model to estimate sentence level semantic similarity.
</nextsent>
<nextsent>we also propose sigmoid scaling of similarity scores and sentence-length dependent modeling.
</nextsent>
<nextsent>the models are evaluated on these meval12 sentence similarity task.
</nextsent>
<nextsent>in this section, two different metrics of word similarity are presented.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X463">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non compositional nature of sentence-level semantic sand the exponential explosion of possible interpretations.
</prevsent>
<prevsent>sts is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (madnani and dorr, 2010) <papid> J10-3003 </papid>and textual entailment, which is directional and based on relations between semantics (dagan et al, 2006).related methods incorporate measurements of similarity at various levels: lexical (malakasiotis and androutsopoulos, 2007), <papid> W07-1407 </papid>syntactic (malakasiotis, 2009; <papid> P09-3004 </papid>zanzotto et al, 2009), and semantic (rinaldi et al, 2003; <papid> W03-1604 </papid>bos and markert, 2005).<papid> H05-1079 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
measures from machine translation evaluation are often used to evaluate lexical level approaches (finch et al, 2005; <papid> I05-5003 </papid>perez and alfonseca, 2005), including bleu(papineni et al, 2002), <papid> P02-1040 </papid>metric based on word gram hit rates.</citsent>
<aftsection>
<nextsent>motivated by bleu, we use n-gram hit rates and word-level semantic similarity scores as features in 565 linear regression model to estimate sentence level semantic similarity.
</nextsent>
<nextsent>we also propose sigmoid scaling of similarity scores and sentence-length dependent modeling.
</nextsent>
<nextsent>the models are evaluated on these meval12 sentence similarity task.
</nextsent>
<nextsent>in this section, two different metrics of word similarity are presented.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X465">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> semantic similarity between words.  </section>
<citcontext>
<prevsection>
<prevsent>the models are evaluated on these meval12 sentence similarity task.
</prevsent>
<prevsent>in this section, two different metrics of word similarity are presented.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
the first is language-agnostic,corpus-based metric requiring no knowledge resources, while the second metric relies on wordnet.corpus-based metric: given corpus, these mantic similarity between two words, wi and wj , is estimated as their pointwise mutual information (church and hanks, 1990): <papid> J90-1003 </papid>i(i, j) = log p?(i,j)p?(i)p?(j) ,where p?(i) and p?(j) are the occurrence probabilities of wi and wj , respectively, while the probability of their co-occurrence is denoted by p?(i, j).</citsent>
<aftsection>
<nextsent>these probabilities are computed according to maximum likelihood estimation.
</nextsent>
<nextsent>the assumption of this metric is that co-occurrence implies semantic similarity.
</nextsent>
<nextsent>during the past decade the web has been used for estimating the required probabilities (turney, 2001;bollegala et al, 2007), by querying web search engines and retrieving the number of hits required to estimate the frequency of individual words and their co-occurrence.
</nextsent>
<nextsent>however, these approaches have failed to obtain state-of-the-art results (bolle gala et al, 2007), unless expensive?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X466">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> semantic similarity between words.  </section>
<citcontext>
<prevsection>
<prevsent>the ia(i, j) metric using the estimated value of ? = 0.8 was shown to significantly out perform i(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (rubenstein and goodenough, 1965; miller and charles, 1998; finkelstein et al, 2002).
</prevsent>
<prevsent>for more details see (iosif and potamianos, 2012a).wordnet-based metrics: for comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above).
</prevsent>
</prevsection>
<citsent citstr=" W06-2501 ">
the best results were obtained by the vector metric (patward han and pedersen, 2006), <papid> W06-2501 </papid>which exploits the lexical information that is included in the wordnet glosses.this metric was incorporated to our proposed approach.</citsent>
<aftsection>
<nextsent>all metrics were computed using the word net::similarity module (pedersen, 2005).
</nextsent>
<nextsent>inspired by bleu (papineni et al, 2002), <papid> P02-1040 </papid>we propose simple regression model that combines evidence from two sources: number of n-gram matches and degree of similarity between non-matchingwords between two sentences.</nextsent>
<nextsent>in order to incorporate word semantic similarity metric into bleu,we apply the following two-pass process: first lexical hits are identified and counted, and then the semantic similarity between n-grams not matched dur respond to all senses, i.e., the denominator of i(i, j) is overestimated causing large underestimation error for similarities between polysemous words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X469">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> experimental procedure and results.  </section>
<citcontext>
<prevsection>
<prevsent>(l1, l2] with model dl2 etc. each of these partial models is linear fusion model as shown in (2).
</prevsent>
<prevsent>in this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 =?.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
initially all sentences are pre-processed by the corenlp (finkel et al, 2005; <papid> P05-1045 </papid>toutanova et al, 2003) <papid> N03-1033 </papid>suite of tools, process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming.</citsent>
<aftsection>
<nextsent>the exact type of pre-processing used depends on the metric used.
</nextsent>
<nextsent>for the plain lexical bleu, we use lemmatization, stemming (of lemmas) and remove all non-contentwords, keeping only nouns, adjectives, verbs and ad verbs.
</nextsent>
<nextsent>for computing semantic similarity scores, we dont use stemming and keep only noun words, since we only have similarities between non-noun words.
</nextsent>
<nextsent>for the computation of semantic similarity we have created dictionary containing all the single-word nouns included in wordnet (approx.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X470">
<title id=" S12-1082.xml">deep purple estimating sentence semantic similarity using ngram regression models and web snippets </title>
<section> experimental procedure and results.  </section>
<citcontext>
<prevsection>
<prevsent>(l1, l2] with model dl2 etc. each of these partial models is linear fusion model as shown in (2).
</prevsent>
<prevsent>in this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 =?.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
initially all sentences are pre-processed by the corenlp (finkel et al, 2005; <papid> P05-1045 </papid>toutanova et al, 2003) <papid> N03-1033 </papid>suite of tools, process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming.</citsent>
<aftsection>
<nextsent>the exact type of pre-processing used depends on the metric used.
</nextsent>
<nextsent>for the plain lexical bleu, we use lemmatization, stemming (of lemmas) and remove all non-contentwords, keeping only nouns, adjectives, verbs and ad verbs.
</nextsent>
<nextsent>for computing semantic similarity scores, we dont use stemming and keep only noun words, since we only have similarities between non-noun words.
</nextsent>
<nextsent>for the computation of semantic similarity we have created dictionary containing all the single-word nouns included in wordnet (approx.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X471">
<title id=" S12-1048.xml">semeval2012 task 3 spatial role labeling </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>this process needs to cover the necessary information and semantics on the one hand, and to maintain the practical feasibility of the automatic annotation of unobserved data on the other hand.in recent research on spatial information and natural language, several annotation schemes have been proposed such as ace, gum, gml, kml, trmlwhich are briefly described and compared to spa tialml scheme in (mitre corporation, 2010).
</prevsent>
<prevsent>but 366 to our knowledge, the main obstacles for employing machine learning in this context and the very limited usage of this effective approach have been (a) the lack of an agreement on unique semantic model for spatial information; (b) the diversity of formal spatial relations; and consequently (c) the lack of annotated data on which machine learning can be employed to learn and extract the spatial relations.
</prevsent>
</prevsection>
<citsent citstr=" L08-1017 ">
the most systematic work in this area includes thespatialml (mani et al, 2008) <papid> L08-1017 </papid>scheme which focuses on geographical information, and the work of (pustejovsky and moszkowicz, 2009) in which the pivot of the spatial information is the spatial verb.</citsent>
<aftsection>
<nextsent>the most recent and active work is the iso-space scheme (pustejovsky et al, 2011) which is basedon the above two schemes.
</nextsent>
<nextsent>the ideas behind iso space are closely related to our annotation scheme in (kordjamshidi et al, 2010b), however it considers more detailed and fine-grained spatial and linguistic elements which makes the preparation of the data for machine learning more difficult.
</nextsent>
<nextsent>spatial information is directly related to the part of the language that can be visualized.
</nextsent>
<nextsent>thus, the extraction of spatial information is useful for multimodal environments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X472">
<title id=" S12-1048.xml">semeval2012 task 3 spatial role labeling </title>
<section> preparation of the dataset.  </section>
<citcontext>
<prevsection>
<prevsent>at the starting point two annotators including onetask-organizer and another non-expert annotator, annotated 325 sentences for the spatial roles and relations.
</prevsent>
<prevsent>the purpose was to realize the disagreement points and prepare set of instructions in way to achieve highest-possible agreement.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
from the first effort an inter-annotator agreement (carletta, 1996)<papid> J96-2004 </papid>of 0.89 for cohens kappa was obtained.</citsent>
<aftsection>
<nextsent>we continued with the third annotator for the remaining 888 sentences.
</nextsent>
<nextsent>the annotator had an explanatory session and received set of instructions and annotated examples to decrease the ambiguity in the annotations.to avoid complexity only the relations that are directly expressed in the sentence are annotated and spatial reasoning was avoided during the annotations.
</nextsent>
<nextsent>sometimes the trajectors and landmarks or both are implicit, meaning that there is no word in the sentence to represent them.
</nextsent>
<nextsent>for example in the sentence come over here, the trajector you is only implicitly present.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X473">
<title id=" S12-1048.xml">semeval2012 task 3 spatial role labeling </title>
<section> systems and results.  </section>
<citcontext>
<prevsection>
<prevsent>for true prediction, an exact match between the ground-truth and all the elements of the predicted relation tag including tr, lm,sp and general-type is required.
</prevsent>
<prevsent>one system with two runs was submitted from the university of texas dallas.
</prevsent>
</prevsection>
<citsent citstr=" S12-1056 ">
the two runs (roberts and harabagiu, 2012), <papid> S12-1056 </papid>utdsprl-supervised1 and utdsprl-supervised2 are based on the joint classification of the spatial triplets in binary classification setting.</citsent>
<aftsection>
<nextsent>to produce the candidate (indicator, trajector, landmark) triples, in the first stage heuristic rules targeting high recall areused.
</nextsent>
<nextsent>then binary support vector machine classifier is employed to predict whether triple is spatial relation or not.
</nextsent>
<nextsent>both runs start with large number of manually engineered features, and use floating forward feature selection to select the most important ones.
</nextsent>
<nextsent>the difference between the tworuns of utdsprl-supervised1 and utdsprl supervised2 is their feature set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X475">
<title id=" P99-1069.xml">estimators for stochastic unification based grammars </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>however, de-veloping stochastic  unification-based  gram-mars (subgs) has not proved as straight- forward as might be hoped.
</prevsent>
<prevsent>the simple  relative frequency  estimator for pcfgs yields the maximum likelihood pa-rameter estimate, which is to say that it minimizes the kulback-liebler divergence be-tween the training and estimated istributions.
</prevsent>
</prevsection>
<citsent citstr=" J97-4005 ">
on the other hand, as abney (1997) <papid> J97-4005 </papid>points out, the context-sensitive dependencies that  unification-based  constraints introduce ren-der the relative frequency estimator suboptimal: in general it does not maximize the likelihood and it is inconsistent.</citsent>
<aftsection>
<nextsent>* this research was supported by the national science foundation (sbr,-9720368), the us army research of-fice (daah04-96-baa5), and office of naval research (n00014-97-1-0249).
</nextsent>
<nextsent>abney (1997) <papid> J97-4005 </papid>proposes markov random field or loglinear model for subgs, and the models described here are instances of abney general framework.</nextsent>
<nextsent>however, the monte-carlo parameter estimation procedure that abney proposes eems to be computationally imprac-tical for reasonable-sized grammars.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X484">
<title id=" P99-1069.xml">estimators for stochastic unification based grammars </title>
<section> features  in subgs.  </section>
<citcontext>
<prevsection>
<prevsent>in our exper-iments, we have used conjugate-gradient op-timization program adapted from the one pre-sented in press et al (1992).
</prevsent>
<prevsent>regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities po(wly ) are as use-ful (if not more useful) as the full probabili-ties p0(w), at least in those cases for which the ultimate goal is syntactic analysis.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
berger et al (1996) <papid> J96-1002 </papid>and jelinek (1997) make this same point and arrive at the same estimator, albeit through maximum entropy argument.</citsent>
<aftsection>
<nextsent>the problem of estimating parameters for log-linear models is not new.
</nextsent>
<nextsent>it is especially dif-ficult in cases, such as ours, where large sam-ple space makes the direct computation of ex-pectations infeasible.
</nextsent>
<nextsent>many applications in spa-tial statistics, involving markov random fields (mrf), are of this nature as well.
</nextsent>
<nextsent>in his seminal development of the mrf approach to spatial statistics, besag introduced  pseudo- likelihood  estimator to address these difficul-ties (besag, 1974; besag, 1975), and in fact our proposal here is an instance of his method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X485">
<title id=" S10-1005.xml">semeval2010 task 7 argument selection and coercion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions.
</prevsent>
<prevsent>in recent years, number of annotation schemes that encode semantic information have been developed and used to produce datasets for training machine learning algorithms.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (palmer et al, 2005; <papid> J05-1004 </papid>ruppenhofer et al,2006; kipper, 2005; burchardt et al, 2006; subi rats, 2004).</citsent>
<aftsection>
<nextsent>in this task, we take this one step further and attempt to capture the compositional history?
</nextsent>
<nextsent>of the argument selection relative to the predicate.
</nextsent>
<nextsent>in particular, this task attempts to identify the operations of type adjustment induced by predicate over its arguments when they do not match its selectional properties.
</nextsent>
<nextsent>the task is defined as fol lows: for each argument of predicate, identify whether the entity in that argument position satisfies the type expected by the predicate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X486">
<title id=" S10-1005.xml">semeval2010 task 7 argument selection and coercion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1) a. john reported in late from washington.
</prevsent>
<prevsent>b. washington reported in late.
</prevsent>
</prevsection>
<citsent citstr=" J91-4003 ">
neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, type coercion or metonymy (hobbs et al, 1993; pustejovsky, 1991; <papid> J91-4003 </papid>nunberg, 1979; egg, 2005) has taken place.the semeval metonymy task (markert and nissim, 2007) <papid> W07-2007 </papid>was good attempt to annotate such metonymic relations over larger data set.</citsent>
<aftsection>
<nextsent>this task involved two types with their metonymicvariants: categories-for-locations (e.g., place for-people) and categories-for-organizations (e.g., organization-for-members).
</nextsent>
<nextsent>one of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus arenot an informative guide for extending the annotation of argument selection more broadly.in fact, the metonymy example in (1) is an instance of much more pervasive phenomenon of type shifting and coercion in argument selection.
</nextsent>
<nextsent>for example, in (2) below, the sense annotation for the verb enjoy should arguably assign similar values to both (2a) and (2b).
</nextsent>
<nextsent>27 figure 1: the matter methodology (2) a. mary enjoyed drinking her beer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X487">
<title id=" S10-1005.xml">semeval2010 task 7 argument selection and coercion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1) a. john reported in late from washington.
</prevsent>
<prevsent>b. washington reported in late.
</prevsent>
</prevsection>
<citsent citstr=" W07-2007 ">
neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, type coercion or metonymy (hobbs et al, 1993; pustejovsky, 1991; <papid> J91-4003 </papid>nunberg, 1979; egg, 2005) has taken place.the semeval metonymy task (markert and nissim, 2007) <papid> W07-2007 </papid>was good attempt to annotate such metonymic relations over larger data set.</citsent>
<aftsection>
<nextsent>this task involved two types with their metonymicvariants: categories-for-locations (e.g., place for-people) and categories-for-organizations (e.g., organization-for-members).
</nextsent>
<nextsent>one of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus arenot an informative guide for extending the annotation of argument selection more broadly.in fact, the metonymy example in (1) is an instance of much more pervasive phenomenon of type shifting and coercion in argument selection.
</nextsent>
<nextsent>for example, in (2) below, the sense annotation for the verb enjoy should arguably assign similar values to both (2a) and (2b).
</nextsent>
<nextsent>27 figure 1: the matter methodology (2) a. mary enjoyed drinking her beer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X489">
<title id=" S10-1005.xml">semeval2010 task 7 argument selection and coercion </title>
<section> resources and corpus development.  </section>
<citcontext>
<prevsection>
<prevsent>during the annotation phase, the annotation judgments were entered into the database, andan adjudicator resolved disagreements.
</prevsent>
<prevsent>the resulting database was then exported in an xml format.
</prevsent>
</prevsection>
<citsent citstr=" W09-3716 ">
1 this task is part of larger effort to annotate text with compositional operations (pustejovsky et al, 2009).<papid> W09-3716 </papid></citsent>
<aftsection>
<nextsent>28 figure 2: corpus development architecture 4.1 dataset construction phase: english.
</nextsent>
<nextsent>for the english dataset, the data construction phase was combined with the annotation phase.the data for the task was created using the following steps: 1.
</nextsent>
<nextsent>the verbs were selected by examining the data.
</nextsent>
<nextsent>from the bnc, using the sketch engine (kilgar riff et al, 2004) as described in (rumshisky and batiukova, 2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X490">
<title id=" S10-1005.xml">semeval2010 task 7 argument selection and coercion </title>
<section> resources and corpus development.  </section>
<citcontext>
<prevsection>
<prevsent>sense inventories were compiled for each verb,.
</prevsent>
<prevsent>with the senses mapped to ontonotes (pradhan et al, 2007) whenever possible.
</prevsent>
</prevsection>
<citsent citstr=" C04-1133 ">
for each sense, set of type templates was compiled using modification of the cpa technique (hanks and pustejovsky, 2005; pustejovsky et al, 2004):<papid> C04-1133 </papid>every argument in the syntactic pattern associated with given sense was assigned type specification.</citsent>
<aftsection>
<nextsent>although particular sense is often compatible with more than one semantic type forgiven argument, this was never the case in our dataset, where no disjoint types were tested.
</nextsent>
<nextsent>the coercive senses of the chosen verbs were associated with the following type templates:a. arrive (at), sense reach destination or goal : human arrive at location b. cancel, sense call off : human cancel eventc.
</nextsent>
<nextsent>deny, sense state or maintain that something is un true: human deny proposition d. finish, sense complete an activity: human finish event e. hear, sense perceive physical sound : human hear sound we used subset of semantic types from the brandeis shallow ontology (bso), which is shallow hierarchy of types developed as part of the cpa effort (hanks, 2009; pustejovsky et al, 2004; <papid> C04-1133 </papid>rumshisky et al, 2006).</nextsent>
<nextsent>types were selected for their prevalence in manually identified selection context patterns developed for several hundred english verbs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X492">
<title id=" S10-1005.xml">semeval2010 task 7 argument selection and coercion </title>
<section> resources and corpus development.  </section>
<citcontext>
<prevsection>
<prevsent>in constructing the italian dataset, we adopted the same methodology used for the english dataset, with the following differences: 1.
</prevsent>
<prevsent>the list of coercive verbs was selected by exam-.
</prevsent>
</prevsection>
<citsent citstr=" E06-2001 ">
ining data from the itwac (baroni and kilgarriff, 2006) <papid> E06-2001 </papid>using the sketch engine (kilgarriff et al, 2004):accusare accuse?, annunciare announce?, arrivare ar rive?, ascoltare listen?, avvisare inform?, chiamarecall?, cominciare begin?, completare complete?, conclude re conclude?, contattare contact?, divorare devour?, echeggiare echo?, finire finish?, infor mare inform?, interrompere interrupt?, leggere read?, raggiun gere reach?, recar(si) go to?, rimbombare resound?, sentire hear?, udire hear?, visit are visit?.</citsent>
<aftsection>
<nextsent>2.
</nextsent>
<nextsent>the coercive senses of the chosen verbs were.
</nextsent>
<nextsent>associated with type templates, some of which are listed listed below.
</nextsent>
<nextsent>whenever possible, senses and type templates were adapted from the italian pattern dictionary (hanks and jezek,2007) and mapped to their simple equivalents (lenci et al, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X493">
<title id=" S10-1074.xml">edinburghltg tempeval2 system description </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>we are currently in the process of developing rule-based date and time grounding component and it is this component that we used for task a, which requires systems to identify the extents of temporal named entities and provide their interpretation.
</prevsent>
<prevsent>the tempeval-2 data also contains event entities and we have adapted the output of our inhouse chunker (grover and tobin, 2006) to identify events for task b, which requires systems to identify event denoting words and to compute range of attributes for them.
</prevsent>
</prevsection>
<citsent citstr=" W08-0603 ">
in future work we will adapt our machine-learning-based relation extraction component (haddow, 2008) <papid> W08-0603 </papid>to recognise relations between spatial and temporal entities and event entities along the lines of the linking tasks.</citsent>
<aftsection>
<nextsent>our ie system is modular pipeline system built around the lt-xml2 1 and lt-ttt2 2 toolsets.documents are converted into our internal document format and are then passed through sequence of linguistic components which each add xml mark-up.
</nextsent>
<nextsent>early stages identify paragraphs,sentences and tokens.
</nextsent>
<nextsent>part-of-speech (pos) tagging is done using the c&c; tagger (curran and clark, 2003<papid> W03-0424 </papid>a) and lemmatisation is done using morpha (minnen et al, 2000).<papid> W00-1427 </papid></nextsent>
<nextsent>we use both rule-based and machine-learning named entity recognition (ner) components, the former implemented using lt-ttt2 and the latter using the c&c; maximum entropy ner tagger(curran and clark, 2003<papid> W03-0424 </papid>b).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X494">
<title id=" S10-1074.xml">edinburghltg tempeval2 system description </title>
<section> the edinburgh ie system.  </section>
<citcontext>
<prevsection>
<prevsent>our ie system is modular pipeline system built around the lt-xml2 1 and lt-ttt2 2 toolsets.documents are converted into our internal document format and are then passed through sequence of linguistic components which each add xml mark-up.
</prevsent>
<prevsent>early stages identify paragraphs,sentences and tokens.
</prevsent>
</prevsection>
<citsent citstr=" W03-0424 ">
part-of-speech (pos) tagging is done using the c&c; tagger (curran and clark, 2003<papid> W03-0424 </papid>a) and lemmatisation is done using morpha (minnen et al, 2000).<papid> W00-1427 </papid></citsent>
<aftsection>
<nextsent>we use both rule-based and machine-learning named entity recognition (ner) components, the former implemented using lt-ttt2 and the latter using the c&c; maximum entropy ner tagger(curran and clark, 2003<papid> W03-0424 </papid>b).</nextsent>
<nextsent>we are experimenting to find the best combination of the two different ner views but this is not an issue in the case of date and time entities since we have taken the decision to use the rule-based output for these.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X498">
<title id=" S10-1074.xml">edinburghltg tempeval2 system description </title>
<section> the edinburgh ie system.  </section>
<citcontext>
<prevsection>
<prevsent>our ie system is modular pipeline system built around the lt-xml2 1 and lt-ttt2 2 toolsets.documents are converted into our internal document format and are then passed through sequence of linguistic components which each add xml mark-up.
</prevsent>
<prevsent>early stages identify paragraphs,sentences and tokens.
</prevsent>
</prevsection>
<citsent citstr=" W00-1427 ">
part-of-speech (pos) tagging is done using the c&c; tagger (curran and clark, 2003<papid> W03-0424 </papid>a) and lemmatisation is done using morpha (minnen et al, 2000).<papid> W00-1427 </papid></citsent>
<aftsection>
<nextsent>we use both rule-based and machine-learning named entity recognition (ner) components, the former implemented using lt-ttt2 and the latter using the c&c; maximum entropy ner tagger(curran and clark, 2003<papid> W03-0424 </papid>b).</nextsent>
<nextsent>we are experimenting to find the best combination of the two different ner views but this is not an issue in the case of date and time entities since we have taken the decision to use the rule-based output for these.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X503">
<title id=" P99-1025.xml">construct algebra analytical dialog management </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>the first is spoken dialog sys-tem that enables user to respond to the open-ended prompt how may help you?
</prevsent>
<prevsent>(hmihy) (gorin et al, 1997).
</prevsent>
</prevsection>
<citsent citstr=" W98-1122 ">
the sys-tem recognizes the words the customer has said (riccardi and bangalore, 1998) <papid> W98-1122 </papid>and ex-tracts the meaning of these words (wright et al, 1998) to determine what service they want, conducting dialog (abella and gorin, 1997; abella et al, 1996) to effec-tively engage the customer in conversa-tion that will result in providing the service they requested.</citsent>
<aftsection>
<nextsent>the second application is to voice post query (vpq) (buntschuh et al., 1998) which provides spoken access to the information in large personnel database (  120,000 entries).
</nextsent>
<nextsent>a user can ask for em-ployee information such as phone number, fax number, work location, or ask to call an employee.
</nextsent>
<nextsent>these applications are signifi- 191 cantly different but they both use the same dialog manager.
</nextsent>
<nextsent>information about the task is defined us-ing an object inheritance hierarchy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X504">
<title id=" S12-1056.xml">utdsprl a joint approach to spatial role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this joint approach allows fora rich feature set based on the complete relation instead of individual relation arguments.our best official submission achieves an f1 measure of 0.573 on relation recognition, best in the task and outperforming the previous best result on the same dataset (0.500).
</prevsent>
<prevsent>a significant amount of spatial information in natural language is encoded in spatial relationships between objects.
</prevsent>
</prevsection>
<citsent citstr=" S12-1048 ">
in this paper, we present our approach for detecting the special case of spatial relations evaluated in semeval-2012 task 3, spatial role labeling (sprl) (kordjamshidi et al, 2012).<papid> S12-1048 </papid>this task considers the most common type of spatial relationships between objects, namely those described with spatial preposition (e.g., in, on, over)or spatial phrase (e.g., in front of, on the left), referred to as the spatial indicator.</citsent>
<aftsection>
<nextsent>a spatial indicator connects an object of interest (the trajec tor) with grounding location (the landmark).
</nextsent>
<nextsent>examples of this type of spatial relationship include: (1) [cars]t parked [in front of]i the [house]l .
</nextsent>
<nextsent>(2) [bushes]t1 and small [trees]t2 [on]i the [hill]l .
</nextsent>
<nextsent>(3) huge [column]l with [football]t [on top]i .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X505">
<title id=" S12-1056.xml">utdsprl a joint approach to spatial role labeling </title>
<section> joint classification.  </section>
<citcontext>
<prevsection>
<prevsent>for trajectors and landmarks, we observe that both may be considered spatial objects, which unlike indicators are not closed class of words.
</prevsent>
<prevsent>instead, we consider noun phrase (np) heads to be spatial objects.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
to overcome part-of-speech errors and increase recall, we incorporate three sources: (1) the np heads from syntactic parse tree (klein and manning, 2003), (<papid> P03-1054 </papid>2) the np heads from chunk parse1, and (3) words that are marked as nouns in at least 66% of instances in treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>this approach identifies all nouns, not just spatial nouns.
</nextsent>
<nextsent>but for the semeval-2012 task 3 data, which is composed of image descriptions, most nouns are spatial objects and no further refinements are necessary.
</nextsent>
<nextsent>fur 1http://www.surdeanu.name/mihai/bios/ ther heuristics (such as using wordnet (fellbaum,1998)) could be used to refine the set of spatial objects if other domains (such as newswire) were to be used.
</nextsent>
<nextsent>our main emphasis in this step, however,is recall: by utilizing these heuristics we greatly reduce the number of negative instances while removing very few positive spatial relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X506">
<title id=" S12-1056.xml">utdsprl a joint approach to spatial role labeling </title>
<section> joint classification.  </section>
<citcontext>
<prevsection>
<prevsent>for trajectors and landmarks, we observe that both may be considered spatial objects, which unlike indicators are not closed class of words.
</prevsent>
<prevsent>instead, we consider noun phrase (np) heads to be spatial objects.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
to overcome part-of-speech errors and increase recall, we incorporate three sources: (1) the np heads from syntactic parse tree (klein and manning, 2003), (<papid> P03-1054 </papid>2) the np heads from chunk parse1, and (3) words that are marked as nouns in at least 66% of instances in treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>this approach identifies all nouns, not just spatial nouns.
</nextsent>
<nextsent>but for the semeval-2012 task 3 data, which is composed of image descriptions, most nouns are spatial objects and no further refinements are necessary.
</nextsent>
<nextsent>fur 1http://www.surdeanu.name/mihai/bios/ ther heuristics (such as using wordnet (fellbaum,1998)) could be used to refine the set of spatial objects if other domains (such as newswire) were to be used.
</nextsent>
<nextsent>our main emphasis in this step, however,is recall: by utilizing these heuristics we greatly reduce the number of negative instances while removing very few positive spatial relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X507">
<title id=" S10-1066.xml">clr linking events and their participants in discourse using a comprehensive framenet dictionary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe the design of the system and the continuing efforts to determine how much of this task can be performed with the available lexical resources.
</prevsent>
<prevsent>changes since the official submission have improved the f-score to 0.266.
</prevsent>
</prevsection>
<citsent citstr=" W04-0803 ">
the semantic role labeling (srl) task has received considerable attention in recent years, with previous tasks in senseval-2 (litkowski, 2004), <papid> W04-0803 </papid>semeval-1 (baker et al, 2007), <papid> W07-2018 </papid>and conll (carreras &amp; marquez, 2004; carreras &amp; marquez, 2005).</citsent>
<aftsection>
<nextsent>the current task, linking events and their participants in discourse, continues the evolution of srl tasks with the intent of identifying null instantiations, i.e., frame elements that are absent from the local context, but potentially recoverable from the wider discourse context.
</nextsent>
<nextsent>cl research participated in one subtask, role recognition and labeling, unable to implement techniques for the null instantiation subtask.
</nextsent>
<nextsent>this paper describes our efforts thus far (clearly work in progress), specifically the implementation of development interface (section 2), the use of specially constructed framenet dictionary (section 3), techniques for performing the role recognition and labeling task (section 4), our results (section 5), and future developments (sec tion 6).
</nextsent>
<nextsent>cl research participated in the linking task by extending its linguistic task analyzer (lta), an interface also used for such tasks as word sense disambiguation and recognizing textual entailment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X508">
<title id=" S10-1066.xml">clr linking events and their participants in discourse using a comprehensive framenet dictionary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe the design of the system and the continuing efforts to determine how much of this task can be performed with the available lexical resources.
</prevsent>
<prevsent>changes since the official submission have improved the f-score to 0.266.
</prevsent>
</prevsection>
<citsent citstr=" W07-2018 ">
the semantic role labeling (srl) task has received considerable attention in recent years, with previous tasks in senseval-2 (litkowski, 2004), <papid> W04-0803 </papid>semeval-1 (baker et al, 2007), <papid> W07-2018 </papid>and conll (carreras &amp; marquez, 2004; carreras &amp; marquez, 2005).</citsent>
<aftsection>
<nextsent>the current task, linking events and their participants in discourse, continues the evolution of srl tasks with the intent of identifying null instantiations, i.e., frame elements that are absent from the local context, but potentially recoverable from the wider discourse context.
</nextsent>
<nextsent>cl research participated in one subtask, role recognition and labeling, unable to implement techniques for the null instantiation subtask.
</nextsent>
<nextsent>this paper describes our efforts thus far (clearly work in progress), specifically the implementation of development interface (section 2), the use of specially constructed framenet dictionary (section 3), techniques for performing the role recognition and labeling task (section 4), our results (section 5), and future developments (sec tion 6).
</nextsent>
<nextsent>cl research participated in the linking task by extending its linguistic task analyzer (lta), an interface also used for such tasks as word sense disambiguation and recognizing textual entailment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X509">
<title id=" S10-1066.xml">clr linking events and their participants in discourse using a comprehensive framenet dictionary </title>
<section> the framenet dictionary.  </section>
<citcontext>
<prevsection>
<prevsent>1 the framenet dictionary attempts to capture all the information in framenet, in form that can be easily accessed and used for tasks such as the linking task.
</prevsent>
<prevsent>this dictionary is also used in general word-sense disambiguation tasks, when all words in text are simultaneously disambiguated with several dictionaries.
</prevsent>
</prevsection>
<citsent citstr=" W07-2021 ">
the framenet dictionary has almost 11,000 entries 2 of four main types: frames, frame-to-frame relations, normal entries, and frame elements 3 . this dictionary was initially described in litkowski (2007), <papid> W07-2021 </papid>but is described in more detail in the following subsections in order to show how the information in these entries is used in the linking task.</citsent>
<aftsection>
<nextsent>3.1 frame entries.
</nextsent>
<nextsent>a framenet frame is entered in the dictionary by preceding its name with ?#?
</nextsent>
<nextsent>sign to distinguish it from other types of entries.
</nextsent>
<nextsent>a frame entry, such as #abandonment, consists of one sense with no part of speech.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X510">
<title id=" S10-1098.xml">yscdsaa an approach to disambiguate sentiment ambiguous adjectives based on saaol </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>considering the grammar system of contemporary chinese, word is one of the most basic linguistic granularities consisting of sentence.
</prevsent>
<prevsent>therefore, as for the sentiment classification of sentence, the sentiment tendency of sentence can be identified on the basis of that of word.
</prevsent>
</prevsection>
<citsent citstr=" J04-3002 ">
wiebe et al  (2004) <papid> J04-3002 </papid>proposed that whether sentence is subjective or objective should be discriminated according to the adjectives in it.</citsent>
<aftsection>
<nextsent>on the basis of general inquirer dictionary, learners dictionary of positive and negative words, hownet , dictionary of positive words and dictionary of negative words etc., wang et al (2009) built word library for chinese sentiment words to discriminate the sentiment category of sentence using the weighted linear combination method.
</nextsent>
<nextsent>unlike the previous researches which have not taken saa into consideration specially in discriminating the sentiment tendency of sentence, in the semeval-2010 task of disambiguating sentiment ambiguous adjectives, systems have to predict the sentiment tendency of these fourteen adjectives within specific context.
</nextsent>
<nextsent>from the view of linguistics, first we developed saa oriented keyword library, then analyzed the relationship between the keywords in the clauses and saa, and classified its positive or negative meaning of saa by extracting the clauses related to saa in the sentence.
</nextsent>
<nextsent>we create saa-oriented library marked as saaol which is made up of positive and negative words irrelevant to context, negative words related to saa (nsaa), positive words related to saa (psaa), and inverse words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X511">
<title id=" S12-1062.xml">uned improving text similarity measures without human assessments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the result is that hbr tends to achieve similar or higher correlation with human assessments than the single measures.
</prevsent>
<prevsent>in order to select the most appropriate single measure, we can meta-evaluate measures in terms of correlation with hbr, which is what the previous figure showed.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
we participated in the sts evaluation campaign employing hbr over automatic text evaluation measures (e.g. rouge (lin, 2004)), <papid> W04-1013 </papid>which are not actually designed for this specific problem.</citsent>
<aftsection>
<nextsent>for this reason our results were suboptimal.
</nextsent>
<nextsent>however, according to our experiments this method seem highly useful for combining and evaluating current systems.
</nextsent>
<nextsent>in this paper, we describe the hbr method and we present experiments employing the rest of participant methods as similarity measures.
</nextsent>
<nextsent>2.1 similarity measures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X512">
<title id=" P98-2166.xml">possessive pronominal anaphor resolution in portuguese written texts </title>
<section> factors in ppa resolution.  </section>
<citcontext>
<prevsection>
<prevsent>these factors will be considered in place of traditional syntactic constraints, which are not suitable for our present problem, as shown in section 2.
</prevsent>
<prevsent>in our proposal, because of the structural complexity of sentences in the domain, we have adopted practical approach, based on simple heuristic rules, with view to avoiding syntactic and semantic analysis.
</prevsent>
</prevsection>
<citsent citstr=" C94-2189 ">
similar strategies have been adopted in several recent works in anaphor esolution, such as t. nasukawa (1994), <papid> C94-2189 </papid>r. mitkov (1996), r. vieira &amp; m. poesio (1997) and others.</citsent>
<aftsection>
<nextsent>we have defined 6 simple factors in ppa resolution (f 1 to f6) based on syntactic, semantic and pragmatic knowledge, aiming to determine ppas antecedents in our specific domain.
</nextsent>
<nextsent>as secondary goal, we apply our proposal also to ppas in different domain (see section 5).
</nextsent>
<nextsent>factors, enunciated as heuristic rules, will act as constraints (f1 to f5) or preferences (f6), as established by j. carbonell (1988).
</nextsent>
<nextsent>3.1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X513">
<title id=" P98-2166.xml">possessive pronominal anaphor resolution in portuguese written texts </title>
<section> factors in ppa resolution.  </section>
<citcontext>
<prevsection>
<prevsent>pragmatic level.
</prevsent>
<prevsent>working together, surface patterns and possessive relationships can deal with many ppas found in our corpus, but we still have two problems to be solved: semantic ambiguity among two or more acceptable candidates and abstract anaphors/antecedents, which cannot be solved by simply applying possessive relationship rules.
</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
for these cases, and possibly for some other cases not included in previous rules, we suggest pragmatic factor, adapted from s. brennan et al(1987) <papid> P87-1022 </papid>centering algorithm.</citsent>
<aftsection>
<nextsent>although sentence center plays crucial role in many works in anaphor esolution, usually limiting the number of candidates to be considered, we notice that, because ppas can refer to almost any np in the sentence (rather than, for example, personal pronouns, which are often related to the sentence center), pragmatic knowledge plays only secondary - but still important - role in our approach.
</nextsent>
<nextsent>we have adapted basic aspects of center algorithm, considering subject/object preference, and domain concepts preference, 1012 suggested by r. mitkov (1996), aiming to estimate the most probable center for intra sentential ppas.
</nextsent>
<nextsent>thus, in case of ambiguity among candidates (after applying factors f1 to f5), we will consider the estimated center as the preferable ppa antecedent.
</nextsent>
<nextsent>this constitutes our final rule: f6 - the sentence center will be preferred among remaining candidates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X514">
<title id=" P99-1039.xml">alternating quantifier scope in ccg </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>the paper shows that movement or equivalent computational structure-changing operations of any kind at the level of logical form can be dispensed with entirely in capturing quantifer scope ambi-guity.
</prevsent>
<prevsent>it offers new semantics whereby the ef-fects of quantifier scope alternation can be obtained by an entirely monotonic derivation, without ype- changing rules.
</prevsent>
</prevsection>
<citsent citstr=" P95-1028 ">
the paper follows fodor (1982), fodor and sag (1982), and park (1995), <papid> P95-1028 </papid>and park (1996) in viewing many apparent scope ambiguities as arising from referential categories rather than true general-ized quantitiers.</citsent>
<aftsection>
<nextsent>it is standard to assume that the ambiguity of sen-tences like (1) is to be accounted for by assigning two logical forms which differ in the scopes as-signed to these quantifiers, as in (2a,b): 1 (1) every boy admires ome saxophonist.
</nextsent>
<nextsent>(2) a. vx.boy  -+ 3y.saxophonis/ a admires  yx b. 3y.saxophonis/ a vx.bo/x -+ admires yx the question then arises of how grammar/parser can assign all and only the correct interpretations to sentences with multiple quantifiers.
</nextsent>
<nextsent>this process has on occasion been explained in terms of  quantifier movement  or essentially * early versions of this paper were presented to audiences at brown u., nyu, and karlov2~ u. prague.
</nextsent>
<nextsent>thanks to jason baldridge, gann bierner, tim fernando, kit fine, polly ja-cobson, mark johnson, aravind joshi, richard kayne, shalom lappin, alex lascarides, suresh manandhar, jaruslav pere grin, jong park, anna szabolcsi, bonnie webber, alistair willis, and the referees for helpful comments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X521">
<title id=" P99-1039.xml">alternating quantifier scope in ccg </title>
<section> computing available readings.  </section>
<citcontext>
<prevsection>
<prevsent>however, the restriction that geach noted seems intrinsically disjunctive, and hence appears to threaten efficiency in both parsing with, and disambiguation f, under-specified repre-sentations.
</prevsent>
<prevsent>the fact that relatively few readings are available and that they are so tightly related to surface struc-ture and derivation means that the technique of in-cremental semantic or probabilistic disambiguation of fully specified partial logical forms mentioned earlier may be more efficient echnique for com-puting the contextually relevant readings.
</prevsent>
</prevsection>
<citsent citstr=" J87-1005 ">
forex- ample, in processing (22) (adapted from hobbs and shieber 1987), <papid> J87-1005 </papid>which park 1995 <papid> P95-1028 </papid>claims to have only four readings, rather than the five predicted by their account, such system can build both readings for the s/np every representative of three companies saw and decide which is more likely, before build-ing both compatible readings of the whole sentence and similarly resolving with respect to statistical or contextual support: (22) every representative of three companies aw some sample.</citsent>
<aftsection>
<nextsent>the above observations imply that only those so- called quantifiers in english which can engender dependency-inducing scope inversion have interpre-tations corresponding to genuine quantifiers.
</nextsent>
<nextsent>the others are not quantificationai tall, but are various types of arbitrary individuals translated as skolem terms.
</nextsent>
<nextsent>these give the appearance of taking nar-row scope when they are bound to truly quantified variables, and of taking wide scope when they are unbound, and therefore  take scope everywhere.
</nextsent>
<nextsent>available readings can be computed monotonic ally from syntactic derivation alone.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X523">
<title id=" P99-1039.xml">alternating quantifier scope in ccg </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>these give the appearance of taking nar-row scope when they are bound to truly quantified variables, and of taking wide scope when they are unbound, and therefore  take scope everywhere.
</prevsent>
<prevsent>available readings can be computed monotonic ally from syntactic derivation alone.
</prevsent>
</prevsection>
<citsent citstr=" J90-1001 ">
the notion of syn-tactic derivation embodied in ccg is the most pow-erful limitation on the number of available read-ings, and allows all logical-form level constraints on scope orderings to be dispensed with, result related to, but more powerful than, that of pereira (1990).<papid> J90-1001 </papid></citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X524">
<title id=" P98-2143.xml">robust pronoun resolution with limited knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluation reports success rate of 89.7% which is better than the suc-cess rates of the approaches selected for comparison and tested on the same data.
</prevsent>
<prevsent>in addition, preliminary experiments show that the approach can be success-fully adapted for other languages with minimum modifications.
</prevsent>
</prevsection>
<citsent citstr=" P89-1032 ">
for the most part, anaphora resolution has focused on traditional linguistic methods (carbonell &amp; brown 1988; carter 1987; hobbs 1978; ingria &amp; stallard 1989; <papid> P89-1032 </papid>lappin &amp; mccord 1990; <papid> J90-4001 </papid>lappin &amp; leass 1994; <papid> J94-4002 </papid>mitkov 1994; <papid> C94-2191 </papid>rich &amp; luperfoy 1988; <papid> A88-1003 </papid>sidner 1979; webber 1979).</citsent>
<aftsection>
<nextsent>however, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.
</nextsent>
<nextsent>while various alternatives have been proposed, making use of e.g. neural networks, situation se-mantics framework, or the principles of reasoning with uncertainty (e.g. connoly et al 1994; mitkov 1995; tin &amp; akman 1995), there is still strong need for the development of robust and effective strategies to meet the demands of practical nlp systems, and to enhance further the automatic pro-cessing of growing language resources.
</nextsent>
<nextsent>several proposals have already addressed the anaphora resolution problem by deliberately imiting the extent which they relyon domain and/or lin-guistic knowledge (baldwin 1997; <papid> W97-1306 </papid>dagan &amp; itai 1990; <papid> C90-3063 </papid>kennedy &amp; boguraev 1996; <papid> C96-1021 </papid>mitkov 1998; nasukawa 1994; williams et al 1996).</nextsent>
<nextsent>our work is continuation of these latest rends in the search for inexpensive, fast and reliable procedures for anaph-ora resolution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X525">
<title id=" P98-2143.xml">robust pronoun resolution with limited knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluation reports success rate of 89.7% which is better than the suc-cess rates of the approaches selected for comparison and tested on the same data.
</prevsent>
<prevsent>in addition, preliminary experiments show that the approach can be success-fully adapted for other languages with minimum modifications.
</prevsent>
</prevsection>
<citsent citstr=" J90-4001 ">
for the most part, anaphora resolution has focused on traditional linguistic methods (carbonell &amp; brown 1988; carter 1987; hobbs 1978; ingria &amp; stallard 1989; <papid> P89-1032 </papid>lappin &amp; mccord 1990; <papid> J90-4001 </papid>lappin &amp; leass 1994; <papid> J94-4002 </papid>mitkov 1994; <papid> C94-2191 </papid>rich &amp; luperfoy 1988; <papid> A88-1003 </papid>sidner 1979; webber 1979).</citsent>
<aftsection>
<nextsent>however, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.
</nextsent>
<nextsent>while various alternatives have been proposed, making use of e.g. neural networks, situation se-mantics framework, or the principles of reasoning with uncertainty (e.g. connoly et al 1994; mitkov 1995; tin &amp; akman 1995), there is still strong need for the development of robust and effective strategies to meet the demands of practical nlp systems, and to enhance further the automatic pro-cessing of growing language resources.
</nextsent>
<nextsent>several proposals have already addressed the anaphora resolution problem by deliberately imiting the extent which they relyon domain and/or lin-guistic knowledge (baldwin 1997; <papid> W97-1306 </papid>dagan &amp; itai 1990; <papid> C90-3063 </papid>kennedy &amp; boguraev 1996; <papid> C96-1021 </papid>mitkov 1998; nasukawa 1994; williams et al 1996).</nextsent>
<nextsent>our work is continuation of these latest rends in the search for inexpensive, fast and reliable procedures for anaph-ora resolution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X526">
<title id=" P98-2143.xml">robust pronoun resolution with limited knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluation reports success rate of 89.7% which is better than the suc-cess rates of the approaches selected for comparison and tested on the same data.
</prevsent>
<prevsent>in addition, preliminary experiments show that the approach can be success-fully adapted for other languages with minimum modifications.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
for the most part, anaphora resolution has focused on traditional linguistic methods (carbonell &amp; brown 1988; carter 1987; hobbs 1978; ingria &amp; stallard 1989; <papid> P89-1032 </papid>lappin &amp; mccord 1990; <papid> J90-4001 </papid>lappin &amp; leass 1994; <papid> J94-4002 </papid>mitkov 1994; <papid> C94-2191 </papid>rich &amp; luperfoy 1988; <papid> A88-1003 </papid>sidner 1979; webber 1979).</citsent>
<aftsection>
<nextsent>however, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.
</nextsent>
<nextsent>while various alternatives have been proposed, making use of e.g. neural networks, situation se-mantics framework, or the principles of reasoning with uncertainty (e.g. connoly et al 1994; mitkov 1995; tin &amp; akman 1995), there is still strong need for the development of robust and effective strategies to meet the demands of practical nlp systems, and to enhance further the automatic pro-cessing of growing language resources.
</nextsent>
<nextsent>several proposals have already addressed the anaphora resolution problem by deliberately imiting the extent which they relyon domain and/or lin-guistic knowledge (baldwin 1997; <papid> W97-1306 </papid>dagan &amp; itai 1990; <papid> C90-3063 </papid>kennedy &amp; boguraev 1996; <papid> C96-1021 </papid>mitkov 1998; nasukawa 1994; williams et al 1996).</nextsent>
<nextsent>our work is continuation of these latest rends in the search for inexpensive, fast and reliable procedures for anaph-ora resolution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X527">
<title id=" P98-2143.xml">robust pronoun resolution with limited knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluation reports success rate of 89.7% which is better than the suc-cess rates of the approaches selected for comparison and tested on the same data.
</prevsent>
<prevsent>in addition, preliminary experiments show that the approach can be success-fully adapted for other languages with minimum modifications.
</prevsent>
</prevsection>
<citsent citstr=" C94-2191 ">
for the most part, anaphora resolution has focused on traditional linguistic methods (carbonell &amp; brown 1988; carter 1987; hobbs 1978; ingria &amp; stallard 1989; <papid> P89-1032 </papid>lappin &amp; mccord 1990; <papid> J90-4001 </papid>lappin &amp; leass 1994; <papid> J94-4002 </papid>mitkov 1994; <papid> C94-2191 </papid>rich &amp; luperfoy 1988; <papid> A88-1003 </papid>sidner 1979; webber 1979).</citsent>
<aftsection>
<nextsent>however, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.
</nextsent>
<nextsent>while various alternatives have been proposed, making use of e.g. neural networks, situation se-mantics framework, or the principles of reasoning with uncertainty (e.g. connoly et al 1994; mitkov 1995; tin &amp; akman 1995), there is still strong need for the development of robust and effective strategies to meet the demands of practical nlp systems, and to enhance further the automatic pro-cessing of growing language resources.
</nextsent>
<nextsent>several proposals have already addressed the anaphora resolution problem by deliberately imiting the extent which they relyon domain and/or lin-guistic knowledge (baldwin 1997; <papid> W97-1306 </papid>dagan &amp; itai 1990; <papid> C90-3063 </papid>kennedy &amp; boguraev 1996; <papid> C96-1021 </papid>mitkov 1998; nasukawa 1994; williams et al 1996).</nextsent>
<nextsent>our work is continuation of these latest rends in the search for inexpensive, fast and reliable procedures for anaph-ora resolution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X528">
<title id=" P98-2143.xml">robust pronoun resolution with limited knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluation reports success rate of 89.7% which is better than the suc-cess rates of the approaches selected for comparison and tested on the same data.
</prevsent>
<prevsent>in addition, preliminary experiments show that the approach can be success-fully adapted for other languages with minimum modifications.
</prevsent>
</prevsection>
<citsent citstr=" A88-1003 ">
for the most part, anaphora resolution has focused on traditional linguistic methods (carbonell &amp; brown 1988; carter 1987; hobbs 1978; ingria &amp; stallard 1989; <papid> P89-1032 </papid>lappin &amp; mccord 1990; <papid> J90-4001 </papid>lappin &amp; leass 1994; <papid> J94-4002 </papid>mitkov 1994; <papid> C94-2191 </papid>rich &amp; luperfoy 1988; <papid> A88-1003 </papid>sidner 1979; webber 1979).</citsent>
<aftsection>
<nextsent>however, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.
</nextsent>
<nextsent>while various alternatives have been proposed, making use of e.g. neural networks, situation se-mantics framework, or the principles of reasoning with uncertainty (e.g. connoly et al 1994; mitkov 1995; tin &amp; akman 1995), there is still strong need for the development of robust and effective strategies to meet the demands of practical nlp systems, and to enhance further the automatic pro-cessing of growing language resources.
</nextsent>
<nextsent>several proposals have already addressed the anaphora resolution problem by deliberately imiting the extent which they relyon domain and/or lin-guistic knowledge (baldwin 1997; <papid> W97-1306 </papid>dagan &amp; itai 1990; <papid> C90-3063 </papid>kennedy &amp; boguraev 1996; <papid> C96-1021 </papid>mitkov 1998; nasukawa 1994; williams et al 1996).</nextsent>
<nextsent>our work is continuation of these latest rends in the search for inexpensive, fast and reliable procedures for anaph-ora resolution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X529">
<title id=" P98-2143.xml">robust pronoun resolution with limited knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.
</prevsent>
<prevsent>while various alternatives have been proposed, making use of e.g. neural networks, situation se-mantics framework, or the principles of reasoning with uncertainty (e.g. connoly et al 1994; mitkov 1995; tin &amp; akman 1995), there is still strong need for the development of robust and effective strategies to meet the demands of practical nlp systems, and to enhance further the automatic pro-cessing of growing language resources.
</prevsent>
</prevsection>
<citsent citstr=" W97-1306 ">
several proposals have already addressed the anaphora resolution problem by deliberately imiting the extent which they relyon domain and/or lin-guistic knowledge (baldwin 1997; <papid> W97-1306 </papid>dagan &amp; itai 1990; <papid> C90-3063 </papid>kennedy &amp; boguraev 1996; <papid> C96-1021 </papid>mitkov 1998; nasukawa 1994; williams et al 1996).</citsent>
<aftsection>
<nextsent>our work is continuation of these latest rends in the search for inexpensive, fast and reliable procedures for anaph-ora resolution.
</nextsent>
<nextsent>it is also an example of how anaphors in specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.
</nextsent>
<nextsent>finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for english, but also for other languages (in our case polish and arabic).
</nextsent>
<nextsent>with view to avoiding complex syntactic, seman-tic and discourse analysis (which is vital for real- world applications), we developed robust, knowl- edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X530">
<title id=" P98-2143.xml">robust pronoun resolution with limited knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.
</prevsent>
<prevsent>while various alternatives have been proposed, making use of e.g. neural networks, situation se-mantics framework, or the principles of reasoning with uncertainty (e.g. connoly et al 1994; mitkov 1995; tin &amp; akman 1995), there is still strong need for the development of robust and effective strategies to meet the demands of practical nlp systems, and to enhance further the automatic pro-cessing of growing language resources.
</prevsent>
</prevsection>
<citsent citstr=" C90-3063 ">
several proposals have already addressed the anaphora resolution problem by deliberately imiting the extent which they relyon domain and/or lin-guistic knowledge (baldwin 1997; <papid> W97-1306 </papid>dagan &amp; itai 1990; <papid> C90-3063 </papid>kennedy &amp; boguraev 1996; <papid> C96-1021 </papid>mitkov 1998; nasukawa 1994; williams et al 1996).</citsent>
<aftsection>
<nextsent>our work is continuation of these latest rends in the search for inexpensive, fast and reliable procedures for anaph-ora resolution.
</nextsent>
<nextsent>it is also an example of how anaphors in specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.
</nextsent>
<nextsent>finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for english, but also for other languages (in our case polish and arabic).
</nextsent>
<nextsent>with view to avoiding complex syntactic, seman-tic and discourse analysis (which is vital for real- world applications), we developed robust, knowl- edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X531">
<title id=" P98-2143.xml">robust pronoun resolution with limited knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.
</prevsent>
<prevsent>while various alternatives have been proposed, making use of e.g. neural networks, situation se-mantics framework, or the principles of reasoning with uncertainty (e.g. connoly et al 1994; mitkov 1995; tin &amp; akman 1995), there is still strong need for the development of robust and effective strategies to meet the demands of practical nlp systems, and to enhance further the automatic pro-cessing of growing language resources.
</prevsent>
</prevsection>
<citsent citstr=" C96-1021 ">
several proposals have already addressed the anaphora resolution problem by deliberately imiting the extent which they relyon domain and/or lin-guistic knowledge (baldwin 1997; <papid> W97-1306 </papid>dagan &amp; itai 1990; <papid> C90-3063 </papid>kennedy &amp; boguraev 1996; <papid> C96-1021 </papid>mitkov 1998; nasukawa 1994; williams et al 1996).</citsent>
<aftsection>
<nextsent>our work is continuation of these latest rends in the search for inexpensive, fast and reliable procedures for anaph-ora resolution.
</nextsent>
<nextsent>it is also an example of how anaphors in specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.
</nextsent>
<nextsent>finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for english, but also for other languages (in our case polish and arabic).
</nextsent>
<nextsent>with view to avoiding complex syntactic, seman-tic and discourse analysis (which is vital for real- world applications), we developed robust, knowl- edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X532">
<title id=" P98-2143.xml">robust pronoun resolution with limited knowledge </title>
<section> the approach.  </section>
<citcontext>
<prevsection>
<prevsent>here  the vcr  is penalised (-1) for being part of the prepositional phrase  into the vcr .
</prevsent>
<prevsent>this preference can be explained in terms of sali-ence from the point of view of the centering theory.
</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
the latter proposes the ranking  subject, direct ob-ject, indirect object  (brennan et al 1987) <papid> P87-1022 </papid>and noun phrases which are parts of prepositional phrases are usually indirect objects.</citsent>
<aftsection>
<nextsent>collocation pattern preference this preference is given to candidates which have an identical collocation pattern with pronoun (2), with pronoun (0).
</nextsent>
<nextsent>the collocation preference here is restricted to the patterns  noun phrase (pronoun), verb  and  verb, noun phrase (pronoun) .
</nextsent>
<nextsent>owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in (dagan &amp; itai 1990).<papid> C90-3063 </papid></nextsent>
<nextsent>example: press the key down and turn the volume up...</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X540">
<title id=" S10-1013.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> participants.  </section>
<citcontext>
<prevsection>
<prevsent>- - most frequent sense - 0.462 0.462 0.020 0.472 0.024 0.437 0.035 - - random baseline - 0.294 0.294 0.308 0.257 table 3: overall results for the domain wsd datasets, ordered by recall.
</prevsent>
<prevsent>this is the only group using hand-tagged data from the target domain.
</prevsent>
</prevsection>
<citsent citstr=" E09-1005 ">
their best run ranked 1st.iiitth: they presented personalized pager ank algorithm over graph constructed from wordnet similar to (agirre and soroa, 2009), <papid> E09-1005 </papid>with two variants.</citsent>
<aftsection>
<nextsent>in the first (iiith1), the vertices of the graph are initial ized following the ranking scores obtained from predominant senses as in (mccarthy et al, 2007).<papid> J07-4005 </papid></nextsent>
<nextsent>in the second (iiith2), the graph is initial ized with keyness values as in 77 0.3 0.35 0.4 0.45 0.5 0.55 rel.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X541">
<title id=" S10-1013.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> participants.  </section>
<citcontext>
<prevsection>
<prevsent>this is the only group using hand-tagged data from the target domain.
</prevsent>
<prevsent>their best run ranked 1st.iiitth: they presented personalized pager ank algorithm over graph constructed from wordnet similar to (agirre and soroa, 2009), <papid> E09-1005 </papid>with two variants.</prevsent>
</prevsection>
<citsent citstr=" J07-4005 ">
in the first (iiith1), the vertices of the graph are initial ized following the ranking scores obtained from predominant senses as in (mccarthy et al, 2007).<papid> J07-4005 </papid></citsent>
<aftsection>
<nextsent>in the second (iiith2), the graph is initial ized with keyness values as in 77 0.3 0.35 0.4 0.45 0.5 0.55 rel.
</nextsent>
<nextsent>cliques rel.
</nextsent>
<nextsent>sem.
</nextsent>
<nextsent>trees-2 rel.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X543">
<title id=" S10-1013.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> participants.  </section>
<citcontext>
<prevsection>
<prevsent>they submitted total of six runs, with the best run ranking 3rd.blc20(sc/bg/scbg): this system is supervised.
</prevsent>
<prevsent>a support vector machine was trained using the usual set of features extracted from context and the most frequent class of the target word.
</prevsent>
</prevsection>
<citsent citstr=" E09-1045 ">
semantic class-based classifiers were built from semcor (izquierdo et al, 2009), <papid> E09-1045 </papid>where the classes were automatically obtained exploiting the structural properties of wordnet.</citsent>
<aftsection>
<nextsent>their best run ranked 5th.treematch: this system uses knowledge based disambiguation method that requires dictionary and untagged text as input.
</nextsent>
<nextsent>a previously developed system (chen et al, 2009) <papid> N09-1004 </papid>was adapted to handle domain specific wsd.</nextsent>
<nextsent>they built domain-specific corpus using words mined from relevant web sites (e.g. wwf and ecnc) asseeds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X544">
<title id=" S10-1013.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> participants.  </section>
<citcontext>
<prevsection>
<prevsent>semantic class-based classifiers were built from semcor (izquierdo et al, 2009), <papid> E09-1045 </papid>where the classes were automatically obtained exploiting the structural properties of wordnet.</prevsent>
<prevsent>their best run ranked 5th.treematch: this system uses knowledge based disambiguation method that requires dictionary and untagged text as input.</prevsent>
</prevsection>
<citsent citstr=" N09-1004 ">
a previously developed system (chen et al, 2009) <papid> N09-1004 </papid>was adapted to handle domain specific wsd.</citsent>
<aftsection>
<nextsent>they built domain-specific corpus using words mined from relevant web sites (e.g. wwf and ecnc) asseeds.
</nextsent>
<nextsent>once parsed the corpus, the used the dependency knowledge to build nodeset that wasused for wsd.
</nextsent>
<nextsent>the background documents provided by the organizers were only used to test how exhaustive the initial seeds were.
</nextsent>
<nextsent>their best run ranked 8th.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X547">
<title id=" S10-1013.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> participants.  </section>
<citcontext>
<prevsection>
<prevsent>the best run ranked 13th.
</prevsent>
<prevsent>nlel-wsd(-pdb): the system used for the participation is based on an ensemble of different methods using fuzzy-borda voting.
</prevsent>
</prevsection>
<citsent citstr=" W07-2097 ">
a similar system was proposed in semeval-2007 task-7 (buscaldi and rosso, 2007).<papid> W07-2097 </papid></citsent>
<aftsection>
<nextsent>in this case, the component method used where the following ones:1) most frequent sense from semcor; 2) conceptual density ; 3) supervised domain relative entropy classifier based on wordnet domains;4) supervised bayesian classifier based on wordnet domains probabilities; and 5) unsupervised knownet-20 classifiers.
</nextsent>
<nextsent>the best run ranked 24th.
</nextsent>
<nextsent>umcc-dlsi (relevant): the team submitted three different runs using knowledge-based system.
</nextsent>
<nextsent>the first two runs use domain vectors and the third is based on cliques, which measure how much concept is correlated to the sentence by obtaining relevant semantic trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X548">
<title id=" S10-1013.xml">semeval2010 task 17 all words word sense disambiguation on a specific domain </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>the first sense baseline in english and chinese corresponds to the most frequent sense, as estimated from out-of-domaincorpora.
</prevsent>
<prevsent>in dutch and italian, it followed the intuitions of the lexicographer.
</prevsent>
</prevsection>
<citsent citstr=" H05-1053 ">
note that we dont have the most frequent sense baseline from the domain texts, which would surely show higher results (koeling et al, 2005).<papid> H05-1053 </papid></citsent>
<aftsection>
<nextsent>domain portability and adaptation of nlp components and word sense disambiguation systems present new challenges.
</nextsent>
<nextsent>the difficulties found by supervised systems to adapt might change the waywe assess the strengths and weaknesses of supervised and knowledge-based wsd systems.
</nextsent>
<nextsent>with this paper we have motivated the creation of anall-words test dataset for wsd on the environment domain in several languages, and presented the overall design of this semeval task.
</nextsent>
<nextsent>one of the goals of the exercise was to show that wsd systems could make use of unannotated background corpora to adapt to the domain and improve their results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X549">
<title id=" S10-1007.xml">semeval2 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes some background, the task definition, the process of data collection and the task results.
</prevsent>
<prevsent>wealso venture few general conclusions before the participating teams present their systems at the semeval-2010 workshop.there were 5 teams who submitted 7 systems.
</prevsent>
</prevsection>
<citsent citstr=" E03-1073 ">
noun compounds (ncs) are sequences of two or more nouns that act as single noun,1 e.g., stem cell, stem cell research, stem cell research organization, etc. lapata and lascarides (2003) <papid> E03-1073 </papid>observe that ncs pose syntactic and semantic challenges for three basic reasons: (1) the compounding proces sis extremely productive in english; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors.</citsent>
<aftsection>
<nextsent>corpus studies have shown that while ncs are very common in english, their frequency distribution follows zip fian or power-law distribution and the majority of ncs encountered will be rare types (tanaka and baldwin, 2003; <papid> W03-1803 </papid>lapata and lascarides, 2003; <papid> E03-1073 </papid>baldwin and tanaka, 2004; <papid> W04-0404 </papid>seaghdha, 2008).</nextsent>
<nextsent>as consequence, natural language processing (nlp) 1 we follow the definition in (downing, 1977).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X550">
<title id=" S10-1007.xml">semeval2 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wealso venture few general conclusions before the participating teams present their systems at the semeval-2010 workshop.there were 5 teams who submitted 7 systems.
</prevsent>
<prevsent>noun compounds (ncs) are sequences of two or more nouns that act as single noun,1 e.g., stem cell, stem cell research, stem cell research organization, etc. lapata and lascarides (2003) <papid> E03-1073 </papid>observe that ncs pose syntactic and semantic challenges for three basic reasons: (1) the compounding proces sis extremely productive in english; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors.</prevsent>
</prevsection>
<citsent citstr=" W03-1803 ">
corpus studies have shown that while ncs are very common in english, their frequency distribution follows zip fian or power-law distribution and the majority of ncs encountered will be rare types (tanaka and baldwin, 2003; <papid> W03-1803 </papid>lapata and lascarides, 2003; <papid> E03-1073 </papid>baldwin and tanaka, 2004; <papid> W04-0404 </papid>seaghdha, 2008).</citsent>
<aftsection>
<nextsent>as consequence, natural language processing (nlp) 1 we follow the definition in (downing, 1977).
</nextsent>
<nextsent>applications cannot afford either to ignore ncs or to assume that they can be handled by relying on dictionary or other static resource.trouble with lexical resources for ncs notwithstanding, nc semantics plays central role in complex knowledge discovery and applications, including but not limited to question answering (qa),machine translation (mt), and information retrieval (ir).
</nextsent>
<nextsent>for example, knowing the (implicit) semantic relation between the nc components can help rank and refine queries in qa and ir, or select promising translation pairs in mt (nakov, 2008a).
</nextsent>
<nextsent>thus, robust semantic interpretation of ncs should be of much help in broad-coverage semantic pro cessing.proposed approaches to modelling nc semantics have used semantic similarity (nastase and szpakowicz, 2003; moldovan et al, 2004; <papid> W04-2609 </papid>kim and baldwin, 2005; <papid> I05-1082 </papid>nastase and szpakowicz, 2006; <papid> W06-3813 </papid>girju, 2007; seaghdha and copestake, 2007) and paraphrasing (vanderwende, 1994; <papid> C94-2125 </papid>kim and baldwin, 2006; <papid> P06-2064 </papid>butnariu and veale, 2008; <papid> C08-1011 </papid>nakov and hearst, 2008).<papid> P08-1052 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X552">
<title id=" S10-1007.xml">semeval2 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wealso venture few general conclusions before the participating teams present their systems at the semeval-2010 workshop.there were 5 teams who submitted 7 systems.
</prevsent>
<prevsent>noun compounds (ncs) are sequences of two or more nouns that act as single noun,1 e.g., stem cell, stem cell research, stem cell research organization, etc. lapata and lascarides (2003) <papid> E03-1073 </papid>observe that ncs pose syntactic and semantic challenges for three basic reasons: (1) the compounding proces sis extremely productive in english; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors.</prevsent>
</prevsection>
<citsent citstr=" W04-0404 ">
corpus studies have shown that while ncs are very common in english, their frequency distribution follows zip fian or power-law distribution and the majority of ncs encountered will be rare types (tanaka and baldwin, 2003; <papid> W03-1803 </papid>lapata and lascarides, 2003; <papid> E03-1073 </papid>baldwin and tanaka, 2004; <papid> W04-0404 </papid>seaghdha, 2008).</citsent>
<aftsection>
<nextsent>as consequence, natural language processing (nlp) 1 we follow the definition in (downing, 1977).
</nextsent>
<nextsent>applications cannot afford either to ignore ncs or to assume that they can be handled by relying on dictionary or other static resource.trouble with lexical resources for ncs notwithstanding, nc semantics plays central role in complex knowledge discovery and applications, including but not limited to question answering (qa),machine translation (mt), and information retrieval (ir).
</nextsent>
<nextsent>for example, knowing the (implicit) semantic relation between the nc components can help rank and refine queries in qa and ir, or select promising translation pairs in mt (nakov, 2008a).
</nextsent>
<nextsent>thus, robust semantic interpretation of ncs should be of much help in broad-coverage semantic pro cessing.proposed approaches to modelling nc semantics have used semantic similarity (nastase and szpakowicz, 2003; moldovan et al, 2004; <papid> W04-2609 </papid>kim and baldwin, 2005; <papid> I05-1082 </papid>nastase and szpakowicz, 2006; <papid> W06-3813 </papid>girju, 2007; seaghdha and copestake, 2007) and paraphrasing (vanderwende, 1994; <papid> C94-2125 </papid>kim and baldwin, 2006; <papid> P06-2064 </papid>butnariu and veale, 2008; <papid> C08-1011 </papid>nakov and hearst, 2008).<papid> P08-1052 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X553">
<title id=" S10-1007.xml">semeval2 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applications cannot afford either to ignore ncs or to assume that they can be handled by relying on dictionary or other static resource.trouble with lexical resources for ncs notwithstanding, nc semantics plays central role in complex knowledge discovery and applications, including but not limited to question answering (qa),machine translation (mt), and information retrieval (ir).
</prevsent>
<prevsent>for example, knowing the (implicit) semantic relation between the nc components can help rank and refine queries in qa and ir, or select promising translation pairs in mt (nakov, 2008a).
</prevsent>
</prevsection>
<citsent citstr=" W04-2609 ">
thus, robust semantic interpretation of ncs should be of much help in broad-coverage semantic pro cessing.proposed approaches to modelling nc semantics have used semantic similarity (nastase and szpakowicz, 2003; moldovan et al, 2004; <papid> W04-2609 </papid>kim and baldwin, 2005; <papid> I05-1082 </papid>nastase and szpakowicz, 2006; <papid> W06-3813 </papid>girju, 2007; seaghdha and copestake, 2007) and paraphrasing (vanderwende, 1994; <papid> C94-2125 </papid>kim and baldwin, 2006; <papid> P06-2064 </papid>butnariu and veale, 2008; <papid> C08-1011 </papid>nakov and hearst, 2008).<papid> P08-1052 </papid></citsent>
<aftsection>
<nextsent>the former body of work seeksto measure the similarity between known and unseen ncs by considering various features, usually context-related.
</nextsent>
<nextsent>in contrast, the latter group uses verb semantics to interpret ncs directly, e.g., olive oil as oil that is extracted from olive(s)?, drug death as death that is caused by drug(s)?, flu shot as shot that prevents flu?.
</nextsent>
<nextsent>the growing popularity ? and expected direct utility ? of paraphrase-based nc semantics has encouraged us to propose an evaluation exercise for the 2010 edition of semeval.
</nextsent>
<nextsent>this paper gives birds-eye view of the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X554">
<title id=" S10-1007.xml">semeval2 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applications cannot afford either to ignore ncs or to assume that they can be handled by relying on dictionary or other static resource.trouble with lexical resources for ncs notwithstanding, nc semantics plays central role in complex knowledge discovery and applications, including but not limited to question answering (qa),machine translation (mt), and information retrieval (ir).
</prevsent>
<prevsent>for example, knowing the (implicit) semantic relation between the nc components can help rank and refine queries in qa and ir, or select promising translation pairs in mt (nakov, 2008a).
</prevsent>
</prevsection>
<citsent citstr=" I05-1082 ">
thus, robust semantic interpretation of ncs should be of much help in broad-coverage semantic pro cessing.proposed approaches to modelling nc semantics have used semantic similarity (nastase and szpakowicz, 2003; moldovan et al, 2004; <papid> W04-2609 </papid>kim and baldwin, 2005; <papid> I05-1082 </papid>nastase and szpakowicz, 2006; <papid> W06-3813 </papid>girju, 2007; seaghdha and copestake, 2007) and paraphrasing (vanderwende, 1994; <papid> C94-2125 </papid>kim and baldwin, 2006; <papid> P06-2064 </papid>butnariu and veale, 2008; <papid> C08-1011 </papid>nakov and hearst, 2008).<papid> P08-1052 </papid></citsent>
<aftsection>
<nextsent>the former body of work seeksto measure the similarity between known and unseen ncs by considering various features, usually context-related.
</nextsent>
<nextsent>in contrast, the latter group uses verb semantics to interpret ncs directly, e.g., olive oil as oil that is extracted from olive(s)?, drug death as death that is caused by drug(s)?, flu shot as shot that prevents flu?.
</nextsent>
<nextsent>the growing popularity ? and expected direct utility ? of paraphrase-based nc semantics has encouraged us to propose an evaluation exercise for the 2010 edition of semeval.
</nextsent>
<nextsent>this paper gives birds-eye view of the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X555">
<title id=" S10-1007.xml">semeval2 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applications cannot afford either to ignore ncs or to assume that they can be handled by relying on dictionary or other static resource.trouble with lexical resources for ncs notwithstanding, nc semantics plays central role in complex knowledge discovery and applications, including but not limited to question answering (qa),machine translation (mt), and information retrieval (ir).
</prevsent>
<prevsent>for example, knowing the (implicit) semantic relation between the nc components can help rank and refine queries in qa and ir, or select promising translation pairs in mt (nakov, 2008a).
</prevsent>
</prevsection>
<citsent citstr=" W06-3813 ">
thus, robust semantic interpretation of ncs should be of much help in broad-coverage semantic pro cessing.proposed approaches to modelling nc semantics have used semantic similarity (nastase and szpakowicz, 2003; moldovan et al, 2004; <papid> W04-2609 </papid>kim and baldwin, 2005; <papid> I05-1082 </papid>nastase and szpakowicz, 2006; <papid> W06-3813 </papid>girju, 2007; seaghdha and copestake, 2007) and paraphrasing (vanderwende, 1994; <papid> C94-2125 </papid>kim and baldwin, 2006; <papid> P06-2064 </papid>butnariu and veale, 2008; <papid> C08-1011 </papid>nakov and hearst, 2008).<papid> P08-1052 </papid></citsent>
<aftsection>
<nextsent>the former body of work seeksto measure the similarity between known and unseen ncs by considering various features, usually context-related.
</nextsent>
<nextsent>in contrast, the latter group uses verb semantics to interpret ncs directly, e.g., olive oil as oil that is extracted from olive(s)?, drug death as death that is caused by drug(s)?, flu shot as shot that prevents flu?.
</nextsent>
<nextsent>the growing popularity ? and expected direct utility ? of paraphrase-based nc semantics has encouraged us to propose an evaluation exercise for the 2010 edition of semeval.
</nextsent>
<nextsent>this paper gives birds-eye view of the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X556">
<title id=" S10-1007.xml">semeval2 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applications cannot afford either to ignore ncs or to assume that they can be handled by relying on dictionary or other static resource.trouble with lexical resources for ncs notwithstanding, nc semantics plays central role in complex knowledge discovery and applications, including but not limited to question answering (qa),machine translation (mt), and information retrieval (ir).
</prevsent>
<prevsent>for example, knowing the (implicit) semantic relation between the nc components can help rank and refine queries in qa and ir, or select promising translation pairs in mt (nakov, 2008a).
</prevsent>
</prevsection>
<citsent citstr=" C94-2125 ">
thus, robust semantic interpretation of ncs should be of much help in broad-coverage semantic pro cessing.proposed approaches to modelling nc semantics have used semantic similarity (nastase and szpakowicz, 2003; moldovan et al, 2004; <papid> W04-2609 </papid>kim and baldwin, 2005; <papid> I05-1082 </papid>nastase and szpakowicz, 2006; <papid> W06-3813 </papid>girju, 2007; seaghdha and copestake, 2007) and paraphrasing (vanderwende, 1994; <papid> C94-2125 </papid>kim and baldwin, 2006; <papid> P06-2064 </papid>butnariu and veale, 2008; <papid> C08-1011 </papid>nakov and hearst, 2008).<papid> P08-1052 </papid></citsent>
<aftsection>
<nextsent>the former body of work seeksto measure the similarity between known and unseen ncs by considering various features, usually context-related.
</nextsent>
<nextsent>in contrast, the latter group uses verb semantics to interpret ncs directly, e.g., olive oil as oil that is extracted from olive(s)?, drug death as death that is caused by drug(s)?, flu shot as shot that prevents flu?.
</nextsent>
<nextsent>the growing popularity ? and expected direct utility ? of paraphrase-based nc semantics has encouraged us to propose an evaluation exercise for the 2010 edition of semeval.
</nextsent>
<nextsent>this paper gives birds-eye view of the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X557">
<title id=" S10-1007.xml">semeval2 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applications cannot afford either to ignore ncs or to assume that they can be handled by relying on dictionary or other static resource.trouble with lexical resources for ncs notwithstanding, nc semantics plays central role in complex knowledge discovery and applications, including but not limited to question answering (qa),machine translation (mt), and information retrieval (ir).
</prevsent>
<prevsent>for example, knowing the (implicit) semantic relation between the nc components can help rank and refine queries in qa and ir, or select promising translation pairs in mt (nakov, 2008a).
</prevsent>
</prevsection>
<citsent citstr=" P06-2064 ">
thus, robust semantic interpretation of ncs should be of much help in broad-coverage semantic pro cessing.proposed approaches to modelling nc semantics have used semantic similarity (nastase and szpakowicz, 2003; moldovan et al, 2004; <papid> W04-2609 </papid>kim and baldwin, 2005; <papid> I05-1082 </papid>nastase and szpakowicz, 2006; <papid> W06-3813 </papid>girju, 2007; seaghdha and copestake, 2007) and paraphrasing (vanderwende, 1994; <papid> C94-2125 </papid>kim and baldwin, 2006; <papid> P06-2064 </papid>butnariu and veale, 2008; <papid> C08-1011 </papid>nakov and hearst, 2008).<papid> P08-1052 </papid></citsent>
<aftsection>
<nextsent>the former body of work seeksto measure the similarity between known and unseen ncs by considering various features, usually context-related.
</nextsent>
<nextsent>in contrast, the latter group uses verb semantics to interpret ncs directly, e.g., olive oil as oil that is extracted from olive(s)?, drug death as death that is caused by drug(s)?, flu shot as shot that prevents flu?.
</nextsent>
<nextsent>the growing popularity ? and expected direct utility ? of paraphrase-based nc semantics has encouraged us to propose an evaluation exercise for the 2010 edition of semeval.
</nextsent>
<nextsent>this paper gives birds-eye view of the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X558">
<title id=" S10-1007.xml">semeval2 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applications cannot afford either to ignore ncs or to assume that they can be handled by relying on dictionary or other static resource.trouble with lexical resources for ncs notwithstanding, nc semantics plays central role in complex knowledge discovery and applications, including but not limited to question answering (qa),machine translation (mt), and information retrieval (ir).
</prevsent>
<prevsent>for example, knowing the (implicit) semantic relation between the nc components can help rank and refine queries in qa and ir, or select promising translation pairs in mt (nakov, 2008a).
</prevsent>
</prevsection>
<citsent citstr=" C08-1011 ">
thus, robust semantic interpretation of ncs should be of much help in broad-coverage semantic pro cessing.proposed approaches to modelling nc semantics have used semantic similarity (nastase and szpakowicz, 2003; moldovan et al, 2004; <papid> W04-2609 </papid>kim and baldwin, 2005; <papid> I05-1082 </papid>nastase and szpakowicz, 2006; <papid> W06-3813 </papid>girju, 2007; seaghdha and copestake, 2007) and paraphrasing (vanderwende, 1994; <papid> C94-2125 </papid>kim and baldwin, 2006; <papid> P06-2064 </papid>butnariu and veale, 2008; <papid> C08-1011 </papid>nakov and hearst, 2008).<papid> P08-1052 </papid></citsent>
<aftsection>
<nextsent>the former body of work seeksto measure the similarity between known and unseen ncs by considering various features, usually context-related.
</nextsent>
<nextsent>in contrast, the latter group uses verb semantics to interpret ncs directly, e.g., olive oil as oil that is extracted from olive(s)?, drug death as death that is caused by drug(s)?, flu shot as shot that prevents flu?.
</nextsent>
<nextsent>the growing popularity ? and expected direct utility ? of paraphrase-based nc semantics has encouraged us to propose an evaluation exercise for the 2010 edition of semeval.
</nextsent>
<nextsent>this paper gives birds-eye view of the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X559">
<title id=" S10-1007.xml">semeval2 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applications cannot afford either to ignore ncs or to assume that they can be handled by relying on dictionary or other static resource.trouble with lexical resources for ncs notwithstanding, nc semantics plays central role in complex knowledge discovery and applications, including but not limited to question answering (qa),machine translation (mt), and information retrieval (ir).
</prevsent>
<prevsent>for example, knowing the (implicit) semantic relation between the nc components can help rank and refine queries in qa and ir, or select promising translation pairs in mt (nakov, 2008a).
</prevsent>
</prevsection>
<citsent citstr=" P08-1052 ">
thus, robust semantic interpretation of ncs should be of much help in broad-coverage semantic pro cessing.proposed approaches to modelling nc semantics have used semantic similarity (nastase and szpakowicz, 2003; moldovan et al, 2004; <papid> W04-2609 </papid>kim and baldwin, 2005; <papid> I05-1082 </papid>nastase and szpakowicz, 2006; <papid> W06-3813 </papid>girju, 2007; seaghdha and copestake, 2007) and paraphrasing (vanderwende, 1994; <papid> C94-2125 </papid>kim and baldwin, 2006; <papid> P06-2064 </papid>butnariu and veale, 2008; <papid> C08-1011 </papid>nakov and hearst, 2008).<papid> P08-1052 </papid></citsent>
<aftsection>
<nextsent>the former body of work seeksto measure the similarity between known and unseen ncs by considering various features, usually context-related.
</nextsent>
<nextsent>in contrast, the latter group uses verb semantics to interpret ncs directly, e.g., olive oil as oil that is extracted from olive(s)?, drug death as death that is caused by drug(s)?, flu shot as shot that prevents flu?.
</nextsent>
<nextsent>the growing popularity ? and expected direct utility ? of paraphrase-based nc semantics has encouraged us to propose an evaluation exercise for the 2010 edition of semeval.
</nextsent>
<nextsent>this paper gives birds-eye view of the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X560">
<title id=" S10-1007.xml">semeval2 task 9 the interpretation of noun compounds using paraphrasing verbs and prepositions </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>its api allows computer program to run tasks easily and collate the subjects?
</prevsent>
<prevsent>responses.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
mturk is becominga popular means of eliciting and collecting linguistic intuitions for nlp research; see snow et al (2008) <papid> D08-1027 </papid>for an overview and further discussion.</citsent>
<aftsection>
<nextsent>even though we recruited human subjects, whom we required to take qualification test,3 2 www.mturk.com 3we soon realized that we also had to offer version of our assignments without qualification test (at lower pay rate) since very few people were willing to take test.
</nextsent>
<nextsent>overall, data collection was time-consuming since many annotators did not follow the instructions.
</nextsent>
<nextsent>we had to monitor their progress and to send them timely messages, pointing out mistakes.
</nextsent>
<nextsent>although them turk service allows task owners to accept or reject individual submissions, rejection was the last resort since it has the triply unpleasant effect of(1) denying the worker her fee, (2) negatively affecting her rating, and (3) lowering our rating as requester.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X561">
<title id=" S12-1101.xml">penn using word similarities to better estimate sentence similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the state of the art model in paraphrase detection uses an unsupervised recursive autoencoder (rae) model based on an unfolding objective that learn feature vectors for phrases in syntactic parse trees (socher et al, 2011).
</prevsent>
<prevsent>the idea of neural language models is to jointly learn an embedding of words into an n-dimensional vector space that capture distributional syntactic and semantic information via the words co-occurrence statistics.
</prevsent>
</prevsection>
<citsent citstr=" P10-1040 ">
further details and evaluations of these embed dings are discussed in turian et al (2010).<papid> P10-1040 </papid></citsent>
<aftsection>
<nextsent>once the distributional syntactic and semantic matrix is learned on an unlabeled corpus, one canuse it for subsequent tasks by using each words vector to represent that word.
</nextsent>
<nextsent>for initial word embed dings, we used the 100-dimensional vectors com 679 puted via the unsupervised method of collobert and weston (2008).
</nextsent>
<nextsent>these word embed dings are matrices of size |v | ? where |v | is the size of the vocabulary and is the dimensionality of the semantic space.
</nextsent>
<nextsent>this matrix usually captures co-occurrence statistics and its values are learned.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X562">
<title id=" S12-1101.xml">penn using word similarities to better estimate sentence similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>for this task, we only use context-oblivious embed dings i.e. one embedding per word type for this task, basedon their model.
</prevsent>
<prevsent>word similarity can then be calculated as cosine similarity between the eigenword representation vectors for any two words.to move from word-level similarity to sentence level few more steps are necessary.
</prevsent>
</prevsection>
<citsent citstr=" P05-1047 ">
we adapted the method of matrix similarity given by stevenson and greenwood (2005).<papid> P05-1047 </papid></citsent>
<aftsection>
<nextsent>one calculates similarity between all pairs of words, and each sentence is represented as binary vector (with elements equal to 1 if word is present and 0 otherwise).
</nextsent>
<nextsent>the similarity between these sentences vectors ~a and~b is given by: s(~a,~b) = ~aw~b |~a||~b| (1)where is semantic similarity matrix containing information about the similarity of word pairs.
</nextsent>
<nextsent>each element in matrix represents the similarity of words according to some lexical or spectral similarity measure.
</nextsent>
<nextsent>2.3 selector similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X563">
<title id=" S10-1087.xml">iiith domain specific word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the test data was disambiguated using the personalized page rank algorithm which was applied to graph constructed from the whole of wordnet in which nodes are initial ized with ranking scores of words and their senses.
</prevsent>
<prevsent>in the competition, our systems achieved comparable accuracy of 53.4 and 52.2, which outperforms the most frequent sense baseline (50.5).
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
the senses in wordnet are ordered according totheir frequency in manually tagged corpus, semcor (miller et al, 1993).<papid> H93-1061 </papid></citsent>
<aftsection>
<nextsent>senses that do not occur in semcor are ordered arbitrarily after those senses of the word that have occurred.
</nextsent>
<nextsent>it is known from the results of senseval2 (cotton et al, 2001) and senseval3 (mihalcea and edmonds, 2004) that first sense heuristic outperforms many wsd systems (see mccarthy et al (2007)).<papid> J07-4005 </papid></nextsent>
<nextsent>the first sense baselines strong performance is due to the skewed frequency distribution of word senses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X564">
<title id=" S10-1087.xml">iiith domain specific word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the senses in wordnet are ordered according totheir frequency in manually tagged corpus, semcor (miller et al, 1993).<papid> H93-1061 </papid></prevsent>
<prevsent>senses that do not occur in semcor are ordered arbitrarily after those senses of the word that have occurred.</prevsent>
</prevsection>
<citsent citstr=" J07-4005 ">
it is known from the results of senseval2 (cotton et al, 2001) and senseval3 (mihalcea and edmonds, 2004) that first sense heuristic outperforms many wsd systems (see mccarthy et al (2007)).<papid> J07-4005 </papid></citsent>
<aftsection>
<nextsent>the first sense baselines strong performance is due to the skewed frequency distribution of word senses.
</nextsent>
<nextsent>wordnet sense distributions based on semcor are clearly useful, however in given domain these distributions may not hold true.
</nextsent>
<nextsent>for example, the first sense for bank?
</nextsent>
<nextsent>in wordnet refers to sloping land beside body of river?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X565">
<title id=" S10-1087.xml">iiith domain specific word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sense would be expected to be more likely than the sloping land beside body of river?
</prevsent>
<prevsent>sense.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
unfortunately, itis not feasible to produce large manually sense annotated corpora for every domain of interest.mccarthy et al (2004) <papid> P04-1036 </papid>propose method to predict sense distributions from raw corpora and use this as first sense heuristic for tagging text withthe predominant sense.</citsent>
<aftsection>
<nextsent>rather than assigning predominant sense in every case, our approach aimsto use these sense distributions collected from do main specific corpora as knowledge source and combine this with information from the context.
</nextsent>
<nextsent>our approach focuses on the strong influence of domain for wsd (buitelaar et al, 2006) and the benefits of focusing on words salient to the do main (koeling et al, 2005).<papid> H05-1053 </papid></nextsent>
<nextsent>words are assigned ranking score based on its keyness (salience) in the given domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X567">
<title id=" S10-1087.xml">iiith domain specific word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, itis not feasible to produce large manually sense annotated corpora for every domain of interest.mccarthy et al (2004) <papid> P04-1036 </papid>propose method to predict sense distributions from raw corpora and use this as first sense heuristic for tagging text withthe predominant sense.</prevsent>
<prevsent>rather than assigning predominant sense in every case, our approach aimsto use these sense distributions collected from do main specific corpora as knowledge source and combine this with information from the context.</prevsent>
</prevsection>
<citsent citstr=" H05-1053 ">
our approach focuses on the strong influence of domain for wsd (buitelaar et al, 2006) and the benefits of focusing on words salient to the do main (koeling et al, 2005).<papid> H05-1053 </papid></citsent>
<aftsection>
<nextsent>words are assigned ranking score based on its keyness (salience) in the given domain.
</nextsent>
<nextsent>we use these word scores as another knowledge source.
</nextsent>
<nextsent>graph based methods have been shown to produce state-of-the-art performance for unsupervised word sense disambiguation (agirre and soroa, 2009; <papid> E09-1005 </papid>sinha and mihalcea, 2007).</nextsent>
<nextsent>these approaches use well-known graph-based techniques to find and exploit the structural properties of the graph underlying particular lexical knowledge base (lkb), such as wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X568">
<title id=" S10-1087.xml">iiith domain specific word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>words are assigned ranking score based on its keyness (salience) in the given domain.
</prevsent>
<prevsent>we use these word scores as another knowledge source.
</prevsent>
</prevsection>
<citsent citstr=" E09-1005 ">
graph based methods have been shown to produce state-of-the-art performance for unsupervised word sense disambiguation (agirre and soroa, 2009; <papid> E09-1005 </papid>sinha and mihalcea, 2007).</citsent>
<aftsection>
<nextsent>these approaches use well-known graph-based techniques to find and exploit the structural properties of the graph underlying particular lexical knowledge base (lkb), such as wordnet.
</nextsent>
<nextsent>these graph based algorithms are appealing because they take into account information drawn from the entire graph as well as from the given context, making them superior to other approaches that rely only on local information individually derived for each word.
</nextsent>
<nextsent>our approach uses the personalized page rank algorithm (agirre and soroa, 2009) <papid> E09-1005 </papid>over graph 387representing wordnet to disambiguate ambiguous words by taking their context into consideration.</nextsent>
<nextsent>we also combine domain-specific information from the knowledge sources, like sense distribution scores and keyword ranking scores, into the graph thus personalizing the graph for the given domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X573">
<title id=" S10-1087.xml">iiith domain specific word sense disambiguation </title>
<section> domain sense ranking.  </section>
<citcontext>
<prevsection>
<prevsent>evaluation results over the semeval data are provided in section 5.
</prevsent>
<prevsent>mccarthy et al (2004) <papid> P04-1036 </papid>propose method for finding predominant senses from raw text.</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
the method uses thesaurus acquired from automatically parsed text based on the method described by lin (1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>this provides the top nearest neighbours for each target word w, along with the distributional similarity score between the target word and each neighbour.
</nextsent>
<nextsent>the senses of word are each assigned score by summing over the distributional similarity scores of its neighbours.
</nextsent>
<nextsent>these are weighted by semantic similarity score (using wordnet similarity score (pedersen et al, 2004) <papid> N04-3012 </papid>between the sense of and the sense of the neighbour that maximizes the semantic similarity score.</nextsent>
<nextsent>more formally, let w = {n 1 , 2 , . . .</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X574">
<title id=" S10-1087.xml">iiith domain specific word sense disambiguation </title>
<section> domain sense ranking.  </section>
<citcontext>
<prevsection>
<prevsent>this provides the top nearest neighbours for each target word w, along with the distributional similarity score between the target word and each neighbour.
</prevsent>
<prevsent>the senses of word are each assigned score by summing over the distributional similarity scores of its neighbours.
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
these are weighted by semantic similarity score (using wordnet similarity score (pedersen et al, 2004) <papid> N04-3012 </papid>between the sense of and the sense of the neighbour that maximizes the semantic similarity score.</citsent>
<aftsection>
<nextsent>more formally, let w = {n 1 , 2 , . . .
</nextsent>
<nextsent>n } be the ordered set of the top scoring neighbours of from the thesaurus with associated distributional similarity scores {dss(w, 1 ), dss(w, 2 ), . . .
</nextsent>
<nextsent>dss(w, k )}.
</nextsent>
<nextsent>let senses(w) be the set of senses of w. for each sense of (ws ? senses(w)) ranking score is obtained by summing over the dss(w, j ) of each neighbour (n ? w ) multiplied by weight.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X576">
<title id=" S10-1087.xml">iiith domain specific word sense disambiguation </title>
<section> domain sense ranking.  </section>
<citcontext>
<prevsection>
<prevsent>we used the background documents provided to the participants in this task as domain specific corpus.
</prevsent>
<prevsent>in general, domain specific corpus can be obtained using domain-specific keywords (kilgarriff et al, 2010).
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
a thesaurus is acquired from automatically parsed background documents using the stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>we used = 5 to built the thesaurus.
</nextsent>
<nextsent>as we increased we found the number of non-domain specific words occurring in the thesaurus increased and negatively affected the sense distributions.
</nextsent>
<nextsent>to counter this, one of our systems iiith2 used slightly modified ranking score by multiplying the effect of each neighbour with its domain keyword ranking score.
</nextsent>
<nextsent>the modified sense ranking msrs(ws ) score of sense ws is msrs(ws ) = ? j n dss(w, j )?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X579">
<title id=" S10-1087.xml">iiith domain specific word sense disambiguation </title>
<section> domain keyword ranking.  </section>
<citcontext>
<prevsection>
<prevsent>iiith1 and iiith2 systems differ in the way senses are ranked.
</prevsent>
<prevsent>iiith1 uses srs(ws ) whereas iiith2 system uses msrs(ws ) for computing sense ranking scores in the given domain.
</prevsent>
</prevsection>
<citsent citstr=" W00-0901 ">
we extracted keywords in the domain by comparing the frequency lists of domain corpora (background documents) and very large general corpus, ukwac (ferraresi et al, 2008), using the method described by rayson and garside (2000).<papid> W00-0901 </papid></citsent>
<aftsection>
<nextsent>for each word in the frequency list of the domain corpora, words(domain), we calculated the log likelihood (ll) statistic as described in rayson and garside (2000).<papid> W00-0901 </papid></nextsent>
<nextsent>we then normalized ll to compute keyword ranking score krs(w) of word words(domain) using 388 krs(w) = ll(w) ? i words(domain) ll(w ) the above score represents the keyness of the word in the given domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X587">
<title id=" P99-1019.xml">bilingual hebrew english generation of possess ives and partitives raising the input abstraction level </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we conclude by evaluating how the resulting input specification language is appropriate for both languages.
</prevsent>
<prevsent>one of the first issues to address when se-lecting syntactic realization component is whether its input specification language fits the desired application.
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
traditionally, syntactic realization components have at-tempted to raise the abstraction level of in- put specifications for two reasons: (1) to pre-serve the possibility of paraphrasing and (2) to make it easy for the sentence planner to map from semantic data to syntactic input as new applications appear, that can- not start generation from semantic in- put because such an input is not available (for example re-generation of sentences from syntactic fragments to produce summaries (barzilay et al, 1999) <papid> P99-1071 </papid>or generation of com-plex nps in hybrid template system for business letters (gedalia, 1996)), this moti-vation has lost some of its strength.</citsent>
<aftsection>
<nextsent>con-sequently,  shallow surface generators  have recently appeared (lavoie and rambow, 1997) (<papid> A97-1039 </papid>busemann and horacek, 1998) <papid> W98-1425 </papid>that require an input considerably less abstract than those required by more traditional re-alization components uch as surge (e1- hadad and robin, 1996) or kpml (bate- man, 1997).</nextsent>
<nextsent>in this paper, we contribute to the de-bate on selecting an appropriate vel of ab-straction by considering the case of bilin-gual generation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X588">
<title id=" P99-1019.xml">bilingual hebrew english generation of possess ives and partitives raising the input abstraction level </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the first issues to address when se-lecting syntactic realization component is whether its input specification language fits the desired application.
</prevsent>
<prevsent>traditionally, syntactic realization components have at-tempted to raise the abstraction level of in- put specifications for two reasons: (1) to pre-serve the possibility of paraphrasing and (2) to make it easy for the sentence planner to map from semantic data to syntactic input as new applications appear, that can- not start generation from semantic in- put because such an input is not available (for example re-generation of sentences from syntactic fragments to produce summaries (barzilay et al, 1999) <papid> P99-1071 </papid>or generation of com-plex nps in hybrid template system for business letters (gedalia, 1996)), this moti-vation has lost some of its strength.</prevsent>
</prevsection>
<citsent citstr=" A97-1039 ">
con-sequently,  shallow surface generators  have recently appeared (lavoie and rambow, 1997) (<papid> A97-1039 </papid>busemann and horacek, 1998) <papid> W98-1425 </papid>that require an input considerably less abstract than those required by more traditional re-alization components uch as surge (e1- hadad and robin, 1996) or kpml (bate- man, 1997).</citsent>
<aftsection>
<nextsent>in this paper, we contribute to the de-bate on selecting an appropriate vel of ab-straction by considering the case of bilin-gual generation.
</nextsent>
<nextsent>we present results ob-tained while developing the hugg syntactic realization component for hebrew (dahan- netzer, 1997).
</nextsent>
<nextsent>one of the goals of this sys-tem is to design generator with an input specification language as similar as possible to that of an english generator, surge in our case . the ideal scenario for bilingual generation is illustrated in figure 1.
</nextsent>
<nextsent>it consists of the 144 john gave book to mary john natan sefer le-mary cat proc partic :lause type relation-type agent affected possessor possessed composite \] possessive lex  john  gender masculine \[cat proper \] \[1\] lex  mary  gender feminine \[1\] cat common \] lex  book/sefer  figure 1  ideal scenario for bilingual gener-ation following steps: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X589">
<title id=" P99-1019.xml">bilingual hebrew english generation of possess ives and partitives raising the input abstraction level </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the first issues to address when se-lecting syntactic realization component is whether its input specification language fits the desired application.
</prevsent>
<prevsent>traditionally, syntactic realization components have at-tempted to raise the abstraction level of in- put specifications for two reasons: (1) to pre-serve the possibility of paraphrasing and (2) to make it easy for the sentence planner to map from semantic data to syntactic input as new applications appear, that can- not start generation from semantic in- put because such an input is not available (for example re-generation of sentences from syntactic fragments to produce summaries (barzilay et al, 1999) <papid> P99-1071 </papid>or generation of com-plex nps in hybrid template system for business letters (gedalia, 1996)), this moti-vation has lost some of its strength.</prevsent>
</prevsection>
<citsent citstr=" W98-1425 ">
con-sequently,  shallow surface generators  have recently appeared (lavoie and rambow, 1997) (<papid> A97-1039 </papid>busemann and horacek, 1998) <papid> W98-1425 </papid>that require an input considerably less abstract than those required by more traditional re-alization components uch as surge (e1- hadad and robin, 1996) or kpml (bate- man, 1997).</citsent>
<aftsection>
<nextsent>in this paper, we contribute to the de-bate on selecting an appropriate vel of ab-straction by considering the case of bilin-gual generation.
</nextsent>
<nextsent>we present results ob-tained while developing the hugg syntactic realization component for hebrew (dahan- netzer, 1997).
</nextsent>
<nextsent>one of the goals of this sys-tem is to design generator with an input specification language as similar as possible to that of an english generator, surge in our case . the ideal scenario for bilingual generation is illustrated in figure 1.
</nextsent>
<nextsent>it consists of the 144 john gave book to mary john natan sefer le-mary cat proc partic :lause type relation-type agent affected possessor possessed composite \] possessive lex  john  gender masculine \[cat proper \] \[1\] lex  mary  gender feminine \[1\] cat common \] lex  book/sefer  figure 1  ideal scenario for bilingual gener-ation following steps: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X590">
<title id=" P99-1019.xml">bilingual hebrew english generation of possess ives and partitives raising the input abstraction level </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in contrast, if the input had specified structure such as indirect-object(prep=to/le, np--mary), then it would not have been abstract enough to serve as bilingual input structure.
</prevsent>
<prevsent>similarly, the english possessive marker is very close to the hebrew  construct state  (smixut): the king palace armon ha-melex palace-cs the-king the following input structure seems, therefore, appropriate for both languages: lex possessor common 1  palace  / armon  \[lexdefinite yes king / melex \] there are, however, divergences between the use of smixut in hebrew and of the pos-sessive marker in english: segovia pupil the pupil of segovia * talmyd segovyah talmyd sel segovyah ? the house windows the windows of the house haionot ha-bayit ha-halonot sel ha-bayit our goal, therefore, is to design an input structure that is abstract enough to let the grammar decide whether to use possessive marker vs. an of-construct in english or sel-construct vs. smixut-construction in hebrew.
</prevsent>
</prevsection>
<citsent citstr=" J94-4004 ">
a similar approach as been adopted in generation (bateman, 1997), (bateman et al., 1991) and in machine translation most notably in (dorr, 1994).<papid> J94-4004 </papid></citsent>
<aftsection>
<nextsent>dorr focuses on di-vergences at the clause level as illustrated by the following example: like mary maria me gusta mi mary pleases me dorr selects representation structure based on jackendoff lexical conceptual structures (lcs) (jackendoff, 1990).
</nextsent>
<nextsent>in the kpml system, the proposed so-lution is based on the systemic notion of  delicacy  and the assumption is that low- delicacy input features (the most abstract ones) remain common to the two target lan-guages and high-delicacy features would dif-fer.
</nextsent>
<nextsent>in this paper, we focus on the input spec-ification for complex nps.
</nextsent>
<nextsent>the main reason for this choice is that the input for nps in surge has remained close to english syn-tax (low abstraction).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X591">
<title id=" P99-1019.xml">bilingual hebrew english generation of possess ives and partitives raising the input abstraction level </title>
<section> possess ives and partitives in.  </section>
<citcontext>
<prevsection>
<prevsent>the choice of one of the three forms seems to be stylistic and vary in spoken and writ-ten hebrew (cf.
</prevsent>
<prevsent>(berman, 1978), (glin- eft, 1989), (ornan, 1964), and discussion in (seikevicz, 1979)).
</prevsent>
</prevsection>
<citsent citstr=" W98-1418 ">
but, in addition to these pragmatic factors and as is the case for the english genitive, the construct state can realize wide variety of semantic relations (dahan-netzer and elhadad, 1998<papid> W98-1418 </papid>b), (azar, 1985), (levi, 1976).</citsent>
<aftsection>
<nextsent>the selection is also matter of preference ranking among com-petitors for the same syntactic slot.
</nextsent>
<nextsent>forex- ample, we have shown in (dahan-netzer and elhadad, 1998<papid> W98-1418 </papid>b) that the semantic relations that can be realized by construct state are the ones defined as classifier in surge.</nextsent>
<nextsent>therefore, the co-occurrence of such rela-tion with another classifier leads to com-petition for the syntactic slot of  classifier  and also contributes to the decision of how to realize possessive.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X641">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and race?.
</prevsent>
<prevsent>the problem posed by mwes is considered hard,but at the same time it is highly relevant and interesting.
</prevsent>
</prevsection>
<citsent citstr=" N10-1089 ">
mwes occur frequently in language and interpreting them correctly would directly improve results in number of tasks in nlp such as translation and parsing (korkontzelos and manandhar, 2010).<papid> N10-1089 </papid></citsent>
<aftsection>
<nextsent>by extension this makes deciding the lexicality ofmwes an important challenge for various fields including machine translation, question answering and information retrieval.
</nextsent>
<nextsent>in this paper we discuss com positionality with respect to noun-noun compounds.
</nextsent>
<nextsent>most computational linguistics literature treats compositionality as binary problem, classifying compounds as either lexical or compositional.
</nextsent>
<nextsent>we show that this approach is too simplistic and argue for the real-valued treatment of compositionality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X642">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>interpreting mwes is difficult task as compound nouns can be freely constructed?
</prevsent>
<prevsent>(sparck jones, 1985), and are thus able to proliferate infinitely.
</prevsent>
</prevsection>
<citsent citstr=" C10-1142 ">
at the same time, semantic composition can take many different forms, making uniform interpretation of compounds impossible (zanzotto et al, 2010).<papid> C10-1142 </papid>most current work on mwes focuses on interpreting compounds and sidesteps the task of determining whether compound is compositional in the first place (butnariu et al, 2010; <papid> S10-1007 </papid>kim and baldwin,2008).</citsent>
<aftsection>
<nextsent>such methods, aimed at learning the semantics of compounds, can roughly be divided into two major strands of research.one group relies on data intensive methods to extract semantics vectors from large corpora (baroniand zamparelli, 2010; <papid> D10-1115 </papid>zanzotto et al, 2010; <papid> C10-1142 </papid>giesbrecht, 2009).</nextsent>
<nextsent>the focus of these approaches is to develop methods for composing the vectors of unigrams into semantic vector representing com pound.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X643">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>interpreting mwes is difficult task as compound nouns can be freely constructed?
</prevsent>
<prevsent>(sparck jones, 1985), and are thus able to proliferate infinitely.
</prevsent>
</prevsection>
<citsent citstr=" S10-1007 ">
at the same time, semantic composition can take many different forms, making uniform interpretation of compounds impossible (zanzotto et al, 2010).<papid> C10-1142 </papid>most current work on mwes focuses on interpreting compounds and sidesteps the task of determining whether compound is compositional in the first place (butnariu et al, 2010; <papid> S10-1007 </papid>kim and baldwin,2008).</citsent>
<aftsection>
<nextsent>such methods, aimed at learning the semantics of compounds, can roughly be divided into two major strands of research.one group relies on data intensive methods to extract semantics vectors from large corpora (baroniand zamparelli, 2010; <papid> D10-1115 </papid>zanzotto et al, 2010; <papid> C10-1142 </papid>giesbrecht, 2009).</nextsent>
<nextsent>the focus of these approaches is to develop methods for composing the vectors of unigrams into semantic vector representing com pound.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X644">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(sparck jones, 1985), and are thus able to proliferate infinitely.
</prevsent>
<prevsent>at the same time, semantic composition can take many different forms, making uniform interpretation of compounds impossible (zanzotto et al, 2010).<papid> C10-1142 </papid>most current work on mwes focuses on interpreting compounds and sidesteps the task of determining whether compound is compositional in the first place (butnariu et al, 2010; <papid> S10-1007 </papid>kim and baldwin,2008).</prevsent>
</prevsection>
<citsent citstr=" D10-1115 ">
such methods, aimed at learning the semantics of compounds, can roughly be divided into two major strands of research.one group relies on data intensive methods to extract semantics vectors from large corpora (baroniand zamparelli, 2010; <papid> D10-1115 </papid>zanzotto et al, 2010; <papid> C10-1142 </papid>giesbrecht, 2009).</citsent>
<aftsection>
<nextsent>the focus of these approaches is to develop methods for composing the vectors of unigrams into semantic vector representing compound.
</nextsent>
<nextsent>some of the work in this area touches on the issue of lexicality, as models learning distributional representations of mwes ideally would first establish whether given mwe is compositional or not (mitchell and lapata, 2010).the other group are knowledge intensive approaches collecting linguistic features (kim and baldwin, 2005; <papid> I05-1082 </papid>korkontzelos and manandhar, 2009).<papid> P09-2017 </papid></nextsent>
<nextsent>tratz and hovy (2010), <papid> P10-1070 </papid>for instance, train classifier for noun compound interpretation on large set of wordnet and thesaurus features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X646">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>such methods, aimed at learning the semantics of compounds, can roughly be divided into two major strands of research.one group relies on data intensive methods to extract semantics vectors from large corpora (baroniand zamparelli, 2010; <papid> D10-1115 </papid>zanzotto et al, 2010; <papid> C10-1142 </papid>giesbrecht, 2009).</prevsent>
<prevsent>the focus of these approaches is to develop methods for composing the vectors of unigrams into semantic vector representing com pound.</prevsent>
</prevsection>
<citsent citstr=" I05-1082 ">
some of the work in this area touches on the issue of lexicality, as models learning distributional representations of mwes ideally would first establish whether given mwe is compositional or not (mitchell and lapata, 2010).the other group are knowledge intensive approaches collecting linguistic features (kim and baldwin, 2005; <papid> I05-1082 </papid>korkontzelos and manandhar, 2009).<papid> P09-2017 </papid></citsent>
<aftsection>
<nextsent>tratz and hovy (2010), <papid> P10-1070 </papid>for instance, train classifier for noun compound interpretation on large set of wordnet and thesaurus features.</nextsent>
<nextsent>combined approaches include kim and baldwin(2008), who interpret noun compounds by extrapolating their semantics from observations where thetwo nouns forming compound are in an intr ansi tive relationship.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X647">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>such methods, aimed at learning the semantics of compounds, can roughly be divided into two major strands of research.one group relies on data intensive methods to extract semantics vectors from large corpora (baroniand zamparelli, 2010; <papid> D10-1115 </papid>zanzotto et al, 2010; <papid> C10-1142 </papid>giesbrecht, 2009).</prevsent>
<prevsent>the focus of these approaches is to develop methods for composing the vectors of unigrams into semantic vector representing com pound.</prevsent>
</prevsection>
<citsent citstr=" P09-2017 ">
some of the work in this area touches on the issue of lexicality, as models learning distributional representations of mwes ideally would first establish whether given mwe is compositional or not (mitchell and lapata, 2010).the other group are knowledge intensive approaches collecting linguistic features (kim and baldwin, 2005; <papid> I05-1082 </papid>korkontzelos and manandhar, 2009).<papid> P09-2017 </papid></citsent>
<aftsection>
<nextsent>tratz and hovy (2010), <papid> P10-1070 </papid>for instance, train classifier for noun compound interpretation on large set of wordnet and thesaurus features.</nextsent>
<nextsent>combined approaches include kim and baldwin(2008), who interpret noun compounds by extrapolating their semantics from observations where thetwo nouns forming compound are in an intr ansi tive relationship.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X648">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the focus of these approaches is to develop methods for composing the vectors of unigrams into semantic vector representing compound.
</prevsent>
<prevsent>some of the work in this area touches on the issue of lexicality, as models learning distributional representations of mwes ideally would first establish whether given mwe is compositional or not (mitchell and lapata, 2010).the other group are knowledge intensive approaches collecting linguistic features (kim and baldwin, 2005; <papid> I05-1082 </papid>korkontzelos and manandhar, 2009).<papid> P09-2017 </papid></prevsent>
</prevsection>
<citsent citstr=" P10-1070 ">
tratz and hovy (2010), <papid> P10-1070 </papid>for instance, train classifier for noun compound interpretation on large set of wordnet and thesaurus features.</citsent>
<aftsection>
<nextsent>combined approaches include kim and baldwin(2008), who interpret noun compounds by extrapolating their semantics from observations where thetwo nouns forming compound are in an intr ansi tive relationship.
</nextsent>
<nextsent>for example extracting the phrase the family owns car?
</nextsent>
<nextsent>from the training data would help learn that the compound family car?
</nextsent>
<nextsent>describes possessor-owned/possessed relationship.some of these supervised classifiers include lexi cality as classification option, considering it jointly with the actual compound interpretation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X649">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>from the training data would help learn that the compound family car?
</prevsent>
<prevsent>describes possessor-owned/possessed relationship.some of these supervised classifiers include lexi cality as classification option, considering it jointly with the actual compound interpretation.
</prevsent>
</prevsection>
<citsent citstr=" C10-1014 ">
next to the work on mwe interpretation there has been some work focused on determining lexicality in its own right (reddy et al, 2011; bu et al, 2010; <papid> C10-1014 </papid>kim and baldwin, 2007).</citsent>
<aftsection>
<nextsent>one possibility is to exploit special properties of lexical mwes such as high statistical association of their constituents (pedersen, 2011) <papid> W11-1306 </papid>or syntactic rigidity (fazly et al, 2009; <papid> J09-1005 </papid>mccarthy et al, 2007).<papid> D07-1039 </papid>however, these approaches are limited in their applicability to compound nouns (reddy et al, 2011).</nextsent>
<nextsent>another method is to compare the semantics ofa compound and its constituents to decide com positionality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X650">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>describes possessor-owned/possessed relationship.some of these supervised classifiers include lexi cality as classification option, considering it jointly with the actual compound interpretation.
</prevsent>
<prevsent>next to the work on mwe interpretation there has been some work focused on determining lexicality in its own right (reddy et al, 2011; bu et al, 2010; <papid> C10-1014 </papid>kim and baldwin, 2007).</prevsent>
</prevsection>
<citsent citstr=" W11-1306 ">
one possibility is to exploit special properties of lexical mwes such as high statistical association of their constituents (pedersen, 2011) <papid> W11-1306 </papid>or syntactic rigidity (fazly et al, 2009; <papid> J09-1005 </papid>mccarthy et al, 2007).<papid> D07-1039 </papid>however, these approaches are limited in their applicability to compound nouns (reddy et al, 2011).</citsent>
<aftsection>
<nextsent>another method is to compare the semantics ofa compound and its constituents to decide com positionality.
</nextsent>
<nextsent>the approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods.
</nextsent>
<nextsent>depending on the chosen representation of semantics these approaches can either be used for supervised classifiers or together with distance metric comparing vector space representations of semantics.
</nextsent>
<nextsent>in binary setting, threshold would then be applied to the result of that distance function (korkontzelos and manandhar, 2009).<papid> P09-2017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X651">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>describes possessor-owned/possessed relationship.some of these supervised classifiers include lexi cality as classification option, considering it jointly with the actual compound interpretation.
</prevsent>
<prevsent>next to the work on mwe interpretation there has been some work focused on determining lexicality in its own right (reddy et al, 2011; bu et al, 2010; <papid> C10-1014 </papid>kim and baldwin, 2007).</prevsent>
</prevsection>
<citsent citstr=" J09-1005 ">
one possibility is to exploit special properties of lexical mwes such as high statistical association of their constituents (pedersen, 2011) <papid> W11-1306 </papid>or syntactic rigidity (fazly et al, 2009; <papid> J09-1005 </papid>mccarthy et al, 2007).<papid> D07-1039 </papid>however, these approaches are limited in their applicability to compound nouns (reddy et al, 2011).</citsent>
<aftsection>
<nextsent>another method is to compare the semantics ofa compound and its constituents to decide com positionality.
</nextsent>
<nextsent>the approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods.
</nextsent>
<nextsent>depending on the chosen representation of semantics these approaches can either be used for supervised classifiers or together with distance metric comparing vector space representations of semantics.
</nextsent>
<nextsent>in binary setting, threshold would then be applied to the result of that distance function (korkontzelos and manandhar, 2009).<papid> P09-2017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X652">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>describes possessor-owned/possessed relationship.some of these supervised classifiers include lexi cality as classification option, considering it jointly with the actual compound interpretation.
</prevsent>
<prevsent>next to the work on mwe interpretation there has been some work focused on determining lexicality in its own right (reddy et al, 2011; bu et al, 2010; <papid> C10-1014 </papid>kim and baldwin, 2007).</prevsent>
</prevsection>
<citsent citstr=" D07-1039 ">
one possibility is to exploit special properties of lexical mwes such as high statistical association of their constituents (pedersen, 2011) <papid> W11-1306 </papid>or syntactic rigidity (fazly et al, 2009; <papid> J09-1005 </papid>mccarthy et al, 2007).<papid> D07-1039 </papid>however, these approaches are limited in their applicability to compound nouns (reddy et al, 2011).</citsent>
<aftsection>
<nextsent>another method is to compare the semantics ofa compound and its constituents to decide com positionality.
</nextsent>
<nextsent>the approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods.
</nextsent>
<nextsent>depending on the chosen representation of semantics these approaches can either be used for supervised classifiers or together with distance metric comparing vector space representations of semantics.
</nextsent>
<nextsent>in binary setting, threshold would then be applied to the result of that distance function (korkontzelos and manandhar, 2009).<papid> P09-2017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X655">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>in real-valued setting the distance metric itself can be used as measure for compositionality (reddy et al, 2011).
</prevsent>
<prevsent>related to the vector space based models, some research focuses on improving the distance metrics used to compare induced semantics (bu et al, 2010).<papid> C10-1014 </papid></prevsent>
</prevsection>
<citsent citstr=" P95-1007 ">
english noun-noun compounds are majority left branching (lauer, 1995), <papid> P95-1007 </papid>with head (the secondelement), modified by an attributive noun (first el ement).</citsent>
<aftsection>
<nextsent>for example:ground floor ? the floor of building at or nearest ground level.2 in this paper, we will use the terms attributive noun (an) and head noun (hn) to refer to the first and second noun in noun compound.
</nextsent>
<nextsent>3.1 real-valued representation.
</nextsent>
<nextsent>lexicality of mwes is frequently treated as binary property (tratz and hovy, 2010; <papid> P10-1070 </papid>o?</nextsent>
<nextsent>seaghdha, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X658">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>derives large part of its semantics from dress?, but the compound also contributes an idiosyncratic element to its meaning.
</prevsent>
<prevsent>2definition from http://www.thefreedictionary.com 133we define lexicality as the degree to which idiosyncrasy contributes to compounds semantics.inversely phrased, the compositionality of compound can be defined as the degree to which its sense is related to the senses of its constituents.3 this graded representation follows sparck jones(1985), who argued that it is not possible to maintain principled distinction between lexicalised and non-lexicalised compounds?.
</prevsent>
</prevsection>
<citsent citstr=" W06-1201 ">
some recent work also supports this view (reddy et al, 2011; bu etal., 2010; <papid> C10-1014 </papid>baldwin, 2006).<papid> W06-1201 </papid></citsent>
<aftsection>
<nextsent>from practical perspective, real-valued representation of compositionality should help improve interpretation of compounds.
</nextsent>
<nextsent>this is especially true when factoring in the respective semantic contributions of its parts.
</nextsent>
<nextsent>3.2 context generation.
</nextsent>
<nextsent>according to the distributional hypothesis, these mantics of lexical item can be expressed by its context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X663">
<title id=" S12-1021.xml">an unsupervised ranking model for noun noun compositionality </title>
<section> generative models.  </section>
<citcontext>
<prevsection>
<prevsent>it would be possible to use an even larger training corpus, but there are limitations as to what extent this is possible.
</prevsent>
<prevsent>the bnc, containing 100 million words, is already one of the largest corpora regularly used in computational linguistics.
</prevsent>
</prevsection>
<citsent citstr=" D07-1090 ">
however,adding more data in an unsupervised sense is unlikely to significantly improve results (brants et al, 2007).<papid> D07-1090 </papid>alternatively, it would be possible to add specific training data that included the noun compounds from the evaluation data sets.</citsent>
<aftsection>
<nextsent>this would, however, compromise the unsupervised nature of our approach, and it thus not an option either.in this paper, we will instead focus on extenuating the effects of data sparsity through other unsupervised means.
</nextsent>
<nextsent>for this purpose we investigate in terpolating on larger set of noun compounds.kim and baldwin (2007) observed that semantic similarity of verb-particle compounds correlates with their lexicality.
</nextsent>
<nextsent>we extend this observation for noun compounds, hypothesising that the lexicality of similar words will be similar.
</nextsent>
<nextsent>we combine this with the assumption that noun compounds sharing aconstituent are likely to be semantically similar (ko rkontzelos and manandhar, 2009).<papid> P09-2017 </papid>using this idea, we can approximate the lexical ity of given compound with the lexicality scores of all compounds sharing either of its constituents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X665">
<title id=" S12-1005.xml">sentence clustering via projection over term clusters </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>commonly, vector similarity measures, such as cosine, are used to define the level of similarity over bag-of-words encoding of the sentences.
</prevsent>
<prevsent>then, standard clustering algorithms can be applied to group sentences into clusters (see steinbach et al  (2000) for an overview).
</prevsent>
</prevsection>
<citsent citstr=" P09-2075 ">
the most common practice is representing the sentences as vectors in term space and applying the k-means clustering algorithm (shen et al  (2011); pasquier (2010); wang et al  (2009); <papid> P09-2075 </papid>nomoto and matsumoto (2001); boros et al  (2001)).</citsent>
<aftsection>
<nextsent>an alternative approach involves partitioning sentence connectivity graph by means of graph clustering algorithm (erkan and radev (2004); zha (2002)).
</nextsent>
<nextsent>the main challenge for any sentence clustering approach is language variability, where the same meaning can be phrased in various ways.
</nextsent>
<nextsent>the shorter the sentences are, the less effective becomes exact matching of their terms.
</nextsent>
<nextsent>compare the following newspaper sentence the bank is phasing out the ez checking package, with no monthly fee charged for balances over $1,500, and is instead offering customers its basic banking account, which carries fee?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X666">
<title id=" S12-1005.xml">sentence clustering via projection over term clusters </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>external resources can be utilized to provide such kind of knowledge, by which sentence representation can be enriched.
</prevsent>
<prevsent>traditionally, wordnet (fellbaum, 1998) has been used for this purpose (she hata (2009); chen et al  (2003); hotho et al  (2003); hatzivassiloglou et al  (2001)).
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
yet, other resources 38 of semantically-related terms can be beneficial, suchas wordnet::similarity (pedersen et al , 2004), <papid> N04-3012 </papid>statistical resources like that of lin (1998) <papid> P98-2127 </papid>or direct (kotlerman et al , 2010), thesauri, wikipedia (hu et al ., 2009), ontologies (suchanek et al , 2007) etc.</citsent>
<aftsection>
<nextsent>this section presents generic sentence clustering scheme, which involves two consecutive steps: (1)generating relevant term clusters based on lexical semantic relatedness and (2) projecting the sentence set over these term clusters.
</nextsent>
<nextsent>below we describe each of the two steps.
</nextsent>
<nextsent>3.1 step 1: obtaining term clusters.
</nextsent>
<nextsent>in order to obtain term clusters, term connectivity graph is constructed for the given sentence set and is clustered as follows: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X667">
<title id=" S12-1005.xml">sentence clustering via projection over term clusters </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>external resources can be utilized to provide such kind of knowledge, by which sentence representation can be enriched.
</prevsent>
<prevsent>traditionally, wordnet (fellbaum, 1998) has been used for this purpose (she hata (2009); chen et al  (2003); hotho et al  (2003); hatzivassiloglou et al  (2001)).
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
yet, other resources 38 of semantically-related terms can be beneficial, suchas wordnet::similarity (pedersen et al , 2004), <papid> N04-3012 </papid>statistical resources like that of lin (1998) <papid> P98-2127 </papid>or direct (kotlerman et al , 2010), thesauri, wikipedia (hu et al ., 2009), ontologies (suchanek et al , 2007) etc.</citsent>
<aftsection>
<nextsent>this section presents generic sentence clustering scheme, which involves two consecutive steps: (1)generating relevant term clusters based on lexical semantic relatedness and (2) projecting the sentence set over these term clusters.
</nextsent>
<nextsent>below we describe each of the two steps.
</nextsent>
<nextsent>3.1 step 1: obtaining term clusters.
</nextsent>
<nextsent>in order to obtain term clusters, term connectivity graph is constructed for the given sentence set and is clustered as follows: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X668">
<title id=" S12-1005.xml">sentence clustering via projection over term clusters </title>
<section> sentence clustering via term clusters.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 step 2: projecting sentences to term.
</prevsent>
<prevsent>clusters to obtain sentence clusters, the given sentence set has to be projected in some manner over the term clusters obtained in step 1.
</prevsent>
</prevsection>
<citsent citstr=" H05-1017 ">
our projection procedure resembles unsupervised text categorization (gliozzo et al , 2005), <papid> H05-1017 </papid>with categories represented by term clusters that are not predefined but rather emerge from the analyzed data: 1.</citsent>
<aftsection>
<nextsent>represent term clusters and sentences as vec-.
</nextsent>
<nextsent>tors in term space and calculate the similarity of each sentence with each of the term clusters.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>assign each sentence to the best-scoring term.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X669">
<title id=" S12-1005.xml">sentence clustering via projection over term clusters </title>
<section> application: clustering customer.  </section>
<citcontext>
<prevsection>
<prevsent>candidate terms that did not appear in the accompanying domain corpus were filtered out as described in section 3.1.
</prevsent>
<prevsent>edges in the term graph were weighted with the number of resources supporting the corresponding edge.
</prevsent>
</prevsection>
<citsent citstr=" W06-3812 ">
to cluster the graph we used the chinese whispers clustering tool3 (biemann, 2006), <papid> W06-3812 </papid>whose algorithm does not require to pre-set the desired number of clusters and is reported to outperform other algorithms for several nlp tasks.to generate the projection, sentences were represented as vectors of terms weighted by their frequency in each sentence.</citsent>
<aftsection>
<nextsent>terms of the term-cluster vectors were weighted by the number of sentence sin which they occur.
</nextsent>
<nextsent>similarity scores were calculated using the cosine measure.
</nextsent>
<nextsent>clusters were labeled with the top terms appearing both in theun derlying term cluster and in the clusters sentences.
</nextsent>
<nextsent>in this section we present the results of evaluating our projection approach, compared to the common k-means clustering method4 applied to:(a) standard bag-of-words representation of sen tences; 2available for download at www.cs.biu.ac.il/ nlp/downloads/direct.html.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X670">
<title id=" S12-1106.xml">buap lexical and semantic similarity for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first approach uses textual similarity on the translated and original versions of the texts, whereas the second approach expands the terms by means of synonyms.
</prevsent>
<prevsent>the evaluation of both approaches show similar behavior which is still close to the average and median.
</prevsent>
</prevsection>
<citsent citstr=" N10-1045 ">
cross-lingual textual entailment (clte) has been recently proposed by (mehdad et al, 2010; <papid> N10-1045 </papid>mehdadet al, 2011) <papid> P11-1134 </papid>as an extension of the textual entailment task (dagan and glickman, 2004).</citsent>
<aftsection>
<nextsent>given text (t ) and an hypothesis (h) in different languages,the clte task consists of determining if the meaning of can be inferred from the meaning of . in this paper we present report of the obtained results after submitting two different runs for thetask 8 of semeval 2012, named cross-lingual textual entailment for content synchronization?
</nextsent>
<nextsent>(negriet al, 2012).<papid> S12-1053 </papid></nextsent>
<nextsent>in this task, the cross-lingual textual entailment addresses textual entailment recognition under new dimension (cross-linguality), and within new challenging application scenario (con tent synchronization).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X671">
<title id=" S12-1106.xml">buap lexical and semantic similarity for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first approach uses textual similarity on the translated and original versions of the texts, whereas the second approach expands the terms by means of synonyms.
</prevsent>
<prevsent>the evaluation of both approaches show similar behavior which is still close to the average and median.
</prevsent>
</prevsection>
<citsent citstr=" P11-1134 ">
cross-lingual textual entailment (clte) has been recently proposed by (mehdad et al, 2010; <papid> N10-1045 </papid>mehdadet al, 2011) <papid> P11-1134 </papid>as an extension of the textual entailment task (dagan and glickman, 2004).</citsent>
<aftsection>
<nextsent>given text (t ) and an hypothesis (h) in different languages,the clte task consists of determining if the meaning of can be inferred from the meaning of . in this paper we present report of the obtained results after submitting two different runs for thetask 8 of semeval 2012, named cross-lingual textual entailment for content synchronization?
</nextsent>
<nextsent>(negriet al, 2012).<papid> S12-1053 </papid></nextsent>
<nextsent>in this task, the cross-lingual textual entailment addresses textual entailment recognition under new dimension (cross-linguality), and within new challenging application scenario (con tent synchronization).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X672">
<title id=" S12-1106.xml">buap lexical and semantic similarity for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cross-lingual textual entailment (clte) has been recently proposed by (mehdad et al, 2010; <papid> N10-1045 </papid>mehdadet al, 2011) <papid> P11-1134 </papid>as an extension of the textual entailment task (dagan and glickman, 2004).</prevsent>
<prevsent>given text (t ) and an hypothesis (h) in different languages,the clte task consists of determining if the meaning of can be inferred from the meaning of . in this paper we present report of the obtained results after submitting two different runs for thetask 8 of semeval 2012, named cross-lingual textual entailment for content synchronization?</prevsent>
</prevsection>
<citsent citstr=" S12-1053 ">
(negriet al, 2012).<papid> S12-1053 </papid></citsent>
<aftsection>
<nextsent>in this task, the cross-lingual textual entailment addresses textual entailment recognition under new dimension (cross-linguality), and within new challenging application scenario (con tent synchronization).
</nextsent>
<nextsent>the task 8 of semeval 2012 may be formally defined as follows: given pair of topically related text fragments (t1 and t2) in different languages, the task consists of automatically annotating it with one of the following entailment judgments: ? bidirectional (t1 ? t2 &amp; t1 ? t2): the two fragments entail each other (semantic equiva lence)?
</nextsent>
<nextsent>forward (t1 ? t2 &amp; t1 ! ?
</nextsent>
<nextsent>t2): unidirectional entailment from t1 to t2?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X673">
<title id=" S12-1106.xml">buap lexical and semantic similarity for cross lingual textual entailment </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>we have established the threshold of 0.5 for the similarity function as evidence of textual entailment.
</prevsent>
<prevsent>since the two sentences to be evaluated are written in two different languages, we have translated each sentence to the other language, so that, we have two sentences in english, and two sentences in the original language (spanish, german, italian and french).
</prevsent>
</prevsection>
<citsent citstr=" D11-1062 ">
we have used the google translate for this purpose 1 . the corpora used in the experiments comes from cross-lingual textual entailment dataset presentedin (negri et al, 2011), <papid> D11-1062 </papid>and provided by the task orga nizers.</citsent>
<aftsection>
<nextsent>we have employed the training dataset only for adjust some parameters of the system, but the approach is knowledge-based and, therefore, it does not need training corpus.
</nextsent>
<nextsent>both, the training and test corpus contain 500 sentences for each language.the textual length is used to determine the entailment judgment (bidirectional, forward, backward, no entailment).
</nextsent>
<nextsent>we have basically, assumed that the length of text may give some evidence of the typeof entailment.
</nextsent>
<nextsent>the decision rules used for determining the entailment judgment are described in section 2.3.in this competition we have submitted two different runs which differ with respect to the type of textual similarity used (lexical vs semantic).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X674">
<title id=" S12-1046.xml">semeval2012 task 1 english lexical simplification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexical simplification is subtask of text simplification (siddharthan, 2006) concerned with replacing words or short phrases by simpler variants in context aware fashion (generally synonyms), which can be understood by wider range of readers.
</prevsent>
<prevsent>it generally envisages certain human target audience that may find it difficult or impossible to understand complex words or phrases, e.g., children, people with poor literacy levels or cognitive disabilities, or second language learners.
</prevsent>
</prevsection>
<citsent citstr=" W07-2009 ">
it is similar in many respects to the task of lexical substitution (mccarthy and navigli, 2007) <papid> W07-2009 </papid>in that it involves determining adequate substitutes in context, but in this case on the basis of predefined criterion: simplicity.</citsent>
<aftsection>
<nextsent>a common pipeline for lexical simplification system includes at least three major components: (i) complexity analysis: selection of words or phrases in text that are considered complex for the reader and/or task at hand; (ii) substitute lookup: search for adequate replacement words or phrases deemed complex in context, e.g., taking synonyms (with the same sense) from thesaurus or finding similarwords/phrases in corpus using distributional similarity metrics; and (iii) context-based ranking: ranking of substitutes according to how simple they are to the reader/task at hand.as an example take the sentence: hitler committed terrible atrocities during the second world war.?
</nextsent>
<nextsent>the system would first identify complex words, e.g. atrocities, then search for substitutes that might adequately replace it.
</nextsent>
<nextsent>a thesaurus lookup would yield the following synonyms: abomination, cruelty, enormity and violation, but enormity should be dropped as it does not fit the context appropriately.
</nextsent>
<nextsent>finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence:hitler committed terrible cruelties during the second world war.?.different from other subtasks of text simplification like syntactic simplification, which have been relatively well studied, lexical simplification has received less attention.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X675">
<title id=" S12-1046.xml">semeval2012 task 1 english lexical simplification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a thesaurus lookup would yield the following synonyms: abomination, cruelty, enormity and violation, but enormity should be dropped as it does not fit the context appropriately.
</prevsent>
<prevsent>finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence:hitler committed terrible cruelties during the second world war.?.different from other subtasks of text simplification like syntactic simplification, which have been relatively well studied, lexical simplification has received less attention.
</prevsent>
</prevsection>
<citsent citstr=" N10-1056 ">
although few recent attempts explicitly address dependency on context (de belder et al, 2010; yatskar et al, 2010; <papid> N10-1056 </papid>biran et al,2011; <papid> P11-2087 </papid>specia, 2010), most approaches are context independent (candido et al, 2009; <papid> W09-2105 </papid>devlin and tait, 1998).</citsent>
<aftsection>
<nextsent>in addition, general deeper understanding 347 of the problem is yet to be gained.
</nextsent>
<nextsent>as first attempt to address this problem in the shape of shared task,the english simplification task at semeval-2012 focuses on the third component, which we believe is the core of the lexical simplification problem.the semeval-2012 shared task on english lexical simplification has been conceived with the following main purposes: advancing the state-of-theart lexical simplification approaches, and providing common framework for evaluation of lexical simplification systems for participants and other researchers interested in the field.
</nextsent>
<nextsent>another central motive of such shared task is to bring awareness to the general vagueness associated with the notion of lexical simplicity.
</nextsent>
<nextsent>our hypothesis is that in addition to the notion of target application/reader, the notion of simplicity is highly context-dependent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X676">
<title id=" S12-1046.xml">semeval2012 task 1 english lexical simplification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a thesaurus lookup would yield the following synonyms: abomination, cruelty, enormity and violation, but enormity should be dropped as it does not fit the context appropriately.
</prevsent>
<prevsent>finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence:hitler committed terrible cruelties during the second world war.?.different from other subtasks of text simplification like syntactic simplification, which have been relatively well studied, lexical simplification has received less attention.
</prevsent>
</prevsection>
<citsent citstr=" P11-2087 ">
although few recent attempts explicitly address dependency on context (de belder et al, 2010; yatskar et al, 2010; <papid> N10-1056 </papid>biran et al,2011; <papid> P11-2087 </papid>specia, 2010), most approaches are context independent (candido et al, 2009; <papid> W09-2105 </papid>devlin and tait, 1998).</citsent>
<aftsection>
<nextsent>in addition, general deeper understanding 347 of the problem is yet to be gained.
</nextsent>
<nextsent>as first attempt to address this problem in the shape of shared task,the english simplification task at semeval-2012 focuses on the third component, which we believe is the core of the lexical simplification problem.the semeval-2012 shared task on english lexical simplification has been conceived with the following main purposes: advancing the state-of-theart lexical simplification approaches, and providing common framework for evaluation of lexical simplification systems for participants and other researchers interested in the field.
</nextsent>
<nextsent>another central motive of such shared task is to bring awareness to the general vagueness associated with the notion of lexical simplicity.
</nextsent>
<nextsent>our hypothesis is that in addition to the notion of target application/reader, the notion of simplicity is highly context-dependent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X677">
<title id=" S12-1046.xml">semeval2012 task 1 english lexical simplification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a thesaurus lookup would yield the following synonyms: abomination, cruelty, enormity and violation, but enormity should be dropped as it does not fit the context appropriately.
</prevsent>
<prevsent>finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence:hitler committed terrible cruelties during the second world war.?.different from other subtasks of text simplification like syntactic simplification, which have been relatively well studied, lexical simplification has received less attention.
</prevsent>
</prevsection>
<citsent citstr=" W09-2105 ">
although few recent attempts explicitly address dependency on context (de belder et al, 2010; yatskar et al, 2010; <papid> N10-1056 </papid>biran et al,2011; <papid> P11-2087 </papid>specia, 2010), most approaches are context independent (candido et al, 2009; <papid> W09-2105 </papid>devlin and tait, 1998).</citsent>
<aftsection>
<nextsent>in addition, general deeper understanding 347 of the problem is yet to be gained.
</nextsent>
<nextsent>as first attempt to address this problem in the shape of shared task,the english simplification task at semeval-2012 focuses on the third component, which we believe is the core of the lexical simplification problem.the semeval-2012 shared task on english lexical simplification has been conceived with the following main purposes: advancing the state-of-theart lexical simplification approaches, and providing common framework for evaluation of lexical simplification systems for participants and other researchers interested in the field.
</nextsent>
<nextsent>another central motive of such shared task is to bring awareness to the general vagueness associated with the notion of lexical simplicity.
</nextsent>
<nextsent>our hypothesis is that in addition to the notion of target application/reader, the notion of simplicity is highly context-dependent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X679">
<title id=" S12-1046.xml">semeval2012 task 1 english lexical simplification </title>
<section> results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>0.397 0.471 table 2: baseline kappa scores on trial and test sets
</prevsent>
<prevsent>6.1 participants.
</prevsent>
</prevsection>
<citsent citstr=" S12-1068 ">
five sites submitted one or more systems to the task, totaling nine systems: annlor-lmbing: this system (ligozat et al,2012) <papid> S12-1068 </papid>relies on language models probabilities, and builds on the principle of the simple frequency baseline.</citsent>
<aftsection>
<nextsent>while the baseline uses google n-grams to rank substitutes, this approach uses microsoft web n-grams in the same way.
</nextsent>
<nextsent>additionally characteristics, such as the contexts of each term to be substituted, were integrated into the system.
</nextsent>
<nextsent>microsoft webn-gram service was used to obtain log likelihood probabilities for text units, composed of the lexical item and 4 words to the left and right from the surrounding context.
</nextsent>
<nextsent>annlor-simple: the system (ligozat et al, 2012) <papid> S12-1068 </papid>is based on simple english wikipedia frequencies, with the motivation that the language used in this version of wikipedia is targeted towards people who are not first language english speakers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X681">
<title id=" S12-1046.xml">semeval2012 task 1 english lexical simplification </title>
<section> results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>sentences and acorpus of normal sentences, syntactic complexity of documents that are similar to the given context, candidate length, and letter-wise recognizability of candidate as measured by trigram lm.
</prevsent>
<prevsent>the first feature sets for co-training combines the syntactic complexity, character trigram lm and basic word length features, resulting in 29 features against the remaining 21.
</prevsent>
</prevsection>
<citsent citstr=" S12-1067 ">
emnlpcph-ord2: this is variant of theemnlpcph-ord1 system where the first feature set pools all syntactic complexity features and wikipedia-based features (28 fea tures) against all the remaining 22 features in the second group.sb-mmsystem: the approach (amoia and romanelli, 2012) <papid> S12-1067 </papid>builds on the baseline definition of simplicity using word frequencies but attempt at defining more linguistically motivated notion of simplicity based on lexical semantics considerations.</citsent>
<aftsection>
<nextsent>it adopts different strategies depending on the syntactic complexity of the substitute.
</nextsent>
<nextsent>for one-word substitutes or common collocations, the system uses its frequency from wordnet as metric.
</nextsent>
<nextsent>in the case of multi-words substitutes the system uses relevance?
</nextsent>
<nextsent>rules that apply (de)compositional semantic criteria and attempts to identify unique content word in the substitute that might better approximate the whole expression.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X682">
<title id=" S12-1046.xml">semeval2012 task 1 english lexical simplification </title>
<section> results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the expression is then assigned the frequency associated to this content word for the ranking.
</prevsent>
<prevsent>after pos tagging and sense disambiguating all substitutes, hand-written rules are used to decompose the meaning of complex phrase and identify the most relevant word conveying the semantics of the whole.
</prevsent>
</prevsection>
<citsent citstr=" S12-1069 ">
unt-simprank: the system (sinha, 2012) <papid> S12-1069 </papid>uses external resources, including the simple english wikipedia corpus, set of spoken english dialogues, transcribed into machine read able form, wordnet, and unigram frequencies (google web1t data).</citsent>
<aftsection>
<nextsent>sim prank scores each substitute by sum of its unigram frequency, its 352 frequency in the simple english wikipedia, its frequency in the spoken corpus, the inverse ofits length, and the number of senses the substitute has in wordnet.
</nextsent>
<nextsent>forgiven context, the substitutes are then reverse-ranked based on their simplicity scores.unt-simpranklight: this is variant of sim prank which does not use unigram frequencies.
</nextsent>
<nextsent>the goal of this system is to check whether memory and time-intensive and non free resource such as the web1t corpus makesa difference over other free and lightweight resources.
</nextsent>
<nextsent>unt-salsa: the only resource salsa depends on is the web1t data, and in particular only3-grams from this corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X683">
<title id=" S12-1046.xml">semeval2012 task 1 english lexical simplification </title>
<section> results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>it leverages the context provided with the dataset by replacing the target place holder one by one with each of the substitutes and their inflections thus building sets of 3-grams for each substitute in given instance.
</prevsent>
<prevsent>the score of any substitute is then thesum of the 3-gram frequencies of all the generated 3-grams for that substitute.
</prevsent>
</prevsection>
<citsent citstr=" S12-1066 ">
uow-shef-simplex: the system (jauhar and specia, 2012) <papid> S12-1066 </papid>uses linear weighted ranking function composed of three features to produce ranking.</citsent>
<aftsection>
<nextsent>these include context sensitive n-gram frequency model, bag-of-words model and feature composed of simplicity oriented psycho linguistic features.
</nextsent>
<nextsent>these three features are combined using an svm ranker that is trained and tuned on the trial dataset.
</nextsent>
<nextsent>6.2 pairwise kappa.
</nextsent>
<nextsent>the official task results and the ranking of the systems are shown in table 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X684">
<title id=" S10-1011.xml">semeval2010 task 14 word sense induction x26disambiguation </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>treating the testing data asnew unseen instances ensures realistic evaluation that allows to evaluate the clustering models of each participating system.
</prevsent>
<prevsent>the evaluation framework of semeval-2010 wsi task considered two types of evaluation.in the first one, unsupervised evaluation, systems?
</prevsent>
</prevsection>
<citsent citstr=" D07-1043 ">
answers were evaluated according to: (1) measure (rosenberg and hirschberg, 2007)<papid> D07-1043 </papid>and(2) paired f-score (artiles et al, 2009).<papid> D09-1056 </papid></citsent>
<aftsection>
<nextsent>neither of these measures were used in the semeval 2007 wsi task.
</nextsent>
<nextsent>manandhar &amp; klapaftis (2009).<papid> W09-2419 </papid></nextsent>
<nextsent>provide more details on the choice of this evaluation setting and its differences with the previousevaluation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X686">
<title id=" S10-1011.xml">semeval2010 task 14 word sense induction x26disambiguation </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>treating the testing data asnew unseen instances ensures realistic evaluation that allows to evaluate the clustering models of each participating system.
</prevsent>
<prevsent>the evaluation framework of semeval-2010 wsi task considered two types of evaluation.in the first one, unsupervised evaluation, systems?
</prevsent>
</prevsection>
<citsent citstr=" D09-1056 ">
answers were evaluated according to: (1) measure (rosenberg and hirschberg, 2007)<papid> D07-1043 </papid>and(2) paired f-score (artiles et al, 2009).<papid> D09-1056 </papid></citsent>
<aftsection>
<nextsent>neither of these measures were used in the semeval 2007 wsi task.
</nextsent>
<nextsent>manandhar &amp; klapaftis (2009).<papid> W09-2419 </papid></nextsent>
<nextsent>provide more details on the choice of this evaluation setting and its differences with the previousevaluation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X688">
<title id=" S10-1011.xml">semeval2010 task 14 word sense induction x26disambiguation </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>answers were evaluated according to: (1) measure (rosenberg and hirschberg, 2007)<papid> D07-1043 </papid>and(2) paired f-score (artiles et al, 2009).<papid> D09-1056 </papid></prevsent>
<prevsent>neither of these measures were used in the semeval 2007 wsi task.</prevsent>
</prevsection>
<citsent citstr=" W09-2419 ">
manandhar &amp; klapaftis (2009).<papid> W09-2419 </papid></citsent>
<aftsection>
<nextsent>provide more details on the choice of this evaluation setting and its differences with the previousevaluation.
</nextsent>
<nextsent>the second type of evaluation, supervised evaluation, follows the supervised evaluation of the semeval-2007 wsi task (agirre and soroa, 2007).<papid> W07-2002 </papid></nextsent>
<nextsent>in this evaluation, induced senses are mapped to gold standard senses using mapping corpus, and systems are then evaluated in standard wsd task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X689">
<title id=" S10-1011.xml">semeval2010 task 14 word sense induction x26disambiguation </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>manandhar &amp; klapaftis (2009).<papid> W09-2419 </papid></prevsent>
<prevsent>provide more details on the choice of this evaluation setting and its differences with the previousevaluation.</prevsent>
</prevsection>
<citsent citstr=" W07-2002 ">
the second type of evaluation, supervised evaluation, follows the supervised evaluation of the semeval-2007 wsi task (agirre and soroa, 2007).<papid> W07-2002 </papid></citsent>
<aftsection>
<nextsent>in this evaluation, induced senses are mapped to gold standard senses using mapping corpus, and systems are then evaluated in standard wsd task.
</nextsent>
<nextsent>2.1 training dataset.
</nextsent>
<nextsent>the target word dataset consisted of 100 words, i.e. 50 nouns and 50 verbs.
</nextsent>
<nextsent>the training dataset for each target noun or verb was created by following web-based semi-automatic method, similar to the method for the construction of topic signatures (agirre et al, 2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X690">
<title id=" S10-1011.xml">semeval2010 task 14 word sense induction x26disambiguation </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>search api 3 and for each query maximum of 1000 pages were downloaded.
</prevsent>
<prevsent>for each page we extracted fragments of text that occurred in    /p  html tags and contained the target word stem.
</prevsent>
</prevsection>
<citsent citstr=" H05-1059 ">
in the final stage, each extracted fragment oftext was pos-tagged using the genia tagger (tsu ruoka and tsujii, 2005) <papid> H05-1059 </papid>and was only retained, if the pos of the target word in the extracted text matched the pos of the target word in our dataset.</citsent>
<aftsection>
<nextsent>2.2 testing dataset.
</nextsent>
<nextsent>the testing dataset consisted of instances of the same target words from the training dataset.
</nextsent>
<nextsent>this dataset is part of ontonotes (hovy et al, 2006).<papid> N06-2015 </papid>we used the sense-tagged dataset in which sentences containing target word instances are tagged with ontonotes (hovy et al, 2006) <papid> N06-2015 </papid>senses.</nextsent>
<nextsent>the texts come from various news sources including cnn, abc and others.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X691">
<title id=" S10-1011.xml">semeval2010 task 14 word sense induction x26disambiguation </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 testing dataset.
</prevsent>
<prevsent>the testing dataset consisted of instances of the same target words from the training dataset.
</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
this dataset is part of ontonotes (hovy et al, 2006).<papid> N06-2015 </papid>we used the sense-tagged dataset in which sentences containing target word instances are tagged with ontonotes (hovy et al, 2006) <papid> N06-2015 </papid>senses.</citsent>
<aftsection>
<nextsent>the texts come from various news sources including cnn, abc and others.
</nextsent>
<nextsent>1 an act that fails 2 an event that does not accomplish its intended purpose 3 http://developer.yahoo.com/search/ [access:10/04/2010] 64 g1 2 3 1 10 10 15 2 20 50 0 3 1 10 60 4 5 0 0 table 3: clusters &amp; gs senses matrix.
</nextsent>
<nextsent>for the purposes of this section we provide an example (table 3) in which target word has 181instances and 3 gs senses.
</nextsent>
<nextsent>a system has generated clustering solution with 4 clusters covering all instances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X700">
<title id=" P99-1047.xml">a decision based approach to rhetorical parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present shift-reduce rhetorical parsing algo-rithm that learns to construct rhetorical structures of texts from corpus of discourse-parse action se-quences.
</prevsent>
<prevsent>the algorithm exploits robust lexical, syn-tactic, and semantic knowledge sources.
</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
the application of decision-based learning tech-niques over rich sets of linguistic features has improved significantly the coverage and perfor-mance of syntactic (and to various degrees eman- tic) parsers (simmons and yu, 1992; magerman, 1995; <papid> P95-1037 </papid>hermjakob and mooney, 1997).<papid> P97-1062 </papid></citsent>
<aftsection>
<nextsent>in this pa-per, we apply similar paradigm to developing rhetorical parser that derives the discourse structure of unrestricted texts.
</nextsent>
<nextsent>crucial to our approach is the reliance on cor-pus of 90 texts which were manually annotated with discourse trees and the adoption of shift-reduce parsing model that is well-suited for learning.
</nextsent>
<nextsent>both the corpus and the parsing model are used to gener-ate learning cases of how texts should be partitioned into elementary discourse units and how discourse units and segments hould be assembled into dis-course trees.
</nextsent>
<nextsent>we used corpus of 90 rhetorical structure trees, which were built manually using rhetorical rela-tions that were defined informally in the style of mann and thompson (1988): 30 trees were built for short personal news stories from the muc7 co-reference corpus (hirschman and chinchor, 1997); 30 trees for scientific texts from the brown corpus; and 30 trees for editorials from the wall street jour-nal (wsj).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X701">
<title id=" P99-1047.xml">a decision based approach to rhetorical parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present shift-reduce rhetorical parsing algo-rithm that learns to construct rhetorical structures of texts from corpus of discourse-parse action se-quences.
</prevsent>
<prevsent>the algorithm exploits robust lexical, syn-tactic, and semantic knowledge sources.
</prevsent>
</prevsection>
<citsent citstr=" P97-1062 ">
the application of decision-based learning tech-niques over rich sets of linguistic features has improved significantly the coverage and perfor-mance of syntactic (and to various degrees eman- tic) parsers (simmons and yu, 1992; magerman, 1995; <papid> P95-1037 </papid>hermjakob and mooney, 1997).<papid> P97-1062 </papid></citsent>
<aftsection>
<nextsent>in this pa-per, we apply similar paradigm to developing rhetorical parser that derives the discourse structure of unrestricted texts.
</nextsent>
<nextsent>crucial to our approach is the reliance on cor-pus of 90 texts which were manually annotated with discourse trees and the adoption of shift-reduce parsing model that is well-suited for learning.
</nextsent>
<nextsent>both the corpus and the parsing model are used to gener-ate learning cases of how texts should be partitioned into elementary discourse units and how discourse units and segments hould be assembled into dis-course trees.
</nextsent>
<nextsent>we used corpus of 90 rhetorical structure trees, which were built manually using rhetorical rela-tions that were defined informally in the style of mann and thompson (1988): 30 trees were built for short personal news stories from the muc7 co-reference corpus (hirschman and chinchor, 1997); 30 trees for scientific texts from the brown corpus; and 30 trees for editorials from the wall street jour-nal (wsj).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X702">
<title id=" P99-1047.xml">a decision based approach to rhetorical parsing </title>
<section> the corpus.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, we also marked two constituency relations that were ubiquitous in our corpora and that often subsumed complex rhetorical constituents.
</prevsent>
<prevsent>these relations were attribution, which was used to la-bel the relation between reporting and reported clause, and apposition.
</prevsent>
</prevsection>
<citsent citstr=" W99-0307 ">
marcu et al (1999) <papid> W99-0307 </papid>discuss in detail the annotation tool and protocol and assess the inter-judge agreement and the reliability of the annotation.</citsent>
<aftsection>
<nextsent>we model the discourse parsing process as se-quence of shift-reduce operations.
</nextsent>
<nextsent>as front-end, the parser uses discourse segmenter, i.e., an algorithm that partitions the input text into edus.
</nextsent>
<nextsent>the dis-course segmenter, which is also decision-based, is presented and evaluated in section 4.
</nextsent>
<nextsent>the input to the parser is an empty stack and an input list that contains asequence of elementary dis-course trees, edts, one edt for each edu produced by the discourse segmenter.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X703">
<title id=" P99-1047.xml">a decision based approach to rhetorical parsing </title>
<section> the discourse segmenter.  </section>
<citcontext>
<prevsection>
<prevsent>the local context consists of window of size 5 that enumerates the part-of-speech (pos) tags of the lexeme under scrutiny and the two lexemes found immediately before and after it.
</prevsent>
<prevsent>the pos tags are determined automatically, using the brill tagger (1995).
</prevsent>
</prevsection>
<citsent citstr=" P97-1013 ">
since discourse markers, such as because and and, have been shown to play ma-jor role in rhetorical parsing (marcu, 1997), <papid> P97-1013 </papid>we also consider list of features that specify whether lex- eme found within the local contextual window is potential discourse marker.</citsent>
<aftsection>
<nextsent>the local context also contains features that estimate whether the lexemes within the window are potential abbreviations.
</nextsent>
<nextsent>the global context reflects features that pertain to the boundary identification process.
</nextsent>
<nextsent>these features specify whether discourse marker that introduces expectations (cristea and webber, 1997) (<papid> P97-1012 </papid>such as although) was used in the sentence under consider-ation, whether there are any commas or dashes be-fore the estimated end of the sentence, and whether there are any verbs in the unit under consideration.</nextsent>
<nextsent>a binary representation the features that char-acterize both the local and global contexts yields learning examples with 2417 features/example.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X704">
<title id=" P99-1047.xml">a decision based approach to rhetorical parsing </title>
<section> the discourse segmenter.  </section>
<citcontext>
<prevsection>
<prevsent>the local context also contains features that estimate whether the lexemes within the window are potential abbreviations.
</prevsent>
<prevsent>the global context reflects features that pertain to the boundary identification process.
</prevsent>
</prevsection>
<citsent citstr=" P97-1012 ">
these features specify whether discourse marker that introduces expectations (cristea and webber, 1997) (<papid> P97-1012 </papid>such as although) was used in the sentence under consider-ation, whether there are any commas or dashes be-fore the estimated end of the sentence, and whether there are any verbs in the unit under consideration.</citsent>
<aftsection>
<nextsent>a binary representation the features that char-acterize both the local and global contexts yields learning examples with 2417 features/example.
</nextsent>
<nextsent>4.3 evaluation.
</nextsent>
<nextsent>we used the c4.5 program (quinlan, 1993) in order to learn decision trees and rules that classify let- corpus # cases bi(%) b2(%) acc(%) muc 14362 91.28 93.1 96.244-0.06 wsj 31309 92.39 94.6 97.144-0.10 brown 72092 93.84 96.8 97.874-0.04 table 1: performance of discourse segmenter that uses decision-tree, non-binary classifier.
</nextsent>
<nextsent>ace action (a) (b) (c) (d) (e) sentence-break (a) 272 4 edu-break (b) 133 3 84 start-parch (c) 4 26 end-paten (d) 20 6 none (e) 2 38 1 4 7555 table 2: confusion matrix for the decision-tree, non-binary classifier (the brown corpus).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X705">
<title id=" P99-1047.xml">a decision based approach to rhetorical parsing </title>
<section> the discourse segmenter.  </section>
<citcontext>
<prevsection>
<prevsent>/ / 2.00 4.00 / ? cases 1o 3 6.00 8.00 i0.00 12.00 edu boundaries.
</prevsent>
<prevsent>the performance is high with re-spect to recognizing sentence boundaries and ends of parenthetical units.
</prevsent>
</prevsection>
<citsent citstr=" J97-2002 ">
the performance with re-spect to identifying sentence boundaries appears to be close to that of systems aimed at identify-ing only sentence boundaries (palmer and hearst, 1997), <papid> J97-2002 </papid>whose accuracy is in the range of 99%.</citsent>
<aftsection>
<nextsent>figure 3: learning curve for discourse segmenter (the muc corpus).
</nextsent>
<nextsent>emes as boundaries of sentences, edus, or parenthet-ical units, or as non-boundaries.
</nextsent>
<nextsent>we learned both from binary (when we could) and non-binary repre-sentations of the cases.
</nextsent>
<nextsent>1 in general the binary rep-resentations yielded slightly better esults than the non-binary representations and the tree classifiers were slightly better than the rule-based ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X708">
<title id=" P99-1047.xml">a decision based approach to rhetorical parsing </title>
<section> the shift-reduce action identifier ,.  </section>
<citcontext>
<prevsection>
<prevsent>3 semantic-similarity-based atures.
</prevsent>
<prevsent>features that denote the semantic similarity be-tween the textual segments ubsumed by the trees in focus.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
this similarity is computed by applying in the style of hearst (1997) <papid> J97-1003 </papid>cosine-based metric on the morphed segments.</citsent>
<aftsection>
<nextsent>features that denote wordnet-based measures of similarity between the bags of words in the promo-tion sets of the trees in focus.
</nextsent>
<nextsent>we use 14 wordnet- based measures of similarity, one for each word-net relation (fellbaum, 1998).
</nextsent>
<nextsent>each of these sim-ilarities is computed using metric similar to the cosine-based metric.
</nextsent>
<nextsent>wordnet-based similarities re-flect the degree of synonymy, antonymy, meronymy, hyponymy, etc. between the textual segments ub- sumed by the trees in focus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X709">
<title id=" P99-1047.xml">a decision based approach to rhetorical parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, they are insufficient for deter-mining correctly the elementary units of discourse and the rhetorical relations that hold between dis-course segments.
</prevsent>
<prevsent>the rhetorical parser presented here is the first that employs learning methods and thorough evalua-tion methodology.
</prevsent>
</prevsection>
<citsent citstr=" C94-2183 ">
all previous parsers aimed at determining the rhetorical structure of unrestricted texts (sumita et al, 1992; kurohashi and nagao, 1994; <papid> C94-2183 </papid>marcu, 1997; <papid> P97-1013 </papid>corston-oliver, 1998)em- ployed manually written rules.</citsent>
<aftsection>
<nextsent>because of the lack of discourse corpora, these parsers did not evaluate the correctness of the discourse trees they built perse, but rather their adequacy for specific purposes: experiments carded out by miike et al (1994) and marcu (1999) showed only that the discourse struc-tures built by rhetorical parsers (sumita et al, 1992; marcu, 1997) <papid> P97-1013 </papid>can be used successfully in order to improve retrieval performance and summarize xt.</nextsent>
<nextsent>in this paper, we presented shift-reduce rhetori-cal parsing algorithm that learns to construct rhetor-ical structures of texts from tagged ata.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X712">
<title id=" P99-1027.xml">should we translate the documents or the queries in cross language information retrieval </title>
<section> trans la ion  mode l.  </section>
<citcontext>
<prevsection>
<prevsent>surprisingly, we find that the hybrid system involving fast document ranslation and monolingual retrieval continues to out- perform monolingual retrieval.
</prevsent>
<prevsent>we thus con-clude that the hybrid system of query and document translation will outperform pure query translation system no matter how high the quality of the query translation.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the algorithm for fast translation, which has been described previously in some de-tail (mccarley and roukos, 1998) and used with considerable success in trec (franz et al, 1999), is descend ent of ibm model 1 (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>our model captures important features of more complex models, such as fertility (the number of french words 209 output when given english word is trans- lated) but ignores complexities such as dis-tortion parameters that are unimportant for ir.
</nextsent>
<nextsent>very fast decoding is achieved by imple-menting it as direct-channel model rather than as source-channel model.
</nextsent>
<nextsent>the ba-sic structure of the english~french model is the probability distribution fl...a, le,,co text(e,)).
</nextsent>
<nextsent>(1) of the fertility ni of an english word ei and set of french words fl...f,~ associated with that english word, given its context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X714">
<title id=" P99-1027.xml">should we translate the documents or the queries in cross language information retrieval </title>
<section> trans la ion  mode l.  </section>
<citcontext>
<prevsection>
<prevsent>(1) of the fertility ni of an english word ei and set of french words fl...f,~ associated with that english word, given its context.
</prevsent>
<prevsent>here we regard the context of word as the pre-ceding and following non-stop words; our ap-proach can easily be extended to other types of contextual features.
</prevsent>
</prevsection>
<citsent citstr=" P91-1022 ">
this model is trained on approximately 5 million sentence pairs of hansard (canadian parliamentary) and un proceedings which have been aligned on sentence-by-sentence basis by the methods of (brown et al, 1991), <papid> P91-1022 </papid>and then further aligned on word-by-word basis by meth-ods similar to (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>the french::~english model can be described by simply interchanging english and french no-tation above.
</nextsent>
<nextsent>it is trained separately on the same training data, using identical proce-dures.
</nextsent>
<nextsent>experiments the document sets used in our experiments were the english and french parts of the doc-ument set used in the trec-6 and trec- 7 clir tracks.
</nextsent>
<nextsent>the english document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X717">
<title id=" S12-1009.xml">statistical thesaurus construction for a morphologically rich language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we explore alternative term representations, complemented by clustering of morphological variants.
</prevsent>
<prevsent>we introduce generic algorithmic scheme for thesaurus construction in mrl, and demonstrate the empirical benefit of our methodology for hebrew thesaurus.
</prevsent>
</prevsection>
<citsent citstr=" W02-0908 ">
corpus-based thesaurus construction has been an active research area (grefenstette, 1994; curran and moens, 2002; <papid> W02-0908 </papid>kilgarriff, 2003; rychly and kilgarriff, 2007).</citsent>
<aftsection>
<nextsent>typically, two statistical approaches for identifying semantic relationships between words were investigated: first-order, co occurrence-based methods which assume that words that occur frequently together are topically related (schutze and pederson, 1997) and second order, distributional similarity methods (hindle, 1990; <papid> P90-1034 </papid>lin, 1998; gasperin et al 2001; weeds and weir, 2003; <papid> W03-1011 </papid>kotlerman et al, 2010), which suggest that words occurring within similar contexts are semantically similar (harris, 1968).</nextsent>
<nextsent>while most prior work focused on english, we are interested in applying these methods to mrl.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X718">
<title id=" S12-1009.xml">statistical thesaurus construction for a morphologically rich language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we introduce generic algorithmic scheme for thesaurus construction in mrl, and demonstrate the empirical benefit of our methodology for hebrew thesaurus.
</prevsent>
<prevsent>corpus-based thesaurus construction has been an active research area (grefenstette, 1994; curran and moens, 2002; <papid> W02-0908 </papid>kilgarriff, 2003; rychly and kilgarriff, 2007).</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
typically, two statistical approaches for identifying semantic relationships between words were investigated: first-order, co occurrence-based methods which assume that words that occur frequently together are topically related (schutze and pederson, 1997) and second order, distributional similarity methods (hindle, 1990; <papid> P90-1034 </papid>lin, 1998; gasperin et al 2001; weeds and weir, 2003; <papid> W03-1011 </papid>kotlerman et al, 2010), which suggest that words occurring within similar contexts are semantically similar (harris, 1968).</citsent>
<aftsection>
<nextsent>while most prior work focused on english, we are interested in applying these methods to mrl.
</nextsent>
<nextsent>such languages, hebrew in our case, are characterized by highly productive morphology which may produce as many as thousands of word forms forgiven root form.
</nextsent>
<nextsent>thesauri usually provide related terms for each entry term (denoted target term).
</nextsent>
<nextsent>since both target and related terms correspond to word lemmas, statistics collection from the corpus would be most directly applied at the lemma level as well, using morphological analyzer and tagger (linden and piitulainen, 2004; <papid> W04-1808 </papid>peirsman et al, 2008; rapp, 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X719">
<title id=" S12-1009.xml">statistical thesaurus construction for a morphologically rich language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we introduce generic algorithmic scheme for thesaurus construction in mrl, and demonstrate the empirical benefit of our methodology for hebrew thesaurus.
</prevsent>
<prevsent>corpus-based thesaurus construction has been an active research area (grefenstette, 1994; curran and moens, 2002; <papid> W02-0908 </papid>kilgarriff, 2003; rychly and kilgarriff, 2007).</prevsent>
</prevsection>
<citsent citstr=" W03-1011 ">
typically, two statistical approaches for identifying semantic relationships between words were investigated: first-order, co occurrence-based methods which assume that words that occur frequently together are topically related (schutze and pederson, 1997) and second order, distributional similarity methods (hindle, 1990; <papid> P90-1034 </papid>lin, 1998; gasperin et al 2001; weeds and weir, 2003; <papid> W03-1011 </papid>kotlerman et al, 2010), which suggest that words occurring within similar contexts are semantically similar (harris, 1968).</citsent>
<aftsection>
<nextsent>while most prior work focused on english, we are interested in applying these methods to mrl.
</nextsent>
<nextsent>such languages, hebrew in our case, are characterized by highly productive morphology which may produce as many as thousands of word forms forgiven root form.
</nextsent>
<nextsent>thesauri usually provide related terms for each entry term (denoted target term).
</nextsent>
<nextsent>since both target and related terms correspond to word lemmas, statistics collection from the corpus would be most directly applied at the lemma level as well, using morphological analyzer and tagger (linden and piitulainen, 2004; <papid> W04-1808 </papid>peirsman et al, 2008; rapp, 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X720">
<title id=" S12-1009.xml">statistical thesaurus construction for a morphologically rich language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such languages, hebrew in our case, are characterized by highly productive morphology which may produce as many as thousands of word forms forgiven root form.
</prevsent>
<prevsent>thesauri usually provide related terms for each entry term (denoted target term).
</prevsent>
</prevsection>
<citsent citstr=" W04-1808 ">
since both target and related terms correspond to word lemmas, statistics collection from the corpus would be most directly applied at the lemma level as well, using morphological analyzer and tagger (linden and piitulainen, 2004; <papid> W04-1808 </papid>peirsman et al, 2008; rapp, 2009).</citsent>
<aftsection>
<nextsent>however, due to the rich and challenging morphology of mrl, such tools often have limited performance.
</nextsent>
<nextsent>in our research, the accuracy of state-of-the-art modern hebrew tagger on cross genre corpus was only about 60%.
</nextsent>
<nextsent>considering such limited performance of morphological processing, we propose schematic methodology for generating co-occurrence based thesaurus in mrl.
</nextsent>
<nextsent>in particular, we propose and investigate three options for term representation, namely surface form, lemma and multiple lemmas, supplemented with clustering of term variants.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X721">
<title id=" S12-1009.xml">statistical thesaurus construction for a morphologically rich language </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>our methodology was applied for statistical measures of first order similarity (word co occurrence).
</prevsent>
<prevsent>these statistics consider the number of times each candidate term co-occurs with the target term in the same document, relative to their total frequencies in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
common cooccurrence metrics are dice coefficient (smadja et al, 1996), pointwise mutual information (pmi) (church and hanks, 1990) <papid> J90-1003 </papid>and log-likelihood test (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>2.1 term representation.
</nextsent>
<nextsent>statistical extraction is affected by term representation in the corpus.
</nextsent>
<nextsent>usually, related terms in thesaurus are lemmas, which can be identified by morphological disambiguation tools.
</nextsent>
<nextsent>however, we present two other approaches for term representation (either target term or candidate related term), which are less dependent on morphological processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X722">
<title id=" S12-1009.xml">statistical thesaurus construction for a morphologically rich language </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>our methodology was applied for statistical measures of first order similarity (word co occurrence).
</prevsent>
<prevsent>these statistics consider the number of times each candidate term co-occurs with the target term in the same document, relative to their total frequencies in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
common cooccurrence metrics are dice coefficient (smadja et al, 1996), pointwise mutual information (pmi) (church and hanks, 1990) <papid> J90-1003 </papid>and log-likelihood test (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>2.1 term representation.
</nextsent>
<nextsent>statistical extraction is affected by term representation in the corpus.
</nextsent>
<nextsent>usually, related terms in thesaurus are lemmas, which can be identified by morphological disambiguation tools.
</nextsent>
<nextsent>however, we present two other approaches for term representation (either target term or candidate related term), which are less dependent on morphological processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X723">
<title id=" S12-1009.xml">statistical thesaurus construction for a morphologically rich language </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>the first measure, given two vertices  terms, extracts all possible lemmas for each term and searches for an overlap of at least one lemma.
</prevsent>
<prevsent>the second measure considers the most probable lemma of the vertices  terms and checks whether these lemmas are equal.
</prevsent>
</prevsection>
<citsent citstr=" P08-1085 ">
the probability of lemma was defined as the sum of probabilities for all morphological analyses containing the lemma, using morpho-lexical context-independent probabilities approximation (goldberg et al, 2008).<papid> P08-1085 </papid></citsent>
<aftsection>
<nextsent>the clustering was done by finding the connected components in our graph of terms using the jung1 implementation (weakcomponentvertexclusterer algorithm with default parameters).
</nextsent>
<nextsent>the connected components are expected to correspond to different lemmas of terms.
</nextsent>
<nextsent>hierarchical clustering methods (jain et al, 1999) were examined as well (single link and complete-link clustering), but they were inferior.
</nextsent>
<nextsent>after applying the clustering algorithm, we reranked the clusters aiming to get the best clusters at the top of clusters list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X725">
<title id=" S12-1009.xml">statistical thesaurus construction for a morphologically rich language </title>
<section> case study: cross-genre hebrew.  </section>
<citcontext>
<prevsection>
<prevsent>it contains 76,710 articles and about 100 million word tokens, and was used for previous ir and nlp research (choueka, 1972; fraenkel, 1976; choueka et al, 1987; kernel et al 2008).
</prevsent>
<prevsent>unfortunately, due to the different genres in the responsa corpus, available tools for hebrew processing perform poorly on this corpus.
</prevsent>
</prevsection>
<citsent citstr=" P06-1084 ">
in preliminary experiment, the pos tagger (adler and elhadad, 2006) <papid> P06-1084 </papid>accuracy on the responsa corpus was less than 60%, while the accuracy of the same tagger on modern hebrew corpora is ~90% (bar haim et al, 2007).</citsent>
<aftsection>
<nextsent>for this project, we utilized the mila hebrew morphological analyzer3 (itai and wintner, 2008; yona and wintner, 2008) and the (adler and elhadad 2006) <papid> P06-1084 </papid>pos tagger for lemma representa tion.</nextsent>
<nextsent>the latter had two important characteristics: the first is flexibility- this tagger allows adapting the estimates of the prior (context-independent) probability of each morphological analysis in an unsupervised manner, from an unlabeled corpus of the target domain (goldberg et al, 2008).<papid> P08-1085 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X729">
<title id=" S12-1009.xml">statistical thesaurus construction for a morphologically rich language </title>
<section> case study: cross-genre hebrew.  </section>
<citcontext>
<prevsection>
<prevsent>for this project, we utilized the mila hebrew morphological analyzer3 (itai and wintner, 2008; yona and wintner, 2008) and the (adler and elhadad 2006) <papid> P06-1084 </papid>pos tagger for lemma representa tion.</prevsent>
<prevsent>the latter had two important characteristics: the first is flexibility- this tagger allows adapting the estimates of the prior (context-independent) probability of each morphological analysis in an unsupervised manner, from an unlabeled corpus of the target domain (goldberg et al, 2008).<papid> P08-1085 </papid></prevsent>
</prevsection>
<citsent citstr=" P08-1083 ">
the second advantage is its mechanism for analyzing unknown tokens (adler et al, 2008).<papid> P08-1083 </papid></citsent>
<aftsection>
<nextsent>since about 50% of the words in our corpora are unknown (with respect to mila lexicon), such mechanism is essential.
</nextsent>
<nextsent>for statistics extraction, we used lucene4.
</nextsent>
<nextsent>we took the top 1000 documents retrieved for the target term and extracted candidate terms from them.
</nextsent>
<nextsent>dice coefficient was used as our co-occurrence measure, most probable lemma was considered for clustering equivalence, and clusters were ranked based on maximization, where the maximal score was multiplied by cluster size.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X730">
<title id=" S10-1056.xml">utdmet combining wordnet and corpus data for argument coercion detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show there sults these approaches obtain on the taskas well as how they can improve traditional feature-based approach.
</prevsent>
<prevsent>argument coercion (a type of metonymy) occurs when the expected semantic class (relative to thea predicate) is substituted for an object of different semantic class.
</prevsent>
</prevsection>
<citsent citstr=" P05-1026 ">
metonymy is pervasive phenomenon in language and the interpretation ofmetonymic expressions can impact tasks from semantic parsing (scheffczyk et al, 2006) to question answering (harabagiu et al, 2005).<papid> P05-1026 </papid></citsent>
<aftsection>
<nextsent>a seminal example in metonymy from (lakoff and johnson, 1980) is: (1) the ham sandwich is waiting for his check.
</nextsent>
<nextsent>the arg1 for the predicate wait is typically an animate, but the ham sandwich?
</nextsent>
<nextsent>is clearly not ananimate.
</nextsent>
<nextsent>rather, the argument is coerced to fulfill the predicates typing requirement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X731">
<title id=" S10-1056.xml">utdmet combining wordnet and corpus data for argument coercion detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this coercion is allowed because an object that would normally fulfill the typing requirement (the customer) can be uniquely identified by an attribute (the ham sandwich he ordered).
</prevsent>
<prevsent>semeval-2010 task 7 (argument selection and coercion?)
</prevsent>
</prevsection>
<citsent citstr=" W09-2414 ">
(pustejovsky and rumshisky,2009) <papid> W09-2414 </papid>was designed to evaluate systems that detect such coercions and provide compositionalhistory?</citsent>
<aftsection>
<nextsent>of argument selection relative to the predicate.
</nextsent>
<nextsent>in order to accomplish this, an argument is annotated with both the semantic class to which it belongs (the source?
</nextsent>
<nextsent>type) as well as the class expected by the predicate (the target?
</nextsent>
<nextsent>type).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X732">
<title id=" S10-1056.xml">utdmet combining wordnet and corpus data for argument coercion detection </title>
<section> page ranking wordnet hypernyms.  </section>
<citcontext>
<prevsection>
<prevsent>252 given set of seed concepts, we mine wordnet for other concepts that may be in the same semantic class.
</prevsent>
<prevsent>clearly, this approach has both practical limitations (wordnet does not contain every possible concept) and linguistic limitations (concepts may belong to different semantic classes based ontheir context).
</prevsent>
</prevsection>
<citsent citstr=" P07-1054 ">
however, given the often vague nature of semantic classes (is building an artifact or location?), access to weighted listof semantic class members can prove useful for arguments not seen in the train set.using (esuli and sebastiani, 2007) <papid> P07-1054 </papid>as inspiration, we have implemented our own naive version of wordnet pagerank.</citsent>
<aftsection>
<nextsent>they use sense disambiguated glosses provided by extended wordnet (harabagiu et al, 1999) <papid> W99-0501 </papid>to link synsets by starting with positive (or negative) sentiment concepts in order to find other concepts with positive (or negative) sentiment values.</nextsent>
<nextsent>for ourtask, however, hypernymy relations are more appropriate for determining given synsets membership in semantic class.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X733">
<title id=" S10-1056.xml">utdmet combining wordnet and corpus data for argument coercion detection </title>
<section> page ranking wordnet hypernyms.  </section>
<citcontext>
<prevsection>
<prevsent>clearly, this approach has both practical limitations (wordnet does not contain every possible concept) and linguistic limitations (concepts may belong to different semantic classes based ontheir context).
</prevsent>
<prevsent>however, given the often vague nature of semantic classes (is building an artifact or location?), access to weighted listof semantic class members can prove useful for arguments not seen in the train set.using (esuli and sebastiani, 2007) <papid> P07-1054 </papid>as inspiration, we have implemented our own naive version of wordnet pagerank.</prevsent>
</prevsection>
<citsent citstr=" W99-0501 ">
they use sense disambiguated glosses provided by extended wordnet (harabagiu et al, 1999) <papid> W99-0501 </papid>to link synsets by starting with positive (or negative) sentiment concepts in order to find other concepts with positive (or negative) sentiment values.</citsent>
<aftsection>
<nextsent>for ourtask, however, hypernymy relations are more appropriate for determining given synsets membership in semantic class.
</nextsent>
<nextsent>hypernymy defines an is-a relationship between the parent class (the hypernym) and one of its child classes(the hyponym).
</nextsent>
<nextsent>furthermore, while page rank assumes directed edges (e.g., hyper links in web page), we use undirected edges.
</nextsent>
<nextsent>in this way, if hypernymof(a, b), then as membership in semantic class strengthens bs and vice versa.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X735">
<title id=" S10-1056.xml">utdmet combining wordnet and corpus data for argument coercion detection </title>
<section> leveraging large corpus of.  </section>
<citcontext>
<prevsection>
<prevsent>as (pustejovsky and rumshisky, 2009) <papid> W09-2414 </papid>elaborate,predicates select an argument from specific semantic class, therefore terms that belong in thesame semantic class should be selected by similar predicates.</prevsent>
<prevsent>however, this assumption is often violated: type coercion allows predicates to have arguments outside their intended semantic class.</prevsent>
</prevsection>
<citsent citstr=" J03-2004 ">
our solution to this problem, partially inspired by(lapata and lascarides, 2003), <papid> J03-2004 </papid>is to collect statistics from an enormous amount of data in order to statistically filter out these coercions.</citsent>
<aftsection>
<nextsent>the english gigaword forth edition corpus1 contains over 8.5 million documents of newswire text collected over 15 year period.
</nextsent>
<nextsent>we processed these documents with the senna2 (collobert and weston, 2009) suite of natural language tools, which includes part-of-speech tagger, phrase chunker, named entity recognizer, and propbank semantic role labeler.
</nextsent>
<nextsent>we chose senna due to its speed, yet it still performs comparably with manystate-of-the-art systems.
</nextsent>
<nextsent>of the 8.5 million documents in english gigaword, 8 million were successfully processed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X736">
<title id=" S10-1056.xml">utdmet combining wordnet and corpus data for argument coercion detection </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 wordnet pagerank.
</prevsent>
<prevsent>we experimented with the output of our wordnetpagerank implementation along three separate di mensions: (1) which sense to use (since we didnot incorporate word sense disambiguation system), (2) whether to use the highest scoring se 3the notable exception to this, however, is arrive, where the data uses the destination argument.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
in the propbank scheme (palmer et al, 2005), <papid> J05-1004 </papid>this would correspond to the arg4, which usually signifies an end state.</citsent>
<aftsection>
<nextsent>4http://svmlight.joachims.org/svm multiclass.html mantic class or every class an argument belonged to, and (3) how to use the weight output by the algorithm.
</nextsent>
<nextsent>the results of these experiments yielded single feature for each class that returns true if the argument is in that class, regardless of weight.
</nextsent>
<nextsent>this resulted in micro-precision score of 75.6%.
</nextsent>
<nextsent>4.2 gigaword predicates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X737">
<title id=" S12-1078.xml">limsi learning semantic similarity by selecting random word subsets </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>although the 5 sets had very different characteristics, we concatenated all training files and trained single model.
</prevsent>
<prevsent>the principal evaluation metrics was pearson correlation coefficient, that we report here.
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
two related other measures were also used (agirre et al, 2012).<papid> S12-1051 </papid>obtained sentence vectors v(s) for were transformed into vectors x?</citsent>
<aftsection>
<nextsent>with several methods: ? sumdiff?: x?
</nextsent>
<nextsent>= (v?(s1) + v?(s2), sgn(v1(s1) ? v1(s2))(v(s1)?
</nextsent>
<nextsent>v(s2))) ? concat?: x?
</nextsent>
<nextsent>= (v(s1), v(s2)), and x??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X738">
<title id=" S12-1090.xml">umccdlsi multidimensional lexical semantic textual similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to establish which features are the most appropriate to improve sts results we participated with three runs using different set of features.
</prevsent>
<prevsent>our best approach reached the position 18 of 89 runs, obtaining general correlation coefficient up to 0.72.
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
semeval 2012 competition for evaluating natural language processing (nlp) systems presents new task called semantic textual similarity (sts) (agirre et al, 2012).<papid> S12-1051 </papid></citsent>
<aftsection>
<nextsent>insts the participating systems must examine the degree of semantic equivalence between two sentences.
</nextsent>
<nextsent>the goal of this task is to create unified framework for the evaluation of semantic textual similarity modules and to characterize their impact on nlp applications.
</nextsent>
<nextsent>sts is related to textual entailment (te) and paraphrase tasks.
</nextsent>
<nextsent>the main difference is that sts 1 integration of semantic resource based on wordnet..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X739">
<title id=" P98-2206.xml">chinese word segmentation without using lexicon and handcrafted training data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, ven the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains.
</prevsent>
<prevsent>an important issue in designing chinese segment ers thus how to reduce the effort of human supervision as much as possible.
</prevsent>
</prevsection>
<citsent citstr=" P97-1041 ">
palmer(1997) <papid> P97-1041 </papid>conducted chinese segrnenter which merely made use of manually segmented corpus(without referring to any lexicon).</citsent>
<aftsection>
<nextsent>a transformation-based algorithm was then explored to learn segmentation rules automatically from the segmented corpus.
</nextsent>
<nextsent>sproat and shih(1993) further proposed method using neither lexicon nor segmented corpus: for input texts, simply grouping character pairs with high value of mutual information into words.
</nextsent>
<nextsent>although this strategy is very simple and has many limitations(e.g., it can only treat bi-character words), the characteristic of it is that it is fully automatic -- the nmtual information between characters can be trained from raw chinese corpus directly.
</nextsent>
<nextsent>following the line of sproat and shih, here we present new algorithm for segmenting chinese texts which depends upon neither lexicon nor any hand-crafted resource.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X740">
<title id=" S12-1018.xml">annotating preferences in negotiation dialogues </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>lets go and see madagascar 2.
</prevsent>
<prevsent>(a) expresses direct positive opinion towards the movie but we do not know if this movie is the mostpreferred.
</prevsent>
</prevsection>
<citsent citstr=" C08-1031 ">
(b) expresses comparative opinion between two movies with respect to their shared features (scenarios) (ganapathibhotla and liu, 2008).<papid> C08-1031 </papid></citsent>
<aftsection>
<nextsent>if actions involving these movies (e.g. seeing them)are clear in the context, such comparative opinion will imply preference, ordering the first season scenario over the second.
</nextsent>
<nextsent>finally, (c) expresses two preferences, one depending on the other.
</nextsent>
<nextsent>the first is that the speaker prefers to go to the cinema over other alternative actions; the second is, given that preference, that he wants to see madagascar 2 over other possible movies.
</nextsent>
<nextsent>reasoning about preferences is also distinct from reasoning about opinions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X741">
<title id=" S12-1018.xml">annotating preferences in negotiation dialogues </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>pi5 b: economy, please.
</prevsent>
<prevsent>our approach to preference acquisition exploits discourse structure and aims to study the impact of discourse for extracting and reasoning on preferences.
</prevsent>
</prevsection>
<citsent citstr=" W11-2023 ">
cadilhac et al (2011) <papid> W11-2023 </papid>show how to compute automatically preference representations for whole stretch of dialogue from the preference representations for elementary discourse units.</citsent>
<aftsection>
<nextsent>our annotation here concentrates on the commitments to preferences expressed in elementary discourse units oredus.
</nextsent>
<nextsent>we analyze how the outcomes and the dependencies between them are linguistically expressed by performing, on each corpus, two-level annotation.
</nextsent>
<nextsent>first, we perform segmentation of the dialogue into edus.
</nextsent>
<nextsent>second, we annotate preferences expressed by the edus.the examples above show the effects of segmentation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X742">
<title id=" S10-1003.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper reports on the sixteen submissions from the five different participating teams.
</prevsent>
<prevsent>word sense disambiguation, the task of selecting the correct sense of an ambiguous word in given context, is well-researched nlp problem (see for example agirre and edmonds (2006) and navigli (2009)), largely boosted by the various senseval and semeval editions.
</prevsent>
</prevsection>
<citsent citstr=" W07-2001 ">
the semeval-2010 cross-lingual word sense disambiguation task focuses on two bottlenecks in current wsd research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other thanenglish) and the growing tendency to evaluate the performance of wsd systems in real application such as machine translation and cross-language information retrieval (see for example agirre et al (2007)).<papid> W07-2001 </papid>the cross-lingual wsd task aims at the development of multilingual dataset to test the feasibility of multilingual wsd.</citsent>
<aftsection>
<nextsent>many studies have already shown the validity of this cross lingual evidence idea (gale et al, 1993; ide et al., 2002; <papid> W02-0808 </papid>ng et al, 2003; <papid> P03-1058 </papid>apidianaki, 2009), <papid> E09-1010 </papid>but until now no benchmark datasets have been available.</nextsent>
<nextsent>for the semeval-2010 competition we developed (i) sense inventory in which the sense distinctions were extracted from the multilingual corpus europarl 1 and (ii) dataset in which the ambiguous words were annotated with the senses from the multilingual sense inventory.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X743">
<title id=" S10-1003.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation, the task of selecting the correct sense of an ambiguous word in given context, is well-researched nlp problem (see for example agirre and edmonds (2006) and navigli (2009)), largely boosted by the various senseval and semeval editions.
</prevsent>
<prevsent>the semeval-2010 cross-lingual word sense disambiguation task focuses on two bottlenecks in current wsd research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other thanenglish) and the growing tendency to evaluate the performance of wsd systems in real application such as machine translation and cross-language information retrieval (see for example agirre et al (2007)).<papid> W07-2001 </papid>the cross-lingual wsd task aims at the development of multilingual dataset to test the feasibility of multilingual wsd.</prevsent>
</prevsection>
<citsent citstr=" W02-0808 ">
many studies have already shown the validity of this cross lingual evidence idea (gale et al, 1993; ide et al., 2002; <papid> W02-0808 </papid>ng et al, 2003; <papid> P03-1058 </papid>apidianaki, 2009), <papid> E09-1010 </papid>but until now no benchmark datasets have been available.</citsent>
<aftsection>
<nextsent>for the semeval-2010 competition we developed (i) sense inventory in which the sense distinctions were extracted from the multilingual corpus europarl 1 and (ii) dataset in which the ambiguous words were annotated with the senses from the multilingual sense inventory.
</nextsent>
<nextsent>the cross-lingual wsd task is lexical sample task for english nouns, in which the word senses are made up of the translations in five languages, viz.
</nextsent>
<nextsent>dutch, french, italian, spanish and german.
</nextsent>
<nextsent>both the sense inventory and the annotated dataset were constructed for sample of 25 nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X744">
<title id=" S10-1003.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation, the task of selecting the correct sense of an ambiguous word in given context, is well-researched nlp problem (see for example agirre and edmonds (2006) and navigli (2009)), largely boosted by the various senseval and semeval editions.
</prevsent>
<prevsent>the semeval-2010 cross-lingual word sense disambiguation task focuses on two bottlenecks in current wsd research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other thanenglish) and the growing tendency to evaluate the performance of wsd systems in real application such as machine translation and cross-language information retrieval (see for example agirre et al (2007)).<papid> W07-2001 </papid>the cross-lingual wsd task aims at the development of multilingual dataset to test the feasibility of multilingual wsd.</prevsent>
</prevsection>
<citsent citstr=" P03-1058 ">
many studies have already shown the validity of this cross lingual evidence idea (gale et al, 1993; ide et al., 2002; <papid> W02-0808 </papid>ng et al, 2003; <papid> P03-1058 </papid>apidianaki, 2009), <papid> E09-1010 </papid>but until now no benchmark datasets have been available.</citsent>
<aftsection>
<nextsent>for the semeval-2010 competition we developed (i) sense inventory in which the sense distinctions were extracted from the multilingual corpus europarl 1 and (ii) dataset in which the ambiguous words were annotated with the senses from the multilingual sense inventory.
</nextsent>
<nextsent>the cross-lingual wsd task is lexical sample task for english nouns, in which the word senses are made up of the translations in five languages, viz.
</nextsent>
<nextsent>dutch, french, italian, spanish and german.
</nextsent>
<nextsent>both the sense inventory and the annotated dataset were constructed for sample of 25 nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X745">
<title id=" S10-1003.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation, the task of selecting the correct sense of an ambiguous word in given context, is well-researched nlp problem (see for example agirre and edmonds (2006) and navigli (2009)), largely boosted by the various senseval and semeval editions.
</prevsent>
<prevsent>the semeval-2010 cross-lingual word sense disambiguation task focuses on two bottlenecks in current wsd research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other thanenglish) and the growing tendency to evaluate the performance of wsd systems in real application such as machine translation and cross-language information retrieval (see for example agirre et al (2007)).<papid> W07-2001 </papid>the cross-lingual wsd task aims at the development of multilingual dataset to test the feasibility of multilingual wsd.</prevsent>
</prevsection>
<citsent citstr=" E09-1010 ">
many studies have already shown the validity of this cross lingual evidence idea (gale et al, 1993; ide et al., 2002; <papid> W02-0808 </papid>ng et al, 2003; <papid> P03-1058 </papid>apidianaki, 2009), <papid> E09-1010 </papid>but until now no benchmark datasets have been available.</citsent>
<aftsection>
<nextsent>for the semeval-2010 competition we developed (i) sense inventory in which the sense distinctions were extracted from the multilingual corpus europarl 1 and (ii) dataset in which the ambiguous words were annotated with the senses from the multilingual sense inventory.
</nextsent>
<nextsent>the cross-lingual wsd task is lexical sample task for english nouns, in which the word senses are made up of the translations in five languages, viz.
</nextsent>
<nextsent>dutch, french, italian, spanish and german.
</nextsent>
<nextsent>both the sense inventory and the annotated dataset were constructed for sample of 25 nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X746">
<title id=" S10-1003.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> task setup.  </section>
<citcontext>
<prevsection>
<prevsent>we selected 6 languages from the 11 european languages represented in the corpus, viz.
</prevsent>
<prevsent>english(our target language), dutch, french, german, italian and spanish.
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
all data were already sentence-aligned using tool based on the gale and church (1991) <papid> P91-1023 </papid>algorithm, which was part of the europarl corpus.</citsent>
<aftsection>
<nextsent>we only considered the 1-1 sentence alignments between english and the five other languages.
</nextsent>
<nextsent>these sentence alignments were made available to the task participants for the five trial words.the sense inventory extracted from the parallel dataset (section 2.2) was used to annotate the sentences in the trial set and the test set, which were extracted from the jrc-acquis multilingual parallel corpus 3 and bnc 4 . 2.2 creation of the sense inventory.
</nextsent>
<nextsent>two steps were taken to obtain multilingual sense inventory: (1) word alignment on the sentences to find the set of possible translations for the set of ambiguous nouns and (2) clustering by meaning (per target word) of the resulting translations.
</nextsent>
<nextsent>giza++ (och and ney, 2003) <papid> J03-1002 </papid>was used to generate the initial word alignments, which were manually verified by certified translator sin all six involved languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X747">
<title id=" S10-1003.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> task setup.  </section>
<citcontext>
<prevsection>
<prevsent>these sentence alignments were made available to the task participants for the five trial words.the sense inventory extracted from the parallel dataset (section 2.2) was used to annotate the sentences in the trial set and the test set, which were extracted from the jrc-acquis multilingual parallel corpus 3 and bnc 4 . 2.2 creation of the sense inventory.
</prevsent>
<prevsent>two steps were taken to obtain multilingual sense inventory: (1) word alignment on the sentences to find the set of possible translations for the set of ambiguous nouns and (2) clustering by meaning (per target word) of the resulting translations.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
giza++ (och and ney, 2003) <papid> J03-1002 </papid>was used to generate the initial word alignments, which were manually verified by certified translator sin all six involved languages.</citsent>
<aftsection>
<nextsent>the human annotators were asked to assign null?
</nextsent>
<nextsent>link to words for which no valid translation could be identified.
</nextsent>
<nextsent>furthermore, they were also asked to provide extra information on compound translations (e.g. the dutch word in vesteringsbank as translation of the english multiword investment bank), fuzzy links, or target words with different pos (e.g. the verb to bank).
</nextsent>
<nextsent>the manually verified translations were clustered by meaning by one annotator.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X748">
<title id=" S10-1003.xml">semeval2010 task 3 cross lingual word sense disambiguation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the first field contains the target word, pos-tag and language code, the second field contains the sentence id and the third field contains thegold standard translations in the target language, enriched with their frequency weight: (2) coach.n.nl 12 :: coach 3; speler-trainer 1; trainer 3; voetbaltrainer 1; coach.n.fr 12 :: capita ine 1; entraneur 3; coach.n.de 12 :: coach 1; fubaltrainer 1; national trainer 2; trainer 3; coach.n.it 12 :: allenatore 3; coach.n.es 12 :: entrenador 3;
</prevsent>
<prevsent>3.1 scoring.
</prevsent>
</prevsection>
<citsent citstr=" W07-2009 ">
to score the participating systems, we use an evaluation scheme which is inspired by the english lexical substitution task in semeval2007 (mccarthy and navigli, 2007).<papid> W07-2009 </papid></citsent>
<aftsection>
<nextsent>we perform both best result evaluation and more relaxed evaluation for the top five results.
</nextsent>
<nextsent>the evaluation is performed using precision andre call (prec and rec in the equations below), and mode precision (m ) and mode recall (m r), where we calculate precision andre call against the translation that is preferred by the majority of annotators, provided that one translation is more frequent than the others.
</nextsent>
<nextsent>for the precision and recall formula we use the following variables.
</nextsent>
<nextsent>let be the set of annotators, the set of test items and i theset of responses for an item ? for annotator ? h. for each ? we calculate the mode (m i) which corresponds to the translation with the highest frequency weight.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X750">
<title id=" S10-1010.xml">semeval2010 task 13 tempeval2 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the ultimate aim of temporal processing is the automatic identification of all temporal referring expressions, events and temporal relations within text.
</prevsent>
<prevsent>however, addressing this aim is beyond the scope of an evaluation challenge and more modest approach is appropriate.
</prevsent>
</prevsection>
<citsent citstr=" W07-2014 ">
the 2007 semeval task, tempeval-1 (verhagen et al, 2007; <papid> W07-2014 </papid>verhagen et al, 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing andfrom the perspective of developing systems capable of addressing the tasks.</citsent>
<aftsection>
<nextsent>1 tempeval-2 is based on tempeval-1, but ismore elaborate in two respects: (i) it is multilingual task, and (ii) it consists of six subtasks rather than three.
</nextsent>
<nextsent>in the rest of this paper, we first introduce the data that we are dealing with.
</nextsent>
<nextsent>which gets us in position to present the list of task introduced by tempeval-2, including some motivation as to why we feel that it is good idea to split up temporal relation classification into subtasks.
</nextsent>
<nextsent>we proceed by shortly describing the data resources and their creation, followed by the performance of the systems that participated in the tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X751">
<title id=" S10-1001.xml">semeval2010 task 1 coreference resolution in multiple languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of coreference resolution, defined as the identification of the expressions in text that refer to the same discourse entity (1), has attracted considerable attention within the nlp community.(1) major league baseball sent its head of security to chicago to review the second incident of an on-field fan attack in the last seven months.
</prevsent>
<prevsent>the league is reviewing security at all ballparks to crackdown on spectator violence.
</prevsent>
</prevsection>
<citsent citstr=" W99-0212 ">
using coreference information has been shown to be beneficial in number of nlp applications including information extraction (mccarthy and lehnert, 1995), text summarization (steinberger et al, 2007), question answering (morton, 1999), <papid> W99-0212 </papid>and machine translation.</citsent>
<aftsection>
<nextsent>there have been few evaluation campaigns on coreference resolution in the past, namely muc (hirschman and chinchor, 1997), ace (doddington et al, 2004), and are (orasan et al, 2008), yet many questions remain open:?
</nextsent>
<nextsent>to what extent is it possible to implement general coreference resolution system portable to different languages?
</nextsent>
<nextsent>how much language-specific tuning is necessary??
</nextsent>
<nextsent>how helpful are morphology, syntax and semantics for solving coreference relations?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X752">
<title id=" S10-1001.xml">semeval2010 task 1 coreference resolution in multiple languages </title>
<section> linguistic resources.  </section>
<citcontext>
<prevsection>
<prevsent>semantic annotations include nes, words senses(linked to an ontology), and coreference information.
</prevsent>
<prevsent>the ontonotes corpus is distributed by the linguistic data consortium.
</prevsent>
</prevsection>
<citsent citstr=" W05-0303 ">
2 german the tuba-d/z corpus (hinrichs et al, 2005) <papid> W05-0303 </papid>is newspaper treebank based on data taken from the daily issues of die tageszeitung?</citsent>
<aftsection>
<nextsent>(taz).
</nextsent>
<nextsent>it currently comprises 794k words manually annotated with semantic and coreference information.
</nextsent>
<nextsent>due to licensing restrictions of the original texts, taz-dvd must be purchased to obtain license.
</nextsent>
<nextsent>2 italian the live memories corpus (rodrguez et al, 2010) will include texts from the italian wikipedia, blogs, news articles, and dialogues 2 free user license agreements for the english and german task datasets were issued to the task participants.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X753">
<title id=" S10-1001.xml">semeval2010 task 1 coreference resolution in multiple languages </title>
<section> linguistic resources.  </section>
<citcontext>
<prevsection>
<prevsent>catalan, spanish, english predicted lemmas and pos were generated using free ling 4 for catalan/spanish and svm tagger 5 for english.
</prevsent>
<prevsent>dependency information and predicate semantic roles were generated with joint parser, syntactic semantic parser.
</prevsent>
</prevsection>
<citsent citstr=" W99-0707 ">
6dutch lemmas, pos and nes were automatically provided by the memory-based shallow parser for dutch (daelemans et al, 1999), <papid> W99-0707 </papid>and dependency information by the alpino parser (van noord et al, 2006).</citsent>
<aftsection>
<nextsent>german lemmas were predicted by treetagger(schmid, 1995), pos and morphology by rftagger (schmid and laws, 2008), and dependency information by malt parser (hall and nivre, 2008).<papid> W08-1007 </papid></nextsent>
<nextsent>italian lemmas and pos were provided by textpro, 7and dependency information by malt parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X754">
<title id=" S10-1001.xml">semeval2010 task 1 coreference resolution in multiple languages </title>
<section> linguistic resources.  </section>
<citcontext>
<prevsection>
<prevsent>dependency information and predicate semantic roles were generated with joint parser, syntactic semantic parser.
</prevsent>
<prevsent>6dutch lemmas, pos and nes were automatically provided by the memory-based shallow parser for dutch (daelemans et al, 1999), <papid> W99-0707 </papid>and dependency information by the alpino parser (van noord et al, 2006).</prevsent>
</prevsection>
<citsent citstr=" W08-1007 ">
german lemmas were predicted by treetagger(schmid, 1995), pos and morphology by rftagger (schmid and laws, 2008), and dependency information by malt parser (hall and nivre, 2008).<papid> W08-1007 </papid></citsent>
<aftsection>
<nextsent>italian lemmas and pos were provided by textpro, 7and dependency information by malt parser.
</nextsent>
<nextsent>8 3the german and dutch training datasets were not completely stable during the competition period due to few errors.
</nextsent>
<nextsent>revised versions were released on march 2 and 20, respectively.
</nextsent>
<nextsent>as to the test datasets, the dutch and italian documents with formatting errors were corrected after the evaluation period, with no variations in the ranking order of systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X755">
<title id=" S10-1001.xml">semeval2010 task 1 coreference resolution in multiple languages </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>since there is no agreement at present on standard measure for coreference resolution evaluation, one of our goals was to compare the rankings produced by four different measures.
</prevsent>
<prevsent>thetask scorer provides results in the two mention based metrics 3 (bagga and baldwin, 1998) and ceaf-?
</prevsent>
</prevsection>
<citsent citstr=" H05-1004 ">
3 (luo, 2005), <papid> H05-1004 </papid>and the two link-based metrics muc (vilain et al, 1995) and blanc(recasens and hovy, in prep).</citsent>
<aftsection>
<nextsent>the first three measures have been widely used, while blanc is proposal of new measure interesting to test.
</nextsent>
<nextsent>the mention detection subtask is measured with recall, precision, and 1 . mentions are rewarded.
</nextsent>
<nextsent>with 1 point if their boundaries coincide with those of the gold np, with 0.5 points if their boundaries are within the gold np including its head, and with 0 otherwise.
</nextsent>
<nextsent>a total of twenty-two participants registered for the task and downloaded the training materials.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X756">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, centering theory (grosz et al , 1995) provides framework to model local coherence by relating the choice of referring expressions to the salience ofan entity at certain stages of discourse.
</prevsent>
<prevsent>an example for global coherence model would be rhetorical structure theory (mann and thompson, 1988), which addresses overall text structure by means of coherence relations between the parts of text.in addition to such theories, computational approaches have been proposed to capture corresponding phenomena empirically.
</prevsent>
</prevsection>
<citsent citstr=" J08-1001 ">
a prominent example is the entity-based model by barzilay and lapata(2008).<papid> J08-1001 </papid></citsent>
<aftsection>
<nextsent>in their approach, local coherence is modeled by the observation of sentence-to-sentence realization patterns of individual entities.
</nextsent>
<nextsent>the learned model reflects key idea from centering theory,namely that adjacent sentences incoherent discourse are likely to involve the same entities.
</nextsent>
<nextsent>one shortcoming of barzilay and lapatas model (and extensions of it) is that it only investigates overt realization patterns in terms of grammatical functions.
</nextsent>
<nextsent>these functions reflect explicit realizations of predicate argument structures (pas), but they do not capture the full range of salience factors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X757">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we develop specific set-up that allows us to further investigate the factors that govern such null-instantiations of argument positions (cf.fillmore et al  (2003)), as special form of coherence inducing element in discourse.
</prevsent>
<prevsent>we henceforth refer to such cases as non-realized arguments.our main hypothesis is that context specific realization patterns for pas can be automatically 218learned from semantically parsed corpus of comparable text pairs.
</prevsent>
</prevsection>
<citsent citstr=" N04-1015 ">
this assumption builds onthe success of previous research, where comparable and parallel texts have been exploited for range of related learning tasks, e.g., unsupervised discourse segmentation (barzilay and lee, 2004) <papid> N04-1015 </papid>and bootstrapping semantic analyzers (titov and kozhevnikov, 2010).<papid> P10-1098 </papid>for our purposes, we are interested in finding corresponding pas across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants.by aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved partici pants.</citsent>
<aftsection>
<nextsent>as first step towards this overall goal, we describe the construction of resource that contains more than 160,000 document pairs that are known totalk about the same events and participants.
</nextsent>
<nextsent>example (1), extracted from our corpus of aligned texts, illustrates this point: both texts report on the same event, in particular the (aligned) event of locating victims in an avalanche.
</nextsent>
<nextsent>while (1.a) explicitly talks about the location of this event, the role remains implicit in the second sentence of (1.b), given that it can be recovered from the preceding sentence.
</nextsent>
<nextsent>in fact, realization of this argument would impede the fluency of discourse by being overly repetitive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X758">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we develop specific set-up that allows us to further investigate the factors that govern such null-instantiations of argument positions (cf.fillmore et al  (2003)), as special form of coherence inducing element in discourse.
</prevsent>
<prevsent>we henceforth refer to such cases as non-realized arguments.our main hypothesis is that context specific realization patterns for pas can be automatically 218learned from semantically parsed corpus of comparable text pairs.
</prevsent>
</prevsection>
<citsent citstr=" P10-1098 ">
this assumption builds onthe success of previous research, where comparable and parallel texts have been exploited for range of related learning tasks, e.g., unsupervised discourse segmentation (barzilay and lee, 2004) <papid> N04-1015 </papid>and bootstrapping semantic analyzers (titov and kozhevnikov, 2010).<papid> P10-1098 </papid>for our purposes, we are interested in finding corresponding pas across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants.by aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved partici pants.</citsent>
<aftsection>
<nextsent>as first step towards this overall goal, we describe the construction of resource that contains more than 160,000 document pairs that are known totalk about the same events and participants.
</nextsent>
<nextsent>example (1), extracted from our corpus of aligned texts, illustrates this point: both texts report on the same event, in particular the (aligned) event of locating victims in an avalanche.
</nextsent>
<nextsent>while (1.a) explicitly talks about the location of this event, the role remains implicit in the second sentence of (1.b), given that it can be recovered from the preceding sentence.
</nextsent>
<nextsent>in fact, realization of this argument would impede the fluency of discourse by being overly repetitive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X759">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we conclude in section 6 and discuss future work.
</prevsent>
<prevsent>datasets comprising parallel texts have been released for various different tasks, including paraphrase extraction and statistical machine translation(smt).
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
while corpora for smt are typically multilingual (e.g. europarl, koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (barzilay and mckeown, 2001; <papid> P01-1008 </papid>huang etal., 2002, inter alia).</citsent>
<aftsection>
<nextsent>each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same.
</nextsent>
<nextsent>a higher degree of variation can be found in the microsoft research paraphrase corpus (e.g. msrpc, dolan and brockett (2005)), <papid> I05-5002 </papid>which consists of paraphrases automatically extracted from different sources.</nextsent>
<nextsent>in the msrpc, however, original discourse contexts are not provided for each sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X760">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while corpora for smt are typically multilingual (e.g. europarl, koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (barzilay and mckeown, 2001; <papid> P01-1008 </papid>huang etal., 2002, inter alia).</prevsent>
<prevsent>each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same.</prevsent>
</prevsection>
<citsent citstr=" I05-5002 ">
a higher degree of variation can be found in the microsoft research paraphrase corpus (e.g. msrpc, dolan and brockett (2005)), <papid> I05-5002 </papid>which consists of paraphrases automatically extracted from different sources.</citsent>
<aftsection>
<nextsent>in the msrpc, however, original discourse contexts are not provided for each sentence.
</nextsent>
<nextsent>in contrast to truly parallel monolingual corpora, there also exist range of comparable corpora that have been used for tasks such as (multi-document) summarization (mckeown and radev, 1995, inter alia).
</nextsent>
<nextsent>corpora for this task are collected manually and hence are rather small.
</nextsent>
<nextsent>our work presents method to automatically construct large corpus of text pairs describing the same underlying events.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X761">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish co 219 herent discourse.
</prevsent>
<prevsent>different aspects related to this setting have been studied in previous work.
</prevsent>
</prevsection>
<citsent citstr=" P07-1041 ">
forex ample, filippova and strube (2007) <papid> P07-1041 </papid>and cahill and riester (2009) <papid> P09-1092 </papid>examine factors that determine constituent order and belz et al  (2009) <papid> W09-2816 </papid>study the conditions for the use of different types of referring ex pressions.</citsent>
<aftsection>
<nextsent>the specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as special form of coherence inducing element in discourse.
</nextsent>
<nextsent>as in the aforementioned work, we are specifically interested in the generation of coherent discourses(e.g. for summarization).
</nextsent>
<nextsent>yet, our work also complements research in discourse analysis.
</nextsent>
<nextsent>a recent example for such work is the semeval 2010 task 10 (ruppenhofer et al , 2010), which aims at linking events and their participants in discourse.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X762">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish co 219 herent discourse.
</prevsent>
<prevsent>different aspects related to this setting have been studied in previous work.
</prevsent>
</prevsection>
<citsent citstr=" P09-1092 ">
forex ample, filippova and strube (2007) <papid> P07-1041 </papid>and cahill and riester (2009) <papid> P09-1092 </papid>examine factors that determine constituent order and belz et al  (2009) <papid> W09-2816 </papid>study the conditions for the use of different types of referring ex pressions.</citsent>
<aftsection>
<nextsent>the specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as special form of coherence inducing element in discourse.
</nextsent>
<nextsent>as in the aforementioned work, we are specifically interested in the generation of coherent discourses(e.g. for summarization).
</nextsent>
<nextsent>yet, our work also complements research in discourse analysis.
</nextsent>
<nextsent>a recent example for such work is the semeval 2010 task 10 (ruppenhofer et al , 2010), which aims at linking events and their participants in discourse.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X763">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish co 219 herent discourse.
</prevsent>
<prevsent>different aspects related to this setting have been studied in previous work.
</prevsent>
</prevsection>
<citsent citstr=" W09-2816 ">
forex ample, filippova and strube (2007) <papid> P07-1041 </papid>and cahill and riester (2009) <papid> P09-1092 </papid>examine factors that determine constituent order and belz et al  (2009) <papid> W09-2816 </papid>study the conditions for the use of different types of referring ex pressions.</citsent>
<aftsection>
<nextsent>the specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as special form of coherence inducing element in discourse.
</nextsent>
<nextsent>as in the aforementioned work, we are specifically interested in the generation of coherent discourses(e.g. for summarization).
</nextsent>
<nextsent>yet, our work also complements research in discourse analysis.
</nextsent>
<nextsent>a recent example for such work is the semeval 2010 task 10 (ruppenhofer et al , 2010), which aims at linking events and their participants in discourse.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X764">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> a corpus for aligning predications </section>
<citcontext>
<prevsection>
<prevsent>we show instances of this phenomenon, in which aligned pas help to resolve implicit role references, in section 4.
</prevsent>
<prevsent>3.2 gold standard annotation.
</prevsent>
</prevsection>
<citsent citstr=" C10-1011 ">
we pre-processed all texts using mate tools (bohnet, 2010; <papid> C10-1011 </papid>bjorkelund et al , 2010), pipeline of natural language processing modules including state-of-the-art semantic role labeler that computes prop/nombank annotations (palmer et al , 2005; <papid> J05-1004 </papid>meyers et al , 2008).</citsent>
<aftsection>
<nextsent>the output was used to providepre-labeled verbal and nominal predicates for annotation.
</nextsent>
<nextsent>we asked two students1 to tag alignments of corresponding predicates in 70 text pairs derived from the created corpus.
</nextsent>
<nextsent>all document pairs were randomly chosen from the afp and apw sections of gigaword with the constraint that each text consists of 100 to 300 words2.
</nextsent>
<nextsent>we chose this constrain tas longer text pairs contain high number of unrelated predicates, making this task difficult to manage for the annotators.sure and possible links.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X765">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> a corpus for aligning predications </section>
<citcontext>
<prevsection>
<prevsent>we show instances of this phenomenon, in which aligned pas help to resolve implicit role references, in section 4.
</prevsent>
<prevsent>3.2 gold standard annotation.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
we pre-processed all texts using mate tools (bohnet, 2010; <papid> C10-1011 </papid>bjorkelund et al , 2010), pipeline of natural language processing modules including state-of-the-art semantic role labeler that computes prop/nombank annotations (palmer et al , 2005; <papid> J05-1004 </papid>meyers et al , 2008).</citsent>
<aftsection>
<nextsent>the output was used to providepre-labeled verbal and nominal predicates for annotation.
</nextsent>
<nextsent>we asked two students1 to tag alignments of corresponding predicates in 70 text pairs derived from the created corpus.
</nextsent>
<nextsent>all document pairs were randomly chosen from the afp and apw sections of gigaword with the constraint that each text consists of 100 to 300 words2.
</nextsent>
<nextsent>we chose this constrain tas longer text pairs contain high number of unrelated predicates, making this task difficult to manage for the annotators.sure and possible links.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X766">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> a corpus for aligning predications </section>
<citcontext>
<prevsection>
<prevsent>we chose this constrain tas longer text pairs contain high number of unrelated predicates, making this task difficult to manage for the annotators.sure and possible links.
</prevsent>
<prevsent>following standard practice in word alignment tasks (cf.
</prevsent>
</prevsection>
<citsent citstr=" J08-4005 ">
cohn et al  (2008)) <papid> J08-4005 </papid>1both annotators are students in computational linguistics, one undergraduate (a) and one postgraduate (b) student.</citsent>
<aftsection>
<nextsent>2this constraint is satisfied by 75.3% of the documents.
</nextsent>
<nextsent>220the annotators were instructed to distinguish between sure (s) and possible (p) alignments, depending on how certainly, in their opinion, two predicates (including their arguments) describe the sameevent.
</nextsent>
<nextsent>the following examples showcases of predicate pairings marked as sure (s link) (2) and as possible (p link) alignments (3):(2) a. the regulator ruled on september 27 that nasdaq too was qualified to bid for omx [.
</nextsent>
<nextsent>]3b.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X767">
<title id=" S12-1030.xml">aligning predicate argument structures in monolingual comparable texts a new corpus for a new task </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>a simple baseline for this task is to align all predicates whose lemmas are identical (samelemma).
</prevsent>
<prevsent>as more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (smt).
</prevsent>
</prevsection>
<citsent citstr=" N06-1014 ">
we train our own word alignment model using the state-of-the-art tool berkeley aligner (liang et al , 2006).<papid> N06-1014 </papid></citsent>
<aftsection>
<nextsent>as word alignment tools require pairs of sentences as input, we first extract paraphrases for this baseline using are-implementation of the paraphrase detection system by wan et al  (2006).
</nextsent>
<nextsent>in the following sections, we abbreviate this model as wordalign.
</nextsent>
<nextsent>5.3 results.
</nextsent>
<nextsent>following cohn et al  (2008) <papid> J08-4005 </papid>we measure precision as the number of predicted alignments also annotated in the gold standard divided by the total number of predictions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X769">
<title id=" S10-1085.xml">jaist clustering and classification based approaches for japanese wsd </title>
<section> japanese word sense disambiguation (wsd).  </section>
<citcontext>
<prevsection>
<prevsent>help that people who work in shop give you ?????????????
</prevsent>
<prevsent>help that is provided by business to customers ???
</prevsent>
</prevsection>
<citsent citstr=" W07-2002 ">
volunteer work dictionary instance (sentence) (a) (b) 2 1 3 figure 1: overview of jaist-1 2.1.1 clustering of word instances as previous work applying clustering techniques for sense induction (schutze, 1998; agirre and soroa, 2007), <papid> W07-2002 </papid>each instance is represented by feature vector.</citsent>
<aftsection>
<nextsent>in jaist-1, the following 4 vectors are used for clustering.collocation vector this vector reflects collocation including the target instance.
</nextsent>
<nextsent>words or poss appearing just before and after the target instance are used as features, i.e. they correspond to one dimension in the vector.
</nextsent>
<nextsent>the weight of each feature is 1 if the feature exists for the instance, or 0 if not.
</nextsent>
<nextsent>context vector the vector reflects words in the context of the target instance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X770">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation (wsd) aims to identify the sense of word in given context, using predefined sense inventory containing the words different meanings (navigli, 2009).
</prevsent>
<prevsent>traditionally, wsd approaches have assumed that each occurrence ofa word is best labeled with single sense.
</prevsent>
</prevsection>
<citsent citstr=" W06-2503 ">
how ever, human annotators often disagree about which sense is present (passonneau et al, 2010), especially in cases where some of the possible senses are closely related (chugur et al, 2002; mccarthy, 2006; <papid> W06-2503 </papid>palmer et al, 2007).</citsent>
<aftsection>
<nextsent>recently, erk et al (2009) <papid> P09-1002 </papid>have shown that in cases of sense ambiguity, graded notion of sense labeling may be most appropriate and help reduce the ambiguity.</nextsent>
<nextsent>specifically, within given context, multiple senses of word may be salient to the reader, with different levels of applicability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X771">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>traditionally, wsd approaches have assumed that each occurrence ofa word is best labeled with single sense.
</prevsent>
<prevsent>how ever, human annotators often disagree about which sense is present (passonneau et al, 2010), especially in cases where some of the possible senses are closely related (chugur et al, 2002; mccarthy, 2006; <papid> W06-2503 </papid>palmer et al, 2007).</prevsent>
</prevsection>
<citsent citstr=" P09-1002 ">
recently, erk et al (2009) <papid> P09-1002 </papid>have shown that in cases of sense ambiguity, graded notion of sense labeling may be most appropriate and help reduce the ambiguity.</citsent>
<aftsection>
<nextsent>specifically, within given context, multiple senses of word may be salient to the reader, with different levels of applicability.
</nextsent>
<nextsent>forex ample, in the sentence ? the athlete won the gold metal due to her hard work and dedication.
</nextsent>
<nextsent>multiple senses could be considered applicable for won?
</nextsent>
<nextsent>according to the wordnet 3.0 sense inventory (fellbaum, 1998):1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X775">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>189therefore, we consider the use of word sense induction (wsi) for gws annotation.
</prevsent>
<prevsent>wsi removes the need for substantial training data by automatically deriving words senses and associated sense features through examining its contextual uses.
</prevsent>
</prevsection>
<citsent citstr=" E12-1060 ">
furthermore, the data-driven sense discovery defines senses as they are present in the corpus, which may identify usages not present in traditional sense inventories (lau et al, 2012).<papid> E12-1060 </papid></citsent>
<aftsection>
<nextsent>last, many wsi models represent senses loosely as abstractions over usages, which potentially may transfer well to expressing gws annotations as blend of their sense usages.
</nextsent>
<nextsent>in this paper, we consider the performance ofwsi models on gws task.
</nextsent>
<nextsent>the contributions of this paper are as follows.
</nextsent>
<nextsent>first, in sec.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X776">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> evaluating gws annotations.  </section>
<citcontext>
<prevsection>
<prevsent>we separate the three objectives as way to evaluate how well different techniques perform on each aspect individually, which may encourage future work in ensemble wsd methods that use combinations of the techniques.
</prevsent>
<prevsent>figure 1 illustrates each evaluation on example annotations.
</prevsent>
</prevsection>
<citsent citstr=" D09-1046 ">
we note that erk and mccarthy (2009) <papid> D09-1046 </papid>have also proposed an alternate set of evaluation measures for gws annotations.</citsent>
<aftsection>
<nextsent>where applicable, we describe and compare their measures to ours for the three objectives.
</nextsent>
<nextsent>in the following definitions, let sig refer to the set of senses {s1, . . .
</nextsent>
<nextsent>, sn} present in context according to the gold standard, and similarly, let sil refer to the set of senses for context as labeled by wsd system using the same sense inventory.
</nextsent>
<nextsent>let peri(sj) refer to the perceived numeric applicability rating of sense sj in context i.detection measures the ability to accurately identify which senses are applicable in given context, independent of their applicability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X781">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> wsi models.  </section>
<citcontext>
<prevsection>
<prevsent>1 has jss1of 0.593, despite its significant differences in ordering and the omission of sense.
</prevsent>
<prevsent>indeed, in cases where the set of senses in test annotation is completely disjoint from the set of gold standard senses, the jss will be positive due to comparing the two distributions against their average; in contrast, the cosine similarity in such cases will be zero, which we argue better matches the expectation that such an annotation does not meet the perception objective.
</prevsent>
</prevsection>
<citsent citstr=" D10-1012 ">
for evaluation we adapt three recent graph-basedwsi methods for the task of graded-sense annota tion: navigli and crisafulli (2010), <papid> D10-1012 </papid>referred to as squares, jurgens (2011), <papid> W11-1104 </papid>referred to as link, and uoy (korkontzelos and manandhar, 2010).<papid> S10-1079 </papid></citsent>
<aftsection>
<nextsent>at an abstract level, these methods operate in two stages.
</nextsent>
<nextsent>first, graph is built, using either words or word pairs as vertices, and edges are added denoting some form of association between the vertices.
</nextsent>
<nextsent>second, senses are derived by clustering or partitioning thegraph.
</nextsent>
<nextsent>we selected these methods based on their superior performance on recent benchmarks and also 1the jsd is distance measure in [0, 1], which we convert to similarity jss = 1?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X782">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> wsi models.  </section>
<citcontext>
<prevsection>
<prevsent>1 has jss1of 0.593, despite its significant differences in ordering and the omission of sense.
</prevsent>
<prevsent>indeed, in cases where the set of senses in test annotation is completely disjoint from the set of gold standard senses, the jss will be positive due to comparing the two distributions against their average; in contrast, the cosine similarity in such cases will be zero, which we argue better matches the expectation that such an annotation does not meet the perception objective.
</prevsent>
</prevsection>
<citsent citstr=" W11-1104 ">
for evaluation we adapt three recent graph-basedwsi methods for the task of graded-sense annota tion: navigli and crisafulli (2010), <papid> D10-1012 </papid>referred to as squares, jurgens (2011), <papid> W11-1104 </papid>referred to as link, and uoy (korkontzelos and manandhar, 2010).<papid> S10-1079 </papid></citsent>
<aftsection>
<nextsent>at an abstract level, these methods operate in two stages.
</nextsent>
<nextsent>first, graph is built, using either words or word pairs as vertices, and edges are added denoting some form of association between the vertices.
</nextsent>
<nextsent>second, senses are derived by clustering or partitioning thegraph.
</nextsent>
<nextsent>we selected these methods based on their superior performance on recent benchmarks and also 1the jsd is distance measure in [0, 1], which we convert to similarity jss = 1?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X783">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> wsi models.  </section>
<citcontext>
<prevsection>
<prevsent>1 has jss1of 0.593, despite its significant differences in ordering and the omission of sense.
</prevsent>
<prevsent>indeed, in cases where the set of senses in test annotation is completely disjoint from the set of gold standard senses, the jss will be positive due to comparing the two distributions against their average; in contrast, the cosine similarity in such cases will be zero, which we argue better matches the expectation that such an annotation does not meet the perception objective.
</prevsent>
</prevsection>
<citsent citstr=" S10-1079 ">
for evaluation we adapt three recent graph-basedwsi methods for the task of graded-sense annota tion: navigli and crisafulli (2010), <papid> D10-1012 </papid>referred to as squares, jurgens (2011), <papid> W11-1104 </papid>referred to as link, and uoy (korkontzelos and manandhar, 2010).<papid> S10-1079 </papid></citsent>
<aftsection>
<nextsent>at an abstract level, these methods operate in two stages.
</nextsent>
<nextsent>first, graph is built, using either words or word pairs as vertices, and edges are added denoting some form of association between the vertices.
</nextsent>
<nextsent>second, senses are derived by clustering or partitioning thegraph.
</nextsent>
<nextsent>we selected these methods based on their superior performance on recent benchmarks and also 1the jsd is distance measure in [0, 1], which we convert to similarity jss = 1?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X790">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> wsi models.  </section>
<citcontext>
<prevsection>
<prevsent>once the set of communities is produced, communities with three or fewer vertices are removed, under the assumption that these communities contain too few features to reliably disambiguate.senses are disambiguated by finding the community with the largest overlap score, computed as the weighted jaccard index.
</prevsent>
<prevsent>for context with the set of features fi and community with features fj , the overlap is measured as |fj | ? |fifj ||fifj | . we adapt this algorithm in three ways.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
first, rather than use co-occurrence frequency to weight edges between terms, we weight edges accord to their statistical association with the g-test (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>the g-test weighting helps remove edges whose large edge weights are due to high corpus frequency but provide no disambiguating information, and the weighting also allows the ? parameter to be more consistently set across corpora of different sizes.
</nextsent>
<nextsent>second, while jurgens (2011) <papid> W11-1104 </papid>used only nouns as vertices in the graph, we include both verbs and adjectives due to needing to identify senses for both.</nextsent>
<nextsent>third, for graded senses, we disambiguate context by reporting all overlapping communities, weighted by their overlap score.uoy korkontzelos and manandhar (2010) <papid> S10-1079 </papid>propose wsi model that builds graph for each term for disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X795">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> wsi models.  </section>
<citcontext>
<prevsection>
<prevsent>pairs with dice coefficient above p4 with either of its constituent terms are removed.
</prevsent>
<prevsent>last, edges are added between nouns and noun pairs according to their conditional probabilities of occurring with each other.
</prevsent>
</prevsection>
<citsent citstr=" W06-3812 ">
edges with conditional probability less than 192 p3 are not included.once the graph has been constructed, the chinese whispers graph partitioning algorithm (bie mann, 2006) <papid> W06-3812 </papid>is used to identify word senses.</citsent>
<aftsection>
<nextsent>each graph partition is assigned separate sense of w. next, each partition is mapped to the set of contexts in the reference corpus in which at least one of its vertices occurs.
</nextsent>
<nextsent>partitions whose context sets are astrict subset of another are merged with the subsuming partition.
</nextsent>
<nextsent>word sense disambiguation occurs by counting the number of overlapping vertices for each partition and selecting the partition with the highest overlap as the sense of w. we extend this to graded annotation by selecting all partitions with at least one vertex present and set the applicability equal to the degree of overlap.
</nextsent>
<nextsent>directly comparing gws annotations from the induced and gold standard sense inventories requires first creating mapping from the induced senses tothe gold standard inventory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X796">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> evaluation across sense inventories.  </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation occurs by counting the number of overlapping vertices for each partition and selecting the partition with the highest overlap as the sense of w. we extend this to graded annotation by selecting all partitions with at least one vertex present and set the applicability equal to the degree of overlap.
</prevsent>
<prevsent>directly comparing gws annotations from the induced and gold standard sense inventories requires first creating mapping from the induced senses tothe gold standard inventory.
</prevsent>
</prevsection>
<citsent citstr=" W06-3814 ">
agirre et al (2006) <papid> W06-3814 </papid>propose sense-mapping procedure, which was used in the previous two semeval wsi tasks (agirre and soroa, 2007; <papid> W07-2002 </papid>manandhar et al, 2010).<papid> S10-1011 </papid></citsent>
<aftsection>
<nextsent>we consider this procedure and two extensions of it to support learning mapping from graded sense annotations.
</nextsent>
<nextsent>the procedure of agirre et al (2006) <papid> W06-3814 </papid>uses three corpora: (1) base corpus from which the senses are derived, (2) mapping corpus annotated with both gold standard senses, denoted gs, and induced senses, denoted is, and (3) test corpus annotated with is senses that will be converted to gs senses.once the senses are induced from the base corpus, the mapping corpus is annotated with is sense sand matrix is built where cell i, initially contains the counts of each time gsj and isi were used to label the same instance.</nextsent>
<nextsent>the rows of this matrix are then normalized such that each cell now represents p(gsj|isi).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X800">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> evaluation across sense inventories.  </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation occurs by counting the number of overlapping vertices for each partition and selecting the partition with the highest overlap as the sense of w. we extend this to graded annotation by selecting all partitions with at least one vertex present and set the applicability equal to the degree of overlap.
</prevsent>
<prevsent>directly comparing gws annotations from the induced and gold standard sense inventories requires first creating mapping from the induced senses tothe gold standard inventory.
</prevsent>
</prevsection>
<citsent citstr=" W07-2002 ">
agirre et al (2006) <papid> W06-3814 </papid>propose sense-mapping procedure, which was used in the previous two semeval wsi tasks (agirre and soroa, 2007; <papid> W07-2002 </papid>manandhar et al, 2010).<papid> S10-1011 </papid></citsent>
<aftsection>
<nextsent>we consider this procedure and two extensions of it to support learning mapping from graded sense annotations.
</nextsent>
<nextsent>the procedure of agirre et al (2006) <papid> W06-3814 </papid>uses three corpora: (1) base corpus from which the senses are derived, (2) mapping corpus annotated with both gold standard senses, denoted gs, and induced senses, denoted is, and (3) test corpus annotated with is senses that will be converted to gs senses.once the senses are induced from the base corpus, the mapping corpus is annotated with is sense sand matrix is built where cell i, initially contains the counts of each time gsj and isi were used to label the same instance.</nextsent>
<nextsent>the rows of this matrix are then normalized such that each cell now represents p(gsj|isi).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X802">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> evaluation across sense inventories.  </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation occurs by counting the number of overlapping vertices for each partition and selecting the partition with the highest overlap as the sense of w. we extend this to graded annotation by selecting all partitions with at least one vertex present and set the applicability equal to the degree of overlap.
</prevsent>
<prevsent>directly comparing gws annotations from the induced and gold standard sense inventories requires first creating mapping from the induced senses tothe gold standard inventory.
</prevsent>
</prevsection>
<citsent citstr=" S10-1011 ">
agirre et al (2006) <papid> W06-3814 </papid>propose sense-mapping procedure, which was used in the previous two semeval wsi tasks (agirre and soroa, 2007; <papid> W07-2002 </papid>manandhar et al, 2010).<papid> S10-1011 </papid></citsent>
<aftsection>
<nextsent>we consider this procedure and two extensions of it to support learning mapping from graded sense annotations.
</nextsent>
<nextsent>the procedure of agirre et al (2006) <papid> W06-3814 </papid>uses three corpora: (1) base corpus from which the senses are derived, (2) mapping corpus annotated with both gold standard senses, denoted gs, and induced senses, denoted is, and (3) test corpus annotated with is senses that will be converted to gs senses.once the senses are induced from the base corpus, the mapping corpus is annotated with is sense sand matrix is built where cell i, initially contains the counts of each time gsj and isi were used to label the same instance.</nextsent>
<nextsent>the rows of this matrix are then normalized such that each cell now represents p(gsj|isi).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X816">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>ratings were done using 5-point ordinal ranking according to the judgements from 1 ? this sense is not applicable to 5 ? this usage exactly reflects this sense.
</prevsent>
<prevsent>annotators used wide-range of responses, leading to many applicable senses per instance.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
we selected the subset of the gws dataset where each term has 50 annotated contexts, which were distributed evenly between semcor (miller et al., 1993) <papid> H93-1061 </papid>and the senseval-3 lexical substitution corpus (mihalcea et al, 2004).<papid> W04-0807 </papid></citsent>
<aftsection>
<nextsent>table 1 summarizes the target terms in this context.
</nextsent>
<nextsent>to prepare the data for evaluation, we constructed the gold standard gws annotations using the mean applicability ratings of all three annotators for each context.
</nextsent>
<nextsent>senses that received mean rating of 1 (not applicable) were not listed in gold standard labeling for that instance.
</nextsent>
<nextsent>all remaining responses were normalized to sum to 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X817">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>ratings were done using 5-point ordinal ranking according to the judgements from 1 ? this sense is not applicable to 5 ? this usage exactly reflects this sense.
</prevsent>
<prevsent>annotators used wide-range of responses, leading to many applicable senses per instance.
</prevsent>
</prevsection>
<citsent citstr=" W04-0807 ">
we selected the subset of the gws dataset where each term has 50 annotated contexts, which were distributed evenly between semcor (miller et al., 1993) <papid> H93-1061 </papid>and the senseval-3 lexical substitution corpus (mihalcea et al, 2004).<papid> W04-0807 </papid></citsent>
<aftsection>
<nextsent>table 1 summarizes the target terms in this context.
</nextsent>
<nextsent>to prepare the data for evaluation, we constructed the gold standard gws annotations using the mean applicability ratings of all three annotators for each context.
</nextsent>
<nextsent>senses that received mean rating of 1 (not applicable) were not listed in gold standard labeling for that instance.
</nextsent>
<nextsent>all remaining responses were normalized to sum to 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X823">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>first, our study only considered three graph based wsi models; future work is needed to assess the capabilities other wsi approaches, such as vector-based or bayesian.
</prevsent>
<prevsent>we are also interested in comparing the performance of the link model with other recently developed all-words wsi approaches such as van de cruys and apidianaki (2011).second, the proposed evaluation relies on supervised mapping to the gold standard sense inventory, which has potential to lose information and incorrectly map new senses not in the gold standard.
</prevsent>
</prevsection>
<citsent citstr=" D07-1043 ">
while unsupervised clustering evaluations such as the v-measure (rosenberg and hirschberg, 2007) <papid> D07-1043 </papid>and paired fscore (artiles et al, 2009) <papid> D09-1056 </papid>are capable of evaluating without such mapping, future work is needed to test extrinsic soft clustering evaluations such as bcubed (amigo?</citsent>
<aftsection>
<nextsent>et al, 2009) or develop analogous techniques that take into account graded class membership used in gws annotations.
</nextsent>
<nextsent>last, we note that our setup normalized the gwsratings into probability distribution, which is standard in the semeval evaluation setup.
</nextsent>
<nextsent>however, this normalization incorrectly transforms gws annotations where no predominant sense was rated at the highest value, e.g., an annotation of only two senses rated as 3 on scale of 1 to 5.
</nextsent>
<nextsent>while these perce pti bil ities may be left un normalized, it is not clear how to compare the induced gws annotations with such mid-interval values, or when the rating scale of the wsi system is potentially unbounded.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X824">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>first, our study only considered three graph based wsi models; future work is needed to assess the capabilities other wsi approaches, such as vector-based or bayesian.
</prevsent>
<prevsent>we are also interested in comparing the performance of the link model with other recently developed all-words wsi approaches such as van de cruys and apidianaki (2011).second, the proposed evaluation relies on supervised mapping to the gold standard sense inventory, which has potential to lose information and incorrectly map new senses not in the gold standard.
</prevsent>
</prevsection>
<citsent citstr=" D09-1056 ">
while unsupervised clustering evaluations such as the v-measure (rosenberg and hirschberg, 2007) <papid> D07-1043 </papid>and paired fscore (artiles et al, 2009) <papid> D09-1056 </papid>are capable of evaluating without such mapping, future work is needed to test extrinsic soft clustering evaluations such as bcubed (amigo?</citsent>
<aftsection>
<nextsent>et al, 2009) or develop analogous techniques that take into account graded class membership used in gws annotations.
</nextsent>
<nextsent>last, we note that our setup normalized the gwsratings into probability distribution, which is standard in the semeval evaluation setup.
</nextsent>
<nextsent>however, this normalization incorrectly transforms gws annotations where no predominant sense was rated at the highest value, e.g., an annotation of only two senses rated as 3 on scale of 1 to 5.
</nextsent>
<nextsent>while these perce pti bil ities may be left un normalized, it is not clear how to compare the induced gws annotations with such mid-interval values, or when the rating scale of the wsi system is potentially unbounded.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X825">
<title id=" S12-1027.xml">an evaluation of graded sense disambiguation using word sense induction </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>while these perce pti bil ities may be left un normalized, it is not clear how to compare the induced gws annotations with such mid-interval values, or when the rating scale of the wsi system is potentially unbounded.
</prevsent>
<prevsent>future workis needed both in gws evaluation and in quantifying applicability along range in gws-based wsi systems to address this issue.
</prevsent>
</prevsection>
<citsent citstr=" P10-4006 ">
all models and data will be released as part of the s-space package (jurgens and stevens, 2010).<papid> P10-4006 </papid>3 3https://github.com/fozziethebeat/s-space 196</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X826">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they form se-mantic units which are usually called discourse seg-ments.
</prevsent>
<prevsent>the global discourse structure of text can be constructed by relating the discourse seg-ments with each other.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
therefore, identifying seg-ment boundaries in text is considered as first step to construct he discourse structure(grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>the use of surface linguistic cues in text for identification of segment boundaries has been exten-sively researched, since it is impractical to assume the use of world knowledge for discourse analysis of real texts.
</nextsent>
<nextsent>among variety of surface cues, lexi-cal cohesion(halliday nd hasan, 1976), the surface relationship among words that are semantically sim-ilar, has recently received much attention and has been widely used for text segmentation(morris and hirst, 1991; <papid> J91-1002 </papid>kozima, 1993; <papid> P93-1041 </papid>hearst, 1994; <papid> P94-1002 </papid>okumura and honda, 1994).<papid> C94-2121 </papid></nextsent>
<nextsent>okumura and honda (okumura and honda, 1994) <papid> C94-2121 </papid>found that the information of lexi-cal cohesion is not enough and incorporation of other surface information may improve the accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X827">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, identifying seg-ment boundaries in text is considered as first step to construct he discourse structure(grosz and sidner, 1986).<papid> J86-3001 </papid></prevsent>
<prevsent>the use of surface linguistic cues in text for identification of segment boundaries has been exten-sively researched, since it is impractical to assume the use of world knowledge for discourse analysis of real texts.</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
among variety of surface cues, lexi-cal cohesion(halliday nd hasan, 1976), the surface relationship among words that are semantically sim-ilar, has recently received much attention and has been widely used for text segmentation(morris and hirst, 1991; <papid> J91-1002 </papid>kozima, 1993; <papid> P93-1041 </papid>hearst, 1994; <papid> P94-1002 </papid>okumura and honda, 1994).<papid> C94-2121 </papid></citsent>
<aftsection>
<nextsent>okumura and honda (okumura and honda, 1994) <papid> C94-2121 </papid>found that the information of lexi-cal cohesion is not enough and incorporation of other surface information may improve the accuracy.</nextsent>
<nextsent>in this paper, we describe method for identi-fying segment boundaries of japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X828">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, identifying seg-ment boundaries in text is considered as first step to construct he discourse structure(grosz and sidner, 1986).<papid> J86-3001 </papid></prevsent>
<prevsent>the use of surface linguistic cues in text for identification of segment boundaries has been exten-sively researched, since it is impractical to assume the use of world knowledge for discourse analysis of real texts.</prevsent>
</prevsection>
<citsent citstr=" P93-1041 ">
among variety of surface cues, lexi-cal cohesion(halliday nd hasan, 1976), the surface relationship among words that are semantically sim-ilar, has recently received much attention and has been widely used for text segmentation(morris and hirst, 1991; <papid> J91-1002 </papid>kozima, 1993; <papid> P93-1041 </papid>hearst, 1994; <papid> P94-1002 </papid>okumura and honda, 1994).<papid> C94-2121 </papid></citsent>
<aftsection>
<nextsent>okumura and honda (okumura and honda, 1994) <papid> C94-2121 </papid>found that the information of lexi-cal cohesion is not enough and incorporation of other surface information may improve the accuracy.</nextsent>
<nextsent>in this paper, we describe method for identi-fying segment boundaries of japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X829">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, identifying seg-ment boundaries in text is considered as first step to construct he discourse structure(grosz and sidner, 1986).<papid> J86-3001 </papid></prevsent>
<prevsent>the use of surface linguistic cues in text for identification of segment boundaries has been exten-sively researched, since it is impractical to assume the use of world knowledge for discourse analysis of real texts.</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
among variety of surface cues, lexi-cal cohesion(halliday nd hasan, 1976), the surface relationship among words that are semantically sim-ilar, has recently received much attention and has been widely used for text segmentation(morris and hirst, 1991; <papid> J91-1002 </papid>kozima, 1993; <papid> P93-1041 </papid>hearst, 1994; <papid> P94-1002 </papid>okumura and honda, 1994).<papid> C94-2121 </papid></citsent>
<aftsection>
<nextsent>okumura and honda (okumura and honda, 1994) <papid> C94-2121 </papid>found that the information of lexi-cal cohesion is not enough and incorporation of other surface information may improve the accuracy.</nextsent>
<nextsent>in this paper, we describe method for identi-fying segment boundaries of japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X830">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, identifying seg-ment boundaries in text is considered as first step to construct he discourse structure(grosz and sidner, 1986).<papid> J86-3001 </papid></prevsent>
<prevsent>the use of surface linguistic cues in text for identification of segment boundaries has been exten-sively researched, since it is impractical to assume the use of world knowledge for discourse analysis of real texts.</prevsent>
</prevsection>
<citsent citstr=" C94-2121 ">
among variety of surface cues, lexi-cal cohesion(halliday nd hasan, 1976), the surface relationship among words that are semantically sim-ilar, has recently received much attention and has been widely used for text segmentation(morris and hirst, 1991; <papid> J91-1002 </papid>kozima, 1993; <papid> P93-1041 </papid>hearst, 1994; <papid> P94-1002 </papid>okumura and honda, 1994).<papid> C94-2121 </papid></citsent>
<aftsection>
<nextsent>okumura and honda (okumura and honda, 1994) <papid> C94-2121 </papid>found that the information of lexi-cal cohesion is not enough and incorporation of other surface information may improve the accuracy.</nextsent>
<nextsent>in this paper, we describe method for identi-fying segment boundaries of japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X832">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>okumura and honda (okumura and honda, 1994) <papid> C94-2121 </papid>found that the information of lexi-cal cohesion is not enough and incorporation of other surface information may improve the accuracy.</prevsent>
<prevsent>in this paper, we describe method for identi-fying segment boundaries of japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion.</prevsent>
</prevsection>
<citsent citstr=" J92-1001 ">
there are variety of methods for combining multiple knowledge sources (linguistic ues)(mcroy, 1992).<papid> J92-1001 </papid></citsent>
<aftsection>
<nextsent>among them, weighted sum of the scores for all cues that reflects their contribution to identifying the correct segment boundaries is often used as the overall measure to rank the possible segment bound-aries.
</nextsent>
<nextsent>in the past researches (kurohashi and nagao, 1994; <papid> C94-2183 </papid>cohen, 1987), <papid> J87-1002 </papid>the weights for each cue tend to be determined by intuition or trial and error.</nextsent>
<nextsent>since determining weights by hand is labor-intensive task and the weights do not always to achieve optimal or even near-optimal performance(rayner et al, 1994), <papid> H94-1040 </papid>we think it is better to determine the weights auto-matically in order to both avoid the need forex- pert hand tuning and achieve performance that is at least locally optimal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X833">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are variety of methods for combining multiple knowledge sources (linguistic ues)(mcroy, 1992).<papid> J92-1001 </papid></prevsent>
<prevsent>among them, weighted sum of the scores for all cues that reflects their contribution to identifying the correct segment boundaries is often used as the overall measure to rank the possible segment bound- aries.</prevsent>
</prevsection>
<citsent citstr=" C94-2183 ">
in the past researches (kurohashi and nagao, 1994; <papid> C94-2183 </papid>cohen, 1987), <papid> J87-1002 </papid>the weights for each cue tend to be determined by intuition or trial and error.</citsent>
<aftsection>
<nextsent>since determining weights by hand is labor-intensive task and the weights do not always to achieve optimal or even near-optimal performance(rayner et al, 1994), <papid> H94-1040 </papid>we think it is better to determine the weights auto-matically in order to both avoid the need forex- pert hand tuning and achieve performance that is at least locally optimal.</nextsent>
<nextsent>we begin by assuming the existence of training texts with the correct segment boundaries and use the method of multiple regres-sion analysis for automatically training the weights.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X834">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are variety of methods for combining multiple knowledge sources (linguistic ues)(mcroy, 1992).<papid> J92-1001 </papid></prevsent>
<prevsent>among them, weighted sum of the scores for all cues that reflects their contribution to identifying the correct segment boundaries is often used as the overall measure to rank the possible segment bound- aries.</prevsent>
</prevsection>
<citsent citstr=" J87-1002 ">
in the past researches (kurohashi and nagao, 1994; <papid> C94-2183 </papid>cohen, 1987), <papid> J87-1002 </papid>the weights for each cue tend to be determined by intuition or trial and error.</citsent>
<aftsection>
<nextsent>since determining weights by hand is labor-intensive task and the weights do not always to achieve optimal or even near-optimal performance(rayner et al, 1994), <papid> H94-1040 </papid>we think it is better to determine the weights auto-matically in order to both avoid the need forex- pert hand tuning and achieve performance that is at least locally optimal.</nextsent>
<nextsent>we begin by assuming the existence of training texts with the correct segment boundaries and use the method of multiple regres-sion analysis for automatically training the weights.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X835">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>among them, weighted sum of the scores for all cues that reflects their contribution to identifying the correct segment boundaries is often used as the overall measure to rank the possible segment bound-aries.
</prevsent>
<prevsent>in the past researches (kurohashi and nagao, 1994; <papid> C94-2183 </papid>cohen, 1987), <papid> J87-1002 </papid>the weights for each cue tend to be determined by intuition or trial and error.</prevsent>
</prevsection>
<citsent citstr=" H94-1040 ">
since determining weights by hand is labor-intensive task and the weights do not always to achieve optimal or even near-optimal performance(rayner et al, 1994), <papid> H94-1040 </papid>we think it is better to determine the weights auto-matically in order to both avoid the need forex- pert hand tuning and achieve performance that is at least locally optimal.</citsent>
<aftsection>
<nextsent>we begin by assuming the existence of training texts with the correct segment boundaries and use the method of multiple regres-sion analysis for automatically training the weights.
</nextsent>
<nextsent>however, there is well-known problem in the meth-ods of automatically training the weights, that the weights tend to be over fitted to the training data.
</nextsent>
<nextsent>in such case, the weights cause the degrade of the performance for other texts.
</nextsent>
<nextsent>it is considered that the over fitting problem is caused by the relatively large number of the parameters (linguistic ues) compared with the size of the training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X837">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> surface linguistic cues for.  </section>
<citcontext>
<prevsection>
<prevsent>if we can give the better value to s(n, + 1) that reflects the real phenomena in the texts more precisely, we think we can expect the better performance.
</prevsent>
<prevsent>however, since we have only the correct segment boundaries that are tagged to the training texts, we decide to give 10 each s(n, + 1) of the segment boundary point and -1 to the non-boundary point.
</prevsent>
</prevsection>
<citsent citstr=" C96-2164 ">
these values were decided by the results of the preliminary experiment with four types of s. watanabe(watanabe, 1996) <papid> C96-2164 </papid>can be considered as related work.</citsent>
<aftsection>
<nextsent>he describes system which auto-matically creates an abstract of newspaper article by selecting important sentences of given text.
</nextsent>
<nextsent>he applies the multiple regression analysis for weight-ing the surface features of sentence in order to determine the importance of sentences.
</nextsent>
<nextsent>each of sentence in training texts is given score that the number of human subjects who judge the sentence as important, divided by the number of all subjects.
</nextsent>
<nextsent>we do not adopt the same method forgiving value to s, because we think that such task by human subjects is labor-intensive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X838">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> surface linguistic cues for.  </section>
<citcontext>
<prevsection>
<prevsent>answer from five human subjects.
</prevsent>
<prevsent>by this ex-.
</prevsent>
</prevsection>
<citsent citstr=" P92-1032 ">
peri ment, we try to clarify the upper bound of the performance of the text segmentation task, which can be considered to indicate the degree of the difficulty of the task(passonneau nd lit-man, 1993; gale et al, 1992).<papid> P92-1032 </papid></citsent>
<aftsection>
<nextsent>figure 1,2 and table 1 show the results of the ex-periments.
</nextsent>
<nextsent>two figures show the system mean per-formance of 14 texts.
</nextsent>
<nextsent>table 1 shows the 5 subjects  mean performance of 14 texts (experiment 6).
</nextsent>
<nextsent>we think table 1 shows the upper bound of the perfor-mance of the text segmentation task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X839">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> surface linguistic cues for.  </section>
<citcontext>
<prevsection>
<prevsent>as future work, we now plan to calculate the weights for subset of the texts by clustering the training texts.
</prevsent>
<prevsent>since there may be some differences among real texts which reflect the differences of their author, their style, their genre, etc., we think that clustering set of the training texts and calculat-ing the weights for each cluster, rather than calcu-lating the weights for the entire set of texts, might improve the accuracy.
</prevsent>
</prevsection>
<citsent citstr=" A94-1010 ">
in the area of speech recogni-tion, to improve the accuracy of the language mod-els, clustering the training data is considered to be promising method for automatic training(carter, 1994; <papid> A94-1010 </papid>iyer et al, 1994).<papid> H94-1014 </papid></citsent>
<aftsection>
<nextsent>carter presents method for clustering the sentences in training corpus au-tomatically into some sub corpora on the criterion of entropy reduction and calculating separate language model parameters for each cluster.
</nextsent>
<nextsent>he asserts that this kind of clustering offers way to improve the performance of model significantly.
</nextsent>
<nextsent>acknowledgments the authors would like to express our gratitude to kadokawa publisher for allowing us to use their thesaurus, and dr.shigenobu aoki of gunma univ. and dr.teruo matsuzawa of ja ist for their sugges-tions of statistical analysis, and dr.thanaruk theer-amunkong of jaist for his suggestions of improve-ments to this paper.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X840">
<title id=" P98-2145.xml">text segmentation with multiple surface linguistic cues </title>
<section> surface linguistic cues for.  </section>
<citcontext>
<prevsection>
<prevsent>as future work, we now plan to calculate the weights for subset of the texts by clustering the training texts.
</prevsent>
<prevsent>since there may be some differences among real texts which reflect the differences of their author, their style, their genre, etc., we think that clustering set of the training texts and calculat-ing the weights for each cluster, rather than calcu-lating the weights for the entire set of texts, might improve the accuracy.
</prevsent>
</prevsection>
<citsent citstr=" H94-1014 ">
in the area of speech recogni-tion, to improve the accuracy of the language mod-els, clustering the training data is considered to be promising method for automatic training(carter, 1994; <papid> A94-1010 </papid>iyer et al, 1994).<papid> H94-1014 </papid></citsent>
<aftsection>
<nextsent>carter presents method for clustering the sentences in training corpus au-tomatically into some sub corpora on the criterion of entropy reduction and calculating separate language model parameters for each cluster.
</nextsent>
<nextsent>he asserts that this kind of clustering offers way to improve the performance of model significantly.
</nextsent>
<nextsent>acknowledgments the authors would like to express our gratitude to kadokawa publisher for allowing us to use their thesaurus, and dr.shigenobu aoki of gunma univ. and dr.teruo matsuzawa of ja ist for their sugges-tions of statistical analysis, and dr.thanaruk theer-amunkong of jaist for his suggestions of improve-ments to this paper.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X841">
<title id=" S12-1097.xml">universityofsheffield two approaches to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>results from the formal evaluation show that both approaches are useful for determining the similarity in meaning between pairs of sentences with the best performance being obtained bythe supervised approach.
</prevsent>
<prevsent>incorporating information from wordnet al improves performance for both approaches.
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
this paper describes the university of sheffields submission to semeval-2012 task 6: semantic text similarity (agirre et al , 2012).<papid> S12-1051 </papid></citsent>
<aftsection>
<nextsent>the task is concerned with determining the degree of semantic equivalence between pair of sentences.
</nextsent>
<nextsent>measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (seo and croft, 2008; bendersky and croft, 2009), textual entailment (szpektor et al , 2004; <papid> W04-3206 </papid>zanzotto et al , 2009), paraphrase detection(barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al , 2004), <papid> C04-1051 </papid>information extraction/question answering (lin andpantel, 2001; stevenson and greenwood, 2005), <papid> P05-1047 </papid>information retrieval (baeza-yates and ribeiro-neto, 1999), short answer grading (pulman and sukkarieh,2005; <papid> W05-0202 </papid>mohler and mihalcea, 2009), <papid> E09-1065 </papid>recommendation (tintarev and masthoff, 2006) and evaluation (papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></nextsent>
<nextsent>many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example (baeza-yates and ribeiro-neto, 1999; papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X842">
<title id=" S12-1097.xml">universityofsheffield two approaches to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the university of sheffields submission to semeval-2012 task 6: semantic text similarity (agirre et al , 2012).<papid> S12-1051 </papid></prevsent>
<prevsent>the task is concerned with determining the degree of semantic equivalence between pair of sentences.</prevsent>
</prevsection>
<citsent citstr=" W04-3206 ">
measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (seo and croft, 2008; bendersky and croft, 2009), textual entailment (szpektor et al , 2004; <papid> W04-3206 </papid>zanzotto et al , 2009), paraphrase detection(barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al , 2004), <papid> C04-1051 </papid>information extraction/question answering (lin andpantel, 2001; stevenson and greenwood, 2005), <papid> P05-1047 </papid>information retrieval (baeza-yates and ribeiro-neto, 1999), short answer grading (pulman and sukkarieh,2005; <papid> W05-0202 </papid>mohler and mihalcea, 2009), <papid> E09-1065 </papid>recommendation (tintarev and masthoff, 2006) and evaluation (papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example (baeza-yates and ribeiro-neto, 1999; papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></nextsent>
<nextsent>in these approaches the similarity of texts is computed as function of the number of matching tokens, or sequences of tokens, they contain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X843">
<title id=" S12-1097.xml">universityofsheffield two approaches to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the university of sheffields submission to semeval-2012 task 6: semantic text similarity (agirre et al , 2012).<papid> S12-1051 </papid></prevsent>
<prevsent>the task is concerned with determining the degree of semantic equivalence between pair of sentences.</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (seo and croft, 2008; bendersky and croft, 2009), textual entailment (szpektor et al , 2004; <papid> W04-3206 </papid>zanzotto et al , 2009), paraphrase detection(barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al , 2004), <papid> C04-1051 </papid>information extraction/question answering (lin andpantel, 2001; stevenson and greenwood, 2005), <papid> P05-1047 </papid>information retrieval (baeza-yates and ribeiro-neto, 1999), short answer grading (pulman and sukkarieh,2005; <papid> W05-0202 </papid>mohler and mihalcea, 2009), <papid> E09-1065 </papid>recommendation (tintarev and masthoff, 2006) and evaluation (papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example (baeza-yates and ribeiro-neto, 1999; papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></nextsent>
<nextsent>in these approaches the similarity of texts is computed as function of the number of matching tokens, or sequences of tokens, they contain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X844">
<title id=" S12-1097.xml">universityofsheffield two approaches to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the university of sheffields submission to semeval-2012 task 6: semantic text similarity (agirre et al , 2012).<papid> S12-1051 </papid></prevsent>
<prevsent>the task is concerned with determining the degree of semantic equivalence between pair of sentences.</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (seo and croft, 2008; bendersky and croft, 2009), textual entailment (szpektor et al , 2004; <papid> W04-3206 </papid>zanzotto et al , 2009), paraphrase detection(barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al , 2004), <papid> C04-1051 </papid>information extraction/question answering (lin andpantel, 2001; stevenson and greenwood, 2005), <papid> P05-1047 </papid>information retrieval (baeza-yates and ribeiro-neto, 1999), short answer grading (pulman and sukkarieh,2005; <papid> W05-0202 </papid>mohler and mihalcea, 2009), <papid> E09-1065 </papid>recommendation (tintarev and masthoff, 2006) and evaluation (papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example (baeza-yates and ribeiro-neto, 1999; papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></nextsent>
<nextsent>in these approaches the similarity of texts is computed as function of the number of matching tokens, or sequences of tokens, they contain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X845">
<title id=" S12-1097.xml">universityofsheffield two approaches to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the university of sheffields submission to semeval-2012 task 6: semantic text similarity (agirre et al , 2012).<papid> S12-1051 </papid></prevsent>
<prevsent>the task is concerned with determining the degree of semantic equivalence between pair of sentences.</prevsent>
</prevsection>
<citsent citstr=" P05-1047 ">
measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (seo and croft, 2008; bendersky and croft, 2009), textual entailment (szpektor et al , 2004; <papid> W04-3206 </papid>zanzotto et al , 2009), paraphrase detection(barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al , 2004), <papid> C04-1051 </papid>information extraction/question answering (lin andpantel, 2001; stevenson and greenwood, 2005), <papid> P05-1047 </papid>information retrieval (baeza-yates and ribeiro-neto, 1999), short answer grading (pulman and sukkarieh,2005; <papid> W05-0202 </papid>mohler and mihalcea, 2009), <papid> E09-1065 </papid>recommendation (tintarev and masthoff, 2006) and evaluation (papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example (baeza-yates and ribeiro-neto, 1999; papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></nextsent>
<nextsent>in these approaches the similarity of texts is computed as function of the number of matching tokens, or sequences of tokens, they contain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X846">
<title id=" S12-1097.xml">universityofsheffield two approaches to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the university of sheffields submission to semeval-2012 task 6: semantic text similarity (agirre et al , 2012).<papid> S12-1051 </papid></prevsent>
<prevsent>the task is concerned with determining the degree of semantic equivalence between pair of sentences.</prevsent>
</prevsection>
<citsent citstr=" W05-0202 ">
measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (seo and croft, 2008; bendersky and croft, 2009), textual entailment (szpektor et al , 2004; <papid> W04-3206 </papid>zanzotto et al , 2009), paraphrase detection(barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al , 2004), <papid> C04-1051 </papid>information extraction/question answering (lin andpantel, 2001; stevenson and greenwood, 2005), <papid> P05-1047 </papid>information retrieval (baeza-yates and ribeiro-neto, 1999), short answer grading (pulman and sukkarieh,2005; <papid> W05-0202 </papid>mohler and mihalcea, 2009), <papid> E09-1065 </papid>recommendation (tintarev and masthoff, 2006) and evaluation (papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example (baeza-yates and ribeiro-neto, 1999; papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></nextsent>
<nextsent>in these approaches the similarity of texts is computed as function of the number of matching tokens, or sequences of tokens, they contain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X847">
<title id=" S12-1097.xml">universityofsheffield two approaches to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the university of sheffields submission to semeval-2012 task 6: semantic text similarity (agirre et al , 2012).<papid> S12-1051 </papid></prevsent>
<prevsent>the task is concerned with determining the degree of semantic equivalence between pair of sentences.</prevsent>
</prevsection>
<citsent citstr=" E09-1065 ">
measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (seo and croft, 2008; bendersky and croft, 2009), textual entailment (szpektor et al , 2004; <papid> W04-3206 </papid>zanzotto et al , 2009), paraphrase detection(barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al , 2004), <papid> C04-1051 </papid>information extraction/question answering (lin andpantel, 2001; stevenson and greenwood, 2005), <papid> P05-1047 </papid>information retrieval (baeza-yates and ribeiro-neto, 1999), short answer grading (pulman and sukkarieh,2005; <papid> W05-0202 </papid>mohler and mihalcea, 2009), <papid> E09-1065 </papid>recommendation (tintarev and masthoff, 2006) and evaluation (papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example (baeza-yates and ribeiro-neto, 1999; papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></nextsent>
<nextsent>in these approaches the similarity of texts is computed as function of the number of matching tokens, or sequences of tokens, they contain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X848">
<title id=" S12-1097.xml">universityofsheffield two approaches to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the university of sheffields submission to semeval-2012 task 6: semantic text similarity (agirre et al , 2012).<papid> S12-1051 </papid></prevsent>
<prevsent>the task is concerned with determining the degree of semantic equivalence between pair of sentences.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (seo and croft, 2008; bendersky and croft, 2009), textual entailment (szpektor et al , 2004; <papid> W04-3206 </papid>zanzotto et al , 2009), paraphrase detection(barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al , 2004), <papid> C04-1051 </papid>information extraction/question answering (lin andpantel, 2001; stevenson and greenwood, 2005), <papid> P05-1047 </papid>information retrieval (baeza-yates and ribeiro-neto, 1999), short answer grading (pulman and sukkarieh,2005; <papid> W05-0202 </papid>mohler and mihalcea, 2009), <papid> E09-1065 </papid>recommendation (tintarev and masthoff, 2006) and evaluation (papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example (baeza-yates and ribeiro-neto, 1999; papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></nextsent>
<nextsent>in these approaches the similarity of texts is computed as function of the number of matching tokens, or sequences of tokens, they contain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X850">
<title id=" S12-1097.xml">universityofsheffield two approaches to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the university of sheffields submission to semeval-2012 task 6: semantic text similarity (agirre et al , 2012).<papid> S12-1051 </papid></prevsent>
<prevsent>the task is concerned with determining the degree of semantic equivalence between pair of sentences.</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (seo and croft, 2008; bendersky and croft, 2009), textual entailment (szpektor et al , 2004; <papid> W04-3206 </papid>zanzotto et al , 2009), paraphrase detection(barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al , 2004), <papid> C04-1051 </papid>information extraction/question answering (lin andpantel, 2001; stevenson and greenwood, 2005), <papid> P05-1047 </papid>information retrieval (baeza-yates and ribeiro-neto, 1999), short answer grading (pulman and sukkarieh,2005; <papid> W05-0202 </papid>mohler and mihalcea, 2009), <papid> E09-1065 </papid>recommendation (tintarev and masthoff, 2006) and evaluation (papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example (baeza-yates and ribeiro-neto, 1999; papineni et al , 2002; <papid> P02-1040 </papid>lin, 2004).<papid> W04-1013 </papid></nextsent>
<nextsent>in these approaches the similarity of texts is computed as function of the number of matching tokens, or sequences of tokens, they contain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X857">
<title id=" S12-1097.xml">universityofsheffield two approaches to semantic text similarity </title>
<section> computing semantic text similarity.  </section>
<citcontext>
<prevsection>
<prevsent>in the sentences the dog sat on the mat?
</prevsent>
<prevsent>and the hound sat on the mat?.
</prevsent>
</prevsection>
<citsent citstr=" W06-2501 ">
to take account of these similarities wordnet-based similarity measures are used (patwardhan and pedersen, 2006).<papid> W06-2501 </papid></citsent>
<aftsection>
<nextsent>any terms that occur in only one of the sentences do not contribute to the similarity score since they will have 0 value in the binary vector.
</nextsent>
<nextsent>any words with 0 value in one of the binary vectors are compared with all of the words in the other sentence andthe similarity values computed.
</nextsent>
<nextsent>the highest similarity value is selected and use to replace the 0 value in that vector, see figure 1.
</nextsent>
<nextsent>(if the similarity score is below the set threshold of 0.5 then the similarity value is not used and in these cases the 0 value remains unaltered.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X858">
<title id=" S12-1097.xml">universityofsheffield two approaches to semantic text similarity </title>
<section> computing semantic text similarity.  </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation two simple and commonly used techniques for word sense disambiguation were applied.
</prevsent>
<prevsent>most frequent sense (mfs) simply selects the first sense in wordnet, i.e., the most common occurring sense for the word.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
this approach is commonly used as baseline for word sense disambiguation (mccarthy et al , 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>lesk (1986) chooses synset by comparing its definition against the sentence and selecting the one with the highest number of words in common.
</nextsent>
<nextsent>similarity measures wordnet-based similarity measures have been found to perform well when used in combination with text similarity measures (mihalcea and corley, 2006) and several of these were compared.
</nextsent>
<nextsent>implementations of these measures from the nltk (bird et al , 2009) were used.
</nextsent>
<nextsent>path distance uses the length of the shortest path between two senses to determine the similarity between them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X859">
<title id=" S10-1080.xml">hermit flexible clustering for the semeval2 wsi task </title>
<section> the word sense induction model.  </section>
<citcontext>
<prevsection>
<prevsent>if the nearest centro id has similarity less than the cluster threshold and there are not clusters, the context forms new cluster.
</prevsent>
<prevsent>we define the similarity between contexts vectors as the cosine similarity.
</prevsent>
</prevsection>
<citsent citstr=" W97-0322 ">
once the corpus has been processed, clusters are repeatedly merged using hac with the average link criteria, following (pedersen and bruce,1997).<papid> W97-0322 </papid></citsent>
<aftsection>
<nextsent>average link clustering defines cluster similarity as the mean cosine similarity of the pairwise similarity of all data points from each cluster.
</nextsent>
<nextsent>cluster merging stops when the two most similar clusters have similarity less than the cluster threshold.
</nextsent>
<nextsent>reaching similarity lower than the cluster threshold signifies that each cluster represents distinct word sense.
</nextsent>
<nextsent>2.3 applying sense labels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X860">
<title id=" S10-1080.xml">hermit flexible clustering for the semeval2 wsi task </title>
<section> evaluation and results.  </section>
<citcontext>
<prevsection>
<prevsent>the supervised method is measured using recall.
</prevsent>
<prevsent>3.1 scoring.
</prevsent>
</prevsection>
<citsent citstr=" D07-1043 ">
the first measure used is the v-measure (rosen berg and hirschberg, 2007), <papid> D07-1043 </papid>which compares the clusters of target contexts to word classes.</citsent>
<aftsection>
<nextsent>this measure rates the homogeneity and completeness of clustering solution.
</nextsent>
<nextsent>solutions that have word clusters formed from one word class are homoge neous; completeness measures the degree to whicha word class is composed of target contexts allocated to single cluster.the second measure, the f-score, is an extension from information retrieval and provides contrasting evaluation metric by using different interpretation of homogeneity and completeness.for the f-score, the precision and recall of all possible context pairs are measured, where word class has the expected context pairs and provided solution contains some word pairs that are correct and others that are unexpected.
</nextsent>
<nextsent>the f-score tendsto discount smaller clusters and clusters that can not be assigned to word class (manandhar et al, 2010).
</nextsent>
<nextsent>3.2 parameter tuning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X861">
<title id=" S12-1057.xml">mixcd system description for evaluating chinese word similarity at semeval2012 </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it is called node-based method.
</prevsent>
<prevsent>(leacock and chodorow, 1998) develops similarity measure based on the distance of two senses and . they focus on hypernymy links and scaled the path length by the overall depth of the tree.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
( ) ( ) (wu and palmer, 1994) <papid> P94-1019 </papid>combines the depth of the lcs of two concepts into similarity score.</citsent>
<aftsection>
<nextsent>( ) ( ( )) ( ) ( ) these approaches are regarded as edge-based methods.
</nextsent>
<nextsent>they are more natural and direct to evaluating semantic similarity in taxonomy.
</nextsent>
<nextsent>but they treat all nodes as the same and do not consider the different information of different nodes.
</nextsent>
<nextsent>(jiang and conrath, 1998) uses the information content of concept instead of its depth.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X862">
<title id=" S12-1057.xml">mixcd system description for evaluating chinese word similarity at semeval2012 </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(jiang and conrath, 1998) uses the information content of concept instead of its depth.
</prevsent>
<prevsent>so both node and edge information can be considered to evaluate the similarity.
</prevsent>
</prevsection>
<citsent citstr=" W05-1203 ">
it performs well in evaluating semantic similarity between two texts (zhang et al, 2008; corley and mihalcea, 2005; <papid> W05-1203 </papid>pedersen, 2010).<papid> N10-1047 </papid></citsent>
<aftsection>
<nextsent>( ) ( ) ( ) ( ( ))semcor is used in jiang work to get the frequency of word with specific sense treated by the lagrange smoothing.
</nextsent>
<nextsent>for semeval-2012 task 4, we use two mrds and one corpus as our knowledge resources.
</nextsent>
<nextsent>one mrd is hit ir-lab tongyici cilin (extended) (cilin) and the other is chinese concept dictionary (ccd).
</nextsent>
<nextsent>the corpus we used in our system is people daily.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X863">
<title id=" S12-1057.xml">mixcd system description for evaluating chinese word similarity at semeval2012 </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(jiang and conrath, 1998) uses the information content of concept instead of its depth.
</prevsent>
<prevsent>so both node and edge information can be considered to evaluate the similarity.
</prevsent>
</prevsection>
<citsent citstr=" N10-1047 ">
it performs well in evaluating semantic similarity between two texts (zhang et al, 2008; corley and mihalcea, 2005; <papid> W05-1203 </papid>pedersen, 2010).<papid> N10-1047 </papid></citsent>
<aftsection>
<nextsent>( ) ( ) ( ) ( ( ))semcor is used in jiang work to get the frequency of word with specific sense treated by the lagrange smoothing.
</nextsent>
<nextsent>for semeval-2012 task 4, we use two mrds and one corpus as our knowledge resources.
</nextsent>
<nextsent>one mrd is hit ir-lab tongyici cilin (extended) (cilin) and the other is chinese concept dictionary (ccd).
</nextsent>
<nextsent>the corpus we used in our system is people daily.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X864">
<title id=" S10-1095.xml">umccdlsi integrative resource for disambiguation task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main purpose of this work is to evaluate and compare our computational resource of wordnets mappings using 3 different methods: relevant semantic tree, relevant semantic tree 2 and an adaptation of k-cliques technique.
</prevsent>
<prevsent>our proposal is non-supervised and knowledge-based system that uses domains ontology and sumo.
</prevsent>
</prevsection>
<citsent citstr=" W07-2073 ">
ambiguity is the task of building up multiple alternative linguistic structures for single input (kozareva et al, 2007).<papid> W07-2073 </papid></citsent>
<aftsection>
<nextsent>word sense disambiguation (wsd) is key enabling technology that automatically chooses the intended sense of word in context.
</nextsent>
<nextsent>in this task, one of the most used lexical database is wordnet (wn) (fellbaum, 1998).
</nextsent>
<nextsent>wn is an online lexical reference system whose design is inspired by current psycho linguistic theories of human lexical memory.
</nextsent>
<nextsent>due to the great popularity of wn in natural language processing (nlp), several authors (magnini and cavaglia, 2000), (niles and pease, 2001), (niles and pease, 2003), (valitutti, 2004) have proposed to incorporate to the semantic net of wn, some taxonomies that characterize, in one or several concepts, the senses of each word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X865">
<title id=" S10-1095.xml">umccdlsi integrative resource for disambiguation task </title>
<section> relevant semantic tree construction.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 relevant semantic tree.
</prevsent>
<prevsent>with this proposal we measure how much concept is correlated to the sentence, similar to reuters vector (magnini et al, 2002), but with different equation.
</prevsent>
</prevsection>
<citsent citstr=" C96-1005 ">
this proposal has partial similarity with the conceptual density (agirre and rigau, 1996) <papid> C96-1005 </papid>and dre levant (vazquez et al, 2004) to get the concepts from hierarchy that they associate with the sentence.</citsent>
<aftsection>
<nextsent>in order to determine the association ratio (ra) of domain in relation to the sentence, the equation 1 is used.
</nextsent>
<nextsent>ra(d, f) = ? i=1 ra(d, i ) (1) where: ra(d,w) = (d,w) ? log 2 (d,w) (d) (2) : is set of words w. i : is i-th word of the phrase . (d,w): is joint probability distribution.
</nextsent>
<nextsent>p (d): is marginal probability.
</nextsent>
<nextsent>from now, vectors are created using the senseval-2s corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X866">
<title id=" S12-1045.xml">fbk exploiting phrasal and contextual clues for negation scope detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system was developed for the participation in the task 1 (closed track) of the *sem 2012 shared task (resolving the scope and focus of negation), where it is ranked 3rd among the participating teams while attaining the highest f1 score for negation cue detection.
</prevsent>
<prevsent>negation is linguistic phenomenon that can alter the meaning of textual segment.
</prevsent>
</prevsection>
<citsent citstr=" W09-1105 ">
while automatic detection of negation expressions (i.e. cues) in free text has been subject of research interest for quite some time (e.g. chapman et al (2001), elkin et al (2005) etc), automatic detection of full scope of negation is relatively new topic (moranteand daelemans, 2009; <papid> W09-1105 </papid>councill et al, 2010).<papid> W10-3110 </papid></citsent>
<aftsection>
<nextsent>detection of negation cues, their scope and corresponding negated events in free text could improve accuracy in other natural language processing (nlp) tasks suchas extraction of factual information from text, sentiment analysis, etc (jia et al, 2009; councill et al, 2010).<papid> W10-3110 </papid>in this paper, we present system that was developed for the participation in the scope detection task of the *sem 2012 shared task1.</nextsent>
<nextsent>the proposed system exploits phrasal and contextual clues apart from various token specific features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X867">
<title id=" S12-1045.xml">fbk exploiting phrasal and contextual clues for negation scope detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system was developed for the participation in the task 1 (closed track) of the *sem 2012 shared task (resolving the scope and focus of negation), where it is ranked 3rd among the participating teams while attaining the highest f1 score for negation cue detection.
</prevsent>
<prevsent>negation is linguistic phenomenon that can alter the meaning of textual segment.
</prevsent>
</prevsection>
<citsent citstr=" W10-3110 ">
while automatic detection of negation expressions (i.e. cues) in free text has been subject of research interest for quite some time (e.g. chapman et al (2001), elkin et al (2005) etc), automatic detection of full scope of negation is relatively new topic (moranteand daelemans, 2009; <papid> W09-1105 </papid>councill et al, 2010).<papid> W10-3110 </papid></citsent>
<aftsection>
<nextsent>detection of negation cues, their scope and corresponding negated events in free text could improve accuracy in other natural language processing (nlp) tasks suchas extraction of factual information from text, sentiment analysis, etc (jia et al, 2009; councill et al, 2010).<papid> W10-3110 </papid>in this paper, we present system that was developed for the participation in the scope detection task of the *sem 2012 shared task1.</nextsent>
<nextsent>the proposed system exploits phrasal and contextual clues apart from various token specific features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X875">
<title id=" S12-1045.xml">fbk exploiting phrasal and contextual clues for negation scope detection </title>
<section> results and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>the other is to check whether predicted negated event is found inside the predicted scope of the corresponding negation cue.
</prevsent>
<prevsent>in this section, we discuss various empirical results on the development data and test data.
</prevsent>
</prevsection>
<citsent citstr=" S12-1035 ">
details regarding the evaluation criteria are described in morante and blanco (2012).<papid> S12-1035 </papid></citsent>
<aftsection>
<nextsent>4.1 results on the development dataset.
</nextsent>
<nextsent>our feature sets are selected after doing number of experiments by combining various potential feature types.
</nextsent>
<nextsent>in these experiments, the system is trained on the training data and tested on development data.
</nextsent>
<nextsent>feature name description lemma1 lemma of the 1st word of the sentence posi part-of-speech of tokeni lemmai lemma of tokeni posi1 pos of tokeni1 iscue if tokeni is negation cue iscuesubtoken if sub token of tokeni is negation cue isspeccharbetcueandcurtok if there is non-alphanumeric token between tokeni and cue ismodal if pos of tokeni is md isdt if pos of tokeni is dt iscueandcurtokindiffnp if tokeni and cue belong to different nps iscueandcurtokindiffvp if tokeni and cue belong to different vps iscueandcurtokindiffsorsbar if tokeni and cue belong to different or sbar belongtosamephrase if the least common phrase of tokeni and cue do not contain other phrase cpcatbetcueandcurtok all common phrase categories (and their counts) that contain tokeni and cue feature conjunctions new features by combining those of tokeni3 to tokeni+1 table 5: feature set for negated event classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X880">
<title id=" S12-1065.xml">ualacant using online machine translation for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this work we describe and evaluate different features derived from these sub-segment translations, which are used by support vector machine classifier to detectcltes.
</prevsent>
<prevsent>we presented this system to these meval 2012 task 8 obtaining an accuracy up to59.8% on the english spanish test set, the second best performing approach in the contest.
</prevsent>
</prevsection>
<citsent citstr=" N10-1045 ">
cross-lingual textual entailment (clte) detection (mehdad et al, 2010) <papid> N10-1045 </papid>is an extension of the textual entailment (te) detection (dagan et al, 2006) problem.</citsent>
<aftsection>
<nextsent>te detection consists of finding out, for two text fragments and in the same language, whether entails from semantic point of view or not.
</nextsent>
<nextsent>clte presents similar problem, but with and written in different languages.
</nextsent>
<nextsent>during the last years, many authors have focused on resolving te detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering (harabagiu and hickl, 2006) <papid> P06-1114 </papid>or machine translation (mt) (mirkin et al, 2009; <papid> P09-1089 </papid>pado?</nextsent>
<nextsent>et al, 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X881">
<title id=" S12-1065.xml">ualacant using online machine translation for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>te detection consists of finding out, for two text fragments and in the same language, whether entails from semantic point of view or not.
</prevsent>
<prevsent>clte presents similar problem, but with and written in different languages.
</prevsent>
</prevsection>
<citsent citstr=" P06-1114 ">
during the last years, many authors have focused on resolving te detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering (harabagiu and hickl, 2006) <papid> P06-1114 </papid>or machine translation (mt) (mirkin et al, 2009; <papid> P09-1089 </papid>pado?</citsent>
<aftsection>
<nextsent>et al, 2009).
</nextsent>
<nextsent>therefore, clte may also be useful for related tasks in which more than one language is involved, such as cross-lingual question answering or cross-lingual information retrieval.
</nextsent>
<nextsent>although clte detection isa relatively new problem, it has already been tackled.
</nextsent>
<nextsent>mehdad et al (2010) <papid> N10-1045 </papid>propose to use machine translation (mt) to translate from lh , the language of , into lt , the language of , and then use any of the state-of-the-art te approaches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X882">
<title id=" S12-1065.xml">ualacant using online machine translation for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>te detection consists of finding out, for two text fragments and in the same language, whether entails from semantic point of view or not.
</prevsent>
<prevsent>clte presents similar problem, but with and written in different languages.
</prevsent>
</prevsection>
<citsent citstr=" P09-1089 ">
during the last years, many authors have focused on resolving te detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering (harabagiu and hickl, 2006) <papid> P06-1114 </papid>or machine translation (mt) (mirkin et al, 2009; <papid> P09-1089 </papid>pado?</citsent>
<aftsection>
<nextsent>et al, 2009).
</nextsent>
<nextsent>therefore, clte may also be useful for related tasks in which more than one language is involved, such as cross-lingual question answering or cross-lingual information retrieval.
</nextsent>
<nextsent>although clte detection isa relatively new problem, it has already been tackled.
</nextsent>
<nextsent>mehdad et al (2010) <papid> N10-1045 </papid>propose to use machine translation (mt) to translate from lh , the language of , into lt , the language of , and then use any of the state-of-the-art te approaches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X884">
<title id=" S12-1065.xml">ualacant using online machine translation for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although clte detection isa relatively new problem, it has already been tackled.
</prevsent>
<prevsent>mehdad et al (2010) <papid> N10-1045 </papid>propose to use machine translation (mt) to translate from lh , the language of , into lt , the language of , and then use any of the state-of-the-art te approaches.</prevsent>
</prevsection>
<citsent citstr=" P11-1134 ">
in later work (mehdad et al, 2011), <papid> P11-1134 </papid>the authors use mt, but in more elaborate way.</citsent>
<aftsection>
<nextsent>they train phrase-based statistical mt (pbsmt) system (koehn et al, 2003) <papid> N03-1017 </papid>translating from lh to lt , and use the translation table obtained as by-product of the training process to extract set of features which are processed by support vector machine classifier (theodoridis and koutroumbas, 2009, sect.</nextsent>
<nextsent>3.7) to decide whether entails or not.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X885">
<title id=" S12-1065.xml">ualacant using online machine translation for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mehdad et al (2010) <papid> N10-1045 </papid>propose to use machine translation (mt) to translate from lh , the language of , into lt , the language of , and then use any of the state-of-the-art te approaches.</prevsent>
<prevsent>in later work (mehdad et al, 2011), <papid> P11-1134 </papid>the authors use mt, but in more elaborate way.</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
they train phrase-based statistical mt (pbsmt) system (koehn et al, 2003) <papid> N03-1017 </papid>translating from lh to lt , and use the translation table obtained as by-product of the training process to extract set of features which are processed by support vector machine classifier (theodoridis and koutroumbas, 2009, sect.</citsent>
<aftsection>
<nextsent>3.7) to decide whether entails or not.
</nextsent>
<nextsent>castillo (2011) discusses another machine learning approach in which the features are obtained from semantic similarity measures based on wordnet (miller, 1995).
</nextsent>
<nextsent>in this work we present new approach to tackle the problem of clte detection using machine learning approach, partly inspired by that of mehdad etal.
</nextsent>
<nextsent>(2011).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X888">
<title id=" S12-1065.xml">ualacant using online machine translation for cross lingual textual entailment </title>
<section> experimental settings.  </section>
<citcontext>
<prevsection>
<prevsent>the experiments designed for this task are aimed at evaluating the features proposed in section 2.
</prevsent>
<prevsent>we evaluate our clte approach using the english?
</prevsent>
</prevsection>
<citsent citstr=" S12-1053 ">
spanish datasets provided in the task 8 of semeval 2012 (negri et al, 2012).<papid> S12-1053 </papid>datasets.</citsent>
<aftsection>
<nextsent>two datasets were provided by the organization of semeval 2012 (negri et al, 2011): <papid> D11-1062 </papid>training set and test set, both composed by setof 500 pairs of sentences.</nextsent>
<nextsent>clte detection is evaluated in both directions, so instances belong to one of these four classes: forward (the sentence in spanish entails the one in english); backward (the sentence in english entails the one in spanish); bidirectional (both sentences entail each other); and no entailment (neither of the sentences entails each other).for the whole dataset, both sentences in each instance were tokenized using the scripts1 included inthe moses mt system (koehn et al, 2007).<papid> P07-2045 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X889">
<title id=" S12-1065.xml">ualacant using online machine translation for cross lingual textual entailment </title>
<section> experimental settings.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluate our clte approach using the english?
</prevsent>
<prevsent>spanish datasets provided in the task 8 of semeval 2012 (negri et al, 2012).<papid> S12-1053 </papid>datasets.</prevsent>
</prevsection>
<citsent citstr=" D11-1062 ">
two datasets were provided by the organization of semeval 2012 (negri et al, 2011): <papid> D11-1062 </papid>training set and test set, both composed by setof 500 pairs of sentences.</citsent>
<aftsection>
<nextsent>clte detection is evaluated in both directions, so instances belong to one of these four classes: forward (the sentence in spanish entails the one in english); backward (the sentence in english entails the one in spanish); bidirectional (both sentences entail each other); and no entailment (neither of the sentences entails each other).for the whole dataset, both sentences in each instance were tokenized using the scripts1 included inthe moses mt system (koehn et al, 2007).<papid> P07-2045 </papid></nextsent>
<nextsent>each sentence was segmented to get al possible sub-segments which were then translated into the other language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X890">
<title id=" S12-1065.xml">ualacant using online machine translation for cross lingual textual entailment </title>
<section> experimental settings.  </section>
<citcontext>
<prevsection>
<prevsent>spanish datasets provided in the task 8 of semeval 2012 (negri et al, 2012).<papid> S12-1053 </papid>datasets.</prevsent>
<prevsent>two datasets were provided by the organization of semeval 2012 (negri et al, 2011): <papid> D11-1062 </papid>training set and test set, both composed by setof 500 pairs of sentences.</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
clte detection is evaluated in both directions, so instances belong to one of these four classes: forward (the sentence in spanish entails the one in english); backward (the sentence in english entails the one in spanish); bidirectional (both sentences entail each other); and no entailment (neither of the sentences entails each other).for the whole dataset, both sentences in each instance were tokenized using the scripts1 included inthe moses mt system (koehn et al, 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>each sentence was segmented to get al possible sub-segments which were then translated into the other language.
</nextsent>
<nextsent>external resources.
</nextsent>
<nextsent>we used three different mt systems to translate the sub-segments from english to spanish, and vice versa: ? apertium:2 free/open-source platform for the development of rule-based mt systems (for cada et al, 2011).
</nextsent>
<nextsent>we used the englishspanishmt system from the projects repository3 (revi sion 34706).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X891">
<title id=" S10-1034.xml">likey unsupervised language independent key phrase extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this article describes the participation of the likey method in the task 5 of the semeval 2010 challenge, automatic key phrase extraction from scientific articles (kim et al, 2010).
</prevsent>
<prevsent>1.1 related work.
</prevsent>
</prevsection>
<citsent citstr=" W03-1028 ">
in statistical key phrase extraction, many variations for term frequency counts have been proposed inthe literature including relative frequencies (dam erau, 1993), collection frequency (hulth, 2003),<papid> W03-1028 </papid>term frequency inverse document frequency (tf idf) (salton and buckley, 1988), among others.</citsent>
<aftsection>
<nextsent>additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (frank et al, 1999), importance of the sentence in which the term occurs (hacohen-kerner, 2003), and widely studied part-of-speech tag patterns, e.g. hulth (2003).<papid> W03-1028 </papid>matsuo and ishizuka (2004) present keyword extraction method using word co-occurrence statis tics.</nextsent>
<nextsent>an unsupervised key phrase extraction method by liu et al (2009) <papid> D09-1027 </papid>uses clustering to find exemplar terms that are then used for key phrase extraction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X893">
<title id=" S10-1034.xml">likey unsupervised language independent key phrase extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in statistical key phrase extraction, many variations for term frequency counts have been proposed inthe literature including relative frequencies (dam erau, 1993), collection frequency (hulth, 2003),<papid> W03-1028 </papid>term frequency inverse document frequency (tf idf) (salton and buckley, 1988), among others.</prevsent>
<prevsent>additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (frank et al, 1999), importance of the sentence in which the term occurs (hacohen-kerner, 2003), and widely studied part-of-speech tag patterns, e.g. hulth (2003).<papid> W03-1028 </papid>matsuo and ishizuka (2004) present keyword extraction method using word co-occurrence statis tics.</prevsent>
</prevsection>
<citsent citstr=" D09-1027 ">
an unsupervised key phrase extraction method by liu et al (2009) <papid> D09-1027 </papid>uses clustering to find exemplar terms that are then used for key phrase extraction.</citsent>
<aftsection>
<nextsent>most of the presented methods require reference corpus or training corpus to producekeyphrases.
</nextsent>
<nextsent>statistical key phrase extraction methods without reference corpora have also been proposed, e.g.
</nextsent>
<nextsent>(matsuo and ishizuka, 2004; brac ewell et al, 2005).
</nextsent>
<nextsent>the later study is carried out for bilingual corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X894">
<title id=" S10-1034.xml">likey unsupervised language independent key phrase extraction </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>dataset reader author combined train 1 824 559 2 223 trial 526 149 621 test 1 204 387 1 466table 2: number of correct answers in reader, author, and combined answer sets for each dataset.
</prevsent>
<prevsent>more detailed information on the dataset can be found in (kim et al, 2010).
</prevsent>
</prevsection>
<citsent citstr=" C08-2021 ">
likey key phrase extraction approach comes from the tradition of statistical machine learning (paukkeri et al, 2008).<papid> C08-2021 </papid></citsent>
<aftsection>
<nextsent>the method has been developed to be as language-independent as possible.
</nextsent>
<nextsent>the only language-specific component needed is corpus in each language.
</nextsent>
<nextsent>this kind of data is readily available online or from other sources.
</nextsent>
<nextsent>likey selects the words and phrases that best crystallize the meaning of the documents by comparing ranks of frequencies in the documents tothose in the reference corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X896">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also pinker (1989) for similar pro- posal).
</prevsent>
<prevsent>thus one would expect verbs that undergo the same alternations to form semantically co-herent class.
</prevsent>
</prevsection>
<citsent citstr=" C96-1055 ">
levin study on dia thesis alternations has influenced recent work on word sense disam-biguation (dorr and jones, 1996), <papid> C96-1055 </papid>machine transla-tion (dang et al, 1998), <papid> P98-1046 </papid>and automatic lexical ac-quisition (mccarthy and korhonen, 1998; <papid> P98-2247 </papid>schulte im walde, 1998).</citsent>
<aftsection>
<nextsent>the objective of this paper is to investigate the ex-tent to which dia thesis alternations are empirically attested in corpus data.
</nextsent>
<nextsent>using the dative and bene- factive alternations as test case we attempt de- termine: (a) if some alternations are more frequent than others, (b) if alternating verbs have frame pref-erences and (c) what the representative members of an alternation are.
</nextsent>
<nextsent>in section 2 we describe and evaluate the set of automatic methods we used to acquire verbs under- going the dative and benefactive alternations.
</nextsent>
<nextsent>we assess the acquired frames using filtering method presented in section 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X897">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also pinker (1989) for similar pro- posal).
</prevsent>
<prevsent>thus one would expect verbs that undergo the same alternations to form semantically co-herent class.
</prevsent>
</prevsection>
<citsent citstr=" P98-1046 ">
levin study on dia thesis alternations has influenced recent work on word sense disam-biguation (dorr and jones, 1996), <papid> C96-1055 </papid>machine transla-tion (dang et al, 1998), <papid> P98-1046 </papid>and automatic lexical ac-quisition (mccarthy and korhonen, 1998; <papid> P98-2247 </papid>schulte im walde, 1998).</citsent>
<aftsection>
<nextsent>the objective of this paper is to investigate the ex-tent to which dia thesis alternations are empirically attested in corpus data.
</nextsent>
<nextsent>using the dative and bene- factive alternations as test case we attempt de- termine: (a) if some alternations are more frequent than others, (b) if alternating verbs have frame pref-erences and (c) what the representative members of an alternation are.
</nextsent>
<nextsent>in section 2 we describe and evaluate the set of automatic methods we used to acquire verbs under- going the dative and benefactive alternations.
</nextsent>
<nextsent>we assess the acquired frames using filtering method presented in section 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X898">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also pinker (1989) for similar pro- posal).
</prevsent>
<prevsent>thus one would expect verbs that undergo the same alternations to form semantically co-herent class.
</prevsent>
</prevsection>
<citsent citstr=" P98-2247 ">
levin study on dia thesis alternations has influenced recent work on word sense disam-biguation (dorr and jones, 1996), <papid> C96-1055 </papid>machine transla-tion (dang et al, 1998), <papid> P98-1046 </papid>and automatic lexical ac-quisition (mccarthy and korhonen, 1998; <papid> P98-2247 </papid>schulte im walde, 1998).</citsent>
<aftsection>
<nextsent>the objective of this paper is to investigate the ex-tent to which dia thesis alternations are empirically attested in corpus data.
</nextsent>
<nextsent>using the dative and bene- factive alternations as test case we attempt de- termine: (a) if some alternations are more frequent than others, (b) if alternating verbs have frame pref-erences and (c) what the representative members of an alternation are.
</nextsent>
<nextsent>in section 2 we describe and evaluate the set of automatic methods we used to acquire verbs under- going the dative and benefactive alternations.
</nextsent>
<nextsent>we assess the acquired frames using filtering method presented in section 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X899">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>sec-tion 2.3).
</prevsent>
<prevsent>we disambiguated the attachment site of pps (cf.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
(3e)) using hindle and rooth (1993) <papid> J93-1005 </papid>lex-ical association score (cf.</citsent>
<aftsection>
<nextsent>section 2.4).
</nextsent>
<nextsent>finally, we recognized benefactive pps (cf.
</nextsent>
<nextsent>(3g)) by exploiting the wordnet axonomy (cf.
</nextsent>
<nextsent>section 2.5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X900">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>2tokens containing noun sequences with length larger than 3 (450 in total) were considered negative instances ofthe double object frame.
</prevsent>
<prevsent>398 g-score ~  2-word compound 1967.68 775.21 87.02 45.40 30.58 29.94 24.04 bank manager tax liability income tax book reviewer designer gear safety plan drama school table 1 : random sample of two word compounds table g-score 3-word compound 574.48 382.92 77.78 48.84 36.44 32.35 23.98 \[\[energy efficiency\] office\] \[\[council tax\] bills\] \[alcohol \[education course\]\] \[hospital \[out-patient department\] \[\[turnout suppressor\] function\] \[\[nature conservation\] resources\] \[\[quality amplifier\] circuits\] 2: random sample of three word compounds for sequences of length 2 not found in wordnet, we used the log-likelihood ratio (g-score) to esti-mate the lexical association between the nouns, in order to determine if they formed compound noun.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
we preferred the log-likelihood ratio to other statis-tical scores, such as the association ratio (church and hanks, 1990) <papid> J90-1003 </papid>or ;(2, since it adequately takes into account the frequency of the co-occurring words and is less sensitive to rare events and corpus- size (dunning, 1993; <papid> J93-1003 </papid>daille, 1996).</citsent>
<aftsection>
<nextsent>we assumed that two nouns cannot be disjoint arguments of the verb if they are lexically associated.
</nextsent>
<nextsent>on this basis, tokens were rejected as instances of the double ob-ject frame if they contained two nouns whose g- score had p-value less than 0.05.
</nextsent>
<nextsent>a two-step process was applied to noun se-quences of length 3: first their bracketing was de-termined and second the g-score was computed be-tween the single noun and the 2-noun sequence.
</nextsent>
<nextsent>we inferred the bracketing by modifying an al-gorithm initially proposed by pustejovsky et al (1993).<papid> J93-2005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X901">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>2tokens containing noun sequences with length larger than 3 (450 in total) were considered negative instances ofthe double object frame.
</prevsent>
<prevsent>398 g-score ~  2-word compound 1967.68 775.21 87.02 45.40 30.58 29.94 24.04 bank manager tax liability income tax book reviewer designer gear safety plan drama school table 1 : random sample of two word compounds table g-score 3-word compound 574.48 382.92 77.78 48.84 36.44 32.35 23.98 \[\[energy efficiency\] office\] \[\[council tax\] bills\] \[alcohol \[education course\]\] \[hospital \[out-patient department\] \[\[turnout suppressor\] function\] \[\[nature conservation\] resources\] \[\[quality amplifier\] circuits\] 2: random sample of three word compounds for sequences of length 2 not found in wordnet, we used the log-likelihood ratio (g-score) to esti-mate the lexical association between the nouns, in order to determine if they formed compound noun.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
we preferred the log-likelihood ratio to other statis-tical scores, such as the association ratio (church and hanks, 1990) <papid> J90-1003 </papid>or ;(2, since it adequately takes into account the frequency of the co-occurring words and is less sensitive to rare events and corpus- size (dunning, 1993; <papid> J93-1003 </papid>daille, 1996).</citsent>
<aftsection>
<nextsent>we assumed that two nouns cannot be disjoint arguments of the verb if they are lexically associated.
</nextsent>
<nextsent>on this basis, tokens were rejected as instances of the double ob-ject frame if they contained two nouns whose g- score had p-value less than 0.05.
</nextsent>
<nextsent>a two-step process was applied to noun se-quences of length 3: first their bracketing was de-termined and second the g-score was computed be-tween the single noun and the 2-noun sequence.
</nextsent>
<nextsent>we inferred the bracketing by modifying an al-gorithm initially proposed by pustejovsky et al (1993).<papid> J93-2005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X902">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>on this basis, tokens were rejected as instances of the double ob-ject frame if they contained two nouns whose g- score had p-value less than 0.05.
</prevsent>
<prevsent>a two-step process was applied to noun se-quences of length 3: first their bracketing was de-termined and second the g-score was computed be-tween the single noun and the 2-noun sequence.
</prevsent>
</prevsection>
<citsent citstr=" J93-2005 ">
we inferred the bracketing by modifying an al-gorithm initially proposed by pustejovsky et al (1993).<papid> J93-2005 </papid></citsent>
<aftsection>
<nextsent>given three nouns 1, n2, n3, if either \[n n2\] or \[n2 n3\] are in the compound noun dictionary, we built structures \[\[nt n2\] n3\] or \[r/l \[n2 n3\]\] accord- ingly; if both \[n n2\] and in2 n3\] appear in the dic-tionary, we chose the most frequent pair; if neither \[n n2\] nor \[n2 n3\] appear in wordnet, we computed the g-score for \[nl n2\] and \[n2 n3\] and chose the pair with highest value (p   0.05).
</nextsent>
<nextsent>tables 1 and 2 display random sample of the compounds the method found (p   0.05).
</nextsent>
<nextsent>2.3.1 evaluation the performance of the linguistic heuristics and the compound etection procedure were evaluated by randomly selecting approximate!y 3,000 corpus to-kens which were previously accepted or rejected as instances of the double object frame.
</nextsent>
<nextsent>two judges de-cided whether the tokens were classified correctly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X904">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>in the case of the  npi for np2  structure, the for-pp must be benefactive.
</prevsent>
<prevsent>4 in older to meet requirements (1)-(3), we first de-termined the attachment site (e.g., verb or noun) of the pp and secondly developed procedure for dis-tinguishing benefactive from non-benefactive pps.
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
several approaches have statistically addressed the problem of prepositional phrase ambiguity, with comparable results (hindle and rooth, 1993; <papid> J93-1005 </papid>collins and brooks, 1995; <papid> W95-0103 </papid>ratnaparkhi, 1998).<papid> P98-2177 </papid></citsent>
<aftsection>
<nextsent>hin-dle and rooth (1993) used partial parser to extract (v, n, p) tuples from corpus, where is the prepo-sition whose attachment is ambiguous between the verb and the noun n. we used variant of the method described in hindle and rooth (1993), <papid> J93-1005 </papid>the main difference being that we applied their lexical association score (a log-likelihood ratio which com-pares the probability of noun versus verb attach- ment) in an unsupervised non-iterative manner.</nextsent>
<nextsent>fur-thermore, the procedure was applied to the special case of tuples containing the prepositions to and for only.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X905">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>in the case of the  npi for np2  structure, the for-pp must be benefactive.
</prevsent>
<prevsent>4 in older to meet requirements (1)-(3), we first de-termined the attachment site (e.g., verb or noun) of the pp and secondly developed procedure for dis-tinguishing benefactive from non-benefactive pps.
</prevsent>
</prevsection>
<citsent citstr=" P98-2177 ">
several approaches have statistically addressed the problem of prepositional phrase ambiguity, with comparable results (hindle and rooth, 1993; <papid> J93-1005 </papid>collins and brooks, 1995; <papid> W95-0103 </papid>ratnaparkhi, 1998).<papid> P98-2177 </papid></citsent>
<aftsection>
<nextsent>hin-dle and rooth (1993) used partial parser to extract (v, n, p) tuples from corpus, where is the prepo-sition whose attachment is ambiguous between the verb and the noun n. we used variant of the method described in hindle and rooth (1993), <papid> J93-1005 </papid>the main difference being that we applied their lexical association score (a log-likelihood ratio which com-pares the probability of noun versus verb attach- ment) in an unsupervised non-iterative manner.</nextsent>
<nextsent>fur-thermore, the procedure was applied to the special case of tuples containing the prepositions to and for only.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X907">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> fi ter ing.  </section>
<citcontext>
<prevsection>
<prevsent>we discarded verbs for which we had very little evidence (frame frequency = 1) and applied rela-tive frequency cutoff: the verb acquired frame fre-quency was compared against its overall frequency in the bnc.
</prevsent>
<prevsent>verbs whose relative frame frequency was lower than an empirically established thresh-old were discarded.
</prevsent>
</prevsection>
<citsent citstr=" C94-1042 ">
the threshold values varied from frame to flame but not from verb to verb and were determined by taking into account for each frame its overall frame frequency which was es-timated from the comlex subcategorization dic-tionary (6,000 verbs) (grishman et al, 1994).<papid> C94-1042 </papid></citsent>
<aftsection>
<nextsent>this meant hat the threshold was higher for less frequent frames (e.g., the double object frame for which only 79 verbs are listed in comlex).
</nextsent>
<nextsent>we also experimented with method suggested by brent (1993) <papid> J93-2002 </papid>which applies the binomial test on frame frequency data.</nextsent>
<nextsent>both methods yielded comparable sults.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X908">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> fi ter ing.  </section>
<citcontext>
<prevsection>
<prevsent>the threshold values varied from frame to flame but not from verb to verb and were determined by taking into account for each frame its overall frame frequency which was es-timated from the comlex subcategorization dic-tionary (6,000 verbs) (grishman et al, 1994).<papid> C94-1042 </papid></prevsent>
<prevsent>this meant hat the threshold was higher for less frequent frames (e.g., the double object frame for which only 79 verbs are listed in comlex).</prevsent>
</prevsection>
<citsent citstr=" J93-2002 ">
we also experimented with method suggested by brent (1993) <papid> J93-2002 </papid>which applies the binomial test on frame frequency data.</citsent>
<aftsection>
<nextsent>both methods yielded comparable sults.
</nextsent>
<nextsent>however, the relative frequency threshold worked slightly better and the results re-ported in the following section are based on this method.
</nextsent>
<nextsent>we acquired 162 verbs for the double object frame, 426 verbs for the  np1 to np2  frame and 962 for the  npl for np2  frame.
</nextsent>
<nextsent>membership in al-ternations was judged as follows: (a) verb partic-ipates in the dative alternation if it has the double object and  np1 to np2  frames and (b) verb dative alternation alternating npi np2 allot, assign, bring, fax, feed, flick, give, grant, guarantee, leave, lend offer, owe, take pass, pay, render, repay, sell, show, teach, tell, throw, toss, write, serve, send, award allocate, bequeath, carry, catapult, cede, concede, drag, drive, extend, ferry, fly, haul, hoist, issue, lease, peddle, pose, preach, push, relay, ship, tug, yield npi to np2 ask, chuck, promise, quote, read, shoot, slip benefactive alternation alternating bake, build, buy, cast, cook, earn, fetch, find, fix, forge, gain, get, keep, knit, leave, make, pour, save procure, secure, set, toss, win, write npi np2 arrange, assemble, carve, choose, compile, design, develop, dig, gather, grind, hire, play, prepare, reserve, run, sew np1 for np2 boil, call, shoot table 5: verbs common in corpus and levin participates in the benefactive alternation if it has the double object and  np1 for np2  frames.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X909">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> productivity.  </section>
<citcontext>
<prevsection>
<prevsent>the productivity estimates discussed here can be potentially useful for treating lexical rules proba-bilistically, and for quantifying the degree to which language users are willing to apply  rule in order 402 bring-take 2 2 1 0.327 future having 19 17 0.89 0.313 give 15 9 0.6 0.55 m.transfer 17 10 0.58 0.66 carry 15 6 0.4 0.056 drive 11 3 0.27 0.03 throwing 30 7 0.23 0.658 send 23 3 0.13 0.181 instr.
</prevsent>
<prevsent>com.
</prevsent>
</prevsection>
<citsent citstr=" W96-0303 ">
18 1 0.05 0.648 slide 5 0 0 0 benefactive alternation class total alt prod typ get 33 17 0.51 0.54 prepare 26 9 0.346 0.55 build 35 12 0.342 0.34 performance 19 1 0.05 0.56 create 20 2 0.1 0.05 table 6: productivity estimates and typicality values for the dative and benefactive alternation to produce novel form (briscoe and copestake, 1996).<papid> W96-0303 </papid></citsent>
<aftsection>
<nextsent>estimating the productivity of an alternation forgiven class does not incorporate information about the frequency of the verbs undergoing the alterna-tion.
</nextsent>
<nextsent>we propose to use frequency data to quantify the typicality of verb or verb class forgiven alter-nation.
</nextsent>
<nextsent>the underlying assumption is that verb is typical for an alternation if it is equally frequent for both frames which are characteristic for the alter-nation.
</nextsent>
<nextsent>thus the typicality of verb can be defined as the conditional probability of the frame given the verb: (framei, verb) (6) p(frameilverb) = y~ fframe n, verb) we calculate pfframeilverb) by dividing f(frame i, verb), the number of times the verb was attested in the corpus with frame i, by ~-~.,, f(frame,,, verb), the overall number of times the verb was attested.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X910">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, frame frequency data was used to estimate whether alternating verbs ex-hibit different preferences forgiven frame (typi- cality).
</prevsent>
<prevsent>however, it has been shown that corpus id-iosyncrasies can affect subcategorization frequen-cies (cf.
</prevsent>
</prevsection>
<citsent citstr=" P98-2184 ">
roland and jurafsky (1998) <papid> P98-2184 </papid>for an exten-sive discussion).</citsent>
<aftsection>
<nextsent>this suggests that different corpora may give different results with respect verbal- ternations.
</nextsent>
<nextsent>for instance, the to-pp frame is poorly  represented in the syntactically annotated version of the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
<nextsent>there are only 26 verbs taking the to-pp frame, of which 20 have frame frequency of 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X911">
<title id=" P99-1051.xml">acquiring lexical generalizations from corpora a case study for dia thesis alternations </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>roland and jurafsky (1998) <papid> P98-2184 </papid>for an exten-sive discussion).</prevsent>
<prevsent>this suggests that different corpora may give different results with respect verbal- ternations.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
for instance, the to-pp frame is poorly  represented in the syntactically annotated version of the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>there are only 26 verbs taking the to-pp frame, of which 20 have frame frequency of 1.
</nextsent>
<nextsent>this indicates that very small number of verbs undergoing the dative alter-nation can be potentially acquired from this corpus.
</nextsent>
<nextsent>in future work we plan to investigate the degree to which corpus differences affect he productivity and typicality estimates for verb alternations.
</nextsent>
<nextsent>this paper explored the degree to which dia thesis alternations can be identified in corpus data via shal-low syntactic processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X912">
<title id=" P98-2229.xml">a model for robust processing of spontaneous speech by integrating viable fragments </title>
<section> the it  format.  </section>
<citcontext>
<prevsection>
<prevsent>we describe this component in section 5.
</prevsent>
<prevsent>the relevant part of the system architecture is shown in figure 1.
</prevsent>
</prevsection>
<citsent citstr=" P98-1024 ">
the vit (short for verb mobil interface term) was designed as common output format for the two alternative and independently developed syntactic- semantic analysis components of the first project phase (bos et al, 1998).<papid> P98-1024 </papid></citsent>
<aftsection>
<nextsent>their internal semantic for-malisms differed, but both had to be attached to 1403   speech _l recognition dr\[ prosody \] \[ nition &amp; hpsg \[ dialogue and parser \[ context \[ n tegrated i._-.~,~vi.i.s,l_,.~ semantic i- ~ -~ -(t, - /  ~ transfer ~ vit \] processing . : ~ l roccssmk ~ i chunk statistical synthesis parser parser figure 1: part of the system architecture.
</nextsent>
<nextsent>single transfer module.
</nextsent>
<nextsent>the need for common out- put format is still present, since there are three al-ternative syntactic-semantic parsing modules in the new verb mobil system, all of which again produce output for just one transfer module.
</nextsent>
<nextsent>(1) vit(vitid(sid(l,a,ge,o,20,l,ge,y, semantics), \[word(montag, 13, \[ii16\]), word(ist,14, \[ii17\]), word(gut,15, \[lllol)l), index(lll3,1109,il04), \[decl(lll2,hl05), gut(lllo,il05), dofw(lll6,ilo5,mon), support(lll7,il04,1110), indef(llll,ilo5,1115,hl06)\], \[ccom_plug(hl05,1114), ccom_plug(h106,1109), in g(ii12,1113), in_g(lll7,1109), in_g(lll6,1115), in_g(llll,lll4), leq(lll4,hlo5),leq(llo9,hl06), leq(llog,hl05)\], is sort(ilo5,time)l, \[\], \[num(ilo5,sg),pers(il05,3)\], \[ta_mood(ilo4,ind), ta_tense(ilo4,pres), ta_perf(ilo4,nonperf)\], \[\] ) the vit can be viewed as theory-independent representation for underspecified semantic repre-sentations (bos et al, 1996).<papid> C96-1024 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X913">
<title id=" P98-2229.xml">a model for robust processing of spontaneous speech by integrating viable fragments </title>
<section> the it  format.  </section>
<citcontext>
<prevsection>
<prevsent>single transfer module.
</prevsent>
<prevsent>the need for common out- put format is still present, since there are three al-ternative syntactic-semantic parsing modules in the new verb mobil system, all of which again produce output for just one transfer module.
</prevsent>
</prevsection>
<citsent citstr=" C96-1024 ">
(1) vit(vitid(sid(l,a,ge,o,20,l,ge,y, semantics), \[word(montag, 13, \[ii16\]), word(ist,14, \[ii17\]), word(gut,15, \[lllol)l), index(lll3,1109,il04), \[decl(lll2,hl05), gut(lllo,il05), dofw(lll6,ilo5,mon), support(lll7,il04,1110), indef(llll,ilo5,1115,hl06)\], \[ccom_plug(hl05,1114), ccom_plug(h106,1109), in g(ii12,1113), in_g(lll7,1109), in_g(lll6,1115), in_g(llll,lll4), leq(lll4,hlo5),leq(llo9,hl06), leq(llog,hl05)\], is sort(ilo5,time)l, \[\], \[num(ilo5,sg),pers(il05,3)\], \[ta_mood(ilo4,ind), ta_tense(ilo4,pres), ta_perf(ilo4,nonperf)\], \[\] ) the vit can be viewed as theory-independent representation for underspecified semantic repre-sentations (bos et al, 1996).<papid> C96-1024 </papid></citsent>
<aftsection>
<nextsent>it specifies aset of dis-course representation structures, drss, (kamp and reyle, 1993).
</nextsent>
<nextsent>if an utterance is structurally ambigu-ous, it will be represented by one vit, which spec-ifies the set of drss corresponding to the different readings of the utterance.
</nextsent>
<nextsent>formally, vit is nine-place prolog term.
</nextsent>
<nextsent>there are slots for an identifier for the input segment to which the vit corresponds, list of the core se-mantic predicates, list of scopal constraints, yn- tactic, prosodic and pragmatic information as well as tense and aspect and sortal information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X914">
<title id=" P98-2229.xml">a model for robust processing of spontaneous speech by integrating viable fragments </title>
<section> approaches to robustness.  </section>
<citcontext>
<prevsection>
<prevsent>while we do not see them as mutually exclusive, we think that the first two present significant problems.
</prevsent>
<prevsent>4.1 before parsing.
</prevsent>
</prevsection>
<citsent citstr=" P92-1008 ">
detection of self corrections on transcriptions be-fore parsing has been explored (bear et al, 1992; <papid> P92-1008 </papid>nakatani and hirschberg, 1993), <papid> P93-1007 </papid>but it is not clear that it will be feasible on whgs, since recognition errors interfere and the search space may explode due to the number of paths.</citsent>
<aftsection>
<nextsent>dealing with recogni-tion errors before parsing is impossible due to lack of structural information.
</nextsent>
<nextsent>4.2 during parsing.
</nextsent>
<nextsent>treating the phenomena mentioned uring parsing would mean that the grammar or the parser would have to be made more liberal, i. e. they would have accept strings which are ungrammatical.
</nextsent>
<nextsent>this is problematic in the context of whg parsing, since the parser has to simultaneously perform two tasks: searching for path to be analysed and analysing it as well.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X915">
<title id=" P98-2229.xml">a model for robust processing of spontaneous speech by integrating viable fragments </title>
<section> approaches to robustness.  </section>
<citcontext>
<prevsection>
<prevsent>while we do not see them as mutually exclusive, we think that the first two present significant problems.
</prevsent>
<prevsent>4.1 before parsing.
</prevsent>
</prevsection>
<citsent citstr=" P93-1007 ">
detection of self corrections on transcriptions be-fore parsing has been explored (bear et al, 1992; <papid> P92-1008 </papid>nakatani and hirschberg, 1993), <papid> P93-1007 </papid>but it is not clear that it will be feasible on whgs, since recognition errors interfere and the search space may explode due to the number of paths.</citsent>
<aftsection>
<nextsent>dealing with recogni-tion errors before parsing is impossible due to lack of structural information.
</nextsent>
<nextsent>4.2 during parsing.
</nextsent>
<nextsent>treating the phenomena mentioned uring parsing would mean that the grammar or the parser would have to be made more liberal, i. e. they would have accept strings which are ungrammatical.
</nextsent>
<nextsent>this is problematic in the context of whg parsing, since the parser has to simultaneously perform two tasks: searching for path to be analysed and analysing it as well.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X917">
<title id=" S10-1060.xml">cambridge parser evaluation using textual entailment by grammatical relation comparison </title>
<section> system.  </section>
<citcontext>
<prevsection>
<prevsent>in the pete task the parser is not evaluated directly on the dataset, since the entailment system actsas intermediary.
</prevsent>
<prevsent>therefore, for pete to be viable parser evaluation scheme, each parser mustbe coupled with an entailment system which accurately reflects the parsers analysis of the data.
</prevsent>
</prevsection>
<citsent citstr=" J07-4004 ">
we used the c&c; parser (clark and curran, 2007), <papid> J07-4004 </papid>which can produce output in the form of grammatical relations (grs), i.e. labelled head dependencies.</citsent>
<aftsection>
<nextsent>for example, (nsubj tired man) for the example in section 1 represents the fact that the np headed by man is the subject of the predicate headed by tired.
</nextsent>
<nextsent>we chose to use the stanford dependency gr scheme (de marneffe et al., 2006), but the same approach should work for other schemes (and other parsers producing grs).
</nextsent>
<nextsent>our entailment system was very simple, and based on the assumption that is simplified version of (true for this task though not for rte in general).
</nextsent>
<nextsent>we parsed both and with the c&c; parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X918">
<title id=" S10-1060.xml">cambridge parser evaluation using textual entailment by grammatical relation comparison </title>
<section> system.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we required grs(h) ? grs(t) to be non-empty (no vacuous positives), but did not restrict this criterion to subjects and objects.
</prevsent>
<prevsent>we used ptb tokenizer 1 for consistency with the parsers training data.
</prevsent>
</prevsection>
<citsent citstr=" W00-1427 ">
we used the morpha lemmatizer (minnen et al, 2000), <papid> W00-1427 </papid>which is built into the c&c; tools, to match tokens across and h; and we converted all tokens to lowercase.</citsent>
<aftsection>
<nextsent>if the parser failed to find spanning analysis for either or h, the entailment decision was no.
</nextsent>
<nextsent>the full pipeline is shown in figure 1.
</nextsent>
<nextsent>a total of 19 systems were submitted.
</nextsent>
<nextsent>the base line score for always yes?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X919">
<title id=" S10-1060.xml">cambridge parser evaluation using textual entailment by grammatical relation comparison </title>
<section> error analysis.  </section>
<citcontext>
<prevsection>
<prevsent>table 3 shows the breakdown of errors.
</prevsent>
<prevsent>the largest category was false negatives due to unbounded dependencies not recovered by the parser, for example it required an energy he no longer possessed to be satirical about his father.
</prevsent>
</prevsection>
<citsent citstr=" D09-1085 ">
somebody no longer possessed the energy..here the parser fails to recover the direct object relation between possess and energy in t. it is known that parsers have difficulty with unbounded dependencies (rimell et al, 2009, <papid> D09-1085 </papid>from which theun bounded examples in this dataset were obtained), so this result is not surprising.</citsent>
<aftsection>
<nextsent>the next category was other parser errors.
</nextsent>
<nextsent>this is miscellaneous category including e.g. err orson coordination, parenthetical elements, identifying the head of clausal subject, and one due to the pos tagger.
</nextsent>
<nextsent>for example, for then at least hewould have place to hang his tools and some thing to work on.
</nextsent>
<nextsent>he would have something to work on., the parser incorrectly coordinated tools and something for t. as result (dobj havesomething) was in grs(h) but not grs(t), yielding an incorrect no.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X920">
<title id=" P99-1067.xml">automatic identification of word translations from unrelated english and german corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>starting with the well-known paper of brown et al.
</prevsent>
<prevsent>(1990) on statistical machine translation, there has been much scientific interest in the alignment of sentences and words in translated texts.
</prevsent>
</prevsection>
<citsent citstr=" J93-1004 ">
many studies how that for nicely parallel corpora high accuracy rates of up to 99% can be achieved for both sentence and word alignment (gale &amp; church, 1993; <papid> J93-1004 </papid>kay &amp; r/sscheisen, 1993).</citsent>
<aftsection>
<nextsent>of course, in practice - due to omissions, trans positions, insertions, and replacements in the process of translation - with real texts there may be all kinds of problems, and therefore ro-bustness still an issue (langlais et al, 1998).<papid> P98-1117 </papid></nextsent>
<nextsent>nevertheless, the results achieved with these algorithms have been found useful for the corn- pil ation of dictionaries, for checking the con-sistency of terminological usage in translations, for assisting the terminological work of trans-lators and interpreters, and for example-based machine translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X921">
<title id=" P99-1067.xml">automatic identification of word translations from unrelated english and german corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1990) on statistical machine translation, there has been much scientific interest in the alignment of sentences and words in translated texts.
</prevsent>
<prevsent>many studies how that for nicely parallel corpora high accuracy rates of up to 99% can be achieved for both sentence and word alignment (gale &amp; church, 1993; <papid> J93-1004 </papid>kay &amp; r/sscheisen, 1993).</prevsent>
</prevsection>
<citsent citstr=" P98-1117 ">
of course, in practice - due to omissions, trans positions, insertions, and replacements in the process of translation - with real texts there may be all kinds of problems, and therefore ro-bustness still an issue (langlais et al, 1998).<papid> P98-1117 </papid></citsent>
<aftsection>
<nextsent>nevertheless, the results achieved with these algorithms have been found useful for the corn- pil ation of dictionaries, for checking the con-sistency of terminological usage in translations, for assisting the terminological work of trans-lators and interpreters, and for example-based machine translation.
</nextsent>
<nextsent>by now, some alignment programs are offered commercially: translation memory tools for translators, such as ibm translation manager or trados  translator workbench, are bundled or can be upgraded with programs for sentence alignment.
</nextsent>
<nextsent>most of the proposed algorithms first con-duct an alignment of sentences, that is, they lo-cate those pairs of sentences that are translations of each other.
</nextsent>
<nextsent>in second step word alignment is performed by analyzing the correspondences of words in each pair of sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X922">
<title id=" P99-1067.xml">automatic identification of word translations from unrelated english and german corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since the first clue does not work with non-parallel texts, the third clue is useless for the identification of the majority of pairs.
</prevsent>
<prevsent>for unrelated languages, it is not applicable anyway.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
in this situation, rapp (1995) <papid> P95-1050 </papid>proposed using clue different from the three mentioned above: his co-occurrence clue is based on the as-sumption that there is correlation between co-occurrence patterns in different languages.</citsent>
<aftsection>
<nextsent>for example, if the words teacher and school co- occur more often than expected by chance in corpus of english, then the german translations of teacher and school, lehrer and schule, should also co-occur more often than expected in corpus of german.
</nextsent>
<nextsent>infeasibility study he showed that this assumption actually holds for the language pair english/german even in the case of unrelated texts.
</nextsent>
<nextsent>when comparing an english and german co-occurrence matrix of corresponding words, he found high corre-lation between the co-occurrence patterns of the two matrices when the rows and columns of both matrices were in corresponding word order, and low correlation when the rows and col-umns were in random order.
</nextsent>
<nextsent>the validity of the co-occurrence clue is ob-vious for parallel corpora, but - as empirically shown by rapp - it also holds for non-parallel corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X924">
<title id=" P99-1067.xml">automatic identification of word translations from unrelated english and german corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>trans positions of text seg-ments have virtually no negative effect, and omissions or insertions are not critical.
</prevsent>
<prevsent>how- ever, the co-occurrence clue when applied to comparable corpora is much weaker than the word-order clue when applied to parallel cor-pora, so larger corpora and well-chosen sta-tistical methods are required.
</prevsent>
</prevsection>
<citsent citstr=" W95-0114 ">
after an attempt with context heterogeneity measure (fung, 1995) <papid> W95-0114 </papid>for identifying word translations, fung based her later work also on the co-occurrence assumption (fung &amp; yee, 1998; <papid> P98-1069 </papid>fung &amp; mckeown, 1997).<papid> W97-0119 </papid></citsent>
<aftsection>
<nextsent>by presup-posing lexicon of seed words, she avoids the prohibitively expensive computational effort en-countered by rapp (1995).<papid> P95-1050 </papid></nextsent>
<nextsent>the method des-cribed here - although developed independently of fung work - goes in the same direction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X925">
<title id=" P99-1067.xml">automatic identification of word translations from unrelated english and german corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>trans positions of text seg-ments have virtually no negative effect, and omissions or insertions are not critical.
</prevsent>
<prevsent>how- ever, the co-occurrence clue when applied to comparable corpora is much weaker than the word-order clue when applied to parallel cor-pora, so larger corpora and well-chosen sta-tistical methods are required.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
after an attempt with context heterogeneity measure (fung, 1995) <papid> W95-0114 </papid>for identifying word translations, fung based her later work also on the co-occurrence assumption (fung &amp; yee, 1998; <papid> P98-1069 </papid>fung &amp; mckeown, 1997).<papid> W97-0119 </papid></citsent>
<aftsection>
<nextsent>by presup-posing lexicon of seed words, she avoids the prohibitively expensive computational effort en-countered by rapp (1995).<papid> P95-1050 </papid></nextsent>
<nextsent>the method des-cribed here - although developed independently of fung work - goes in the same direction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X926">
<title id=" P99-1067.xml">automatic identification of word translations from unrelated english and german corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>trans positions of text seg-ments have virtually no negative effect, and omissions or insertions are not critical.
</prevsent>
<prevsent>how- ever, the co-occurrence clue when applied to comparable corpora is much weaker than the word-order clue when applied to parallel cor-pora, so larger corpora and well-chosen sta-tistical methods are required.
</prevsent>
</prevsection>
<citsent citstr=" W97-0119 ">
after an attempt with context heterogeneity measure (fung, 1995) <papid> W95-0114 </papid>for identifying word translations, fung based her later work also on the co-occurrence assumption (fung &amp; yee, 1998; <papid> P98-1069 </papid>fung &amp; mckeown, 1997).<papid> W97-0119 </papid></citsent>
<aftsection>
<nextsent>by presup-posing lexicon of seed words, she avoids the prohibitively expensive computational effort en-countered by rapp (1995).<papid> P95-1050 </papid></nextsent>
<nextsent>the method des-cribed here - although developed independently of fung work - goes in the same direction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X930">
<title id=" P99-1067.xml">automatic identification of word translations from unrelated english and german corpora </title>
<section> simulation.  </section>
<citcontext>
<prevsection>
<prevsent>the idea is to eliminate word-frequency effects and to empha-size significant word pairs by comparing their observed co-occurrence counts with their ex-pected co-occurrence counts.
</prevsent>
<prevsent>in the past, for this purpose number of measures have been pro-posed.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
they were based on mutual information (church &amp; hanks, 1989), <papid> P89-1010 </papid>conditional probabili-ties (rapp, 1996), or on some standard statisti-cal tests, such as the chi-square test or the log- likelihood ratio (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>for the pur-pose of this paper, we decided to use the log- likelihood ratio, which is theoretically well justified and more appropriate for sparse data than chi-square.
</nextsent>
<nextsent>in preliminary experiments it also led to slightly better results than the con-ditional probability measure.
</nextsent>
<nextsent>results based on mutual information or co-occurrence counts were significantly worse.
</nextsent>
<nextsent>for efficient compu-tation of the log-likelihood ratio we used the fol-lowing formula: 2 kiin - 2 log ~ = ~ ki~ log c~rj i,j~{l,2} ki ln -- kl2n = kll log c-~-+kl2 log c, r2 ? k21n -- k22 + k21 log ~ + g22 log c2r2 where 1 =kll +k12 2 =k21 +k22 l = kit + k2t rz = ki2 + k22 n=kl l+k12+k21+k22 with parameters kij expressed in terms of corpus frequencies: kl~ = frequency of common occurrence of word and word kl2 = corpus frequency of word - kll k21 = corpus frequency of word - kll k22 = size of corpus (no. of tokens) - corpus frequency of - corpus frequency of all co-occurrence vectors were transformed us-ing this formula.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X931">
<title id=" P99-1067.xml">automatic identification of word translations from unrelated english and german corpora </title>
<section> simulation.  </section>
<citcontext>
<prevsection>
<prevsent>the idea is to eliminate word-frequency effects and to empha-size significant word pairs by comparing their observed co-occurrence counts with their ex-pected co-occurrence counts.
</prevsent>
<prevsent>in the past, for this purpose number of measures have been pro-posed.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
they were based on mutual information (church &amp; hanks, 1989), <papid> P89-1010 </papid>conditional probabili-ties (rapp, 1996), or on some standard statisti-cal tests, such as the chi-square test or the log- likelihood ratio (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>for the pur-pose of this paper, we decided to use the log- likelihood ratio, which is theoretically well justified and more appropriate for sparse data than chi-square.
</nextsent>
<nextsent>in preliminary experiments it also led to slightly better results than the con-ditional probability measure.
</nextsent>
<nextsent>results based on mutual information or co-occurrence counts were significantly worse.
</nextsent>
<nextsent>for efficient compu-tation of the log-likelihood ratio we used the fol-lowing formula: 2 kiin - 2 log ~ = ~ ki~ log c~rj i,j~{l,2} ki ln -- kl2n = kll log c-~-+kl2 log c, r2 ? k21n -- k22 + k21 log ~ + g22 log c2r2 where 1 =kll +k12 2 =k21 +k22 l = kit + k2t rz = ki2 + k22 n=kl l+k12+k21+k22 with parameters kij expressed in terms of corpus frequencies: kl~ = frequency of common occurrence of word and word kl2 = corpus frequency of word - kll k21 = corpus frequency of word - kll k22 = size of corpus (no. of tokens) - corpus frequency of - corpus frequency of all co-occurrence vectors were transformed us-ing this formula.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X937">
<title id=" P99-1067.xml">automatic identification of word translations from unrelated english and german corpora </title>
<section> discussion and conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>524 german test word baby brot frau gelb h~iuschen kind kohl krankheit m~idchen musik ofen pfeifen religion schaf soldat strabe siib tabak weib whisky expected trans-lation and rank baby 1 bread 1 woman 2 yellow 1 cottage 2 child 1 cabbage 17074 sickness 86 baby bread man yellow bungalow child major disease top five translations as automatically generated child mother daughter father cheese meat food butter woman boy friend wife blue red pink green cottage house hut village daughter son father mother kohl thatcher gorbachev bush illness aids patient doctor girl 1 girl music 1 music dance stove 3 heat oven stove house whistle 3 linesman referee whistle blow offside religion 1 sheep 1 soldier 1 street 2 boy man brother lady theatre musical song burn religion culture faith religious belief sheep cattle cow pig goat soldier army troop force civilian road street city town walk sweet smell delicious taste love sweet 1 tobacco 1 white 46 whiskey 11 tobacco cigarette consumption nicotine drink know say thought see think whisky beer scotch bottle wine table 1: results for 20 of the 100 test words (for full list see http://www.fask.uni-mainz.de/user/rappl)
</prevsent>
<prevsent>the method escribed can be seen as simple case of the gradient descent method proposed by rapp (1995), <papid> P95-1050 </papid>which does not need an initial lexicon but is computationally prohibitively ex- pensive.</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
it can also be considered as an exten-sion from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by schtitze (1993), grefenstette (1994), ruge (1995), rapp (1996), lin (1998), <papid> P98-2127 </papid>and others.</citsent>
<aftsection>
<nextsent>some of these authors perform shallow or full syntactical analysis before constructing the co-occurrence vectors.
</nextsent>
<nextsent>others reduce the size of the co-occurrence matrices by performing singular value decomposition.
</nextsent>
<nextsent>however, in yet un-published work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better esults than the approach described here when applied to the monolingual case (see also grefenstette, 1993), <papid> W93-0113 </papid>so we did not try to include these me-thods in our system.</nextsent>
<nextsent>nevertheless, both methods are of technical value since they lead to re-duction in the size of the co-occurrence matri- ces.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X938">
<title id=" P99-1067.xml">automatic identification of word translations from unrelated english and german corpora </title>
<section> discussion and conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>some of these authors perform shallow or full syntactical analysis before constructing the co-occurrence vectors.
</prevsent>
<prevsent>others reduce the size of the co-occurrence matrices by performing singular value decomposition.
</prevsent>
</prevsection>
<citsent citstr=" W93-0113 ">
however, in yet un-published work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better esults than the approach described here when applied to the monolingual case (see also grefenstette, 1993), <papid> W93-0113 </papid>so we did not try to include these me-thods in our system.</citsent>
<aftsection>
<nextsent>nevertheless, both methods are of technical value since they lead to re-duction in the size of the co-occurrence matri-ces.
</nextsent>
<nextsent>future work has to approach the difficult problem of ambiguity resolution, which has not been dealt with here.
</nextsent>
<nextsent>one possibility would be to semantically disambiguate he words in the corpora beforehand, another to look at co-oc- curren ces between significant word sequences instead of co-occurrences between single words.
</nextsent>
<nextsent>to conclude with, let us add some specula-tion by mentioning that the ability to identify word translations from non-parallel texts can be seen as an indicator in favor of the association ist view of human language acquisition (see also landauer &amp; dumais, 1997, and wettler &amp; rapp, 1993).<papid> W93-0310 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X939">
<title id=" P99-1067.xml">automatic identification of word translations from unrelated english and german corpora </title>
<section> discussion and conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>future work has to approach the difficult problem of ambiguity resolution, which has not been dealt with here.
</prevsent>
<prevsent>one possibility would be to semantically disambiguate he words in the corpora beforehand, another to look at co-oc- curren ces between significant word sequences instead of co-occurrences between single words.
</prevsent>
</prevsection>
<citsent citstr=" W93-0310 ">
to conclude with, let us add some specula-tion by mentioning that the ability to identify word translations from non-parallel texts can be seen as an indicator in favor of the association ist view of human language acquisition (see also landauer &amp; dumais, 1997, and wettler &amp; rapp, 1993).<papid> W93-0310 </papid></citsent>
<aftsection>
<nextsent>it gives us an idea of how it is possible to derive the meaning of unknown words from texts by only presupposing limited number of known words and then iteratively expanding this knowledge base.
</nextsent>
<nextsent>one possibility to get the pro- 525 cess going would be to learn vocabulary lists as in school, another to simply acquire the names of items in the physical world.
</nextsent>
<nextsent>acknowledgements thank manfred wettler, gisela zunker-rapp, wolfgang lezius, and anita todd for their sup-port of this work.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X940">
<title id=" P98-2242.xml">embedding new information into referring expressions </title>
<section> components of referring expression.  </section>
<citcontext>
<prevsection>
<prevsent>these- mantic constraint will be introduced in section 3.
</prevsent>
<prevsent>on the other hand, the possibility of adding non- referring part can make some realisations of ref-erent preferred over others.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
when generating re-ferring expressions, multiple factors should be con-sidered, which include centering theory (grosz et al, 1995) <papid> J95-2003 </papid>and stylistic preferences such as avoid-ing too many repetitions.</citsent>
<aftsection>
<nextsent>if we are to satisfy all constraints to some extent, we may need to con-sider more than one possible realisation of refer-ent, choosing among those that do not significantly affect the coherence of the text.
</nextsent>
<nextsent>then one of the realisations that is most suitable for adding new in-formation can be selected.
</nextsent>
<nextsent>a great amount of work has been done on gener-ating various types of referring expressions, which addresses the referring part, while little has ad-dressed the generation issues with respect the other part, except hat in (scott and desouza, 1990), the relation between embedding and rhetorical rela-tions is discussed and several heuristics for combin-ing sentences using embedding are given.
</nextsent>
<nextsent>but this is far from enough for generating an appropriate - ferring expression.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X941">
<title id=" S12-1006.xml">the use of granularity in rhetorical relation prediction </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>rhetorical relation prediction has received considerable attention and has been shown to be useful for text summarization (marcu, 1998).
</prevsent>
<prevsent>prediction tasks relyon number of features (discourse connectives, part of speech, etc.)
</prevsent>
</prevsection>
<citsent citstr=" P02-1047 ">
(marcu and echihabi, 2002; <papid> P02-1047 </papid>lapata and lascarides, 2004).<papid> N04-1020 </papid></citsent>
<aftsection>
<nextsent>a wide range of accuracies are also reported - 33.96% (marcu and echihabi, 2002) <papid> P02-1047 </papid>to 70.70% (lapata and lascarides,2004) <papid> N04-1020 </papid>for all rhetorical relations and, for individual relations, contrast (43.64%) and continuation (83.35%) (sporleder and lascarides, 2005).</nextsent>
<nextsent>44 we seek to predict the inventory of rhetorical relations defined in segmented discourse representation theory (sdrt?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X942">
<title id=" S12-1006.xml">the use of granularity in rhetorical relation prediction </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>rhetorical relation prediction has received considerable attention and has been shown to be useful for text summarization (marcu, 1998).
</prevsent>
<prevsent>prediction tasks relyon number of features (discourse connectives, part of speech, etc.)
</prevsent>
</prevsection>
<citsent citstr=" N04-1020 ">
(marcu and echihabi, 2002; <papid> P02-1047 </papid>lapata and lascarides, 2004).<papid> N04-1020 </papid></citsent>
<aftsection>
<nextsent>a wide range of accuracies are also reported - 33.96% (marcu and echihabi, 2002) <papid> P02-1047 </papid>to 70.70% (lapata and lascarides,2004) <papid> N04-1020 </papid>for all rhetorical relations and, for individual relations, contrast (43.64%) and continuation (83.35%) (sporleder and lascarides, 2005).</nextsent>
<nextsent>44 we seek to predict the inventory of rhetorical relations defined in segmented discourse representation theory (sdrt?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X945">
<title id=" S12-1006.xml">the use of granularity in rhetorical relation prediction </title>
<section> data and methods.  </section>
<citcontext>
<prevsection>
<prevsent>positive 515 (47%) background 315 (61%) elaboration 161 (31%) explanation 39 (7%) negative 59 (5%) consequence 16 (26%) result 43 (71%) maintenance 490 (44%) alternation 76 (14%) continuation 30 (6%) narration 384 (78%) embedded (em) or not - and granularity shift categories which are an organization of the sdrt rhetorical relations (asher and lascarides, 2003), summarized in table 1.all 25 discourses were annotated by one of the authors using only reference sheet.
</prevsent>
<prevsent>the other author independently coded 80% of the data (20 discourses,four from each mode).
</prevsent>
</prevsection>
<citsent citstr=" W11-0119 ">
average agreement and cohens kappa (cohen, 1960) statistics were computed and are within acceptable ranges: tense (99.65 / .9945), aspect (99.30 / .9937), sdrt (77.42 / .6850), and event (75.88 / .6362).these results are consistent with previously reported annotations for rhetorical relations (sporleder and lascarides, 2005; howald and katz, 2011), <papid> W11-0119 </papid>event verbs and durations, tense and aspect (puscasu and mititelu, 2008; <papid> L08-1563 </papid>wiebe et al, 1997).<papid> W97-0320 </papid></citsent>
<aftsection>
<nextsent>positive,negative and maintained granularities were not annotated, but mm report kappa between .8500 and 1.
</nextsent>
<nextsent>the distribution of these granularities, based on.
</nextsent>
<nextsent>the organization of the annotated rhetorical relations is presented in table 1.
</nextsent>
<nextsent>3.2 machine learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X946">
<title id=" S12-1006.xml">the use of granularity in rhetorical relation prediction </title>
<section> data and methods.  </section>
<citcontext>
<prevsection>
<prevsent>positive 515 (47%) background 315 (61%) elaboration 161 (31%) explanation 39 (7%) negative 59 (5%) consequence 16 (26%) result 43 (71%) maintenance 490 (44%) alternation 76 (14%) continuation 30 (6%) narration 384 (78%) embedded (em) or not - and granularity shift categories which are an organization of the sdrt rhetorical relations (asher and lascarides, 2003), summarized in table 1.all 25 discourses were annotated by one of the authors using only reference sheet.
</prevsent>
<prevsent>the other author independently coded 80% of the data (20 discourses,four from each mode).
</prevsent>
</prevsection>
<citsent citstr=" L08-1563 ">
average agreement and cohens kappa (cohen, 1960) statistics were computed and are within acceptable ranges: tense (99.65 / .9945), aspect (99.30 / .9937), sdrt (77.42 / .6850), and event (75.88 / .6362).these results are consistent with previously reported annotations for rhetorical relations (sporleder and lascarides, 2005; howald and katz, 2011), <papid> W11-0119 </papid>event verbs and durations, tense and aspect (puscasu and mititelu, 2008; <papid> L08-1563 </papid>wiebe et al, 1997).<papid> W97-0320 </papid></citsent>
<aftsection>
<nextsent>positive,negative and maintained granularities were not annotated, but mm report kappa between .8500 and 1.
</nextsent>
<nextsent>the distribution of these granularities, based on.
</nextsent>
<nextsent>the organization of the annotated rhetorical relations is presented in table 1.
</nextsent>
<nextsent>3.2 machine learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X947">
<title id=" S12-1006.xml">the use of granularity in rhetorical relation prediction </title>
<section> data and methods.  </section>
<citcontext>
<prevsection>
<prevsent>positive 515 (47%) background 315 (61%) elaboration 161 (31%) explanation 39 (7%) negative 59 (5%) consequence 16 (26%) result 43 (71%) maintenance 490 (44%) alternation 76 (14%) continuation 30 (6%) narration 384 (78%) embedded (em) or not - and granularity shift categories which are an organization of the sdrt rhetorical relations (asher and lascarides, 2003), summarized in table 1.all 25 discourses were annotated by one of the authors using only reference sheet.
</prevsent>
<prevsent>the other author independently coded 80% of the data (20 discourses,four from each mode).
</prevsent>
</prevsection>
<citsent citstr=" W97-0320 ">
average agreement and cohens kappa (cohen, 1960) statistics were computed and are within acceptable ranges: tense (99.65 / .9945), aspect (99.30 / .9937), sdrt (77.42 / .6850), and event (75.88 / .6362).these results are consistent with previously reported annotations for rhetorical relations (sporleder and lascarides, 2005; howald and katz, 2011), <papid> W11-0119 </papid>event verbs and durations, tense and aspect (puscasu and mititelu, 2008; <papid> L08-1563 </papid>wiebe et al, 1997).<papid> W97-0320 </papid></citsent>
<aftsection>
<nextsent>positive,negative and maintained granularities were not annotated, but mm report kappa between .8500 and 1.
</nextsent>
<nextsent>the distribution of these granularities, based on.
</nextsent>
<nextsent>the organization of the annotated rhetorical relations is presented in table 1.
</nextsent>
<nextsent>3.2 machine learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X948">
<title id=" S12-1006.xml">the use of granularity in rhetorical relation prediction </title>
<section> discussion and conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>where clause, tense, aspect and event are readily automata ble, rhetorical relations and granularity are less so.
</prevsent>
<prevsent>automatically extracting such information from an annotated corpus such as the penn discourse tree bank is certainly feasible.
</prevsent>
</prevsection>
<citsent citstr=" P09-1076 ">
however, the distribution of gen resin this corpus is somewhat limited (i.e., predominately news text (webber, 2009)) <papid> P09-1076 </papid>and calls into question the generalize ability of results to other modes of discourse.</citsent>
<aftsection>
<nextsent>overall, we have demonstrated that the inclusion of granularity-based organization in the machine learning prediction of rhetorical relations increases performance by 37%, which is roughly 14% above previous reported results for broader range of discourses and relations.
</nextsent>
<nextsent>47 acknowledgments thank you to jeff ondich and ultra lingua for facilitating this research and to four anonymous *sem reviewers for insightful and constructive comments.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X949">
<title id=" S10-1012.xml">semeval2010 task japanese wsd </title>
<section> japanese word sense disambiguation (wsd).  </section>
<citcontext>
<prevsection>
<prevsent>all previous japanese sense-tagged corpora.
</prevsent>
<prevsent>were from newspaper articles, while sense tagged corpora were constructed in english on balanced corpora, such as brown corpus and bnc corpus.
</prevsent>
</prevsection>
<citsent citstr=" I08-7018 ">
the first balanced corpus of contemporary written japanese (bccwj corpus) is now being constructed as part of national project in japan (maekawa, 2008), <papid> I08-7018 </papid>and we are now constructing sense-tagged corpus based on it.</citsent>
<aftsection>
<nextsent>therefore, the task will use the first balanced japanese sense-tagged corpus.because balanced corpus consists of documents from multiple genres, the corpus can be divided into multiple sub-corpora of genre.
</nextsent>
<nextsent>in supervised learning approaches on word sense disambiguation, because word sense distribution might vary across different sub-corpora, we need to take into account the genres of training and test corpora.
</nextsent>
<nextsent>therefore, word sense disambiguation on balanced corpus requires tackling kind of do main (genre) adaptation problem (chang and ng, 2006; agirre and de lacalle, 2008).
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X950">
<title id=" S10-1012.xml">semeval2010 task japanese wsd </title>
<section> japanese word sense disambiguation (wsd).  </section>
<citcontext>
<prevsection>
<prevsent>the annotated information in the training data is as follows: ? morphological information the document was annotated with morphological information (word boundaries, partof-speech (pos) tag, base form, and reading) for all words.
</prevsent>
<prevsent>all the morphological information was automatically annotated usingchasen2 with unidic and was manually post edited.
</prevsent>
</prevsection>
<citsent citstr=" P97-1008 ">
1due to space limits, we unfortunately cannot present the statistics of the training and test data, such as the number of instances in different genres, the number of instances for new word sense, and the jensen shannon (js) divergence(lin, 1991; dagan et al, 1997) <papid> P97-1008 </papid>between the word sense distributions of two different genres.</citsent>
<aftsection>
<nextsent>we hope we will present them in another paper in the near future.
</nextsent>
<nextsent>2http://chasen-legacy.sourceforge.jp/ ? genre code each document was assigned code indicating its genre from the aforementioned list.
</nextsent>
<nextsent>word sense ids 3,437 word types in the data were annotated for sense ids, and the data contain 31,611sense-tagged instances that include 2,500 instances for the 50 target words.
</nextsent>
<nextsent>words assigned with sense ids satisfied the following conditions: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X951">
<title id=" S12-1059.xml">ukp computing semantic textual similarity by combining multiple content similarity measures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our final models, one per dataset, consist of log-linear combination of about 20 features, out of the possible 300+ features implemented.
</prevsent>
<prevsent>the goal of the pilot semantic textual similarity(sts) task at semeval-2012 is to measure the degree of semantic equivalence between pairs of sentences.
</prevsent>
</prevsection>
<citsent citstr=" P02-1020 ">
sts is fundamental to variety of tasks and applications such as question answering (lin and pantel, 2001), text reuse detection (clough et al., 2002) <papid> P02-1020 </papid>or automatic essay grading (attali and burstein, 2006).</citsent>
<aftsection>
<nextsent>sts is also closely related to textual entailment (te) (dagan et al, 2006) and paraphrase recognition (dolan et al, 2004).
</nextsent>
<nextsent>it differs from both tasks, though, insofar as those operate on binary similarity decisions while sts is defined as graded notion of similarity.
</nextsent>
<nextsent>sts further requires bidirectional similarity relationship to hold betweena pair of sentences rather than unidirectional entailment relation as for the te task.a multitude of measures for computing similarity between texts have been proposed in the past based on surface-level and/or semantic content features (mihalcea et al, 2006; landauer et al, 1998;gabrilovich and markovitch, 2007).
</nextsent>
<nextsent>the existing measures exhibit two major limitations, though: firstly, measures are typically used in separation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X952">
<title id=" S12-1059.xml">ukp computing semantic textual similarity by combining multiple content similarity measures </title>
<section> results on test data.  </section>
<citcontext>
<prevsection>
<prevsent>the provided baseline is shown at the bottom of this table.
</prevsent>
<prevsent>metrics all (r = .823)4 and mean (r = .677), and #2 for allnrm (r = .857).
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
an exhaustive overview of all participating systems can be found in the sts task description (agirre et al, 2012).<papid> S12-1051 </papid></citsent>
<aftsection>
<nextsent>in this paper, we presented the ukp system, which performed best across the three official evaluation metrics in the pilot semantic textual similarity (sts) task at semeval-2012.
</nextsent>
<nextsent>while we did not reach the highest scores on any of the single datasets, our system was most robust across different data.
</nextsent>
<nextsent>in future work, it would be interesting to inspect the performance of system that combines the output of all participating systems in single linear model.
</nextsent>
<nextsent>we also propose that two major issues with the datasets are tackled in future work: (a) it is unclear how to judge similarity between pairs of texts which contain contextual references such as on monday vs. after the thanksgiving weekend.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X953">
<title id=" S10-1009.xml">semeval2010 task 12 parser evaluation using textual ent ailments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(no)pete is an evaluation scheme based on natural human linguistic competence (i.e. the ability to comprehend sentences and answer simple yes/no questions about them).
</prevsent>
<prevsent>we believe systems should try to model natural human linguistic competence rather than their dubious competence in artificial tagging tasks.
</prevsent>
</prevsection>
<citsent citstr=" H91-1060 ">
the parseval measures introduced nearly two decades ago (black et al, 1991) <papid> H91-1060 </papid>still dominate the field of parser evaluation.</citsent>
<aftsection>
<nextsent>these methods compare phrase-structure bracketings produced by the parser with bracketings in the annotated corpus, or treebank?.
</nextsent>
<nextsent>parser evaluation using short textualentailments has the following advantages compared to treebank based evaluation.consistency: recognizing syntactic entail ments is more natural task for people than treebank annotation.
</nextsent>
<nextsent>focusing on natural human competence makes it practical to collect high quality evaluation data from untrained annotators.
</nextsent>
<nextsent>the pete dataset was annotated by untrained amazon mechanical turk workers at an insignificant cost and each annotation is based on the unanimous agreement of at least three workers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X954">
<title id=" S10-1009.xml">semeval2010 task 12 parser evaluation using textual ent ailments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the pete dataset was annotated by untrained amazon mechanical turk workers at an insignificant cost and each annotation is based on the unanimous agreement of at least three workers.
</prevsent>
<prevsent>in contrast, of the 36306 constituent strings that appear multiple times in the penn treebank (marcus et al, 1994), 5646 (15%) have multiple conflicting annotations.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
if indicative of the general level of inconsistency, 15% is very high number given that the state of the art parsers claim f-scores above 90% (charniak and johnson, 2005).<papid> P05-1022 </papid>relevance: pete automatically focuses attention on semantically relevant phenomena rather than differences in annotation style or linguistic convention.</citsent>
<aftsection>
<nextsent>whether phrase is tagged adjp vsadvp rarely affects semantic interpretation.
</nextsent>
<nextsent>attaching the wrong subject to verb or the wrong prepositional phrase to noun changes the meaning of the sentence.
</nextsent>
<nextsent>standard treebank based evaluation metrics do not distinguish between semantically relevant and irrelevant errors (bonnema etal., 1997).<papid> P97-1021 </papid></nextsent>
<nextsent>in pete semantically relevant differences lead to different ent ailments, semantically irrelevant differences do not.framework independence: entailment recognition is formalism independent task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X955">
<title id=" S10-1009.xml">semeval2010 task 12 parser evaluation using textual ent ailments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>whether phrase is tagged adjp vsadvp rarely affects semantic interpretation.
</prevsent>
<prevsent>attaching the wrong subject to verb or the wrong prepositional phrase to noun changes the meaning of the sentence.
</prevsent>
</prevsection>
<citsent citstr=" P97-1021 ">
standard treebank based evaluation metrics do not distinguish between semantically relevant and irrelevant errors (bonnema etal., 1997).<papid> P97-1021 </papid></citsent>
<aftsection>
<nextsent>in pete semantically relevant differences lead to different ent ailments, semantically irrelevant differences do not.framework independence: entailment recognition is formalism independent task.
</nextsent>
<nextsent>a common evaluation method for parsers that do not use the penn treebank formalism is to automatically convert the penn treebank to the appropriate formalism and to perform treebank based evaluation (nivre et al, 2007a; hockenmaier and steedman, 51 2007).
</nextsent>
<nextsent>the inevitable conversion errors compound the already mentioned problems of treebank basedevaluation.
</nextsent>
<nextsent>in addition, manually designed treebanks do not naturally lend themselves to unsupervised parser evaluation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X958">
<title id=" S10-1009.xml">semeval2010 task 12 parser evaluation using textual ent ailments </title>
<section> baselines.  </section>
<citcontext>
<prevsection>
<prevsent>gr ent ailments direct object 42% nominal subject 33% reduced relative clause 21% relative clause 13% passive nominal subject 6% object of preposition 5% prepositional modifier 4% conjunct 2% adverbial modifier 2% free relative 2%table 1: most frequent grammatical relations encountered in the entailments.
</prevsent>
<prevsent>in order to establish baseline results for this task, we built an entailment decision system for conll format dependency files and tested several publicly available parsers.
</prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
the parsers used were the berkeley parser (petrov and klein, 2007), charniak parser (charniak and johnson, 2005), <papid> P05-1022 </papid>collins parser (collins, 2003), <papid> J03-4003 </papid>malt parser (nivre et al, 2007b), mst parser (mcdonald et al, 2005) <papid> H05-1066 </papid>and stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>each parser was trained on sections 02-21 of the wsj section of penn treebank.
</nextsent>
<nextsent>outputs of phrase structure parsers were automatically annotated with function tags using blah etas function tagger (blaheta and charniak, 2000) <papid> A00-2031 </papid>and converted tothe dependency structure with lth constituent to-dependency conversion tool (johansson and nugues, 2007).</nextsent>
<nextsent>to decide the ent ailments both the test and hypothesis sentences were parsed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X959">
<title id=" S10-1009.xml">semeval2010 task 12 parser evaluation using textual ent ailments </title>
<section> baselines.  </section>
<citcontext>
<prevsection>
<prevsent>gr ent ailments direct object 42% nominal subject 33% reduced relative clause 21% relative clause 13% passive nominal subject 6% object of preposition 5% prepositional modifier 4% conjunct 2% adverbial modifier 2% free relative 2%table 1: most frequent grammatical relations encountered in the entailments.
</prevsent>
<prevsent>in order to establish baseline results for this task, we built an entailment decision system for conll format dependency files and tested several publicly available parsers.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
the parsers used were the berkeley parser (petrov and klein, 2007), charniak parser (charniak and johnson, 2005), <papid> P05-1022 </papid>collins parser (collins, 2003), <papid> J03-4003 </papid>malt parser (nivre et al, 2007b), mst parser (mcdonald et al, 2005) <papid> H05-1066 </papid>and stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>each parser was trained on sections 02-21 of the wsj section of penn treebank.
</nextsent>
<nextsent>outputs of phrase structure parsers were automatically annotated with function tags using blah etas function tagger (blaheta and charniak, 2000) <papid> A00-2031 </papid>and converted tothe dependency structure with lth constituent to-dependency conversion tool (johansson and nugues, 2007).</nextsent>
<nextsent>to decide the ent ailments both the test and hypothesis sentences were parsed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X961">
<title id=" S10-1009.xml">semeval2010 task 12 parser evaluation using textual ent ailments </title>
<section> baselines.  </section>
<citcontext>
<prevsection>
<prevsent>gr ent ailments direct object 42% nominal subject 33% reduced relative clause 21% relative clause 13% passive nominal subject 6% object of preposition 5% prepositional modifier 4% conjunct 2% adverbial modifier 2% free relative 2%table 1: most frequent grammatical relations encountered in the entailments.
</prevsent>
<prevsent>in order to establish baseline results for this task, we built an entailment decision system for conll format dependency files and tested several publicly available parsers.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
the parsers used were the berkeley parser (petrov and klein, 2007), charniak parser (charniak and johnson, 2005), <papid> P05-1022 </papid>collins parser (collins, 2003), <papid> J03-4003 </papid>malt parser (nivre et al, 2007b), mst parser (mcdonald et al, 2005) <papid> H05-1066 </papid>and stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>each parser was trained on sections 02-21 of the wsj section of penn treebank.
</nextsent>
<nextsent>outputs of phrase structure parsers were automatically annotated with function tags using blah etas function tagger (blaheta and charniak, 2000) <papid> A00-2031 </papid>and converted tothe dependency structure with lth constituent to-dependency conversion tool (johansson and nugues, 2007).</nextsent>
<nextsent>to decide the ent ailments both the test and hypothesis sentences were parsed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X963">
<title id=" S10-1009.xml">semeval2010 task 12 parser evaluation using textual ent ailments </title>
<section> baselines.  </section>
<citcontext>
<prevsection>
<prevsent>the parsers used were the berkeley parser (petrov and klein, 2007), charniak parser (charniak and johnson, 2005), <papid> P05-1022 </papid>collins parser (collins, 2003), <papid> J03-4003 </papid>malt parser (nivre et al, 2007b), mst parser (mcdonald et al, 2005) <papid> H05-1066 </papid>and stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></prevsent>
<prevsent>each parser was trained on sections 02-21 of the wsj section of penn treebank.</prevsent>
</prevsection>
<citsent citstr=" A00-2031 ">
outputs of phrase structure parsers were automatically annotated with function tags using blah etas function tagger (blaheta and charniak, 2000) <papid> A00-2031 </papid>and converted tothe dependency structure with lth constituent to-dependency conversion tool (johansson and nugues, 2007).</citsent>
<aftsection>
<nextsent>to decide the ent ailments both the test and hypothesis sentences were parsed.
</nextsent>
<nextsent>all the content words in the hypothesis sentence were determined by using part-of-speech tags and dependency relations.
</nextsent>
<nextsent>after applying some heuristics such as active-passive conversion, the extracted dependency path between the content words was searched in the dependency graph of the test sentence.
</nextsent>
<nextsent>in this search process, same relation types for the direct relations between the content word pairs and isomorphic subgraphs in the test and hypothesis sentences were required for the yes?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X964">
<title id=" S10-1009.xml">semeval2010 task 12 parser evaluation using textual ent ailments </title>
<section> systems.  </section>
<citcontext>
<prevsection>
<prevsent>baseline of 51.83%.
</prevsent>
<prevsent>most systems started the entailment decision process by extracting syntactic dependencies, grammatical relations, or predicates by parsing thetext and hypothesis sentences.
</prevsent>
</prevsection>
<citsent citstr=" J07-4004 ">
several submissions, including the top two scoring systems used the c&c; parser (clark and curran, 2007) <papid> J07-4004 </papid>which is based on combinatory categorical grammar(ccg) formalism.</citsent>
<aftsection>
<nextsent>others used dependency structures produced by malt parser (nivre et al, 2007b), mst parser (mcdonald et al, 2005) <papid> H05-1066 </papid>and stanford parser (klein and manning, 2003).<papid> P03-1054 </papid>after the parsing step, the decision for the entailment was based on the comparison of relations, predicates, or dependency paths between the textand the hypothesis.</nextsent>
<nextsent>most systems relied on heuristic methods of comparison.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X969">
<title id=" S10-1027.xml">uhd cross lingual word sense disambiguation using multilingual cooccurrence graphs </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>these typically transform knowledge resource such as wordnet (fellbaum, 1998) into graph and apply graph algorithms to perform wsd.
</prevsent>
<prevsent>in our work, we follow this line of research and apply graph-based methods to multilingual co-occurrence graphs which are automatically created from parallel corpora.
</prevsent>
</prevsection>
<citsent citstr=" W06-1669 ">
our method is heavily inspired by previous proposals from veronis (2004, hyperlex) and agirre et al (2006).<papid> W06-1669 </papid></citsent>
<aftsection>
<nextsent>hyper lex performs graph-based wsd based on co-occurrence graphs: given monolingual corpus, for each target word graphis built where nodes represent content words cooccurring with the target word in context, and edges connect the words which co-occur in these contexts.
</nextsent>
<nextsent>the second step iteratively selects the node with highest degree in the graph (root hub) and removes it along with its adjacent nodes.
</nextsent>
<nextsent>each such selection corresponds to isolating high density component of the graph, in order to select sense of the target word.
</nextsent>
<nextsent>in the last step the root hubs are linked to the target word and the minimum spanning tree (mst) of the graph is computed to disambiguate the target word in context.agirre et al (2006) <papid> W06-1669 </papid>compare hyper lex with an alternative method to detect the root hubs based on page rank (brin and page, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X971">
<title id=" S10-1027.xml">uhd cross lingual word sense disambiguation using multilingual cooccurrence graphs </title>
<section> graph-based cross-lingual wsd.  </section>
<citcontext>
<prevsection>
<prevsent>(v , t, l ) 16: for each i ? l 17: for each j ? l , 6= 18: if i and j co-occur in l then 19: ml ? ml ?
</prevsent>
<prevsent>(v , j ) 20: return ml (v , t, l ) receives translation label t. formally, let v ? s be the contexts where sand co occur, and v lthe word-aligned contexts in language of v , where s is translated as l . then.
</prevsent>
</prevsection>
<citsent citstr=" P09-1030 ">
each edge between nodes s and l is labeled with translation label (lines 14-15): this includes translation of in v l, its frequency of translation and the information of whether the translation is monosemous, as found in multilingual dictionary, i.e. euro wordnet (vossen, 1998) and pan dictionary (mausam et al, 2009).<papid> P09-1030 </papid></citsent>
<aftsection>
<nextsent>finally, the multilingual graph is further extended by inserting all possible co-occurrence edges (v , j ) ? lbetween the nodes for the target language (lines 16 19, i.e. we apply the step from section 3.1 to and l ).
</nextsent>
<nextsent>as result of the algorithm, the multilingual graph is returned (line 20).
</nextsent>
<nextsent>3.3 computing root hubs.
</nextsent>
<nextsent>we compute the root hubs in the multilingual graph to discriminate the senses of the target word in the source language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X973">
<title id=" S10-1027.xml">uhd cross lingual word sense disambiguation using multilingual cooccurrence graphs </title>
<section> results and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>we submitted two runs for the task (uhd-1 and uhd-2 henceforth).
</prevsent>
<prevsent>since we were interested in assessing the impact of using different resources with our methodology, we automatically built multilingual graphs from different sentence-aligned corpora, i.e. europarl (koehn, 2005) for uhd-1, augmented with the jrc-acquis corpus (steinberger et al, 2006) for uhd-2 1 . both corpora were tagged and lemma-.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
tized with tree tagger (schmid, 1994) and word aligned using giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>for german, in order to avoid the sparseness deriving from the high productivity of compounds, we performed morphological analysis using morphisto (zielinski et al, 2009).
</nextsent>
<nextsent>to build the multilingual graph (section 3.2),we used minimum frequency threshold of 2 occurrences for word to be inserted as node, and retained only those edges with weight lessor equal to 0.7.
</nextsent>
<nextsent>after constructing the multilingual graph, we additionally removed those translations with frequency count lower than 10 (7 in the case of german, due to the large amount of compounds).
</nextsent>
<nextsent>finally, the translations generated for the best evaluation setting were obtained by applying the following rule onto the ranked answer translations: add translation tr while count(tr ) ? count(tr i1 )/3, where is the i-th ranked translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X976">
<title id=" P98-2244.xml">optimal multi paragraph text segmentation by dynamic programming </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>what the user often is after, however, is mi- crodocument: part of the document that contains the occurrences and is reasonably self-contained.
</prevsent>
<prevsent>micro documents can be created by utilizing lex-ical cohesion (term repetition and semantic rela- tions) present in the text.
</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
there exist several meth-ods of calculating similarity curve, or sequence of similarity values, representing the lexical cohe-sion of successive constituents ( uch as paragraphs) of text (see, e.g., (hearst, 1994; <papid> P94-1002 </papid>hearst, 1997; <papid> J97-1003 </papid>koz- ima, 1993; <papid> P93-1041 </papid>morris and hirst, 1991; <papid> J91-1002 </papid>yaari, 1997; youmans, 1991)).</citsent>
<aftsection>
<nextsent>methods for deciding the loca-tions of fragment boundaries are, however, not that common, and those that exist are often rather heuris-tic in nature.
</nextsent>
<nextsent>to evaluate our fragmentation method, to be ex-plained in section 2, we calculate the paragraph similarities as follows.
</nextsent>
<nextsent>we employ stemming, re-move stop words, and count the frequencies of the remaining words, i.e., terms.
</nextsent>
<nextsent>then we take pre-defined number, e.g., 50, of the most frequent terms to represent the paragraph, and count the similar-ity using the cosine coefficient (see, e.g., (salton, 1989)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X977">
<title id=" P98-2244.xml">optimal multi paragraph text segmentation by dynamic programming </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>what the user often is after, however, is mi- crodocument: part of the document that contains the occurrences and is reasonably self-contained.
</prevsent>
<prevsent>micro documents can be created by utilizing lex-ical cohesion (term repetition and semantic rela- tions) present in the text.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
there exist several meth-ods of calculating similarity curve, or sequence of similarity values, representing the lexical cohe-sion of successive constituents ( uch as paragraphs) of text (see, e.g., (hearst, 1994; <papid> P94-1002 </papid>hearst, 1997; <papid> J97-1003 </papid>koz- ima, 1993; <papid> P93-1041 </papid>morris and hirst, 1991; <papid> J91-1002 </papid>yaari, 1997; youmans, 1991)).</citsent>
<aftsection>
<nextsent>methods for deciding the loca-tions of fragment boundaries are, however, not that common, and those that exist are often rather heuris-tic in nature.
</nextsent>
<nextsent>to evaluate our fragmentation method, to be ex-plained in section 2, we calculate the paragraph similarities as follows.
</nextsent>
<nextsent>we employ stemming, re-move stop words, and count the frequencies of the remaining words, i.e., terms.
</nextsent>
<nextsent>then we take pre-defined number, e.g., 50, of the most frequent terms to represent the paragraph, and count the similar-ity using the cosine coefficient (see, e.g., (salton, 1989)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X978">
<title id=" P98-2244.xml">optimal multi paragraph text segmentation by dynamic programming </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>what the user often is after, however, is mi- crodocument: part of the document that contains the occurrences and is reasonably self-contained.
</prevsent>
<prevsent>micro documents can be created by utilizing lex-ical cohesion (term repetition and semantic rela- tions) present in the text.
</prevsent>
</prevsection>
<citsent citstr=" P93-1041 ">
there exist several meth-ods of calculating similarity curve, or sequence of similarity values, representing the lexical cohe-sion of successive constituents ( uch as paragraphs) of text (see, e.g., (hearst, 1994; <papid> P94-1002 </papid>hearst, 1997; <papid> J97-1003 </papid>koz- ima, 1993; <papid> P93-1041 </papid>morris and hirst, 1991; <papid> J91-1002 </papid>yaari, 1997; youmans, 1991)).</citsent>
<aftsection>
<nextsent>methods for deciding the loca-tions of fragment boundaries are, however, not that common, and those that exist are often rather heuris-tic in nature.
</nextsent>
<nextsent>to evaluate our fragmentation method, to be ex-plained in section 2, we calculate the paragraph similarities as follows.
</nextsent>
<nextsent>we employ stemming, re-move stop words, and count the frequencies of the remaining words, i.e., terms.
</nextsent>
<nextsent>then we take pre-defined number, e.g., 50, of the most frequent terms to represent the paragraph, and count the similar-ity using the cosine coefficient (see, e.g., (salton, 1989)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X979">
<title id=" P98-2244.xml">optimal multi paragraph text segmentation by dynamic programming </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>what the user often is after, however, is mi- crodocument: part of the document that contains the occurrences and is reasonably self-contained.
</prevsent>
<prevsent>micro documents can be created by utilizing lex-ical cohesion (term repetition and semantic rela- tions) present in the text.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
there exist several meth-ods of calculating similarity curve, or sequence of similarity values, representing the lexical cohe-sion of successive constituents ( uch as paragraphs) of text (see, e.g., (hearst, 1994; <papid> P94-1002 </papid>hearst, 1997; <papid> J97-1003 </papid>koz- ima, 1993; <papid> P93-1041 </papid>morris and hirst, 1991; <papid> J91-1002 </papid>yaari, 1997; youmans, 1991)).</citsent>
<aftsection>
<nextsent>methods for deciding the loca-tions of fragment boundaries are, however, not that common, and those that exist are often rather heuris-tic in nature.
</nextsent>
<nextsent>to evaluate our fragmentation method, to be ex-plained in section 2, we calculate the paragraph similarities as follows.
</nextsent>
<nextsent>we employ stemming, re-move stop words, and count the frequencies of the remaining words, i.e., terms.
</nextsent>
<nextsent>then we take pre-defined number, e.g., 50, of the most frequent terms to represent the paragraph, and count the similar-ity using the cosine coefficient (see, e.g., (salton, 1989)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X981">
<title id=" S12-1047.xml">semeval2012 task 2 measuring degrees of relational similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as part of the task, we have assembled the first dataset of graded relational similarity ratings across 79 relation categories.
</prevsent>
<prevsent>three teams submitted six systems, which were evaluated using two methods.
</prevsent>
</prevsection>
<citsent citstr=" J06-3003 ">
relational similarity measures the degree of correspondence between two relations, where instance pairs that have high relational similarity are said tobe analogous, i.e., to express the same relation (tur ney, 2006).<papid> J06-3003 </papid></citsent>
<aftsection>
<nextsent>however, class of analogous relations may still have significant variability in the degree of relational similarity of its members.
</nextsent>
<nextsent>consider the four word pairs dog:bark, cat:meow, floor:squeak, and car:honk.
</nextsent>
<nextsent>we could say that these four x:ypairs are all instances of the semantic relation entity:sound; that is, is an entity that characteristically makes the sound . within class of analogous pairs, certain pairs are more characteristic of the relation.
</nextsent>
<nextsent>for example, many would agree that dog:bark and cat:meow are better prototypes of the entity:sound relation than floor:squeak.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X982">
<title id=" S12-1047.xml">semeval2012 task 2 measuring degrees of relational similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, many would agree that dog:bark and cat:meow are better prototypes of the entity:sound relation than floor:squeak.
</prevsent>
<prevsent>our task requires automatic systems to quantify the degree ofprototypicality of target pair by measuring there lational similarity between it and pairs that are given as defining examples of particular relation.so far, most work in semantic relations has focused on differences between relation categories for classifying new relation instances.
</prevsent>
</prevsection>
<citsent citstr=" S10-1006 ">
past semeval tasks that use relations have focused largely on discrete classification (girju et al, 2007; hendrickx et al., 2010) <papid> S10-1006 </papid>and paraphrasing the relations connecting noun compounds with verb (butnariu et al, 2010), <papid> S10-1007 </papid>which is also form of discrete classification due to the lack of continuous degrees.</citsent>
<aftsection>
<nextsent>however, there issome loss of information in any discrete classification of semantic relations.
</nextsent>
<nextsent>furthermore, while some discrete classifiers provide degree of confidence or probability for relation classification, there is no priori reason that such values would correspond to human prototypicality judgments.
</nextsent>
<nextsent>our proposed task is distinct from these past tasks in that we focus on measuring the degree of relational similarity.1a graded measure of the degree of relational similarity would tell us that dog:bark is more similar tocat:meow than to floor:squeak.
</nextsent>
<nextsent>the discrete classification entity:sound drops this information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X983">
<title id=" S12-1047.xml">semeval2012 task 2 measuring degrees of relational similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, many would agree that dog:bark and cat:meow are better prototypes of the entity:sound relation than floor:squeak.
</prevsent>
<prevsent>our task requires automatic systems to quantify the degree ofprototypicality of target pair by measuring there lational similarity between it and pairs that are given as defining examples of particular relation.so far, most work in semantic relations has focused on differences between relation categories for classifying new relation instances.
</prevsent>
</prevsection>
<citsent citstr=" S10-1007 ">
past semeval tasks that use relations have focused largely on discrete classification (girju et al, 2007; hendrickx et al., 2010) <papid> S10-1006 </papid>and paraphrasing the relations connecting noun compounds with verb (butnariu et al, 2010), <papid> S10-1007 </papid>which is also form of discrete classification due to the lack of continuous degrees.</citsent>
<aftsection>
<nextsent>however, there issome loss of information in any discrete classification of semantic relations.
</nextsent>
<nextsent>furthermore, while some discrete classifiers provide degree of confidence or probability for relation classification, there is no priori reason that such values would correspond to human prototypicality judgments.
</nextsent>
<nextsent>our proposed task is distinct from these past tasks in that we focus on measuring the degree of relational similarity.1a graded measure of the degree of relational similarity would tell us that dog:bark is more similar tocat:meow than to floor:squeak.
</nextsent>
<nextsent>the discrete classification entity:sound drops this information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X984">
<title id=" S12-1047.xml">semeval2012 task 2 measuring degrees of relational similarity </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>researchers in psychology and linguistics have considered many different categorizations of semantic relations.
</prevsent>
<prevsent>the particular relation categorization isoften driven by both the type of data and the intended application.
</prevsent>
</prevsection>
<citsent citstr=" P08-1052 ">
nastase and szpakowicz (2003)propose two-level hierarchy for noun-modifier relations, which has been widely used (nakov and hearst, 2008; <papid> P08-1052 </papid>nastase et al, 2006; turney and littman, 2005; turney, 2005).</citsent>
<aftsection>
<nextsent>others have used classifications based on the requirements for specific task, such as information extraction (pantel and pennacchiotti, 2006) <papid> P06-1015 </papid>or biomedical applications (stephens et al, 2001).we adopt the relation classification scheme of bejar et al (1991), which includes ten high-level categories (e.g., cause-purpose and space-time).each category has between five and ten more refined subcategories (e.g., cause-purpose includes cause:effect and action:goal), for total of79 distinct subcategories.</nextsent>
<nextsent>although these categories do not reflect all possible semantic relations, they greatly expand the coverage of relation types from those used in past relation-based semeval tasks (girju et al, 2007; hendrickx etal., 2010), <papid> S10-1006 </papid>which used only seven and nine relation types, respectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X985">
<title id=" S12-1047.xml">semeval2012 task 2 measuring degrees of relational similarity </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>the particular relation categorization isoften driven by both the type of data and the intended application.
</prevsent>
<prevsent>nastase and szpakowicz (2003)propose two-level hierarchy for noun-modifier relations, which has been widely used (nakov and hearst, 2008; <papid> P08-1052 </papid>nastase et al, 2006; turney and littman, 2005; turney, 2005).</prevsent>
</prevsection>
<citsent citstr=" P06-1015 ">
others have used classifications based on the requirements for specific task, such as information extraction (pantel and pennacchiotti, 2006) <papid> P06-1015 </papid>or biomedical applications (stephens et al, 2001).we adopt the relation classification scheme of bejar et al (1991), which includes ten high-level categories (e.g., cause-purpose and space-time).each category has between five and ten more refined subcategories (e.g., cause-purpose includes cause:effect and action:goal), for total of79 distinct subcategories.</citsent>
<aftsection>
<nextsent>although these categories do not reflect all possible semantic relations, they greatly expand the coverage of relation types from those used in past relation-based semeval tasks (girju et al, 2007; hendrickx etal., 2010), <papid> S10-1006 </papid>which used only seven and nine relation types, respectively.</nextsent>
<nextsent>furthermore, the classification includes many of the fundamental relations, e.g., taxonomic and part:whole, while also including relations between variety of partsof speech and less common relations, such as reference (e.g., sign:significant) and nonat tribute (e.g., agent:atypical action).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X987">
<title id=" S12-1047.xml">semeval2012 task 2 measuring degrees of relational similarity </title>
<section> task data.  </section>
<citcontext>
<prevsection>
<prevsent>please do not use names of people, places, or things in your examples (e.g., europe?, kleenex?).
</prevsent>
<prevsent>(1) : (2) : (3) : (4) : figure 1: an example of the two questions for phase 1.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
annotations needed, we used amazon mechanical turk (mturk),2 which is popular choice in computational linguistics for gathering large numbers of human responses to linguistic questions (snow et al, 2008; <papid> D08-1027 </papid>mohammad and turney, 2010).<papid> W10-0204 </papid></citsent>
<aftsection>
<nextsent>we refer to the mturk workers as turkers.
</nextsent>
<nextsent>the dataset was built in two phases.
</nextsent>
<nextsent>in the first phase, turkers were given three paradigmatic examples of subcategory and asked to create new pairs that instantiate the same relation as the paradigms.in the second phase, people were asked to distinguish the new pairs from the first phase according to the degree to which they are good representatives of the given subcategory.
</nextsent>
<nextsent>phase 1 in the first phase, we built upon the paradigmatic examples of bejar et al (1991), who provided one to ten examples for each subcategory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X988">
<title id=" S12-1047.xml">semeval2012 task 2 measuring degrees of relational similarity </title>
<section> task data.  </section>
<citcontext>
<prevsection>
<prevsent>please do not use names of people, places, or things in your examples (e.g., europe?, kleenex?).
</prevsent>
<prevsent>(1) : (2) : (3) : (4) : figure 1: an example of the two questions for phase 1.
</prevsent>
</prevsection>
<citsent citstr=" W10-0204 ">
annotations needed, we used amazon mechanical turk (mturk),2 which is popular choice in computational linguistics for gathering large numbers of human responses to linguistic questions (snow et al, 2008; <papid> D08-1027 </papid>mohammad and turney, 2010).<papid> W10-0204 </papid></citsent>
<aftsection>
<nextsent>we refer to the mturk workers as turkers.
</nextsent>
<nextsent>the dataset was built in two phases.
</nextsent>
<nextsent>in the first phase, turkers were given three paradigmatic examples of subcategory and asked to create new pairs that instantiate the same relation as the paradigms.in the second phase, people were asked to distinguish the new pairs from the first phase according to the degree to which they are good representatives of the given subcategory.
</nextsent>
<nextsent>phase 1 in the first phase, we built upon the paradigmatic examples of bejar et al (1991), who provided one to ten examples for each subcategory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X989">
<title id=" S12-1047.xml">semeval2012 task 2 measuring degrees of relational similarity </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>table 2: descriptions of the participating teams and systems.
</prevsent>
<prevsent>as the most illustrative and the least associated asthe least illustrative.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
therefore, we propose second baseline where pairs are rated according to their pointwise mutual information (pmi) (church and hanks, 1990), <papid> J90-1003 </papid>which measures the statistical association between two words.</citsent>
<aftsection>
<nextsent>for this baseline, the prototypicality rating given to word pair is simply the pmi score for the pair.
</nextsent>
<nextsent>for two terms and y, pmi(x, y) is defined as log2 ( p(x,y) p(x)p(y) ) where p(?)
</nextsent>
<nextsent>denotes the probability of term or pair of terms.
</nextsent>
<nextsent>the pmi score was calculated using the method ofturney (2001) on corpus of approximately 50 billion tokens, indexed by the wumpus search engine.4 to calculate p(x, y), we recorded all co-occurrences of both terms within ten-word window.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X990">
<title id=" S12-1109.xml">sagan a machine translation approach for cross lingual textual entailment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in this manner, sts is filling the gap between te and paraphrase.
</prevsent>
<prevsent>2.3 cross-lingual textual entailment.
</prevsent>
</prevsection>
<citsent citstr=" N10-1045 ">
there are few previous works on clte, the first one was the definition of this new task (mehdad et al, 2010).<papid> N10-1045 </papid></citsent>
<aftsection>
<nextsent>afterwards, the creation of clte corpus by using mechanical turk is described on (negri et al, 2011) <papid> D11-1062 </papid>and corpus freely available for clte is published (castillo, 2011).</nextsent>
<nextsent>to our knowledge, two approach are proposed to address this new challenging task, one consist of using machine translation to move on towards monolingual textual entailment scenario and then apply classic techniques for rte (mehdad et al, 2010; <papid> N10-1045 </papid>castillo and cardenas, 2011), and the other is based on exploit databases of paraphrases (mehdad et al, 2011).<papid> P11-1134 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X991">
<title id=" S12-1109.xml">sagan a machine translation approach for cross lingual textual entailment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 cross-lingual textual entailment.
</prevsent>
<prevsent>there are few previous works on clte, the first one was the definition of this new task (mehdad et al, 2010).<papid> N10-1045 </papid></prevsent>
</prevsection>
<citsent citstr=" D11-1062 ">
afterwards, the creation of clte corpus by using mechanical turk is described on (negri et al, 2011) <papid> D11-1062 </papid>and corpus freely available for clte is published (castillo, 2011).</citsent>
<aftsection>
<nextsent>to our knowledge, two approach are proposed to address this new challenging task, one consist of using machine translation to move on towards monolingual textual entailment scenario and then apply classic techniques for rte (mehdad et al, 2010; <papid> N10-1045 </papid>castillo and cardenas, 2011), and the other is based on exploit databases of paraphrases (mehdad et al, 2011).<papid> P11-1134 </papid></nextsent>
<nextsent>both techniques obtained similar results and the accuracy achieved by them is not static ally significant difference.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X993">
<title id=" S12-1109.xml">sagan a machine translation approach for cross lingual textual entailment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there are few previous works on clte, the first one was the definition of this new task (mehdad et al, 2010).<papid> N10-1045 </papid></prevsent>
<prevsent>afterwards, the creation of clte corpus by using mechanical turk is described on (negri et al, 2011) <papid> D11-1062 </papid>and corpus freely available for clte is published (castillo, 2011).</prevsent>
</prevsection>
<citsent citstr=" P11-1134 ">
to our knowledge, two approach are proposed to address this new challenging task, one consist of using machine translation to move on towards monolingual textual entailment scenario and then apply classic techniques for rte (mehdad et al, 2010; <papid> N10-1045 </papid>castillo and cardenas, 2011), and the other is based on exploit databases of paraphrases (mehdad et al, 2011).<papid> P11-1134 </papid></citsent>
<aftsection>
<nextsent>both techniques obtained similar results and the accuracy achieved by them is not static ally significant difference.
</nextsent>
<nextsent>in previous work (castillo, 2010; castillo and cardenas, 2011) we addressed the clte focusing on english-spanish language pair and released bilingual textual entailment corpus.
</nextsent>
<nextsent>this paper is based on that work in order to tackling the problem across different language pairs spanish-english (spa-eng), italian-english (ita-eng), french english (fra-eng) and german-english (gereng) and we also used an approach based on machine translation.
</nextsent>
<nextsent>sagan is clte system (castillo and cardenas, 2010) which has taken part of several challenges, including the textual analysis conference 2009 and tac 2010, and the semantic textual simi lari 722 ty semeval 2012 (aguirre et al, 2012; castillo and estrella, 2012) <papid> W12-3103 </papid>and cross lingual textual entailment for content synchronization as part of the semeval 2012 (negri et al, 2012).<papid> S12-1053 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X994">
<title id=" S12-1109.xml">sagan a machine translation approach for cross lingual textual entailment </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>in previous work (castillo, 2010; castillo and cardenas, 2011) we addressed the clte focusing on english-spanish language pair and released bilingual textual entailment corpus.
</prevsent>
<prevsent>this paper is based on that work in order to tackling the problem across different language pairs spanish-english (spa-eng), italian-english (ita-eng), french english (fra-eng) and german-english (gereng) and we also used an approach based on machine translation.
</prevsent>
</prevsection>
<citsent citstr=" W12-3103 ">
sagan is clte system (castillo and cardenas, 2010) which has taken part of several challenges, including the textual analysis conference 2009 and tac 2010, and the semantic textual simi lari 722 ty semeval 2012 (aguirre et al, 2012; castillo and estrella, 2012) <papid> W12-3103 </papid>and cross lingual textual entailment for content synchronization as part of the semeval 2012 (negri et al, 2012).<papid> S12-1053 </papid></citsent>
<aftsection>
<nextsent>the system is based on machine learning approach and it utilizes eight wordnet-based (fellbaum, 1998) similarity measures with the purpose of obtaining the maximum similarity between two concepts.
</nextsent>
<nextsent>we used svm as classifier with polynomial kernel.
</nextsent>
<nextsent>the system determines the entailment based on the semantic similarity of two texts (t,h) viewed as function of the semantic similarity of the constituent words of both phrases.
</nextsent>
<nextsent>thereby, we expect that combining word to word similarity metrics to text level would be good indicator of text to text similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X995">
<title id=" S12-1109.xml">sagan a machine translation approach for cross lingual textual entailment </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>in previous work (castillo, 2010; castillo and cardenas, 2011) we addressed the clte focusing on english-spanish language pair and released bilingual textual entailment corpus.
</prevsent>
<prevsent>this paper is based on that work in order to tackling the problem across different language pairs spanish-english (spa-eng), italian-english (ita-eng), french english (fra-eng) and german-english (gereng) and we also used an approach based on machine translation.
</prevsent>
</prevsection>
<citsent citstr=" S12-1053 ">
sagan is clte system (castillo and cardenas, 2010) which has taken part of several challenges, including the textual analysis conference 2009 and tac 2010, and the semantic textual simi lari 722 ty semeval 2012 (aguirre et al, 2012; castillo and estrella, 2012) <papid> W12-3103 </papid>and cross lingual textual entailment for content synchronization as part of the semeval 2012 (negri et al, 2012).<papid> S12-1053 </papid></citsent>
<aftsection>
<nextsent>the system is based on machine learning approach and it utilizes eight wordnet-based (fellbaum, 1998) similarity measures with the purpose of obtaining the maximum similarity between two concepts.
</nextsent>
<nextsent>we used svm as classifier with polynomial kernel.
</nextsent>
<nextsent>the system determines the entailment based on the semantic similarity of two texts (t,h) viewed as function of the semantic similarity of the constituent words of both phrases.
</nextsent>
<nextsent>thereby, we expect that combining word to word similarity metrics to text level would be good indicator of text to text similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X996">
<title id=" S12-1109.xml">sagan a machine translation approach for cross lingual textual entailment </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>thereby, we expect that combining word to word similarity metrics to text level would be good indicator of text to text similarity.
</prevsent>
<prevsent>these text-to-text similarity measures are based on the following word-to-word similarity metrics: (resnik, 1995), (lin, 1997), (jiang and conrath, 1997), (pirr?
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
and seco, 2008), (wu and palmer, 1994), <papid> P94-1019 </papid>path metric, (leacock and chodorow, 1998), and semantic similarity to sentence level named semsim (castillo and cardenas, 2010).</citsent>
<aftsection>
<nextsent>additional information about how to produce feature vectors as well as each word- and sentence level metric can be found in (castillo, 2011).
</nextsent>
<nextsent>the architecture of the system is shown in figure 1.
</nextsent>
<nextsent>wordnet * clte_deu-eng, * clte_fra-eng, * clte_spa-eng, *clte_ita-eng, * clte_deu+fra+spa+ita eng, *clte_deu+fra+spa+ita eng+rte3-ts-cl * clte_deu-eng, * clte_fra-eng, * clte_spa-eng, *clte_ita-eng clte adaptation layer te engine entailment result bidirectional backward google tra slate forward knowledge resources web resources training sets test sets rte3-4c+rte4-4c rte3-4c training sets no entailment pre-processing fig.1.
</nextsent>
<nextsent>system architecture in the preprocessing module we performed string normalization across different languages by using lookup table for lexical entries, and then date and time normalization is carried out.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X999">
<title id=" P99-1029.xml">using mutual information to resolve query translation ambiguities and query term weighting </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper focuses on the last two problems: pruning translations and calculating the weights for translation alternatives.
</prevsent>
<prevsent>we first describe the overall query translation process and the extent to which the ambiguity problem arises in korean-english cross-language ir.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
we then propose relatively simple yet effective method for resolving translation disambiguation using mutual information (mi) (church and hanks, 1990) <papid> J90-1003 </papid>statistics obtained only from the target document collection.</citsent>
<aftsection>
<nextsent>in this method, mutual 223 information is used not only to select the best candidate but also to assign weight to query terms in the target language.
</nextsent>
<nextsent>1 overall query translation process.
</nextsent>
<nextsent>our korean-to-english query translation scheme works in four stages: keyword selection, dictionary-based query translation, bilingual word sense disambiguation, and query term weighting.
</nextsent>
<nextsent>although none of the common resources such as dictionaries, thesauri, and corpora alone is complete enough to produce high quality english queries, we decided to use bilingual dictionary at the second stage and target-language corpus for the third and the fourth stages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1000">
<title id=" P99-1029.xml">using mutual information to resolve query translation ambiguities and query term weighting </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.2 dictionary-based query translation.
</prevsent>
<prevsent>the second stage does the actual query translation based on dictionary look-up, by applying both word-by-word translation and phrase-level translation.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
for the correct identification of phrases in korean query, it would help to identify the lexical relations and produce statistical information on pairs of words in text corpus as in smadja (1993).<papid> J93-1007 </papid></citsent>
<aftsection>
<nextsent>since the bilingual dictionary lacks some words that are essential for correct interpretation of the korean query, it is important to identify unknown words such as foreign words and trans literate hem into english strings that need to be matched against an english dictionary (jeong et al , 1997).
</nextsent>
<nextsent>1.3 selection of the correct translations.
</nextsent>
<nextsent>at the word disambiguation stage, we filter out the extraneous words generated blindly from the dictionary lookup process.
</nextsent>
<nextsent>in addition to the pos tagger, we employed bilingual word disambiguation technique using the co-occurrence information extracted from the collection of target documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1002">
<title id=" S10-1053.xml">uvtwsd1 a cross lingual word sense disambiguation system </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the context in which these keywords are searched for is exactly one sentence, i.e. the sentence in which the target word occurs.
</prevsent>
<prevsent>this is due to the test data simply not supplying wider context.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
the method used to extract these keywords (k) is proposed by (ng and lee, 1996) <papid> P96-1006 </papid>and used also in the research of (hoste et al, 2002).</citsent>
<aftsection>
<nextsent>assume we have focus word , more precisely, lemma and part-of-speech tag pair of one of the target words.
</nextsent>
<nextsent>we also have one of its aligned translations/senses s, which in this implementation is also lemma.
</nextsent>
<nextsent>we can now estimate (s|k), the probability of sense s, given keyword k, by dividing s,k local .
</nextsent>
<nextsent>(the number of occurrences of possible local context word with particular focus word lemma pos combination and with particular sense s) by k local (the number of occurrences of possible local context keyword loc with particular focus word-pos combination regardless of its sense).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1003">
<title id=" S10-1053.xml">uvtwsd1 a cross lingual word sense disambiguation system </title>
<section> discussion and conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>43.94 40.62 44.84 40.68 &amp; 32.38 39.01 &amp; 38.78 table 2: uvt-wsd1 results in comparison to other participants in the word-sense disambiguation task in our system, we used the same configuration of feature extraction, or voter over set of configurations, for all word experts.
</prevsent>
<prevsent>the actual classifier parameters however, do differ per word expert, as they are the result of the automatic parameter optimisation algorithm.
</prevsent>
</prevsection>
<citsent citstr=" W04-0827 ">
selecting different feature extraction configurations per word expert would be logical next step to attempt to boost results even further, as been done in (decadt et al, 2004).<papid> W04-0827 </papid>keeping in mind the fact that different word experts may perform differently, some general conclusions can be drawn from the experiments onthe trial data.</citsent>
<aftsection>
<nextsent>it appears to be beneficial to include lemma features, rather than just word features.
</nextsent>
<nextsent>however, adding part-of-speech features tends to have negative impact.
</nextsent>
<nextsent>for these local context features, the optimum context size is often two features to the left and two features to the right of the focus word, cf.
</nextsent>
<nextsent>(hendrickx et al, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1004">
<title id=" P98-2247.xml">detecting verbal participation in dia thesis alternations </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>the window broke.
</prevsent>
<prevsent>levin (1993) investigation of alternations summarises the research done and demonstrates the utility of alternation forma-tion for classifying verbs.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
some studies have re-cently recognised the potential for using diathe- sis alternations within automatic lexical acquisi-tion (ribas, 1995; korhonen, 1997; briscoe and carroll, 1997).<papid> A97-1052 </papid></citsent>
<aftsection>
<nextsent>this paper shows how corpus data can be used to automatically detect which verbs un-dergo these alternations.
</nextsent>
<nextsent>automatic acquisi-tion avoids the costly overheads of manual approach and allows for the fact that pred-icate behaviour varies between sublanguages, domains and across time.
</nextsent>
<nextsent>subcategorization frames (scfs) are acquired for each verb and 1this work was partially funded by cec le1 project  sparkle .
</nextsent>
<nextsent>we also acknowledge support from uk epsrc project  pset: practical simplification of en-glish text .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1006">
<title id=" P99-1028.xml">resolving translation ambiguity and target polysemy in cross language information retrieval </title>
<section> translation ambiguity and polysemy.  </section>
<citcontext>
<prevsection>
<prevsent>this method considers the content around the translation equivalents to decide the best target word.
</prevsent>
<prevsent>the translation of query term can be disambiguated using the co-occurrence of the translation equivalents of this term and other terms.
</prevsent>
</prevsection>
<citsent citstr=" H89-2012 ">
we adopt mutual information (church, et al., 1989) <papid> H89-2012 </papid>to measure the strength.</citsent>
<aftsection>
<nextsent>this disambiguation method performs good translations even when the multi-term phrases are not found in the bilingual dictionary, or the phrases are not identified in the source language.
</nextsent>
<nextsent>before discussion, we take chinese-english information retrieval as an example to explain our methods.
</nextsent>
<nextsent>consider the chinese query  ,~i~ ~5-  (yin2hang2) to an english collection again.
</nextsent>
<nextsent>the ambiguity grows from none (source side) to 9 senses (target side) during query translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1007">
<title id=" S10-1071.xml">heideltime high quality rule based extraction and normalization of temporal expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper,we describe our system heideltime for the extraction and normalization of temporal expressions in english documents.
</prevsent>
<prevsent>it was the best-performingsystem in task for english of the tempe val 2 challenge 1 . the purpose of this challenge was.
</prevsent>
</prevsection>
<citsent citstr=" W09-2418 ">
to evaluate different systems for temporal tagging as well as event and temporal relation extraction since competitive evaluation helps to drive forward research, and temporal annotation is important for many nlp tasks (pustejovsky and verhagen, 2009).<papid> W09-2418 </papid></citsent>
<aftsection>
<nextsent>the annotation scheme for temporal expressions, events, and relations is based ontimeml, the iso standard for temporal annotation 2 .before using temporal information in other applications is possible, the first task to solve is to extract and normalize temporal expressions (task of the challenge, annotated as timex3).
</nextsent>
<nextsent>there 1 http://semeval2.fbk.eu/ 2 http://www.timeml.org/are two types of approaches to address this prob lem: rule-based and machine learning ones.
</nextsent>
<nextsent>we decided to develop rule-based system since normalization can then be supervised in much easier way.
</nextsent>
<nextsent>furthermore, respective systems allow for modular extensions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1008">
<title id=" S10-1071.xml">heideltime high quality rule based extraction and normalization of temporal expressions </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>algorithm 1 applyrules.
</prevsent>
<prevsent>foreach sentence in document adddatestocas(date rules, cas); addtimestocas(time rules, cas); adddurationstocas(dur rules, cas); addsetstocas(set rules, cas); end foreach foreach timex3 incas disambiguatevalues(cas); end foreach removeinvalidsfromcas(cas); 2.3 functionality of heideltime.
</prevsent>
</prevsection>
<citsent citstr=" W01-1309 ">
there are many ways to textually describe temporal expressions, either explicitly, implicitly or relatively (schilder and habel, 2001).<papid> W01-1309 </papid></citsent>
<aftsection>
<nextsent>the extraction for all temporal expressions works in the same way, but assigning the value attributes has to be done differently.
</nextsent>
<nextsent>explicit temporal expressions are fully specified, i.e., the value attribute can directly explicit temporal expressions date r1 = (remonth) g1 (reday) g2 , (refully ear) g3 norm r1(g1,g2,g3) = g3-normmonth(g1)-normday(g2) implicit temporal expressions date r2 = (reholiday) g1 (refully ear) g2 norm r2(g1,g2) = g2-normholiday(g1) table 2: extraction parts and normalization parts of two sample rules.
</nextsent>
<nextsent>be assigned using the corresponding normalization function of the rule.
</nextsent>
<nextsent>for example, the explicit expression march 11, 1982 can be extracted with the rule date r1 of table 2 containing the resourcesremonth, reday, and refully ear (regular expressions for possible month, day and year tokens of date phrase, respectively).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1009">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>through evaluations using several stateof-the-art semantic relatedness systems, applied on standard datasets, we show that multilingual approach is better suited for this task, and leads to improvements of up to 47% with respect to the monolingual baseline.
</prevsent>
<prevsent>semantic relatedness is the task of quantifying the strength of the semantic connection between textual units, be they words, sentences, or documents.for instance, one may want to determine howse mantically related are two words such as car and automobile, or two pieces of text such as love animals and own pet.
</prevsent>
</prevsection>
<citsent citstr=" W09-3009 ">
it is one of the main tasks explored in the field of natural language processing,as it lies at the core of large number of applications such as information retrieval (ponte and croft, 1998), query reformulation (metzler et al, 2007; yih and meek, 2007; sahami and heilman, 2006;broder et al, 2008), image retrieval (leong and mihalcea, 2009; <papid> W09-3009 </papid>goodrum, 2000), plagiarism detection(hoad and zobel, 2003; shivakumar and garcia molina, 1995; broder et al, 1997; heintze, 1996; brin et al, 1995; manber, 1994), information flow (metzler et al, 2005), sponsored search (broder etal., 2008), short answer grading (mohler and mihalcea, 2009<papid> E09-1065 </papid>a; pulman and sukkarieh, 2005; <papid> W05-0202 </papid>mitchell et al, 2002), and textual entailment (dagan et al, 2005).</citsent>
<aftsection>
<nextsent>the typical approach to semantic relatedness is to either measure the distance between the constituent words by using knowledge base such as wordnet or roget (e.g., (leacock and chodorow, 1998;lesk, 1986; jarmasz and szpakowicz, 2003; pedersen et al, 2004)), <papid> N04-3012 </papid>or to calculate the similarity between the word distributions in very large corpora (e.g., (landauer et al, 1991; lin, 1998; gabrilovich and markovitch, 2007)).</nextsent>
<nextsent>with almost no exception, these methods have been applied on one language ata time ? english, most of the time, although measures of relatedness have also been explored on languages such as german (zesch et al, 2007), <papid> N07-2052 </papid>chinese (li et al, 2005), japanese (kazama et al, 2010), <papid> P10-1026 </papid>and others.in this paper, we take step further and explore joint multilingual semantic relatedness metric, which aggregates semantic relatedness scores measured on several different languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1010">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>through evaluations using several stateof-the-art semantic relatedness systems, applied on standard datasets, we show that multilingual approach is better suited for this task, and leads to improvements of up to 47% with respect to the monolingual baseline.
</prevsent>
<prevsent>semantic relatedness is the task of quantifying the strength of the semantic connection between textual units, be they words, sentences, or documents.for instance, one may want to determine howse mantically related are two words such as car and automobile, or two pieces of text such as love animals and own pet.
</prevsent>
</prevsection>
<citsent citstr=" E09-1065 ">
it is one of the main tasks explored in the field of natural language processing,as it lies at the core of large number of applications such as information retrieval (ponte and croft, 1998), query reformulation (metzler et al, 2007; yih and meek, 2007; sahami and heilman, 2006;broder et al, 2008), image retrieval (leong and mihalcea, 2009; <papid> W09-3009 </papid>goodrum, 2000), plagiarism detection(hoad and zobel, 2003; shivakumar and garcia molina, 1995; broder et al, 1997; heintze, 1996; brin et al, 1995; manber, 1994), information flow (metzler et al, 2005), sponsored search (broder etal., 2008), short answer grading (mohler and mihalcea, 2009<papid> E09-1065 </papid>a; pulman and sukkarieh, 2005; <papid> W05-0202 </papid>mitchell et al, 2002), and textual entailment (dagan et al, 2005).</citsent>
<aftsection>
<nextsent>the typical approach to semantic relatedness is to either measure the distance between the constituent words by using knowledge base such as wordnet or roget (e.g., (leacock and chodorow, 1998;lesk, 1986; jarmasz and szpakowicz, 2003; pedersen et al, 2004)), <papid> N04-3012 </papid>or to calculate the similarity between the word distributions in very large corpora (e.g., (landauer et al, 1991; lin, 1998; gabrilovich and markovitch, 2007)).</nextsent>
<nextsent>with almost no exception, these methods have been applied on one language ata time ? english, most of the time, although measures of relatedness have also been explored on languages such as german (zesch et al, 2007), <papid> N07-2052 </papid>chinese (li et al, 2005), japanese (kazama et al, 2010), <papid> P10-1026 </papid>and others.in this paper, we take step further and explore joint multilingual semantic relatedness metric, which aggregates semantic relatedness scores measured on several different languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1011">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>through evaluations using several stateof-the-art semantic relatedness systems, applied on standard datasets, we show that multilingual approach is better suited for this task, and leads to improvements of up to 47% with respect to the monolingual baseline.
</prevsent>
<prevsent>semantic relatedness is the task of quantifying the strength of the semantic connection between textual units, be they words, sentences, or documents.for instance, one may want to determine howse mantically related are two words such as car and automobile, or two pieces of text such as love animals and own pet.
</prevsent>
</prevsection>
<citsent citstr=" W05-0202 ">
it is one of the main tasks explored in the field of natural language processing,as it lies at the core of large number of applications such as information retrieval (ponte and croft, 1998), query reformulation (metzler et al, 2007; yih and meek, 2007; sahami and heilman, 2006;broder et al, 2008), image retrieval (leong and mihalcea, 2009; <papid> W09-3009 </papid>goodrum, 2000), plagiarism detection(hoad and zobel, 2003; shivakumar and garcia molina, 1995; broder et al, 1997; heintze, 1996; brin et al, 1995; manber, 1994), information flow (metzler et al, 2005), sponsored search (broder etal., 2008), short answer grading (mohler and mihalcea, 2009<papid> E09-1065 </papid>a; pulman and sukkarieh, 2005; <papid> W05-0202 </papid>mitchell et al, 2002), and textual entailment (dagan et al, 2005).</citsent>
<aftsection>
<nextsent>the typical approach to semantic relatedness is to either measure the distance between the constituent words by using knowledge base such as wordnet or roget (e.g., (leacock and chodorow, 1998;lesk, 1986; jarmasz and szpakowicz, 2003; pedersen et al, 2004)), <papid> N04-3012 </papid>or to calculate the similarity between the word distributions in very large corpora (e.g., (landauer et al, 1991; lin, 1998; gabrilovich and markovitch, 2007)).</nextsent>
<nextsent>with almost no exception, these methods have been applied on one language ata time ? english, most of the time, although measures of relatedness have also been explored on languages such as german (zesch et al, 2007), <papid> N07-2052 </papid>chinese (li et al, 2005), japanese (kazama et al, 2010), <papid> P10-1026 </papid>and others.in this paper, we take step further and explore joint multilingual semantic relatedness metric, which aggregates semantic relatedness scores measured on several different languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1012">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic relatedness is the task of quantifying the strength of the semantic connection between textual units, be they words, sentences, or documents.for instance, one may want to determine howse mantically related are two words such as car and automobile, or two pieces of text such as love animals and own pet.
</prevsent>
<prevsent>it is one of the main tasks explored in the field of natural language processing,as it lies at the core of large number of applications such as information retrieval (ponte and croft, 1998), query reformulation (metzler et al, 2007; yih and meek, 2007; sahami and heilman, 2006;broder et al, 2008), image retrieval (leong and mihalcea, 2009; <papid> W09-3009 </papid>goodrum, 2000), plagiarism detection(hoad and zobel, 2003; shivakumar and garcia molina, 1995; broder et al, 1997; heintze, 1996; brin et al, 1995; manber, 1994), information flow (metzler et al, 2005), sponsored search (broder etal., 2008), short answer grading (mohler and mihalcea, 2009<papid> E09-1065 </papid>a; pulman and sukkarieh, 2005; <papid> W05-0202 </papid>mitchell et al, 2002), and textual entailment (dagan et al, 2005).</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
the typical approach to semantic relatedness is to either measure the distance between the constituent words by using knowledge base such as wordnet or roget (e.g., (leacock and chodorow, 1998;lesk, 1986; jarmasz and szpakowicz, 2003; pedersen et al, 2004)), <papid> N04-3012 </papid>or to calculate the similarity between the word distributions in very large corpora (e.g., (landauer et al, 1991; lin, 1998; gabrilovich and markovitch, 2007)).</citsent>
<aftsection>
<nextsent>with almost no exception, these methods have been applied on one language ata time ? english, most of the time, although measures of relatedness have also been explored on languages such as german (zesch et al, 2007), <papid> N07-2052 </papid>chinese (li et al, 2005), japanese (kazama et al, 2010), <papid> P10-1026 </papid>and others.in this paper, we take step further and explore joint multilingual semantic relatedness metric, which aggregates semantic relatedness scores measured on several different languages.</nextsent>
<nextsent>specifically, in our method, in order to measure there latedness of two textual units, we first determine their relatedness in multiple languages, and consequently infer final relatedness score by averaging the scores calculated in the individual languages.our hypothesis is that multilingual representation can enrich the relatedness space and address relevant issues such as polysemy (i.e., find that two occurrences of the same word in language l1 represent two different meanings because of different translations in language l2) and synonymy (i.e., find that two words in language l1 are related because they have the same translation in language l2).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1013">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is one of the main tasks explored in the field of natural language processing,as it lies at the core of large number of applications such as information retrieval (ponte and croft, 1998), query reformulation (metzler et al, 2007; yih and meek, 2007; sahami and heilman, 2006;broder et al, 2008), image retrieval (leong and mihalcea, 2009; <papid> W09-3009 </papid>goodrum, 2000), plagiarism detection(hoad and zobel, 2003; shivakumar and garcia molina, 1995; broder et al, 1997; heintze, 1996; brin et al, 1995; manber, 1994), information flow (metzler et al, 2005), sponsored search (broder etal., 2008), short answer grading (mohler and mihalcea, 2009<papid> E09-1065 </papid>a; pulman and sukkarieh, 2005; <papid> W05-0202 </papid>mitchell et al, 2002), and textual entailment (dagan et al, 2005).</prevsent>
<prevsent>the typical approach to semantic relatedness is to either measure the distance between the constituent words by using knowledge base such as wordnet or roget (e.g., (leacock and chodorow, 1998;lesk, 1986; jarmasz and szpakowicz, 2003; pedersen et al, 2004)), <papid> N04-3012 </papid>or to calculate the similarity between the word distributions in very large corpora (e.g., (landauer et al, 1991; lin, 1998; gabrilovich and markovitch, 2007)).</prevsent>
</prevsection>
<citsent citstr=" N07-2052 ">
with almost no exception, these methods have been applied on one language ata time ? english, most of the time, although measures of relatedness have also been explored on languages such as german (zesch et al, 2007), <papid> N07-2052 </papid>chinese (li et al, 2005), japanese (kazama et al, 2010), <papid> P10-1026 </papid>and others.in this paper, we take step further and explore joint multilingual semantic relatedness metric, which aggregates semantic relatedness scores measured on several different languages.</citsent>
<aftsection>
<nextsent>specifically, in our method, in order to measure there latedness of two textual units, we first determine their relatedness in multiple languages, and consequently infer final relatedness score by averaging the scores calculated in the individual languages.our hypothesis is that multilingual representation can enrich the relatedness space and address relevant issues such as polysemy (i.e., find that two occurrences of the same word in language l1 represent two different meanings because of different translations in language l2) and synonymy (i.e., find that two words in language l1 are related because they have the same translation in language l2).
</nextsent>
<nextsent>we show that by measuring relatedness in multilingual space, we are able to improve over traditional relatedness measure that relies exclusively on monolingual representation.through experiments using several state-of-theart measures of relatedness, applied on multilingual space including english, arabic, spanish, and romanian, we aim to answer the following research 20 questions: (1) does the task of semantic relatedness benefit from multilingual representation, ascom pared to monolingual one?
</nextsent>
<nextsent>(2) does the translation quality affect the results?
</nextsent>
<nextsent>and (3) do the findings hold for different relatedness datasets?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1014">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is one of the main tasks explored in the field of natural language processing,as it lies at the core of large number of applications such as information retrieval (ponte and croft, 1998), query reformulation (metzler et al, 2007; yih and meek, 2007; sahami and heilman, 2006;broder et al, 2008), image retrieval (leong and mihalcea, 2009; <papid> W09-3009 </papid>goodrum, 2000), plagiarism detection(hoad and zobel, 2003; shivakumar and garcia molina, 1995; broder et al, 1997; heintze, 1996; brin et al, 1995; manber, 1994), information flow (metzler et al, 2005), sponsored search (broder etal., 2008), short answer grading (mohler and mihalcea, 2009<papid> E09-1065 </papid>a; pulman and sukkarieh, 2005; <papid> W05-0202 </papid>mitchell et al, 2002), and textual entailment (dagan et al, 2005).</prevsent>
<prevsent>the typical approach to semantic relatedness is to either measure the distance between the constituent words by using knowledge base such as wordnet or roget (e.g., (leacock and chodorow, 1998;lesk, 1986; jarmasz and szpakowicz, 2003; pedersen et al, 2004)), <papid> N04-3012 </papid>or to calculate the similarity between the word distributions in very large corpora (e.g., (landauer et al, 1991; lin, 1998; gabrilovich and markovitch, 2007)).</prevsent>
</prevsection>
<citsent citstr=" P10-1026 ">
with almost no exception, these methods have been applied on one language ata time ? english, most of the time, although measures of relatedness have also been explored on languages such as german (zesch et al, 2007), <papid> N07-2052 </papid>chinese (li et al, 2005), japanese (kazama et al, 2010), <papid> P10-1026 </papid>and others.in this paper, we take step further and explore joint multilingual semantic relatedness metric, which aggregates semantic relatedness scores measured on several different languages.</citsent>
<aftsection>
<nextsent>specifically, in our method, in order to measure there latedness of two textual units, we first determine their relatedness in multiple languages, and consequently infer final relatedness score by averaging the scores calculated in the individual languages.our hypothesis is that multilingual representation can enrich the relatedness space and address relevant issues such as polysemy (i.e., find that two occurrences of the same word in language l1 represent two different meanings because of different translations in language l2) and synonymy (i.e., find that two words in language l1 are related because they have the same translation in language l2).
</nextsent>
<nextsent>we show that by measuring relatedness in multilingual space, we are able to improve over traditional relatedness measure that relies exclusively on monolingual representation.through experiments using several state-of-theart measures of relatedness, applied on multilingual space including english, arabic, spanish, and romanian, we aim to answer the following research 20 questions: (1) does the task of semantic relatedness benefit from multilingual representation, ascom pared to monolingual one?
</nextsent>
<nextsent>(2) does the translation quality affect the results?
</nextsent>
<nextsent>and (3) do the findings hold for different relatedness datasets?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1015">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>semantic relatedness.
</prevsent>
<prevsent>the approaches for semantic relatedness that have been considered to datecan be grouped into knowledge-based and corpus based.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
knowledge-based methods derive measure of relatedness by utilizing lexical resources and ontologies such as wordnet (miller, 1995) to measure definitional overlap (lesk, 1986), term distance within graphical taxonomy (leacock and chodorow, 1998), term depth in the taxonomy as measure of specificity (wu and palmer, 1994), <papid> P94-1019 </papid>andothers.</citsent>
<aftsection>
<nextsent>the application of such measures to language other than english requires the availability of the lexical resource in that language; furthermore, even though taxonomies such as wordnet (miller, 1995) are available in number of languages1, their coverage is still limited, and oftentimes they are not publicly available.
</nextsent>
<nextsent>for these reasons, in multilingual settings, these measures often become untractable.
</nextsent>
<nextsent>on the other side, corpus-based measures such as latent semantic analysis (lsa) (landauer et al, 1991), explicit semantic analysis (esa) (gabrilovich and markovitch, 2007),salient semantic analysis (ssa) (hassan and mihalcea, 2011), pointwise mutual information (pmi) (church and hanks, 1990), <papid> J90-1003 </papid>pmi-ir (turney, 2001),second order pmi (islam and inkpen, 2006), hy per space analogues to language (hal) (burgess et al, 1998) and distributional similarity (lin, 1998)employ probabilistic approaches to decode these mantics of words.</nextsent>
<nextsent>they consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words, and thus they can be easily transferred 1http://www.illc.uva.nl/eurowordnet/ to new language provided that large corpus in that language is available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1016">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the application of such measures to language other than english requires the availability of the lexical resource in that language; furthermore, even though taxonomies such as wordnet (miller, 1995) are available in number of languages1, their coverage is still limited, and oftentimes they are not publicly available.
</prevsent>
<prevsent>for these reasons, in multilingual settings, these measures often become untractable.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
on the other side, corpus-based measures such as latent semantic analysis (lsa) (landauer et al, 1991), explicit semantic analysis (esa) (gabrilovich and markovitch, 2007),salient semantic analysis (ssa) (hassan and mihalcea, 2011), pointwise mutual information (pmi) (church and hanks, 1990), <papid> J90-1003 </papid>pmi-ir (turney, 2001),second order pmi (islam and inkpen, 2006), hy per space analogues to language (hal) (burgess et al, 1998) and distributional similarity (lin, 1998)employ probabilistic approaches to decode these mantics of words.</citsent>
<aftsection>
<nextsent>they consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words, and thus they can be easily transferred 1http://www.illc.uva.nl/eurowordnet/ to new language provided that large corpus in that language is available.
</nextsent>
<nextsent>multilingual natural language processing.
</nextsent>
<nextsent>also relevant is the work done on multilingual text processing, which attempts to improve the performance of different natural language processing tasks by integrating information drawn from multiple languages.
</nextsent>
<nextsent>for instance, (cohn and lapata, 2007) <papid> P07-1092 </papid>explore the use of triangulation for machine translation, where multiple translation models are learned using multilingual parallel corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1017">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>multilingual natural language processing.
</prevsent>
<prevsent>also relevant is the work done on multilingual text processing, which attempts to improve the performance of different natural language processing tasks by integrating information drawn from multiple languages.
</prevsent>
</prevsection>
<citsent citstr=" P07-1092 ">
for instance, (cohn and lapata, 2007) <papid> P07-1092 </papid>explore the use of triangulation for machine translation, where multiple translation models are learned using multilingual parallel corpora.</citsent>
<aftsection>
<nextsent>the model was found especially beneficial for languages where the training dataset was small, thus suggesting that this method may be particularly useful for languages with scarce resources.
</nextsent>
<nextsent>(davidov and rappoport, 2009) <papid> D09-1089 </papid>experiment with the use of multiple languages to enhance an existing lexicon.</nextsent>
<nextsent>in their experiments, using three source languages and 45 intermediate languages, they find that the multilingual resource scan lead to significant improvements in concept ex pansion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1018">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, (cohn and lapata, 2007) <papid> P07-1092 </papid>explore the use of triangulation for machine translation, where multiple translation models are learned using multilingual parallel corpora.</prevsent>
<prevsent>the model was found especially beneficial for languages where the training dataset was small, thus suggesting that this method may be particularly useful for languages with scarce resources.</prevsent>
</prevsection>
<citsent citstr=" D09-1089 ">
(davidov and rappoport, 2009) <papid> D09-1089 </papid>experiment with the use of multiple languages to enhance an existing lexicon.</citsent>
<aftsection>
<nextsent>in their experiments, using three source languages and 45 intermediate languages, they find that the multilingual resource scan lead to significant improvements in concept expansion.
</nextsent>
<nextsent>(banea et al, 2010) <papid> C10-1004 </papid>explore the use of parallel multilingual corpora to improve subjectivity classification in target language, finding that the use of multilingual representations for subjectivity analysis improves over the monolingual classifiers.</nextsent>
<nextsent>similarly, (banea and mihalcea, 2011) <papid> W11-0104 </papid>investigate the use of multilingual contexts for word sense dis ambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1019">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(davidov and rappoport, 2009) <papid> D09-1089 </papid>experiment with the use of multiple languages to enhance an existing lexicon.</prevsent>
<prevsent>in their experiments, using three source languages and 45 intermediate languages, they find that the multilingual resource scan lead to significant improvements in concept ex pansion.</prevsent>
</prevsection>
<citsent citstr=" C10-1004 ">
(banea et al, 2010) <papid> C10-1004 </papid>explore the use of parallel multilingual corpora to improve subjectivity classification in target language, finding that the use of multilingual representations for subjectivity analysis improves over the monolingual classifiers.</citsent>
<aftsection>
<nextsent>similarly, (banea and mihalcea, 2011) <papid> W11-0104 </papid>investigate the use of multilingual contexts for word sense dis ambiguation.</nextsent>
<nextsent>by leveraging on the translations ofthe annotated contexts in multiple languages, multilingual thematic space emerges that better disam biguates target words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1020">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in their experiments, using three source languages and 45 intermediate languages, they find that the multilingual resource scan lead to significant improvements in concept expansion.
</prevsent>
<prevsent>(banea et al, 2010) <papid> C10-1004 </papid>explore the use of parallel multilingual corpora to improve subjectivity classification in target language, finding that the use of multilingual representations for subjectivity analysis improves over the monolingual classifiers.</prevsent>
</prevsection>
<citsent citstr=" W11-0104 ">
similarly, (banea and mihalcea, 2011) <papid> W11-0104 </papid>investigate the use of multilingual contexts for word sense dis ambiguation.</citsent>
<aftsection>
<nextsent>by leveraging on the translations ofthe annotated contexts in multiple languages, multilingual thematic space emerges that better disam biguates target words.
</nextsent>
<nextsent>finally, there are two lines of work that explore semantic distances in multilingual space.
</nextsent>
<nextsent>first, (besancon and rajman, 2002) examine the notion that the distances between document vectors within language correlate with the distances between their corresponding vectors in parallel corpus.
</nextsent>
<nextsent>these findings provide clues about the possibility of reliable semantic knowledge transfer across language boundaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1021">
<title id=" S12-1003.xml">measuring semantic relatedness using multilingual representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>first, (besancon and rajman, 2002) examine the notion that the distances between document vectors within language correlate with the distances between their corresponding vectors in parallel corpus.
</prevsent>
<prevsent>these findings provide clues about the possibility of reliable semantic knowledge transfer across language boundaries.
</prevsent>
</prevsection>
<citsent citstr=" D09-1124 ">
second, (hassan and mihalcea, 2009)<papid> D09-1124 </papid>propose framework to compute semantic relatedness between two words in different languages,by considering wikipedia articles in multiple lan guages.</citsent>
<aftsection>
<nextsent>the method differs from the one proposed here, as we aggregate relatedness over monolingual spaces rather than measuring cross-lingual relatedness, and we do not specifically use the inter-wiki links between wikipedia pages.
</nextsent>
<nextsent>21
</nextsent>
<nextsent>in this work, we focus on corpus-based metrics because of their unsupervised nature, their flexibility, scala bility, and portability to different languages.
</nextsent>
<nextsent>specifically, we utilize three popular models, lsa (landauer et al, 1991), esa (gabrilovichand markovitch, 2007), and ssa (hassan and mihalcea, 2011).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1024">
<title id=" S10-1023.xml">fcc modeling probabilities with giza for task 2 and 3 of semeval2 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is claimed that wsd is essential for those applications that require of language comprehension modules such as search engines, machine translation systems, automatic answer machines, second life agents, etc. moreover, with the huge amounts of information in internet and the fact that this information is continuo sly growing in different languages, we are encourage to deal with cross lingual scenarios where wsd systems are also needed.
</prevsent>
<prevsent>despite the wsd task has been studied for long time, the expected feeling is that wsd should be integrated into real applications such as mono and multi-lingual search engines, machine translation systems, automatic answer machines,etc (agirre and edmonds, 2006).
</prevsent>
</prevsection>
<citsent citstr=" P07-1005 ">
different studies on this issue have demonstrated that those applications benefit from wsd, such as in the case of machine translation (chan et al, 2007; <papid> P07-1005 </papid>carpuat and wu., 2007).</citsent>
<aftsection>
<nextsent>on the other hand, lexical substitution (ls) refers to the process of finding substitute word for source word in given sentence.
</nextsent>
<nextsent>the ls task needs to be approached by firstly disambiguating the source word, therefore, these two tasks (wsd and ls) are somehow related.
</nextsent>
<nextsent>since we are describing the modules of our system, we did not provide information of the datasets used.
</nextsent>
<nextsent>for details about the corpora, see the task description paper for both tasks (#2 and #3) in this volume (mihalcea et al, 2010; lefever and hoste, 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1025">
<title id=" P98-2248.xml">target word selection as proximity in semantic space </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main purpose of the present investigation was to determine the extent hat this hypothesis was supported.
</prevsent>
<prevsent>1.2 related work.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
dagan and itai (1994) <papid> J94-4003 </papid>have also addressed the lexical selection problem from the tl point of view.</citsent>
<aftsection>
<nextsent>their algorithm uses information about local co-occurrence probabilities for all possible tl pairs of words that can result from translating each pair of words (verb/noun plus argument/modifier) in the sl sentence, and only 1496 makes decision if the preference is statistically significant.
</nextsent>
<nextsent>in work aimed at lexical choice in generation, edmonds (1997) <papid> P97-1067 </papid>uses information about significant local co-occurrences to choose which of set of synonyms most typical in given context.</nextsent>
<nextsent>the present paper differs from these approaches in that local co-occurrence behaviour is not considered relevant, but rather an estimate of semantic relatedness between the tl context and each candidate translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1026">
<title id=" P98-2248.xml">target word selection as proximity in semantic space </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dagan and itai (1994) <papid> J94-4003 </papid>have also addressed the lexical selection problem from the tl point of view.</prevsent>
<prevsent>their algorithm uses information about local co-occurrence probabilities for all possible tl pairs of words that can result from translating each pair of words (verb/noun plus argument/modifier) in the sl sentence, and only 1496 makes decision if the preference is statistically significant.</prevsent>
</prevsection>
<citsent citstr=" P97-1067 ">
in work aimed at lexical choice in generation, edmonds (1997) <papid> P97-1067 </papid>uses information about significant local co-occurrences to choose which of set of synonyms most typical in given context.</citsent>
<aftsection>
<nextsent>the present paper differs from these approaches in that local co-occurrence behaviour is not considered relevant, but rather an estimate of semantic relatedness between the tl context and each candidate translation.
</nextsent>
<nextsent>to assess the proposed semantic distance (sd) method for target word selection, used an english-spanish parallel corpus for testing and evaluation.
</nextsent>
<nextsent>several features of real mt system were incorporated in order that the experiment mimic the type of information available to the lexical selection component.
</nextsent>
<nextsent>investigation was restricted to the translation of content words: common ouns, verbs, adjectives and adverbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1027">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an overview of participating systems is provided and their results are summarized.
</prevsent>
<prevsent>semantic representation of text has received considerable attention these past years.
</prevsent>
</prevsection>
<citsent citstr=" N09-2004 ">
while early shallow approaches have been proven useful for several natural language processing applications (wu andfung, 2009; <papid> N09-2004 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>shen and la pata, 2007), <papid> D07-1002 </papid>the field is moving towards analyzing and processing complex linguistic phenomena, suchas metaphor (shutova, 2010) <papid> P10-1071 </papid>or modality and negation (morante and sporleder, 2012).the *sem 2012 shared task is devoted to negation, specifically, to resolving its scope and focus.</citsent>
<aftsection>
<nextsent>negation is grammatical category that comprises devices used to reverse the truth value of propositions.
</nextsent>
<nextsent>broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (huddleston and pullum, 2002).
</nextsent>
<nextsent>although negation is very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in partial man ner.the interest in automatically processing negation originated in the medical domain (chapman et al, 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed.
</nextsent>
<nextsent>the annotation of negation and hedge cues and their scope in the bio scope corpus (vincze et al, 2008) represented pioneering effort.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1028">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an overview of participating systems is provided and their results are summarized.
</prevsent>
<prevsent>semantic representation of text has received considerable attention these past years.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
while early shallow approaches have been proven useful for several natural language processing applications (wu andfung, 2009; <papid> N09-2004 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>shen and la pata, 2007), <papid> D07-1002 </papid>the field is moving towards analyzing and processing complex linguistic phenomena, suchas metaphor (shutova, 2010) <papid> P10-1071 </papid>or modality and negation (morante and sporleder, 2012).the *sem 2012 shared task is devoted to negation, specifically, to resolving its scope and focus.</citsent>
<aftsection>
<nextsent>negation is grammatical category that comprises devices used to reverse the truth value of propositions.
</nextsent>
<nextsent>broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (huddleston and pullum, 2002).
</nextsent>
<nextsent>although negation is very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in partial man ner.the interest in automatically processing negation originated in the medical domain (chapman et al, 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed.
</nextsent>
<nextsent>the annotation of negation and hedge cues and their scope in the bio scope corpus (vincze et al, 2008) represented pioneering effort.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1029">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an overview of participating systems is provided and their results are summarized.
</prevsent>
<prevsent>semantic representation of text has received considerable attention these past years.
</prevsent>
</prevsection>
<citsent citstr=" D07-1002 ">
while early shallow approaches have been proven useful for several natural language processing applications (wu andfung, 2009; <papid> N09-2004 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>shen and la pata, 2007), <papid> D07-1002 </papid>the field is moving towards analyzing and processing complex linguistic phenomena, suchas metaphor (shutova, 2010) <papid> P10-1071 </papid>or modality and negation (morante and sporleder, 2012).the *sem 2012 shared task is devoted to negation, specifically, to resolving its scope and focus.</citsent>
<aftsection>
<nextsent>negation is grammatical category that comprises devices used to reverse the truth value of propositions.
</nextsent>
<nextsent>broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (huddleston and pullum, 2002).
</nextsent>
<nextsent>although negation is very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in partial man ner.the interest in automatically processing negation originated in the medical domain (chapman et al, 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed.
</nextsent>
<nextsent>the annotation of negation and hedge cues and their scope in the bio scope corpus (vincze et al, 2008) represented pioneering effort.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1030">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an overview of participating systems is provided and their results are summarized.
</prevsent>
<prevsent>semantic representation of text has received considerable attention these past years.
</prevsent>
</prevsection>
<citsent citstr=" P10-1071 ">
while early shallow approaches have been proven useful for several natural language processing applications (wu andfung, 2009; <papid> N09-2004 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>shen and la pata, 2007), <papid> D07-1002 </papid>the field is moving towards analyzing and processing complex linguistic phenomena, suchas metaphor (shutova, 2010) <papid> P10-1071 </papid>or modality and negation (morante and sporleder, 2012).the *sem 2012 shared task is devoted to negation, specifically, to resolving its scope and focus.</citsent>
<aftsection>
<nextsent>negation is grammatical category that comprises devices used to reverse the truth value of propositions.
</nextsent>
<nextsent>broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (huddleston and pullum, 2002).
</nextsent>
<nextsent>although negation is very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in partial man ner.the interest in automatically processing negation originated in the medical domain (chapman et al, 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed.
</nextsent>
<nextsent>the annotation of negation and hedge cues and their scope in the bio scope corpus (vincze et al, 2008) represented pioneering effort.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1031">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although negation is very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in partial man ner.the interest in automatically processing negation originated in the medical domain (chapman et al, 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed.
</prevsent>
<prevsent>the annotation of negation and hedge cues and their scope in the bio scope corpus (vincze et al, 2008) represented pioneering effort.
</prevsent>
</prevsection>
<citsent citstr=" W10-3001 ">
this corpus boosted research on scope resolution, especially since it was used in the conll 2010 shared task (conll st 2010) on hedge detection (farkas et al, 2010).<papid> W10-3001 </papid>negation has also been studied in sentiment analysis (wiegand et al, 2010) <papid> W10-3111 </papid>as means to determine the polarity of sentiments and opinions.whereas several scope detectors have been developed using bio scope (morante and daelemans,2009; <papid> W09-1105 </papid>velldal et al, 2012),<papid> J12-2005 </papid>there is lack of corpora and tools to process negation in general domain texts.</citsent>
<aftsection>
<nextsent>this is why we have prepared new corpora for scope and focus detection.
</nextsent>
<nextsent>scope is annotated in conan doyle stories (cd-sco corpus).
</nextsent>
<nextsent>for each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a).
</nextsent>
<nextsent>focus is annotated on top of propbank, which uses the wsj section of the penn treebank (pb-foc corpus).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1033">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although negation is very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in partial man ner.the interest in automatically processing negation originated in the medical domain (chapman et al, 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed.
</prevsent>
<prevsent>the annotation of negation and hedge cues and their scope in the bio scope corpus (vincze et al, 2008) represented pioneering effort.
</prevsent>
</prevsection>
<citsent citstr=" W10-3111 ">
this corpus boosted research on scope resolution, especially since it was used in the conll 2010 shared task (conll st 2010) on hedge detection (farkas et al, 2010).<papid> W10-3001 </papid>negation has also been studied in sentiment analysis (wiegand et al, 2010) <papid> W10-3111 </papid>as means to determine the polarity of sentiments and opinions.whereas several scope detectors have been developed using bio scope (morante and daelemans,2009; <papid> W09-1105 </papid>velldal et al, 2012),<papid> J12-2005 </papid>there is lack of corpora and tools to process negation in general domain texts.</citsent>
<aftsection>
<nextsent>this is why we have prepared new corpora for scope and focus detection.
</nextsent>
<nextsent>scope is annotated in conan doyle stories (cd-sco corpus).
</nextsent>
<nextsent>for each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a).
</nextsent>
<nextsent>focus is annotated on top of propbank, which uses the wsj section of the penn treebank (pb-foc corpus).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1034">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although negation is very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in partial man ner.the interest in automatically processing negation originated in the medical domain (chapman et al, 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed.
</prevsent>
<prevsent>the annotation of negation and hedge cues and their scope in the bio scope corpus (vincze et al, 2008) represented pioneering effort.
</prevsent>
</prevsection>
<citsent citstr=" W09-1105 ">
this corpus boosted research on scope resolution, especially since it was used in the conll 2010 shared task (conll st 2010) on hedge detection (farkas et al, 2010).<papid> W10-3001 </papid>negation has also been studied in sentiment analysis (wiegand et al, 2010) <papid> W10-3111 </papid>as means to determine the polarity of sentiments and opinions.whereas several scope detectors have been developed using bio scope (morante and daelemans,2009; <papid> W09-1105 </papid>velldal et al, 2012),<papid> J12-2005 </papid>there is lack of corpora and tools to process negation in general domain texts.</citsent>
<aftsection>
<nextsent>this is why we have prepared new corpora for scope and focus detection.
</nextsent>
<nextsent>scope is annotated in conan doyle stories (cd-sco corpus).
</nextsent>
<nextsent>for each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a).
</nextsent>
<nextsent>focus is annotated on top of propbank, which uses the wsj section of the penn treebank (pb-foc corpus).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1035">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although negation is very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in partial man ner.the interest in automatically processing negation originated in the medical domain (chapman et al, 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed.
</prevsent>
<prevsent>the annotation of negation and hedge cues and their scope in the bio scope corpus (vincze et al, 2008) represented pioneering effort.
</prevsent>
</prevsection>
<citsent citstr=" J12-2005 ">
this corpus boosted research on scope resolution, especially since it was used in the conll 2010 shared task (conll st 2010) on hedge detection (farkas et al, 2010).<papid> W10-3001 </papid>negation has also been studied in sentiment analysis (wiegand et al, 2010) <papid> W10-3111 </papid>as means to determine the polarity of sentiments and opinions.whereas several scope detectors have been developed using bio scope (morante and daelemans,2009; <papid> W09-1105 </papid>velldal et al, 2012),<papid> J12-2005 </papid>there is lack of corpora and tools to process negation in general domain texts.</citsent>
<aftsection>
<nextsent>this is why we have prepared new corpora for scope and focus detection.
</nextsent>
<nextsent>scope is annotated in conan doyle stories (cd-sco corpus).
</nextsent>
<nextsent>for each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a).
</nextsent>
<nextsent>focus is annotated on top of propbank, which uses the wsj section of the penn treebank (pb-foc corpus).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1037">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>the tools used cannot have been developed or tuned using the annotations of the test set.
</prevsent>
<prevsent>regardless of the track, teams were allowed to submit their final results on the test set using system trained on both the training and developmentsets.
</prevsent>
</prevsection>
<citsent citstr=" W08-2121 ">
the data format is the same as in several previous conll shared tasks (surdeanu et al, 2008).<papid> W08-2121 </papid>sentences are separated by blank line.</citsent>
<aftsection>
<nextsent>each sentence consists of sequence of tokens, and new line is used for each token.
</nextsent>
<nextsent>2.1 task 1: scope resolution.
</nextsent>
<nextsent>task 1 aimed at resolving the scope of negation cues and detecting negated events.
</nextsent>
<nextsent>the task is divided into 3 subtasks: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1040">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>the corpus was preprocessed at the university of oslo.
</prevsent>
<prevsent>tokenization was obtained by the ptb compliant tokenizer that is part of the lingo english resource grammar.
</prevsent>
</prevsection>
<citsent citstr=" H05-1059 ">
4 4http://moin.delph-in.net/ 268 apart from the gold annotations, the corpus was provided to participants with additional annotations: ? lemmatization using the genia tagger (tsuruokaand tsujii, 2005), <papid> H05-1059 </papid>version 3.0.1, with the ?-nt?</citsent>
<aftsection>
<nextsent>command line option.
</nextsent>
<nextsent>genia pos tags are complemented with tnt pos tags for increased compatibility with the original ptb.?
</nextsent>
<nextsent>parsing with the charniak and johnson (2005) <papid> P05-1022 </papid>reranking parser.5 for compatibility with ptb conventions, the top-level nodes in parse trees (s1?), were removed.</nextsent>
<nextsent>the conversion of ptb-style syntax trees into conll-style format was performed using the conll 2005 shared task software.6 3.2 pb-foc: focus annotation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1041">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>command line option.
</prevsent>
<prevsent>genia pos tags are complemented with tnt pos tags for increased compatibility with the original ptb.?
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
parsing with the charniak and johnson (2005) <papid> P05-1022 </papid>reranking parser.5 for compatibility with ptb conventions, the top-level nodes in parse trees (s1?), were removed.</citsent>
<aftsection>
<nextsent>the conversion of ptb-style syntax trees into conll-style format was performed using the conll 2005 shared task software.6 3.2 pb-foc: focus annotation.
</nextsent>
<nextsent>we have adapted the only previous annotation effort targeting focus of negation for pb-foc (blanco and moldovan, 2011).<papid> P11-1059 </papid></nextsent>
<nextsent>this corpus provides focus annotation on top of propbank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1042">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>parsing with the charniak and johnson (2005) <papid> P05-1022 </papid>reranking parser.5 for compatibility with ptb conventions, the top-level nodes in parse trees (s1?), were removed.</prevsent>
<prevsent>the conversion of ptb-style syntax trees into conll-style format was performed using the conll 2005 shared task software.6 3.2 pb-foc: focus annotation.</prevsent>
</prevsection>
<citsent citstr=" P11-1059 ">
we have adapted the only previous annotation effort targeting focus of negation for pb-foc (blanco and moldovan, 2011).<papid> P11-1059 </papid></citsent>
<aftsection>
<nextsent>this corpus provides focus annotation on top of propbank.
</nextsent>
<nextsent>it targets exclusively verbal neg ations marked with mneg in propbank and selects as focus the semantic role containing themost likely focus.
</nextsent>
<nextsent>the motivation behind their approach, annotation guidelines and examples can be found in the aforementioned paper.
</nextsent>
<nextsent>we gathered all neg ations from sections 0221,23 and 24 and discarded neg ations for which the focus or propbank annotations were not sound, leaving 3,544 instances.7 for each verbal negation, pb foc provides the current sentence, and the previous and next sentences as context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1043">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>the motivation behind their approach, annotation guidelines and examples can be found in the aforementioned paper.
</prevsent>
<prevsent>we gathered all neg ations from sections 0221,23 and 24 and discarded neg ations for which the focus or propbank annotations were not sound, leaving 3,544 instances.7 for each verbal negation, pb foc provides the current sentence, and the previous and next sentences as context.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
for each sentence,along with the gold focus annotations, pb-foc contains the following additional annotations: ? token number; ? pos tags using the brill tagger (brill, 1992);?<papid> A92-1021 </papid></citsent>
<aftsection>
<nextsent>named entities using the stanford named entity recognizer recognizer (finkel et al, 2005); ? <papid> P05-1045 </papid>chunks using the chunker by phan (2006);?</nextsent>
<nextsent>syntactic tree using the charniak parser (char niak, 2000); ? <papid> A00-2018 </papid>dependency tree derived from the syntactic tree (de marneffe et al, 2006); erg tokenization, http://moin.delph-in.net/ repptop 5november 2009 release available from brown university.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1044">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>we gathered all neg ations from sections 0221,23 and 24 and discarded neg ations for which the focus or propbank annotations were not sound, leaving 3,544 instances.7 for each verbal negation, pb foc provides the current sentence, and the previous and next sentences as context.
</prevsent>
<prevsent>for each sentence,along with the gold focus annotations, pb-foc contains the following additional annotations: ? token number; ? pos tags using the brill tagger (brill, 1992);?<papid> A92-1021 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
named entities using the stanford named entity recognizer recognizer (finkel et al, 2005); ? <papid> P05-1045 </papid>chunks using the chunker by phan (2006);?</citsent>
<aftsection>
<nextsent>syntactic tree using the charniak parser (char niak, 2000); ? <papid> A00-2018 </papid>dependency tree derived from the syntactic tree (de marneffe et al, 2006); erg tokenization, http://moin.delph-in.net/ repptop 5november 2009 release available from brown university.</nextsent>
<nextsent>6http://www.lsi.upc.edu/srlconll/ srlconll-1.1.tgz 7the original focus annotation targeted the 3,993 neg ations marked with mneg in the whole propbank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1045">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>for each sentence,along with the gold focus annotations, pb-foc contains the following additional annotations: ? token number; ? pos tags using the brill tagger (brill, 1992);?<papid> A92-1021 </papid></prevsent>
<prevsent>named entities using the stanford named entity recognizer recognizer (finkel et al, 2005); ? <papid> P05-1045 </papid>chunks using the chunker by phan (2006);?</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
syntactic tree using the charniak parser (char niak, 2000); ? <papid> A00-2018 </papid>dependency tree derived from the syntactic tree (de marneffe et al, 2006); erg tokenization, http://moin.delph-in.net/ repptop 5november 2009 release available from brown university.</citsent>
<aftsection>
<nextsent>6http://www.lsi.upc.edu/srlconll/ srlconll-1.1.tgz 7the original focus annotation targeted the 3,993 neg ations marked with mneg in the whole propbank.
</nextsent>
<nextsent>train devel test 1 role 2,210 515 672 2 roles 89 15 38 3 roles 3 0 2 all 2,302 530 712 em an ti ro le fo cu be lo ng to a1 980 222 309 am-neg 592 138 172 am-tmp 161 35 46 am-mnr 127 27 38 a2 112 28 36 a0 94 23 31 none 88 19 35 am-adv 78 23 26 c-a1 46 6 16 am-pnc 33 8 12 am-loc 25 4 10 a4 11 2 5 r-a1 10 2 2 other 40 8 16 table 2: basic numeric analysis for pb-foc.
</nextsent>
<nextsent>the first 4 rows indicate the number of unique roles each negation belongs to, the rest indicate the counts for each role.
</nextsent>
<nextsent>semantic roles using the labeler described by (punyakanok et al, 2008); <papid> J08-2005 </papid>and ? verbal negation, indicates with n? if that token correspond to verbal negation for which focus must be predicted.figure 2 provides sample of pb-foc.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1046">
<title id=" S12-1035.xml">sem 2012 shared task resolving the scope and focus of negation </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>train devel test 1 role 2,210 515 672 2 roles 89 15 38 3 roles 3 0 2 all 2,302 530 712 em an ti ro le fo cu be lo ng to a1 980 222 309 am-neg 592 138 172 am-tmp 161 35 46 am-mnr 127 27 38 a2 112 28 36 a0 94 23 31 none 88 19 35 am-adv 78 23 26 c-a1 46 6 16 am-pnc 33 8 12 am-loc 25 4 10 a4 11 2 5 r-a1 10 2 2 other 40 8 16 table 2: basic numeric analysis for pb-foc.
</prevsent>
<prevsent>the first 4 rows indicate the number of unique roles each negation belongs to, the rest indicate the counts for each role.
</prevsent>
</prevsection>
<citsent citstr=" J08-2005 ">
semantic roles using the labeler described by (punyakanok et al, 2008); <papid> J08-2005 </papid>and ? verbal negation, indicates with n? if that token correspond to verbal negation for which focus must be predicted.figure 2 provides sample of pb-foc.</citsent>
<aftsection>
<nextsent>knowing that the original focus annotations were done ontop of propbank and that focus corresponds to single role, semantic role information is key to predict the focus.
</nextsent>
<nextsent>in table 2, we show some basic numeric analysis regarding focus annotation and the automatically obtained semantic role labels.
</nextsent>
<nextsent>most instances of focus belong to single role in the three splits and the most common role focus belongs to is a1, followed by am-neg, m-tmp and m-mnr.
</nextsent>
<nextsent>note that some instances have at least one word that doesnot belong to any role (88 in training, 19 in development and 35 in test).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1048">
<title id=" P98-2186.xml">part of speech tagging using a network of linear separators </title>
<section> the  pos problem.  </section>
<citcontext>
<prevsection>
<prevsent>the problem has numerous application in information retrieval, machine translation, speech recognition, and appears to be an im-portant intermediate stage in many natural lan-guage understanding related inferences.
</prevsent>
<prevsent>in recent years, number of approaches have been tried for solving the problem.
</prevsent>
</prevsection>
<citsent citstr=" W95-0101 ">
the most notable methods are based on hidden markov models(hmm)(kupiec, 1992; schiitze, 1995), transformation rules(brill, 1995; <papid> W95-0101 </papid>brill, 1997), and multi-layer neural networks(schmid, 1994).<papid> C94-1027 </papid></citsent>
<aftsection>
<nextsent>hmm taggers use manually tagged training data to compute statistics on features.
</nextsent>
<nextsent>for example, they can estimate lexical probabili-ties prob(wordlta9) and contextual probabili-ties prob( tag lprev ious tags).
</nextsent>
<nextsent>on the testing stage, the taggers conduct search in the space of pos tags to arrive at the most probable pos labeling with respect the computed statistics.
</nextsent>
<nextsent>that is, given sentence, the taggers assign in the sentence sequence of tags that maximize the product of lexical and contextual probabil-ities over all words in the sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1050">
<title id=" P98-2186.xml">part of speech tagging using a network of linear separators </title>
<section> the  pos problem.  </section>
<citcontext>
<prevsection>
<prevsent>the problem has numerous application in information retrieval, machine translation, speech recognition, and appears to be an im-portant intermediate stage in many natural lan-guage understanding related inferences.
</prevsent>
<prevsent>in recent years, number of approaches have been tried for solving the problem.
</prevsent>
</prevsection>
<citsent citstr=" C94-1027 ">
the most notable methods are based on hidden markov models(hmm)(kupiec, 1992; schiitze, 1995), transformation rules(brill, 1995; <papid> W95-0101 </papid>brill, 1997), and multi-layer neural networks(schmid, 1994).<papid> C94-1027 </papid></citsent>
<aftsection>
<nextsent>hmm taggers use manually tagged training data to compute statistics on features.
</nextsent>
<nextsent>for example, they can estimate lexical probabili-ties prob(wordlta9) and contextual probabili-ties prob( tag lprev ious tags).
</nextsent>
<nextsent>on the testing stage, the taggers conduct search in the space of pos tags to arrive at the most probable pos labeling with respect the computed statistics.
</nextsent>
<nextsent>that is, given sentence, the taggers assign in the sentence sequence of tags that maximize the product of lexical and contextual probabil-ities over all words in the sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1062">
<title id=" P98-2186.xml">part of speech tagging using a network of linear separators </title>
<section> the  pos problem.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we address the pos problem with no unknown words (the closed world as- sumption) from the standpoint of snow.
</prevsent>
<prevsent>that is, we represent pos tagger as network of linear separators and use winnow for learning weights of the network.
</prevsent>
</prevsection>
<citsent citstr=" W98-0717 ">
the snow approach has been successfully applied to other prob-lems of natural language processing(golding and roth, 1998; krymolowski and roth, 1998; <papid> W98-0717 </papid>roth, 1998).</citsent>
<aftsection>
<nextsent>however, this problem offers ad-ditional challenges to the snow architecture and algorithms.
</nextsent>
<nextsent>first, we are trying to learn multi-class predictor, where the number of classes is unusually large(about 50) for such learning problems.
</nextsent>
<nextsent>second, evaluating hypoth-esis in testing is done in presence of attribute noise.
</nextsent>
<nextsent>the reason is that input features of the network are computed with respect parts of speech of words, which are initially assigned from lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1063">
<title id=" S10-1049.xml">isi automatic classification of relations between nominals using a maximum entropy classifier </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our system, based upon maximum entropy classifier trained using large number of boolean features, received the third highest score.
</prevsent>
<prevsent>semantic interpretation of the relations between nominals in text is an area of growing interest within natural language processing (nlp).
</prevsent>
</prevsection>
<citsent citstr=" W04-0404 ">
it has potential uses for variety of tasks including machine translation (baldwin and tanaka, 2004) <papid> W04-0404 </papid>and question answering (ahn et al, 2005).</citsent>
<aftsection>
<nextsent>the related and more narrowly-focused problem of automatic interpretation of noun compounds is the focus of another semeval task (butnariu et al, 2009).<papid> W09-2416 </papid></nextsent>
<nextsent>in this paper, we discuss the overall setup of semeval 2010 task 8 (hendrickx et al, 2010), present the system we used to participate, and discuss our systems performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1064">
<title id=" S10-1049.xml">isi automatic classification of relations between nominals using a maximum entropy classifier </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic interpretation of the relations between nominals in text is an area of growing interest within natural language processing (nlp).
</prevsent>
<prevsent>it has potential uses for variety of tasks including machine translation (baldwin and tanaka, 2004) <papid> W04-0404 </papid>and question answering (ahn et al, 2005).</prevsent>
</prevsection>
<citsent citstr=" W09-2416 ">
the related and more narrowly-focused problem of automatic interpretation of noun compounds is the focus of another semeval task (butnariu et al, 2009).<papid> W09-2416 </papid></citsent>
<aftsection>
<nextsent>in this paper, we discuss the overall setup of semeval 2010 task 8 (hendrickx et al, 2010), present the system we used to participate, and discuss our systems performance.
</nextsent>
<nextsent>our system, which consists of maximum entropy classifier trained using large variety of boolean features, received the third highest official score of all the entries.
</nextsent>
<nextsent>the groundwork for semeval 2010 task 8 was laid by an earlier semeval task (girju et al, 2007).
</nextsent>
<nextsent>for semeval 2007 task 4, participants provided yes or no answers as to whether particular relation held for each test example.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1066">
<title id=" S10-1049.xml">isi automatic classification of relations between nominals using a maximum entropy classifier </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for semeval 2010, instead of providing binary out put for single class, participants were required to perform multi-way classification, that is, select the most appropriate relation from set of 10 relations including the other relation.
</prevsent>
<prevsent>the selection of semantic relation for pairof nominals within sentence is somewhat similar to the task of noun compound interpretation, which is more restricted problem focused only upon the nouns within noun compounds.
</prevsent>
</prevsection>
<citsent citstr=" I05-1082 ">
some of the recent work on this problem includes that of butnariu et al (2009), <papid> W09-2416 </papid>girju (2007), girju et al (2005), kim and baldwin (2005), <papid> I05-1082 </papid>nakov (2008), nastase et al (2006), turney (2006), <papid> J06-3003 </papid>and ? saghdha and copestake (2009).</citsent>
<aftsection>
<nextsent>the task is, given pair of nominals within their sentence context, select the most appropriate semantic relation from the set of available relations and indicate the direction of the relation.
</nextsent>
<nextsent>though the final score was based upon the output of the system trained using the whole training dataset,participants were also required to submit three additional label sets using the first 12.5%, 25%, and 50% of the training data.
</nextsent>
<nextsent>3.1 relation scheme.
</nextsent>
<nextsent>the relations were taken from earlier work on noun compounds by nastase and szpakowicz (2003).a total of 10 relations were used including cause-effect, component-whole, content-container, entity-origin, entity-destination, instrument-agency, member-collection, message-topic, other, and product-producer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1067">
<title id=" S10-1049.xml">isi automatic classification of relations between nominals using a maximum entropy classifier </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for semeval 2010, instead of providing binary out put for single class, participants were required to perform multi-way classification, that is, select the most appropriate relation from set of 10 relations including the other relation.
</prevsent>
<prevsent>the selection of semantic relation for pairof nominals within sentence is somewhat similar to the task of noun compound interpretation, which is more restricted problem focused only upon the nouns within noun compounds.
</prevsent>
</prevsection>
<citsent citstr=" J06-3003 ">
some of the recent work on this problem includes that of butnariu et al (2009), <papid> W09-2416 </papid>girju (2007), girju et al (2005), kim and baldwin (2005), <papid> I05-1082 </papid>nakov (2008), nastase et al (2006), turney (2006), <papid> J06-3003 </papid>and ? saghdha and copestake (2009).</citsent>
<aftsection>
<nextsent>the task is, given pair of nominals within their sentence context, select the most appropriate semantic relation from the set of available relations and indicate the direction of the relation.
</nextsent>
<nextsent>though the final score was based upon the output of the system trained using the whole training dataset,participants were also required to submit three additional label sets using the first 12.5%, 25%, and 50% of the training data.
</nextsent>
<nextsent>3.1 relation scheme.
</nextsent>
<nextsent>the relations were taken from earlier work on noun compounds by nastase and szpakowicz (2003).a total of 10 relations were used including cause-effect, component-whole, content-container, entity-origin, entity-destination, instrument-agency, member-collection, message-topic, other, and product-producer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1068">
<title id=" S10-1049.xml">isi automatic classification of relations between nominals using a maximum entropy classifier </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>the training data also provides the correct relation for each example.
</prevsent>
<prevsent>4.1 classifier.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
we use maximum entropy (berger et al, 1996) <papid> J96-1002 </papid>classifier trained using large number of boolean features.</citsent>
<aftsection>
<nextsent>maximum entropy classifiers have proven effective for variety of nlp problems including word sense disambiguation (tratz et al,2007; <papid> W07-2057 </papid>ye and baldwin, 2007).<papid> W07-2051 </papid></nextsent>
<nextsent>we use the implementation provided in the mallet machine learning toolkit (mccallum, 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1069">
<title id=" S10-1049.xml">isi automatic classification of relations between nominals using a maximum entropy classifier </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 classifier.
</prevsent>
<prevsent>we use maximum entropy (berger et al, 1996) <papid> J96-1002 </papid>classifier trained using large number of boolean features.</prevsent>
</prevsection>
<citsent citstr=" W07-2057 ">
maximum entropy classifiers have proven effective for variety of nlp problems including word sense disambiguation (tratz et al,2007; <papid> W07-2057 </papid>ye and baldwin, 2007).<papid> W07-2051 </papid></citsent>
<aftsection>
<nextsent>we use the implementation provided in the mallet machine learning toolkit (mccallum, 2002).
</nextsent>
<nextsent>we used the default gaussian prior parameter value of 1.0.
</nextsent>
<nextsent>4.2 features used.
</nextsent>
<nextsent>we generate features from individual words, including both the nominals and their context, and from combinations of the nominals.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1070">
<title id=" S10-1049.xml">isi automatic classification of relations between nominals using a maximum entropy classifier </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 classifier.
</prevsent>
<prevsent>we use maximum entropy (berger et al, 1996) <papid> J96-1002 </papid>classifier trained using large number of boolean features.</prevsent>
</prevsection>
<citsent citstr=" W07-2051 ">
maximum entropy classifiers have proven effective for variety of nlp problems including word sense disambiguation (tratz et al,2007; <papid> W07-2057 </papid>ye and baldwin, 2007).<papid> W07-2051 </papid></citsent>
<aftsection>
<nextsent>we use the implementation provided in the mallet machine learning toolkit (mccallum, 2002).
</nextsent>
<nextsent>we used the default gaussian prior parameter value of 1.0.
</nextsent>
<nextsent>4.2 features used.
</nextsent>
<nextsent>we generate features from individual words, including both the nominals and their context, and from combinations of the nominals.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1071">
<title id=" S12-1020.xml">simple and phrasal implicatives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it extends the description of simple im plicative verbs to phrasal implicatives as take the time to and waste the chance to.
</prevsent>
<prevsent>it shows that the implicative signatures of over 300verb-noun collocations depend both on these mantic type of the verb and the semantic type of the noun in systematic way.
</prevsent>
</prevsection>
<citsent citstr=" W06-3907 ">
there is substantial body of literature on the semantics of english complement constructions starting with (kiparsky and kiparsky, 1970) and (kart tunen, 1971; karttunen, 1973), including (rudanko, 1989; rudanko, 2002; nairn et al, 2006; <papid> W06-3907 </papid>egan, 2008).</citsent>
<aftsection>
<nextsent>these studies have developed semantic classification of verbs and verb-noun collocations that take sentential complements.
</nextsent>
<nextsent>they focus on constructions that give rise to implied commitments that the author cannot disavow without being incoherent or without contradicting herself.
</nextsent>
<nextsent>for example, (1a) presupposes that kim had not rescheduled the meeting, (1b) entails that she didnt and presupposes that she intended to reschedule it.
</nextsent>
<nextsent>(1) a. kim forgot that she had not rescheduled the meeting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1074">
<title id=" P99-1064.xml">computational lexical semantics incrementality and the socalled punctuality of events </title>
<section> similar examples were proposed by declerck.  </section>
<citcontext>
<prevsection>
<prevsent>in this sense, his pudding and the chicken do not behave like delimiting arguments, and those non-atomic situations are non-incremental (\[-atm\],\[-inc\]).
</prevsent>
<prevsent>some sort of non-argumental odometer seems to be required.
</prevsent>
</prevsection>
<citsent citstr=" E99-1032 ">
in the case of (13) and (14), digest and cook receive scalar result state, i.e., one that varies as time passes: john chicken becomes (as whole) closer to being (finally) cooked as time passes in (14), and john pudding gradually turns (as whole, and not bit by bit) into nutriments inside his stomach in (13) (see caudal (1999<papid> E99-1032 </papid>a/b) for treatment of such data).</citsent>
<aftsection>
<nextsent>i will refer to this kind of incremental-like reading as scalarity.
</nextsent>
<nextsent>if one considers (15), things are somewhat different, as there exists some sort of predetermined series of stages through which one should pass in order to register at the university: johnson is closer and closer to being registered at the university as his father goes through them.
</nextsent>
<nextsent>i will refer to this kind of data as gradual scenarios.
</nextsent>
<nextsent>i will turn now to the computational treatment of incremental non-atomic events (section 4), before suggesting some ways of accounting for non-incremental non-atomic ones (section 5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1084">
<title id=" P98-2180.xml">mindnet acquiring and structuring semantic information from text </title>
<section> broad-coverage parsing.  </section>
<citcontext>
<prevsection>
<prevsent>the parser has not been specially tuned to process dictionary definitions.
</prevsent>
<prevsent>all enhancements to the parser are geared to handle the immense variety of general text, of which dictionary definitions are simply modest subset.
</prevsent>
</prevsection>
<citsent citstr=" P85-1037 ">
there have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., chodorow et al  1985), <papid> P85-1037 </papid>specially constructed definition parsers (e.g., wilks et al  1996, vossen 1995), and even general coverage syntactic parsers (e.g., briscoe and carroll 1993).<papid> J93-1002 </papid></citsent>
<aftsection>
<nextsent>however, none of these has succeeded in producing the breadth of semantic relations across entire dictionaries that has been produced for mindnet.
</nextsent>
<nextsent>vanderwende (1996) describes in detail the methodology used in the extraction of the semantic relations comprising mindnet.
</nextsent>
<nextsent>a truly broad-coverage parser is an essential component of this process, and it is the basis for extending it to other sources of information such as encyclopedias nd text corpora.
</nextsent>
<nextsent>the different ypes of labeled, semantic relations extracted by parsing for inclusion in mindnet are given in the table below: 1098 attribute goal cause co-agent color deep_object deep.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1085">
<title id=" P98-2180.xml">mindnet acquiring and structuring semantic information from text </title>
<section> broad-coverage parsing.  </section>
<citcontext>
<prevsection>
<prevsent>the parser has not been specially tuned to process dictionary definitions.
</prevsent>
<prevsent>all enhancements to the parser are geared to handle the immense variety of general text, of which dictionary definitions are simply modest subset.
</prevsent>
</prevsection>
<citsent citstr=" J93-1002 ">
there have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., chodorow et al  1985), <papid> P85-1037 </papid>specially constructed definition parsers (e.g., wilks et al  1996, vossen 1995), and even general coverage syntactic parsers (e.g., briscoe and carroll 1993).<papid> J93-1002 </papid></citsent>
<aftsection>
<nextsent>however, none of these has succeeded in producing the breadth of semantic relations across entire dictionaries that has been produced for mindnet.
</nextsent>
<nextsent>vanderwende (1996) describes in detail the methodology used in the extraction of the semantic relations comprising mindnet.
</nextsent>
<nextsent>a truly broad-coverage parser is an essential component of this process, and it is the basis for extending it to other sources of information such as encyclopedias nd text corpora.
</nextsent>
<nextsent>the different ypes of labeled, semantic relations extracted by parsing for inclusion in mindnet are given in the table below: 1098 attribute goal cause co-agent color deep_object deep.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1086">
<title id=" P98-2180.xml">mindnet acquiring and structuring semantic information from text </title>
<section> labeled, semantic relations.  </section>
<citcontext>
<prevsection>
<prevsent>the different ypes of labeled, semantic relations extracted by parsing for inclusion in mindnet are given in the table below: 1098 attribute goal cause co-agent color deep_object deep.
</prevsent>
<prevsent>subject domain hypernym location manner material means possessor purpose size source subclass modifier equivalent part user synonym time table 1.
</prevsent>
</prevsection>
<citsent citstr=" C90-2067 ">
current set of semantic relation types mindnet these relation types may be contrasted with simple co-occurrence statistics used to create network structures from dictionaries by researchers including veronis and ide (1990), <papid> C90-2067 </papid>kozima and furugori (1993), and wilks et al  (1996).</citsent>
<aftsection>
<nextsent>labeled relations, while more difficult to obtain, provide greater power for resolving both structural attachment and word sense ambiguities.
</nextsent>
<nextsent>while many researchers have acknowledged the utility of labeled relations, they have been at times either unable (e.g., for lack of sufficiently powerful parser) or unwilling (e.g., focused on purely statistical methods) to make the effort to obtain them.
</nextsent>
<nextsent>this deficiency limits the characterization word pairs such as river~bank (wilks et al  1996) and write~pen (veronis and ide 1990) <papid> C90-2067 </papid>to simple relatedness, whereas the labeled relations of mindnet specify precisely the relations river---part-- bank and write---means--- pen.</nextsent>
<nextsent>the automatic extraction of semantic relations (or semrels) from definition or example sentence for mindnet produces hierarchical structure of these relations, representing the entire definition or sentence from which they came.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1089">
<title id=" P98-2180.xml">mindnet acquiring and structuring semantic information from text </title>
<section> full inversion of structures.  </section>
<citcontext>
<prevsection>
<prevsent>words were not related backward to any of the headwords whose definitions mentioned them, and words co-occurring in the same definition were not related directly.
</prevsent>
<prevsent>in the fully inverted structures tored in mindnet, however, all words are cross-linked, no matter where they appear.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
the massive network of inverted semrel structures contained in mindnet invalidates the criticism leveled against dictionary-based methods by yarowsky (1992) <papid> C92-2070 </papid>and ide and veronis (1993) that lkbs created from mrds provide spotty coverage of language at best.</citsent>
<aftsection>
<nextsent>experiments described elsewhere (richardson 1997) demonstrate the comprehensive coverage of the information contained in mindnet.
</nextsent>
<nextsent>some statistics indicating the size (rounded to the nearest thousand) of the current version of mindnet and the processing time required to create it are provided in the table below.
</nextsent>
<nextsent>the definitions and example sentences are from the longman dictionary of contemporary english (ldoce) and the american heritage dictionary, 3 ra edition (ahd3).
</nextsent>
<nextsent>1099 dictionaries used ldoce &amp; ahd 3 time to create (on p2/266) 7 hours headwords 159,000 definitions (n, v, adj) .191,000 example sentences (n, v, adj) unique semantic relations 58,000 713,000 inverted structures 1,047,000 linked headwords 91,000 table 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1090">
<title id=" P98-2180.xml">mindnet acquiring and structuring semantic information from text </title>
<section> similarity and inference.  </section>
<citcontext>
<prevsection>
<prevsent>two general strategies have been described in the literature for identifying substitutional similarity.
</prevsent>
<prevsent>one is based on identifying direct, paradigmatic relations between the words, such as hypernym or synonym.
</prevsent>
</prevsection>
<citsent citstr=" C96-1005 ">
for example, paradigmatic relations in wordnet have been used by many to determine similarity, including li et al  (1995) and agirre and rigau (1996).<papid> C96-1005 </papid></citsent>
<aftsection>
<nextsent>the other strategy is based on identifying syntagmatic relations with other words that similar words have in common.
</nextsent>
<nextsent>syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., brown et al  1992, <papid> J92-4003 </papid>yarowsky 1992), <papid> C92-2070 </papid>as well as in similar predicate- argument structure contexts (e.g., grishman and sterling 1994).<papid> C94-2119 </papid></nextsent>
<nextsent>there have been number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., hearst and grefenstette 1992, resnik 1995).<papid> W95-0105 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1091">
<title id=" P98-2180.xml">mindnet acquiring and structuring semantic information from text </title>
<section> similarity and inference.  </section>
<citcontext>
<prevsection>
<prevsent>for example, paradigmatic relations in wordnet have been used by many to determine similarity, including li et al  (1995) and agirre and rigau (1996).<papid> C96-1005 </papid></prevsent>
<prevsent>the other strategy is based on identifying syntagmatic relations with other words that similar words have in common.</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., brown et al  1992, <papid> J92-4003 </papid>yarowsky 1992), <papid> C92-2070 </papid>as well as in similar predicate- argument structure contexts (e.g., grishman and sterling 1994).<papid> C94-2119 </papid></citsent>
<aftsection>
<nextsent>there have been number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., hearst and grefenstette 1992, resnik 1995).<papid> W95-0105 </papid></nextsent>
<nextsent>however, none of these has completely integrated both syntagmatic and paradigmatic nformation into single repository, as is the case with mindnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1093">
<title id=" P98-2180.xml">mindnet acquiring and structuring semantic information from text </title>
<section> similarity and inference.  </section>
<citcontext>
<prevsection>
<prevsent>for example, paradigmatic relations in wordnet have been used by many to determine similarity, including li et al  (1995) and agirre and rigau (1996).<papid> C96-1005 </papid></prevsent>
<prevsent>the other strategy is based on identifying syntagmatic relations with other words that similar words have in common.</prevsent>
</prevsection>
<citsent citstr=" C94-2119 ">
syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., brown et al  1992, <papid> J92-4003 </papid>yarowsky 1992), <papid> C92-2070 </papid>as well as in similar predicate- argument structure contexts (e.g., grishman and sterling 1994).<papid> C94-2119 </papid></citsent>
<aftsection>
<nextsent>there have been number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., hearst and grefenstette 1992, resnik 1995).<papid> W95-0105 </papid></nextsent>
<nextsent>however, none of these has completely integrated both syntagmatic and paradigmatic nformation into single repository, as is the case with mindnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1094">
<title id=" P98-2180.xml">mindnet acquiring and structuring semantic information from text </title>
<section> similarity and inference.  </section>
<citcontext>
<prevsection>
<prevsent>the other strategy is based on identifying syntagmatic relations with other words that similar words have in common.
</prevsent>
<prevsent>syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., brown et al  1992, <papid> J92-4003 </papid>yarowsky 1992), <papid> C92-2070 </papid>as well as in similar predicate- argument structure contexts (e.g., grishman and sterling 1994).<papid> C94-2119 </papid></prevsent>
</prevsection>
<citsent citstr=" W95-0105 ">
there have been number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., hearst and grefenstette 1992, resnik 1995).<papid> W95-0105 </papid></citsent>
<aftsection>
<nextsent>however, none of these has completely integrated both syntagmatic and paradigmatic nformation into single repository, as is the case with mindnet.
</nextsent>
<nextsent>the mindnet similarity procedure is based on the top-ranked (by weight) semrel paths between words.
</nextsent>
<nextsent>for example, some of the top semrel paths in mindnet between pen and pencil, are shown below: pen6-means---draw--means-- pencil pen --means--write--means--~pencil pen--hyp-- instrument~--hyp---pencil pen--hyp-- write--means---~pencil pen6-means--write6--hyp--pencil table 3.
</nextsent>
<nextsent>highly weighted semrel paths between pen and pencil in the above example, pattern of semrel symmetry clearly emerges in many of the paths.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1096">
<title id=" S12-1107.xml">dirrelcond3 detecting textual entailment across languages with conditions on directional text relatedness scores </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>there are relatively few entailment heuristics that exploit the directional nature of the entailment relation.
</prevsent>
</prevsection>
<citsent citstr=" S12-1053 ">
cross-lingual text entailment(clte), besides introducing the extra dimension of cross-linguality, also requires to determine the exact direction of the entailment relation, to provide content synchronization(negri et al, 2012).<papid> S12-1053 </papid></citsent>
<aftsection>
<nextsent>our system uses simple dictionary lookup combined with heuristic conditions to determine the possible directions of entailment between the two texts written in different languages.
</nextsent>
<nextsent>the key members of the conditions were derived from (cor ley and mihalcea, 2005) <papid> W05-1203 </papid>formula initially for text similarity, while the entailment condition used as starting point was that from (tatar et al, 2009).</nextsent>
<nextsent>we show the results obtained by our implementation of this simple and fast approach at the clte task from the semeval 2012 challenge.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1097">
<title id=" S12-1107.xml">dirrelcond3 detecting textual entailment across languages with conditions on directional text relatedness scores </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>cross-lingual text entailment(clte), besides introducing the extra dimension of cross-linguality, also requires to determine the exact direction of the entailment relation, to provide content synchronization(negri et al, 2012).<papid> S12-1053 </papid></prevsent>
<prevsent>our system uses simple dictionary lookup combined with heuristic conditions to determine the possible directions of entailment between the two texts written in different languages.</prevsent>
</prevsection>
<citsent citstr=" W05-1203 ">
the key members of the conditions were derived from (cor ley and mihalcea, 2005) <papid> W05-1203 </papid>formula initially for text similarity, while the entailment condition used as starting point was that from (tatar et al, 2009).</citsent>
<aftsection>
<nextsent>we show the results obtained by our implementation of this simple and fast approach at the clte task from the semeval 2012 challenge.
</nextsent>
<nextsent>recognizing textual entailment (te) is key taskfor many natural language processing (nlp) problems.
</nextsent>
<nextsent>it consists in determining if an entailment relation exists between two texts: the text (t) and the hypothesis (h).
</nextsent>
<nextsent>the notation ? says that the meaning of can be inferred from t, in order words,h does not introduce any novel information with respect to t.even though rte challenges lead to many approaches for finding textual entailment, fewer authors exploited the directional character of the entailment relation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1099">
<title id=" S12-1107.xml">dirrelcond3 detecting textual entailment across languages with conditions on directional text relatedness scores </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>due to the fact that the entailment relation, unlike the equivalence relation, is not symmetric, if ? , it is less likely that the reverse ? can also hold (tatar et al, 2009).
</prevsent>
<prevsent>the novel cross-lingual text entailment (clte) approach increases the complexity of the traditional te task in two way, both of which have been only partially researched and have promise for great potential (negri et al, 2012): ? <papid> S12-1053 </papid>the two texts are no longer written in the same language (cross-linguality);?</prevsent>
</prevsection>
<citsent citstr=" N10-1045 ">
the entailment needs to be queried in both directions (content synchronization).mehdad et al (2010) <papid> N10-1045 </papid>presented initial research directions and experiments for the cross-lingual context and explored possible application scenarios.</citsent>
<aftsection>
<nextsent>the semantic similarity formula from (corley and mihalcea, 2005) <papid> W05-1203 </papid>defines the similarity of pair of documents differently depending on with respect to which text it is computed.</nextsent>
<nextsent>the formula involves only the set of open-class words (nouns, verbs, adjectives and adverbs) from each text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1104">
<title id=" S12-1107.xml">dirrelcond3 detecting textual entailment across languages with conditions on directional text relatedness scores </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>no entail, if rel(t,h)t or rel(t,h)h   ? bidir, if abs(rel(t,h)t , rel(t,h)h)   ? forward, if rel(t,h)h   rel(t,h)t + ? backwd, otherwise (4)
</prevsent>
<prevsent>the clte task provided researchers with training sets of 500 sentence pairs (one english, one foreign) already annotated with the type of entailment that exists between them (forward?, backward?, bidirectional?, no entailment?).
</prevsent>
</prevsection>
<citsent citstr=" D11-1062 ">
there was one training set for each french-english, german-english,italian-english, spanish-english language combination (negri et al, 2011).<papid> D11-1062 </papid></citsent>
<aftsection>
<nextsent>the test set consisted in similarly structured 500 pairs for each language pair but without annotations.
</nextsent>
<nextsent>the mentioned entailment judgment types were uniformly distributed, both in the case of the development and the test dataset.
</nextsent>
<nextsent>the dirrelcond3 system participated at theclte task with four runs for each of the above language combinations.
</nextsent>
<nextsent>regarding the results, the accuracies obtained are summarized in table 1 as per centages.figures 1, 2, 3, 4 show the precision, recall and fmeasure for the forward?, backward?, no entail ment?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1107">
<title id=" P99-1066.xml">automatic compensation for parser figureofmerit flaws </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>incomplete constituents ( edges ) are stored in an agenda.
</prevsent>
<prevsent>the exhaustion of the agenda definitively marks the comple-tion of the parsing algorithm, but the parse needn ake that long; mready in the early work on chart parsing, (kay, 1970) suggests that by ordering the agenda one can find parse without resorting to an exhaustive search.
</prevsent>
</prevsection>
<citsent citstr=" H90-1053 ">
the introduction of statistical pars-ing brought with an obvious tactic for rank-ing the agenda: (bobrow, 1990) and (chi- trao and grishman, 1990) <papid> H90-1053 </papid>first used proba-bilistic context free grammars (pcfgs) to generate probabilities for use in figure of merit (fom).</citsent>
<aftsection>
<nextsent>later work introduced other foms formed from pcfg data (kochman and kupin, 1991); (<papid> H91-1045 </papid>magerman and marcus, 1991); <papid> H91-1044 </papid>and (miller and fox, 1994).<papid> H94-1051 </papid></nextsent>
<nextsent>more recently, we have seen parse times lowered by several orders of magnitude.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1108">
<title id=" P99-1066.xml">automatic compensation for parser figureofmerit flaws </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the exhaustion of the agenda definitively marks the comple-tion of the parsing algorithm, but the parse needn ake that long; mready in the early work on chart parsing, (kay, 1970) suggests that by ordering the agenda one can find parse without resorting to an exhaustive search.
</prevsent>
<prevsent>the introduction of statistical pars-ing brought with an obvious tactic for rank-ing the agenda: (bobrow, 1990) and (chi- trao and grishman, 1990) <papid> H90-1053 </papid>first used proba-bilistic context free grammars (pcfgs) to generate probabilities for use in figure of merit (fom).</prevsent>
</prevsection>
<citsent citstr=" H91-1045 ">
later work introduced other foms formed from pcfg data (kochman and kupin, 1991); (<papid> H91-1045 </papid>magerman and marcus, 1991); <papid> H91-1044 </papid>and (miller and fox, 1994).<papid> H94-1051 </papid></citsent>
<aftsection>
<nextsent>more recently, we have seen parse times lowered by several orders of magnitude.
</nextsent>
<nextsent>the (caraballo and charniak, 1998) <papid> J98-2004 </papid>article con-siders number of different figures of merit for ordering the agenda, and ultimately rec-ommends one that reduces the number of edges required for full parse into the thou- sands.</nextsent>
<nextsent>(goldwater et al, 1998) (henceforth \[gold98\]) introduces an edge-based tech-nique, (instead of constituent-based), which drops the average dge count into the hun- dreds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1109">
<title id=" P99-1066.xml">automatic compensation for parser figureofmerit flaws </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the exhaustion of the agenda definitively marks the comple-tion of the parsing algorithm, but the parse needn ake that long; mready in the early work on chart parsing, (kay, 1970) suggests that by ordering the agenda one can find parse without resorting to an exhaustive search.
</prevsent>
<prevsent>the introduction of statistical pars-ing brought with an obvious tactic for rank-ing the agenda: (bobrow, 1990) and (chi- trao and grishman, 1990) <papid> H90-1053 </papid>first used proba-bilistic context free grammars (pcfgs) to generate probabilities for use in figure of merit (fom).</prevsent>
</prevsection>
<citsent citstr=" H91-1044 ">
later work introduced other foms formed from pcfg data (kochman and kupin, 1991); (<papid> H91-1045 </papid>magerman and marcus, 1991); <papid> H91-1044 </papid>and (miller and fox, 1994).<papid> H94-1051 </papid></citsent>
<aftsection>
<nextsent>more recently, we have seen parse times lowered by several orders of magnitude.
</nextsent>
<nextsent>the (caraballo and charniak, 1998) <papid> J98-2004 </papid>article con-siders number of different figures of merit for ordering the agenda, and ultimately rec-ommends one that reduces the number of edges required for full parse into the thou- sands.</nextsent>
<nextsent>(goldwater et al, 1998) (henceforth \[gold98\]) introduces an edge-based tech-nique, (instead of constituent-based), which drops the average dge count into the hun- dreds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1110">
<title id=" P99-1066.xml">automatic compensation for parser figureofmerit flaws </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the exhaustion of the agenda definitively marks the comple-tion of the parsing algorithm, but the parse needn ake that long; mready in the early work on chart parsing, (kay, 1970) suggests that by ordering the agenda one can find parse without resorting to an exhaustive search.
</prevsent>
<prevsent>the introduction of statistical pars-ing brought with an obvious tactic for rank-ing the agenda: (bobrow, 1990) and (chi- trao and grishman, 1990) <papid> H90-1053 </papid>first used proba-bilistic context free grammars (pcfgs) to generate probabilities for use in figure of merit (fom).</prevsent>
</prevsection>
<citsent citstr=" H94-1051 ">
later work introduced other foms formed from pcfg data (kochman and kupin, 1991); (<papid> H91-1045 </papid>magerman and marcus, 1991); <papid> H91-1044 </papid>and (miller and fox, 1994).<papid> H94-1051 </papid></citsent>
<aftsection>
<nextsent>more recently, we have seen parse times lowered by several orders of magnitude.
</nextsent>
<nextsent>the (caraballo and charniak, 1998) <papid> J98-2004 </papid>article con-siders number of different figures of merit for ordering the agenda, and ultimately rec-ommends one that reduces the number of edges required for full parse into the thou- sands.</nextsent>
<nextsent>(goldwater et al, 1998) (henceforth \[gold98\]) introduces an edge-based tech-nique, (instead of constituent-based), which drops the average dge count into the hun- dreds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1111">
<title id=" P99-1066.xml">automatic compensation for parser figureofmerit flaws </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>later work introduced other foms formed from pcfg data (kochman and kupin, 1991); (<papid> H91-1045 </papid>magerman and marcus, 1991); <papid> H91-1044 </papid>and (miller and fox, 1994).<papid> H94-1051 </papid></prevsent>
<prevsent>more recently, we have seen parse times lowered by several orders of magnitude.</prevsent>
</prevsection>
<citsent citstr=" J98-2004 ">
the (caraballo and charniak, 1998) <papid> J98-2004 </papid>article con-siders number of different figures of merit for ordering the agenda, and ultimately rec-ommends one that reduces the number of edges required for full parse into the thou- sands.</citsent>
<aftsection>
<nextsent>(goldwater et al, 1998) (henceforth \[gold98\]) introduces an edge-based tech-nique, (instead of constituent-based), which drops the average dge count into the hun-dreds.
</nextsent>
<nextsent>however, if we establish  perfection  as the minimum number of edges needed to generate the correct parse 47.5 edges on av-erage in our corpus--we can hope for still more improvement.
</nextsent>
<nextsent>this paper looks at two new figures of merit, both of which take the \[gold98\] figure (of  independent  merit) as starting point in cmculating new figure 513 of merit for each edge, taking into account some additional information.
</nextsent>
<nextsent>our work fur-ther lowers the average dge count, bringing it from the hundreds into the dozens.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1119">
<title id=" S12-1033.xml">emotional tweets </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>emotion analysis can be applied to all kinds of text, but certain domains and modes of communication tend to have more overt expressions of emotions than others.
</prevsent>
<prevsent>genereux and evans (2006), mihalcea and liu (2006), and neviarouskaya et al (2009) analyzed web-logs.
</prevsent>
</prevsection>
<citsent citstr=" H05-1073 ">
alm et al (2005) <papid> H05-1073 </papid>and francisco and gervas (2006) worked on fairy tales.</citsent>
<aftsection>
<nextsent>boucouvalas (2002), john et al (2006), and mohammad (2012a) explored emotions in novels.
</nextsent>
<nextsent>zhe and boucouvalas (2002), holzman and pottenger (2003), and ma et al (2005) annotated chat messages for emotions.
</nextsent>
<nextsent>liu et al.
</nextsent>
<nextsent>(2003) and mohammad and yang (2011) worked on email data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1120">
<title id=" S12-1033.xml">emotional tweets </title>
<section> creating the twitter emotion corpus.  </section>
<citcontext>
<prevsection>
<prevsent>are the emotion annotations consistent, despite the large number of annotators, despite no control over their socio-economic and cultural background, despite the many ways in whichhashtags are used, and despite the many idiosyncracies of tweets??
</prevsent>
<prevsent>do the hashtag annotations match with the intuitions of trained judge swe chose to collect tweets with hash tags corresponding to the six ekman emotions: #anger, #dis gust, #fear, #happy, #sadness, and #surprise.
</prevsent>
</prevsection>
<citsent citstr=" D10-1124 ">
eisenstein et al (2010) <papid> D10-1124 </papid>collected about 380,000 tweets2 from twitters official api.3 similarly, go et al (2009) collected 1.6 million tweets.4 however, these datasets had less than 50 tweets that contained emotion-word hashtags.</citsent>
<aftsection>
<nextsent>therefore, we abandoned the search-in-corpora approach in favor of the one described below.
</nextsent>
<nextsent>2http://www.ark.cs.cmu.edu/geotext 3https://dev.twitter.com/docs/streaming-api 4https://sites.google.com/site/twittersentimenthelp 248 4.1 hashtag-based search on the twitter.
</nextsent>
<nextsent>search api the archivist5 is free online service that helps users extract tweets using twitters search api.6 for any given query, archivist first obtains up to1500 tweets from the previous seven days.
</nextsent>
<nextsent>subsequently, it polls the twitter search api every few hours to obtain newer tweets that match the query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1121">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>forantibiotics?), and composite terms (e.g., maydecember?
</prevsent>
<prevsent>or virus/worm?).
</prevsent>
</prevsection>
<citsent citstr=" W97-0313 ">
to address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (riloff and shepherd, 1997; <papid> W97-0313 </papid>roark and charniak, 1998; <papid> P98-2182 </papid>phillips and riloff, 2002; <papid> W02-1017 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>ng, 2007; <papid> P07-1068 </papid>mcintosh and curran, 2009; <papid> P09-1045 </papid>mcintosh, 2010), <papid> D10-1035 </papid>but accuracy is still far from perfect.our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction.</citsent>
<aftsection>
<nextsent>our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis.
</nextsent>
<nextsent>bootstrapping methods for semantic lexicon induction (e.g., (riloff and jones, 1999; thelen and riloff, 2002; <papid> W02-1028 </papid>mcintosh and curran, 2009)) <papid> P09-1045 </papid>collect corpus-widestatistics for individual words based on shared contextual patterns.</nextsent>
<nextsent>in contrast, classifiers for semantic tagging (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003; <papid> P03-1043 </papid>huang and riloff, 2010)) <papid> P10-1029 </papid>label word instance sand focus on the local context surrounding each in stance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1122">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>forantibiotics?), and composite terms (e.g., maydecember?
</prevsent>
<prevsent>or virus/worm?).
</prevsent>
</prevsection>
<citsent citstr=" P98-2182 ">
to address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (riloff and shepherd, 1997; <papid> W97-0313 </papid>roark and charniak, 1998; <papid> P98-2182 </papid>phillips and riloff, 2002; <papid> W02-1017 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>ng, 2007; <papid> P07-1068 </papid>mcintosh and curran, 2009; <papid> P09-1045 </papid>mcintosh, 2010), <papid> D10-1035 </papid>but accuracy is still far from perfect.our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction.</citsent>
<aftsection>
<nextsent>our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis.
</nextsent>
<nextsent>bootstrapping methods for semantic lexicon induction (e.g., (riloff and jones, 1999; thelen and riloff, 2002; <papid> W02-1028 </papid>mcintosh and curran, 2009)) <papid> P09-1045 </papid>collect corpus-widestatistics for individual words based on shared contextual patterns.</nextsent>
<nextsent>in contrast, classifiers for semantic tagging (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003; <papid> P03-1043 </papid>huang and riloff, 2010)) <papid> P10-1029 </papid>label word instance sand focus on the local context surrounding each in stance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1123">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>forantibiotics?), and composite terms (e.g., maydecember?
</prevsent>
<prevsent>or virus/worm?).
</prevsent>
</prevsection>
<citsent citstr=" W02-1017 ">
to address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (riloff and shepherd, 1997; <papid> W97-0313 </papid>roark and charniak, 1998; <papid> P98-2182 </papid>phillips and riloff, 2002; <papid> W02-1017 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>ng, 2007; <papid> P07-1068 </papid>mcintosh and curran, 2009; <papid> P09-1045 </papid>mcintosh, 2010), <papid> D10-1035 </papid>but accuracy is still far from perfect.our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction.</citsent>
<aftsection>
<nextsent>our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis.
</nextsent>
<nextsent>bootstrapping methods for semantic lexicon induction (e.g., (riloff and jones, 1999; thelen and riloff, 2002; <papid> W02-1028 </papid>mcintosh and curran, 2009)) <papid> P09-1045 </papid>collect corpus-widestatistics for individual words based on shared contextual patterns.</nextsent>
<nextsent>in contrast, classifiers for semantic tagging (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003; <papid> P03-1043 </papid>huang and riloff, 2010)) <papid> P10-1029 </papid>label word instance sand focus on the local context surrounding each in stance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1124">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>forantibiotics?), and composite terms (e.g., maydecember?
</prevsent>
<prevsent>or virus/worm?).
</prevsent>
</prevsection>
<citsent citstr=" W02-1028 ">
to address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (riloff and shepherd, 1997; <papid> W97-0313 </papid>roark and charniak, 1998; <papid> P98-2182 </papid>phillips and riloff, 2002; <papid> W02-1017 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>ng, 2007; <papid> P07-1068 </papid>mcintosh and curran, 2009; <papid> P09-1045 </papid>mcintosh, 2010), <papid> D10-1035 </papid>but accuracy is still far from perfect.our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction.</citsent>
<aftsection>
<nextsent>our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis.
</nextsent>
<nextsent>bootstrapping methods for semantic lexicon induction (e.g., (riloff and jones, 1999; thelen and riloff, 2002; <papid> W02-1028 </papid>mcintosh and curran, 2009)) <papid> P09-1045 </papid>collect corpus-widestatistics for individual words based on shared contextual patterns.</nextsent>
<nextsent>in contrast, classifiers for semantic tagging (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003; <papid> P03-1043 </papid>huang and riloff, 2010)) <papid> P10-1029 </papid>label word instance sand focus on the local context surrounding each in stance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1127">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>forantibiotics?), and composite terms (e.g., maydecember?
</prevsent>
<prevsent>or virus/worm?).
</prevsent>
</prevsection>
<citsent citstr=" P07-1068 ">
to address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (riloff and shepherd, 1997; <papid> W97-0313 </papid>roark and charniak, 1998; <papid> P98-2182 </papid>phillips and riloff, 2002; <papid> W02-1017 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>ng, 2007; <papid> P07-1068 </papid>mcintosh and curran, 2009; <papid> P09-1045 </papid>mcintosh, 2010), <papid> D10-1035 </papid>but accuracy is still far from perfect.our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction.</citsent>
<aftsection>
<nextsent>our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis.
</nextsent>
<nextsent>bootstrapping methods for semantic lexicon induction (e.g., (riloff and jones, 1999; thelen and riloff, 2002; <papid> W02-1028 </papid>mcintosh and curran, 2009)) <papid> P09-1045 </papid>collect corpus-widestatistics for individual words based on shared contextual patterns.</nextsent>
<nextsent>in contrast, classifiers for semantic tagging (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003; <papid> P03-1043 </papid>huang and riloff, 2010)) <papid> P10-1029 </papid>label word instance sand focus on the local context surrounding each in stance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1128">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>forantibiotics?), and composite terms (e.g., maydecember?
</prevsent>
<prevsent>or virus/worm?).
</prevsent>
</prevsection>
<citsent citstr=" P09-1045 ">
to address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (riloff and shepherd, 1997; <papid> W97-0313 </papid>roark and charniak, 1998; <papid> P98-2182 </papid>phillips and riloff, 2002; <papid> W02-1017 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>ng, 2007; <papid> P07-1068 </papid>mcintosh and curran, 2009; <papid> P09-1045 </papid>mcintosh, 2010), <papid> D10-1035 </papid>but accuracy is still far from perfect.our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction.</citsent>
<aftsection>
<nextsent>our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis.
</nextsent>
<nextsent>bootstrapping methods for semantic lexicon induction (e.g., (riloff and jones, 1999; thelen and riloff, 2002; <papid> W02-1028 </papid>mcintosh and curran, 2009)) <papid> P09-1045 </papid>collect corpus-widestatistics for individual words based on shared contextual patterns.</nextsent>
<nextsent>in contrast, classifiers for semantic tagging (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003; <papid> P03-1043 </papid>huang and riloff, 2010)) <papid> P10-1029 </papid>label word instance sand focus on the local context surrounding each in stance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1129">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>forantibiotics?), and composite terms (e.g., maydecember?
</prevsent>
<prevsent>or virus/worm?).
</prevsent>
</prevsection>
<citsent citstr=" D10-1035 ">
to address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (riloff and shepherd, 1997; <papid> W97-0313 </papid>roark and charniak, 1998; <papid> P98-2182 </papid>phillips and riloff, 2002; <papid> W02-1017 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>ng, 2007; <papid> P07-1068 </papid>mcintosh and curran, 2009; <papid> P09-1045 </papid>mcintosh, 2010), <papid> D10-1035 </papid>but accuracy is still far from perfect.our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction.</citsent>
<aftsection>
<nextsent>our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis.
</nextsent>
<nextsent>bootstrapping methods for semantic lexicon induction (e.g., (riloff and jones, 1999; thelen and riloff, 2002; <papid> W02-1028 </papid>mcintosh and curran, 2009)) <papid> P09-1045 </papid>collect corpus-widestatistics for individual words based on shared contextual patterns.</nextsent>
<nextsent>in contrast, classifiers for semantic tagging (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003; <papid> P03-1043 </papid>huang and riloff, 2010)) <papid> P10-1029 </papid>label word instance sand focus on the local context surrounding each in stance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1135">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis.
</prevsent>
<prevsent>bootstrapping methods for semantic lexicon induction (e.g., (riloff and jones, 1999; thelen and riloff, 2002; <papid> W02-1028 </papid>mcintosh and curran, 2009)) <papid> P09-1045 </papid>collect corpus-widestatistics for individual words based on shared contextual patterns.</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
in contrast, classifiers for semantic tagging (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003; <papid> P03-1043 </papid>huang and riloff, 2010)) <papid> P10-1029 </papid>label word instance sand focus on the local context surrounding each in stance.</citsent>
<aftsection>
<nextsent>the difference between these approaches isthat semantic taggers make decisions based on single context and can assign different labels to different instances, whereas lexicon induction algorithms compile corpus statistics from multiple instances of word and typically assign each word to single semantic category.1 we also hypothesize that coreference resolution can be exploited to infer semantic1this approach would be untenable for broad-coverage semantic knowledge acquisition, but within specialized domain most words have dominant word sense.
</nextsent>
<nextsent>our experimental results support this assumption.
</nextsent>
<nextsent>199 class labels.
</nextsent>
<nextsent>intuitively, if we know that two noun phrases are co referent, then they probably belong to the same high-level semantic category (e.g., dog?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1138">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis.
</prevsent>
<prevsent>bootstrapping methods for semantic lexicon induction (e.g., (riloff and jones, 1999; thelen and riloff, 2002; <papid> W02-1028 </papid>mcintosh and curran, 2009)) <papid> P09-1045 </papid>collect corpus-widestatistics for individual words based on shared contextual patterns.</prevsent>
</prevsection>
<citsent citstr=" P03-1043 ">
in contrast, classifiers for semantic tagging (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003; <papid> P03-1043 </papid>huang and riloff, 2010)) <papid> P10-1029 </papid>label word instance sand focus on the local context surrounding each in stance.</citsent>
<aftsection>
<nextsent>the difference between these approaches isthat semantic taggers make decisions based on single context and can assign different labels to different instances, whereas lexicon induction algorithms compile corpus statistics from multiple instances of word and typically assign each word to single semantic category.1 we also hypothesize that coreference resolution can be exploited to infer semantic1this approach would be untenable for broad-coverage semantic knowledge acquisition, but within specialized domain most words have dominant word sense.
</nextsent>
<nextsent>our experimental results support this assumption.
</nextsent>
<nextsent>199 class labels.
</nextsent>
<nextsent>intuitively, if we know that two noun phrases are co referent, then they probably belong to the same high-level semantic category (e.g., dog?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1139">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis.
</prevsent>
<prevsent>bootstrapping methods for semantic lexicon induction (e.g., (riloff and jones, 1999; thelen and riloff, 2002; <papid> W02-1028 </papid>mcintosh and curran, 2009)) <papid> P09-1045 </papid>collect corpus-widestatistics for individual words based on shared contextual patterns.</prevsent>
</prevsection>
<citsent citstr=" P10-1029 ">
in contrast, classifiers for semantic tagging (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003; <papid> P03-1043 </papid>huang and riloff, 2010)) <papid> P10-1029 </papid>label word instance sand focus on the local context surrounding each in stance.</citsent>
<aftsection>
<nextsent>the difference between these approaches isthat semantic taggers make decisions based on single context and can assign different labels to different instances, whereas lexicon induction algorithms compile corpus statistics from multiple instances of word and typically assign each word to single semantic category.1 we also hypothesize that coreference resolution can be exploited to infer semantic1this approach would be untenable for broad-coverage semantic knowledge acquisition, but within specialized domain most words have dominant word sense.
</nextsent>
<nextsent>our experimental results support this assumption.
</nextsent>
<nextsent>199 class labels.
</nextsent>
<nextsent>intuitively, if we know that two noun phrases are co referent, then they probably belong to the same high-level semantic category (e.g., dog?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1147">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our results show that the induced dictionaries yield better performance than aninstance-based semantic tagger, achieving higher accuracy with comparable levels of recall.
</prevsent>
<prevsent>several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (riloff and shepherd, 1997), <papid> W97-0313 </papid>syntactic structures (roark and charniak,1998; <papid> P98-2182 </papid>phillips and riloff, 2002), <papid> W02-1017 </papid>and contextual patterns (riloff and jones, 1999; thelen and riloff, 2002; <papid> W02-1028 </papid>mcintosh and curran, 2008; mcintosh and curran, 2009).<papid> P09-1045 </papid></prevsent>
</prevsection>
<citsent citstr=" N09-1033 ">
to improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (vyasand pantel, 2009), <papid> N09-1033 </papid>automatically discovered negative classes (mcintosh, 2010), <papid> D10-1035 </papid>and distributional similarity metrics to recognize concept drift (mcin tosh and curran, 2009).<papid> P09-1045 </papid></citsent>
<aftsection>
<nextsent>phillips and riloff (2002)<papid> W02-1017 </papid>used co-training (blum and mitchell, 1998) to exploit three simple classifiers that each recognized different type of syntactic structure.</nextsent>
<nextsent>the research most closely related to ours is an ensemble-basedmethod for automatic thesaurus construction (curran, 2002).<papid> W02-1029 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1152">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (vyasand pantel, 2009), <papid> N09-1033 </papid>automatically discovered negative classes (mcintosh, 2010), <papid> D10-1035 </papid>and distributional similarity metrics to recognize concept drift (mcin tosh and curran, 2009).<papid> P09-1045 </papid></prevsent>
<prevsent>phillips and riloff (2002)<papid> W02-1017 </papid>used co-training (blum and mitchell, 1998) to exploit three simple classifiers that each recognized different type of syntactic structure.</prevsent>
</prevsection>
<citsent citstr=" W02-1029 ">
the research most closely related to ours is an ensemble-basedmethod for automatic thesaurus construction (curran, 2002).<papid> W02-1029 </papid></citsent>
<aftsection>
<nextsent>however, that goal was to acquire fine grained semantic information that is more akin to synonymy (e.g., words similar to house?), whereas we associate words with high-level semantic classes (e.g., house?
</nextsent>
<nextsent>is transient structure).
</nextsent>
<nextsent>semantic class tagging is closely related to named entity recognition (ner) (e.g., (bikel et al, 1997; collins and singer, 1999; <papid> W99-0613 </papid>cucerzan and yarowsky,1999; <papid> W99-0612 </papid>fleischman and hovy, 2002)).<papid> C02-1130 </papid></nextsent>
<nextsent>some bootstrapping methods have been used for ner (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003) <papid> P03-1043 </papid>to learn from unannotated texts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1156">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, that goal was to acquire fine grained semantic information that is more akin to synonymy (e.g., words similar to house?), whereas we associate words with high-level semantic classes (e.g., house?
</prevsent>
<prevsent>is transient structure).
</prevsent>
</prevsection>
<citsent citstr=" W99-0612 ">
semantic class tagging is closely related to named entity recognition (ner) (e.g., (bikel et al, 1997; collins and singer, 1999; <papid> W99-0613 </papid>cucerzan and yarowsky,1999; <papid> W99-0612 </papid>fleischman and hovy, 2002)).<papid> C02-1130 </papid></citsent>
<aftsection>
<nextsent>some bootstrapping methods have been used for ner (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003) <papid> P03-1043 </papid>to learn from unannotated texts.</nextsent>
<nextsent>however, most ner systems will not label nominal noun phrases (e.g., they will not identify the dentist?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1157">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, that goal was to acquire fine grained semantic information that is more akin to synonymy (e.g., words similar to house?), whereas we associate words with high-level semantic classes (e.g., house?
</prevsent>
<prevsent>is transient structure).
</prevsent>
</prevsection>
<citsent citstr=" C02-1130 ">
semantic class tagging is closely related to named entity recognition (ner) (e.g., (bikel et al, 1997; collins and singer, 1999; <papid> W99-0613 </papid>cucerzan and yarowsky,1999; <papid> W99-0612 </papid>fleischman and hovy, 2002)).<papid> C02-1130 </papid></citsent>
<aftsection>
<nextsent>some bootstrapping methods have been used for ner (e.g., (collins and singer, 1999; <papid> W99-0613 </papid>niu et al, 2003) <papid> P03-1043 </papid>to learn from unannotated texts.</nextsent>
<nextsent>however, most ner systems will not label nominal noun phrases (e.g., they will not identify the dentist?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1163">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recently, (huangand riloff, 2010) <papid> P10-1029 </papid>developed bootstrapping technique that induces semantic tagger from unannotated texts.</prevsent>
<prevsent>we use their system in our ensemble.</prevsent>
</prevsection>
<citsent citstr=" P08-1119 ">
there has also been work on extracting semantic class members from the web (e.g., (pasca, 2004; etzioni et al, 2005; kozareva et al, 2008; <papid> P08-1119 </papid>carlson et al., 2009)).<papid> W09-2201 </papid></citsent>
<aftsection>
<nextsent>this line of research is fundamentally different from ours because these techniques benefit from the vast repository of information available on the web and are therefore designed to harvest wide swath of general-purpose semantic information.
</nextsent>
<nextsent>our research is aimed at acquiring domain-specific semantic dictionaries using collection of documents representing specialized domain.
</nextsent>
<nextsent>induction 3.1 motivation.
</nextsent>
<nextsent>our research combines three fundamentally different techniques into an ensemble-based bootstrapping framework for semantic lexicon induction:pattern-based dictionary induction, contextual semantic tagging, and coreference resolution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1164">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recently, (huangand riloff, 2010) <papid> P10-1029 </papid>developed bootstrapping technique that induces semantic tagger from unannotated texts.</prevsent>
<prevsent>we use their system in our ensemble.</prevsent>
</prevsection>
<citsent citstr=" W09-2201 ">
there has also been work on extracting semantic class members from the web (e.g., (pasca, 2004; etzioni et al, 2005; kozareva et al, 2008; <papid> P08-1119 </papid>carlson et al., 2009)).<papid> W09-2201 </papid></citsent>
<aftsection>
<nextsent>this line of research is fundamentally different from ours because these techniques benefit from the vast repository of information available on the web and are therefore designed to harvest wide swath of general-purpose semantic information.
</nextsent>
<nextsent>our research is aimed at acquiring domain-specific semantic dictionaries using collection of documents representing specialized domain.
</nextsent>
<nextsent>induction 3.1 motivation.
</nextsent>
<nextsent>our research combines three fundamentally different techniques into an ensemble-based bootstrapping framework for semantic lexicon induction:pattern-based dictionary induction, contextual semantic tagging, and coreference resolution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1168">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> ensemble-based semantic lexicon.  </section>
<citcontext>
<prevsection>
<prevsent>many noun phrases are singletons (i.e., they are not co referent with any other nps), which limits the set of words that can be learned using coreference chains.
</prevsent>
<prevsent>furthermore, coreference re solvers make mistakes, so the accuracy of the induced lexicons depends on the quality of the chains.
</prevsent>
</prevsection>
<citsent citstr=" P10-2029 ">
for our experiments, we used reconcile (stoyanov et al, 2010), <papid> P10-2029 </papid>freely available supervised coreference resolver.</citsent>
<aftsection>
<nextsent>3.3 ensemble-based bootstrapping.
</nextsent>
<nextsent>framework figure 1 shows the architecture of our ensemble based bootstrapping framework.
</nextsent>
<nextsent>initially, each lexicon only contains the seed nouns.
</nextsent>
<nextsent>each componenthypothesizes set of candidate words for each semantic class, based on its own criteria.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1169">
<title id=" S12-1028.xml">ensemble based semantic lexicon induction for semantic tagging </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>however, the annotators measured their agreement on 10 documents and achieved muc scores of precision = .82, recall = .86, f-measure = .84.
</prevsent>
<prevsent>203the remaining 10 documents and measured inter annotator agreement with cohens kappa (?)
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
(car letta, 1996).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>the ? score for these 10 documents was0.91, indicating high level of agreement.
</nextsent>
<nextsent>the annotators then adjudicated their disagreements on all 23 documents to create the gold standard.
</nextsent>
<nextsent>4.4 dictionary evaluation.
</nextsent>
<nextsent>to assess the quality of the lexicons, we estimated their accuracy by compiling external word lists from freely available sources such as wikipedia14 and wordnet (miller, 1990).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1170">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the semantic annotations are coarse semantic categories or entries from distributional thesaurus, assigned either heuristic ally or by pre-trained tagger.
</prevsent>
<prevsent>we test this using two test corpora in different domains with various sources of training data.the best reduces error rate in dependency score by 1% on average, while some methods produce substantial decreases in performance.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
most start-of-the-art natural language parsers (char niak, 2000; <papid> A00-2018 </papid>clark and curran, 2004; <papid> P04-1014 </papid>collins, 1997) <papid> P97-1003 </papid>use lexicalised features for parse ranking.</citsent>
<aftsection>
<nextsent>these are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the trainingdata.
</nextsent>
<nextsent>in the absence of reliable fine-grained statistics forgiven token, various strategies are possible.
</nextsent>
<nextsent>there will often be statistics available for coarser categories, such as the pos of the particular token.however, it is possible that these coarser representations discard too much, missing out information which could be valuable to the parse ranking.
</nextsent>
<nextsent>an intermediate level of representation could provide valuable additional information here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1171">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the semantic annotations are coarse semantic categories or entries from distributional thesaurus, assigned either heuristic ally or by pre-trained tagger.
</prevsent>
<prevsent>we test this using two test corpora in different domains with various sources of training data.the best reduces error rate in dependency score by 1% on average, while some methods produce substantial decreases in performance.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
most start-of-the-art natural language parsers (char niak, 2000; <papid> A00-2018 </papid>clark and curran, 2004; <papid> P04-1014 </papid>collins, 1997) <papid> P97-1003 </papid>use lexicalised features for parse ranking.</citsent>
<aftsection>
<nextsent>these are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the trainingdata.
</nextsent>
<nextsent>in the absence of reliable fine-grained statistics forgiven token, various strategies are possible.
</nextsent>
<nextsent>there will often be statistics available for coarser categories, such as the pos of the particular token.however, it is possible that these coarser representations discard too much, missing out information which could be valuable to the parse ranking.
</nextsent>
<nextsent>an intermediate level of representation could provide valuable additional information here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1172">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the semantic annotations are coarse semantic categories or entries from distributional thesaurus, assigned either heuristic ally or by pre-trained tagger.
</prevsent>
<prevsent>we test this using two test corpora in different domains with various sources of training data.the best reduces error rate in dependency score by 1% on average, while some methods produce substantial decreases in performance.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
most start-of-the-art natural language parsers (char niak, 2000; <papid> A00-2018 </papid>clark and curran, 2004; <papid> P04-1014 </papid>collins, 1997) <papid> P97-1003 </papid>use lexicalised features for parse ranking.</citsent>
<aftsection>
<nextsent>these are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the trainingdata.
</nextsent>
<nextsent>in the absence of reliable fine-grained statistics forgiven token, various strategies are possible.
</nextsent>
<nextsent>there will often be statistics available for coarser categories, such as the pos of the particular token.however, it is possible that these coarser representations discard too much, missing out information which could be valuable to the parse ranking.
</nextsent>
<nextsent>an intermediate level of representation could provide valuable additional information here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1173">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however if the parse ranker has also been supplied with information about synonyms or hypernyms of the lexemes in the training data, it could possibly have generalised, to learn that pps containing nouns related to seeing instruments often modify verbs relating to observation (in preference to nouns denoting inanimate objects), while plant flora can often be modified by pps relating to appendages of plants such as leaves.
</prevsent>
<prevsent>this is not necessarily applicable only to ppattachment, but may help in range of other syntactic phenomena, such as distinguishing between complements and modifiers of verbs.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
228 the synonyms or hypernyms could take the formof any grouping which relates word forms with semantic or syntactic commonality ? such as label from the wordnet (miller, 1995) hierarchy, subcategorisation frame (for verbs) or closely related terms from distributional thesaurus (lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>we present work here on using various levels of semantic generalisation as an attempt to improve parse selection accuracy with the english resource grammar (erg: flickinger (2000)), precision hpsg-based grammar of english.
</nextsent>
<nextsent>2.1 parse selection for precision grammars.
</nextsent>
<nextsent>the focus of this work is on parsing using handcrafted precision hpsg-based grammars, and in particular the erg.
</nextsent>
<nextsent>while these grammars are carefully crafted to avoid over generation, the ambiguity of natural languages means that there will unavoidably be multiple candidate parses licensed by the grammar for any non-trivial sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1175">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>between the set of all candidate parses.
</prevsent>
<prevsent>a widely-used method to achieve this is outlined in velldal (2007).
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
we feed both correct and incorrect parses licensed by the grammar to the tadmtoolkit (malouf, 2002), <papid> W02-2018 </papid>and learn maximum entropy model.</citsent>
<aftsection>
<nextsent>this method is used by zhang et al (2007) and mackinlay et al (2011) inter alia.
</nextsent>
<nextsent>one important implementation detail is that rather than exhaustively ranking all candidates out of possibly many thousands of trees, zhang et al (2007) showed that it was possible to use selective unpacking?, which means that the exhaustive parse forest can be represented compactly as packed forest?, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory.
</nextsent>
<nextsent>2.2 semantic generalisation for parse ranking.
</nextsent>
<nextsent>above, we outlined number of reasons why semantic generalisation of lexemes could enable parsers to make more efficient use of training data,and indeed, there has been some prior work investigating this possibility.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1176">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 semantic generalisation for parse ranking.
</prevsent>
<prevsent>above, we outlined number of reasons why semantic generalisation of lexemes could enable parsers to make more efficient use of training data,and indeed, there has been some prior work investigating this possibility.
</prevsent>
</prevsection>
<citsent citstr=" P08-1037 ">
agirre et al (2008) <papid> P08-1037 </papid>applied two state-of-the-art treebank parsers to the sense tagged subset of the brown corpus version of the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>and added sense annotation to the training data to evaluate their impact on parse selection and specifically on pp attachment.</citsent>
<aftsection>
<nextsent>the annotations they used were oracle sense annotations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general.
</nextsent>
<nextsent>the sense annotations were either the wordnet synset id or the coarse semantic file, which we explain in more detail below, and replaced the original tokens inthe training data.
</nextsent>
<nextsent>the largest improvement in parsing f-score was 6.9% reduction in error rate for the bikel parser (bikel, 2002), boosting the f-score from 0.841 to 0.852, using the noun super sense only.more recently, agirre et al (2011) <papid> P11-2123 </papid>largely reproduced these results with dependency parser.fujita et al (2007) <papid> W07-1204 </papid>add sense information to im prove parse ranking with jacy (siegel and bender,2002), <papid> W02-1210 </papid>an hpsg-based grammar which uses similar machinery to the erg.</nextsent>
<nextsent>they use baseline syntactic features, and also add semantic features based on dependency triples extracted from the semantic representations of the sentence trees output by the parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1178">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 semantic generalisation for parse ranking.
</prevsent>
<prevsent>above, we outlined number of reasons why semantic generalisation of lexemes could enable parsers to make more efficient use of training data,and indeed, there has been some prior work investigating this possibility.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
agirre et al (2008) <papid> P08-1037 </papid>applied two state-of-the-art treebank parsers to the sense tagged subset of the brown corpus version of the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>and added sense annotation to the training data to evaluate their impact on parse selection and specifically on pp attachment.</citsent>
<aftsection>
<nextsent>the annotations they used were oracle sense annotations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general.
</nextsent>
<nextsent>the sense annotations were either the wordnet synset id or the coarse semantic file, which we explain in more detail below, and replaced the original tokens inthe training data.
</nextsent>
<nextsent>the largest improvement in parsing f-score was 6.9% reduction in error rate for the bikel parser (bikel, 2002), boosting the f-score from 0.841 to 0.852, using the noun super sense only.more recently, agirre et al (2011) <papid> P11-2123 </papid>largely reproduced these results with dependency parser.fujita et al (2007) <papid> W07-1204 </papid>add sense information to im prove parse ranking with jacy (siegel and bender,2002), <papid> W02-1210 </papid>an hpsg-based grammar which uses similar machinery to the erg.</nextsent>
<nextsent>they use baseline syntactic features, and also add semantic features based on dependency triples extracted from the semantic representations of the sentence trees output by the parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1179">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the annotations they used were oracle sense annotations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general.
</prevsent>
<prevsent>the sense annotations were either the wordnet synset id or the coarse semantic file, which we explain in more detail below, and replaced the original tokens inthe training data.
</prevsent>
</prevsection>
<citsent citstr=" P11-2123 ">
the largest improvement in parsing f-score was 6.9% reduction in error rate for the bikel parser (bikel, 2002), boosting the f-score from 0.841 to 0.852, using the noun super sense only.more recently, agirre et al (2011) <papid> P11-2123 </papid>largely reproduced these results with dependency parser.fujita et al (2007) <papid> W07-1204 </papid>add sense information to im prove parse ranking with jacy (siegel and bender,2002), <papid> W02-1210 </papid>an hpsg-based grammar which uses similar machinery to the erg.</citsent>
<aftsection>
<nextsent>they use baseline syntactic features, and also add semantic features based on dependency triples extracted from the semantic representations of the sentence trees output by the parser.
</nextsent>
<nextsent>the dataset they use has human-assigned sense tags from japanese lexical hierarchy, which they use as source of annotations.
</nextsent>
<nextsent>the dependency triples are modified in each feature set by replacing elements of the semantic triples with corresponding senses or hypernyms.
</nextsent>
<nextsent>in the best-performing configuration, they use both syntactic and semantic features with multiple levels of the the semantic hierarchy from combined feature sets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1180">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the annotations they used were oracle sense annotations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general.
</prevsent>
<prevsent>the sense annotations were either the wordnet synset id or the coarse semantic file, which we explain in more detail below, and replaced the original tokens inthe training data.
</prevsent>
</prevsection>
<citsent citstr=" W07-1204 ">
the largest improvement in parsing f-score was 6.9% reduction in error rate for the bikel parser (bikel, 2002), boosting the f-score from 0.841 to 0.852, using the noun super sense only.more recently, agirre et al (2011) <papid> P11-2123 </papid>largely reproduced these results with dependency parser.fujita et al (2007) <papid> W07-1204 </papid>add sense information to im prove parse ranking with jacy (siegel and bender,2002), <papid> W02-1210 </papid>an hpsg-based grammar which uses similar machinery to the erg.</citsent>
<aftsection>
<nextsent>they use baseline syntactic features, and also add semantic features based on dependency triples extracted from the semantic representations of the sentence trees output by the parser.
</nextsent>
<nextsent>the dataset they use has human-assigned sense tags from japanese lexical hierarchy, which they use as source of annotations.
</nextsent>
<nextsent>the dependency triples are modified in each feature set by replacing elements of the semantic triples with corresponding senses or hypernyms.
</nextsent>
<nextsent>in the best-performing configuration, they use both syntactic and semantic features with multiple levels of the the semantic hierarchy from combined feature sets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1182">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the annotations they used were oracle sense annotations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general.
</prevsent>
<prevsent>the sense annotations were either the wordnet synset id or the coarse semantic file, which we explain in more detail below, and replaced the original tokens inthe training data.
</prevsent>
</prevsection>
<citsent citstr=" W02-1210 ">
the largest improvement in parsing f-score was 6.9% reduction in error rate for the bikel parser (bikel, 2002), boosting the f-score from 0.841 to 0.852, using the noun super sense only.more recently, agirre et al (2011) <papid> P11-2123 </papid>largely reproduced these results with dependency parser.fujita et al (2007) <papid> W07-1204 </papid>add sense information to im prove parse ranking with jacy (siegel and bender,2002), <papid> W02-1210 </papid>an hpsg-based grammar which uses similar machinery to the erg.</citsent>
<aftsection>
<nextsent>they use baseline syntactic features, and also add semantic features based on dependency triples extracted from the semantic representations of the sentence trees output by the parser.
</nextsent>
<nextsent>the dataset they use has human-assigned sense tags from japanese lexical hierarchy, which they use as source of annotations.
</nextsent>
<nextsent>the dependency triples are modified in each feature set by replacing elements of the semantic triples with corresponding senses or hypernyms.
</nextsent>
<nextsent>in the best-performing configuration, they use both syntactic and semantic features with multiple levels of the the semantic hierarchy from combined feature sets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1187">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>we apply two relatively simple strategies.
</prevsent>
<prevsent>we noted in section 2.2 that agirre et al (2008) <papid> P08-1037 </papid>found thatthe semantic file was useful.</prevsent>
</prevsection>
<citsent citstr=" W06-1670 ">
this is the coarse lexicographic category label, elsewhere denoted super sense (ciaramita and altun, 2006), <papid> W06-1670 </papid>which is the terminology we use.</citsent>
<aftsection>
<nextsent>nouns are divided into 26 coarse categories such as animal?, quantity?
</nextsent>
<nextsent>or phenomenon?, and verbs into 15 categories such asperception?
</nextsent>
<nextsent>or consumption?.
</nextsent>
<nextsent>in some configurations, denoted ss, we tag each open-class token with one of the super sense labels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1188">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>3.4.2 disambiguating senses we return now to the question of determination of the synset forgiven token.
</prevsent>
<prevsent>one frequently used and robust strategy is to lemmatise and pos tag each token, and assign it the first-listed sense from wordnet (which may or may not be based on actual frequency counts).
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
we pos-tag using tnt(brants, 2000) <papid> A00-1031 </papid>and lemmatise using wordnets native lemmatiser.</citsent>
<aftsection>
<nextsent>this yields leaf-level synset, making it suitable as source of annotations for both ss and hp.
</nextsent>
<nextsent>we denote this wnf?
</nextsent>
<nextsent>for wordnet first?
</nextsent>
<nextsent>(shown in parentheses after ss or hp).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1192">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>we ignore multi-token named entity outputs from super sense tagger, as these would introduce confounding factor in our experiments and also reduce comparability of the results with the wnf method.
</prevsent>
<prevsent>3.4.3 distributional thesaurus method final configuration attempts to avoid the needfor curated resources such as wordnet, instead using an automatically-constructed distributional thesaurus (lin, 1998).<papid> P98-2127 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
we use the thesaurus from mccarthy et al (2004), <papid> P04-1036 </papid>constructed along these lines using the grammatical relations from rasp (briscoe and carroll, 2002) applied to 90 millions words of text from the british national corpus.</citsent>
<aftsection>
<nextsent>231 root_frag np_frg_c hdn_bnp_c aj-hdn_norm_c legal_a1  legal  n_pl_olr issue_n1  issues  figure 1: erg derivation tree for the phrase legal issues [n_-_c_le  issues ] [n_pl_olr n_-_c_le  issues ] [aj-hdn_norm_c n_pl_olr n_-_c_le  issues ] (a) original features [n_-_c_le noun.cognition] [n_pl_olr n_-_c_le noun.cognition] [aj-hdn_norm_c n_pl_olr n_-_c_le noun.cognition] (b) additional features in leaf mode, which augment the original features [noun.cognition  issues ] [n_pl_olr noun.cognition  issues ] [aj-hdn_norm_c n_pl_olr noun.cognition  issues ] (c) additional features in leaf-parent (p?) mode, which augment the original features figure 2: examples of features extracted from for  issues  node in figure 1 with grand parenting level of 2 or less to apply the mapping, we pos-tag the text withtnt as usual, and for each noun, verb and adjective we lemmatise the token (with wordnet again, falling back to the surface form if this fails), and look up the corresponding entry in the thesaurus.
</nextsent>
<nextsent>ifthere is match, we select the top five most similar entries (or fewer if there are less than five), and use these new entries to create additional features, as well as adding feature for the lemma itself in allcases.
</nextsent>
<nextsent>this method is denoted ldt for lin distributional thesaurus?.
</nextsent>
<nextsent>we note that many other methods could be used to select these, such as different numbers of synonyms, or dynamically changing the number of synonyms based on threshold against the top similarity score, but this is not something we evaluate in this preliminary investigation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1193">
<title id=" S12-1031.xml">the effects of semantic annotations on precision parse ranking </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>it is somewhat surprising that we did not achieve reliable performance gains which were seen in the related work described above.
</prevsent>
<prevsent>one possible explanation is that the model training parameters weresuboptimal for this dataset since the characteristics of the data are somewhat different than without sense annotations.
</prevsent>
</prevsection>
<citsent citstr=" W96-0209 ">
the failure to improve somewhat mirrors the results of clark (2001), who was attempting to improve the parse ranking performance of the unification-based based probabilistic parser of carroll and briscoe (1996).<papid> W96-0209 </papid></citsent>
<aftsection>
<nextsent>clark (2001) used dependencies to rank parses, and wordnet-based techniques to generalise this model and learn selectional preferences, but failed to improve performance over the structural (i.e. non-dependency) ranking in the original parser.
</nextsent>
<nextsent>additionally, perhaps the changes we applied in this work to the parse ranking could possibly have been more effective with features based on semantic dependences as used by fujita et al (2007), <papid> W07-1204 </papid>although we outlined reasons why we wished to avoid this approach.</nextsent>
<nextsent>this work is preliminary and there is room for more exploration in this space.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1195">
<title id=" P98-2191.xml">maximum entropy model learning of the translation rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this topic, baum (1972) pro-posed em algorithm, which was basis of forward-backward algorithm for the hidden markov model (hmm) and inside-outside algorithm (lafferty, 1993) for the pr0babilis- tic context free grammar (pcfg).
</prevsent>
<prevsent>however, these methods have problems such as in-creasing optimization costs which is due to lot of parameters.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
therefore, estimating natural language model based on the max-imum entropy (me) method (pietra et al, 1995; berger et al, 1996) <papid> J96-1002 </papid>has been high-lighted recently.</citsent>
<aftsection>
<nextsent>on the other hand, dictionaries for multi-lingual natural anguage processing such as the machine translation has been made by human hand usually.
</nextsent>
<nextsent>however, since this work requires great deal of labor and it is difficult to keep description of dictionar-ies consistent, he researches of automatical dictionaries making for machine translation (translation rules) from corpora become ac-tive recently (kay and rsschesen, 1993; kaji and aizono, 1996).<papid> C96-1006 </papid></nextsent>
<nextsent>in this paper, we notice that estimating language model based on me method is suitable for learning the translation rules, and propose several methods to resolve prob-lems in adapting me method to learning the translation rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1196">
<title id=" P98-2191.xml">maximum entropy model learning of the translation rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, estimating natural language model based on the max-imum entropy (me) method (pietra et al, 1995; berger et al, 1996) <papid> J96-1002 </papid>has been high-lighted recently.</prevsent>
<prevsent>on the other hand, dictionaries for multi-lingual natural anguage processing such as the machine translation has been made by human hand usually.</prevsent>
</prevsection>
<citsent citstr=" C96-1006 ">
however, since this work requires great deal of labor and it is difficult to keep description of dictionar-ies consistent, he researches of automatical dictionaries making for machine translation (translation rules) from corpora become ac-tive recently (kay and rsschesen, 1993; kaji and aizono, 1996).<papid> C96-1006 </papid></citsent>
<aftsection>
<nextsent>in this paper, we notice that estimating language model based on me method is suitable for learning the translation rules, and propose several methods to resolve prob-lems in adapting me method to learning the translation rules.
</nextsent>
<nextsent>if there exist (xl, yl ,..., {xt, yt) 6 ? such that each xi is translated into yi in the parallel corpora x,y, then its empiri-cal probability distribution/5 obtained from observed training data is defined by: p(x,y) - c(x,y) (1) ex, c(x,y) where c(x, y) is the number of times that is translated into in the training data.
</nextsent>
<nextsent>however, since it is difficult to observe translating between words actually, c(x, y) is approximated with equation (2) for sentence aligned parallel corpora.
</nextsent>
<nextsent> (x,y) c(x, y) = (2) where x~ is i-th sentence in x. we denote that sentence xi is translated into sentence y/ in aligned parallel corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1201">
<title id=" P99-1042.xml">deep read a reading comprehension system </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>la (sentence): by giving it 6,457 of his books, thomas jefferson helped get it started.
</prevsent>
<prevsent>lb (bag): {6,457 books by get giving helped his it jefferson of started thomas} extraction of information content from text, both in documents and questions, then consists of tokenizing words and determining sentence boundary punctuation.
</prevsent>
</prevsection>
<citsent citstr=" J97-2002 ">
for english written text, both of these tasks are relatively easy although not trivial--see palmer and hearst (1997).<papid> J97-2002 </papid></citsent>
<aftsection>
<nextsent>the search subtask consists of finding the best match between the word set representing the question and the sets representing sentences in the document.
</nextsent>
<nextsent>our system measures the match by size of the intersection of the two word sets.
</nextsent>
<nextsent>for example, the question in (2a) would receive an intersection score of 1 because of the mutual set element books.
</nextsent>
<nextsent>2a (question): who gave books to the new library?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1202">
<title id=" P99-1042.xml">deep read a reading comprehension system </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>for example, due to the name thomas jefferson, the word set in (lb) would be extended by :person, as would the word set (2b) because it is who question.
</prevsent>
<prevsent>this would increase the matching score by one.
</prevsent>
</prevsection>
<citsent citstr=" C96-1047 ">
the system makes use of the alembic automated named entity system (vilain and day 1996) <papid> C96-1047 </papid>for finding named entities.</citsent>
<aftsection>
<nextsent>in similar vein, we also created simple common noun classification module using wordnet (miller 1990).
</nextsent>
<nextsent>it works by looking up all nouns of the text and adding person or location classes if any of noun senses is subsumed by the appropriate wordnet class.
</nextsent>
<nextsent>we also created filtering module that ranks sentences higher if they contain the appropriate class identifier, even though they may have fewer matching words, e.g., if the bag representation a sentence does not contain :person, it is ranked lower as an answer to who question than sentences which do contain :person.
</nextsent>
<nextsent>finally, the system contains an extension which substitutes the referent of personal pronouns for the pronoun in the bag representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1203">
<title id=" P99-1042.xml">deep read a reading comprehension system </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>name identification provided consistent gains.
</prevsent>
<prevsent>the alembic name tagger was developed for newswire text and used here with no modifications.
</prevsent>
</prevsection>
<citsent citstr=" M93-1007 ">
we created hand-tagged named entity data, which allowed us to measure the performance of alembic: the accuracy (f- measure) was 76.5; see chinchor and sundheim (1993) <papid> M93-1007 </papid>for description of the standard muc scoring metric.</citsent>
<aftsection>
<nextsent>this also allowed us to simulate perfect tagging, and we were able to determine how much we might gain by improving the name tagging by tuning it to this domain.
</nextsent>
<nextsent>as the results indicate, there would be little gain from improved name tagging.
</nextsent>
<nextsent>however, some modules that seemed to have little effect with automatic name tagging provided small gains with perfect name tagging, specifically wordnet common noun semantics and automatic pronoun resolution.
</nextsent>
<nextsent>329 when used in combination with the filtering module, these also seemed to help.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1204">
<title id=" P99-1042.xml">deep read a reading comprehension system </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, the hand-tagged reference resolution data allowed us to evaluate automatic coreference resolution.
</prevsent>
<prevsent>the latter was combination of name coreference, as determined by alembic, and heuristic resolution of personal pronouns to the most recent prior named person.
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
using the muc coreference scoring algorithm (see vilain et al 1995), <papid> M95-1005 </papid>this had precision of 77% and recall of 18%.</citsent>
<aftsection>
<nextsent>3 the use of full, hand- tagged reference resolution caused substantial increase of the answdrecall metric.
</nextsent>
<nextsent>this was because the system substitutes the antecedent for all referring expressions, improving the word- based measure.
</nextsent>
<nextsent>this did not, however, provide an increase in the sentence-based measures.
</nextsent>
<nextsent>finally, we plan to do similar human labeling experiments for semantic lass identification, to determine the potential effect of this knowledge source.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1205">
<title id=" P99-1030.xml">analysis system of speech acts and discourse structures using maximum entropy model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>request ask-ref ask-ref response ask-tel response acknowledge inform r~mmse igure 1 : an example f d ia logue i th speech acts in this paper, we propose dialogue analysis model to determine both the speech acts of utterances and the discourse structure of dialogue using maximum entropy model.
</prevsent>
<prevsent>in the proposed model, the speech act analysis and the discourse structure analysis are combined in one framework so that they can easily provide feedback to each other.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
for the discourse structure analysis, we suggest statistical model with discourse segment boundaries (dsbs) similar to the idea of gaps suggested for statistical parsing (collins (1996)).<papid> P96-1025 </papid></citsent>
<aftsection>
<nextsent>for training, we use corpus tagged with various discourse knowledge.
</nextsent>
<nextsent>to overcome the problem of data sparseness, which is common for corpus-based works, we use split partial context as well as whole context.
</nextsent>
<nextsent>after explaining the tagged dialogue corpus we used in section 1, we discuss the statistical models in detail in section 2.
</nextsent>
<nextsent>in section 3, we explain experimental results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1206">
<title id=" P99-1030.xml">analysis system of speech acts and discourse structures using maximum entropy model </title>
<section> statistical models.  </section>
<citcontext>
<prevsection>
<prevsent>generally, dialogues have hierarchical discourse structure.
</prevsent>
<prevsent>so we approximate the context as speech acts of utterances that are hierarchically recent to the utterance.
</prevsent>
</prevsection>
<citsent citstr=" J96-2005 ">
an utterance is hierarchically recent to an utterance if is adjacent b in the tree structure of the discourse (walker (1996)).<papid> J96-2005 </papid></citsent>
<aftsection>
<nextsent>equation (3) represents the approximated contextual probability in the case of using trigram where uj and u~ are hierarchically recent to the utterance u, where  k -1 . 232 p(si s\],, - ,) = p(si sj, sk) (3) as result, the statistical model for speech act analysis is represented in equation (4).
</nextsent>
<nextsent>p(s, u,, 0 = p(si s,,, - ,)p(ui s,) = p(si is j, sk)p(pi \[st) (4) 2.2 discourse structure analysis model.
</nextsent>
<nextsent>2.2.1 discourse segment boundary tagging we define set of discourse segment boundaries (dsbs) as the markers for discourse structure tagging.
</nextsent>
<nextsent>a dsb represents the relationship between two consecutive utterances in dialogue.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1207">
<title id=" S12-1041.xml">uio1 constituent based discriminative ranking for negation resolution </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>this paper describes the first of two systems submitted from the university of oslo (uio) to the 2012 *sem shared task on resolving negation.
</prevsent>
</prevsection>
<citsent citstr=" J12-2005 ">
our submission is an adaption of the negation system of velldal et al (2012), <papid> J12-2005 </papid>which combines svm cue classification with svm-based ranking of syntactic constituents for scope resolution.</citsent>
<aftsection>
<nextsent>the approach further extends our prior work in that we also identify factual negated events.
</nextsent>
<nextsent>while submitted forthe closed track, the system was the topper former in the shared task overall.
</nextsent>
<nextsent>the first joint conference on lexical and computational semantics (*sem 2012) hosts shared task on resolving negation (morante and blanco, 2012).<papid> S12-1035 </papid>this involves the subtasks of (i) identifying negation cues, (ii) identifying the in-sentence scope of these cues, and (iii) identifying negated (and factual) events.</nextsent>
<nextsent>this paper describes system submitted by the language technology group at the university of oslo (uio).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1208">
<title id=" S12-1041.xml">uio1 constituent based discriminative ranking for negation resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the approach further extends our prior work in that we also identify factual negated events.
</prevsent>
<prevsent>while submitted forthe closed track, the system was the topper former in the shared task overall.
</prevsent>
</prevsection>
<citsent citstr=" S12-1035 ">
the first joint conference on lexical and computational semantics (*sem 2012) hosts shared task on resolving negation (morante and blanco, 2012).<papid> S12-1035 </papid>this involves the subtasks of (i) identifying negation cues, (ii) identifying the in-sentence scope of these cues, and (iii) identifying negated (and factual) events.</citsent>
<aftsection>
<nextsent>this paper describes system submitted by the language technology group at the university of oslo (uio).
</nextsent>
<nextsent>our starting point is the negation system developed by velldal et al (2012) <papid> J12-2005 </papid>for the domain of biomedical texts, an svm-based system for classifying cues and ranking syntactic constituents to resolve cue scopes.</nextsent>
<nextsent>however, we extend and adapt this system in several important respects, such as in terms of the underlying linguistic formalisms thatare used, the textual domain, handling of morphological cues and discontinuous scopes, and in that the current system also identifies negated events.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1214">
<title id=" S12-1041.xml">uio1 constituent based discriminative ranking for negation resolution </title>
<section> constituent-based scope resolution.  </section>
<citcontext>
<prevsection>
<prevsent>during the development of our scope resolution system we have pursued both rule-based and data driven approach.
</prevsent>
<prevsent>both are rooted in the assumption that the scope of neg ations corresponds to syntactically meaningful unit.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
our starting point here willbe the syntactic analyses provided by the task organizers (see figure 1), generated using the reranking parser of charniak and johnson (2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>how ever, as alignment between scope annotations and syntactic units is not straightforward for all cases, we apply several exception rules that slacken?
</nextsent>
<nextsent>the requirements for alignment, as discussed in section 3.1.
</nextsent>
<nextsent>in sections 3.2 and 3.3 we detail our rule-based and data-driven approaches, respectively.note that the predictions of the rule-based component will be incorporated as features in the learned model, similarly to the set-up described by read et al.
</nextsent>
<nextsent>(2011).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1215">
<title id=" S10-1055.xml">humb automatic key term extraction from scientific articles in grobid </title>
<section> normalize each candidate by lower casing.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 structural features.
</prevsent>
<prevsent>one of the goals of grobid is to realize reliable conversions of technical and scientific documents in pdf to fully compliant tei 3 documents.
</prevsent>
</prevsection>
<citsent citstr=" N04-1042 ">
this conversion implies first the recognition of the different sections of the document, then the extraction of all header meta data and references.the analysis is realized in grobid with conditional random fields (crf) (peng and mccallum, 2004) <papid> N04-1042 </papid>exploiting large amount of trainingdata.</citsent>
<aftsection>
<nextsent>we added to this training set few acm documents manually annotated and obtained very high performance for field recognitions, between97% (section titles, reference titles) and 99% (ti tle, abstract) accuracy for the tasks collection.
</nextsent>
<nextsent>authors commonly introduce the main concepts of written communication in the header (title, abstract, table of contents), the introduction, the 1 morpho-syntactic annotation framework, see http://pauillac.inria.fr/ clerger/maf/ 2 http://wing.comp.nus.edu.sg/downloads/keyphrasecorpus 3 text encoding initiative (tei), http://www.tei-c.org.
</nextsent>
<nextsent>section titles, the conclusion and the reference list.similarly human readers/annotators typically focus their attention on the same document parts.we introduced thus the following 6 binary features characterizing the position of term with respect to the document structure for each candidate:present in the title, in the abstract, in the introduction, in at least one section titles, in the conclusion, in at least one reference or book title.in addition, we used the following standard feature: the position of the first occurrence, calculated as the number of words which precede the first occurrence of the term divided by the number of words in the document, similarly as, for instance, (witten et al, 1999).
</nextsent>
<nextsent>3.2 content features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1216">
<title id=" S10-1055.xml">humb automatic key term extraction from scientific articles in grobid </title>
<section> normalize each candidate by lower casing.  </section>
<citcontext>
<prevsection>
<prevsent>phrase ness the phrase ness measures the lexical cohesion of sequence of words in given document, i.e. the degree to which it can be considered as phrase.
</prevsent>
<prevsent>this measure is classically usedfor term extraction and can relyon different techniques, usually evaluating the ability of sequence of words to appear as stable phrase more often than just by chance.
</prevsent>
</prevsection>
<citsent citstr=" C02-1142 ">
we applied here the generalized dice coeficient (gdc) as introduced by(park et al, 2002), <papid> C02-1142 </papid>applicable to any arbitrary gram of words (n ? 2).</citsent>
<aftsection>
<nextsent>forgiven term , | |being the number of words in , freq(t ) the frequency of occurrence of and freq(w i) the frequency of occurrence of the word i , we have: gdc(t ) = | | log 10 (freq(t ))freq(t ) ? i freq(w ) we used default value for single word, because,in this case, the association measure is not meaningful as it depends only on the frequency.
</nextsent>
<nextsent>informative ness the informative ness of term is the degree to which the term is representative of document given collection of documents.
</nextsent>
<nextsent>once again many measures can be relevant, and we opt for the standard tf-idf value which is used in most of the key phrase extraction systems, see for instance (witten et al, 1999) or (medelyan and 249 witten, 2008).
</nextsent>
<nextsent>the tf-idf score for term in document is given by: tf-idf(t,d) = freq(t,d) | | log 2 count(t ) where | | is the number of words in d, count(t ) is the number of occurrence of the termt in the global corpus, and is the number of documents in the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1217">
<title id=" S10-1039.xml">seerlab a system for extracting key phrases from scholarly documents </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>to remedy such limitations, for each document we also include its top 30 most frequent unigrams, its top 30 non-unigram ngrams and the acronyms found in the document as candidates.
</prevsent>
<prevsent>our method of extracting candidate key phrases differs from most previous work.
</prevsent>
</prevsection>
<citsent citstr=" W09-2902 ">
previous work (kim and kan, 2009; <papid> W09-2902 </papid>nguyen and kan, 2007) useshand-crafted regular expressions for candidate ex tractions.</citsent>
<aftsection>
<nextsent>many of these rules also require pos (part of speech) inputs.
</nextsent>
<nextsent>in contrast, our method is corpus-driven and requires no additional input from the pos tagger.
</nextsent>
<nextsent>additionally, our approach 1 http://www.informatik.uni-trier.de/ ley/db/index.html allows us to effectively include phrases that appear only once in the document as candidates, as long as they appear more than twice in the dblp data.
</nextsent>
<nextsent>2.3 ranking keyphrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1218">
<title id=" P99-1034.xml">a unification based approach to morphosyntactic parsing of agglutinative and other highly inflectional languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the method is similar to the one of goldberg &amp; k=ilm=in (1992) used in the bug system: the description is theoretically infinite, hut there is finite performance limit when running.
</prevsent>
<prevsent>3 the idea has something in common with the pc-kimmo.
</prevsent>
</prevsection>
<citsent citstr=" C92-3145 ">
based analyzer of the university of pennsylvania (karp et al 1992).<papid> C92-3145 </papid></citsent>
<aftsection>
<nextsent>our compression ratio is around 20%.
</nextsent>
<nextsent>261 guages are handled as  gestalts  by native speakers, instead of parsing them on-line.
</nextsent>
<nextsent>4 this idea is not new in the literature: according to bybee,  psycho linguistic argument for treating (some) ending sequences as wholes comes from the observation that children acquiring inflec-tional languages eldom make errors involving the order of morphemes in word.
</nextsent>
<nextsent>(bybee 1985) another source is karlsson:  the endings and entries are often listed as wholes, especially in close-knit combinations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1219">
<title id=" P99-1034.xml">a unification based approach to morphosyntactic parsing of agglutinative and other highly inflectional languages </title>
<section> comparison with other methods.  </section>
<citcontext>
<prevsection>
<prevsent>( could use computer to make fun for while ): input: elsz~tmit6g~pezgethettem internal segmentation: el\[prefix\]+sz~mit6\[stem 1 \]+g~p\[stem2\]+ +ezgethet\[deriv.aff.\]+tem\[infl.aff\] output: ei\[vpref\]+s~it6\[adj\]+g~p\[n\]+ez\[n2v\]+ +get\[freq\]+het\[opt\]+tem\[past-sg- 1 \]
</prevsent>
<prevsent>there are only few general, reversible mor-phological systems that are suitable for more than single language.
</prevsent>
</prevsection>
<citsent citstr=" A88-1031 ">
in addition to the well-known two-level morphology (koskenniemi 1983) and its modifications (karttunen 1993) it is worth mentioning the nabu system (slocum 1988).<papid> A88-1031 </papid></citsent>
<aftsection>
<nextsent>there are some morphological description sys-tems showing some features in common with humor 99 - like paradigmatic morphology (cal- der 1989), <papid> E89-1008 </papid>or the paradigm description language (anick &amp; artemieff 1992) - <papid> C92-1014 </papid>but they don have 12 the meta-dictionary shown in the example compiles.</nextsent>
<nextsent>with humor lexicon compiler without any changes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1220">
<title id=" P99-1034.xml">a unification based approach to morphosyntactic parsing of agglutinative and other highly inflectional languages </title>
<section> comparison with other methods.  </section>
<citcontext>
<prevsection>
<prevsent>there are only few general, reversible mor-phological systems that are suitable for more than single language.
</prevsent>
<prevsent>in addition to the well-known two-level morphology (koskenniemi 1983) and its modifications (karttunen 1993) it is worth mentioning the nabu system (slocum 1988).<papid> A88-1031 </papid></prevsent>
</prevsection>
<citsent citstr=" E89-1008 ">
there are some morphological description sys-tems showing some features in common with humor 99 - like paradigmatic morphology (cal- der 1989), <papid> E89-1008 </papid>or the paradigm description language (anick &amp; artemieff 1992) - <papid> C92-1014 </papid>but they don have 12 the meta-dictionary shown in the example compiles.</citsent>
<aftsection>
<nextsent>with humor lexicon compiler without any changes.
</nextsent>
<nextsent>large-scale implementations.
</nextsent>
<nextsent>two-level mor-phology is reversible, orthography-based sys-tem that has several advantages from linguist point f view.
</nextsent>
<nextsent>namely, the morpho-phone- mic/graphemic rules can be formalized in gen-eral and very elegant way.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1221">
<title id=" P99-1034.xml">a unification based approach to morphosyntactic parsing of agglutinative and other highly inflectional languages </title>
<section> comparison with other methods.  </section>
<citcontext>
<prevsection>
<prevsent>there are only few general, reversible mor-phological systems that are suitable for more than single language.
</prevsent>
<prevsent>in addition to the well-known two-level morphology (koskenniemi 1983) and its modifications (karttunen 1993) it is worth mentioning the nabu system (slocum 1988).<papid> A88-1031 </papid></prevsent>
</prevsection>
<citsent citstr=" C92-1014 ">
there are some morphological description sys-tems showing some features in common with humor 99 - like paradigmatic morphology (cal- der 1989), <papid> E89-1008 </papid>or the paradigm description language (anick &amp; artemieff 1992) - <papid> C92-1014 </papid>but they don have 12 the meta-dictionary shown in the example compiles.</citsent>
<aftsection>
<nextsent>with humor lexicon compiler without any changes.
</nextsent>
<nextsent>large-scale implementations.
</nextsent>
<nextsent>two-level mor-phology is reversible, orthography-based sys-tem that has several advantages from linguist point f view.
</nextsent>
<nextsent>namely, the morpho-phone- mic/graphemic rules can be formalized in gen-eral and very elegant way.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1222">
<title id=" S12-1038.xml">ucm2 a rule based approach to infer the scope of negation via dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an initial version of the system was developed to handle the annotations of the bio scope corpus.
</prevsent>
<prevsent>for the present version, we have changed, omitted or extended the rules and the lexicon of cues (allowing prefix and suffix negation cues, suchas impossible or meaningless), to make it suit able for the present task.
</prevsent>
</prevsection>
<citsent citstr=" S12-1035 ">
one of the challenges of the *sem shared task (morante and blanco, 2012) <papid> S12-1035 </papid>is to infer and classify the scope and event associated to neg ations, givena training and development corpus based on conan doyle stories (morante and daelemans, 2012).negation, simple in concept, is complex but essential phenomenon in any language.</citsent>
<aftsection>
<nextsent>it turns an affirmative statement into negative one, changing the meaning completely.
</nextsent>
<nextsent>we believe therefore that being able to handle and classify neg ations we would be able to improve several text mining applications.
</nextsent>
<nextsent>previous to this shared task, we can find several systems that handle the scope of negation in the stateof the art.
</nextsent>
<nextsent>this is complex problem, because it requires, first, to find and capture the negation cues, and second, based on either syntactic or semantic representations, to identify the words that are directly (or indirectly) affected by these negation cues.one of the main works that started this trend in natural language processing was published by morante steam (2008), published by morante steam (2009), in which they presented machine learning approach for the biomedical domain evaluating it on the bio scope corpus.in 2010, workshop on negation and speculation in natural language processing (moranteand sporleder, 2010) was held in uppsala, sweden.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1224">
<title id=" S12-1038.xml">ucm2 a rule based approach to infer the scope of negation via dependency parsing </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>however, taking into account this fact, and the results obtained, weare tempted to say that our system presents competitive results.
</prevsent>
<prevsent>292 we believe that the present system has lot ofroom for improvement: (i) improve the management of sentences with more than one scope modifying the scope classification algorithm and the postprocessing step, (ii) replacing the dependency parser with state-of-the-art parser in order to get higher performance, or (iii) proposing different way of getting reliable lexicon of cues, by using semantic approach that informs if the word has negative meaning in the context of the sentence.
</prevsent>
</prevsection>
<citsent citstr=" W08-2121 ">
again, this could be achieved by using one of the parsers presented in the conll 2008 shared task (surdeanu et al., 2008).<papid> W08-2121 </papid></citsent>
<aftsection>
<nextsent>acknowledgments this research is funded by the spanish ministry of education and science (tin2009-14659-c03-01 project).
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1225">
<title id=" S10-1043.xml">unitn partofspeech counting in relation extraction </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>unitn is part-of-speech context counter.
</prevsent>
<prevsent>given as input plain text with part-of-speech and end-of-sentence markers annotated it outputsa numerical feature vector that gives representation of sentence.
</prevsent>
</prevsection>
<citsent citstr=" L08-1408 ">
for part-of-speech and end-of sentence annotation used textpro, tool for nlp that showed state-of-the-art performance for pos tagging (see pianta et al, 2008).<papid> L08-1408 </papid></citsent>
<aftsection>
<nextsent>the pos tagset is the one used in the bnc, described at http: //pie.usna.edu/poscodes.html.
</nextsent>
<nextsent>features in the vector can be tailored for specific tasks, in this case 20 features were used in total.
</nextsent>
<nextsent>they are: 1.
</nextsent>
<nextsent>number of prepositions in sentence..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1226">
<title id=" S12-1036.xml">uab coral a preliminary study for resolving the scope of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, finding the negation and its scope is important in tasks where the negation and assertion information need to be treated differently.
</prevsent>
<prevsent>however, most of the systems developed for processing natural language data do not consider neg ations present in the sentences.
</prevsent>
</prevsection>
<citsent citstr=" D08-1075 ">
although various works (morante et al, 2008; <papid> D08-1075 </papid>morante and daelemans, 2009; <papid> W09-1105 </papid>li et al, 2010; councill et al, 2010;<papid> W10-3110 </papid>apostolova et al, 2011) <papid> P11-2049 </papid>have dealt with the identification of neg ations and their scope in sentences, this is still challenging task.</citsent>
<aftsection>
<nextsent>the first task in *sem 2012 shared task (morante and blanco, 2012) <papid> S12-1035 </papid>is concerned with finding the scope of negation.</nextsent>
<nextsent>the task includes identifying: i) negation cues, ii) scope of negation, and iii) negated event for each negation present in the sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1227">
<title id=" S12-1036.xml">uab coral a preliminary study for resolving the scope of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, finding the negation and its scope is important in tasks where the negation and assertion information need to be treated differently.
</prevsent>
<prevsent>however, most of the systems developed for processing natural language data do not consider neg ations present in the sentences.
</prevsent>
</prevsection>
<citsent citstr=" W09-1105 ">
although various works (morante et al, 2008; <papid> D08-1075 </papid>morante and daelemans, 2009; <papid> W09-1105 </papid>li et al, 2010; councill et al, 2010;<papid> W10-3110 </papid>apostolova et al, 2011) <papid> P11-2049 </papid>have dealt with the identification of neg ations and their scope in sentences, this is still challenging task.</citsent>
<aftsection>
<nextsent>the first task in *sem 2012 shared task (morante and blanco, 2012) <papid> S12-1035 </papid>is concerned with finding the scope of negation.</nextsent>
<nextsent>the task includes identifying: i) negation cues, ii) scope of negation, and iii) negated event for each negation present in the sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1228">
<title id=" S12-1036.xml">uab coral a preliminary study for resolving the scope of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, finding the negation and its scope is important in tasks where the negation and assertion information need to be treated differently.
</prevsent>
<prevsent>however, most of the systems developed for processing natural language data do not consider neg ations present in the sentences.
</prevsent>
</prevsection>
<citsent citstr=" W10-3110 ">
although various works (morante et al, 2008; <papid> D08-1075 </papid>morante and daelemans, 2009; <papid> W09-1105 </papid>li et al, 2010; councill et al, 2010;<papid> W10-3110 </papid>apostolova et al, 2011) <papid> P11-2049 </papid>have dealt with the identification of neg ations and their scope in sentences, this is still challenging task.</citsent>
<aftsection>
<nextsent>the first task in *sem 2012 shared task (morante and blanco, 2012) <papid> S12-1035 </papid>is concerned with finding the scope of negation.</nextsent>
<nextsent>the task includes identifying: i) negation cues, ii) scope of negation, and iii) negated event for each negation present in the sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1229">
<title id=" S12-1036.xml">uab coral a preliminary study for resolving the scope of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, finding the negation and its scope is important in tasks where the negation and assertion information need to be treated differently.
</prevsent>
<prevsent>however, most of the systems developed for processing natural language data do not consider neg ations present in the sentences.
</prevsent>
</prevsection>
<citsent citstr=" P11-2049 ">
although various works (morante et al, 2008; <papid> D08-1075 </papid>morante and daelemans, 2009; <papid> W09-1105 </papid>li et al, 2010; councill et al, 2010;<papid> W10-3110 </papid>apostolova et al, 2011) <papid> P11-2049 </papid>have dealt with the identification of neg ations and their scope in sentences, this is still challenging task.</citsent>
<aftsection>
<nextsent>the first task in *sem 2012 shared task (morante and blanco, 2012) <papid> S12-1035 </papid>is concerned with finding the scope of negation.</nextsent>
<nextsent>the task includes identifying: i) negation cues, ii) scope of negation, and iii) negated event for each negation present in the sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1230">
<title id=" S12-1036.xml">uab coral a preliminary study for resolving the scope of negation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, most of the systems developed for processing natural language data do not consider neg ations present in the sentences.
</prevsent>
<prevsent>although various works (morante et al, 2008; <papid> D08-1075 </papid>morante and daelemans, 2009; <papid> W09-1105 </papid>li et al, 2010; councill et al, 2010;<papid> W10-3110 </papid>apostolova et al, 2011) <papid> P11-2049 </papid>have dealt with the identification of neg ations and their scope in sentences, this is still challenging task.</prevsent>
</prevsection>
<citsent citstr=" S12-1035 ">
the first task in *sem 2012 shared task (morante and blanco, 2012) <papid> S12-1035 </papid>is concerned with finding the scope of negation.</citsent>
<aftsection>
<nextsent>the task includes identifying: i) negation cues, ii) scope of negation, and iii) negated event for each negation present in the sentences.
</nextsent>
<nextsent>negation cue is word, part of word, or combination of words that carries the negation information.
</nextsent>
<nextsent>scope of negation in sentence is the longest group of words in the sentence that is influenced by the negation cue.
</nextsent>
<nextsent>negated event is the shortest group of words that is actually affected by the negation cue.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1233">
<title id=" S10-1093.xml">kyoto an integrated system for specific domain wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>task (agirre et al,2010).
</prevsent>
<prevsent>the goal of our participation was to evaluate the preliminary release of the integrated system for specific domain wsd developed for the kyoto project 1 . besides, we wanted to test the.
</prevsent>
</prevsection>
<citsent citstr=" W09-2420 ">
performance of our domain specific wsd system(agirre et al, 2009) <papid> W09-2420 </papid>on this test set, and to integrate the thesaurus construction software (tybots) developed for the project.</citsent>
<aftsection>
<nextsent>the system can be run for any language and domain if provided with alexical knowledge base and some background documents on the domain.we will first present the components of our system, followed by the experimental design and the 1 http://www.kyoto-project.eu results.
</nextsent>
<nextsent>finally, the conclusions are presented.
</nextsent>
<nextsent>wsd we will present in turn ukb, the tybots, and the lexical knowledge-bases used.
</nextsent>
<nextsent>2.1 ukb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1234">
<title id=" S10-1093.xml">kyoto an integrated system for specific domain wsd </title>
<section> the kyoto system for domain specific.  </section>
<citcontext>
<prevsection>
<prevsent>wsd we will present in turn ukb, the tybots, and the lexical knowledge-bases used.
</prevsent>
<prevsent>2.1 ukb.
</prevsent>
</prevsection>
<citsent citstr=" E09-1005 ">
ukb is knowledge-based unsupervised wsd system which exploits the structure of an under lying language knowledge base (lkb) and finds the most relevant concepts given an input context (agirre and soroa, 2009).<papid> E09-1005 </papid></citsent>
<aftsection>
<nextsent>ukb starts by taking the lkb as graph of concepts = (v,e)with set of vertices derived from lkb concepts and set of edges representing relations among them.
</nextsent>
<nextsent>giving an input context, ukb applies the so called personalized page rank (haveliwala, 2002) over it to obtain the most representative senses for the context.
</nextsent>
<nextsent>page rank (brin and page, 1998) is method for scoring the vertices of graph according to each nodes structural importance.
</nextsent>
<nextsent>the algorithm can be viewed as random walk process that postulate the existence of particle that randomly traverses the graph, but at any time may jump to new vertex with given damping factor (alsocalled teleport probability).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1239">
<title id=" S10-1093.xml">kyoto an integrated system for specific domain wsd </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>on second step each target word is disambiguated using the most related words as context (see below).
</prevsent>
<prevsent>for instance, in order to disambiguate the word environment, we would not take into account the context of occurrence (as in section 3.2), but we would use the list of most related words in the thesaurus (e.g. biodiversity, agriculture, ecosystem, nature, life, climate, . . .?).
</prevsent>
</prevsection>
<citsent citstr=" J07-4005 ">
using ukb over these contexts we obtain the most predominant sense for each target word in the domain(mccarthy et al, 2007), <papid> J07-4005 </papid>which is used to label all occurrences of the target word in the test dataset.</citsent>
<aftsection>
<nextsent>in order to build the thesaurus with the lists of related words, we used tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. chinese, dutch, english, and italian.
</nextsent>
<nextsent>we used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents inkaf.
</nextsent>
<nextsent>we used the tybots with the following settings.
</nextsent>
<nextsent>we discarded co-occurring words with frequencies below 10 5 . distributional similarity was.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1240">
<title id=" S10-1093.xml">kyoto an integrated system for specific domain wsd </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>we used the tybots with the following settings.
</prevsent>
<prevsent>we discarded co-occurring words with frequencies below 10 5 . distributional similarity was.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
computed using (lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>finally, we used up to 50 related words for each target word.
</nextsent>
<nextsent>as in run1, we used the monolingual graphs for the lkbs in each language.
</nextsent>
<nextsent>3.4 run3: ukb using related words and.
</nextsent>
<nextsent>bilingual graphs the third run is exactly the same as run2, except that we used bilingual graphs instead of monolingual ones for all languages other than english (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1242">
<title id=" S10-1077.xml">jucsetemp a first step towards evaluating events time expressions and temporal relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is also important in wide range of nlp applications that include temporal question answering, machine translation and document summarization.
</prevsent>
<prevsent>in the literature, temporal relation identification based on machine learning approaches can be found in boguraev et el.
</prevsent>
</prevsection>
<citsent citstr=" P06-1095 ">
(2005), mani et al (2006), <papid> P06-1095 </papid>chambers et al (2007) <papid> P07-2044 </papid>and some of the tempe val 2007 participants (verhagen et al, 2007).<papid> W07-2014 </papid></citsent>
<aftsection>
<nextsent>most of these works tried to improve classification accuracies through feature engineering.
</nextsent>
<nextsent>the performance of any machine learning based system is often limited by the amount of available training data.
</nextsent>
<nextsent>mani et al (2006) <papid> P06-1095 </papid>introduced temporal reasoning component that greatly expands the available training data.</nextsent>
<nextsent>the training set was increased by factor of 10 by computing the closure of the various temporal relations that exist in the training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1243">
<title id=" S10-1077.xml">jucsetemp a first step towards evaluating events time expressions and temporal relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is also important in wide range of nlp applications that include temporal question answering, machine translation and document summarization.
</prevsent>
<prevsent>in the literature, temporal relation identification based on machine learning approaches can be found in boguraev et el.
</prevsent>
</prevsection>
<citsent citstr=" P07-2044 ">
(2005), mani et al (2006), <papid> P06-1095 </papid>chambers et al (2007) <papid> P07-2044 </papid>and some of the tempe val 2007 participants (verhagen et al, 2007).<papid> W07-2014 </papid></citsent>
<aftsection>
<nextsent>most of these works tried to improve classification accuracies through feature engineering.
</nextsent>
<nextsent>the performance of any machine learning based system is often limited by the amount of available training data.
</nextsent>
<nextsent>mani et al (2006) <papid> P06-1095 </papid>introduced temporal reasoning component that greatly expands the available training data.</nextsent>
<nextsent>the training set was increased by factor of 10 by computing the closure of the various temporal relations that exist in the training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1244">
<title id=" S10-1077.xml">jucsetemp a first step towards evaluating events time expressions and temporal relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is also important in wide range of nlp applications that include temporal question answering, machine translation and document summarization.
</prevsent>
<prevsent>in the literature, temporal relation identification based on machine learning approaches can be found in boguraev et el.
</prevsent>
</prevsection>
<citsent citstr=" W07-2014 ">
(2005), mani et al (2006), <papid> P06-1095 </papid>chambers et al (2007) <papid> P07-2044 </papid>and some of the tempe val 2007 participants (verhagen et al, 2007).<papid> W07-2014 </papid></citsent>
<aftsection>
<nextsent>most of these works tried to improve classification accuracies through feature engineering.
</nextsent>
<nextsent>the performance of any machine learning based system is often limited by the amount of available training data.
</nextsent>
<nextsent>mani et al (2006) <papid> P06-1095 </papid>introduced temporal reasoning component that greatly expands the available training data.</nextsent>
<nextsent>the training set was increased by factor of 10 by computing the closure of the various temporal relations that exist in the training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1249">
<title id=" S10-1077.xml">jucsetemp a first step towards evaluating events time expressions and temporal relations </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>has value of 0 for most cases and is only set to 1, when 1,t ts s?
</prevsent>
<prevsent>are certain states and the observation has certain properties.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
here, we set parameters ? to maximize the penalized log-likelihood using limited-memory bfgs (sha and pereira, 2003) <papid> N03-1028 </papid>quasi-newton method that is significantly more efficient, and which results in only minor changes inaccuracy due to changes in ? .</citsent>
<aftsection>
<nextsent>we use the opennlp c++ based crf++ package 3 , simple, customizable, and open source implementation of crf for segmenting /labeling sequential data.
</nextsent>
<nextsent>3.3 features of tasks c, d, and f. we extract the gold-standard time bank features for events and times in order to train/test the crf.
</nextsent>
<nextsent>in the present work, we mainly use the various combinations of the following features: (i).
</nextsent>
<nextsent>part of speech (pos) of event terms: it denotes the pos information of the event.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1250">
<title id=" P99-1014.xml">inducing a semantically annotated lexicon via embased clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an important challenge in computational lin-guistics concerns the construction of large-scale computational lexicons for the numerous natu-ral languages where very large samples of lan-guage use are now available.
</prevsent>
<prevsent>resnik (1993) ini-tiated research into the automatic acquisition of semantic selectional restrictions.
</prevsent>
</prevsection>
<citsent citstr=" C94-2123 ">
ribas (1994) <papid> C94-2123 </papid>presented an approach which takes into account the syntactic position of the elements whose se-mantic relation is to be acquired.</citsent>
<aftsection>
<nextsent>however, those and most of the following approaches require as prerequisite fixed taxonomy of semantic rela-tions.
</nextsent>
<nextsent>this is problem because (i) entailment hierarchies are presently available for few lan-guages, and (ii) we regard it as an open ques-tion whether and to what degree xisting designs for lexical hierarchies are appropriate for repre-senting lexical meaning.
</nextsent>
<nextsent>both of these consid-erations uggest he relevance of inductive and experimental pproaches to the construction of lexicons with semantic information.
</nextsent>
<nextsent>this paper presents method for automatic induction of semantically annotated subcatego-rization frames from unannotated corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1251">
<title id=" P99-1014.xml">inducing a semantically annotated lexicon via embased clustering </title>
<section> em-based luster ing.  </section>
<citcontext>
<prevsection>
<prevsent>the basic ideas of our em-based clus-tering approach were presented in rooth (ms).
</prevsent>
<prevsent>our approach constr asts with the merely heuris-tic and empirical justification of similarity-based approaches to clustering (dagan et al, to ap- pear) for which so far no clear probabilistic interpretation has been given.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
the probability model we use can be found earlier in pereira et al (1993).<papid> P93-1024 </papid></citsent>
<aftsection>
<nextsent>however, in contrast this ap- 104 class 17 prob 0.0265 0.0437 0 .0302 0 .0344 0 .0337 0 .0329 0 .0257 0 .0196 0 .0177 0 .0169 0 .0156 0 .0134 10 .0129 0 .0120 0 .0102 0 .0099 0 .0099 0 .0088 0 .0088 0 .0080 0 .0078 increase.as:s nc rease .aso :o fa l .as :s pay .aso :o reduce.aso:o i se .as :s exceed.aso:o exceed.aso:s af fec .aso :o grow.as :s inc lude.aso:s reach .aso :s decl ine.as:s lose.aso:o act .aso :s improve .aso :o include .aso :o cut .aso :o show.aso :o vary .as :s o~~ ~ .~.~ ~ ~ . ~  : : : : : : : : : : : : : : : : : : : : :   :   : : : : . ? ?
</nextsent>
<nextsent>s ? ?
</nextsent>
<nextsent>s ? ? ?
</nextsent>
<nextsent>s ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1252">
<title id=" P99-1014.xml">inducing a semantically annotated lexicon via embased clustering </title>
<section> em-based luster ing.  </section>
<citcontext>
<prevsection>
<prevsent>s ? ?
</prevsent>
<prevsent>figure 1: class proach, our statistical inference method for clus-tering is formalized clearly as an em-algorithm.
</prevsent>
</prevsection>
<citsent citstr=" W97-0309 ">
approaches to probabilistic lustering similar to ours were presented recently in saul and pereira (1997) <papid> W97-0309 </papid>and hofmann and puzicha (1998).</citsent>
<aftsection>
<nextsent>there also em-algorithms for similar probability mod-els have been derived, but applied only to sim-pler tasks not involving combination of em- based clustering models as in our lexicon induc-tion experiment.
</nextsent>
<nextsent>for further applications of our clustering model see rooth et al (1998).
</nextsent>
<nextsent>we seek to derive joint distribution of verb- noun pairs from large sample of pairs of verbs e and nouns e n. the key idea is to view and as conditioned on hidden class e c, where the classes are given no prior interpreta-tion.
</nextsent>
<nextsent>the semantically smoothed probability of pair (v, n) is defined to be: p(v,n) = ~~p(c ,v ,n )= ~- \]p(c)p(vjc)p(njc) cec cec the joint distribution p(c,v,n) is defined by p(c, v, n) = p(c)p(vlc)p(n\[c ).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1254">
<title id=" P99-1014.xml">inducing a semantically annotated lexicon via embased clustering </title>
<section> em-based luster ing.  </section>
<citcontext>
<prevsection>
<prevsent>: :1 .11 :1 :  1 ? ?
</prevsent>
<prevsent>figure 5: class 8: dispositions 0.977992 0.948099 0.923698 0.908378 0.877338 0.876083 0.803479 0.672409 0.583314 decrease double increase decline rise soar fall slow diminish 0.560727 0.476524 0.42842 0.365586 0.365374 0.292716 0.280183 0.238182 drop grow vary improve climb flow cut mount 0.741467 ansteigen 0.720221 steigen 0.693922 absinken 0.656021 sinken 0.438486 schrumpfen 0.375039 zuriickgehen 0.316081 anwachsen 0.215156 stagnieren 0.160317 wachsen 0.154633 hinzukommen (go up) (rise) (sink) (go down) (shrink) (decrease) (increase) (stagnate) (grow) (be added) figure 8: scalar motion verbs corpus of german subordinate clauses, yielding 418290 tokens (318086 types) of pairs of verbs or adjectives and nouns.
</prevsent>
</prevsection>
<citsent citstr=" P99-1035 ">
the lexicalized proba-bilistic grammar for german used is described in beil et al (1999).<papid> P99-1035 </papid></citsent>
<aftsection>
<nextsent>we compared the ger-man example of scalar motion verbs to the lin-guistic classification of verbs given by schuh- macher (1986) and found an agreement of our classification with the class of  einfache an- derungsverben  (simple verbs of change) except for the verbs anwachsen (increase) and stag- nieren(stagnate) which were not classified there at all.
</nextsent>
<nextsent>fig.
</nextsent>
<nextsent>i0 shows the most probable pair of classes for increase as transitive verb, together with estimated frequencies for the head filler pair.
</nextsent>
<nextsent>note that the object label 17 is the class found with in transitive scalar motion verbs; this cor-respondence is exploited in the next section.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1255">
<title id=" S12-1074.xml">njuparser achievements on semantic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>task 5 of semeval-2012 tries to find approaches to improve chinese sematic dependency parsing (sdp).
</prevsent>
<prevsent>sdp is kind of dependency parsing.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
currently, there are many dependency parsers available, such as eisners probabilistic dependency parser (eisner, 1996), <papid> C96-1058 </papid>mcdonalds mst parser (mcdonald et al 2005<papid> P05-1012 </papid>a; mcdonald et al 2005<papid> P05-1012 </papid>b) and nivres malt parser (nivre, 2006).</citsent>
<aftsection>
<nextsent>despite of elaborate models, lots of problems still exist in dependency parsing.
</nextsent>
<nextsent>for example, sentence length has been proved to show great impact on the parsing performance.
</nextsent>
<nextsent>(li et al, 2010) used two-stage approach based on sentence fragment for high-order graph-based dependency parsing.
</nextsent>
<nextsent>lacking of linguistic knowledge is also blamed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1256">
<title id=" S12-1074.xml">njuparser achievements on semantic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>task 5 of semeval-2012 tries to find approaches to improve chinese sematic dependency parsing (sdp).
</prevsent>
<prevsent>sdp is kind of dependency parsing.
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
currently, there are many dependency parsers available, such as eisners probabilistic dependency parser (eisner, 1996), <papid> C96-1058 </papid>mcdonalds mst parser (mcdonald et al 2005<papid> P05-1012 </papid>a; mcdonald et al 2005<papid> P05-1012 </papid>b) and nivres malt parser (nivre, 2006).</citsent>
<aftsection>
<nextsent>despite of elaborate models, lots of problems still exist in dependency parsing.
</nextsent>
<nextsent>for example, sentence length has been proved to show great impact on the parsing performance.
</nextsent>
<nextsent>(li et al, 2010) used two-stage approach based on sentence fragment for high-order graph-based dependency parsing.
</nextsent>
<nextsent>lacking of linguistic knowledge is also blamed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1265">
<title id=" S12-1067.xml">sb mmsystem  using decompositional semantics for lexical simplification </title>
<section> the mmsystem.  </section>
<citcontext>
<prevsection>
<prevsent>step 1: pos-tagging in the first step, context andthe associated substitutes are parsed1 so to obtain flat representation of their syntax.
</prevsent>
<prevsent>basically at this level, we collect part-of-speech information for all content words in the context as well as in the substitute list.step 2: relevance rules in the second step, depending on the syntactic representation of the substitutes, the system selects relevance rule that identifies the one-word lexical form that will be used for representing the meaning of the whole substitute.step 3: word sense tagging the system applies word sense tagging and assigns wordnet sense to the target words and their candidate substitutes.
</prevsent>
</prevsection>
<citsent citstr=" P05-3019 ">
in this step, we relyon the senserelate::targetword package (patwardhan et al, 2005) <papid> P05-3019 </papid>and use the lesk algorithm (lesk, 1986) for word sense disambigua tion.</citsent>
<aftsection>
<nextsent>step 4: substitute ranking following (carroll etal., 1999) that pointed out that rare words generally have only one sense, in order to associate frequency index to each candidate substitute (wi), we use the number of senses associated by wordnet to lexical item of given part of speech, as an approximation of its frequency(fi).
</nextsent>
<nextsent>further, we extract from wordnet the frequency of the word sense (fwnsi) associated to the lexical item wi at step 3.
</nextsent>
<nextsent>words not found in wordnet it assigned null frequency (fi = 0, fwnsi = 0).
</nextsent>
<nextsent>finally, we rank the substitute in the following way: ? if f1 6= f2 w1   w2, if f1   f2 and w2   w1 otherwise, ? else if f1 = f2 w1   w2, if fwns1   fwns2 and w2   w1 otherwise.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1266">
<title id=" S12-1067.xml">sb mmsystem  using decompositional semantics for lexical simplification </title>
<section> the mmsystem.  </section>
<citcontext>
<prevsection>
<prevsent>table 1 shows an example of data processing.
</prevsent>
<prevsent>3.2 relevance rules.
</prevsent>
</prevsection>
<citsent citstr=" H05-1113 ">
relying on previous work on compositional semantics of multi-word-expression (reddy et al, 2011; venkatapathy and joshi, 2005; <papid> H05-1113 </papid>baldwin et al, 2003) <papid> W03-1812 </papid>we defined set of hand-written rules to assign the relevant meaning to complex substitute.</citsent>
<aftsection>
<nextsent>relevance rules are used to decompose the meaning of complex structure and identify the most relevant word conveying the semantics of the whole, so that the frequency associated to the whole lexical form is approximated by the frequency of this most relevant form: ? one-word lexical item is mapped to itself, e.g. run.v ? run.v ? multi-word lexical form including only one content word is mapped to this content word, e.g. not.neg nice.a? nice.a or be.cop able.a? able.a? in the case of multi-word lexical item including more than one content word, we take into account the syntactic structure of the lexical item and apply heuristics to decide which content word is more relevant for the meaning of the whole.
</nextsent>
<nextsent>the heuristics we used are based on the empirical analysis of the trial dataset provided by the task 1 organizers that contains 1we used the stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>484 about 300 contexts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1267">
<title id=" S12-1067.xml">sb mmsystem  using decompositional semantics for lexical simplification </title>
<section> the mmsystem.  </section>
<citcontext>
<prevsection>
<prevsent>table 1 shows an example of data processing.
</prevsent>
<prevsent>3.2 relevance rules.
</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
relying on previous work on compositional semantics of multi-word-expression (reddy et al, 2011; venkatapathy and joshi, 2005; <papid> H05-1113 </papid>baldwin et al, 2003) <papid> W03-1812 </papid>we defined set of hand-written rules to assign the relevant meaning to complex substitute.</citsent>
<aftsection>
<nextsent>relevance rules are used to decompose the meaning of complex structure and identify the most relevant word conveying the semantics of the whole, so that the frequency associated to the whole lexical form is approximated by the frequency of this most relevant form: ? one-word lexical item is mapped to itself, e.g. run.v ? run.v ? multi-word lexical form including only one content word is mapped to this content word, e.g. not.neg nice.a? nice.a or be.cop able.a? able.a? in the case of multi-word lexical item including more than one content word, we take into account the syntactic structure of the lexical item and apply heuristics to decide which content word is more relevant for the meaning of the whole.
</nextsent>
<nextsent>the heuristics we used are based on the empirical analysis of the trial dataset provided by the task 1 organizers that contains 1we used the stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>484 about 300 contexts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1268">
<title id=" S12-1067.xml">sb mmsystem  using decompositional semantics for lexical simplification </title>
<section> the mmsystem.  </section>
<citcontext>
<prevsection>
<prevsent>relying on previous work on compositional semantics of multi-word-expression (reddy et al, 2011; venkatapathy and joshi, 2005; <papid> H05-1113 </papid>baldwin et al, 2003) <papid> W03-1812 </papid>we defined set of hand-written rules to assign the relevant meaning to complex substitute.</prevsent>
<prevsent>relevance rules are used to decompose the meaning of complex structure and identify the most relevant word conveying the semantics of the whole, so that the frequency associated to the whole lexical form is approximated by the frequency of this most relevant form: ? one-word lexical item is mapped to itself, e.g. run.v ? run.v ? multi-word lexical form including only one content word is mapped to this content word, e.g. not.neg nice.a? nice.a or be.cop able.a? able.a? in the case of multi-word lexical item including more than one content word, we take into account the syntactic structure of the lexical item and apply heuristics to decide which content word is more relevant for the meaning of the whole.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
the heuristics we used are based on the empirical analysis of the trial dataset provided by the task 1 organizers that contains 1we used the stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>484 about 300 contexts.
</nextsent>
<nextsent>as an example consider lexical item including verb construction with structure v1 + to + v2 that is mapped by our rules to the second verb form v2, e.g. try.v1 to escape.v2 ? escape.v2.table 2 shows some examples of relevance rules defined in the mmsystem.
</nextsent>
<nextsent>syntax example form + prep engage for cop + adj be able adj cop + be worried adv + anxiously anticipate adv adj+n adnormal growth adj n1 + n2 death penalty n1 n1 + prepof + n2 person of authority n2 v+n take notice v1+to+v2 try to escape v2 table 2: example of relevance rules.these relevance rules allow for preliminary investigation of the nature of lexical complexity.
</nextsent>
<nextsent>for instance, we found that in many cases, it is the modifying element of complex expression that is responsible for shift in lexical complexity: (4) a. lie say falsely say un truthfully b. sample  typical sample   representative sample
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1269">
<title id=" S12-1015.xml">lexical semantic typologies from bilingual corpora  a framework </title>
<section> possible data sources.  </section>
<citcontext>
<prevsection>
<prevsent>sejane and eger (2012) conduct preliminary studyof their approach on the open-source bilingual dictionaries dicts.info (http://www.dicts.info/uddl.php).
</prevsent>
<prevsent>the disadvantage with using bilingual dictionaries is of course that they are scarcely available (and much less freely available); moreover, for the above described semantic association networks, it may be of crucial importance to have comparable data sources; e.g. using general-purpose dictionary in one caseand technical dictionary in the other, or using dictionaries of vastly different sizes may severely affect the quality of results.1we more generally propose to use bilingual corpora for the problem of inducing semantic association networks, where we particularly have e.g. sentence-aligned corpora like the europarl corpus (koehn, 2005) in mind (see also the study of rama and borin (2011) on cognates, with europarl as the data basis).
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
then, translation relations ti may be induced from these corpora by applying statistical machine translation approach such as the moses toolkit (koehn et al, 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>the translation relations may thus be probabilistic instead of binary, which may either be resolved via thresholding or by modifying equation (1) as in di(u, v) = ? xw [li] pr[utix] + pr[xtiv] 2 or di(u, v) = ? xw [li] pr[utix] ? pr[xtiv], both of which have (1) as special cases.
</nextsent>
<nextsent>1as another aspect, sejane and eger (2012) concluded that the sizes and partly the qualities of their bilingual dictionaries were, throughout, not fully adequate for their intentions.
</nextsent>
<nextsent>91figure 1: bilingual dictionaries german-english and german-latin and induced lexical semantic association networks, english and latin versions of german.
</nextsent>
<nextsent>note the similarities and differences; mann man? and mensch human have link in both versions but there is path between mann and frau woman?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1270">
<title id=" P99-1053.xml">a syntactic framework for speech repairs and other disruptions </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>this is especially true of machine translators and meeting analysis programs that deal with human-human dialog.
</prevsent>
<prevsent>speech recognizers have started to adapt to spoken dialog (ver- sus read speech).
</prevsent>
</prevsection>
<citsent citstr=" P97-1033 ">
recent language mod-els (heeman and allen, 1997), (<papid> P97-1033 </papid>stolcke and shriberg, 1996), (siu and ostendorf, 1996) take into account the fact that word co-occurrences may be disrupted by editing terms 1 and speech repairs (take the tanker mean the boxcar).</citsent>
<aftsection>
<nextsent>these language models detect repairs as they process the input; however, like past work on speech repair detection, they do not 1here, we define diting terms as set of 30-40 words that signal hesitations (urn) and speech re-pairs (i mean) and give meta-comments on the ut-terance (right).
</nextsent>
<nextsent>specify how speech repairs hould be handled by the parser.
</nextsent>
<nextsent>(hindle, 1983) <papid> P83-1019 </papid>and (bear et al., 1992) <papid> P92-1008 </papid>performed speech repair identifi-cation in their parsers, and removed the cor-rected material (reparandum) from consider- ation.</nextsent>
<nextsent>(hindle, 1983) <papid> P83-1019 </papid>states that repairs are available for semantic analysis but provides no details on the representation to be used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1271">
<title id=" P99-1053.xml">a syntactic framework for speech repairs and other disruptions </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>these language models detect repairs as they process the input; however, like past work on speech repair detection, they do not 1here, we define diting terms as set of 30-40 words that signal hesitations (urn) and speech re-pairs (i mean) and give meta-comments on the ut-terance (right).
</prevsent>
<prevsent>specify how speech repairs hould be handled by the parser.
</prevsent>
</prevsection>
<citsent citstr=" P83-1019 ">
(hindle, 1983) <papid> P83-1019 </papid>and (bear et al., 1992) <papid> P92-1008 </papid>performed speech repair identifi-cation in their parsers, and removed the cor-rected material (reparandum) from consider- ation.</citsent>
<aftsection>
<nextsent>(hindle, 1983) <papid> P83-1019 </papid>states that repairs are available for semantic analysis but provides no details on the representation to be used.</nextsent>
<nextsent>clearly repairs should be available for se-mantic analysis as they play role in di-alog structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1273">
<title id=" P99-1053.xml">a syntactic framework for speech repairs and other disruptions </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>these language models detect repairs as they process the input; however, like past work on speech repair detection, they do not 1here, we define diting terms as set of 30-40 words that signal hesitations (urn) and speech re-pairs (i mean) and give meta-comments on the ut-terance (right).
</prevsent>
<prevsent>specify how speech repairs hould be handled by the parser.
</prevsent>
</prevsection>
<citsent citstr=" P92-1008 ">
(hindle, 1983) <papid> P83-1019 </papid>and (bear et al., 1992) <papid> P92-1008 </papid>performed speech repair identifi-cation in their parsers, and removed the cor-rected material (reparandum) from consider- ation.</citsent>
<aftsection>
<nextsent>(hindle, 1983) <papid> P83-1019 </papid>states that repairs are available for semantic analysis but provides no details on the representation to be used.</nextsent>
<nextsent>clearly repairs should be available for se-mantic analysis as they play role in di-alog structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1279">
<title id=" S12-1019.xml">selecting corpus semantic models for neuro linguistic decoding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using models of semantics to learn the mapping between concepts and the neural activity which they elicit during neuro imaging experiments.
</prevsent>
<prevsent>this was achieved with linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fmri activity that should be elicited by an unseen stimulus concept.
</prevsent>
</prevsection>
<citsent citstr=" D09-1065 ">
follow-on work has experimented with other neuro imaging modalities (murphy et al ,2009), <papid> D09-1065 </papid>and with range of semantic models including elicited property norms (chang et al , 2011), corpus derived models (devereux and kelly, 2010; pereira et al , 2011) and structured ontologies (jelodar et al , 2010).</citsent>
<aftsection>
<nextsent>the current state-of-the-art performance onthis task is achieved using models that are hand tailored in some respect, whether using manual annotation tasks (palatucci et al , 2009), use of domain-appropriate curated corpus (pereira et al , 2011), or selection of particular collocates to suit the concepts to be described (mitchell et al , 2008).
</nextsent>
<nextsent>while these approaches are clearly very successful, it is questionable whether theyare general solution to describe the various parts-of-speech and semantic domains that make up speakers vocabulary.
</nextsent>
<nextsent>the mitchell et al  (2008) 25-verb model would probably have to be extended to describe the lexicon at large, and it is unclear whether such compact model could be maintained.
</nextsent>
<nextsent>while wikipedia (pereiraet al , 2011) has very broad and increasing cov 114erage, it is possible that it will remain inadequate for specialist vocabularies, or for less studied languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1280">
<title id=" S12-1019.xml">selecting corpus semantic models for neuro linguistic decoding </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>ppmi up-weights cooccurrences between rare words, yielding positive values for collocations that are more common than would be expected by chance (i.e. ifword distributions were independent), and discards negative values that represent patterns ofco-occurrences that are rarer than one would expect by chance.
</prevsent>
<prevsent>it has been shown to perform well generally, with both word- and document level statistics, in raw and dimensionality reduced forms (bullinaria and levy, 2007; turney and pantel, 2010).2 ppmiwf = { pmiwf if pmiwf   0 0 otherwise (1) pmiwf = log ( p(w, f) p(w)p(f) ) (2) frequency threshold is commonly applied for three reasons: low-frequency co-occurrencecounts are more noisy; pmi is positively biased towards hapax co-occurrences; and due to zipfian distributions cut-off dramatically reduces the amount of data to be processed.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
many authors use threshold of approximately 50-100 occurrences for word-collocate models (lund and burgess, 1996; lin, 1998; <papid> P98-2127 </papid>rapp, 2003).</citsent>
<aftsection>
<nextsent>since bullinaria and levy (2007) find improving performance with models using progressively lower cutoffs we explored two cut-offs of 20 and 50 which equate to low co-occurrencesthresholds of 0.00125 or 0.003125 per million re spectively; for the word-region model we chose threshold of 2 occurrences of target term ina document, to keep the input features to reasonable dimensionality (bradford, 2008).
</nextsent>
<nextsent>after applying these operations to the input data from each model, the resulting dimension2preliminary analyses confirmed that ppmi performed as well or better than alternatives including log likelihood, tf-idf, and log-entropy.
</nextsent>
<nextsent>ality ranged widely, from about 500 thousand,to tens of millions.
</nextsent>
<nextsent>a singular value decomposition (svd) was applied to identify the 1000dimensions within each model with the greatest explanatory power, which also has the effect of combining similar dimensions (such as synonyms, inflectional variants, topically similar documents) into common components, and discarding more noisy dimensions in the data.again there is variation in the number of dimension that authors use: here we experiment with 300 and 1000.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1282">
<title id=" S12-1019.xml">selecting corpus semantic models for neuro linguistic decoding </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>we did not use stop-list, as bullinaria and levy (2007) found co-occurrencewith very high frequency words also to be informative for semantic tasks.
</prevsent>
<prevsent>we also expect that the subsequent steps of normalizing with ppmi,reduction with svd, and use of regular ised regression should be able to recognize when such high-frequency words are not informative and then discount these, without the need for such assumptions upfront.
</prevsent>
</prevsection>
<citsent citstr=" W02-0109 ">
the stemmed model is slight variation on the word-form model, where the same statistics are aggregated after applying lancaster stemming (paice, 1990; loper and bird, 2002).<papid> W02-0109 </papid></citsent>
<aftsection>
<nextsent>the directional model, inspired by schutze and pedersen (1993), is also derived from theword-form model, but differentiates between cooccurrence to the left or to the right of the target word, with features such as {john l, cake r}.
</nextsent>
<nextsent>the part-of-speech model (kanejiya et al ,2003; <papid> W03-0208 </papid>widdows, 2003) <papid> N03-1036 </papid>replaces each lowercase word-token with its part-of-speech disam biguated form (e.g. likes vbz, cake nn ).</nextsent>
<nextsent>these annotations were extracted from the full dependency parse described below.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1283">
<title id=" S12-1019.xml">selecting corpus semantic models for neuro linguistic decoding </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>the stemmed model is slight variation on the word-form model, where the same statistics are aggregated after applying lancaster stemming (paice, 1990; loper and bird, 2002).<papid> W02-0109 </papid></prevsent>
<prevsent>the directional model, inspired by schutze and pedersen (1993), is also derived from theword-form model, but differentiates between cooccurrence to the left or to the right of the target word, with features such as {john l, cake r}.</prevsent>
</prevsection>
<citsent citstr=" W03-0208 ">
the part-of-speech model (kanejiya et al ,2003; <papid> W03-0208 </papid>widdows, 2003) <papid> N03-1036 </papid>replaces each lowercase word-token with its part-of-speech disam biguated form (e.g. likes vbz, cake nn ).</citsent>
<aftsection>
<nextsent>these annotations were extracted from the full dependency parse described below.
</nextsent>
<nextsent>the sequence model draws on range of work that uses word sequence patterns (lin andpantel, 2001; almuhareb and poesio, 2004; <papid> W04-3221 </papid>ba roni et al , 2010), and may also be considered an approximation of models that use shallow syntactic analysis (grefenstette, 1994; curran and moens, 2002).<papid> W02-0908 </papid></nextsent>
<nextsent>all distinct token sequences up to length 4 either side of the target word were counted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1284">
<title id=" S12-1019.xml">selecting corpus semantic models for neuro linguistic decoding </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>the stemmed model is slight variation on the word-form model, where the same statistics are aggregated after applying lancaster stemming (paice, 1990; loper and bird, 2002).<papid> W02-0109 </papid></prevsent>
<prevsent>the directional model, inspired by schutze and pedersen (1993), is also derived from theword-form model, but differentiates between cooccurrence to the left or to the right of the target word, with features such as {john l, cake r}.</prevsent>
</prevsection>
<citsent citstr=" N03-1036 ">
the part-of-speech model (kanejiya et al ,2003; <papid> W03-0208 </papid>widdows, 2003) <papid> N03-1036 </papid>replaces each lowercase word-token with its part-of-speech disam biguated form (e.g. likes vbz, cake nn ).</citsent>
<aftsection>
<nextsent>these annotations were extracted from the full dependency parse described below.
</nextsent>
<nextsent>the sequence model draws on range of work that uses word sequence patterns (lin andpantel, 2001; almuhareb and poesio, 2004; <papid> W04-3221 </papid>ba roni et al , 2010), and may also be considered an approximation of models that use shallow syntactic analysis (grefenstette, 1994; curran and moens, 2002).<papid> W02-0908 </papid></nextsent>
<nextsent>all distinct token sequences up to length 4 either side of the target word were counted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1285">
<title id=" S12-1019.xml">selecting corpus semantic models for neuro linguistic decoding </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>the part-of-speech model (kanejiya et al ,2003; <papid> W03-0208 </papid>widdows, 2003) <papid> N03-1036 </papid>replaces each lowercase word-token with its part-of-speech disam biguated form (e.g. likes vbz, cake nn ).</prevsent>
<prevsent>these annotations were extracted from the full dependency parse described below.</prevsent>
</prevsection>
<citsent citstr=" W04-3221 ">
the sequence model draws on range of work that uses word sequence patterns (lin andpantel, 2001; almuhareb and poesio, 2004; <papid> W04-3221 </papid>ba roni et al , 2010), and may also be considered an approximation of models that use shallow syntactic analysis (grefenstette, 1994; curran and moens, 2002).<papid> W02-0908 </papid></citsent>
<aftsection>
<nextsent>all distinct token sequences up to length 4 either side of the target word were counted.
</nextsent>
<nextsent>finally the dependency model uses full dependency parse, which might be considered the most informed representation of the word col locate relationships instantiated in corpus sentences, and this approach has been used by several authors (lin, 1998; <papid> P98-2127 </papid>pado?</nextsent>
<nextsent>and lapata, 2007; baroni and lenci, 2010).<papid> J10-4006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1286">
<title id=" S12-1019.xml">selecting corpus semantic models for neuro linguistic decoding </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>the part-of-speech model (kanejiya et al ,2003; <papid> W03-0208 </papid>widdows, 2003) <papid> N03-1036 </papid>replaces each lowercase word-token with its part-of-speech disam biguated form (e.g. likes vbz, cake nn ).</prevsent>
<prevsent>these annotations were extracted from the full dependency parse described below.</prevsent>
</prevsection>
<citsent citstr=" W02-0908 ">
the sequence model draws on range of work that uses word sequence patterns (lin andpantel, 2001; almuhareb and poesio, 2004; <papid> W04-3221 </papid>ba roni et al , 2010), and may also be considered an approximation of models that use shallow syntactic analysis (grefenstette, 1994; curran and moens, 2002).<papid> W02-0908 </papid></citsent>
<aftsection>
<nextsent>all distinct token sequences up to length 4 either side of the target word were counted.
</nextsent>
<nextsent>finally the dependency model uses full dependency parse, which might be considered the most informed representation of the word col locate relationships instantiated in corpus sentences, and this approach has been used by several authors (lin, 1998; <papid> P98-2127 </papid>pado?</nextsent>
<nextsent>and lapata, 2007; baroni and lenci, 2010).<papid> J10-4006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1289">
<title id=" S12-1019.xml">selecting corpus semantic models for neuro linguistic decoding </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>all distinct token sequences up to length 4 either side of the target word were counted.
</prevsent>
<prevsent>finally the dependency model uses full dependency parse, which might be considered the most informed representation of the word col locate relationships instantiated in corpus sentences, and this approach has been used by several authors (lin, 1998; <papid> P98-2127 </papid>pado?</prevsent>
</prevsection>
<citsent citstr=" J10-4006 ">
and lapata, 2007; baroni and lenci, 2010).<papid> J10-4006 </papid></citsent>
<aftsection>
<nextsent>the features used are pairs of dependency relation and lexeme corresponding to each edge linked to target word of interest (e.g. likes subj ).
</nextsent>
<nextsent>the parser used here was malt, which achieves accuracies of85% when deriving labelled dependencies on english text (hall et al , 2007).<papid> D07-1097 </papid></nextsent>
<nextsent>the features produced by this module are much more limited, to those words that have direct dependency relation with the word of interest.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1290">
<title id=" S12-1019.xml">selecting corpus semantic models for neuro linguistic decoding </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>and lapata, 2007; baroni and lenci, 2010).<papid> J10-4006 </papid></prevsent>
<prevsent>the features used are pairs of dependency relation and lexeme corresponding to each edge linked to target word of interest (e.g. likes subj ).</prevsent>
</prevsection>
<citsent citstr=" D07-1097 ">
the parser used here was malt, which achieves accuracies of85% when deriving labelled dependencies on english text (hall et al , 2007).<papid> D07-1097 </papid></citsent>
<aftsection>
<nextsent>the features produced by this module are much more limited, to those words that have direct dependency relation with the word of interest.
</nextsent>
<nextsent>2.3 linear learning model.
</nextsent>
<nextsent>a linear regression model will allow us to evaluate how well given model of word semantic scan be used to predict brain activity.
</nextsent>
<nextsent>we follow the analysis in mitchell et al  (2008) and subsequently adopted by several other research groups (see murphy et al , 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1291">
<title id=" S10-1057.xml">utd classifying semantic relations by combining lexical and semantic resources </title>
<section> contextual and lexical features.  </section>
<citcontext>
<prevsection>
<prevsent>for example the words into, produced, and caused are likely to occur in entity-destination, product-producer, and cause-effect relations, respectively.
</prevsent>
<prevsent>using the prefixes of length 5 for the words between the nominals provides kind of stemming (produced ? produ, caused ? cause).
</prevsent>
</prevsection>
<citsent citstr=" W07-2085 ">
inspired by feature from (beamer et al, 2007),<papid> W07-2085 </papid>we extract coarse-grained part of speech sequence for the words between the nominals.</citsent>
<aftsection>
<nextsent>this is accomplished by building string using the first letter of each tokens treebank pos tag.
</nextsent>
<nextsent>this feature is motivated by the fact that relations such as member-collection usually invoke prepositional phrases such as: of, in the, and of various.
</nextsent>
<nextsent>the corresponding pos sequences we extract are: i?,i d?, and j?.
</nextsent>
<nextsent>finally, we also use the number of words between the nominals as feature because relations such as product-producer and entity-origin often have no intervening tokens (e.g., organ builder or coconut oil).syntactic and semantic parses capture long distance relationships between phrases in sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1292">
<title id=" S10-1057.xml">utd classifying semantic relations by combining lexical and semantic resources </title>
<section> contextual and lexical features.  </section>
<citcontext>
<prevsection>
<prevsent>our dependency features are based on paths in the dependency treeof length 1 and length 2.
</prevsent>
<prevsent>the paths encode the dependencies and words those dependencies attach to.
</prevsent>
</prevsection>
<citsent citstr=" P05-3014 ">
to generalize the paths, some of the features replace verbs in the path with their top-level levin class, as determined by running word sense disambiguation system (mihalcea and csomai, 2005) <papid> P05-3014 </papid>followed by lookup in verbnet 4 . one of the fea-.</citsent>
<aftsection>
<nextsent>tures for length 2 paths generalizes further by replacing all words with their location relative to the nominals, either before, between, or after.
</nextsent>
<nextsent>consider example 117 from table 1.
</nextsent>
<nextsent>the length 2 dependency path (feature deppathlen2verbnet) neatly captures the fact that 1 is the subject of verb falling into levin class 27, and 2is the direct object.
</nextsent>
<nextsent>levin class 27 is the class of engender verbs, such as cause, spawn, and generate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1297">
<title id=" S10-1057.xml">utd classifying semantic relations by combining lexical and semantic resources </title>
<section> pre-existing relation features.  </section>
<citcontext>
<prevsection>
<prevsent>for some examples the context and the individual nominal affiliations provide little help in determining the semantic relation, such as example5884 from before (i.e., corn flour).
</prevsent>
<prevsent>these examples require knowledge of the interaction between the nominals and we cannot rely solely on determining the role of one nominal or the other.
</prevsent>
</prevsection>
<citsent citstr=" N07-4013 ">
we turned to text runner (yates et al,2007) <papid> N07-4013 </papid>as large source of background knowledge about pre-existing relations between nom inals.</citsent>
<aftsection>
<nextsent>text runner is query able database of noun-verb-noun triples extracted from large corpus of webpages.
</nextsent>
<nextsent>for example, the phrases retrieved from text runner for corn flourinclude: is ground into?, to make?, to obtain?, and makes?.
</nextsent>
<nextsent>querying in the reverse direction, for flour corn?
</nextsent>
<nextsent>returns phrases such as: contain?, filled with?, comprises?, and is made from?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1298">
<title id=" S12-1076.xml">ets discriminative edit models for paraphrase scoring </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>many problems in natural language processing can be viewed as variations of the task of measuring the semantic textual similarity between short texts.
</prevsent>
<prevsent>however, many systems that address these tasks focus on single task and may or may not generalize well.
</prevsent>
</prevsection>
<citsent citstr=" W09-0441 ">
in this work, we extend an existing machine translation metric, terp (snover et al, 2009<papid> W09-0441 </papid>a), by adding support for more detailed feature types and by implementing discriminative learningalgorithm.</citsent>
<aftsection>
<nextsent>these additions facilitate applications of our system, called perp, to similarity tasks other than machine translation evaluation, such as paraphrase recognition.
</nextsent>
<nextsent>inthe semeval 2012 semantic textual similarity task, perp performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline.
</nextsent>
<nextsent>techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (nielsen et al, 2008; <papid> W08-0902 </papid>leacock and chodorow, 2003), question answering (wang et al, 2007), <papid> D07-1003 </papid>machine translation evaluation (przybocki et al, 2009; snover et al, 2009<papid> W09-0441 </papid>a), etc.an important aspect of this problem is that similarity is not binary.</nextsent>
<nextsent>sentences can be very semantically similar, such that they might be called paraphrases of each other.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1304">
<title id=" S12-1076.xml">ets discriminative edit models for paraphrase scoring </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these additions facilitate applications of our system, called perp, to similarity tasks other than machine translation evaluation, such as paraphrase recognition.
</prevsent>
<prevsent>inthe semeval 2012 semantic textual similarity task, perp performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline.
</prevsent>
</prevsection>
<citsent citstr=" W08-0902 ">
techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (nielsen et al, 2008; <papid> W08-0902 </papid>leacock and chodorow, 2003), question answering (wang et al, 2007), <papid> D07-1003 </papid>machine translation evaluation (przybocki et al, 2009; snover et al, 2009<papid> W09-0441 </papid>a), etc.an important aspect of this problem is that similarity is not binary.</citsent>
<aftsection>
<nextsent>sentences can be very semantically similar, such that they might be called paraphrases of each other.
</nextsent>
<nextsent>they might be completely different.
</nextsent>
<nextsent>or, they might be somewhere in between.
</nextsent>
<nextsent>indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on continuum ofsimilarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1305">
<title id=" S12-1076.xml">ets discriminative edit models for paraphrase scoring </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these additions facilitate applications of our system, called perp, to similarity tasks other than machine translation evaluation, such as paraphrase recognition.
</prevsent>
<prevsent>inthe semeval 2012 semantic textual similarity task, perp performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline.
</prevsent>
</prevsection>
<citsent citstr=" D07-1003 ">
techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (nielsen et al, 2008; <papid> W08-0902 </papid>leacock and chodorow, 2003), question answering (wang et al, 2007), <papid> D07-1003 </papid>machine translation evaluation (przybocki et al, 2009; snover et al, 2009<papid> W09-0441 </papid>a), etc.an important aspect of this problem is that similarity is not binary.</citsent>
<aftsection>
<nextsent>sentences can be very semantically similar, such that they might be called paraphrases of each other.
</nextsent>
<nextsent>they might be completely different.
</nextsent>
<nextsent>or, they might be somewhere in between.
</nextsent>
<nextsent>indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on continuum ofsimilarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1318">
<title id=" S12-1076.xml">ets discriminative edit models for paraphrase scoring </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with 17?
</prevsent>
<prevsent>versus substituting a? for an?).
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
in addition, the heuristic learning algorithm, which involves perturbing the weight vector by small amounts as ingrid search, seems un scalable to larger sets of overlapping features.therefore, here, we use terps inference algorithms that find low cost edit sequences but use discriminative learning algorithm based on the perceptron (rosenblatt, 1958; collins, 2002) <papid> W02-1001 </papid>to estimate edit cost parameters, along with an expanded feature set for broader coverage of the phenomena that are relevant to sentence-to-sentence similarity.</citsent>
<aftsection>
<nextsent>we 529 refer to this new approach as paraphrase edit rate with the perceptron (perp).
</nextsent>
<nextsent>in addition to describing perp, we discuss how it was applied for the semeval 2012 semantic textual similarity (sts) task.
</nextsent>
<nextsent>in this work, our goal is to create system that can take as input two sentences (or short texts) x1 and x2and produce as output prediction y?
</nextsent>
<nextsent>for how similar they are.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1319">
<title id=" S12-1076.xml">ets discriminative edit models for paraphrase scoring </title>
<section> problem definition.  </section>
<citcontext>
<prevsection>
<prevsent>for how similar they are.
</prevsent>
<prevsent>here, we use the 0 to 5 ordinal scale from the sts task, where increasing values indicate greater semantic similarity.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
the sts task data includes five subtasks with text pairs from different sources: the microsoft research paraphrase corpus (dolan et al, 2004) (<papid> C04-1051 </papid>msrpar), the microsoft research video corpus (chen and dolan, 2011) (<papid> P11-1020 </papid>msrvid), statistical machine translation output of parliament proceedings (koehn, 2005)(smt-eur).</citsent>
<aftsection>
<nextsent>for each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values were provided for training and development.
</nextsent>
<nextsent>in addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from ontonotes (pradhan and xue, 2009) <papid> N09-4006 </papid>and wordnet (fellbaum, 1998) (onwn), and machine translations of sentences from news conversations (smt-news).</nextsent>
<nextsent>for all five sources, the held-out test set contained several hundred text pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1320">
<title id=" S12-1076.xml">ets discriminative edit models for paraphrase scoring </title>
<section> problem definition.  </section>
<citcontext>
<prevsection>
<prevsent>for how similar they are.
</prevsent>
<prevsent>here, we use the 0 to 5 ordinal scale from the sts task, where increasing values indicate greater semantic similarity.
</prevsent>
</prevsection>
<citsent citstr=" P11-1020 ">
the sts task data includes five subtasks with text pairs from different sources: the microsoft research paraphrase corpus (dolan et al, 2004) (<papid> C04-1051 </papid>msrpar), the microsoft research video corpus (chen and dolan, 2011) (<papid> P11-1020 </papid>msrvid), statistical machine translation output of parliament proceedings (koehn, 2005)(smt-eur).</citsent>
<aftsection>
<nextsent>for each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values were provided for training and development.
</nextsent>
<nextsent>in addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from ontonotes (pradhan and xue, 2009) <papid> N09-4006 </papid>and wordnet (fellbaum, 1998) (onwn), and machine translations of sentences from news conversations (smt-news).</nextsent>
<nextsent>for all five sources, the held-out test set contained several hundred text pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1321">
<title id=" S12-1076.xml">ets discriminative edit models for paraphrase scoring </title>
<section> problem definition.  </section>
<citcontext>
<prevsection>
<prevsent>the sts task data includes five subtasks with text pairs from different sources: the microsoft research paraphrase corpus (dolan et al, 2004) (<papid> C04-1051 </papid>msrpar), the microsoft research video corpus (chen and dolan, 2011) (<papid> P11-1020 </papid>msrvid), statistical machine translation output of parliament proceedings (koehn, 2005)(smt-eur).</prevsent>
<prevsent>for each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values were provided for training and development.</prevsent>
</prevsection>
<citsent citstr=" N09-4006 ">
in addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from ontonotes (pradhan and xue, 2009) <papid> N09-4006 </papid>and wordnet (fellbaum, 1998) (onwn), and machine translations of sentences from news conversations (smt-news).</citsent>
<aftsection>
<nextsent>for all five sources, the held-out test set contained several hundred text pairs.
</nextsent>
<nextsent>see the task description (agirre et al, 2012) <papid> S12-1051 </papid>for additional details.</nextsent>
<nextsent>in this section, we briefly describe the ter and terp machine translation metrics, and how the perp system extends them in order to better model semantic textual similarity.ter (snover et al, 2006) uses greedy search algorithm to find set of edits to convert one of the paired input sentences into the other.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1322">
<title id=" S12-1076.xml">ets discriminative edit models for paraphrase scoring </title>
<section> problem definition.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from ontonotes (pradhan and xue, 2009) <papid> N09-4006 </papid>and wordnet (fellbaum, 1998) (onwn), and machine translations of sentences from news conversations (smt-news).</prevsent>
<prevsent>for all five sources, the held-out test set contained several hundred text pairs.</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
see the task description (agirre et al, 2012) <papid> S12-1051 </papid>for additional details.</citsent>
<aftsection>
<nextsent>in this section, we briefly describe the ter and terp machine translation metrics, and how the perp system extends them in order to better model semantic textual similarity.ter (snover et al, 2006) uses greedy search algorithm to find set of edits to convert one of the paired input sentences into the other.
</nextsent>
<nextsent>we can view this set of edits as an alignment between the two input sentences x1 and x2, and when two words in x1 and x2, respectively, are part of an edit operation,we say that those words are aligned.1 unlike tradi 1for machine translation evaluation with terp and perp, x1 is systems hypothesis and x2 is reference translation.
</nextsent>
<nextsent>for tional edit distance measures, ter allow for shifts?
</nextsent>
<nextsent>that is, edits that change the positions of words or phrases in the input sentence x1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1354">
<title id=" S10-1031.xml">dfki keywe ranking key phrases extracted from scientific articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the most common approach towards candidate extraction is to generate all n-grams up to particular length and filter them using stopwordlists.
</prevsent>
<prevsent>lately, more sophisticated candidate extraction methods, usually based on additional linguistic information (e.g. pos tags), have been proposed and shown to produce better results (e.g.hulth (2004)).
</prevsent>
</prevsection>
<citsent citstr=" N09-1070 ">
liu et al (2009) <papid> N09-1070 </papid>restrict their candidate list to verb, noun and adjective words.</citsent>
<aftsection>
<nextsent>kim and kan (2009) <papid> W09-2902 </papid>generate regular expression rules to extract simplex nouns and nominal phrases.</nextsent>
<nextsent>as the majority of technical terms is in nominal group positions 2 , we assume that the same holds true for key phrases and apply an adapted nominal group chunker to extract key phrase candidates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1355">
<title id=" S10-1031.xml">dfki keywe ranking key phrases extracted from scientific articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>lately, more sophisticated candidate extraction methods, usually based on additional linguistic information (e.g. pos tags), have been proposed and shown to produce better results (e.g.hulth (2004)).
</prevsent>
<prevsent>liu et al (2009) <papid> N09-1070 </papid>restrict their candidate list to verb, noun and adjective words.</prevsent>
</prevsection>
<citsent citstr=" W09-2902 ">
kim and kan (2009) <papid> W09-2902 </papid>generate regular expression rules to extract simplex nouns and nominal phrases.</citsent>
<aftsection>
<nextsent>as the majority of technical terms is in nominal group positions 2 , we assume that the same holds true for key phrases and apply an adapted nominal group chunker to extract key phrase candidates.
</nextsent>
<nextsent>the selection process is usually based on some supervised learning algorithm, e.g. naive bayes (frank et al, 1999), genetic algorithms (turney,1999), neural networks (wang et al, 2005) or decision trees (medelyan et al, 2009).<papid> D09-1137 </papid></nextsent>
<nextsent>unsupervised approaches have also been proposed, e.g. by mihalcea and tarau (2004) <papid> W04-3252 </papid>and liu et al (2009).<papid> N09-1070 </papid>however, as for the shared task, annotated training data was available, we opted for an approach based on supervised learning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1356">
<title id=" S10-1031.xml">dfki keywe ranking key phrases extracted from scientific articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kim and kan (2009) <papid> W09-2902 </papid>generate regular expression rules to extract simplex nouns and nominal phrases.</prevsent>
<prevsent>as the majority of technical terms is in nominal group positions 2 , we assume that the same holds true for key phrases and apply an adapted nominal group chunker to extract key phrase candidates.</prevsent>
</prevsection>
<citsent citstr=" D09-1137 ">
the selection process is usually based on some supervised learning algorithm, e.g. naive bayes (frank et al, 1999), genetic algorithms (turney,1999), neural networks (wang et al, 2005) or decision trees (medelyan et al, 2009).<papid> D09-1137 </papid></citsent>
<aftsection>
<nextsent>unsupervised approaches have also been proposed, e.g. by mihalcea and tarau (2004) <papid> W04-3252 </papid>and liu et al (2009).<papid> N09-1070 </papid>however, as for the shared task, annotated training data was available, we opted for an approach based on supervised learning.</nextsent>
<nextsent>1 http://semeval2.fbk.eu/semeval2.phplocation=tasks#t6 2experiments on 100 manually annotated scientific abstracts from the biology domain showed that 94% of technical terms are in nominal group position (eichler et al, 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1358">
<title id=" S10-1031.xml">dfki keywe ranking key phrases extracted from scientific articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as the majority of technical terms is in nominal group positions 2 , we assume that the same holds true for key phrases and apply an adapted nominal group chunker to extract key phrase candidates.
</prevsent>
<prevsent>the selection process is usually based on some supervised learning algorithm, e.g. naive bayes (frank et al, 1999), genetic algorithms (turney,1999), neural networks (wang et al, 2005) or decision trees (medelyan et al, 2009).<papid> D09-1137 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
unsupervised approaches have also been proposed, e.g. by mihalcea and tarau (2004) <papid> W04-3252 </papid>and liu et al (2009).<papid> N09-1070 </papid>however, as for the shared task, annotated training data was available, we opted for an approach based on supervised learning.</citsent>
<aftsection>
<nextsent>1 http://semeval2.fbk.eu/semeval2.phplocation=tasks#t6 2experiments on 100 manually annotated scientific abstracts from the biology domain showed that 94% of technical terms are in nominal group position (eichler et al, 2009).
</nextsent>
<nextsent>150
</nextsent>
<nextsent>rather than extracting candidates from the full text of the article, we restrict our search for candidate sto the first 2000 characters starting with the abstract 3 . we also extract title and general terms.
</nextsent>
<nextsent>for use in the feature construction process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1362">
<title id=" S10-1031.xml">dfki keywe ranking key phrases extracted from scientific articles </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>average token count measures the average occurrence of the individual (lemmatized) tokens of the term in the document.
</prevsent>
<prevsent>our assumption is that candidates with high average token count are more likely to be keyphrases.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
point-wise mutual information (pmi, church and hanks (1989)) <papid> P89-1010 </papid>is used to capture the semantic relatedness of the candidate to the topic of the document.</citsent>
<aftsection>
<nextsent>a similar feature is introduced by turney (2003), who, in first pass, ranks the candidates based on base feature set, and then reranks them by calculating the statistical association between the given candidate and the top candidates from the first pass.
</nextsent>
<nextsent>to avoid the two-pass method, rather than calculating inter-candidate association, we calculate the association of each candidate to the terms specified in the general terms section of the paper.
</nextsent>
<nextsent>like turney, we calculate pmi based on web search results (in our case, using msn).
</nextsent>
<nextsent>the feature maxpmi captures the maximum pmi score achieved with the lemmatized candidate and any of the general terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1363">
<title id=" S12-1080.xml">irit textual similarity combining conceptual similarity with an ngram comparison method </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the participation of theirit team to semeval 2012 task 6 (semantic textual similarity).
</prevsent>
<prevsent>the method used consists of n-gram based comparison method combined with conceptual similarity measure that uses wordnet to calculate the similarity between pair of concepts.
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
the system used for the participation of the irit team (composed by members of the research group ssig and melodi) to the semantic textual similarity (sts) task (agirre et al, 2012) <papid> S12-1051 </papid>is based on two sub-modules: ? module that calculates the similarity between sentences using n-gram based similarity; ? module that calculates the similarity between concepts in the two sentences, using concept similarity measure and wordnet (miller, 1995) as resource.in figure 1, we show the structure of the system and the connections between the main compo nents.</citsent>
<aftsection>
<nextsent>the input phrases are passed on one hand directly to the n-gram similarity module, and on the other they are annoted with the stanford pos tagger (toutanova et al, 2003).<papid> N03-1033 </papid></nextsent>
<nextsent>all nouns and verbs are extracted from the tagged phrases and wordnet is searched for synsets corresponding to the extracted nouns and nouns associated to the verbs by the derived terms relationship.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1364">
<title id=" S12-1080.xml">irit textual similarity combining conceptual similarity with an ngram comparison method </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the method used consists of n-gram based comparison method combined with conceptual similarity measure that uses wordnet to calculate the similarity between pair of concepts.
</prevsent>
<prevsent>the system used for the participation of the irit team (composed by members of the research group ssig and melodi) to the semantic textual similarity (sts) task (agirre et al, 2012) <papid> S12-1051 </papid>is based on two sub-modules: ? module that calculates the similarity between sentences using n-gram based similarity; ? module that calculates the similarity between concepts in the two sentences, using concept similarity measure and wordnet (miller, 1995) as resource.in figure 1, we show the structure of the system and the connections between the main compo nents.</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
the input phrases are passed on one hand directly to the n-gram similarity module, and on the other they are annoted with the stanford pos tagger (toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>all nouns and verbs are extracted from the tagged phrases and wordnet is searched for synsets corresponding to the extracted nouns and nouns associated to the verbs by the derived terms relationship.
</nextsent>
<nextsent>the synsets are the concepts used by the conceptual similarity module to phrases n-gram similarity module pos tagger google web 1t wordnet concept similarity module score geometric average and normalisation concept extraction figure 1: schema of the system.calculate the concept similarity.
</nextsent>
<nextsent>each module calculates similarity score using its own method; the final similarity value is calculated as the geometric average between the two scores, multiplied by 5 in order to comply with the task specifications.
</nextsent>
<nextsent>the n-gram based similarity relies on the idea that two sentences are semantically related if they contain long enough sub-sequence of non-empty terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1365">
<title id=" S12-1080.xml">irit textual similarity combining conceptual similarity with an ngram comparison method </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>google web 1t (brants and franz, 2006) has been used to calculate term idf, which is used as measure of the importance of the terms.
</prevsent>
<prevsent>the conceptual similarity is based on the idea that, given an ontology, two concepts are semantically similar if their distance from common ancestor is smallenough.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
we used three different measures: the wu palmer similarity measure (wu and palmer, 1994) <papid> P94-1019 </papid>and two proxigenea?</citsent>
<aftsection>
<nextsent>measures (dudognon et al, 2010).
</nextsent>
<nextsent>in the following we will explain in detail how 552 each similarity module works.
</nextsent>
<nextsent>n-gram based similarity is based on the clustered keywords positional distance (ckpd) model proposed in (buscaldi et al, 2009).
</nextsent>
<nextsent>this model was originally proposed for passage retrieval in the fieldof question answering (qa), and it has been implemented in the jirs system1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1370">
<title id=" S10-1096.xml">hrwsd system description for all words word sense disambiguation on a specific domain at semeval2010 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the document describes the knowledge based domain-wsd system using heuristic rules (knowledge-base).
</prevsent>
<prevsent>this hrwsd system delivered the best performance (55.9%) among all chinese systems in semeval-2010 task 17: all-words wsd on specific domain.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
word sense disambiguation (wsd) is essential for language understanding systems such as information retrieval, summarization, and machine translation systems (dagan and itai, 1994; <papid> J94-4003 </papid>schutze and pedersen, 1995; ng and zelle, 1997).</citsent>
<aftsection>
<nextsent>in particular due to the rapid development of other issues in computational linguistics, wsd has been considered the next important task to be solved.
</nextsent>
<nextsent>among various wsd tasks, the lexical sample task can achieve precision rate more than 70% in chinese, so can the all-words task in english, but currently no chinese all-words wsd system is available.
</nextsent>
<nextsent>this study proposes an all-words wsd system conducted on specific domain which can achieve 55.9% precision rate.
</nextsent>
<nextsent>this system makes use of certain characteristics of wordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1371">
<title id=" S12-1094.xml">unt a supervised synergistic approach to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>measures of text similarity have been used for along time in applications in natural language processing and related areas.
</prevsent>
<prevsent>one of the earliest applications of text similarity is perhaps the vector space model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in collection in reversed order of their similarity to the given query (salton and lesk, 1971).
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
text similarity has also been used for relevance feedback and text classification (rocchio, 1971), word sense disambiguation (lesk, 1986; schutze, 1998), and more recently for extractive summarization (salton et al , 1997), and methods for automatic evaluation of machine translation (papineni et al , 2002) <papid> P02-1040 </papid>or text summarization (lin and hovy, 2003).<papid> N03-1020 </papid></citsent>
<aftsection>
<nextsent>measures of text similarity were also found useful for the evaluation of text coherence (lapata and barzilay, 2005).
</nextsent>
<nextsent>earlier work on this task has primarily focused on simple lexical matching methods, which produce similarity score based on the number of lexical units that occur in both input segments.
</nextsent>
<nextsent>improvements to this simple method have considered stemming, stop-word removal, part-of-speech tagging, longestsubsequence matching, as well as various weighting and normalization factors (salton and buckley, 1997).
</nextsent>
<nextsent>while successful to certain degree, these lexical similarity methods cannot always identify the semantic similarity of texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1372">
<title id=" S12-1094.xml">unt a supervised synergistic approach to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>measures of text similarity have been used for along time in applications in natural language processing and related areas.
</prevsent>
<prevsent>one of the earliest applications of text similarity is perhaps the vector space model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in collection in reversed order of their similarity to the given query (salton and lesk, 1971).
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
text similarity has also been used for relevance feedback and text classification (rocchio, 1971), word sense disambiguation (lesk, 1986; schutze, 1998), and more recently for extractive summarization (salton et al , 1997), and methods for automatic evaluation of machine translation (papineni et al , 2002) <papid> P02-1040 </papid>or text summarization (lin and hovy, 2003).<papid> N03-1020 </papid></citsent>
<aftsection>
<nextsent>measures of text similarity were also found useful for the evaluation of text coherence (lapata and barzilay, 2005).
</nextsent>
<nextsent>earlier work on this task has primarily focused on simple lexical matching methods, which produce similarity score based on the number of lexical units that occur in both input segments.
</nextsent>
<nextsent>improvements to this simple method have considered stemming, stop-word removal, part-of-speech tagging, longestsubsequence matching, as well as various weighting and normalization factors (salton and buckley, 1997).
</nextsent>
<nextsent>while successful to certain degree, these lexical similarity methods cannot always identify the semantic similarity of texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1373">
<title id=" S12-1094.xml">unt a supervised synergistic approach to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while successful to certain degree, these lexical similarity methods cannot always identify the semantic similarity of texts.
</prevsent>
<prevsent>for instance, there is an obvious similarity between the text segments own dog and have an animal, but most of the current text similarity metrics will fail in identifying any kind of connection between these texts.more recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (jiang and conrath, 1997; leacock and chodorow, 1998; lin, 1998; resnik, 1995)) within semantic similarity method that works for entire texts.
</prevsent>
</prevsection>
<citsent citstr=" P11-1076 ">
the methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into text similarity score (mihalcea et al , 2006; islam and inkpen, 2009; hassan and mihalcea, 2011; mohler et al , 2011), <papid> P11-1076 </papid>or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as latent semantic analysis (landauer et al , 1997), explicit semantic analysis (gabrilovich and markovitch, 2007), or salient semantic analysis (hassan and mihalcea, 2011).</citsent>
<aftsection>
<nextsent>in this paper, we describe the system with which 635we participated in the semeval 2012 task on semantic text similarity (agirre et al , 2012).<papid> S12-1051 </papid></nextsent>
<nextsent>the system builds upon our earlier work on corpus-basedand knowledge-based methods of text semantic similarity (mihalcea et al , 2006; hassan and mihalcea, 2011; mohler et al , 2011), <papid> P11-1076 </papid>and combines all these previous methods into meta-system by using machine learning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1375">
<title id=" S12-1094.xml">unt a supervised synergistic approach to semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, there is an obvious similarity between the text segments own dog and have an animal, but most of the current text similarity metrics will fail in identifying any kind of connection between these texts.more recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (jiang and conrath, 1997; leacock and chodorow, 1998; lin, 1998; resnik, 1995)) within semantic similarity method that works for entire texts.
</prevsent>
<prevsent>the methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into text similarity score (mihalcea et al , 2006; islam and inkpen, 2009; hassan and mihalcea, 2011; mohler et al , 2011), <papid> P11-1076 </papid>or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as latent semantic analysis (landauer et al , 1997), explicit semantic analysis (gabrilovich and markovitch, 2007), or salient semantic analysis (hassan and mihalcea, 2011).</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
in this paper, we describe the system with which 635we participated in the semeval 2012 task on semantic text similarity (agirre et al , 2012).<papid> S12-1051 </papid></citsent>
<aftsection>
<nextsent>the system builds upon our earlier work on corpus-basedand knowledge-based methods of text semantic similarity (mihalcea et al , 2006; hassan and mihalcea, 2011; mohler et al , 2011), <papid> P11-1076 </papid>and combines all these previous methods into meta-system by using machine learning.</nextsent>
<nextsent>the framework provided bythe task organizers also enabled us to perform an indepth analysis of the various components used in our system, and draw conclusions concerning the role played by the different resources, features, and algorithms in building state-of-the-art semantic text similarity system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1378">
<title id=" S12-1094.xml">unt a supervised synergistic approach to semantic text similarity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>knowledge-based methods derive measure of relatedness by utilizing lexical resources and ontologies such as wordnet (miller, 1995) to measure definitional overlap, term distance within graphical taxonomy, or term depth in the taxonomy as measure of specificity.
</prevsent>
<prevsent>we explore several of these measures indepth in section 3.3.1.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
on the other side, corpus-based measures such as latent semantic analysis (lsa) (landauer et al , 1997), explicit semantic analysis (esa) (gabrilovich and markovitch, 2007), salient semantic analysis(ssa) (hassan and mihalcea, 2011), pointwise mutual information (pmi) (church and hanks, 1990), <papid> J90-1003 </papid>pmi-ir (turney, 2001), second order pmi (islamand inkpen, 2006), hyper space analogues to language (burgess et al , 1998) and distributional similarity (lin, 1998) employ probabilistic approaches to decode the semantics of words.</citsent>
<aftsection>
<nextsent>they consist of unsupervised methods that utilize the contextual information and patterns observed in raw text tobuild semantic profiles of words.
</nextsent>
<nextsent>unlike knowledge based methods, which suffer from limited coverage,corpus-based measures are able to induce similarity between any given two words, as long as they appear in the very large corpus used as training.
</nextsent>
<nextsent>the system we proposed for the semeval 2012 semantic textual similarity task builds upon both knowledge- and corpus-based methods previously described in (mihalcea et al , 2006; hassan and mihalcea, 2011; mohler et al , 2011).<papid> P11-1076 </papid></nextsent>
<nextsent>the predictions of these independent systems, paired with additional salient features, are leveraged by meta-system that employs machine learning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1382">
<title id=" S12-1094.xml">unt a supervised synergistic approach to semantic text similarity </title>
<section> semantic textual similarity system.  </section>
<citcontext>
<prevsection>
<prevsent>our meta-system uses several features, which can be grouped into knowledge-based, corpus-based, and bipartite graph matching, as described below.
</prevsent>
<prevsent>the abbreviations appearing between parentheses by each method allow for easy cross-referencing with the evaluations provided in table 1.
</prevsent>
</prevsection>
<citsent citstr=" E09-1065 ">
3.3.1 knowledge-based semantic similarity features following prior work from our group (mihalceaet al , 2006; mohler and mihalcea, 2009), <papid> E09-1065 </papid>we employ several wordnet-based similarity metrics for the task of sentence-level similarity.</citsent>
<aftsection>
<nextsent>briefly, for each open-class word in one of the input texts, we compute the maximum semantic similarity (using the wordnet::similarity package (pedersen et al , 2004)) <papid> N04-3012 </papid>that can be obtained by pairing it with any open-class word in the other input text.</nextsent>
<nextsent>all the word-to-word similarity scores obtained in this way are summed and normalized to the length of the two input texts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1383">
<title id=" S12-1094.xml">unt a supervised synergistic approach to semantic text similarity </title>
<section> semantic textual similarity system.  </section>
<citcontext>
<prevsection>
<prevsent>the abbreviations appearing between parentheses by each method allow for easy cross-referencing with the evaluations provided in table 1.
</prevsent>
<prevsent>3.3.1 knowledge-based semantic similarity features following prior work from our group (mihalceaet al , 2006; mohler and mihalcea, 2009), <papid> E09-1065 </papid>we employ several wordnet-based similarity metrics for the task of sentence-level similarity.</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
briefly, for each open-class word in one of the input texts, we compute the maximum semantic similarity (using the wordnet::similarity package (pedersen et al , 2004)) <papid> N04-3012 </papid>that can be obtained by pairing it with any open-class word in the other input text.</citsent>
<aftsection>
<nextsent>all the word-to-word similarity scores obtained in this way are summed and normalized to the length of the two input texts.
</nextsent>
<nextsent>we provide below short description for each of the similarity metrics employed by this system3.
</nextsent>
<nextsent>the shortest path (path) similarity is determined as: simpath = 1 length (1)where length is the length of the shortest path between two concepts using node-counting (including the end nodes).
</nextsent>
<nextsent>the leacock &amp; chodorow (leacock and chodorow, 1998) (lch) similarity is determined as: simlch = ? log length 2 (2)where length is the length of the shortest path between two concepts using node-counting, and is the maximum depth of the taxonomy.the lesk (lesk) similarity of two concepts is defined as function of the overlap between the corresponding definitions, as provided by dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1384">
<title id=" S12-1094.xml">unt a supervised synergistic approach to semantic text similarity </title>
<section> semantic textual similarity system.  </section>
<citcontext>
<prevsection>
<prevsent>the leacock &amp; chodorow (leacock and chodorow, 1998) (lch) similarity is determined as: simlch = ? log length 2 (2)where length is the length of the shortest path between two concepts using node-counting, and is the maximum depth of the taxonomy.the lesk (lesk) similarity of two concepts is defined as function of the overlap between the corresponding definitions, as provided by dictionary.
</prevsent>
<prevsent>it is based on an algorithm proposed by lesk (1986) as solution for word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
the wu &amp; palmer (wu and palmer, 1994) (<papid> P94-1019 </papid>wup ) similarity metric measures the depth of two given concepts in the wordnet taxonomy, and the depth of the least common subsumer (lcs), and combines these figures into similarity score: simwup = 2 ? depth(lcs) depth(concept1) + depth(concept2) (3) the measure introduced by resnik (resnik, 1995) (res) returns the information content (ic) of the lcs of two concepts: simres = ic(lcs) (4) where ic is defined as: ic(c) = ? logp (c) (5)and (c) is the probability of encountering an instance of concept in large corpus.</citsent>
<aftsection>
<nextsent>the measure introduced by lin (lin, 1998) (lin) builds on resniks measure of similarity, and adds normalization factor consisting of the information content of the two input concepts: simlin = 2 ? ic(lcs) ic(concept1) + ic(concept2) (6) 3we point out that the similarity metric proposed by hirst &amp;st.; onge was not considered due to the time constraints associated with the sts task.
</nextsent>
<nextsent>637 we also consider the jiang &amp; conrath (jiang and conrath, 1997) (jcn ) measure of similarity: simjnc = 1 ic(concept1) + ic(concept2)?
</nextsent>
<nextsent>2 ? ic(lcs) (7)each of the measures listed above is used as feature by our meta-system.
</nextsent>
<nextsent>3.3.2 corpus-based semantic similarity features while most of the corpus-based methods induce semantic profiles in word-space, where the semantic profile of word is expressed in terms of its cooccurrence with other words, lsa, esa and ssa stand out as different, since they relyon concept space representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1387">
<title id=" S12-1094.xml">unt a supervised synergistic approach to semantic text similarity </title>
<section> semantic textual similarity system.  </section>
<citcontext>
<prevsection>
<prevsent>of these, 32 are based upon the bag-of-words semantic similarity of the subgraphs using the metrics described in section 3.3.1 as well as wikipedia-trained lsa model.
</prevsent>
<prevsent>the remaining 32 features are lexico-syntactic features associated with the parent nodes of the subgraphs and are described in more detail in our earlier paper.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
we then calculate weights associated with these features using an averaged version of the perceptron algorithm (freund and schapire, 1999; collins, 2002) <papid> W02-1001 </papid>trained on set of 32 manually annotated instructor/student answer pairs selected from the short-answer grading corpus (mm2011).</citsent>
<aftsection>
<nextsent>these pairs contain 7303 node pairs (656 matches, 6647 non-matches).
</nextsent>
<nextsent>once the weights are calculated, asimilarity score for each pair of nodes can be computed by taking the dot product of the feature vector with the weights.in the second stage, the node similarity scores calculated in the previous step are used to find an optimal alignment for the pair of dependency graphs.
</nextsent>
<nextsent>we begin with bipartite graph where each node in one graph is represented by node on the left side of the bipartite graph and each node in the other 4we here use the output of the stanford dependency pars erin collapse/propagate mode with some modifications as described in our earlier work.
</nextsent>
<nextsent>5with the exception of the four features based upon the hirst &amp; st.onge similarity metric, these are equivalent to the features used in previous work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1388">
<title id=" P98-2230.xml">machine translation with a stochastic grammatical channel </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>we introduce stochastic grammatical channel model for machine translation, that synthesizes sev-eral desirable characteristics of both statistical and grammatical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" P96-1021 ">
as with the pure statistical translation model described by wu (1996) (<papid> P96-1021 </papid>in which bracketing transduction gram-mar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis pace can be performed in polynomial time, and robustness heuristics arise naturally from language-independent inversion- transduction model.</citsent>
<aftsection>
<nextsent>however, unlike pure statisti-cal translation models, the generated output string is guaranteed to conform to given target gram-mar. the model employs only (1) translation lexicon, (2) context-free grammar for the target language, and (3) bigram language model.
</nextsent>
<nextsent>the fact that no explicit bilingual translation rules are used makes the model easily portable to variety of source languages.
</nextsent>
<nextsent>initial experiments show that it also achieves ignificant speed gains over our ear-lier model.
</nextsent>
<nextsent>speed of statistical machine translation methods has long been an issue.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1391">
<title id=" P98-2230.xml">machine translation with a stochastic grammatical channel </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>the sbtg channel made exhaustive search possible through dynamic programming, in-stead of previous  stack search  heuristics.
</prevsent>
<prevsent>trans-lation accuracy was not compromised, because the sbtg is apparently flexible enough to model word- order variation (between english and chinese) even though it eliminates large portions of the space of 1408 word alignments.
</prevsent>
</prevsection>
<citsent citstr=" W95-0106 ">
the sbtg can be regarded as model of the language-universal hypothesis that closely related arguments end to stay together (wu, 1995<papid> W95-0106 </papid>a; wu, 1995<papid> W95-0106 </papid>b).</citsent>
<aftsection>
<nextsent>in this paper we introduce generalization of wu method with the objectives of 1.
</nextsent>
<nextsent>increasing translation speed further, 2.
</nextsent>
<nextsent>improving meaning-preservation accuracy, 3.
</nextsent>
<nextsent>improving rammaticality of the output, and 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1411">
<title id=" P98-2230.xml">machine translation with a stochastic grammatical channel </title>
<section> review: noisy channel model.  </section>
<citcontext>
<prevsection>
<prevsent>like wu sbtg model, the translation hypothesis space can be ex-haustively searched in polynomial time, as shown in section 5.
</prevsent>
<prevsent>the experiments discussed in section 6 show promising results for these directions.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
the statistical translation model introduced by ibm (brown et al, 1990) <papid> J90-2002 </papid>views translation as noisy channel process.</citsent>
<aftsection>
<nextsent>the underlying enerative model contains stochastic chinese (input) sentence gen-erator whose output is  corrupted  by the transla-tion channel to produce english (output) sentences.
</nextsent>
<nextsent>assume, as we do throughout this paper, that the input language is english and the task is to trans-late into chinese.
</nextsent>
<nextsent>in the ibm system, the language model employs imple n-grams, while the transla-tion model employs several sets of parameters as discussed below.
</nextsent>
<nextsent>estimation of the parameters has been described elsewhere (brown et al, 1993).<papid> J93-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1412">
<title id=" P98-2230.xml">machine translation with a stochastic grammatical channel </title>
<section> review: noisy channel model.  </section>
<citcontext>
<prevsection>
<prevsent>assume, as we do throughout this paper, that the input language is english and the task is to trans-late into chinese.
</prevsent>
<prevsent>in the ibm system, the language model employs imple n-grams, while the transla-tion model employs several sets of parameters as discussed below.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
estimation of the parameters has been described elsewhere (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>translation is performed in the reverse direction from generation, as usual for recognition under gen-erative models.
</nextsent>
<nextsent>for each english sentence to be translated, the system attempts to find the chinese sentence c, such that: c* = argmaxpr(cle ) = argmaxpr(ele ) pr(c) (1) g in the ibm model, the search for the optimal c, is performed using best-first heuristic  stack search  similar to a* methods.
</nextsent>
<nextsent>one of the primary obstacles to making the statis-tical translation approach practical is slow speed of translation, as performed in a* fashion.
</nextsent>
<nextsent>this price is paid for the robustness that is obtained by using very flexible language and translation models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1436">
<title id=" P98-2230.xml">machine translation with a stochastic grammatical channel </title>
<section> a sitg channel model  </section>
<citcontext>
<prevsection>
<prevsent>this description corresponds toone of the simplest ones,  model 2 ; search costs for the more complex models are correspondingly higher.
</prevsent>
<prevsent>the translation channel we propose is based on the recently introduced bilingual anguage model-ing approach.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
the model employs stochastic ver-sion of an inversion transduction grammar or itg (wu, 1995<papid> W95-0106 </papid>c; wu, 1995<papid> W95-0106 </papid>d; wu, 1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>this formal-ism was originally developed for the purpose of par-allel corpus annotation, with applications for brack-eting, alignment, and segmentation.
</nextsent>
<nextsent>subsequently, method was developed to use special case of the itgrthe aforementioned btgrfor the translation task itself (wu, 1996).<papid> P96-1021 </papid></nextsent>
<nextsent>the next few paragraphs briefly review the main properties of itgs, before we describe the sitg channel.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1455">
<title id=" P98-2230.xml">machine translation with a stochastic grammatical channel </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>the grammatical channel was tested in the silc translation system.
</prevsent>
<prevsent>the translation lexicon was partly constructed by training on government tran-scripts from the hkust english-chinese paral-lel bilingual corpus, and partly entered by hand.
</prevsent>
</prevsection>
<citsent citstr=" P94-1012 ">
the corpus was sentence-aligned statistically (wu, 1994); <papid> P94-1012 </papid>chinese words and collocations were ex-tracted (fung and wu, 1994; <papid> P94-1012 </papid>wu and fung, 1994); <papid> A94-1030 </papid>then translation pairs were learned via an em pro-cedure (wu and xia, 1995).</citsent>
<aftsection>
<nextsent>together with hand- constructed entries, the resulting english vocabu-lary is approximately 9,500 words and the chinese vocabulary is approximately 14,500 words, with many-to-many translation mapping averaging 2.56 chinese translations per english word.
</nextsent>
<nextsent>since the lexicon content is mixed, we approximate ransla- tion probabilities by using the unigram distribution of the target vocabulary from small monolingual corpus.
</nextsent>
<nextsent>noise still exists in the lexicon.
</nextsent>
<nextsent>the chinese grammar we used is not tight-- it was written for robust parsing purposes, and as such it over-generates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1457">
<title id=" P98-2230.xml">machine translation with a stochastic grammatical channel </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>the grammatical channel was tested in the silc translation system.
</prevsent>
<prevsent>the translation lexicon was partly constructed by training on government tran-scripts from the hkust english-chinese paral-lel bilingual corpus, and partly entered by hand.
</prevsent>
</prevsection>
<citsent citstr=" A94-1030 ">
the corpus was sentence-aligned statistically (wu, 1994); <papid> P94-1012 </papid>chinese words and collocations were ex-tracted (fung and wu, 1994; <papid> P94-1012 </papid>wu and fung, 1994); <papid> A94-1030 </papid>then translation pairs were learned via an em pro-cedure (wu and xia, 1995).</citsent>
<aftsection>
<nextsent>together with hand- constructed entries, the resulting english vocabu-lary is approximately 9,500 words and the chinese vocabulary is approximately 14,500 words, with many-to-many translation mapping averaging 2.56 chinese translations per english word.
</nextsent>
<nextsent>since the lexicon content is mixed, we approximate ransla- tion probabilities by using the unigram distribution of the target vocabulary from small monolingual corpus.
</nextsent>
<nextsent>noise still exists in the lexicon.
</nextsent>
<nextsent>the chinese grammar we used is not tight-- it was written for robust parsing purposes, and as such it over-generates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1458">
<title id=" S12-1040.xml">ugroningen negation detection with discourse representation structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>negation lies at the heart of deductive inference, ofwhich consistency checking (searching for contradictions in texts) is prime example in natural language understanding.
</prevsent>
<prevsent>it shouldnt therefore come as surprise that detecting negation and adequately representing its scope is of utmost importance in computational semantics.
</prevsent>
</prevsection>
<citsent citstr=" W08-2222 ">
in this paper we present and evaluate system that transforms texts into logical formulas ? using the c&c; tools and boxer (bos, 2008) ? <papid> W08-2222 </papid>in the context of the shared task on recognising negation in english texts (morante and blanco, 2012).<papid> S12-1035 </papid></citsent>
<aftsection>
<nextsent>we will first sketch the background and the basics of the formalism that we employ in our analysis of negation (section 2).
</nextsent>
<nextsent>in section 3 we explain howwe detect negation cues and scope.
</nextsent>
<nextsent>finally, in section 4 we present the results obtained in the shared task, and we discuss them in section 5.
</nextsent>
<nextsent>the semantic representations that are used in this shared task on detecting negation in texts are constructed by means of pipeline of natural language processing components, of which the backbone is provided by the c&c; tools and boxer (curran etal., 2007).<papid> P07-2009 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1459">
<title id=" S12-1040.xml">ugroningen negation detection with discourse representation structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>negation lies at the heart of deductive inference, ofwhich consistency checking (searching for contradictions in texts) is prime example in natural language understanding.
</prevsent>
<prevsent>it shouldnt therefore come as surprise that detecting negation and adequately representing its scope is of utmost importance in computational semantics.
</prevsent>
</prevsection>
<citsent citstr=" S12-1035 ">
in this paper we present and evaluate system that transforms texts into logical formulas ? using the c&c; tools and boxer (bos, 2008) ? <papid> W08-2222 </papid>in the context of the shared task on recognising negation in english texts (morante and blanco, 2012).<papid> S12-1035 </papid></citsent>
<aftsection>
<nextsent>we will first sketch the background and the basics of the formalism that we employ in our analysis of negation (section 2).
</nextsent>
<nextsent>in section 3 we explain howwe detect negation cues and scope.
</nextsent>
<nextsent>finally, in section 4 we present the results obtained in the shared task, and we discuss them in section 5.
</nextsent>
<nextsent>the semantic representations that are used in this shared task on detecting negation in texts are constructed by means of pipeline of natural language processing components, of which the backbone is provided by the c&c; tools and boxer (curran etal., 2007).<papid> P07-2009 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1460">
<title id=" S12-1040.xml">ugroningen negation detection with discourse representation structures </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in section 3 we explain howwe detect negation cues and scope.
</prevsent>
<prevsent>finally, in section 4 we present the results obtained in the shared task, and we discuss them in section 5.
</prevsent>
</prevsection>
<citsent citstr=" P07-2009 ">
the semantic representations that are used in this shared task on detecting negation in texts are constructed by means of pipeline of natural language processing components, of which the backbone is provided by the c&c; tools and boxer (curran etal., 2007).<papid> P07-2009 </papid></citsent>
<aftsection>
<nextsent>this tool chain is currently in use semi automatically for constructing large semantically annotated corpus, the groningen meaning bank (basile et al, 2012).
</nextsent>
<nextsent>the c&c; tools are applied for tagging the data with part-of-speech and supertags and for syntactic parsing, using the formalism of combinatory categorial grammar, ccg (steedman, 2001).
</nextsent>
<nextsent>the output of the parser, ccg derivations, form the input of boxer, producing formal semantic representations in the form of discourse representation structures (drss), the basic meaning-carrying structure sin the framework of discourse representation theory (kamp and reyle, 1993).
</nextsent>
<nextsent>drt is widely accepted formal theory of natural language meaning that has been used to study wide range of linguistic 301    iprpnpv0.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1461">
<title id=" S10-1051.xml">ucd goggle a hybrid system for noun compound paraphrasing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>research on noun compounds involves two main tasks: nc detection and nc interpretation.
</prevsent>
<prevsent>the latter has been studied in the context of many natural language applications, including question answering, machine translation, information retrieval, and information extraction.
</prevsent>
</prevsection>
<citsent citstr=" P06-2064 ">
the use of multiple paraphrases as semanticintepretation of noun compounds has recently be come popular (kim and baldwin, 2006; <papid> P06-2064 </papid>nakov and hearst, 2006; butnariu and veale, 2008; <papid> C08-1011 </papid>nakov, 2008).</citsent>
<aftsection>
<nextsent>the best paraphrases are those which most aptly characterize the relationship between the modifier noun and the head noun.
</nextsent>
<nextsent>the aim of this current work is to provide aran king for list of paraphrases that best approximates human rankings for the same paraphrases.
</nextsent>
<nextsent>we have created system called ucd-goggle, which uses semantic knowledge acquired from google n-grams together with human-preferencesmined from training data.
</nextsent>
<nextsent>three major components are involved in our system: b-score,produced by bayesian algorithm using semantic knowledge from the n-grams corpus with smoothing layer of additional inference; t -score captures human preferences observed in the tail distribution of training data; and p-score captures pairwise paraphrase preferences calculated from the training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1462">
<title id=" S10-1051.xml">ucd goggle a hybrid system for noun compound paraphrasing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>research on noun compounds involves two main tasks: nc detection and nc interpretation.
</prevsent>
<prevsent>the latter has been studied in the context of many natural language applications, including question answering, machine translation, information retrieval, and information extraction.
</prevsent>
</prevsection>
<citsent citstr=" C08-1011 ">
the use of multiple paraphrases as semanticintepretation of noun compounds has recently be come popular (kim and baldwin, 2006; <papid> P06-2064 </papid>nakov and hearst, 2006; butnariu and veale, 2008; <papid> C08-1011 </papid>nakov, 2008).</citsent>
<aftsection>
<nextsent>the best paraphrases are those which most aptly characterize the relationship between the modifier noun and the head noun.
</nextsent>
<nextsent>the aim of this current work is to provide aran king for list of paraphrases that best approximates human rankings for the same paraphrases.
</nextsent>
<nextsent>we have created system called ucd-goggle, which uses semantic knowledge acquired from google n-grams together with human-preferencesmined from training data.
</nextsent>
<nextsent>three major components are involved in our system: b-score,produced by bayesian algorithm using semantic knowledge from the n-grams corpus with smoothing layer of additional inference; t -score captures human preferences observed in the tail distribution of training data; and p-score captures pairwise paraphrase preferences calculated from the training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1465">
<title id=" P98-2163.xml">recognition of the coherence relation between tel inked clauses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our linguistic and pragmatic om- petence nables us to read inconceivable relations even when two clauses are co present without any overt cues, i.e., in parataxis.
</prevsent>
<prevsent>there has been variety of definitions for coher-ence relations (see (hovy and maier, 1993) for survey).
</prevsent>
</prevsection>
<citsent citstr=" J92-4007 ">
however, the definitions are rather vague and they are often recognized to be underspecified (moore and pollack, 1992; <papid> J92-4007 </papid>fukumoto and tsujii, 1994).<papid> C94-2192 </papid></citsent>
<aftsection>
<nextsent>this paper attempts to explicate how such coherence relations arise between segments of dis-course.
</nextsent>
<nextsent>we focus on re-linkage in japanese - - translational equivalent of english and-linkage, since mere para taxis ranges over too widely to capture the underlying principles on the coherence relations.
</nextsent>
<nextsent>we consider that coherence relations are cate-gories each of which has its prototypical instances and marginal ones.
</nextsent>
<nextsent>as with all instances of catego- riz ations, the prototypical cases of each relation are clearly distinguishable from one another.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1466">
<title id=" P98-2163.xml">recognition of the coherence relation between tel inked clauses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our linguistic and pragmatic om- petence nables us to read inconceivable relations even when two clauses are co present without any overt cues, i.e., in parataxis.
</prevsent>
<prevsent>there has been variety of definitions for coher-ence relations (see (hovy and maier, 1993) for survey).
</prevsent>
</prevsection>
<citsent citstr=" C94-2192 ">
however, the definitions are rather vague and they are often recognized to be underspecified (moore and pollack, 1992; <papid> J92-4007 </papid>fukumoto and tsujii, 1994).<papid> C94-2192 </papid></citsent>
<aftsection>
<nextsent>this paper attempts to explicate how such coherence relations arise between segments of dis-course.
</nextsent>
<nextsent>we focus on re-linkage in japanese - - translational equivalent of english and-linkage, since mere para taxis ranges over too widely to capture the underlying principles on the coherence relations.
</nextsent>
<nextsent>we consider that coherence relations are cate-gories each of which has its prototypical instances and marginal ones.
</nextsent>
<nextsent>as with all instances of catego- riz ations, the prototypical cases of each relation are clearly distinguishable from one another.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1467">
<title id=" P98-2163.xml">recognition of the coherence relation between tel inked clauses </title>
<section> 100 100  </section>
<citcontext>
<prevsection>
<prevsent>since we have focused on te-linkage in this paper, we need not to consider how clauses are combined.
</prevsent>
<prevsent>however, to detect the discourse structure, we need to extend the method so as to deal with the relations between sentences.
</prevsent>
</prevsection>
<citsent citstr=" C94-2183 ">
we must estimate some kind of reliable scores among possible segments and choose the relation having the maximum score (kurohashi and nagao, 1994).<papid> C94-2183 </papid></citsent>
<aftsection>
<nextsent>these issues remain to be studied in the future.
</nextsent>
<nextsent>6 summary.
</nextsent>
<nextsent>since the semantic relations exhibited by re-linkage vary so diversely, it has been claimed that the inter-preter must infer the intended relationship on the basis of extra linguistic knowledge.
</nextsent>
<nextsent>the particulars of individual common sense knowledge are crucial to understanding any discourse (hobbs et al, 1993; asher and lascarides, 1995).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1468">
<title id=" P98-2172.xml">reference resolution beyond coreference a conceptual frame and its application </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but here mrs are also characterized by an intrinsic activation factor, evolving along the text, which cannot be man-aged in the coreference paradigm.
</prevsent>
<prevsent>1.5 activation.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
the activation of an mr is computed accord-ing to salience factors (this technique is de-scribed for instance by lappin and leass (1994)).<papid> J94-4002 </papid></citsent>
<aftsection>
<nextsent>our salience factors are: de-activation in time, re-activation by various types of re, re-activation according to the function of there.
</nextsent>
<nextsent>among the mrs which pass the selection, activation is used to decide whether the current re is added to an mr (the most active) or if new mr is created.
</nextsent>
<nextsent>activation is thus dy-namic factor, which changes for each mr ac-cording to the position in the text and the pre-vious reference resolution decisions.
</nextsent>
<nextsent>theoretical studies of discourse processing have long been advocating use of various rep-resentations for discourse referents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1469">
<title id=" P98-2172.xml">reference resolution beyond coreference a conceptual frame and its application </title>
<section> comparison with other works.  </section>
<citcontext>
<prevsection>
<prevsent>evans (1985) and recanati (1993) are both close to our pro-posals, however they neither give computa-tional implementation nor an evaluation on real texts.
</prevsent>
<prevsent>sidner work (1979) on focus led to salience factors and activations, but proved too demanding for an unrestricted use.
</prevsent>
</prevsection>
<citsent citstr=" M95-1017 ">
a more operational system using se-mantic representation of referents is for in-stance lasie (gaizauskas et al 1995), <papid> M95-1017 </papid>pre-sented at muc-6, which relies however lot on task-dependent knowledge.</citsent>
<aftsection>
<nextsent>the system doesn seem to use activation cues.
</nextsent>
<nextsent>another system (luperfoy 1992) <papid> P92-1004 </papid>uses  discourse pegs  to model referents and was applied successfully to man-machine dialogue task.</nextsent>
<nextsent>from theoretical point of view, the model presented by appelt and kronfeld (1987) is in its background close to ours.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1470">
<title id=" P98-2172.xml">reference resolution beyond coreference a conceptual frame and its application </title>
<section> comparison with other works.  </section>
<citcontext>
<prevsection>
<prevsent>a more operational system using se-mantic representation of referents is for in-stance lasie (gaizauskas et al 1995), <papid> M95-1017 </papid>pre-sented at muc-6, which relies however lot on task-dependent knowledge.</prevsent>
<prevsent>the system doesn seem to use activation cues.</prevsent>
</prevsection>
<citsent citstr=" P92-1004 ">
another system (luperfoy 1992) <papid> P92-1004 </papid>uses  discourse pegs  to model referents and was applied successfully to man-machine dialogue task.</citsent>
<aftsection>
<nextsent>from theoretical point of view, the model presented by appelt and kronfeld (1987) is in its background close to ours.
</nextsent>
<nextsent>be-ing further developed according to the speech acts theory, it relies however on models of in-tentions and beliefs of communicating agents which seem uneasy to implement for discourse understanding.
</nextsent>
<nextsent>2 .2 robust , lower - leve systems some of the robust approaches derive from anaphora resolution (e.g., boguraev and ken-nedy (1996)) because the antecedent / ana-phoric links are particular sort of coreference links, which disambiguate pronouns.
</nextsent>
<nextsent>most of these systems however emain within the co-reference paradigm, as defined by the muc-6 coreference task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1471">
<title id=" P98-2172.xml">reference resolution beyond coreference a conceptual frame and its application </title>
<section> comparison with other works.  </section>
<citcontext>
<prevsection>
<prevsent>most of these systems however emain within the co-reference paradigm, as defined by the muc-6 coreference task.
</prevsent>
<prevsent>numerous low-level tech-niques have been developed, using generally pattern-matching between potentially corefer- ent strings (e.g., mccarthy and lehnert 1995).
</prevsent>
</prevsection>
<citsent citstr=" M95-1010 ">
an interesting solution has been pro-posed by lin (1995) <papid> M95-1010 </papid>using constraint solving to group res into mrs. while this idea fits the mr paradigm, it doesn work well incremen-tally, which makes use of activation impossible.</citsent>
<aftsection>
<nextsent>2.3 advantages of the mr paradigm.
</nextsent>
<nextsent>grouping res into mrs brings decisive ad- 1049 vantage even without conceptual knowledge.
</nextsent>
<nextsent>first, it suppresses an artificial ambiguity of coreference solution: if re1 and re2 areal- ready known as co referent, coref(re1, re2), there is no conceptual difference between coref(re3, re1) and coref(re3, re2), so these two possibilities houldn be examined sepa-rately.
</nextsent>
<nextsent>moreover, the system of coreference links makes it very time-consuming to find out whether rei andrej are co referent, whereas mrs provide reusable storing of all the already acquired information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1473">
<title id=" S10-1064.xml">cityudac disambiguating sentiment ambiguous adjectives within context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>contextual polarity based on sentiment lexicon.
</prevsent>
<prevsent>the results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
in recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the nlp field (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>pang et al, 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>hu and liu, 2004; pang and lee, 2008).</citsent>
<aftsection>
<nextsent>it has many applications such as social media monitoring, market research, and public relations.
</nextsent>
<nextsent>some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context.
</nextsent>
<nextsent>such words can be called dynamic sentiment-ambiguous adjectives (saas).
</nextsent>
<nextsent>however, saas have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1474">
<title id=" S10-1064.xml">cityudac disambiguating sentiment ambiguous adjectives within context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>contextual polarity based on sentiment lexicon.
</prevsent>
<prevsent>the results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
in recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the nlp field (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>pang et al, 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>hu and liu, 2004; pang and lee, 2008).</citsent>
<aftsection>
<nextsent>it has many applications such as social media monitoring, market research, and public relations.
</nextsent>
<nextsent>some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context.
</nextsent>
<nextsent>such words can be called dynamic sentiment-ambiguous adjectives (saas).
</nextsent>
<nextsent>however, saas have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1475">
<title id=" S10-1064.xml">cityudac disambiguating sentiment ambiguous adjectives within context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>contextual polarity based on sentiment lexicon.
</prevsent>
<prevsent>the results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
in recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the nlp field (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>pang et al, 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>hu and liu, 2004; pang and lee, 2008).</citsent>
<aftsection>
<nextsent>it has many applications such as social media monitoring, market research, and public relations.
</nextsent>
<nextsent>some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context.
</nextsent>
<nextsent>such words can be called dynamic sentiment-ambiguous adjectives (saas).
</nextsent>
<nextsent>however, saas have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1476">
<title id=" S10-1064.xml">cityudac disambiguating sentiment ambiguous adjectives within context </title>
<section> our approach for disambiguating.  </section>
<citcontext>
<prevsection>
<prevsent>saas to disambiguating saas, we use the maximum entropy algorithm and the sentiment lexicon based method, and also combine them together.
</prevsent>
<prevsent>3.1 the maximum entropy-based method.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
maximum entropy classification (maxent) is technique which has proven effective in number of natural language processing applications (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>le zhangs maximum entropy tool2 is used for classification.
</nextsent>
<nextsent>the chinese sentences are segmented into words using production segmentation system.
</nextsent>
<nextsent>unigrams of words are used as basic features for classification.
</nextsent>
<nextsent>bigrams are also tried, but does not show improvement, and thus are not described in details here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1477">
<title id=" P98-2146.xml">using language resources in an intelligent tutoring system for french </title>
<section> geta used resources.  </section>
<citcontext>
<prevsection>
<prevsent>this allows the teacher to take responsibility of the degree of unstructured or of focused learning.
</prevsent>
<prevsent>for many years geta has been working on mt systems from and into french.
</prevsent>
</prevsection>
<citsent citstr=" C94-1070 ">
an impressive core of linguistic knowledge is available but has not yet been experimented on in building language learning software, though work is underway for integration of heterogeneous nlp components, boitet &amp; seligman (1994).<papid> C94-1070 </papid></citsent>
<aftsection>
<nextsent>ariane for example, uses special purpose rule-writing formalisms for each of its morphological and lexical modules both for analysis and for generation, with strict separation of algorithmic and linguistic knowledge, hutchins &amp; somers (1992).
</nextsent>
<nextsent>the following modules from geta were used in our experiment 2 : a. morphological agent.
</nextsent>
<nextsent>-atef for the morphological analysis sub- agent.
</nextsent>
<nextsent>-sygmor for the morpho logica generation sub-agent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1478">
<title id=" P99-1040.xml">automatic detection of poor speech recognition at the dialogue level </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to improve performance through such adaptation, system must first be able to identify, in real time, salient properties of an ongoing dialogue that call for some useful change in system strategy.
</prevsent>
<prevsent>in other words, adaptive systems hould try to auto-matically identify actionable properties of ongoing dialogues.
</prevsent>
</prevsection>
<citsent citstr=" P98-2129 ">
previous work has shown that speech recognition performance is an important predictor of user satis-faction, and that changes in dialogue behavior im-pact speech recognition performance (walker et al, 1998b; litman et al, 1998; <papid> P98-2129 </papid>kamm et al, 1998).</citsent>
<aftsection>
<nextsent>therefore, in this work, we focus on the task of au-tomatically detecting poor speech recognition per-formance in several spoken dialogue systems devel-oped at at&t; labs.
</nextsent>
<nextsent>rather than hand-crafting rules that classify speech recognition performance in an ongoing dialogue, we take machine learning ap-proach.
</nextsent>
<nextsent>we begin with collection of system logs from actual dialogues that were labeled by humans as having had  good  or  bad  speech recognition (the training set).
</nextsent>
<nextsent>we then apply standard machine learning algorithms to this training set in the hope of discovering, in principled manner, classifiers that can automatically detect poor speech recogni-tion during novel dialogues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1482">
<title id=" P99-1040.xml">automatic detection of poor speech recognition at the dialogue level </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>this leads to 52% baseline accuracy.
</prevsent>
<prevsent>the rejection% accuracy rates arise from classifier that has access to the percentage of dia-logue utterances in which the system played re-jection message to the user.
</prevsent>
</prevsection>
<citsent citstr=" H92-1009 ">
previous research sug-gests that this acoustic feature predicts misrecogni- tions because users modify their pronunciation response to system rejection messages in such way as to lead to further misunderstandings (shriberg et al., 1992; <papid> H92-1009 </papid>levow, 1998).<papid> P98-1122 </papid></citsent>
<aftsection>
<nextsent>however, despite our ex-pectations, the rejection% accuracy rate is not better than the baseline at our desired level of sta-tistical significance.
</nextsent>
<nextsent>using the efficiency features does improve the performance of the classifier significantly above the baseline (61%).
</nextsent>
<nextsent>these features, however, tend to reflect the particular experimental tasks that the users were doing.
</nextsent>
<nextsent>the exp-params (experimental parameters) features are even more specific to this dialogue corpus than the efficiency features: these features consist of the name of the system, the experimen- 5accuracy rates are statistically significantly different when the accuracies plus or minus twice the standard error do not overlap (cohen, 1995), p. 134.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1483">
<title id=" P99-1040.xml">automatic detection of poor speech recognition at the dialogue level </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>this leads to 52% baseline accuracy.
</prevsent>
<prevsent>the rejection% accuracy rates arise from classifier that has access to the percentage of dia-logue utterances in which the system played re-jection message to the user.
</prevsent>
</prevsection>
<citsent citstr=" P98-1122 ">
previous research sug-gests that this acoustic feature predicts misrecogni- tions because users modify their pronunciation response to system rejection messages in such way as to lead to further misunderstandings (shriberg et al., 1992; <papid> H92-1009 </papid>levow, 1998).<papid> P98-1122 </papid></citsent>
<aftsection>
<nextsent>however, despite our ex-pectations, the rejection% accuracy rate is not better than the baseline at our desired level of sta-tistical significance.
</nextsent>
<nextsent>using the efficiency features does improve the performance of the classifier significantly above the baseline (61%).
</nextsent>
<nextsent>these features, however, tend to reflect the particular experimental tasks that the users were doing.
</nextsent>
<nextsent>the exp-params (experimental parameters) features are even more specific to this dialogue corpus than the efficiency features: these features consist of the name of the system, the experimen- 5accuracy rates are statistically significantly different when the accuracies plus or minus twice the standard error do not overlap (cohen, 1995), p. 134.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1486">
<title id=" S10-1020.xml">corry a system for coreference resolution </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>corry is system for coreference resolution in english.
</prevsent>
<prevsent>it supports both local (soon et al.
</prevsent>
</prevsection>
<citsent citstr=" N07-1030 ">
(2001)-style) and global (integer linear programming, denis and baldridge (2007) <papid> N07-1030 </papid>style) models of coreference.</citsent>
<aftsection>
<nextsent>corry relies on rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons.
</nextsent>
<nextsent>three runs have been submitted for the semeval task 1 on coreference resolution (recasens et al, 2010), optimizing corrys performance for blanc (recasens and hovy, in prep), muc (vilain et al, 1995) <papid> M95-1005 </papid>and ceaf (luo, 2005).<papid> H05-1004 </papid></nextsent>
<nextsent>corry runs have shown the best performance level among all the systems in their track for the corresponding metric.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1487">
<title id=" S10-1020.xml">corry a system for coreference resolution </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>(2001)-style) and global (integer linear programming, denis and baldridge (2007) <papid> N07-1030 </papid>style) models of coreference.</prevsent>
<prevsent>corry relies on rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons.</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
three runs have been submitted for the semeval task 1 on coreference resolution (recasens et al, 2010), optimizing corrys performance for blanc (recasens and hovy, in prep), muc (vilain et al, 1995) <papid> M95-1005 </papid>and ceaf (luo, 2005).<papid> H05-1004 </papid></citsent>
<aftsection>
<nextsent>corry runs have shown the best performance level among all the systems in their track for the corresponding metric.
</nextsent>
<nextsent>corry is system for coreference resolution in english.
</nextsent>
<nextsent>it supports both local (soon et al (2001)-<papid> J01-4004 </papid>style) and global (ilp, denis and baldridge (2007)-<papid> N07-1030 </papid>style) models of coreference.</nextsent>
<nextsent>the backbone of the system is family of svm classifiers for pairs of mentions:each mention type receives its own classifier.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1489">
<title id=" S10-1020.xml">corry a system for coreference resolution </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>(2001)-style) and global (integer linear programming, denis and baldridge (2007) <papid> N07-1030 </papid>style) models of coreference.</prevsent>
<prevsent>corry relies on rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons.</prevsent>
</prevsection>
<citsent citstr=" H05-1004 ">
three runs have been submitted for the semeval task 1 on coreference resolution (recasens et al, 2010), optimizing corrys performance for blanc (recasens and hovy, in prep), muc (vilain et al, 1995) <papid> M95-1005 </papid>and ceaf (luo, 2005).<papid> H05-1004 </papid></citsent>
<aftsection>
<nextsent>corry runs have shown the best performance level among all the systems in their track for the corresponding metric.
</nextsent>
<nextsent>corry is system for coreference resolution in english.
</nextsent>
<nextsent>it supports both local (soon et al (2001)-<papid> J01-4004 </papid>style) and global (ilp, denis and baldridge (2007)-<papid> N07-1030 </papid>style) models of coreference.</nextsent>
<nextsent>the backbone of the system is family of svm classifiers for pairs of mentions:each mention type receives its own classifier.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1491">
<title id=" S10-1020.xml">corry a system for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>corry runs have shown the best performance level among all the systems in their track for the corresponding metric.
</prevsent>
<prevsent>corry is system for coreference resolution in english.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
it supports both local (soon et al (2001)-<papid> J01-4004 </papid>style) and global (ilp, denis and baldridge (2007)-<papid> N07-1030 </papid>style) models of coreference.</citsent>
<aftsection>
<nextsent>the backbone of the system is family of svm classifiers for pairs of mentions:each mention type receives its own classifier.
</nextsent>
<nextsent>a separate anaphoricity classifier is learned for the ilpsetting.
</nextsent>
<nextsent>corry relies on rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons.
</nextsent>
<nextsent>corry has only participated in the open?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1493">
<title id=" S10-1020.xml">corry a system for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>corry relies on rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons.
</prevsent>
<prevsent>corry has only participated in the open?
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
setting,as it has already number of preprocessing modules integrated into the system: the stanford nlp toolkit for parsing (klein and manning, 2003) <papid> P03-1054 </papid>andne-tagging (finkel et al, 2005), <papid> P05-1045 </papid>wordnet for semantic classes and the u.s. census data for assigning gender values to person names.three runs have been submitted for these meval task 1 on coreference resolution, optimizing corrys performance for blanc, muc and ceaf.</citsent>
<aftsection>
<nextsent>the runs differ with respect to the model (local for blanc, global for muc and ceaf) and the definition of mention types.
</nextsent>
<nextsent>in our previous study (uryupina, 2008) <papid> L08-1049 </papid>we have shown that up to 35% recall and 20% precision errors in coreference resolution for muc corpora aredue to inaccurate mention detection.</nextsent>
<nextsent>we have therefore invested substantial efforts into our mention detection module.most state-of-the-art coreference resolution systems operate either on gold markables or on the output of an ace-style mention detection module.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1494">
<title id=" S10-1020.xml">corry a system for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>corry relies on rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons.
</prevsent>
<prevsent>corry has only participated in the open?
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
setting,as it has already number of preprocessing modules integrated into the system: the stanford nlp toolkit for parsing (klein and manning, 2003) <papid> P03-1054 </papid>andne-tagging (finkel et al, 2005), <papid> P05-1045 </papid>wordnet for semantic classes and the u.s. census data for assigning gender values to person names.three runs have been submitted for these meval task 1 on coreference resolution, optimizing corrys performance for blanc, muc and ceaf.</citsent>
<aftsection>
<nextsent>the runs differ with respect to the model (local for blanc, global for muc and ceaf) and the definition of mention types.
</nextsent>
<nextsent>in our previous study (uryupina, 2008) <papid> L08-1049 </papid>we have shown that up to 35% recall and 20% precision errors in coreference resolution for muc corpora aredue to inaccurate mention detection.</nextsent>
<nextsent>we have therefore invested substantial efforts into our mention detection module.most state-of-the-art coreference resolution systems operate either on gold markables or on the output of an ace-style mention detection module.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1495">
<title id=" S10-1020.xml">corry a system for coreference resolution </title>
<section> preprocessing and mention extraction.  </section>
<citcontext>
<prevsection>
<prevsent>setting,as it has already number of preprocessing modules integrated into the system: the stanford nlp toolkit for parsing (klein and manning, 2003) <papid> P03-1054 </papid>andne-tagging (finkel et al, 2005), <papid> P05-1045 </papid>wordnet for semantic classes and the u.s. census data for assigning gender values to person names.three runs have been submitted for these meval task 1 on coreference resolution, optimizing corrys performance for blanc, muc and ceaf.</prevsent>
<prevsent>the runs differ with respect to the model (local for blanc, global for muc and ceaf) and the definition of mention types.</prevsent>
</prevsection>
<citsent citstr=" L08-1049 ">
in our previous study (uryupina, 2008) <papid> L08-1049 </papid>we have shown that up to 35% recall and 20% precision errors in coreference resolution for muc corpora aredue to inaccurate mention detection.</citsent>
<aftsection>
<nextsent>we have therefore invested substantial efforts into our mention detection module.most state-of-the-art coreference resolution systems operate either on gold markables or on the output of an ace-style mention detection module.
</nextsent>
<nextsent>we are not aware of extensive studies on mention extraction algorithms for such datasets as semeval (ontonotes) where mentions are complex nps not constrained with respect to their semantic types.we relyon the stanford nlp toolkit for extracting named entities (finkel et al, 2005) <papid> P05-1045 </papid>and parse trees for each sentence (klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>we then merge the output of the ne-tagger and the parser to create list of mentions in the following way: 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1498">
<title id=" S10-1020.xml">corry a system for coreference resolution </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>our anaphoricity classifier is used by the ilp model.
</prevsent>
<prevsent>it relies on 26 boolean/continuous features.
</prevsent>
</prevsection>
<citsent citstr=" P03-2012 ">
more details on the classifier itself can be found in (uryupina, 2003).<papid> P03-2012 </papid></citsent>
<aftsection>
<nextsent>1corry supports number of machine learning algorithms: c4.5, timbl, ripper, maxent and svm.
</nextsent>
<nextsent>see uryupina (2006) for comparison of corrys performance with different learners.
</nextsent>
<nextsent>corry supports both global and local views of coreference.
</nextsent>
<nextsent>our evaluation experiments (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1501">
<title id=" S10-1020.xml">corry a system for coreference resolution </title>
<section> modeling.  </section>
<citcontext>
<prevsection>
<prevsent>we, thus, relyon family of classifiers, with the same feature set and the same machine learner.
</prevsent>
<prevsent>the exact definition of mention types is parameter to be determined empirically on the development set.
</prevsent>
</prevsection>
<citsent citstr=" P08-2012 ">
our global model is largely motivated by denis and baldridge (2007), <papid> N07-1030 </papid>denis and baldridge (2008) and finkel and manning(2008).<papid> P08-2012 </papid></citsent>
<aftsection>
<nextsent>following these studies, we use integer linear programming to find the most globally optimal solution, given the decisions made by our coreference and anaphoricity classifiers.
</nextsent>
<nextsent>in general, an ilp problem is determined by an objective function to be maximized (or minimized) and set of task-specific constraints.
</nextsent>
<nextsent>the function is defined by costs link  i,j  , and dnew reflecting potential gains and losses for committing to specific variable assignments.
</nextsent>
<nextsent>we assume that costs can be positive (for pairs of markables that are likely to be coreferent) or negative (for pairs of markables thatare unlikely to be coreferent).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1508">
<title id=" S12-1085.xml">sranjans  semantic textual similarity using maximal weighted bipartite graph matching </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic textual similarity (sts) measures the degree of semantic equivalence between texts.
</prevsent>
<prevsent>the goal of this task is to create unified framework for the evaluation of semantic textual similarity modules and to characterize their impact on nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
the task is part of the semantic evaluation 2012 workshop (agirre et al., 2012).<papid> S12-1051 </papid></citsent>
<aftsection>
<nextsent>sts is related to both textual entailment and paraphrase, but differs in number of ways and it is more directly applicable to number of nlptasks.
</nextsent>
<nextsent>also, sts is graded similarity notion this graded bidirectional nature of sts is useful for nlp tasks such as mt evaluation, information extraction, question answering, and summarization.
</nextsent>
<nextsent>we propose lexical similarity approach to grade the similarity of two sentences, where amaximal weighted bipartite match is found between the tokens of the two sentences.
</nextsent>
<nextsent>the approach is robust enough to apply across different datasets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1509">
<title id=" S12-1085.xml">sranjans  semantic textual similarity using maximal weighted bipartite graph matching </title>
<section> task evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to the above sources, the test datasets also contained the following sources :?
</prevsent>
<prevsent>smt news : this dataset consists of machine translated news conversation sentence pairs.
</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
on wn : this dataset consists of pairs of sentences where the first comes fromontonotes(hovy et al, 2006) <papid> N06-2015 </papid>and the second from wordnet definition.</citsent>
<aftsection>
<nextsent>hence, the sentences are rather phrases.
</nextsent>
<nextsent>3.2 baseline.
</nextsent>
<nextsent>the task organizers have used the following baseline scoring scheme.
</nextsent>
<nextsent>scores are produced using simple word overlap baseline system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1510">
<title id=" S12-1085.xml">sranjans  semantic textual similarity using maximal weighted bipartite graph matching </title>
<section> system 1.  </section>
<citcontext>
<prevsection>
<prevsent>4.2.1 wordnet based word similarity the is-a hierarchy of wordnet is used in calculating the word similarity of two words.
</prevsent>
<prevsent>nouns and verbs have separate is-a hierarchies.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
we use the lin word-sense similarity measure (lin , 1998<papid> P98-2127 </papid>a).</citsent>
<aftsection>
<nextsent>adjectives and adverbs do not have an is-a hierarchy and hence do not figure in the lin similarity measure.
</nextsent>
<nextsent>to disambiguate the wordnet sense of word in sentence, variant of the simplified lesk algorithm (kilgarriff and j. rosenzweig , 2000) is used.
</nextsent>
<nextsent>wordnet based word similarity has the following drawbacks : ? sparse in named entity content : similarity of named entities with other words becomes infeasible to calculate.
</nextsent>
<nextsent>doesnt support cross-pos similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1514">
<title id=" S12-1085.xml">sranjans  semantic textual similarity using maximal weighted bipartite graph matching </title>
<section> system 2.  </section>
<citcontext>
<prevsection>
<prevsent>the tokens resulting from this can be multi-word because of named entities.
</prevsent>
<prevsent>this tokenization strategy gives us the best results among all our three runs.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
for capturing and normalizing the abovementioned expressions, we make use of the stanford ner toolkit (finkel et al, 2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>some normalized samples are mentioned in figure 2.
</nextsent>
<nextsent>when grading the similarity of multi-word tokens, we use second level maximal bipartite match, which is normalized by the smaller of the two multi-word token lengths.
</nextsent>
<nextsent>thus, similarity between two multi-word tokens t1 and t2 is defined as: sim(t1, t2) = maximalbipartitematchsum(t1,t2) min(words(t1),words(t2)) this was done to ensure that complete named entity in the first sentence matches exactly with partial named entity (indicating the same entity as the first) in the second sentence.
</nextsent>
<nextsent>for eg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1515">
<title id=" S12-1085.xml">sranjans  semantic textual similarity using maximal weighted bipartite graph matching </title>
<section> system 3.  </section>
<citcontext>
<prevsection>
<prevsent>583
</prevsent>
<prevsent>in system 3, in addition to system 2, we heuris tically capture compound nouns, adjectivally and numerically modified words like passenger plane?, easy job?, 10 years?
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
etc. using the pos based regular expression [jj |nn |cd]nn pos tagging is done using the stanford pos tagger toolkit (toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>to make matching more context dependent, rather than just bag of words approach, we naively attempt to capture the similarity of the contexts of two tokens.
</nextsent>
<nextsent>we define the context of word in sentence as all the words in the sentence which are grammatically related toit.
</nextsent>
<nextsent>the grammatical relations are all the collapsed dependencies produced by the stanford dependency parser (marneffe et al, 2006).
</nextsent>
<nextsent>the context of multi-word token is defined as the union of contexts of all the words in it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1516">
<title id=" P98-2222.xml">using leading text for news summaries evaluation results and implications for commercial summarization applications </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>impact of lead restrictions on boo- lean retrieval quality (20-query test) search able lead document processing software consists of 500-statement pl1 program and 23- rule sentence and paragraph boundary recognition grammar, and operates in mainfiame mvs envi-ronment.
</prevsent>
<prevsent>search able lead processes over 500,000 characters (90 news documents) per cpu second.
</prevsent>
</prevsection>
<citsent citstr=" C92-3162 ">
there is growing body of research into approaches for generating text summaries, including approaches based on sentence xtraction (kupiec et al, 1995), text generation from templates (mckeown and radev, 1995) and machine-assisted abstraction (tsou et al, 1992).<papid> C92-3162 </papid></citsent>
<aftsection>
<nextsent>brandow et al (1995) reported on sentence xtraction approach called the auto- marie news extraction system, or anes.
</nextsent>
<nextsent>anes combined statistical corpus analysis, signature word selection and sentence weighting to select sentences for inclusion in summaries.
</nextsent>
<nextsent>by varying the number of sentences elected, anes-generated extracts could meet argeted summary lengths.
</nextsent>
<nextsent>anes was evaluated using corpus of 250 docu-ments from newswire, magazine and newspaper publications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1517">
<title id=" P98-2222.xml">using leading text for news summaries evaluation results and implications for commercial summarization applications </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>a dynamic summary generator, per-haps using readers  queries to guide it, can help readers focus on those parts of document that are most relevant to them.
</prevsent>
<prevsent>third, hybrid approach to summary generation may improve acceptability for news documents.
</prevsent>
</prevsection>
<citsent citstr=" A97-1042 ">
lin &amp; hovy (1997) <papid> A97-1042 </papid>describe methods for identifying the likely locations of topic-bearing sentences.</citsent>
<aftsection>
<nextsent>comparing the content of leading text extracts to predictions of topic-bearing sentences may help us predict where leading text fails as summary so that we can direct more sophisticated approaches to those documents.
</nextsent>
<nextsent>the commercial use of leading text summaries such as search able lead by no means suggests that news summarization is solved problem.
</nextsent>
<nextsent>there are number of data types where leading text has di-minished or no value as summary.
</nextsent>
<nextsent>where it does succeed, an approach like search able lead may serve as starting point for improved leading text summaries or as benchmark for comparing alter-natives that are not restricted by the limits inherent in leading text approaches tosummarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1518">
<title id=" S12-1039.xml">uconcordia clac negation focus detection at sem 2012 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to assess factual information as asserted or not, it is important to distinguish the difference between (1) (a) newt gingrich not conceding race after losing florida primary(b) newt gingrich conceding race after losing florida primary this distinction is important and cannot be properly inferred from the surrounding context, not conceding race after losing is in fact contrary to expectation in the original headline (1a), and the constructed (1b) is more likely in isolation.
</prevsent>
<prevsent>negation has been addressed as task in itself, rather than as component of other tasks in recent shared tasks and workshops.
</prevsent>
</prevsection>
<citsent citstr=" W10-3001 ">
detection of negation cues and negation scope at conll (farkas et al, 2010), <papid> W10-3001 </papid>bionlp (kim et al, 2011) <papid> W11-1802 </papid>and the negation and speculation in nlp workshop (morante and sporleder, 2010) laid the foundation for the *sem 2012 shared task.</citsent>
<aftsection>
<nextsent>while the scope detection has.
</nextsent>
<nextsent>been extended to fictional text in this task, an important progression from the newspaper and biomedical genres, the newly defined focus detection for negation introduces the important question: what isthe intended opposition in (1a)?
</nextsent>
<nextsent>the negation trigger is not, the scope of the negation is the entire verb phrase, but which aspect of the verb phrase is underscored as being at variance with reality, that is, which of the following possible (for the sake of linguistic argument only) continuations is the more likely one: (2) . . .
</nextsent>
<nextsent>, santo rum does.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1519">
<title id=" S12-1039.xml">uconcordia clac negation focus detection at sem 2012 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to assess factual information as asserted or not, it is important to distinguish the difference between (1) (a) newt gingrich not conceding race after losing florida primary(b) newt gingrich conceding race after losing florida primary this distinction is important and cannot be properly inferred from the surrounding context, not conceding race after losing is in fact contrary to expectation in the original headline (1a), and the constructed (1b) is more likely in isolation.
</prevsent>
<prevsent>negation has been addressed as task in itself, rather than as component of other tasks in recent shared tasks and workshops.
</prevsent>
</prevsection>
<citsent citstr=" W11-1802 ">
detection of negation cues and negation scope at conll (farkas et al, 2010), <papid> W10-3001 </papid>bionlp (kim et al, 2011) <papid> W11-1802 </papid>and the negation and speculation in nlp workshop (morante and sporleder, 2010) laid the foundation for the *sem 2012 shared task.</citsent>
<aftsection>
<nextsent>while the scope detection has.
</nextsent>
<nextsent>been extended to fictional text in this task, an important progression from the newspaper and biomedical genres, the newly defined focus detection for negation introduces the important question: what isthe intended opposition in (1a)?
</nextsent>
<nextsent>the negation trigger is not, the scope of the negation is the entire verb phrase, but which aspect of the verb phrase is underscored as being at variance with reality, that is, which of the following possible (for the sake of linguistic argument only) continuations is the more likely one: (2) . . .
</nextsent>
<nextsent>, santo rum does.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1520">
<title id=" S12-1039.xml">uconcordia clac negation focus detection at sem 2012 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is important to note hat this notion of focus is not syntactically determined as shown in (3) (eventhough we use syntactic heuristics here to approximate it) but pragmatically and it correlates with pronunciation stress, as discussed in linguistics by (han and romero, 2001).
</prevsent>
<prevsent>more recently, focus negation has been identified as special use (poletto, 2008).
</prevsent>
</prevsection>
<citsent citstr=" P11-1059 ">
the difference of scope and focus of negation are elaborated by (partee, 1993), and have been used for computational use by (blanco and moldovan, 2011).<papid> P11-1059 </papid></citsent>
<aftsection>
<nextsent>the *sem 2012 task 2 on focus detection builds on recent negation scope detection capabilities and introduces gold standard to identify the focus item.
</nextsent>
<nextsent>focus of negation is annotated over 3,993 sentences in the wsj section of the penn treebank marked with mneg in propbank.
</nextsent>
<nextsent>it accounts for verbal, analytical and clausal relation to negation trigger; the role most likely to correspond to the focus was selected as focus.
</nextsent>
<nextsent>all sentences of the training data contain negation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1522">
<title id=" S12-1039.xml">uconcordia clac negation focus detection at sem 2012 </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, the trigger terms and conditions are heavily stacked with biomedical domain specific terms.
</prevsent>
<prevsent>outside the biomedical text community, sentiment and opinion analysis research features negation detection (wilson, 2008).
</prevsent>
</prevsection>
<citsent citstr=" W10-3111 ">
current gold standard annotations for explicit negation as well as related phenomena include time bank (pustejovsky et al, 2003), mpqa (wiebe et al, 2005), and bio-scope (vincze et al, 2008).(wiegand et al, 2010) <papid> W10-3111 </papid>presents flat feature combination approach of features of different granularity and analytic sophistication, since in opinion mining the boundary between negation and negative expressions is fluid.</citsent>
<aftsection>
<nextsent>clac labs?
</nextsent>
<nextsent>general, lightweight negation module is intended to be embedded in any processing pipeline.
</nextsent>
<nextsent>the heuristics-based system is composed of three modules for the gate (cunninghamet al, 2011) environment: the first component detects and annotates explicit negation cues present inthe corpus, the second component detects and annotates the syntactic scope of the detected instances of verbal negation, and the third component implements focus heuristics for negation.
</nextsent>
<nextsent>the first two steps were developed independently, drawing ondata from mpqa (wiebe et al, 2005) and time bank (pustejovsky et al, 2003) with validation on bio-scope (vincze et al, 2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1523">
<title id=" S12-1039.xml">uconcordia clac negation focus detection at sem 2012 </title>
<section> clacs neg focus.  </section>
<citcontext>
<prevsection>
<prevsent>parser-based, our focus detection pipeline requires as input entire sentences.
</prevsent>
<prevsent>therefore, the first step requires the extraction of each sentence utilizing the supplied token numbers and save them in the correctformat.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
the system then performs standard preprocessing: sentence splitting, tokenization, parsing using the stanford parser (klein and manning, 2003;<papid> P03-1054 </papid>de marneffe and manning, 2006) and morphological preprocessing.</citsent>
<aftsection>
<nextsent>note that neg focus does not useany propbank annotations nor other provided training annotations, resulting in an independent, parser based stand-alone module.
</nextsent>
<nextsent>3.2 detection of negation triggers.
</nextsent>
<nextsent>the focus detection task only considers the explicit negation cues not, nor, never.
</nextsent>
<nextsent>the first step in neg focus is thus to identify these triggers in the sentences using an explicit negation trigger word list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1526">
<title id=" P98-2155.xml">constituent based accent prediction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a better understand-ing of the effects of discourse context on accentual variation is needed not only to fully model this fun-damental prosodic feature for text-to-speech (tts) synthesis systems, but also to further the integration of prosody into speech understanding and concept- to-speech (cts) synthesis ystems at the appropri-ate level of linguistic representation.
</prevsent>
<prevsent>this paper presents an empirically motivated the-ory of the discourse focusing function of accent.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
the theory describes for the first time the interacting contributions to accent prediction made by factors related to the local and global attentional status of discourse referents in discourse model (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>the ability of the focusing features to predict accent for blind test corpus is examined using machine learning.
</nextsent>
<nextsent>because attentional status is property of referring expressions, novel ap-proach to accent prediction is proposed to allow for the integration of word-based and constituent-based linguistic features in the models to be learned.
</nextsent>
<nextsent>the task of accent assignment is redefined as the prediction of patterns of deviation from citation form accentuation.
</nextsent>
<nextsent>crucially, these deviations are captured at the constituent level.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1527">
<title id=" P98-2155.xml">constituent based accent prediction </title>
<section> accent and attention.  </section>
<citcontext>
<prevsection>
<prevsent>we propose new theory of the relationship be-tween accent and attention, based on an enriched taxonomy of given/new information status provided by both the local (centering) and global (fo- cus stack model) attentional state models in grosz and sidner discourse modeling theory (1986).
</prevsent>
<prevsent>939 analysis of 20-minute spontaneous story-telling monologue identified separate but interacting con-tributions of grammatical function, form of refer- ring expression and accentuation 2 in conveying the attentional status of discourse referent.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
these in-teractions can be formally expressed in the frame-work of attentional modeling by the following prin-ciples of interpretation: ? the lexical form of referring expres-sion indicates the level of attentional processing, i.e., pronouns involve local focusing while full lex-ical forms involve global focusing (grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>the grammatical function of referring ex-pression reflects the local attentional status of the referent, i.e., subject position generally holds the highest ranking member of the forward-looking centers list (cf list), while direct object holds the next highest ranking member of the cf list (grosz et al, 1995; <papid> J95-2003 </papid>kameyama, 1985).</nextsent>
<nextsent>the accenting of referring expression serves as an inference cueto shift attention to new backward-looking center (cb), or to mark the global (re)introduction a referent; lack of ac-cent serves as an inference cueto maintain atten- tional focus on the cb, cf list members or global referents (nakatani, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1530">
<title id=" P98-2155.xml">constituent based accent prediction </title>
<section> constituent-based experiments.  </section>
<citcontext>
<prevsection>
<prevsent>minimal, non-recursive 940 accent class tts-assigned accenting actual accenting citation little shopping area little shopping area we we supra reduced one pretty nice ambiance the green line subway yet another right turn one pretty nice ambiance the green line subway yet another right turn shift very fast five minute lunch very fast five minute lunch table 3: examples of citation-based accent classes.
</prevsent>
<prevsent>accented words appear in boldface.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
np constituents, referred to as basenps, are au-tomatically identified using collins  (1996) <papid> P96-1025 </papid>lexical dependency parser.</citsent>
<aftsection>
<nextsent>in the following complex np, basenps appear in square brackets: \[the brown stone apartment building\] on \[the corner\] of\[beacon and mass ave\].
</nextsent>
<nextsent>basenps are semi-automatically la-beled for lexical, syntactic, local focus and global focus features.
</nextsent>
<nextsent>table 2 provides summary corpus statistics.
</nextsent>
<nextsent>a rule-based machine learning program, corpus measure total no.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1531">
<title id=" P98-2155.xml">constituent based accent prediction </title>
<section> constituent-based experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for example, the lemma sequence for the np, the harvard square stop, is {the, harvard, square, t, stop}.
</prevsent>
<prevsent>the corresponding broad class sequence is {determiner, noun, noun, noun, noun}.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
broad class tags are derived using brill (1995) <papid> J95-4004 </papid>part-of-speech tagger, and word lemma information is produced by newtts (sproat, 1997).</citsent>
<aftsection>
<nextsent>pos information is used to assign accenting in nearly all speech synthesis ystems.
</nextsent>
<nextsent>initial word- based experiments on our corpus showed that broad class categories performed slightly better than both the function-content distinction and the pos tags themselves, giving 69%-81% correct word predic-tions (nakatani, 1997).
</nextsent>
<nextsent>3.3.2 syntactic constituency features the clause type feature represents global syn-tactic constituency information, while the basenp type feature represents local or np-internal syntac-tic constituency information.
</nextsent>
<nextsent>four clause types are coded: matrix, subordinate, predicate complement and relative.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1534">
<title id=" P98-2155.xml">constituent based accent prediction </title>
<section> constituent-based experiments.  </section>
<citcontext>
<prevsection>
<prevsent>hand-labeled grammatical functions include sttbject, direct object, indirect ob-ject, predicate complement, adfimct.
</prevsent>
<prevsent>form of ex-pression feature values are .adverbial noun, cardi-nal, definite np, demonstrative np, indefinite np, pronoun, proper name, quantifier np, verbal noun, etc. 3.3.4 global focus feature the global focusing status of basenps is computed using two sets of analyses: discourse segmenta- tions and coreference coding.
</prevsent>
</prevsection>
<citsent citstr=" P96-1038 ">
expert discourse structure analyses are used to derive consensus segment ations, consisting of discourse bound-aries whose coding all three label ers agreed upon (hirschberg and nakatani, 1996).<papid> P96-1038 </papid></citsent>
<aftsection>
<nextsent>the consensus labels for segment-initial boundaries provide lin-ear segmentation of discourse into discourse seg-ments.
</nextsent>
<nextsent>coreferential relations are coded by two la- belers using dtt (discourse tagging tool) (aone and bennett, 1995).
</nextsent>
<nextsent>to compute coreference chains, only the relation of strict coference is used.
</nextsent>
<nextsent>two nps, npl and np2, are in strict coreference rela-tionship, when np2 occurs after npl in the discourse and realizes the same discourse ntity that is real-ized by npl.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1537">
<title id=" S10-1086.xml">mss investigating the effectiveness of domain combinations and topic features for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the test data also include documents from q&a; site on the www (oc); (2) test data include new senses (called x) that are not defined in dictionary.
</prevsent>
<prevsent>there is much previous research on wsd.
</prevsent>
</prevsection>
<citsent citstr=" D07-1050 ">
in the case of japanese, unsupervised approaches such as extended lesk have performed well (bald win et al , 2010), although they are outperformed by supervised approaches (tanaka et al , 2007;<papid> D07-1050 </papid>murata et al , 2003).</citsent>
<aftsection>
<nextsent>therefore, we selected supervised approach and constructed support vector machines (svm) and maximum entropy (mem)classifiers using common features and topic features.
</nextsent>
<nextsent>we performed extensive experiments to investigate the best combinations of domains for training.we describe the data in section 2, and our system in section 3.
</nextsent>
<nextsent>then in section 4, we show the results and provide some discussion.
</nextsent>
<nextsent>2.1 given data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1538">
<title id=" S10-1086.xml">mss investigating the effectiveness of domain combinations and topic features for word sense disambiguation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>3.1.2 bag-of-words for each target word w, we got all base forms of the content words within the same document or within the same article for newspapers (pn).
</prevsent>
<prevsent>we refer to the model that uses these baseline features as bow.
</prevsent>
</prevsection>
<citsent citstr=" D07-1108 ">
3.1.3 topic feature sin the semeval-2007 english wsd tasks, system incorporating topic features achieved the highest accuracy (cai et al , 2007).<papid> D07-1108 </papid></citsent>
<aftsection>
<nextsent>inspired by (cai et al , 2007), <papid> D07-1108 </papid>we also used topic features.their approach uses bayesian topic models (la tent dirichlet al location: lda) to infer topics in an unsupervised fashion.</nextsent>
<nextsent>then the inferred topics 2we use acc as an abbreviation of accusative postposition.are added as features to reduce the sparsity problem with word-only features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1540">
<title id=" P98-2152.xml">japanese ocr error correction using character shape similarity and statistical language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>if the tok-enized string is not in the dictionary, it is non- word.
</prevsent>
<prevsent>for non-word, correction candidates are re-trieved from the dictionary by approximate string match techniques using context-independent word distance measures such as edit distance (wagner and fischer, 1974) and ngram distance (angell et al, 1983).
</prevsent>
</prevsection>
<citsent citstr=" P96-1010 ">
recently, statistical language models and feature- based method have been used for context-sensitive spelling correction, where errors are corrected con-sidering the context in which the error occurs (church and gale, 1991; mays et al, 1991; golding and schabes, 1996).<papid> P96-1010 </papid></citsent>
<aftsection>
<nextsent>similar techniques are used for correcting the output of english ocrs (tong and evans, 1996) <papid> W96-0108 </papid>and english speech recognizers (ring- ger and allen, 1996).</nextsent>
<nextsent>there are two problems in japanese (and chinese) spelling correction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1541">
<title id=" P98-2152.xml">japanese ocr error correction using character shape similarity and statistical language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for non-word, correction candidates are re-trieved from the dictionary by approximate string match techniques using context-independent word distance measures such as edit distance (wagner and fischer, 1974) and ngram distance (angell et al, 1983).
</prevsent>
<prevsent>recently, statistical language models and feature- based method have been used for context-sensitive spelling correction, where errors are corrected con-sidering the context in which the error occurs (church and gale, 1991; mays et al, 1991; golding and schabes, 1996).<papid> P96-1010 </papid></prevsent>
</prevsection>
<citsent citstr=" W96-0108 ">
similar techniques are used for correcting the output of english ocrs (tong and evans, 1996) <papid> W96-0108 </papid>and english speech recognizers (ring- ger and allen, 1996).</citsent>
<aftsection>
<nextsent>there are two problems in japanese (and chinese) spelling correction.
</nextsent>
<nextsent>the first is the word boundary problem.
</nextsent>
<nextsent>it is impossible to use isolated word error correction techniques because there are no delimiters between words.
</nextsent>
<nextsent>the second is the short word prob-lem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1542">
<title id=" P98-2152.xml">japanese ocr error correction using character shape similarity and statistical language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word distance measures are useless because the average word length is short (  2), and the charac-ter set is large (  3000).
</prevsent>
<prevsent>there are much larger number of one edit distance neighbors for word, compared with english.
</prevsent>
</prevsection>
<citsent citstr=" C96-2136 ">
recently, the first problem was solved by selecting the most likely word sequence from all combinations of exactly and approximately matched words using viterbi-like word segmentation algorithm and sta-tistical anguage model considering unknown words and non-words (nagata, 1996).<papid> C96-2136 </papid></citsent>
<aftsection>
<nextsent>however, the second problem is not solved yet, at least elegantly.
</nextsent>
<nextsent>the so-lution presented in (nagata, 1996) <papid> C96-2136 </papid>which sorts list of one edit distance words considering the context in which it will be placed is inaccurate because the context itself might include some errors.</nextsent>
<nextsent>in this paper, we present context-independent approximate word match method using character shape similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1563">
<title id=" S12-1066.xml">uowshef simplex  lexical simplicity ranking based on contextual and psycho linguistic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system outperforms very strong baseline, and ranked first on the shared task.
</prevsent>
<prevsent>lexical simplification revolves around replacing words by their simplest synonym in context aware fashion.
</prevsent>
</prevsection>
<citsent citstr=" W07-2009 ">
it is similar in many respects to the task of lexical substitution (mccarthy and navigli, 2007) <papid> W07-2009 </papid>in that it involves elements of selectional preference on the basis of central predefined criterion (sim plicity in the current case), as well as sensitivity to context.lexical simplification envisages principally human target audience, and can greatly benefit children, second language learners, people with low literacy levels or cognitive disabilities, and in general facilitate the dissemination of knowledge to wider audiences.</citsent>
<aftsection>
<nextsent>we experimented with number of features thatwe posited might be inherently linked with textual simplicity and selected the three that seemed the most promising on an evaluation with the trialdataset.
</nextsent>
<nextsent>these include contextual and psych olin guistic components.
</nextsent>
<nextsent>when combined using an svm 1developed by co-organizers of the shared taskranker to build model, such model provides results that offer statistically significant improvement over very strong context-independent base line.
</nextsent>
<nextsent>the system ranked first overall on the lexical simplification task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1564">
<title id=" S12-1066.xml">uowshef simplex  lexical simplicity ranking based on contextual and psycho linguistic features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>lexical simplification has received considerably less interest in the nlp community as compared with syntactic simplification.
</prevsent>
<prevsent>however, there are number of notable works related to the topic.
</prevsent>
</prevsection>
<citsent citstr=" N10-1056 ">
in particular yatskar et al (2010) <papid> N10-1056 </papid>leverage the relations between simple wikipedia and english wikipedia to extract simplification pairs.</citsent>
<aftsection>
<nextsent>biran et al(2011) <papid> P11-2087 </papid>extend this base methodology to apply lexical simplification to input sentences.</nextsent>
<nextsent>de belder and moens (2010), in contrast, provide more general architecture for the task, with scope for possible extension to other languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1565">
<title id=" S12-1066.xml">uowshef simplex  lexical simplicity ranking based on contextual and psycho linguistic features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, there are number of notable works related to the topic.
</prevsent>
<prevsent>in particular yatskar et al (2010) <papid> N10-1056 </papid>leverage the relations between simple wikipedia and english wikipedia to extract simplification pairs.</prevsent>
</prevsection>
<citsent citstr=" P11-2087 ">
biran et al(2011) <papid> P11-2087 </papid>extend this base methodology to apply lexical simplification to input sentences.</citsent>
<aftsection>
<nextsent>de belder and moens (2010), in contrast, provide more general architecture for the task, with scope for possible extension to other languages.
</nextsent>
<nextsent>these studies and others have envisaged range of different target user groups including children(de belder and moens, 2010), people with low literacy levels (aluisio et al, 2008) and aphasic readers (carroll et al, 1998).
</nextsent>
<nextsent>the current work differs from previous research in that it envisages stand-alone lexical simplification system based on linguistically motivated and cognitive principles within the framework of shared task.
</nextsent>
<nextsent>its core methodology remains open to integration into larger text simplification system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1566">
<title id=" S12-1066.xml">uowshef simplex  lexical simplicity ranking based on contextual and psycho linguistic features </title>
<section> the simplex lexical simplification.  </section>
<citcontext>
<prevsection>
<prevsent>for example,given the following context with an empty place holder, and its candidate substitutes: context: during the siege , george robertson had appointed shuja-ul-mulk, who was boy only 12 years old andthe youngest surviving son of aman-ul mulk, as the ruler of chitral.
</prevsent>
<prevsent>candidates: {clever} {smart} {intelligent} {bright} system is required to produce ranking, e.g.: system: {intelligent} {bright} {clever, smart}note that ties were permitted and that all candidates needed to be included in the system rankings.
</prevsent>
</prevsection>
<citsent citstr=" W07-2091 ">
system in an approach similar to what hassan et al (2007)<papid> W07-2091 </papid>used for lexical substitution, simplex ranks candidates based on weighted linear scoring function, which has the generalized form: (cn,i) = ? mm 1 rm (cn,i) where cn,i is the candidate substitute to be scored, and each rm is standalone ranking function that attributes to each candidate its rank based on its uniquely associated features.</citsent>
<aftsection>
<nextsent>based on this scoring,candidates for context are ranked in descending order of scores.in the development of the system we experimented with number of these features including ranking based on word length, number of syllables, scoring with 2-step cluster and rank architecture,latent semantic analysis, and average point-wise mutual information between the candidate and neighboring words in the context.
</nextsent>
<nextsent>however, the features which were intuitively the simplest proved, in the end, to give the best results.they were selected based on their superior performance on the trial dataset and their competitiveness with the strong simple frequency baseline.
</nextsent>
<nextsent>these stand-alone features are described in what follows.
</nextsent>
<nextsent>4.1 adapted n-gram model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1567">
<title id=" S12-1075.xml">polyucomp combining semantic vectors with skip bigrams for semantic textual similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given set of sentence pairs, participants are required to assign to each sentence pair similarity score.
</prevsent>
<prevsent>because sentence has only limited amount of content words, it is not easy to determine sentence similarities because of the sparseness issue.
</prevsent>
</prevsection>
<citsent citstr=" W99-0625 ">
hatzivassiloglou et al (1999) <papid> W99-0625 </papid>proposed to use linguistic features as indicators of text similarity to address the problem of sparse representation of sentences.</citsent>
<aftsection>
<nextsent>mihalcea et al (2006) measured sentence similarity using component words in sentences.
</nextsent>
<nextsent>li et al (2006) proposed to incorporate the semantic vector and word order to calculate sentence similarity.
</nextsent>
<nextsent>in our approach to the sts task, semantic vector is used and the semantic relatedness between words is derived from two sources: wordnet and wikipedia.
</nextsent>
<nextsent>because wordnet is limited in its coverage, wikipedia is used as candidate for determining word similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1568">
<title id=" S12-1075.xml">polyucomp combining semantic vectors with skip bigrams for semantic textual similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word order, however, is not considered in semantic vector.
</prevsent>
<prevsent>as semantic information are coded in sentences according to its order of writing, and in our systems, content words may not be adjacent to each other, we proposed to use skip bigrams to represent the structure of sentences.
</prevsent>
</prevsection>
<citsent citstr=" P04-1077 ">
skip bigrams, generally speaking, are pairs of words in sentence order with arbitrary gap (lin and och, 2004<papid> P04-1077 </papid>a).</citsent>
<aftsection>
<nextsent>different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (lin and och, 2004<papid> P04-1077 </papid>a), the skip bigrams we used are weighted by decaying factor of the skipping gap in sentence, giving higher scores to closer occurrences of skip bigrams.</nextsent>
<nextsent>it is reasonable to assume that similar sentences should have more overlapping skip bi grams, and the gaps in their shared skip bigrams should also be similar.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1572">
<title id=" S12-1075.xml">polyucomp combining semantic vectors with skip bigrams for semantic textual similarity </title>
<section> similarity between sentences.  </section>
<citcontext>
<prevsection>
<prevsent>let be the term set with sorted list of content words, t=(t1, t2,?, tn).
</prevsent>
<prevsent>without loss of generality, let sentence s=(w1 w2wm) where wj is content word and wj is word in t. let the vector space of the sentence be vss = (v1, v2, ?, vn).
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
then the value of vi is assigned as follows, where the similarity function sim(ti, wj) is calculated according to the path measure (pedersen et al., 2004) <papid> N04-3012 </papid>using the wordnet, formally defined as, ),(1),( jiji wtdistwtsim ? where dist(ti, wj) is the shortest path from ti, to wj by counting nodes in the wordnet taxonomy.</citsent>
<aftsection>
<nextsent>based on this, the semantic vectors for the two example sentences will be, svs1 = (1, 0.25, 1, 1, 1, 1, 0.33, 1, 0, 1) and svs2 = (0.25, 1, 0, 1, 1, 1, 1, 1, 1, 0.33) based on the two semantic vectors, the cosine metric is used to measure sentence similarity.
</nextsent>
<nextsent>in the wordnet, the entry chairman in the joint set is most similar to the word chief in sentence s2.
</nextsent>
<nextsent>in practice, however, this entry might be closer to the word presides than to the word chief.
</nextsent>
<nextsent>therefore, we try to obtain the semantic relatedness using the wikipedia for sentence and find that the entry chairman is closest to the word presides.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1573">
<title id=" S12-1004.xml">towards building a multilingual semantic network identifying inter lingual links in wikipedia </title>
<section> manual evaluation of the inter lingual.  </section>
<citcontext>
<prevsection>
<prevsent>table 3 shows the number of direct links extracted from the ten wikipedias we currently work with, as well as the number of paths that we add by enforcing the symmetry and transit ivity properties.
</prevsent>
<prevsent>links the translation links in wikipedia, whether added by the wikipedia editors (direct links), or inferred by the heuristics described in the previous section, are not guaranteed for quality.
</prevsent>
</prevsection>
<citsent citstr=" P10-1087 ">
in fact, previous work (de melo and weikum, 2010<papid> P10-1087 </papid>b) has shown that large number of the links created by the wikipedia users are incorrect, connecting articles that are not translations of each other, subsections of articles, or disambiguation pages.</citsent>
<aftsection>
<nextsent>we have therefore decided to run manual annotation study in order to determine thequality of the inter lingual links.
</nextsent>
<nextsent>the resulting annotation can serve both as gold standard for evaluating the quality of predicted links, and as supervision for machine learning model that would automatically detect translation links.
</nextsent>
<nextsent>33 language pair 0 1 2 3 4 (english, german) 46 8 29 2 110 (english, spanish) 22 19 19 13 123 (italian, french) 30 7 19 7 132 (spanish, italian) 21 8 17 13 136 table 6: number of annotations on scale of 0-4 for each pair of languages from the large pool of links directly available in wikipedia or inferred automatically through symmetry and transit ivity, we sampled and then manually annotated 195 pairs of articles for each of four language pairs: (english, german), (english, spanish), (italian, french), and (spanish, italian).
</nextsent>
<nextsent>the four language pairs were determined based on the native or near-native knowledge available in the group of annotators in our research group.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1583">
<title id=" S12-1004.xml">towards building a multilingual semantic network identifying inter lingual links in wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in related work, (de melo and weikum, 2010<papid> P10-1087 </papid>a) worked on similar problem in which they combined all the existing multilingualwikipedias to build stable, large multilingual tax onomy.</prevsent>
<prevsent>the inter lingual links have also been used for cross-lingual information retrieval (nguyen et al,2009) or to generate bilingual parallel corpora (mo hammadi and qasemaghaee, 2010).</prevsent>
</prevsection>
<citsent citstr=" D09-1124 ">
(ni et al, 2011) used multilingual editions of wikipedia tomine topics for the task of cross lingual text classification, while (hassan and mihalcea, 2009) <papid> D09-1124 </papid>usedwikipedias in different languages to measure cross lingual semantic relatedness between concepts and texts in different languages.</citsent>
<aftsection>
<nextsent>(bharadwaj et al, 2010) explored the use of the multilingual links to mine dictionaries for under-resourced languages.
</nextsent>
<nextsent>they developed an iterative approach to construct parallel corpus, using the inter lingual links, info boxes, category pages, and abstracts, which they then be used to extract bilingual dictionary.
</nextsent>
<nextsent>(navigli and ponzetto, 2010) <papid> P10-1023 </papid>explored the connections that can be drawn between wikipedia and wordnet.</nextsent>
<nextsent>while no attempts were made to complete the existing link structure of wikipedia, the authors made use of machine translation to enrich the resource.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1584">
<title id=" S12-1004.xml">towards building a multilingual semantic network identifying inter lingual links in wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(bharadwaj et al, 2010) explored the use of the multilingual links to mine dictionaries for under-resourced languages.
</prevsent>
<prevsent>they developed an iterative approach to construct parallel corpus, using the inter lingual links, info boxes, category pages, and abstracts, which they then be used to extract bilingual dictionary.
</prevsent>
</prevsection>
<citsent citstr=" P10-1023 ">
(navigli and ponzetto, 2010) <papid> P10-1023 </papid>explored the connections that can be drawn between wikipedia and wordnet.</citsent>
<aftsection>
<nextsent>while no attempts were made to complete the existing link structure of wikipedia, the authors made use of machine translation to enrich the resource.
</nextsent>
<nextsent>the two previous works most closely related toours are the systems introduced in (sorg and cimiano, 2008) and (de melo and weikum, 2010<papid> P10-1087 </papid>a; de melo and weikum, 2010<papid> P10-1087 </papid>b).</nextsent>
<nextsent>(sorg and cimiano,2008) designed system that predicts new inter lin gual links by using classification based approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1600">
<title id=" S10-1018.xml">sucre a modular system for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a considerable engineering effort is needed for the full coreference resolution task, and significant part of this effort concerns feature engineering.
</prevsent>
<prevsent>thus, system which is ableto extract the features based on feature definition language can help the researcher reduce the implementation effort needed for feature extraction.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
most methods of coreference resolution, if providing baseline, usually use feature set similar to (soon et al, 2001) or (ng and cardie, 2002) <papid> P02-1014 </papid>and do the feature extraction in the preprocessing stage.</citsent>
<aftsection>
<nextsent>sucre has been developed to provide more flexible method for feature engineering of coreference resolution.
</nextsent>
<nextsent>it has novel approach to model an unstructured text corpus in structured framework by using relational database model and regular feature definition language to define and extract the features.
</nextsent>
<nextsent>relational databases area well-known technology for structured data modeling and are supported by wide array of software and tools.
</nextsent>
<nextsent>converting text corpus to/fromits equivalent relational database model is straight forward in our framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1601">
<title id=" S10-1018.xml">sucre a modular system for coreference resolution </title>
<section> decoding (applicable on test data).  </section>
<citcontext>
<prevsection>
<prevsent>3 results.
</prevsent>
<prevsent>table 2 shows the results of sucre and the best competitor system on the test portions of the six languages from semeval-2010 task 1.
</prevsent>
</prevsection>
<citsent citstr=" H05-1004 ">
four different evaluation metrics were used to rank the participating systems: muc (vilain et al, 1995), 3 (bagga and baldwin, 1998), ceaf (luo, 2005) <papid> H05-1004 </papid>and blanc (recasens and hovy, in prep).</citsent>
<aftsection>
<nextsent>sucre has the best results in regular closed annotation track of english and german (for all metrics).
</nextsent>
<nextsent>its results for gold closed annotation track of both english and german are the bestin muc and blanc scoring metrics (muc: english +27.1 german +32.5, blanc: english +9.5 german +9.0) and for ceaf and 3(ceaf: english -1.3 german -4.8, 3 : english -2.1 german-4.8); in comparison to the second ranked system, the performance is clearly better in the first case and slightly better in the second.
</nextsent>
<nextsent>this result shows that sucre has been optimized in way that achieves good results on the four different scoring metrics.
</nextsent>
<nextsent>we view this good performance as demonstration of the strength of sucre: our 94 method of feature extraction, definition and tuning is uniform and can be optimized and applied to all languages and tracks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1602">
<title id=" S12-1099.xml">sagan an approach to semantic textual similarity based on textual entailment </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>a concept is cluster of synonymous terms that is called synset in wordnet.
</prevsent>
<prevsent>these text-to-text similarity measures are based on the following word-to-word similarity metrics: (resnik, 1995), (lin, 1997), (jiang and conrath, 1997), (pirr?
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
and seco, 2008), (wu &amp; palmer, 1994), <papid> P94-1019 </papid>path metric, (leacock &amp; chodorow, 1998), and semantic similarity to sentence level named semsim (castillo and cardenas,2010).</citsent>
<aftsection>
<nextsent>pre-processing similarity score msr word level semantic metrics extraction features svm with regression test set: msr, msrvid,europarl, smt-news, wn run 1 normalizer stemmer parser resnik semsimw&plin; ...
</nextsent>
<nextsent>sentence level semantic metric msr+msrvid run 2 run 3 msr+msrvid +europarl training sets: ...
</nextsent>
<nextsent>fig.1.
</nextsent>
<nextsent>system architecture the system construct model of the semantic similarity of two texts (t,h) as function of the semantic similarity of the constituent words of both phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1603">
<title id=" S12-1099.xml">sagan an approach to semantic textual similarity based on textual entailment </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>having into account that in our training phase we obtained decrease in performance using rte datasets we decided not to submit any run using the rte datasets.
</prevsent>
<prevsent>4.2 submission to the sts shared task.
</prevsent>
</prevsection>
<citsent citstr=" I05-5002 ">
our participation in the shared task consisted of three different runs using svm classifier with regression; the runs were set up as follows: - run 1: system trained on subset of the microsoft research paraphrase corpus (dolan and brockett, 2005), <papid> I05-5002 </papid>named msr and consisting of 750 pairs of sentences marked with degree of similarity from 5 to 0.</citsent>
<aftsection>
<nextsent>- run 2: in addition to the msr corpus we incorporated another 750 sentences extracted from the microsoft research video description corpus (msrvid), annotated in the same way as msr.
</nextsent>
<nextsent>- run 3: to the 1500 sentences from the msr and msrvid corpus we incorporated 734 pairs of sentences from the europarl corpus used as development set in the wmt 2008; all sentences are annotated with the degree of similarity from 5 to 0.
</nextsent>
<nextsent>it is very interesting to note that we used the same system configurations for every dataset of each run.
</nextsent>
<nextsent>in this manner, we did not perform any kind of tuning to particular dataset before our submission.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1604">
<title id=" S12-1099.xml">sagan an approach to semantic textual similarity based on textual entailment </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, further application of other metrics should be one in order to find the most representative and fair evaluation metric for this task.
</prevsent>
<prevsent>finally, while promising results were obtained with our system, it still needs to be tested on diversity of settings.
</prevsent>
</prevsection>
<citsent citstr=" W12-3103 ">
this is work in progress, as the system is being tested as metric for the evaluation of machine translation, as reported in (castillo and estrella, 2012).<papid> W12-3103 </papid></citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1605">
<title id=" S12-1073.xml">icta system combination for chinese semantic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, their grains are finer than syntactic relations, e.g., the syntactic subject can be agent or experiencer.
</prevsent>
<prevsent>readers can refer to (wanxiang che, 2012) for detailed introduction.figure 1: the pipeline of our system, where we combine the results of three dependency parsers and use max entropy classifier to predict the semantic relations.
</prevsent>
</prevsection>
<citsent citstr=" W08-2121 ">
different from most methods proposed inconll-2008 1 and 2009 2, in which some researchers build joint model to simultaneously generate dependency structure and its syntactic relations (surdeanu et al, 2008; <papid> W08-2121 </papid>hajic?</citsent>
<aftsection>
<nextsent>et al, 2009), here,we first employ several parsers to generate dependency structure and then propose method to combine their outputs.
</nextsent>
<nextsent>after that, we label relation between each head and its modifier via the traversal of this refined parse tree.
</nextsent>
<nextsent>the reason why we use pipeline model while not joint model is thatthe number of semantic relations annotated by organizers is more than 120 types, while in the former task is only 21 types.
</nextsent>
<nextsent>compared to the former task, the large number of types will obviously drop the performance of classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1606">
<title id=" S12-1073.xml">icta system combination for chinese semantic dependency parsing </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>in the followings, we will first introduce the details of our strategy for dependency structure construction.
</prevsent>
<prevsent>2.1 parsers.
</prevsent>
</prevsection>
<citsent citstr=" W04-0308 ">
we implement three transition-based dependency parsers with three different parsing algorithms: nivres arc standard, nivres arc eager (see nivre(2004) <papid> W04-0308 </papid>for comparison between the two nivre al gorithms), and liangs dynamic algorithm(huang and sagae, 2010).<papid> P10-1110 </papid></citsent>
<aftsection>
<nextsent>we use these algorithms for several reasons: first, they are easy to implement and their reported performance are approaching to state-of-the-art.
</nextsent>
<nextsent>second, their outputs are projective, which is consistent with given corpus.
</nextsent>
<nextsent>2.2 parser combination.
</nextsent>
<nextsent>we use the similar method presented in hallet al (2011) to advance the accuracy of parses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1607">
<title id=" S12-1073.xml">icta system combination for chinese semantic dependency parsing </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>in the followings, we will first introduce the details of our strategy for dependency structure construction.
</prevsent>
<prevsent>2.1 parsers.
</prevsent>
</prevsection>
<citsent citstr=" P10-1110 ">
we implement three transition-based dependency parsers with three different parsing algorithms: nivres arc standard, nivres arc eager (see nivre(2004) <papid> W04-0308 </papid>for comparison between the two nivre al gorithms), and liangs dynamic algorithm(huang and sagae, 2010).<papid> P10-1110 </papid></citsent>
<aftsection>
<nextsent>we use these algorithms for several reasons: first, they are easy to implement and their reported performance are approaching to state-of-the-art.
</nextsent>
<nextsent>second, their outputs are projective, which is consistent with given corpus.
</nextsent>
<nextsent>2.2 parser combination.
</nextsent>
<nextsent>we use the similar method presented in hallet al (2011) to advance the accuracy of parses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1612">
<title id=" S10-1050.xml">ecnu effective semantic relations classification without complicated features or multiple external corpora </title>
<section> most previous work at semeval 2007 task.  </section>
<citcontext>
<prevsection>
<prevsent>that is, the steps in the feature extraction process are tobe simple and direct for the purpose of reducing errors possibly introduced by many nlp tools.
</prevsent>
<prevsent>furthermore, unified (global) feature set is set up for all relations rather than for each relation.
</prevsent>
</prevsection>
<citsent citstr=" P08-1027 ">
4 leveraged on external theauri or corpora(whether unannotated or annotated) (davi dov and rappoport, 2008), (<papid> P08-1027 </papid>costello, 2007), (beamer et al, 2007) and (nakov and hearst, 2008) that make the task adaption to different domains and languages more difficult, since they would not have such manually classified or annotated corpus available.</citsent>
<aftsection>
<nextsent>from practical point of view, our system would make use of less resources.
</nextsent>
<nextsent>4 constructed several local classifiers on different algorithms or different feature subsets, one for each relation (hendrickx et al, 2007)and (davidov and rappoport, 2008).<papid> P08-1027 </papid></nextsent>
<nextsent>our approach is to build global classifier for all relations in practical nlp settings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1614">
<title id=" S12-1064.xml">hdu cross lingual textual entailment with smt features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we learn separate binary classifiers for each entailment direction and combine them to obtain four entailment relations.
</prevsent>
<prevsent>our system yielded the best overall score for three out of four language pairs.
</prevsent>
</prevsection>
<citsent citstr=" N10-1045 ">
cross-lingual textual entailment (clte) (mehdad et al., 2010) <papid> N10-1045 </papid>is an extension of textual entailment (te)(dagan and glickman, 2004).</citsent>
<aftsection>
<nextsent>the task of recognizing entailment is to determine whether hypothesis can be semantically inferred from text . the clte task adds cross-lingual dimension to the problem by considering sentence pairs, where and are in different languages.
</nextsent>
<nextsent>the semeval-2012 clte task (negri et al, 2012) <papid> S12-1053 </papid>asks participants tojudge entailment pairs in four language combinations1, defining four target entailment relations, forward, backward, bidirectional and no entailment.we investigate this problem in statistical learning framework, which allows us to combine cross lingual word alignment features as well as common1spanish-english (es-en), italian-english (it-en), french english (fr-en) and german-english (de-en).monolingual entailment metrics, such as bag-ofwords overlap, edit distance and monolingual alignments on translations of and , using standard statistical machine translation (smt) tools andre sources.</nextsent>
<nextsent>our goal is to address this task without deep processing components to make it easily portable across languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1615">
<title id=" S12-1064.xml">hdu cross lingual textual entailment with smt features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cross-lingual textual entailment (clte) (mehdad et al., 2010) <papid> N10-1045 </papid>is an extension of textual entailment (te)(dagan and glickman, 2004).</prevsent>
<prevsent>the task of recognizing entailment is to determine whether hypothesis can be semantically inferred from text . the clte task adds cross-lingual dimension to the problem by considering sentence pairs, where and are in different languages.</prevsent>
</prevsection>
<citsent citstr=" S12-1053 ">
the semeval-2012 clte task (negri et al, 2012) <papid> S12-1053 </papid>asks participants tojudge entailment pairs in four language combinations1, defining four target entailment relations, forward, backward, bidirectional and no entailment.we investigate this problem in statistical learning framework, which allows us to combine cross lingual word alignment features as well as common1spanish-english (es-en), italian-english (it-en), french english (fr-en) and german-english (de-en).monolingual entailment metrics, such as bag-ofwords overlap, edit distance and monolingual alignments on translations of and , using standard statistical machine translation (smt) tools andre sources.</citsent>
<aftsection>
<nextsent>our goal is to address this task without deep processing components to make it easily portable across languages.
</nextsent>
<nextsent>we argue that the cross-lingual entailment task can benefit from direct alignments between and , since large amount of bilingual parallel data is available, which naturally models synonymy and paraphrasing across languages.
</nextsent>
<nextsent>with the yearly recognizing textual entailment (rte) challenge (dagan et al, 2006), there has beena lot of work on monolingual te.
</nextsent>
<nextsent>we therefore include established monolingual features in our approach, such as alignment scores (maccartney et al., 2008), <papid> D08-1084 </papid>edit distance and bag-of-words lexical overlap measures (kouylekov and negri, 2010).<papid> P10-4008 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1616">
<title id=" S12-1064.xml">hdu cross lingual textual entailment with smt features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we argue that the cross-lingual entailment task can benefit from direct alignments between and , since large amount of bilingual parallel data is available, which naturally models synonymy and paraphrasing across languages.
</prevsent>
<prevsent>with the yearly recognizing textual entailment (rte) challenge (dagan et al, 2006), there has beena lot of work on monolingual te.
</prevsent>
</prevsection>
<citsent citstr=" D08-1084 ">
we therefore include established monolingual features in our approach, such as alignment scores (maccartney et al., 2008), <papid> D08-1084 </papid>edit distance and bag-of-words lexical overlap measures (kouylekov and negri, 2010).<papid> P10-4008 </papid></citsent>
<aftsection>
<nextsent>so far, the only work on clte that we are aware of is mehdad et al (2010), <papid> N10-1045 </papid>where the problem is reduced to monolingual entailment using machine translation, and mehdad et al (2011), <papid> P11-1134 </papid>which exploits parallel corpora for generating features based on phrase alignments as input to an svm.</nextsent>
<nextsent>our approach combines ideas from both, mostly resembling mehdadet al (2011).<papid> P11-1134 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1617">
<title id=" S12-1064.xml">hdu cross lingual textual entailment with smt features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we argue that the cross-lingual entailment task can benefit from direct alignments between and , since large amount of bilingual parallel data is available, which naturally models synonymy and paraphrasing across languages.
</prevsent>
<prevsent>with the yearly recognizing textual entailment (rte) challenge (dagan et al, 2006), there has beena lot of work on monolingual te.
</prevsent>
</prevsection>
<citsent citstr=" P10-4008 ">
we therefore include established monolingual features in our approach, such as alignment scores (maccartney et al., 2008), <papid> D08-1084 </papid>edit distance and bag-of-words lexical overlap measures (kouylekov and negri, 2010).<papid> P10-4008 </papid></citsent>
<aftsection>
<nextsent>so far, the only work on clte that we are aware of is mehdad et al (2010), <papid> N10-1045 </papid>where the problem is reduced to monolingual entailment using machine translation, and mehdad et al (2011), <papid> P11-1134 </papid>which exploits parallel corpora for generating features based on phrase alignments as input to an svm.</nextsent>
<nextsent>our approach combines ideas from both, mostly resembling mehdadet al (2011).<papid> P11-1134 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1619">
<title id=" S12-1064.xml">hdu cross lingual textual entailment with smt features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>with the yearly recognizing textual entailment (rte) challenge (dagan et al, 2006), there has beena lot of work on monolingual te.
</prevsent>
<prevsent>we therefore include established monolingual features in our approach, such as alignment scores (maccartney et al., 2008), <papid> D08-1084 </papid>edit distance and bag-of-words lexical overlap measures (kouylekov and negri, 2010).<papid> P10-4008 </papid></prevsent>
</prevsection>
<citsent citstr=" P11-1134 ">
so far, the only work on clte that we are aware of is mehdad et al (2010), <papid> N10-1045 </papid>where the problem is reduced to monolingual entailment using machine translation, and mehdad et al (2011), <papid> P11-1134 </papid>which exploits parallel corpora for generating features based on phrase alignments as input to an svm.</citsent>
<aftsection>
<nextsent>our approach combines ideas from both, mostly resembling mehdadet al (2011).<papid> P11-1134 </papid></nextsent>
<nextsent>there are, however, several differ ences; we use word translation probabilities instead of phrase tables and model monolingual and cross lingual alignment separately.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1621">
<title id=" S12-1064.xml">hdu cross lingual textual entailment with smt features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>con 467versely, pado?
</prevsent>
<prevsent>et al (2009) showed that textual entailment features can be used for measuring mt quality, indicating strong relatedness of the two problems.
</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
the clte task is also related to the problem of identifying parallel sentence pairs in non-parallel corpus, so we adapt alignment-based features from munteanu and marcu (2005), <papid> J05-4003 </papid>where maximum entropy classifier was used to judge if two sentences are sufficiently parallel.</citsent>
<aftsection>
<nextsent>regarding the view on entailment, maccartney and manning (2007) <papid> W07-1431 </papid>proposed the decomposition of top-level entailment, such as equivalence (which corresponds to the clte bidirectional class), into atomic forward and backward entailment predictions, which is mirrored in our multi-label approach with two binary classifiers.</nextsent>
<nextsent>the semeval-2012 clte task emerges from the monolingual rte task; however the perception of entailment differs slightly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1622">
<title id=" S12-1064.xml">hdu cross lingual textual entailment with smt features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>et al (2009) showed that textual entailment features can be used for measuring mt quality, indicating strong relatedness of the two problems.
</prevsent>
<prevsent>the clte task is also related to the problem of identifying parallel sentence pairs in non-parallel corpus, so we adapt alignment-based features from munteanu and marcu (2005), <papid> J05-4003 </papid>where maximum entropy classifier was used to judge if two sentences are sufficiently parallel.</prevsent>
</prevsection>
<citsent citstr=" W07-1431 ">
regarding the view on entailment, maccartney and manning (2007) <papid> W07-1431 </papid>proposed the decomposition of top-level entailment, such as equivalence (which corresponds to the clte bidirectional class), into atomic forward and backward entailment predictions, which is mirrored in our multi-label approach with two binary classifiers.</citsent>
<aftsection>
<nextsent>the semeval-2012 clte task emerges from the monolingual rte task; however the perception of entailment differs slightly.
</nextsent>
<nextsent>in clte, the sentences t1 and t2 are of roughly the same length and the entailment is predicted in both directions.
</nextsent>
<nextsent>negri et al.
</nextsent>
<nextsent>(2011) states that the clte pairs were created by paraphrasing an english sentence and leaving out or adding information to construct modified sentence e?, which was then translated into different language2, yielding sentence and thus creating bilingual entailment pair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1623">
<title id=" S12-1064.xml">hdu cross lingual textual entailment with smt features </title>
<section> smt features for clte.  </section>
<citcontext>
<prevsection>
<prevsent>jaccard coefficient, sim(a,b) = |ab||ab| ? overlap coefficient, sim(a,b) = |ab|min(|a|,|b|) we also compute the lexical overlap on bigrams and trigrams.
</prevsent>
<prevsent>in addition, we include simple distance measure based on string edit distance ed, summing up over all distances between every token in and its most similar token in b, where we assume that the corresponding token is the one with the smallest edit distance: ? dist(a,b) = log ? aa min bb ed(a, b) 3.3 meteor features.
</prevsent>
</prevsection>
<citsent citstr=" W11-2107 ">
the meteor scoring tool (denkowski and lavie, 2011) <papid> W11-2107 </papid>for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language.</citsent>
<aftsection>
<nextsent>meteor uses stemming, paraphrase tables and synonym collections to align words between the two sentences and scores the resulting alignment.
</nextsent>
<nextsent>we include the overall weighted meteor score both for (e, (f )) 3http://translate.google.com/ 468and (f, (e))4 as well as separate alignment precision, recall and fragmentation scores for (e, (f )).
</nextsent>
<nextsent>3.4 monolingual alignment features.
</nextsent>
<nextsent>we use the alignments output by the meteor-1.3 scorer for (e, (f ))5 to calculate the following metrics: ? number of unaligned words ? percentage of aligned words ? length of the longest unaligned sub sequence 3.5 cross-lingual alignment features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1624">
<title id=" S12-1064.xml">hdu cross lingual textual entailment with smt features </title>
<section> smt features for clte.  </section>
<citcontext>
<prevsection>
<prevsent>3.4 monolingual alignment features.
</prevsent>
<prevsent>we use the alignments output by the meteor-1.3 scorer for (e, (f ))5 to calculate the following metrics: ? number of unaligned words ? percentage of aligned words ? length of the longest unaligned sub sequence 3.5 cross-lingual alignment features.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
we calculate ibm model 1 word alignments (brown et al, 1993) <papid> J93-2003 </papid>with giza++ (och and ney, 2003) <papid> J03-1002 </papid>on dataset concatenated from europarl-v66 (koehn, 2005) and bilingual dictionary obtained from dict.cc7 for coverage.</citsent>
<aftsection>
<nextsent>we then heuristic ally align each word in with the word in for which we find the highest word translation probability p(e|f) and vice versa.
</nextsent>
<nextsent>words for which no translation isfound are considered unaligned.
</nextsent>
<nextsent>from this alignment a, we derive the following features both for and (resulting in total of eight cross-lingual alignment features): ? number of unaligned words ? percentage of aligned words ? alignment score 1|e| ? ee p(e|a(e)) ? length of the longest unaligned subsequence
</nextsent>
<nextsent>to account for the different data ranges, we normalized all feature value distributions to the normal distribution (0), distribution (13), so that 99% of the feature values are in [1, 1].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1625">
<title id=" S12-1064.xml">hdu cross lingual textual entailment with smt features </title>
<section> smt features for clte.  </section>
<citcontext>
<prevsection>
<prevsent>3.4 monolingual alignment features.
</prevsent>
<prevsent>we use the alignments output by the meteor-1.3 scorer for (e, (f ))5 to calculate the following metrics: ? number of unaligned words ? percentage of aligned words ? length of the longest unaligned sub sequence 3.5 cross-lingual alignment features.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we calculate ibm model 1 word alignments (brown et al, 1993) <papid> J93-2003 </papid>with giza++ (och and ney, 2003) <papid> J03-1002 </papid>on dataset concatenated from europarl-v66 (koehn, 2005) and bilingual dictionary obtained from dict.cc7 for coverage.</citsent>
<aftsection>
<nextsent>we then heuristic ally align each word in with the word in for which we find the highest word translation probability p(e|f) and vice versa.
</nextsent>
<nextsent>words for which no translation isfound are considered unaligned.
</nextsent>
<nextsent>from this alignment a, we derive the following features both for and (resulting in total of eight cross-lingual alignment features): ? number of unaligned words ? percentage of aligned words ? alignment score 1|e| ? ee p(e|a(e)) ? length of the longest unaligned subsequence
</nextsent>
<nextsent>to account for the different data ranges, we normalized all feature value distributions to the normal distribution (0), distribution (13), so that 99% of the feature values are in [1, 1].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1626">
<title id=" S10-1035.xml">wingnus key phrase extraction utilizing document logical structure </title>
<section> candidate phrase identification.  </section>
<citcontext>
<prevsection>
<prevsent>the idea is to limit the search scope of our candidate identification system while maintaining coverage.
</prevsent>
<prevsent>we propose new ap caption, footnote, and reference lines.
</prevsent>
</prevsection>
<citsent citstr=" W09-2902 ">
167 proach, which extracts candidates according to the regular expression rules discussed in (kim and kan, 2009).<papid> W09-2902 </papid></citsent>
<aftsection>
<nextsent>however, instead of using the whole document text as input, we abridge the input text at different levels from full to minimal.
</nextsent>
<nextsent>input description cand com recall minimal title + headers 30,702 1,312 63.72% + abs + intro medium min + rw 44,975 1,414 68.67% + conclusion full 1 med + body 1 73,958 1,580 76.74% full 2 med + body 2 90,624 1,635 79.41% full 3 med + body 3 101,006 1,672 81.20% full med + body 121,378 1,737 84.36% full text original text 148,411 1,766 85.77%table 2: different levels of abridged inputs computed on the training data.
</nextsent>
<nextsent>cand shows the number of candidate key phrases extracted foreach input type; com gives the number of correct key phrases appear as candidates; recall is computed with respect to the total number of key phrases in the original texts (2059).
</nextsent>
<nextsent>results in table 2 show that we could gathera recall of 63.72% when considering significantly abridged form of the input culled from title, headers, abstract (abs) and introduction (in tro) ? minimal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1630">
<title id=" S10-1094.xml">cfilt resource conscious approaches for all words domain specific wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>domain specific wsd exhibits high level of accuracy even for the all-words scenario (khapra et al., 2010) - provided training and testing are on the same domain.
</prevsent>
<prevsent>however, the effort of creating the training corpus - annotated sense marked corpora - for every domain of interest has always been matter of concern.
</prevsent>
</prevsection>
<citsent citstr=" J07-4005 ">
therefore, attempts have been made to develop unsupervised (mccarthy et al, 2007; <papid> J07-4005 </papid>koeling et al, 2005) <papid> H05-1053 </papid>and knowledge based techniques (agirre et al, 2009) <papid> W09-2420 </papid>for wsd which do not need sense marked corpora.</citsent>
<aftsection>
<nextsent>however, such approaches have not proved effective, since they typically do not perform better than the wordnet first sense baseline accuracy in the all-words sce nario.motivated by the desire to develop annotation lean all-words domain specific techniques forwsd we propose two resource conscious approaches.
</nextsent>
<nextsent>the first approach is knowledge based approach which focuses on retaining only domain specific synsets in the wordnet using two step pruning process.
</nextsent>
<nextsent>in the first step, the wordnet graph is restricted to only those synsets which contain words appearing in an untagged domain specific corpus.
</nextsent>
<nextsent>in the second step, the graph is pruned further by retaining only the largest connected components of the pruned graph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1631">
<title id=" S10-1094.xml">cfilt resource conscious approaches for all words domain specific wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>domain specific wsd exhibits high level of accuracy even for the all-words scenario (khapra et al., 2010) - provided training and testing are on the same domain.
</prevsent>
<prevsent>however, the effort of creating the training corpus - annotated sense marked corpora - for every domain of interest has always been matter of concern.
</prevsent>
</prevsection>
<citsent citstr=" H05-1053 ">
therefore, attempts have been made to develop unsupervised (mccarthy et al, 2007; <papid> J07-4005 </papid>koeling et al, 2005) <papid> H05-1053 </papid>and knowledge based techniques (agirre et al, 2009) <papid> W09-2420 </papid>for wsd which do not need sense marked corpora.</citsent>
<aftsection>
<nextsent>however, such approaches have not proved effective, since they typically do not perform better than the wordnet first sense baseline accuracy in the all-words sce nario.motivated by the desire to develop annotation lean all-words domain specific techniques forwsd we propose two resource conscious approaches.
</nextsent>
<nextsent>the first approach is knowledge based approach which focuses on retaining only domain specific synsets in the wordnet using two step pruning process.
</nextsent>
<nextsent>in the first step, the wordnet graph is restricted to only those synsets which contain words appearing in an untagged domain specific corpus.
</nextsent>
<nextsent>in the second step, the graph is pruned further by retaining only the largest connected components of the pruned graph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1632">
<title id=" S10-1094.xml">cfilt resource conscious approaches for all words domain specific wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>domain specific wsd exhibits high level of accuracy even for the all-words scenario (khapra et al., 2010) - provided training and testing are on the same domain.
</prevsent>
<prevsent>however, the effort of creating the training corpus - annotated sense marked corpora - for every domain of interest has always been matter of concern.
</prevsent>
</prevsection>
<citsent citstr=" W09-2420 ">
therefore, attempts have been made to develop unsupervised (mccarthy et al, 2007; <papid> J07-4005 </papid>koeling et al, 2005) <papid> H05-1053 </papid>and knowledge based techniques (agirre et al, 2009) <papid> W09-2420 </papid>for wsd which do not need sense marked corpora.</citsent>
<aftsection>
<nextsent>however, such approaches have not proved effective, since they typically do not perform better than the wordnet first sense baseline accuracy in the all-words sce nario.motivated by the desire to develop annotation lean all-words domain specific techniques forwsd we propose two resource conscious approaches.
</nextsent>
<nextsent>the first approach is knowledge based approach which focuses on retaining only domain specific synsets in the wordnet using two step pruning process.
</nextsent>
<nextsent>in the first step, the wordnet graph is restricted to only those synsets which contain words appearing in an untagged domain specific corpus.
</nextsent>
<nextsent>in the second step, the graph is pruned further by retaining only the largest connected components of the pruned graph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1636">
<title id=" S10-1094.xml">cfilt resource conscious approaches for all words domain specific wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>each target word in given sentence is then disambiguatedusing an iterative disambiguation process by considering only those candidate synsets which appear in the top-k largest connected components.
</prevsent>
<prevsent>our knowledge based approach performed better than current state of the art knowledge based approach (agirre et al, 2009).<papid> W09-2420 </papid></prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
also, the precision was better than the wordnet first sense baseline even though the f-score was slightly lower than the baseline.the second approach is weakly supervised approach which uses few hand labeled examples for the most frequent words in the target domain in addition to the publicly available mixed-domainsemcor (miller et al, 1993) <papid> H93-1061 </papid>corpus.</citsent>
<aftsection>
<nextsent>the underlying assumption is that words exhibit one sense per domain?
</nextsent>
<nextsent>phenomenon and hence even as fewas 5 training examples per word would be sufficient to identify the predominant sense of the most frequent words in the target domain.
</nextsent>
<nextsent>further, once the most frequent words have been disambiguated using the predominant sense, they can provide strong clues for disambiguating other words in the 421 sentence.
</nextsent>
<nextsent>our weakly supervised system gave thebest performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain.the remainder of this paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1638">
<title id=" S10-1094.xml">cfilt resource conscious approaches for all words domain specific wsd </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in section 6 we present results and discussions followed by conclusion in section 7.
</prevsent>
<prevsent>there are two important lines of work for do main specific wsd.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
the first focuses on target word specific wsd where the results are reported on handful of target words (41-191 words) on three lexical sample datasets, viz., dso corpus (ng and lee, 1996), <papid> P96-1006 </papid>medline corpus (weeber et al., 2001) and the corpus of koeling et al (2005).<papid> H05-1053 </papid></citsent>
<aftsection>
<nextsent>the second focuses on all-words domain specific wsd where the results are reported on large annotated corpora from two domains, viz., tourism and health (khapra et al, 2010).
</nextsent>
<nextsent>in the target word setting, it has been shown that unsupervised methods (mccarthy et al, 2007) <papid> J07-4005 </papid>and knowledge based methods (agirre et al, 2009) <papid> W09-2420 </papid>can do better than wordnet first sense baseline andin some cases can also outperform supervised ap proaches.</nextsent>
<nextsent>however, since these systems have been tested only for certain target words, the question of their utility in all words wsd it still open . in the all words setting, khapra et al (2010) have shown significant improvements over the wordnet first sense baseline using fully supervised approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1643">
<title id=" S10-1094.xml">cfilt resource conscious approaches for all words domain specific wsd </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, since these systems have been tested only for certain target words, the question of their utility in all words wsd it still open . in the all words setting, khapra et al (2010) have shown significant improvements over the wordnet first sense baseline using fully supervised approach.
</prevsent>
<prevsent>however, the need for sense annotated corpus in the domain of interest is matter of concern and provides motivation for adapting their approach to annotation scarce scenarios.
</prevsent>
</prevsection>
<citsent citstr=" P07-1007 ">
here, wetake inspiration from the target-word specific results reported by chan and ng (2007) <papid> P07-1007 </papid>where by using just 30% of the target data they obtained the same performance as that obtained by using the entire target data.</citsent>
<aftsection>
<nextsent>we take the fully supervised approach of(khapra et al, 2010) and convert it to weakly supervised approach by using only handful of hand labeled examples for the most frequent words appearing in the target domain.
</nextsent>
<nextsent>for the remaining words we use the sense distributions learnt from semcor (miller et al, 1993) <papid> H93-1061 </papid>which is publicly available mixed domain corpus.</nextsent>
<nextsent>our approach is thus based on the annotate-little from the targetdomain?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1650">
<title id=" S10-1038.xml">unpmc naive approach to extract key phrases from scientific articles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typical approaches for automatically extracting terms use linguistic preprocessing which involves morphosyntactic analysis such as part-of-speech tagging and phrase chunking, and statistical postprocessing such as log likelihood which compares theterm frequencies in document against their expected frequencies derived in bigger text.
</prevsent>
<prevsent>besides, extracting terms prefers syntactically plausible noun phrases (nps) which are mainly multi words terms.
</prevsent>
</prevsection>
<citsent citstr=" W09-2902 ">
kim and kan (2009) <papid> W09-2902 </papid>report that mostof key phrases are often simple words than less of ten compound words 2 .the task for extracting key phrases tend to include analyzing the document structure.</citsent>
<aftsection>
<nextsent>especially, extracting key phrases from well-structuredscientific articles should consider cross-section information (nguyen and kan, 2007).
</nextsent>
<nextsent>this information has been explored to assess the suitability of features during learning in kim and kan (2009).<papid> W09-2902 </papid></nextsent>
<nextsent>extracting key phrases, however, is more than to extracting terminology or analyzing the document structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1655">
<title id=" S10-1058.xml">uvt memory based pairwise ranking of paraphrasing verbs </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>we combine the lcs with both verb phrases into one feature.google n-gram features we use the google gram corpus to count co-occurence frequencies of certain n-grams.
</prevsent>
<prevsent>an nc occurring often together with certain verb should indicate that that verb is good paraphrase for the nc.
</prevsent>
</prevsection>
<citsent citstr=" W05-0603 ">
using web textfor various nlp-tasks has been proven to be useful (lapata and keller, 2005), also for nc interpretation (nakov and hearst, 2005).<papid> W05-0603 </papid></citsent>
<aftsection>
<nextsent>because of data sparseness and the unlikelihood of finding aperfect match for certain n-gram, we adopt different strategies for constructing features.
</nextsent>
<nextsent>first ofall, we try to relax the matching conditions by applying certain regular expression.
</nextsent>
<nextsent>given the nc abortion problem?
</nextsent>
<nextsent>and the paraphrasing verb be related to?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1656">
<title id=" S12-1050.xml">semeval2012 task 5 chinese semantic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>predicate-argument relations and ignores the semantic relations between noun and its modifier.cial predicates.
</prevsent>
<prevsent>therefore, there are infinite semantic roles to be learned, as the number of predicates is not fixed.
</prevsent>
</prevsection>
<citsent citstr=" W03-1707 ">
although the propbank (xue and palmer, 2003) <papid> W03-1707 </papid>normalizes these semantic roles into certain symbols, such as arg0-arg5, the same symbol can have different semantic meanings when paired with different predicates, and thus cannot be learned well.semantic dependency parsing is therefore proposed to solve the two problems above for chinese.</citsent>
<aftsection>
<nextsent>firstly, the proposed method analyzes all the words semantic roles in sentence and specifies the concrete semantic relation of each word pair.
</nextsent>
<nextsent>afterward, this work analyzes and summarizes all the possible semantic roles, obtaining over 100 of them,and then uses these semantic roles to specify these mantic relation for each word pair.
</nextsent>
<nextsent>dependency parsing (kubler et al, 2009) is based on dependency grammar.
</nextsent>
<nextsent>it has several advantages, such as concise formalization, easy comprehension, high efficiency, and so on.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1657">
<title id=" S12-1050.xml">semeval2012 task 5 chinese semantic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dependency parsing has been studied intensively in recent decades, with most related work focusing on syntactic structure.many research papers on chinese linguistics demonstrate the remarkable difference between semantics and syntax (jin, 2001; zhou and zhang, 2003).
</prevsent>
<prevsent>chinese is meaning-combined language with very flexible syntax, and semantics are more stable than syntax.
</prevsent>
</prevsection>
<citsent citstr=" W03-1712 ">
the word is the basic unit of semantics, and the structure and meaning of sentence consists mainly of series of semantic dependencies between individual words (li et al, 2003).<papid> W03-1712 </papid></citsent>
<aftsection>
<nextsent>thus, reason able endeavor is to exploit dependency parsing for semantic analysis of chinese languages.
</nextsent>
<nextsent>figure 1 shows an example of chinese semantic dependency parsing.
</nextsent>
<nextsent>378 international monetary fund organization turn down for global economy increasing of prediction d-genetived-restrictive d-restrictive agent prep-dependd-genetive d-domain aux-depend d-restrictivecontent root figure 1: an example of chinese semantic dependency parsing.
</nextsent>
<nextsent>figure 1 shows that chinese semantic dependency parsing looks very similar to traditional syntax dominated dependency parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1658">
<title id=" S12-1050.xml">semeval2012 task 5 chinese semantic dependency parsing </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>all the sentences are in one text file, with each sentence separated by ablank line.
</prevsent>
<prevsent>each sentence consists of one or more tokens, and each token is represented on one line consisting of 10 fields.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
buchholz and marsi (2006) <papid> W06-2920 </papid>provide more detailed information on the format.</citsent>
<aftsection>
<nextsent>fields are separated from each other by tab.
</nextsent>
<nextsent>only five of the 10 fields are used: token id, form, pos tagger,head, and deprel.
</nextsent>
<nextsent>head denotes the semantic dependency of each word, and deprel denotes the corresponding semantic relations of the dependency.
</nextsent>
<nextsent>in the data, the lemma column is filled with the form and the cpostag column with the postag.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1659">
<title id=" S12-1050.xml">semeval2012 task 5 chinese semantic dependency parsing </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>the original sentence is thus divided into two types of parts that can be parsed separately.
</prevsent>
<prevsent>the first type is sr phrase parsing, and the second involves the replacement of sr phrases with either their head or the sr of the head.
</prevsent>
</prevsection>
<citsent citstr=" D11-1109 ">
finally, the paper takes graph-based parser (li et al, 2011) <papid> D11-1109 </papid>as the semantic dependency parser for all parts.</citsent>
<aftsection>
<nextsent>these three systems differ in their phrase identification strategies.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>nju-parser-1, nju-parser-2.
</nextsent>
<nextsent>the nju-parser is based on the state-of-the art mst parser (mcdonald, 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1660">
<title id=" S12-1050.xml">semeval2012 task 5 chinese semantic dependency parsing </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>zhijun wu-1.
</prevsent>
<prevsent>this system extends the second-order of the mst parser by adding third-order features, and then applying this model to chinese semantic dependency parsing.
</prevsent>
</prevsection>
<citsent citstr=" P10-1001 ">
in contrast to koo and collins (2010) <papid> P10-1001 </papid>this system does not implement the third-order model using dynamic programming, as it requires o(n4) time.</citsent>
<aftsection>
<nextsent>it first first obtained the k-best results of second-order models and then added the third-order features into the results.
</nextsent>
<nextsent>4.
</nextsent>
<nextsent>ict-1.
</nextsent>
<nextsent>the ict semantic dependency parser employsa system-combining strategy to obtain the dependency structure and then uses the classifier from le zhangs maximum entropy modeling toolkit4 to predict the semantic relation foreach dependency.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1661">
<title id=" S12-1050.xml">semeval2012 task 5 chinese semantic dependency parsing </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>ict-1.
</prevsent>
<prevsent>the ict semantic dependency parser employsa system-combining strategy to obtain the dependency structure and then uses the classifier from le zhangs maximum entropy modeling toolkit4 to predict the semantic relation foreach dependency.
</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
the system-combining strategy involves three steps: ? parsing each sentence using nivres arc standard, nivres arc eager (nivre and nilsson, 2005; <papid> P05-1013 </papid>nivre, 2008), <papid> J08-4003 </papid>and liangs dynamic algorithm (huang and sagae, 2010); ? <papid> P10-1110 </papid>combining parses given by the three parsers into weighted directed graph; ? using the chu-liu-edmonds algorithm to search for the final parse for each sen tence.</citsent>
<aftsection>
<nextsent>svm-1-rev we didnt receive the system description of these two systems.
</nextsent>
<nextsent>5 results &amp; analysis.
</nextsent>
<nextsent>las is the main evaluation metric in chinese semantic dependency parsing, whereas uas is the secondary metric.
</nextsent>
<nextsent>table 4 shows the results for these two indicators in all participating systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1662">
<title id=" S12-1050.xml">semeval2012 task 5 chinese semantic dependency parsing </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>ict-1.
</prevsent>
<prevsent>the ict semantic dependency parser employsa system-combining strategy to obtain the dependency structure and then uses the classifier from le zhangs maximum entropy modeling toolkit4 to predict the semantic relation foreach dependency.
</prevsent>
</prevsection>
<citsent citstr=" J08-4003 ">
the system-combining strategy involves three steps: ? parsing each sentence using nivres arc standard, nivres arc eager (nivre and nilsson, 2005; <papid> P05-1013 </papid>nivre, 2008), <papid> J08-4003 </papid>and liangs dynamic algorithm (huang and sagae, 2010); ? <papid> P10-1110 </papid>combining parses given by the three parsers into weighted directed graph; ? using the chu-liu-edmonds algorithm to search for the final parse for each sen tence.</citsent>
<aftsection>
<nextsent>svm-1-rev we didnt receive the system description of these two systems.
</nextsent>
<nextsent>5 results &amp; analysis.
</nextsent>
<nextsent>las is the main evaluation metric in chinese semantic dependency parsing, whereas uas is the secondary metric.
</nextsent>
<nextsent>table 4 shows the results for these two indicators in all participating systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1663">
<title id=" S12-1050.xml">semeval2012 task 5 chinese semantic dependency parsing </title>
<section> participating systems.  </section>
<citcontext>
<prevsection>
<prevsent>ict-1.
</prevsent>
<prevsent>the ict semantic dependency parser employsa system-combining strategy to obtain the dependency structure and then uses the classifier from le zhangs maximum entropy modeling toolkit4 to predict the semantic relation foreach dependency.
</prevsent>
</prevsection>
<citsent citstr=" P10-1110 ">
the system-combining strategy involves three steps: ? parsing each sentence using nivres arc standard, nivres arc eager (nivre and nilsson, 2005; <papid> P05-1013 </papid>nivre, 2008), <papid> J08-4003 </papid>and liangs dynamic algorithm (huang and sagae, 2010); ? <papid> P10-1110 </papid>combining parses given by the three parsers into weighted directed graph; ? using the chu-liu-edmonds algorithm to search for the final parse for each sen tence.</citsent>
<aftsection>
<nextsent>svm-1-rev we didnt receive the system description of these two systems.
</nextsent>
<nextsent>5 results &amp; analysis.
</nextsent>
<nextsent>las is the main evaluation metric in chinese semantic dependency parsing, whereas uas is the secondary metric.
</nextsent>
<nextsent>table 4 shows the results for these two indicators in all participating systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1664">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>capturing such implicit semantic roles and linking them to their antecedents is challenging problem.
</prevsent>
<prevsent>but it bears immense potential for establishing discourse coherence and for getting closer to the aim of true nlu.
</prevsent>
</prevsection>
<citsent citstr=" W09-2417 ">
linking of implicit semantic roles in discourse has recently been introduced as shared task in the semeval 2010 competition linking events and their participants in discourse (ruppenhofer et al,2009, <papid> W09-2417 </papid>2010).</citsent>
<aftsection>
<nextsent>the task consists in detecting unfilled semantic roles of events and determining antecedents in the discourse context that these roles ? the work reported in this paper is based on masters thesis conducted at heidelberg university (silberer, 2011).can be understood to refer to.
</nextsent>
<nextsent>in (1), e.g., the predicate jealousy introduces two implicit roles, one for the experiencer, the other for the object of jealousy involved.
</nextsent>
<nextsent>these roles can be bound to watson and the speaker (i) in the non-local preceding context.
</nextsent>
<nextsent>(1) watson wont allow that know anything of art but that is mere jealousy because our views upon the subject differ.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1665">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> implicit role reference: short history.  </section>
<citcontext>
<prevsection>
<prevsent>the phenomenon of implicit role reference is not new.
</prevsent>
<prevsent>it has been studied in number of early approaches.
</prevsent>
</prevsection>
<citsent citstr=" P86-1004 ">
palmer et al (1986) <papid> P86-1004 </papid>treated unfilled semantic roles as special cases of anaphora and coreference resolution (cr).</citsent>
<aftsection>
<nextsent>resolution was guided by domain knowledge encoded in knowledge based system.
</nextsent>
<nextsent>similarly, whittemore et al (1991) <papid> P91-1003 </papid>analyzed the resolution of unexpressed event roles as special case of cr.</nextsent>
<nextsent>a formalization in drt was fully worked out, but automation was not addressed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1666">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> implicit role reference: short history.  </section>
<citcontext>
<prevsection>
<prevsent>palmer et al (1986) <papid> P86-1004 </papid>treated unfilled semantic roles as special cases of anaphora and coreference resolution (cr).</prevsent>
<prevsent>resolution was guided by domain knowledge encoded in knowledge based system.</prevsent>
</prevsection>
<citsent citstr=" P91-1003 ">
similarly, whittemore et al (1991) <papid> P91-1003 </papid>analyzed the resolution of unexpressed event roles as special case of cr.</citsent>
<aftsection>
<nextsent>a formalization in drt was fully worked out, but automation was not addressed.
</nextsent>
<nextsent>later studies emphasize the role of implicit role reference in frame-semantic discourse analysis.
</nextsent>
<nextsent>fillmore and baker (2001) provide an analysis of 1 newspaper text that indicates the importance of frames and roles in establishing discourse coherence.
</nextsent>
<nextsent>burchardt et al (2005) offer formalization of the involved factors: the interplay of frames and frame relations with factors of contextual contiguity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1667">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> implicit role reference: short history.  </section>
<citcontext>
<prevsection>
<prevsent>the work includes no automation, but suggests acorpus-based approach using antecedent-role coreference patterns collected from corpora.tetreault (2002), finally, offers an automated analysis for resolving implicit role reference.
</prevsent>
<prevsent>the small scale study is embedded in rule-based cr setup.semeval 2010 task 10: linking roles.
</prevsent>
</prevsection>
<citsent citstr=" S10-1008 ">
triggered by the semeval 2010 competition (ruppen hofer et al, 2010), <papid> S10-1008 </papid>research on resolving implicit role reference has gained momentum again, in field where both semantic role labeling (srl) and coreference resolution have seen tremendous progress.however, the systems that participated in the nionly task on implicit role resolution achieved moderate success in the initial subtasks: (i) recognition of implicit roles and (ii) classification as discourse-bound vs. existential interpretation (dni vs. ini).</citsent>
<aftsection>
<nextsent>yet, (iii) identification of role antecedents was bluntly unsuccessful, with around 1% f-score.
</nextsent>
<nextsent>ruppenhofer et al clearly relate the task to coreference resolution.
</nextsent>
<nextsent>the participating systems, though, framed the task as special case of srl.chen et al (2010)<papid> S10-1059 </papid>participated with their srl system semafor (das et al, 2010).<papid> N10-1138 </papid></nextsent>
<nextsent>they cast the task as one of extended srl, by admitting constituents from larger context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1668">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> implicit role reference: short history.  </section>
<citcontext>
<prevsection>
<prevsent>yet, (iii) identification of role antecedents was bluntly unsuccessful, with around 1% f-score.
</prevsent>
<prevsent>ruppenhofer et al clearly relate the task to coreference resolution.
</prevsent>
</prevsection>
<citsent citstr=" S10-1059 ">
the participating systems, though, framed the task as special case of srl.chen et al (2010)<papid> S10-1059 </papid>participated with their srl system semafor (das et al, 2010).<papid> N10-1138 </papid></citsent>
<aftsection>
<nextsent>they cast the task as one of extended srl, by admitting constituents from larger context.
</nextsent>
<nextsent>to overcome the lack andsparsity of syntactic path features, they include lexical association and similarity scores for semantic roles and role fillers; classical srl order and distance features are adapted to larger distances.
</nextsent>
<nextsent>venses++ by tonelli and delmonte (2010) <papid> S10-1065 </papid>isa semantic processing system that includes lexico semantic processing, anaphora resolution and deep semantic resolution components.</nextsent>
<nextsent>anaphora resolution is performed in rule-based manner; pronom inals are replaced with their antecedents?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1670">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> implicit role reference: short history.  </section>
<citcontext>
<prevsection>
<prevsent>yet, (iii) identification of role antecedents was bluntly unsuccessful, with around 1% f-score.
</prevsent>
<prevsent>ruppenhofer et al clearly relate the task to coreference resolution.
</prevsent>
</prevsection>
<citsent citstr=" N10-1138 ">
the participating systems, though, framed the task as special case of srl.chen et al (2010)<papid> S10-1059 </papid>participated with their srl system semafor (das et al, 2010).<papid> N10-1138 </papid></citsent>
<aftsection>
<nextsent>they cast the task as one of extended srl, by admitting constituents from larger context.
</nextsent>
<nextsent>to overcome the lack andsparsity of syntactic path features, they include lexical association and similarity scores for semantic roles and role fillers; classical srl order and distance features are adapted to larger distances.
</nextsent>
<nextsent>venses++ by tonelli and delmonte (2010) <papid> S10-1065 </papid>isa semantic processing system that includes lexico semantic processing, anaphora resolution and deep semantic resolution components.</nextsent>
<nextsent>anaphora resolution is performed in rule-based manner; pronom inals are replaced with their antecedents?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1671">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> implicit role reference: short history.  </section>
<citcontext>
<prevsection>
<prevsent>they cast the task as one of extended srl, by admitting constituents from larger context.
</prevsent>
<prevsent>to overcome the lack andsparsity of syntactic path features, they include lexical association and similarity scores for semantic roles and role fillers; classical srl order and distance features are adapted to larger distances.
</prevsent>
</prevsection>
<citsent citstr=" S10-1065 ">
venses++ by tonelli and delmonte (2010) <papid> S10-1065 </papid>isa semantic processing system that includes lexico semantic processing, anaphora resolution and deep semantic resolution components.</citsent>
<aftsection>
<nextsent>anaphora resolution is performed in rule-based manner; pronom inals are replaced with their antecedents?
</nextsent>
<nextsent>lexical information.
</nextsent>
<nextsent>for role linking, the system applies diverse heuristics including search for predicate argument structures with compatible arguments, aswell as semantic relatedness scores between potential fillers of (overt and implicit) semantic roles.
</nextsent>
<nextsent>more recently tonelli and delmonte (2011) <papid> W11-0908 </papid>recur to leaner approach for role binding, estimating relevance score for potential antecedents from role fillers observed in training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1672">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> implicit role reference: short history.  </section>
<citcontext>
<prevsection>
<prevsent>lexical information.
</prevsent>
<prevsent>for role linking, the system applies diverse heuristics including search for predicate argument structures with compatible arguments, aswell as semantic relatedness scores between potential fillers of (overt and implicit) semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" W11-0908 ">
more recently tonelli and delmonte (2011) <papid> W11-0908 </papid>recur to leaner approach for role binding, estimating relevance score for potential antecedents from role fillers observed in training.</citsent>
<aftsection>
<nextsent>they report an f-scoreof 8 points for role binding on semeval data.
</nextsent>
<nextsent>how ever, being strongly lexicalized, their trained model seems heavily dependent on the training data.
</nextsent>
<nextsent>ruppenhofer et al (2011) use semantic types for identifying dni role antecedents, reporting an error reduction of 14% on chen et al (2010)<papid> S10-1059 </papid>s results.</nextsent>
<nextsent>the poor performance results in the semeval task clearly indicate the difficulty of resolving implicit role reference.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1674">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> implicit role reference: short history.  </section>
<citcontext>
<prevsection>
<prevsent>the poor performance results in the semeval task clearly indicate the difficulty of resolving implicit role reference.
</prevsent>
<prevsent>a major factor seems to relate to datasparsity: the training set covers only 245 dni annotations linked to an antecedent.linking implicit arguments of nominals.
</prevsent>
</prevsection>
<citsent citstr=" P10-1160 ">
gerber and chai (2010) (<papid> P10-1160 </papid>g&c; henceforth) investigate closely related task of argument binding, tied to the linking of implicit arguments for nominal predicates using the propbank role labeling scheme.</citsent>
<aftsection>
<nextsent>in contrast to the semeval task, which focuses on verbs and nouns, their system is only applied to nouns and is restricted to 10 predicates with substantial training set sizes (avg: 125, median: 103).
</nextsent>
<nextsent>g&c; propose discriminative model that selects an antecedent for an implicit role from an extended context window.
</nextsent>
<nextsent>the approach incorporates some aspects relating to cr that go beyond the srl oriented semeval systems: candidate representation includes information about all the candidates?
</nextsent>
<nextsent>co referent mentions (determined by automatic cr), in particular their semantic roles (provided by goldannotations) and wordnet synsets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1675">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> casting implicit role linking as an.  </section>
<citcontext>
<prevsection>
<prevsent>2 in this paper, we explicitly formulate implicit role linking as an anaphora resolution task.
</prevsent>
<prevsent>this is in line with the predominant conception in early work, and also highlights the close relationship with zero anaphora (kameyama, 1985).
</prevsent>
</prevsection>
<citsent citstr=" P09-2022 ">
computational treatments of zero anaphora (e.g., imamura et al (2009)) <papid> P09-2022 </papid>are in fact employing techniques well-known from srl.</citsent>
<aftsection>
<nextsent>recent work by iida and poesio (2011), <papid> P11-1081 </papid>by contrast, offers an analysis of zero anaphora in acr architecture.</nextsent>
<nextsent>further support comes from psy cho linguistic studies in garrod and terras (2000), who establish commonalities between implicit role reference and other types of anaphora resolution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1676">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> casting implicit role linking as an.  </section>
<citcontext>
<prevsection>
<prevsent>this is in line with the predominant conception in early work, and also highlights the close relationship with zero anaphora (kameyama, 1985).
</prevsent>
<prevsent>computational treatments of zero anaphora (e.g., imamura et al (2009)) <papid> P09-2022 </papid>are in fact employing techniques well-known from srl.</prevsent>
</prevsection>
<citsent citstr=" P11-1081 ">
recent work by iida and poesio (2011), <papid> P11-1081 </papid>by contrast, offers an analysis of zero anaphora in acr architecture.</citsent>
<aftsection>
<nextsent>further support comes from psy cho linguistic studies in garrod and terras (2000), who establish commonalities between implicit role reference and other types of anaphora resolution.
</nextsent>
<nextsent>the contributions of our work are as follows:i. we cast implicit role binding as cr task, using an entity-mention model and discriminative classification for antecedent selection.
</nextsent>
<nextsent>ii.
</nextsent>
<nextsent>we examine the effectiveness of model features for classical srl vs. cr features to clarify the nature of this special phenomenon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1677">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> casting implicit role linking as an.  </section>
<citcontext>
<prevsection>
<prevsent>i. an entity-mention model for anaphoric role resolution.
</prevsent>
<prevsent>in our model implicit roles that are discourse-bound (i.e. classified as dni) are treated as anaphoric, similar to zero anaphora: the implicit role will be bound to discourse antecedent.
</prevsent>
</prevsection>
<citsent citstr=" C10-1017 ">
in line with recent research in cr, we adopt an entity-mention model, where an entity is represented by all mentions pertaining to coreference chain (see i.a. rahman and ng (2011), cai and strube(2010)).<papid> C10-1017 </papid></citsent>
<aftsection>
<nextsent>our model is based on binary classifier decisions that take as input the anaphoric role and an entity candidate from the preceding discourse.
</nextsent>
<nextsent>the final classification of role linking to an entity is obtained by discriminative ranking of the binary clas sifiers?
</nextsent>
<nextsent>probability estimates.
</nextsent>
<nextsent>details on the system architecture are given in section 3.2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1678">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> casting implicit role linking as an.  </section>
<citcontext>
<prevsection>
<prevsent>the next step consists in the creation of (training) instances for classification including the extraction of features for all instances.
</prevsent>
<prevsent>an instance instej ,dk consists of the active dnidk, its frame and candidate entity ej ? ek.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
instance creation follows an entity-based adaption of the standard procedure of soon et al (2001), <papid> J01-4004 </papid>which has been applied by yang et al (2004), <papid> C04-1033 </papid>yang et al (2008).<papid> P08-1096 </papid></citsent>
<aftsection>
<nextsent>processing the discourse from left to right, for each dni dk, instances ik are created by processing ek from right to left according to each entitys most recent mention, starting with the entity closest to dk.
</nextsent>
<nextsent>note that, as entities instead of mentions are considered, only one instance is created for an entity which is mentioned several times in the search space.
</nextsent>
<nextsent>in training, the instance creation stops when the correct antecedent, i.e. positive instance, as well as at least one negative instance have been found.1(3) classification.
</nextsent>
<nextsent>from the acquired training instances we learn binary classifier that predicts foran instance instej ,dk whether it is positive, i.e. entity ej is correct antecedent for dni dk.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1679">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> casting implicit role linking as an.  </section>
<citcontext>
<prevsection>
<prevsent>the next step consists in the creation of (training) instances for classification including the extraction of features for all instances.
</prevsent>
<prevsent>an instance instej ,dk consists of the active dnidk, its frame and candidate entity ej ? ek.
</prevsent>
</prevsection>
<citsent citstr=" C04-1033 ">
instance creation follows an entity-based adaption of the standard procedure of soon et al (2001), <papid> J01-4004 </papid>which has been applied by yang et al (2004), <papid> C04-1033 </papid>yang et al (2008).<papid> P08-1096 </papid></citsent>
<aftsection>
<nextsent>processing the discourse from left to right, for each dni dk, instances ik are created by processing ek from right to left according to each entitys most recent mention, starting with the entity closest to dk.
</nextsent>
<nextsent>note that, as entities instead of mentions are considered, only one instance is created for an entity which is mentioned several times in the search space.
</nextsent>
<nextsent>in training, the instance creation stops when the correct antecedent, i.e. positive instance, as well as at least one negative instance have been found.1(3) classification.
</nextsent>
<nextsent>from the acquired training instances we learn binary classifier that predicts foran instance instej ,dk whether it is positive, i.e. entity ej is correct antecedent for dni dk.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1680">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> casting implicit role linking as an.  </section>
<citcontext>
<prevsection>
<prevsent>the next step consists in the creation of (training) instances for classification including the extraction of features for all instances.
</prevsent>
<prevsent>an instance instej ,dk consists of the active dnidk, its frame and candidate entity ej ? ek.
</prevsent>
</prevsection>
<citsent citstr=" P08-1096 ">
instance creation follows an entity-based adaption of the standard procedure of soon et al (2001), <papid> J01-4004 </papid>which has been applied by yang et al (2004), <papid> C04-1033 </papid>yang et al (2008).<papid> P08-1096 </papid></citsent>
<aftsection>
<nextsent>processing the discourse from left to right, for each dni dk, instances ik are created by processing ek from right to left according to each entitys most recent mention, starting with the entity closest to dk.
</nextsent>
<nextsent>note that, as entities instead of mentions are considered, only one instance is created for an entity which is mentioned several times in the search space.
</nextsent>
<nextsent>in training, the instance creation stops when the correct antecedent, i.e. positive instance, as well as at least one negative instance have been found.1(3) classification.
</nextsent>
<nextsent>from the acquired training instances we learn binary classifier that predicts foran instance instej ,dk whether it is positive, i.e. entity ej is correct antecedent for dni dk.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1681">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> casting implicit role linking as an.  </section>
<citcontext>
<prevsection>
<prevsent>we obtain classifications for all instances in ik.
</prevsent>
<prevsent>among the positive classified instances, we select the antecedent with the highest estimate.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
that is, we apply the best-first strategy (ng and cardie, 2002).<papid> P02-1014 </papid></citsent>
<aftsection>
<nextsent>in case of tie, we choose the antecedent which is closer to the target.
</nextsent>
<nextsent>if no instance is classified as positive, dk is left unfilled.
</nextsent>
<nextsent>4.1 semeval 2010 task and dataset.
</nextsent>
<nextsent>we adhere to the semeval 2010 task by ruppenhofer et al (2009) <papid> W09-2417 </papid>as test bed for our experiments.the main focus of our work is on part (iii), the identification of antecedents for dnis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1683">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> data and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the test data comprise 710 nis (349 dnis, 361 inis), of which 259 dnis are linked.
</prevsent>
<prevsent>4.2 heuristic data acquisition.
</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
since the training data has critically small amount of linked dnis, we heuristic ally labeled training data on the basis of datasets with manually annotated coreference information: ontonotes 3.0 (hovy et al, 2006), <papid> N06-2015 </papid>as well as ace-2 (mitchell et al, 2003) and muc-6 (chinchor and sundheim, 2003).ontonotes 3.0 was merged with gold srl annotations from the conll-2005 shared task.</citsent>
<aftsection>
<nextsent>by means of semlink-1.1 (loper et al, 2007) and amapping included in the semeval data, these propbank (pb, palmer et al (2005)) <papid> J05-1004 </papid>annotations were 1we additionally impose several restrictions, e.g., valid candidate must not already fill another role of the active frame.</nextsent>
<nextsent>4 #ent avg avg #frames #frame#dni #dni #ent/doc size types types semeval 141 141 9 1,370 317 245 155 onotes 7899 23 3 12,770 258 2,220 270 ace-2 3564 11 4 58,204 757 4,265 578 muc-6 1841 15 3 20,140 654 997 310 corpus coref semantic roles onotes manual manual pb conll05, ported to fn ace-2 manual automatic fn (semafor) muc-6 manual automatic fn (semafor) table 1: semeval vs. heuristic ally acquired data mapped to their framenet (fn) counterparts, if ex istent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1684">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> data and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 heuristic data acquisition.
</prevsent>
<prevsent>since the training data has critically small amount of linked dnis, we heuristic ally labeled training data on the basis of datasets with manually annotated coreference information: ontonotes 3.0 (hovy et al, 2006), <papid> N06-2015 </papid>as well as ace-2 (mitchell et al, 2003) and muc-6 (chinchor and sundheim, 2003).ontonotes 3.0 was merged with gold srl annotations from the conll-2005 shared task.</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
by means of semlink-1.1 (loper et al, 2007) and amapping included in the semeval data, these propbank (pb, palmer et al (2005)) <papid> J05-1004 </papid>annotations were 1we additionally impose several restrictions, e.g., valid candidate must not already fill another role of the active frame.</citsent>
<aftsection>
<nextsent>4 #ent avg avg #frames #frame#dni #dni #ent/doc size types types semeval 141 141 9 1,370 317 245 155 onotes 7899 23 3 12,770 258 2,220 270 ace-2 3564 11 4 58,204 757 4,265 578 muc-6 1841 15 3 20,140 654 997 310 corpus coref semantic roles onotes manual manual pb conll05, ported to fn ace-2 manual automatic fn (semafor) muc-6 manual automatic fn (semafor) table 1: semeval vs. heuristic ally acquired data mapped to their framenet (fn) counterparts, if existent.
</nextsent>
<nextsent>for the ace-2 and muc-6 corpora, we usedsemafor (das and smith, 2011) <papid> P11-1144 </papid>for automatic annotation with fn semantic roles.</nextsent>
<nextsent>from these datasets we acquired heuristic ally annotated instances of role linking using the strategy explained in 3.1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1685">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> data and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>by means of semlink-1.1 (loper et al, 2007) and amapping included in the semeval data, these propbank (pb, palmer et al (2005)) <papid> J05-1004 </papid>annotations were 1we additionally impose several restrictions, e.g., valid candidate must not already fill another role of the active frame.</prevsent>
<prevsent>4 #ent avg avg #frames #frame#dni #dni #ent/doc size types types semeval 141 141 9 1,370 317 245 155 onotes 7899 23 3 12,770 258 2,220 270 ace-2 3564 11 4 58,204 757 4,265 578 muc-6 1841 15 3 20,140 654 997 310 corpus coref semantic roles onotes manual manual pb conll05, ported to fn ace-2 manual automatic fn (semafor) muc-6 manual automatic fn (semafor) table 1: semeval vs. heuristic ally acquired data mapped to their framenet (fn) counterparts, if ex istent.</prevsent>
</prevsection>
<citsent citstr=" P11-1144 ">
for the ace-2 and muc-6 corpora, we usedsemafor (das and smith, 2011) <papid> P11-1144 </papid>for automatic annotation with fn semantic roles.</citsent>
<aftsection>
<nextsent>from these datasets we acquired heuristic ally annotated instances of role linking using the strategy explained in 3.1.
</nextsent>
<nextsent>table 1 summarizes the resulting training data.
</nextsent>
<nextsent>the heuristic ally labeled data extends the manually labeled dni instances by an order of magnitude.
</nextsent>
<nextsent>4.3 model parameters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1686">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> data and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>9: selprefs.
</prevsent>
<prevsent>we compute selectional preferences following the information-theoretic approach of resnik (1993), approach of resnik (1996).
</prevsent>
</prevsection>
<citsent citstr=" P07-1028 ">
similar to erk (2007), <papid> P07-1028 </papid>weused an adapted version which we computed for semantic roles by means of the fn database rather than for verb argument positions.</citsent>
<aftsection>
<nextsent>the wordnet classes over which the preferences are defined are wordnet lexicographers files (supersenses).
</nextsent>
<nextsent>the selectional association values ?(dni, ss) of the dnis selectional preferences are retrieved for the super sense ss of each candidate antecedentshead.
</nextsent>
<nextsent>as for feat.
</nextsent>
<nextsent>1, we define candidates feature value by its rankin the ordered list of these s. 4.5 experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1688">
<title id=" S12-1001.xml">casting implicit role linking as an anaphora resolution task </title>
<section> data and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>if an antecedent em can be determined for predicted dni, the role is labeled as such and linked to em.
</prevsent>
<prevsent>as the dnis role has been filled now, competing or redundant dnis are removed from df before moving to the next predicted dni.
</prevsent>
</prevsection>
<citsent citstr=" W11-1907 ">
only dnis for which an antecedent is found are labeled as such.exp2 is evaluated on both gold coreference annotation and automatically assigned coreference chains, using the cr system of cai et al (2011).<papid> W11-1907 </papid></citsent>
<aftsection>
<nextsent>5.1 exp1: dni linking evaluation.
</nextsent>
<nextsent>table 4 shows the best performing models for dni linking for each parameter setting8.
</nextsent>
<nextsent>we compare them to strong baseline prom (last row) that links each dni to the antecedent candidate with highest prominence score.
</nextsent>
<nextsent>its f1-score is beaten by the other models, with gain of 7.2 points for model m1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1690">
<title id=" S10-1063.xml">tipsem english and spanish evaluating crfs and semantic roles in tempeval2 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the automatic treatment of time expressions, events and their relations over natural language text consists of making temporal elements explicit through system that identifies and annotates them following standard scheme.
</prevsent>
<prevsent>this information is crucial for other natural language processing (nlp) areas, such as summarization or question answering.
</prevsent>
</prevsection>
<citsent citstr=" W07-2014 ">
the relevance of temporal information has been reflected in specialized conferences (schilder et al, 2007) and evaluation forums (verhagen et al, 2007).<papid> W07-2014 </papid></citsent>
<aftsection>
<nextsent>we present system to tackle the six different tasks related to multilingual temporal information treatment proposed in tempeval-2.
</nextsent>
<nextsent>particularly, in this evaluation exercise, timeml (pustejovsky et al, 2003) is adopted as temporal annotationscheme.
</nextsent>
<nextsent>in this manner, the tasks require participating systems to automatically annotate different timeml elements.
</nextsent>
<nextsent>firstly, task consists of determining the extent of time expressions as defined by the timeml timex3 tag, as well as the attributes type?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1691">
<title id=" S10-1063.xml">tipsem english and spanish evaluating crfs and semantic roles in tempeval2 </title>
<section> approach motivation.  </section>
<citcontext>
<prevsection>
<prevsent>secondly, some timex3 and event elements are denoted by sequences of words, therefore the crfs are very appropriate.
</prevsent>
<prevsent>2.2 semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
semantic role labeling (srl) has achieved important results in the last years (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moreda et al, 2007).</citsent>
<aftsection>
<nextsent>for each predicate in sentence, semantic roles identify all constituents, determining their arguments (agent, patient, etc.)and their adjuncts (locative, temporal, etc.).
</nextsent>
<nextsent>figure 2 illustrates semantic role labeled sentence.
</nextsent>
<nextsent>figure 2: semantic roles example semantic roles provide structural relations of the predicates in which timeml elements may 1iob2 format: (b)egin, (i)nside, and (o)utside participate.
</nextsent>
<nextsent>beyond syntactic relations expressed by means of the different types of phrases, semantic roles give further information about semantic relations between the arguments of predicate.due to the fact that roles represent high level information, they are more independent from word tokens.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1692">
<title id=" S10-1063.xml">tipsem english and spanish evaluating crfs and semantic roles in tempeval2 </title>
<section> our approach: tipsem.  </section>
<citcontext>
<prevsection>
<prevsent>syntactic: different timeml elements are contained in particular types of phrases.
</prevsent>
<prevsent>this feature tries to capture this fact by considering phrase level syntactic information.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
the syntactic tree was obtained using charniak parser (charniak and johnson, 2005) <papid> P05-1022 </papid>for english, and ancora for spanish.?</citsent>
<aftsection>
<nextsent>polarity, tense and aspect: these were obtained using pos and set of handcrafted rules (e.g., will+verb ? future).
</nextsent>
<nextsent>the semantic level features used to enhance the training framework of the crf model are: ? role: for each token, we considered the role regarding the verb the token depends on.to get semantic roles, ccg srl tool (pun yakanok et al, 2004) <papid> W04-2421 </papid>was used for english, and ancora for spanish.?</nextsent>
<nextsent>governing verb: the verb to which the current token holds particular role.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1693">
<title id=" S10-1063.xml">tipsem english and spanish evaluating crfs and semantic roles in tempeval2 </title>
<section> our approach: tipsem.  </section>
<citcontext>
<prevsection>
<prevsent>the syntactic tree was obtained using charniak parser (charniak and johnson, 2005) <papid> P05-1022 </papid>for english, and ancora for spanish.?</prevsent>
<prevsent>polarity, tense and aspect: these were obtained using pos and set of handcrafted rules (e.g., will+verb ? future).</prevsent>
</prevsection>
<citsent citstr=" W04-2421 ">
the semantic level features used to enhance the training framework of the crf model are: ? role: for each token, we considered the role regarding the verb the token depends on.to get semantic roles, ccg srl tool (pun yakanok et al, 2004) <papid> W04-2421 </papid>was used for english, and ancora for spanish.?</citsent>
<aftsection>
<nextsent>governing verb: the verb to which the current token holds particular role.
</nextsent>
<nextsent>this may distinguish tokens appearing under the influence of different verbs.
</nextsent>
<nextsent>role+verb combination: the previous two features were combined to capture the relation between them.
</nextsent>
<nextsent>this introduces additional information by distinguishing roles depending on different verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1694">
<title id=" S12-1087.xml">uniba distributional semantics for textual similarity </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" S12-1051 ">
semeval-2012 semantic textual similarity (sts)task (agirre et al, 2012) <papid> S12-1051 </papid>aims at providing general framework to examine the degree of semantic equivalence between two sentences.?</citsent>
<aftsection>
<nextsent>we propose an approach to semantic textual similarity based on distributional models of words,where the geometrical metaphor of meaning is exploited.
</nextsent>
<nextsent>distributional models are grounded on the distributional hypothesis (harris, 1968), according to which the meaning of word is determined by the set of textual contexts in which it appears.
</nextsent>
<nextsent>these models represent words as vectors in high dimensional vector space.
</nextsent>
<nextsent>word vectors are built from large corpus in such way that vector dimensions reflect the different uses (or contexts) of word inthe corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1695">
<title id=" S12-1087.xml">uniba distributional semantics for textual similarity </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>however, all of them intend to represent semantics ata word scale.
</prevsent>
<prevsent>although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words.
</prevsent>
</prevsection>
<citsent citstr=" W11-2505 ">
some efforts toward this direction are emerging (clark and pulman, 2007; clark et al, 2008; mitchell and lapata, 2010; coecke et al, 2010; basile et al, 2011; <papid> W11-2505 </papid>clarke,2012), <papid> J12-1002 </papid>which resulted in theoretical work corroborated by empirical evaluation on how small fragments of text compose (e.g. noun-noun, adjective noun, and verb-noun pairs).</citsent>
<aftsection>
<nextsent>our approach to sts is inspired by the latest developments about semantic compositionality and distributional models.
</nextsent>
<nextsent>the general methodology is based on the construction of semantic space endowed 591 with vector addition operator.
</nextsent>
<nextsent>the vector addition sums the word vectors of each pair of sentences involved in the evaluation.
</nextsent>
<nextsent>the result consists of two vectors whose similarity can be computed by co sine similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1696">
<title id=" S12-1087.xml">uniba distributional semantics for textual similarity </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>however, all of them intend to represent semantics ata word scale.
</prevsent>
<prevsent>although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words.
</prevsent>
</prevsection>
<citsent citstr=" J12-1002 ">
some efforts toward this direction are emerging (clark and pulman, 2007; clark et al, 2008; mitchell and lapata, 2010; coecke et al, 2010; basile et al, 2011; <papid> W11-2505 </papid>clarke,2012), <papid> J12-1002 </papid>which resulted in theoretical work corroborated by empirical evaluation on how small fragments of text compose (e.g. noun-noun, adjective noun, and verb-noun pairs).</citsent>
<aftsection>
<nextsent>our approach to sts is inspired by the latest developments about semantic compositionality and distributional models.
</nextsent>
<nextsent>the general methodology is based on the construction of semantic space endowed 591 with vector addition operator.
</nextsent>
<nextsent>the vector addition sums the word vectors of each pair of sentences involved in the evaluation.
</nextsent>
<nextsent>the result consists of two vectors whose similarity can be computed by co sine similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1697">
<title id=" S12-1087.xml">uniba distributional semantics for textual similarity </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, it allow sus to solve efficiently the problem of reducing dimensions, which is one of the key features used to uncover the latent semantic dimensions?
</prevsent>
<prevsent>of word distribution.
</prevsent>
</prevsection>
<citsent citstr=" L08-1028 ">
ri1 (widdows and ferraro, 2008) <papid> L08-1028 </papid>is based on the concept of random projection according to which high dimensional vectors chosen randomly are nearly orthogonal?.</citsent>
<aftsection>
<nextsent>formally, given an ? matrix and an ? matrix made up of m-dimensional random vectors, we define new n?
</nextsent>
<nextsent>k matrix as follows: bn,k = an,mrm,k    (1) the new matrix has the property to preserve the distance between points scaled by multiplicative factor (johnson and linden strauss, 1984).
</nextsent>
<nextsent>specifically, ri creates the semantic space bn,k in two steps (we consider fixed window of terms as context): 1.
</nextsent>
<nextsent>a context vector is assigned to each term.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1699">
<title id=" S12-1077.xml">sbdlrhmn a rule based human interpretation system for semantic textual similarity task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as such one of its main goals, the system suggests set of domain-free rules to help the human annotator in scoring semantic equivalence of two sentences.
</prevsent>
<prevsent>the second system is our baseline in which we use the cosine similarity between the words in each sentence pair.
</prevsent>
</prevsection>
<citsent citstr=" E09-1065 ">
accurately establishing sentence semantic similarity would provide one of the key ingredients for solutions to many text-related applications, such as automatic grading systems (mohler and mihalcea, 2009), <papid> E09-1065 </papid>paraphrasing (fernando and stevenson, 2008), text entailment (corley et al, 2005) and summarization (erkan and radev, 2004).</citsent>
<aftsection>
<nextsent>current approaches for computing semantic similarity between pair of sentences focus on analyzing their shared words (salton, 1989), structures (hu et al 2011;mandreoli et al 2002), semantics (mihalcea et al 2006; le el al. 2006; hatzivassiloglou, 1999) or any of their combinations (liu et al 2008; foltz et al 1998).
</nextsent>
<nextsent>the goal is to arrive at score which increases proportionally with the relatedness between the two sentences.
</nextsent>
<nextsent>yet, they are not concerned with scoring the interpretations of such relatedness (zhang et al 2011; jesus et al 2011; wenyin et al 2010; liu et al 2008).
</nextsent>
<nextsent>semantic textual similarity (sts), semeval 12 task 6 (agirre et al 2012), <papid> S12-1051 </papid>measures the degree.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1700">
<title id=" S12-1077.xml">sbdlrhmn a rule based human interpretation system for semantic textual similarity task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal is to arrive at score which increases proportionally with the relatedness between the two sentences.
</prevsent>
<prevsent>yet, they are not concerned with scoring the interpretations of such relatedness (zhang et al 2011; jesus et al 2011; wenyin et al 2010; liu et al 2008).
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
semantic textual similarity (sts), semeval 12 task 6 (agirre et al 2012), <papid> S12-1051 </papid>measures the degree.</citsent>
<aftsection>
<nextsent>of semantic equivalence between pair of sentences by comparing meaningful contents within sentence.
</nextsent>
<nextsent>the assigned scores range from 0 to 5 for each sentence pair with the following interpreta tions: (5) completely equivalent, (4) mostly equivalent pair with missing unimportant information, (3) roughly equivalent with missing important information, (2) not equivalent, but sharing some details, (1) not equivalent but sharing the same topic and (0) not equivalent and on different topics.
</nextsent>
<nextsent>the goal of developing our rule-based system was to identify knowledge representations which have possibly all task human interpretations.
</nextsent>
<nextsent>meanwhile, the system domain-free rules aim to help the human annotator in scoring semantic equivalence of sentence pair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1703">
<title id=" S12-1077.xml">sbdlrhmn a rule based human interpretation system for semantic textual similarity task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>third, wordnet (miller, 1995) and adapted lesk algorithm for word sense disambiguation (banerjee and pedersen, 2010) are used to compute each sentence word semantic relatedness to the other sentence.
</prevsent>
<prevsent>reverb (etzioni et al 2011) augments wordnet in case of uncovered words and helps us to discriminate the topics of sentences.
</prevsent>
</prevsection>
<citsent citstr=" W07-1417 ">
we use (blake, 2007) <papid> W07-1417 </papid>thought to compare the sentence pair words with each other.</citsent>
<aftsection>
<nextsent>finally, we evolve rule-based module to present the human heuristics when he interprets the relatedness of the sentence pair meaningful contents.
</nextsent>
<nextsent>throughout our training and testing experiments, we used task6 corpora (agirre et al 2012) <papid> S12-1051 </papid>namely msrpar, msrvid, smteuroparl, onwn and smtnews; where: - msrpar is 1500 pairs of sentences of msr paraphrase, microsoft research paraphrase cor pus; 750 for training and 750 for testing.</nextsent>
<nextsent>- msrvid is 1500 pairs of sentences of msr video, microsoft research video description corpus; 750 for training and 750 for testing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1706">
<title id=" S12-1077.xml">sbdlrhmn a rule based human interpretation system for semantic textual similarity task </title>
<section> the proposed systems.  </section>
<citcontext>
<prevsection>
<prevsent>so, we shortened the sentence length to subject-vp which includes the underlying comparable words.
</prevsent>
<prevsent>relatedness score (s1, s2) unrelated 0  = ws  0.3 weakly related 0.3  = ws  0.85 strongly related ws  = 0.85 table 1 ? mapping relatedness to wordnet similarity table 1 describes the proposed system wordnet thresholds through our relatedness definitions.
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
the thresholds were thoroughly selected depending on our analysis for the wordnet hierarchary and semantic similarity measures (pedersen et al, 2004).<papid> N04-3012 </papid></citsent>
<aftsection>
<nextsent>we obsereved that while most of the nearest tree sibilings and parent-child nodes scores have more than 0.85 wordnet semantic scores, most of the farther est ones have scores less than 0.3.
</nextsent>
<nextsent>in between these extremes, there is group of scattered tree nodes which ranges from 0.3 to 0.85.
</nextsent>
<nextsent>the number of nodes per each mentioned group is related to the semantic simlarity measure technique.
</nextsent>
<nextsent>2.4 semantics ? using reverb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1713">
<title id=" S10-1091.xml">hitcir an unsupervised wsd system based on domain most frequent sense estimation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the recall of this system is 43.5% on semeval 2 task 17 english dataset.
</prevsent>
<prevsent>tagging polysemous word with its most frequent sense (mfs) is popular back-off heuristic in word sense disambiguation (wsd) systems whenthe training data is inadequate.
</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
in past evaluations, mfs from wordnet performed even better than most of the unsupervised systems (snyder and palmer, 2004; <papid> W04-0811 </papid>navigli et al, 2007).<papid> W07-2006 </papid></citsent>
<aftsection>
<nextsent>mfs is usually obtained from large scale sense tagged corpus, such as semcor (miller et al, 1994).
</nextsent>
<nextsent>however, some polysemous words have different mfs in different domains.
</nextsent>
<nextsent>for example, in the koeling et al (2005) <papid> H05-1053 </papid>corpus, target word coach means manager?</nextsent>
<nextsent>mostly in the sports domain but means bus?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1714">
<title id=" S10-1091.xml">hitcir an unsupervised wsd system based on domain most frequent sense estimation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the recall of this system is 43.5% on semeval 2 task 17 english dataset.
</prevsent>
<prevsent>tagging polysemous word with its most frequent sense (mfs) is popular back-off heuristic in word sense disambiguation (wsd) systems whenthe training data is inadequate.
</prevsent>
</prevsection>
<citsent citstr=" W07-2006 ">
in past evaluations, mfs from wordnet performed even better than most of the unsupervised systems (snyder and palmer, 2004; <papid> W04-0811 </papid>navigli et al, 2007).<papid> W07-2006 </papid></citsent>
<aftsection>
<nextsent>mfs is usually obtained from large scale sense tagged corpus, such as semcor (miller et al, 1994).
</nextsent>
<nextsent>however, some polysemous words have different mfs in different domains.
</nextsent>
<nextsent>for example, in the koeling et al (2005) <papid> H05-1053 </papid>corpus, target word coach means manager?</nextsent>
<nextsent>mostly in the sports domain but means bus?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1715">
<title id=" S10-1091.xml">hitcir an unsupervised wsd system based on domain most frequent sense estimation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mfs is usually obtained from large scale sense tagged corpus, such as semcor (miller et al, 1994).
</prevsent>
<prevsent>however, some polysemous words have different mfs in different domains.
</prevsent>
</prevsection>
<citsent citstr=" H05-1053 ">
for example, in the koeling et al (2005) <papid> H05-1053 </papid>corpus, target word coach means manager?</citsent>
<aftsection>
<nextsent>mostly in the sports domain but means bus?
</nextsent>
<nextsent>mostly in the finance domain.
</nextsent>
<nextsent>so when the mfs is applied to specific domains, it needs to be re-estimated.mccarthy et al (2007) <papid> J07-4005 </papid>proposed an unsupervised predominant word sense acquisition method which obtains domain specific mfs without sense tagged corpus.</nextsent>
<nextsent>in their method, thesaurus, inwhich words are connected with their distributional similarity, is constructed from the domain raw text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1716">
<title id=" S10-1091.xml">hitcir an unsupervised wsd system based on domain most frequent sense estimation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mostly in the sports domain but means bus?
</prevsent>
<prevsent>mostly in the finance domain.
</prevsent>
</prevsection>
<citsent citstr=" J07-4005 ">
so when the mfs is applied to specific domains, it needs to be re-estimated.mccarthy et al (2007) <papid> J07-4005 </papid>proposed an unsupervised predominant word sense acquisition method which obtains domain specific mfs without sense tagged corpus.</citsent>
<aftsection>
<nextsent>in their method, thesaurus, inwhich words are connected with their distributional similarity, is constructed from the domain raw text.
</nextsent>
<nextsent>word senses are ranked by their prevalence score which is calculated using the thesaurus and the sense inventory.in this paper, we propose another way to construct the thesaurus.
</nextsent>
<nextsent>we use statistical machine figure 1: the architecture of hit-cir translation (smt) techniques to extract paraphrase pairs from bilingual parallel text.
</nextsent>
<nextsent>in this way, we avoid calculating similarities between every pair of words and could find semantic similar words or compounds which have dissimilar distributions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1718">
<title id=" S10-1091.xml">hitcir an unsupervised wsd system based on domain most frequent sense estimation </title>
<section> dataset and system settings.  </section>
<citcontext>
<prevsection>
<prevsent>thesize of the raw text is around 15.5mb after simple text cleaning.
</prevsent>
<prevsent>the test data is from wwf andecnc, and contains 1398 occurrence of 436 target words.
</prevsent>
</prevsection>
<citsent citstr=" P07-1096 ">
for the implementation, we used bpos (shen et al., 2007) <papid> P07-1096 </papid>for the pos tagging.</citsent>
<aftsection>
<nextsent>the maximum 409 number of the neighbor word of each target word was set to 50.
</nextsent>
<nextsent>we employed giza++ 1 and moses 2to get the phrase table from the bilingual parallel corpus.
</nextsent>
<nextsent>thewordnet::similarity package 3 was applied for the implement of the lesk word sense similarity algorithm.
</nextsent>
<nextsent>for the target word that is not in the polysemous word list, we use the mfs from wordnet as the back-off method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1719">
<title id=" S12-1060.xml">takelab systems for measuring semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we predict the human ratings of sentence similarity using support vector regression model with multiple features measuring word-overlap similarity and syntax similarity.
</prevsent>
<prevsent>out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall pearson ? 2nd and 3rd, normalized pearson ? 1st and 3rd, weighted mean ? 2nd and 5th).
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
natural language processing tasks such as text classification (sebastiani, 2002), text summarization(lin and hovy, 2003; <papid> N03-1020 </papid>ali guliyev, 2009), information retrieval (park et al, 2005), and word sense disambiguation (schutze, 1998) relyon measure of semantic similarity of textual documents.</citsent>
<aftsection>
<nextsent>research predominantly focused either on the document similarity (salton et al, 1975; maguitman et al, 2005) or the word similarity (budanitsky and hirst, 2006; <papid> J06-1003 </papid>agirre et al, 2009).<papid> N09-1003 </papid></nextsent>
<nextsent>evaluating the similarity of short texts such as sentences or paragraphs (islam and inkpen, 2008; mihalcea et al, 2006; oliva et al., 2011) received less attention from the research community.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1720">
<title id=" S12-1060.xml">takelab systems for measuring semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall pearson ? 2nd and 3rd, normalized pearson ? 1st and 3rd, weighted mean ? 2nd and 5th).
</prevsent>
<prevsent>natural language processing tasks such as text classification (sebastiani, 2002), text summarization(lin and hovy, 2003; <papid> N03-1020 </papid>ali guliyev, 2009), information retrieval (park et al, 2005), and word sense disambiguation (schutze, 1998) relyon measure of semantic similarity of textual documents.</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
research predominantly focused either on the document similarity (salton et al, 1975; maguitman et al, 2005) or the word similarity (budanitsky and hirst, 2006; <papid> J06-1003 </papid>agirre et al, 2009).<papid> N09-1003 </papid></citsent>
<aftsection>
<nextsent>evaluating the similarity of short texts such as sentences or paragraphs (islam and inkpen, 2008; mihalcea et al, 2006; oliva et al., 2011) received less attention from the research community.
</nextsent>
<nextsent>the task of recognizing paraphrases (michel et al, 2011; socher et al, 2011; wan et al., 2006) is sufficiently similar to reuse some of the techniques.this paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the semeval-2012 semantic text similarity task (agirre et al, 2012).<papid> S12-1051 </papid></nextsent>
<nextsent>we propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1721">
<title id=" S12-1060.xml">takelab systems for measuring semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall pearson ? 2nd and 3rd, normalized pearson ? 1st and 3rd, weighted mean ? 2nd and 5th).
</prevsent>
<prevsent>natural language processing tasks such as text classification (sebastiani, 2002), text summarization(lin and hovy, 2003; <papid> N03-1020 </papid>ali guliyev, 2009), information retrieval (park et al, 2005), and word sense disambiguation (schutze, 1998) relyon measure of semantic similarity of textual documents.</prevsent>
</prevsection>
<citsent citstr=" N09-1003 ">
research predominantly focused either on the document similarity (salton et al, 1975; maguitman et al, 2005) or the word similarity (budanitsky and hirst, 2006; <papid> J06-1003 </papid>agirre et al, 2009).<papid> N09-1003 </papid></citsent>
<aftsection>
<nextsent>evaluating the similarity of short texts such as sentences or paragraphs (islam and inkpen, 2008; mihalcea et al, 2006; oliva et al., 2011) received less attention from the research community.
</nextsent>
<nextsent>the task of recognizing paraphrases (michel et al, 2011; socher et al, 2011; wan et al., 2006) is sufficiently similar to reuse some of the techniques.this paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the semeval-2012 semantic text similarity task (agirre et al, 2012).<papid> S12-1051 </papid></nextsent>
<nextsent>we propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1723">
<title id=" S12-1060.xml">takelab systems for measuring semantic text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>research predominantly focused either on the document similarity (salton et al, 1975; maguitman et al, 2005) or the word similarity (budanitsky and hirst, 2006; <papid> J06-1003 </papid>agirre et al, 2009).<papid> N09-1003 </papid></prevsent>
<prevsent>evaluating the similarity of short texts such as sentences or paragraphs (islam and inkpen, 2008; mihalcea et al, 2006; oliva et al., 2011) received less attention from the research community.</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
the task of recognizing paraphrases (michel et al, 2011; socher et al, 2011; wan et al., 2006) is sufficiently similar to reuse some of the techniques.this paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the semeval-2012 semantic text similarity task (agirre et al, 2012).<papid> S12-1051 </papid></citsent>
<aftsection>
<nextsent>we propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses.
</nextsent>
<nextsent>our two systems, simple and syntax, use supervised machine learning, more specifically the support vector regression (svr), to combine large amount of features computed from pairs of sentences.
</nextsent>
<nextsent>the two systems differ in the set of features they employ.our systems placed in the top 5 (out of 89 submitted systems) for all three aggregate correlation measures: 2nd (syntax) and 3rd (simple) for overall pearson, 1st (simple) and 3rd (syntax) for normalized pearson, and 2nd (simple) and 5th (syntax) for weighted mean.
</nextsent>
<nextsent>the rest of the paper is structured as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1726">
<title id=" S12-1060.xml">takelab systems for measuring semantic text similarity </title>
<section> word similarity measures.  </section>
<citcontext>
<prevsection>
<prevsent>given two words, their similarity can be estimated by considering their relative positions within the knowledge base hierarchy.all of our knowledge-based word similarity measures are based on wordnet.
</prevsent>
<prevsent>some measures use the concept of lowest common subsumer (lcs) of concepts c1 and c2, which represents the lowest node in the wordnet hierarchy that is hypernym of both c1 and c2.
</prevsent>
</prevsection>
<citsent citstr=" P06-4018 ">
we use the nltk library (bird, 2006) <papid> P06-4018 </papid>to compute the pathlen similarity (leacock and chodorow, 1998) and lin similarity (lin, 1998)measures.</citsent>
<aftsection>
<nextsent>a single word often denotes several concepts, depending on its context.
</nextsent>
<nextsent>in order to compute the similarity score for pair of words, we take the maximum similarity score over all possible pairs of concepts (i.e., wordnet synsets).
</nextsent>
<nextsent>2.2 corpus-based word similarity.
</nextsent>
<nextsent>distributional lexical semantics models determine the meaning of word through the set of all contexts in which the word appears.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1732">
<title id=" S12-1060.xml">takelab systems for measuring semantic text similarity </title>
<section> words are tokenized using the penn treebank.  </section>
<citcontext>
<prevsection>
<prevsent>(l1,l2)p sim(l1, l2) max(length(s1), length(s2)) where is the set of lemma pairs obtained by greedyalignment.
</prevsent>
<prevsent>we take advantage of greedy align overlap in two features: one computes glao(?, ?) by using the lin similarity for ssim(?, ?) in (2), while the other feature uses the distributional (lsa) similarity to calculate ssim(?, ?).
</prevsent>
</prevsection>
<citsent citstr=" P08-1028 ">
vector space sentence similarity this measure is motivated by the idea of compositionality of distributional vectors (mitchell and lapata, 2008).<papid> P08-1028 </papid></citsent>
<aftsection>
<nextsent>we represent each sentence as single distributional vector u(?)
</nextsent>
<nextsent>by summing the distributional (i.e., lsa) vector of each word in the sentence s: u(s) = ? ws xw, where xw is the vector representation of the word w. another similar representation uw (?)
</nextsent>
<nextsent>uses the information content ic(w) to weigh the lsa vector of each word before summation: uw (s) = ? ws ic(w)xw.
</nextsent>
<nextsent>the simple system uses |cos(u(s1), u(s2))| and|cos(uw (s1), uw (s2))| for the vector space sentence similarity features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1739">
<title id=" S10-1040.xml">sztergak  feature engineering for key phrase extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recent state-ofthe-art systems treat this kind of task as supervised learning task, in which phrases of document should be classified with respect to their key phrase characteristics based on manually labeled corpora and various feature values.this paper focuses on the task of key phrase extraction from scientific papers and we shall introduce new features that can significantly improve the overall performance.
</prevsent>
<prevsent>although the experimental results presented here are solely based on scientific articles, due to the robustness and universality of the features, our approach is expected to achieve good results when applied on other domains as well.
</prevsent>
</prevsection>
<citsent citstr=" D09-1027 ">
in key phrase extraction tasks, phrases are extracted from one document that are the most characteristic of its content (liu et al, 2009; <papid> D09-1027 </papid>wit tenet al, 1999).</citsent>
<aftsection>
<nextsent>in these approaches key phrase extraction is treated as classification task, in which certain n-grams of specific document act as key phrase candidates, and the task is to classify them as proper key phrases or not.while frank et al (1999) exploited domain specific knowledge to improve the quality of automatic tagging, others like liu et al (2009) <papid> D09-1027 </papid>analyze term co-occurence graphs.</nextsent>
<nextsent>it was nguyen and kan (2007) who dealt with the special characteristics of scientific papers and introduced the state-of-the art feature set to key phrase extraction tasks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1741">
<title id=" S10-1040.xml">sztergak  feature engineering for key phrase extraction </title>
<section> the sztergak system.  </section>
<citcontext>
<prevsection>
<prevsent>lines unlikely to contain valuable information were also excluded from the documents.
</prevsent>
<prevsent>these lines were identified according to statistical data of their surface forms (e.g. the average andthe deviation of line lengths) and regular expressions.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
lastly, section and sentence boundaries were found in rule-based way, and the pos and syntactic tagging (using the stanford parser (kleinand manning, 2003)) <papid> P03-1054 </papid>of each sentence were carried out.when syntactically parsed sentences were obtained, key phrase aspirants were extracted.</citsent>
<aftsection>
<nextsent>the 1 to 4-long token sequences that did not start or end with stop word and consisted only of pos-codesof an adjective, noun or verb were defined to be possible key phrases (resulting in classification instances).
</nextsent>
<nextsent>tokens of key phrase aspirants were stemmed to store them in uniform way, but they were also appended by the pos-code of the derived form, so that the same root forms were distinguished if they came from tokens having different pos-codes, like there shown in table 1.
</nextsent>
<nextsent>textual appearance canonical form regulations regul nns regulation regul nn regulates regul vbz regulated regul vbn table 1: standardization of document terms.
</nextsent>
<nextsent>3.2 the extended feature set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1742">
<title id=" S12-1014.xml">unsupervised disambiguation of image captions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>which sense of crane?
</prevsent>
<prevsent>with images the answer is clear.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
quantity of text (yarowsky, 1995).<papid> P95-1026 </papid></citsent>
<aftsection>
<nextsent>however, if the only available text is the crane was so massive it blocked the sun?
</nextsent>
<nextsent>(see fig.
</nextsent>
<nextsent>1), then text-only disambiguation becomes much more difficult; human could do little more than guess.
</nextsent>
<nextsent>but if an image is available, the intended sense is much clearer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1743">
<title id=" S12-1014.xml">unsupervised disambiguation of image captions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we develop an unsupervised wsd algorithm based on yarowskys that uses words in short caption along with visual words?
</prevsent>
<prevsent>from the captioned image to choose the best of two possible senses of an ambiguous keyword describing the content of the image.language-vision integration is quickly developing field, and number of researchers have explored the possibility of combining text and visual feature sin various multimodal tasks.
</prevsent>
</prevsection>
<citsent citstr=" W11-0120 ">
leong and mihalcea (2011) <papid> W11-0120 </papid>explored semantic relatedness between words and images to better exploit multimodal content.</citsent>
<aftsection>
<nextsent>jamieson et al  (2009) and feng and lapata (2010) <papid> N10-1125 </papid>combined text and vision to perform effective image annotation.</nextsent>
<nextsent>barnard and colleagues (2003), barnard and colleagues (2005) showed that supervised wsd by could be improved with visual features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1744">
<title id=" S12-1014.xml">unsupervised disambiguation of image captions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from the captioned image to choose the best of two possible senses of an ambiguous keyword describing the content of the image.language-vision integration is quickly developing field, and number of researchers have explored the possibility of combining text and visual feature sin various multimodal tasks.
</prevsent>
<prevsent>leong and mihalcea (2011) <papid> W11-0120 </papid>explored semantic relatedness between words and images to better exploit multimodal content.</prevsent>
</prevsection>
<citsent citstr=" N10-1125 ">
jamieson et al  (2009) and feng and lapata (2010) <papid> N10-1125 </papid>combined text and vision to perform effective image annotation.</citsent>
<aftsection>
<nextsent>barnard and colleagues (2003), barnard and colleagues (2005) showed that supervised wsd by could be improved with visual features.
</nextsent>
<nextsent>here we show that unsupervised wsd can similarly be improved.
</nextsent>
<nextsent>lo eff, alm and forsyth (2006) and saenko and darrell (2008) combined visual and textual information to solve related task, image sense disambiguation, in 85 an unsupervised fashion.
</nextsent>
<nextsent>in loeff et al work, little gain was realized when visual features were added to great deal of text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1745">
<title id=" S12-1014.xml">unsupervised disambiguation of image captions </title>
<section> creation of the dataset.  </section>
<citcontext>
<prevsection>
<prevsent>(top) and mouse?
</prevsent>
<prevsent>(bottom).
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
imcor dataset by associating images from the corel database with text from the semcor corpus (miller et al , 1993).<papid> H93-1061 </papid></citsent>
<aftsection>
<nextsent>loeff et al  (2006) <papid> P06-2071 </papid>and saenko and darrell (2008) used yahoo!s image search to gather images with their associated web pages.</nextsent>
<nextsent>while these datasets contain images paired with text, the textual contexts are much larger than typical captions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1746">
<title id=" S12-1014.xml">unsupervised disambiguation of image captions </title>
<section> creation of the dataset.  </section>
<citcontext>
<prevsection>
<prevsent>(bottom).
</prevsent>
<prevsent>imcor dataset by associating images from the corel database with text from the semcor corpus (miller et al , 1993).<papid> H93-1061 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-2071 ">
loeff et al  (2006) <papid> P06-2071 </papid>and saenko and darrell (2008) used yahoo!s image search to gather images with their associated web pages.</citsent>
<aftsection>
<nextsent>while these datasets contain images paired with text, the textual contexts are much larger than typical captions.
</nextsent>
<nextsent>3.1 captioning images.
</nextsent>
<nextsent>to develop large set of sense-annotated image?
</nextsent>
<nextsent>caption pairs with focus on caption-sized text, we turned to ima genet (deng et al , 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1747">
<title id=" S12-1014.xml">unsupervised disambiguation of image captions </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>a topic model is learned where the relatedness of topic to sense is based on the probabilities of that topic generating the seed words from its dictionary definitions.
</prevsent>
<prevsent>analogously to k-means, we learn model for text alone, and model for text augmented with visual information.
</prevsent>
</prevsection>
<citsent citstr=" W07-2086 ">
for unsupervised wsd (applied to text only),we use wordnet::senserelate::targetword, here after pbp (patwardhan et al , 2007), <papid> W07-2086 </papid>the highest scoring unsupervised lexical sample word sense disambiguation algorithm at semeval07 (pradhan et al ., 2007).<papid> W07-2016 </papid></citsent>
<aftsection>
<nextsent>pbp treats the nearby words around the target word as bag, and uses the wordnet hierarchy to assign similarity score between the possible senses of words in the context, and possible senses of the target word.
</nextsent>
<nextsent>as our captions are fairly short, we use the entire caption as context.
</nextsent>
<nextsent>the most important result is the gain inaccuracy after adding visual features.
</nextsent>
<nextsent>while the average gain across all words is slight, it is significant at   0.02 (using paired t-test).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1748">
<title id=" S12-1014.xml">unsupervised disambiguation of image captions </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>a topic model is learned where the relatedness of topic to sense is based on the probabilities of that topic generating the seed words from its dictionary definitions.
</prevsent>
<prevsent>analogously to k-means, we learn model for text alone, and model for text augmented with visual information.
</prevsent>
</prevsection>
<citsent citstr=" W07-2016 ">
for unsupervised wsd (applied to text only),we use wordnet::senserelate::targetword, here after pbp (patwardhan et al , 2007), <papid> W07-2086 </papid>the highest scoring unsupervised lexical sample word sense disambiguation algorithm at semeval07 (pradhan et al ., 2007).<papid> W07-2016 </papid></citsent>
<aftsection>
<nextsent>pbp treats the nearby words around the target word as bag, and uses the wordnet hierarchy to assign similarity score between the possible senses of words in the context, and possible senses of the target word.
</nextsent>
<nextsent>as our captions are fairly short, we use the entire caption as context.
</nextsent>
<nextsent>the most important result is the gain inaccuracy after adding visual features.
</nextsent>
<nextsent>while the average gain across all words is slight, it is significant at   0.02 (using paired t-test).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1749">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>human language users?
</prevsent>
<prevsent>knowledge about selectional preferences has been implicated in analyses of metaphor processing (wilks, 1978) and in psycho linguistic studies of comprehension (rayneret al , 2004).
</prevsent>
</prevsection>
<citsent citstr=" P09-2019 ">
in natural language processing, automatically acquired preference models have been shown to aid number of tasks, including semantic role labelling (zapirain et al , 2009), <papid> P09-2019 </papid>parsing (zhou et al , 2011) <papid> P11-1156 </papid>and lexical disambiguation (thater et al ., 2010; <papid> P10-1097 </papid>o?</citsent>
<aftsection>
<nextsent>seaghdha and korhonen, 2011).
</nextsent>
<nextsent>it is tempting to assume that with large enough corpus, preference learning reduces to simple language modelling task that can be solved by counting predicate-argument co-occurrences.
</nextsent>
<nextsent>indeed, keller and lapata (2003) <papid> J03-3005 </papid>show that relatively good performance at plausibility estimation can be attained by submitting queries to web search engine.</nextsent>
<nextsent>how ever, there are many scenarios where this approach is insufficient: for languages and language domains where web-scale data is unavailable, for predicate types (e.g., inference rules or semantic roles) that cannot be retrieved by keyword search and for applications where accurate models of rarer words arerequired.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1750">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>human language users?
</prevsent>
<prevsent>knowledge about selectional preferences has been implicated in analyses of metaphor processing (wilks, 1978) and in psycho linguistic studies of comprehension (rayneret al , 2004).
</prevsent>
</prevsection>
<citsent citstr=" P11-1156 ">
in natural language processing, automatically acquired preference models have been shown to aid number of tasks, including semantic role labelling (zapirain et al , 2009), <papid> P09-2019 </papid>parsing (zhou et al , 2011) <papid> P11-1156 </papid>and lexical disambiguation (thater et al ., 2010; <papid> P10-1097 </papid>o?</citsent>
<aftsection>
<nextsent>seaghdha and korhonen, 2011).
</nextsent>
<nextsent>it is tempting to assume that with large enough corpus, preference learning reduces to simple language modelling task that can be solved by counting predicate-argument co-occurrences.
</nextsent>
<nextsent>indeed, keller and lapata (2003) <papid> J03-3005 </papid>show that relatively good performance at plausibility estimation can be attained by submitting queries to web search engine.</nextsent>
<nextsent>how ever, there are many scenarios where this approach is insufficient: for languages and language domains where web-scale data is unavailable, for predicate types (e.g., inference rules or semantic roles) that cannot be retrieved by keyword search and for applications where accurate models of rarer words arerequired.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1751">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>human language users?
</prevsent>
<prevsent>knowledge about selectional preferences has been implicated in analyses of metaphor processing (wilks, 1978) and in psycho linguistic studies of comprehension (rayneret al , 2004).
</prevsent>
</prevsection>
<citsent citstr=" P10-1097 ">
in natural language processing, automatically acquired preference models have been shown to aid number of tasks, including semantic role labelling (zapirain et al , 2009), <papid> P09-2019 </papid>parsing (zhou et al , 2011) <papid> P11-1156 </papid>and lexical disambiguation (thater et al ., 2010; <papid> P10-1097 </papid>o?</citsent>
<aftsection>
<nextsent>seaghdha and korhonen, 2011).
</nextsent>
<nextsent>it is tempting to assume that with large enough corpus, preference learning reduces to simple language modelling task that can be solved by counting predicate-argument co-occurrences.
</nextsent>
<nextsent>indeed, keller and lapata (2003) <papid> J03-3005 </papid>show that relatively good performance at plausibility estimation can be attained by submitting queries to web search engine.</nextsent>
<nextsent>how ever, there are many scenarios where this approach is insufficient: for languages and language domains where web-scale data is unavailable, for predicate types (e.g., inference rules or semantic roles) that cannot be retrieved by keyword search and for applications where accurate models of rarer words arerequired.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1752">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>seaghdha and korhonen, 2011).
</prevsent>
<prevsent>it is tempting to assume that with large enough corpus, preference learning reduces to simple language modelling task that can be solved by counting predicate-argument co-occurrences.
</prevsent>
</prevsection>
<citsent citstr=" J03-3005 ">
indeed, keller and lapata (2003) <papid> J03-3005 </papid>show that relatively good performance at plausibility estimation can be attained by submitting queries to web search engine.</citsent>
<aftsection>
<nextsent>how ever, there are many scenarios where this approach is insufficient: for languages and language domains where web-scale data is unavailable, for predicate types (e.g., inference rules or semantic roles) that cannot be retrieved by keyword search and for applications where accurate models of rarer words arerequired.
</nextsent>
<nextsent>o? seaghdha (2010) shows that the web based approach is reliably outperformed by more complex models trained on smaller corpora for less frequent predicate-argument combinations.
</nextsent>
<nextsent>models that induce level of semantic representation, such as probabilistic latent variable models, have further advantage in that they can provide rich structured information for downstream tasks such as lexical disambiguation (o? seaghdha and korhonen, 2011) and semantic relation mining (yao et al , 2011).<papid> D11-1135 </papid></nextsent>
<nextsent>recent research has investigated the potential of bayesian probabilistic models such as latentdirichlet allocation (lda) for modelling selectional preferences (o? seaghdha, 2010; ritter et al ,2010; <papid> P10-1044 </papid>reisinger and mooney, 2011).<papid> D11-1130 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1753">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, there are many scenarios where this approach is insufficient: for languages and language domains where web-scale data is unavailable, for predicate types (e.g., inference rules or semantic roles) that cannot be retrieved by keyword search and for applications where accurate models of rarer words arerequired.
</prevsent>
<prevsent>o? seaghdha (2010) shows that the web based approach is reliably outperformed by more complex models trained on smaller corpora for less frequent predicate-argument combinations.
</prevsent>
</prevsection>
<citsent citstr=" D11-1135 ">
models that induce level of semantic representation, such as probabilistic latent variable models, have further advantage in that they can provide rich structured information for downstream tasks such as lexical disambiguation (o? seaghdha and korhonen, 2011) and semantic relation mining (yao et al , 2011).<papid> D11-1135 </papid></citsent>
<aftsection>
<nextsent>recent research has investigated the potential of bayesian probabilistic models such as latentdirichlet allocation (lda) for modelling selectional preferences (o? seaghdha, 2010; ritter et al ,2010; <papid> P10-1044 </papid>reisinger and mooney, 2011).<papid> D11-1130 </papid></nextsent>
<nextsent>these models are flexible and robust, yielding superior performance compared to previous approaches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1754">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>o? seaghdha (2010) shows that the web based approach is reliably outperformed by more complex models trained on smaller corpora for less frequent predicate-argument combinations.
</prevsent>
<prevsent>models that induce level of semantic representation, such as probabilistic latent variable models, have further advantage in that they can provide rich structured information for downstream tasks such as lexical disambiguation (o? seaghdha and korhonen, 2011) and semantic relation mining (yao et al , 2011).<papid> D11-1135 </papid></prevsent>
</prevsection>
<citsent citstr=" P10-1044 ">
recent research has investigated the potential of bayesian probabilistic models such as latentdirichlet allocation (lda) for modelling selectional preferences (o? seaghdha, 2010; ritter et al ,2010; <papid> P10-1044 </papid>reisinger and mooney, 2011).<papid> D11-1130 </papid></citsent>
<aftsection>
<nextsent>these models are flexible and robust, yielding superior performance compared to previous approaches.
</nextsent>
<nextsent>in this paper we present preliminary study of analogous 170 models that make use of lexical hierarchy (in our case the wordnet hierarchy).
</nextsent>
<nextsent>we describe two broad classes of probabilistic models over wordnet andhow they can be implemented in bayesian framework.
</nextsent>
<nextsent>the two main potential advantages of incorporating wordnet information are: (a) improved predictions about rare and out-of-vocabulary argu ments; (b) the ability to perform syntactic word sense disambiguation with principled probabilistic model and without the need for an additional step that heuristic ally maps latent variables onto wordnet senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1755">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>o? seaghdha (2010) shows that the web based approach is reliably outperformed by more complex models trained on smaller corpora for less frequent predicate-argument combinations.
</prevsent>
<prevsent>models that induce level of semantic representation, such as probabilistic latent variable models, have further advantage in that they can provide rich structured information for downstream tasks such as lexical disambiguation (o? seaghdha and korhonen, 2011) and semantic relation mining (yao et al , 2011).<papid> D11-1135 </papid></prevsent>
</prevsection>
<citsent citstr=" D11-1130 ">
recent research has investigated the potential of bayesian probabilistic models such as latentdirichlet allocation (lda) for modelling selectional preferences (o? seaghdha, 2010; ritter et al ,2010; <papid> P10-1044 </papid>reisinger and mooney, 2011).<papid> D11-1130 </papid></citsent>
<aftsection>
<nextsent>these models are flexible and robust, yielding superior performance compared to previous approaches.
</nextsent>
<nextsent>in this paper we present preliminary study of analogous 170 models that make use of lexical hierarchy (in our case the wordnet hierarchy).
</nextsent>
<nextsent>we describe two broad classes of probabilistic models over wordnet andhow they can be implemented in bayesian framework.
</nextsent>
<nextsent>the two main potential advantages of incorporating wordnet information are: (a) improved predictions about rare and out-of-vocabulary argu ments; (b) the ability to perform syntactic word sense disambiguation with principled probabilistic model and without the need for an additional step that heuristic ally maps latent variables onto wordnet senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1757">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the wordnet hierarchy.
</prevsent>
<prevsent>for example, the direct object slot of the verb eat can be associated with the sub hierarchy rooted at the synset food#n#1, as all hyponyms ofthat synset are assumed to be edible and the immediate hypernym of the synset, substance#n#1, is too general given that many substances are rarely eaten.1 this leads to the notion of cutting?
</prevsent>
</prevsection>
<citsent citstr=" J98-2002 ">
the hierarchy atone or more positions (li and abe, 1998).<papid> J98-2002 </papid></citsent>
<aftsection>
<nextsent>the modelling task then becomes that of finding the cuts that are maximally general without overgeneralising.
</nextsent>
<nextsent>liand abe (1998) <papid> J98-2002 </papid>propose model in which the appropriate cut is selected according to the minimum description length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising sum of model description length and data description length.</nextsent>
<nextsent>the probability of predicate taking as its argument an synset is modelled as: pla(s|p, r) = (s|cs,p,r)p (c|p) (2) where cs,p,r is the portion of the cut learned for that dominates s. the distribution (s|cs,p,r) is held to be uniform over all synsets dominated by cs,p,r,while (c|p) is given by maximum likelihood es timate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1759">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>liand abe (1998) <papid> J98-2002 </papid>propose model in which the appropriate cut is selected according to the minimum description length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising sum of model description length and data description length.</prevsent>
<prevsent>the probability of predicate taking as its argument an synset is modelled as: pla(s|p, r) = (s|cs,p,r)p (c|p) (2) where cs,p,r is the portion of the cut learned for that dominates s. the distribution (s|cs,p,r) is held to be uniform over all synsets dominated by cs,p,r,while (c|p) is given by maximum likelihood es timate.</prevsent>
</prevsection>
<citsent citstr=" J02-2003 ">
clark and weir (2002) <papid> J02-2003 </papid>present model that, while not explicitly described as cut-based, likewise seeksto find the right level of generalisation for an obser vation.</citsent>
<aftsection>
<nextsent>in this case, the hypernym at which to cutis chosen by chi-squared test: if the aggregate preference of for classes in the sub hierarchy rooted at differs significantly from the individual preferences of for the immediate children of c, the hierarchy is cut below c. the probability of taking synset as its argument is given by: pcw(s|p, r) = (p|cs,p,r, r) (s|r) (p|r) ? ss (p|cs?,p,r, r) (s?|r) (p|r) (3)where cs,p,r is the root node of the sub hierarchy containing that was selected for p.an alternative approach to modelling with wordnet uses its hierarchical structure to define markov model with transitions from senses to senses and from senses to words.
</nextsent>
<nextsent>the intuition here is that each observation is generated by walk?
</nextsent>
<nextsent>from the root of the hierarchy to leaf node and emitting the word 1in this paper we use wordnet version 3.0, except where stated otherwise.
</nextsent>
<nextsent>171 corresponding to the leaf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1760">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>from the root of the hierarchy to leaf node and emitting the word 1in this paper we use wordnet version 3.0, except where stated otherwise.
</prevsent>
<prevsent>171 corresponding to the leaf.
</prevsent>
</prevsection>
<citsent citstr=" W99-0901 ">
abney and light (1999) <papid> W99-0901 </papid>proposed such model for selectional preferences, trained via em, but failed to achieve competitive performance on pseudo disambiguation task.</citsent>
<aftsection>
<nextsent>the models described above have subsequently been used in many different studies.
</nextsent>
<nextsent>for exam ple: mccarthy and carroll (2003) <papid> J03-4004 </papid>use li and abes method in word sense disambiguation setting;schulte im walde et al  (2008) use their mdl approach as part of system for syntactic and semantic subcategorisation frame learning; shutova (2010)<papid> N10-1147 </papid>deploys resniks method for metaphor interpretation.</nextsent>
<nextsent>brockmann and lapata (2003) <papid> E03-1034 </papid>report comparative evaluation in which the methods of resnik and clark and weir outpeform li and abes method on plausibility estimation task.much recent work on preference learning has focused on purely distributional methods that do notuse predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1762">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>abney and light (1999) <papid> W99-0901 </papid>proposed such model for selectional preferences, trained via em, but failed to achieve competitive performance on pseudo disambiguation task.</prevsent>
<prevsent>the models described above have subsequently been used in many different studies.</prevsent>
</prevsection>
<citsent citstr=" J03-4004 ">
for exam ple: mccarthy and carroll (2003) <papid> J03-4004 </papid>use li and abes method in word sense disambiguation setting;schulte im walde et al  (2008) use their mdl approach as part of system for syntactic and semantic subcategorisation frame learning; shutova (2010)<papid> N10-1147 </papid>deploys resniks method for metaphor interpretation.</citsent>
<aftsection>
<nextsent>brockmann and lapata (2003) <papid> E03-1034 </papid>report comparative evaluation in which the methods of resnik and clark and weir outpeform li and abes method on plausibility estimation task.much recent work on preference learning has focused on purely distributional methods that do notuse predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone.</nextsent>
<nextsent>these methods can be vector based (erk et al , 2010; <papid> J10-4007 </papid>thater et al , 2010), <papid> P10-1097 </papid>discriminative (bergsma et al , 2008) <papid> D08-1007 </papid>or probabilistic (o? seaghdha, 2010; ritter et al , 2010; <papid> P10-1044 </papid>reisinger and mooney, 2011).<papid> D11-1130 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1763">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>abney and light (1999) <papid> W99-0901 </papid>proposed such model for selectional preferences, trained via em, but failed to achieve competitive performance on pseudo disambiguation task.</prevsent>
<prevsent>the models described above have subsequently been used in many different studies.</prevsent>
</prevsection>
<citsent citstr=" N10-1147 ">
for exam ple: mccarthy and carroll (2003) <papid> J03-4004 </papid>use li and abes method in word sense disambiguation setting;schulte im walde et al  (2008) use their mdl approach as part of system for syntactic and semantic subcategorisation frame learning; shutova (2010)<papid> N10-1147 </papid>deploys resniks method for metaphor interpretation.</citsent>
<aftsection>
<nextsent>brockmann and lapata (2003) <papid> E03-1034 </papid>report comparative evaluation in which the methods of resnik and clark and weir outpeform li and abes method on plausibility estimation task.much recent work on preference learning has focused on purely distributional methods that do notuse predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone.</nextsent>
<nextsent>these methods can be vector based (erk et al , 2010; <papid> J10-4007 </papid>thater et al , 2010), <papid> P10-1097 </papid>discriminative (bergsma et al , 2008) <papid> D08-1007 </papid>or probabilistic (o? seaghdha, 2010; ritter et al , 2010; <papid> P10-1044 </papid>reisinger and mooney, 2011).<papid> D11-1130 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1764">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>the models described above have subsequently been used in many different studies.
</prevsent>
<prevsent>for exam ple: mccarthy and carroll (2003) <papid> J03-4004 </papid>use li and abes method in word sense disambiguation setting;schulte im walde et al  (2008) use their mdl approach as part of system for syntactic and semantic subcategorisation frame learning; shutova (2010)<papid> N10-1147 </papid>deploys resniks method for metaphor interpretation.</prevsent>
</prevsection>
<citsent citstr=" E03-1034 ">
brockmann and lapata (2003) <papid> E03-1034 </papid>report comparative evaluation in which the methods of resnik and clark and weir outpeform li and abes method on plausibility estimation task.much recent work on preference learning has focused on purely distributional methods that do notuse predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone.</citsent>
<aftsection>
<nextsent>these methods can be vector based (erk et al , 2010; <papid> J10-4007 </papid>thater et al , 2010), <papid> P10-1097 </papid>discriminative (bergsma et al , 2008) <papid> D08-1007 </papid>or probabilistic (o? seaghdha, 2010; ritter et al , 2010; <papid> P10-1044 </papid>reisinger and mooney, 2011).<papid> D11-1130 </papid></nextsent>
<nextsent>in the probabilistic category, bayesian models based on the topic modelling?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1765">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>for exam ple: mccarthy and carroll (2003) <papid> J03-4004 </papid>use li and abes method in word sense disambiguation setting;schulte im walde et al  (2008) use their mdl approach as part of system for syntactic and semantic subcategorisation frame learning; shutova (2010)<papid> N10-1147 </papid>deploys resniks method for metaphor interpretation.</prevsent>
<prevsent>brockmann and lapata (2003) <papid> E03-1034 </papid>report comparative evaluation in which the methods of resnik and clark and weir outpeform li and abes method on plausibility estimation task.much recent work on preference learning has focused on purely distributional methods that do notuse predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone.</prevsent>
</prevsection>
<citsent citstr=" J10-4007 ">
these methods can be vector based (erk et al , 2010; <papid> J10-4007 </papid>thater et al , 2010), <papid> P10-1097 </papid>discriminative (bergsma et al , 2008) <papid> D08-1007 </papid>or probabilistic (o? seaghdha, 2010; ritter et al , 2010; <papid> P10-1044 </papid>reisinger and mooney, 2011).<papid> D11-1130 </papid></citsent>
<aftsection>
<nextsent>in the probabilistic category, bayesian models based on the topic modelling?
</nextsent>
<nextsent>framework (blei et al , 2003b) have been shown to achieve state-of-the-art performance in number of evaluation settings; the models considered in this paper are also related to this framework.
</nextsent>
<nextsent>in machine learning, researchers have proposeda variety of topic modelling methods where the latent variables are arranged in hierarchical structure(blei et al , 2003a; mimno et al , 2007).
</nextsent>
<nextsent>in contrast to the present work, these models use relatively shallow hierarchy (e.g., 3 levels) and any hierarchy node can in principle emit any vocabulary item; they thus provide poor match for our goal of modelling over wordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1767">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>for exam ple: mccarthy and carroll (2003) <papid> J03-4004 </papid>use li and abes method in word sense disambiguation setting;schulte im walde et al  (2008) use their mdl approach as part of system for syntactic and semantic subcategorisation frame learning; shutova (2010)<papid> N10-1147 </papid>deploys resniks method for metaphor interpretation.</prevsent>
<prevsent>brockmann and lapata (2003) <papid> E03-1034 </papid>report comparative evaluation in which the methods of resnik and clark and weir outpeform li and abes method on plausibility estimation task.much recent work on preference learning has focused on purely distributional methods that do notuse predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone.</prevsent>
</prevsection>
<citsent citstr=" D08-1007 ">
these methods can be vector based (erk et al , 2010; <papid> J10-4007 </papid>thater et al , 2010), <papid> P10-1097 </papid>discriminative (bergsma et al , 2008) <papid> D08-1007 </papid>or probabilistic (o? seaghdha, 2010; ritter et al , 2010; <papid> P10-1044 </papid>reisinger and mooney, 2011).<papid> D11-1130 </papid></citsent>
<aftsection>
<nextsent>in the probabilistic category, bayesian models based on the topic modelling?
</nextsent>
<nextsent>framework (blei et al , 2003b) have been shown to achieve state-of-the-art performance in number of evaluation settings; the models considered in this paper are also related to this framework.
</nextsent>
<nextsent>in machine learning, researchers have proposeda variety of topic modelling methods where the latent variables are arranged in hierarchical structure(blei et al , 2003a; mimno et al , 2007).
</nextsent>
<nextsent>in contrast to the present work, these models use relatively shallow hierarchy (e.g., 3 levels) and any hierarchy node can in principle emit any vocabulary item; they thus provide poor match for our goal of modelling over wordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1779">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>following the evaluation in o?
</prevsent>
<prevsent>seaghdha (2010), with which we wish to compare,we use pearson and spearman ? correlation coefficients as performance measures.
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
all models were trained on the 90-million word2for related argument in the context of topic model evaluation, see chang et al  (2009).written component of the british national cor pus,3 lemmatised, pos-tagged and parsed with the rasp toolkit (briscoe et al , 2006).<papid> P06-4020 </papid></citsent>
<aftsection>
<nextsent>we removed predicates occurring with just one argument type and all tokens containing non-alphabetic characters.the resulting datasets consist of 3,587,172 verb object observations (7,954 predicate types, 80,107 argument types), 3,732,470 noun-noun observations (68,303 predicate types, 105,425 argument types) and 3,843,346 adjective-noun observations (29,975 predicate types, 62,595 argument types).
</nextsent>
<nextsent>all the bayesian models were trained by gibbs sampling, as outlined above.
</nextsent>
<nextsent>for each model we run three sampling chains for 1,000 iterations and average the plausibility predictions for each to produce final prediction (w|p) for each predicate-argument item.
</nextsent>
<nextsent>as the evaluation demands an estimate of the joint probability (w, p) we multiply the predicted (w|p) by predicate probability (p|r) estimated from relative corpus frequencies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1788">
<title id=" S12-1025.xml">modelling selectional preferences in a lexical hierarchy </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>previous studies have demonstrated the effectiveness of distributional bayesian selectional preference models for predicting lexical substitutes (o? seaghdha and korhonen, 2011) but these models lack principled way to map word onto its most likely wordnet sense.
</prevsent>
<prevsent>the methods presented in this paper offer promising solution tothis issue.
</prevsent>
</prevsection>
<citsent citstr=" P06-1100 ">
another potential research direction is integration of semantic relation extraction algorithms with wordnet or other lexical resources, along the lines of pennacchiotti and pantel (2006) <papid> P06-1100 </papid>and van durme et al  (2009).</citsent>
<aftsection>
<nextsent>acknowledgements the work in this paper was funded by the epsrc (uk) grant ep/g051070/1, eu grant 7fp-itc 248064 and the royal society, (uk).
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1789">
<title id=" S12-1104.xml">celi an experiment with cross language textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>this paper presents celis participation in the semeval cross-lingual textual entailment for content synchronization task.
</prevsent>
</prevsection>
<citsent citstr=" P11-1134 ">
the cross-lingual textual entailment task (clte) is new task that addresses textual entailment (te)(bentivogli et. al., 2011), targeting the cross lingual content synchronization scenario proposed in (mehdad et. al., 2011) <papid> P11-1134 </papid>and (negri et. al., 2011).<papid> D11-1062 </papid></citsent>
<aftsection>
<nextsent>the task has interesting application scenarios thatcan be investigated.
</nextsent>
<nextsent>some of them are content synchronization and cross language query alignment.
</nextsent>
<nextsent>the task is defined by the organizers as follows: given pair of topically related text fragments (t1and t2) in different languages, the clte task consists of automatically annotating it with one of the following entailment judgments: ? bidirectional: the two fragments entail each other (semantic equivalence) ? forward: unidirectional entailment from t1 to t2 ? backward: unidirectional entailment from t2 to t1 ? no entailment: there is no entailment between t1 and t2 in this task, both t1 and t2 are assumed to be true statements; hence in the dataset there are no contradictory pairs.
</nextsent>
<nextsent>example for spanish english pairs: ? bidirectional mozart naci en la ciudad de salz burgo mozart was born in salzburg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1790">
<title id=" S12-1104.xml">celi an experiment with cross language textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>this paper presents celis participation in the semeval cross-lingual textual entailment for content synchronization task.
</prevsent>
</prevsection>
<citsent citstr=" D11-1062 ">
the cross-lingual textual entailment task (clte) is new task that addresses textual entailment (te)(bentivogli et. al., 2011), targeting the cross lingual content synchronization scenario proposed in (mehdad et. al., 2011) <papid> P11-1134 </papid>and (negri et. al., 2011).<papid> D11-1062 </papid></citsent>
<aftsection>
<nextsent>the task has interesting application scenarios thatcan be investigated.
</nextsent>
<nextsent>some of them are content synchronization and cross language query alignment.
</nextsent>
<nextsent>the task is defined by the organizers as follows: given pair of topically related text fragments (t1and t2) in different languages, the clte task consists of automatically annotating it with one of the following entailment judgments: ? bidirectional: the two fragments entail each other (semantic equivalence) ? forward: unidirectional entailment from t1 to t2 ? backward: unidirectional entailment from t2 to t1 ? no entailment: there is no entailment between t1 and t2 in this task, both t1 and t2 are assumed to be true statements; hence in the dataset there are no contradictory pairs.
</nextsent>
<nextsent>example for spanish english pairs: ? bidirectional mozart naci en la ciudad de salz burgo mozart was born in salzburg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1791">
<title id=" S12-1104.xml">celi an experiment with cross language textual entailment </title>
<section> our approach to clte.  </section>
<citcontext>
<prevsection>
<prevsent>forward mozart naci en la ciudad de salz burgo mozart was born on the 27th january 1756 in salzburg.
</prevsent>
<prevsent>backward mozart naci el 27 de enero de 1756 en salz burgo mozart was born in 1756 in the city of salzburg ? no entailment mozart naci el 27 de enero de 1756 en salz burgo mozart was born to leopold and anna maria pertl mozart.
</prevsent>
</prevsection>
<citsent citstr=" S12-1053 ">
in our participation in the 2012 semeval cross lingual textual entailment for content synchronization task (negri et. al., 2012) <papid> S12-1053 </papid>we have developed an approach based on cross-language text similarity.we have modified our cross-language query similarity system tlike to handle longer texts.</citsent>
<aftsection>
<nextsent>our approach is based on four main resources: ? system for natural language processing able to perform for each relevant language basic tasks such as part of speech disambiguation, lemmatization and named entity recognition.?
</nextsent>
<nextsent>a set of word based bilingual translation modules.
</nextsent>
<nextsent>696?
</nextsent>
<nextsent>a semantic component able to associate semantic vector ial representation to words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1792">
<title id=" P99-1033.xml">dependency parsing with an extended finite state approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have applied the parser to de-pendency parsing of turkish.
</prevsent>
<prevsent>recent advances in the development of sophisticated tools for building finite state systems (e.g., xrce finite state tools (karttunen et al, 1996), atgzt tools (mohri et al, 1998)) have fostered the develop-ment of quite complex finite state systems for natu-ral language processing.
</prevsent>
</prevsection>
<citsent citstr=" C90-2040 ">
in the last several years, there have been number of studies on develop-ing finite state parsing systems, (koskenniemi, 1990; <papid> C90-2040 </papid>koskenniemi et al, 1992; <papid> C92-1027 </papid>grefenstette, 1996; ait- mokhtar and chanod, 1997).</citsent>
<aftsection>
<nextsent>there have also been number of approaches to natural anguage pars-ing using extended finite state approaches in which finite state engine is applied multiple times to the input, or various derivatives thereof, until some stop-ping condition is reached.
</nextsent>
<nextsent>roche (1997) presents an approach for parsing in which the input is itera- tively bracketed using finite state transducer.
</nextsent>
<nextsent>ab- ney(1996) presents finite state parsing approach in which tagged sentence is parsed by transducers which progressively transform the input to sequences of symbols representing phrasal constituents.
</nextsent>
<nextsent>this paper presents an approach to dependency parsing using an extended finite state model resembling the approaches of roche and abney.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1793">
<title id=" P99-1033.xml">dependency parsing with an extended finite state approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have applied the parser to de-pendency parsing of turkish.
</prevsent>
<prevsent>recent advances in the development of sophisticated tools for building finite state systems (e.g., xrce finite state tools (karttunen et al, 1996), atgzt tools (mohri et al, 1998)) have fostered the develop-ment of quite complex finite state systems for natu-ral language processing.
</prevsent>
</prevsection>
<citsent citstr=" C92-1027 ">
in the last several years, there have been number of studies on develop-ing finite state parsing systems, (koskenniemi, 1990; <papid> C90-2040 </papid>koskenniemi et al, 1992; <papid> C92-1027 </papid>grefenstette, 1996; ait- mokhtar and chanod, 1997).</citsent>
<aftsection>
<nextsent>there have also been number of approaches to natural anguage pars-ing using extended finite state approaches in which finite state engine is applied multiple times to the input, or various derivatives thereof, until some stop-ping condition is reached.
</nextsent>
<nextsent>roche (1997) presents an approach for parsing in which the input is itera- tively bracketed using finite state transducer.
</nextsent>
<nextsent>ab- ney(1996) presents finite state parsing approach in which tagged sentence is parsed by transducers which progressively transform the input to sequences of symbols representing phrasal constituents.
</nextsent>
<nextsent>this paper presents an approach to dependency parsing using an extended finite state model resembling the approaches of roche and abney.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1794">
<title id=" P99-1033.xml">dependency parsing with an extended finite state approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper presents an approach to dependency parsing using an extended finite state model resembling the approaches of roche and abney.
</prevsent>
<prevsent>the parser pro-duces outputs that encode alabeled ependency tree representation the syntactic relations between the words in the sentence.
</prevsent>
</prevsection>
<citsent citstr=" J94-3001 ">
we assume that the reader is familiar with the basic concepts of finite state transducers (fst here- after), finite state devices that map between two reg-ular languages and (kaplan and kay, 1994).<papid> J94-3001 </papid></citsent>
<aftsection>
<nextsent>dependency approaches to syntactic representation use the notion of syntactic relation to associate sur-face lexical items.
</nextsent>
<nextsent>the book by mel~uk (1988) presents comprehensive exposition of dependency syntax.
</nextsent>
<nextsent>computational pproaches to dependency syntax have recently become quite popular (e.g., workshop dedicated to computational pproaches to dependency grammars has been held at col- ing/acl 98 conference).
</nextsent>
<nextsent>j~irvinen and tapana- ninen have demonstrated an efficient wide-coverage dependency parser for english (tapanainen and j~irvinen, 1997; jrvinen and tapanainen, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1795">
<title id=" P99-1033.xml">dependency parsing with an extended finite state approach </title>
<section> dependency  syntax.  </section>
<citcontext>
<prevsection>
<prevsent>j~irvinen and tapana- ninen have demonstrated an efficient wide-coverage dependency parser for english (tapanainen and j~irvinen, 1997; jrvinen and tapanainen, 1998).
</prevsent>
<prevsent>the work of sleator and temperley(1991) on link grammar, an essentially exicalized variant of depen-dency grammar, has also proved to be interesting in number of aspects.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
dependency-based statistical language modeling and analysis have also become quite popular in statistical natural language process-ing (lafferty et al, 1992; eisner, 1996; <papid> C96-1058 </papid>chelba and et al, 1997).</citsent>
<aftsection>
<nextsent>robinson(1970) gives four axioms for well-formed dependency structures, which have been assumed in almost all computational pproaches.
</nextsent>
<nextsent>in depen-dency structure of sentence (i) one and only one word is independent, i.e., not linked to some other word, (ii) all others depend directly on some word, (iii) no word depends on more than one other, and, (iv) if word depends directly on b, and some word intervenes between them (in linear order), then depends directly on or on b, or on some other intervening word.
</nextsent>
<nextsent>this last condition of pro-jectivity (or various extensions of it; see e.g., lau and huang (1994)) is usually assumed by most com-putational approaches to dependency grammars as constraint for filtering configurations, and has also been used as simplifying condition in statistical approaches for inducing dependencies from corpora (e.g., yiiret(1998).)
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1796">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>coreference resolution is an important subtask ina wide array of natural language processing problems, among them information extraction, question answering, and machine translation.
</prevsent>
<prevsent>the availability of corpora annotated with coreference relation shas led to the development of diverse set of supervised learning approaches for coreference.
</prevsent>
</prevsection>
<citsent citstr=" D09-1120 ">
while learning models enjoy largely undisputed role in many nlp applications, deterministic models based on rich sets of expert rules for coreference have been shown recently to achieve performance rivaling, if not exceeding, the performance of state of the art machine learning approaches (haghighi and klein, 2009; <papid> D09-1120 </papid>raghunathan et al, 2010).<papid> D10-1048 </papid></citsent>
<aftsection>
<nextsent>in particular, the top performing system in the conll 2011 shared task (pradhan et al, 2011) <papid> W11-1901 </papid>is multi-pass system that applies tiers of deterministic coreference sieves from highest to lowest precision (lee et al, 2011).<papid> W11-1902 </papid></nextsent>
<nextsent>the precise constructs sieve, for example, creates coreference links between mentions that are foundto match patterns of apposition, predicate nomina tives, acronyms, demonyms, or relative pronouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1798">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>coreference resolution is an important subtask ina wide array of natural language processing problems, among them information extraction, question answering, and machine translation.
</prevsent>
<prevsent>the availability of corpora annotated with coreference relation shas led to the development of diverse set of supervised learning approaches for coreference.
</prevsent>
</prevsection>
<citsent citstr=" D10-1048 ">
while learning models enjoy largely undisputed role in many nlp applications, deterministic models based on rich sets of expert rules for coreference have been shown recently to achieve performance rivaling, if not exceeding, the performance of state of the art machine learning approaches (haghighi and klein, 2009; <papid> D09-1120 </papid>raghunathan et al, 2010).<papid> D10-1048 </papid></citsent>
<aftsection>
<nextsent>in particular, the top performing system in the conll 2011 shared task (pradhan et al, 2011) <papid> W11-1901 </papid>is multi-pass system that applies tiers of deterministic coreference sieves from highest to lowest precision (lee et al, 2011).<papid> W11-1902 </papid></nextsent>
<nextsent>the precise constructs sieve, for example, creates coreference links between mentions that are foundto match patterns of apposition, predicate nomina tives, acronyms, demonyms, or relative pronouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1800">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the availability of corpora annotated with coreference relation shas led to the development of diverse set of supervised learning approaches for coreference.
</prevsent>
<prevsent>while learning models enjoy largely undisputed role in many nlp applications, deterministic models based on rich sets of expert rules for coreference have been shown recently to achieve performance rivaling, if not exceeding, the performance of state of the art machine learning approaches (haghighi and klein, 2009; <papid> D09-1120 </papid>raghunathan et al, 2010).<papid> D10-1048 </papid></prevsent>
</prevsection>
<citsent citstr=" W11-1901 ">
in particular, the top performing system in the conll 2011 shared task (pradhan et al, 2011) <papid> W11-1901 </papid>is multi-pass system that applies tiers of deterministic coreference sieves from highest to lowest precision (lee et al, 2011).<papid> W11-1902 </papid></citsent>
<aftsection>
<nextsent>the precise constructs sieve, for example, creates coreference links between mentions that are foundto match patterns of apposition, predicate nomina tives, acronyms, demonyms, or relative pronouns.
</nextsent>
<nextsent>this is high precision sieve, correspondingly it isamong the first sieves to be applied.
</nextsent>
<nextsent>the pronoun match sieve links an anaphoric pronoun with the first antecedent mention that agrees in number and gender with the pronoun, based on an ordering of the antecedents that uses syntactic rules to model discourse salience.
</nextsent>
<nextsent>this is the last sieve to be applied, due to its lower overall precision, as estimated on development data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1801">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the availability of corpora annotated with coreference relation shas led to the development of diverse set of supervised learning approaches for coreference.
</prevsent>
<prevsent>while learning models enjoy largely undisputed role in many nlp applications, deterministic models based on rich sets of expert rules for coreference have been shown recently to achieve performance rivaling, if not exceeding, the performance of state of the art machine learning approaches (haghighi and klein, 2009; <papid> D09-1120 </papid>raghunathan et al, 2010).<papid> D10-1048 </papid></prevsent>
</prevsection>
<citsent citstr=" W11-1902 ">
in particular, the top performing system in the conll 2011 shared task (pradhan et al, 2011) <papid> W11-1901 </papid>is multi-pass system that applies tiers of deterministic coreference sieves from highest to lowest precision (lee et al, 2011).<papid> W11-1902 </papid></citsent>
<aftsection>
<nextsent>the precise constructs sieve, for example, creates coreference links between mentions that are foundto match patterns of apposition, predicate nomina tives, acronyms, demonyms, or relative pronouns.
</nextsent>
<nextsent>this is high precision sieve, correspondingly it isamong the first sieves to be applied.
</nextsent>
<nextsent>the pronoun match sieve links an anaphoric pronoun with the first antecedent mention that agrees in number and gender with the pronoun, based on an ordering of the antecedents that uses syntactic rules to model discourse salience.
</nextsent>
<nextsent>this is the last sieve to be applied, due to its lower overall precision, as estimated on development data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1802">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is the last sieve to be applied, due to its lower overall precision, as estimated on development data.
</prevsent>
<prevsent>while very successful, this deterministic multi-pass sieve approach to coreference can nevertheless be quite unwieldy when one seeks to integrate new sources of knowledge in order to improve the resolution performance.
</prevsent>
</prevsection>
<citsent citstr=" P05-1021 ">
pronoun resolution, for example, was shown by yang et al (2005) <papid> P05-1021 </papid>to benefit from semantic compatibility information extracted from search engine statistics.</citsent>
<aftsection>
<nextsent>the semantic compatibility between candidate antecedents and the pronoun context induces new ordering between the antecedents.
</nextsent>
<nextsent>one possibility for using compatibility scores in the deterministic system is to ignore the salience-based ordering and replace it withthe new compatibility-based ordering.
</nextsent>
<nextsent>the draw 11 back of this simple approach is that now discourse salience, an important signal in pronoun resolution, is completely ignored.
</nextsent>
<nextsent>ideally, we would want touse both discourse salience and semantic compatibility when ranking the candidate antecedents ofthe pronoun, something that can be achieved naturally in discriminative learning approach that usesthe two rankings as different, but overlapping, features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1809">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> semantic compatibility features.  </section>
<citcontext>
<prevsection>
<prevsent>its definition assumes that we can compute comp(m, cj), the semantic compatibility between candidate antecedent mention and the pronoun context cj . for the possessive pronoun its, we extract the syntactic head of the mention andre place the pronoun with the mention head in the possessive context.
</prevsent>
<prevsent>we use the resulting possessive pronoun context pcj(h) to define the semantic compatibility as the following conditional probability: comp(m, cj) = logp (pcj(h)|h) (5) = logp (pcj(h))?
</prevsent>
</prevsection>
<citsent citstr=" N10-2012 ">
logp (h) to compute the n-gram probabilities (pcj(h)) andp (h) in equation 6, we use the language models provided by the microsoft web n-gram corpus (wang et al, 2010), <papid> N10-2012 </papid>as described in the next sec tion.figure 1 shows an example of possessive neutral pronoun context, together with the set of candidate antecedents that agree in number and gender with the pronoun, from the current and previous 3sentences.</citsent>
<aftsection>
<nextsent>each candidate antecedent is given an index that reflects its ranking in the discourse salience based ordering.
</nextsent>
<nextsent>we see that discourse salience does not help here, as the most salient mention is not the correct antecedent.
</nextsent>
<nextsent>the figure also shows the 14 in 1946, the nine justices dismissed case[7] involving the apportionment[8] of congressional districts.
</nextsent>
<nextsent>that view[6] would slowly change.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1812">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>the newswire section of the corpus contains 128 documents annotated with gold mentions and coreference information, where coreference is marked only between mentions that belong to one of seven semantic classes: person, organization, location, geo-political entity, facility, vehicle, and weapon.
</prevsent>
<prevsent>this set of documents has been used before to evaluate coreference resolution sys 16 system mentions r f1 dt gold, all 88.1 73.3 80.0 ac gold, all 88.7 73.5 80.4 dt gold, neutral 82.5 51.5 63.4 ac gold, neutral 83.0 52.1 64.0 dt auto, neutral 84.4 34.9 49.3 ac auto, neutral 86.1 40.0 54.6 table 2: b3 comparative results on ace 2004.
</prevsent>
</prevsection>
<citsent citstr=" D08-1068 ">
tems in (poon and domingos, 2008; <papid> D08-1068 </papid>haghighi and klein, 2009; <papid> D09-1120 </papid>raghunathan et al, 2010), <papid> D10-1048 </papid>with the best results so far obtained by the deterministic sieve system of lee at al.</citsent>
<aftsection>
<nextsent>(2011).
</nextsent>
<nextsent>there are 11,398 annotated gold mentions, out of which 135 are possessive neutral pronouns its and 88 are neutral pronouns it in subject-verb-object triple.
</nextsent>
<nextsent>given the very small number of neutral pronouns, in order to obtain reliable estimates for the model parameters we tested the adaptive clustering algorithm in 16 fold cross validation scenario.
</nextsent>
<nextsent>thus, the set of 128 documents was split into 16 folds, where each fold contains 120 documents for training and 8 documents for testing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1820">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another cause for the smaller increase in performance was that the pronominal contexts were less discriminative in the conll data, especially for the neutral pronoun it.
</prevsent>
<prevsent>when evaluated only on links that contained at least one possessive neutral pronoun its, the improvement in f1 increased at 1.9%, as shown in table 3.
</prevsent>
</prevsection>
<citsent citstr=" N07-1011 ">
closest to our clustering approach from section 2 is the error-driven first-order probabilistic model of culotta et al (2007).<papid> N07-1011 </papid></citsent>
<aftsection>
<nextsent>among significant difference swe mention that our model is non-probabilistic, simpler and easier to understand and implement.
</nextsent>
<nextsent>furthermore, the update step does not stop after the first clustering error, instead the algorithm learns and uses clustering threshold ? to determine when to stop during training and testing.
</nextsent>
<nextsent>this required the design of method to order cluster pairs in which the clusters may not be consistent with the true coreference chains, which led to the introduction of the goodness function in equation 1 as new scoring measure for cluster pairs.
</nextsent>
<nextsent>the strategy of continuing the clustering during training as long as an adaptive threshold is met better matches the training with the testing, and was observed to lead to better performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1821">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this required the design of method to order cluster pairs in which the clusters may not be consistent with the true coreference chains, which led to the introduction of the goodness function in equation 1 as new scoring measure for cluster pairs.
</prevsent>
<prevsent>the strategy of continuing the clustering during training as long as an adaptive threshold is met better matches the training with the testing, and was observed to lead to better performance.
</prevsent>
</prevsection>
<citsent citstr=" D09-1101 ">
the cluster ranking model of rahman and ng (2009) <papid> D09-1101 </papid>proceeds in left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster.</citsent>
<aftsection>
<nextsent>compared to it, our adaptive clustering approach is less constrained: it uses only weak, partial ordering between coreference decisions, and does not require singleton cluster at every clustering step.
</nextsent>
<nextsent>this allows clustering to start in any section of the document where coreference decisions are easier to make, and thus create accurate clusters earlier in the process.
</nextsent>
<nextsent>the use of semantic knowledge for coreference resolution has been studied before in number of works, among them (ponzetto and strube, 2006), (<papid> N06-1025 </papid>bengtson and roth, 2008), (<papid> D08-1031 </papid>lee et al, 2011), <papid> W11-1902 </papid>and (rahman and ng, 2011).</nextsent>
<nextsent>the focus in these studies has been on the semantic similarity between mention and candidate antecedent, or the parallelism between the semantic role structures in which the two appear.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1822">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>compared to it, our adaptive clustering approach is less constrained: it uses only weak, partial ordering between coreference decisions, and does not require singleton cluster at every clustering step.
</prevsent>
<prevsent>this allows clustering to start in any section of the document where coreference decisions are easier to make, and thus create accurate clusters earlier in the process.
</prevsent>
</prevsection>
<citsent citstr=" N06-1025 ">
the use of semantic knowledge for coreference resolution has been studied before in number of works, among them (ponzetto and strube, 2006), (<papid> N06-1025 </papid>bengtson and roth, 2008), (<papid> D08-1031 </papid>lee et al, 2011), <papid> W11-1902 </papid>and (rahman and ng, 2011).</citsent>
<aftsection>
<nextsent>the focus in these studies has been on the semantic similarity between mention and candidate antecedent, or the parallelism between the semantic role structures in which the two appear.
</nextsent>
<nextsent>one of the earliest methods for usingpredicate-argument frequencies in pronoun resolution is that of dagan and itai (1990).<papid> C90-3063 </papid></nextsent>
<nextsent>closer to our use of semantic compatibility features for pronouns are the approaches of kehler et al (2004) <papid> N04-1037 </papid>and yang et al (2005).<papid> P05-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1823">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>compared to it, our adaptive clustering approach is less constrained: it uses only weak, partial ordering between coreference decisions, and does not require singleton cluster at every clustering step.
</prevsent>
<prevsent>this allows clustering to start in any section of the document where coreference decisions are easier to make, and thus create accurate clusters earlier in the process.
</prevsent>
</prevsection>
<citsent citstr=" D08-1031 ">
the use of semantic knowledge for coreference resolution has been studied before in number of works, among them (ponzetto and strube, 2006), (<papid> N06-1025 </papid>bengtson and roth, 2008), (<papid> D08-1031 </papid>lee et al, 2011), <papid> W11-1902 </papid>and (rahman and ng, 2011).</citsent>
<aftsection>
<nextsent>the focus in these studies has been on the semantic similarity between mention and candidate antecedent, or the parallelism between the semantic role structures in which the two appear.
</nextsent>
<nextsent>one of the earliest methods for usingpredicate-argument frequencies in pronoun resolution is that of dagan and itai (1990).<papid> C90-3063 </papid></nextsent>
<nextsent>closer to our use of semantic compatibility features for pronouns are the approaches of kehler et al (2004) <papid> N04-1037 </papid>and yang et al (2005).<papid> P05-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1825">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the use of semantic knowledge for coreference resolution has been studied before in number of works, among them (ponzetto and strube, 2006), (<papid> N06-1025 </papid>bengtson and roth, 2008), (<papid> D08-1031 </papid>lee et al, 2011), <papid> W11-1902 </papid>and (rahman and ng, 2011).</prevsent>
<prevsent>the focus in these studies has been on the semantic similarity between mention and candidate antecedent, or the parallelism between the semantic role structures in which the two appear.</prevsent>
</prevsection>
<citsent citstr=" C90-3063 ">
one of the earliest methods for usingpredicate-argument frequencies in pronoun resolution is that of dagan and itai (1990).<papid> C90-3063 </papid></citsent>
<aftsection>
<nextsent>closer to our use of semantic compatibility features for pronouns are the approaches of kehler et al (2004) <papid> N04-1037 </papid>and yang et al (2005).<papid> P05-1021 </papid></nextsent>
<nextsent>the last work showed that pronoun resolution can be improved by incorporating semantic compatibility features derived from search engine statistics in the twin-candidate model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1826">
<title id=" S12-1002.xml">adaptive clustering for coreference resolution with deterministic rules and web based language models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the focus in these studies has been on the semantic similarity between mention and candidate antecedent, or the parallelism between the semantic role structures in which the two appear.
</prevsent>
<prevsent>one of the earliest methods for usingpredicate-argument frequencies in pronoun resolution is that of dagan and itai (1990).<papid> C90-3063 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-1037 ">
closer to our use of semantic compatibility features for pronouns are the approaches of kehler et al (2004) <papid> N04-1037 </papid>and yang et al (2005).<papid> P05-1021 </papid></citsent>
<aftsection>
<nextsent>the last work showed that pronoun resolution can be improved by incorporating semantic compatibility features derived from search engine statistics in the twin-candidate model.
</nextsent>
<nextsent>in our approach, we use web-based language models to compute semantic compatibility features for neutral pronouns and show that they can improve performance over state-of-the-art coreference resolution system.
</nextsent>
<nextsent>the use of language models instead of search engine statistics is more practical, as they eliminate the latency involved in using search engine queries.
</nextsent>
<nextsent>web based language models can be built on readily available web n-gram corpora, such as googles web 1t 5-gram corpus (brants and franz, 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1829">
<title id=" S10-1025.xml">swat cross lingual lexical substitution using local context matching bilingual dictionaries and machine translation </title>
<section> scoring.  </section>
<citcontext>
<prevsection>
<prevsent>in this task, participants were asked to substitute single marked word in an english sentence with the most appropriate spanish translation(s) given the context.
</prevsent>
<prevsent>on the surface, our two systems are very similar, performing monolingual lexical substitution and using translation tools and bilingual dictionaries to make the transition from english to spanish.
</prevsent>
</prevsection>
<citsent citstr=" W07-2009 ">
the task organizers used two scoring metrics adapted from the semeval-2007 english lexical substitution task (mccarthy and navigli, 2007).<papid> W07-2009 </papid></citsent>
<aftsection>
<nextsent>for each test item i, human annotators provided multi set of substitutions, i , that formed the goldstandard.
</nextsent>
<nextsent>given system-provided multi set answer ifor test item i, the best score for single test item is computed using (1).
</nextsent>
<nextsent>systems were allowed to provide an unlimited number of responses in i, but each items best score was divided by the number of answers provided in i . best score = ? ss frequency(s ? i ) |s | ? |t | (1)the out-of-ten score, henceforth oot, limited systems to maximum of 10 responses for each test item.
</nextsent>
<nextsent>unlike the best scoring method, the final score for each test item in the oot method is not divided by the actual number of responses provided by the system; therefore, systems could maximize their score by always providing exactly 10 responses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1833">
<title id=" S10-1025.xml">swat cross lingual lexical substitution using local context matching bilingual dictionaries and machine translation </title>
<section> systems.  </section>
<citcontext>
<prevsection>
<prevsent>the swat-s system performed these two steps in the reverse or der: first, the english sentences were translated into spanish and then the monolingual lexical substitution algorithm was run on the translated out put to provide ranked list of spanish substitutes.
</prevsent>
<prevsent>3.1 syntagmatic coherence.
</prevsent>
</prevsection>
<citsent citstr=" W07-2029 ">
the monolingual lexical substitution algorithm used by both systems is an implementation of the syntagmatic coherence criterion used by the irst2system (giuliano et al, 2007) <papid> W07-2029 </papid>in the semeval 2007 lexical substitution task..</citsent>
<aftsection>
<nextsent>for sentence w containing the target word w, the irst2 algorithm first compiles set, e, of candidate substitutes for from dictionary, thesaurus, or other lexical resource.
</nextsent>
<nextsent>for each ? e, e is formed by substituting for in w . each.
</nextsent>
<nextsent>n-gram (2 ? ? 5) of econtaining the substitute is assigned score, , equal to how frequently the n-gram appeared in large corpus.
</nextsent>
<nextsent>for all triples (e, n, f) where   0, we add (e, n, f) to ? .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1835">
<title id=" S10-1025.xml">swat cross lingual lexical substitution using local context matching bilingual dictionaries and machine translation </title>
<section> systems.  </section>
<citcontext>
<prevsection>
<prevsent>e ? is then sorted by n, with ties broken by . the highest ranked item in ?, therefore, is the triple containing the synonym that appeared in the longest, most frequently occurring n-gram.
</prevsent>
<prevsent>note that each candidate substitute can appear multiple times in ? : once for each value of n. the list becomes the final output of the syn tag matic coherence criterion, providing ranking for all candidate substitutes in e. 3.2 the swat-e system.
</prevsent>
</prevsection>
<citsent citstr=" W02-0109 ">
3.2.1 resources the swat-e system used the english web1t 5gram corpus (brants and franz, 2006), the spanish section of the web1t european 5-gram corpus (brants and franz, 2009), rogets online thesaurus 1 , nltks implementation of the lancaster stemmer (loper and bird, 2002), <papid> W02-0109 </papid>googles online english-spanish dictionary 2 , and spanishdicts online dictionary 3 . we formed single spanish-.</citsent>
<aftsection>
<nextsent>english dictionary by combining the translations found in both dictionaries.
</nextsent>
<nextsent>1 http://thesaurus.com 2 http://www.google.com/dictionary 3 http://www.spanishdict.com 3.2.2 ranking substitutes the first step in the swat-e algorithm is to createa ranked list of english substitutes.
</nextsent>
<nextsent>for each english test sentence w containing the target wordw, we use the syntagmatic coherence criterion described above to create ?, ranking of the synonyms of taken from rogets thesaurus.
</nextsent>
<nextsent>we use the lancaster stemmer to ensure that we count all morphologically similar lexical substitutes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1836">
<title id=" S10-1025.xml">swat cross lingual lexical substitution using local context matching bilingual dictionaries and machine translation </title>
<section> systems.  </section>
<citcontext>
<prevsection>
<prevsent>(we never returned five candidates).
</prevsent>
<prevsent>3.3 swat-s.
</prevsent>
</prevsection>
<citsent citstr=" C94-1027 ">
3.3.1 resources the swat-s system used both googles 4and ya hoos 5 online translation tools, the spanish section of the web1t european 5-gram corpus, rogets online thesaurus, tree tagger (schmid, 1994) <papid> C94-1027 </papid>for 4 http://translate.google.com/ 5 http://babelfish.yahoo.com/morphological analysis and both googles and ya hoos 6 english-spanish dictionaries.</citsent>
<aftsection>
<nextsent>we formed single spanish-english dictionary by combining the translations found in both dictionaries.
</nextsent>
<nextsent>3.3.2 ranking substitutes to find the cross-lingual lexical substitutes for atarget word in an english sentence, we first translate the sentence into spanish and then use the syntagmatic coherence criterion on the translated spanish sentence.
</nextsent>
<nextsent>in order to perform this monolingual spanish lexical substitution, we need to be able to identify the target word we are attempting to substitute in the translated sentence.
</nextsent>
<nextsent>we experimented with using moses (koehn et al, 2007) <papid> P07-2045 </papid>to perform the machine translation and produce word alignment but we found that googles online translation tool produced better translations than moses did when trained on the europarl data we had available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1837">
<title id=" S10-1025.xml">swat cross lingual lexical substitution using local context matching bilingual dictionaries and machine translation </title>
<section> systems.  </section>
<citcontext>
<prevsection>
<prevsent>3.3.2 ranking substitutes to find the cross-lingual lexical substitutes for atarget word in an english sentence, we first translate the sentence into spanish and then use the syntagmatic coherence criterion on the translated spanish sentence.
</prevsent>
<prevsent>in order to perform this monolingual spanish lexical substitution, we need to be able to identify the target word we are attempting to substitute in the translated sentence.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
we experimented with using moses (koehn et al, 2007) <papid> P07-2045 </papid>to perform the machine translation and produce word alignment but we found that googles online translation tool produced better translations than moses did when trained on the europarl data we had available.</citsent>
<aftsection>
<nextsent>in the original english sentence, the target word is marked with an xml tag.
</nextsent>
<nextsent>we had hoped that googles translation tool would preserve the xml tag around the translated target word, but that was not the case.
</nextsent>
<nextsent>we also experimented with using quotation marks around the target word instead of the xml tag.
</nextsent>
<nextsent>the translation tool often preserved quotation marks around the target word, but also yielded different, and anecdotally worse, translation than the same sentence without the quotation marks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1840">
<title id=" S10-1030.xml">deriunlp a context based approach to automatic key phrase extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>typically any key phrase extraction system works in two stages.
</prevsent>
<prevsent>in the first stage general set of candidates is selected by extracting the tokens of text.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
in the second stage unsupervised approaches combine set of features in rank to select the most important key phrases and supervised approaches use training corpus to learn key phrase extraction model.mihalcea and tarau (2004) <papid> W04-3252 </papid>propose an unsupervised approach that considers single tokens as vertices of graph and co-occurrence relations between tokens as edges.</citsent>
<aftsection>
<nextsent>candidates are ranked using page rank and adjacent keywords are merged into key phrases in post-processing step.
</nextsent>
<nextsent>the frequency of noun phrase heads is exploited by barker and cornacchia (2000), using noun phra sesas candidates and ranking them based on term frequency and term length.
</nextsent>
<nextsent>kea is supervised system that uses all n-grams of certain length, naive bayes classifier and tf-idf and position features (frank et al, 1999).
</nextsent>
<nextsent>turney (2000) introduces extractor, supervised system that selects stems and stemmed n-gramsas candidates and tunes its parameters (mainly related to frequency, position, length) with genetic algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1841">
<title id=" S10-1030.xml">deriunlp a context based approach to automatic key phrase extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kea is supervised system that uses all n-grams of certain length, naive bayes classifier and tf-idf and position features (frank et al, 1999).
</prevsent>
<prevsent>turney (2000) introduces extractor, supervised system that selects stems and stemmed n-gramsas candidates and tunes its parameters (mainly related to frequency, position, length) with genetic algorithm.
</prevsent>
</prevsection>
<citsent citstr=" N04-4005 ">
hulth (2004) <papid> N04-4005 </papid>experiments with three types of candidate terms (i.e., n-grams, noun phrase chunks and part-of-speech tagged words 146that match set of patterns) and constructs classifiers by rule induction using features such as term frequency, collection frequency, relative position and pos tags.the candidate selection method is the main difference between our approach and previous work.</citsent>
<aftsection>
<nextsent>we did not use only general description of termto select candidates, but we also took into consideration context information.
</nextsent>
<nextsent>method skill types are important domain words that are general enough to be used in different sub fields and that reflect theoretical or practical expertise.
</nextsent>
<nextsent>consider for instance the following extracts from scientific articles: ...analysis of historical trends...
</nextsent>
<nextsent>...duplicate photo detection algorithm ...
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1842">
<title id=" S10-1030.xml">deriunlp a context based approach to automatic key phrase extraction </title>
<section> the skill types candidate selection.  </section>
<citcontext>
<prevsection>
<prevsent>some of these skill types are valid for any scientific area (e.g. approach?, method?, analysis?, solution?)
</prevsent>
<prevsent>but we can also identify domain specific skill types, e.g., for computer science implementation?, algorithm?, development?, framework?, for physics proof?, prin ciples?, explanation?
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
and for chemistry law?,composition?, mechanism?, reaction?, struc ture?.our system is based on the gate natural language processing framework (cunningham et al, 2002) <papid> P02-1022 </papid>and it uses the annie ie system included in the standard gate distribution for text tokenization, sentence splitting and part-of-speechtagging.</citsent>
<aftsection>
<nextsent>the gate processing pipeline is depicted in figure 1, where the light grey boxes embody components available as part of the gate framework whereas the dark grey boxes represent components implemented as part of our system.
</nextsent>
<nextsent>we manually extract set of 81 single word skill types for the computer science field by analysing word frequencies for topics from the acm classification system 1 . the skill types that appear most.
</nextsent>
<nextsent>1 acm classification system: http://www.acm.
</nextsent>
<nextsent>org/about/class/ figure 1: gate processing pipeline frequently in key phrases given in the training set are system?, model?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1843">
<title id=" S10-1030.xml">deriunlp a context based approach to automatic key phrase extraction </title>
<section> ranking and filtering.  </section>
<citcontext>
<prevsection>
<prevsent>therefore we define the rank for topic as: 147 method 5p 5r 5f 10p 10r 10f 15p 15r 15f tf-idf 22 7.5 11.19 17.7 12.07 14.35 14.93 15.28 15.1 nb 21.4 7.3 10.89 17.3 11.8 14.03 14.53 14.87 14.7 me 21.4 7.3 10.89 17.3 11.8 14.03 14.53 14.87 14.7 deriunlp 27.4 9.35 13.94 23 15.69 18.65 22 22.51 22.25 dub 15.83 5.13 7.75 13.40 8.68 10.54 13.33 12.96 13.14 table 1: baseline and deriunlp performance aver combined keywords system 5p 5r 5f 10p 10r 10f 15p 15r 15f best 39.0 13.3 19.8 32.0 21.8 26.0 27.2 27.8 27.5 average 29.6 10.1 15 26.1 17.8 21.2 21.9 22.4 22.2 worst 9.4 3.2 4.8 5.9 4.0 4.8 5.3 5.4 5.3 deriunlp 27.4 9.4 13.9 23.0 15.7 18.7 22.0 22.5 22.3 table 2: performance over combined keywords i,j = tn ? fn ? tfidf i,j where i is the rank for the candidate and the document j, tn iis the normalized number of tokens (number of tokens divided by the maximum number of tokens for keyphrase), fn iis the normalized collection frequency of the candidate inthe context of skill type (collection frequency divided by the maximum collection frequency), and tfidf is the tf-idf for candidate and topic (computed based on extracted topics not based on all words).
</prevsent>
<prevsent>filtering.
</prevsent>
</prevsection>
<citsent citstr=" C08-2021 ">
several approaches (paukkeri et al, 2008; <papid> C08-2021 </papid>tomokiyo and hurst, 2003) <papid> W03-1805 </papid>use reference corpus for key phrase extraction.</citsent>
<aftsection>
<nextsent>we decided touse the documents available on the web as reference corpus, therefore we use an external web search engine to filter out the candidates that aretoo general from the final result set.
</nextsent>
<nextsent>if candidate has more than 10 9 hits on the web it is too general to be included in the final result set.
</nextsent>
<nextsent>a lot of noise is introduced by general combination ofwords that could appear in any document.
</nextsent>
<nextsent>we remove candidates longer than eight words and we ignore key phrases that have one letter words or that include non-alphanumerical characters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1844">
<title id=" S10-1030.xml">deriunlp a context based approach to automatic key phrase extraction </title>
<section> ranking and filtering.  </section>
<citcontext>
<prevsection>
<prevsent>therefore we define the rank for topic as: 147 method 5p 5r 5f 10p 10r 10f 15p 15r 15f tf-idf 22 7.5 11.19 17.7 12.07 14.35 14.93 15.28 15.1 nb 21.4 7.3 10.89 17.3 11.8 14.03 14.53 14.87 14.7 me 21.4 7.3 10.89 17.3 11.8 14.03 14.53 14.87 14.7 deriunlp 27.4 9.35 13.94 23 15.69 18.65 22 22.51 22.25 dub 15.83 5.13 7.75 13.40 8.68 10.54 13.33 12.96 13.14 table 1: baseline and deriunlp performance aver combined keywords system 5p 5r 5f 10p 10r 10f 15p 15r 15f best 39.0 13.3 19.8 32.0 21.8 26.0 27.2 27.8 27.5 average 29.6 10.1 15 26.1 17.8 21.2 21.9 22.4 22.2 worst 9.4 3.2 4.8 5.9 4.0 4.8 5.3 5.4 5.3 deriunlp 27.4 9.4 13.9 23.0 15.7 18.7 22.0 22.5 22.3 table 2: performance over combined keywords i,j = tn ? fn ? tfidf i,j where i is the rank for the candidate and the document j, tn iis the normalized number of tokens (number of tokens divided by the maximum number of tokens for keyphrase), fn iis the normalized collection frequency of the candidate inthe context of skill type (collection frequency divided by the maximum collection frequency), and tfidf is the tf-idf for candidate and topic (computed based on extracted topics not based on all words).
</prevsent>
<prevsent>filtering.
</prevsent>
</prevsection>
<citsent citstr=" W03-1805 ">
several approaches (paukkeri et al, 2008; <papid> C08-2021 </papid>tomokiyo and hurst, 2003) <papid> W03-1805 </papid>use reference corpus for key phrase extraction.</citsent>
<aftsection>
<nextsent>we decided touse the documents available on the web as reference corpus, therefore we use an external web search engine to filter out the candidates that aretoo general from the final result set.
</nextsent>
<nextsent>if candidate has more than 10 9 hits on the web it is too general to be included in the final result set.
</nextsent>
<nextsent>a lot of noise is introduced by general combination ofwords that could appear in any document.
</nextsent>
<nextsent>we remove candidates longer than eight words and we ignore key phrases that have one letter words or that include non-alphanumerical characters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1845">
<title id=" S10-1075.xml">usfd2 annotating temporal expresions and tlinks for tempeval2 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>usfd2 identified temporal expressions successfully, and correctly classified their type in 90% of cases.
</prevsent>
<prevsent>determining the relation between an event and time expression in the same sentence was performed at 63% accuracy, the second highest score in this part of the challenge.
</prevsent>
</prevsection>
<citsent citstr=" W09-2418 ">
the tempeval-2 (pustejovsky and verhagen, 2009) <papid> W09-2418 </papid>challenge proposes six tasks.</citsent>
<aftsection>
<nextsent>our system tackles three of these: task ? identifying time expressions, assigning timex3 attribute values, and anchoring them; task ? determining the temporal relation between an event and time in the same sentence; and task ? determining the temporal relation between two main events in consecutive sentences.
</nextsent>
<nextsent>for our participation in the task, we decided to employ both rule- and ml-classifier based approaches.
</nextsent>
<nextsent>temporal expressions are dealt with by sets of rules and regular expressions, and relation labelling performed by nltks1 maximum entropy classifier with rule-based processing applied during feature generation.
</nextsent>
<nextsent>the features (described in full in section 2) included attributes 1see http://www.nltk.org/ . from the tempeval-2 training data annotation, augmented by features that can be directly derived from the annotated texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1846">
<title id=" S10-1075.xml">usfd2 annotating temporal expresions and tlinks for tempeval2 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>temporal expressions are dealt with by sets of rules and regular expressions, and relation labelling performed by nltks1 maximum entropy classifier with rule-based processing applied during feature generation.
</prevsent>
<prevsent>the features (described in full in section 2) included attributes 1see http://www.nltk.org/ . from the tempeval-2 training data annotation, augmented by features that can be directly derived from the annotated texts.
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
there are two main aimsof this work: (1) to create rule-based temporal expression annotator that includes knowledge from work published since gutime (mani and wilson, 2000) <papid> P00-1010 </papid>and measure its performance, and (2) to measure the performance of classifier that includes features based on temporal signals.our entry to the challenge, usfd2, is successor to usfd (hepple et al, 2007).<papid> W07-2098 </papid></citsent>
<aftsection>
<nextsent>in the rest ofthis paper, we will describe how usfd2 is constructed (section 2), and then go on to discuss its overall performance and the impact of some internal parameters on specific tempe val tasks.regarding classifiers, we found that despite using identical feature sets across relation classification tasks, performance varied significantly.
</nextsent>
<nextsent>we also found that usfd2 performance trends withtempeval-2 did not match those seen when classifiers were trained on other data while performing similar tasks.
</nextsent>
<nextsent>the paper closes with comments about future work.
</nextsent>
<nextsent>the tempeval-2 training and test sets are partitioned into data for entity recognition and description, and data for temporal relation classification.we will first discuss our approach for temporal expression recognition, description and anchoring,and then discuss our approach to two of there lation labelling tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1847">
<title id=" S10-1075.xml">usfd2 annotating temporal expresions and tlinks for tempeval2 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>temporal expressions are dealt with by sets of rules and regular expressions, and relation labelling performed by nltks1 maximum entropy classifier with rule-based processing applied during feature generation.
</prevsent>
<prevsent>the features (described in full in section 2) included attributes 1see http://www.nltk.org/ . from the tempeval-2 training data annotation, augmented by features that can be directly derived from the annotated texts.
</prevsent>
</prevsection>
<citsent citstr=" W07-2098 ">
there are two main aimsof this work: (1) to create rule-based temporal expression annotator that includes knowledge from work published since gutime (mani and wilson, 2000) <papid> P00-1010 </papid>and measure its performance, and (2) to measure the performance of classifier that includes features based on temporal signals.our entry to the challenge, usfd2, is successor to usfd (hepple et al, 2007).<papid> W07-2098 </papid></citsent>
<aftsection>
<nextsent>in the rest ofthis paper, we will describe how usfd2 is constructed (section 2), and then go on to discuss its overall performance and the impact of some internal parameters on specific tempe val tasks.regarding classifiers, we found that despite using identical feature sets across relation classification tasks, performance varied significantly.
</nextsent>
<nextsent>we also found that usfd2 performance trends withtempeval-2 did not match those seen when classifiers were trained on other data while performing similar tasks.
</nextsent>
<nextsent>the paper closes with comments about future work.
</nextsent>
<nextsent>the tempeval-2 training and test sets are partitioned into data for entity recognition and description, and data for temporal relation classification.we will first discuss our approach for temporal expression recognition, description and anchoring,and then discuss our approach to two of there lation labelling tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1849">
<title id=" S10-1075.xml">usfd2 annotating temporal expresions and tlinks for tempeval2 </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>if we are anchoring date, the offset is applied to dct, and date granularity adjusted according tothe coarse st temporal primitive present ? forex ample, if dct is 1997-06-12 and our timex is six months ago, value of 1997-01 is assigned, as it is unlikely that the temporal expression refers to the day precisely six months ago, unless followed by the word today.
</prevsent>
<prevsent>where weekday names are found, we used baldwins 7-day window (baldwin, 2002) to anchor these to calendrical timeline.
</prevsent>
</prevsection>
<citsent citstr=" C08-1070 ">
this technique has been found to be accurate over 94% of the time with newswire text (mazur and dale, 2008).<papid> C08-1070 </papid></citsent>
<aftsection>
<nextsent>where dates are found that do not specify year or clear temporal direction marker (e.g., april 17 vs. last july), our algorithm counts the number of days between dct and the next occurrence of that date.
</nextsent>
<nextsent>if this is over limit , then the date is assumed to be last year.
</nextsent>
<nextsent>this is very general rule and does not take into account the tendency of very-precisely-described dates to be closer to dct, and far off dates to be looselyspecified.
</nextsent>
<nextsent>an of 14 days gives the highest performance based on the tempeval-2 training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1850">
<title id=" S10-1075.xml">usfd2 annotating temporal expresions and tlinks for tempeval2 </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>338 table 2: sample of signals and the tempeval-2 temporal relation they suggest.
</prevsent>
<prevsent>signal phrase suggested relation previous after ahead of before so far overlap thereafter before in anticipation of before follows after since then before soon after after as of overlap-or-after throughout overlap 2.2 labelling temporal relations.
</prevsent>
</prevsection>
<citsent citstr=" P06-1095 ">
our approach for labelling temporal relations (ortlinks) is based on nltks maximum entropy classifier, using the feature sets initially proposed in mani et al (2006).<papid> P06-1095 </papid></citsent>
<aftsection>
<nextsent>features that describe temporal signals have been shown to givea 30% performance boost in tlinks that employ signal (derczynski and gaizauskas, 2010).thus, the features in mani et al (2006) <papid> P06-1095 </papid>are augmented with those used to describe signals detailed in derczynski and gaizauskas (2010), with some slight changes.</nextsent>
<nextsent>firstly, as there are no specific tlink/signal associations in the tempe val 2 data (unlike time bank (pustejovsky et al,2003)), usfd2 needs to perform signal identification and then associate signals with temporal relation between two events or timexes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1853">
<title id=" S10-1075.xml">usfd2 annotating temporal expresions and tlinks for tempeval2 </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>secondly, look-up list is used to provide tlink label hints based on signal word.
</prevsent>
<prevsent>a list of features employed by usfd2 is in table 1.
</prevsent>
</prevsection>
<citsent citstr=" W07-2052 ">
we used simplified version of the approach in cheng et al (2007) <papid> W07-2052 </papid>to identify signal words.</citsent>
<aftsection>
<nextsent>this involved the creation of list of signal phrases that occur in time bank with frequency of 2 or more, and associating signal from this list with temporal entity if it is in the same sentence and clause.
</nextsent>
<nextsent>the textually nearest signal is chosen in the case of conflict.
</nextsent>
<nextsent>as this list of signal phrases only contained 42 entries, we also decided to define most-likely?
</nextsent>
<nextsent>temporal relation for each signal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1855">
<title id=" S10-1075.xml">usfd2 annotating temporal expresions and tlinks for tempeval2 </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>accuracy in task was good (63%), despite the lack of explicit signal/tlink associations and the absence of sophisticated signal recognition and association mechanism.
</prevsent>
<prevsent>this is higher than usfd2s accuracy in task (45%) though the latter is harder task, as most tempeval-2 systems performed significantly worse at this task than event/timex relation classification.
</prevsent>
</prevsection>
<citsent citstr=" W07-2046 ">
signal information was not relied on by manytempeval 2007 systems (min et al (2007) <papid> W07-2046 </papid>dis 339cusses signals to some extent but the system described only includes single feature ? the signal text), and certainly no processing of this data was performed for that challenge.</citsent>
<aftsection>
<nextsent>usfd2 begin sto leverage this information, and gives very competitive performance at event/timex classification.
</nextsent>
<nextsent>in this case, the signals provided an increase from 61.5% to 63.1% predictive accuracy in task c. the small size of the improvement might be due to the crude and un evaluated signal identification and association system that we implemented.
</nextsent>
<nextsent>the performance of classifier based approaches to temporal link labelling seems to be levelling off ? the 60%-70% relation labelling accuracy of work such as mani et al (2006) <papid> P06-1095 </papid>has not been greatly exceeded.</nextsent>
<nextsent>this performance level is stil lthe peak of the current generation of systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1858">
<title id=" S10-1075.xml">usfd2 annotating temporal expresions and tlinks for tempeval2 </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the performance of classifier based approaches to temporal link labelling seems to be levelling off ? the 60%-70% relation labelling accuracy of work such as mani et al (2006) <papid> P06-1095 </papid>has not been greatly exceeded.</prevsent>
<prevsent>this performance level is stil lthe peak of the current generation of systems.</prevsent>
</prevsection>
<citsent citstr=" P09-1046 ">
recent improvements, while employing novel approaches to the task that relyon constraints between temporal link types or on complex linguistic information beyond that describable by timeml attributes, still yield marginal improvements (e.g. yoshikawa et al (2009)).<papid> P09-1046 </papid></citsent>
<aftsection>
<nextsent>it seems that to breakthrough this performance wall?, we need to continue to innovate with and discuss temporal relation labelling, using information and knowledge from many sources to build practical high performance systems.
</nextsent>
<nextsent>in this paper, we have presented usfd2, novel system that annotates temporal expressions and temporal links in text.
</nextsent>
<nextsent>the system relies on new hand-crafted rules, existing rule sets, machine learning and temporal signal information to make its decisions.
</nextsent>
<nextsent>although some of the tempeval-2 tasks are difficult, usfd2 manages to create good and useful annotations of temporal information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1859">
<title id=" S12-1071.xml">buap a first approximation to relational similarity measuring </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>we describe system proposed for measuring the degree of relational similarity beetwen pair of words at the task #2 of semeval 2012.the approach presented is based on vector ial representation using the following fea tures: i) the context surrounding the words with windows size = 3, ii) knowledge extracted from wordnet to discover several semantic relationships, such as meronymy, hy ponymy, hypernymy, and part-whole between pair of words, iii) the description of the pairs with their pos tag, morphological information (gender, person), and iv) the average number of words separating the two words in text.
</prevsent>
</prevsection>
<citsent citstr=" S12-1047 ">
the task # 2 of semeval 2012 focuses on measuring the degree of relational similarity between the reference words pairs (training) and the test pairs forgiven class (jurgens et al, 2012).<papid> S12-1047 </papid></citsent>
<aftsection>
<nextsent>the training dataset consists of 10 classes and the testing dataset consists of the 69 classes.
</nextsent>
<nextsent>these datasets as well as the particularities of the task are better described at overview paper (jurgens et al,2012).<papid> S12-1047 </papid></nextsent>
<nextsent>in this paper we report the approach submitted to the competition, which is based on vector space model representation for each pair (salton et al., 1975).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1861">
<title id=" S12-1071.xml">buap a first approximation to relational similarity measuring </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these datasets as well as the particularities of the task are better described at overview paper (jurgens et al,2012).<papid> S12-1047 </papid></prevsent>
<prevsent>in this paper we report the approach submitted to the competition, which is based on vector space model representation for each pair (salton et al., 1975).</prevsent>
</prevsection>
<citsent citstr=" S10-1043 ">
with respect to the type of features used,we have observed that fabio celli (celli, 2010) <papid> S10-1043 </papid>considers that contextual information is useful, as wellthe lexical and semantic information are in the extraction of semantic relationships task.</citsent>
<aftsection>
<nextsent>additionally, in (chen et al, 2010) <papid> S10-1050 </papid>and (negri and kouylekov, 2010) <papid> S10-1044 </papid>are proposed wordnet based features with the same purpose.</nextsent>
<nextsent>in the experiments carried out in this paper, we use set of lexical, semantic, wordnet-based and contextual features which allows to construct the vectors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1863">
<title id=" S12-1071.xml">buap a first approximation to relational similarity measuring </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we report the approach submitted to the competition, which is based on vector space model representation for each pair (salton et al., 1975).
</prevsent>
<prevsent>with respect to the type of features used,we have observed that fabio celli (celli, 2010) <papid> S10-1043 </papid>considers that contextual information is useful, as wellthe lexical and semantic information are in the extraction of semantic relationships task.</prevsent>
</prevsection>
<citsent citstr=" S10-1050 ">
additionally, in (chen et al, 2010) <papid> S10-1050 </papid>and (negri and kouylekov, 2010) <papid> S10-1044 </papid>are proposed wordnet based features with the same purpose.</citsent>
<aftsection>
<nextsent>in the experiments carried out in this paper, we use set of lexical, semantic, wordnet-based and contextual features which allows to construct the vectors.
</nextsent>
<nextsent>actually, we have tested subset of the 20 contextual features proposed by celli (celli, 2010) <papid> S10-1043 </papid>and some of those proposed by chen (chen et al, 2010) <papid> S10-1050 </papid>and negri (negri and kouylekov, 2010).<papid> S10-1044 </papid>the cosine similarity measure is used for determining the degree of relational similarity (frakes and baeza-yates, 1992) among the vectors.</nextsent>
<nextsent>the rest of this paper is structured as follows.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1865">
<title id=" S12-1071.xml">buap a first approximation to relational similarity measuring </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we report the approach submitted to the competition, which is based on vector space model representation for each pair (salton et al., 1975).
</prevsent>
<prevsent>with respect to the type of features used,we have observed that fabio celli (celli, 2010) <papid> S10-1043 </papid>considers that contextual information is useful, as wellthe lexical and semantic information are in the extraction of semantic relationships task.</prevsent>
</prevsection>
<citsent citstr=" S10-1044 ">
additionally, in (chen et al, 2010) <papid> S10-1050 </papid>and (negri and kouylekov, 2010) <papid> S10-1044 </papid>are proposed wordnet based features with the same purpose.</citsent>
<aftsection>
<nextsent>in the experiments carried out in this paper, we use set of lexical, semantic, wordnet-based and contextual features which allows to construct the vectors.
</nextsent>
<nextsent>actually, we have tested subset of the 20 contextual features proposed by celli (celli, 2010) <papid> S10-1043 </papid>and some of those proposed by chen (chen et al, 2010) <papid> S10-1050 </papid>and negri (negri and kouylekov, 2010).<papid> S10-1044 </papid>the cosine similarity measure is used for determining the degree of relational similarity (frakes and baeza-yates, 1992) among the vectors.</nextsent>
<nextsent>the rest of this paper is structured as follows.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1871">
<title id=" S10-1082.xml">ksu kdd word sense induction by clustering in topic space </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ambiguity of meaning is inherent in natural language because the deliverer of words tries to minimize the size of the vocabulary set he uses.
</prevsent>
<prevsent>therefore, sizable portion of this vocabulary is polyse mous and the intended meaning of such words can be encoded in their context.
</prevsent>
</prevsection>
<citsent citstr=" D07-1108 ">
due to the knowledge acquisition bottleneck problem and scarcity in training data (cai et al ., 2007), <papid> D07-1108 </papid>unsupervised corpus based approaches could be favored over supervised ones in word sense disambiguation (wsd) tasks.</citsent>
<aftsection>
<nextsent>similar efforts in this area include work by cai et al  (cai et al , 2007) <papid> D07-1108 </papid>in which they use latent dirichlet al ocation (lda) topic models to extract the global context topic and use it as feature along other baseline features.</nextsent>
<nextsent>another technique uses clustering based approach with wordnet as an external resource for disambiguation without relying on training data (anaya-sanchez et al , 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1873">
<title id=" S10-1082.xml">ksu kdd word sense induction by clustering in topic space </title>
<section> evaluation measures.  </section>
<citcontext>
<prevsection>
<prevsent>for k-topicstopic model, each topics distribution can be represented as point in k-dimensional topic space.these points can be clustered into different clusters, each representing word sense.
</prevsent>
<prevsent>we used mallets k-means clustering algorithm with cosine similarity to measure the distance between different topic distributions in the topic space.
</prevsent>
</prevsection>
<citsent citstr=" W09-2419 ">
we use the same unsupervised evaluation measures used in semeval-2 (manandhar and klapaftis, 2009).<papid> W09-2419 </papid></citsent>
<aftsection>
<nextsent>these measures do not require de scriptivethe v-measure is used for unsupervised evaluation.
</nextsent>
<nextsent>it is the harmonic mean of the homogeneity and completeness.
</nextsent>
<nextsent>homogeneity is measure of the degree that each formed cluster consists of data points that belong to single gold standard (gs) class as defined below.
</nextsent>
<nextsent>homogeneity = 1 ? h(gsc) h(gs) (1) h(gs) = ? gs?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1874">
<title id=" S10-1026.xml">colepl and colslm an unsupervised wsd approach to multilingual lexical substitution tasks 2 and 3 semeval 2010 </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>our system ranks last for dutch among 3 systems and it is middle of the pack for the spanish language task.in general we note that the results for oot are naturally higher than for best since by design it is more relaxed measure.
</prevsent>
<prevsent>our work mainly investigates the influence ofwsd on providing machine translation candidates.
</prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
carpuat &amp; wu (2007) <papid> D07-1007 </papid>and chan et al(2007) <papid> P07-1005 </papid>show wsd improves mt. however, in (carpuat&amp; wu, 2007) <papid> D07-1007 </papid>classical wsd is missing by ignoring predefined senses.</citsent>
<aftsection>
<nextsent>they treat translation candidates as sense labels, then find linguistic features in the english side, and cast the disambiguation process as classification problem.
</nextsent>
<nextsent>of relevance also to our work is that related to the task of english monolingual lexical substitution.
</nextsent>
<nextsent>for example some of the approaches that participated in the semeval 2007 excercise include the following.
</nextsent>
<nextsent>yuret (2007) used statistical language model based on large corpus to assign likelihoods to each candidate substitutes for target word in sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1875">
<title id=" S10-1026.xml">colepl and colslm an unsupervised wsd approach to multilingual lexical substitution tasks 2 and 3 semeval 2010 </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>our system ranks last for dutch among 3 systems and it is middle of the pack for the spanish language task.in general we note that the results for oot are naturally higher than for best since by design it is more relaxed measure.
</prevsent>
<prevsent>our work mainly investigates the influence ofwsd on providing machine translation candidates.
</prevsent>
</prevsection>
<citsent citstr=" P07-1005 ">
carpuat &amp; wu (2007) <papid> D07-1007 </papid>and chan et al(2007) <papid> P07-1005 </papid>show wsd improves mt. however, in (carpuat&amp; wu, 2007) <papid> D07-1007 </papid>classical wsd is missing by ignoring predefined senses.</citsent>
<aftsection>
<nextsent>they treat translation candidates as sense labels, then find linguistic features in the english side, and cast the disambiguation process as classification problem.
</nextsent>
<nextsent>of relevance also to our work is that related to the task of english monolingual lexical substitution.
</nextsent>
<nextsent>for example some of the approaches that participated in the semeval 2007 excercise include the following.
</nextsent>
<nextsent>yuret (2007) used statistical language model based on large corpus to assign likelihoods to each candidate substitutes for target word in sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1877">
<title id=" S10-1008.xml">semeval2010 task 10 linking events and their participants in discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the shared task we looked at one particular aspect of cross-sentence links between argument structures, namely linking locally uninstantiated roles to their co-referents in the wider discourse context (if suchco-referents exist).
</prevsent>
<prevsent>this task is potentially beneficial for number of nlp applications, such as information extraction, question answering or text summarization.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
semantic role labeling (srl) has been defined as sentence-level natural-language processing taskin which semantic roles are assigned to the syntactic arguments of predicate (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>semantic roles describe the function of the participants in an event.
</nextsent>
<nextsent>identifying the semantic roles of the predicates in text allows knowing who did what to whom when where how, etc.however, semantic role labeling as it is currently defined misses lot of information due to the fact that it is viewed as sentence-internaltask.
</nextsent>
<nextsent>hence, relations between different local semantic argument structures are disregarded.
</nextsent>
<nextsent>this view of srl as sentence-internal task is partly due to the fact that large-scale manual annotation projects such as framenet 1 and propbank 2typ ically present their annotations lexico graphically by lemma rather than by source text.it is clear that there is an interplay between local argument structure and the surrounding discourse (fillmore, 1977).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1878">
<title id=" S10-1008.xml">semeval2010 task 10 linking events and their participants in discourse </title>
<section> null instantiations.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, the omission in (4) is lexically specific: the verb arrive allows the goal to be unspecified but the verb reach, also member of the arriving frame, does not.
</prevsent>
<prevsent>(4) we arrived 0 goal at 8pm.
</prevsent>
</prevsection>
<citsent citstr=" P86-1004 ">
3palmer et al (1986) <papid> P86-1004 </papid>treatment of uninstantiated essential roles?</citsent>
<aftsection>
<nextsent>is very similar (see also palmer (1990)).the above two examples also illustrate the second major dimension of variation.
</nextsent>
<nextsent>whereas, in (3)the protagonist making the mistake is only existentially bound within the discourse (instance of in definite null instantiation, ini), the goal location in (4) is an entity that must be accessible to speaker and hearer from the discourse or its context (def inite null instantiation, dni).
</nextsent>
<nextsent>finally, note that the licensing construction or lexical item fully and reliably determines the interpretation.
</nextsent>
<nextsent>whereas missing by-phrases have always an indefinite interpretation, whenever arrive omits the goal lexi cally, the goal has to be interpreted as definite, as it is in (4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1879">
<title id=" S10-1008.xml">semeval2010 task 10 linking events and their participants in discourse </title>
<section> description of the task.  </section>
<citcontext>
<prevsection>
<prevsent>we originally intended to offer the participants choice of two different tasks: full task, in which the test set was only annotated with gold standard word senses (i.e., frames) for the target word sand the participants had to perform role recogni tion/labeling and null instantiation linking, and ni only task, in which the test set was already annotated with gold standard semantic argument structures and the participants only had to recognize definite null instantiations and find links to antecedents in the wider context (ni linking).
</prevsent>
<prevsent>however, it turned out that the basic semantic role labeling task was already quite challenging for our dataset.
</prevsent>
</prevsection>
<citsent citstr=" W07-2018 ">
previous shared tasks have shown that frame-semantic srl of running text is hard problem (baker et al , 2007), <papid> W07-2018 </papid>partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available.</citsent>
<aftsection>
<nextsent>in our case the difficulty was increased because our data came from new genre and do main (i.e., crime fiction, see section 3.2).
</nextsent>
<nextsent>hence,we decided to add standard srl, i.e., role recognition and labeling, as third task (srl only).
</nextsent>
<nextsent>this task did not involve ni linking.
</nextsent>
<nextsent>3.2 data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1880">
<title id=" S10-1069.xml">schwa pete using ccg dependencies with the cx26c parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is extrinsic in the sense that it evaluates parsers on task, rather than direct comparison of their output against some gold standard.
</prevsent>
<prevsent>however, it requires only minimal task-specific logic, and the proposed ent ailments are designed to be inferrable based on syntactic information alone.
</prevsent>
</prevsection>
<citsent citstr=" P07-1032 ">
our system used the c&c; parser (clark and curran, 2007<papid> P07-1032 </papid>a), which uses the combinatory categorial grammar formalism (ccg, steedman, 2000).</citsent>
<aftsection>
<nextsent>we used the ccgbank-style dependency output of the parser (hockenmaier and steedman,2007), <papid> J07-3004 </papid>which is directed graph of head-child relations labelled with the heads lexical category and the argument slot filled by the child.</nextsent>
<nextsent>we divided the dependency graphs of the truth and hypothesis sentences into predicates that consisted of head word and its immediate children.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1882">
<title id=" S10-1069.xml">schwa pete using ccg dependencies with the cx26c parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, it requires only minimal task-specific logic, and the proposed ent ailments are designed to be inferrable based on syntactic information alone.
</prevsent>
<prevsent>our system used the c&c; parser (clark and curran, 2007<papid> P07-1032 </papid>a), which uses the combinatory categorial grammar formalism (ccg, steedman, 2000).</prevsent>
</prevsection>
<citsent citstr=" J07-3004 ">
we used the ccgbank-style dependency output of the parser (hockenmaier and steedman,2007), <papid> J07-3004 </papid>which is directed graph of head-child relations labelled with the heads lexical category and the argument slot filled by the child.</citsent>
<aftsection>
<nextsent>we divided the dependency graphs of the truth and hypothesis sentences into predicates that consisted of head word and its immediate children.
</nextsent>
<nextsent>for instance, the parsers analysis of the sentence totals include only vehicle sales reported in period might produce predicates like include(totals, sales), only(include), and reported(sales).
</nextsent>
<nextsent>if at least one such predicate matches in the two parses,we predict entailment.
</nextsent>
<nextsent>we consider single predicate match sufficient for entailment because the lexical categories and slots that constitute our dependency labels are often different in the hypothesis sentence due to the generation process used in the task.the single predicate heuristic gives us an over all accuracy of 70% on the test set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1885">
<title id=" S10-1069.xml">schwa pete using ccg dependencies with the cx26c parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this established an upper bound of 87% f-score for our approach.
</prevsent>
<prevsent>this upper bound suggests that there is still work to be done before the system allows transparent evaluation of the parser.
</prevsent>
</prevsection>
<citsent citstr=" P09-2014 ">
however, cross framework parser evaluation is difficult problem: previous attempts to evaluate the c&c; parser on grammatical relations (clark and curran, 2007<papid> P07-1032 </papid>b) and penn treebank-trees (clark and curran, 2009) <papid> P09-2014 </papid>have also produced upper bounds between 80 and 90% f-score.</citsent>
<aftsection>
<nextsent>our pete system was much easier to produce than either of these previous attempts at cross-framework parser evaluation, suggesting that this may be promising approach to difficult problem.
</nextsent>
<nextsent>313 totals include only vehicle sales reported in period.
</nextsent>
<nextsent>np (s\np )/np (s\np )\(s\np ) n/n s\np ((s\np )\(s\np ))/np np  ?     (s\np )/np ? np (s\np )\(s\np )   s\np ? np\np   np   s\np   figure 1: an example ccg derivation, showing how the categories assigned to words are combined to form sentence.
</nextsent>
<nextsent>the arrows indicate the direction of application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1890">
<title id=" S10-1069.xml">schwa pete using ccg dependencies with the cx26c parser </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>it is these dependencies that form the basis for our predicates in this task.
</prevsent>
<prevsent>only (s\np )\(s\np ) 1 include vehicle n/n 1 sales in ((s\np )\(s\np ))/np 2 period in ((s\np )\(s\np ))/np 1 reported reported s\np 1 sales include (s\np )/np 2 sales include (s\np )/np 1 totals figure 2: the dependencies represented by the derivation in figure 1.recent work has seen the development of high performance parsers built on the ccg formalism.
</prevsent>
</prevsection>
<citsent citstr=" D09-1085 ">
clark and curran (2007<papid> P07-1032 </papid>a) demonstrate the use of techniques like adaptive super tagging, parallel isation and dynamic-programming chart parsing algorithm to implement the c&c; parser, highly efficient ccg parser that performs well against parsers built on different formalisms (rimell et al, 2009).<papid> D09-1085 </papid></citsent>
<aftsection>
<nextsent>we use this parser for the pete task.
</nextsent>
<nextsent>the performance of statistical parsers is largely function of the quality of the corpora they are trained on.
</nextsent>
<nextsent>for this task, we used models derived from the ccgbank corpus ? transformation of the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>including ccg derivations and dependencies (hockenmaier, 2003<papid> P03-1046 </papid>a).</nextsent>
<nextsent>it was created to further ccg research by providing large corpus of appropriately annotated data, and has been shown to be suitable forthe training of high-performance parsers (hocken maier, 2003<papid> P03-1046 </papid>b; clark and curran, 2004).<papid> P04-1014 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1891">
<title id=" S10-1069.xml">schwa pete using ccg dependencies with the cx26c parser </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>we use this parser for the pete task.
</prevsent>
<prevsent>the performance of statistical parsers is largely function of the quality of the corpora they are trained on.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
for this task, we used models derived from the ccgbank corpus ? transformation of the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>including ccg derivations and dependencies (hockenmaier, 2003<papid> P03-1046 </papid>a).</citsent>
<aftsection>
<nextsent>it was created to further ccg research by providing large corpus of appropriately annotated data, and has been shown to be suitable forthe training of high-performance parsers (hocken maier, 2003<papid> P03-1046 </papid>b; clark and curran, 2004).<papid> P04-1014 </papid></nextsent>
<nextsent>our system used the c&c; parser to parse the truth and hypothesis sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1892">
<title id=" S10-1069.xml">schwa pete using ccg dependencies with the cx26c parser </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>we use this parser for the pete task.
</prevsent>
<prevsent>the performance of statistical parsers is largely function of the quality of the corpora they are trained on.
</prevsent>
</prevsection>
<citsent citstr=" P03-1046 ">
for this task, we used models derived from the ccgbank corpus ? transformation of the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>including ccg derivations and dependencies (hockenmaier, 2003<papid> P03-1046 </papid>a).</citsent>
<aftsection>
<nextsent>it was created to further ccg research by providing large corpus of appropriately annotated data, and has been shown to be suitable forthe training of high-performance parsers (hocken maier, 2003<papid> P03-1046 </papid>b; clark and curran, 2004).<papid> P04-1014 </papid></nextsent>
<nextsent>our system used the c&c; parser to parse the truth and hypothesis sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1894">
<title id=" S10-1069.xml">schwa pete using ccg dependencies with the cx26c parser </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the performance of statistical parsers is largely function of the quality of the corpora they are trained on.
</prevsent>
<prevsent>for this task, we used models derived from the ccgbank corpus ? transformation of the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>including ccg derivations and dependencies (hockenmaier, 2003<papid> P03-1046 </papid>a).</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
it was created to further ccg research by providing large corpus of appropriately annotated data, and has been shown to be suitable forthe training of high-performance parsers (hocken maier, 2003<papid> P03-1046 </papid>b; clark and curran, 2004).<papid> P04-1014 </papid></citsent>
<aftsection>
<nextsent>our system used the c&c; parser to parse the truth and hypothesis sentences.
</nextsent>
<nextsent>we took the dependencies generated by the parser and processed these to generate predicates encoding the canonical form of the head word, its required arguments, and their order.
</nextsent>
<nextsent>we then attempted to unify the predicates from the hypothesis sentence with the predicates in the truth sentence.
</nextsent>
<nextsent>a successful unification of predicates and occurs when the headwords of and are identical and their argument slots arealso identical.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1895">
<title id=" S10-1069.xml">schwa pete using ccg dependencies with the cx26c parser </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>to investigate the relative contributions of these two classes of errors, we manually annotated the 66 development sentences with ccgderivations.
</prevsent>
<prevsent>this allowed us to evaluate our system using gold standard parses.
</prevsent>
</prevsection>
<citsent citstr=" W09-3306 ">
only one annotator was available, so we were unable to calculate inter-annotator agreement scores to examine the quality of our annotations.the annotation was prepared with the annotation tool used by honnibal et al (2009).<papid> W09-3306 </papid></citsent>
<aftsection>
<nextsent>the tool presents the user with ccg derivation produced by the c&c; parser.
</nextsent>
<nextsent>the user can then correct the lexical categories, or add bracket constraints to the parser using the algorithm described by djordjevic and curran (2006), and reparse the sentence until the derivation desired is produced.
</nextsent>
<nextsent>our results with gold standard dependencies are 315shown in table 2.
</nextsent>
<nextsent>the accuracy is 87%, establishing fairly low upper bound for our approach tothe task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1901">
<title id=" S10-1067.xml">pkuhit an event detection system based on instances expansion and rich syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, in the ed module, complete situation description of the sentence can be achieved by combining the results of the wsd module and the srl module.
</prevsent>
<prevsent>for the wsd module, we consider the subtask as general wsd problem.
</prevsent>
</prevsection>
<citsent citstr=" H93-1052 ">
first of all, we automatically extract many instances from an untagged chinese corpus using heuristic rule inspired by yarowsky (1993).<papid> H93-1052 </papid></citsent>
<aftsection>
<nextsent>then we train nave bayesian (nb) classifier based on both the extracted instances and the official training data.
</nextsent>
<nextsent>we then use the nb classifier to predict situation the description formula and natural explanation of each target verb in testing data.
</nextsent>
<nextsent>for the srl module, we use rich syntactic feature-based learning method.
</nextsent>
<nextsent>as the state-of the-art method in the field of srl, feature-based method represents predicate-argument structure (pas) by flat vector using set of linguistic features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1903">
<title id=" S10-1067.xml">pkuhit an event detection system based on instances expansion and rich syntactic features </title>
<section> experimental results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in the srl module, we use the training data provided by semeval-2010 to train the svm classifiers without any external resources.
</prevsent>
<prevsent>the training data contain 4,608 sentences, 100 target predicates and 13,926 arguments.
</prevsent>
</prevsection>
<citsent citstr=" P03-1056 ">
we use the svm-light toolkit (joachims, 1999) for the implementation of svm, and use the stanford parser (levy and manning, 2003) <papid> P03-1056 </papid>as the parser and the constituent-to-dependency converter.</citsent>
<aftsection>
<nextsent>we employ the linear kernel for svm and set the regularization parameter to the default value which is the reciprocal of the average euclidean norm of the training data.
</nextsent>
<nextsent>the evaluation results of our srl module on the official test data are shown in table 3, where ab?, sr?, pt?
</nextsent>
<nextsent>and er?
</nextsent>
<nextsent>represent argument boundary, semantic role tag, phrase type tag, and event role tag.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1904">
<title id=" S12-1102.xml">soft cardinality  ml learning adaptive similarity functions for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thirdly,the machine-learning approach aims to learn similarity function based on vector representation of texts using subset of texts for training and learning function (bilenko and mooney, 2003).
</prevsent>
<prevsent>the three methods of adaptability can also be used in variety of combinations, e.g. term weighting in combination with machine learning (debole and sebastiani, 2003; lan et al, 2005).
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
finally, to achieve adaptability, other approaches use datasets considerably larger, such as large corpora or the web, e.g. distributional similarity (lee, 1999).<papid> P99-1004 </papid>in the machine-learning approach, vector representation of texts is used in conjunction with an algorithm of classification or regression (alpaydin, 2004).</citsent>
<aftsection>
<nextsent>each vector of features f1, f2, . . .
</nextsent>
<nextsent>, fm?
</nextsent>
<nextsent>is associated to each pair ti, tj?
</nextsent>
<nextsent>of texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1905">
<title id=" S12-1102.xml">soft cardinality  ml learning adaptive similarity functions for cross lingual textual entailment </title>
<section> learning similarity functions from.  </section>
<citcontext>
<prevsection>
<prevsent>cardinal ities different similarity measures use different knowledge, identify different types of commonalities, and compare objects with different granularity.
</prevsent>
<prevsent>in many of the automatic text-processing applications, the qualities of several similarity functions may be required to achieve the final task.
</prevsent>
</prevsection>
<citsent citstr=" W07-1407 ">
the combination of similarity scores with machine-learning algorithm to obtain unified effect for particular task is common practice (bilenko et al,2003; malakasiotis and androutsopoulos, 2007; <papid> W07-1407 </papid>malakasiotis, 2009).<papid> P09-3004 </papid></citsent>
<aftsection>
<nextsent>for each pair of texts for comparison, thereis provided vector representation based on multiple similarity scores as set of features.
</nextsent>
<nextsent>in addition, class attribute is associated with each vector which contains the objective of the task or the gold standard to be learned by the machine-learning algorithm.however, the similarity scores conceal important information when the task requires dealing with directional problems, i.e. whenever the order of comparing each pair of texts is related with the class attribute.
</nextsent>
<nextsent>for instance,textual entailment is directional task since it is necessary to recognize whether the first text entails the second text or vice versa.
</nextsent>
<nextsent>this problem can be addressed using asymmetric similarity functions and including scores for sim(a,b) and sim(b,a) in the resulting vector for each pair a,b?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1906">
<title id=" S12-1102.xml">soft cardinality  ml learning adaptive similarity functions for cross lingual textual entailment </title>
<section> learning similarity functions from.  </section>
<citcontext>
<prevsection>
<prevsent>cardinal ities different similarity measures use different knowledge, identify different types of commonalities, and compare objects with different granularity.
</prevsent>
<prevsent>in many of the automatic text-processing applications, the qualities of several similarity functions may be required to achieve the final task.
</prevsent>
</prevsection>
<citsent citstr=" P09-3004 ">
the combination of similarity scores with machine-learning algorithm to obtain unified effect for particular task is common practice (bilenko et al,2003; malakasiotis and androutsopoulos, 2007; <papid> W07-1407 </papid>malakasiotis, 2009).<papid> P09-3004 </papid></citsent>
<aftsection>
<nextsent>for each pair of texts for comparison, thereis provided vector representation based on multiple similarity scores as set of features.
</nextsent>
<nextsent>in addition, class attribute is associated with each vector which contains the objective of the task or the gold standard to be learned by the machine-learning algorithm.however, the similarity scores conceal important information when the task requires dealing with directional problems, i.e. whenever the order of comparing each pair of texts is related with the class attribute.
</nextsent>
<nextsent>for instance,textual entailment is directional task since it is necessary to recognize whether the first text entails the second text or vice versa.
</nextsent>
<nextsent>this problem can be addressed using asymmetric similarity functions and including scores for sim(a,b) and sim(b,a) in the resulting vector for each pair a,b?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1907">
<title id=" S12-1102.xml">soft cardinality  ml learning adaptive similarity functions for cross lingual textual entailment </title>
<section> learning similarity functions from.  </section>
<citcontext>
<prevsection>
<prevsent>for instance,textual entailment is directional task since it is necessary to recognize whether the first text entails the second text or vice versa.
</prevsent>
<prevsent>this problem can be addressed using asymmetric similarity functions and including scores for sim(a,b) and sim(b,a) in the resulting vector for each pair a,b?.
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
nevertheless, the similarity measures that are more commonly used are symmetric, e.g. edit distance (levenshtein, 1966), lcs (hirschberg, 1977),cosine similarity, and many of the current semantic relatedness measures (pedersen et al, 2004).<papid> N04-3012 </papid></citsent>
<aftsection>
<nextsent>although, there are asymmetric measures such as the monge-elkan measure (1996) and the measure proposed by corley and mihalcea (corley and mihalcea, 2005), <papid> W05-1203 </papid>they are outnumbered by the symmetric measures.</nextsent>
<nextsent>clearly, this situation restricts the use of the machine learning as method of combination for directional problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1908">
<title id=" S12-1102.xml">soft cardinality  ml learning adaptive similarity functions for cross lingual textual entailment </title>
<section> learning similarity functions from.  </section>
<citcontext>
<prevsection>
<prevsent>this problem can be addressed using asymmetric similarity functions and including scores for sim(a,b) and sim(b,a) in the resulting vector for each pair a,b?.
</prevsent>
<prevsent>nevertheless, the similarity measures that are more commonly used are symmetric, e.g. edit distance (levenshtein, 1966), lcs (hirschberg, 1977),cosine similarity, and many of the current semantic relatedness measures (pedersen et al, 2004).<papid> N04-3012 </papid></prevsent>
</prevsection>
<citsent citstr=" W05-1203 ">
although, there are asymmetric measures such as the monge-elkan measure (1996) and the measure proposed by corley and mihalcea (corley and mihalcea, 2005), <papid> W05-1203 </papid>they are outnumbered by the symmetric measures.</citsent>
<aftsection>
<nextsent>clearly, this situation restricts the use of the machine learning as method of combination for directional problems.
</nextsent>
<nextsent>alternatively, we propose the construction of vector for each pair of texts using cardinal ities instead of similarity scores.
</nextsent>
<nextsent>moreover, using cardinal ities rather than similarity scores allows the machine-learning algorithm to discover patterns to cope with directional tasks.
</nextsent>
<nextsent>basically, we propose to use set with six features for each cardinality function: |a|, |b|, |a ? b|, |a ? b|, |ab| and |b a|.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1909">
<title id=" S12-1102.xml">soft cardinality  ml learning adaptive similarity functions for cross lingual textual entailment </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>four balanced datasets were provided using the following languagepairs: german-english (deu-eng), french-english (fraeng), italian-english (ita-eng) and spanish-english (spaeng).
</prevsent>
<prevsent>the evaluation measure for experiments was accuracy, i.e. the ratio of correctly predicted pairs by the total number of predictions.
</prevsent>
</prevsection>
<citsent citstr=" S12-1053 ">
for comprehensive description of the task see (negri et al, 2012).<papid> S12-1053 </papid></citsent>
<aftsection>
<nextsent>4.2 experiments.
</nextsent>
<nextsent>given that each pair of texts t1, t2?
</nextsent>
<nextsent>are in different languages, pair of translations t1 , t2?
</nextsent>
<nextsent>were provided using google translate service.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1910">
<title id=" S12-1102.xml">soft cardinality  ml learning adaptive similarity functions for cross lingual textual entailment </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>and t 1 , t2?
</prevsent>
<prevsent>were in the same language.then, all produced pairs were pre-processed by removing stop-words in their respective languages.
</prevsent>
</prevsection>
<citsent citstr=" W02-0109 ">
finally, all texts were lemmatized using porters stemmer (1980) for english and snowball stem mers for other languages using an implementation provided by the nltk (loper and bird, 2002).<papid> W02-0109 </papid></citsent>
<aftsection>
<nextsent>then, different set of features were generated using similarity scores or cardinalities.
</nextsent>
<nextsent>while each symmetric similarity function generates 2 features i)sim(t1, t2) and ii)sim(t t1 , t2), asymmetric functions generate two additional features iii)sim(t t2 , t1) and iv)sim(t2, t 1).
</nextsent>
<nextsent>686 on the other hand, each cardinality function generates 12 features: i) |t1|, ii) |t t2 |, iii) |t1 ? t 2 |, iv) |t1 ? t 2 |, v) |t1 ? t2 |, vi) |t 2 ? t1|, vii) |t 1 |, viii) |t2|, ix) |t t1 ? t2|, x) |t 1 ? t2|, xi) |t 1 ? t2|, and xii) |t2 ? t 1 |.
</nextsent>
<nextsent>various combinations of cardinal ities, symmetric and asymmetric functions were used to generate the following feature sets: sym.simscores: scores of the following symmetric similarity functions: jaccard, dice, and cosine coefficients using classical cardinality and soft cardinality(edit-distance as auxiliar sim.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1911">
<title id=" S10-1042.xml">uvt the uvt term extraction system in the key phrase extraction task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the case of keyphraseassignment, suitable key phrases from an existing knowledge resource, such as controlled vocabulary, or thesaurus are assigned to documents based on classification of their content.
</prevsent>
<prevsent>in key phrase extraction, the phrases are mined from the document itself.
</prevsent>
</prevsection>
<citsent citstr=" W03-1805 ">
supervised approaches to the problem of key phrase extraction include the naive bayes-based kea algorithms (gordon etal., 1999) (medelyan and witten, 2006), decision tree-based and the genetic algorithm-based genex (turney, 1999), and the probabilistic kl divergence-based language model (tomokiyo and hurst, 2003).<papid> W03-1805 </papid></citsent>
<aftsection>
<nextsent>research in key phrase extraction proposes the detection of key phrases basedon various statistics-based, or pattern-based features.
</nextsent>
<nextsent>statistical measures investigated focus primarily on key phrase frequency measures, whereaspattern-features include noun phrase pattern filtering, identification of key phrase head and respective frequencies (barker and cornacchia, 2000), document section position of the key phrase (e.g., (medelyan and witten, 2006)) and key phrase coherence (turney, 2003).
</nextsent>
<nextsent>in this paper, we present an unsupervised approach which combinespattern-based morphosyntactic rules with statistical measure, the c-value measure (frantzi et al, 2000) which originates from research in the fieldof automatic term recognition and was initially designed for specialised domain terminology acquisition.
</nextsent>
<nextsent>the input documents in the key phrase extraction task were scientific articles converted from their originally published form to plain text.due to this process, some compound hyphenated words are erroneously converted into single word (e.g., resourcemanagement?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1912">
<title id=" S10-1042.xml">uvt the uvt term extraction system in the key phrase extraction task </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.5).
</prevsent>
<prevsent>2.1 linguistic pre-processing.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
for morphosyntactic analysis, we used the maxent (ratnaparkhi, 1996) <papid> W96-0213 </papid>pos tagger implementation of the opennlp tool suite 1 . in order to improve.</citsent>
<aftsection>
<nextsent>tagging accuracy, irregular text, such as urls,inline references, and recurrent patterns indicating footers and mathematical formulas are filtered prior to tagging.
</nextsent>
<nextsent>2.2 basic document structuring.
</nextsent>
<nextsent>document structuring is based on identified recurrent patterns, such as common section titles and legend indicators (e.g., abstract?, table...?),section headers numbering and preserved formatting, such as newline characters.
</nextsent>
<nextsent>thus, the document sections that the system may recognise are: title, abstract, introduction, conclusion, acknowledgements, references, header (for any other section headers and legends) and main (for any other document section text).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1913">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>despite this fact, most ir systems treat documents as indivisible units and index them in their entirety.
</prevsent>
<prevsent>this is problematic for two reasons.
</prevsent>
</prevsection>
<citsent citstr=" W95-0110 ">
first, most relevance metrics are based on word frequency, which can be viewed as function of the topic being discussed (church and gale, 1995).<papid> W95-0110 </papid></citsent>
<aftsection>
<nextsent>(for example, the word header is rare in general english, but it enjoys higher frequency in documents about soccer.)
</nextsent>
<nextsent>in general, word frequency is good indicator of whether document is relevant a query, but consider long document containing only one section relevant a query.
</nextsent>
<nextsent>if keyword is used only in the pertinent section, its overall frequency in the document will be low and, as result, the document as whole may be judged irrelevant despite the relevance of one section.
</nextsent>
<nextsent>the second reason it would be beneficial to index sections of documents that, once search engine has identified relevant document, users would benefit from direct access to the relevant sections.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1914">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>much research has been devoted to the task of structuring text--that is dividing texts into units based on information within the text.
</prevsent>
<prevsent>this work falls roughly into two categories.
</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
topic segmentation focuses on identifying topically- coherent blocks of text several sentences through several paragraphs in length (e.g. see hearst, 1994).<papid> P94-1002 </papid></citsent>
<aftsection>
<nextsent>the prime motivation for identifying such units is to improve performance on language- processing or ir tasks.
</nextsent>
<nextsent>discourse segmentation, on the other hand, is often finer-grained, and focuses on identifying relations between utterances (e.g. grosz and sidner, 1986 <papid> J86-3001 </papid>or hirschberg and grosz, 1992).<papid> H92-1089 </papid></nextsent>
<nextsent>many topic segment ations algorithms have been proposed in the literature.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1917">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>topic segmentation focuses on identifying topically- coherent blocks of text several sentences through several paragraphs in length (e.g. see hearst, 1994).<papid> P94-1002 </papid></prevsent>
<prevsent>the prime motivation for identifying such units is to improve performance on language- processing or ir tasks.</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
discourse segmentation, on the other hand, is often finer-grained, and focuses on identifying relations between utterances (e.g. grosz and sidner, 1986 <papid> J86-3001 </papid>or hirschberg and grosz, 1992).<papid> H92-1089 </papid></citsent>
<aftsection>
<nextsent>many topic segment ations algorithms have been proposed in the literature.
</nextsent>
<nextsent>there is not enough space to review them all here, so we will focus on describing representative sample that covers most of the features used to predict the location of boundaries.
</nextsent>
<nextsent>see (reynar, 1998) for more thorough review.
</nextsent>
<nextsent>youmans devised technique called the vocabulary management profile based on the location of first uses of word types.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1918">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>topic segmentation focuses on identifying topically- coherent blocks of text several sentences through several paragraphs in length (e.g. see hearst, 1994).<papid> P94-1002 </papid></prevsent>
<prevsent>the prime motivation for identifying such units is to improve performance on language- processing or ir tasks.</prevsent>
</prevsection>
<citsent citstr=" H92-1089 ">
discourse segmentation, on the other hand, is often finer-grained, and focuses on identifying relations between utterances (e.g. grosz and sidner, 1986 <papid> J86-3001 </papid>or hirschberg and grosz, 1992).<papid> H92-1089 </papid></citsent>
<aftsection>
<nextsent>many topic segment ations algorithms have been proposed in the literature.
</nextsent>
<nextsent>there is not enough space to review them all here, so we will focus on describing representative sample that covers most of the features used to predict the location of boundaries.
</nextsent>
<nextsent>see (reynar, 1998) for more thorough review.
</nextsent>
<nextsent>youmans devised technique called the vocabulary management profile based on the location of first uses of word types.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1919">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>youmans devised technique called the vocabulary management profile based on the location of first uses of word types.
</prevsent>
<prevsent>he posited that large clusters of first uses frequently followed topic boundaries since new topics generally introduce new vocabulary items (youmans, 1991).
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
morris and hirst developed an algorithm (morris and hirst, 1991) <papid> J91-1002 </papid>based on lexical cohesion relations (halliday and hasan, 1976).</citsent>
<aftsection>
<nextsent>they used roget 1977 thesaurus to identify synonyms and other cohesion relations.
</nextsent>
<nextsent>kozima defined measure called the lexical cohesion profile (lcp) based on spreading activation within semantic network derived from.
</nextsent>
<nextsent>a machine-readable dictionary.
</nextsent>
<nextsent>he identified topic boundaries where the lcp score was low (kozima, 1993).<papid> P93-1041 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1920">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>kozima defined measure called the lexical cohesion profile (lcp) based on spreading activation within semantic network derived from.
</prevsent>
<prevsent>a machine-readable dictionary.
</prevsent>
</prevsection>
<citsent citstr=" P93-1041 ">
he identified topic boundaries where the lcp score was low (kozima, 1993).<papid> P93-1041 </papid></citsent>
<aftsection>
<nextsent>hearst developed technique called text tiling that automatically divides expository texts into multi-paragraph segments using the vector space model from ir (hearst, 1994).<papid> P94-1002 </papid></nextsent>
<nextsent>topic boundaries were positioned where the similarity between the block of text before and after the boundary was low.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1924">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>hearst developed technique called text tiling that automatically divides expository texts into multi-paragraph segments using the vector space model from ir (hearst, 1994).<papid> P94-1002 </papid></prevsent>
<prevsent>topic boundaries were positioned where the similarity between the block of text before and after the boundary was low.</prevsent>
</prevsection>
<citsent citstr=" P94-1050 ">
in previous work (reynar, 1994), <papid> P94-1050 </papid>we described method of finding topic boundaries using an optimisation algorithm based on word repetition that was inspired by visualization technique known as dot plotting (helfman, 1994).</citsent>
<aftsection>
<nextsent>ponte and croft predict topic boundaries using model of likely topic length and query expansion technique called local content analysis that maps sets of words into space of concepts (ponte and croft, 1997).
</nextsent>
<nextsent>richmond, smith and amitay designed an algorithm for topic segmentation that weighted words based on their frequency within document and subsequently used these weights in formula based on the distance between repetitions of word types (richmond et al, 1997).<papid> W97-0305 </papid></nextsent>
<nextsent>beeferman, berger and lafferty used the relative performance of two statistical language models and cue words to identify topic boundaries (beeferman et al, 1997).<papid> W97-0304 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1925">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in previous work (reynar, 1994), <papid> P94-1050 </papid>we described method of finding topic boundaries using an optimisation algorithm based on word repetition that was inspired by visualization technique known as dot plotting (helfman, 1994).</prevsent>
<prevsent>ponte and croft predict topic boundaries using model of likely topic length and query expansion technique called local content analysis that maps sets of words into space of concepts (ponte and croft, 1997).</prevsent>
</prevsection>
<citsent citstr=" W97-0305 ">
richmond, smith and amitay designed an algorithm for topic segmentation that weighted words based on their frequency within document and subsequently used these weights in formula based on the distance between repetitions of word types (richmond et al, 1997).<papid> W97-0305 </papid></citsent>
<aftsection>
<nextsent>beeferman, berger and lafferty used the relative performance of two statistical language models and cue words to identify topic boundaries (beeferman et al, 1997).<papid> W97-0304 </papid></nextsent>
<nextsent>prior work on topic segmentation has exploited many different hints about where topic boundaries lie.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1926">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>ponte and croft predict topic boundaries using model of likely topic length and query expansion technique called local content analysis that maps sets of words into space of concepts (ponte and croft, 1997).
</prevsent>
<prevsent>richmond, smith and amitay designed an algorithm for topic segmentation that weighted words based on their frequency within document and subsequently used these weights in formula based on the distance between repetitions of word types (richmond et al, 1997).<papid> W97-0305 </papid></prevsent>
</prevsection>
<citsent citstr=" W97-0304 ">
beeferman, berger and lafferty used the relative performance of two statistical language models and cue words to identify topic boundaries (beeferman et al, 1997).<papid> W97-0304 </papid></citsent>
<aftsection>
<nextsent>prior work on topic segmentation has exploited many different hints about where topic boundaries lie.
</nextsent>
<nextsent>the algorithms we present use many cues from the literature as well as novel ones.
</nextsent>
<nextsent>our approach is statistical in nature and weights evidence based on its utility in segmenting training corpus.
</nextsent>
<nextsent>as result, we do not use clues to form hard and fast rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1928">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> new clues for topic segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>instead, they all contribute vidence used to either increase or decrease the likelihood of proposing topic boundary between two regions of text.
</prevsent>
<prevsent>3.1 domain-specific cue phrases.
</prevsent>
</prevsection>
<citsent citstr=" J93-3003 ">
many discourse segmentation techniques (e.g. hirschberg and litman, 1993) <papid> J93-3003 </papid>as well as some topic segmentation algorithms relyon cue words and phrases (e.g. beeferman et al, 1997), <papid> W97-0304 </papid>but the types of cue words used vary greatly.</citsent>
<aftsection>
<nextsent>those we employ are highly domain specific.
</nextsent>
<nextsent>taking an : 358 example from the broadcast news domain where we will demonstrate the effectiveness of our algorithms, the phrase joining us is good indicator that topic shift has just occurred because news anchors frequently say things such as joining us to discuss the crisis in kosovo is congressman... when beginning new stories.
</nextsent>
<nextsent>consequently, our algorithms use the presence of phrases uch as this one to boost the probability of topic boundary having occurred.
</nextsent>
<nextsent>joining us good evening brought you by this just in welcome back  person ame   station  this is  person ame  table 1: sampling of domain-specific cue phrases we employ.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1930">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> new clues for topic segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>the results we present later in the paper ely solely on manually identified cues phrases.
</prevsent>
<prevsent>identifying complex cue phrases involves pattern matching and determining whether particular word sequences belong to various classes.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
to address this, we built named entity recognition system in the spirit of those used for the message understanding conference evaluations (e.g. bikel et al, 1997).<papid> A97-1029 </papid></citsent>
<aftsection>
<nextsent>our named entity recognizer used maximum entropy model, built with adwait ratnaparkhi tools (ratnaparkhi, 1996) <papid> W96-0213 </papid>to label word sequences as either person, place, company or none of the above based on local cues including the surrounding words and whether honorifics (e.g. mrs. or gen.) or corporate designators (e.g. corp. or inc.) were present.</nextsent>
<nextsent>our algorithm labelling accuracy of 96.0% by token was sufficient for our purposes, but performance is not directly comparable to the muc competitors .</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1931">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> new clues for topic segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>identifying complex cue phrases involves pattern matching and determining whether particular word sequences belong to various classes.
</prevsent>
<prevsent>to address this, we built named entity recognition system in the spirit of those used for the message understanding conference evaluations (e.g. bikel et al, 1997).<papid> A97-1029 </papid></prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
our named entity recognizer used maximum entropy model, built with adwait ratnaparkhi tools (ratnaparkhi, 1996) <papid> W96-0213 </papid>to label word sequences as either person, place, company or none of the above based on local cues including the surrounding words and whether honorifics (e.g. mrs. or gen.) or corporate designators (e.g. corp. or inc.) were present.</citsent>
<aftsection>
<nextsent>our algorithm labelling accuracy of 96.0% by token was sufficient for our purposes, but performance is not directly comparable to the muc competitors .
</nextsent>
<nextsent>though we trained from the same data, we preprocessed the data to remove punctuation and capitalization so the model could be applied to broadcast news data that lacked these helpful clues.
</nextsent>
<nextsent>we separately identified television etwork acronyms using simple regular expressions.
</nextsent>
<nextsent>3.2 word bigram frequency.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1936">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> our algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>we designed two algorithms for topic segmentation.
</prevsent>
<prevsent>the first is based solely on word frequency and the second combines the results of the first with other sources of evidence.
</prevsent>
</prevsection>
<citsent citstr=" C92-3145 ">
both of these algorithms are applied to text following some preprocessing including tokenization, conversion to lowercase and the application of lemmatizer (karp et al. , 1992).<papid> C92-3145 </papid></citsent>
<aftsection>
<nextsent>4.1 word frequency algorithm.
</nextsent>
<nextsent>our word frequency algorithm uses katz g model (katz, 1996).
</nextsent>
<nextsent>the model stipulates that words occur in documents either topically or non- topically.
</nextsent>
<nextsent>the model defines topical words as those that occur more than 1 time, while non- topical words occur only once.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1941">
<title id=" P99-1046.xml">statistical models for topic segmentation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>our second technique, statistical model that combined numerous clues about segmentation, performs better than the first, but requires egmented training data.
</prevsent>
<prevsent>we showed an improvement on simple ir task to demonstrate he potential of topic segmentation algorithms for improving ir.
</prevsent>
</prevsection>
<citsent citstr=" A97-1026 ">
other potential uses of these algorithms include better language modeling by building topic-based language models, improving nlp algorithms (e.g. coreference resolution), summarization, hypertext linking (salton and buckley, 1992), automated essay grading (burstein et al, 1997) <papid> A97-1026 </papid>and topic detection and tracking (tdt program committee, 1998).</citsent>
<aftsection>
<nextsent>some of these are discussed in (reynar, 1998), and others will be addressed in future work.
</nextsent>
<nextsent>acknowledgements my thanks to the anonymous reviewers and the members of my thesis committee, mitch marcus, aravind joshi, mark liberman, julia hirschberg and lyle ungar for useful feedback.
</nextsent>
<nextsent>thanks also to dan melamed for use of his smoothing tools and to adwait ratnaparkhi for use of his maximum entropy modelling software.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1942">
<title id=" S10-1059.xml">semafor frame argument resolution with loglinear models </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>extending the semafor 1.0 frame semantic parser (das et al, 2010a; outlined in 3), we detect null instantiations via simple two-stage pipeline: the first stage predicts whether given role is null-instantiated, and the second stage (4) predicts how it is null-instantiated, if it is not overt.
</prevsent>
<prevsent>we report performance on the semeval 2010 test set under the full-srl and ni-only conditions.
</prevsent>
</prevsection>
<citsent citstr=" W07-2018 ">
the semeval 2007 task on frame-semantic parsing (baker et al, 2007) <papid> W07-2018 </papid>provided small (about 50,000 words and 2,000 sentences) dataset of news text, travel guides, and bureaucratic accounts of weapons stockpiles.</citsent>
<aftsection>
<nextsent>sentences in this dataset were fully annotated with frames and their arguments.
</nextsent>
<nextsent>the semeval 2010 task (ruppenhofer etal., 2010) adds annotated data in the fiction do main: parts of two sherlock holmes stories by arthur conan doyle.
</nextsent>
<nextsent>the semeval 2010 training set consists of the semeval 2007 data plusone document from the new domain.
</nextsent>
<nextsent>this document has about 7800 words in 438 sentences; it has 1492 annotated frame instances, including3169 (overt and null-instantiated) argument annotations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1943">
<title id=" S12-1051.xml">semeval2012 task 6 a pilot on semantic textual similarity </title>
<section> source datasets.  </section>
<citcontext>
<prevsection>
<prevsent>we may, however, explore using te pairs for sts in the future.microsoft research (msr) has pioneered the acquisition of paraphrases with two manually annotated datasets.
</prevsent>
<prevsent>the first, called msr paraphrase (msrpar for short) has been widely used to evaluate text similarity algorithms.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
it contains 5801 pairs of sentences gleaned over period of 18 months from thousands of news sources on the web (dolan etal., 2004).<papid> C04-1051 </papid></citsent>
<aftsection>
<nextsent>67% of the pairs were tagged as paraphrases.
</nextsent>
<nextsent>the inter annotator agreement is between 82% and 84%.
</nextsent>
<nextsent>complete meaning equivalence is not required, and the annotation guidelines allowed for some relaxation.
</nextsent>
<nextsent>the pairs which were annotated as not being paraphrases ranged from completely unrelated semantically, to partially overlapping, to those that were almost-but-not-quite semantically equivalent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1944">
<title id=" S12-1051.xml">semeval2012 task 6 a pilot on semantic textual similarity </title>
<section> source datasets.  </section>
<citcontext>
<prevsection>
<prevsent>we sampled 1500 pairs overall, which we split 50% for training and 50% for testing.
</prevsent>
<prevsent>the second dataset from msr is the msr video paraphrase corpus (msrvid for short).
</prevsent>
</prevsection>
<citsent citstr=" P11-1020 ">
the authors showed brief video segments to annotators from amazon mechanical turk (amt) and were asked 1http://search.cpan.org/mlehmann/ string-similarity-1.04/similarity.pm 386 figure 1: video and corresponding descriptions from msrvid figure 2: definition and instructions for annotation to provide one-sentence description of the main action or event in the video (chen and dolan, 2011).<papid> P11-1020 </papid></citsent>
<aftsection>
<nextsent>nearly 120 thousand sentences were collected for 2000 videos.
</nextsent>
<nextsent>the sentences can be taken to be roughly parallel descriptions, and they included sentences for many languages.
</nextsent>
<nextsent>figure 1 shows video and corresponding descriptions.the sampling procedure from this dataset is similar to that for msrpar.
</nextsent>
<nextsent>we construct two bags of data to draw samples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1945">
<title id=" S12-1051.xml">semeval2012 task 6 a pilot on semantic textual similarity </title>
<section> source datasets.  </section>
<citcontext>
<prevsection>
<prevsent>the test data is comprised of all europarl human evaluated fr-en pairs from wmt 2008 that contain 16 white space delimited tokens or less.
</prevsent>
<prevsent>in addition, we selected two other datasets that were used as out-of-domain testing.
</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
one of them comprised of all the human ranked fr-en system submissions from the wmt 2007 news conversation test set, resulting in 351 unique system reference pairs.2 the second set is radically different as it comprised 750 pairs of glosses from ontonotes 4.0 (hovy et al, 2006) <papid> N06-2015 </papid>and wordnet 3.1 (fellbaum,1998) senses.</citsent>
<aftsection>
<nextsent>the mapping of the senses of both resources comprised 110k sense pairs.
</nextsent>
<nextsent>the similarity between the sense pairs was generated using simple word overlap.
</nextsent>
<nextsent>50% of the pairs were sampled from senses which were deemed as equivalent senses, the rest from senses which did not map to one another.
</nextsent>
<nextsent>in this first dataset we defined straightforward likert scale ranging from 5 to 0, but we decided to provide definitions for each value in the scale (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1946">
<title id=" S12-1029.xml">an exact dual decomposition algorithm for shallow semantic parsing with constraints </title>
<section> collective argument identification.  </section>
<citcontext>
<prevsection>
<prevsent>4 we noticed in the annotated data, in some cases, the requires?.
</prevsent>
<prevsent>constraint is violated by the framenet annotators.
</prevsent>
</prevsection>
<citsent citstr=" P10-1160 ">
this happens mostly when one of the required roles is absent in the sentence containing the predicate, but is rather instantiated in an earlier sentence; see gerber and chai (2010).<papid> P10-1160 </papid></citsent>
<aftsection>
<nextsent>we apply the hard constraint in eq.
</nextsent>
<nextsent>10, though extending our algorithm to seek arguments outside the sentence is straightforward (chen et al , 2010).<papid> S10-1059 </papid></nextsent>
<nextsent>211 2.2 linguistic constraints from framenet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1947">
<title id=" S12-1029.xml">an exact dual decomposition algorithm for shallow semantic parsing with constraints </title>
<section> collective argument identification.  </section>
<citcontext>
<prevsection>
<prevsent>this happens mostly when one of the required roles is absent in the sentence containing the predicate, but is rather instantiated in an earlier sentence; see gerber and chai (2010).<papid> P10-1160 </papid></prevsent>
<prevsent>we apply the hard constraint in eq.</prevsent>
</prevsection>
<citsent citstr=" S10-1059 ">
10, though extending our algorithm to seek arguments outside the sentence is straightforward (chen et al , 2010).<papid> S10-1059 </papid></citsent>
<aftsection>
<nextsent>211 2.2 linguistic constraints from framenet.
</nextsent>
<nextsent>although enforcing the four different sets of constraints above is intuitive from general linguistic perspective, we ground their use in definitive linguistic information present in the framenet lexicon (fillmore et al , 2003).
</nextsent>
<nextsent>framenet, along with lists of semantic frames, associated semantic roles, and predicates that could evoke the frames, gives us asmall number of annotated sentences with frame semantic analysis.
</nextsent>
<nextsent>from the annotated data, we gathered that only 3.6% of the time is role inst anti ated multiple times by different spans in sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1948">
<title id=" S12-1029.xml">an exact dual decomposition algorithm for shallow semantic parsing with constraints </title>
<section> collective argument identification.  </section>
<citcontext>
<prevsection>
<prevsent>this justifies the uniqueness constraint enforced by eq.
</prevsent>
<prevsent>3.
</prevsent>
</prevsection>
<citsent citstr=" W07-2048 ">
use of such constraint is also consistent with prior work in frame-semantic parsing (johans son and nugues, 2007; <papid> W07-2048 </papid>das et al , 2010<papid> N10-1138 </papid>a).</citsent>
<aftsection>
<nextsent>similarly,we found that in the annotations, no arguments overlapped with each other forgiven predicate.
</nextsent>
<nextsent>hence, the overlap constraints in eq.
</nextsent>
<nextsent>5 are also justified.
</nextsent>
<nextsent>our third and fourth sets of constraints, presentedin eqs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1949">
<title id=" S12-1029.xml">an exact dual decomposition algorithm for shallow semantic parsing with constraints </title>
<section> collective argument identification.  </section>
<citcontext>
<prevsection>
<prevsent>this justifies the uniqueness constraint enforced by eq.
</prevsent>
<prevsent>3.
</prevsent>
</prevsection>
<citsent citstr=" N10-1138 ">
use of such constraint is also consistent with prior work in frame-semantic parsing (johans son and nugues, 2007; <papid> W07-2048 </papid>das et al , 2010<papid> N10-1138 </papid>a).</citsent>
<aftsection>
<nextsent>similarly,we found that in the annotations, no arguments overlapped with each other forgiven predicate.
</nextsent>
<nextsent>hence, the overlap constraints in eq.
</nextsent>
<nextsent>5 are also justified.
</nextsent>
<nextsent>our third and fourth sets of constraints, presentedin eqs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1952">
<title id=" S12-1029.xml">an exact dual decomposition algorithm for shallow semantic parsing with constraints </title>
<section> collective argument identification.  </section>
<citcontext>
<prevsection>
<prevsent>4 experiments and results.
</prevsent>
<prevsent>4.1 dataset, preprocessing, and learning.
</prevsent>
</prevsection>
<citsent citstr=" W07-2018 ">
in our experiments, we use framenet 1.5, which contains lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicate argument annotations (a superset of the semeval shared task dataset; baker et al , 2007).<papid> W07-2018 </papid></citsent>
<aftsection>
<nextsent>we used thesame split asdas and smith (2011), <papid> P11-1144 </papid>with 55 documents for training (containing 19,582 frame anno tations) and 23 for testing (with 4,458 annotations).we randomly selected 4,462 predicates in the training set as development data.</nextsent>
<nextsent>the raw sentences in all the training and test documents were preprocessed using mxpost (ratnaparkhi, 1996) and the mst dependency parser (mcdonald et al , 2005).the state-of-the-art system for this task is se mafor, an open source tool (das et al , 2010<papid> N10-1138 </papid>a)5that provides baseline benchmark for our new al gorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1953">
<title id=" S12-1029.xml">an exact dual decomposition algorithm for shallow semantic parsing with constraints </title>
<section> collective argument identification.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 dataset, preprocessing, and learning.
</prevsent>
<prevsent>in our experiments, we use framenet 1.5, which contains lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicate argument annotations (a superset of the semeval shared task dataset; baker et al , 2007).<papid> W07-2018 </papid></prevsent>
</prevsection>
<citsent citstr=" P11-1144 ">
we used thesame split asdas and smith (2011), <papid> P11-1144 </papid>with 55 documents for training (containing 19,582 frame anno tations) and 23 for testing (with 4,458 annotations).we randomly selected 4,462 predicates in the training set as development data.</citsent>
<aftsection>
<nextsent>the raw sentences in all the training and test documents were preprocessed using mxpost (ratnaparkhi, 1996) and the mst dependency parser (mcdonald et al , 2005).the state-of-the-art system for this task is se mafor, an open source tool (das et al , 2010<papid> N10-1138 </papid>a)5that provides baseline benchmark for our new al gorithm.</nextsent>
<nextsent>we use the components of semafor as-is to define the features and train the weights ? used in the scoring function c. we also use its 5http://www.ark.cs.cmu.edu/semafor 214 heuristic mechanism to find potential spans st forgiven predicate t. semafor learns weights using `2-penalized log-likelihood; we augmented its dev set-tuning procedure to tune both the regularization strength and the ad3 penalty strength ?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1960">
<title id=" S12-1029.xml">an exact dual decomposition algorithm for shallow semantic parsing with constraints </title>
<section> collective argument identification.  </section>
<citcontext>
<prevsection>
<prevsent>5 related work.
</prevsent>
<prevsent>semantic role labeling: most srl systems use conventions from propbank (kingsbury and palmer, 2002) and nombank (meyers et al , 2004), which store information about verbal and nominal predicates and corresponding symbolic and meaning specific semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
a separate line of work, including this paper, investigates srl systems that use framenet conventions; while less popular, these systems, pioneered by gildea and jurafsky (2002), <papid> J02-3001 </papid>consider predicates of wider variety of syntactic categories, use semantic frame abstractions, and employ explicit role labels.</citsent>
<aftsection>
<nextsent>a common trait in prior work has been the use of two-stage model that identifies arguments first, then labels them.
</nextsent>
<nextsent>they are treated jointly here, unlike what has typically been done in propbank-style srl (ma`rquez et al , 2008).
</nextsent>
<nextsent>dual decomposition: rush et al  (2010) proposed subgradient-based dual decomposition as way of combining models which are tractable individually,but not jointly, by solving relaxation of the original problem.
</nextsent>
<nextsent>this was followed by work adopting this method for syntax and translation (koo et al , 2010; <papid> D10-1125 </papid>auli and lopez, 2011; <papid> P11-1048 </papid>denero and macherey, 2011; <papid> P11-1043 </papid>rush and collins, 2011; chang and collins, 2011).<papid> D11-1003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1961">
<title id=" S12-1029.xml">an exact dual decomposition algorithm for shallow semantic parsing with constraints </title>
<section> collective argument identification.  </section>
<citcontext>
<prevsection>
<prevsent>they are treated jointly here, unlike what has typically been done in propbank-style srl (ma`rquez et al , 2008).
</prevsent>
<prevsent>dual decomposition: rush et al  (2010) proposed subgradient-based dual decomposition as way of combining models which are tractable individually,but not jointly, by solving relaxation of the original problem.
</prevsent>
</prevsection>
<citsent citstr=" D10-1125 ">
this was followed by work adopting this method for syntax and translation (koo et al , 2010; <papid> D10-1125 </papid>auli and lopez, 2011; <papid> P11-1048 </papid>denero and macherey, 2011; <papid> P11-1043 </papid>rush and collins, 2011; chang and collins, 2011).<papid> D11-1003 </papid></citsent>
<aftsection>
<nextsent>recently, martins et al  (2011b) showed thatthe success of subgradient-based dual decomposition strongly relies on breaking down the original problem into good?
</nextsent>
<nextsent>decomposition, i.e., one with few overlapping components.
</nextsent>
<nextsent>this leaves out many declarative constrained problems, for which such good decomposition is not readily available.
</nextsent>
<nextsent>for those, martins et al  (2011b) proposed the ad3 algorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1962">
<title id=" S12-1029.xml">an exact dual decomposition algorithm for shallow semantic parsing with constraints </title>
<section> collective argument identification.  </section>
<citcontext>
<prevsection>
<prevsent>they are treated jointly here, unlike what has typically been done in propbank-style srl (ma`rquez et al , 2008).
</prevsent>
<prevsent>dual decomposition: rush et al  (2010) proposed subgradient-based dual decomposition as way of combining models which are tractable individually,but not jointly, by solving relaxation of the original problem.
</prevsent>
</prevsection>
<citsent citstr=" P11-1048 ">
this was followed by work adopting this method for syntax and translation (koo et al , 2010; <papid> D10-1125 </papid>auli and lopez, 2011; <papid> P11-1048 </papid>denero and macherey, 2011; <papid> P11-1043 </papid>rush and collins, 2011; chang and collins, 2011).<papid> D11-1003 </papid></citsent>
<aftsection>
<nextsent>recently, martins et al  (2011b) showed thatthe success of subgradient-based dual decomposition strongly relies on breaking down the original problem into good?
</nextsent>
<nextsent>decomposition, i.e., one with few overlapping components.
</nextsent>
<nextsent>this leaves out many declarative constrained problems, for which such good decomposition is not readily available.
</nextsent>
<nextsent>for those, martins et al  (2011b) proposed the ad3 algorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1963">
<title id=" S12-1029.xml">an exact dual decomposition algorithm for shallow semantic parsing with constraints </title>
<section> collective argument identification.  </section>
<citcontext>
<prevsection>
<prevsent>they are treated jointly here, unlike what has typically been done in propbank-style srl (ma`rquez et al , 2008).
</prevsent>
<prevsent>dual decomposition: rush et al  (2010) proposed subgradient-based dual decomposition as way of combining models which are tractable individually,but not jointly, by solving relaxation of the original problem.
</prevsent>
</prevsection>
<citsent citstr=" P11-1043 ">
this was followed by work adopting this method for syntax and translation (koo et al , 2010; <papid> D10-1125 </papid>auli and lopez, 2011; <papid> P11-1048 </papid>denero and macherey, 2011; <papid> P11-1043 </papid>rush and collins, 2011; chang and collins, 2011).<papid> D11-1003 </papid></citsent>
<aftsection>
<nextsent>recently, martins et al  (2011b) showed thatthe success of subgradient-based dual decomposition strongly relies on breaking down the original problem into good?
</nextsent>
<nextsent>decomposition, i.e., one with few overlapping components.
</nextsent>
<nextsent>this leaves out many declarative constrained problems, for which such good decomposition is not readily available.
</nextsent>
<nextsent>for those, martins et al  (2011b) proposed the ad3 algorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1964">
<title id=" S12-1029.xml">an exact dual decomposition algorithm for shallow semantic parsing with constraints </title>
<section> collective argument identification.  </section>
<citcontext>
<prevsection>
<prevsent>they are treated jointly here, unlike what has typically been done in propbank-style srl (ma`rquez et al , 2008).
</prevsent>
<prevsent>dual decomposition: rush et al  (2010) proposed subgradient-based dual decomposition as way of combining models which are tractable individually,but not jointly, by solving relaxation of the original problem.
</prevsent>
</prevsection>
<citsent citstr=" D11-1003 ">
this was followed by work adopting this method for syntax and translation (koo et al , 2010; <papid> D10-1125 </papid>auli and lopez, 2011; <papid> P11-1048 </papid>denero and macherey, 2011; <papid> P11-1043 </papid>rush and collins, 2011; chang and collins, 2011).<papid> D11-1003 </papid></citsent>
<aftsection>
<nextsent>recently, martins et al  (2011b) showed thatthe success of subgradient-based dual decomposition strongly relies on breaking down the original problem into good?
</nextsent>
<nextsent>decomposition, i.e., one with few overlapping components.
</nextsent>
<nextsent>this leaves out many declarative constrained problems, for which such good decomposition is not readily available.
</nextsent>
<nextsent>for those, martins et al  (2011b) proposed the ad3 algorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1965">
<title id=" S12-1017.xml">combining resources for mwe token classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we present novel results illuminating the effectiveness of contextual semantic vectors at mwe-token classification.
</prevsent>
<prevsent>the openmwe corpus (hashimoto and kawahara, 2009) is gold-standard corpus of over 100, 000 japanese mwe-tokens covering 146 types.
</prevsent>
</prevsection>
<citsent citstr=" C10-2078 ">
it is the largest resource we are aware of which has hand annotated instances of mwes which are ambiguous between literal and idiomatic interpretation, and has been used by hashimoto and kawahara (2009) and fothergill and baldwin (2011) for supervised classification of mwe-tokens using features capturing lexico-syntactic variation and traditional semantic features borrowed from word sense disambiguation (wsd) . similar work in other languages has been performed by li and sporleder (2010) <papid> C10-2078 </papid>and diab and bhutada (2009).<papid> W09-2903 </papid></citsent>
<aftsection>
<nextsent>we build on this work in exploring the use of mwe-type-level features drawn from an idiom dictionary for mwe identification.
</nextsent>
<nextsent>100hashimoto and kawahara (2009) developed variety of features capturing lexico-syntactic variation but only one ? boolean feature for internal mod ification?, which fired only when non-constituent word appeared between constituent words in anmwe-token ? had an appreciable impact on classification.
</nextsent>
<nextsent>however, they found that this effect was far overshadowed by semantic context features inspired by wsd.
</nextsent>
<nextsent>that is, treating each mwe-type as word with two senses and performing sense disambiguation was far more successful than any features basedon lexico-syntactic characteristics of idioms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1966">
<title id=" S12-1017.xml">combining resources for mwe token classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we present novel results illuminating the effectiveness of contextual semantic vectors at mwe-token classification.
</prevsent>
<prevsent>the openmwe corpus (hashimoto and kawahara, 2009) is gold-standard corpus of over 100, 000 japanese mwe-tokens covering 146 types.
</prevsent>
</prevsection>
<citsent citstr=" W09-2903 ">
it is the largest resource we are aware of which has hand annotated instances of mwes which are ambiguous between literal and idiomatic interpretation, and has been used by hashimoto and kawahara (2009) and fothergill and baldwin (2011) for supervised classification of mwe-tokens using features capturing lexico-syntactic variation and traditional semantic features borrowed from word sense disambiguation (wsd) . similar work in other languages has been performed by li and sporleder (2010) <papid> C10-2078 </papid>and diab and bhutada (2009).<papid> W09-2903 </papid></citsent>
<aftsection>
<nextsent>we build on this work in exploring the use of mwe-type-level features drawn from an idiom dictionary for mwe identification.
</nextsent>
<nextsent>100hashimoto and kawahara (2009) developed variety of features capturing lexico-syntactic variation but only one ? boolean feature for internal mod ification?, which fired only when non-constituent word appeared between constituent words in anmwe-token ? had an appreciable impact on classification.
</nextsent>
<nextsent>however, they found that this effect was far overshadowed by semantic context features inspired by wsd.
</nextsent>
<nextsent>that is, treating each mwe-type as word with two senses and performing sense disambiguation was far more successful than any features basedon lexico-syntactic characteristics of idioms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1967">
<title id=" S12-1017.xml">combining resources for mwe token classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>that is, treating each mwe-type as word with two senses and performing sense disambiguation was far more successful than any features basedon lexico-syntactic characteristics of idioms.
</prevsent>
<prevsent>intuitively, we would expect that if we had access to arich inventory of expression-specific type-level features encoding the ability of the expression to participate in different syntactic alternations, we should be better equipped to disambiguate token occurrences of that expression.
</prevsent>
</prevsection>
<citsent citstr=" J09-1005 ">
indeed, the work of fazly et al (2009) <papid> J09-1005 </papid>would appear to support this hypothesis, in that the authors used unsupervised methods to learn type-level preferences for range of mwe types, and demonstrated that these could be successfully applied to token-level disambiguation task.hashimoto and kawahara (2009) trained individual classifiers for each mwe-type in their corpus and tested them only on instances of the type they were trained on.</citsent>
<aftsection>
<nextsent>in contrast to this type specialised classification, fothergill and baldwin (2011) trained classifiers on subset of mwe-types and tested on instances of the remaining held-out mwe-types.
</nextsent>
<nextsent>the motivation for this cross type classification was to test the use of data from the openmwe corpus for mwe-token classification of mwe-types with no gold-standard data available(which are by far the majority).
</nextsent>
<nextsent>fothergill and baldwin (2011) introduced features for cross type classification which captured features of the mwe-type,reasoning that similar expressions would have similar propensity for idiomaticity.
</nextsent>
<nextsent>we introduce new mwe-type features expressing the modifiability of constituents based on information extracted from an mwe dictionary with wide coverage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1968">
<title id=" S12-1017.xml">combining resources for mwe token classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, this turned out not to be the case, with by far the most successful results arising again from use ofwsd features.
</prevsent>
<prevsent>this surprising result raises the possibility of distributional similarity between the contexts of idiomatic mwe-tokens of different mwe types, however the result was not explained or explored further.
</prevsent>
</prevsection>
<citsent citstr=" P11-1017 ">
in this paper we offer new insights into the distributional similarity hypothesis.the recently-published jdmwe (japanese dictionary of multiword expressions) encodes type level information on thousands of japanese mwes (shudo et al, 2011).<papid> P11-1017 </papid></citsent>
<aftsection>
<nextsent>a subset of the dictionary has been released, and overlaps to some extent with themwe-types in the openmwe corpus.
</nextsent>
<nextsent>jdmwe encodes information about lexico-syntactic variations allowed by each mwe-type it contains.
</nextsent>
<nextsent>for example, the expression hana wo motaseru ? literally to have [someone] hold flowers?
</nextsent>
<nextsent>but figuratively to let [someone] take the credit?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1969">
<title id=" S10-1092.xml">racai unsupervised wsd experiments  semeval2 task 17 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the best of them has been ranked the 12th by the task organizers out of 29 judged runs.
</prevsent>
<prevsent>referring to the last semeval (semeval-1, (agirre et al, 2007a)) and to our recent work (ion and tefnescu, 2009), unsupervised word sense disambiguation (wsd) is still at the bottom of wsd systems ranking with significant loss in performance when compared to supervised approaches.
</prevsent>
</prevsection>
<citsent citstr=" W09-2420 ">
with task #17 @ semeval-2, this observation is (probably 1 ) reinforced but another issue is re-brought to light: the difficulty of supervised wsd systems to adapt to given domain (agirre et al, 2009).<papid> W09-2420 </papid></citsent>
<aftsection>
<nextsent>with general scores lower with at least 3% than 3 years ago in task #17 @ semeval-1 which was supposedly harder task (general, no particular domain wsd was required for all words), we observe that supervised wsd is certainly more difficult to implement in real world application.
</nextsent>
<nextsent>our unsupervised wsd approach benefited from the specification of this years task #17 which was domain-limited wsd, meaning that the disambiguation would be applied to content words drawn from specific domain: the surrounding environment.
</nextsent>
<nextsent>we worked under the assumption that term of the given domain 1 at the time of the writing we only know the systems rank-.
</nextsent>
<nextsent>ing without the supervised/unsupervised distinction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1970">
<title id=" S10-1092.xml">racai unsupervised wsd experiments  semeval2 task 17 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ing without the supervised/unsupervised distinction.
</prevsent>
<prevsent>would have the same meaning with all its occurrences throughout the text.
</prevsent>
</prevsection>
<citsent citstr=" H93-1052 ">
this hypothesis has been put forth by yarowsky (1993) <papid> H93-1052 </papid>as the one sense per discourse?</citsent>
<aftsection>
<nextsent>hypothesis (ospd for short).
</nextsent>
<nextsent>the task organizers offered set of back ground documents with no sense annotations to the competitors who want to train/tune their systems using data from the same domain as the official test set.
</nextsent>
<nextsent>working with the ospd hypothesis, we set off to construct/test domain specific wsd models from/on this corpus using the wordnet domains (bentivogli et al, 2004).<papid> W04-2214 </papid></nextsent>
<nextsent>for testing purposes, we have constructed an inhouse gold standard from this corpus that comprises of 1601 occurrences of 204 terms of the surrounding environment?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1971">
<title id=" S10-1092.xml">racai unsupervised wsd experiments  semeval2 task 17 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hypothesis (ospd for short).
</prevsent>
<prevsent>the task organizers offered set of back ground documents with no sense annotations to the competitors who want to train/tune their systems using data from the same domain as the official test set.
</prevsent>
</prevsection>
<citsent citstr=" W04-2214 ">
working with the ospd hypothesis, we set off to construct/test domain specific wsd models from/on this corpus using the wordnet domains (bentivogli et al, 2004).<papid> W04-2214 </papid></citsent>
<aftsection>
<nextsent>for testing purposes, we have constructed an inhouse gold standard from this corpus that comprises of 1601 occurrences of 204 terms of the surrounding environment?
</nextsent>
<nextsent>domain that have been automatically extracted with the highest confidence.
</nextsent>
<nextsent>we have observed that our gold standard (which has been independently annotated by 3 annotators but on non-overlapping sections which led to having no inter-annotator agreement scores) obeys the ospd hypothesis which we think that is appropriate to domain limited wsd.
</nextsent>
<nextsent>in what follows, we will briefly acknowledge the usage of wordnet domains in wsd, we will then describe the construction of the corpus of the background documents including here the creation of an in-house gold standard, we will then briefly describe our three wsd algorithms and finally we will conclude with discussion on the ranking of our runs among the 29 evaluated by the task organizers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1975">
<title id=" S10-1092.xml">racai unsupervised wsd experiments  semeval2 task 17 </title>
<section> the description of the systems.  </section>
<citcontext>
<prevsection>
<prevsent>the lexical chain procedure is function of two wn synsets, lxc(s1, s2), that returns semantic relation path that one can follow to reach s2 from s1.
</prevsent>
<prevsent>on the path from s2 to s1 there are synsets (k ? 0) and between 2 adjacent syn sets there is wn semantic relation.
</prevsent>
</prevsection>
<citsent citstr=" C02-1167 ">
each lexical chain can be assigned certain score that we interpret as measure of the semantic similarity (ss) between s1 and s2 (see (ion and tefnescu, 2009) and (moldovan and novischi, 2002) <papid> C02-1167 </papid>for more details).</citsent>
<aftsection>
<nextsent>thus, the higher the value of ss(s1, s2), the higher the semantic similarity between s1 and s2.
</nextsent>
<nextsent>we have observed that using racai-1 on our in-house test set but allowing it to select the most frequent 2 senses of lemma with pos from the wsd model, we obtain whopping 82% accuracy.
</nextsent>
<nextsent>with this observation, we tried to program racai-2 to make binary selection from the first 2 most frequent senses of lemma with pos from the wsd model in order to approach the 82% percent accuracy limit which would have been very good result.
</nextsent>
<nextsent>the algorithm is as follows: for lemma with pos and lemma lc with pos pc from the context (sentence) of l, compute the best lexical chain between any of the first 2 senses of and any of the first 2 senses of lc according to the wsd model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1976">
<title id=" S12-1108.xml">ict a translation based method for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results show that our method achieves promising results but not perfect results compared to other participants.
</prevsent>
<prevsent>in cross-lingual textual entailment task (clte) of 2012, the organizers hold task for cross lingual textual entailment.
</prevsent>
</prevsection>
<citsent citstr=" S12-1053 ">
the cross-lingual textual entailment task addresses textual entailment (te) recognition under new dimension (cross-linguality), and within new challenging application scenario (content synchronization) readers can refer to m. negri et al 2012.<papid> S12-1053 </papid>s., for more detailed introduction.</citsent>
<aftsection>
<nextsent>1 textual entailment, on the other hand, recognize, generate, or extract pairs of natural language expressions, and infer that if one element is true, whether the other element is also true.
</nextsent>
<nextsent>several methods are proposed by previous researchers.
</nextsent>
<nextsent>there have been some workshops on textual entailment in recent years.
</nextsent>
<nextsent>the recognizing textual entailment challenges (bar-haim et al 2006; giampiccolo, magnini, dagan, &amp; dolan, 2007; giampiccolo, dang, magnini, dagan, &amp; dolan, 2008), currently in the 7th year, provide additional significant thrust.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1977">
<title id=" S12-1108.xml">ict a translation based method for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>different methods may operate at different levels of representation of the input expressions.
</prevsent>
<prevsent>for example, they may treat the input expressions simply as surface strings, they may operate on syntactic or semantic representations of the input expressions, or on representations combining information from different 1http://edits.fbk.eu/ 715levels.
</prevsent>
</prevsection>
<citsent citstr=" W03-1604 ">
logic-based approach is to map the language expressions to logical meaning representations, and then relyon logical entailment checks, possibly by invoking theorem provers (rinaldi et al., 2003; <papid> W03-1604 </papid>bos &amp; markert, 2005; <papid> H05-1079 </papid>tatu &amp; moldovan, 2005, <papid> H05-1047 </papid>2007).</citsent>
<aftsection>
<nextsent>an alternative to use logical meaning representations is to start by mapping each word of the input language expressions to vector that shows how strongly the word co-occurs with particular other words in corpora (lin, 1998b), possibly also taking into account syntactic information, for example requiring that the co-occurring words participate in particular syntactic dependencies (pado &amp; lapata, 2007).
</nextsent>
<nextsent>several textual entailment recognizing methods operate directly on the input surface strings.
</nextsent>
<nextsent>for example, they compute the string edit distance (levenshtein, 1966) of the two input strings, the number of their common words, or combinations of several string similarity measures (malakasiotis &amp; androutsopoulos, 2007).<papid> W07-1407 </papid></nextsent>
<nextsent>dependency grammar parsers (melcuk, 1987; ku bler, mcdonald, &amp; nivre, 2009) are popular in textual entailment research.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1978">
<title id=" S12-1108.xml">ict a translation based method for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>different methods may operate at different levels of representation of the input expressions.
</prevsent>
<prevsent>for example, they may treat the input expressions simply as surface strings, they may operate on syntactic or semantic representations of the input expressions, or on representations combining information from different 1http://edits.fbk.eu/ 715levels.
</prevsent>
</prevsection>
<citsent citstr=" H05-1079 ">
logic-based approach is to map the language expressions to logical meaning representations, and then relyon logical entailment checks, possibly by invoking theorem provers (rinaldi et al., 2003; <papid> W03-1604 </papid>bos &amp; markert, 2005; <papid> H05-1079 </papid>tatu &amp; moldovan, 2005, <papid> H05-1047 </papid>2007).</citsent>
<aftsection>
<nextsent>an alternative to use logical meaning representations is to start by mapping each word of the input language expressions to vector that shows how strongly the word co-occurs with particular other words in corpora (lin, 1998b), possibly also taking into account syntactic information, for example requiring that the co-occurring words participate in particular syntactic dependencies (pado &amp; lapata, 2007).
</nextsent>
<nextsent>several textual entailment recognizing methods operate directly on the input surface strings.
</nextsent>
<nextsent>for example, they compute the string edit distance (levenshtein, 1966) of the two input strings, the number of their common words, or combinations of several string similarity measures (malakasiotis &amp; androutsopoulos, 2007).<papid> W07-1407 </papid></nextsent>
<nextsent>dependency grammar parsers (melcuk, 1987; ku bler, mcdonald, &amp; nivre, 2009) are popular in textual entailment research.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1979">
<title id=" S12-1108.xml">ict a translation based method for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>different methods may operate at different levels of representation of the input expressions.
</prevsent>
<prevsent>for example, they may treat the input expressions simply as surface strings, they may operate on syntactic or semantic representations of the input expressions, or on representations combining information from different 1http://edits.fbk.eu/ 715levels.
</prevsent>
</prevsection>
<citsent citstr=" H05-1047 ">
logic-based approach is to map the language expressions to logical meaning representations, and then relyon logical entailment checks, possibly by invoking theorem provers (rinaldi et al., 2003; <papid> W03-1604 </papid>bos &amp; markert, 2005; <papid> H05-1079 </papid>tatu &amp; moldovan, 2005, <papid> H05-1047 </papid>2007).</citsent>
<aftsection>
<nextsent>an alternative to use logical meaning representations is to start by mapping each word of the input language expressions to vector that shows how strongly the word co-occurs with particular other words in corpora (lin, 1998b), possibly also taking into account syntactic information, for example requiring that the co-occurring words participate in particular syntactic dependencies (pado &amp; lapata, 2007).
</nextsent>
<nextsent>several textual entailment recognizing methods operate directly on the input surface strings.
</nextsent>
<nextsent>for example, they compute the string edit distance (levenshtein, 1966) of the two input strings, the number of their common words, or combinations of several string similarity measures (malakasiotis &amp; androutsopoulos, 2007).<papid> W07-1407 </papid></nextsent>
<nextsent>dependency grammar parsers (melcuk, 1987; ku bler, mcdonald, &amp; nivre, 2009) are popular in textual entailment research.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1980">
<title id=" S12-1108.xml">ict a translation based method for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an alternative to use logical meaning representations is to start by mapping each word of the input language expressions to vector that shows how strongly the word co-occurs with particular other words in corpora (lin, 1998b), possibly also taking into account syntactic information, for example requiring that the co-occurring words participate in particular syntactic dependencies (pado &amp; lapata, 2007).
</prevsent>
<prevsent>several textual entailment recognizing methods operate directly on the input surface strings.
</prevsent>
</prevsection>
<citsent citstr=" W07-1407 ">
for example, they compute the string edit distance (levenshtein, 1966) of the two input strings, the number of their common words, or combinations of several string similarity measures (malakasiotis &amp; androutsopoulos, 2007).<papid> W07-1407 </papid></citsent>
<aftsection>
<nextsent>dependency grammar parsers (melcuk, 1987; ku bler, mcdonald, &amp; nivre, 2009) are popular in textual entailment research.
</nextsent>
<nextsent>however, cross-lingual textual entailment brings some problems on past algorithms.
</nextsent>
<nextsent>on the other hand, many methods cant be applied to it directly.
</nextsent>
<nextsent>in this paper, we propose translation based method for cross-lingual textual entailment, which has been described in mehdad et al 2010.<papid> N10-1045 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1981">
<title id=" S12-1108.xml">ict a translation based method for cross lingual textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, cross-lingual textual entailment brings some problems on past algorithms.
</prevsent>
<prevsent>on the other hand, many methods cant be applied to it directly.
</prevsent>
</prevsection>
<citsent citstr=" N10-1045 ">
in this paper, we propose translation based method for cross-lingual textual entailment, which has been described in mehdad et al 2010.<papid> N10-1045 </papid></citsent>
<aftsection>
<nextsent>first, we translate one part of the text, which termed as t1?
</nextsent>
<nextsent>and written in one language, into english, which termed as t2?.
</nextsent>
<nextsent>then, we use edits, an open source package, to recognize entailment relations between two parts.
</nextsent>
<nextsent>large-scale experiments are conducted on four language pairs, french-english, spanish-english, italian-english and german english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1982">
<title id=" S12-1108.xml">ict a translation based method for cross lingual textual entailment </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 machine translation.
</prevsent>
<prevsent>recently, machine translation has attracted intensive attention and has been well studied in natural language community.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
effective models, such as phrase-based model (koehn et al, 2003), <papid> N03-1017 </papid>hierarchical phrase-based model (hpb) (chiang, 2005), <papid> P05-1033 </papid>and syntax-based (liu et al, 2006) <papid> P06-1077 </papid>model have been proposed to improve the translation quality.</citsent>
<aftsection>
<nextsent>however, since current translation models require parallel corpus to extract translation rules, while parallel corpus on some language pairs such as italian-english and spanish-english are hard to obtain, therefore, we could use google translation toolkit (gtt) to generate translation.
</nextsent>
<nextsent>specifically, wmt 2 released some bilingual corpus for training, thus we use some portion to train french-english translation engine using hierarchical phrase-based model.
</nextsent>
<nextsent>we also exploit system combination technique (a rosti et al, 2007) to improve translation quality via blending the translation of our models and gtts. it is worth noting that gtt only gives 1-best translation, thus we duplicate 50 times to generate 50-best for system combination.
</nextsent>
<nextsent>2 http://www.statmt.org/wmt12/ 716 2.2 textual entailment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1983">
<title id=" S12-1108.xml">ict a translation based method for cross lingual textual entailment </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 machine translation.
</prevsent>
<prevsent>recently, machine translation has attracted intensive attention and has been well studied in natural language community.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
effective models, such as phrase-based model (koehn et al, 2003), <papid> N03-1017 </papid>hierarchical phrase-based model (hpb) (chiang, 2005), <papid> P05-1033 </papid>and syntax-based (liu et al, 2006) <papid> P06-1077 </papid>model have been proposed to improve the translation quality.</citsent>
<aftsection>
<nextsent>however, since current translation models require parallel corpus to extract translation rules, while parallel corpus on some language pairs such as italian-english and spanish-english are hard to obtain, therefore, we could use google translation toolkit (gtt) to generate translation.
</nextsent>
<nextsent>specifically, wmt 2 released some bilingual corpus for training, thus we use some portion to train french-english translation engine using hierarchical phrase-based model.
</nextsent>
<nextsent>we also exploit system combination technique (a rosti et al, 2007) to improve translation quality via blending the translation of our models and gtts. it is worth noting that gtt only gives 1-best translation, thus we duplicate 50 times to generate 50-best for system combination.
</nextsent>
<nextsent>2 http://www.statmt.org/wmt12/ 716 2.2 textual entailment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1984">
<title id=" S12-1108.xml">ict a translation based method for cross lingual textual entailment </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 machine translation.
</prevsent>
<prevsent>recently, machine translation has attracted intensive attention and has been well studied in natural language community.
</prevsent>
</prevsection>
<citsent citstr=" P06-1077 ">
effective models, such as phrase-based model (koehn et al, 2003), <papid> N03-1017 </papid>hierarchical phrase-based model (hpb) (chiang, 2005), <papid> P05-1033 </papid>and syntax-based (liu et al, 2006) <papid> P06-1077 </papid>model have been proposed to improve the translation quality.</citsent>
<aftsection>
<nextsent>however, since current translation models require parallel corpus to extract translation rules, while parallel corpus on some language pairs such as italian-english and spanish-english are hard to obtain, therefore, we could use google translation toolkit (gtt) to generate translation.
</nextsent>
<nextsent>specifically, wmt 2 released some bilingual corpus for training, thus we use some portion to train french-english translation engine using hierarchical phrase-based model.
</nextsent>
<nextsent>we also exploit system combination technique (a rosti et al, 2007) to improve translation quality via blending the translation of our models and gtts. it is worth noting that gtt only gives 1-best translation, thus we duplicate 50 times to generate 50-best for system combination.
</nextsent>
<nextsent>2 http://www.statmt.org/wmt12/ 716 2.2 textual entailment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1985">
<title id=" S10-1006.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our goal was to create testbed for automatic classification of semantic relations.
</prevsent>
<prevsent>in developing the task we met several challenges: selecting suitable set of relations, specifying the annotation procedure, and deciding on the details of the task itself.
</prevsent>
</prevsection>
<citsent citstr=" W09-2415 ">
they are discussed briefly in section 2; seealso hendrickx et al (2009), <papid> W09-2415 </papid>which includes survey of related work.</citsent>
<aftsection>
<nextsent>the direct predecessor of task 8 was classification of semantic relations between nominals, task 4 at semeval-1 (girju et al, 2009), ? university of lisbon, iris@clul.ul.pt ? university of melbourne, snkim@csse.unimelb.edu.au ? information sciences institute/university of southern california, kozareva@isi.edu ? national university of singapore, nakov@comp.nus.edu.sg ? university of cambridge, do242@cl.cam.ac.uk ? university of stuttgart, pado@ims.uni-stuttgart.de ??
</nextsent>
<nextsent>yahoo!
</nextsent>
<nextsent>inc., pennacc@yahoo-inc.com ??
</nextsent>
<nextsent>fondazione bruno kessler, romano@fbk.eu ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1986">
<title id=" S10-1006.xml">semeval2010 task 8 multi way classification of semantic relations between pairs of nominals </title>
<section> dataset creation.  </section>
<citcontext>
<prevsection>
<prevsent>1our objective is to annotate instances of semantic relations which are true in the sense of holding in the most plausible truth-conditional interpretation of the sentence.
</prevsent>
<prevsent>this is in the tradition of the textual entailment or information validation paradigm (dagan et al, 2009), and in contrast to aboutness?
</prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
annotation such as semantic roles (carreras and m`arquez, 2004) or the bionlp2009 task (kim et al, 2009) <papid> W09-1401 </papid>where negated relations are also labelled as positive.</citsent>
<aftsection>
<nextsent>similarly, we exclude instances of semantic relations which hold only in speculative or counterfactural scenarios.
</nextsent>
<nextsent>in practice, this means disallowing annotations within the scope of modals or neg ations, e.g., smoking may/may not have caused cancer in this case.?
</nextsent>
<nextsent>we accept as relation arguments only noun phrases with common-noun heads.
</nextsent>
<nextsent>this distinguishes our task from much work in information extraction, which tends to focus on specific classes of named entities and on considerably more fine grained relations than we do.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1987">
<title id=" P98-2218.xml">project for production of closed caption tv programs for the hearing impaired </title>
<section> preliminary research results.  </section>
<citcontext>
<prevsection>
<prevsent>to compute importance measures for each sentence, we need to find first keywords of the text.
</prevsent>
<prevsent>we tested high-frequency key word method (luhn 1957, edumundson 1969) and tf-idf-based (text frequency, inverse document frequency) method.
</prevsent>
</prevsection>
<citsent citstr=" W97-0509 ">
we evaluated the two methods using ten thousand tv news texts, and found that high-frequency key word method showed slightly better results than the method based on tf-idf scores (wakao et al 1997).<papid> W97-0509 </papid></citsent>
<aftsection>
<nextsent>3.1.2 rules fo shortening text another way of reducing the number of characters in japanese text, thus summarising the text, is to shorten or delete parts of the sentences.
</nextsent>
<nextsent>for example, if sentence nds with sahen verb followed by its inflection, or helping verbs or particles to express proper politeness, it does not change the meaning much even if we keep only the verb stem (or sahen noun) and delete the rest of it.
</nextsent>
<nextsent>this is one of the ways found in the captions to shorten or delete unimportant parts of the sentences.
</nextsent>
<nextsent>we analysed texts and captions in tv news program which is broadcast fully captioned for the hearing impaired in japan.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1988">
<title id=" S10-1022.xml">tanl1 coreference resolution by parse analysis and similarity clustering </title>
<section> overview </section>
<citcontext>
<prevsection>
<prevsent>a maximum entropy classifier is trained to predict how likely two mentions refer to the same entity.
</prevsent>
<prevsent>this is followed by greedy procedure whose purpose is to cluster mentions into entities.
</prevsent>
</prevsection>
<citsent citstr=" P05-1020 ">
according to ng (2005), <papid> P05-1020 </papid>most learning based coreference systems can be defined by four ele ments: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set used to represent training or test instance, and the clustering algorithm used to coordinate the coreference classification decisions.</citsent>
<aftsection>
<nextsent>in the following we will detail our approach by making explicit the strategies used in each of above mentioned components.
</nextsent>
<nextsent>the data model used by our system is based on the concepts of entity and mention.
</nextsent>
<nextsent>the collection of mentions referring to the same object in document forms an entity.
</nextsent>
<nextsent>a mention is an instance referring to an object: it is represented by the start and end positions in sentence, type and sequence number.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1990">
<title id=" S10-1022.xml">tanl1 coreference resolution by parse analysis and similarity clustering </title>
<section> determining coreference.  </section>
<citcontext>
<prevsection>
<prevsent>this strategy has been described as best-first clustering by ng (2005).<papid> P05-1020 </papid></prevsent>
<prevsent>in principle the process is not optimal since, once mention is assigned to an entity, it cannot be later assigned to another entity to which it more likely refers.</prevsent>
</prevsection>
<citsent citstr=" P04-1018 ">
luo et al (2004) <papid> P04-1018 </papid>propose an approach based on the bell tree to address this problem.</citsent>
<aftsection>
<nextsent>despite this potential limitation, our system performed quite well.
</nextsent>
<nextsent>we used the data as supplied by the task organizers for all languages except italian.
</nextsent>
<nextsent>a modified version of the hunpos tagger (halcsy, kornai &amp; oravecz, 2007; attardi et al, 2009) was used to add to the italian training and development corpora more accurate pos tags than those supplied, as well as missing information about morphology.
</nextsent>
<nextsent>the pos tagger we used, in fact is capable of tagging sentences with detailed pos tags, which include morphological information; this was added to column pfeats in the data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1991">
<title id=" S10-1024.xml">combining dictionaries and contextual information for cross lingual lexical substitution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>source words include nouns, adjectives, adverbs and verbs.
</prevsent>
<prevsent>1, 000 occurrences of such words are given along with short context (a sentence).this task resembles that of word sense disambiguation (wsd) within machine translation(mt).
</prevsent>
</prevsection>
<citsent citstr=" P07-1006 ">
a few approaches have recently been proposed using standard wsd features to learn models using translations instead of senses (specia et al., 2007; <papid> P07-1006 </papid>carpuat and wu, 2007; <papid> D07-1007 </papid>chan and ng, 2007).</citsent>
<aftsection>
<nextsent>in such approaches, the global wsd score is added as feature to statistical mt systems, along with additional features, to help the system on its choice for the best translation of source word or phrase.
</nextsent>
<nextsent>we exploit contextual information in alternative ways to standard wsd features and supervised approaches.
</nextsent>
<nextsent>our two systems - uspwlv and wlvusp - use two main components: (i) list of possible translations for the source word regardless of its context; and (ii) contextual model that ranks such translations for each occurrence of the source word given its context.
</nextsent>
<nextsent>while these components constitute the core of most wsd systems, the way they are created and integrated in our systems differs from standard approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1992">
<title id=" S10-1024.xml">combining dictionaries and contextual information for cross lingual lexical substitution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>source words include nouns, adjectives, adverbs and verbs.
</prevsent>
<prevsent>1, 000 occurrences of such words are given along with short context (a sentence).this task resembles that of word sense disambiguation (wsd) within machine translation(mt).
</prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
a few approaches have recently been proposed using standard wsd features to learn models using translations instead of senses (specia et al., 2007; <papid> P07-1006 </papid>carpuat and wu, 2007; <papid> D07-1007 </papid>chan and ng, 2007).</citsent>
<aftsection>
<nextsent>in such approaches, the global wsd score is added as feature to statistical mt systems, along with additional features, to help the system on its choice for the best translation of source word or phrase.
</nextsent>
<nextsent>we exploit contextual information in alternative ways to standard wsd features and supervised approaches.
</nextsent>
<nextsent>our two systems - uspwlv and wlvusp - use two main components: (i) list of possible translations for the source word regardless of its context; and (ii) contextual model that ranks such translations for each occurrence of the source word given its context.
</nextsent>
<nextsent>while these components constitute the core of most wsd systems, the way they are created and integrated in our systems differs from standard approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1993">
<title id=" S12-1022.xml">expanding the range of tractable scope underspecified semantic representations </title>
<section> a(y,website(y), </section>
<citcontext>
<prevsection>
<prevsent>if we pull out every first, we produce the fully-scoped formula:
</prevsent>
<prevsent>every(x, politician(x), has(x, y)) if we had pulled out first, we would have had the other reading, with every having wide scope.
</prevsent>
</prevsection>
<citsent citstr=" J87-1005 ">
hobbs and shieber (1987) <papid> J87-1005 </papid>extend this formalism to support operators (such as not) and present an enumeration algorithm that is more efficient than the naive wrapping approach.since the introduction of quasi logical form (al shawi and crouch, 1992), <papid> P92-1005 </papid>there has been lot ofwork on designing constraint-based underspecifica tion formalisms where the readings of ur are not defined in constructive fashion as shown above, but rather by set of constraints.</citsent>
<aftsection>
<nextsent>a fully-scoped structure is reading iff it satisfies all the constraints.
</nextsent>
<nextsent>the advantage of these frameworks is that as the processing goes deeper, new (say pragmatically-driven)constraints can be added to the representation in order to filter out unwanted readings.
</nextsent>
<nextsent>hole semantics (bos, 1996; bos, 2002), constraint language for lambda structures (clls) (egg et al , 2001),and minimal recur sion semantics (mrs) (copes take et al , 2001) <papid> P01-1019 </papid>are among these frameworks.</nextsent>
<nextsent>in an effort to bridge the gap between the above formalisms, graph theoretic model of scope under specification was defined by bodirsky et al  (2004), called weakly normal dominance graphs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1994">
<title id=" S12-1022.xml">expanding the range of tractable scope underspecified semantic representations </title>
<section> a(y,website(y), </section>
<citcontext>
<prevsection>
<prevsent>if we pull out every first, we produce the fully-scoped formula:
</prevsent>
<prevsent>every(x, politician(x), has(x, y)) if we had pulled out first, we would have had the other reading, with every having wide scope.
</prevsent>
</prevsection>
<citsent citstr=" P92-1005 ">
hobbs and shieber (1987) <papid> J87-1005 </papid>extend this formalism to support operators (such as not) and present an enumeration algorithm that is more efficient than the naive wrapping approach.since the introduction of quasi logical form (al shawi and crouch, 1992), <papid> P92-1005 </papid>there has been lot ofwork on designing constraint-based underspecifica tion formalisms where the readings of ur are not defined in constructive fashion as shown above, but rather by set of constraints.</citsent>
<aftsection>
<nextsent>a fully-scoped structure is reading iff it satisfies all the constraints.
</nextsent>
<nextsent>the advantage of these frameworks is that as the processing goes deeper, new (say pragmatically-driven)constraints can be added to the representation in order to filter out unwanted readings.
</nextsent>
<nextsent>hole semantics (bos, 1996; bos, 2002), constraint language for lambda structures (clls) (egg et al , 2001),and minimal recur sion semantics (mrs) (copes take et al , 2001) <papid> P01-1019 </papid>are among these frameworks.</nextsent>
<nextsent>in an effort to bridge the gap between the above formalisms, graph theoretic model of scope under specification was defined by bodirsky et al  (2004), called weakly normal dominance graphs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1995">
<title id=" S12-1022.xml">expanding the range of tractable scope underspecified semantic representations </title>
<section> a(y,website(y), </section>
<citcontext>
<prevsection>
<prevsent>a fully-scoped structure is reading iff it satisfies all the constraints.
</prevsent>
<prevsent>the advantage of these frameworks is that as the processing goes deeper, new (say pragmatically-driven)constraints can be added to the representation in order to filter out unwanted readings.
</prevsent>
</prevsection>
<citsent citstr=" P01-1019 ">
hole semantics (bos, 1996; bos, 2002), constraint language for lambda structures (clls) (egg et al , 2001),and minimal recur sion semantics (mrs) (copes take et al , 2001) <papid> P01-1019 </papid>are among these frameworks.</citsent>
<aftsection>
<nextsent>in an effort to bridge the gap between the above formalisms, graph theoretic model of scope under specification was defined by bodirsky et al  (2004), called weakly normal dominance graphs.
</nextsent>
<nextsent>this 142 figure 1: ug for every child of politician runs.framework and its ancestor, dominance constraints (althaus et al , 2003), are broad frameworks for solving constrained tree structures in general.
</nextsent>
<nextsent>when it comes to scope under specification, some ofthe terminology becomes counter-intuitive.
</nextsent>
<nextsent>therefore, here we first define (scope) underspecifica tion graphs (ug), notational variant of weakly normal dominance graphs, solely defined to model scope underspecification.1 figure 1 shows ug for the following sentence.the big circles and the dot nodes are usually referred to as the hole nodes (or simply holes) and the label nodes (or simply labels) respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1996">
<title id=" S12-1022.xml">expanding the range of tractable scope underspecified semantic representations </title>
<section> a(y,website(y), </section>
<citcontext>
<prevsection>
<prevsent>143 figure 4: two of the solutions to the ug in figure 3.to-label constraints (e.g. the constraint between ev ery(x) and run(x) in figure 1) are not allowed.
</prevsent>
<prevsent>using sample grammar for clls, koller et al (2003) conjecture that the syntax/semantics interface of clls only generates underspecified representations that follow the definition of net and hence can be solved in polynomial time.
</prevsent>
</prevsection>
<citsent citstr=" P03-1047 ">
they also prove that the same efficient algorithms can be used tosolve the under specification structures of hole semantics which satisfy the net condition.unlike hole semantics and clls, mrs implicitly carries label-to-label constraints; hence the concept of net could not be applied to mrs. in order to address this, niehren and thater (2003) <papid> P03-1047 </papid>define the notion of weak net and conjecture that it covers all semantically complete mrs structures occur ring in practice.</citsent>
<aftsection>
<nextsent>fuchss et al  (2004) <papid> P04-1032 </papid>supported the claim by investigating mrs structures in the redwoods corpus (oepen et al , 2002).<papid> C02-2025 </papid></nextsent>
<nextsent>later coherent sentences were found in other corpora or suggested by other researchers (see section 6.2.2 in thater(2007)), whose ur violates the net condition, invalidating the conjecture.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1997">
<title id=" S12-1022.xml">expanding the range of tractable scope underspecified semantic representations </title>
<section> a(y,website(y), </section>
<citcontext>
<prevsection>
<prevsent>using sample grammar for clls, koller et al (2003) conjecture that the syntax/semantics interface of clls only generates underspecified representations that follow the definition of net and hence can be solved in polynomial time.
</prevsent>
<prevsent>they also prove that the same efficient algorithms can be used tosolve the under specification structures of hole semantics which satisfy the net condition.unlike hole semantics and clls, mrs implicitly carries label-to-label constraints; hence the concept of net could not be applied to mrs. in order to address this, niehren and thater (2003) <papid> P03-1047 </papid>define the notion of weak net and conjecture that it covers all semantically complete mrs structures occur ring in practice.</prevsent>
</prevsection>
<citsent citstr=" P04-1032 ">
fuchss et al  (2004) <papid> P04-1032 </papid>supported the claim by investigating mrs structures in the redwoods corpus (oepen et al , 2002).<papid> C02-2025 </papid></citsent>
<aftsection>
<nextsent>later coherent sentences were found in other corpora or suggested by other researchers (see section 6.2.2 in thater(2007)), whose ur violates the net condition, invalidating the conjecture.
</nextsent>
<nextsent>however, violating the net condition occurs in similar way in those examples,suggesting family of non-net structures, characterized in section 4.2.
</nextsent>
<nextsent>since then, it has been an open question whether there exists tractable superset of weak nets, covering this family of non-net ugs.in the rest of this paper, we answer this question.
</nextsent>
<nextsent>we modify the definition of weak netto define superset of it, which we call super net.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X1998">
<title id=" S12-1022.xml">expanding the range of tractable scope underspecified semantic representations </title>
<section> a(y,website(y), </section>
<citcontext>
<prevsection>
<prevsent>using sample grammar for clls, koller et al (2003) conjecture that the syntax/semantics interface of clls only generates underspecified representations that follow the definition of net and hence can be solved in polynomial time.
</prevsent>
<prevsent>they also prove that the same efficient algorithms can be used tosolve the under specification structures of hole semantics which satisfy the net condition.unlike hole semantics and clls, mrs implicitly carries label-to-label constraints; hence the concept of net could not be applied to mrs. in order to address this, niehren and thater (2003) <papid> P03-1047 </papid>define the notion of weak net and conjecture that it covers all semantically complete mrs structures occur ring in practice.</prevsent>
</prevsection>
<citsent citstr=" C02-2025 ">
fuchss et al  (2004) <papid> P04-1032 </papid>supported the claim by investigating mrs structures in the redwoods corpus (oepen et al , 2002).<papid> C02-2025 </papid></citsent>
<aftsection>
<nextsent>later coherent sentences were found in other corpora or suggested by other researchers (see section 6.2.2 in thater(2007)), whose ur violates the net condition, invalidating the conjecture.
</nextsent>
<nextsent>however, violating the net condition occurs in similar way in those examples,suggesting family of non-net structures, characterized in section 4.2.
</nextsent>
<nextsent>since then, it has been an open question whether there exists tractable superset of weak nets, covering this family of non-net ugs.in the rest of this paper, we answer this question.
</nextsent>
<nextsent>we modify the definition of weak netto define superset of it, which we call super net.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2000">
<title id=" S12-1022.xml">expanding the range of tractable scope underspecified semantic representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, we define notion of coherence called heart-connectednessand show that all heart-connected cf-ur structures can be solved efficiently.
</prevsent>
<prevsent>we also show that heart-connected cf-ur covers the family of non-net structures, so cf-ur is in fact the first framework to address the non-net structures.
</prevsent>
</prevsection>
<citsent citstr=" P08-1026 ">
inspite of that, cf ur is quite restricted and does not allow for adding new constraints after semantic composition.in recent work, koller et al  (2008) <papid> P08-1026 </papid>suggest using regular tree grammars for scope underspeci fic ation, probabilistic version of which could be used to find the best reading.</citsent>
<aftsection>
<nextsent>the framework goes beyond the formalisms discussed in this paper and is expressively complete in ebert (2005)s sense of completeness, i.e. it is able to describe any subset of the readings of ur.
</nextsent>
<nextsent>however, this power comes at the cost of exponential complexity.
</nextsent>
<nextsent>in practice, rtg is built on top of weak nets, benefiting from the compactness of this framework to remain tractable.
</nextsent>
<nextsent>being superset of weak net, super net provides more powerful core for rtg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2001">
<title id=" S12-1022.xml">expanding the range of tractable scope underspecified semantic representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in practice, rtg is built on top of weak nets, benefiting from the compactness of this framework to remain tractable.
</prevsent>
<prevsent>being superset of weak net, super net provides more powerful core for rtg.
</prevsent>
</prevsection>
<citsent citstr=" P10-1004 ">
koller and thater (2010) <papid> P10-1004 </papid>address the problem of finding the weakest readings of ur, which are those entailed by some reading(s), but not entailing any other reading of the ur.</citsent>
<aftsection>
<nextsent>by only considering the weakest readings, the space of solutions will be dramatically reduced.
</nextsent>
<nextsent>note that entailment using the weakest readings is sound but not complete.
</nextsent>
<nextsent>weakly normal dominance graph brings many current constraint-based formalisms under uniform framework, but its configurability is intractable in its general form.
</nextsent>
<nextsent>in this paper, we present tractable subset of this framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2002">
<title id=" S12-1011.xml">learning semantics and selectional preference of adjective noun pairs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we analyse the effect of lexical context on these relationships, and the efficacy ofthe latent semantic representation for disam biguating word meaning.
</prevsent>
<prevsent>developing models of the meanings of words and phrases is key challenge for computational linguistics.
</prevsent>
</prevsection>
<citsent citstr=" P05-1004 ">
distributed representations are useful in capturing such meaning for individual words (sato et al,2008; maas and ng, 2010; curran, 2005).<papid> P05-1004 </papid></citsent>
<aftsection>
<nextsent>however, finding compelling account of semantic com positionality that utilises such representations has proven more difficult and is an active research topic (mitchell and lapata, 2008; <papid> P08-1028 </papid>baroni and zamparelli, 2010; <papid> D10-1115 </papid>grefenstette and sadrzadeh, 2011).<papid> D11-1129 </papid></nextsent>
<nextsent>it is in this area that our paper makes its contribution.the dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2003">
<title id=" S12-1011.xml">learning semantics and selectional preference of adjective noun pairs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>developing models of the meanings of words and phrases is key challenge for computational linguistics.
</prevsent>
<prevsent>distributed representations are useful in capturing such meaning for individual words (sato et al,2008; maas and ng, 2010; curran, 2005).<papid> P05-1004 </papid></prevsent>
</prevsection>
<citsent citstr=" P08-1028 ">
however, finding compelling account of semantic com positionality that utilises such representations has proven more difficult and is an active research topic (mitchell and lapata, 2008; <papid> P08-1028 </papid>baroni and zamparelli, 2010; <papid> D10-1115 </papid>grefenstette and sadrzadeh, 2011).<papid> D11-1129 </papid></citsent>
<aftsection>
<nextsent>it is in this area that our paper makes its contribution.the dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques.
</nextsent>
<nextsent>however, such approaches failto generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference.
</nextsent>
<nextsent>we propose probabilistic model of the semantic representations for nouns and modifiers.
</nextsent>
<nextsent>the foundation of this model is latent variable representation of noun and adjective semantics together with their compositional probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2004">
<title id=" S12-1011.xml">learning semantics and selectional preference of adjective noun pairs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>developing models of the meanings of words and phrases is key challenge for computational linguistics.
</prevsent>
<prevsent>distributed representations are useful in capturing such meaning for individual words (sato et al,2008; maas and ng, 2010; curran, 2005).<papid> P05-1004 </papid></prevsent>
</prevsection>
<citsent citstr=" D10-1115 ">
however, finding compelling account of semantic com positionality that utilises such representations has proven more difficult and is an active research topic (mitchell and lapata, 2008; <papid> P08-1028 </papid>baroni and zamparelli, 2010; <papid> D10-1115 </papid>grefenstette and sadrzadeh, 2011).<papid> D11-1129 </papid></citsent>
<aftsection>
<nextsent>it is in this area that our paper makes its contribution.the dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques.
</nextsent>
<nextsent>however, such approaches failto generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference.
</nextsent>
<nextsent>we propose probabilistic model of the semantic representations for nouns and modifiers.
</nextsent>
<nextsent>the foundation of this model is latent variable representation of noun and adjective semantics together with their compositional probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2005">
<title id=" S12-1011.xml">learning semantics and selectional preference of adjective noun pairs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>developing models of the meanings of words and phrases is key challenge for computational linguistics.
</prevsent>
<prevsent>distributed representations are useful in capturing such meaning for individual words (sato et al,2008; maas and ng, 2010; curran, 2005).<papid> P05-1004 </papid></prevsent>
</prevsection>
<citsent citstr=" D11-1129 ">
however, finding compelling account of semantic com positionality that utilises such representations has proven more difficult and is an active research topic (mitchell and lapata, 2008; <papid> P08-1028 </papid>baroni and zamparelli, 2010; <papid> D10-1115 </papid>grefenstette and sadrzadeh, 2011).<papid> D11-1129 </papid></citsent>
<aftsection>
<nextsent>it is in this area that our paper makes its contribution.the dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques.
</nextsent>
<nextsent>however, such approaches failto generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference.
</nextsent>
<nextsent>we propose probabilistic model of the semantic representations for nouns and modifiers.
</nextsent>
<nextsent>the foundation of this model is latent variable representation of noun and adjective semantics together with their compositional probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2007">
<title id=" S12-1011.xml">learning semantics and selectional preference of adjective noun pairs </title>
<section> adjective-noun model.  </section>
<citcontext>
<prevsection>
<prevsent>c ni) 2.2 parameterization and inference.
</prevsent>
<prevsent>we use gibbs sampling to estimate the distributionsofn andm , integrating out the multinomial parameters (griffiths and steyvers, 2004).
</prevsent>
</prevsection>
<citsent citstr=" N09-1036 ">
the dirich let parameters ? are drawn independently from ?(1), (1) distribution, and are re sampled using slice sampling at frequent intervals throughout the sampling process (johnson and goldwater, 2009).<papid> N09-1036 </papid></citsent>
<aftsection>
<nextsent>this vague?
</nextsent>
<nextsent>prior encourages sparse draws from thedirichlet distribution.
</nextsent>
<nextsent>the number of noun and adjective classes and was set to 50 each; other sizes (100),  other sizes (150) did not significantly alter results.
</nextsent>
<nextsent>as our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2008">
<title id=" S12-1011.xml">learning semantics and selectional preference of adjective noun pairs </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the number of noun and adjective classes and was set to 50 each; other sizes (100),  other sizes (150) did not significantly alter results.
</prevsent>
<prevsent>as our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit.
</prevsent>
</prevsection>
<citsent citstr=" W03-1022 ">
we test the first hypothesis,that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using sense clustering evaluation by ciaramita and johnson (2003).<papid> W03-1022 </papid></citsent>
<aftsection>
<nextsent>the second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (clark and weir, 2002; <papid> J02-2003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999) <papid> P99-1014 </papid>and bigram plausibility (keller and lapata, 2003) <papid> J03-3005 </papid>tests.to test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (modi), where the adjective class is drawn first, in turn generating both context and the noun class.</nextsent>
<nextsent>in addition, we evaluate copies of both models ignoring context (modnc and modinc).we use the british national corpus (bnc), training on 90 percent and testing on 10 percent of the corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2009">
<title id=" S12-1011.xml">learning semantics and selectional preference of adjective noun pairs </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>as our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit.
</prevsent>
<prevsent>we test the first hypothesis,that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using sense clustering evaluation by ciaramita and johnson (2003).<papid> W03-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" J02-2003 ">
the second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (clark and weir, 2002; <papid> J02-2003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999) <papid> P99-1014 </papid>and bigram plausibility (keller and lapata, 2003) <papid> J03-3005 </papid>tests.to test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (modi), where the adjective class is drawn first, in turn generating both context and the noun class.</citsent>
<aftsection>
<nextsent>in addition, we evaluate copies of both models ignoring context (modnc and modinc).we use the british national corpus (bnc), training on 90 percent and testing on 10 percent of the corpus.
</nextsent>
<nextsent>results are reported after 2,000 iterations including burn-in period of 200 iterations.
</nextsent>
<nextsent>classes are marginalised over every 10th iteration.
</nextsent>
<nextsent>4.1 super sense tagging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2010">
<title id=" S12-1011.xml">learning semantics and selectional preference of adjective noun pairs </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>as our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit.
</prevsent>
<prevsent>we test the first hypothesis,that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using sense clustering evaluation by ciaramita and johnson (2003).<papid> W03-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
the second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (clark and weir, 2002; <papid> J02-2003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999) <papid> P99-1014 </papid>and bigram plausibility (keller and lapata, 2003) <papid> J03-3005 </papid>tests.to test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (modi), where the adjective class is drawn first, in turn generating both context and the noun class.</citsent>
<aftsection>
<nextsent>in addition, we evaluate copies of both models ignoring context (modnc and modinc).we use the british national corpus (bnc), training on 90 percent and testing on 10 percent of the corpus.
</nextsent>
<nextsent>results are reported after 2,000 iterations including burn-in period of 200 iterations.
</nextsent>
<nextsent>classes are marginalised over every 10th iteration.
</nextsent>
<nextsent>4.1 super sense tagging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2011">
<title id=" S12-1011.xml">learning semantics and selectional preference of adjective noun pairs </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>as our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit.
</prevsent>
<prevsent>we test the first hypothesis,that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using sense clustering evaluation by ciaramita and johnson (2003).<papid> W03-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" P99-1014 ">
the second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (clark and weir, 2002; <papid> J02-2003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999) <papid> P99-1014 </papid>and bigram plausibility (keller and lapata, 2003) <papid> J03-3005 </papid>tests.to test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (modi), where the adjective class is drawn first, in turn generating both context and the noun class.</citsent>
<aftsection>
<nextsent>in addition, we evaluate copies of both models ignoring context (modnc and modinc).we use the british national corpus (bnc), training on 90 percent and testing on 10 percent of the corpus.
</nextsent>
<nextsent>results are reported after 2,000 iterations including burn-in period of 200 iterations.
</nextsent>
<nextsent>classes are marginalised over every 10th iteration.
</nextsent>
<nextsent>4.1 super sense tagging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2012">
<title id=" S12-1011.xml">learning semantics and selectional preference of adjective noun pairs </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>as our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit.
</prevsent>
<prevsent>we test the first hypothesis,that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using sense clustering evaluation by ciaramita and johnson (2003).<papid> W03-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" J03-3005 ">
the second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (clark and weir, 2002; <papid> J02-2003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999) <papid> P99-1014 </papid>and bigram plausibility (keller and lapata, 2003) <papid> J03-3005 </papid>tests.to test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (modi), where the adjective class is drawn first, in turn generating both context and the noun class.</citsent>
<aftsection>
<nextsent>in addition, we evaluate copies of both models ignoring context (modnc and modinc).we use the british national corpus (bnc), training on 90 percent and testing on 10 percent of the corpus.
</nextsent>
<nextsent>results are reported after 2,000 iterations including burn-in period of 200 iterations.
</nextsent>
<nextsent>classes are marginalised over every 10th iteration.
</nextsent>
<nextsent>4.1 super sense tagging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2026">
<title id=" S12-1011.xml">learning semantics and selectional preference of adjective noun pairs </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>ranks bottom in the gold standard of the unseen set, difficult foreigner?
</prevsent>
<prevsent>ranks in the top ten.
</prevsent>
</prevsection>
<citsent citstr=" J10-4007 ">
72 recent work (o? seaghdha, 2010; erk et al,2010) <papid> J10-4007 </papid>approximated plausibility with joint probability (jp).</citsent>
<aftsection>
<nextsent>we believe that for semantic plausibility (not probability!)
</nextsent>
<nextsent>mutual information (mi), which factors out acutal frequencies, is better metric.4 we report results using jp, mi and mi2.
</nextsent>
<nextsent>seen unseen ? ? alta vista .650 ? .480 ? bnc (rasp) .543 .622 .135 .102 pado?
</nextsent>
<nextsent>et al .479 .570 .120 .138 lda .594 .558 .468 .459 rooth-lda .575 .599 .501 .469 dual-lda .460 .400 .334 .278 mod (jp) .495 .413 .286 .276 mod (mi) .394 .425 .471 .457 mod (mi2) .575 .501 .430 .408 modnc (jp) .626 .505 .357 .369 modnc (mi) .628 .574 .427 .385 modnc (mi2) .701 .623 .423 .394 table 3: results (pearson and spearman ? correlations) on the keller and lapata (2003) <papid> J03-3005 </papid>plausibility data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2029">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> the centro id attribute model.  </section>
<citcontext>
<prevsection>
<prevsent>discussion.
</prevsent>
<prevsent>the central feature of cam is that it avoids word sense disambiguation, although itstill relies on predefined sense inventory (word net, through corelex).
</prevsent>
</prevsection>
<citsent citstr=" E09-1045 ">
our use of monosemous words to represent meta senses and meta alternations goes beyond previous work which uses monosemous words to disambiguate polysemous words in context (izquierdo et al, 2009; <papid> E09-1045 </papid>navigli and velardi, 2005).</citsent>
<aftsection>
<nextsent>because of its focus on avoiding disambiguation,cam simplifies the representation of meta alternations and polysemous words to single centro id vectors.
</nextsent>
<nextsent>in the future, we plan to induce word senses (schutze, 1998; pantel and lin, 2002; reisinger and mooney, 2010), <papid> N10-1013 </papid>which will allow for more flexible and realistic models.</nextsent>
<nextsent>153 abs abstraction ent entity loc location prt part act act evt event log geo.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2030">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> the centro id attribute model.  </section>
<citcontext>
<prevsection>
<prevsent>our use of monosemous words to represent meta senses and meta alternations goes beyond previous work which uses monosemous words to disambiguate polysemous words in context (izquierdo et al, 2009; <papid> E09-1045 </papid>navigli and velardi, 2005).</prevsent>
<prevsent>because of its focus on avoiding disambiguation,cam simplifies the representation of meta alternations and polysemous words to single centro id vec tors.</prevsent>
</prevsection>
<citsent citstr=" N10-1013 ">
in the future, we plan to induce word senses (schutze, 1998; pantel and lin, 2002; reisinger and mooney, 2010), <papid> N10-1013 </papid>which will allow for more flexible and realistic models.</citsent>
<aftsection>
<nextsent>153 abs abstraction ent entity loc location prt part act act evt event log geo.
</nextsent>
<nextsent>location psy psychol.
</nextsent>
<nextsent>feature agt agent fod food mea measure qud definite quantity anm animal frm form mic microorganism qui indefinite quantity art artifact grb biolog.
</nextsent>
<nextsent>group nat natural body rel relation atr attribute grp grouping phm phenomenon spc space cel cell grs social group pho physical object sta state chm chemical hum human plt plant sub substance com communication lfr living being pos possession tme time con consequence lme linear measure pro process pro process table 2: core lexs basic types with their corresponding wordnet anchors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2031">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>while an alternative would be to rank meta alternations forgiven polysemous lemma, the method chosen here has the benefit of providing data on the performance of individual meta senses and meta alternations.
</prevsent>
<prevsent>4.1 data.
</prevsent>
</prevsection>
<citsent citstr=" J07-4004 ">
all modeling and data extraction was carried out on the written part of the british national corpus (bnc; burnage and dunlop (1992)) parsed with the c&c; tools (clark and curran, 2007).<papid> J07-4004 </papid></citsent>
<aftsection>
<nextsent>6 for the evaluation, we focus on disemous words, words which instantiate exactly two meta senses according to wordnet.
</nextsent>
<nextsent>for each meta alternation(m,m?), we evaluate cam on set of disemous targets (lemmas that instantiate (m,m?)) and disemous dis tractors (lemmas that do not).
</nextsent>
<nextsent>we define three types of distractors: (1) dis tractors sharing with the targets (but not m?), (2) dis tractors sharing mwith the targets (but not m), and (3) dis tractors sharing neither.
</nextsent>
<nextsent>in this way, we ensure that cam cannot obtain good results by merely modeling the similarity of targets to either or m?, which would rather be coarse-grained word sense modeling task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2032">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>alternatively, it can be computed by macro-averaging, that is, by normalizing the individual vectors before averaging.
</prevsent>
<prevsent>this gives equal weight to the each lemma or meta sense, respectively.
</prevsent>
</prevsection>
<citsent citstr=" W04-0837 ">
macro-averaging in repa thus assumes that senses are equally distributed, which is an oversimplification, as word senses are known to present skewed distributions (mccarthy et al, 2004) <papid> W04-0837 </papid>and vectors for words with predominant sense will be similar to the dominant meta sense vector.</citsent>
<aftsection>
<nextsent>micro-averaging partially models sense skewed ness under the assumption that word frequency correlates with sense frequency.similarity measure.
</nextsent>
<nextsent>as the vector similarity measure in eq.
</nextsent>
<nextsent>(5), we use the standard cosine similarity (lee, 1999).<papid> P99-1004 </papid></nextsent>
<nextsent>it ranges between 1 and 1, with 1 denoting maximum similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2033">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>micro-averaging partially models sense skewed ness under the assumption that word frequency correlates with sense frequency.similarity measure.
</prevsent>
<prevsent>as the vector similarity measure in eq.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
(5), we use the standard cosine similarity (lee, 1999).<papid> P99-1004 </papid></citsent>
<aftsection>
<nextsent>it ranges between 1 and 1, with 1 denoting maximum similarity.
</nextsent>
<nextsent>in the current model where the vectors do not contain negative counts, the range is [0; 1].
</nextsent>
<nextsent>effect of parameters the four parameters of section 4.3 (three space types, macro-/micro-averagingfor repm and repa, and log-likelihood trans forma tion) correspond to 24 instantiations of cam.figure 1 shows the influence of the four parameters.
</nextsent>
<nextsent>the only significant difference is tied to the use of lexicalized vector spaces (gramlex / lex are better than gram).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2034">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>indeed, for agent-human, the alternation which most benefits from the micro-averagingsetting, the targets are much more similar to the human meta sense (which is approximately 8 times as frequent as agent) than to the agent meta sense.
</prevsent>
<prevsent>the latter contains anything that can have an effect on something, e.g. emulsifier, force, valium.
</prevsent>
</prevsection>
<citsent citstr=" J10-4007 ">
the targets for agent-human, in contrast, contain words such as engineer, manipulator, operative, which alternate between an agentive role played by person and the person herself.while lacking in clear improvement, log likelihood transformation tends to reduce variance,consistent with the effect previously found in selectional preference modeling (erk et al, 2010).<papid> J10-4007 </papid></citsent>
<aftsection>
<nextsent>overall performance although the performance of the cam models is still far from perfect, all 24 models obtain map scores of 0.35 or above, while the random baseline is at 0.313, and the overall frequency baseline at 0.291.
</nextsent>
<nextsent>thus, all models consistently outperform both baselines.
</nextsent>
<nextsent>a bootstrapresampling test (efron and tibshirani, 1994) confirmed that the difference to the frequency baseline is significant at   0.01 for all 24 models.
</nextsent>
<nextsent>the difference to the random baseline is significant at   0.01 for 23 models and at   0.05 for the remaining model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2035">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a pioneering study is buitelaar (1998), who accounts for regular polysemy through the corelex resource (cf.
</prevsent>
<prevsent>section 3).
</prevsent>
</prevsection>
<citsent citstr=" N01-1010 ">
a similar effort is carried out by tomuro (2001), <papid> N01-1010 </papid>but he represents regular polysemy at the level of senses.</citsent>
<aftsection>
<nextsent>recently, utt and pado?
</nextsent>
<nextsent>(2011) explore the differences between between idiosyncratic and regular polysemy patterns building on corelex.
</nextsent>
<nextsent>lapata (2000) focuses 157on the default meaning arising from word combinations, as opposed to the polysemy of single words as in this study.
</nextsent>
<nextsent>meta alternations other than regular polysemy,such as metonymy, play crucial role in information extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2036">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>meta alternations other than regular polysemy,such as metonymy, play crucial role in information extraction.
</prevsent>
<prevsent>for instance, the meta alternation social group-geographical location corresponds to an ambiguity between the location organization named entity classes which isknown to be hard problem in named entity recognition and classification (markert and nissim, 2009).
</prevsent>
</prevsection>
<citsent citstr=" D11-1063 ">
metaphorical meta alternations have also received attention recently (turney et al, 2011)<papid> D11-1063 </papid>on structural level, the prediction of meta alternations shows clear correspondence to analogy prediction as approached in turney (2006) (<papid> J06-3003 </papid>carpen ter:wood is analogous to mason:stone, but not tophotograph:camera).</citsent>
<aftsection>
<nextsent>the framework defined in section 2 conceptualizes our task in way parallel to that of ana logical reasoning, modeling not first-ordersemantic similarity, but second-order?
</nextsent>
<nextsent>semantic relations.
</nextsent>
<nextsent>however, the two tasks cannot be approached with the same methods, as turneys model relies on contexts linking two nouns in corpus sentences (what does do to b?).
</nextsent>
<nextsent>in contrast, we are interested in relations within words, namely between word senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2037">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>meta alternations other than regular polysemy,such as metonymy, play crucial role in information extraction.
</prevsent>
<prevsent>for instance, the meta alternation social group-geographical location corresponds to an ambiguity between the location organization named entity classes which isknown to be hard problem in named entity recognition and classification (markert and nissim, 2009).
</prevsent>
</prevsection>
<citsent citstr=" J06-3003 ">
metaphorical meta alternations have also received attention recently (turney et al, 2011)<papid> D11-1063 </papid>on structural level, the prediction of meta alternations shows clear correspondence to analogy prediction as approached in turney (2006) (<papid> J06-3003 </papid>carpen ter:wood is analogous to mason:stone, but not tophotograph:camera).</citsent>
<aftsection>
<nextsent>the framework defined in section 2 conceptualizes our task in way parallel to that of ana logical reasoning, modeling not first-ordersemantic similarity, but second-order?
</nextsent>
<nextsent>semantic relations.
</nextsent>
<nextsent>however, the two tasks cannot be approached with the same methods, as turneys model relies on contexts linking two nouns in corpus sentences (what does do to b?).
</nextsent>
<nextsent>in contrast, we are interested in relations within words, namely between word senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2038">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, the two tasks cannot be approached with the same methods, as turneys model relies on contexts linking two nouns in corpus sentences (what does do to b?).
</prevsent>
<prevsent>in contrast, we are interested in relations within words, namely between word senses.
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
we cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (gale et al, 1992).<papid> H92-1045 </papid></citsent>
<aftsection>
<nextsent>a concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based wsd (yarowsky, 1992; <papid> C92-2070 </papid>curran, 2005; <papid> P05-1004 </papid>izquierdo et al, 2009), <papid> E09-1045 </papid>and indeed, the cam might be used for class-based wsd as well.</nextsent>
<nextsent>however, our emphasis lies rather on modelingpolysemy across words (meta alternations), some thing that is absent in wsd, class-based or not.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2039">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, we are interested in relations within words, namely between word senses.
</prevsent>
<prevsent>we cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (gale et al, 1992).<papid> H92-1045 </papid></prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
a concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based wsd (yarowsky, 1992; <papid> C92-2070 </papid>curran, 2005; <papid> P05-1004 </papid>izquierdo et al, 2009), <papid> E09-1045 </papid>and indeed, the cam might be used for class-based wsd as well.</citsent>
<aftsection>
<nextsent>however, our emphasis lies rather on modelingpolysemy across words (meta alternations), some thing that is absent in wsd, class-based or not.
</nextsent>
<nextsent>the only exception, to our knowledge, is ando (2006), <papid> W06-2911 </papid>who pools the labeled examples for all words from dataset for learning, implicitly exploiting regularities in sense alternations.</nextsent>
<nextsent>meta senses also bear close resemblance to the notion of semantic class as used in lexical acquisition (hindle, 1990; <papid> P90-1034 </papid>merlo and stevenson, 2001;<papid> J01-3003 </papid>schulte im walde, 2006; joanis et al, 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2040">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, we are interested in relations within words, namely between word senses.
</prevsent>
<prevsent>we cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (gale et al, 1992).<papid> H92-1045 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1004 ">
a concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based wsd (yarowsky, 1992; <papid> C92-2070 </papid>curran, 2005; <papid> P05-1004 </papid>izquierdo et al, 2009), <papid> E09-1045 </papid>and indeed, the cam might be used for class-based wsd as well.</citsent>
<aftsection>
<nextsent>however, our emphasis lies rather on modelingpolysemy across words (meta alternations), some thing that is absent in wsd, class-based or not.
</nextsent>
<nextsent>the only exception, to our knowledge, is ando (2006), <papid> W06-2911 </papid>who pools the labeled examples for all words from dataset for learning, implicitly exploiting regularities in sense alternations.</nextsent>
<nextsent>meta senses also bear close resemblance to the notion of semantic class as used in lexical acquisition (hindle, 1990; <papid> P90-1034 </papid>merlo and stevenson, 2001;<papid> J01-3003 </papid>schulte im walde, 2006; joanis et al, 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2042">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based wsd (yarowsky, 1992; <papid> C92-2070 </papid>curran, 2005; <papid> P05-1004 </papid>izquierdo et al, 2009), <papid> E09-1045 </papid>and indeed, the cam might be used for class-based wsd as well.</prevsent>
<prevsent>however, our emphasis lies rather on modelingpolysemy across words (meta alternations), some thing that is absent in wsd, class-based or not.</prevsent>
</prevsection>
<citsent citstr=" W06-2911 ">
the only exception, to our knowledge, is ando (2006), <papid> W06-2911 </papid>who pools the labeled examples for all words from dataset for learning, implicitly exploiting regularities in sense alternations.</citsent>
<aftsection>
<nextsent>meta senses also bear close resemblance to the notion of semantic class as used in lexical acquisition (hindle, 1990; <papid> P90-1034 </papid>merlo and stevenson, 2001;<papid> J01-3003 </papid>schulte im walde, 2006; joanis et al, 2008).</nextsent>
<nextsent>how ever, in most of this research polysemy is ignored.a few exceptions use soft clustering for multiple assignment of verbs to semantic classes (pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999; <papid> P99-1014 </papid>korhonen et al, 2003), <papid> P03-1009 </papid>and boleda et al (to appear) explicitly model regular polysemy for adjectives.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2043">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, our emphasis lies rather on modelingpolysemy across words (meta alternations), some thing that is absent in wsd, class-based or not.
</prevsent>
<prevsent>the only exception, to our knowledge, is ando (2006), <papid> W06-2911 </papid>who pools the labeled examples for all words from dataset for learning, implicitly exploiting regularities in sense alternations.</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
meta senses also bear close resemblance to the notion of semantic class as used in lexical acquisition (hindle, 1990; <papid> P90-1034 </papid>merlo and stevenson, 2001;<papid> J01-3003 </papid>schulte im walde, 2006; joanis et al, 2008).</citsent>
<aftsection>
<nextsent>how ever, in most of this research polysemy is ignored.a few exceptions use soft clustering for multiple assignment of verbs to semantic classes (pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999; <papid> P99-1014 </papid>korhonen et al, 2003), <papid> P03-1009 </papid>and boleda et al (to appear) explicitly model regular polysemy for adjectives.</nextsent>
<nextsent>we have argued that modeling regular polysemy and other ana logical processes will help improve current models of word meaning in empirical computational semantics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2044">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, our emphasis lies rather on modelingpolysemy across words (meta alternations), some thing that is absent in wsd, class-based or not.
</prevsent>
<prevsent>the only exception, to our knowledge, is ando (2006), <papid> W06-2911 </papid>who pools the labeled examples for all words from dataset for learning, implicitly exploiting regularities in sense alternations.</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
meta senses also bear close resemblance to the notion of semantic class as used in lexical acquisition (hindle, 1990; <papid> P90-1034 </papid>merlo and stevenson, 2001;<papid> J01-3003 </papid>schulte im walde, 2006; joanis et al, 2008).</citsent>
<aftsection>
<nextsent>how ever, in most of this research polysemy is ignored.a few exceptions use soft clustering for multiple assignment of verbs to semantic classes (pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999; <papid> P99-1014 </papid>korhonen et al, 2003), <papid> P03-1009 </papid>and boleda et al (to appear) explicitly model regular polysemy for adjectives.</nextsent>
<nextsent>we have argued that modeling regular polysemy and other ana logical processes will help improve current models of word meaning in empirical computational semantics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2045">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the only exception, to our knowledge, is ando (2006), <papid> W06-2911 </papid>who pools the labeled examples for all words from dataset for learning, implicitly exploiting regularities in sense alternations.</prevsent>
<prevsent>meta senses also bear close resemblance to the notion of semantic class as used in lexical acquisition (hindle, 1990; <papid> P90-1034 </papid>merlo and stevenson, 2001;<papid> J01-3003 </papid>schulte im walde, 2006; joanis et al, 2008).</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
how ever, in most of this research polysemy is ignored.a few exceptions use soft clustering for multiple assignment of verbs to semantic classes (pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999; <papid> P99-1014 </papid>korhonen et al, 2003), <papid> P03-1009 </papid>and boleda et al (to appear) explicitly model regular polysemy for adjectives.</citsent>
<aftsection>
<nextsent>we have argued that modeling regular polysemy and other ana logical processes will help improve current models of word meaning in empirical computational semantics.
</nextsent>
<nextsent>we have presented formal framework to represent and operate with regular sense alternations, as well as first simple instantiation of theframework.
</nextsent>
<nextsent>we have conducted an evaluation of different implementations of this model in the new task of determining whether words match given sense alternation.
</nextsent>
<nextsent>all models significantly outperform the baselines when considered as whole, and the best implementation outperforms the baselines for 73.3% of the tested alternations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2046">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the only exception, to our knowledge, is ando (2006), <papid> W06-2911 </papid>who pools the labeled examples for all words from dataset for learning, implicitly exploiting regularities in sense alternations.</prevsent>
<prevsent>meta senses also bear close resemblance to the notion of semantic class as used in lexical acquisition (hindle, 1990; <papid> P90-1034 </papid>merlo and stevenson, 2001;<papid> J01-3003 </papid>schulte im walde, 2006; joanis et al, 2008).</prevsent>
</prevsection>
<citsent citstr=" P99-1014 ">
how ever, in most of this research polysemy is ignored.a few exceptions use soft clustering for multiple assignment of verbs to semantic classes (pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999; <papid> P99-1014 </papid>korhonen et al, 2003), <papid> P03-1009 </papid>and boleda et al (to appear) explicitly model regular polysemy for adjectives.</citsent>
<aftsection>
<nextsent>we have argued that modeling regular polysemy and other ana logical processes will help improve current models of word meaning in empirical computational semantics.
</nextsent>
<nextsent>we have presented formal framework to represent and operate with regular sense alternations, as well as first simple instantiation of theframework.
</nextsent>
<nextsent>we have conducted an evaluation of different implementations of this model in the new task of determining whether words match given sense alternation.
</nextsent>
<nextsent>all models significantly outperform the baselines when considered as whole, and the best implementation outperforms the baselines for 73.3% of the tested alternations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2047">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the only exception, to our knowledge, is ando (2006), <papid> W06-2911 </papid>who pools the labeled examples for all words from dataset for learning, implicitly exploiting regularities in sense alternations.</prevsent>
<prevsent>meta senses also bear close resemblance to the notion of semantic class as used in lexical acquisition (hindle, 1990; <papid> P90-1034 </papid>merlo and stevenson, 2001;<papid> J01-3003 </papid>schulte im walde, 2006; joanis et al, 2008).</prevsent>
</prevsection>
<citsent citstr=" P03-1009 ">
how ever, in most of this research polysemy is ignored.a few exceptions use soft clustering for multiple assignment of verbs to semantic classes (pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999; <papid> P99-1014 </papid>korhonen et al, 2003), <papid> P03-1009 </papid>and boleda et al (to appear) explicitly model regular polysemy for adjectives.</citsent>
<aftsection>
<nextsent>we have argued that modeling regular polysemy and other ana logical processes will help improve current models of word meaning in empirical computational semantics.
</nextsent>
<nextsent>we have presented formal framework to represent and operate with regular sense alternations, as well as first simple instantiation of theframework.
</nextsent>
<nextsent>we have conducted an evaluation of different implementations of this model in the new task of determining whether words match given sense alternation.
</nextsent>
<nextsent>all models significantly outperform the baselines when considered as whole, and the best implementation outperforms the baselines for 73.3% of the tested alternations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2048">
<title id=" S12-1023.xml">regular polysemy a distributional model </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>other improvements on the model and evaluation will be to develop more informed baselines that capture semantic shifts, as well as to test alternate weighting schemes for the co-occurrence vectors (e.g. pmi) and to use larger corpora than the bnc.
</prevsent>
<prevsent>the second step is to go beyond the limited in-vitroevaluation we have presented here by integrating alternation prediction into larger nlp tasks.
</prevsent>
</prevsection>
<citsent citstr=" P08-1078 ">
knowledge about alternations can play an important role in counteracting sparseness in many tasks that involve semantic compatibility, e.g., testing the applicability of lexical inference rules (szpektor et al, 2008).<papid> P08-1078 </papid></citsent>
<aftsection>
<nextsent>acknowledgements this research is partially funded by the spanish ministry of science and innovation (ffi2010-15006,tin2009-14715-c04-04), the agaur (2010 bp a00070), the german research foundation (sfb 732), and the eu (pascal2; fp7-ict-216886).
</nextsent>
<nextsent>it is largely inspired on course by ann copestake at u. pompeu fabra (2008).
</nextsent>
<nextsent>we thank marco baroni, katrin erk, and the reviewers of this and four other conferences for valuable feedback.
</nextsent>
<nextsent>158
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2049">
<title id=" S12-1068.xml">annlor a nave notation system for lexical outputs ranking </title>
<section> simple english wikipedia based system.  </section>
<citcontext>
<prevsection>
<prevsent>our hypothesis seems to be correct due to the results we obtained.
</prevsent>
<prevsent>morevover, the simple english wikipedia has been used previously in work on automatic text simplification, e.g.
</prevsent>
</prevsection>
<citsent citstr=" C10-1152 ">
(zhu et al., 2010).<papid> C10-1152 </papid></citsent>
<aftsection>
<nextsent>1http://infolingu.univ-mlv.fr/ donneeslinguistiques/dictionnaires/ telechargement.html 488first, we produced plain text version of the simple english wikipedia.
</nextsent>
<nextsent>we downloaded the dump dated february 27, 2012 and extracted the textual contents using the wikipedia2text tool.2 the final plain text file contains approximately 10 million words.
</nextsent>
<nextsent>we extracted word n-grams (n ranging from 1 to 3) and their frequencies from this corpus thanks tothe text-nsp perl module 3 and its count.pl program, which produces the list of n-grams of document, with their frequencies.
</nextsent>
<nextsent>table 1 gives the number of n-grams produced.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2050">
<title id=" S10-1017.xml">relax cor a global relaxation labeling approach to coreference resolution </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the partitions that the vertex can be assigned.
</prevsent>
<prevsent>a vertex with index can be in the first partitions (i.e. i = i).
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
88 distance and position: dist: distance betweenm andm in sentences: number dist men: distance betweenm andm in mentions: number appositive: one mention is in apposition with the other: y,n i/j in quotes:m i/j is in quotes or inside np or sentence in quotes: y,n i/j first:m i/j is the first mention in the sentence: y,n lexical: i/j def np:m i/j is definitive np: y,n i/j dem np:m i/j is demonstrative np: y,n i/j indef np:m i/j is an indefinite np: y,n str match: string matching ofm andm : y,n pro str: both are pronouns and their strings match: y,n pn str: both are proper names and their strings match: y,n nonpro str: string matching like in soon et al (2001) <papid> J01-4004 </papid>and mentions are not pronouns: y,n head match: string matching of np heads: y,n morphological: number: the number of both mentions match: y,n,u gender: the gender of both mentions match: y,n,u agreement: gender and number of both mentions match: y,n,u i/j third person:m i/j is 3rd person: y,n proper name: both mentions are proper names: y,n,u i/j person:m i/j is person (pronoun or proper name in list): y,n animacy: animacy of both mentions match (persons, objects): y,n i/j reflexive:m i/j is reflexive pronoun: y,n i/j type:m i/j is pronoun (p), entity (e) or nominal (n) syntactic: nested: one mention is included in the other: y,n maximalnp: both mentions have the same np parent or they are nested: y,n i/j maximalnp:m i/j is not included in any other mention: y,n i/j embedded:m i/j is noun and is not maximal np: y,n binding: conditions and of binding theory: y,n semantic: semclass: semantic class of both mentions match: y,n,u (the same as (soon et al, 2001)) <papid> J01-4004 </papid>alias: one mention is an alias of the other: y,n,u (only entities, else unknown) i/j srl arg: semantic role ofm i/j : n,0,1,2,3,4,m,l srl sameverb: both mentions have semantic role for the same verb: y,n figure 1: feature functions used.</citsent>
<aftsection>
<nextsent>2.2 training process.
</nextsent>
<nextsent>each pair of mentions (m , j) in training document is evaluated by the set of feature functions shown in figure 1.
</nextsent>
<nextsent>the values returned by these functions form positive example when the pair of mentions corefer, and negative one otherwise.three specialized models are constructed depending on the type of anaphor mention (m ) of the pair: pronoun, named entity or nominal.
</nextsent>
<nextsent>a decision tree is generated for each specialized model and set of rules is extracted with c4.5 rule-learning algorithm (quinlan, 1993).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2052">
<title id=" S10-1002.xml">semeval2010 task 2 cross lingual lexical substitution </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>one early and notable work was the senseval-2 japanese translation task (kurohashi, 2001) that obtained alternative translation records of typical usages ofa test word, also referred to as translation memory.
</prevsent>
<prevsent>systems could either select the most appropriate translation memory record for each instance and were scored against gold-standard set of annotations, or they could provide translation that was scored by translation experts after the results were submitted.
</prevsent>
</prevsection>
<citsent citstr=" W07-2010 ">
in contrast to this work, in ourtask we provided actual translations for target instances in advance, rather than predetermine translations using lexicographers or relyon post-hoc evaluation, which does not permit evaluation of new systems after the competition.previous standalone wsd tasks based on parallel data have obtained distinct translations for senses as listed in dictionary (ng and chan, 2007).<papid> W07-2010 </papid></citsent>
<aftsection>
<nextsent>in this way fine-grained senses with thesame translations can be lumped together, how ever this does not fully allow for the fact that some senses for the same words may have some translations in common but also others that are not (sinha et al, 2009).in our task, we collected dataset which allows instances of the same word to have some translations in common, while not necessitatinga clustering of translations from specific resource into senses (in comparison to lefever and hoste (2010)).
</nextsent>
<nextsent>1 resnik and yarowsky (2000) also1though in that task note that it is possible for translation to occur in more than one cluster.
</nextsent>
<nextsent>it will be interesting to 9 conducted experiments using words in context, rather than predefined sense-inventory however in these experiments the annotators were asked fora single preferred translation.
</nextsent>
<nextsent>in our case, we allowed annotators to supply as many translations as they felt were equally valid.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2053">
<title id=" S10-1002.xml">semeval2010 task 2 cross lingual lexical substitution </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>this allows usto examine more subtle relationships between usages and to allow partial credit to systems thatget close approximation to the annotators?
</prevsent>
<prevsent>translations.
</prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
unlike full blown machine translation task (carpuat and wu, 2007), <papid> D07-1007 </papid>annotators and systems are not required to translate the whole context but just the target word.</citsent>
<aftsection>
<nextsent>substitution task the english lexical substitution task (hereafterreferred to as lexsub) was run at semeval 2007 (mccarthy and navigli, 2007; <papid> W07-2009 </papid>mccarthy and navigli, 2009).</nextsent>
<nextsent>lexsub was proposed as task which, while requiring contextual disambiguation, did not presuppose specific sense inventory.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2054">
<title id=" S10-1002.xml">semeval2010 task 2 cross lingual lexical substitution </title>
<section> background: the english lexical.  </section>
<citcontext>
<prevsection>
<prevsent>translations.
</prevsent>
<prevsent>unlike full blown machine translation task (carpuat and wu, 2007), <papid> D07-1007 </papid>annotators and systems are not required to translate the whole context but just the target word.</prevsent>
</prevsection>
<citsent citstr=" W07-2009 ">
substitution task the english lexical substitution task (hereafterreferred to as lexsub) was run at semeval 2007 (mccarthy and navigli, 2007; <papid> W07-2009 </papid>mccarthy and navigli, 2009).</citsent>
<aftsection>
<nextsent>lexsub was proposed as task which, while requiring contextual disambiguation, did not presuppose specific sense inventory.
</nextsent>
<nextsent>infact, it is quite possible to use alternative representations of meaning, such as those proposed by schutze (1998) and pantel and lin (2002).
</nextsent>
<nextsent>the motivation for substitution task was that it would reflect capabilities that might be useful for natural language processing tasks such as paraphrasing and textual entailment, while not requiring complete system that might mask system capabilities at lexical level and make participation in the task difficult for small research teams.the task required systems to produce substitute word for word in context.
</nextsent>
<nextsent>the data was collected for 201 words from open class parts-ofspeech (pos) (i.e. nouns, verbs, adjectives and ad verbs).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2056">
<title id=" S12-1010.xml">sorting out the most confusing english phrasal verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tions like that directly given in experience that marks berkeley out as an empiricist . this paper is targeting to build an automatic learner which can recognize true phrasal verb from its ortho graphically identical construction with verb and prepositional phrase.
</prevsent>
<prevsent>similar to other types of multiword expressions (mwes) (sag et al,2002), the syntactic complexity and semantic idiosyncrasies of phrasal verbs pose many particular challenges in empirical natural language processing (nlp).
</prevsent>
</prevsection>
<citsent citstr=" P03-1065 ">
even though few of previous work shave explored this identification problem empirically (li et al, 2003; <papid> P03-1065 </papid>kim and baldwin, 2009) and theoretically (jackendoff, 2002), we argue in this paper that this context sensitive identification problem is not so easy as conceivably shown before, especially when it is used to handle those more compositional phrasal verbs which are empirically used either way in the corpus as true phrasal verb or simplex verb with preposition combination.</citsent>
<aftsection>
<nextsent>in addition, there is still lack of adequate resources or benchmark datasets to identify and treat phrasal 65 verbs within given context.
</nextsent>
<nextsent>this research is alsoan attempt to bridge this gap by constructing publicly available dataset which focuses on some of the most commonly used phrasal verbs within their most confusing contexts.
</nextsent>
<nextsent>our study in this paper focuses on six of the most frequently used verbs, take, make, have, get, doand give and their combination with nineteen common prepositions or particles, such as on, in, up etc. we categorize these phrasal verbs according to their continuum of compositionality, splitting them into two groups based on the biggest gap within this scale, and build discriminative learner which uses easily available syntactic and lexical features to analyze them comparatively.
</nextsent>
<nextsent>this learner achieves 79.4% overall accuracy for the whole dataset and learns the most from the more compositional data with 51.2% error reduction over its 46.6% baseline.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2057">
<title id=" S12-1010.xml">sorting out the most confusing english phrasal verbs </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>careful linguistic descriptions and investigations reveal wide range of english phrasal verbs that are syntactically uniform,but diverge largely in semantics, argument structure and lexical status.
</prevsent>
<prevsent>the complexity and idiosyncrasies of english phrasal verbs also pose special challenge to computational linguistics and attract considerable amount of interest and investigation for their extraction, disambiguation as well as identification.
</prevsent>
</prevsection>
<citsent citstr=" W03-1808 ">
recent computational research on english phrasal verbs have been focused on increasing the coverage and scala bility of phrasal verbs by either extracting unlisted phrasal verbs from large corpora (villavicencio, 2003; <papid> W03-1808 </papid>villavicencio, 2006),or constructing productive lexical rules to generate new cases (villanvicencio and copestake, 2003).some other researchers follow the semantic regularities of the particles associated with these phrasal verbs and concentrate on disambiguation of phrasal 2it is written in the preface of that dictionary.</citsent>
<aftsection>
<nextsent>verb semantics, such as the investigation of the most common particle up by (cook and stevenson, 2006).<papid> W06-1207 </papid></nextsent>
<nextsent>research on token identification of phrasal verbs is much less compared to the extraction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2058">
<title id=" S12-1010.xml">sorting out the most confusing english phrasal verbs </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the complexity and idiosyncrasies of english phrasal verbs also pose special challenge to computational linguistics and attract considerable amount of interest and investigation for their extraction, disambiguation as well as identification.
</prevsent>
<prevsent>recent computational research on english phrasal verbs have been focused on increasing the coverage and scala bility of phrasal verbs by either extracting unlisted phrasal verbs from large corpora (villavicencio, 2003; <papid> W03-1808 </papid>villavicencio, 2006),or constructing productive lexical rules to generate new cases (villanvicencio and copestake, 2003).some other researchers follow the semantic regularities of the particles associated with these phrasal verbs and concentrate on disambiguation of phrasal 2it is written in the preface of that dictionary.</prevsent>
</prevsection>
<citsent citstr=" W06-1207 ">
verb semantics, such as the investigation of the most common particle up by (cook and stevenson, 2006).<papid> W06-1207 </papid></citsent>
<aftsection>
<nextsent>research on token identification of phrasal verbs is much less compared to the extraction.
</nextsent>
<nextsent>(li etal., 2003) <papid> P03-1065 </papid>describes regular expression based simple system.</nextsent>
<nextsent>regular expression based method requires human constructed regular patterns and can not make predictions for out-of-vocabulary phrasal verbs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2060">
<title id=" S12-1010.xml">sorting out the most confusing english phrasal verbs </title>
<section> identification of english phrasal verbs.  </section>
<citcontext>
<prevsection>
<prevsent>thus, this feature value is vp-pp.
</prevsent>
<prevsent>our feature extractor is implemented in java through publicly available nlp library4 via the tool called curator (clarke etal., 2012).
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
the shallow parser is publicly available (punyakanok and roth, 2001)5 and the parser we use is from (charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>3.1 data preparation and annotation.
</nextsent>
<nextsent>all sentences in our dataset are extracted from bnc(xml edition), balanced syn chronic corpus containing 100 million words collected from various sources of british english.
</nextsent>
<nextsent>we first construct list of phrasal verbs for the six verbs that we are interested in from two resources, wn3.0 (fellbaum, 1998) and direct6.
</nextsent>
<nextsent>since these targeted verbs are also commonly used in english light verb constructions(lvcs), we filter out lvcs in our list using publicly available lvc corpus (tu and roth, 2011).<papid> W11-0807 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2061">
<title id=" S12-1010.xml">sorting out the most confusing english phrasal verbs </title>
<section> identification of english phrasal verbs.  </section>
<citcontext>
<prevsection>
<prevsent>all sentences in our dataset are extracted from bnc(xml edition), balanced syn chronic corpus containing 100 million words collected from various sources of british english.
</prevsent>
<prevsent>we first construct list of phrasal verbs for the six verbs that we are interested in from two resources, wn3.0 (fellbaum, 1998) and direct6.
</prevsent>
</prevsection>
<citsent citstr=" W11-0807 ">
since these targeted verbs are also commonly used in english light verb constructions(lvcs), we filter out lvcs in our list using publicly available lvc corpus (tu and roth, 2011).<papid> W11-0807 </papid></citsent>
<aftsection>
<nextsent>the result list consists of total of 245 phrasal verbs.
</nextsent>
<nextsent>we then search over bnc and find sentences for all of them.
</nextsent>
<nextsent>we choose the frequency threshold to be 25 and generate list of 122 phrasal verbs.
</nextsent>
<nextsent>finally we manually pick out 23 of these phrasal verbs and sample randomly 10% extracted sentences for each of them for annotation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2062">
<title id=" P98-2142.xml">integrated control of chart items for error repair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system ranks possible repairs by penalty scores which are based on both grammar-dependent factors (e.g. the significance of the repaired constituent in local tree) and grammar-independent factors (e.g. error types).
</prevsent>
<prevsent>this paper focuses on the heterarchical processing of integrated- agenda items (i.e. chart items) at three levels, in the context of single error recovery.
</prevsent>
</prevsection>
<citsent citstr=" J83-3003 ">
weischedel and sondheimer (1983) <papid> J83-3003 </papid>described two types of ill-formedness: relative (i.e. limitations of the computer system) and absolute (e.g. misspel ings, mis typing, agreement violation etc).</citsent>
<aftsection>
<nextsent>these two types of problem cause ill-formedness of sentence at various levels, including typographical, ortho graphical, morphological, phonological, syntactic, semantic, and pragmatic levels.
</nextsent>
<nextsent>typographical spelling errors have been studied by many people (damerau, 1964; peterson, 1980; pollock and zamora, 1983).
</nextsent>
<nextsent>mitton (1987) found large proportion of real-word errors were orthographical: to--  too, were ---  where . at the sentential level, types of syntactic errors such as co-occurrence violations, ellipsis, conjunction errors, and extraneous terms have been studied (young, eastman, and oakman, 1991).
</nextsent>
<nextsent>in addition, min (1996) found 0.6% of words mis spelt (447/68966) in 300 email messages, leading to about 12.0% of the 3728 sentences having- errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2063">
<title id=" P98-2142.xml">integrated control of chart items for error repair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mitton (1987) found large proportion of real-word errors were orthographical: to--  too, were ---  where . at the sentential level, types of syntactic errors such as co-occurrence violations, ellipsis, conjunction errors, and extraneous terms have been studied (young, eastman, and oakman, 1991).
</prevsent>
<prevsent>in addition, min (1996) found 0.6% of words mis spelt (447/68966) in 300 email messages, leading to about 12.0% of the 3728 sentences having- errors.
</prevsent>
</prevsection>
<citsent citstr=" A92-1015 ">
various systems have focused on the recovery of ill-formed text at the morpho-syntactic level (vosse, 1992), <papid> A92-1015 </papid>the syntactic level (irons, 1963; lyon, 1974), and the semantic level (fass and wilks, 1983; <papid> J83-3004 </papid>carbonell and hayes, 1983).</citsent>
<aftsection>
<nextsent>those systems identified and repaired errors in various ways, including using grammar-specific rules (meta- rules) (weischedel and sondheimer, 1983), <papid> J83-3003 </papid>least-cost error recovery based on chart parsing (lyon, 1974; anderson and backhouse, 1981), semantic preferences (fass and wilks, 1983), <papid> J83-3004 </papid>and heuristic approaches based on shift-reduce parser (vosse, 1992).<papid> A92-1015 </papid></nextsent>
<nextsent>systems that focus on particular level miss errors that can only be detected using higher level knowledge.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2064">
<title id=" P98-2142.xml">integrated control of chart items for error repair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mitton (1987) found large proportion of real-word errors were orthographical: to--  too, were ---  where . at the sentential level, types of syntactic errors such as co-occurrence violations, ellipsis, conjunction errors, and extraneous terms have been studied (young, eastman, and oakman, 1991).
</prevsent>
<prevsent>in addition, min (1996) found 0.6% of words mis spelt (447/68966) in 300 email messages, leading to about 12.0% of the 3728 sentences having- errors.
</prevsent>
</prevsection>
<citsent citstr=" J83-3004 ">
various systems have focused on the recovery of ill-formed text at the morpho-syntactic level (vosse, 1992), <papid> A92-1015 </papid>the syntactic level (irons, 1963; lyon, 1974), and the semantic level (fass and wilks, 1983; <papid> J83-3004 </papid>carbonell and hayes, 1983).</citsent>
<aftsection>
<nextsent>those systems identified and repaired errors in various ways, including using grammar-specific rules (meta- rules) (weischedel and sondheimer, 1983), <papid> J83-3003 </papid>least-cost error recovery based on chart parsing (lyon, 1974; anderson and backhouse, 1981), semantic preferences (fass and wilks, 1983), <papid> J83-3004 </papid>and heuristic approaches based on shift-reduce parser (vosse, 1992).<papid> A92-1015 </papid></nextsent>
<nextsent>systems that focus on particular level miss errors that can only be detected using higher level knowledge.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2070">
<title id=" P98-2142.xml">integrated control of chart items for error repair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the automatic correction of ill-formed sentences by using integrated information from three levels (lexical, syntactic, and semantic).
</prevsent>
<prevsent>the chapter system (chart parser for two- stage error recovery), performs two-stage error recovery using generalised top-down chart parsing for the syntax phase (cf.
</prevsent>
</prevsection>
<citsent citstr=" P89-1013 ">
mellish, 1989; <papid> P89-1013 </papid>kato, 1994).<papid> A94-1018 </papid></citsent>
<aftsection>
<nextsent>it uses an augmented context-free grammar, which covers verb subcategorisations, pass ives, yes/no and wh- questions, inite relative clauses, and equi/sor phenomena.
</nextsent>
<nextsent>the semantic processing uses conceptual hierarchy and act templates (fass and wilks, 1983), <papid> J83-3004 </papid>that express semantic restrictions.</nextsent>
<nextsent>surface case processing is used to help extract meaning (grishman and peng, 1988) <papid> A88-1009 </papid>by mapping surface cases to their corresponding conceptual cases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2072">
<title id=" P98-2142.xml">integrated control of chart items for error repair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the automatic correction of ill-formed sentences by using integrated information from three levels (lexical, syntactic, and semantic).
</prevsent>
<prevsent>the chapter system (chart parser for two- stage error recovery), performs two-stage error recovery using generalised top-down chart parsing for the syntax phase (cf.
</prevsent>
</prevsection>
<citsent citstr=" A94-1018 ">
mellish, 1989; <papid> P89-1013 </papid>kato, 1994).<papid> A94-1018 </papid></citsent>
<aftsection>
<nextsent>it uses an augmented context-free grammar, which covers verb subcategorisations, pass ives, yes/no and wh- questions, inite relative clauses, and equi/sor phenomena.
</nextsent>
<nextsent>the semantic processing uses conceptual hierarchy and act templates (fass and wilks, 1983), <papid> J83-3004 </papid>that express semantic restrictions.</nextsent>
<nextsent>surface case processing is used to help extract meaning (grishman and peng, 1988) <papid> A88-1009 </papid>by mapping surface cases to their corresponding conceptual cases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2076">
<title id=" P98-2142.xml">integrated control of chart items for error repair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it uses an augmented context-free grammar, which covers verb subcategorisations, pass ives, yes/no and wh- questions, inite relative clauses, and equi/sor phenomena.
</prevsent>
<prevsent>the semantic processing uses conceptual hierarchy and act templates (fass and wilks, 1983), <papid> J83-3004 </papid>that express semantic restrictions.</prevsent>
</prevsection>
<citsent citstr=" A88-1009 ">
surface case processing is used to help extract meaning (grishman and peng, 1988) <papid> A88-1009 </papid>by mapping surface cases to their corresponding conceptual cases.</citsent>
<aftsection>
<nextsent>unlike other systems that 862 have focused on error recovery at particular level (damerau, 1964; mellish, 1989; <papid> P89-1013 </papid>fass and wilks, 1983), <papid> J83-3004 </papid>chapter uses an integrated agenda system, which integrates lexical, syntactic, surface case, and semantic processing.</nextsent>
<nextsent>chapter uses syntactic and semantic information to correct spelling errors detected, including real-word errors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2087">
<title id=" S10-1033.xml">sjtultlab chunk based method for key phrase extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wu and agogino (wu and agogino, 2004)proposed an automated key phrase extraction algorithm using non dominated sorting multi objective genetic algorithm.
</prevsent>
<prevsent>kumar and srinathan(kumar and sri nathan, 2008) used n-gram filtration technique and weight of words for key phrase extraction from scientific articles.
</prevsent>
</prevsection>
<citsent citstr=" W09-2902 ">
for this evaluation task, kim and kan (kimand kan, 2009) <papid> W09-2902 </papid>tackled two major issues in automatic key phrase extraction using scientific articles: candidate selection and feature engineer ing.</citsent>
<aftsection>
<nextsent>they also re-examined the existing features broadly used for the supervised approach.
</nextsent>
<nextsent>different from previous systems, our system uses chunk based method to extract keyphrasesfrom scientific articles.
</nextsent>
<nextsent>domain-specific information is used to find out useful parts in document.the chunk based method is used to extract candidates of key phrases in document.
</nextsent>
<nextsent>keywords of adocument are used to select key phrases from can didates.in the following, section 2 will describe the architecture of the system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2088">
<title id=" S10-1019.xml">ubiu a language independent system for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after concentration on rule-based systems (cf.
</prevsent>
<prevsent>e.g.
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
(mitkov, 1998; <papid> P98-2143 </papid>poesio et al, 2002; markert and nissim, 2005)), <papid> J05-3004 </papid>machine learning methods were embraced (cf.</citsent>
<aftsection>
<nextsent>e.g.
</nextsent>
<nextsent>(soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002)).<papid> P02-1014 </papid></nextsent>
<nextsent>however, machine learning based coreference resolution is only possible for very small number of languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2089">
<title id=" S10-1019.xml">ubiu a language independent system for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after concentration on rule-based systems (cf.
</prevsent>
<prevsent>e.g.
</prevsent>
</prevsection>
<citsent citstr=" J05-3004 ">
(mitkov, 1998; <papid> P98-2143 </papid>poesio et al, 2002; markert and nissim, 2005)), <papid> J05-3004 </papid>machine learning methods were embraced (cf.</citsent>
<aftsection>
<nextsent>e.g.
</nextsent>
<nextsent>(soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002)).<papid> P02-1014 </papid></nextsent>
<nextsent>however, machine learning based coreference resolution is only possible for very small number of languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2090">
<title id=" S10-1019.xml">ubiu a language independent system for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(mitkov, 1998; <papid> P98-2143 </papid>poesio et al, 2002; markert and nissim, 2005)), <papid> J05-3004 </papid>machine learning methods were embraced (cf.</prevsent>
<prevsent>e.g.</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
(soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002)).<papid> P02-1014 </papid></citsent>
<aftsection>
<nextsent>however, machine learning based coreference resolution is only possible for very small number of languages.
</nextsent>
<nextsent>in order to make such resources available for wider range of languages, language independent systems are often regarded as partial solution.
</nextsent>
<nextsent>to this day, there have been only few systems reported that work on multiple languages (mitkov, 1999; harabagiu and maiorano, 2000; <papid> A00-1020 </papid>luo and zitouni, 2005).<papid> H05-1083 </papid></nextsent>
<nextsent>however, all of those systems were geared towards predefined language sets.in this paper, we present language independent system that does require syntactic resources for each language but does not require any effort for adapting the system to new language, except for minimal effort required to adapt the feature extractor to the new language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2091">
<title id=" S10-1019.xml">ubiu a language independent system for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(mitkov, 1998; <papid> P98-2143 </papid>poesio et al, 2002; markert and nissim, 2005)), <papid> J05-3004 </papid>machine learning methods were embraced (cf.</prevsent>
<prevsent>e.g.</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
(soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002)).<papid> P02-1014 </papid></citsent>
<aftsection>
<nextsent>however, machine learning based coreference resolution is only possible for very small number of languages.
</nextsent>
<nextsent>in order to make such resources available for wider range of languages, language independent systems are often regarded as partial solution.
</nextsent>
<nextsent>to this day, there have been only few systems reported that work on multiple languages (mitkov, 1999; harabagiu and maiorano, 2000; <papid> A00-1020 </papid>luo and zitouni, 2005).<papid> H05-1083 </papid></nextsent>
<nextsent>however, all of those systems were geared towards predefined language sets.in this paper, we present language independent system that does require syntactic resources for each language but does not require any effort for adapting the system to new language, except for minimal effort required to adapt the feature extractor to the new language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2092">
<title id=" S10-1019.xml">ubiu a language independent system for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, machine learning based coreference resolution is only possible for very small number of languages.
</prevsent>
<prevsent>in order to make such resources available for wider range of languages, language independent systems are often regarded as partial solution.
</prevsent>
</prevsection>
<citsent citstr=" A00-1020 ">
to this day, there have been only few systems reported that work on multiple languages (mitkov, 1999; harabagiu and maiorano, 2000; <papid> A00-1020 </papid>luo and zitouni, 2005).<papid> H05-1083 </papid></citsent>
<aftsection>
<nextsent>however, all of those systems were geared towards predefined language sets.in this paper, we present language independent system that does require syntactic resources for each language but does not require any effort for adapting the system to new language, except for minimal effort required to adapt the feature extractor to the new language.
</nextsent>
<nextsent>the system was completely developed within 4 months, and will be extended to new languages in the future.
</nextsent>
<nextsent>the ubiu system aims at being language independent system in that it uses combination of machine learning, in the form of memory-based learning (mbl) in the implementation of timbl(daelemans et al, 2007), and language independent features.
</nextsent>
<nextsent>mbl uses similarity metric to find the nearest neighbors in the training data in order to classify new example, and it has been shown to work well for nlp problems (daelemans and vanden bosch, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2093">
<title id=" S10-1019.xml">ubiu a language independent system for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, machine learning based coreference resolution is only possible for very small number of languages.
</prevsent>
<prevsent>in order to make such resources available for wider range of languages, language independent systems are often regarded as partial solution.
</prevsent>
</prevsection>
<citsent citstr=" H05-1083 ">
to this day, there have been only few systems reported that work on multiple languages (mitkov, 1999; harabagiu and maiorano, 2000; <papid> A00-1020 </papid>luo and zitouni, 2005).<papid> H05-1083 </papid></citsent>
<aftsection>
<nextsent>however, all of those systems were geared towards predefined language sets.in this paper, we present language independent system that does require syntactic resources for each language but does not require any effort for adapting the system to new language, except for minimal effort required to adapt the feature extractor to the new language.
</nextsent>
<nextsent>the system was completely developed within 4 months, and will be extended to new languages in the future.
</nextsent>
<nextsent>the ubiu system aims at being language independent system in that it uses combination of machine learning, in the form of memory-based learning (mbl) in the implementation of timbl(daelemans et al, 2007), and language independent features.
</nextsent>
<nextsent>mbl uses similarity metric to find the nearest neighbors in the training data in order to classify new example, and it has been shown to work well for nlp problems (daelemans and vanden bosch, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2094">
<title id=" S10-1090.xml">gplsiixa using semantic classes to acquire monosemous training examples from domain texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the evaluation reveal that the best results are achieved training with semcor and the background examples from monosemous words, obtaining results above the first sense baseline and the fifth best position in the competition rank.
</prevsent>
<prevsent>as empirically demonstrated by the last senseval and semeval exercises, assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed.
</prevsent>
</prevsection>
<citsent citstr=" W00-1322 ">
in fact, supervised word-based wsd systems are very dependent of the corpora used for training and testing the system (escudero et al , 2000).<papid> W00-1322 </papid></citsent>
<aftsection>
<nextsent>one possible reason could be the use of inappropriate level of abstraction.
</nextsent>
<nextsent>most supervised systems simply model each polysemous word as classification problem where each class corresponds to particular synset of the word.
</nextsent>
<nextsent>but, wordnet (wn) has been widely criticized for being sense repository that often provides too fine grained sense distinctions for higher level applications like machine translation or question &amp; answering.
</nextsent>
<nextsent>in fact, wsd at this level of granularity has resisted all attempts of inferring robust broad-coverage models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2095">
<title id=" S10-1090.xml">gplsiixa using semantic classes to acquire monosemous training examples from domain texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but, wordnet (wn) has been widely criticized for being sense repository that often provides too fine grained sense distinctions for higher level applications like machine translation or question &amp; answering.
</prevsent>
<prevsent>in fact, wsd at this level of granularity has resisted all attempts of inferring robust broad-coverage models.
</prevsent>
</prevsection>
<citsent citstr=" P06-1014 ">
it seems that many word sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word sense annotated ex amples.thus, some research has been focused on deriving different word-sense groupings to overcome the fine grained distinctions of wn (hearst and schutze, 1993), (peters et al , 1998), (mihalceaand moldovan, 2001), (agirre and lopez dela calle, 2003), (navigli, 2006) <papid> P06-1014 </papid>and (snow et al , 2007).<papid> D07-1107 </papid></citsent>
<aftsection>
<nextsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.
</nextsent>
<nextsent>in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005), (<papid> P05-1004 </papid>kohomban and lee, 2005) <papid> P05-1005 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></nextsent>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2096">
<title id=" S10-1090.xml">gplsiixa using semantic classes to acquire monosemous training examples from domain texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but, wordnet (wn) has been widely criticized for being sense repository that often provides too fine grained sense distinctions for higher level applications like machine translation or question &amp; answering.
</prevsent>
<prevsent>in fact, wsd at this level of granularity has resisted all attempts of inferring robust broad-coverage models.
</prevsent>
</prevsection>
<citsent citstr=" D07-1107 ">
it seems that many word sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word sense annotated ex amples.thus, some research has been focused on deriving different word-sense groupings to overcome the fine grained distinctions of wn (hearst and schutze, 1993), (peters et al , 1998), (mihalceaand moldovan, 2001), (agirre and lopez dela calle, 2003), (navigli, 2006) <papid> P06-1014 </papid>and (snow et al , 2007).<papid> D07-1107 </papid></citsent>
<aftsection>
<nextsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.
</nextsent>
<nextsent>in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005), (<papid> P05-1004 </papid>kohomban and lee, 2005) <papid> P05-1005 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></nextsent>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2097">
<title id=" S10-1090.xml">gplsiixa using semantic classes to acquire monosemous training examples from domain texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it seems that many word sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word sense annotated ex amples.thus, some research has been focused on deriving different word-sense groupings to overcome the fine grained distinctions of wn (hearst and schutze, 1993), (peters et al , 1998), (mihalceaand moldovan, 2001), (agirre and lopez dela calle, 2003), (navigli, 2006) <papid> P06-1014 </papid>and (snow et al , 2007).<papid> D07-1107 </papid></prevsent>
<prevsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.</prevsent>
</prevsection>
<citsent citstr=" W97-0811 ">
in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005), (<papid> P05-1004 </papid>kohomban and lee, 2005) <papid> P05-1005 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></citsent>
<aftsection>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.
</nextsent>
<nextsent>mostof the later approaches used the original lexico graphical files of wn (more recently called supersenses) as very coarse grained sense distinctions.
</nextsent>
<nextsent>we suspect that selecting the appropriate level of abstraction could be on between both levels.
</nextsent>
<nextsent>thus, we use the semantic classes modeled by the basic level concepts 1 (blc) (izquierdo et al ,2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2098">
<title id=" S10-1090.xml">gplsiixa using semantic classes to acquire monosemous training examples from domain texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it seems that many word sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word sense annotated ex amples.thus, some research has been focused on deriving different word-sense groupings to overcome the fine grained distinctions of wn (hearst and schutze, 1993), (peters et al , 1998), (mihalceaand moldovan, 2001), (agirre and lopez dela calle, 2003), (navigli, 2006) <papid> P06-1014 </papid>and (snow et al , 2007).<papid> D07-1107 </papid></prevsent>
<prevsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.</prevsent>
</prevsection>
<citsent citstr=" W03-1022 ">
in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005), (<papid> P05-1004 </papid>kohomban and lee, 2005) <papid> P05-1005 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></citsent>
<aftsection>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.
</nextsent>
<nextsent>mostof the later approaches used the original lexico graphical files of wn (more recently called supersenses) as very coarse grained sense distinctions.
</nextsent>
<nextsent>we suspect that selecting the appropriate level of abstraction could be on between both levels.
</nextsent>
<nextsent>thus, we use the semantic classes modeled by the basic level concepts 1 (blc) (izquierdo et al ,2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2099">
<title id=" S10-1090.xml">gplsiixa using semantic classes to acquire monosemous training examples from domain texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it seems that many word sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word sense annotated ex amples.thus, some research has been focused on deriving different word-sense groupings to overcome the fine grained distinctions of wn (hearst and schutze, 1993), (peters et al , 1998), (mihalceaand moldovan, 2001), (agirre and lopez dela calle, 2003), (navigli, 2006) <papid> P06-1014 </papid>and (snow et al , 2007).<papid> D07-1107 </papid></prevsent>
<prevsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.</prevsent>
</prevsection>
<citsent citstr=" P05-1004 ">
in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005), (<papid> P05-1004 </papid>kohomban and lee, 2005) <papid> P05-1005 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></citsent>
<aftsection>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.
</nextsent>
<nextsent>mostof the later approaches used the original lexico graphical files of wn (more recently called supersenses) as very coarse grained sense distinctions.
</nextsent>
<nextsent>we suspect that selecting the appropriate level of abstraction could be on between both levels.
</nextsent>
<nextsent>thus, we use the semantic classes modeled by the basic level concepts 1 (blc) (izquierdo et al ,2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2100">
<title id=" S10-1090.xml">gplsiixa using semantic classes to acquire monosemous training examples from domain texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it seems that many word sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word sense annotated ex amples.thus, some research has been focused on deriving different word-sense groupings to overcome the fine grained distinctions of wn (hearst and schutze, 1993), (peters et al , 1998), (mihalceaand moldovan, 2001), (agirre and lopez dela calle, 2003), (navigli, 2006) <papid> P06-1014 </papid>and (snow et al , 2007).<papid> D07-1107 </papid></prevsent>
<prevsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.</prevsent>
</prevsection>
<citsent citstr=" P05-1005 ">
in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005), (<papid> P05-1004 </papid>kohomban and lee, 2005) <papid> P05-1005 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></citsent>
<aftsection>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.
</nextsent>
<nextsent>mostof the later approaches used the original lexico graphical files of wn (more recently called supersenses) as very coarse grained sense distinctions.
</nextsent>
<nextsent>we suspect that selecting the appropriate level of abstraction could be on between both levels.
</nextsent>
<nextsent>thus, we use the semantic classes modeled by the basic level concepts 1 (blc) (izquierdo et al ,2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2101">
<title id=" S10-1090.xml">gplsiixa using semantic classes to acquire monosemous training examples from domain texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it seems that many word sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word sense annotated ex amples.thus, some research has been focused on deriving different word-sense groupings to overcome the fine grained distinctions of wn (hearst and schutze, 1993), (peters et al , 1998), (mihalceaand moldovan, 2001), (agirre and lopez dela calle, 2003), (navigli, 2006) <papid> P06-1014 </papid>and (snow et al , 2007).<papid> D07-1107 </papid></prevsent>
<prevsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.</prevsent>
</prevsection>
<citsent citstr=" W06-1670 ">
in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005), (<papid> P05-1004 </papid>kohomban and lee, 2005) <papid> P05-1005 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></citsent>
<aftsection>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.
</nextsent>
<nextsent>mostof the later approaches used the original lexico graphical files of wn (more recently called supersenses) as very coarse grained sense distinctions.
</nextsent>
<nextsent>we suspect that selecting the appropriate level of abstraction could be on between both levels.
</nextsent>
<nextsent>thus, we use the semantic classes modeled by the basic level concepts 1 (blc) (izquierdo et al ,2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2102">
<title id=" S10-1090.xml">gplsiixa using semantic classes to acquire monosemous training examples from domain texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we suspect that selecting the appropriate level of abstraction could be on between both levels.
</prevsent>
<prevsent>thus, we use the semantic classes modeled by the basic level concepts 1 (blc) (izquierdo et al ,2007).
</prevsent>
</prevsection>
<citsent citstr=" E09-1045 ">
our previous research using blc empirically demonstrated that this automatically derived 1 http://adimen.si.ehu.es/web/blc 402 set of meanings groups senses into an adequate level of abstraction in order to perform class-based word sense disambiguation (wsd) (izquierdo et al ., 2009).<papid> E09-1045 </papid></citsent>
<aftsection>
<nextsent>now, we also show that class-basedwsd allows to successfully incorporate monose mous examples from the domain text.
</nextsent>
<nextsent>in fact, the robustness of our class-based wsd approach is shown by our system that just uses the sem cor examples (sc).
</nextsent>
<nextsent>it performs without any kind of domain adaptation as the most frequent sense (mfs) baseline.
</nextsent>
<nextsent>this paper describes our participation in semeval-2010 task 17 (agirre et al , 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2112">
<title id=" S12-1100.xml">uow semantically informed text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we train the regression algorithm with different classes of similarity metrics: i) lexical,ii) syntactic and iii) semantic.
</prevsent>
<prevsent>the lexical similarity metrics are: i) cosine similarity using bag-of words representation, and ii) precision, recall and f-measure of content words.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the syntactic metric computes bleu (papineni et al, 2002), <papid> P02-1040 </papid>machine translation evaluation metric, over labels of base phrases (chunks).</citsent>
<aftsection>
<nextsent>two semantic metrics are used: metric based on the preservation of named entities and tine (rios et al, 2011).<papid> W11-2112 </papid></nextsent>
<nextsent>named entities are matched by type and content: while the type has tomatch exactly, the content is compared with the assistance of distributional thesaurus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2113">
<title id=" S12-1100.xml">uow semantically informed text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the lexical similarity metrics are: i) cosine similarity using bag-of words representation, and ii) precision, recall and f-measure of content words.
</prevsent>
<prevsent>the syntactic metric computes bleu (papineni et al, 2002), <papid> P02-1040 </papid>machine translation evaluation metric, over labels of base phrases (chunks).</prevsent>
</prevsection>
<citsent citstr=" W11-2112 ">
two semantic metrics are used: metric based on the preservation of named entities and tine (rios et al, 2011).<papid> W11-2112 </papid></citsent>
<aftsection>
<nextsent>named entities are matched by type and content: while the type has tomatch exactly, the content is compared with the assistance of distributional thesaurus.
</nextsent>
<nextsent>tine is metric proposed to measure adequacy in machine translation and favors similar semantic frames.
</nextsent>
<nextsent>tine attempts to align verb predicates, assuming one to-one correspondence between semantic roles, and considering ontologies for inexact alignment.
</nextsent>
<nextsent>the surface realization of the arguments is compared using distributional thesaurus and the cosine similarity metric.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2114">
<title id=" S12-1100.xml">uow semantically informed text similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tine attempts to align verb predicates, assuming one to-one correspondence between semantic roles, and considering ontologies for inexact alignment.
</prevsent>
<prevsent>the surface realization of the arguments is compared using distributional thesaurus and the cosine similarity metric.
</prevsent>
</prevsection>
<citsent citstr=" W10-1751 ">
finally, we use meteor (denkowskiand lavie, 2010), <papid> W10-1751 </papid>also common metric for machine translation evaluation, that also computes inexact word overlap as at way of measuring the impact of our semantic metrics.</citsent>
<aftsection>
<nextsent>the lexical and syntactic metrics complement the semantic metrics in dealing with the phenomena observed in the tasks dataset.
</nextsent>
<nextsent>for instance, from the msrvid dataset: s1 two men are playing football.
</nextsent>
<nextsent>s2 two men are practicing football.in this case, as typical of paraphrasing, the situation and participants are the same while the surface realization differs, but playing can be considered similar to practicing.
</nextsent>
<nextsent>from the smt-eur dataset: s3 the council of europe, along with the court of human rights, has wealth of experience of such forms of supervision, and we can build on these.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2115">
<title id=" S12-1100.xml">uow semantically informed text similarity </title>
<section> similarity metrics.  </section>
<citcontext>
<prevsection>
<prevsent>the goal of the metric is to deal with synonym entities.
</prevsent>
<prevsent>first, named entities are grouped by class (e.g.organization), and then the content of the named entities within the same classes is compared throughcosine similarity.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
if the surface realization is different, we retrieve words that share the same context with the named entity using dekang lins distributional thesaurus (lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>therefore, the cosine similarity will have more information than just the named entities themselves.
</nextsent>
<nextsent>for example, from the sentence pair s9 and s10: s9 companies include ibm corp. ...
</nextsent>
<nextsent>674s10 companies include international business machines ...
</nextsent>
<nextsent>the entity from s9: ibm corp. and the entity from s10: international business machines have the same tag organization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2117">
<title id=" S12-1100.xml">uow semantically informed text similarity </title>
<section> similarity metrics.  </section>
<citcontext>
<prevsection>
<prevsent>the metric complements lexical matching with shallow semantic component to better address adequacy in machine translation evaluation.
</prevsent>
<prevsent>the main contribution of such metric is to provide more flexible way of measuring the overlap between shallow semantic representations (semantic role la bels) that considers both the semantic structure ofthe sentence and the content of the semantic components.
</prevsent>
</prevsection>
<citsent citstr=" W04-3205 ">
this metric allows to match synonym predicates by using verb ontologies such as verbnet (schuler, 2006) and verb ocean (chklovski and pantel, 2004) <papid> W04-3205 </papid>and distributional semantics similarity metrics, suchas dekang lins thesaurus (lin, 1998), <papid> P98-2127 </papid>where previous semantic metrics only perform exact match of predicate structures and arguments.</citsent>
<aftsection>
<nextsent>for example, in verbnet the verbs spook and terrify share the same class amuse-31.1, and in verb ocean the verb dress is related to the verb wear, so these are considered matches in tine.
</nextsent>
<nextsent>the main sources of errors in this metric are the matching of unrelated verbs and the lack of coverage of the ontologies.
</nextsent>
<nextsent>for example, for s11 and s12, remain and say are (incorrectly) related as given by verbocean.
</nextsent>
<nextsent>s11 if snowfalls on the slopes this week, christmas will sell out too, says schiefert.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2119">
<title id=" S10-1068.xml">372comparing the benefit of different dependency parsers for textual entailment using syntactic constraints only </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the most widespread method is to evaluate the number of correctly recognized units according to certain gold standard.
</prevsent>
<prevsent>for depend ency-based units unlabeled or labeled attachment scores (percentage of correctly classified dependency relations, either with or without the dependency relation type) are usually used (cf.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
buchholz and marsi, 2006).<papid> W06-2920 </papid></citsent>
<aftsection>
<nextsent>however, parsing is very rarely goal in itself.
</nextsent>
<nextsent>in most cases it is necessary preprocessing step for certain application.
</nextsent>
<nextsent>therefore it is usually not the best option to decide which parser suits one goals best by purely looking on its performance on some standard test dataset.
</nextsent>
<nextsent>it is rather more sensible to analyse whether the parser is able to recognise those syntactic units or relations, which are most relevant for one applica tion.the shared task #12 pete in the semeval 2010 evaluation exercises on semantic evalu-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2120">
<title id=" S12-1088.xml">unitor combining semantic text similarity functions through sv regression </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an effective method to compute similarity between short texts or sentences has many applications in natural language processing (mihalcea et al, 2006) and related areas such as information retrieval, e.g. to improve the effectiveness of semantic search engine (sahami and heilman, 2006), or databases, where text similarity can be used in schema matching to solve semantic heterogeneity (islam and inkpen, 2008).sts is here modeled as support vector (sv) regression problem, where sv regress or learns the similarity function over text pairs.
</prevsent>
<prevsent>regression learning has been already applied to different nlp tasks.
</prevsent>
</prevsection>
<citsent citstr=" P05-1015 ">
in (pang and lee, 2005) <papid> P05-1015 </papid>it is applied to opinion mining, in particular to the rating-inference problem, wherein one must determine an author evaluation with respect to multi-point scale.</citsent>
<aftsection>
<nextsent>in (albrechtand hwa, 2007) <papid> P07-1038 </papid>method is proposed for developing sentence-level mt evaluation metrics using regression learning without directly relying on human reference translations.</nextsent>
<nextsent>in (biadsy et al, 2008) <papid> P08-1092 </papid>it has been used to rank candidate sentences for the task of producing biographies from wikipedia.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2121">
<title id=" S12-1088.xml">unitor combining semantic text similarity functions through sv regression </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>regression learning has been already applied to different nlp tasks.
</prevsent>
<prevsent>in (pang and lee, 2005) <papid> P05-1015 </papid>it is applied to opinion mining, in particular to the rating-inference problem, wherein one must determine an author evaluation with respect to multi-point scale.</prevsent>
</prevsection>
<citsent citstr=" P07-1038 ">
in (albrechtand hwa, 2007) <papid> P07-1038 </papid>method is proposed for developing sentence-level mt evaluation metrics using regression learning without directly relying on human reference translations.</citsent>
<aftsection>
<nextsent>in (biadsy et al, 2008) <papid> P08-1092 </papid>it has been used to rank candidate sentences for the task of producing biographies from wikipedia.</nextsent>
<nextsent>finally, in (becker et al, 2011) sv regress or has been usedto rank questions within their context in the multimodal tutorial dialogue problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2122">
<title id=" S12-1088.xml">unitor combining semantic text similarity functions through sv regression </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (pang and lee, 2005) <papid> P05-1015 </papid>it is applied to opinion mining, in particular to the rating-inference problem, wherein one must determine an author evaluation with respect to multi-point scale.</prevsent>
<prevsent>in (albrechtand hwa, 2007) <papid> P07-1038 </papid>method is proposed for developing sentence-level mt evaluation metrics using regression learning without directly relying on human reference translations.</prevsent>
</prevsection>
<citsent citstr=" P08-1092 ">
in (biadsy et al, 2008) <papid> P08-1092 </papid>it has been used to rank candidate sentences for the task of producing biographies from wikipedia.</citsent>
<aftsection>
<nextsent>finally, in (becker et al, 2011) sv regress or has been usedto rank questions within their context in the multimodal tutorial dialogue problem.
</nextsent>
<nextsent>in this paper, the semantic relatedness between two sentences is modeled as combination of different similarity functions, each describing the analogy between the two texts according to specific semantic perspective: in this way, we aim at capturing syntactic and lexical equivalences between sentences and exploiting either topical relatedness or paradigmatic similarity between individual words.
</nextsent>
<nextsent>the variety of semantic evidences that system can employ here grows quickly, according to the genre and complexity of the targeted sentences.
</nextsent>
<nextsent>we thus propose to combine such body of evidence to learna comprehensive scoring function = f(~x) over individual measures from labeled data through sv re gression: is the gold similarity score (provided byhuman annotators), while ~x is the vector of the different individual scores, provided by the chosen similarity functions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2123">
<title id=" S12-1088.xml">unitor combining semantic text similarity functions through sv regression </title>
<section> combining different similarity function.  </section>
<citcontext>
<prevsection>
<prevsent>this makes sentence similarity to depend on the set of individual compounds, e.g. subject-verb relationship instances.
</prevsent>
<prevsent>while basic lexical information can still be obtained by distributional analysis, phrase level figure 1: example of dependency graph similarity can be here modeled as specific function of the co-occurring words, i.e. complex algebraic composition of their corresponding word vectors.
</prevsent>
</prevsection>
<citsent citstr=" W10-2802 ">
differently from the document-oriented case used in the lsa function, base lexical vectors are here derived from co-occurrence counts in word space, built according to the method discussed in(sahlgren, 2006; croce and previtali, 2010).<papid> W10-2802 </papid></citsent>
<aftsection>
<nextsent>in order to keep dimensionality as low as possible, svd is also applied here (annesi et al, 2012).
</nextsent>
<nextsent>the result is that every noun, verb, adjective and adverb is then projected in the reduced word space and then different composition functions can be applied as discussed in (mitchell and lapata, 2010) or (annesi et al., 2012).convolution kernel-based similarity.
</nextsent>
<nextsent>the similarity function is here the smoothed partial tree kernel (sptk) proposed in (croce et al, 2011).<papid> D11-1096 </papid></nextsent>
<nextsent>this convolution kernel estimates the similarity between sentences, according to the syntactic and lexical information in both sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2124">
<title id=" S12-1088.xml">unitor combining semantic text similarity functions through sv regression </title>
<section> combining different similarity function.  </section>
<citcontext>
<prevsection>
<prevsent>in order to keep dimensionality as low as possible, svd is also applied here (annesi et al, 2012).
</prevsent>
<prevsent>the result is that every noun, verb, adjective and adverb is then projected in the reduced word space and then different composition functions can be applied as discussed in (mitchell and lapata, 2010) or (annesi et al., 2012).convolution kernel-based similarity.
</prevsent>
</prevsection>
<citsent citstr=" D11-1096 ">
the similarity function is here the smoothed partial tree kernel (sptk) proposed in (croce et al, 2011).<papid> D11-1096 </papid></citsent>
<aftsection>
<nextsent>this convolution kernel estimates the similarity between sentences, according to the syntactic and lexical information in both sentences.
</nextsent>
<nextsent>syntactic representation of sentence like man is riding bicycle?
</nextsent>
<nextsent>is derived from the dependency parse tree, as shownin fig.
</nextsent>
<nextsent>1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2131">
<title id=" S12-1088.xml">unitor combining semantic text similarity functions through sv regression </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the tree representation described in sec.
</prevsent>
<prevsent>2.1 allows to define 3 different kernels, i.e. sptkloct , sptklct and sptkgrct . similarity between lexical nodes is estimated as the cosine similarity in the co-occurrence word space described above, as in (croce et al, 2011).<papid> D11-1096 </papid></prevsent>
</prevsection>
<citsent citstr=" W07-2048 ">
in all corpus analysis and experiments, sentences are processed with the lth dependency parser, described in (johansson and nugues, 2007), <papid> W07-2048 </papid>for part of-speech tagging and lemmatization.</citsent>
<aftsection>
<nextsent>dependency parsing of datasets is required for the sptk application.
</nextsent>
<nextsent>finally, svm-lighttk is employed for thesv regression learning to combine specific similarity functions.
</nextsent>
<nextsent>3.2 evaluating the impact of unsupervised.
</nextsent>
<nextsent>model stable 1 compares the pearson correlation of different similarity functions described in section 2.1, i.e. mainly the results of the unsupervised approaches, against the challenge training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2135">
<title id=" S12-1092.xml">fbk machine translation evaluation and word similarity metrics for semantic textual similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our best model achieves 60.77% correlation with human judgements (mean score) and ranked 20out of 88 submitted runs in the mean ranking, where the average correlation across all the sub-portions of the test set is considered.
</prevsent>
<prevsent>the semantic textual similarity (sts) task proposed at semeval 2012 consists of examining the degree of semantic equivalence between two sentences and assigning score to quantify such similarity ranging from 0 (the two texts are about different topics) to 5 (the two texts are semantically equivalent).
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
the complete description of the task, the datasets and the evaluation methodology adopted can be found in (agirre et al, 2012).<papid> S12-1051 </papid></citsent>
<aftsection>
<nextsent>typical approaches to measure semantic textual similarity exploit information at the lexical level.
</nextsent>
<nextsent>the proposed solutions range from calculating the overlap of common words between the two text segments (salton et al, 1997) to the application of knowledge-based and corpus-based word similarity metrics to cope with the low recall achieved by on simple lexical matching (mihalcea et al, 2006).
</nextsent>
<nextsent>our participation in the sts task is inspired by previous work on paraphrase recognition, in which machine translation (mt) evaluation metrics are used to identify whether pair of sentences are semantically equivalent or not (finch and hwang, 2005; wan et al, 2006).
</nextsent>
<nextsent>our approach to semantic textual similarity makes use of not only lexical information but also syntactic and semantic information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2139">
<title id=" S12-1092.xml">fbk machine translation evaluation and word similarity metrics for semantic textual similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.1.1 lexical level at the lexical level we explored different n-gram and edit distance based metrics.
</prevsent>
<prevsent>the difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
we used the following n-gram-basedmetrics: bleu (papineni et al, 2002), <papid> P02-1040 </papid>nist (dod dington, 2002), rouge (lin and och, 2004), <papid> P04-1077 </papid>gtm (melamed et al, 2003), <papid> N03-2021 </papid>meteor (banerjee and lavie, 2005).<papid> W05-0909 </papid></citsent>
<aftsection>
<nextsent>besides those, we also used metrics based on edit distance.
</nextsent>
<nextsent>such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/into the other (the lower the number of edit operations, the higher the similarity score).
</nextsent>
<nextsent>the edit distance-based metrics used were: wer (nie?
</nextsent>
<nextsent>en et al., 2000), per (tillmann et al, 1997), ter (snover et al, 2006) and ter-plus (snover et al, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2140">
<title id=" S12-1092.xml">fbk machine translation evaluation and word similarity metrics for semantic textual similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.1.1 lexical level at the lexical level we explored different n-gram and edit distance based metrics.
</prevsent>
<prevsent>the difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results.
</prevsent>
</prevsection>
<citsent citstr=" P04-1077 ">
we used the following n-gram-basedmetrics: bleu (papineni et al, 2002), <papid> P02-1040 </papid>nist (dod dington, 2002), rouge (lin and och, 2004), <papid> P04-1077 </papid>gtm (melamed et al, 2003), <papid> N03-2021 </papid>meteor (banerjee and lavie, 2005).<papid> W05-0909 </papid></citsent>
<aftsection>
<nextsent>besides those, we also used metrics based on edit distance.
</nextsent>
<nextsent>such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/into the other (the lower the number of edit operations, the higher the similarity score).
</nextsent>
<nextsent>the edit distance-based metrics used were: wer (nie?
</nextsent>
<nextsent>en et al., 2000), per (tillmann et al, 1997), ter (snover et al, 2006) and ter-plus (snover et al, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2141">
<title id=" S12-1092.xml">fbk machine translation evaluation and word similarity metrics for semantic textual similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.1.1 lexical level at the lexical level we explored different n-gram and edit distance based metrics.
</prevsent>
<prevsent>the difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results.
</prevsent>
</prevsection>
<citsent citstr=" N03-2021 ">
we used the following n-gram-basedmetrics: bleu (papineni et al, 2002), <papid> P02-1040 </papid>nist (dod dington, 2002), rouge (lin and och, 2004), <papid> P04-1077 </papid>gtm (melamed et al, 2003), <papid> N03-2021 </papid>meteor (banerjee and lavie, 2005).<papid> W05-0909 </papid></citsent>
<aftsection>
<nextsent>besides those, we also used metrics based on edit distance.
</nextsent>
<nextsent>such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/into the other (the lower the number of edit operations, the higher the similarity score).
</nextsent>
<nextsent>the edit distance-based metrics used were: wer (nie?
</nextsent>
<nextsent>en et al., 2000), per (tillmann et al, 1997), ter (snover et al, 2006) and ter-plus (snover et al, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2142">
<title id=" S12-1092.xml">fbk machine translation evaluation and word similarity metrics for semantic textual similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.1.1 lexical level at the lexical level we explored different n-gram and edit distance based metrics.
</prevsent>
<prevsent>the difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results.
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
we used the following n-gram-basedmetrics: bleu (papineni et al, 2002), <papid> P02-1040 </papid>nist (dod dington, 2002), rouge (lin and och, 2004), <papid> P04-1077 </papid>gtm (melamed et al, 2003), <papid> N03-2021 </papid>meteor (banerjee and lavie, 2005).<papid> W05-0909 </papid></citsent>
<aftsection>
<nextsent>besides those, we also used metrics based on edit distance.
</nextsent>
<nextsent>such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/into the other (the lower the number of edit operations, the higher the similarity score).
</nextsent>
<nextsent>the edit distance-based metrics used were: wer (nie?
</nextsent>
<nextsent>en et al., 2000), per (tillmann et al, 1997), ter (snover et al, 2006) and ter-plus (snover et al, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2143">
<title id=" S12-1092.xml">fbk machine translation evaluation and word similarity metrics for semantic textual similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.1.2 syntactic level the syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp).
</prevsent>
<prevsent>constituency trees we reproduced by the max-ent reranking parser (char niak, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W05-0904 ">
the constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of given part-of-speech; matching in function of given constituency type; and syntactic tree matching (stm) metric proposed by (liu and gildea, 2005).<papid> W05-0904 </papid>dependency trees were obtained using mini par (lin, 2003).</citsent>
<aftsection>
<nextsent>two types of metrics were used to calculate the similarity between two texts using dependency trees.
</nextsent>
<nextsent>in the first, different similarity measures were calculated taking into consideration three different perspectives: overlap of words that hang in the same level or in deeper level of the dependency tree; overlap between words that hang directly from terminal nodes given specified part of-speech; and overlap between words that are ruledby non-terminal nodes given specified grammatical relation (subject, object, relative clause, among others).
</nextsent>
<nextsent>the second type is an implementation of the head-word chain matching introduced in (liu and gildea, 2005).<papid> W05-0904 </papid></nextsent>
<nextsent>the shallow syntax approach proposed by(gimenez, 2008) uses three different tools to explore the parts-of-speech, word lemmas and base phrases chunks, respectively: svmtool (gimenez and ma`rquez, 2004), free ling (carreras et al, 2004) and phreco (carreras et al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2147">
<title id=" S12-1092.xml">fbk machine translation evaluation and word similarity metrics for semantic textual similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.1.3 semantic level at the semantic level we aplored three different types of information, namely: discourse representations, named entities and semantic roles.
</prevsent>
<prevsent>here after they are respectively referred to as dr, ne, andsr features.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
the discourse relations are automatically annotated using the c&c; tools (clark and curran, 2004).<papid> P04-1014 </papid></citsent>
<aftsection>
<nextsent>the following metrics using semantic tree representations were proposed by (gimenez,2008).
</nextsent>
<nextsent>a metric similar to the stm in which semantic trees are used instead of constituency trees;the overlapping between discourse representation structures according to their type; and the morphosyntactic overlapping of discourse representation structures that share the same type.named entities metrics are calculated by comparing the entities that appear in each text.
</nextsent>
<nextsent>the named entities were annotated using the bios package (surdeanu et al, 2005).
</nextsent>
<nextsent>two types of metrics were used: the overlapping between the named entities in each sentence according to their type andthe matching between the named entities in function of their type.semantic roles were automatically annotated using the swirl package (surdeanu and turmo, 2005).<papid> W05-0635 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2148">
<title id=" S12-1092.xml">fbk machine translation evaluation and word similarity metrics for semantic textual similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>a metric similar to the stm in which semantic trees are used instead of constituency trees;the overlapping between discourse representation structures according to their type; and the morphosyntactic overlapping of discourse representation structures that share the same type.named entities metrics are calculated by comparing the entities that appear in each text.
</prevsent>
<prevsent>the named entities were annotated using the bios package (surdeanu et al, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W05-0635 ">
two types of metrics were used: the overlapping between the named entities in each sentence according to their type andthe matching between the named entities in function of their type.semantic roles were automatically annotated using the swirl package (surdeanu and turmo, 2005).<papid> W05-0635 </papid></citsent>
<aftsection>
<nextsent>the arguments and adjuncts annotated ineach sentence are compared according to three different metrics: overlapping between the semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical realization.
</nextsent>
<nextsent>2.2 word similarity metrics.
</nextsent>
<nextsent>besides the mt evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics.
</nextsent>
<nextsent>for that, we followed distributional and knowledge-based word similarity approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2149">
<title id=" S12-1092.xml">fbk machine translation evaluation and word similarity metrics for semantic textual similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>besides the mt evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics.
</prevsent>
<prevsent>for that, we followed distributional and knowledge-based word similarity approach.
</prevsent>
</prevsection>
<citsent citstr=" N10-1146 ">
2.2.1 distributional word similarity as some previous work on semantic textual textual similarity (mihalcea et al, 2006) and textual entailment (kouylekov et al, 2010; mehdad et al, 2010) <papid> N10-1146 </papid>have shown, distributional word similarity measures can improve the performance of both tasks by allowing matches between terms that are lexicallydifferent.</citsent>
<aftsection>
<nextsent>we measure the word similarity computing set of latent semantic analysis (lsa) metrics over wikipedia.
</nextsent>
<nextsent>the 200,000 most visited articles of wikipedia were extracted and cleaned to build the 626 term-by-document matrix using the jlsi tool2.using this model we designed three different similarity metrics that compute the similarity between all elements in one text with all elements in the other text.
</nextsent>
<nextsent>for two metrics we calculate the similarities between different parts-of-speech: (i) similarity over nouns and adjectives, and (ii) similarity over verbs.
</nextsent>
<nextsent>the third metric computes the similarity between all words in the two sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2150">
<title id=" S12-1092.xml">fbk machine translation evaluation and word similarity metrics for semantic textual similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>yago2 contains knowledge about 10 million entities and more than 120 million facts about these entities.in order to link the entities in the text to the entities in yago2 we have used the wiki machine?
</prevsent>
<prevsent>(twm) tool4.
</prevsent>
</prevsection>
<citsent citstr=" J09-4007 ">
the tool solves the linking problem by disambiguating each entity mention in the text (excluding pronouns) using wikipedia to provide the sense inventory and the training data (giuliano et al., 2009).<papid> J09-4007 </papid></citsent>
<aftsection>
<nextsent>after preprocessing the datasets with twm the entities are annotated with their respective wikipedia entries represented by their urls.
</nextsent>
<nextsent>using the entitys url it is possible to retrieve the wordnet synsets related to the entitys entry in yago2 and explore different knowledge-based metrics to compute word similarity between entities.in our experiments we selected three different algorithms to calculate word similarity using yago2: wu-palmer (zhibiao and palmer, 1994), the leacock-chodorow (leacock et al, 1998) <papid> J98-1006 </papid>and 2http://hlt.fbk.eu/en/technology/jlsi 3http://www.geonames.org/ 4http://thewikimachine.fbk.eu/html/ index.html the path distance (score based on the shortest path that connects the senses in the wordnet hyper nym/hyponym taxonomy).</nextsent>
<nextsent>two classes of metrics were designed: (i) the average of the similarity between all the entities in each sentence and (ii) the similarity of the pair of elements which have the shortest path in the wordnet taxonomy among all possible pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2151">
<title id=" S12-1092.xml">fbk machine translation evaluation and word similarity metrics for semantic textual similarity </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the tool solves the linking problem by disambiguating each entity mention in the text (excluding pronouns) using wikipedia to provide the sense inventory and the training data (giuliano et al., 2009).<papid> J09-4007 </papid></prevsent>
<prevsent>after preprocessing the datasets with twm the entities are annotated with their respective wikipedia entries represented by their urls.</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
using the entitys url it is possible to retrieve the wordnet synsets related to the entitys entry in yago2 and explore different knowledge-based metrics to compute word similarity between entities.in our experiments we selected three different algorithms to calculate word similarity using yago2: wu-palmer (zhibiao and palmer, 1994), the leacock-chodorow (leacock et al, 1998) <papid> J98-1006 </papid>and 2http://hlt.fbk.eu/en/technology/jlsi 3http://www.geonames.org/ 4http://thewikimachine.fbk.eu/html/ index.html the path distance (score based on the shortest path that connects the senses in the wordnet hyper nym/hyponym taxonomy).</citsent>
<aftsection>
<nextsent>two classes of metrics were designed: (i) the average of the similarity between all the entities in each sentence and (ii) the similarity of the pair of elements which have the shortest path in the wordnet taxonomy among all possible pairs.
</nextsent>
<nextsent>there are six different metrics using the three algorithms in total.
</nextsent>
<nextsent>an extra metric was designed using only twm.
</nextsent>
<nextsent>the metric is calculated by taking the number of common entities in the two sentences divided by the total number of entities annotated in the two sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2153">
<title id=" S10-1081.xml">duluthwsi sense clusters applied to the sense induction task of semeval2 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the duluth-mix system was avariation of wsi that used the combination of training and test data to create theco-occurrence matrix.
</prevsent>
<prevsent>the duluth-r system was series of random baselines.
</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
the duluth systems in the sense induction task of semeval-2 (manandhar et al, 2010) were based on sense clusters (v1.01), freely available open source software package which relies on the premise that words with similar meanings will occur in similar contexts (purandare and pedersen,2004).<papid> W04-2406 </papid></citsent>
<aftsection>
<nextsent>the data for the sense induction task included 100 ambiguous words made up of 50 noun sand 50 verbs.
</nextsent>
<nextsent>there were total of 8,915 test instances and 879,807 training instances provided.
</nextsent>
<nextsent>note that neither the training nor the test data was sense tagged.
</nextsent>
<nextsent>the training data was made available as resource for participants, with the understanding that system evaluation would be done on the test instances only.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2154">
<title id=" S10-1081.xml">duluthwsi sense clusters applied to the sense induction task of semeval2 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the wsi and mix systems almost always represented the test instances using second order cooccurrences, where each word in test instance is replaced by vector that shows the words with which it co-occurs.
</prevsent>
<prevsent>the word vectors that make up test instance are averaged together to make up new representation for thatinstance.
</prevsent>
</prevsection>
<citsent citstr=" N06-4007 ">
all the test instances for word are clustered, and the number of senses is automatically predicted by either the pk2 measure or adapted gap statistic (pedersen and kulkarni, 2006).<papid> N06-4007 </papid>in the duluth systems the co-occurrence matrices are either based on order-dependent bigrams or unordered pairs of words, both of which can be separated by up to some given number of intervening words.</citsent>
<aftsection>
<nextsent>bigrams are used to preserve distinctions between collocations such as cat house and house cat, whereas cooccurrences do not consider order and would treat these two as being equivalent.
</nextsent>
<nextsent>the duluth-wsi systems build co-occurrence matrices from the test data by identifying bigrams or cooccurrences that occur with up to eight intermediate words between them in instances of ambiguous nouns, and up to 23 intermediate words for the verbs.
</nextsent>
<nextsent>any bigram (bi) or cooccurrence (co) that occurs more than 5 times with up to the allowed number of intervening words and has statistical significance of 0.95 or above according tothe left-sided fishers exact test was selected (pedersen et al, 1996).
</nextsent>
<nextsent>some of the wsi systems reduce the cooccurrence matrix to 300 dimensions using singular value decomposition (svd).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2155">
<title id=" S10-1081.xml">duluthwsi sense clusters applied to the sense induction task of semeval2 </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the evaluation of unsupervised sense discrimination and induction systems is still not standardized, so an important part of any exercise like semeval-2 is to scrutinize the evaluation measures used in order to determine to what degree they are providing useful and reasonable way of evaluating system results.
</prevsent>
<prevsent>5.1 evaluation measures.
</prevsent>
</prevsection>
<citsent citstr=" D07-1043 ">
each participating system was scored by three different evaluation methods: the v-measure (rosen berg and hirschberg, 2007), <papid> D07-1043 </papid>the supervised recall measure (agirre and soroa, 2007), <papid> W07-2002 </papid>and the paired f-score (artiles et al, 2009).<papid> D09-1056 </papid></citsent>
<aftsection>
<nextsent>the results of the evaluation are in some sense confusing - system that ranks near the top according to one measure may rank at the bottom or middle of another.there was not any single system that did well according to all of the different measures.
</nextsent>
<nextsent>the situation is so extreme that in some cases system would perform near the top in one measure, and then below random baselines in another.
</nextsent>
<nextsent>these stark differences suggest real need for continued development of other methods for evaluating unsupervised sense induction.
</nextsent>
<nextsent>one minimum expectation of an evaluation measure is that it should expose and identify random baselines by giving them low scores that clearly distinguish them from actual participatingsystems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2156">
<title id=" S10-1081.xml">duluthwsi sense clusters applied to the sense induction task of semeval2 </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the evaluation of unsupervised sense discrimination and induction systems is still not standardized, so an important part of any exercise like semeval-2 is to scrutinize the evaluation measures used in order to determine to what degree they are providing useful and reasonable way of evaluating system results.
</prevsent>
<prevsent>5.1 evaluation measures.
</prevsent>
</prevsection>
<citsent citstr=" W07-2002 ">
each participating system was scored by three different evaluation methods: the v-measure (rosen berg and hirschberg, 2007), <papid> D07-1043 </papid>the supervised recall measure (agirre and soroa, 2007), <papid> W07-2002 </papid>and the paired f-score (artiles et al, 2009).<papid> D09-1056 </papid></citsent>
<aftsection>
<nextsent>the results of the evaluation are in some sense confusing - system that ranks near the top according to one measure may rank at the bottom or middle of another.there was not any single system that did well according to all of the different measures.
</nextsent>
<nextsent>the situation is so extreme that in some cases system would perform near the top in one measure, and then below random baselines in another.
</nextsent>
<nextsent>these stark differences suggest real need for continued development of other methods for evaluating unsupervised sense induction.
</nextsent>
<nextsent>one minimum expectation of an evaluation measure is that it should expose and identify random baselines by giving them low scores that clearly distinguish them from actual participatingsystems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2157">
<title id=" S10-1081.xml">duluthwsi sense clusters applied to the sense induction task of semeval2 </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the evaluation of unsupervised sense discrimination and induction systems is still not standardized, so an important part of any exercise like semeval-2 is to scrutinize the evaluation measures used in order to determine to what degree they are providing useful and reasonable way of evaluating system results.
</prevsent>
<prevsent>5.1 evaluation measures.
</prevsent>
</prevsection>
<citsent citstr=" D09-1056 ">
each participating system was scored by three different evaluation methods: the v-measure (rosen berg and hirschberg, 2007), <papid> D07-1043 </papid>the supervised recall measure (agirre and soroa, 2007), <papid> W07-2002 </papid>and the paired f-score (artiles et al, 2009).<papid> D09-1056 </papid></citsent>
<aftsection>
<nextsent>the results of the evaluation are in some sense confusing - system that ranks near the top according to one measure may rank at the bottom or middle of another.there was not any single system that did well according to all of the different measures.
</nextsent>
<nextsent>the situation is so extreme that in some cases system would perform near the top in one measure, and then below random baselines in another.
</nextsent>
<nextsent>these stark differences suggest real need for continued development of other methods for evaluating unsupervised sense induction.
</nextsent>
<nextsent>one minimum expectation of an evaluation measure is that it should expose and identify random baselines by giving them low scores that clearly distinguish them from actual participatingsystems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2158">
<title id=" S10-1081.xml">duluthwsi sense clusters applied to the sense induction task of semeval2 </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the recall attained on the test split by classifier learned on the training split becomes the measure of the unsupervised system.
</prevsent>
<prevsent>two different splits were used, with 80% or 60% of the test instances for training, and the remainder for testing.
</prevsent>
</prevsection>
<citsent citstr=" W07-2087 ">
this evaluation method was also used in semeval-1, where (pedersen, 2007) <papid> W07-2087 </papid>noted that it seemed to compress the results of all the systems into narrow band that converged around the most frequent sense result.</citsent>
<aftsection>
<nextsent>the same appears to have happened in 2010.
</nextsent>
<nextsent>the supervised recall of the most frequent sense baseline (mfs) is 58 or .59(depending on the split), and the majority of participating systems (and even some of the random baselines) fall in range of scores from .56 to .62(a band of .06).
</nextsent>
<nextsent>this blurs distinctions among participating systems with each other and with random baselines.
</nextsent>
<nextsent>the number of senses actually assigned by the classifier learned from the training split to the instances in the test split is quite small, regardless ofthe number of senses discovered by the participating system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2160">
<title id=" S12-1032.xml">a probabilistic lexical model for ranking textual inferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of identifying texts which share semantic content arises as general need in many natural language processing applications.
</prevsent>
<prevsent>for instance, paraphrasing application has to recognize texts which convey roughly the same content, and summarization application needs to single out texts which contain the content stated by other texts.
</prevsent>
</prevsection>
<citsent citstr=" D08-1009 ">
we refer to this general task as textual inference similar to prior use of this term (raina et al, 2005; schoenmackers et al., 2008; <papid> D08-1009 </papid>haghighi et al, 2005).<papid> H05-1049 </papid>in many textual inference scenarios the setting requires classification decision of whether the inference relation holds or not.</citsent>
<aftsection>
<nextsent>but in other scenarios ranking according to inference likelihood would be the natural task.
</nextsent>
<nextsent>in this work we focus on ranking textual inferences; given sentence and corpus,the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible.
</nextsent>
<nextsent>most naturally, this is the case in question answering (qa), where systems search for passages that cover the semantic components of the question.
</nextsent>
<nextsent>a recent line of research was dedicated to this task (wang et al, 2007; <papid> D07-1003 </papid>heilman and smith, 2010; <papid> N10-1145 </papid>wang and manning, 2010).<papid> C10-1131 </papid>a related scenario is the task of recognizing textual entailment (rte) within corpus (bentivogli et al, 2010)1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2161">
<title id=" S12-1032.xml">a probabilistic lexical model for ranking textual inferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of identifying texts which share semantic content arises as general need in many natural language processing applications.
</prevsent>
<prevsent>for instance, paraphrasing application has to recognize texts which convey roughly the same content, and summarization application needs to single out texts which contain the content stated by other texts.
</prevsent>
</prevsection>
<citsent citstr=" H05-1049 ">
we refer to this general task as textual inference similar to prior use of this term (raina et al, 2005; schoenmackers et al., 2008; <papid> D08-1009 </papid>haghighi et al, 2005).<papid> H05-1049 </papid>in many textual inference scenarios the setting requires classification decision of whether the inference relation holds or not.</citsent>
<aftsection>
<nextsent>but in other scenarios ranking according to inference likelihood would be the natural task.
</nextsent>
<nextsent>in this work we focus on ranking textual inferences; given sentence and corpus,the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible.
</nextsent>
<nextsent>most naturally, this is the case in question answering (qa), where systems search for passages that cover the semantic components of the question.
</nextsent>
<nextsent>a recent line of research was dedicated to this task (wang et al, 2007; <papid> D07-1003 </papid>heilman and smith, 2010; <papid> N10-1145 </papid>wang and manning, 2010).<papid> C10-1131 </papid>a related scenario is the task of recognizing textual entailment (rte) within corpus (bentivogli et al, 2010)1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2162">
<title id=" S12-1032.xml">a probabilistic lexical model for ranking textual inferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this work we focus on ranking textual inferences; given sentence and corpus,the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible.
</prevsent>
<prevsent>most naturally, this is the case in question answering (qa), where systems search for passages that cover the semantic components of the question.
</prevsent>
</prevsection>
<citsent citstr=" D07-1003 ">
a recent line of research was dedicated to this task (wang et al, 2007; <papid> D07-1003 </papid>heilman and smith, 2010; <papid> N10-1145 </papid>wang and manning, 2010).<papid> C10-1131 </papid>a related scenario is the task of recognizing textual entailment (rte) within corpus (bentivogli et al, 2010)1.</citsent>
<aftsection>
<nextsent>in this task, inference systems should identify, forgiven hypothesis, the sentences which entail it in given corpus.
</nextsent>
<nextsent>even though rte was presented as classification task, it has an appealing potential as ranking task as well.
</nextsent>
<nextsent>for instance, one may want to find texts that validate claim such as cellular radiation is dangerous for children, or to learn more about it from newswire corpus.
</nextsent>
<nextsent>to that end, one should look for additional mentions of this claim such as extensive usage of cell phones may be harmful for youngsters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2164">
<title id=" S12-1032.xml">a probabilistic lexical model for ranking textual inferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this work we focus on ranking textual inferences; given sentence and corpus,the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible.
</prevsent>
<prevsent>most naturally, this is the case in question answering (qa), where systems search for passages that cover the semantic components of the question.
</prevsent>
</prevsection>
<citsent citstr=" N10-1145 ">
a recent line of research was dedicated to this task (wang et al, 2007; <papid> D07-1003 </papid>heilman and smith, 2010; <papid> N10-1145 </papid>wang and manning, 2010).<papid> C10-1131 </papid>a related scenario is the task of recognizing textual entailment (rte) within corpus (bentivogli et al, 2010)1.</citsent>
<aftsection>
<nextsent>in this task, inference systems should identify, forgiven hypothesis, the sentences which entail it in given corpus.
</nextsent>
<nextsent>even though rte was presented as classification task, it has an appealing potential as ranking task as well.
</nextsent>
<nextsent>for instance, one may want to find texts that validate claim such as cellular radiation is dangerous for children, or to learn more about it from newswire corpus.
</nextsent>
<nextsent>to that end, one should look for additional mentions of this claim such as extensive usage of cell phones may be harmful for youngsters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2165">
<title id=" S12-1032.xml">a probabilistic lexical model for ranking textual inferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this work we focus on ranking textual inferences; given sentence and corpus,the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible.
</prevsent>
<prevsent>most naturally, this is the case in question answering (qa), where systems search for passages that cover the semantic components of the question.
</prevsent>
</prevsection>
<citsent citstr=" C10-1131 ">
a recent line of research was dedicated to this task (wang et al, 2007; <papid> D07-1003 </papid>heilman and smith, 2010; <papid> N10-1145 </papid>wang and manning, 2010).<papid> C10-1131 </papid>a related scenario is the task of recognizing textual entailment (rte) within corpus (bentivogli et al, 2010)1.</citsent>
<aftsection>
<nextsent>in this task, inference systems should identify, forgiven hypothesis, the sentences which entail it in given corpus.
</nextsent>
<nextsent>even though rte was presented as classification task, it has an appealing potential as ranking task as well.
</nextsent>
<nextsent>for instance, one may want to find texts that validate claim such as cellular radiation is dangerous for children, or to learn more about it from newswire corpus.
</nextsent>
<nextsent>to that end, one should look for additional mentions of this claim such as extensive usage of cell phones may be harmful for youngsters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2167">
<title id=" S12-1032.xml">a probabilistic lexical model for ranking textual inferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this can be done by ranking the corpus passages by their likelihood to entail the claim, where the top ranked passages are likely to contain additional relevant information.
</prevsent>
<prevsent>two main approaches have been used to address textual inference (for either ranking or classification).
</prevsent>
</prevsection>
<citsent citstr=" P03-1003 ">
one is based on transformations over syntactic parse trees (echihabi and marcu, 2003; <papid> P03-1003 </papid>heilman and smith, 2010).<papid> N10-1145 </papid></citsent>
<aftsection>
<nextsent>some works in this line describe probabilistic generative process in which the parse tree of the question is generated from the passage (wang et al, 2007; <papid> D07-1003 </papid>wang and manning, 2010).<papid> C10-1131 </papid></nextsent>
<nextsent>in the second approach, lexical models have been employed for textual inference (mackinlay and baldwin, 2009; clark and harrison, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2173">
<title id=" S12-1032.xml">a probabilistic lexical model for ranking textual inferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typi 1http://www.nist.gov/tac/2010/rte/index.html 237 cally, lexical models consider text fragment as bag of terms and split the inference decision into two steps.
</prevsent>
<prevsent>the first is term-level estimation of the inference likelihood for each term independently,based on direct lexical match and on lexical knowledge resources.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
some commonly used resources are wordnet (fellbaum, 1998), distributional-similarity thesauri (lin, 1998), <papid> P98-2127 </papid>and web knowledge resources such as (suchanek et al, 2007).</citsent>
<aftsection>
<nextsent>the second step is making final sentence-level decision based onthese estimations for the component terms.
</nextsent>
<nextsent>lexical models have the advantage of being fast and easy to utilize (e.g. no dependency on parsing tools) while being highly competitive with top performing systems, e.g. the system of majumdar and bhattacharyya (2010).in this work, we investigate how well such lexical models can perform in textual inference ranking scenarios.
</nextsent>
<nextsent>however, while lexical models usually apply heuristic methods, we would like to pursue principled learning-based generative framework, in analogy to the approaches for syntactic-based inference.
</nextsent>
<nextsent>an attractive work in this spirit is presented in (shnarch et al, 2011<papid> W11-2402 </papid>a), that propose model which is both lexical and probabilistic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2174">
<title id=" S12-1032.xml">a probabilistic lexical model for ranking textual inferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexical models have the advantage of being fast and easy to utilize (e.g. no dependency on parsing tools) while being highly competitive with top performing systems, e.g. the system of majumdar and bhattacharyya (2010).in this work, we investigate how well such lexical models can perform in textual inference ranking scenarios.
</prevsent>
<prevsent>however, while lexical models usually apply heuristic methods, we would like to pursue principled learning-based generative framework, in analogy to the approaches for syntactic-based inference.
</prevsent>
</prevsection>
<citsent citstr=" W11-2402 ">
an attractive work in this spirit is presented in (shnarch et al, 2011<papid> W11-2402 </papid>a), that propose model which is both lexical and probabilistic.</citsent>
<aftsection>
<nextsent>later, shnarch etal.
</nextsent>
<nextsent>(2011b) improved this model and reported results that outperformed previous lexical models and were on par with state-of-the-art rte models.
</nextsent>
<nextsent>whereas their term-level model provides means to integrate lexical knowledge in probabilistic manner, their sentence-level model depends to great extent on heuristic normalizations which were introduced to incorporate prominent aspects of the sentence-level decision.
</nextsent>
<nextsent>this deviates their model from pure probabilistic methodology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2228">
<title id=" S12-1032.xml">a probabilistic lexical model for ranking textual inferences </title>
<section> evaluations and results.  </section>
<citcontext>
<prevsection>
<prevsent>the training and test sets roughly contain 5700 and 1500 pairs correspondingly.
</prevsent>
<prevsent>5the dataset was kindly provided to us by mengqiu wang and is available for download athttp://www.cs.stanford.edu/mengqiu/data/qg-emnlp07 data.tgz.
</prevsent>
</prevsection>
<citsent citstr=" N03-1013 ">
242 method plmtl utilizes wordnet and the catvar (categorial variation) derivations database (habash and dorr, 2003) <papid> N03-1013 </papid>as generic and publicly available lexical knowledge resources, when question and answer terms are restricted to the first wordnet sense.</citsent>
<aftsection>
<nextsent>in order to be consistent with (shnarch et al, 2011<papid> W11-2402 </papid>b), the best performing model of prior work,we restricted our model to utilize only these two resources which they used.</nextsent>
<nextsent>however, additional lexical resources can be provided as input to our model (e.g. distributional similarity-base thesaurus).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2281">
<title id=" S10-1097.xml">twitter based system using twitter for disambiguating sentiment ambiguous adjectives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to solve the task, one has to deal not only with the semantics of the context, but also with the psychological aspects of human perception of emotions from the written text.
</prevsent>
<prevsent>in our approach, we use twitter1 microblogging platform to retrieve emotional messages and form two sets of texts: messages with positive emotions and those with negative ones (pak and paroubek, 1http://twitter.com2010).
</prevsent>
</prevsection>
<citsent citstr=" P05-2008 ">
we use emoticons2 as indicators of an emotion (read, 2005) <papid> P05-2008 </papid>to automatically classify texts into positive or negative sets.</citsent>
<aftsection>
<nextsent>the reason we use twitter is because it allows us to collect the data with minimal supervision efforts.
</nextsent>
<nextsent>it provides an api3 which makes the data retrieval process much more easier then web based search or other resources.
</nextsent>
<nextsent>after the dataset of emotional texts has been obtained, we build classifier based on n-grams nave bayes approach.
</nextsent>
<nextsent>we tested two approaches to build sentiment classifier: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2286">
<title id=" S10-1097.xml">twitter based system using twitter for disambiguating sentiment ambiguous adjectives </title>
<section> our method.  </section>
<citcontext>
<prevsection>
<prevsent>for example, sentence do not like fish?
</prevsent>
<prevsent>will form three bigrams: do+not?, do+not like?, not+like fish?.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
sucha procedure improves the accuracy of the classification since the negation plays special role in opinion and sentiment expression (wilson et al, 2005).<papid> H05-1044 </papid></citsent>
<aftsection>
<nextsent>in english, we used negative particles noand not?.
</nextsent>
<nextsent>in chinese, we used the following par ticles: 1.
</nextsent>
<nextsent>is not + noun 2.
</nextsent>
<nextsent>does not + verb, will not + verb 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2287">
<title id=" S10-1097.xml">twitter based system using twitter for disambiguating sentiment ambiguous adjectives </title>
<section> our method.  </section>
<citcontext>
<prevsection>
<prevsent>we assume that target adjective has the same sentiment polarity as the whole text, because in general the lengths of the given texts are small.
</prevsent>
<prevsent>since we have sets of equal number of positive and negative messages, we simplify the equation: (s|m) = (m |s) (m) (2)6an abbreviation for retweet, which means citation or re posting of message 437 (s|m) ? (m |s) (3) we train bayes classifiers which use presence of an n-grams as binary feature.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
we have experimented with unigrams, bigrams, and trigrams.pang et al (pang et al, 2002) <papid> W02-1011 </papid>reported that unigrams outperform bigrams when doing sentiment classification of movie reviews, but dave et al(dave et al, 2003) have obtained contrary re sults: bigrams and trigrams worked better for the product-review polarity classification.</citsent>
<aftsection>
<nextsent>we tried to determine the best settings for our microblogging data.
</nextsent>
<nextsent>on the one hand high-order n-grams, such as trigrams, should capture patterns of sentiments expressions better.
</nextsent>
<nextsent>on the other hand, unigrams should provide good coverage of the data.
</nextsent>
<nextsent>therefore we combine three classifiers that are based on different n-gram orders (unigrams, bigrams and trigrams).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2288">
<title id=" S10-1083.xml">pengyuanpku extracting infrequent sense instance with the same ngram pattern for the semeval2010 task 15 </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it is very hard for us to evaluate our system from the viewpoint of smoothness and instance sense distribution.
</prevsent>
<prevsent>to our knowledge, the methods of auto acquiring sense-labeled instances include using parallel corpora like gale et al (1992) and ng et al.
</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
(2003), extracting by monosemous relative of wordnet like leacock et al (1998), <papid> J98-1006 </papid>mihalcea and moldovan (1999), agirre and martnez (2004), <papid> W04-3204 </papid>martnez et al (2006) and pengyuan et al.</citsent>
<aftsection>
<nextsent>(2008).
</nextsent>
<nextsent>the method proposed by mihalcea and moldovan (2000) is also an effective way.
</nextsent>
<nextsent>we participated in the semeval-2010 task 15 on infrequent sense identification for mandarin text to speech systems.
</nextsent>
<nextsent>official results show our system which extract infrequent sense instances is effective.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2289">
<title id=" S10-1083.xml">pengyuanpku extracting infrequent sense instance with the same ngram pattern for the semeval2010 task 15 </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it is very hard for us to evaluate our system from the viewpoint of smoothness and instance sense distribution.
</prevsent>
<prevsent>to our knowledge, the methods of auto acquiring sense-labeled instances include using parallel corpora like gale et al (1992) and ng et al.
</prevsent>
</prevsection>
<citsent citstr=" W04-3204 ">
(2003), extracting by monosemous relative of wordnet like leacock et al (1998), <papid> J98-1006 </papid>mihalcea and moldovan (1999), agirre and martnez (2004), <papid> W04-3204 </papid>martnez et al (2006) and pengyuan et al.</citsent>
<aftsection>
<nextsent>(2008).
</nextsent>
<nextsent>the method proposed by mihalcea and moldovan (2000) is also an effective way.
</nextsent>
<nextsent>we participated in the semeval-2010 task 15 on infrequent sense identification for mandarin text to speech systems.
</nextsent>
<nextsent>official results show our system which extract infrequent sense instances is effective.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2290">
<title id=" P99-1016.xml">automatic construction of a hypernymlabeled noun hierarchy from text </title>
<section> building the noun hierarchy.  </section>
<citcontext>
<prevsection>
<prevsent>the internal nodes of the resulting tree are then labeled with hypernyms for the nouns clustered underneath them, also based on data extracted from the wall street jour-nal.
</prevsent>
<prevsent>the resulting hierarchy is evaluated by human judges, and future research directions are discussed.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
the first stage in constructing our hierar-chy is to build an unlabeled hierarchy of nouns using bottom-up clustering methods (see, e.g., brown et al (1992)).<papid> J92-4003 </papid></citsent>
<aftsection>
<nextsent>nouns are clustered based on conjunction and apposi- tive data collected from the wall street jour-nal corpus.
</nextsent>
<nextsent>some of the data comes from the parsed files 2-21 of the wall street journal penn treebank corpus (marcus et al, 1993), <papid> J93-2004 </papid>and additional parsed text was obtained by parsing the 1987 wall street journal text us-ing the parser described in charniak et al (1998).<papid> W98-1115 </papid></nextsent>
<nextsent>from this parsed text, we identified all conjunctions of noun phrases (e.g.,  execu-tive vice-president and treasurer  or  scien-tific equipment, apparatus and disposables ) and all appositives (e.g.,  james h. rosen- field, former cbs inc. executive  or  boe-ing, defense contractor ).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2291">
<title id=" P99-1016.xml">automatic construction of a hypernymlabeled noun hierarchy from text </title>
<section> building the noun hierarchy.  </section>
<citcontext>
<prevsection>
<prevsent>the first stage in constructing our hierar-chy is to build an unlabeled hierarchy of nouns using bottom-up clustering methods (see, e.g., brown et al (1992)).<papid> J92-4003 </papid></prevsent>
<prevsent>nouns are clustered based on conjunction and apposi- tive data collected from the wall street jour-nal corpus.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
some of the data comes from the parsed files 2-21 of the wall street journal penn treebank corpus (marcus et al, 1993), <papid> J93-2004 </papid>and additional parsed text was obtained by parsing the 1987 wall street journal text us-ing the parser described in charniak et al (1998).<papid> W98-1115 </papid></citsent>
<aftsection>
<nextsent>from this parsed text, we identified all conjunctions of noun phrases (e.g.,  execu-tive vice-president and treasurer  or  scien-tific equipment, apparatus and disposables ) and all appositives (e.g.,  james h. rosen- field, former cbs inc. executive  or  boe-ing, defense contractor ).
</nextsent>
<nextsent>the idea here is that nouns in conjunctions or appositives tend to be semantically related, as discussed in riloff and shepherd (1997) <papid> W97-0313 </papid>and roark and charniak (1998).<papid> P98-2182 </papid></nextsent>
<nextsent>taking the headwords of each np and stemming them results in data for about 50,000 distinct nouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2292">
<title id=" P99-1016.xml">automatic construction of a hypernymlabeled noun hierarchy from text </title>
<section> building the noun hierarchy.  </section>
<citcontext>
<prevsection>
<prevsent>the first stage in constructing our hierar-chy is to build an unlabeled hierarchy of nouns using bottom-up clustering methods (see, e.g., brown et al (1992)).<papid> J92-4003 </papid></prevsent>
<prevsent>nouns are clustered based on conjunction and apposi- tive data collected from the wall street jour-nal corpus.</prevsent>
</prevsection>
<citsent citstr=" W98-1115 ">
some of the data comes from the parsed files 2-21 of the wall street journal penn treebank corpus (marcus et al, 1993), <papid> J93-2004 </papid>and additional parsed text was obtained by parsing the 1987 wall street journal text us-ing the parser described in charniak et al (1998).<papid> W98-1115 </papid></citsent>
<aftsection>
<nextsent>from this parsed text, we identified all conjunctions of noun phrases (e.g.,  execu-tive vice-president and treasurer  or  scien-tific equipment, apparatus and disposables ) and all appositives (e.g.,  james h. rosen- field, former cbs inc. executive  or  boe-ing, defense contractor ).
</nextsent>
<nextsent>the idea here is that nouns in conjunctions or appositives tend to be semantically related, as discussed in riloff and shepherd (1997) <papid> W97-0313 </papid>and roark and charniak (1998).<papid> P98-2182 </papid></nextsent>
<nextsent>taking the headwords of each np and stemming them results in data for about 50,000 distinct nouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2293">
<title id=" P99-1016.xml">automatic construction of a hypernymlabeled noun hierarchy from text </title>
<section> building the noun hierarchy.  </section>
<citcontext>
<prevsection>
<prevsent>some of the data comes from the parsed files 2-21 of the wall street journal penn treebank corpus (marcus et al, 1993), <papid> J93-2004 </papid>and additional parsed text was obtained by parsing the 1987 wall street journal text us-ing the parser described in charniak et al (1998).<papid> W98-1115 </papid></prevsent>
<prevsent>from this parsed text, we identified all conjunctions of noun phrases (e.g.,  execu-tive vice-president and treasurer  or  scien-tific equipment, apparatus and disposables ) and all appositives (e.g.,  james h. rosen- field, former cbs inc. executive  or  boe-ing, defense contractor ).</prevsent>
</prevsection>
<citsent citstr=" W97-0313 ">
the idea here is that nouns in conjunctions or appositives tend to be semantically related, as discussed in riloff and shepherd (1997) <papid> W97-0313 </papid>and roark and charniak (1998).<papid> P98-2182 </papid></citsent>
<aftsection>
<nextsent>taking the headwords of each np and stemming them results in data for about 50,000 distinct nouns.
</nextsent>
<nextsent>a vector is created for each noun contain-ing counts for how many times each other noun appears in conjunction or appositive with it.
</nextsent>
<nextsent>we can then measure the similarity of the vectors for two nouns by computing the cosine of the angle between these vec-tors, as v*w cos (v, w) - ivi iw to compare the similarity of two groups of nouns, we define similarity as the average of the cosines between each pair of nouns made up of one noun from each of the two groups.
</nextsent>
<nextsent>sim(a,b) = ev,wcos (v,w) size(a)size(b) where ranges over all vectors for nouns 120 in group a, ranges over the vectors for group b, and size(x) represents he number of nouns which are descendants of node x. we want to create tree of all of the nouns in this data using standard bottom-up clus-tering techniques as follows: put each noun into its own node.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2296">
<title id=" P99-1016.xml">automatic construction of a hypernymlabeled noun hierarchy from text </title>
<section> building the noun hierarchy.  </section>
<citcontext>
<prevsection>
<prevsent>some of the data comes from the parsed files 2-21 of the wall street journal penn treebank corpus (marcus et al, 1993), <papid> J93-2004 </papid>and additional parsed text was obtained by parsing the 1987 wall street journal text us-ing the parser described in charniak et al (1998).<papid> W98-1115 </papid></prevsent>
<prevsent>from this parsed text, we identified all conjunctions of noun phrases (e.g.,  execu-tive vice-president and treasurer  or  scien-tific equipment, apparatus and disposables ) and all appositives (e.g.,  james h. rosen- field, former cbs inc. executive  or  boe-ing, defense contractor ).</prevsent>
</prevsection>
<citsent citstr=" P98-2182 ">
the idea here is that nouns in conjunctions or appositives tend to be semantically related, as discussed in riloff and shepherd (1997) <papid> W97-0313 </papid>and roark and charniak (1998).<papid> P98-2182 </papid></citsent>
<aftsection>
<nextsent>taking the headwords of each np and stemming them results in data for about 50,000 distinct nouns.
</nextsent>
<nextsent>a vector is created for each noun contain-ing counts for how many times each other noun appears in conjunction or appositive with it.
</nextsent>
<nextsent>we can then measure the similarity of the vectors for two nouns by computing the cosine of the angle between these vec-tors, as v*w cos (v, w) - ivi iw to compare the similarity of two groups of nouns, we define similarity as the average of the cosines between each pair of nouns made up of one noun from each of the two groups.
</nextsent>
<nextsent>sim(a,b) = ev,wcos (v,w) size(a)size(b) where ranges over all vectors for nouns 120 in group a, ranges over the vectors for group b, and size(x) represents he number of nouns which are descendants of node x. we want to create tree of all of the nouns in this data using standard bottom-up clus-tering techniques as follows: put each noun into its own node.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2297">
<title id=" P99-1016.xml">automatic construction of a hypernymlabeled noun hierarchy from text </title>
<section> assigning hypernyms.  </section>
<citcontext>
<prevsection>
<prevsent>our next step is to try to label each of the internal nodes with hypernym describing its descendant ouns.
</prevsent>
<prevsent>following wordnet, word is said to be hyperuym of word if native speakers of english accept he sentence  is (kind of) a.,, to determine possible hypernyms for particular noun, we use the same parsed text described in the previous section.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
as sug-gested in hearst (1992), <papid> C92-2082 </papid>we can find some hypernym data in the text by looking for conjunctions involving the word  other , as in  x, y, and other zs  (patterns 3 and 4 in hearst).</citsent>
<aftsection>
<nextsent>from this phrase we can extract that is likely hypernym for both and y. this data is extracted from the parsed text, and for each noun we construct avector of hypernyms, with value of if word has been seen as hypernym for this noun and 0 otherwise.
</nextsent>
<nextsent>these vectors are associated with the leaves of the binary tree constructed in the previous ection.
</nextsent>
<nextsent>for each internal node of the tree, we con-struct vector of hypernyms by adding to-gether the vectors of its children.
</nextsent>
<nextsent>we then assign hypernym to this node by sim-ply choosing the hypernym with the largest value in this vector; that is, the hypernym which appeared with the largest number of the node descendant nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2304">
<title id=" P99-1016.xml">automatic construction of a hypernymlabeled noun hierarchy from text </title>
<section> discussion and future.  </section>
<citcontext>
<prevsection>
<prevsent>how- ever, domain-specific text tends to greatly constrain which senses of word will appear, and if the learned hierarchy is intended for use with the same type of text from which it was learned, it is possible that this would be of limited benefit.
</prevsent>
<prevsent>we used parsed text for these experiments because we believed we would get better re-sults and the parsed data was readily avail-able.
</prevsent>
</prevsection>
<citsent citstr=" P88-1027 ">
however, it would be interesting to see if parsing is necessary or if we can get equivalent or nearly-equivalent results doing some simpler text processing, as suggested in ahlswede and evens (1988).<papid> P88-1027 </papid></citsent>
<aftsection>
<nextsent>both hearst (1992) <papid> C92-2082 </papid>and riloff and shepherd (1997) <papid> W97-0313 </papid>use un parsed text.</nextsent>
<nextsent>pereira et al (1993) <papid> P93-1024 </papid>used clustering to build an unlabeled hierarchy of nouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2311">
<title id=" P99-1016.xml">automatic construction of a hypernymlabeled noun hierarchy from text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, it would be interesting to see if parsing is necessary or if we can get equivalent or nearly-equivalent results doing some simpler text processing, as suggested in ahlswede and evens (1988).<papid> P88-1027 </papid></prevsent>
<prevsent>both hearst (1992) <papid> C92-2082 </papid>and riloff and shepherd (1997) <papid> W97-0313 </papid>use un parsed text.</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
pereira et al (1993) <papid> P93-1024 </papid>used clustering to build an unlabeled hierarchy of nouns.</citsent>
<aftsection>
<nextsent>their hier-archy is constructed top-down, rather than bottom-up, with nouns being allowed mem-bership in multiple clusters.
</nextsent>
<nextsent>their cluster-ing is based on verb-object relations rather than on the noun-noun relations that we use.
</nextsent>
<nextsent>future work on our project will include an attempt incorporate verb-object data as well in the clustering process.
</nextsent>
<nextsent>the tree they construct is also binary with some internal nodes which seem to be  artificial , but for evaluation purposes they disregard the tree structure and consider only the leaf nodes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2320">
<title id=" S10-1084.xml">rali automatic weighting of text window distances </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiments on semeval word sense disambiguation tasks showed considerable improvements.
</prevsent>
<prevsent>the meaning of word can be defined by the words that accompany it in the text.
</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
this is the principle often used in previous studies on word sense disambiguation (wsd) (ide and vronis,1998; <papid> J98-1001 </papid>navigli, 2009).</citsent>
<aftsection>
<nextsent>in general, the accompanying words form context vector of the target word, or probability distribution of the contextwords.
</nextsent>
<nextsent>for example, under the unigram bag-of word assumption, this means building p(x|t) = count(x,t) ? ? count(x ? ,t) , where count(x, t) is the count of co-occurrences of word with the target word under certain criterion.
</nextsent>
<nextsent>in most studies, and should co-occur within window of up to words or sentences.
</nextsent>
<nextsent>the bounds are usually selected asto maximize system performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2321">
<title id=" S10-1084.xml">rali automatic weighting of text window distances </title>
<section> uses of text windows.  </section>
<citcontext>
<prevsection>
<prevsent>they suggest using windows of up to 100 words.
</prevsent>
<prevsent>another example can be found in wsd systems, where shorter window ispreferred.
</prevsent>
</prevsection>
<citsent citstr=" D07-1108 ">
in semeval-2007, top performing systems on wsd tasks, such as nus-ml (cai et al, 2007), <papid> D07-1108 </papid>made use of bag-of-word features around the target word.</citsent>
<aftsection>
<nextsent>in this case, they found that the best results can be achieved using window size of 3.
</nextsent>
<nextsent>375both these systems limit the size of their windows for different purposes.
</nextsent>
<nextsent>the former aims to model the topic of the documents containing the word rather than the words meaning.
</nextsent>
<nextsent>the latter limits the size because bag-of-word features further from the target word would not be sufficiently related to its meaning (ide and vronis, 1998).<papid> J98-1001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2323">
<title id=" S10-1084.xml">rali automatic weighting of text window distances </title>
<section> experiments on semeval-2007 english.  </section>
<citcontext>
<prevsection>
<prevsent>we therefore suggest using the observation of several consecutive decreases of all except ? 1 as an end criterion.
</prevsent>
<prevsent>we used 10 consecutive steps for our experiments.
</prevsent>
</prevsection>
<citsent citstr=" W07-2016 ">
lexical sample the semeval workshop holds wsd tasks such as the english lexical sample (els) (pradhan et al, 2007).<papid> W07-2016 </papid></citsent>
<aftsection>
<nextsent>it consists of selected set of polysemous words, contained within passages where sense taken from sense inventory is manually annotated.
</nextsent>
<nextsent>the task is to create supervised classifiers maximizing accuracy on test data.
</nextsent>
<nextsent>since there are only 50 words and instances arefew, we judged there was not enough data to compute weights.
</nextsent>
<nextsent>instead, we used the ap newswire corpus of the trec collection (cd 1 &amp; 2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2327">
<title id=" S12-1024.xml">extracting a semantic lexicon of french adjectives from a large lexicographic dictionary </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, in section 4, we present quantitative evaluation of our method and qualitative evaluation of our data, and discuss their results.
</prevsent>
<prevsent>we conclude on some perspectives for future work.
</prevsent>
</prevsection>
<citsent citstr=" C96-2142 ">
it is well established that there are different types of adjectives distinguished by properties, such as gradation and marked ness, and by their semantic and syntactic behaviors (antonymy, selectional preferences) (fellbaum et al, 1993; raskin and nirenburg, 1996).<papid> C96-2142 </papid></citsent>
<aftsection>
<nextsent>wordnet, for example, distinguishes different types of adjectives according to their semantic and syntactic behaviors: descriptive, reference-modifying, color and relational adjectives (fellbaum et al, 1993).
</nextsent>
<nextsent>however, it mainly accounts for the first and the last types of adjectives.
</nextsent>
<nextsent>descrip 1for other possible nlp applications of lexicons encoded with the lexical function formalism, see schwab and lafourcade (2007).
</nextsent>
<nextsent>161 tive adjectives are organized in adjectival synsets that are mostly related through antonymy (heavy?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2328">
<title id=" S12-1024.xml">extracting a semantic lexicon of french adjectives from a large lexicographic dictionary </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>fellbaum et al (1993:36) acknowledge the existence of more diverse relations to nominal synsets, but, to our knowledge, these are not accounted for in wordnet.
</prevsent>
<prevsent>this limitation is also present in the open access french version of the princeton wordnet, wolf (sagot and fiser, 2012).
</prevsent>
</prevsection>
<citsent citstr=" A00-2006 ">
this limitation has led projects extending wordnet to other languages, like euro wordnet, ital wordnet or wordnet.pt, to add few more relations to account for this diversity (alonge et al, 2000; <papid> A00-2006 </papid>marrafa and mendes, 2006;<papid> P06-2072 </papid>vossen, 2002).</citsent>
<aftsection>
<nextsent>the number of new relations is however limited.
</nextsent>
<nextsent>as can be seen, wordnet-type approaches focus on relating adjectival synsets using few semantic relations, mostly antonymy and plain related to relations.our goal is to achieve finer, and thus richer, semantic characterization of the relations holding between french adjectives and other words from all syntactic categories using the formalism of lexical functions.
</nextsent>
<nextsent>we assume that the type of the adjective is reflected in the structure of its lexicographic definition.
</nextsent>
<nextsent>thus, to extract semantically relevant information from adjectival definitions, we propose to create different types of rules accounting for this diversity of defining structures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2329">
<title id=" S12-1024.xml">extracting a semantic lexicon of french adjectives from a large lexicographic dictionary </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>fellbaum et al (1993:36) acknowledge the existence of more diverse relations to nominal synsets, but, to our knowledge, these are not accounted for in wordnet.
</prevsent>
<prevsent>this limitation is also present in the open access french version of the princeton wordnet, wolf (sagot and fiser, 2012).
</prevsent>
</prevsection>
<citsent citstr=" P06-2072 ">
this limitation has led projects extending wordnet to other languages, like euro wordnet, ital wordnet or wordnet.pt, to add few more relations to account for this diversity (alonge et al, 2000; <papid> A00-2006 </papid>marrafa and mendes, 2006;<papid> P06-2072 </papid>vossen, 2002).</citsent>
<aftsection>
<nextsent>the number of new relations is however limited.
</nextsent>
<nextsent>as can be seen, wordnet-type approaches focus on relating adjectival synsets using few semantic relations, mostly antonymy and plain related to relations.our goal is to achieve finer, and thus richer, semantic characterization of the relations holding between french adjectives and other words from all syntactic categories using the formalism of lexical functions.
</nextsent>
<nextsent>we assume that the type of the adjective is reflected in the structure of its lexicographic definition.
</nextsent>
<nextsent>thus, to extract semantically relevant information from adjectival definitions, we propose to create different types of rules accounting for this diversity of defining structures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2330">
<title id=" S12-1049.xml">semeval2012 task 4 evaluating chinese word similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of word similarity is to compute the similarity degree between words.
</prevsent>
<prevsent>it is widely used in natural language processing to alleviate data sparseness which is an open problem in this field.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
many research have focus on english language (lin, 1998; <papid> P98-2127 </papid>curran and moens, 2003; dinu and lapata, 2010), <papid> D10-1113 </papid>some of which relyon the manual created thesaurus such as wordnet (budanitsky and hirst, 2006), <papid> J06-1003 </papid>some of which obtain the similarity of the words via large scale corpus (lee, 1999), <papid> P99-1004 </papid>and some research integrate both thesaurus and corpus (fujii et al, 1997).</citsent>
<aftsection>
<nextsent>this task tries to evaluate the approach on word similarity for chinese language.
</nextsent>
<nextsent>to the best of our knowledge, this is first release of benchmark data for this study.
</nextsent>
<nextsent>in english language, there are two data sets: rubenstein and goodenough (1965) and finkelstein et al (2002) created ranking of word pairs as the benchmark data.
</nextsent>
<nextsent>both of them are manually annotated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2331">
<title id=" S12-1049.xml">semeval2012 task 4 evaluating chinese word similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of word similarity is to compute the similarity degree between words.
</prevsent>
<prevsent>it is widely used in natural language processing to alleviate data sparseness which is an open problem in this field.
</prevsent>
</prevsection>
<citsent citstr=" D10-1113 ">
many research have focus on english language (lin, 1998; <papid> P98-2127 </papid>curran and moens, 2003; dinu and lapata, 2010), <papid> D10-1113 </papid>some of which relyon the manual created thesaurus such as wordnet (budanitsky and hirst, 2006), <papid> J06-1003 </papid>some of which obtain the similarity of the words via large scale corpus (lee, 1999), <papid> P99-1004 </papid>and some research integrate both thesaurus and corpus (fujii et al, 1997).</citsent>
<aftsection>
<nextsent>this task tries to evaluate the approach on word similarity for chinese language.
</nextsent>
<nextsent>to the best of our knowledge, this is first release of benchmark data for this study.
</nextsent>
<nextsent>in english language, there are two data sets: rubenstein and goodenough (1965) and finkelstein et al (2002) created ranking of word pairs as the benchmark data.
</nextsent>
<nextsent>both of them are manually annotated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2332">
<title id=" S12-1049.xml">semeval2012 task 4 evaluating chinese word similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of word similarity is to compute the similarity degree between words.
</prevsent>
<prevsent>it is widely used in natural language processing to alleviate data sparseness which is an open problem in this field.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
many research have focus on english language (lin, 1998; <papid> P98-2127 </papid>curran and moens, 2003; dinu and lapata, 2010), <papid> D10-1113 </papid>some of which relyon the manual created thesaurus such as wordnet (budanitsky and hirst, 2006), <papid> J06-1003 </papid>some of which obtain the similarity of the words via large scale corpus (lee, 1999), <papid> P99-1004 </papid>and some research integrate both thesaurus and corpus (fujii et al, 1997).</citsent>
<aftsection>
<nextsent>this task tries to evaluate the approach on word similarity for chinese language.
</nextsent>
<nextsent>to the best of our knowledge, this is first release of benchmark data for this study.
</nextsent>
<nextsent>in english language, there are two data sets: rubenstein and goodenough (1965) and finkelstein et al (2002) created ranking of word pairs as the benchmark data.
</nextsent>
<nextsent>both of them are manually annotated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2333">
<title id=" S12-1049.xml">semeval2012 task 4 evaluating chinese word similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of word similarity is to compute the similarity degree between words.
</prevsent>
<prevsent>it is widely used in natural language processing to alleviate data sparseness which is an open problem in this field.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
many research have focus on english language (lin, 1998; <papid> P98-2127 </papid>curran and moens, 2003; dinu and lapata, 2010), <papid> D10-1113 </papid>some of which relyon the manual created thesaurus such as wordnet (budanitsky and hirst, 2006), <papid> J06-1003 </papid>some of which obtain the similarity of the words via large scale corpus (lee, 1999), <papid> P99-1004 </papid>and some research integrate both thesaurus and corpus (fujii et al, 1997).</citsent>
<aftsection>
<nextsent>this task tries to evaluate the approach on word similarity for chinese language.
</nextsent>
<nextsent>to the best of our knowledge, this is first release of benchmark data for this study.
</nextsent>
<nextsent>in english language, there are two data sets: rubenstein and goodenough (1965) and finkelstein et al (2002) created ranking of word pairs as the benchmark data.
</nextsent>
<nextsent>both of them are manually annotated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2334">
<title id=" S12-1049.xml">semeval2012 task 4 evaluating chinese word similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by definition, the similarity of the word to itself should be 10.
</prevsent>
<prevsent>a fractional score is allowed.
</prevsent>
</prevsection>
<citsent citstr=" P06-1046 ">
it should be noted that besides the rank of word pairs, the thesaurus such as roget thesaurus are often used for word similarity study (gorman and curran, 2006).<papid> P06-1046 </papid></citsent>
<aftsection>
<nextsent>the paper is organized as follows.
</nextsent>
<nextsent>in section 2 we describe in detail the process of the data preparation.
</nextsent>
<nextsent>section 3 introduces the four participating systems.
</nextsent>
<nextsent>section 4 reports their results and gives brief discussion..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2335">
<title id=" S12-1049.xml">semeval2012 task 4 evaluating chinese word similarity </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>( , )s ? ?
</prevsent>
<prevsent>is the minimum number of adjacent trans positions needing to bring ? and ?
</prevsent>
</prevsection>
<citsent citstr=" J06-4002 ">
(lapata, 2006).<papid> J06-4002 </papid></citsent>
<aftsection>
<nextsent>in this metric, tau value ranges from -1 to +1 and -1 means that the two ranks are inverse to each other and +1 means the identical rank.
</nextsent>
<nextsent>from table 4, we can see that except the final system, three of them got the positive tau value.
</nextsent>
<nextsent>it is regret that the tau is very small even if the mixcc system is the best one.
</nextsent>
<nextsent>we organize an evaluation task focuses on word similarity in chinese language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2336">
<title id=" S12-1105.xml">fbk cross lingual textual entailment without translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our best run achieved an accuracy of 50.4% on the spanish-englishdataset (with the average score and the median system respectively achieving 40.7% and 34.6%), demonstrating the effectiveness of apure?
</prevsent>
<prevsent>cross-lingual approach that avoids intermediate translations.
</prevsent>
</prevsection>
<citsent citstr=" N10-1045 ">
so far, cross-lingual textual entailment (clte) (mehdad et al, 2010) <papid> N10-1045 </papid>has been applied to: i) available te datasets (yes?/no? uni-directional relations between monolingual pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (negri andmehdad, 2010), <papid> W10-0734 </papid>and ii) machine translation evaluation datasets (mehdad et al, 2012<papid> W12-3122 </papid>b).</citsent>
<aftsection>
<nextsent>the content synchronization task represents challenging application scenario to test the capabilities of clte systems, by proposing richer inventory of phenomena (i.e. bidirectional?/forward?/backward?/no entailment?
</nextsent>
<nextsent>multi-directional entailment relations).
</nextsent>
<nextsent>multi-directional clte recognition can be seenas the identification of semantic equivalence and information disparity between two topically related sentences, at the cross-lingual level.
</nextsent>
<nextsent>this is core aspect of the multilingual content synchronization task, which represents challenging application scenario for variety of nlp technologies, and shared research framework for the integration of semantics and mt technology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2337">
<title id=" S12-1105.xml">fbk cross lingual textual entailment without translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our best run achieved an accuracy of 50.4% on the spanish-englishdataset (with the average score and the median system respectively achieving 40.7% and 34.6%), demonstrating the effectiveness of apure?
</prevsent>
<prevsent>cross-lingual approach that avoids intermediate translations.
</prevsent>
</prevsection>
<citsent citstr=" W10-0734 ">
so far, cross-lingual textual entailment (clte) (mehdad et al, 2010) <papid> N10-1045 </papid>has been applied to: i) available te datasets (yes?/no? uni-directional relations between monolingual pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (negri andmehdad, 2010), <papid> W10-0734 </papid>and ii) machine translation evaluation datasets (mehdad et al, 2012<papid> W12-3122 </papid>b).</citsent>
<aftsection>
<nextsent>the content synchronization task represents challenging application scenario to test the capabilities of clte systems, by proposing richer inventory of phenomena (i.e. bidirectional?/forward?/backward?/no entailment?
</nextsent>
<nextsent>multi-directional entailment relations).
</nextsent>
<nextsent>multi-directional clte recognition can be seenas the identification of semantic equivalence and information disparity between two topically related sentences, at the cross-lingual level.
</nextsent>
<nextsent>this is core aspect of the multilingual content synchronization task, which represents challenging application scenario for variety of nlp technologies, and shared research framework for the integration of semantics and mt technology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2338">
<title id=" S12-1105.xml">fbk cross lingual textual entailment without translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our best run achieved an accuracy of 50.4% on the spanish-englishdataset (with the average score and the median system respectively achieving 40.7% and 34.6%), demonstrating the effectiveness of apure?
</prevsent>
<prevsent>cross-lingual approach that avoids intermediate translations.
</prevsent>
</prevsection>
<citsent citstr=" W12-3122 ">
so far, cross-lingual textual entailment (clte) (mehdad et al, 2010) <papid> N10-1045 </papid>has been applied to: i) available te datasets (yes?/no? uni-directional relations between monolingual pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (negri andmehdad, 2010), <papid> W10-0734 </papid>and ii) machine translation evaluation datasets (mehdad et al, 2012<papid> W12-3122 </papid>b).</citsent>
<aftsection>
<nextsent>the content synchronization task represents challenging application scenario to test the capabilities of clte systems, by proposing richer inventory of phenomena (i.e. bidirectional?/forward?/backward?/no entailment?
</nextsent>
<nextsent>multi-directional entailment relations).
</nextsent>
<nextsent>multi-directional clte recognition can be seenas the identification of semantic equivalence and information disparity between two topically related sentences, at the cross-lingual level.
</nextsent>
<nextsent>this is core aspect of the multilingual content synchronization task, which represents challenging application scenario for variety of nlp technologies, and shared research framework for the integration of semantics and mt technology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2343">
<title id=" S12-1105.xml">fbk cross lingual textual entailment without translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the clte methods proposed so far adopt eithera pivoting approach?
</prevsent>
<prevsent>(translation of the two in put texts into the same language, as in (mehdad et al., 2010)), <papid> N10-1045 </papid>or an integrated solution?</prevsent>
</prevsection>
<citsent citstr=" P11-1134 ">
that exploits bilingual phrase tables to capture lexical relations and contextual information (mehdad et al, 2011).<papid> P11-1134 </papid></citsent>
<aftsection>
<nextsent>the promising results achieved with the integrated approach still relyon phrasal matching techniques that disregard relevant semantic aspects of the problem.
</nextsent>
<nextsent>by filling this gap integrating linguistically motivated features, in our participation, we propose an approach that combines lexical, syntactic and semantic features within machine learning framework (mehdad et al, 2012<papid> W12-3122 </papid>a).our submitted runs have been produced by training and optimizing multiclass and binary svm classifiers, over the spanish-english (spa-eng) development set.</nextsent>
<nextsent>in both cases, our results were positive, showing significant improvements over the median systems and average scores obtained by partic ipants.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2348">
<title id=" S12-1105.xml">fbk cross lingual textual entailment without translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>cross-lingual approach that avoids the recourse to external mt components.
</prevsent>
<prevsent>701
</prevsent>
</prevsection>
<citsent citstr=" S12-1053 ">
in our experiment we used the spa-eng portion of the dataset described in (negri et al, 2012; <papid> S12-1053 </papid>negriet al, 2011), <papid> D11-1062 </papid>consisting of 500 multi-directional entailment pairs which was provided to train the systems and 500 pairs for the submission.</citsent>
<aftsection>
<nextsent>each pair inthe dataset is annotated with bidirectional?, for ward?, backward?
</nextsent>
<nextsent>or no entailment?
</nextsent>
<nextsent>judgements.
</nextsent>
<nextsent>2.1 approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2349">
<title id=" S12-1105.xml">fbk cross lingual textual entailment without translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>cross-lingual approach that avoids the recourse to external mt components.
</prevsent>
<prevsent>701
</prevsent>
</prevsection>
<citsent citstr=" D11-1062 ">
in our experiment we used the spa-eng portion of the dataset described in (negri et al, 2012; <papid> S12-1053 </papid>negriet al, 2011), <papid> D11-1062 </papid>consisting of 500 multi-directional entailment pairs which was provided to train the systems and 500 pairs for the submission.</citsent>
<aftsection>
<nextsent>each pair inthe dataset is annotated with bidirectional?, for ward?, backward?
</nextsent>
<nextsent>or no entailment?
</nextsent>
<nextsent>judgements.
</nextsent>
<nextsent>2.1 approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2350">
<title id=" S12-1105.xml">fbk cross lingual textual entailment without translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>then, in case of failure with exact matching, lexical matching is performed at the same three levels.
</prevsent>
<prevsent>to reduce redundant matches, the lexical matches between pairs of phrases which have already been identified as exact matches are not considered.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
once the matching phase for each n-gram level has been concluded, the number of matchesmatchn and the number of phrases in the hypothesis h(n) is used to estimate the portion of phrases in that are matched at each level (equation 1).1 since languages can express the same meaning with different amounts of words, phrase with length in can match phrase with any length in t. matchn = matchn |h(n)| (1) in order to build english-spanish phrase tables for our experiments, we used the freely available europarl v.4, news commentary and united nations spanish-english parallel corpora released for the wmt10 shared translation task.2 we run the tree tagger (schmid, 1995) and snowball stemmer (porter, 2001) for preprocessing, and used the giza++ (och and ney, 2000) <papid> P00-1056 </papid>toolkit to align the tokenized corpora at the word level.</citsent>
<aftsection>
<nextsent>subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the moses toolkit (koehn et al., 2007).<papid> P07-2045 </papid></nextsent>
<nextsent>2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2352">
<title id=" S12-1105.xml">fbk cross lingual textual entailment without translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>to reduce redundant matches, the lexical matches between pairs of phrases which have already been identified as exact matches are not considered.
</prevsent>
<prevsent>once the matching phase for each n-gram level has been concluded, the number of matchesmatchn and the number of phrases in the hypothesis h(n) is used to estimate the portion of phrases in that are matched at each level (equation 1).1 since languages can express the same meaning with different amounts of words, phrase with length in can match phrase with any length in t. matchn = matchn |h(n)| (1) in order to build english-spanish phrase tables for our experiments, we used the freely available europarl v.4, news commentary and united nations spanish-english parallel corpora released for the wmt10 shared translation task.2 we run the tree tagger (schmid, 1995) and snowball stemmer (porter, 2001) for preprocessing, and used the giza++ (och and ney, 2000) <papid> P00-1056 </papid>toolkit to align the tokenized corpora at the word level.</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the moses toolkit (koehn et al., 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>2.
</nextsent>
<nextsent>dependency relation (dr) matching tar-.
</nextsent>
<nextsent>gets the increase of clte precision.
</nextsent>
<nextsent>by adding syntactic constraints to the matching process, dr features aim to reduce wrong matches often occurring at the lexical level.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2356">
<title id=" S12-1081.xml">dss text similarity using lexical alignments of form distributional semantics and grammatical relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results are promising, with pearsons coefficients on each individual dataset ranging from .3765 to .7761 for our relatively simple heuristics based systems that do not require training on different datasets.
</prevsent>
<prevsent>we provide some analysis of the results and also provide results forour data using spear mans, which as non parametric measure which we argue is better able to reflect the merits of the different systems (average is ranked between the others).
</prevsent>
</prevsection>
<citsent citstr=" S12-1051 ">
our motivation for the systems entered in the ststask (agirre et al, 2012) <papid> S12-1051 </papid>was to model the contribu ? the first author is visiting scholar on the erasmus mundus masters program in language and communication technologies?</citsent>
<aftsection>
<nextsent>(lct, 20070060).
</nextsent>
<nextsent>tion of each linguistic component of both sentence sto the similarity of the texts by finding an alignment.
</nextsent>
<nextsent>ultimately such system could be exploited for ranking candidate paraphrases of chunk of text of any length.
</nextsent>
<nextsent>we envisage system as outlined in the future work section.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2357">
<title id=" S12-1081.xml">dss text similarity using lexical alignments of form distributional semantics and grammatical relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the dis 1see the guidelines given to the annotators at http://www.cs.columbia.edu/weiwei/workshop/ instructions.pdf2this is more or less semantic equivalence since the annotators were instructed to focus on meaning http://www.
</prevsent>
<prevsent>dianamccarthy.co.uk/files/instructions.pdf.tributional similarity measures perform at similar level to the knowledge-based measures that use wordnet.
</prevsent>
</prevsection>
<citsent citstr=" E09-1065 ">
mohler and mihalcea (2009) <papid> E09-1065 </papid>adapt this work for automatic short answer grading, that is matching candidate answer to one supplied bythe tutor.</citsent>
<aftsection>
<nextsent>mohler et al (2011) <papid> P11-1076 </papid>take this application forward, combining lexical semantic similarity measures with graph-alignment which considers dependency graphs using the stanford dependency parser (de marneffe et al, 2006) in terms of lexical,semantic and syntactic features.</nextsent>
<nextsent>a score is then provided for each node in the graph.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2358">
<title id=" S12-1081.xml">dss text similarity using lexical alignments of form distributional semantics and grammatical relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>dianamccarthy.co.uk/files/instructions.pdf.tributional similarity measures perform at similar level to the knowledge-based measures that use wordnet.
</prevsent>
<prevsent>mohler and mihalcea (2009) <papid> E09-1065 </papid>adapt this work for automatic short answer grading, that is matching candidate answer to one supplied bythe tutor.</prevsent>
</prevsection>
<citsent citstr=" P11-1076 ">
mohler et al (2011) <papid> P11-1076 </papid>take this application forward, combining lexical semantic similarity measures with graph-alignment which considers dependency graphs using the stanford dependency parser (de marneffe et al, 2006) in terms of lexical,semantic and syntactic features.</citsent>
<aftsection>
<nextsent>a score is then provided for each node in the graph.
</nextsent>
<nextsent>the features are combined using machine learning.the systems we propose likewise use lexical similarity and dependency relations, but in simple heuristic formulation without man-made thesaurus such as wordnet and without machine learning.
</nextsent>
<nextsent>we lemmatize and part-of-speech tag the data using tree tagger (schmid, 1994).
</nextsent>
<nextsent>we process the tagged data with default settings of the malt parser (nivreet al, 2007) to dependency parse the data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2359">
<title id=" S12-1081.xml">dss text similarity using lexical alignments of form distributional semantics and grammatical relations </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>we also show average ?.
</prevsent>
<prevsent>this is macro average of the spear mans value for the 5 datasets without weighting by the number of sentence pairs.
</prevsent>
</prevsection>
<citsent citstr=" P08-1028 ">
1110note that spear mans ? is often little lower than pearsons (mitchell and lapata, 2008) <papid> P08-1028 </papid>11we do recognise the difficulty in determining metrics on new pilot study.</citsent>
<aftsection>
<nextsent>the task organisers are making every effort to make it clear that this enterprise is pilot, not competition and that they welcome feedback.
</nextsent>
<nextsent>562
</nextsent>
<nextsent>the systems were developed in less than week including the time with the test data.
</nextsent>
<nextsent>there aremany trivial fixes that may improve the basic algorithm, such as decapitalising proper nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2360">
<title id=" S12-1016.xml">non atomic classification to improve a semantic role labeler for a low resource language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experiments show that the cross-frame generalization methods lead to 27% reduction in the number of errors made by the classifier.
</prevsent>
<prevsent>for previously unseen frames, the reduction is even more significant: 50%.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
the framenet lexical database and annotated corpora, based on the theory of semantic frames (fillmore et al, 2003), have allowed the implementation of automatic systems to extract semantic roles (gildea and jurafsky, 2002; <papid> J02-3001 </papid>johansson and nugues, 2007; <papid> W07-2048 </papid>ma`rquez et al, 2008; das et al, 2010).<papid> N10-1138 </papid></citsent>
<aftsection>
<nextsent>since the original framenet is developed for the english language, most research on semantic role extraction has focused exclusively on english.
</nextsent>
<nextsent>however, the english framenet has inspired similar efforts for other languages.
</nextsent>
<nextsent>for instance, the ongoing development of swedish framenet (borin et al., 2010) allows us to investigate the feasibility of using this resource in constructing an automatic role-semantic analyzer for swedish.
</nextsent>
<nextsent>however, due to the fact that the swedish framenet annotation process is in fairly early stage, not much annotated material is available, and this limits the performance attainable by automatic classifiers trained on these data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2361">
<title id=" S12-1016.xml">non atomic classification to improve a semantic role labeler for a low resource language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experiments show that the cross-frame generalization methods lead to 27% reduction in the number of errors made by the classifier.
</prevsent>
<prevsent>for previously unseen frames, the reduction is even more significant: 50%.
</prevsent>
</prevsection>
<citsent citstr=" W07-2048 ">
the framenet lexical database and annotated corpora, based on the theory of semantic frames (fillmore et al, 2003), have allowed the implementation of automatic systems to extract semantic roles (gildea and jurafsky, 2002; <papid> J02-3001 </papid>johansson and nugues, 2007; <papid> W07-2048 </papid>ma`rquez et al, 2008; das et al, 2010).<papid> N10-1138 </papid></citsent>
<aftsection>
<nextsent>since the original framenet is developed for the english language, most research on semantic role extraction has focused exclusively on english.
</nextsent>
<nextsent>however, the english framenet has inspired similar efforts for other languages.
</nextsent>
<nextsent>for instance, the ongoing development of swedish framenet (borin et al., 2010) allows us to investigate the feasibility of using this resource in constructing an automatic role-semantic analyzer for swedish.
</nextsent>
<nextsent>however, due to the fact that the swedish framenet annotation process is in fairly early stage, not much annotated material is available, and this limits the performance attainable by automatic classifiers trained on these data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2362">
<title id=" S12-1016.xml">non atomic classification to improve a semantic role labeler for a low resource language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experiments show that the cross-frame generalization methods lead to 27% reduction in the number of errors made by the classifier.
</prevsent>
<prevsent>for previously unseen frames, the reduction is even more significant: 50%.
</prevsent>
</prevsection>
<citsent citstr=" N10-1138 ">
the framenet lexical database and annotated corpora, based on the theory of semantic frames (fillmore et al, 2003), have allowed the implementation of automatic systems to extract semantic roles (gildea and jurafsky, 2002; <papid> J02-3001 </papid>johansson and nugues, 2007; <papid> W07-2048 </papid>ma`rquez et al, 2008; das et al, 2010).<papid> N10-1138 </papid></citsent>
<aftsection>
<nextsent>since the original framenet is developed for the english language, most research on semantic role extraction has focused exclusively on english.
</nextsent>
<nextsent>however, the english framenet has inspired similar efforts for other languages.
</nextsent>
<nextsent>for instance, the ongoing development of swedish framenet (borin et al., 2010) allows us to investigate the feasibility of using this resource in constructing an automatic role-semantic analyzer for swedish.
</nextsent>
<nextsent>however, due to the fact that the swedish framenet annotation process is in fairly early stage, not much annotated material is available, and this limits the performance attainable by automatic classifiers trained on these data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2363">
<title id=" S12-1016.xml">non atomic classification to improve a semantic role labeler for a low resource language </title>
<section> the swedish framenet.  </section>
<citcontext>
<prevsection>
<prevsent>this cross-frame generalization method reduces the number of errors made by the classifier by 27%, improving the accuracy from 54.4 to 66.5.
</prevsent>
<prevsent>when evaluating on frames for which the classifier has not been trained, the accuracy improves from 7.2 (random performance) to 53.4, 50% error reduction.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the swedish framenet, swefn, is lexical resource underdevelopment (friberg heppin and toporowska gronostaj, 2012), based on the english version of framenet constructed by the berkeley research group (baker et al, 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>it is found on the swefn website1, and is available as free resource.
</nextsent>
<nextsent>the swefn frames and frame names correspond to the english ones, with some exceptions, as dothe selection of frame elements including definitions and internal relations.
</nextsent>
<nextsent>the meta-informationabout the frames, such as semantic relations between frames, is also transferred from the berkley framenet.
</nextsent>
<nextsent>compared to the berkeley framenet,swefn is expanded with information about the do main of the frames, at present: general language, the medical and the art domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2364">
<title id=" S12-1016.xml">non atomic classification to improve a semantic role labeler for a low resource language </title>
<section> system implementation.  </section>
<citcontext>
<prevsection>
<prevsent>semantic roles following most previous implementations, we used syntactic parse tree as the basis of the semantic role extraction; we assumed that every semantic role span coincides with the projection of subtree in the syntactic tree.
</prevsent>
<prevsent>the tasks of segmentation and labeling then reduce to classification problem on syntactic tree nodes.
</prevsent>
</prevsection>
<citsent citstr=" W08-2123 ">
each sentence was parsed by the lth dependency parser (johansson and nugues, 2008<papid> W08-2123 </papid>a), which we trained on swedish treebank(nilsson et al, 2005).</citsent>
<aftsection>
<nextsent>figure 1 shows sentence annotated with dependency tree and semantic roles.the semantic role labeling classifier was implemented as linear multiclass classifier with flexible output space depending on the frame of the given predicate; we trained this classifier using an online learning algorithm (crammer et al, 2006).
</nextsent>
<nextsent>in addition, we imposed uniqueness constraint on the labels output by the classifier, so that every role may appear only once forgiven predicate.
</nextsent>
<nextsent>we considered large number of features for the classifier (table 1).
</nextsent>
<nextsent>most of these are commonly used features taken from the standard literature on semantic role labeling.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2372">
<title id=" S12-1016.xml">non atomic classification to improve a semantic role labeler for a low resource language </title>
<section> system implementation.  </section>
<citcontext>
<prevsection>
<prevsent>the features containing saldo id refer to the entry identifiers in the saldo lexicon.
</prevsent>
<prevsent>note that the pos tags have coarse and fine variants, such as verb and verb-finite present-active respectively, and we used both of them.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
semantic role classifiers rely heavily on lexical features (johansson and nugues, 2008<papid> W08-2123 </papid>b), and thismay lead to brittleness; in order to increase robustness, we added features based on hierarchical clusters constructed using the brown algorithm (brown et al, 1992).<papid> J92-4003 </papid></citsent>
<aftsection>
<nextsent>the brown algorithm clusters word into hierarchies represented as bit strings.
</nextsent>
<nextsent>based on tuning on development set, we found that it was best not to use the full bit string, but only prefix if the string was longer than 12 bits.
</nextsent>
<nextsent>frame dependency relation path frame elements position voice argument head saldo id argument head lemma argument head pos (fine) predicate pos (fine) argument pos (coarse) argument right child pos (coarse) argument word predicate word cluster argument word cluster table 1: list of classifier features.
</nextsent>
<nextsent>3.2 classifier using non-atomic semantic role labels the classifier described above is quite typical example of how semantic role classifiers are normally implemented: each frame is independent of all other frames.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="X2373">
<title id=" S12-1016.xml">non atomic classification to improve a semantic role labeler for a low resource language </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this also makes sense from theoretical point of view, since predicting multiple labels allows the machine learner to learn general facts as well as specifics.
</prevsent>
<prevsent>this work builds on previous work in multi-label classification.
</prevsent>
</prevsection>
<citsent citstr=" P09-1003 ">
for the task of framenet semantic role classification, the work most closely related toours is that by matsubayashi et al (2009), <papid> P09-1003 </papid>which defined classifier making use of role groups; the effect of the role groups turns out to be similar to our non-atomic classification approach.our experiments showed very significant error re ductions.</citsent>
<aftsection>
<nextsent>this was especially notable in the case of out-frame evaluation, which is to be expected since the baseline in this case was random selection.
</nextsent>
<nextsent>thebest classifier used all three types of label decomposition, and achieved 26% in-frame and 50% out-frame error reduction.
</nextsent>
<nextsent>acknowledgements the research presented here was supported by the swedish research council (the project swedishframenet++, vr dnr 2010-6013) and by the university of gothenburg through its support of the centre for language technology and sprakbanken (the swedish language bank).
</nextsent>
<nextsent>98
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>