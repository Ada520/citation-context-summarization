<paper>
<cited id="ZH0">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by measuring predicate-argument structure similarity based on the argument mapping, and formulating the predicate-argument structure mapping problem as linear-assignment problem, the system achieved 84.9% f-score using automatic srl, only 3.7% f-score lower than using gold standard srl.
</prevsent>
<prevsent>the mapping output covered 49.6% of the annotated chinese predicates (which contains predicate adjectives that often have no parallel annotations in english) and 80.7% of annotated english predicates, suggesting its potential as avaluable resource for improving word alignment and reranking mt output.
</prevsent>
</prevsection>
<citsent citstr=" N09-2004 ">
as the demand for semantically consistent machine translation rises (wu and fung, 2009<papid> N09-2004 </papid>a), the needfor comprehensive semantic mapping tool has be come more apparent.</citsent>
<aftsection>
<nextsent>with the current architecture of machine translation decoders, few ways of incorporating semantics in mt output include using word sense disambiguation to select the correct target translation (carpuat and wu, 2007) <papid> D07-1007 </papid>and reordering/reranking mt output based on semantic consistencies (wu and fung, 2009<papid> N09-2004 </papid>b) (carpuat et al, 2010).<papid> P10-2033 </papid></nextsent>
<nextsent>while comprehensive semantic mapping tool can supplement or improve the results of such techniques, there are many other exciting ideas we can explore: with automatic srl, we can improve coverage (and possibly accuracy) of chinese semantic class generation (wu et al, 2010) by running the system on large, unannotated parallel corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the mapping output covered 49.6% of the annotated chinese predicates (which contains predicate adjectives that often have no parallel annotations in english) and 80.7% of annotated english predicates, suggesting its potential as avaluable resource for improving word alignment and reranking mt output.
</prevsent>
<prevsent>as the demand for semantically consistent machine translation rises (wu and fung, 2009<papid> N09-2004 </papid>a), the needfor comprehensive semantic mapping tool has be come more apparent.</prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
with the current architecture of machine translation decoders, few ways of incorporating semantics in mt output include using word sense disambiguation to select the correct target translation (carpuat and wu, 2007) <papid> D07-1007 </papid>and reordering/reranking mt output based on semantic consistencies (wu and fung, 2009<papid> N09-2004 </papid>b) (carpuat et al, 2010).<papid> P10-2033 </papid></citsent>
<aftsection>
<nextsent>while comprehensive semantic mapping tool can supplement or improve the results of such techniques, there are many other exciting ideas we can explore: with automatic srl, we can improve coverage (and possibly accuracy) of chinese semantic class generation (wu et al, 2010) by running the system on large, unannotated parallel corpus.
</nextsent>
<nextsent>using predicate-argument mappings as constraints, itmay be possibly to improve srl output by performing joint inference of srl in source and target languages simultaneously, much like what burkett and klein (2008) <papid> D08-1092 </papid>was able to achieve with syntactic pars ing.</nextsent>
<nextsent>as the foundation of many machine translation decoders (deneefe and knight, 2009), <papid> D09-1076 </papid>word alignment has continuously played an important role in machine translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH7">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the mapping output covered 49.6% of the annotated chinese predicates (which contains predicate adjectives that often have no parallel annotations in english) and 80.7% of annotated english predicates, suggesting its potential as avaluable resource for improving word alignment and reranking mt output.
</prevsent>
<prevsent>as the demand for semantically consistent machine translation rises (wu and fung, 2009<papid> N09-2004 </papid>a), the needfor comprehensive semantic mapping tool has be come more apparent.</prevsent>
</prevsection>
<citsent citstr=" P10-2033 ">
with the current architecture of machine translation decoders, few ways of incorporating semantics in mt output include using word sense disambiguation to select the correct target translation (carpuat and wu, 2007) <papid> D07-1007 </papid>and reordering/reranking mt output based on semantic consistencies (wu and fung, 2009<papid> N09-2004 </papid>b) (carpuat et al, 2010).<papid> P10-2033 </papid></citsent>
<aftsection>
<nextsent>while comprehensive semantic mapping tool can supplement or improve the results of such techniques, there are many other exciting ideas we can explore: with automatic srl, we can improve coverage (and possibly accuracy) of chinese semantic class generation (wu et al, 2010) by running the system on large, unannotated parallel corpus.
</nextsent>
<nextsent>using predicate-argument mappings as constraints, itmay be possibly to improve srl output by performing joint inference of srl in source and target languages simultaneously, much like what burkett and klein (2008) <papid> D08-1092 </papid>was able to achieve with syntactic pars ing.</nextsent>
<nextsent>as the foundation of many machine translation decoders (deneefe and knight, 2009), <papid> D09-1076 </papid>word alignment has continuously played an important role in machine translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH8">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with the current architecture of machine translation decoders, few ways of incorporating semantics in mt output include using word sense disambiguation to select the correct target translation (carpuat and wu, 2007) <papid> D07-1007 </papid>and reordering/reranking mt output based on semantic consistencies (wu and fung, 2009<papid> N09-2004 </papid>b) (carpuat et al, 2010).<papid> P10-2033 </papid></prevsent>
<prevsent>while comprehensive semantic mapping tool can supplement or improve the results of such techniques, there are many other exciting ideas we can explore: with automatic srl, we can improve coverage (and possibly accuracy) of chinese semantic class generation (wu et al, 2010) by running the system on large, unannotated parallel corpus.</prevsent>
</prevsection>
<citsent citstr=" D08-1092 ">
using predicate-argument mappings as constraints, itmay be possibly to improve srl output by performing joint inference of srl in source and target languages simultaneously, much like what burkett and klein (2008) <papid> D08-1092 </papid>was able to achieve with syntactic pars ing.</citsent>
<aftsection>
<nextsent>as the foundation of many machine translation decoders (deneefe and knight, 2009), <papid> D09-1076 </papid>word alignment has continuously played an important role in machine translation.</nextsent>
<nextsent>there have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments of syntactic structures (zhang et al, 2007; marecek, 2009a).our hypothesis is that the predicate-argument structure alignments can abstract away from language specific syntactic variation and provide more robust, semantically coherent alignment across sen tences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH9">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while comprehensive semantic mapping tool can supplement or improve the results of such techniques, there are many other exciting ideas we can explore: with automatic srl, we can improve coverage (and possibly accuracy) of chinese semantic class generation (wu et al, 2010) by running the system on large, unannotated parallel corpus.
</prevsent>
<prevsent>using predicate-argument mappings as constraints, itmay be possibly to improve srl output by performing joint inference of srl in source and target languages simultaneously, much like what burkett and klein (2008) <papid> D08-1092 </papid>was able to achieve with syntactic pars ing.</prevsent>
</prevsection>
<citsent citstr=" D09-1076 ">
as the foundation of many machine translation decoders (deneefe and knight, 2009), <papid> D09-1076 </papid>word alignment has continuously played an important role in machine translation.</citsent>
<aftsection>
<nextsent>there have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments of syntactic structures (zhang et al, 2007; marecek, 2009a).our hypothesis is that the predicate-argument structure alignments can abstract away from language specific syntactic variation and provide more robust, semantically coherent alignment across sentences.
</nextsent>
<nextsent>we begin by running giza++ (och and ney, 2003), <papid> J03-1002 </papid>one of the most popular alignment tools, to obtain automatic word alignments between parallelenglish/chinese corpora.</nextsent>
<nextsent>to achieve broader coverage of semantic mappings than just those anno 21 tated in parallel propbank-ed corpora, we attempt to map automatically generated predicate-argumentstructures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH10">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as the foundation of many machine translation decoders (deneefe and knight, 2009), <papid> D09-1076 </papid>word alignment has continuously played an important role in machine translation.</prevsent>
<prevsent>there have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments of syntactic structures (zhang et al, 2007; marecek, 2009a).our hypothesis is that the predicate-argument structure alignments can abstract away from language specific syntactic variation and provide more robust, semantically coherent alignment across sen tences.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we begin by running giza++ (och and ney, 2003), <papid> J03-1002 </papid>one of the most popular alignment tools, to obtain automatic word alignments between parallelenglish/chinese corpora.</citsent>
<aftsection>
<nextsent>to achieve broader coverage of semantic mappings than just those anno 21 tated in parallel propbank-ed corpora, we attempt to map automatically generated predicate-argumentstructures.
</nextsent>
<nextsent>for each chinese and english verb predicate pairs within parallel sentence, we examine the quality of both the predicate and argument alignment (using giza++ word alignment output)and devise many-to-many argument mapping technique.
</nextsent>
<nextsent>from that, we pose predicate-argument mapping as linear assignment problem (optimizing the total similarity of the mapping) and solve it with the kuhn-munkres method (kuhn, 1955).
</nextsent>
<nextsent>with this approach, we were able to incur only small predicate-argument f-score degradation over using manual propbank annotation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH12">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the system differs from ours inthat it only provided one-to-one mapping of numbered arguments and may not be able to detect predicate mapping with no lexical relations that are nevertheless semantically related.
</prevsent>
<prevsent>later, wu andfung (2009<papid> N09-2004 </papid>b) used parallel semantic roles to im prove mt system outputs.</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
given the outputs from moses (koehn et al, 2007), <papid> P07-2045 </papid>machine translation decoder, they reordered the outputs based on the best predicate-argument mapping.</citsent>
<aftsection>
<nextsent>the resulting system showed 0.5 point bleu score improvement even though the bleu metric often discounts improvement in semantic consistency of mt output.
</nextsent>
<nextsent>choi et al (2009) (<papid> W09-3020 </papid>and later wu et al (2010))showed how to enhance chinese-english verb alignments by exploring predicate-argument structure alignment using parallel propbanks.</nextsent>
<nextsent>the resulting system showed improvement over pure giza++ alignment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH13">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>given the outputs from moses (koehn et al, 2007), <papid> P07-2045 </papid>machine translation decoder, they reordered the outputs based on the best predicate-argument mapping.</prevsent>
<prevsent>the resulting system showed 0.5 point bleu score improvement even though the bleu metric often discounts improvement in semantic consistency of mt output.</prevsent>
</prevsection>
<citsent citstr=" W09-3020 ">
choi et al (2009) (<papid> W09-3020 </papid>and later wu et al (2010))showed how to enhance chinese-english verb alignments by exploring predicate-argument structure alignment using parallel propbanks.</citsent>
<aftsection>
<nextsent>the resulting system showed improvement over pure giza++ alignment.
</nextsent>
<nextsent>those two systems differs from ours in that they operated on gold standard parses and semantic roles.
</nextsent>
<nextsent>the systems also did not provide explicit argument mapping between the aligned predicate-argument structures.
</nextsent>
<nextsent>to perform automatic semantic mapping, we needan annotated corpus to evaluate the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH14">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>we used the portion of the penn chinese treebank with word alignment annotation as the basis for evaluating semantic mapping.
</prevsent>
<prevsent>the word-aligned portion, containing around 2000 parallel sentences, is exclusive to xinhua news (and covers around 50% of the xinhua corpus in the chinese treebank).
</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
we then merged the word alignment annotation with the treebank and propbank annotation of ontonotes4.0 (hovy et al, 2006), <papid> N06-2015 </papid>which includes wide array of data sources like broadcast news, news wire, magazine, web text, etc. small percentage of the2000 sentences were discarded because of tokenization differences.</citsent>
<aftsection>
<nextsent>we dubbed the resulting 1939 parallel sentences as the triple-gold xinhua corpus.
</nextsent>
<nextsent>22 3.2 word alignment.
</nextsent>
<nextsent>we chose giza++ (och and ney, 2003) <papid> J03-1002 </papid>as our word alignment tool primarily because of its popularity,though there are other alternatives like lacoste julien et al (2006).</nextsent>
<nextsent>3.3 phrase structure parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH16">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>we chose giza++ (och and ney, 2003) <papid> J03-1002 </papid>as our word alignment tool primarily because of its popularity,though there are other alternatives like lacoste julien et al (2006).</prevsent>
<prevsent>3.3 phrase structure parsing.</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
we chose the berkeley parser (petrov and klein, 2007) <papid> N07-1051 </papid>for phrase structure parsing since it has been tested on both english and chinese corpora and can be easily retrained.</citsent>
<aftsection>
<nextsent>3.4 semantic role labeling.
</nextsent>
<nextsent>for semantic role labeling (srl), we built our own system using fairly standard approach: srl isposed as multi-class classification problem requiring the identification of argument candidates foreach predicate and their argument types.
</nextsent>
<nextsent>typically, argument identification and argument labeling are performed in two separate stages because of time/resource constraints during training/labeling.
</nextsent>
<nextsent>for our system, we chose liblinear (fan et al,2008), library for large linear classification problems, as the classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH17">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>under such conditions, mixing arguments and non-arguments within the same class may produce sub-optimal results for binary classifier.
</prevsent>
<prevsent>to address these issues, we built apairwise multi-class classifier (using simple majority voting) on top of liblinear.
</prevsent>
</prevsection>
<citsent citstr=" W05-0635 ">
the resulting english srl system, evaluated using the conll 2005 methodology, achieved 77.3% f-score on the wsj corpus, comparable tothe leading system (surdeanu and turmo, 2005) <papid> W05-0635 </papid>using single parser output.</citsent>
<aftsection>
<nextsent>the chinese srl system, on the other hand, achieved 74.4% f-score on the triple-gold xinhua corpus (similar but not directly comparable to wu et al (2006) and xue (2008)<papid> J08-2004 </papid>because of differences in treebank/propbank revisions as well as differences in test set).</nextsent>
<nextsent>4.1 argument mapping.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH19">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>to address these issues, we built apairwise multi-class classifier (using simple majority voting) on top of liblinear.
</prevsent>
<prevsent>the resulting english srl system, evaluated using the conll 2005 methodology, achieved 77.3% f-score on the wsj corpus, comparable tothe leading system (surdeanu and turmo, 2005) <papid> W05-0635 </papid>using single parser output.</prevsent>
</prevsection>
<citsent citstr=" J08-2004 ">
the chinese srl system, on the other hand, achieved 74.4% f-score on the triple-gold xinhua corpus (similar but not directly comparable to wu et al (2006) and xue (2008)<papid> J08-2004 </papid>because of differences in treebank/propbank revisions as well as differences in test set).</citsent>
<aftsection>
<nextsent>4.1 argument mapping.
</nextsent>
<nextsent>to produce good predicate-argument mapping, we needed to consider 2 things: whether good argument mapping can be produced based on argument type only, and whether each argument only maps to one argument in the target language.
</nextsent>
<nextsent>23 4.1.1 predicate-dependent argument mapping theoretically, propbank numbered arguments are supposed to be consistent across predicates: arg0 typically denotes the agent of the predicate and arg1 the theme.
</nextsent>
<nextsent>while this consistency may hold true for predicates in the same language, as fung et al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH20">
<title id=" W11-1003.xml">semantic mapping using automatic word alignment and semantic role labeling </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>is the standard conll 2005 evaluation metric,oracle?
</prevsent>
<prevsent>is the oracle srl based on automatic parser output, and word match?
</prevsent>
</prevsection>
<citsent citstr=" C10-2158 ">
is scoring based on length of argument overlap with the reference cantly worse is arg0 (almost 10% f-score lower).this is likely caused by dropped pronouns in chinese sentences (yang and xue, 2010), <papid> C10-2158 </papid>making it harder for both the syntactic and semantic parsers to identify the correct subject.</citsent>
<aftsection>
<nextsent>we also report the srl result scored at word level instead of at argument level (79.4% f-score for chinese and 75.5% for english).
</nextsent>
<nextsent>the conll 2005 shared task scoring (surdeanu and turmo, 2005) <papid> W05-0635 </papid>discounts arguments that are not perfect word span match, even if the system output is semantically close to the reference argument.</nextsent>
<nextsent>while this is important in some applications of srl, for other applications like improving word alignment with srl, improving recall on approximate arguments may bea better trade-off than having high precision on perfectly matched arguments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH26">
<title id=" W10-4121.xml">active learning based corpus annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this technique has wide and many real world applications, such as e-commerce, business intelligence, information monitoring, public opinion poll, e-learning, newspaper and publication compilation, and business management.
</prevsent>
<prevsent>for instance, typical opinion mining system produces statistical results from online product reviews, which can be used by potential customers when deciding which model to choose, by manufacturers to find out the possible areas of improvement, and by dealers for sales plan evaluation (yao et al 2008).
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
according to kim and hovy (2004), <papid> C04-1200 </papid>an opinion is composed of four parts, namely, topic, holder, sentiment, and claim, in which the holder expresses the claim including positive or negative sentiment towards the topic.</citsent>
<aftsection>
<nextsent>for example, in the sentence like this car, is the holder, like is the positive sentiment, car is the topic, and the whole sentence is the claim.
</nextsent>
<nextsent>research on chinese opinion mining technology requires the support of annotated corpus for chinese opinioned-subjective text.
</nextsent>
<nextsent>since the corpus includes deep level information related to word segmentation, part-of-speech, syntax, semantics, opinioned elements, and some other information, the finished annotation is very complicated.
</nextsent>
<nextsent>hence, it is necessary to develop an automatic tool to facilitate the work of annotators so that the efficiency and accuracy of annotation can be improved.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH27">
<title id=" W11-1216.xml">towards a data model for the universal corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>access is permitted to the file store, the database, and an application programming interface.
</prevsent>
<prevsent>all aspects of the universal corpus are open, and we invite community participation in its design and implementation, and in supplying and using its data.
</prevsent>
</prevsection>
<citsent citstr=" P10-1010 ">
we have previously proposed community dataset of annotated text spanning very large number of languages, with consistent annotation and format that enables automatic cross-linguistic processing on an unprecedented scale (abney and bird, 2010).<papid> P10-1010 </papid></citsent>
<aftsection>
<nextsent>here we set out the data model in detail, and invite members of the computational linguistics community to begin work on the first version of the dataset.
</nextsent>
<nextsent>the targeted annotation generalizes over three widely-used kinds of data: (1) simple bitexts, that is, tokenized texts and their translations, which are widely used for training machine translation sys tems; (2) inter linear glossed text (igt), which adds lemmas, morphological features and parts of speech,and is the de facto standard in the documentary linguistics literature; and (3) dependency parses, which add head pointer and relation name for each word,and are gaining popularity as representations of syntactic structure.
</nextsent>
<nextsent>we do not expect all texts to have equal richness of annotation; rather, these are the degrees of annotation we wish to explicitly accommodate.
</nextsent>
<nextsent>keeping the annotation lightweight is primary desideratum.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH29">
<title id=" W10-4113.xml">selecting optimal feature template subset for crfs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, empirical studies on the efficiency and effectiveness of the method are conducted in the field of chinese text chunking, whose performance is ranked the first place in task two of cips-parseval-2009.
</prevsent>
<prevsent>conditional random fields (crfs) are the state of-the-art models for sequential labeling problem.
</prevsent>
</prevsection>
<citsent citstr=" W03-0430 ">
in natural language processing, two aspects of crfs have been investigated sufficiently: one is to apply it to new tasks, such as named entity recognition (mccallum and li, 2003; <papid> W03-0430 </papid>li and mccallum, 2003; settles, 2004), <papid> W04-1221 </papid>part-of-speech tagging (lafferty et al, 2001), shallow parsing (sha and pereira, 2003), <papid> N03-1028 </papid>and language modeling (roark et al, 2004); <papid> P04-1007 </papid>the other is to exploit new training methods for crfs, such as improved iterative scaling (laf ferty et al, 2001), l-bfgs (mccallum, 2003) and gradient tree boosting (dietterich et al, 2004).</citsent>
<aftsection>
<nextsent>one of the critical steps is to select optimal feature subset before employing crfs.
</nextsent>
<nextsent>mccallum (2003) suggested an efficient method of feature induction by iteratively increasing conditional log likelihood for discrete features.
</nextsent>
<nextsent>however, since there are millions of features and feature selection is an np problem, this is intractable when searching optimal feature subset.
</nextsent>
<nextsent>therefore, it is necessary that selects feature at feature template level, which reduces input scale from millions of features to tens or hundreds of candidate templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH30">
<title id=" W10-4113.xml">selecting optimal feature template subset for crfs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, empirical studies on the efficiency and effectiveness of the method are conducted in the field of chinese text chunking, whose performance is ranked the first place in task two of cips-parseval-2009.
</prevsent>
<prevsent>conditional random fields (crfs) are the state of-the-art models for sequential labeling problem.
</prevsent>
</prevsection>
<citsent citstr=" W04-1221 ">
in natural language processing, two aspects of crfs have been investigated sufficiently: one is to apply it to new tasks, such as named entity recognition (mccallum and li, 2003; <papid> W03-0430 </papid>li and mccallum, 2003; settles, 2004), <papid> W04-1221 </papid>part-of-speech tagging (lafferty et al, 2001), shallow parsing (sha and pereira, 2003), <papid> N03-1028 </papid>and language modeling (roark et al, 2004); <papid> P04-1007 </papid>the other is to exploit new training methods for crfs, such as improved iterative scaling (laf ferty et al, 2001), l-bfgs (mccallum, 2003) and gradient tree boosting (dietterich et al, 2004).</citsent>
<aftsection>
<nextsent>one of the critical steps is to select optimal feature subset before employing crfs.
</nextsent>
<nextsent>mccallum (2003) suggested an efficient method of feature induction by iteratively increasing conditional log likelihood for discrete features.
</nextsent>
<nextsent>however, since there are millions of features and feature selection is an np problem, this is intractable when searching optimal feature subset.
</nextsent>
<nextsent>therefore, it is necessary that selects feature at feature template level, which reduces input scale from millions of features to tens or hundreds of candidate templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH31">
<title id=" W10-4113.xml">selecting optimal feature template subset for crfs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, empirical studies on the efficiency and effectiveness of the method are conducted in the field of chinese text chunking, whose performance is ranked the first place in task two of cips-parseval-2009.
</prevsent>
<prevsent>conditional random fields (crfs) are the state of-the-art models for sequential labeling problem.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
in natural language processing, two aspects of crfs have been investigated sufficiently: one is to apply it to new tasks, such as named entity recognition (mccallum and li, 2003; <papid> W03-0430 </papid>li and mccallum, 2003; settles, 2004), <papid> W04-1221 </papid>part-of-speech tagging (lafferty et al, 2001), shallow parsing (sha and pereira, 2003), <papid> N03-1028 </papid>and language modeling (roark et al, 2004); <papid> P04-1007 </papid>the other is to exploit new training methods for crfs, such as improved iterative scaling (laf ferty et al, 2001), l-bfgs (mccallum, 2003) and gradient tree boosting (dietterich et al, 2004).</citsent>
<aftsection>
<nextsent>one of the critical steps is to select optimal feature subset before employing crfs.
</nextsent>
<nextsent>mccallum (2003) suggested an efficient method of feature induction by iteratively increasing conditional log likelihood for discrete features.
</nextsent>
<nextsent>however, since there are millions of features and feature selection is an np problem, this is intractable when searching optimal feature subset.
</nextsent>
<nextsent>therefore, it is necessary that selects feature at feature template level, which reduces input scale from millions of features to tens or hundreds of candidate templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH32">
<title id=" W10-4113.xml">selecting optimal feature template subset for crfs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, empirical studies on the efficiency and effectiveness of the method are conducted in the field of chinese text chunking, whose performance is ranked the first place in task two of cips-parseval-2009.
</prevsent>
<prevsent>conditional random fields (crfs) are the state of-the-art models for sequential labeling problem.
</prevsent>
</prevsection>
<citsent citstr=" P04-1007 ">
in natural language processing, two aspects of crfs have been investigated sufficiently: one is to apply it to new tasks, such as named entity recognition (mccallum and li, 2003; <papid> W03-0430 </papid>li and mccallum, 2003; settles, 2004), <papid> W04-1221 </papid>part-of-speech tagging (lafferty et al, 2001), shallow parsing (sha and pereira, 2003), <papid> N03-1028 </papid>and language modeling (roark et al, 2004); <papid> P04-1007 </papid>the other is to exploit new training methods for crfs, such as improved iterative scaling (laf ferty et al, 2001), l-bfgs (mccallum, 2003) and gradient tree boosting (dietterich et al, 2004).</citsent>
<aftsection>
<nextsent>one of the critical steps is to select optimal feature subset before employing crfs.
</nextsent>
<nextsent>mccallum (2003) suggested an efficient method of feature induction by iteratively increasing conditional log likelihood for discrete features.
</nextsent>
<nextsent>however, since there are millions of features and feature selection is an np problem, this is intractable when searching optimal feature subset.
</nextsent>
<nextsent>therefore, it is necessary that selects feature at feature template level, which reduces input scale from millions of features to tens or hundreds of candidate templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH34">
<title id=" W10-4113.xml">selecting optimal feature template subset for crfs </title>
<section> parameter estimation for crfs;.  </section>
<citcontext>
<prevsection>
<prevsent>considering the different yi-1 for every word in sentence, we need compute p(yi|yi-1, xi) (n.c) times for memms_2.
</prevsent>
<prevsent>reducing the average number of candidate label can help to improve the decoding efficiency.
</prevsent>
</prevsection>
<citsent citstr=" W00-0729 ">
and in most cases, the yi-1 in p(yi|yi-1, xi) is not necessary (koeling, 2000; <papid> W00-0729 </papid>osbome, 2000).</citsent>
<aftsection>
<nextsent>therefore, to reduce the average number of candidate labels c, it is reasonable to use an me model to filter the candidate label.
</nextsent>
<nextsent>given threshold (0  =  = 1), the candidate label filtering algorithm is as follows: 1.
</nextsent>
<nextsent>cp = 0;.
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH35">
<title id=" W10-4109.xml">bigram hmm with context distribution clustering for unsupervised chinese partofspeech tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiment results show the out put state sequence of hmm are highly correlated to the latent annotations of gold pos tags, in context of clustering similarity measures.
</prevsent>
<prevsent>the other experiments on real application, unsupervised dependency parsing, reveal that the output sequence can replace the manually annotated tags without loss of accuracies.
</prevsent>
</prevsection>
<citsent citstr=" P07-1094 ">
recently latent variable model has shown great potential in recovering the underlying structures.for example, the task of pos tagging is to recover the appropriate sequence structure given the input word sequence (goldwater and griffiths, 2007).<papid> P07-1094 </papid></citsent>
<aftsection>
<nextsent>one of the most popular example of latent models is hidden markov model (hmm), which has been extensively studied for many years (rabiner, 1989).
</nextsent>
<nextsent>the key problem of hmm is how to find an optimal hidden state number and the topology appropriately.in most cases, the topology of hmm is predefined by exploiting the domain or empirical knowledge.
</nextsent>
<nextsent>this topology will be fixed during the whole process.
</nextsent>
<nextsent>therefore how to select the optimal topology for certain application or set of training data is still problem, because many researches show that varying the size of the state space greatly affects the performance of hmm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH36">
<title id=" W10-4109.xml">bigram hmm with context distribution clustering for unsupervised chinese partofspeech tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the bottom-up approaches require huge computational cost in deciding the states tobe merged, which makes it impractical for applications with large state space.
</prevsent>
<prevsent>in this paper, we focus on the latter approaches.another problem in hmm is that em algorithm might yield local maximum value.
</prevsent>
</prevsection>
<citsent citstr=" D07-1031 ">
johnson (2007) <papid> D07-1031 </papid>points out that training hmm with em gives poor results because it leads to fairly flat distribution of hidden states when the empirical distribution is highly skewed.</citsent>
<aftsection>
<nextsent>a multinomial prior, which favors sparse distribution, is good choice for natural language tasks.
</nextsent>
<nextsent>in this paper, we proposed new procedure for inferring thehmm topology and estimating its parameters simultaneously.
</nextsent>
<nextsent>gibbs sampling has been used in infinite hmm (ihmm) (beal et al, 2001; fox et al., 2008; van gael et al, 2008) for inference.unfortunately gibbs sampling is slow and difficult to be converged.
</nextsent>
<nextsent>in this paper, we proposed the variational bayesian inference for the adaptive hmm model with dirichlet prior.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH37">
<title id=" W10-4109.xml">bigram hmm with context distribution clustering for unsupervised chinese partofspeech tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in each iteration, we replaced only one hidden state with two new states until convergence.to reduce the number of observation variables, the words are pre-clustered and represented by the exemplar within the same cluster.
</prevsent>
<prevsent>it is one-to-many clustering, because the same word play different roles under different contexts.
</prevsent>
</prevsection>
<citsent citstr=" W03-1011 ">
we evaluate the similarity between the distribution of contexts, with the assumption that the context distribution implies syntactic pattern of the given word (zelling, 1968; weeds andweir, 2003).<papid> W03-1011 </papid></citsent>
<aftsection>
<nextsent>with this clustering, more contextual information can be considered without increasing the model complexity.
</nextsent>
<nextsent>a relatively simple model is important for unsupervised task interms of computational burden and data sparseness.
</nextsent>
<nextsent>this is the reason why we do not increase the order of hmm(kaji and kitsuregawa, 2008; <papid> C08-1051 </papid>headden et al, 2008).<papid> C08-1042 </papid></nextsent>
<nextsent>with unsupervised algorithms, there are two aspects to be evaluated (van gael et al, 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH38">
<title id=" W10-4109.xml">bigram hmm with context distribution clustering for unsupervised chinese partofspeech tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with this clustering, more contextual information can be considered without increasing the model complexity.
</prevsent>
<prevsent>a relatively simple model is important for unsupervised task interms of computational burden and data sparseness.
</prevsent>
</prevsection>
<citsent citstr=" C08-1051 ">
this is the reason why we do not increase the order of hmm(kaji and kitsuregawa, 2008; <papid> C08-1051 </papid>headden et al, 2008).<papid> C08-1042 </papid></citsent>
<aftsection>
<nextsent>with unsupervised algorithms, there are two aspects to be evaluated (van gael et al, 2009).
</nextsent>
<nextsent>fist one is how good the outcome clusters are.we compare the hmm results with the manually pos tags and report the similarity measures based on information theory.
</nextsent>
<nextsent>on the other hand,we test how good the outputs act as an intermediate results.
</nextsent>
<nextsent>in many natural language tasks, the inputs are word class, not the actual lexical item, for reason of sparsity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH39">
<title id=" W10-4109.xml">bigram hmm with context distribution clustering for unsupervised chinese partofspeech tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with this clustering, more contextual information can be considered without increasing the model complexity.
</prevsent>
<prevsent>a relatively simple model is important for unsupervised task interms of computational burden and data sparseness.
</prevsent>
</prevsection>
<citsent citstr=" C08-1042 ">
this is the reason why we do not increase the order of hmm(kaji and kitsuregawa, 2008; <papid> C08-1051 </papid>headden et al, 2008).<papid> C08-1042 </papid></citsent>
<aftsection>
<nextsent>with unsupervised algorithms, there are two aspects to be evaluated (van gael et al, 2009).
</nextsent>
<nextsent>fist one is how good the outcome clusters are.we compare the hmm results with the manually pos tags and report the similarity measures based on information theory.
</nextsent>
<nextsent>on the other hand,we test how good the outputs act as an intermediate results.
</nextsent>
<nextsent>in many natural language tasks, the inputs are word class, not the actual lexical item, for reason of sparsity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH40">
<title id=" W10-4109.xml">bigram hmm with context distribution clustering for unsupervised chinese partofspeech tagging </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>to standardize the measures to have fixed bounds, (strehl and ghosh, 2003) defined the normalized mutual information (nmi) as: nmi(cr,cg) = i(cr,cg) ? h(cr)h(cg) (11) nmi takes its lower bound of 0 if no information is shared by two clusters and the upper bound of 1 if two clusterings are identical.
</prevsent>
<prevsent>the nmi however, still has problems, whose variation is sensitive to the choice of the number of clusters.
</prevsent>
</prevsection>
<citsent citstr=" D07-1043 ">
rosenberg and hirschberg (2007) <papid> D07-1043 </papid>proposed v-measure to combine two desirable properties of clustering: homogeneity (h) and completeness (c) as follows: = 1 ? h(cg|cr)/h(cg) = 1 ? h(cr |cg)/h(cr) = 2hc/(h + c) (12) generally homogeneity and completeness runs in opposite way, whose harmonic mean (i.e. v-measure) is comprise score, just like f-score for the precision and recall.let us first examine the contextual word clustering performance.</citsent>
<aftsection>
<nextsent>the vi score between distributional word categories and gold standard is 2.39.
</nextsent>
<nextsent>the nmi and v-measure score are 0.53 and 0.48, respectively.the clustering performance of the hmm outputs are reported in figure 2.
</nextsent>
<nextsent>the best vi score achieved was 3.9524, while v-measure was 62.09% and nmi reached 0.8051.
</nextsent>
<nextsent>previous 40 60 80 100 120 140 160 180 200 220 2403.8 4 4.2 4.4 4.6 4.8 5 (a) vi score 40 60 80 100 120 140 160 180 200 220 2400 0.1 0.2 0.3 0.4 0.5 0.6 0.7 nmihomogeneitycompletenessvmeasure (b) normalized scores figure 2: clustering evaluation metrics against number of hidden states work of chinese tagging focuses on the tagging accuracies, e.g. wang (wang and schuurmans, ) and huang et al (huang et al, 2007).<papid> D07-1117 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH41">
<title id=" W10-4109.xml">bigram hmm with context distribution clustering for unsupervised chinese partofspeech tagging </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the nmi and v-measure score are 0.53 and 0.48, respectively.the clustering performance of the hmm outputs are reported in figure 2.
</prevsent>
<prevsent>the best vi score achieved was 3.9524, while v-measure was 62.09% and nmi reached 0.8051.
</prevsent>
</prevsection>
<citsent citstr=" D07-1117 ">
previous 40 60 80 100 120 140 160 180 200 220 2403.8 4 4.2 4.4 4.6 4.8 5 (a) vi score 40 60 80 100 120 140 160 180 200 220 2400 0.1 0.2 0.3 0.4 0.5 0.6 0.7 nmihomogeneitycompletenessvmeasure (b) normalized scores figure 2: clustering evaluation metrics against number of hidden states work of chinese tagging focuses on the tagging accuracies, e.g. wang (wang and schuurmans, ) and huang et al (huang et al, 2007).<papid> D07-1117 </papid></citsent>
<aftsection>
<nextsent>to our knowledge, this is the first work to report the distributional clustering similarity measures based on informatics view for chinese . similar works can be found on english of wsj corpus (van gael et al, 2009).
</nextsent>
<nextsent>their best results of vi, v-measure, achieved with pitman-yor prior,were 3.73 and 59%.
</nextsent>
<nextsent>we believe the chinese results are not good as english correspondences because of the rich unknown words in chinese (tseng et al, 2005).<papid> I05-3005 </papid></nextsent>
<nextsent>5.2 dependency parsing evaluation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH42">
<title id=" W10-4109.xml">bigram hmm with context distribution clustering for unsupervised chinese partofspeech tagging </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>to our knowledge, this is the first work to report the distributional clustering similarity measures based on informatics view for chinese . similar works can be found on english of wsj corpus (van gael et al, 2009).
</prevsent>
<prevsent>their best results of vi, v-measure, achieved with pitman-yor prior,were 3.73 and 59%.
</prevsent>
</prevsection>
<citsent citstr=" I05-3005 ">
we believe the chinese results are not good as english correspondences because of the rich unknown words in chinese (tseng et al, 2005).<papid> I05-3005 </papid></citsent>
<aftsection>
<nextsent>5.2 dependency parsing evaluation.
</nextsent>
<nextsent>the next experiment is to test the goodness of the outcome states of our model in the context of real tasks.
</nextsent>
<nextsent>in this work, we consider unsupervised dependency parsing for fully unsupervised system.
</nextsent>
<nextsent>the dependency parsing is to extract the dependency graph whose nodes are the words of the given sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH43">
<title id=" W10-4109.xml">bigram hmm with context distribution clustering for unsupervised chinese partofspeech tagging </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, experiments show the hidden states are correlated to the latent annotations of the standard pos tags.the future work includes to improve the performance by incorporating small amount of supervision.
</prevsent>
<prevsent>the typical supervision used before is dictionary extracted from large corpus like chinese gigaword.
</prevsent>
</prevsection>
<citsent citstr=" N06-1041 ">
another interesting idea is to select some exemplars (haghighi and klein, 2006).<papid> N06-1041 </papid></citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH44">
<title id=" W10-4115.xml">exploring deep belief network for chinese relation extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>relatively, less attention was drawn on chinese relation extraction.
</prevsent>
<prevsent>however, its importance is being gradually recognized.
</prevsent>
</prevsection>
<citsent citstr=" L08-1305 ">
for instance, zhang et al  (2008) <papid> L08-1305 </papid>combined position information, entity type and context features in feature-based approach and che (2005) introduced the edit distance kernel over the original chinese string representation.</citsent>
<aftsection>
<nextsent>dbn is new feature-based approach for nlp tasks.
</nextsent>
<nextsent>according to the work by hinton (2006), dbn consisted of several layers including multiple restricted boltzmann machine (rbm) layers and back propagation (bp) layer.
</nextsent>
<nextsent>it was reported to perform very well in many classification problems (ackley, 1985), which is from the origin of its ability to scale gracefully and be computationally tractable when applied to high dimensional feature vectors.
</nextsent>
<nextsent>furthermore, to against the combinations of feature were intricate, it detected in variant representations from local translations of the input by deep architecture.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH45">
<title id=" W10-4115.xml">exploring deep belief network for chinese relation extraction </title>
<section> deep belief network for chinese.  </section>
<citcontext>
<prevsection>
<prevsent>3.5.1 character-based features since chinese text is written without word boundaries, the word-level features are limited by the efficiency of word segmentation results.
</prevsent>
<prevsent>in the paper presented by h. jing (2003) and some others, they observed that pure character based models can even outperform word-based models.
</prevsent>
</prevsection>
<citsent citstr=" P08-2023 ">
li et al (2008) <papid> P08-2023 </papid>work relying on character-based features also achieved significant performance in relation extraction.</citsent>
<aftsection>
<nextsent>we denote the character dictionary as d={d1, d2, ?, dn}.
</nextsent>
<nextsent>in our experiment, is 1500.
</nextsent>
<nextsent>to an e, its character-based feature vector is v(e)={ v1, v2, ?, vn }.
</nextsent>
<nextsent>each unit vi can be valued as equation 8.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH46">
<title id=" W11-0415.xml">a gold standard corpus of early modern german </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most existing nlp tools are tuned to perform well on modern language data, but perform considerably worse on historical, non-standardised data (rayson et al,2007).
</prevsent>
<prevsent>this paper describes gold standard sub corpus of germanc which has been manually annotated by two human annotators for pos tags, lem mas, and normalised spelling variants.
</prevsent>
</prevsection>
<citsent citstr=" W10-2209 ">
the corpus will be used to test and adapt modern nlp tools on historical data, and will be of interest to other currentcorpus-based projects in historical linguistics (jur ish, 2010; <papid> W10-2209 </papid>fass hauer, 2011; dipper, 2010).</citsent>
<aftsection>
<nextsent>2.1 germanc.
</nextsent>
<nextsent>in order to enable corpus-linguistic investigations, the germanc corpus aims to be representative on three different levels.
</nextsent>
<nextsent>first of all, the corpus includes range of text types: four orally-oriented genres (dramas, newspapers, letters, and sermons), and four print-oriented ones (narrative prose, and humanities,scientific, and legal texts).
</nextsent>
<nextsent>secondly, in order to enable historical developments to be traced, the period has been divided into three fifty year sections(1650-1700, 1700-1750, and 1750-1800).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH48">
<title id=" W11-0415.xml">a gold standard corpus of early modern german </title>
<section> pwrel: interrogative pronoun used as rela-.  </section>
<citcontext>
<prevsection>
<prevsent>the remaining 475 cases occur as na (291), or as one of the new relative markers pwavrel (69), pwrel (57), ptkrel (38), and pavrel (20).
</prevsent>
<prevsent>4 annotation procedure and agreement.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
in order to produce the gold standard annotations in germanc-gs we used the gate platform, which facilitates automatic as well as manual annotation(cunningham et al 2002).<papid> P02-1022 </papid></citsent>
<aftsection>
<nextsent>initially, gates german language plugin4 was used to obtain word tokens and sentence boundaries.
</nextsent>
<nextsent>the output was manually inspected and corrected by one annotator, who manually added layer of normalised spelling variants (norm).
</nextsent>
<nextsent>this annotation layer was then usedas input for the tree tagger (schmid, 1994), obtaining annotations in terms of lemmas (lemma) and pos tags (pos).
</nextsent>
<nextsent>all annotations (norm, lemma,and pos) were subsequently corrected by two annotators, and all disagreements were reconciled to produce the gold standard.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH49">
<title id=" W11-0415.xml">a gold standard corpus of early modern german </title>
<section> pwrel: interrogative pronoun used as rela-.  </section>
<citcontext>
<prevsection>
<prevsent>all annotations (norm, lemma,and pos) were subsequently corrected by two annotators, and all disagreements were reconciled to produce the gold standard.
</prevsent>
<prevsent>table 3 shows the over all agreement for the three annotation types across germanc-gs (measured in accuracy).the agreement values demonstrate that normalised word forms and lemmas are relatively easy to determine for the annotators, with 96.9% and 95.5% agreement, respectively.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
pos tags, on the other, represent more of challenge with only 91.6% 4http://gate.ac.uk/sale/tao/splitch15.html norm lemma pos agreed tokens (out of 57,845) 56,052 55,217 52,959 accuracy (%) 96.9% 95.5% 91.6% table 3: inter-annotator agreement agreement between two annotators, which is considerably lower than the agreement level reported for annotating corpus of modern german using stts, at 98.6% (brants, 2000<papid> A00-1031 </papid>a).</citsent>
<aftsection>
<nextsent>while more detailed analysis of the results remains to be carried out, an initial study shows that pos agreement is lower in earlier texts (89.3% in period p1) compared to later ones (93.1% in p3).
</nextsent>
<nextsent>it is likely that substantial amount of disagreements in the earlier texts are due to the larger number of unfamiliar word forms and variants on the one hand, and foreign word tokens on the other.
</nextsent>
<nextsent>these represent problem as from modern view point it is not always easy to decide which words were foreign?
</nextsent>
<nextsent>to language and which ones native?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH54">
<title id=" W11-0139.xml">ball game a corpus for computational semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe our corpus model, the annotation tool used to create the corpus, and conclude by discussing applications of this corpus in semantics research and natural language processing.
</prevsent>
<prevsent>the use of large annotated corpora and treebanks has led to many fruitful research programs in computational linguistics.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
at the time of this writing, marcus et al (1993), <papid> J93-2004 </papid>which introduces the university of pennsylvania treebank,1 has been cited by over 3000 subsequent papers.2 such treebanks are invaluable for the training and testing of large-scale syntactic parsers and numerous other applications in the field of computational syntax.unfortunately for the field of computational semantics, there are few corresponding annotated corpora or treebanks representing the formalized meaning of natural language sentences, mainly because there is very little agreement on what such representation of meaning would look like for arbitrary text.</citsent>
<aftsection>
<nextsent>to overcome this obstacle, several recent studies have turned to the arena of sports, pairing natural language with game statistics in several domains, including robocup soccer (liang et al, 2009; <papid> P09-1011 </papid>chen et al, 2010), soccer (theune and klabbers, 1998; saggion et al, 2003), <papid> E03-2014 </papid>american football (barzilay and lapata, 2005; <papid> H05-1042 </papid>liang et al, 2009), <papid> P09-1011 </papid>and baseball (fleischman, 2007).</nextsent>
<nextsent>we have adapted this approach in the creation of semantics-oriented corpus, using the domain of major-league baseball.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH55">
<title id=" W11-0139.xml">ball game a corpus for computational semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the use of large annotated corpora and treebanks has led to many fruitful research programs in computational linguistics.
</prevsent>
<prevsent>at the time of this writing, marcus et al (1993), <papid> J93-2004 </papid>which introduces the university of pennsylvania treebank,1 has been cited by over 3000 subsequent papers.2 such treebanks are invaluable for the training and testing of large-scale syntactic parsers and numerous other applications in the field of computational syntax.unfortunately for the field of computational semantics, there are few corresponding annotated corpora or treebanks representing the formalized meaning of natural language sentences, mainly because there is very little agreement on what such representation of meaning would look like for arbitrary text.</prevsent>
</prevsection>
<citsent citstr=" P09-1011 ">
to overcome this obstacle, several recent studies have turned to the arena of sports, pairing natural language with game statistics in several domains, including robocup soccer (liang et al, 2009; <papid> P09-1011 </papid>chen et al, 2010), soccer (theune and klabbers, 1998; saggion et al, 2003), <papid> E03-2014 </papid>american football (barzilay and lapata, 2005; <papid> H05-1042 </papid>liang et al, 2009), <papid> P09-1011 </papid>and baseball (fleischman, 2007).</citsent>
<aftsection>
<nextsent>we have adapted this approach in the creation of semantics-oriented corpus, using the domain of major-league baseball.
</nextsent>
<nextsent>the information state of baseball game can be represented with small number of variables, such as who is on which base, who is batting, who is playing each position, and the current score and inning.
</nextsent>
<nextsent>there is even standard way of representing updates to this information state.3 this makes baseball logical stepping stone to fuller representation of the world.
</nextsent>
<nextsent>we also chose baseball for this corpus because of the volume of data available, in the form of both natural language descriptions of events and language-independent game statistics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH56">
<title id=" W11-0139.xml">ball game a corpus for computational semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the use of large annotated corpora and treebanks has led to many fruitful research programs in computational linguistics.
</prevsent>
<prevsent>at the time of this writing, marcus et al (1993), <papid> J93-2004 </papid>which introduces the university of pennsylvania treebank,1 has been cited by over 3000 subsequent papers.2 such treebanks are invaluable for the training and testing of large-scale syntactic parsers and numerous other applications in the field of computational syntax.unfortunately for the field of computational semantics, there are few corresponding annotated corpora or treebanks representing the formalized meaning of natural language sentences, mainly because there is very little agreement on what such representation of meaning would look like for arbitrary text.</prevsent>
</prevsection>
<citsent citstr=" E03-2014 ">
to overcome this obstacle, several recent studies have turned to the arena of sports, pairing natural language with game statistics in several domains, including robocup soccer (liang et al, 2009; <papid> P09-1011 </papid>chen et al, 2010), soccer (theune and klabbers, 1998; saggion et al, 2003), <papid> E03-2014 </papid>american football (barzilay and lapata, 2005; <papid> H05-1042 </papid>liang et al, 2009), <papid> P09-1011 </papid>and baseball (fleischman, 2007).</citsent>
<aftsection>
<nextsent>we have adapted this approach in the creation of semantics-oriented corpus, using the domain of major-league baseball.
</nextsent>
<nextsent>the information state of baseball game can be represented with small number of variables, such as who is on which base, who is batting, who is playing each position, and the current score and inning.
</nextsent>
<nextsent>there is even standard way of representing updates to this information state.3 this makes baseball logical stepping stone to fuller representation of the world.
</nextsent>
<nextsent>we also chose baseball for this corpus because of the volume of data available, in the form of both natural language descriptions of events and language-independent game statistics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH57">
<title id=" W11-0139.xml">ball game a corpus for computational semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the use of large annotated corpora and treebanks has led to many fruitful research programs in computational linguistics.
</prevsent>
<prevsent>at the time of this writing, marcus et al (1993), <papid> J93-2004 </papid>which introduces the university of pennsylvania treebank,1 has been cited by over 3000 subsequent papers.2 such treebanks are invaluable for the training and testing of large-scale syntactic parsers and numerous other applications in the field of computational syntax.unfortunately for the field of computational semantics, there are few corresponding annotated corpora or treebanks representing the formalized meaning of natural language sentences, mainly because there is very little agreement on what such representation of meaning would look like for arbitrary text.</prevsent>
</prevsection>
<citsent citstr=" H05-1042 ">
to overcome this obstacle, several recent studies have turned to the arena of sports, pairing natural language with game statistics in several domains, including robocup soccer (liang et al, 2009; <papid> P09-1011 </papid>chen et al, 2010), soccer (theune and klabbers, 1998; saggion et al, 2003), <papid> E03-2014 </papid>american football (barzilay and lapata, 2005; <papid> H05-1042 </papid>liang et al, 2009), <papid> P09-1011 </papid>and baseball (fleischman, 2007).</citsent>
<aftsection>
<nextsent>we have adapted this approach in the creation of semantics-oriented corpus, using the domain of major-league baseball.
</nextsent>
<nextsent>the information state of baseball game can be represented with small number of variables, such as who is on which base, who is batting, who is playing each position, and the current score and inning.
</nextsent>
<nextsent>there is even standard way of representing updates to this information state.3 this makes baseball logical stepping stone to fuller representation of the world.
</nextsent>
<nextsent>we also chose baseball for this corpus because of the volume of data available, in the form of both natural language descriptions of events and language-independent game statistics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH60">
<title id=" W11-0501.xml">plans toward automated chat summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>similarly, boiney et al (2008) observed that with us air force operators, when they returned to duty from an interruption, another operator in the same room verbally updates them with summary of what had recently taken place in thechat rooms and where they can find the important information.
</prevsent>
<prevsent>both of these situations are motivations for chat summarization, since watchstanders and operators could use automatically generated summaries to quickly orient themselves with the current situation.while our motivation is from military perspective, chat summarization is also applicable to otherdomains.
</prevsent>
</prevsection>
<citsent citstr=" P05-1037 ">
for example, chat is used for communication in multinational companies (handel and herb sleb, 2002), open source meetings (shihab et al, 2009; zhou and hovy, 2005), <papid> P05-1037 </papid>and distance learning (osman and herring, 2007).</citsent>
<aftsection>
<nextsent>summarization could aid people who missed meetings or students who wish to study past material in summarized format.even though chat summarization has many potential uses, there has been little research on this topic (section 3).
</nextsent>
<nextsent>one possible reason for this is that chat is difficult medium to analyze: its characteristics make it difficult to apply traditional nlp techniques.it has uncommon features such as frequent use of abbreviations, acronyms, deletion of subject pronouns, use of emoticons, abbreviation of nicknames, and stripping of vowels from words to reduce number of keystrokes (werry, 1996).
</nextsent>
<nextsent>chat is also characterized by conversation threads becoming entangled due to multiple conversations taking place simultaneously in multi participant chat, i.e., chat composed of three or more users within the same chat room (herring, 1999; herring, 2010).
</nextsent>
<nextsent>the interwoven threads then make it more difficult to comprehend individual conversations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH61">
<title id=" W11-0501.xml">plans toward automated chat summarization </title>
<section> our summarization challenge.  </section>
<citcontext>
<prevsection>
<prevsent>with our example,the summary answers all three questions: it identifies the two speakers solo and skywlkr, it identifies that they are talking about ben, and that the result is skywlkr thinks ben is great while solo thinks ben causes trouble.
</prevsent>
<prevsent>the key challenge to thread summarization willbe finding, extracting, and summarizing the individual conversation threads.
</prevsent>
</prevsection>
<citsent citstr=" J10-3004 ">
this requires the ability to detect and extract threads, which has become ofgreat interest in recent research (duchon and jackson, 2010; elsner and charniak, 2010; <papid> J10-3004 </papid>elsner and schudy, 2009; <papid> W09-1803 </papid>ramachandran et al, 2010; wangand oard, 2009).<papid> N09-1023 </papid></citsent>
<aftsection>
<nextsent>thread disentanglement and summarization will have to be done online, with conversation threads being updated every time new message appears.
</nextsent>
<nextsent>another challenge will be processing incomplete conversations, since some messages may be incorrectly classified into the wrong conversation threads.
</nextsent>
<nextsent>these issues will need to be addressed as this research progresses.
</nextsent>
<nextsent>2.2 temporal summarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH62">
<title id=" W11-0501.xml">plans toward automated chat summarization </title>
<section> our summarization challenge.  </section>
<citcontext>
<prevsection>
<prevsent>with our example,the summary answers all three questions: it identifies the two speakers solo and skywlkr, it identifies that they are talking about ben, and that the result is skywlkr thinks ben is great while solo thinks ben causes trouble.
</prevsent>
<prevsent>the key challenge to thread summarization willbe finding, extracting, and summarizing the individual conversation threads.
</prevsent>
</prevsection>
<citsent citstr=" W09-1803 ">
this requires the ability to detect and extract threads, which has become ofgreat interest in recent research (duchon and jackson, 2010; elsner and charniak, 2010; <papid> J10-3004 </papid>elsner and schudy, 2009; <papid> W09-1803 </papid>ramachandran et al, 2010; wangand oard, 2009).<papid> N09-1023 </papid></citsent>
<aftsection>
<nextsent>thread disentanglement and summarization will have to be done online, with conversation threads being updated every time new message appears.
</nextsent>
<nextsent>another challenge will be processing incomplete conversations, since some messages may be incorrectly classified into the wrong conversation threads.
</nextsent>
<nextsent>these issues will need to be addressed as this research progresses.
</nextsent>
<nextsent>2.2 temporal summarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH63">
<title id=" W11-0501.xml">plans toward automated chat summarization </title>
<section> our summarization challenge.  </section>
<citcontext>
<prevsection>
<prevsent>with our example,the summary answers all three questions: it identifies the two speakers solo and skywlkr, it identifies that they are talking about ben, and that the result is skywlkr thinks ben is great while solo thinks ben causes trouble.
</prevsent>
<prevsent>the key challenge to thread summarization willbe finding, extracting, and summarizing the individual conversation threads.
</prevsent>
</prevsection>
<citsent citstr=" N09-1023 ">
this requires the ability to detect and extract threads, which has become ofgreat interest in recent research (duchon and jackson, 2010; elsner and charniak, 2010; <papid> J10-3004 </papid>elsner and schudy, 2009; <papid> W09-1803 </papid>ramachandran et al, 2010; wangand oard, 2009).<papid> N09-1023 </papid></citsent>
<aftsection>
<nextsent>thread disentanglement and summarization will have to be done online, with conversation threads being updated every time new message appears.
</nextsent>
<nextsent>another challenge will be processing incomplete conversations, since some messages may be incorrectly classified into the wrong conversation threads.
</nextsent>
<nextsent>these issues will need to be addressed as this research progresses.
</nextsent>
<nextsent>2.2 temporal summarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH65">
<title id=" W11-0501.xml">plans toward automated chat summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their approach performed well, outperforming baseline approach and achieving an f-score of 0.52.
</prevsent>
<prevsent>there has also been work on summarization of media which share some similarities to chat.
</prevsent>
</prevsection>
<citsent citstr=" J02-4003 ">
for example, zechner (2002) <papid> J02-4003 </papid>examined summarization of multiparty dialogues and murray et al (2005) <papid> W05-0905 </papid>examined summarization of meeting recordings.</citsent>
<aftsection>
<nextsent>bothof these media share in common with chat the difficulty of summarizing conversations with multiple participants.
</nextsent>
<nextsent>a difference with chat is that both ofthese publications focused on one conversation sequentially while chat is characterized by multiple, unrelated conversations taking place simultaneously.
</nextsent>
<nextsent>newman and blitzer (2003) described the beginning stages of their work on summarizing archived discussions of news groups and mailing lists.
</nextsent>
<nextsent>this has some similarity with conversations, but difference is that news groups and mailing lists have metadatato help differentiate the threaded conversations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH66">
<title id=" W11-0501.xml">plans toward automated chat summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their approach performed well, outperforming baseline approach and achieving an f-score of 0.52.
</prevsent>
<prevsent>there has also been work on summarization of media which share some similarities to chat.
</prevsent>
</prevsection>
<citsent citstr=" W05-0905 ">
for example, zechner (2002) <papid> J02-4003 </papid>examined summarization of multiparty dialogues and murray et al (2005) <papid> W05-0905 </papid>examined summarization of meeting recordings.</citsent>
<aftsection>
<nextsent>bothof these media share in common with chat the difficulty of summarizing conversations with multiple participants.
</nextsent>
<nextsent>a difference with chat is that both ofthese publications focused on one conversation sequentially while chat is characterized by multiple, unrelated conversations taking place simultaneously.
</nextsent>
<nextsent>newman and blitzer (2003) described the beginning stages of their work on summarizing archived discussions of news groups and mailing lists.
</nextsent>
<nextsent>this has some similarity with conversations, but difference is that news groups and mailing lists have metadatato help differentiate the threaded conversations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH74">
<title id=" W11-0501.xml">plans toward automated chat summarization </title>
<section> planned approach.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore,we will incorporate past work on dialogue act tagging in chat (wu et al, 2005) to both improve summarization and create framework for the next two steps.
</prevsent>
<prevsent>however, there is one limitation with their work: the templates used for tagging were manually created, which is both time-intensive and fragile.
</prevsent>
</prevsection>
<citsent citstr=" N10-1020 ">
to overcome this, we plan to use an unsupervised learning approach to discover dialogue acts (ritter et al, 2010).<papid> N10-1020 </papid></citsent>
<aftsection>
<nextsent>4.2 thread extension.
</nextsent>
<nextsent>the second step will be to extend summarization to thread summaries.
</nextsent>
<nextsent>this will require leveraging thread disentanglement techniques, with the possibility of using multiple techniques to improve the capability of finding whole conversation threads.
</nextsent>
<nextsent>forthe summary generations, we will first create extractive summaries before extending the summarizer to generate abs tractive summaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH79">
<title id=" W11-1701.xml">cats rule and dogs drool classifying stance in online debate </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results suggest that methods that take into account the dia logic context of such posts might be fruitful.
</prevsent>
<prevsent>recent work has highlighted the challenges of identifying the stance that speaker holds towards aparticular political, social or technical topic.
</prevsent>
</prevsection>
<citsent citstr=" W06-2915 ">
classifying stance involves identifying holistic subjective disposition, beyond the word or sentence (lin et al, 2006; <papid> W06-2915 </papid>malouf and mullen, 2008; greene andres nik, 2009; <papid> N09-1057 </papid>somasundaran and wiebe, 2009; <papid> P09-1026 </papid>somasundaran and wiebe, 2010).<papid> W10-0214 </papid></citsent>
<aftsection>
<nextsent>our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussionwebsites vary great deal.
</nextsent>
<nextsent>one important contextual variable, discussed at length below, is the percentage of posts that are rebuttals to previous posts,which varies in our data from 34% to 80%.
</nextsent>
<nextsent>the ability to explicitly rebut previous post gives these debates both mono logic and dia logic properties (biber,1991; crystal, 2001; fox tree, 2010); compare figure 1 to figure 2.
</nextsent>
<nextsent>we believe that discussions containing many rebuttal links require different type of analysis than other types of debates or discussions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH80">
<title id=" W11-1701.xml">cats rule and dogs drool classifying stance in online debate </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results suggest that methods that take into account the dia logic context of such posts might be fruitful.
</prevsent>
<prevsent>recent work has highlighted the challenges of identifying the stance that speaker holds towards aparticular political, social or technical topic.
</prevsent>
</prevsection>
<citsent citstr=" N09-1057 ">
classifying stance involves identifying holistic subjective disposition, beyond the word or sentence (lin et al, 2006; <papid> W06-2915 </papid>malouf and mullen, 2008; greene andres nik, 2009; <papid> N09-1057 </papid>somasundaran and wiebe, 2009; <papid> P09-1026 </papid>somasundaran and wiebe, 2010).<papid> W10-0214 </papid></citsent>
<aftsection>
<nextsent>our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussionwebsites vary great deal.
</nextsent>
<nextsent>one important contextual variable, discussed at length below, is the percentage of posts that are rebuttals to previous posts,which varies in our data from 34% to 80%.
</nextsent>
<nextsent>the ability to explicitly rebut previous post gives these debates both mono logic and dia logic properties (biber,1991; crystal, 2001; fox tree, 2010); compare figure 1 to figure 2.
</nextsent>
<nextsent>we believe that discussions containing many rebuttal links require different type of analysis than other types of debates or discussions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH81">
<title id=" W11-1701.xml">cats rule and dogs drool classifying stance in online debate </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results suggest that methods that take into account the dia logic context of such posts might be fruitful.
</prevsent>
<prevsent>recent work has highlighted the challenges of identifying the stance that speaker holds towards aparticular political, social or technical topic.
</prevsent>
</prevsection>
<citsent citstr=" P09-1026 ">
classifying stance involves identifying holistic subjective disposition, beyond the word or sentence (lin et al, 2006; <papid> W06-2915 </papid>malouf and mullen, 2008; greene andres nik, 2009; <papid> N09-1057 </papid>somasundaran and wiebe, 2009; <papid> P09-1026 </papid>somasundaran and wiebe, 2010).<papid> W10-0214 </papid></citsent>
<aftsection>
<nextsent>our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussionwebsites vary great deal.
</nextsent>
<nextsent>one important contextual variable, discussed at length below, is the percentage of posts that are rebuttals to previous posts,which varies in our data from 34% to 80%.
</nextsent>
<nextsent>the ability to explicitly rebut previous post gives these debates both mono logic and dia logic properties (biber,1991; crystal, 2001; fox tree, 2010); compare figure 1 to figure 2.
</nextsent>
<nextsent>we believe that discussions containing many rebuttal links require different type of analysis than other types of debates or discussions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH84">
<title id=" W11-1701.xml">cats rule and dogs drool classifying stance in online debate </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results suggest that methods that take into account the dia logic context of such posts might be fruitful.
</prevsent>
<prevsent>recent work has highlighted the challenges of identifying the stance that speaker holds towards aparticular political, social or technical topic.
</prevsent>
</prevsection>
<citsent citstr=" W10-0214 ">
classifying stance involves identifying holistic subjective disposition, beyond the word or sentence (lin et al, 2006; <papid> W06-2915 </papid>malouf and mullen, 2008; greene andres nik, 2009; <papid> N09-1057 </papid>somasundaran and wiebe, 2009; <papid> P09-1026 </papid>somasundaran and wiebe, 2010).<papid> W10-0214 </papid></citsent>
<aftsection>
<nextsent>our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussionwebsites vary great deal.
</nextsent>
<nextsent>one important contextual variable, discussed at length below, is the percentage of posts that are rebuttals to previous posts,which varies in our data from 34% to 80%.
</nextsent>
<nextsent>the ability to explicitly rebut previous post gives these debates both mono logic and dia logic properties (biber,1991; crystal, 2001; fox tree, 2010); compare figure 1 to figure 2.
</nextsent>
<nextsent>we believe that discussions containing many rebuttal links require different type of analysis than other types of debates or discussions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH95">
<title id=" W11-1701.xml">cats rule and dogs drool classifying stance in online debate </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however agreement vs. disagreement is not labelled across quote/response pairs and wang &amp; rose (2010) do not attempt to distinguish these different discourserelations.
</prevsent>
<prevsent>rather they show that they can use variant of lsa to identify parent post, given response post, with approximately 70% accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W11-0702 ">
a recent paper by (abbott et al, 2011) <papid> W11-0702 </papid>examines agreement and disagreement in quote/response pairs in idealog ical and nonidealogical online forum discussions, and shows that you can distinguish the agreement relation with 68% accuracy.</citsent>
<aftsection>
<nextsent>their results indicate that contextual features do improve performance for identifying the agreement relation between quotes and responses.other work has utilized the social network structure of online forums, either with or without textual features of particular posts (malouf and mullen, 2008; mishne and glance, 2006; murakami and raymond, 2010; <papid> C10-2100 </papid>agrawal et al, 2003).</nextsent>
<nextsent>however this work does not examine the way that the dia logic structure varies by topic, as we do, and the threading structure of their debates does not distinguish between agreement and disagreement re sponses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH96">
<title id=" W11-1701.xml">cats rule and dogs drool classifying stance in online debate </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>rather they show that they can use variant of lsa to identify parent post, given response post, with approximately 70% accuracy.
</prevsent>
<prevsent>a recent paper by (abbott et al, 2011) <papid> W11-0702 </papid>examines agreement and disagreement in quote/response pairs in idealog ical and nonidealogical online forum discussions, and shows that you can distinguish the agreement relation with 68% accuracy.</prevsent>
</prevsection>
<citsent citstr=" C10-2100 ">
their results indicate that contextual features do improve performance for identifying the agreement relation between quotes and responses.other work has utilized the social network structure of online forums, either with or without textual features of particular posts (malouf and mullen, 2008; mishne and glance, 2006; murakami and raymond, 2010; <papid> C10-2100 </papid>agrawal et al, 2003).</citsent>
<aftsection>
<nextsent>however this work does not examine the way that the dia logic structure varies by topic, as we do, and the threading structure of their debates does not distinguish between agreement and disagreement responses.
</nextsent>
<nextsent>(mishne and glance, 2006) show that most replies to blog posts are disagreements, while agarwals work assumed that adjacent posts always disagree, and did not use any of the information in thetext.
</nextsent>
<nextsent>murakami &amp; raymond (2010) <papid> C10-2100 </papid>show that simple rules for identifying disagreement, defined on the textual content of the post, can improve over agarwals results and (malouf and mullen, 2008)show that combination of textual and social net 2 work features provides the best performance.</nextsent>
<nextsent>weleave the incorporation of social network information for stance classification to future work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH114">
<title id=" W11-1701.xml">cats rule and dogs drool classifying stance in online debate </title>
<section> features and learning methods.  </section>
<citcontext>
<prevsection>
<prevsent>the dependency parse forgiven sentence is set of triples, composed of grammatical relation and the pair of words for which the grammatical relation holds (reli, wj , wk), where reli is the dependency relation among words wj and wk.
</prevsent>
<prevsent>the word wj is the head of the dependency relation.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we use the stanford parser to parse the utterances in the posts and extract dependency features (de marneffe et al, 2006; klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>generalized dependency.
</nextsent>
<nextsent>to create generalized dependencies, we back off?
</nextsent>
<nextsent>the head word in each of the above features to its part-of-speech tag (joshi and penstein-rose?, 2009).
</nextsent>
<nextsent>joshi &amp; roses results suggested that this approach would work better tha neither fully lexicalized or fully generalized dependency features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH120">
<title id=" W11-1407.xml">automatic gapfill question generation from textbooks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(a) hydrogen (b) covalent (c) ionic (d) double (correct answer: covalent) in gap-fill question (gfq) such as the one above, we refer to the sentence with the gap as the question sentence (qs) and the sentence in the text that is used to generate the qs as the gap-fill sentence (gfs).
</prevsent>
<prevsent>the word(s) which is removed from gfs to form the qs is referred to as the key while the three alternatives in the question are called as dis tractors, as they are used to distract the students from the correct answer.
</prevsent>
</prevsection>
<citsent citstr=" W05-0210 ">
previous works in gfqg (sumita et al, 2005; <papid> W05-0210 </papid>john lee and stephanie seneff, 2007; lin et al, 2007; pino et al, 2009; smith et al, 2010) have mostly worked in the domain of english language learning.</citsent>
<aftsection>
<nextsent>gap-fill questions have been generated totest students knowledge of english in using the correct verbs (sumita et al, 2005), <papid> W05-0210 </papid>prepositions (john lee and stephanie seneff, 2007) and adjectives (lin et al, 2007) in sentences.</nextsent>
<nextsent>pino et al (2009) and smith et al (2010) have generated gfqs to teach and evaluate students vocabulary.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH122">
<title id=" W11-1407.xml">automatic gapfill question generation from textbooks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our gfqg system takes document with its title as an input and produces list of gap-fill questions as 1a sentence is deemed informative if it has the relevant course knowledge which can be questioned.
</prevsent>
<prevsent>56 output.
</prevsent>
</prevsection>
<citsent citstr=" H05-1103 ">
unlike previous works (brown et al, 2005;<papid> H05-1103 </papid>smith et al, 2010) it doesnt use any external resource for dis tractor selection, making it adaptable to text from any domain.</citsent>
<aftsection>
<nextsent>its simplicity makes it useful not only as an aid for teachers to prepare gap-fillquestions but also for students who need an automatic question generator to aid their learning from textbook.
</nextsent>
<nextsent>a biology textbook campbell biology, 6th edition has been used for work in this paper.
</nextsent>
<nextsent>we have reported results of our system on 2 chapters (the structure and function of macromolecules and an introduction to metabolism ) of unit 1.
</nextsent>
<nextsent>each chapter contains sections and subsections with their respective topic headings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH123">
<title id=" W11-1407.xml">automatic gapfill question generation from textbooks </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 key selection.
</prevsent>
<prevsent>for each sentence selected in the previous stage,the key selection stage identifies the most appropriate key from the sentence to ask the question on.
</prevsent>
</prevsection>
<citsent citstr=" W06-1416 ">
previous works in this area, smith et al (2010) take keys as an input and, karamanis et al (2006) <papid> W06-1416 </papid>and mitkov et al (2006) select keys on the basis of term frequency and regular expressions on nouns.</citsent>
<aftsection>
<nextsent>then they search for sentences which contain that particular key in it.
</nextsent>
<nextsent>since their approaches generate gap-fill questions only with one blank, they could end up with trivial gfq, especially in case of conjunctions.
</nextsent>
<nextsent>2because, since, when, thus, however, although, for example and for instance connectives have been included.
</nextsent>
<nextsent>58 (a) dt jjs nns in nn nns vbp jj nns cc jj nns potential keys selection [the strongest kind] of [chemical bonds] are [covalent bond and ionic bond].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH133">
<title id=" W11-1713.xml">automatic emotion classification for interpersonal communication </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the techniques that have been used for emotion classification can roughly be divided into pattern-basedmethods and machine-learning methods.
</prevsent>
<prevsent>an often 104 used technique in pattern-based approaches is to use pre-defined lists of keywords which help determine an instances overall emotion contents.
</prevsent>
</prevsection>
<citsent citstr=" W10-0203 ">
the aesop system by goyal et al (2010), <papid> W10-0203 </papid>for instance, attempts to analyze the affective state of characters in fables by identifying affective verbs and by using set of projection rules to calculate the verbs?</citsent>
<aftsection>
<nextsent>influence on their patients.
</nextsent>
<nextsent>another possible approach whichwe subscribe to?
</nextsent>
<nextsent>is to let machine learner determine the appropriate emotion class.
</nextsent>
<nextsent>mishne (2005)and keshtkar and inkpen (2009), for instance, attempt to classify live journal posts according to their mood using support vector machines trained with frequency features, length-related features, semantic orientation features and features representing special symbols.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH134">
<title id=" W11-0324.xml">a normalized cut alignment model for mapping hierarchical semantic structures onto spoken documents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this model optimizes normalized cut graph-partitioning criterion and considers local tree constraints at the same time.
</prevsent>
<prevsent>the experimental results show the advantage of this model over viterbi-like, sequential alignment, under typical speech recognition errors.
</prevsent>
</prevsection>
<citsent citstr=" W05-0602 ">
learning semantic structures of written text has been studied in number of specific tasks, which include,but not limited to, those finding semantic representations for individual sentences (ge and mooney, 2005; <papid> W05-0602 </papid>zettlemoyer and collins, 2005; lu et al, 2008), <papid> D08-1082 </papid>and those constructing hierarchical structures among sentences or larger text blocks (marcu, 2000; branavan et al, 2007).<papid> P07-1069 </papid></citsent>
<aftsection>
<nextsent>the inverse problem of the latter kind, e.g., aligning certain form of already existing semantic hierarchies with the corresponding text sequence, is not so much prominent problem for written text as it is for spoken documents.
</nextsent>
<nextsent>in this paper, we study specific type of such problem, inwhich hierarchical browsing structure, i.e., electronic slides of oral presentations, have already existed, the goal being to impose such structure onto the transcripts of the corresponding speech, with the aim to help index and access spoken documents as such.
</nextsent>
<nextsent>navigating audio documents is often inherently much more difficult than browsing text; an obvious solution, in relying on human beings?
</nextsent>
<nextsent>ability to read text, is to conduct speech-to-text conversion through automatic speech recognition (asr).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH135">
<title id=" W11-0324.xml">a normalized cut alignment model for mapping hierarchical semantic structures onto spoken documents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this model optimizes normalized cut graph-partitioning criterion and considers local tree constraints at the same time.
</prevsent>
<prevsent>the experimental results show the advantage of this model over viterbi-like, sequential alignment, under typical speech recognition errors.
</prevsent>
</prevsection>
<citsent citstr=" D08-1082 ">
learning semantic structures of written text has been studied in number of specific tasks, which include,but not limited to, those finding semantic representations for individual sentences (ge and mooney, 2005; <papid> W05-0602 </papid>zettlemoyer and collins, 2005; lu et al, 2008), <papid> D08-1082 </papid>and those constructing hierarchical structures among sentences or larger text blocks (marcu, 2000; branavan et al, 2007).<papid> P07-1069 </papid></citsent>
<aftsection>
<nextsent>the inverse problem of the latter kind, e.g., aligning certain form of already existing semantic hierarchies with the corresponding text sequence, is not so much prominent problem for written text as it is for spoken documents.
</nextsent>
<nextsent>in this paper, we study specific type of such problem, inwhich hierarchical browsing structure, i.e., electronic slides of oral presentations, have already existed, the goal being to impose such structure onto the transcripts of the corresponding speech, with the aim to help index and access spoken documents as such.
</nextsent>
<nextsent>navigating audio documents is often inherently much more difficult than browsing text; an obvious solution, in relying on human beings?
</nextsent>
<nextsent>ability to read text, is to conduct speech-to-text conversion through automatic speech recognition (asr).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH136">
<title id=" W11-0324.xml">a normalized cut alignment model for mapping hierarchical semantic structures onto spoken documents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this model optimizes normalized cut graph-partitioning criterion and considers local tree constraints at the same time.
</prevsent>
<prevsent>the experimental results show the advantage of this model over viterbi-like, sequential alignment, under typical speech recognition errors.
</prevsent>
</prevsection>
<citsent citstr=" P07-1069 ">
learning semantic structures of written text has been studied in number of specific tasks, which include,but not limited to, those finding semantic representations for individual sentences (ge and mooney, 2005; <papid> W05-0602 </papid>zettlemoyer and collins, 2005; lu et al, 2008), <papid> D08-1082 </papid>and those constructing hierarchical structures among sentences or larger text blocks (marcu, 2000; branavan et al, 2007).<papid> P07-1069 </papid></citsent>
<aftsection>
<nextsent>the inverse problem of the latter kind, e.g., aligning certain form of already existing semantic hierarchies with the corresponding text sequence, is not so much prominent problem for written text as it is for spoken documents.
</nextsent>
<nextsent>in this paper, we study specific type of such problem, inwhich hierarchical browsing structure, i.e., electronic slides of oral presentations, have already existed, the goal being to impose such structure onto the transcripts of the corresponding speech, with the aim to help index and access spoken documents as such.
</nextsent>
<nextsent>navigating audio documents is often inherently much more difficult than browsing text; an obvious solution, in relying on human beings?
</nextsent>
<nextsent>ability to read text, is to conduct speech-to-text conversion through automatic speech recognition (asr).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH137">
<title id=" W11-0324.xml">a normalized cut alignment model for mapping hierarchical semantic structures onto spoken documents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>navigating audio documents is often inherently much more difficult than browsing text; an obvious solution, in relying on human beings?
</prevsent>
<prevsent>ability to read text, is to conduct speech-to-text conversion through automatic speech recognition (asr).
</prevsent>
</prevsection>
<citsent citstr=" N10-1006 ">
implicitly, solutions as such change the conventional speaking-for-hearing construals: now speech can be read through its transcripts, though, in most cases, it was not intended for this purpose, which in turn raises new set of problems.the convenience and efficiency of reading transcripts (stark et al, 2000; munteanu et al, 2006)are first affected by errors produced in transcription channels for various reasons, though if the goalis only to browse salient excerpts, recognition errors on the extracts can be reduced by considering asr confidence scores (xie and liu, 2010; <papid> N10-1006 </papid>hori and furui, 2003; zechner and waibel, 2000): <papid> A00-2025 </papid>trading off the expected salience of excerpts with their recognition-error rate could actually result in the improvement of excerpt quality in terms of the amount of important content being correctly presented (zechner and waibel, 2000).<papid> A00-2025 </papid></citsent>
<aftsection>
<nextsent>even if transcription quality were not problem, browsing transcripts is not straightforward.
</nextsent>
<nextsent>when intended to be read, written documents are almost always presented as more than uninterrupted strings of text.
</nextsent>
<nextsent>consider that for many written documents, e.g., books, indicative structures such as sec tion/subsection headings and tables-of-contents are standard constituents created manually to help readers.
</nextsent>
<nextsent>structures of this kind, even when existing, are rarely aligned with spoken documents completely.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH138">
<title id=" W11-0324.xml">a normalized cut alignment model for mapping hierarchical semantic structures onto spoken documents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>navigating audio documents is often inherently much more difficult than browsing text; an obvious solution, in relying on human beings?
</prevsent>
<prevsent>ability to read text, is to conduct speech-to-text conversion through automatic speech recognition (asr).
</prevsent>
</prevsection>
<citsent citstr=" A00-2025 ">
implicitly, solutions as such change the conventional speaking-for-hearing construals: now speech can be read through its transcripts, though, in most cases, it was not intended for this purpose, which in turn raises new set of problems.the convenience and efficiency of reading transcripts (stark et al, 2000; munteanu et al, 2006)are first affected by errors produced in transcription channels for various reasons, though if the goalis only to browse salient excerpts, recognition errors on the extracts can be reduced by considering asr confidence scores (xie and liu, 2010; <papid> N10-1006 </papid>hori and furui, 2003; zechner and waibel, 2000): <papid> A00-2025 </papid>trading off the expected salience of excerpts with their recognition-error rate could actually result in the improvement of excerpt quality in terms of the amount of important content being correctly presented (zechner and waibel, 2000).<papid> A00-2025 </papid></citsent>
<aftsection>
<nextsent>even if transcription quality were not problem, browsing transcripts is not straightforward.
</nextsent>
<nextsent>when intended to be read, written documents are almost always presented as more than uninterrupted strings of text.
</nextsent>
<nextsent>consider that for many written documents, e.g., books, indicative structures such as sec tion/subsection headings and tables-of-contents are standard constituents created manually to help readers.
</nextsent>
<nextsent>structures of this kind, even when existing, are rarely aligned with spoken documents completely.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH140">
<title id=" W11-0324.xml">a normalized cut alignment model for mapping hierarchical semantic structures onto spoken documents </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>flat structures of spoken documents much previous work, similar to its written-text counterpart, has attempted to find certain flat structures of spoken documents, such as topic and slide boundaries.
</prevsent>
<prevsent>for example, the work of (chen and heng, 2003; rud darraju, 2006; zhu et al, 2008) aims to find slide boundaries in the corresponding lecture transcripts.
</prevsent>
</prevsection>
<citsent citstr=" P07-1064 ">
malioutov et al (2007) <papid> P07-1064 </papid>developed an approach to detecting topic boundaries of lecture recordings by finding repeated acoustic patterns.</citsent>
<aftsection>
<nextsent>none of this work, however, has involved hierarchical structures of spoken document.
</nextsent>
<nextsent>research has also resorted to other multimedia channels, e.g., video (liu et al, 2002; wang et al, 2003; fan et al, 2006), to detect slide transitions.
</nextsent>
<nextsent>this type of research, however, is unlikely to recover semantic structures in more details than slide boundaries.
</nextsent>
<nextsent>hierarchical structures of spoken documents recently, research has started to align hierarchical browsing structures with spoken documents, given that inferring such structures directly from spoken documents is still too challenging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH141">
<title id=" W11-0324.xml">a normalized cut alignment model for mapping hierarchical semantic structures onto spoken documents </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this type of research, however, is unlikely to recover semantic structures in more details than slide boundaries.
</prevsent>
<prevsent>hierarchical structures of spoken documents recently, research has started to align hierarchical browsing structures with spoken documents, given that inferring such structures directly from spoken documents is still too challenging.
</prevsent>
</prevsection>
<citsent citstr=" C10-2177 ">
zhu et al (2010)<papid> C10-2177 </papid>investigates bullet-slide alignment by first sequen tializing bullet trees with pre-order walk before conducting alignment, through which the problem is reduced to string-to-string alignment problem and an efficient viterbi-like method can be naturally applied.</citsent>
<aftsection>
<nextsent>in this paper, we use such sequential alignment as our baseline, which takes standard dynamic-programming process to find the optimal path on an m-by-n similarity matrix, where and denote the number of bullets and utterances in lecture, respectively.
</nextsent>
<nextsent>specifically, we chose the path that maps each bullet to an utterance to achieve the highest total bullet-utterance similarity score; this path can be found within standard o(mn2) time complexity.a pre-order walk of the hierarchical tree is natural choice, since speakers of presentations often follow such order in developing their talk; i.e., they often talk about bullet first and then each of its children in sequence.
</nextsent>
<nextsent>a pre-order walk is also assumed by branavan et al (2007) <papid> P07-1069 </papid>in their table-of-content generation task, problem in which hierarchical structure has already been assumed (aligned) with span of written text, but the title of each node needs to be generated.in principle, such sequential-alignment approach allows bullet to be only aligned to one utterance in the end, which does not model the basic properties of the problem well, where the content in bullet is often repeated not only when the speaker talks about it but also, very likely, when he discusses the descendant bullets.</nextsent>
<nextsent>second, we suspect that speech recognition errors, when happening on the critical anchoring words that bridging the alignment, would make sequential-alignment algorithm much less robust, compared with methods based on many to-many alignment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH144">
<title id=" W11-0324.xml">a normalized cut alignment model for mapping hierarchical semantic structures onto spoken documents </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, we should also note that the benefit of formulating the problem as sequential alignment problem is its computational efficiency: the solution can be calculated with conventionalviterbi-like algorithms.
</prevsent>
<prevsent>this property is also important for the task, since the length of spoken document, such as lecture, is often long enough to make inefficient algorithms practically intractable.an important question is therefore how to, in principle, model the problem better.
</prevsent>
</prevsection>
<citsent citstr=" P06-1004 ">
the second is howtime efficient the model is. malioutov and barzilay (2006) <papid> P06-1004 </papid>describe dynamic-programming version 211 of normalized-cut-based model in solving topic segmentation problem for spoken documents.</citsent>
<aftsection>
<nextsent>inspired by their work, we will propose model based on graph partitioning in finding the correspondence between bullets and the regions of transcripts that discuss them; the proposed model runs in polyno mial time.
</nextsent>
<nextsent>we will empirically show its benefit on both improving the alignment performance over sequential alignment and its robustness to speech recognition errors.
</nextsent>
<nextsent>we are given speech sequence = u1, u2, ..., un ,where ui is an utterance, and the corresponding hierarchical structure, which, in our work here, is asequence of lecture slides containing set of slide titles and bullets, = {b1, b2, ..., bm}, organized in tree structure ( ,?,?), where   is the root of the tree that concatenates all slides of lecture; i.e., each slide is child of the root   and each slides bullets form subtree.
</nextsent>
<nextsent>in the rest of this paper, the word bullet means both the title of slide (if any) and any bulletin it, if not otherwise noted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH149">
<title id=" W11-0324.xml">a normalized cut alignment model for mapping hierarchical semantic structures onto spoken documents </title>
<section> experiment set-up.  </section>
<citcontext>
<prevsection>
<prevsent>the second model is an advanced one using the same acoustic model.
</prevsent>
<prevsent>however, its language model was trained on domain-related documents obtained from the web through searching the words appearing on slides, as suggested by munteanu et al (2007).
</prevsent>
</prevsection>
<citsent citstr=" W06-1644 ">
this yielded wer of 0.43, which is atypical wer for lectures and conference presentations (leeuwis et al, 2003; hsu and glass, 2006; <papid> W06-1644 </papid>munteanu et al, 2007), though lower wer is possible in more ideal condition (glass et al, 2007), e.g., when the same course from the previous semester by the same instructor is available.</citsent>
<aftsection>
<nextsent>the 3gram language models were trained using the cmu cam language modelling toolkit (clarkson and rosenfeld, 1997), and the transcripts were generated with the sonic toolkit (pellom, 2001).
</nextsent>
<nextsent>the out of-vocabulary rates are 0.3% in the output of asr model 1 and 0.1% in that of model 2, respectively.
</nextsent>
<nextsent>both bullets and automatic transcripts were stemmed and stop words in them were removed.
</nextsent>
<nextsent>we then calculated the similarity between bullet and an utterance with the number of overlapping words shared, normalized by their lengths.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH152">
<title id=" W10-4219.xml">applying semantic frame theory to automate natural language template generation from ontology statements </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.2 the language generation module.
</prevsent>
<prevsent>the kind of language generation system discussed here consists of language generation module that is guided by linguistic principles to map itsnon-linguistic input (i.e. set of logical state ments) to syntactic and semantic templates.
</prevsent>
</prevsection>
<citsent citstr=" W98-1425 ">
this kind of generation system follows the approaches that have been discussed elsewhere (reiter, 1999; busemann and horacek, 1998; <papid> W98-1425 </papid>geldof and van de velde, 1997; reiter and mellish, 1993).</citsent>
<aftsection>
<nextsent>the goal of the proposed module is to associate an ontology statement with relevant syntactic and semantic specifications.
</nextsent>
<nextsent>this generation process should be carried out during micro planning (cf.reiter and dale (2000)) before aggregation andre ferring expression generation take place.
</nextsent>
<nextsent>1.3 the knowledge representation.
</nextsent>
<nextsent>the knowledge representation which serves as the input to the language generator is structured ontology specified in the web ontology language (owl) (berners-lee, 2004) on which programs can perform logical reasoning over data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH153">
<title id=" W10-4219.xml">applying semantic frame theory to automate natural language template generation from ontology statements </title>
<section> discussion and related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the next step of our work, when we proceed with aggregations and discourse generation we intend to utilize the essential information provided by these elements.currently, the ontology properties are lexicalized manually, process which relies solely on the frames and the ontology class hierarchies.
</prevsent>
<prevsent>to in crease efficiency and accuracy, additional lexical resources such as wordnet must be integrated into the system.
</prevsent>
</prevsection>
<citsent citstr=" P98-1099 ">
this kind of integration has already proved feasible in the context of nlg (jing and mckeown, 1998) <papid> P98-1099 </papid>and has several implications for automatic lexicalization.</citsent>
<aftsection>
<nextsent>in this paper we presented on-going research on applying semantic frame theory to automate natural language template generation.
</nextsent>
<nextsent>the proposed method has many advantages.first, the extracted templates and syntactic alternations provide varying degrees of complexity of linguistic entities which eliminate the need for manual input of language-specific heuristics.
</nextsent>
<nextsent>second,the division of phases and the separation of the different tasks enables flexibility and re-use possibilities.
</nextsent>
<nextsent>this is in particular appealing for modular nlg systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH154">
<title id=" W11-0310.xml">gender attribution tracing stylo metric evidence beyond topic and genre </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is broad range of potential applications across computational linguistics and social science where statistical techniques for gender attribution can be useful: e.g., they can help understanding demographic characteristics of user-created web text today, which can provide new insight to social science as well as intelligent marketing and opinion mining.
</prevsent>
<prevsent>models for gender attribution can also help tracking changes to gender-specific styles in language over different domain and time.
</prevsent>
</prevsection>
<citsent citstr=" D10-1021 ">
gender detectors can be useful to guide the style of writing as well, if one needs to assume the style of specific gender for imaginative writing.although some recent work has shown the efficacy of machine learning techniques to gender attribution (e.g., koppel et al (2002), mukherjee andliu (2010)), <papid> D10-1021 </papid>we conjecture that the reported performance might be overly optimistic under scrutiny due to non-stylistic factors such as topic bias in gender that can make the gender detection task easier.</citsent>
<aftsection>
<nextsent>indeed, recent research on web blogs reports that there is substantial gender bias in topics (e.g., janssen and murachver (2004), argamon et al (2007)) as well as in genre (e.g., herring and paolillo (2006)).
</nextsent>
<nextsent>in order to address this concern, we perform the first comparative study of machine learning techniques for gender attribution after deliberately removing gender bias in topics and genre.
</nextsent>
<nextsent>furthermore, making the task even more realistic (and challenging), we experiment with cross-topic and cross genre gender attribution, and provide statistical evidence to gender-specific styles in language beyond topic and genre.
</nextsent>
<nextsent>five specific questions we aim to investigate are: 78 q1 are there truly gender-specific characteristics in language?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH159">
<title id=" W11-0310.xml">gender attribution tracing stylo metric evidence beyond topic and genre </title>
<section> statistical techniques.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 deep syntactic patterns using.
</prevsent>
<prevsent>probabilistic context free gram mara probabilistic context-free grammar (pcfg) captures syntactic regularities beyond shallow ngram based lexico-syntactic patterns.
</prevsent>
</prevsection>
<citsent citstr=" P10-2008 ">
raghavan et al(2010) <papid> P10-2008 </papid>recently introduced the use of pcfg for authorship attribution for the first time, and demonstrated that it is highly effective for learning stylistic patterns for authorship attribution.</citsent>
<aftsection>
<nextsent>we therefore explore the use of pcfg for gender attribution.
</nextsent>
<nextsent>we give very concise description here, referring to raghavan et al (2010) <papid> P10-2008 </papid>for more details.</nextsent>
<nextsent>(1) train generic pcfg parser go on manually tree-banked corpus such as wsj or brown.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH162">
<title id=" W11-0310.xml">gender attribution tracing stylo metric evidence beyond topic and genre </title>
<section> statistical techniques.  </section>
<citcontext>
<prevsection>
<prevsent>, and the gender corresponding to the higher score.
</prevsent>
<prevsent>note that pcfg models can be considered as kindof language models, where probabilistic context free grammars are used to find the patterns in language, rather than n-grams.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we use the implementation of klein and manning (2003) <papid> P03-1054 </papid>for pcfg models.</citsent>
<aftsection>
<nextsent>4.2 shallow lexico-syntactic patterns using.
</nextsent>
<nextsent>token-level language models token-based (i.e. word-based) language model shave been employed in wide variety of nlp applications, including those that require stylo metric analysis, e.g., authorship attribution (e.g., uzner and katz (2005)), and wikipedia vandalism detection(wang and mckeown, 2010).<papid> C10-1129 </papid></nextsent>
<nextsent>we expect that token based language models will be effective in learning shallow lexico-syntactic patterns of gender specific language styles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH163">
<title id=" W11-0310.xml">gender attribution tracing stylo metric evidence beyond topic and genre </title>
<section> statistical techniques.  </section>
<citcontext>
<prevsection>
<prevsent>we use the implementation of klein and manning (2003) <papid> P03-1054 </papid>for pcfg models.</prevsent>
<prevsent>4.2 shallow lexico-syntactic patterns using.</prevsent>
</prevsection>
<citsent citstr=" C10-1129 ">
token-level language models token-based (i.e. word-based) language model shave been employed in wide variety of nlp applications, including those that require stylo metric analysis, e.g., authorship attribution (e.g., uzner and katz (2005)), and wikipedia vandalism detection(wang and mckeown, 2010).<papid> C10-1129 </papid></citsent>
<aftsection>
<nextsent>we expect that token based language models will be effective in learning shallow lexico-syntactic patterns of gender specific language styles.
</nextsent>
<nextsent>we therefore experiment with un igram, bigram, and trigram token-level models, and name them as tlm(n=1), tlm(n=2), tlm(n=3), respectively, where tlm stands for token-based 80 lexicon based deep syntax morphology b.o.w. shallow lex-syntax gender gender pcfg clm clm clm me tlm tlm tlm data type genie guesser n=1 n=2 n=3 n=1 n=2 n=3 male only 72.1 68.6 53.4 65.8 69.0 63.4 57.6 67.1 67.8 66.2 female only 27.1 06.4 74.8 57.6 73.6 76.8 73.8 60.1 64.2 64.2 all 50.0 37.5 64.1 61.70 71.3 70.3 65.8 63.7 66.1 65.4 table 1: overall accuracy of topic-balanced gender attribution on blog data (experiment-i) language models.
</nextsent>
<nextsent>we use the ling pipe package5 for experiments.
</nextsent>
<nextsent>4.3 shallow morphological patterns using.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH164">
<title id=" W11-0310.xml">gender attribution tracing stylo metric evidence beyond topic and genre </title>
<section> statistical techniques.  </section>
<citcontext>
<prevsection>
<prevsent>4.3 shallow morphological patterns using.
</prevsent>
<prevsent>character-level language models next we explore the use of character-level language models to investigate whether there are morphological patterns that characterize gender-specificstyles in language.
</prevsent>
</prevsection>
<citsent citstr=" N03-1025 ">
despite its simplicity, previous research have reported that character-level language models are effective for authorship attribution (e.g., peng et al (2003<papid> N03-1025 </papid>b)) as well as genre classification (e.g., peng et al (2003<papid> N03-1025 </papid>a), wu et al (2010)).<papid> P10-1077 </papid></citsent>
<aftsection>
<nextsent>we experiment with unigram, bigram, and trigram character-level models, and name them as clm(n=1), clm(n=2), clm(n=3), respectively, where clm stands for character-based languagemodels.
</nextsent>
<nextsent>we again make use of the ling pipe package for experiments.
</nextsent>
<nextsent>note that there has been no previous research that directly compares the performance of character level language models to that of pcfg based models for author attribution, not to mention for gender attribution.
</nextsent>
<nextsent>4.4 bag of words using.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH172">
<title id=" W11-0310.xml">gender attribution tracing stylo metric evidence beyond topic and genre </title>
<section> statistical techniques.  </section>
<citcontext>
<prevsection>
<prevsent>4.3 shallow morphological patterns using.
</prevsent>
<prevsent>character-level language models next we explore the use of character-level language models to investigate whether there are morphological patterns that characterize gender-specificstyles in language.
</prevsent>
</prevsection>
<citsent citstr=" P10-1077 ">
despite its simplicity, previous research have reported that character-level language models are effective for authorship attribution (e.g., peng et al (2003<papid> N03-1025 </papid>b)) as well as genre classification (e.g., peng et al (2003<papid> N03-1025 </papid>a), wu et al (2010)).<papid> P10-1077 </papid></citsent>
<aftsection>
<nextsent>we experiment with unigram, bigram, and trigram character-level models, and name them as clm(n=1), clm(n=2), clm(n=3), respectively, where clm stands for character-based languagemodels.
</nextsent>
<nextsent>we again make use of the ling pipe package for experiments.
</nextsent>
<nextsent>note that there has been no previous research that directly compares the performance of character level language models to that of pcfg based models for author attribution, not to mention for gender attribution.
</nextsent>
<nextsent>4.4 bag of words using.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH173">
<title id=" W11-0310.xml">gender attribution tracing stylo metric evidence beyond topic and genre </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>note that our two datasets are created to specifically answer the following question: are there gender specific characteristics in language beyond gender 5available at http://alias-i.com/lingpipe/ preferences in topics and genre?
</prevsent>
<prevsent>one way to answer this question is to test whether statistical models can detect gender attribution on dataset that is drastically different from the training data in topic and genre.
</prevsent>
</prevsection>
<citsent citstr=" W06-1615 ">
of course, it is known fact that machine learning techniques do not transfer well across different domains (e.g., blitzer et al (2006)).<papid> W06-1615 </papid></citsent>
<aftsection>
<nextsent>however,if they can still perform considerably better than random prediction, then it would prove that there is indeed gender-specific stylo metric characteristics beyond topic and genre.
</nextsent>
<nextsent>in what follows, we present five different experimental settings across two different dataset to compare in-domain and cross-domainperformance of various techniques for gender attribution.
</nextsent>
<nextsent>5.1 experiments with blog dataset.
</nextsent>
<nextsent>first we conduct two different experiments using the blog data in the order of increasing difficulty.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH177">
<title id=" W11-0907.xml">using grammar rule clusters for semantic relation classification </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P06-1055 ">
automatically-derived grammars, such as the split-and-merge model, have proven helpful in parsing (petrov et al , 2006).<papid> P06-1055 </papid></citsent>
<aftsection>
<nextsent>as such grammars are refined, latent information is recovered which may be usable for linguistic tasks besides parsing.
</nextsent>
<nextsent>in this paper, we present and examine new method of semantic relation classification: using automatically derived grammar rule clusters as robust knowledge source for semantic relation classification.
</nextsent>
<nextsent>we examine performance of this feature group on the semeval 2010 relation classification corpus, and find that it improves performance over both more coarse-grainedand more fine-grained syntactic and collocational features in semantic relation classification.
</nextsent>
<nextsent>in the process of discovering refined grammar starting from rules in the original treebank grammar, latent-variable grammars recover latent information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH179">
<title id=" W11-0907.xml">using grammar rule clusters for semantic relation classification </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>in our system, classification was 19-way, direction-sensitive1between the classifications: entity-origin, entity destination, cause-effect, product-producer,content-container, instrument-agency, member collection, component-whole, message-topic, and other (non-directional).
</prevsent>
<prevsent>the model was trained on the 8000-instance training section of these meval 2010 task 8 semantic relations corpus.
</prevsent>
</prevsection>
<citsent citstr=" S10-1006 ">
distribution of the training data is shown in table 1 (hendrickx et al , 2010).<papid> S10-1006 </papid></citsent>
<aftsection>
<nextsent>class count % of data other 1410 17.63% cause-effect 1003 12.54% component-whole 941 11.76% entity-destination 845 10.56% product-producer 717 8.96% entity-origin 716 8.95% member-collection 690 8.63% message-topic 634 7.92% content-container 540 6.75% instrument-agency 504 6.30%table 1: class distribution in the training section of semeval 2010 task 8 semantic relations corpus.
</nextsent>
<nextsent>we tested the model on the 2717-instance testing section of the same corpus.
</nextsent>
<nextsent>for each instance, the user was provided with sentence containing two marked entities, e1 and e2.
</nextsent>
<nextsent>we structured the task such that, for each instance, we chose the best semantic relation out of the 19 available.in this paper, we use grammatical cluster information (i.e., recovered latent information) from the berkeley parser (petrov 2006) as semantic features of syntactic origin to classify semantic relations in the semeval 2010 semantic relations corpus, in 1i.e., with content-container relation, the nominal that isthe container and the nominal that is the content cannot be reversed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH183">
<title id=" W11-0907.xml">using grammar rule clusters for semantic relation classification </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>the classification of semantic relations has been proposed to help nlp tasks ranging from word sense disambiguation, language modelling, paraphrasing, and recognising textual entailment (hendrickx et al , 2010).<papid> S10-1006 </papid></prevsent>
<prevsent>semantic world knowledge is crucial for accurate semantic classification of many types, and sources range from the hand-crafted-yet sparse (such aswordnet) to the robust-yet-noisy (such as the in ternet).</prevsent>
</prevsection>
<citsent citstr=" S10-1047 ">
for this community task, teams proposeda variety of knowledge sources and other features for their relation classification, from knowledge databases (tymoshenko and giuliano, 2010), <papid> S10-1047 </papid>wordnet (rink and harabagiu, 2010), <papid> S10-1057 </papid>wikipedia (szarvas and gurevych, 2010), to formal linguistic levin classes (rink and harabagiu, 2010), <papid> S10-1057 </papid>to collocational metrics (rink and harabagiu, 2010) <papid> S10-1057 </papid>and stems (chen et al , 2010).<papid> S10-1050 </papid></citsent>
<aftsection>
<nextsent>syntactic features present special benefits to any semantic classification task: they can generalize over the local context in ways that collocational metrics cannot, and unlike knowledge database sources which assign the most common word sense to word, syntactic features are sensitive to the words sense, as determined by the local context of the word.
</nextsent>
<nextsent>several teams in semeval 2010 task 8 used syntactic features for semantic relation classification.
</nextsent>
<nextsent>chen et al  (2010) <papid> S10-1050 </papid>use feature set of the syntactic parent node held in common by the two nominals.</nextsent>
<nextsent>rink and harabagiu (2010) <papid> S10-1057 </papid>use feature set of dependency paths of length 1 or 2 from the dependency tree around the two nominals.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH184">
<title id=" W11-0907.xml">using grammar rule clusters for semantic relation classification </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>the classification of semantic relations has been proposed to help nlp tasks ranging from word sense disambiguation, language modelling, paraphrasing, and recognising textual entailment (hendrickx et al , 2010).<papid> S10-1006 </papid></prevsent>
<prevsent>semantic world knowledge is crucial for accurate semantic classification of many types, and sources range from the hand-crafted-yet sparse (such aswordnet) to the robust-yet-noisy (such as the in ternet).</prevsent>
</prevsection>
<citsent citstr=" S10-1057 ">
for this community task, teams proposeda variety of knowledge sources and other features for their relation classification, from knowledge databases (tymoshenko and giuliano, 2010), <papid> S10-1047 </papid>wordnet (rink and harabagiu, 2010), <papid> S10-1057 </papid>wikipedia (szarvas and gurevych, 2010), to formal linguistic levin classes (rink and harabagiu, 2010), <papid> S10-1057 </papid>to collocational metrics (rink and harabagiu, 2010) <papid> S10-1057 </papid>and stems (chen et al , 2010).<papid> S10-1050 </papid></citsent>
<aftsection>
<nextsent>syntactic features present special benefits to any semantic classification task: they can generalize over the local context in ways that collocational metrics cannot, and unlike knowledge database sources which assign the most common word sense to word, syntactic features are sensitive to the words sense, as determined by the local context of the word.
</nextsent>
<nextsent>several teams in semeval 2010 task 8 used syntactic features for semantic relation classification.
</nextsent>
<nextsent>chen et al  (2010) <papid> S10-1050 </papid>use feature set of the syntactic parent node held in common by the two nominals.</nextsent>
<nextsent>rink and harabagiu (2010) <papid> S10-1057 </papid>use feature set of dependency paths of length 1 or 2 from the dependency tree around the two nominals.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH189">
<title id=" W11-0907.xml">using grammar rule clusters for semantic relation classification </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>the classification of semantic relations has been proposed to help nlp tasks ranging from word sense disambiguation, language modelling, paraphrasing, and recognising textual entailment (hendrickx et al , 2010).<papid> S10-1006 </papid></prevsent>
<prevsent>semantic world knowledge is crucial for accurate semantic classification of many types, and sources range from the hand-crafted-yet sparse (such aswordnet) to the robust-yet-noisy (such as the in ternet).</prevsent>
</prevsection>
<citsent citstr=" S10-1050 ">
for this community task, teams proposeda variety of knowledge sources and other features for their relation classification, from knowledge databases (tymoshenko and giuliano, 2010), <papid> S10-1047 </papid>wordnet (rink and harabagiu, 2010), <papid> S10-1057 </papid>wikipedia (szarvas and gurevych, 2010), to formal linguistic levin classes (rink and harabagiu, 2010), <papid> S10-1057 </papid>to collocational metrics (rink and harabagiu, 2010) <papid> S10-1057 </papid>and stems (chen et al , 2010).<papid> S10-1050 </papid></citsent>
<aftsection>
<nextsent>syntactic features present special benefits to any semantic classification task: they can generalize over the local context in ways that collocational metrics cannot, and unlike knowledge database sources which assign the most common word sense to word, syntactic features are sensitive to the words sense, as determined by the local context of the word.
</nextsent>
<nextsent>several teams in semeval 2010 task 8 used syntactic features for semantic relation classification.
</nextsent>
<nextsent>chen et al  (2010) <papid> S10-1050 </papid>use feature set of the syntactic parent node held in common by the two nominals.</nextsent>
<nextsent>rink and harabagiu (2010) <papid> S10-1057 </papid>use feature set of dependency paths of length 1 or 2 from the dependency tree around the two nominals.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH193">
<title id=" W11-0907.xml">using grammar rule clusters for semantic relation classification </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>rink and harabagiu (2010) <papid> S10-1057 </papid>use feature set of dependency paths of length 1 or 2 from the dependency tree around the two nominals.</prevsent>
<prevsent>47 2.2 grammatical cluster information.</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
for our investigations, we used the berkeley parser (petrov et al 2006, <papid> P06-1055 </papid>petrov and klein 2007) <papid> N07-1051 </papid>as source of grammar rule clusters.</citsent>
<aftsection>
<nextsent>we used the eng sm6.gr off-the-shelf model.the berkeley parser starts with an initial grammar extracted from wall street journal corpus sections 2-22.
</nextsent>
<nextsent>the parser then tries to learn set of rule probabilities over latent annotations to maximize the likelihood of the training trees using expectation maximization (em).
</nextsent>
<nextsent>consider sentence and its unannotated tree , non-terminal spanning (r, t), and its children and spanning (r, s) and (s, t).
</nextsent>
<nextsent>ax is subsymbolofa, bx ofb, and cx of c. we calculate the posterior probability of all annotated rules and positions for each training set tree in the expectation step (petrov et al , 2006): (<papid> P06-1055 </papid>1) ((r, s, t, ax ? bycz) | w, ) ? pout(r, t, ax) ??(ax ? bycz)pin(r, s, by)pin(s, t, cz) the probabilities from the expectation step act as weighted observations to update the rule probabilities in the maximization step: ?(ax ? bycz) := #{ax ? bycz} y?,z?#{ax ? bycz?}</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH195">
<title id=" W11-0907.xml">using grammar rule clusters for semantic relation classification </title>
<section> task description.  </section>
<citcontext>
<prevsection>
<prevsent>note that all features, collocational and syntactic, were used for discovering semantic knowledge.the crayola  e1 box /e1  contained two  e2 pencils /e2 .
</prevsent>
<prevsent>sw the-dt, crayola-jj, contain-vbd, two-cd, pencil-nns, box-nn ibw contained, two, contained two ocw crayola-jj, box-nn, contain-vbd, pencil-nns pos-tags vbd, cd, vbdcd ids vbd6, cd1, vbd6cd1table 2: sample sentence and its accompanying features.
</prevsent>
</prevsection>
<citsent citstr=" W07-2051 ">
collocational features: ? surrounding words (sw): from ye and baldwins (2007) <papid> W07-2051 </papid>preposition sense disambiguation system, this set of features consists of lemmas of all of the words within window of seven words before and after each of e1 ande2.</citsent>
<aftsection>
<nextsent>features are not, however, marked with relative location, as we found that this reduced ac curacy.?
</nextsent>
<nextsent>in-between words (ibw): this bag of features consists of the string of words occuring in the sentence in between e1 and e2, exclusive, as well as all the sub strings of those consecutive words.
</nextsent>
<nextsent>we tried marking each feature with its relative location, but we found that results improved without location marking, and so we do not use location marking in these experiments.
</nextsent>
<nextsent>2note that cluster ids are only meaningful when compared to other cluster ids split from the same parent node.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH199">
<title id=" W11-0605.xml">topdown recognizers for  </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>consequently, this structure can be interpreted by standard compositional semantics and may be appropriate for incremental?
</prevsent>
<prevsent>models of sentence interpretation (cf.
</prevsent>
</prevsection>
<citsent citstr=" H05-1102 ">
haddock, 1989; chambers et al, 2004; shen and joshi, 2005; <papid> H05-1102 </papid>altmann and mirkovic?, 2009; demberg and keller, 2009, kato and matsubara, 2009; schuler, 2010).</citsent>
<aftsection>
<nextsent>and like human parsing, when used with 39backtracking or beam search, td memory demands need not continually increase with sentencelength: fixed bound on stack depth and on backtrack or beam depth suffices for infinitely many sentences.
</nextsent>
<nextsent>furthermore, td parsing provides explicit,relevant left contexts?
</nextsent>
<nextsent>for probabilistic conditioning (roark and johnson, 1999; <papid> P99-1054 </papid>roark, 2001; <papid> J01-2004 </papid>roark, 2004).</nextsent>
<nextsent>but it has not been clear until recently how to apply this method to chomskian syntax or any of the other mcs grammar formalisms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH200">
<title id=" W11-0605.xml">topdown recognizers for  </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and like human parsing, when used with 39backtracking or beam search, td memory demands need not continually increase with sentencelength: fixed bound on stack depth and on backtrack or beam depth suffices for infinitely many sentences.
</prevsent>
<prevsent>furthermore, td parsing provides explicit,relevant left contexts?
</prevsent>
</prevsection>
<citsent citstr=" P99-1054 ">
for probabilistic conditioning (roark and johnson, 1999; <papid> P99-1054 </papid>roark, 2001; <papid> J01-2004 </papid>roark, 2004).</citsent>
<aftsection>
<nextsent>but it has not been clear until recently how to apply this method to chomskian syntax or any of the other mcs grammar formalisms.
</nextsent>
<nextsent>there have been some proposals along these lines, but they have either been unnecessarily complex or applicable to only restricted to range of grammatical proposals (chesi, 2007; mainguy, 2010).this paper extends td parsing to minimalist context free grammars (mcfgs) in certain normal form and presents minimalist grammars (mgs) as succinct representation for some of those mcfgs.with this extension, the td parsing method handles an infinite range of mcfgs that encompasses,strongly and weakly, an infinite range of (many variants of) mgs in very transparent and direct way.the parsing method can be defined in complete detail very easily, and, abstracting away from limitations of time and memory, it is prov ably sound and complete for all those grammars.
</nextsent>
<nextsent>the td recognizer for mcfgs is presented in 4, generalizing and adapting ideas from earlier work (mainguy, 2010; villemonte de la clergerie, 2002).
</nextsent>
<nextsent>instead of using stack memory, this recognizer uses priority queue,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH201">
<title id=" W11-0605.xml">topdown recognizers for  </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and like human parsing, when used with 39backtracking or beam search, td memory demands need not continually increase with sentencelength: fixed bound on stack depth and on backtrack or beam depth suffices for infinitely many sentences.
</prevsent>
<prevsent>furthermore, td parsing provides explicit,relevant left contexts?
</prevsent>
</prevsection>
<citsent citstr=" J01-2004 ">
for probabilistic conditioning (roark and johnson, 1999; <papid> P99-1054 </papid>roark, 2001; <papid> J01-2004 </papid>roark, 2004).</citsent>
<aftsection>
<nextsent>but it has not been clear until recently how to apply this method to chomskian syntax or any of the other mcs grammar formalisms.
</nextsent>
<nextsent>there have been some proposals along these lines, but they have either been unnecessarily complex or applicable to only restricted to range of grammatical proposals (chesi, 2007; mainguy, 2010).this paper extends td parsing to minimalist context free grammars (mcfgs) in certain normal form and presents minimalist grammars (mgs) as succinct representation for some of those mcfgs.with this extension, the td parsing method handles an infinite range of mcfgs that encompasses,strongly and weakly, an infinite range of (many variants of) mgs in very transparent and direct way.the parsing method can be defined in complete detail very easily, and, abstracting away from limitations of time and memory, it is prov ably sound and complete for all those grammars.
</nextsent>
<nextsent>the td recognizer for mcfgs is presented in 4, generalizing and adapting ideas from earlier work (mainguy, 2010; villemonte de la clergerie, 2002).
</nextsent>
<nextsent>instead of using stack memory, this recognizer uses priority queue,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH205">
<title id=" W11-0605.xml">topdown recognizers for  </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>when there is no derivation, though, an un pruned search can fail to terminate; pruning rule can guarantee termination in such cases.
</prevsent>
<prevsent>those results extend to the mcfg recognizers proposed here.
</prevsent>
</prevsection>
<citsent citstr=" P10-1121 ">
various applications have found it better touse beam search with top-down recognition of left or right-corner transforms of cf grammars (roark, 2001; <papid> J01-2004 </papid>roark, 2004; schuler, 2010; wu et al, 2010); <papid> P10-1121 </papid>those transforms can (but need not always) disrupt grammatical connectedness as noted in 5.3.</citsent>
<aftsection>
<nextsent>workin progress explores the possibilities for such strategies in incremental mcfg parsing.
</nextsent>
<nextsent>it would also be interesting to generalize hales (2011) rational parser?
</nextsent>
<nextsent>to these grammars.
</nextsent>
<nextsent>acknowledgments thanks to thomas mainguy, sarah vanwagenenand eric villemonte de la clergerie for helpful discussions of this material.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH206">
<title id=" W11-1209.xml">extracting parallel phrases from comparable data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>comparable documents are not strictly parallel, but contain rough translations of each other, with overlapping information.
</prevsent>
<prevsent>a good example for comparable documents is the newswire text produced by multilingual news organizations such as afp or reuters.
</prevsent>
</prevsection>
<citsent citstr=" W04-3208 ">
the degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (fung and cheung, 2004).<papid> W04-3208 </papid>the web is by far the largest source of comparable data.</citsent>
<aftsection>
<nextsent>resnik and smith (2003) <papid> J03-3002 </papid>exploit the similarities in url structure, document structure and other clues for mining the web for parallel docu ments.</nextsent>
<nextsent>wikipedia has become an attractive source of comparable documents in more recent work (smith et al, 2010).<papid> N10-1063 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH207">
<title id=" W11-1209.xml">extracting parallel phrases from comparable data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a good example for comparable documents is the newswire text produced by multilingual news organizations such as afp or reuters.
</prevsent>
<prevsent>the degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (fung and cheung, 2004).<papid> W04-3208 </papid>the web is by far the largest source of comparable data.</prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
resnik and smith (2003) <papid> J03-3002 </papid>exploit the similarities in url structure, document structure and other clues for mining the web for parallel docu ments.</citsent>
<aftsection>
<nextsent>wikipedia has become an attractive source of comparable documents in more recent work (smith et al, 2010).<papid> N10-1063 </papid></nextsent>
<nextsent>comparable corpora may contain parallel data indifferent levels of granularity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH208">
<title id=" W11-1209.xml">extracting parallel phrases from comparable data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (fung and cheung, 2004).<papid> W04-3208 </papid>the web is by far the largest source of comparable data.</prevsent>
<prevsent>resnik and smith (2003) <papid> J03-3002 </papid>exploit the similarities in url structure, document structure and other clues for mining the web for parallel docu ments.</prevsent>
</prevsection>
<citsent citstr=" N10-1063 ">
wikipedia has become an attractive source of comparable documents in more recent work (smith et al, 2010).<papid> N10-1063 </papid></citsent>
<aftsection>
<nextsent>comparable corpora may contain parallel data indifferent levels of granularity.
</nextsent>
<nextsent>this includes: parallel documents, parallel sentence pairs, or parallel sub-sentential fragments.
</nextsent>
<nextsent>to simplify the proces sand reduce the computational overhead, the parallel sentence extraction is typically divided into twotasks.
</nextsent>
<nextsent>first, document level alignment is identified between comparable documents, and second,the parallel sentences are detected within the identified document pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH209">
<title id=" W11-1209.xml">extracting parallel phrases from comparable data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to simplify the proces sand reduce the computational overhead, the parallel sentence extraction is typically divided into twotasks.
</prevsent>
<prevsent>first, document level alignment is identified between comparable documents, and second,the parallel sentences are detected within the identified document pairs.
</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
cross-lingual information retrieval methods (munteanu and marcu, 2005) <papid> J05-4003 </papid>and 61 proceedings of the 4th workshop on building and using comparable corpora, pages 6168, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.</citsent>
<aftsection>
<nextsent>c2011 association for computational linguistics figure 1: sample comparable sentences that contain parallel phrases other similarity measures (fung and cheung, 2004) <papid> W04-3208 </papid>have been used for the document alignment task.zhao and vogel (2002) have extended parallel sentence alignment algorithms to identify parallel sentence pairs within comparable news corpora.</nextsent>
<nextsent>tillmann and xu (2009) <papid> N09-2024 </papid>introduced system that performs both tasks in single run without any document level pre-filtering.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH211">
<title id=" W11-1209.xml">extracting parallel phrases from comparable data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cross-lingual information retrieval methods (munteanu and marcu, 2005) <papid> J05-4003 </papid>and 61 proceedings of the 4th workshop on building and using comparable corpora, pages 6168, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.</prevsent>
<prevsent>c2011 association for computational linguistics figure 1: sample comparable sentences that contain parallel phrases other similarity measures (fung and cheung, 2004) <papid> W04-3208 </papid>have been used for the document alignment task.zhao and vogel (2002) have extended parallel sentence alignment algorithms to identify parallel sentence pairs within comparable news corpora.</prevsent>
</prevsection>
<citsent citstr=" N09-2024 ">
tillmann and xu (2009) <papid> N09-2024 </papid>introduced system that performs both tasks in single run without any document level pre-filtering.</citsent>
<aftsection>
<nextsent>such system is useful when document level boundaries are not available in the comparable corpus.
</nextsent>
<nextsent>even if two comparable documents have few orno parallel sentence pairs, there could still be parallel sub-sentential fragments, including word translation pairs, named entities, and long phrase pairs.
</nextsent>
<nextsent>the ability to identify these pairs would create valuable resource for smt, especially for low-resource languages.
</nextsent>
<nextsent>the first attempt to detect sub-sentential fragments from comparable sentences is (munteanuand marcu, 2006).<papid> P06-1011 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH212">
<title id=" W11-1209.xml">extracting parallel phrases from comparable data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even if two comparable documents have few orno parallel sentence pairs, there could still be parallel sub-sentential fragments, including word translation pairs, named entities, and long phrase pairs.
</prevsent>
<prevsent>the ability to identify these pairs would create valuable resource for smt, especially for low-resource languages.
</prevsent>
</prevsection>
<citsent citstr=" P06-1011 ">
the first attempt to detect sub-sentential fragments from comparable sentences is (munteanuand marcu, 2006).<papid> P06-1011 </papid></citsent>
<aftsection>
<nextsent>quirk et al (2007) later extended this work by proposing two generative models for comparable sentences and showed improvements when applied to cross-domain test data.
</nextsent>
<nextsent>in both these approaches the extracted fragment datawas used as additional training data to train alignment models.
</nextsent>
<nextsent>kumano et al (2007) have proposed phrasal alignment approach for comparable corpora using the joint probability smt model.
</nextsent>
<nextsent>while this approach is appealing for low-resource scenarios as it does not require any seed parallel corpus, the high computational cost is deterrent in its applicability to large corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH213">
<title id=" W11-1209.xml">extracting parallel phrases from comparable data </title>
<section> parallel phrase extraction.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 viterbi alignment.
</prevsent>
<prevsent>here we use the typical phrase extraction approach used by statistical machine translation systems: obtain word alignment models for both directions (source to target and target to source), combine theviterbi paths using one of many heuristics, and extract phrase pairs from the combined alignment.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
we used moses toolkit (koehn et al, 2007) <papid> P07-2045 </papid>for this task.to obtain the word alignments for comparable sentence pairs, we performed forced alignment using the trained models.</citsent>
<aftsection>
<nextsent>2.2 binary classifier.
</nextsent>
<nextsent>we used maximum entropy classifier as our second approach to extract parallel phrase pairs from comparable sentences.
</nextsent>
<nextsent>such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents(munteanu and marcu, 2005).<papid> J05-4003 </papid></nextsent>
<nextsent>our classifier is similar, but we apply it at phrase level rather than at sentence level.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH216">
<title id=" W11-1209.xml">extracting parallel phrases from comparable data </title>
<section> parallel phrase extraction.  </section>
<citcontext>
<prevsection>
<prevsent>z(s, ) is the normalization factor.
</prevsent>
<prevsent>in the feature vector for phrase pair (s, ), each feature appears twice, once for each class ? {0, 1}.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the feature set we use is inspired by munteanu and marcu (2005) <papid> J05-4003 </papid>who define the features based on ibm model-1 (brown et al, 1993) <papid> J93-2003 </papid>alignments for source and target pairs.</citsent>
<aftsection>
<nextsent>however, in our experiments, the features are computed primarily on ibm model-1 probabilities (i.e. lexicon).
</nextsent>
<nextsent>we do not explicitly compute ibm model-1 alignments.
</nextsent>
<nextsent>to compute coverage features, we identify alignment points for which ibm model-1 probability is above threshold.
</nextsent>
<nextsent>we produce two sets of features based on ibm model-1 probabilities obtained by training in both directions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH219">
<title id=" W11-1710.xml">developing japanese wordnet affect for analyzing emotions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the majority of subjective analysis methods that are related to emotion is based on textual keywords spotting that use specific lexical resources.
</prevsent>
<prevsent>sentiwordnet (baccianella et al, 2010) is lexical resource that assigns positive, negative and objective scores to each wordnet synset (miller, 1995).
</prevsent>
</prevsection>
<citsent citstr=" L08-1086 ">
subjectivity word list (banea et al, 2008) <papid> L08-1086 </papid>assigns words with the strong or weak subjectivity and prior polarities of types positive, negative and neu tral.</citsent>
<aftsection>
<nextsent>affective lexicon (strapparava and valitutti, 2004), one of the most efficient resources of emotion analysis, contains emotion words.
</nextsent>
<nextsent>to the best of our knowledge, these lexical resources have been created for english.
</nextsent>
<nextsent>a recent study shows that non-native english speakers support the growing use of the internet1.
</nextsent>
<nextsent>hence, there is demand for automatic text analysis tools and linguistic resources for languages other than english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH220">
<title id=" W11-1710.xml">developing japanese wordnet affect for analyzing emotions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the numbers of entries in the expanded word lists are increased by 69.77% and 74.60% at synset and word levels respectively.
</prevsent>
<prevsent>we have mapped the synsetid of the wordnet affect lists with the syn setid of the wordnet 3.03.
</prevsent>
</prevsection>
<citsent citstr=" W09-3401 ">
this mapping helps in expanding the wordnet affect lists with the recent version of sentiwordnet 3.0 4 as well as translating with the japanese wordnet (bond et al, 2009).<papid> W09-3401 </papid></citsent>
<aftsection>
<nextsent>some affect synsets (e.g., 00115193-a huffy, mad, sore) are not translated into japanese as there are no equivalent synset in the japanese wordnet.
</nextsent>
<nextsent>primarily, we have developed baseline system based on the japanese wordnet affect and carried out the evaluation on japanese judgement corpus of 89 sentences.
</nextsent>
<nextsent>the system achieves the average f-score of 36.39% with respect to six emotion classes.
</nextsent>
<nextsent>we have also incorporated an open source japanese morphological analyser 5 . the performance of the system has been increased by 4.1% in average f-score with respect to six emotion classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH222">
<title id=" W11-1710.xml">developing japanese wordnet affect for analyzing emotions </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>prepa 3 http://wordnet.princeton.edu/wordnet/download/ 4 http://sentiwordnet.isti.cnr.it/ 5 http://mecab.sourceforge.net/ 6 http://translate.google.com/# ration of the translated japanese corpus, different experiments and evaluations based on morphology and the annotated emotion scores are elaborated in section 4.
</prevsent>
<prevsent>finally section 5 concludes the paper.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
the extraction and annotation of subjective terms started with machine learning approaches (hat zivassiloglou and mckeown, 1997).<papid> P97-1023 </papid></citsent>
<aftsection>
<nextsent>some well known sentiment lexicons have been developed, such as subjective adjective list (baroni and vegnaduzzo, 2004), english sentiwordnet (esuli et. al., 2006), taboadas adjective list (voll and taboada, 2007), subjectivity word list (banea et al., 2008) <papid> L08-1086 </papid>etc. andreevskaia and bergler (2006) present method for extracting positive or negative sentiment bearing adjectives from wordnet using the sentiment tag extraction program (step).</nextsent>
<nextsent>the proposed methods in (wiebe and riloff, 2006) automatically generate resources for subjectivity analysis for new target language from the available resources for english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH224">
<title id=" W11-1710.xml">developing japanese wordnet affect for analyzing emotions </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, an automatically generated and scored sentiment lexicon, sentiful (neviarouskaya et al, 2009), its expansion, morphological modifications and distinguishing sentiment features also shows the contributory results.
</prevsent>
<prevsent>but, all of the abovementioned resources are in english and have been used in coarse grained sentiment analysis (e.g., positive, negative or neutral).
</prevsent>
</prevsection>
<citsent citstr=" P05-1017 ">
the proposed method in (takamura et al, 2005) <papid> P05-1017 </papid>extracts semantic orientations from small number of seed words with high accuracy in the experiments on english as well as japanese lexicons.</citsent>
<aftsection>
<nextsent>but, it was also aimed for sentiment bearing words.
</nextsent>
<nextsent>instead of english wordnet affect (strapparava and valitutti, 2004), there are few attempts in other languages such as, russian and romanian (bobicev et al, 2010), bengali (das and bandyopadhyay, 2010) etc. our present approach is similar to some of these approaches but in contrast, we have evaluated our japanese wordnet affect on the semeval 2007 affect sensing corpus translated into japanese.
</nextsent>
<nextsent>in recent trends, the application of mechanical turk for generating emotion lexicon (mo hammad and turney, 2010) <papid> W10-0204 </papid>shows promising results.</nextsent>
<nextsent>in the present task, we have incorporated the open source, available and accessible resources to achieve our goals.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH225">
<title id=" W11-1710.xml">developing japanese wordnet affect for analyzing emotions </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>but, it was also aimed for sentiment bearing words.
</prevsent>
<prevsent>instead of english wordnet affect (strapparava and valitutti, 2004), there are few attempts in other languages such as, russian and romanian (bobicev et al, 2010), bengali (das and bandyopadhyay, 2010) etc. our present approach is similar to some of these approaches but in contrast, we have evaluated our japanese wordnet affect on the semeval 2007 affect sensing corpus translated into japanese.
</prevsent>
</prevsection>
<citsent citstr=" W10-0204 ">
in recent trends, the application of mechanical turk for generating emotion lexicon (mo hammad and turney, 2010) <papid> W10-0204 </papid>shows promising results.</citsent>
<aftsection>
<nextsent>in the present task, we have incorporated the open source, available and accessible resources to achieve our goals.
</nextsent>
<nextsent>81
</nextsent>
<nextsent>3.1 wordnet affect.
</nextsent>
<nextsent>the english wordnet affect, based on ekmans six emotion types is small lexical resource compared to the complete wordnet but its affective annotation helps in emotion analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH228">
<title id=" W11-0215.xml">towards exhaustive event extraction for protein modifications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these ie approaches are frequently termed event extraction (ananiadou et al, 2010).
</prevsent>
<prevsent>while protein modifications have been considered in numerous ie studies in the domain (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W10-1903 ">
(fried manet al, 2001; rzhetsky et al, 2004; hu et al, 2005; narayanaswamy et al, 2005; saric et al, 2006; yuan et al, 2006; lee et al, 2008; ohta etal., 2010), <papid> W10-1903 </papid>event extraction efforts have brought increased focus also on the extraction of protein modi fications: in the bionlp shared task series that has popularized event extraction, the 2009 shared task (kim et al, 2009) <papid> W09-1401 </papid>involved the extraction of nine event types including one ptm, and in the 2011follow-up event (kim et al, 2011) <papid> W11-1801 </papid>the epi genet ics and post-translational modifications (epi) task(ohta et al, 2011) <papid> W11-1803 </papid>targeted six ptm types, their re 114verse reactions, and statements regarding their catalysis.</citsent>
<aftsection>
<nextsent>the results of these tasks were promising, suggesting that the single ptm type could be extracted at over 80% f-score (buyko et al, 2009) <papid> W09-1403 </papid>and thecore arguments of the larger set at nearly 70% score (bjorne and salakoski, 2011).</nextsent>
<nextsent>the increasing availability of systems capable of detailed ie for protein modifications, their high performance also for multiple modifications types, and demonstrations of the scala bility of the technology to the full scale of the literature (bjorne et al, 2010) are highly encouraging for automatic extraction of protein modifications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH229">
<title id=" W11-0215.xml">towards exhaustive event extraction for protein modifications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these ie approaches are frequently termed event extraction (ananiadou et al, 2010).
</prevsent>
<prevsent>while protein modifications have been considered in numerous ie studies in the domain (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
(fried manet al, 2001; rzhetsky et al, 2004; hu et al, 2005; narayanaswamy et al, 2005; saric et al, 2006; yuan et al, 2006; lee et al, 2008; ohta etal., 2010), <papid> W10-1903 </papid>event extraction efforts have brought increased focus also on the extraction of protein modi fications: in the bionlp shared task series that has popularized event extraction, the 2009 shared task (kim et al, 2009) <papid> W09-1401 </papid>involved the extraction of nine event types including one ptm, and in the 2011follow-up event (kim et al, 2011) <papid> W11-1801 </papid>the epi genet ics and post-translational modifications (epi) task(ohta et al, 2011) <papid> W11-1803 </papid>targeted six ptm types, their re 114verse reactions, and statements regarding their catalysis.</citsent>
<aftsection>
<nextsent>the results of these tasks were promising, suggesting that the single ptm type could be extracted at over 80% f-score (buyko et al, 2009) <papid> W09-1403 </papid>and thecore arguments of the larger set at nearly 70% score (bjorne and salakoski, 2011).</nextsent>
<nextsent>the increasing availability of systems capable of detailed ie for protein modifications, their high performance also for multiple modifications types, and demonstrations of the scala bility of the technology to the full scale of the literature (bjorne et al, 2010) are highly encouraging for automatic extraction of protein modifications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH230">
<title id=" W11-0215.xml">towards exhaustive event extraction for protein modifications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these ie approaches are frequently termed event extraction (ananiadou et al, 2010).
</prevsent>
<prevsent>while protein modifications have been considered in numerous ie studies in the domain (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W11-1801 ">
(fried manet al, 2001; rzhetsky et al, 2004; hu et al, 2005; narayanaswamy et al, 2005; saric et al, 2006; yuan et al, 2006; lee et al, 2008; ohta etal., 2010), <papid> W10-1903 </papid>event extraction efforts have brought increased focus also on the extraction of protein modi fications: in the bionlp shared task series that has popularized event extraction, the 2009 shared task (kim et al, 2009) <papid> W09-1401 </papid>involved the extraction of nine event types including one ptm, and in the 2011follow-up event (kim et al, 2011) <papid> W11-1801 </papid>the epi genet ics and post-translational modifications (epi) task(ohta et al, 2011) <papid> W11-1803 </papid>targeted six ptm types, their re 114verse reactions, and statements regarding their catalysis.</citsent>
<aftsection>
<nextsent>the results of these tasks were promising, suggesting that the single ptm type could be extracted at over 80% f-score (buyko et al, 2009) <papid> W09-1403 </papid>and thecore arguments of the larger set at nearly 70% score (bjorne and salakoski, 2011).</nextsent>
<nextsent>the increasing availability of systems capable of detailed ie for protein modifications, their high performance also for multiple modifications types, and demonstrations of the scala bility of the technology to the full scale of the literature (bjorne et al, 2010) are highly encouraging for automatic extraction of protein modifications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH231">
<title id=" W11-0215.xml">towards exhaustive event extraction for protein modifications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these ie approaches are frequently termed event extraction (ananiadou et al, 2010).
</prevsent>
<prevsent>while protein modifications have been considered in numerous ie studies in the domain (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W11-1803 ">
(fried manet al, 2001; rzhetsky et al, 2004; hu et al, 2005; narayanaswamy et al, 2005; saric et al, 2006; yuan et al, 2006; lee et al, 2008; ohta etal., 2010), <papid> W10-1903 </papid>event extraction efforts have brought increased focus also on the extraction of protein modi fications: in the bionlp shared task series that has popularized event extraction, the 2009 shared task (kim et al, 2009) <papid> W09-1401 </papid>involved the extraction of nine event types including one ptm, and in the 2011follow-up event (kim et al, 2011) <papid> W11-1801 </papid>the epi genet ics and post-translational modifications (epi) task(ohta et al, 2011) <papid> W11-1803 </papid>targeted six ptm types, their re 114verse reactions, and statements regarding their catalysis.</citsent>
<aftsection>
<nextsent>the results of these tasks were promising, suggesting that the single ptm type could be extracted at over 80% f-score (buyko et al, 2009) <papid> W09-1403 </papid>and thecore arguments of the larger set at nearly 70% score (bjorne and salakoski, 2011).</nextsent>
<nextsent>the increasing availability of systems capable of detailed ie for protein modifications, their high performance also for multiple modifications types, and demonstrations of the scala bility of the technology to the full scale of the literature (bjorne et al, 2010) are highly encouraging for automatic extraction of protein modifications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH232">
<title id=" W11-0215.xml">towards exhaustive event extraction for protein modifications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while protein modifications have been considered in numerous ie studies in the domain (e.g.
</prevsent>
<prevsent>(fried manet al, 2001; rzhetsky et al, 2004; hu et al, 2005; narayanaswamy et al, 2005; saric et al, 2006; yuan et al, 2006; lee et al, 2008; ohta etal., 2010), <papid> W10-1903 </papid>event extraction efforts have brought increased focus also on the extraction of protein modi fications: in the bionlp shared task series that has popularized event extraction, the 2009 shared task (kim et al, 2009) <papid> W09-1401 </papid>involved the extraction of nine event types including one ptm, and in the 2011follow-up event (kim et al, 2011) <papid> W11-1801 </papid>the epi genet ics and post-translational modifications (epi) task(ohta et al, 2011) <papid> W11-1803 </papid>targeted six ptm types, their re 114verse reactions, and statements regarding their catalysis.</prevsent>
</prevsection>
<citsent citstr=" W09-1403 ">
the results of these tasks were promising, suggesting that the single ptm type could be extracted at over 80% f-score (buyko et al, 2009) <papid> W09-1403 </papid>and thecore arguments of the larger set at nearly 70% score (bjorne and salakoski, 2011).</citsent>
<aftsection>
<nextsent>the increasing availability of systems capable of detailed ie for protein modifications, their high performance also for multiple modifications types, and demonstrations of the scala bility of the technology to the full scale of the literature (bjorne et al, 2010) are highly encouraging for automatic extraction of protein modifications.
</nextsent>
<nextsent>however, previous efforts have been restricted by the relatively narrow scope of targeted modification types.
</nextsent>
<nextsent>in the present study, we seek to address the task in full by identifying all modifications of substantial biological significance and creating an annotated resource with effectively complete type-level coverage.
</nextsent>
<nextsent>we additionally present preliminary extraction results to assess the difficulty of exhaustive modification extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH237">
<title id=" W11-0215.xml">towards exhaustive event extraction for protein modifications </title>
<section> annotation.  </section>
<citcontext>
<prevsection>
<prevsent>118 4.1 entity and event annotation.
</prevsent>
<prevsent>to maximize compatibility with existing event annotated resources, we chose to follow the general representation and annotation guidelines applied in the annotation of genia/bionlp st resources, specifically the bionlp st 2011 epi taskcorpus.
</prevsent>
</prevsection>
<citsent citstr=" W09-1313 ">
correspondingly, we followed the genia gene/gene product (ohta et al, 2009) <papid> W09-1313 </papid>annotation guidelines for marking protein mentions, extended the genia event corpus guidelines (kim et al., 2008) for the annotation of protein modification events, and marked catalysis events following the epi task representation.</citsent>
<aftsection>
<nextsent>for compatibility, we also marked event negation and speculation as in theseresources.
</nextsent>
<nextsent>we followed the go definitions for individual modification types, and in the rare cases where modification discussed in text had no existing go definition, we extrapolated from the way in which protein modifications are generally defined in go, consulting other domain ontologies andre sources (section 3.1) as necessary.
</nextsent>
<nextsent>4.2 document selection.
</nextsent>
<nextsent>as the distribution of protein modifications in pubmed is extremely skewed, random sampling would recover almost solely instances of major types such as phosphorylation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH241">
<title id=" W11-0215.xml">towards exhaustive event extraction for protein modifications </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we modified the shared task evaluation software to support the newly defined event types and ran experiments with the standard approximate span matching and partial recursive matching criteria (see (kim et al, 2009)).<papid> W09-1401 </papid>we further follow the epi task evaluation in reporting results separately for the extraction of only theme and cause arguments (core task) and for the full argument set.</prevsent>
<prevsent>5.2 event extraction method.</prevsent>
</prevsection>
<citsent citstr=" C10-1088 ">
we applied the event mine event extraction system(miwa et al, 2010<papid> C10-1088 </papid>a; miwa et al, 2010<papid> C10-1088 </papid>b), an svm based pipeline system using an architecture similar to that of the best-performing system in the bionlp st09 (bjorne et al, 2009); we refer to the studies of miwa et al for detailed description of the base system.</citsent>
<aftsection>
<nextsent>for analysing sentence structure, we applied the mogura 2.4.1 (matsuzaki and miyao, 2007) and gdep beta2 (sagae and tsujii, 2007) <papid> D07-1111 </papid>parsers.for the present study, we modified the base event mine system as follows.</nextsent>
<nextsent>first, to improve efficiency and generalizability, instead of using all words as trigger candidates as in the base system, we filtered candidates using dictionary extracted from training data and expanded by using the umls specialist lexicon (bodenreider, 2004) and the hypernyms?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH245">
<title id=" W11-0215.xml">towards exhaustive event extraction for protein modifications </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5.2 event extraction method.
</prevsent>
<prevsent>we applied the event mine event extraction system(miwa et al, 2010<papid> C10-1088 </papid>a; miwa et al, 2010<papid> C10-1088 </papid>b), an svm based pipeline system using an architecture similar to that of the best-performing system in the bionlp st09 (bjorne et al, 2009); we refer to the studies of miwa et al for detailed description of the base system.</prevsent>
</prevsection>
<citsent citstr=" D07-1111 ">
for analysing sentence structure, we applied the mogura 2.4.1 (matsuzaki and miyao, 2007) and gdep beta2 (sagae and tsujii, 2007) <papid> D07-1111 </papid>parsers.for the present study, we modified the base event mine system as follows.</citsent>
<aftsection>
<nextsent>first, to improve efficiency and generalizability, instead of using all words as trigger candidates as in the base system, we filtered candidates using dictionary extracted from training data and expanded by using the umls specialist lexicon (bodenreider, 2004) and the hypernyms?
</nextsent>
<nextsent>and similar to?
</nextsent>
<nextsent>relations in wordnet (fellbaum,1998).
</nextsent>
<nextsent>second, to allow generalization across argument types, we added support for solving single classification problem for event argument detection instead of solving multiple classification problems separated by argument types.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH246">
<title id=" W11-0506.xml">abs tractive summarization of line graphs from popular media </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, the graphic in figure 1 is intended to convey changing trend in sea levels ? relatively flat from 1900 to 1930 and then rising from 1930 to 2003.
</prevsent>
<prevsent>thus, using clarks view of language, information graphics are means of communication.research has shown that the content of information graphics in popular media is usually not repeated in the text of the accompanying article (car berry et al, 2006).
</prevsent>
</prevsection>
<citsent citstr=" P05-1028 ">
the captions of such graphics are also often uninformative or convey little of the graphics high-level message (elzer et al, 2005).<papid> P05-1028 </papid></citsent>
<aftsection>
<nextsent>this contrasts with scientific documents in which graphics are often used to visualize data, with explicit references to the graphic being used to explain their content (e.g., as shown in fig.
</nextsent>
<nextsent>a...?).
</nextsent>
<nextsent>information graphics in popular media contribute to the overall communicative goal of multimodal document and should not be ignored.
</nextsent>
<nextsent>our work is concerned with the summarization of information graphics from popular media.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH247">
<title id=" W11-0506.xml">abs tractive summarization of line graphs from popular media </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such summaries have several major applications: 1) they can be integrated with the summary of multimodal documents text, thereby producing richer summary of the overall documents content; 2) they can be stored in digital library along with the graphic itself and used to retrieve appropriate graphics in response to user queries; and 3) for individuals with sight impairments, they can be used along with ascreen reader to convey not only the text of document, but also the content of the documents graphics.
</prevsent>
<prevsent>in this paper we present our work on summarizing line graphs.
</prevsent>
</prevsection>
<citsent citstr=" W08-1103 ">
this builds on our previous efforts into summarizing bar charts (demir et al, 2008;<papid> W08-1103 </papid>elzer et al, 2011); however, line graphs have different messages and communicative signals than bar charts and their continuous nature requires different processing.</citsent>
<aftsection>
<nextsent>in addition, very different set of visual features must be taken into account in deciding the importance of including proposition in summary.
</nextsent>
<nextsent>most summarization research has focused on extractive techniques by which segments of text are extracted and put together to form the summary.
</nextsent>
<nextsent>41 ? 102468 19001 020 5060 7080 90 03 3040 2000 10 8.9 1.979 inches over the past century.
</nextsent>
<nextsent>annual difference from seattles in the seattle area, for example, he pacific ocean has risen nearly they are rising about 0 .040.09 f an inch ea ch year.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH248">
<title id=" W11-0506.xml">abs tractive summarization of line graphs from popular media </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>the main issues are identifying the knowledge conveyed by graphic, selecting the concepts that should be conveyed in summary, and integrating them into coherent natural language sentences.
</prevsent>
<prevsent>as noted in the introduction, information graphic sin popular media generally have high-level message that they are intended to convey.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
this message constitutes the primary communicative or discourse goal (grosz and sidner, 1986) <papid> J86-3001 </papid>of the graphic and captures its main contribution to the overall discourse goal of the entire document.</citsent>
<aftsection>
<nextsent>however, the graphic also includes salient features that are important components of the graphics content.
</nextsent>
<nextsent>for example, the graphic in figure 1 is very jagged with sharp fluctuations, indicating that short-term changes have been inconsistent.
</nextsent>
<nextsent>since the graphics intended message represents its primary discourse goal, we contend that this message should form the core or focus of the graphics summary.
</nextsent>
<nextsent>the salient features should be used to augment the summary of the graph and elaborate on its intended message.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH249">
<title id=" W11-0506.xml">abs tractive summarization of line graphs from popular media </title>
<section> identifying elaborative propositions.  </section>
<citcontext>
<prevsection>
<prevsent>w2 = |v| ? cov(|v|, s ) (3) this way, the stronger certain visual feature is in agiven line graph, the higher the weight for the associated proposition.type 3 rules (message category + visual fea ture) differ only from type 2 rules in that they are restricted to particular intended message category, rather than any line graph having the visual feature in question.
</prevsent>
<prevsent>for example, proposition comparing the slope of two trends may be appropriate for graph in the change-trend message category, butdoes not make sense for line graph with only single trend (e.g., rising-trend).
</prevsent>
</prevsection>
<citsent citstr=" W10-4202 ">
once all propositions have been extracted and ranked, these weights are passed along to graph based content selection framework (demir et al,2010) <papid> W10-4202 </papid>that iteratively selects for inclusion in the initial summary those propositions which provide the best coverage of the highest-ranked information.</citsent>
<aftsection>
<nextsent>4.3 sample rule application.
</nextsent>
<nextsent>figures 1 and 4 consist of two different line graphs with the same intended message category: change trend.
</nextsent>
<nextsent>figure 1 shows stable trend in annual sea level difference from 1900 to 1930, followed by rising trend through 2003, while figure 4 shows rising trend in durango sales from 1997 to 1999,followed by falling trend through 2006.
</nextsent>
<nextsent>propositions associated with type 1 rules will have the same weights for both graphs, but propositions related to visual features may have different weights.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH250">
<title id=" W11-0506.xml">abs tractive summarization of line graphs from popular media </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>the values vary lot...?, the trend is unstable...?), possibly displacing type 1 proposition that would still appear in the summary for the graph in figure 4.
</prevsent>
<prevsent>once the propositions that should be included in the summary have been selected, they must be coherently organized and realized as natural language sentences.
</prevsent>
</prevsection>
<citsent citstr=" W96-0501 ">
we anticipate using the fuf/surge surface realizer (elhadad and robin, 1996); <papid> W96-0501 </papid>our collected corpus of line graph summaries provides large set of real-world expressions to draw from when crafting the surface realization forms our system will produce for the final-output summaries.our summarization methodology must also be eval uated.</citsent>
<aftsection>
<nextsent>in particular, we must evaluate the rules for identifying the additional informational propositions that are used to elaborate the overall intended message, and the quality of the summaries both in terms of content and coherence.
</nextsent>
<nextsent>image summarization has focused on constructing smaller image that contains the important content of larger image (shi et al, 2009), selecting set of representative images that summarize collection of images (baratis et al, 2008), or constructing new diagram that summarizes one or more diagrams(futrelle, 1999).
</nextsent>
<nextsent>however, all of these efforts produce an image as the end product, not textual summary of the content of the image(s).ferres et al (2007) developed system for conveying graphs to blind users, but it generates the same basic information for each instance of graph type (e.g., line graphs) regardless of the individual graphs specific characteristics.
</nextsent>
<nextsent>efforts toward summarizing multimodal documents containing graphics have included nave approaches relying on captions and direct references to the image in the text (bhatia et al, 2009), while content-based image analysis and nlp techniques are being combined for multimodal document indexing and retrieval in the medical domain (neveol et al, 2009).jing and mckeown (1999) approached abstrac tive summarization as text-to-text generation task, modifying sentences from the original document via editing and rewriting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH251">
<title id=" W11-0420.xml">discourse constrained temporal annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also present analysis of the current temporal classification scheme and propose ways to improve it in future work.
</prevsent>
<prevsent>event-based temporal inference is fundamental natural language technology aimed at determining the temporal anchoring and relative temporal ordering between events in text.
</prevsent>
</prevsection>
<citsent citstr=" C10-2058 ">
it supports wide range of natural language applications such as information extraction (ji, 2010), <papid> C10-2058 </papid>question answering (harabagiu and bejan, 2005; harabagiu and bejan, 2006) and text summarization (lin andhovy, 2001; barzilay et al, 2002).</citsent>
<aftsection>
<nextsent>creating consistently annotated domain-independent data sufficient to train automatic systems has been the bottleneck.
</nextsent>
<nextsent>while low-level temporal annotation tasks such as identifying events and time expressions are relatively straightforward and can be done with high consistency, high-level tasks necessary to eventually arrange events in document in temporal order have proved to be much more challenging.among these high-level tasks, the task of annotating the temporal relation between main events stands out as probably the most challenging.
</nextsent>
<nextsent>this task wasthe only task in the tempe val campaigns (verha genet al, 2009; verhagen et al, 2010) <papid> S10-1010 </papid>to deal with inter-sentential temporal relations, and also the only one to directly tackle event ordering.</nextsent>
<nextsent>the idea isthat events covered in an article are scattered in different sentences, with some, presumably important ones, expressed as predicates in prominent positions of sentence (i.e. the main event?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH252">
<title id=" W11-0420.xml">discourse constrained temporal annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>creating consistently annotated domain-independent data sufficient to train automatic systems has been the bottleneck.
</prevsent>
<prevsent>while low-level temporal annotation tasks such as identifying events and time expressions are relatively straightforward and can be done with high consistency, high-level tasks necessary to eventually arrange events in document in temporal order have proved to be much more challenging.among these high-level tasks, the task of annotating the temporal relation between main events stands out as probably the most challenging.
</prevsent>
</prevsection>
<citsent citstr=" S10-1010 ">
this task wasthe only task in the tempe val campaigns (verha genet al, 2009; verhagen et al, 2010) <papid> S10-1010 </papid>to deal with inter-sentential temporal relations, and also the only one to directly tackle event ordering.</citsent>
<aftsection>
<nextsent>the idea isthat events covered in an article are scattered in different sentences, with some, presumably important ones, expressed as predicates in prominent positions of sentence (i.e. the main event?
</nextsent>
<nextsent>of the sentence).
</nextsent>
<nextsent>by relating main events from different sentences of an article temporally, one could get something of chain of important events from the article.
</nextsent>
<nextsent>this task, in both previously reported attempts, one for english (verhagen et al, 2009) and the other for chinese (xue and zhou, 2010), <papid> C10-2156 </papid>has the lowest inter-annotator agreement (at 65%) among all tasks focusing on annotating temporal relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH253">
<title id=" W11-0420.xml">discourse constrained temporal annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>of the sentence).
</prevsent>
<prevsent>by relating main events from different sentences of an article temporally, one could get something of chain of important events from the article.
</prevsent>
</prevsection>
<citsent citstr=" C10-2156 ">
this task, in both previously reported attempts, one for english (verhagen et al, 2009) and the other for chinese (xue and zhou, 2010), <papid> C10-2156 </papid>has the lowest inter-annotator agreement (at 65%) among all tasks focusing on annotating temporal relations.</citsent>
<aftsection>
<nextsent>verhagen et al (2009) attribute the difficulty, shared by all tasks annotating temporal relations, mainly to twofactors: rampant temporal vagueness in natural language and the fact that annotators are not allowed to skip hard-to-classify cases.
</nextsent>
<nextsent>xue and zhou (2010) <papid> C10-2156 </papid>take closer look at this task specifically.</nextsent>
<nextsent>they report that part of the difficulty comes from wrong?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH259">
<title id=" W11-0420.xml">discourse constrained temporal annotation </title>
<section> annotation scheme.  </section>
<citcontext>
<prevsection>
<prevsent>the scheme for the temporal annotation is covered in sections 2.2 and 2.3.
</prevsent>
<prevsent>2.1 discourse-constrained selection of main.
</prevsent>
</prevsection>
<citsent citstr=" L08-1093 ">
events and their pairs 2.1.1 discourse annotation scheme the pdtb adopts lexically grounded approach to discourse relation annotation (prasad et al, 2008).<papid> L08-1093 </papid></citsent>
<aftsection>
<nextsent>based on discourse connectives like since?, and?, and however?, discourse relation is treated as predicate taking two abstract objects (aos) (such as events, states, and propositions) as arguments.
</nextsent>
<nextsent>for example, in the sentence below, since?
</nextsent>
<nextsent>is the lexical anchor of the relation between arg1 and arg2 (example from prasad et al (2007)).
</nextsent>
<nextsent>(1) since [arg2 mcdonalds menu prices rose this year], [arg1 the actual decline may have been more].this notion is generalized to cover discourse relations that do not have lexical anchor, i.e. implicit discourse relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH261">
<title id=" W11-1411.xml">measuring language development in early childhood education a case study of grammar checking in child language transcripts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another application of grammar checking is in improving parsers performance for ungrammatical sentences.
</prevsent>
<prevsent>since most parsers are trained on written data consisting mostly of grammatical sentences, the parsers face issues when parsing ungrammatical sentences.
</prevsent>
</prevsection>
<citsent citstr=" W10-2107 ">
automatic detection and correction of these ungrammatical sentences would improve the parsers performance by detecting the ungrammatical sentences and performing second parse on the corrected sentences (caines and buttery, 2010).<papid> W10-2107 </papid></citsent>
<aftsection>
<nextsent>from an education perspective,measuring language skills has been extensively explored.
</nextsent>
<nextsent>there are systems in place that automatically detect and correct errors for second language learners (eeg-olofsson and knuttson, 2003; leacock et al., 2010).one method used in measuring language development is the analysis of transcripts of child language speech.
</nextsent>
<nextsent>child language transcripts are samples of childs utterances during specified period of time.
</nextsent>
<nextsent>educators and speech language pathologists use these samples to measure language development.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH262">
<title id=" W11-1411.xml">measuring language development in early childhood education a case study of grammar checking in child language transcripts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we compare rule based systems to statistical systems and investigate the useof different features.
</prevsent>
<prevsent>we found the statistical systems performed better than the rule based systems for most error categories.
</prevsent>
</prevsection>
<citsent citstr=" W07-0604 ">
while there has been considerable work (sagae etal., 2007) <papid> W07-0604 </papid>done on annotating child language transcripts for grammatical relations, as far as we know,there has been no work done on automatic grammar checking of child language transcripts.</citsent>
<aftsection>
<nextsent>mostof the existing work in automatic grammar checking has been done on written text.
</nextsent>
<nextsent>spoken language on the other hand, presents challenges such as dis fluencies and false restarts which are not present in written text.
</nextsent>
<nextsent>we believe that the specific research challenges that are encountered in detecting and correcting child language transcripts warrant more detailed examination.caines and buttery (2010) <papid> W10-2107 </papid>focused on identifying sentences with the missing auxiliary verb in the progressive aspect constructions.</nextsent>
<nextsent>they used logistic regression to predict the presence of zero auxiliary occurrence in the spoken british national corpus (bnc).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH264">
<title id=" W11-1411.xml">measuring language development in early childhood education a case study of grammar checking in child language transcripts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their model identified zero auxiliary constructions with 96.9% accuracy.
</prevsent>
<prevsent>they also demonstrated how their model canbe integrated into existing parsing tools, thereby increasing the number of successful parses for zero auxiliary constructions by 30%.
</prevsent>
</prevsection>
<citsent citstr=" P08-1021 ">
lee and seneff (2008) <papid> P08-1021 </papid>described system for verb error correction using template matching on parse trees in two ways.</citsent>
<aftsection>
<nextsent>their work focused on correcting the error types related to subject-verb agreement, auxiliary agreement and complementation.
</nextsent>
<nextsent>they considered the irregularities in parse trees caused by verb error forms and used n-gram counts to filter proposed corrections.
</nextsent>
<nextsent>they used the aquaintcorpus of english news text to detect the irregularities in the parse trees caused by verb error forms.they reported an accuracy of 98.93% for verb errors related to subject-verb agreement, and 98.94% for verb errors related to auxiliary agreement andcomplementation.
</nextsent>
<nextsent>bowden and fox (2002) developed system to detect and explain errors made bynon-native english speakers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH265">
<title id=" W11-1411.xml">measuring language development in early childhood education a case study of grammar checking in child language transcripts </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we found that for some error categories such as the missing auxiliary, there was high inter-annotator agreement of 95.32%, whereas for other categories such as wrong verb usage and missing articles, there was 1we will perform independent annotation of the errors and calculate inter-annotator agreement based on these independent annotations less agreement (64.2% and 65.3% respectively).
</prevsent>
<prevsent>in particular, we found low inter-annotator agreement on utterances that have errors that could be assigned to multiple categories.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the transcripts were parsed using the charniak parser (charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>since the paradise dataset consists of childrens utterances, and many of them have not mastered the language, we observed that processing these transcripts is challenging.
</nextsent>
<nextsent>as is prevalent in spoken language corpora, these transcripts had disfluencies, false restarts and incomplete utterances, which sometimes pose problems to the parser.we conducted experiments in detecting errors related to the usage of the -ing participle, subject auxiliary agreement, missing copulae, missing verb, subject-verb agreement and missing infinitivemarker to?.
</nextsent>
<nextsent>for each of these categories, we constructed one rule based classifier using regular expressions based on the parse tree structure, an alternating decision tree classifier that used rules as features and naive bayes multinomial classifier that used variety of features.
</nextsent>
<nextsent>for every category, we performed 10 fold cross validation using all the utterances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH268">
<title id=" W11-0301.xml">modeling syntactic context improves morphological segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we encourage this behavior by encoding transition distribution over adjacent words, using string match cues as proxy for grammatical agreement.
</prevsent>
<prevsent>we evaluate our model on the standard arabic treebank.
</prevsent>
</prevsection>
<citsent citstr=" N09-1024 ">
our full model yields 86.2% accuracy, outperforming the best published results (poon et al., 2009) <papid> N09-1024 </papid>by 8.5%.</citsent>
<aftsection>
<nextsent>we also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories.
</nextsent>
<nextsent>overall, our results demonstrate that incorporating syntactic information is promising direction for improving morphological analysis.
</nextsent>
<nextsent>research in unsupervised morphological segmentation has gained momentum in recent years bringing about significant developments to the area.these advances include novel bayesian formulations (goldwater et al, 2006; <papid> P06-1085 </papid>creutz and lagus,2007; johnson, 2008), <papid> W08-0704 </papid>methods for incorporating rich features in unsupervised log-linear models(poon et al, 2009) <papid> N09-1024 </papid>and the development of multilingual morphological segment ers (snyder and barzilay, 2008<papid> P08-1084 </papid>a).</nextsent>
<nextsent>our work most closely relates to approaches thataim to incorporate syntactic information into morphological analysis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH269">
<title id=" W11-0301.xml">modeling syntactic context improves morphological segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories.
</prevsent>
<prevsent>overall, our results demonstrate that incorporating syntactic information is promising direction for improving morphological analysis.
</prevsent>
</prevsection>
<citsent citstr=" P06-1085 ">
research in unsupervised morphological segmentation has gained momentum in recent years bringing about significant developments to the area.these advances include novel bayesian formulations (goldwater et al, 2006; <papid> P06-1085 </papid>creutz and lagus,2007; johnson, 2008), <papid> W08-0704 </papid>methods for incorporating rich features in unsupervised log-linear models(poon et al, 2009) <papid> N09-1024 </papid>and the development of multilingual morphological segment ers (snyder and barzilay, 2008<papid> P08-1084 </papid>a).</citsent>
<aftsection>
<nextsent>our work most closely relates to approaches thataim to incorporate syntactic information into morphological analysis.
</nextsent>
<nextsent>surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (toutanova and johnson, 2008; <papid> W08-0704 </papid>habash and rambow, 2005; <papid> P05-1071 </papid>dasgupta and ng, 2007; adler and elhadad, 2006).<papid> P06-1084 </papid></nextsent>
<nextsent>toutanova and cherry (2009)<papid> P09-1055 </papid>were the first to systematically study how to incorporate part-of-speech information into lemmati zation and empirically demonstrate the benefits ofthis combination.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH272">
<title id=" W11-0301.xml">modeling syntactic context improves morphological segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories.
</prevsent>
<prevsent>overall, our results demonstrate that incorporating syntactic information is promising direction for improving morphological analysis.
</prevsent>
</prevsection>
<citsent citstr=" W08-0704 ">
research in unsupervised morphological segmentation has gained momentum in recent years bringing about significant developments to the area.these advances include novel bayesian formulations (goldwater et al, 2006; <papid> P06-1085 </papid>creutz and lagus,2007; johnson, 2008), <papid> W08-0704 </papid>methods for incorporating rich features in unsupervised log-linear models(poon et al, 2009) <papid> N09-1024 </papid>and the development of multilingual morphological segment ers (snyder and barzilay, 2008<papid> P08-1084 </papid>a).</citsent>
<aftsection>
<nextsent>our work most closely relates to approaches thataim to incorporate syntactic information into morphological analysis.
</nextsent>
<nextsent>surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (toutanova and johnson, 2008; <papid> W08-0704 </papid>habash and rambow, 2005; <papid> P05-1071 </papid>dasgupta and ng, 2007; adler and elhadad, 2006).<papid> P06-1084 </papid></nextsent>
<nextsent>toutanova and cherry (2009)<papid> P09-1055 </papid>were the first to systematically study how to incorporate part-of-speech information into lemmati zation and empirically demonstrate the benefits ofthis combination.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH275">
<title id=" W11-0301.xml">modeling syntactic context improves morphological segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories.
</prevsent>
<prevsent>overall, our results demonstrate that incorporating syntactic information is promising direction for improving morphological analysis.
</prevsent>
</prevsection>
<citsent citstr=" P08-1084 ">
research in unsupervised morphological segmentation has gained momentum in recent years bringing about significant developments to the area.these advances include novel bayesian formulations (goldwater et al, 2006; <papid> P06-1085 </papid>creutz and lagus,2007; johnson, 2008), <papid> W08-0704 </papid>methods for incorporating rich features in unsupervised log-linear models(poon et al, 2009) <papid> N09-1024 </papid>and the development of multilingual morphological segment ers (snyder and barzilay, 2008<papid> P08-1084 </papid>a).</citsent>
<aftsection>
<nextsent>our work most closely relates to approaches thataim to incorporate syntactic information into morphological analysis.
</nextsent>
<nextsent>surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (toutanova and johnson, 2008; <papid> W08-0704 </papid>habash and rambow, 2005; <papid> P05-1071 </papid>dasgupta and ng, 2007; adler and elhadad, 2006).<papid> P06-1084 </papid></nextsent>
<nextsent>toutanova and cherry (2009)<papid> P09-1055 </papid>were the first to systematically study how to incorporate part-of-speech information into lemmati zation and empirically demonstrate the benefits ofthis combination.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH277">
<title id=" W11-0301.xml">modeling syntactic context improves morphological segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>research in unsupervised morphological segmentation has gained momentum in recent years bringing about significant developments to the area.these advances include novel bayesian formulations (goldwater et al, 2006; <papid> P06-1085 </papid>creutz and lagus,2007; johnson, 2008), <papid> W08-0704 </papid>methods for incorporating rich features in unsupervised log-linear models(poon et al, 2009) <papid> N09-1024 </papid>and the development of multilingual morphological segment ers (snyder and barzilay, 2008<papid> P08-1084 </papid>a).</prevsent>
<prevsent>our work most closely relates to approaches thataim to incorporate syntactic information into morphological analysis.</prevsent>
</prevsection>
<citsent citstr=" P05-1071 ">
surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (toutanova and johnson, 2008; <papid> W08-0704 </papid>habash and rambow, 2005; <papid> P05-1071 </papid>dasgupta and ng, 2007; adler and elhadad, 2006).<papid> P06-1084 </papid></citsent>
<aftsection>
<nextsent>toutanova and cherry (2009)<papid> P09-1055 </papid>were the first to systematically study how to incorporate part-of-speech information into lemmati zation and empirically demonstrate the benefits ofthis combination.</nextsent>
<nextsent>while our high-level goal is similar, our respective problem formulations are distinct.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH278">
<title id=" W11-0301.xml">modeling syntactic context improves morphological segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>research in unsupervised morphological segmentation has gained momentum in recent years bringing about significant developments to the area.these advances include novel bayesian formulations (goldwater et al, 2006; <papid> P06-1085 </papid>creutz and lagus,2007; johnson, 2008), <papid> W08-0704 </papid>methods for incorporating rich features in unsupervised log-linear models(poon et al, 2009) <papid> N09-1024 </papid>and the development of multilingual morphological segment ers (snyder and barzilay, 2008<papid> P08-1084 </papid>a).</prevsent>
<prevsent>our work most closely relates to approaches thataim to incorporate syntactic information into morphological analysis.</prevsent>
</prevsection>
<citsent citstr=" P06-1084 ">
surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (toutanova and johnson, 2008; <papid> W08-0704 </papid>habash and rambow, 2005; <papid> P05-1071 </papid>dasgupta and ng, 2007; adler and elhadad, 2006).<papid> P06-1084 </papid></citsent>
<aftsection>
<nextsent>toutanova and cherry (2009)<papid> P09-1055 </papid>were the first to systematically study how to incorporate part-of-speech information into lemmati zation and empirically demonstrate the benefits ofthis combination.</nextsent>
<nextsent>while our high-level goal is similar, our respective problem formulations are distinct.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH279">
<title id=" W11-0301.xml">modeling syntactic context improves morphological segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our work most closely relates to approaches thataim to incorporate syntactic information into morphological analysis.
</prevsent>
<prevsent>surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (toutanova and johnson, 2008; <papid> W08-0704 </papid>habash and rambow, 2005; <papid> P05-1071 </papid>dasgupta and ng, 2007; adler and elhadad, 2006).<papid> P06-1084 </papid></prevsent>
</prevsection>
<citsent citstr=" P09-1055 ">
toutanova and cherry (2009)<papid> P09-1055 </papid>were the first to systematically study how to incorporate part-of-speech information into lemmati zation and empirically demonstrate the benefits ofthis combination.</citsent>
<aftsection>
<nextsent>while our high-level goal is similar, our respective problem formulations are distinct.
</nextsent>
<nextsent>toutanova and cherry (2009)<papid> P09-1055 </papid> have considered asemi-supervised setting where an initial morphological dictionary and tagging lexicon are provided but the model also has access to unlabeled data.</nextsent>
<nextsent>since alemmatizer and tagger trained in isolation may produce mutually inconsistent assignments, and their method employs log-linear reranker to reconcile these decisions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH295">
<title id=" W11-0301.xml">modeling syntactic context improves morphological segmentation </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>,k}) w|t ? dirichlet(w|t,w t)2we design the model as such since the dependencies between affixes and the pos tag are much stronger than those between the stems and tags.
</prevsent>
<prevsent>in our preliminary experiments, when stems are also generated conditioned on the tag, spurious stems are easily created and associated with garbage-collecting tags.
</prevsent>
</prevsection>
<citsent citstr=" D10-1083 ">
4 here, t refers to the set of word types that are generated by tag t. in other words, conditioned on tag t, we can only generate word from the set of word types inw which is generated earlier (lee et al., 2010).<papid> D10-1083 </papid>token-seg model the model captures the morphological agreement between adjacent segment ations using first-order markov chain.</citsent>
<aftsection>
<nextsent>the probability of drawing sequence of segment ations is given by pseg(s|w ,s,t ,l,?,?)
</nextsent>
<nextsent>(si1,si) p(si|si1)for each pair of segment ations si1 and si, we de termine: (1) if they should exhibit morpho-syntacticagreement, and (2) if their morphological segmenta tions are consistent.
</nextsent>
<nextsent>to answer the first question, we first obtain the final suffix for each of them.
</nextsent>
<nextsent>next, we obtain n, the length of the longer suffix.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH296">
<title id=" W11-0301.xml">modeling syntactic context improves morphological segmentation </title>
<section> inference.  </section>
<citcontext>
<prevsection>
<prevsent>the integration induces dependences between all token occurrences of word type which results in ascending facto rials defined as ?(m) = ?(?
</prevsent>
<prevsent>+ 1) ? ?
</prevsent>
</prevsection>
<citsent citstr=" N10-1082 ">
+ ? 1) (liang et al, 2010).<papid> N10-1082 </papid></citsent>
<aftsection>
<nextsent>mitis the number of tokens that have pos tag t, mi is the number of tokens wi, and it?|t is the number of tokens t-to-t?
</nextsent>
<nextsent>transitions.
</nextsent>
<nextsent>(both exclude counts contributed by tokens belong to word type wi.)
</nextsent>
<nextsent>|w t| is the number of word types with tag t. token-seg model ? mi1 1 ? mi2 2 ? mi3 3 (5)here,mi1 refers to the number of transitions involving token occurrences of word type wi that exhibit morphological agreement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH332">
<title id=" W10-4116.xml">exploring english lexicon knowledge for chinese sentiment analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sentiment analysis aims to understand subjective information such as opinions, attitudes, and feelings expressed in text.
</prevsent>
<prevsent>it has become hot topic in recent years because of the explosion in availability of peoples attitudes and opinions expressed in social media including blogs, discussion forums, tweets, etc. research in sentiment analysis has mainly focused on the english language.
</prevsent>
</prevsection>
<citsent citstr=" D08-1014 ">
there have been few studies in sentiment analysis in other languages due to the lack of resources, such as subjectivity lexicons consisting of list of words marked with their respective polarity (positive, negative or neutral) and manually labeled subjectivity corpora with documents labeled with their polarity.pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the english translation of foreign language text (banea et al , 2008; <papid> D08-1014 </papid>bautin et al , 2008; wan, 2009).<papid> P09-1027 </papid></citsent>
<aftsection>
<nextsent>the major problem is that they cannot be generalized well when there is domain mismatch between the source and target languages.
</nextsent>
<nextsent>there have also been increasing interests in exploiting bootstrapping style approaches for weakly-supervised sentiment classification in languages other than english (zagibalov and carroll, 2008<papid> I08-1040 </papid>b; zagibalov and carroll, 2008<papid> I08-1040 </papid>a; qiu et al , 2009).</nextsent>
<nextsent>other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (tan et al , 2008) or combining sentiment classification outputs from different experimental settings (wan, 2008).<papid> D08-1058 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH333">
<title id=" W10-4116.xml">exploring english lexicon knowledge for chinese sentiment analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sentiment analysis aims to understand subjective information such as opinions, attitudes, and feelings expressed in text.
</prevsent>
<prevsent>it has become hot topic in recent years because of the explosion in availability of peoples attitudes and opinions expressed in social media including blogs, discussion forums, tweets, etc. research in sentiment analysis has mainly focused on the english language.
</prevsent>
</prevsection>
<citsent citstr=" P09-1027 ">
there have been few studies in sentiment analysis in other languages due to the lack of resources, such as subjectivity lexicons consisting of list of words marked with their respective polarity (positive, negative or neutral) and manually labeled subjectivity corpora with documents labeled with their polarity.pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the english translation of foreign language text (banea et al , 2008; <papid> D08-1014 </papid>bautin et al , 2008; wan, 2009).<papid> P09-1027 </papid></citsent>
<aftsection>
<nextsent>the major problem is that they cannot be generalized well when there is domain mismatch between the source and target languages.
</nextsent>
<nextsent>there have also been increasing interests in exploiting bootstrapping style approaches for weakly-supervised sentiment classification in languages other than english (zagibalov and carroll, 2008<papid> I08-1040 </papid>b; zagibalov and carroll, 2008<papid> I08-1040 </papid>a; qiu et al , 2009).</nextsent>
<nextsent>other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (tan et al , 2008) or combining sentiment classification outputs from different experimental settings (wan, 2008).<papid> D08-1058 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH334">
<title id=" W10-4116.xml">exploring english lexicon knowledge for chinese sentiment analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there have been few studies in sentiment analysis in other languages due to the lack of resources, such as subjectivity lexicons consisting of list of words marked with their respective polarity (positive, negative or neutral) and manually labeled subjectivity corpora with documents labeled with their polarity.pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the english translation of foreign language text (banea et al , 2008; <papid> D08-1014 </papid>bautin et al , 2008; wan, 2009).<papid> P09-1027 </papid></prevsent>
<prevsent>the major problem is that they cannot be generalized well when there is domain mismatch between the source and target languages.</prevsent>
</prevsection>
<citsent citstr=" I08-1040 ">
there have also been increasing interests in exploiting bootstrapping style approaches for weakly-supervised sentiment classification in languages other than english (zagibalov and carroll, 2008<papid> I08-1040 </papid>b; zagibalov and carroll, 2008<papid> I08-1040 </papid>a; qiu et al , 2009).</citsent>
<aftsection>
<nextsent>other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (tan et al , 2008) or combining sentiment classification outputs from different experimental settings (wan, 2008).<papid> D08-1058 </papid></nextsent>
<nextsent>nevertheless, all these approaches are either complex or require careful tuning of domain and data specific parameters.this paper proposes weakly-supervised approach for chinese sentiment classification by incorporating language-specific lexical knowledge obtained from available english sentiment lexicons through machine translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH354">
<title id=" W10-4116.xml">exploring english lexicon knowledge for chinese sentiment analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the major problem is that they cannot be generalized well when there is domain mismatch between the source and target languages.
</prevsent>
<prevsent>there have also been increasing interests in exploiting bootstrapping style approaches for weakly-supervised sentiment classification in languages other than english (zagibalov and carroll, 2008<papid> I08-1040 </papid>b; zagibalov and carroll, 2008<papid> I08-1040 </papid>a; qiu et al , 2009).</prevsent>
</prevsection>
<citsent citstr=" D08-1058 ">
other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (tan et al , 2008) or combining sentiment classification outputs from different experimental settings (wan, 2008).<papid> D08-1058 </papid></citsent>
<aftsection>
<nextsent>nevertheless, all these approaches are either complex or require careful tuning of domain and data specific parameters.this paper proposes weakly-supervised approach for chinese sentiment classification by incorporating language-specific lexical knowledge obtained from available english sentiment lexicons through machine translation.
</nextsent>
<nextsent>unlike other cross-lingual sentiment classification methods which often require labeled corpora for training and therefore hinder their applicability for cross-domain sentiment analysis, the proposed approach does not require labeled documents.
</nextsent>
<nextsent>moreover, as opposed to existing weakly supervised sentiment classification approaches which are rather complex, slow, and require careful parameter tuning, the proposed approach is simple and computationally efficient; rendering more suitable for online and real-time sentiment classification from the web.our experimental results on the chinese reviews of four different product types show thatthe lda model performs as well as the supervised classifiers such as naive bayes and support vector machines trained from labeled corpora.
</nextsent>
<nextsent>although this paper primarily studies sentiment analysis in chinese, the proposed approach is applicable to any other language so long as machine translation engine is available between the selected language and english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH355">
<title id=" W10-4116.xml">exploring english lexicon knowledge for chinese sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, section 6 concludes the paper.
</prevsent>
<prevsent>pilot studies on cross-lingual sentiment analysis rely on english corpora for subjectivity classification in other languages.
</prevsent>
</prevsection>
<citsent citstr=" P07-1123 ">
for example, mihalcea et al  (2007) <papid> P07-1123 </papid>make use of bilingual lexicon and manually translated parallel text to generate the resources to build subjectivity classifiers based on support vector machines (svms) and naive bayes (nb) in new language; banea et al .</citsent>
<aftsection>
<nextsent>(2008) use machine translation to produce corpus in new language and train svms andnb for subjectivity classification in the new language.
</nextsent>
<nextsent>bautin et al  (2008) also utilize machine translation to perform sentiment analysis on the english translation of foreign language text.more recently, wan (2009) <papid> P09-1027 </papid>proposed co training approach to tackle the problem of cross lingual sentiment classification by leveraging an available english corpus for chinese sentiment classification.</nextsent>
<nextsent>similar to the approach proposed in (banea et al , 2008), <papid> D08-1014 </papid>wans method also uses machine translation to produced labeled chinese review corpus from the available labeled english review data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH395">
<title id=" W10-4154.xml">combine person name and person identity recognition and document clustering for chinese person name disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, chinese text is written in continuous character strings without word gap.
</prevsent>
<prevsent>it leads to the problem that some person names may be segmented into wrong forms.
</prevsent>
</prevsection>
<citsent citstr=" P04-1076 ">
in the recent years, there have been many researches on person name disambiguation (fleischman and hovy 2004; li et al 2004; niu et al 2004; <papid> P04-1076 </papid>bekkerman and mccallum 2005; chen and martin 2007; <papid> W07-2024 </papid>song et al 2009).</citsent>
<aftsection>
<nextsent>to promote the research in this area, web people search (weps and weps2) provides standard evaluation, which focuses on information extraction of personal named-entities in web data (ar tiles et al, 2007; <papid> W07-2012 </papid>artiles et al, 2009; sekine and artiles, 2009).</nextsent>
<nextsent>generally speaking, both cluster based techniques which cluster documents corresponding to one person with similar contexts, global features and document features (han et al 2004; pedersen et al 2005; elmacioglu et al 2007; <papid> W07-2058 </papid>pedersen and anagha 2007; rao et al 2007) <papid> W07-2042 </papid>and information extraction based techniques which recognizes/extracts the description features of one person name (heyl and neumann 2007; chen et al 2009) are adopted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH396">
<title id=" W10-4154.xml">combine person name and person identity recognition and document clustering for chinese person name disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, chinese text is written in continuous character strings without word gap.
</prevsent>
<prevsent>it leads to the problem that some person names may be segmented into wrong forms.
</prevsent>
</prevsection>
<citsent citstr=" W07-2024 ">
in the recent years, there have been many researches on person name disambiguation (fleischman and hovy 2004; li et al 2004; niu et al 2004; <papid> P04-1076 </papid>bekkerman and mccallum 2005; chen and martin 2007; <papid> W07-2024 </papid>song et al 2009).</citsent>
<aftsection>
<nextsent>to promote the research in this area, web people search (weps and weps2) provides standard evaluation, which focuses on information extraction of personal named-entities in web data (ar tiles et al, 2007; <papid> W07-2012 </papid>artiles et al, 2009; sekine and artiles, 2009).</nextsent>
<nextsent>generally speaking, both cluster based techniques which cluster documents corresponding to one person with similar contexts, global features and document features (han et al 2004; pedersen et al 2005; elmacioglu et al 2007; <papid> W07-2058 </papid>pedersen and anagha 2007; rao et al 2007) <papid> W07-2042 </papid>and information extraction based techniques which recognizes/extracts the description features of one person name (heyl and neumann 2007; chen et al 2009) are adopted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH397">
<title id=" W10-4154.xml">combine person name and person identity recognition and document clustering for chinese person name disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it leads to the problem that some person names may be segmented into wrong forms.
</prevsent>
<prevsent>in the recent years, there have been many researches on person name disambiguation (fleischman and hovy 2004; li et al 2004; niu et al 2004; <papid> P04-1076 </papid>bekkerman and mccallum 2005; chen and martin 2007; <papid> W07-2024 </papid>song et al 2009).</prevsent>
</prevsection>
<citsent citstr=" W07-2012 ">
to promote the research in this area, web people search (weps and weps2) provides standard evaluation, which focuses on information extraction of personal named-entities in web data (ar tiles et al, 2007; <papid> W07-2012 </papid>artiles et al, 2009; sekine and artiles, 2009).</citsent>
<aftsection>
<nextsent>generally speaking, both cluster based techniques which cluster documents corresponding to one person with similar contexts, global features and document features (han et al 2004; pedersen et al 2005; elmacioglu et al 2007; <papid> W07-2058 </papid>pedersen and anagha 2007; rao et al 2007) <papid> W07-2042 </papid>and information extraction based techniques which recognizes/extracts the description features of one person name (heyl and neumann 2007; chen et al 2009) are adopted.</nextsent>
<nextsent>considering that these evaluations are only applied to english text, cips-sighan 2010 bakeoff proposed the first evaluation campaign on chinese person name disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH398">
<title id=" W10-4154.xml">combine person name and person identity recognition and document clustering for chinese person name disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the recent years, there have been many researches on person name disambiguation (fleischman and hovy 2004; li et al 2004; niu et al 2004; <papid> P04-1076 </papid>bekkerman and mccallum 2005; chen and martin 2007; <papid> W07-2024 </papid>song et al 2009).</prevsent>
<prevsent>to promote the research in this area, web people search (weps and weps2) provides standard evaluation, which focuses on information extraction of personal named-entities in web data (ar tiles et al, 2007; <papid> W07-2012 </papid>artiles et al, 2009; sekine and artiles, 2009).</prevsent>
</prevsection>
<citsent citstr=" W07-2058 ">
generally speaking, both cluster based techniques which cluster documents corresponding to one person with similar contexts, global features and document features (han et al 2004; pedersen et al 2005; elmacioglu et al 2007; <papid> W07-2058 </papid>pedersen and anagha 2007; rao et al 2007) <papid> W07-2042 </papid>and information extraction based techniques which recognizes/extracts the description features of one person name (heyl and neumann 2007; chen et al 2009) are adopted.</citsent>
<aftsection>
<nextsent>considering that these evaluations are only applied to english text, cips-sighan 2010 bakeoff proposed the first evaluation campaign on chinese person name disambiguation.
</nextsent>
<nextsent>in this evaluation, corresponding to given index person name string, the systems are required to recognize each identical person having the index string as substring and classify the document corresponding to each identical person into group.
</nextsent>
<nextsent>this paper presents the design and implementation of hitsz_cityu system in this bakeoff.
</nextsent>
<nextsent>this system incorporates both recogni tion/extract technique and clustering technique for person name disambiguation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH399">
<title id=" W10-4154.xml">combine person name and person identity recognition and document clustering for chinese person name disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the recent years, there have been many researches on person name disambiguation (fleischman and hovy 2004; li et al 2004; niu et al 2004; <papid> P04-1076 </papid>bekkerman and mccallum 2005; chen and martin 2007; <papid> W07-2024 </papid>song et al 2009).</prevsent>
<prevsent>to promote the research in this area, web people search (weps and weps2) provides standard evaluation, which focuses on information extraction of personal named-entities in web data (ar tiles et al, 2007; <papid> W07-2012 </papid>artiles et al, 2009; sekine and artiles, 2009).</prevsent>
</prevsection>
<citsent citstr=" W07-2042 ">
generally speaking, both cluster based techniques which cluster documents corresponding to one person with similar contexts, global features and document features (han et al 2004; pedersen et al 2005; elmacioglu et al 2007; <papid> W07-2058 </papid>pedersen and anagha 2007; rao et al 2007) <papid> W07-2042 </papid>and information extraction based techniques which recognizes/extracts the description features of one person name (heyl and neumann 2007; chen et al 2009) are adopted.</citsent>
<aftsection>
<nextsent>considering that these evaluations are only applied to english text, cips-sighan 2010 bakeoff proposed the first evaluation campaign on chinese person name disambiguation.
</nextsent>
<nextsent>in this evaluation, corresponding to given index person name string, the systems are required to recognize each identical person having the index string as substring and classify the document corresponding to each identical person into group.
</nextsent>
<nextsent>this paper presents the design and implementation of hitsz_cityu system in this bakeoff.
</nextsent>
<nextsent>this system incorporates both recogni tion/extract technique and clustering technique for person name disambiguation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH400">
<title id=" W11-0141.xml">corpus based approaches to processing the scope of negation cues an evaluation of the state of the art </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>positive polarity is used by speakers to put information as fact in the world, whereas negative polarity is used to put information as counter fact, fact that does not hold in the world.
</prevsent>
<prevsent>negation is linguistic resource used to express negative polarity.
</prevsent>
</prevsection>
<citsent citstr=" H05-2018 ">
although the treatment of these topics in computational linguistics is relatively new compared to other areas like machine translation, parsing or semantic role labeling, incorporating information about modality and polarity has been shown to be useful for number of applications, such as biomedical text processing (di marco and mercer, 2005; chapman et al, 2001), opinion mining and sentiment analysis (wilson et al, 2005), <papid> H05-2018 </papid>recognizing textual entailment (snow et al, 2006), <papid> N06-1005 </papid>and automatic style checking (ganter and strube, 2009).<papid> P09-2044 </papid></citsent>
<aftsection>
<nextsent>in general, the treatment of modality and negation is very relevant for computational applications that process factuality (saur??, 2008).
</nextsent>
<nextsent>for example, information extraction systems may be confronted with fragments of texts like the one presented in (1)1, which contains two negation cues2 (not, un-) and one speculation cue (likely) that affect the factuality of the events being expressed: (1) the atovaquone/proguanil combination has not been widely used yet in west africa so it is unlikely that the patient was initially infected with an atovaquone-resistant strain.
</nextsent>
<nextsent>so far two main tasks have been addressed within the natural language processing (nlp) community: (i) the detection of various forms of polarity and modality and (ii) the identification of the scope of negation and modality cues.
</nextsent>
<nextsent>in this paper we reflect on the achievements of the recently introduced scope finding task (section 2), we analyse the scope model that existing systems can process (section 3), and we point out aspects of the scope finding task that would be different based on observations from corpus from different domain (section 4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH401">
<title id=" W11-0141.xml">corpus based approaches to processing the scope of negation cues an evaluation of the state of the art </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>positive polarity is used by speakers to put information as fact in the world, whereas negative polarity is used to put information as counter fact, fact that does not hold in the world.
</prevsent>
<prevsent>negation is linguistic resource used to express negative polarity.
</prevsent>
</prevsection>
<citsent citstr=" N06-1005 ">
although the treatment of these topics in computational linguistics is relatively new compared to other areas like machine translation, parsing or semantic role labeling, incorporating information about modality and polarity has been shown to be useful for number of applications, such as biomedical text processing (di marco and mercer, 2005; chapman et al, 2001), opinion mining and sentiment analysis (wilson et al, 2005), <papid> H05-2018 </papid>recognizing textual entailment (snow et al, 2006), <papid> N06-1005 </papid>and automatic style checking (ganter and strube, 2009).<papid> P09-2044 </papid></citsent>
<aftsection>
<nextsent>in general, the treatment of modality and negation is very relevant for computational applications that process factuality (saur??, 2008).
</nextsent>
<nextsent>for example, information extraction systems may be confronted with fragments of texts like the one presented in (1)1, which contains two negation cues2 (not, un-) and one speculation cue (likely) that affect the factuality of the events being expressed: (1) the atovaquone/proguanil combination has not been widely used yet in west africa so it is unlikely that the patient was initially infected with an atovaquone-resistant strain.
</nextsent>
<nextsent>so far two main tasks have been addressed within the natural language processing (nlp) community: (i) the detection of various forms of polarity and modality and (ii) the identification of the scope of negation and modality cues.
</nextsent>
<nextsent>in this paper we reflect on the achievements of the recently introduced scope finding task (section 2), we analyse the scope model that existing systems can process (section 3), and we point out aspects of the scope finding task that would be different based on observations from corpus from different domain (section 4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH402">
<title id=" W11-0141.xml">corpus based approaches to processing the scope of negation cues an evaluation of the state of the art </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>positive polarity is used by speakers to put information as fact in the world, whereas negative polarity is used to put information as counter fact, fact that does not hold in the world.
</prevsent>
<prevsent>negation is linguistic resource used to express negative polarity.
</prevsent>
</prevsection>
<citsent citstr=" P09-2044 ">
although the treatment of these topics in computational linguistics is relatively new compared to other areas like machine translation, parsing or semantic role labeling, incorporating information about modality and polarity has been shown to be useful for number of applications, such as biomedical text processing (di marco and mercer, 2005; chapman et al, 2001), opinion mining and sentiment analysis (wilson et al, 2005), <papid> H05-2018 </papid>recognizing textual entailment (snow et al, 2006), <papid> N06-1005 </papid>and automatic style checking (ganter and strube, 2009).<papid> P09-2044 </papid></citsent>
<aftsection>
<nextsent>in general, the treatment of modality and negation is very relevant for computational applications that process factuality (saur??, 2008).
</nextsent>
<nextsent>for example, information extraction systems may be confronted with fragments of texts like the one presented in (1)1, which contains two negation cues2 (not, un-) and one speculation cue (likely) that affect the factuality of the events being expressed: (1) the atovaquone/proguanil combination has not been widely used yet in west africa so it is unlikely that the patient was initially infected with an atovaquone-resistant strain.
</nextsent>
<nextsent>so far two main tasks have been addressed within the natural language processing (nlp) community: (i) the detection of various forms of polarity and modality and (ii) the identification of the scope of negation and modality cues.
</nextsent>
<nextsent>in this paper we reflect on the achievements of the recently introduced scope finding task (section 2), we analyse the scope model that existing systems can process (section 3), and we point out aspects of the scope finding task that would be different based on observations from corpus from different domain (section 4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH403">
<title id=" W11-0141.xml">corpus based approaches to processing the scope of negation cues an evaluation of the state of the art </title>
<section> achievements in scope processing.  </section>
<citcontext>
<prevsection>
<prevsent>350
</prevsent>
<prevsent>in the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems.
</prevsent>
</prevsection>
<citsent citstr=" P07-1125 ">
annotation has been performed at different levels: word (hassan and radev, 2010), expression (baker et al, 2010; toprak et al, 2010), sentence (medlock and briscoe, 2007), <papid> P07-1125 </papid>event (saur??</citsent>
<aftsection>
<nextsent>and pustejovsky, 2009), discourse relation (prasad et al, 2006), <papid> W06-0305 </papid>text (amancio et al, 2010), and scope of negation and modality cues (vincze et al, 2008).thanks to the existence of the bio scope corpus, the scope processing task was recently introduced.</nextsent>
<nextsent>bio scope is freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH404">
<title id=" W11-0141.xml">corpus based approaches to processing the scope of negation cues an evaluation of the state of the art </title>
<section> achievements in scope processing.  </section>
<citcontext>
<prevsection>
<prevsent>in the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems.
</prevsent>
<prevsent>annotation has been performed at different levels: word (hassan and radev, 2010), expression (baker et al, 2010; toprak et al, 2010), sentence (medlock and briscoe, 2007), <papid> P07-1125 </papid>event (saur??</prevsent>
</prevsection>
<citsent citstr=" W06-0305 ">
and pustejovsky, 2009), discourse relation (prasad et al, 2006), <papid> W06-0305 </papid>text (amancio et al, 2010), and scope of negation and modality cues (vincze et al, 2008).thanks to the existence of the bio scope corpus, the scope processing task was recently introduced.</citsent>
<aftsection>
<nextsent>bio scope is freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope.
</nextsent>
<nextsent>the scope processing task is concerned with determining at sentence level which tokens are affected by modality and negation cues.
</nextsent>
<nextsent>it was first modelled as classification problem by morante et al (2008).<papid> D08-1075 </papid></nextsent>
<nextsent>later on several systems have been trained on the same corpus (morante and daelemans, 2009; <papid> W09-1304 </papid>ozgur and radev, 2009; agarwal and yu, 2010; li et al, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH405">
<title id=" W11-0141.xml">corpus based approaches to processing the scope of negation cues an evaluation of the state of the art </title>
<section> achievements in scope processing.  </section>
<citcontext>
<prevsection>
<prevsent>bio scope is freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope.
</prevsent>
<prevsent>the scope processing task is concerned with determining at sentence level which tokens are affected by modality and negation cues.
</prevsent>
</prevsection>
<citsent citstr=" D08-1075 ">
it was first modelled as classification problem by morante et al (2008).<papid> D08-1075 </papid></citsent>
<aftsection>
<nextsent>later on several systems have been trained on the same corpus (morante and daelemans, 2009; <papid> W09-1304 </papid>ozgur and radev, 2009; agarwal and yu, 2010; li et al, 2010).</nextsent>
<nextsent>councill et al (2010) process scopes of negation cues in different corpus of product reviews, but this corpus is not publicly available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH406">
<title id=" W11-0141.xml">corpus based approaches to processing the scope of negation cues an evaluation of the state of the art </title>
<section> achievements in scope processing.  </section>
<citcontext>
<prevsection>
<prevsent>the scope processing task is concerned with determining at sentence level which tokens are affected by modality and negation cues.
</prevsent>
<prevsent>it was first modelled as classification problem by morante et al (2008).<papid> D08-1075 </papid></prevsent>
</prevsection>
<citsent citstr=" W09-1304 ">
later on several systems have been trained on the same corpus (morante and daelemans, 2009; <papid> W09-1304 </papid>ozgur and radev, 2009; agarwal and yu, 2010; li et al, 2010).</citsent>
<aftsection>
<nextsent>councill et al (2010) process scopes of negation cues in different corpus of product reviews, but this corpus is not publicly available.
</nextsent>
<nextsent>the conll shared task 2010 on learning to detect hedges and their scope in natural language text (farkas et al, 2010) boosted research on the topic.
</nextsent>
<nextsent>it consisted of identifying sentences containing uncertainty and recognizing speculative text spans inside sentences.
</nextsent>
<nextsent>participating systems would, for example, produce the tagged sentence in (2)3, in which propose, suggest and possible are identified as hedge cues and their scope is marked in agreement with the gold standard.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH407">
<title id=" W11-0141.xml">corpus based approaches to processing the scope of negation cues an evaluation of the state of the art </title>
<section> achievements in scope processing.  </section>
<citcontext>
<prevsection>
<prevsent>it consisted of identifying sentences containing uncertainty and recognizing speculative text spans inside sentences.
</prevsent>
<prevsent>participating systems would, for example, produce the tagged sentence in (2)3, in which propose, suggest and possible are identified as hedge cues and their scope is marked in agreement with the gold standard.
</prevsent>
</prevsection>
<citsent citstr=" W10-3006 ">
(2) we [propose propose that the existence of the alternative alignments, specific to distinct groups of genes, [suggest suggests presence of different synchronization modes between the two organisms and [possible possible functional decoupling of particular physiological gene networks in the course of evolution possible]suggest]propose] . the best system (morante et al, 2010) <papid> W10-3006 </papid>for hedge scope finding in the conll st 2010 scores 57.32 f-score.</citsent>
<aftsection>
<nextsent>although the results are lower than the scores obtained in other well established tasks (i.e. semantic role labeling, dependency parsing), we can say that setting the first step towards automatic scope processing is an achievement.
</nextsent>
<nextsent>however, it can be useful to revise the characteristics of the scopes that the systems learn to process, not from technical machine learning perspective, but from the linguistic annotation perspective, since the annotation model that systems learn determines the quality of the system output and the knowledge that can be inferred from the scopes.
</nextsent>
<nextsent>most existing scope label ers have been trained on the bio scope corpus.
</nextsent>
<nextsent>thus, the model of scope that these systems learn is determined by the characteristics of scope as they have been annotated in bioscope.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH408">
<title id=" W11-1013.xml">combining statistical and semantic approaches to the translation of ontologies and taxonomies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it seems natural that for applications that use such ontologies and taxonomies, translation of the natural language descriptions associated with them is required in order to adapt these methods to new languages.
</prevsent>
<prevsent>currently, there has been some work on this in the context of ontology localisation, such as espinoza et al (2008) and (2009), cimiano etal.
</prevsent>
</prevsection>
<citsent citstr=" P10-1023 ">
(2010), fu et al (2010) and navigli and pen zetto (2010).<papid> P10-1023 </papid></citsent>
<aftsection>
<nextsent>however, this work has focused on the case in which exact or partial translations are found in other similar resources such as bilingual lexica.
</nextsent>
<nextsent>instead, in this paper we look at how we may gain an adequate translation using statistical machine translation approaches that also utilise the semantic information beyond the label or term describing the concept, that is relations among the concepts in the ontology, as well as the attributes or properties that describe concepts, as will be explained in more detail in section 2.
</nextsent>
<nextsent>current work in machine translation has shown that word sense disambiguation can play an important role by using the surrounding words as context to disambiguate terms (carpuat and wu, 2007) (<papid> D07-1007 </papid>apidianaki, 2009).<papid> E09-1010 </papid></nextsent>
<nextsent>such techniques have 116 been extrapolated to the translation of taxonomiesand ontologies, in which the context?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH409">
<title id=" W11-1013.xml">combining statistical and semantic approaches to the translation of ontologies and taxonomies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, this work has focused on the case in which exact or partial translations are found in other similar resources such as bilingual lexica.
</prevsent>
<prevsent>instead, in this paper we look at how we may gain an adequate translation using statistical machine translation approaches that also utilise the semantic information beyond the label or term describing the concept, that is relations among the concepts in the ontology, as well as the attributes or properties that describe concepts, as will be explained in more detail in section 2.
</prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
current work in machine translation has shown that word sense disambiguation can play an important role by using the surrounding words as context to disambiguate terms (carpuat and wu, 2007) (<papid> D07-1007 </papid>apidianaki, 2009).<papid> E09-1010 </papid></citsent>
<aftsection>
<nextsent>such techniques have 116 been extrapolated to the translation of taxonomiesand ontologies, in which the context?
</nextsent>
<nextsent>of taxonomy or ontology label corresponds to the ontology structure that surrounds the label in question.
</nextsent>
<nextsent>this structure, which is made up of the lexical information provided by labels and the semantic information provided by the ontology structure, defines the sense of the concept and can be exploited in the disambiguation process (espinoza et al, 2008).
</nextsent>
<nextsent>translation 2.1 formal definition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH410">
<title id=" W11-1013.xml">combining statistical and semantic approaches to the translation of ontologies and taxonomies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, this work has focused on the case in which exact or partial translations are found in other similar resources such as bilingual lexica.
</prevsent>
<prevsent>instead, in this paper we look at how we may gain an adequate translation using statistical machine translation approaches that also utilise the semantic information beyond the label or term describing the concept, that is relations among the concepts in the ontology, as well as the attributes or properties that describe concepts, as will be explained in more detail in section 2.
</prevsent>
</prevsection>
<citsent citstr=" E09-1010 ">
current work in machine translation has shown that word sense disambiguation can play an important role by using the surrounding words as context to disambiguate terms (carpuat and wu, 2007) (<papid> D07-1007 </papid>apidianaki, 2009).<papid> E09-1010 </papid></citsent>
<aftsection>
<nextsent>such techniques have 116 been extrapolated to the translation of taxonomiesand ontologies, in which the context?
</nextsent>
<nextsent>of taxonomy or ontology label corresponds to the ontology structure that surrounds the label in question.
</nextsent>
<nextsent>this structure, which is made up of the lexical information provided by labels and the semantic information provided by the ontology structure, defines the sense of the concept and can be exploited in the disambiguation process (espinoza et al, 2008).
</nextsent>
<nextsent>translation 2.1 formal definition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH411">
<title id=" W11-1013.xml">combining statistical and semantic approaches to the translation of ontologies and taxonomies </title>
<section> evaluation metrics for taxonomy and.  </section>
<citcontext>
<prevsection>
<prevsent>ontology translation given the linguistic differences in taxonomy and ontology labels, it seems necessary to investigate the effectiveness of various metrics for the evaluation of translation quality.
</prevsent>
<prevsent>there are number of metrics that are widely used for evaluating translation.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
here we will focus on some of the most widely used, namely bleu (papineni et al, 2002), <papid> P02-1040 </papid>nist (doddington, 2002), meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>and wer (mccowan et al, 2004).however, it is not clear which of these methods correlate best with human evaluation, particularly for the ontologies with short labels.</citsent>
<aftsection>
<nextsent>to evaluate thiswe collected mixture of ontologies with short labels on the topics of human diseases, agriculture, geometry and project management, producing 437labels.
</nextsent>
<nextsent>these were translated with web translation services from english to spanish, in particular google translate3, yahoo!
</nextsent>
<nextsent>babelfish4 and sdl freetranslation5.
</nextsent>
<nextsent>having obtained translations foreach label in the ontology we calculated the evaluation scores using the four metrics mentioned above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH412">
<title id=" W11-1013.xml">combining statistical and semantic approaches to the translation of ontologies and taxonomies </title>
<section> evaluation metrics for taxonomy and.  </section>
<citcontext>
<prevsection>
<prevsent>ontology translation given the linguistic differences in taxonomy and ontology labels, it seems necessary to investigate the effectiveness of various metrics for the evaluation of translation quality.
</prevsent>
<prevsent>there are number of metrics that are widely used for evaluating translation.
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
here we will focus on some of the most widely used, namely bleu (papineni et al, 2002), <papid> P02-1040 </papid>nist (doddington, 2002), meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>and wer (mccowan et al, 2004).however, it is not clear which of these methods correlate best with human evaluation, particularly for the ontologies with short labels.</citsent>
<aftsection>
<nextsent>to evaluate thiswe collected mixture of ontologies with short labels on the topics of human diseases, agriculture, geometry and project management, producing 437labels.
</nextsent>
<nextsent>these were translated with web translation services from english to spanish, in particular google translate3, yahoo!
</nextsent>
<nextsent>babelfish4 and sdl freetranslation5.
</nextsent>
<nextsent>having obtained translations foreach label in the ontology we calculated the evaluation scores using the four metrics mentioned above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH414">
<title id=" W11-1013.xml">combining statistical and semantic approaches to the translation of ontologies and taxonomies </title>
<section> approaches for taxonomy and ontology.  </section>
<citcontext>
<prevsection>
<prevsent>this is particularly valid in the caseof ontologies which frequently contain subject?
</prevsent>
<prevsent>an notations6 for not only the whole data structure but often individual elements.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
to demonstrate this we tried to translate the ifrs 2009 taxonomy using the moses decoder (koehn et al, 2007), <papid> P07-2045 </papid>which we trained on the europarl corpus (koehn, 2005), translating from spanish to english.</citsent>
<aftsection>
<nextsent>as the ifrs taxonomy is on the topic of finance and accounting, we 6for example from the dublin core vocabulary: see http: //dublincore.org/ 119 baseline with domain adaptation wer?
</nextsent>
<nextsent>0.135 0.138 meteor 0.324 0.335 nist 1.229 1.278 bleu 0.090 0.116 table 4: results of domain-adapted translation.
</nextsent>
<nextsent>lower wer scores are better chose all terms from our wikipedia corpus which belonged to categories containing the words: fi nance?, financial?, accounting?, accountancy?,bank?, banking?, economy?, economic?, in vestment?, insurance and actuarial?
</nextsent>
<nextsent>and as such we had domain corpus of approximately 5000 terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH415">
<title id=" W11-1013.xml">combining statistical and semantic approaches to the translation of ontologies and taxonomies </title>
<section> approaches for taxonomy and ontology.  </section>
<citcontext>
<prevsection>
<prevsent>lower wer scores are better chose all terms from our wikipedia corpus which belonged to categories containing the words: fi nance?, financial?, accounting?, accountancy?,bank?, banking?, economy?, economic?, in vestment?, insurance and actuarial?
</prevsent>
<prevsent>and as such we had domain corpus of approximately 5000 terms.
</prevsent>
</prevsection>
<citsent citstr=" C08-1125 ">
we then proceeded to recompute the phrase table using the methodology as described in wu et al, (2008), <papid> C08-1125 </papid>computing the probabilities as follows for some weighting factor 0   ?   1: p(e|f) = p1(e|f) + (1?</citsent>
<aftsection>
<nextsent>?)pd(e|f) where p1 is the europarl trained probability and pd the scores on our domain subset.
</nextsent>
<nextsent>the evaluation for these metrics is given in table 4.
</nextsent>
<nextsent>as can be seen with the exception of the wer metric, the domain adaption does seem to help in translation, which corroborates the results obtained by other authors.
</nextsent>
<nextsent>5.2 syntactic analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH416">
<title id=" W11-0142.xml">classifying arabic verbs using sibling classes </title>
<section> arabic verbnet and class structure.  </section>
<citcontext>
<prevsection>
<prevsent>at the same level thematic roles and their restrictions are encoded.
</prevsent>
<prevsent>the important information about the class resides in the frames reflecting alternations where the verbs can appear.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
every frameis represented as an example sentence, syntactic structure and semantic structure containing semantic predicates and their arguments and temporal information in way similar to moens and steedman (1988).<papid> J88-2003 </papid></citsent>
<aftsection>
<nextsent>every class can have subclasses for cases where members deviate from the prototypical verb in some non central points.
</nextsent>
<nextsent>a subclass recursively reflects the same structure as the main class and can (therefore) itself have subclasses.
</nextsent>
<nextsent>a subclass inherits all properties of the main class and is placed in such way that the members in the top level are closed for the information it adds.
</nextsent>
<nextsent>this fact hinders putting derived verbs participating in alternations into the main class or in one of the subclasses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH417">
<title id=" W11-1304.xml">distributional semantics and compositionality 2011 shared task description and results </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in an application, this could play the role of compositional ity prior that could, e.g., be stored in dictionary.
</prevsent>
<prevsent>there is long-living tradition within the research 21 community working on multiword units (mwus) to automatically classify mwus into either compositional or non-compositional ones.
</prevsent>
</prevsection>
<citsent citstr=" W06-1203 ">
however, it hasbeen often noted that compositionality comes in degrees, and binary classification is not valid enough in many cases (bannard et al, 2003; katz and giesbrecht, 2006).<papid> W06-1203 </papid></citsent>
<aftsection>
<nextsent>to the best of our knowledge, this has been the first attempt to offer dataset and shared task that allows to explicitly evaluate the models of graded compositionality.
</nextsent>
<nextsent>for the shared task, we aimed to get compositionality scores for phrases frequently occurring in corpora.
</nextsent>
<nextsent>since distributional models need large corpora to perform reliable statistics, and these statistics are more reliable for frequent items, we chose to restrict the candidate set to the most frequent phrases from the freely available wacky1 web corpora (baroni etal., 2009).
</nextsent>
<nextsent>those are currently downloadable for english, french, german and italian.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH418">
<title id=" W11-0136.xml">algebraic approaches to compositional distributional semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the framework is also applied to models of textual entailment, and logical and onto logical representations of natural language meaning.
</prevsent>
<prevsent>in this paper, we identify three general techniques for constructing algebras.
</prevsent>
</prevsection>
<citsent citstr=" W10-2806 ">
using quotient algebras to impose relations on free algebra, as described in (clarke et al, 2010).?<papid> W10-2806 </papid></citsent>
<aftsection>
<nextsent>defining finite-dimensional algebras using matrices.
</nextsent>
<nextsent>any finite-dimensional algebra can be described in this way; we have investigated the possibility of learning such algebras using least squares regression.
</nextsent>
<nextsent>constructing algebras from semi group to give it vector space properties.
</nextsent>
<nextsent>we sketch possible method of using this technique, identified by clarke (2007), to endow logical semantics with vector space nature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH420">
<title id=" W11-0136.xml">algebraic approaches to compositional distributional semantics </title>
<section> learning finite-dimensional algebras.  </section>
<citcontext>
<prevsection>
<prevsent>we extracted list of verbadjectivenoun sequences, and used latent semantic analysis (deerwester et al, 1990) to generate dimensional vectors for the 160 most common adjectives and nouns, and pairs of these adjectives and nouns.
</prevsent>
<prevsent>our initial results indicate that the learnt parameters tend to get very large when using least squares to find the parameters, leading to poor results; we plan to investigate other methods such as linear optimisation.
</prevsent>
</prevsection>
<citsent citstr=" W10-2805 ">
guevara (2010) <papid> W10-2805 </papid>proposed related method of learning composition which used linear regression to learn how components compose.</citsent>
<aftsection>
<nextsent>his model is however much more restrictive than ours in that the value of component in the product depends only on that same component in the composed vectors, whereas in our model, the value of the component can depend on all components in the composed vectors.
</nextsent>
<nextsent>baroni and zamparelli (2010) took similar approach, in which adjectives are modelled as matrices acting on the space of nouns, and the matrices are learnt using least squares regression.
</nextsent>
<nextsent>the algebra products we propose learning are more general than matrix products; in addition we do not need to distinguish between words which are represented as matrices and words which are represented as vectors.
</nextsent>
<nextsent>whilst the previous two techniques we have discussed are very general, and allow corpus data to be easily incorporated into the composition definition, our implementations are currently long way from being able to represent the complexities of natural language semantics that is currently possible with logical semantics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH421">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>we propose the use of non parametric bayesian model, the hierarchical dirichlet process (hdp), for the task of word sense induction.
</prevsent>
</prevsection>
<citsent citstr=" E09-1013 ">
results are shown through comparison against latent dirich let allocation (lda), parametric bayesian model employed by brody and lapata (2009) <papid> E09-1013 </papid>for this task.</citsent>
<aftsection>
<nextsent>we find that the two models achieve similar levels of induction quality, while the hdp confers the advantage of automatically inducing variable number of senses per word, as compared to manually fixing the number of senses priori, as in lda.
</nextsent>
<nextsent>this flexibility allows for the model to adapt toterms with greater or lesser polysemy, when evidenced by corpus distributional statistics.
</nextsent>
<nextsent>when trained on out-of-domain data, experimental results confirm the models ability to make use of restricted set of topically coherent induced senses, when then applied in restricted domain.
</nextsent>
<nextsent>word sense induction (wsi) is the task of automatically discovering latent senses for each word type, across collection of that words tokens situated incontext.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH423">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wsi differs from word sense disambiguation (wsd) in that the task does not assume access to some pre specified sense inventory.
</prevsent>
<prevsent>this amount sto clustering task: instances of word are partitioned into the same bin based on whether system deems them to have the same underlying meaning.
</prevsent>
</prevsection>
<citsent citstr=" E03-1020 ">
a large body of related work can be found in (schutze, 1998; pantel and lin, 2002; dorow and widdows, 2003; <papid> E03-1020 </papid>purandare and pedersen, 2004; <papid> W04-2406 </papid>bordag, 2006; <papid> E06-1018 </papid>niu et al , 2007; <papid> W07-2037 </papid>pedersen, 2007; <papid> W07-2087 </papid>brody and lapata, 2009; <papid> E09-1013 </papid>li et al , 2010; <papid> P10-1116 </papid>klapaftis and manandhar, 2010).<papid> D10-1073 </papid></citsent>
<aftsection>
<nextsent>brody and lapata (2009) <papid> E09-1013 </papid>(b&l; herein) showed that the parametric bayesian model, latent dirichlet allocation (lda), could be successfully employed for this task, as compared to previous results published for the wsi component of semeval 20071 (agirre and soroa, 2007).<papid> W07-2002 </papid></nextsent>
<nextsent>a deficiency of the lda model for wsi is that the number of senses needs to be manually specified priori, either separately for each word type, or (as done by b&l;) some fixed value that is shared globally across all types.nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (vlachos et al , 2009; <papid> W09-0210 </papid>reisinger and mooney, 2010).<papid> D10-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH424">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wsi differs from word sense disambiguation (wsd) in that the task does not assume access to some pre specified sense inventory.
</prevsent>
<prevsent>this amount sto clustering task: instances of word are partitioned into the same bin based on whether system deems them to have the same underlying meaning.
</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
a large body of related work can be found in (schutze, 1998; pantel and lin, 2002; dorow and widdows, 2003; <papid> E03-1020 </papid>purandare and pedersen, 2004; <papid> W04-2406 </papid>bordag, 2006; <papid> E06-1018 </papid>niu et al , 2007; <papid> W07-2037 </papid>pedersen, 2007; <papid> W07-2087 </papid>brody and lapata, 2009; <papid> E09-1013 </papid>li et al , 2010; <papid> P10-1116 </papid>klapaftis and manandhar, 2010).<papid> D10-1073 </papid></citsent>
<aftsection>
<nextsent>brody and lapata (2009) <papid> E09-1013 </papid>(b&l; herein) showed that the parametric bayesian model, latent dirichlet allocation (lda), could be successfully employed for this task, as compared to previous results published for the wsi component of semeval 20071 (agirre and soroa, 2007).<papid> W07-2002 </papid></nextsent>
<nextsent>a deficiency of the lda model for wsi is that the number of senses needs to be manually specified priori, either separately for each word type, or (as done by b&l;) some fixed value that is shared globally across all types.nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (vlachos et al , 2009; <papid> W09-0210 </papid>reisinger and mooney, 2010).<papid> D10-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH425">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wsi differs from word sense disambiguation (wsd) in that the task does not assume access to some pre specified sense inventory.
</prevsent>
<prevsent>this amount sto clustering task: instances of word are partitioned into the same bin based on whether system deems them to have the same underlying meaning.
</prevsent>
</prevsection>
<citsent citstr=" E06-1018 ">
a large body of related work can be found in (schutze, 1998; pantel and lin, 2002; dorow and widdows, 2003; <papid> E03-1020 </papid>purandare and pedersen, 2004; <papid> W04-2406 </papid>bordag, 2006; <papid> E06-1018 </papid>niu et al , 2007; <papid> W07-2037 </papid>pedersen, 2007; <papid> W07-2087 </papid>brody and lapata, 2009; <papid> E09-1013 </papid>li et al , 2010; <papid> P10-1116 </papid>klapaftis and manandhar, 2010).<papid> D10-1073 </papid></citsent>
<aftsection>
<nextsent>brody and lapata (2009) <papid> E09-1013 </papid>(b&l; herein) showed that the parametric bayesian model, latent dirichlet allocation (lda), could be successfully employed for this task, as compared to previous results published for the wsi component of semeval 20071 (agirre and soroa, 2007).<papid> W07-2002 </papid></nextsent>
<nextsent>a deficiency of the lda model for wsi is that the number of senses needs to be manually specified priori, either separately for each word type, or (as done by b&l;) some fixed value that is shared globally across all types.nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (vlachos et al , 2009; <papid> W09-0210 </papid>reisinger and mooney, 2010).<papid> D10-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH426">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wsi differs from word sense disambiguation (wsd) in that the task does not assume access to some pre specified sense inventory.
</prevsent>
<prevsent>this amount sto clustering task: instances of word are partitioned into the same bin based on whether system deems them to have the same underlying meaning.
</prevsent>
</prevsection>
<citsent citstr=" W07-2037 ">
a large body of related work can be found in (schutze, 1998; pantel and lin, 2002; dorow and widdows, 2003; <papid> E03-1020 </papid>purandare and pedersen, 2004; <papid> W04-2406 </papid>bordag, 2006; <papid> E06-1018 </papid>niu et al , 2007; <papid> W07-2037 </papid>pedersen, 2007; <papid> W07-2087 </papid>brody and lapata, 2009; <papid> E09-1013 </papid>li et al , 2010; <papid> P10-1116 </papid>klapaftis and manandhar, 2010).<papid> D10-1073 </papid></citsent>
<aftsection>
<nextsent>brody and lapata (2009) <papid> E09-1013 </papid>(b&l; herein) showed that the parametric bayesian model, latent dirichlet allocation (lda), could be successfully employed for this task, as compared to previous results published for the wsi component of semeval 20071 (agirre and soroa, 2007).<papid> W07-2002 </papid></nextsent>
<nextsent>a deficiency of the lda model for wsi is that the number of senses needs to be manually specified priori, either separately for each word type, or (as done by b&l;) some fixed value that is shared globally across all types.nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (vlachos et al , 2009; <papid> W09-0210 </papid>reisinger and mooney, 2010).<papid> D10-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH427">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wsi differs from word sense disambiguation (wsd) in that the task does not assume access to some pre specified sense inventory.
</prevsent>
<prevsent>this amount sto clustering task: instances of word are partitioned into the same bin based on whether system deems them to have the same underlying meaning.
</prevsent>
</prevsection>
<citsent citstr=" W07-2087 ">
a large body of related work can be found in (schutze, 1998; pantel and lin, 2002; dorow and widdows, 2003; <papid> E03-1020 </papid>purandare and pedersen, 2004; <papid> W04-2406 </papid>bordag, 2006; <papid> E06-1018 </papid>niu et al , 2007; <papid> W07-2037 </papid>pedersen, 2007; <papid> W07-2087 </papid>brody and lapata, 2009; <papid> E09-1013 </papid>li et al , 2010; <papid> P10-1116 </papid>klapaftis and manandhar, 2010).<papid> D10-1073 </papid></citsent>
<aftsection>
<nextsent>brody and lapata (2009) <papid> E09-1013 </papid>(b&l; herein) showed that the parametric bayesian model, latent dirichlet allocation (lda), could be successfully employed for this task, as compared to previous results published for the wsi component of semeval 20071 (agirre and soroa, 2007).<papid> W07-2002 </papid></nextsent>
<nextsent>a deficiency of the lda model for wsi is that the number of senses needs to be manually specified priori, either separately for each word type, or (as done by b&l;) some fixed value that is shared globally across all types.nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (vlachos et al , 2009; <papid> W09-0210 </papid>reisinger and mooney, 2010).<papid> D10-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH429">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wsi differs from word sense disambiguation (wsd) in that the task does not assume access to some pre specified sense inventory.
</prevsent>
<prevsent>this amount sto clustering task: instances of word are partitioned into the same bin based on whether system deems them to have the same underlying meaning.
</prevsent>
</prevsection>
<citsent citstr=" P10-1116 ">
a large body of related work can be found in (schutze, 1998; pantel and lin, 2002; dorow and widdows, 2003; <papid> E03-1020 </papid>purandare and pedersen, 2004; <papid> W04-2406 </papid>bordag, 2006; <papid> E06-1018 </papid>niu et al , 2007; <papid> W07-2037 </papid>pedersen, 2007; <papid> W07-2087 </papid>brody and lapata, 2009; <papid> E09-1013 </papid>li et al , 2010; <papid> P10-1116 </papid>klapaftis and manandhar, 2010).<papid> D10-1073 </papid></citsent>
<aftsection>
<nextsent>brody and lapata (2009) <papid> E09-1013 </papid>(b&l; herein) showed that the parametric bayesian model, latent dirichlet allocation (lda), could be successfully employed for this task, as compared to previous results published for the wsi component of semeval 20071 (agirre and soroa, 2007).<papid> W07-2002 </papid></nextsent>
<nextsent>a deficiency of the lda model for wsi is that the number of senses needs to be manually specified priori, either separately for each word type, or (as done by b&l;) some fixed value that is shared globally across all types.nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (vlachos et al , 2009; <papid> W09-0210 </papid>reisinger and mooney, 2010).<papid> D10-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH430">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wsi differs from word sense disambiguation (wsd) in that the task does not assume access to some pre specified sense inventory.
</prevsent>
<prevsent>this amount sto clustering task: instances of word are partitioned into the same bin based on whether system deems them to have the same underlying meaning.
</prevsent>
</prevsection>
<citsent citstr=" D10-1073 ">
a large body of related work can be found in (schutze, 1998; pantel and lin, 2002; dorow and widdows, 2003; <papid> E03-1020 </papid>purandare and pedersen, 2004; <papid> W04-2406 </papid>bordag, 2006; <papid> E06-1018 </papid>niu et al , 2007; <papid> W07-2037 </papid>pedersen, 2007; <papid> W07-2087 </papid>brody and lapata, 2009; <papid> E09-1013 </papid>li et al , 2010; <papid> P10-1116 </papid>klapaftis and manandhar, 2010).<papid> D10-1073 </papid></citsent>
<aftsection>
<nextsent>brody and lapata (2009) <papid> E09-1013 </papid>(b&l; herein) showed that the parametric bayesian model, latent dirichlet allocation (lda), could be successfully employed for this task, as compared to previous results published for the wsi component of semeval 20071 (agirre and soroa, 2007).<papid> W07-2002 </papid></nextsent>
<nextsent>a deficiency of the lda model for wsi is that the number of senses needs to be manually specified priori, either separately for each word type, or (as done by b&l;) some fixed value that is shared globally across all types.nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (vlachos et al , 2009; <papid> W09-0210 </papid>reisinger and mooney, 2010).<papid> D10-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH433">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this amount sto clustering task: instances of word are partitioned into the same bin based on whether system deems them to have the same underlying meaning.
</prevsent>
<prevsent>a large body of related work can be found in (schutze, 1998; pantel and lin, 2002; dorow and widdows, 2003; <papid> E03-1020 </papid>purandare and pedersen, 2004; <papid> W04-2406 </papid>bordag, 2006; <papid> E06-1018 </papid>niu et al , 2007; <papid> W07-2037 </papid>pedersen, 2007; <papid> W07-2087 </papid>brody and lapata, 2009; <papid> E09-1013 </papid>li et al , 2010; <papid> P10-1116 </papid>klapaftis and manandhar, 2010).<papid> D10-1073 </papid></prevsent>
</prevsection>
<citsent citstr=" W07-2002 ">
brody and lapata (2009) <papid> E09-1013 </papid>(b&l; herein) showed that the parametric bayesian model, latent dirichlet allocation (lda), could be successfully employed for this task, as compared to previous results published for the wsi component of semeval 20071 (agirre and soroa, 2007).<papid> W07-2002 </papid></citsent>
<aftsection>
<nextsent>a deficiency of the lda model for wsi is that the number of senses needs to be manually specified priori, either separately for each word type, or (as done by b&l;) some fixed value that is shared globally across all types.nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (vlachos et al , 2009; <papid> W09-0210 </papid>reisinger and mooney, 2010).<papid> D10-1114 </papid></nextsent>
<nextsent>in this work we first independently verify the results of b&l;, and then tackle the limitation on fixing the number of senses through the use of the hierarchical dirichlet process (hdp) (teh et al , 2006), non parametric bayesian model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH435">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a large body of related work can be found in (schutze, 1998; pantel and lin, 2002; dorow and widdows, 2003; <papid> E03-1020 </papid>purandare and pedersen, 2004; <papid> W04-2406 </papid>bordag, 2006; <papid> E06-1018 </papid>niu et al , 2007; <papid> W07-2037 </papid>pedersen, 2007; <papid> W07-2087 </papid>brody and lapata, 2009; <papid> E09-1013 </papid>li et al , 2010; <papid> P10-1116 </papid>klapaftis and manandhar, 2010).<papid> D10-1073 </papid></prevsent>
<prevsent>brody and lapata (2009) <papid> E09-1013 </papid>(b&l; herein) showed that the parametric bayesian model, latent dirichlet allocation (lda), could be successfully employed for this task, as compared to previous results published for the wsi component of semeval 20071 (agirre and soroa, 2007).<papid> W07-2002 </papid></prevsent>
</prevsection>
<citsent citstr=" W09-0210 ">
a deficiency of the lda model for wsi is that the number of senses needs to be manually specified priori, either separately for each word type, or (as done by b&l;) some fixed value that is shared globally across all types.nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (vlachos et al , 2009; <papid> W09-0210 </papid>reisinger and mooney, 2010).<papid> D10-1114 </papid></citsent>
<aftsection>
<nextsent>in this work we first independently verify the results of b&l;, and then tackle the limitation on fixing the number of senses through the use of the hierarchical dirichlet process (hdp) (teh et al , 2006), non parametric bayesian model.
</nextsent>
<nextsent>we show this approach leads to results of similar quality as lda, when using bag-of-words context model, in addition to allowing for variability in the number of senses across different words and domains.
</nextsent>
<nextsent>when trained on restricted domain corpus for which manually labeled sense data was present, we verify that the model may be tuned to posit similar number of senses as determined by human judges.
</nextsent>
<nextsent>when trained on broader domain collection, we show that the number of induced senses increase, in line with the intuition that wider set of genres should leadto greater diversity in underlying meanings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH436">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a large body of related work can be found in (schutze, 1998; pantel and lin, 2002; dorow and widdows, 2003; <papid> E03-1020 </papid>purandare and pedersen, 2004; <papid> W04-2406 </papid>bordag, 2006; <papid> E06-1018 </papid>niu et al , 2007; <papid> W07-2037 </papid>pedersen, 2007; <papid> W07-2087 </papid>brody and lapata, 2009; <papid> E09-1013 </papid>li et al , 2010; <papid> P10-1116 </papid>klapaftis and manandhar, 2010).<papid> D10-1073 </papid></prevsent>
<prevsent>brody and lapata (2009) <papid> E09-1013 </papid>(b&l; herein) showed that the parametric bayesian model, latent dirichlet allocation (lda), could be successfully employed for this task, as compared to previous results published for the wsi component of semeval 20071 (agirre and soroa, 2007).<papid> W07-2002 </papid></prevsent>
</prevsection>
<citsent citstr=" D10-1114 ">
a deficiency of the lda model for wsi is that the number of senses needs to be manually specified priori, either separately for each word type, or (as done by b&l;) some fixed value that is shared globally across all types.nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (vlachos et al , 2009; <papid> W09-0210 </papid>reisinger and mooney, 2010).<papid> D10-1114 </papid></citsent>
<aftsection>
<nextsent>in this work we first independently verify the results of b&l;, and then tackle the limitation on fixing the number of senses through the use of the hierarchical dirichlet process (hdp) (teh et al , 2006), non parametric bayesian model.
</nextsent>
<nextsent>we show this approach leads to results of similar quality as lda, when using bag-of-words context model, in addition to allowing for variability in the number of senses across different words and domains.
</nextsent>
<nextsent>when trained on restricted domain corpus for which manually labeled sense data was present, we verify that the model may be tuned to posit similar number of senses as determined by human judges.
</nextsent>
<nextsent>when trained on broader domain collection, we show that the number of induced senses increase, in line with the intuition that wider set of genres should leadto greater diversity in underlying meanings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH440">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> bayesian word sense induction.  </section>
<citcontext>
<prevsection>
<prevsent>k??
</prevsent>
<prevsent>n?[1,nm]m?[1,m ]k?[1,k ] figure 1: latent dirichlet allocation (lda) for wsi.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
as in prior work including b&l;, we relyon the intuition that the senses of words are hinted at by their contextual information (yarowsky, 1992).<papid> C92-2070 </papid>from the perspective of generative process, neighboring words of target are generated by the targets underlying sense.2 both lda and hdp define graphical models that generate collections of discrete data.</citsent>
<aftsection>
<nextsent>the sense of target word is first drawn from distribution and then the context of this word is generated according to that distribution.
</nextsent>
<nextsent>but while lda assumes fixed, finite set of distributions, the hdp draws from an infinite set of distributions generated by dirichlet process.
</nextsent>
<nextsent>this section details the distinction.
</nextsent>
<nextsent>figure 1 shows the lda model for word sense induction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH443">
<title id=" W11-1102.xml">non parametric bayesian word sense induction </title>
<section> experiment setting.  </section>
<citcontext>
<prevsection>
<prevsent>specifically, teh et al  (2006) describes the posterior sampling in the chinese restaurant franchise.
</prevsent>
<prevsent>4f-score of 86.9% (10w), as compared to 87.3% (10w+5w).5we relied on implementations of lda and hdp respectively from mallet (mccallum, 2002), and wang (2010).the penn treebank (marcus et al , 1994) and contains 15,852 instances of excerpts on 35 nouns.
</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
all the nouns are hand-annotated with their ontonotes senses (hovy et al , 2006), <papid> N06-2015 </papid>with an average of 3.9 senses per word.</citsent>
<aftsection>
<nextsent>evaluation method wsi is an unsupervised task that results in sense clusters with no explicit mapping to manually annotated sense data.
</nextsent>
<nextsent>to derive such mapping, we follow the supervised evaluation strategy of agirre and soroa (2007).<papid> W07-2002 </papid></nextsent>
<nextsent>annotated senses from semeval-2007 are partitioned into standard mapping set (72%), dev set (14%) and test set (14%).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH446">
<title id=" W11-0105.xml">using inverse lambda and generalization to translate english to formal languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, one of of our goals in this paper is to develop general methodologies that can be used in translating natural language to desired kr language.
</prevsent>
<prevsent>there have been several learning based approaches, mainly from two groups at mit and austin.
</prevsent>
</prevsection>
<citsent citstr=" P07-1121 ">
these include the following works: zettlemoyer and collins (2005), kate and mooney (2006), wong and mooney (2006), wong and mooney (2007), <papid> P07-1121 </papid>lu et al (2008), <papid> D08-1082 </papid>zettlemoyer and collins (2007) <papid> D07-1071 </papid>and ge and mooney (2009).<papid> P09-1069 </papid></citsent>
<aftsection>
<nextsent>given training corpus of natural language sentences coupled with their desired representations, these approaches learn model capable of translating sentences to desired meaning representation.
</nextsent>
<nextsent>for example, in the work by zettlemoyer and collins (2005), set of handcrafted rules is used to learn syntactic categories and semantic representations of words based on combinatorial categorial grammar (ccg), as described by steedman (2000), and ?-calculus formulas, as discussed by gamut (1991).
</nextsent>
<nextsent>the later work of zettlemoyer and collins (2007), <papid> D07-1071 </papid>also uses handcrafted rules.</nextsent>
<nextsent>the austin group has several papers over the years.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH448">
<title id=" W11-0105.xml">using inverse lambda and generalization to translate english to formal languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, one of of our goals in this paper is to develop general methodologies that can be used in translating natural language to desired kr language.
</prevsent>
<prevsent>there have been several learning based approaches, mainly from two groups at mit and austin.
</prevsent>
</prevsection>
<citsent citstr=" D08-1082 ">
these include the following works: zettlemoyer and collins (2005), kate and mooney (2006), wong and mooney (2006), wong and mooney (2007), <papid> P07-1121 </papid>lu et al (2008), <papid> D08-1082 </papid>zettlemoyer and collins (2007) <papid> D07-1071 </papid>and ge and mooney (2009).<papid> P09-1069 </papid></citsent>
<aftsection>
<nextsent>given training corpus of natural language sentences coupled with their desired representations, these approaches learn model capable of translating sentences to desired meaning representation.
</nextsent>
<nextsent>for example, in the work by zettlemoyer and collins (2005), set of handcrafted rules is used to learn syntactic categories and semantic representations of words based on combinatorial categorial grammar (ccg), as described by steedman (2000), and ?-calculus formulas, as discussed by gamut (1991).
</nextsent>
<nextsent>the later work of zettlemoyer and collins (2007), <papid> D07-1071 </papid>also uses handcrafted rules.</nextsent>
<nextsent>the austin group has several papers over the years.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH449">
<title id=" W11-0105.xml">using inverse lambda and generalization to translate english to formal languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, one of of our goals in this paper is to develop general methodologies that can be used in translating natural language to desired kr language.
</prevsent>
<prevsent>there have been several learning based approaches, mainly from two groups at mit and austin.
</prevsent>
</prevsection>
<citsent citstr=" D07-1071 ">
these include the following works: zettlemoyer and collins (2005), kate and mooney (2006), wong and mooney (2006), wong and mooney (2007), <papid> P07-1121 </papid>lu et al (2008), <papid> D08-1082 </papid>zettlemoyer and collins (2007) <papid> D07-1071 </papid>and ge and mooney (2009).<papid> P09-1069 </papid></citsent>
<aftsection>
<nextsent>given training corpus of natural language sentences coupled with their desired representations, these approaches learn model capable of translating sentences to desired meaning representation.
</nextsent>
<nextsent>for example, in the work by zettlemoyer and collins (2005), set of handcrafted rules is used to learn syntactic categories and semantic representations of words based on combinatorial categorial grammar (ccg), as described by steedman (2000), and ?-calculus formulas, as discussed by gamut (1991).
</nextsent>
<nextsent>the later work of zettlemoyer and collins (2007), <papid> D07-1071 </papid>also uses handcrafted rules.</nextsent>
<nextsent>the austin group has several papers over the years.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH451">
<title id=" W11-0105.xml">using inverse lambda and generalization to translate english to formal languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, one of of our goals in this paper is to develop general methodologies that can be used in translating natural language to desired kr language.
</prevsent>
<prevsent>there have been several learning based approaches, mainly from two groups at mit and austin.
</prevsent>
</prevsection>
<citsent citstr=" P09-1069 ">
these include the following works: zettlemoyer and collins (2005), kate and mooney (2006), wong and mooney (2006), wong and mooney (2007), <papid> P07-1121 </papid>lu et al (2008), <papid> D08-1082 </papid>zettlemoyer and collins (2007) <papid> D07-1071 </papid>and ge and mooney (2009).<papid> P09-1069 </papid></citsent>
<aftsection>
<nextsent>given training corpus of natural language sentences coupled with their desired representations, these approaches learn model capable of translating sentences to desired meaning representation.
</nextsent>
<nextsent>for example, in the work by zettlemoyer and collins (2005), set of handcrafted rules is used to learn syntactic categories and semantic representations of words based on combinatorial categorial grammar (ccg), as described by steedman (2000), and ?-calculus formulas, as discussed by gamut (1991).
</nextsent>
<nextsent>the later work of zettlemoyer and collins (2007), <papid> D07-1071 </papid>also uses handcrafted rules.</nextsent>
<nextsent>the austin group has several papers over the years.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH459">
<title id=" W11-0105.xml">using inverse lambda and generalization to translate english to formal languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many of their works including the one by ge and mooney (2009) <papid> P09-1069 </papid>use word alignment method to learn semantic lexicon and learn rules for composing meaning representation.</prevsent>
<prevsent>35 similar to the work by ge and mooney (2009), <papid> P09-1069 </papid>we use an existing syntactic parser to parse natural language.</prevsent>
</prevsection>
<citsent citstr=" J07-4004 ">
however we use ccg parser, as described by clark and curran (2007), <papid> J07-4004 </papid>to parse sentences, use lambda calculus for meaning representation, use the ccg parsing to compose meaning and have an initial dictionary.</citsent>
<aftsection>
<nextsent>note that unlike the work by ge and mooney (2009), <papid> P09-1069 </papid>we do not need to learn rules for composing meaning representation.</nextsent>
<nextsent>we use novel method to learn semantic lexicon which is based on two inverse lambda operators that allow us to compute given and such that f@g = or g@f = . compared to the work by zettlemoyer and collins (2005), we use the same learning approach but use completely different approach in lexical generation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH476">
<title id=" W11-0105.xml">using inverse lambda and generalization to translate english to formal languages </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we compared our systems with the performance results of several alternative systems for which the performance data is available in the literature.
</prevsent>
<prevsent>in particular, we used the performance data given by ge and mooney (2009).<papid> P09-1069 </papid></prevsent>
</prevsection>
<citsent citstr=" W05-0602 ">
the systems that we compared with are: the syn0, syn20 and goldsyn systems by ge and mooney (2009), <papid> P09-1069 </papid>the system scissor by ge and mooney (2005), <papid> W05-0602 </papid>an svm based system krips by kate and mooney (2006), synchronous grammar based system wasp by wong and mooney (2007), <papid> P07-1121 </papid>the ccg based system by zettlemoyer and collins (2007) <papid> D07-1071 </papid>and the work by lu et al (2008).<papid> D08-1082 </papid></citsent>
<aftsection>
<nextsent>please note that many of these approaches require different parsers, human supervision or other additional tools, while our approach requires syntactic parse of the sentences and an initial dictionary.
</nextsent>
<nextsent>our and their reported results for the respective corpora are given in the tables 2 and 3.
</nextsent>
<nextsent>precision recall f-measure inverse+ 93.41 89.04 91.17 inverse 91.12 85.78 88.37 goldsyn 91.94 88.18 90.02 wasp 91.95 86.59 89.19 z&c; 91.63 86.07 88.76 scissor 95.50 77.20 85.38 krisp 93.34 71.70 81.10 lu at al. 89.30 81.50 85.20 table 2: performance on geoquery.
</nextsent>
<nextsent>precision recall f-measure inverse+(i) 87.67 79.08 83.15 inverse+ 85.74 76.63 80.92 goldsyn 84.73 74.00 79.00 syn20 85.37 70.00 76.92 syn0 87.01 67.00 75.71 wasp 88.85 61.93 72.99 krisp 85.20 61.85 71.67 scissor 89.50 73.70 80.80 lu at al. 82.50 67.70 74.40 table 3: performance on clang.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH488">
<title id=" W11-1711.xml">improving a method for quantifying readers impressions of news articles with a regression equation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kumamoto and tanaka (2005) have proposed word co-occurrence-based method for quantifyingreaders?
</prevsent>
<prevsent>impressions of news articles in real numbers.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
however, this method is similar to turneys method (turney, 2002), <papid> P02-1053 </papid>and it is considered to be japanese version of this method in the broad sense.turneys method is one for classifying various genres of written reviews into recommended?</citsent>
<aftsection>
<nextsent>or not recommended.?
</nextsent>
<nextsent>his method extracts phrases with specific patterns from text, and calculates pointwise mutual information pmi(i, excellent?)
</nextsent>
<nextsent>between phrase and the reference word excellent,?
</nextsent>
<nextsent>and pmi(i, poor?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH489">
<title id=" W11-1712.xml">feature selection for sentiment analysis based on content and syntax models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this reduces to finding all the syntactic words in the document and disregarding the entities.
</prevsent>
<prevsent>taking another look at the example modifiers, we might assume that all of the relevant indicators forsa come from specific parts of speech categories such as adjectives and adverbs, while other parts of speech classes such as nouns are more relevant for general text classification, and can be discarded.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
however, as demonstrated by pang et al  (2002)<papid> W02-1011 </papid>pang and lee (2004), <papid> P04-1035 </papid>hu and liu (2004), and riloff et al  (2003), <papid> W03-0404 </papid>there are some nouns and verbs that are useful sentiment indicators as well.</citsent>
<aftsection>
<nextsent>therefore, clear distinction cannot be made along parts of speech categories.to address this issue, we propose feature selection scheme in which we can obtain important sentiment indicators that: 1.
</nextsent>
<nextsent>do not relyon specific parts of speech classes.
</nextsent>
<nextsent>while maintaining the focus on syntax words.
</nextsent>
<nextsent>sentiment while keeping nouns that do.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH494">
<title id=" W11-1712.xml">feature selection for sentiment analysis based on content and syntax models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this reduces to finding all the syntactic words in the document and disregarding the entities.
</prevsent>
<prevsent>taking another look at the example modifiers, we might assume that all of the relevant indicators forsa come from specific parts of speech categories such as adjectives and adverbs, while other parts of speech classes such as nouns are more relevant for general text classification, and can be discarded.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
however, as demonstrated by pang et al  (2002)<papid> W02-1011 </papid>pang and lee (2004), <papid> P04-1035 </papid>hu and liu (2004), and riloff et al  (2003), <papid> W03-0404 </papid>there are some nouns and verbs that are useful sentiment indicators as well.</citsent>
<aftsection>
<nextsent>therefore, clear distinction cannot be made along parts of speech categories.to address this issue, we propose feature selection scheme in which we can obtain important sentiment indicators that: 1.
</nextsent>
<nextsent>do not relyon specific parts of speech classes.
</nextsent>
<nextsent>while maintaining the focus on syntax words.
</nextsent>
<nextsent>sentiment while keeping nouns that do.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH495">
<title id=" W11-1712.xml">feature selection for sentiment analysis based on content and syntax models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this reduces to finding all the syntactic words in the document and disregarding the entities.
</prevsent>
<prevsent>taking another look at the example modifiers, we might assume that all of the relevant indicators forsa come from specific parts of speech categories such as adjectives and adverbs, while other parts of speech classes such as nouns are more relevant for general text classification, and can be discarded.
</prevsent>
</prevsection>
<citsent citstr=" W03-0404 ">
however, as demonstrated by pang et al  (2002)<papid> W02-1011 </papid>pang and lee (2004), <papid> P04-1035 </papid>hu and liu (2004), and riloff et al  (2003), <papid> W03-0404 </papid>there are some nouns and verbs that are useful sentiment indicators as well.</citsent>
<aftsection>
<nextsent>therefore, clear distinction cannot be made along parts of speech categories.to address this issue, we propose feature selection scheme in which we can obtain important sentiment indicators that: 1.
</nextsent>
<nextsent>do not relyon specific parts of speech classes.
</nextsent>
<nextsent>while maintaining the focus on syntax words.
</nextsent>
<nextsent>sentiment while keeping nouns that do.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH499">
<title id=" W11-1712.xml">feature selection for sentiment analysis based on content and syntax models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, this scheme picks upon many entity words that do not contain any subjectivity.
</prevsent>
<prevsent>the most common approach, used by researchers such asdas and chen (2007), starts with manually created lexicon specific to their particular do main whereas others (hurst and nigam, 2004; yi etal., 2003) attempt to craft general-purpose opinion lexicon that can be used across domains.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
more recent lexicon-based approaches (ding et al , 2008; hu and liu, 2004; kim and hovy, 2004; <papid> C04-1200 </papid>riloff et al ., 2003) <papid> W03-0404 </papid>begin with small set of seed?</citsent>
<aftsection>
<nextsent>words and bootstrap this set through synonym detection or various on-line resources to obtain larger lexicon.
</nextsent>
<nextsent>however, lexicon-based approaches have several key difficulties.
</nextsent>
<nextsent>first, they take time to compile.
</nextsent>
<nextsent>whitelaw et al  (2005) report that their feature selection process took 20 person-hours, since it involves work done by human annotators.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH506">
<title id=" W11-1712.xml">feature selection for sentiment analysis based on content and syntax models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>first, they take time to compile.
</prevsent>
<prevsent>whitelaw et al  (2005) report that their feature selection process took 20 person-hours, since it involves work done by human annotators.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
in separate qualitative experiments done by pang et al  (2002)<papid> W02-1011 </papid>97 wilson et al  (2005) <papid> H05-1044 </papid>and kim and hovy (2004), <papid> C04-1200 </papid>the agreement between human judges when given list of sentiment-bearing words is as low as 58% and no higher than 76%.</citsent>
<aftsection>
<nextsent>in addition, some words may not be frequent enough for classification algorithm.
</nextsent>
<nextsent>2.2 topic modelling and hmm-lda.
</nextsent>
<nextsent>topic models such as latent dirichlet allocation (lda) are generative models that allow document sto be explained by set of unobserved (latent) topics.
</nextsent>
<nextsent>hidden markov model lda (hmm-lda)(griffiths et al , 2005) is topic model that simultaneously models topics and syntactic structure in collection of documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH524">
<title id=" W11-1008.xml">reestimation of reified rules in semi ring parsing and biparsing </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>to further generalize the building of the hypergraph the parsing algorithma deductive system can be used.
</prevsent>
<prevsent>by defining hand-full of deductive rules that describe how items can be constructed, the full complexities of parsing algorithm can be very succinctly summarized.
</prevsent>
</prevsection>
<citsent citstr=" J99-4004 ">
deductive systems to represent parsers and semi rings to calculate the desired values for the parses were introduced in goodman (1999).<papid> J99-4004 </papid></citsent>
<aftsection>
<nextsent>in this paper we will reify the grammar rules by moving them from the meta level to the object level effectively making them first-class citizens ofthe parse trees, which are no longer weighted hypergraphs, but mul/add-graphs.
</nextsent>
<nextsent>this move allows usto calculate rule expectations for expectation maximization (dempster et al, 1977) as part of the parsing process, which significantly shortens turn-overtime for experimenting with different grammar formalisms.
</nextsent>
<nextsent>another approach which achieve similar goal is to use expectation semi ring (eisner, 2001; eisner,2002; <papid> P02-1001 </papid>li and eisner, 2009).<papid> D09-1005 </papid></nextsent>
<nextsent>in this semi ring, all values are pairs of probabilities and expectations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH525">
<title id=" W11-1008.xml">reestimation of reified rules in semi ring parsing and biparsing </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper we will reify the grammar rules by moving them from the meta level to the object level effectively making them first-class citizens ofthe parse trees, which are no longer weighted hypergraphs, but mul/add-graphs.
</prevsent>
<prevsent>this move allows usto calculate rule expectations for expectation maximization (dempster et al, 1977) as part of the parsing process, which significantly shortens turn-overtime for experimenting with different grammar formalisms.
</prevsent>
</prevsection>
<citsent citstr=" P02-1001 ">
another approach which achieve similar goal is to use expectation semi ring (eisner, 2001; eisner,2002; <papid> P02-1001 </papid>li and eisner, 2009).<papid> D09-1005 </papid></citsent>
<aftsection>
<nextsent>in this semi ring, all values are pairs of probabilities and expectations.
</nextsent>
<nextsent>theinside-outside algorithm with the expectation semi ring requires the usual inside and outside calculations over the probability part of the semi ring values, followed by third traversal over the parse forest to populate the expectation part of the semiringvalues.
</nextsent>
<nextsent>the approach taken in this paper also requires the usual inside and outside calculations, buto third traversal of the parse forest.
</nextsent>
<nextsent>instead, the proposed approach requires two passes over the rules of the grammar per em iteration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH526">
<title id=" W11-1008.xml">reestimation of reified rules in semi ring parsing and biparsing </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper we will reify the grammar rules by moving them from the meta level to the object level effectively making them first-class citizens ofthe parse trees, which are no longer weighted hypergraphs, but mul/add-graphs.
</prevsent>
<prevsent>this move allows usto calculate rule expectations for expectation maximization (dempster et al, 1977) as part of the parsing process, which significantly shortens turn-overtime for experimenting with different grammar formalisms.
</prevsent>
</prevsection>
<citsent citstr=" D09-1005 ">
another approach which achieve similar goal is to use expectation semi ring (eisner, 2001; eisner,2002; <papid> P02-1001 </papid>li and eisner, 2009).<papid> D09-1005 </papid></citsent>
<aftsection>
<nextsent>in this semi ring, all values are pairs of probabilities and expectations.
</nextsent>
<nextsent>theinside-outside algorithm with the expectation semi ring requires the usual inside and outside calculations over the probability part of the semi ring values, followed by third traversal over the parse forest to populate the expectation part of the semiringvalues.
</nextsent>
<nextsent>the approach taken in this paper also requires the usual inside and outside calculations, buto third traversal of the parse forest.
</nextsent>
<nextsent>instead, the proposed approach requires two passes over the rules of the grammar per em iteration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH529">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>traditionally, this was tackled as concept-to-text realization problem.
</prevsent>
<prevsent>however, such needs apply sometimes to cases where new text should be derived from some existing texts, an instance of text-to-textgeneration.
</prevsent>
</prevsection>
<citsent citstr=" P09-1094 ">
the general idea is not anymore to produce text from data, but to transform text so as to ensure that it has desirable properties appropriate for some intended application (zhao et al, 2009).<papid> P09-1094 </papid></citsent>
<aftsection>
<nextsent>for example, one may want text to be shorter (cohnand lapata, 2008), <papid> C08-1018 </papid>tailored to some reader profile (zhu et al, 2010), <papid> C10-1152 </papid>compliant with some specific norms (max, 2004), or more adapted for subsequent machine processing tasks (chandrasekar et al., 1996).<papid> C96-2183 </papid></nextsent>
<nextsent>the generation process must produce text having meaning which is compatible withthe definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraphrasing for text simplification), while ensuring that it remains grammatically correct.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH530">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, such needs apply sometimes to cases where new text should be derived from some existing texts, an instance of text-to-textgeneration.
</prevsent>
<prevsent>the general idea is not anymore to produce text from data, but to transform text so as to ensure that it has desirable properties appropriate for some intended application (zhao et al, 2009).<papid> P09-1094 </papid></prevsent>
</prevsection>
<citsent citstr=" C08-1018 ">
for example, one may want text to be shorter (cohnand lapata, 2008), <papid> C08-1018 </papid>tailored to some reader profile (zhu et al, 2010), <papid> C10-1152 </papid>compliant with some specific norms (max, 2004), or more adapted for subsequent machine processing tasks (chandrasekar et al., 1996).<papid> C96-2183 </papid></citsent>
<aftsection>
<nextsent>the generation process must produce text having meaning which is compatible withthe definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraphrasing for text simplification), while ensuring that it remains grammatically correct.
</nextsent>
<nextsent>its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from one text to another is very dependent on the rewriting context.
</nextsent>
<nextsent>the wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (madnani and dorr, 2010), <papid> J10-3003 </papid>the inherent polysemy of such linguistic units andthe pragmatic constraints on their uses make it im possible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at lexical level (zhao et al., 2007).</nextsent>
<nextsent>hence, automatic contextual validation of candidate rewritings is fundamental issue for text paraphrasing with phrasal units.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH531">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, such needs apply sometimes to cases where new text should be derived from some existing texts, an instance of text-to-textgeneration.
</prevsent>
<prevsent>the general idea is not anymore to produce text from data, but to transform text so as to ensure that it has desirable properties appropriate for some intended application (zhao et al, 2009).<papid> P09-1094 </papid></prevsent>
</prevsection>
<citsent citstr=" C10-1152 ">
for example, one may want text to be shorter (cohnand lapata, 2008), <papid> C08-1018 </papid>tailored to some reader profile (zhu et al, 2010), <papid> C10-1152 </papid>compliant with some specific norms (max, 2004), or more adapted for subsequent machine processing tasks (chandrasekar et al., 1996).<papid> C96-2183 </papid></citsent>
<aftsection>
<nextsent>the generation process must produce text having meaning which is compatible withthe definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraphrasing for text simplification), while ensuring that it remains grammatically correct.
</nextsent>
<nextsent>its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from one text to another is very dependent on the rewriting context.
</nextsent>
<nextsent>the wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (madnani and dorr, 2010), <papid> J10-3003 </papid>the inherent polysemy of such linguistic units andthe pragmatic constraints on their uses make it im possible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at lexical level (zhao et al., 2007).</nextsent>
<nextsent>hence, automatic contextual validation of candidate rewritings is fundamental issue for text paraphrasing with phrasal units.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH532">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, such needs apply sometimes to cases where new text should be derived from some existing texts, an instance of text-to-textgeneration.
</prevsent>
<prevsent>the general idea is not anymore to produce text from data, but to transform text so as to ensure that it has desirable properties appropriate for some intended application (zhao et al, 2009).<papid> P09-1094 </papid></prevsent>
</prevsection>
<citsent citstr=" C96-2183 ">
for example, one may want text to be shorter (cohnand lapata, 2008), <papid> C08-1018 </papid>tailored to some reader profile (zhu et al, 2010), <papid> C10-1152 </papid>compliant with some specific norms (max, 2004), or more adapted for subsequent machine processing tasks (chandrasekar et al., 1996).<papid> C96-2183 </papid></citsent>
<aftsection>
<nextsent>the generation process must produce text having meaning which is compatible withthe definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraphrasing for text simplification), while ensuring that it remains grammatically correct.
</nextsent>
<nextsent>its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from one text to another is very dependent on the rewriting context.
</nextsent>
<nextsent>the wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (madnani and dorr, 2010), <papid> J10-3003 </papid>the inherent polysemy of such linguistic units andthe pragmatic constraints on their uses make it im possible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at lexical level (zhao et al., 2007).</nextsent>
<nextsent>hence, automatic contextual validation of candidate rewritings is fundamental issue for text paraphrasing with phrasal units.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH533">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the generation process must produce text having meaning which is compatible withthe definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraphrasing for text simplification), while ensuring that it remains grammatically correct.
</prevsent>
<prevsent>its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from one text to another is very dependent on the rewriting context.
</prevsent>
</prevsection>
<citsent citstr=" J10-3003 ">
the wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (madnani and dorr, 2010), <papid> J10-3003 </papid>the inherent polysemy of such linguistic units andthe pragmatic constraints on their uses make it im possible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at lexical level (zhao et al., 2007).</citsent>
<aftsection>
<nextsent>hence, automatic contextual validation of candidate rewritings is fundamental issue for text paraphrasing with phrasal units.
</nextsent>
<nextsent>in this article, we tackle the problem of what we call targeted paraphrasing, defined as the rewriting of subpart of sentence, as in e.g.
</nextsent>
<nextsent>(resnik et al,2010) <papid> D10-1013 </papid>where it is applied to making parts of sentences easier to translate automatically.</nextsent>
<nextsent>while this problem is simpler than full sentence rewriting, its study is justified as it should be handled correctly for the more complex task to be successful.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH534">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, automatic contextual validation of candidate rewritings is fundamental issue for text paraphrasing with phrasal units.
</prevsent>
<prevsent>in this article, we tackle the problem of what we call targeted paraphrasing, defined as the rewriting of subpart of sentence, as in e.g.
</prevsent>
</prevsection>
<citsent citstr=" D10-1013 ">
(resnik et al,2010) <papid> D10-1013 </papid>where it is applied to making parts of sentences easier to translate automatically.</citsent>
<aftsection>
<nextsent>while this problem is simpler than full sentence rewriting, its study is justified as it should be handled correctly for the more complex task to be successful.
</nextsent>
<nextsent>more over, being simpler, it offers evaluation scenarios which make the performance on the task easier toassess.
</nextsent>
<nextsent>our particular experiments here aim to assist wikipedia contributor in revising text to im prove its quality.
</nextsent>
<nextsent>for this, we use collection of phrases that have been rewritten in wikipedia, and test the substitutability of paraphrases coming from repertoire of sub-sentential paraphrases acquired 10from different sources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH536">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> targeted paraphrasing for text revision.  </section>
<citcontext>
<prevsection>
<prevsent>in thework by resnik et al (2010), <papid> D10-1013 </papid>rephrasings for specific phrases are acquired through crowdsourcing.</prevsent>
<prevsent>difficult-to-translate phrases in the source text are first identified, and monolingual contributors are asked to provide rephrasings in context.</prevsent>
</prevsection>
<citsent citstr=" E09-1082 ">
collected rephrasings can then be used as input for machine translation system, which can positively exploit the increased variety in expression to produce more confident translations for better estimated source units (schroeder et al, 2009).<papid> E09-1082 </papid>2 for instance, the phrase in bold in the sentence the number of people known to have died has now reached 358 can be rewritten as 1) who died, 2) identified to have died and 3) known to have passed away.</citsent>
<aftsection>
<nextsent>all such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey meaning which is reasonably close to the original wording.
</nextsent>
<nextsent>the task of rewriting complete sentences has also been addressed in various works (e.g.
</nextsent>
<nextsent>(barzilay and lee, 2003; <papid> N03-1003 </papid>quirk et al, 2004; <papid> W04-3219 </papid>zhao et al, 2010)).<papid> C10-1149 </papid></nextsent>
<nextsent>itposes, however, numerous other challenges, in particular regarding how it could be correctly evaluated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH537">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> targeted paraphrasing for text revision.  </section>
<citcontext>
<prevsection>
<prevsent>all such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey meaning which is reasonably close to the original wording.
</prevsent>
<prevsent>the task of rewriting complete sentences has also been addressed in various works (e.g.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
(barzilay and lee, 2003; <papid> N03-1003 </papid>quirk et al, 2004; <papid> W04-3219 </papid>zhao et al, 2010)).<papid> C10-1149 </papid></citsent>
<aftsection>
<nextsent>itposes, however, numerous other challenges, in particular regarding how it could be correctly evaluated.
</nextsent>
<nextsent>human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality.
</nextsent>
<nextsent>using sentential paraphrases to support given task (e.g. providing alternative reference translations for optimizing statistical machine translation systems (madnani et al, 2008)) 2it is to be noted that, in the scenario presented in (resnik et al., 2010), <papid> D10-1013 </papid>monolingual contributors cannot predict how useful their rewritings will be to the underlying machine translation engine used.</nextsent>
<nextsent>11 can be seen as proxy for extrinsic evaluation of the quality of paraphrases, but it is not clear from published results that improvements on the task are clearly correlated with the quality of the produced paraphrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH539">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> targeted paraphrasing for text revision.  </section>
<citcontext>
<prevsection>
<prevsent>all such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey meaning which is reasonably close to the original wording.
</prevsent>
<prevsent>the task of rewriting complete sentences has also been addressed in various works (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
(barzilay and lee, 2003; <papid> N03-1003 </papid>quirk et al, 2004; <papid> W04-3219 </papid>zhao et al, 2010)).<papid> C10-1149 </papid></citsent>
<aftsection>
<nextsent>itposes, however, numerous other challenges, in particular regarding how it could be correctly evaluated.
</nextsent>
<nextsent>human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality.
</nextsent>
<nextsent>using sentential paraphrases to support given task (e.g. providing alternative reference translations for optimizing statistical machine translation systems (madnani et al, 2008)) 2it is to be noted that, in the scenario presented in (resnik et al., 2010), <papid> D10-1013 </papid>monolingual contributors cannot predict how useful their rewritings will be to the underlying machine translation engine used.</nextsent>
<nextsent>11 can be seen as proxy for extrinsic evaluation of the quality of paraphrases, but it is not clear from published results that improvements on the task are clearly correlated with the quality of the produced paraphrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH540">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> targeted paraphrasing for text revision.  </section>
<citcontext>
<prevsection>
<prevsent>all such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey meaning which is reasonably close to the original wording.
</prevsent>
<prevsent>the task of rewriting complete sentences has also been addressed in various works (e.g.
</prevsent>
</prevsection>
<citsent citstr=" C10-1149 ">
(barzilay and lee, 2003; <papid> N03-1003 </papid>quirk et al, 2004; <papid> W04-3219 </papid>zhao et al, 2010)).<papid> C10-1149 </papid></citsent>
<aftsection>
<nextsent>itposes, however, numerous other challenges, in particular regarding how it could be correctly evaluated.
</nextsent>
<nextsent>human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality.
</nextsent>
<nextsent>using sentential paraphrases to support given task (e.g. providing alternative reference translations for optimizing statistical machine translation systems (madnani et al, 2008)) 2it is to be noted that, in the scenario presented in (resnik et al., 2010), <papid> D10-1013 </papid>monolingual contributors cannot predict how useful their rewritings will be to the underlying machine translation engine used.</nextsent>
<nextsent>11 can be seen as proxy for extrinsic evaluation of the quality of paraphrases, but it is not clear from published results that improvements on the task are clearly correlated with the quality of the produced paraphrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH542">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> targeted paraphrasing for text revision.  </section>
<citcontext>
<prevsection>
<prevsent>11 can be seen as proxy for extrinsic evaluation of the quality of paraphrases, but it is not clear from published results that improvements on the task are clearly correlated with the quality of the produced paraphrases.
</prevsent>
<prevsent>lastly, automatic metrics have been proposed for evaluating the grammaticality of sentences (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P07-1044 ">
(mutton et al, 2007)).<papid> P07-1044 </papid></citsent>
<aftsection>
<nextsent>automatic evaluation of sentential paraphrases has not produced any consensual results so far, as they do not integratetask-specific considerations and can be strongly biased towards some paraphrasing techniques.
</nextsent>
<nextsent>in this work, we tackle the comparatively more modest task of sub-sentential paraphrasing applied to text revision.
</nextsent>
<nextsent>in order to use an unbiased task, we use corpus of naturally-occurring rewritings from an authoring memory of wikipedia articles.
</nextsent>
<nextsent>we use the wicopaco corpus (max and wisniewski, 2010), collection of local rephrasings from the edit history of wikipedia which contains instances of lexical, syntactical and semantic rephrasings (dutrey et al, 2011), the latter type being illustrated by the following example: ce vers de nuit rhenane dapollinaire [qui parat presque sans structure rythmique?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH544">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> automatic sub-sentential paraphrase.  </section>
<citcontext>
<prevsection>
<prevsent>the techniques proposed have strong relationship to the type of text corpus used 3this verse from apollinaires nuit rhenane [which seems almost without rhythmic structure ? whose cesura is as if hidden].
</prevsent>
<prevsent>for acquisition, mainly: ? pairs of sentential paraphrases (monolingual parallel corpora) allow for good precision but evidently low recall (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
(barzilay and mckeown, 2001; <papid> P01-1008 </papid>pang et al, 2003; <papid> N03-1024 </papid>cohn et al., 2008; <papid> J08-4005 </papid>bouamor et al, 2011)) ? <papid> P11-2069 </papid>pairs of bilingual sentences (bilingual parallelcorpora) allow for comparatively better recall (e.g.</citsent>
<aftsection>
<nextsent>(bannard and callison-burch, 2005; kok and brockett, 2010))?<papid> N10-1017 </papid></nextsent>
<nextsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH545">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> automatic sub-sentential paraphrase.  </section>
<citcontext>
<prevsection>
<prevsent>the techniques proposed have strong relationship to the type of text corpus used 3this verse from apollinaires nuit rhenane [which seems almost without rhythmic structure ? whose cesura is as if hidden].
</prevsent>
<prevsent>for acquisition, mainly: ? pairs of sentential paraphrases (monolingual parallel corpora) allow for good precision but evidently low recall (e.g.
</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
(barzilay and mckeown, 2001; <papid> P01-1008 </papid>pang et al, 2003; <papid> N03-1024 </papid>cohn et al., 2008; <papid> J08-4005 </papid>bouamor et al, 2011)) ? <papid> P11-2069 </papid>pairs of bilingual sentences (bilingual parallelcorpora) allow for comparatively better recall (e.g.</citsent>
<aftsection>
<nextsent>(bannard and callison-burch, 2005; kok and brockett, 2010))?<papid> N10-1017 </papid></nextsent>
<nextsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH546">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> automatic sub-sentential paraphrase.  </section>
<citcontext>
<prevsection>
<prevsent>the techniques proposed have strong relationship to the type of text corpus used 3this verse from apollinaires nuit rhenane [which seems almost without rhythmic structure ? whose cesura is as if hidden].
</prevsent>
<prevsent>for acquisition, mainly: ? pairs of sentential paraphrases (monolingual parallel corpora) allow for good precision but evidently low recall (e.g.
</prevsent>
</prevsection>
<citsent citstr=" J08-4005 ">
(barzilay and mckeown, 2001; <papid> P01-1008 </papid>pang et al, 2003; <papid> N03-1024 </papid>cohn et al., 2008; <papid> J08-4005 </papid>bouamor et al, 2011)) ? <papid> P11-2069 </papid>pairs of bilingual sentences (bilingual parallelcorpora) allow for comparatively better recall (e.g.</citsent>
<aftsection>
<nextsent>(bannard and callison-burch, 2005; kok and brockett, 2010))?<papid> N10-1017 </papid></nextsent>
<nextsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH547">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> automatic sub-sentential paraphrase.  </section>
<citcontext>
<prevsection>
<prevsent>the techniques proposed have strong relationship to the type of text corpus used 3this verse from apollinaires nuit rhenane [which seems almost without rhythmic structure ? whose cesura is as if hidden].
</prevsent>
<prevsent>for acquisition, mainly: ? pairs of sentential paraphrases (monolingual parallel corpora) allow for good precision but evidently low recall (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P11-2069 ">
(barzilay and mckeown, 2001; <papid> P01-1008 </papid>pang et al, 2003; <papid> N03-1024 </papid>cohn et al., 2008; <papid> J08-4005 </papid>bouamor et al, 2011)) ? <papid> P11-2069 </papid>pairs of bilingual sentences (bilingual parallelcorpora) allow for comparatively better recall (e.g.</citsent>
<aftsection>
<nextsent>(bannard and callison-burch, 2005; kok and brockett, 2010))?<papid> N10-1017 </papid></nextsent>
<nextsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH548">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> automatic sub-sentential paraphrase.  </section>
<citcontext>
<prevsection>
<prevsent>for acquisition, mainly: ? pairs of sentential paraphrases (monolingual parallel corpora) allow for good precision but evidently low recall (e.g.
</prevsent>
<prevsent>(barzilay and mckeown, 2001; <papid> P01-1008 </papid>pang et al, 2003; <papid> N03-1024 </papid>cohn et al., 2008; <papid> J08-4005 </papid>bouamor et al, 2011)) ? <papid> P11-2069 </papid>pairs of bilingual sentences (bilingual parallelcorpora) allow for comparatively better recall (e.g.</prevsent>
</prevsection>
<citsent citstr=" N10-1017 ">
(bannard and callison-burch, 2005; kok and brockett, 2010))?<papid> N10-1017 </papid></citsent>
<aftsection>
<nextsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.
</nextsent>
<nextsent>(barzilay and lee, 2003; <papid> N03-1003 </papid>li et al, 2005; <papid> I05-5007 </papid>bhagat and ravichandran, 2008; <papid> P08-1077 </papid>deleger and zweigenbaum, 2009) although the precision of such techniques can insome cases be formulated with regards to predefined reference set (cohn et al, 2008), <papid> J08-4005 </papid>it should more generally be assessed in the specific context of some use of the paraphrase pair.</nextsent>
<nextsent>this refers tothe problem of substituability in context (e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH551">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> automatic sub-sentential paraphrase.  </section>
<citcontext>
<prevsection>
<prevsent>(bannard and callison-burch, 2005; kok and brockett, 2010))?<papid> N10-1017 </papid></prevsent>
<prevsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</prevsent>
</prevsection>
<citsent citstr=" I05-5007 ">
(barzilay and lee, 2003; <papid> N03-1003 </papid>li et al, 2005; <papid> I05-5007 </papid>bhagat and ravichandran, 2008; <papid> P08-1077 </papid>deleger and zweigenbaum, 2009) although the precision of such techniques can insome cases be formulated with regards to predefined reference set (cohn et al, 2008), <papid> J08-4005 </papid>it should more generally be assessed in the specific context of some use of the paraphrase pair.</citsent>
<aftsection>
<nextsent>this refers tothe problem of substituability in context (e.g.
</nextsent>
<nextsent>(con nor and roth, 2007; zhao et al, 2007)), which is well studied field at the lexical level and the object of evaluation campains (mccarthy and navigli, 2009).
</nextsent>
<nextsent>contextual phrase substitution poses the additional challenge that phrases are rarer than words, so that building contextual and grammatical models to ensure that the generated rephrasings are both semantically compatible and grammatical is more complicated (e.g.
</nextsent>
<nextsent>(callison-burch, 2008)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH552">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> automatic sub-sentential paraphrase.  </section>
<citcontext>
<prevsection>
<prevsent>(bannard and callison-burch, 2005; kok and brockett, 2010))?<papid> N10-1017 </papid></prevsent>
<prevsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</prevsent>
</prevsection>
<citsent citstr=" P08-1077 ">
(barzilay and lee, 2003; <papid> N03-1003 </papid>li et al, 2005; <papid> I05-5007 </papid>bhagat and ravichandran, 2008; <papid> P08-1077 </papid>deleger and zweigenbaum, 2009) although the precision of such techniques can insome cases be formulated with regards to predefined reference set (cohn et al, 2008), <papid> J08-4005 </papid>it should more generally be assessed in the specific context of some use of the paraphrase pair.</citsent>
<aftsection>
<nextsent>this refers tothe problem of substituability in context (e.g.
</nextsent>
<nextsent>(con nor and roth, 2007; zhao et al, 2007)), which is well studied field at the lexical level and the object of evaluation campains (mccarthy and navigli, 2009).
</nextsent>
<nextsent>contextual phrase substitution poses the additional challenge that phrases are rarer than words, so that building contextual and grammatical models to ensure that the generated rephrasings are both semantically compatible and grammatical is more complicated (e.g.
</nextsent>
<nextsent>(callison-burch, 2008)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH555">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> web-based contextual validation.  </section>
<citcontext>
<prevsection>
<prevsent>the specific nature of the text units that we are dealing with calls for careful treatment: in the general scenario, it is unlikely that any supervised corpus would contain enough information for appropriate modeling of the substituability in context decision.
</prevsent>
<prevsent>it is therefore tempting to consider using the web as the largest available information source,in spite of several of its known limitations, including that data can be of varying quality.
</prevsent>
</prevsection>
<citsent citstr=" I05-5001 ">
it has however been shown that large range of nlp applications can be improved by exploiting n-gram counts from the web (using web document counts as proxy) (lapata and keller, 2005).paraphrase identification has been addressed previously, both using features computed from an offline corpus (brockett and dolan, 2005) <papid> I05-5001 </papid>and features computed from web queries (zhao et al, 2007).</citsent>
<aftsection>
<nextsent>however, to our knowledge previous work exploiting information from the web was limited to the identification of lexical paraphrases.
</nextsent>
<nextsent>although the probability of finding phrase occurrences significantly increases by considering the web, some phrases are still very rare or not present in search engine indexes.
</nextsent>
<nextsent>as in (brockett and dolan, 2005), <papid> I05-5001 </papid>we tackle our paraphrase identification task as one of monolingualclassification.</nextsent>
<nextsent>more precisely, considering an original phrase within the context of sentence s, we seek to determine whether candidate paraphrase p?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH557">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> web-based contextual validation.  </section>
<citcontext>
<prevsection>
<prevsent>hedit = ter(lemorig, lempara) (1)note that this model is not derived from information from the web, in contrast to all the models described next.language model score the likelihood of sentence can be good indicator of its grammatical ity (mutton, 2006).
</prevsent>
<prevsent>language model probabilities can now be obtained from web counts.
</prevsent>
</prevsection>
<citsent citstr=" N10-2012 ">
in our experiments, we used the microsoft web n-gram ser vice6 for research (wang et al, 2010) <papid> N10-2012 </papid>to obtain log likelihood scores for text units.7 however, this scoreis certainly not sufficient as it does not take the original wording into account.</citsent>
<aftsection>
<nextsent>we therefore used ratioof the language model score of the paraphrased sentence with the language model score of the original 6http://research.microsoft.com/en-us/ collaboration/focus/cs/web-ngram.aspx7note that in order to query on french text, we had to remove all diacritics for the service to behave correctly, independently of encodings: careful examination of ranked hypotheses showed that this trick allowed us to obtain results coherent with expectations.
</nextsent>
<nextsent>sentence, after normalization by sentence length of the language model scores (onishi et al, 2010): <papid> P10-2001 </papid>hlm ratio = lm(para) lm(orig) = lm(para)1/length(para) lm(orig)1/length(orig) (2) contextless thematic model scores cooccurring words are used in distributional semantics to account for common meanings of words.</nextsent>
<nextsent>we build vector representations of cooccurrences for both the original phrase and its paraphrase p?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH558">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> web-based contextual validation.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, we used the microsoft web n-gram ser vice6 for research (wang et al, 2010) <papid> N10-2012 </papid>to obtain log likelihood scores for text units.7 however, this scoreis certainly not sufficient as it does not take the original wording into account.</prevsent>
<prevsent>we therefore used ratioof the language model score of the paraphrased sentence with the language model score of the original 6http://research.microsoft.com/en-us/ collaboration/focus/cs/web-ngram.aspx7note that in order to query on french text, we had to remove all diacritics for the service to behave correctly, independently of encodings: careful examination of ranked hypotheses showed that this trick allowed us to obtain results coherent with expectations.</prevsent>
</prevsection>
<citsent citstr=" P10-2001 ">
sentence, after normalization by sentence length of the language model scores (onishi et al, 2010): <papid> P10-2001 </papid>hlm ratio = lm(para) lm(orig) = lm(para)1/length(para) lm(orig)1/length(orig) (2) contextless thematic model scores cooccurring words are used in distributional semantics to account for common meanings of words.</citsent>
<aftsection>
<nextsent>we build vector representations of cooccurrences for both the original phrase and its paraphrase p?.
</nextsent>
<nextsent>our contextless thematic model is built in the following fashion: wequery search engine to retrieve the top document snippets for phrase p. we then count frequencies for all content words in these snippets, and keep the set of words appearing more than fraction of . we then build vector (thematic profile) of dimension |w | where values are computed by the following formula: tnocontorig [w] = count(p, w) count(p) (3) 14where count(x) correspond to the number of documents containing given exact phrase or word according to the search engine used and count(x, y) correspond to the number of documents containing simultaneously both.
</nextsent>
<nextsent>we then compute the same thematic profile for the paraphrase p?, using only the subset of words : tnocontpara [w] = count(p?, w) count(p) (4) finally, we compute similarity between the two profiles by taking the cosinus between their two vec tors: hnocontthem = tnocontorig ? nocont para ||tnocontorig || ? ||t nocont para || (5) in all our experiments, we used the yahoo!
</nextsent>
<nextsent>search boss8 web service for obtaining web counts and retrieving snippets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH559">
<title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the second one, boundlm, considers sentence as paraphrase whenever the counts for the bigrams crossing the left and right boundary of the sub-sentential paraphrase is higher than 10.
</prevsent>
<prevsent>syntactic dependency baseline when rewriting asubpart of sentence, the fact that syntactic dependencies between the rewritten phrase and its context are the same than those of the original phrase and the same context can provide some information 16 about the grammatical and semantic substituability of the two phrases (zhao et al, 2007; max and zock, 2008).
</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
we thus build syntactic dependencies for both the original and rewritten sentence, using the french version (candito et al, 2010) of the berkeley probabilistic parser (petrov and klein, 2007), <papid> N07-1051 </papid>and consider the subset of dependencies for the two sentences that exist between word inside the phrase under focus and word outside it (deporig anddeppara).</citsent>
<aftsection>
<nextsent>our contdep baseline considers sentence as paraphrase iff deppara = deporig.
</nextsent>
<nextsent>5.3 evaluation results.
</nextsent>
<nextsent>we used the models described in section 4 to build svm classifier using the libsvm package (chang and lin, 2001).
</nextsent>
<nextsent>accuracy results are reported on figure 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH560">
<title id=" W11-0805.xml">detecting multiword expressions improves word sense disambiguation </title>
<section> mwe detection algorithms by arranz.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W04-0823 ">
arranz et al describe their talp word sense disambiguation system in (castillo et al, 2004) <papid> W04-0823 </papid>and(arranz et al, 2005).</citsent>
<aftsection>
<nextsent>the details of the wsd procedure are not critical here; what is important is that their preprocessing system attempted to detect mwes that could later be disambiguated by the wsd algorithm.
</nextsent>
<nextsent>this preprocessing occurred as apipeline that tokenized the text, assigned part-of speech tag, and finally determined lemma for each stemmable word.
</nextsent>
<nextsent>this information was then passed to mwe candidate identifier3 whose output was then filtered by an mwe selector.
</nextsent>
<nextsent>the resulting list of mwes, along with all remaining tokens, were then passed into the wsd algorithm for disambiguation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH561">
<title id=" W11-0805.xml">detecting multiword expressions improves word sense disambiguation </title>
<section> extension of results.  </section>
<citcontext>
<prevsection>
<prevsent>second, for those wishing to further this work, or build upon it, the most frequent-sense strategy is easily implemented.we used jsemcor (finlayson, 2008a) to interface with the semcor data files.
</prevsent>
<prevsent>we used wordnet version 1.6 with the original version of sem cor4.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
each token in each sentence in the brown1 and brown2 concordances of semcor was assigned part of speech tag calculated using the stanford java nlp library (toutanova et al, 2003), <papid> N03-1033 </papid>as well as aset of lemmas calculated using the mit java wordnet interface (finlayson, 2008b).</citsent>
<aftsection>
<nextsent>this data was the input to each mwe detection strategy.there was one major difference between our detector implementations and arranz, stemming from major difference between xwn and semcor: semcor contains large number of proper nouns,whereas xwn glosses contain almost none.
</nextsent>
<nextsent>therefore our detector implementations included simple proper noun mwe detector, which marked all unbroken runs of tokens tagged as proper nouns as proper noun mwe.
</nextsent>
<nextsent>this proper noun detector was run first, before the baseline and best detectors, and the proper noun mwes detected took precedence over the mwes detected in later stages.baseline mwe detection this mwe detection strategy was called none/longest-match-left 4the latest version of wordnet is 3.0, but semcor has notbeen manually updated for wordnet versions later than 1.6.
</nextsent>
<nextsent>automatically updated versions of semcor are available, but they contain numerous errors resulting from deleted sense entries, and the sense assignments and multi-word identifications have not been adjusted to take into account new entries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH562">
<title id=" W11-0601.xml">testing the robustness of online word segmentation effects of linguistic diversity and phonetic variation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>speech contains very few explicit boundaries between linguistic units: silent pauses often mark utterance boundaries, but boundaries between smaller units (e.g. words) are absent most of the time.
</prevsent>
<prevsent>procedures by which infants could develop word segmentation strategies have been discussed at length, from both psycho linguistic and computational point of view.
</prevsent>
</prevsection>
<citsent citstr=" J01-3002 ">
many models relying on statistical information have been proposed, and some of them exhibit satisfactory performance: mbdp-1 (brent,1999), ngs-u (venkataraman, 2001) <papid> J01-3002 </papid>and dp (goldwater, griffiths and johnson, 2009) can be considered state-of-the-art.</citsent>
<aftsection>
<nextsent>though there is evidence that prosodic, phonotactic and co articulation cues may count more than statistics (johnson and jusczyk, 2001), it is still matter of interest to know how much can be learned without linguistic cues.
</nextsent>
<nextsent>to usevenkataramans words, we are interested in the performance of bare-bones statistical models.?
</nextsent>
<nextsent>the aforementioned computational simulations have two major downsides.
</nextsent>
<nextsent>first, all models of language acquisition should generalize to typo logically different languages; however, the word segmentation experiments mentioned above have never been carried out on phonemic ally transcribed, child directed speech in languages other than english.second, these experiments use phonemic ally transcribed corpora as the input and, as such, make the implicit simplifying assumption that, when children learn to segment speech into words, they have already learned phonological rules and know how to reduce the inherent variability in speech to finite (and rather small) number of abstract categories: the phonemes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH563">
<title id=" W11-0601.xml">testing the robustness of online word segmentation effects of linguistic diversity and phonetic variation </title>
<section> robustness benchmark.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 word segmentation models.
</prevsent>
<prevsent>the segmentation task can be summarized as fol lows: given corpus of utterances in which word boundaries have been deleted, the model has to put them back.
</prevsent>
</prevsection>
<citsent citstr=" W04-1307 ">
though we did not challenge the usual idealization that children are able to segment speech into discrete, phoneme-sized units, modeling language acquisition imposes significant constraints on the models (brent, 1999; gambell and yang, 2004):<papid> W04-1307 </papid>they must generalize to different (if not all) languages, start without any knowledge specific to aparticular language, learn in an unsupervised manner and, most importantly, operate incrementally.</citsent>
<aftsection>
<nextsent>online learning is sound desideratum for any model of language acquisition: indeed, human language-processors do not wait, in brents words, until the corpus of all utterances they will ever hear becomes available?.
</nextsent>
<nextsent>therefore, we favored aninfant-plausible?
</nextsent>
<nextsent>setting and only considered on line word segmentation models, namely mbdp-1 (brent, 1999) and ngs-u (venkataraman, 2001).<papid> J01-3002 </papid></nextsent>
<nextsent>even if dp (goldwater et al , 2009) was shown to be more flexible than both mbdp-1 and ngs-u, we did not include goldwater et al batch model, nor recent online variants by pearl et al  (in press), in the benchmark.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH568">
<title id=" W11-0601.xml">testing the robustness of online word segmentation effects of linguistic diversity and phonetic variation </title>
<section> robustness benchmark.  </section>
<citcontext>
<prevsection>
<prevsent>foreach sample, elementary textual statistics are presented in table 1.
</prevsent>
<prevsent>the english corpus contains 9790 utterances from the bernstein ratner corpus that were automatically transcribed and manually corrected by brent and cartwright (1996).
</prevsent>
</prevsection>
<citsent citstr=" P08-1016 ">
it has been used in many word segmentation experiments (brent, 1999; venkataraman, 2001; <papid> J01-3002 </papid>batchelder, 2002; fleck, 2008; <papid> P08-1016 </papid>goldwater et al , 2009; among others) and can be considered de facto standard.</citsent>
<aftsection>
<nextsent>the french and the japanese corpora were both made by le calvez (2007), the former by automatically transcribing the champaud, leveille?
</nextsent>
<nextsent>and ron dal corpora, the latter by automatically transcribing the ishii and noji corpora from romaji to phonemes.
</nextsent>
<nextsent>to get samples comparable in size to the english corpus, 10,000 utterances were selected at random in each of le calvezs corpora.
</nextsent>
<nextsent>all transcription choices made by the authors in terms of phonemic inventory and word segmentation were respected.1 2.3 variation sources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH569">
<title id=" W11-0601.xml">testing the robustness of online word segmentation effects of linguistic diversity and phonetic variation </title>
<section> robustness benchmark.  </section>
<citcontext>
<prevsection>
<prevsent>we refer tothis quantity, similar to type-token ratio, as the corporas lexical complexity.
</prevsent>
<prevsent>as allophonic variation is context-dependent, the increase in lexical complexity is, in this condition, limited by the phonotactic constraints of the language: the fewer contexts phoneme appears in, the fewer contextual al lophones it can have.
</prevsent>
</prevsection>
<citsent citstr=" W08-2109 ">
by contrast, the upper limit is much higher in the control condition, as phoneme 1some transcription choices made by brent and cartwright are questionable (blanchard and heinz, 2008).<papid> W08-2109 </papid></citsent>
<aftsection>
<nextsent>yet, we used the canonical version of the corpus for the sake of comparability.
</nextsent>
<nextsent>substitutions are context-free.from computational point of view, the application of allophonic rules increases both the number of symbols in the alphabet and, as byproduct, the lexical complexity.
</nextsent>
<nextsent>obviously, when any kind of noise or variation is added, there is less information in the data to learn from.
</nextsent>
<nextsent>we can therefore presume that the probability mass will be scattered, and that as aconsequence, statistical models relying on word grams statistics will do worse than with phonemic inputs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH573">
<title id=" W11-0601.xml">testing the robustness of online word segmentation effects of linguistic diversity and phonetic variation </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>the mediocre performance of the models strengthens the hypotheses that phonological knowledge is acquired in large part before the construction of lexicon (jusczyk, 1997), or that allophonic rules and word segment ations could be acquired jointly (so that neither is prerequisite for the other): children cannot extract words from fluent speech without knowing how to undo at least part of contextual variation.
</prevsent>
<prevsent>thus, the knowledge of allophonic rules seems to be prerequisite for accurate segmentation.
</prevsent>
</prevsection>
<citsent citstr=" W08-0704 ">
recent simulations of word segmentation and lexical induction suggest that using phonological knowledge (venkataraman, 2001; <papid> J01-3002 </papid>blanchard and heinz, 2008), <papid> W08-2109 </papid>modeling morpho phonological structure (johnson, 2008) <papid> W08-0704 </papid>or preserving sub segmental variation (rytting et al , 2010) invariably increases performance.</citsent>
<aftsection>
<nextsent>vice versa, martinet al  (submitted) have shown that the algorithm proposed by peperkamp et al  (2006) for undoing allophonic variation crashes in the face of realistic input (i.e. many allophones), and that it can be saved if it has approximate knowledge of word boundaries.
</nextsent>
<nextsent>further research is needed, at both an experimental and computational level, to explore the performance and suitability of an online model that combines the acquisition of allophonic variation with that of word segmentation.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH574">
<title id=" W11-0321.xml">authorship attribution with latent dirichlet allocation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that our approach yields state-of-the-art performance for both few and many candidate authors, in cases where these authors wrote enough texts to be modelled effectively.
</prevsent>
<prevsent>the problem of authorship attribution ? attributing texts to their original authors ? has received considerable attention in the last decade (juola, 2006;stamatatos, 2009).
</prevsent>
</prevsection>
<citsent citstr=" C04-1088 ">
most of the work in this field focuses on cases where texts must be attributed to oneof few candidate authors, e.g., (mosteller and wallace, 1964; gamon, 2004).<papid> C04-1088 </papid></citsent>
<aftsection>
<nextsent>recently, researchers have turned their attention to scenarios with tens to thousands of candidate authors (koppel et al , 2011).
</nextsent>
<nextsent>in this paper, we study authorship attribution with few to many candidate authors, and introduce new method that achieves state-of-the-art performance in the latter case.
</nextsent>
<nextsent>our approach to authorship attribution consists of building models of authors and their texts using latent dirichlet allocation (lda) (blei et al , 2003).
</nextsent>
<nextsent>we compare these models to models built from texts with unknown authors to find the most likely authors of these texts (section 3.2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH575">
<title id=" W11-0321.xml">authorship attribution with latent dirichlet allocation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>multi-class svms have also been suggested, but they generally perform comparably toova while taking longer to train (rifkin and klau tau, 2004).
</prevsent>
<prevsent>hence, using svms for scenarios with many candidate authors is problematic (koppel et al ., 2011).
</prevsent>
</prevsection>
<citsent citstr=" C10-1008 ">
recent approaches to employing binary svms consider class similarity to improve performance (bickerstaffe and zukerman, 2010; <papid> C10-1008 </papid>chenget al , 2007).</citsent>
<aftsection>
<nextsent>we leave experiments with such approaches for future work (section 5).
</nextsent>
<nextsent>in this paper, we focus on authorship attribution with many candidate authors.
</nextsent>
<nextsent>this problem was previously addressed by madigan et al  (2005) and luy ckx and daelemans (2008), <papid> C08-1065 </papid>who worked on datasets with texts by 114 and 145 authors respectively.</nextsent>
<nextsent>in both cases, the reported results were much poorer than those reported in the binary case.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH576">
<title id=" W11-0321.xml">authorship attribution with latent dirichlet allocation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we leave experiments with such approaches for future work (section 5).
</prevsent>
<prevsent>in this paper, we focus on authorship attribution with many candidate authors.
</prevsent>
</prevsection>
<citsent citstr=" C08-1065 ">
this problem was previously addressed by madigan et al  (2005) and luy ckx and daelemans (2008), <papid> C08-1065 </papid>who worked on datasets with texts by 114 and 145 authors respectively.</citsent>
<aftsection>
<nextsent>in both cases, the reported results were much poorer than those reported in the binary case.
</nextsent>
<nextsent>more recently, koppel et al  (2011) considered author similarity to handle cases with thousands of candidateauthors.
</nextsent>
<nextsent>their method, which we use as our base line, is described in section 3.1.our approach to authorship attribution utilises latent dirichlet allocation (lda) (blei et al , 2003) to build models of authors from their texts.
</nextsent>
<nextsent>ldais generative probabilistic model that is traditionally used to find topics in textual data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH578">
<title id=" W11-1804.xml">overview of the infectious diseases id task of bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>seven teams submitted final results to the task, with the highest-performing system achieving 56% f-score in the full task, comparable to state-of-the-art performance in the established bionlp09 task.
</prevsent>
<prevsent>the results indicate that event extraction methods generalize well to new domains and full-text publications and are applicable to the extraction of events relevant to the molecular mechanisms of infectious diseases.
</prevsent>
</prevsection>
<citsent citstr=" W11-1802 ">
the infectious diseases (id) task of the bionlpshared task 2011 (kim et al, 2011<papid> W11-1802 </papid>a) is an information extraction task focusing on the biomolecular mechanisms of infectious diseases.</citsent>
<aftsection>
<nextsent>the primary target of the task is event extraction (ananiadou et al., 2010), broadly following the task setup of the bionlp09 shared task (bionlp st09) (kim et al., 2009).<papid> W09-1401 </papid></nextsent>
<nextsent>the task concentrates on the specific domain of two-component systems (tcss, or two-component regulatory systems), mechanism widely used by bacteria to sense and respond to the environment(thomason and kay, 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH586">
<title id=" W11-1804.xml">overview of the infectious diseases id task of bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results indicate that event extraction methods generalize well to new domains and full-text publications and are applicable to the extraction of events relevant to the molecular mechanisms of infectious diseases.
</prevsent>
<prevsent>the infectious diseases (id) task of the bionlpshared task 2011 (kim et al, 2011<papid> W11-1802 </papid>a) is an information extraction task focusing on the biomolecular mechanisms of infectious diseases.</prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
the primary target of the task is event extraction (ananiadou et al., 2010), broadly following the task setup of the bionlp09 shared task (bionlp st09) (kim et al., 2009).<papid> W09-1401 </papid></citsent>
<aftsection>
<nextsent>the task concentrates on the specific domain of two-component systems (tcss, or two-component regulatory systems), mechanism widely used by bacteria to sense and respond to the environment(thomason and kay, 2000).
</nextsent>
<nextsent>typical tcss consist of two proteins, membrane-associated sensor kinase and cytoplasmic response regulator.
</nextsent>
<nextsent>the sensor kinase monitors changes in the environment while the response regulator mediates an adaptive response, usually through differential expression of target genes (mascher et al, 2006).
</nextsent>
<nextsent>tcss have many functions, but those of particular interest for infectious disease researchers include virulence, respon seto antibiotics, quorum sensing, and bacterial cell attachment (krell et al, 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH587">
<title id=" W11-1804.xml">overview of the infectious diseases id task of bionlp shared task 2011 </title>
<section> task setting.  </section>
<citcontext>
<prevsection>
<prevsent>as named entity recognition (ner) is considered in other prominent domain evaluations (krallinger et al, 2008), we have chosen to isolate aspects of extraction performance relating toner from the main task of interest, event extraction, by providing participants with human-created gold annotations for core entities.
</prevsent>
<prevsent>these annotations are briefly presented in the following.
</prevsent>
</prevsection>
<citsent citstr=" W09-1313 ">
mentions of names of genes and their products (rna and proteins) are annotated with single type, without differentiating between sub types, following the guidelines of the genia ggp corpus(ohta et al, 2009).<papid> W09-1313 </papid></citsent>
<aftsection>
<nextsent>this type is named protein to maintain consistency with related tasks (e.g. bionlp st09), despite slight inaccuracy for cases specifically referencing rna or dna forms.
</nextsent>
<nextsent>two-component systems, consisting of two proteins, frequently have names derived from the names of the proteins involved (e.g. phop-phor or ssra/ssrb).
</nextsent>
<nextsent>mentions of tcss are annotated astwo-component-system, nesting protein annotations if present.
</nextsent>
<nextsent>regulons and operons are collections of genes whose expression is jointly regulated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH598">
<title id=" W11-1804.xml">overview of the infectious diseases id task of bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>final results to the task were successfully submitted by seven participants.
</prevsent>
<prevsent>table 5 summarizes the information provided by the participating teams.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
we note that full parsing is applied in all systems, with the specific choice of the parser of charniak and johnson (2005) <papid> P05-1022 </papid>with the biomedical domain modelof mcclosky (2009) and conversion into the stanford dependency representation (de marneffe et al, 2006) being adopted by five participants.</citsent>
<aftsection>
<nextsent>further, five of the seven systems are predominantly machine learning-based.
</nextsent>
<nextsent>these can be seen as extensions of trends that were noted in analysis of the bionlp 30 nlp events other resources rank team org word parse trig.
</nextsent>
<nextsent>arg.
</nextsent>
<nextsent>group.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH599">
<title id=" W11-1804.xml">overview of the infectious diseases id task of bionlp shared task 2011 </title>
<section> pnnl.  </section>
<citcontext>
<prevsection>
<prevsent>several participants compiled dictionaries of event trigger words and two dictionaries of hedge words from the data.
</prevsent>
<prevsent>four teams, including the threetop-ranking, used the ge task corpus as supplementary material, indicating that the ge annotations are largely compatible with id ones (see detailed results below).
</prevsent>
</prevsection>
<citsent citstr=" W11-1827 ">
this is encouraging for future applications of the event extraction approach: as manual annotation requires considerable effort and time, the ability to use existing annotations is important for the feasibility of adaptation of the approach to new domains.while several participants made use of supporting syntactic analyses provided by the organizers (stenetorp et al, 2011), none applied the analyses for supporting tasks, such as coreference or entity relation extraction results ? at least in cases due to time constraints (kilicoglu and bergler, 2011).<papid> W11-1827 </papid></citsent>
<aftsection>
<nextsent>5.2 evaluation results.
</nextsent>
<nextsent>table 6 presents the primary results by event type, and table 7 summarizes these results.
</nextsent>
<nextsent>the full task requires the extraction of additional arguments and event modifications and involves multiple novel challenges from previously addressed domain tasks including new sub domain, full-text documents, several new entity types and new event category.
</nextsent>
<nextsent>team recall prec.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH600">
<title id=" W11-1804.xml">overview of the infectious diseases id task of bionlp shared task 2011 </title>
<section> pnnl.  </section>
<citcontext>
<prevsection>
<prevsent>team recall prec.
</prevsent>
<prevsent>f-score faust 48.03 65.97 55.59 umass 46.92 62.02 53.42 stanford 46.30 55.86 50.63 concordu 49.00 40.27 44.21 uturku 37.85 48.62 42.57 pnnl 27.75 52.36 36.27 predx 22.56 35.18 27.49 table 7: primary evaluation results.
</prevsent>
</prevsection>
<citsent citstr=" C10-1088 ">
nevertheless, extraction performance for the top systems is comparable to the state-of-the-art results for the established bionlp st09 task (miwa et al, 2010) <papid> C10-1088 </papid>as well as its repetition as the 2011 ge task (kim et al, 2011<papid> W11-1802 </papid>b), where the highest overall result for the primary evaluation criteria was also 56% score for the faust system (riedel et al, 2011).</citsent>
<aftsection>
<nextsent>this result is encouraging regarding the ability of the extraction approach and methods to generalize to new domains as well as their applicability specifically to texts on the molecular mechanisms of infectious diseases.
</nextsent>
<nextsent>we note that there is substantial variation in the relative performance of systems for different entity types.
</nextsent>
<nextsent>for example, stanford (mcclosky et al., 2011) <papid> W11-1806 </papid>has relatively low performance for simple events but achieves the highest result for process, while uturku (bjorne and salakoski, 2011) results show roughly the reverse.</nextsent>
<nextsent>this suggests further potential for improvement from system combinations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH609">
<title id=" W11-1804.xml">overview of the infectious diseases id task of bionlp shared task 2011 </title>
<section> pnnl.  </section>
<citcontext>
<prevsection>
<prevsent>this result is encouraging regarding the ability of the extraction approach and methods to generalize to new domains as well as their applicability specifically to texts on the molecular mechanisms of infectious diseases.
</prevsent>
<prevsent>we note that there is substantial variation in the relative performance of systems for different entity types.
</prevsent>
</prevsection>
<citsent citstr=" W11-1806 ">
for example, stanford (mcclosky et al., 2011) <papid> W11-1806 </papid>has relatively low performance for simple events but achieves the highest result for process, while uturku (bjorne and salakoski, 2011) results show roughly the reverse.</citsent>
<aftsection>
<nextsent>this suggests further potential for improvement from system combinations.
</nextsent>
<nextsent>31 faust umass stanford concordu uturku pnnl predx size gene expression 70.68 66.43 54.00 56.57 64.88 53.33 0.00 512 transcription 69.66 68.24 60.00 70.89 57.14 0.00 53.85 77 protein catabolism 75.00 72.73 20.00 66.67 33.33 11.76 0.00 33 phosphorylation 64.00 66.67 40.00 54.55 60.61 64.29 40.00 69 localization 33.33 14.29 31.58 20.00 66.67 20.69 0.00 49 simple event total 68.47 63.55 52.72 56.78 62.67 43.87 18.18 740 binding 31.30 34.62 23.44 40.00 22.22 20.00 28.28 156 process 65.69 62.26 73.57 67.17 41.57 51.04 53.27 901 non-regulation total 63.78 60.68 63.59 62.43 46.39 47.34 43.65 1797 regulation 35.44 30.49 17.67 19.43 22.96 0.00 2.16 267 positive regulation 47.50 49.49 34.78 23.41 41.28 24.60 21.02 455 negative regulation 58.86 60.45 44.44 47.96 52.11 25.70 9.49 260 regulation total 47.07 46.65 33.02 28.87 39.49 18.45 9.71 982 subtotal 57.28 55.03 52.09 46.60 43.33 37.53 28.38 2779 negation 0.00 0.00 0.00 22.92 32.91 0.00 0.00 96 speculation 0.00 0.00 0.00 3.23 15.00 0.00 0.00 44 modification total 0.00 0.00 0.00 11.82 26.89 0.00 0.00 140 total 55.59 53.42 50.63 44.21 42.57 36.27 27.49 2919 table 6: primary evaluation f-scores by event type.
</nextsent>
<nextsent>the size?
</nextsent>
<nextsent>column gives the number of annotations of each type in the given data (training+development).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH620">
<title id=" W11-1804.xml">overview of the infectious diseases id task of bionlp shared task 2011 </title>
<section> pnnl.  </section>
<citcontext>
<prevsection>
<prevsent>the results for faust, umass and stanford were similar in both tasks, while the concordu result was 6% points higher forge and the uturku result over 10% points higher forge, ranking third after faust and umass.
</prevsent>
<prevsent>these results suggest that while the faust and umass systems in particular have some systematic (e.g. architectural) advantage at both tasks, much of the performance difference observed here between the top three systems and those of concordu and uturku is due to strengths or weaknesses specific to id. possible weaknesses may relate to the treatment of multiple core entity types (vs. only protein in ge) or challenges related to nested entity annotations (not appearing in ge).
</prevsent>
</prevsection>
<citsent citstr=" W11-1818 ">
a possible id-specific strength of the threetop-ranking systems is the use of ge data for training: riedel and mccallum (2011) report an estimated 7% point improvement and mcclosky et al (2011) <papid> W11-1806 </papid>3% point improvement from use of thisdata; mcgrath et al (2011) <papid> W11-1818 </papid>estimate 1% point improvement from direct corpus combination.</citsent>
<aftsection>
<nextsent>the integration strategies applied in training these systems 32 team recall prec.
</nextsent>
<nextsent>f-score ? faust 50.62 66.06 57.32 1.73 umass 49.45 62.11 55.06 1.64 stanford 48.87 56.03 52.20 1.57 concordu 50.77 43.25 46.71 2.50 uturku 38.79 49.35 43.44 0.87 pnnl 29.36 52.62 37.69 1.42 predx 23.67 35.18 28.30 0.81 table 8: core task evaluation results.
</nextsent>
<nextsent>the ? column gives the f-score difference to the corresponding full task (primary) result.
</nextsent>
<nextsent>could potentially be applied also with other systems, an experiment that could further clarify the relative strengths of the various systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH621">
<title id=" W11-1804.xml">overview of the infectious diseases id task of bionlp shared task 2011 </title>
<section> pnnl.  </section>
<citcontext>
<prevsection>
<prevsent>the ? column gives the f-score difference to the corresponding full task (primary) result.
</prevsent>
<prevsent>could potentially be applied also with other systems, an experiment that could further clarify the relative strengths of the various systems.
</prevsent>
</prevsection>
<citsent citstr=" W11-1803 ">
the top-ranking five systems all participated also in the epi task (ohta et al, 2011), <papid> W11-1803 </papid>for which uturku ranked first with faust having comparable performance for the core task.</citsent>
<aftsection>
<nextsent>while this supports the conclusion that id performance differences do not reflect simple universal ranking of the systems, due to many substantial differences between the id and epi setups it is not straightforward to identify specific reasons for relative differences to performance at epi.
</nextsent>
<nextsent>table 8 summarizes the core task results.
</nextsent>
<nextsent>there are only modest and largely consistent differences to the corresponding full task results, reflecting in part the relative sparseness of additional arguments: in the training data, for example, only approximately 3% of instances of event types that can potentially take additional arguments had at least one additional argument.
</nextsent>
<nextsent>while event modifications represent further 4% of full task extraction targets not required for the core task, the overall low extraction performance for additional arguments and modifications limits the practical effect of these annotation categories on the performance difference between systems addressing only the core targets and those addressing the full task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH622">
<title id=" W10-4218.xml">generating natural language descriptions of z test cases </title>
<section> future and related work.  </section>
<citcontext>
<prevsection>
<prevsent>review automatically paraphrases specifications developed with meta view (sorenson et al, 1988), an academic research meta system that facilitates the construction of case environments to support software specification tasks.
</prevsent>
<prevsent>coscoy (1997) describes mechanism based on program extraction, for generating explanations of formal proofs in the calculus of inductive constructions, implemented in the coq proof assistant (bertot and cas teran, 2004).
</prevsent>
</prevsection>
<citsent citstr=" A97-1037 ">
lavoie et al(1997) <papid> A97-1037 </papid>present modex, tool that generates cus tomizable descriptions of the relations between classes in object-oriented models specified in the odl standard (cattell and barry, 1997).</citsent>
<aftsection>
<nextsent>bertani et al (1999) describe controlled natural language approach to translating formal specifications written in an extension of trio (ghezzi et al, 1990)by transforming syntactic trees in trio into syntactic trees of the controlled language.
</nextsent>
<nextsent>the solutions presented in the related work above are highly dependant on particular aspects4somewhat along the lines of what wilcock (2005) describes for xml-based nlg.5this approach is similar to the method proposed by kittredge and lavoie (1998) for generating weather forecasts.
</nextsent>
<nextsent>6salek et al (1994) also give comprehensive survey of related work for generating nl explanations for particular specification languages (most of which are now obsolete).
</nextsent>
<nextsent>of the source language and do not apply directly to specifications written in z. to our knowledge,no work has been done towards producing nl descriptions of specifications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH623">
<title id=" W11-1725.xml">sentimatrix 8211 multilingual sentiment analysis service </title>
<section> sentimatrix compared with state-of-.  </section>
<citcontext>
<prevsection>
<prevsent>future work and conclusions are briefly described in section 7.
</prevsent>
<prevsent>the-art comprehensive state of the art in the field of sentiment analysis, together with potential applications of such opinion identification tools, is presented in (pang and lee, 2008).
</prevsent>
</prevsection>
<citsent citstr=" C90-2069 ">
starting from the early 1990s, the research on sentiment-analysis and point of views generally assumed the existence of sub-systems for rather sophisticated nlp tasks, ranging from parsing to the resolution of pragmatic ambiguities (hearst, 1992; wiebe 1990 <papid> C90-2069 </papid>and 1994).</citsent>
<aftsection>
<nextsent>in sentimatrix, in order to identify the sentiment user expresses about specific product or company, the company name must be first identified in the text.
</nextsent>
<nextsent>named 189 entity recognition (ner) systems typically use linguistic grammar-based techniques or statistical models (an overview is presented in (nadeau and satoshi sekine.
</nextsent>
<nextsent>2007)).
</nextsent>
<nextsent>hand-crafted grammar based systems typically obtain better precision, but at the cost of lower recall and months of work by experienced computational linguists.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH624">
<title id=" W11-0401.xml">on the development of the rst spanish treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(taboada and mann, 2006b).
</prevsent>
<prevsent>nevertheless, most of these works have been developed for english, german or portuguese.
</prevsent>
</prevsection>
<citsent citstr=" W04-0213 ">
this is due to the fact that at present corpora annotated with rst relations are available only for these languages (for english: carlson et al, 2002, taboada and renkema, 2008; for german: stede, 2004; <papid> W04-0213 </papid>for portuguese: pardo et al., 2008) and there are automatic rst parsers for two of them (for english: marcu, 2000; for portuguese: pardo et al, 2008) or automatic rst segment ers (for english: tofiloski et al, 2009).<papid> P09-2020 </papid></citsent>
<aftsection>
<nextsent>scientific community working on rst applied to spanish is very small.
</nextsent>
<nextsent>for example, bouayad-agha et al (2006) apply rst to text generation in several languages, spanish among them.
</nextsent>
<nextsent>da cunha et al (2007) develop summarization system for medical texts in spanish based on rst.
</nextsent>
<nextsent>da cunha and iruskieta (2010) perform contrastive analysis of spanish and basque texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH626">
<title id=" W11-0401.xml">on the development of the rst spanish treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(taboada and mann, 2006b).
</prevsent>
<prevsent>nevertheless, most of these works have been developed for english, german or portuguese.
</prevsent>
</prevsection>
<citsent citstr=" P09-2020 ">
this is due to the fact that at present corpora annotated with rst relations are available only for these languages (for english: carlson et al, 2002, taboada and renkema, 2008; for german: stede, 2004; <papid> W04-0213 </papid>for portuguese: pardo et al., 2008) and there are automatic rst parsers for two of them (for english: marcu, 2000; for portuguese: pardo et al, 2008) or automatic rst segment ers (for english: tofiloski et al, 2009).<papid> P09-2020 </papid></citsent>
<aftsection>
<nextsent>scientific community working on rst applied to spanish is very small.
</nextsent>
<nextsent>for example, bouayad-agha et al (2006) apply rst to text generation in several languages, spanish among them.
</nextsent>
<nextsent>da cunha et al (2007) develop summarization system for medical texts in spanish based on rst.
</nextsent>
<nextsent>da cunha and iruskieta (2010) perform contrastive analysis of spanish and basque texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH627">
<title id=" W11-0401.xml">on the development of the rst spanish treebank </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>in section 5, we establish some conclusions and future work.
</prevsent>
<prevsent>the most known rst corpus is the rst discourse treebank, for english (carlson et al, 2002a, 2002b).
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
it includes 385 texts of the journalistic domain, extracted from the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>such as cultural reviews, editorials, economy articles, etc. 347 texts are used as learning corpus and 38 texts are used as test corpus.</citsent>
<aftsection>
<nextsent>it contains 176,389 words and 21,789 edus.
</nextsent>
<nextsent>13.8% of the texts (that is, 53) were annotated by two people with list of 78 relations.
</nextsent>
<nextsent>for annotation, the annotation tool rsttool 2 (o donnell, 2000) <papid> W00-1434 </papid>was used, with some adaptations.</nextsent>
<nextsent>the principal advantages of this corpus stand on the high number of annotated texts (for the moment it is the biggest rst corpus) and the clarity of the annotation method (specified in the annotation manual by carlson and marcu, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH628">
<title id=" W11-0401.xml">on the development of the rst spanish treebank </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>it contains 176,389 words and 21,789 edus.
</prevsent>
<prevsent>13.8% of the texts (that is, 53) were annotated by two people with list of 78 relations.
</prevsent>
</prevsection>
<citsent citstr=" W00-1434 ">
for annotation, the annotation tool rsttool 2 (o donnell, 2000) <papid> W00-1434 </papid>was used, with some adaptations.</citsent>
<aftsection>
<nextsent>the principal advantages of this corpus stand on the high number of annotated texts (for the moment it is the biggest rst corpus) and the clarity of the annotation method (specified in the annotation manual by carlson and marcu, 2001).
</nextsent>
<nextsent>however, some drawbacks remain.
</nextsent>
<nextsent>the corpus is not free, it is not on-line and it only includes texts of one domain (journalistic).
</nextsent>
<nextsent>for english there is also the discourse relations reference corpus (taboada and renkema, 2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH631">
<title id=" W11-0401.xml">on the development of the rst spanish treebank </title>
<section> the rst spanish treebank.  </section>
<citcontext>
<prevsection>
<prevsent>although there are several corpora annotated with discourse relations, there is not corpus of this type for spanish.
</prevsent>
<prevsent>as sierra (2008) states, corpus consists of compilation of set of written and/or spoken texts sharing some characteristics, created for certain investigation purposes.
</prevsent>
</prevsection>
<citsent citstr=" P10-5004 ">
according to hovy (2010), <papid> P10-5004 </papid>we use 7 core questions in corpus design, detailed in the next subsections.</citsent>
<aftsection>
<nextsent>3.1 selecting corpus.
</nextsent>
<nextsent>for the rst spanish treebank, we wanted to include short texts (finally, the average is 197 words by text; the longest containing 1,051 words and the shortest, 25) in order to get best on-line visualization of the rst trees.
</nextsent>
<nextsent>moreover, in the first stage of the project, we preferred to select specialized texts of very different areas, although in the future we plan to include also non specialized texts (ex.
</nextsent>
<nextsent>blogs, news, websites) in order to guarantee the representativity of the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH641">
<title id=" W11-0401.xml">on the development of the rst spanish treebank </title>
<section> the rst spanish treebank.  </section>
<citcontext>
<prevsection>
<prevsent>xml (with dtd) has been used, in order the corpus can be reused for several aplications.
</prevsent>
<prevsent>in the future, we plan to use the standard xces.
</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
7 to know more about resources development, linguistic annotation or inter-annotator agreement, we recommend: palmer et al (on-line), palmer and xue (2010), and artstein and poesio (2008).<papid> J08-4004 </papid></citsent>
<aftsection>
<nextsent>spanish treebank the rst spanish treebank interface is freely available on-line7.
</nextsent>
<nextsent>it allows the visualization and downloading of all the texts in txt format, with their corresponding annotated trees in rsttool format (rs3), as well as in image format (jpg).
</nextsent>
<nextsent>each text includes its title, its reference, its web link (if it is an on-line text) and its number of words.
</nextsent>
<nextsent>the interface shows texts by areas and allows the user to select sub corpus (including individual files or folders containing several files).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH642">
<title id=" W11-1507.xml">enrichment and structuring of archival description meta data </title>
<section> enrichment and structuring method.  </section>
<citcontext>
<prevsection>
<prevsent>these measures can be coarsely classified into two categories: unithood-based and termhood-based.
</prevsent>
<prevsent>unithood-based approaches measure the attachment strength among the constituents of candidateterm.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
for example, some unithood-based measures are frequency of co-occurrence, hypothesis testing statistics, log-likelihood ratios test (dunning, 1993) <papid> J93-1003 </papid>and pointwise mutual information (churchand hanks, 1990).</citsent>
<aftsection>
<nextsent>termhood-based approaches attempt to measure the degree up to which candidate expression is valid term, i.e. refers to specialised concept.
</nextsent>
<nextsent>they attempt to measure this degree by considering nested ness information, namely the fre 47 quencies of candidate terms and their subsequences.examples of such approaches are c-value and nc value (frantzi et al, 2000) and the statistical barrier method (nakagawa, 2000).it has been experimentally shown that termhood based approaches to automatic term extraction out perform unithood-based ones and that c-value(frantzi et al, 2000) is among the best performing termhood-based approaches (korkontzelos et al., 2008).
</nextsent>
<nextsent>for this reason, we choose to employ the c-value measure in our pipeline.
</nextsent>
<nextsent>c-value exploits nested ness and comes together with computationally efficient algorithm, which scores candidate multi-word terms according to the measure, considering:- the total frequency of occurrence of the candidate term; - the frequency of the candidate term as part of longer candidate terms; - the number of these distinct longer candidates; - the length of the candidate term (in tokens).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH643">
<title id=" W10-4212.xml">charting the potential of description logic for the generation of referring expressions </title>
<section> dog love.(dog woman) = {d1},.  </section>
<citcontext>
<prevsection>
<prevsent>other aspects of quality depend in part on details, such as the question in which order atomic concepts are combined during phase (1), and analogously during later phases.however this approach does not extend the expressive power of gre.
</prevsent>
<prevsent>this is not because of some specific lapse on the part of the authors: it seems to have escaped the gre community as whole that relations can enter resin variety of alternative ways.
</prevsent>
</prevsection>
<citsent citstr=" W08-1107 ">
furthermore, the above algorithm considers only the abox, therefore background information 1areces et al  (areces et al , 2008) <papid> W08-1107 </papid>consider several dl fragments (e.g., alc and el).</citsent>
<aftsection>
<nextsent>which referring expressions are expressible, in their framework, depends on which dl fragment is chosen.
</nextsent>
<nextsent>existential quantification, however, is the only quantifier that was used, and inverse relations are not considered.will not be used.
</nextsent>
<nextsent>it follows that the domain always has fixed single interpretation/model.
</nextsent>
<nextsent>consequently the algorithm essentially uses model checking, rather than full reasoning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH647">
<title id=" W10-4123.xml">cmdmc a dia chronic digital museum of chinese mandarin </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>based on this resource, we intend to analyze the syntactic correlations of prosodic phrase in broadcasting news speech on cnr, and compare the phonetic and prosodic features in movie dialogues among several same-name movies in different historical eras.
</prevsent>
<prevsent>4.1 correlation between syntax &amp; prosody.
</prevsent>
</prevsection>
<citsent citstr=" H91-1073 ">
in english, there is strong correlation between prosodic phrase boundaries and syntactic phrase boundaries (price et al 1991).<papid> H91-1073 </papid></citsent>
<aftsection>
<nextsent>that is to say, prosodic phrase boundaries can play an important role in understanding utterance as punctuation marks do in written language.
</nextsent>
<nextsent>an investigation propose that boundary strength according to the measure, which the boundary strength is applied to syntactic structures and the phrase structure is viewed as an immediate constituency tree exclusively, corresponds much more closely to empirical prosodic boundary strength than does syntactic boundary strength according to standard measure (abney, 1992).<papid> H92-1086 </papid></nextsent>
<nextsent>in greek, some study indicated that prosodic phrasing has 95% identification rate, and major effect on final tonal boundaries (botinis et al 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH648">
<title id=" W10-4123.xml">cmdmc a dia chronic digital museum of chinese mandarin </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>in english, there is strong correlation between prosodic phrase boundaries and syntactic phrase boundaries (price et al 1991).<papid> H91-1073 </papid></prevsent>
<prevsent>that is to say, prosodic phrase boundaries can play an important role in understanding utterance as punctuation marks do in written language.</prevsent>
</prevsection>
<citsent citstr=" H92-1086 ">
an investigation propose that boundary strength according to the measure, which the boundary strength is applied to syntactic structures and the phrase structure is viewed as an immediate constituency tree exclusively, corresponds much more closely to empirical prosodic boundary strength than does syntactic boundary strength according to standard measure (abney, 1992).<papid> H92-1086 </papid></citsent>
<aftsection>
<nextsent>in greek, some study indicated that prosodic phrasing has 95% identification rate, and major effect on final tonal boundaries (botinis et al 2004).
</nextsent>
<nextsent>in chinese, some researchers also proposed statistical model to predict prosodic words from lexical words.
</nextsent>
<nextsent>in their model, both length of the word and the tagging from pos are two essential features to predict prosodic words, and the results showed approximately 90% of prediction for prosodic words (chen at el.
</nextsent>
<nextsent>2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH649">
<title id=" W10-4210.xml">natural reference to objects in a visual domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years there has been increased interest in generating referential expressions that are natural, e.g., like those produced by people.
</prevsent>
<prevsent>although research on the generation of referring expressions has examined different aspects of how people generate reference, there has been surprisingly little research on how people refer to objects in real-world setting.
</prevsent>
</prevsection>
<citsent citstr=" J03-1003 ">
this paper addresses this issue, and we begin formulating the requirements for an reg algorithm that refers to visible three-dimensional objects in the real world.reference to objects in visual domain provides straightforward extension of the sorts of reference reg research already tends to consider.toy examples outline reference to objects, people, and animals that are perceptually available before the speaker begins generating an utterance (dale and reiter, 1995; krahmer et al, 2003; <papid> J03-1003 </papid>van deemter et al, 2006; areces et al, 2008).<papid> W08-1107 </papid></citsent>
<aftsection>
<nextsent>example referents may be referred to by their color, size, type (dog?
</nextsent>
<nextsent>or cup?), whether or not they have beard, etc. typically, the reference process proceeds by comparing the properties of the referent with the properties of all the other items in the set.
</nextsent>
<nextsent>the final expression roughly conforms to the gricean maxims (grice, 1975).
</nextsent>
<nextsent>however, when the goal is to generate natural reference, this framework is too simple.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH650">
<title id=" W10-4210.xml">natural reference to objects in a visual domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years there has been increased interest in generating referential expressions that are natural, e.g., like those produced by people.
</prevsent>
<prevsent>although research on the generation of referring expressions has examined different aspects of how people generate reference, there has been surprisingly little research on how people refer to objects in real-world setting.
</prevsent>
</prevsection>
<citsent citstr=" W08-1107 ">
this paper addresses this issue, and we begin formulating the requirements for an reg algorithm that refers to visible three-dimensional objects in the real world.reference to objects in visual domain provides straightforward extension of the sorts of reference reg research already tends to consider.toy examples outline reference to objects, people, and animals that are perceptually available before the speaker begins generating an utterance (dale and reiter, 1995; krahmer et al, 2003; <papid> J03-1003 </papid>van deemter et al, 2006; areces et al, 2008).<papid> W08-1107 </papid></citsent>
<aftsection>
<nextsent>example referents may be referred to by their color, size, type (dog?
</nextsent>
<nextsent>or cup?), whether or not they have beard, etc. typically, the reference process proceeds by comparing the properties of the referent with the properties of all the other items in the set.
</nextsent>
<nextsent>the final expression roughly conforms to the gricean maxims (grice, 1975).
</nextsent>
<nextsent>however, when the goal is to generate natural reference, this framework is too simple.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH651">
<title id=" W10-4210.xml">natural reference to objects in a visual domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the final expression roughly conforms to the gricean maxims (grice, 1975).
</prevsent>
<prevsent>however, when the goal is to generate natural reference, this framework is too simple.
</prevsent>
</prevsection>
<citsent citstr=" J84-2002 ">
the form reference takes is profoundly affected by modality, task, and audience (chapanis et al, 1977; cohen, 1984; <papid> J84-2002 </papid>clark and wilkes-gibbs, 1986), and even when these aspects are controlled, different people will refer differently to the same object (mitchell,2008).</citsent>
<aftsection>
<nextsent>in light of this, we isolate one kind of natural reference and begin building the algorithmicframework necessary to generate the observed lan guage.psycholinguistic research has examined reference in variety of settings, which may inform research on natural reg, but it is not always clear how to extend this work to computational model.
</nextsent>
<nextsent>this is true in part because these studies favor an analysis of reference in the context of collabora tion; reference is embedded within language, and language is often joint activity.
</nextsent>
<nextsent>however, most research on referring expression generation supposes solitary generating agent.1 this tacitly assumes that reference will be taking place in monologue setting, rather than dialogue or group setting.
</nextsent>
<nextsent>indeed, the goal of most reg algorithm sis to produce uniquely distinguishing, one-shot referring expressions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH652">
<title id=" W10-4210.xml">natural reference to objects in a visual domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, the goal of most reg algorithm sis to produce uniquely distinguishing, one-shot referring expressions.
</prevsent>
<prevsent>studies on natural reference usually use two person (speaker-listener) communication task (e.g., flavell et al, 1968; krauss and glucks berg, 1969; ford and olson, 1975).
</prevsent>
</prevsection>
<citsent citstr=" J95-3003 ">
this research has 1a notable exception is heeman and hirst (1995).<papid> J95-3003 </papid></citsent>
<aftsection>
<nextsent>shown that reference is more accurate and efficient when it incorporates things like gesture and gaze (clark and krych, 2004).
</nextsent>
<nextsent>there is trade-off in effort between initiating noun phrase and refashioning it so that both speakers understand the referent (clark and wilkes-gibbs, 1986), and speakers communicate to form lexical pacts on how to refer to an object (sacks and schegloff, 1979; brennan and clark, 1996).
</nextsent>
<nextsent>mutual understanding of referents is achieved in part by referring within subset of potential referents (clark et al, 1983; beun and cremers, 1998).
</nextsent>
<nextsent>a few studies have compared monologue to dialogue reference, and have shown that monologue references tend to be harder for later listener to disambiguate (clark and krych, 2004) and that subsequent references tend to be longer than those in dialogues (krauss and weinheimer, 1967).aiming to generate natural reference in monologue setting raises questions about what an algorithm should use to produce utterances like those produced by people.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH655">
<title id=" W10-4210.xml">natural reference to objects in a visual domain </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>human behavior is anchored in space, and spatial information is essential for our ability to navigate the world we live in.
</prevsent>
<prevsent>however, referring expression generation algorithms geared towards spatial representations have oversimplified this tendency, keeping objects within the realm of two dimensions and only looking at the spatial relations between objects.
</prevsent>
</prevsection>
<citsent citstr=" E06-1041 ">
for example, funakoshi et al (2004) and gatt (2006) <papid> E06-1041 </papid>focus on how objects should be clustered together to form groups.</citsent>
<aftsection>
<nextsent>this utilizes some of the spatial information between objects, but does not address the spatial, three-dimensional nature of objects themselves.
</nextsent>
<nextsent>rather, objects exist as entities that may be grouped with other entities in set or singled out as individual objects; they do not have their own spatial characteristics.
</nextsent>
<nextsent>similarly, one of the strengths of the graph-based algorithm(krahmer et al, 2003) <papid> J03-1003 </papid>is its ability to generate expressions that involve relations between objects, and these include spatial ones (next to?, on topof?, etc.).</nextsent>
<nextsent>in all these approaches, however, objects are essentially one-dimensional, represented as individual nodes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH658">
<title id=" W11-1308.xml">detecting compositionality using semantic vector space models based on syntactic context shared task system description </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>a system could learn these semantic relations from large quantities of natural language text, to build an independent semantic resource, background knowledge base (bkb) (penas and hovy, 2010).from dependency-parsed corpus, we automatically harvest meaning-bearing patterns, matching the dependency trees to set of pre-specified syntactic patterns, similarly to (pado and lapata, 2007).
</prevsent>
<prevsent>patterns are matched to dependency trees to produce propositions, carriers of minimal semantic units.their frequency in the collection is the fundamental source of our representation.our participation, due to time constraints, is restricted to adjective-noun pairs in english.
</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
our hypothesis can be spelled out as: words (or word compounds) with similar syntactic contexts are semantically similar.the intuition behind our approach is that non compositional compounds are units of meaning.then, the meaning of an adjective-noun combination that is not compositional should be different from the meaning of the noun alone; for similar 43 approaches, see (baldwin et al, 2003; <papid> W03-1812 </papid>katz and giesbrecht, 2006; <papid> W06-1203 </papid>mitchell and lapata, 2010).</citsent>
<aftsection>
<nextsent>we propose studying the distributional semantics of aadjective-noun compound; in particular, we will represent it via its syntactic contexts.
</nextsent>
<nextsent>2.1 adjective-noun compounds.
</nextsent>
<nextsent>given particular adjective-noun compound, denoted a, n?, we want to measure its composition ality by comparing its syntactic contexts to those ofthe noun: n?.
</nextsent>
<nextsent>after exploring the dataset we realized that considering nouns alone introduced noise, as contexts of the target and different meanings of the noun might be hard to separate; in order to soften this problem we decided to compare the occurrences of the a, n?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH659">
<title id=" W11-1308.xml">detecting compositionality using semantic vector space models based on syntactic context shared task system description </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>a system could learn these semantic relations from large quantities of natural language text, to build an independent semantic resource, background knowledge base (bkb) (penas and hovy, 2010).from dependency-parsed corpus, we automatically harvest meaning-bearing patterns, matching the dependency trees to set of pre-specified syntactic patterns, similarly to (pado and lapata, 2007).
</prevsent>
<prevsent>patterns are matched to dependency trees to produce propositions, carriers of minimal semantic units.their frequency in the collection is the fundamental source of our representation.our participation, due to time constraints, is restricted to adjective-noun pairs in english.
</prevsent>
</prevsection>
<citsent citstr=" W06-1203 ">
our hypothesis can be spelled out as: words (or word compounds) with similar syntactic contexts are semantically similar.the intuition behind our approach is that non compositional compounds are units of meaning.then, the meaning of an adjective-noun combination that is not compositional should be different from the meaning of the noun alone; for similar 43 approaches, see (baldwin et al, 2003; <papid> W03-1812 </papid>katz and giesbrecht, 2006; <papid> W06-1203 </papid>mitchell and lapata, 2010).</citsent>
<aftsection>
<nextsent>we propose studying the distributional semantics of aadjective-noun compound; in particular, we will represent it via its syntactic contexts.
</nextsent>
<nextsent>2.1 adjective-noun compounds.
</nextsent>
<nextsent>given particular adjective-noun compound, denoted a, n?, we want to measure its composition ality by comparing its syntactic contexts to those ofthe noun: n?.
</nextsent>
<nextsent>after exploring the dataset we realized that considering nouns alone introduced noise, as contexts of the target and different meanings of the noun might be hard to separate; in order to soften this problem we decided to compare the occurrences of the a, n?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH660">
<title id=" W11-1308.xml">detecting compositionality using semantic vector space models based on syntactic context shared task system description </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>pukwac adds to ukwac layer of syntactic dependency annotation.
</prevsent>
<prevsent>the corpus hasbeen pos-tagged and lemmatized with the treetagger2.
</prevsent>
</prevsection>
<citsent citstr=" C04-1010 ">
the dependency parse was done with malt parser (nivre and scholz, 2004).<papid> C04-1010 </papid></citsent>
<aftsection>
<nextsent>2.2 implementation details.
</nextsent>
<nextsent>we defined set of 19 syntactic patterns that define interesting relations in which an a, n?
</nextsent>
<nextsent>pair might take part, trying to exploit the dependencies produced by the malt parser (nivre and scholz, 2004), <papid> C04-1010 </papid>including: ? relations to verb, other than the auxiliary to be and to have: subject; object; indirect object;subject of passive construction; logical subject of passive construction.?</nextsent>
<nextsent>the relations defined in the previous point, enriched with noun that acts as the other element of [subject-verb-object] or [subject-passive verb-logical subject] construction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH662">
<title id=" W10-4161.xml">person name disambiguation based on topic model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kozareva (2008) focuses on the resolution of the web people search problem through the integration of domain information, which can represent relationship between contexts and is learned from wordnet.
</prevsent>
<prevsent>poboc clustering (cleuziou et al , 2004) is used which builds weighted graph with weights being the similarity among the objects.
</prevsent>
</prevsection>
<citsent citstr=" W07-2042 ">
another way is to utilize universal data repositories as external knowledge sources (rao et al , 2007; <papid> W07-2042 </papid>kalmar and blume, 2007; <papid> W07-2030 </papid>pedersen and kulkarni; 2007) in order to give more realistic frequency for proper name or measure whether bigram is collocation.</citsent>
<aftsection>
<nextsent>phan et al  (2008) presents general framework for building classifiers that deal with short and sparse text and web segments by making the most of hidden topics discovered from large-scale data collections.
</nextsent>
<nextsent>samuel brody et al  (2009) adopt novel bayesian approach and formalize the word sense induction problem in generative model.
</nextsent>
<nextsent>previous work using the weps1 (artiles et al , 2007) <papid> W07-2012 </papid>or weps2 dataset (artiles et al , 2009) shows that standard document clustering methods can deliver excellent performance if similarity measure is enough good to represent relationship of context.</nextsent>
<nextsent>the study in chinese pnd is still in its infancy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH663">
<title id=" W10-4161.xml">person name disambiguation based on topic model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kozareva (2008) focuses on the resolution of the web people search problem through the integration of domain information, which can represent relationship between contexts and is learned from wordnet.
</prevsent>
<prevsent>poboc clustering (cleuziou et al , 2004) is used which builds weighted graph with weights being the similarity among the objects.
</prevsent>
</prevsection>
<citsent citstr=" W07-2030 ">
another way is to utilize universal data repositories as external knowledge sources (rao et al , 2007; <papid> W07-2042 </papid>kalmar and blume, 2007; <papid> W07-2030 </papid>pedersen and kulkarni; 2007) in order to give more realistic frequency for proper name or measure whether bigram is collocation.</citsent>
<aftsection>
<nextsent>phan et al  (2008) presents general framework for building classifiers that deal with short and sparse text and web segments by making the most of hidden topics discovered from large-scale data collections.
</nextsent>
<nextsent>samuel brody et al  (2009) adopt novel bayesian approach and formalize the word sense induction problem in generative model.
</nextsent>
<nextsent>previous work using the weps1 (artiles et al , 2007) <papid> W07-2012 </papid>or weps2 dataset (artiles et al , 2009) shows that standard document clustering methods can deliver excellent performance if similarity measure is enough good to represent relationship of context.</nextsent>
<nextsent>the study in chinese pnd is still in its infancy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH664">
<title id=" W10-4161.xml">person name disambiguation based on topic model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>phan et al  (2008) presents general framework for building classifiers that deal with short and sparse text and web segments by making the most of hidden topics discovered from large-scale data collections.
</prevsent>
<prevsent>samuel brody et al  (2009) adopt novel bayesian approach and formalize the word sense induction problem in generative model.
</prevsent>
</prevsection>
<citsent citstr=" W07-2012 ">
previous work using the weps1 (artiles et al , 2007) <papid> W07-2012 </papid>or weps2 dataset (artiles et al , 2009) shows that standard document clustering methods can deliver excellent performance if similarity measure is enough good to represent relationship of context.</citsent>
<aftsection>
<nextsent>the study in chinese pnd is still in its infancy.
</nextsent>
<nextsent>person name detection is often necessary in chinese.
</nextsent>
<nextsent>at present, the main technology of person name recognition is used statistical models, and the hybrid approach.
</nextsent>
<nextsent>liu et al  (2000) designed chinese person name recognition system based on statistical methods, using samples of names from the text corpus and the real amount of statistical data to improve the system performance, while the shortcoming is that samples of name database are too small, resulting in low recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH665">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>text-to-text generation extends to such varied tasks as summarization (knight andmarcu, 2002), question-answering (lin and pan tel, 2001), machine translation, and paraphrase generation.
</prevsent>
<prevsent>sentential paraphrase generation (spg) is the process of transforming source sentence into atarget sentence in the same language which differs in form from the source sentence, but approximates its meaning.
</prevsent>
</prevsection>
<citsent citstr=" P07-1059 ">
paraphrasing is often used as subtask in more complex nlp applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (lin and pantel, 2001; riezler et al, 2007), <papid> P07-1059 </papid>or to generate paraphrases of sentences that failed to translate (callison-burch et al, 2006).</citsent>
<aftsection>
<nextsent>paraphrasing has also been used in the evaluation of machine translation system output (russo-lassner et al, 2006; kauchak and barzilay, 2006; <papid> N06-1058 </papid>zhouet al, 2006).<papid> W06-1610 </papid></nextsent>
<nextsent>adding certain constraints to paraphrasing allows for additional useful applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH666">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sentential paraphrase generation (spg) is the process of transforming source sentence into atarget sentence in the same language which differs in form from the source sentence, but approximates its meaning.
</prevsent>
<prevsent>paraphrasing is often used as subtask in more complex nlp applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (lin and pantel, 2001; riezler et al, 2007), <papid> P07-1059 </papid>or to generate paraphrases of sentences that failed to translate (callison-burch et al, 2006).</prevsent>
</prevsection>
<citsent citstr=" N06-1058 ">
paraphrasing has also been used in the evaluation of machine translation system output (russo-lassner et al, 2006; kauchak and barzilay, 2006; <papid> N06-1058 </papid>zhouet al, 2006).<papid> W06-1610 </papid></citsent>
<aftsection>
<nextsent>adding certain constraints to paraphrasing allows for additional useful applications.
</nextsent>
<nextsent>when constraint is specified that paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (knight and marcu, 2002; barzilay and lee, 2003) <papid> N03-1003 </papid>as well as for text simplification for question answering or subtitle generation (daelemans et al, 2004).we regard spg as monolingual machine translation task, where the source and target languages are the same (quirk et al, 2004).<papid> W04-3219 </papid></nextsent>
<nextsent>however, there are two problems that have to be dealt with tomake this approach work, namely obtaining sufficient amount of examples, and proper evaluation methodology.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH667">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sentential paraphrase generation (spg) is the process of transforming source sentence into atarget sentence in the same language which differs in form from the source sentence, but approximates its meaning.
</prevsent>
<prevsent>paraphrasing is often used as subtask in more complex nlp applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (lin and pantel, 2001; riezler et al, 2007), <papid> P07-1059 </papid>or to generate paraphrases of sentences that failed to translate (callison-burch et al, 2006).</prevsent>
</prevsection>
<citsent citstr=" W06-1610 ">
paraphrasing has also been used in the evaluation of machine translation system output (russo-lassner et al, 2006; kauchak and barzilay, 2006; <papid> N06-1058 </papid>zhouet al, 2006).<papid> W06-1610 </papid></citsent>
<aftsection>
<nextsent>adding certain constraints to paraphrasing allows for additional useful applications.
</nextsent>
<nextsent>when constraint is specified that paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (knight and marcu, 2002; barzilay and lee, 2003) <papid> N03-1003 </papid>as well as for text simplification for question answering or subtitle generation (daelemans et al, 2004).we regard spg as monolingual machine translation task, where the source and target languages are the same (quirk et al, 2004).<papid> W04-3219 </papid></nextsent>
<nextsent>however, there are two problems that have to be dealt with tomake this approach work, namely obtaining sufficient amount of examples, and proper evaluation methodology.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH668">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>paraphrasing has also been used in the evaluation of machine translation system output (russo-lassner et al, 2006; kauchak and barzilay, 2006; <papid> N06-1058 </papid>zhouet al, 2006).<papid> W06-1610 </papid></prevsent>
<prevsent>adding certain constraints to paraphrasing allows for additional useful applications.</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
when constraint is specified that paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (knight and marcu, 2002; barzilay and lee, 2003) <papid> N03-1003 </papid>as well as for text simplification for question answering or subtitle generation (daelemans et al, 2004).we regard spg as monolingual machine translation task, where the source and target languages are the same (quirk et al, 2004).<papid> W04-3219 </papid></citsent>
<aftsection>
<nextsent>however, there are two problems that have to be dealt with tomake this approach work, namely obtaining sufficient amount of examples, and proper evaluation methodology.
</nextsent>
<nextsent>as callison-burch et al(2008) argue, automatic evaluation of paraphrasing is problematic.
</nextsent>
<nextsent>the essence of spg is to generate sentence that is structurally different fromthe source.
</nextsent>
<nextsent>automatic evaluation metrics in related fields such as machine translation operate on notion of similarity, while paraphrasing centers around achieving dissimilarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH669">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>paraphrasing has also been used in the evaluation of machine translation system output (russo-lassner et al, 2006; kauchak and barzilay, 2006; <papid> N06-1058 </papid>zhouet al, 2006).<papid> W06-1610 </papid></prevsent>
<prevsent>adding certain constraints to paraphrasing allows for additional useful applications.</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
when constraint is specified that paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (knight and marcu, 2002; barzilay and lee, 2003) <papid> N03-1003 </papid>as well as for text simplification for question answering or subtitle generation (daelemans et al, 2004).we regard spg as monolingual machine translation task, where the source and target languages are the same (quirk et al, 2004).<papid> W04-3219 </papid></citsent>
<aftsection>
<nextsent>however, there are two problems that have to be dealt with tomake this approach work, namely obtaining sufficient amount of examples, and proper evaluation methodology.
</nextsent>
<nextsent>as callison-burch et al(2008) argue, automatic evaluation of paraphrasing is problematic.
</nextsent>
<nextsent>the essence of spg is to generate sentence that is structurally different fromthe source.
</nextsent>
<nextsent>automatic evaluation metrics in related fields such as machine translation operate on notion of similarity, while paraphrasing centers around achieving dissimilarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH670">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>so far, paraphrasing data sets of sufficient size have been mostly lacking.
</prevsent>
<prevsent>we argue that the headlines aggregated by google news offer an attractive avenue.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
currently not many resources are available forparaphrasing; one example is the microsoft paraphrase corpus (msr) (dolan et al, 2004; <papid> C04-1051 </papid>nelken and shieber, 2006), <papid> E06-1021 </papid>which with its 139,000 aligned police investigate doherty drug pics doherty under police investigation police to probe pete pics pete doherty arrested in drug-photo probe rocker photographed injecting unconscious fan doherty injected unconscious fan with drug?</citsent>
<aftsection>
<nextsent>photos may show pete doherty injecting passed-out fan doherty injected female fan?
</nextsent>
<nextsent>figure 1: part of sample headline cluster, with aligned paraphrases paraphrases can be considered relatively small.
</nextsent>
<nextsent>inthis study we explore the use of large, automatically acquired aligned paraphrase corpus.
</nextsent>
<nextsent>our method consists of crawling the headlines aggregated and clustered by google news and then aligning paraphrases within each of these clusters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH671">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>so far, paraphrasing data sets of sufficient size have been mostly lacking.
</prevsent>
<prevsent>we argue that the headlines aggregated by google news offer an attractive avenue.
</prevsent>
</prevsection>
<citsent citstr=" E06-1021 ">
currently not many resources are available forparaphrasing; one example is the microsoft paraphrase corpus (msr) (dolan et al, 2004; <papid> C04-1051 </papid>nelken and shieber, 2006), <papid> E06-1021 </papid>which with its 139,000 aligned police investigate doherty drug pics doherty under police investigation police to probe pete pics pete doherty arrested in drug-photo probe rocker photographed injecting unconscious fan doherty injected unconscious fan with drug?</citsent>
<aftsection>
<nextsent>photos may show pete doherty injecting passed-out fan doherty injected female fan?
</nextsent>
<nextsent>figure 1: part of sample headline cluster, with aligned paraphrases paraphrases can be considered relatively small.
</nextsent>
<nextsent>inthis study we explore the use of large, automatically acquired aligned paraphrase corpus.
</nextsent>
<nextsent>our method consists of crawling the headlines aggregated and clustered by google news and then aligning paraphrases within each of these clusters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH672">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>if the similarity exceeds defined upper threshold it is accepted; if it is below defined lower threshold it is rejected.
</prevsent>
<prevsent>in the case that it lies between the thresholds, the process is repeated but then with word vectors taken from snippet from the corresponding news article.
</prevsent>
</prevsection>
<citsent citstr=" W09-0621 ">
this method, described in earlier work wubben et al (2009), <papid> W09-0621 </papid>was reported to yield precision of 0.76 and recall of 0.41 on clustering actual dutch paraphrases in headline corpus.</citsent>
<aftsection>
<nextsent>we adapted this method to english.
</nextsent>
<nextsent>our data consists of english headlines that appeared in google news over the period of april to september 2006.
</nextsent>
<nextsent>using this method we end up with corpus of 7,400,144 pairwise alignments of 1,025,605 unique headlines1.
</nextsent>
<nextsent>in our approach we use the collection of automatically obtained aligned headlines to traina paraphrase generation model using phrase based mt framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH673">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> paraphrasing methods.  </section>
<citcontext>
<prevsection>
<prevsent>in our approach we use the collection of automatically obtained aligned headlines to traina paraphrase generation model using phrase based mt framework.
</prevsent>
<prevsent>we compare this approach to word substitution baseline.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the generated paraphrases along with their source head 1this list of aligned pairs is available at http://ilk.uvt.nl/swubben/resources.htmllines are presented to human judges, whose ratings are compared to the bleu (papineni et al, 2002), <papid> P02-1040 </papid>meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>and rouge (lin, 2004) <papid> W04-1013 </papid>automatic evaluation metrics.</citsent>
<aftsection>
<nextsent>3.1 phrase-based mt. we use the moses package to train phrase-based machine translation model (pbmt) (koehn et al, 2007).<papid> P07-2045 </papid></nextsent>
<nextsent>such model normally finds best translation e?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH674">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> paraphrasing methods.  </section>
<citcontext>
<prevsection>
<prevsent>in our approach we use the collection of automatically obtained aligned headlines to traina paraphrase generation model using phrase based mt framework.
</prevsent>
<prevsent>we compare this approach to word substitution baseline.
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
the generated paraphrases along with their source head 1this list of aligned pairs is available at http://ilk.uvt.nl/swubben/resources.htmllines are presented to human judges, whose ratings are compared to the bleu (papineni et al, 2002), <papid> P02-1040 </papid>meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>and rouge (lin, 2004) <papid> W04-1013 </papid>automatic evaluation metrics.</citsent>
<aftsection>
<nextsent>3.1 phrase-based mt. we use the moses package to train phrase-based machine translation model (pbmt) (koehn et al, 2007).<papid> P07-2045 </papid></nextsent>
<nextsent>such model normally finds best translation e?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH675">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> paraphrasing methods.  </section>
<citcontext>
<prevsection>
<prevsent>in our approach we use the collection of automatically obtained aligned headlines to traina paraphrase generation model using phrase based mt framework.
</prevsent>
<prevsent>we compare this approach to word substitution baseline.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
the generated paraphrases along with their source head 1this list of aligned pairs is available at http://ilk.uvt.nl/swubben/resources.htmllines are presented to human judges, whose ratings are compared to the bleu (papineni et al, 2002), <papid> P02-1040 </papid>meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>and rouge (lin, 2004) <papid> W04-1013 </papid>automatic evaluation metrics.</citsent>
<aftsection>
<nextsent>3.1 phrase-based mt. we use the moses package to train phrase-based machine translation model (pbmt) (koehn et al, 2007).<papid> P07-2045 </papid></nextsent>
<nextsent>such model normally finds best translation e?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH676">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> paraphrasing methods.  </section>
<citcontext>
<prevsection>
<prevsent>we compare this approach to word substitution baseline.
</prevsent>
<prevsent>the generated paraphrases along with their source head 1this list of aligned pairs is available at http://ilk.uvt.nl/swubben/resources.htmllines are presented to human judges, whose ratings are compared to the bleu (papineni et al, 2002), <papid> P02-1040 </papid>meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>and rouge (lin, 2004) <papid> W04-1013 </papid>automatic evaluation metrics.</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
3.1 phrase-based mt. we use the moses package to train phrase-based machine translation model (pbmt) (koehn et al, 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>such model normally finds best translation e?
</nextsent>
<nextsent>of text in language to text in language by combining translation model p(f |e) with language model p(e): e?
</nextsent>
<nextsent>= argmax ee? p(f |e)p(e)giza++ is used to perform the word alignments (och and ney, 2003) <papid> J03-1002 </papid>which are then used in the moses pipeline to generate phrase alignment sin order to build the paraphrase model.</nextsent>
<nextsent>we first to kenize our data before training recaser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH677">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> paraphrasing methods.  </section>
<citcontext>
<prevsection>
<prevsent>such model normally finds best translation e?
</prevsent>
<prevsent>of text in language to text in language by combining translation model p(f |e) with language model p(e): e?
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
= argmax ee? p(f |e)p(e)giza++ is used to perform the word alignments (och and ney, 2003) <papid> J03-1002 </papid>which are then used in the moses pipeline to generate phrase alignment sin order to build the paraphrase model.</citsent>
<aftsection>
<nextsent>we first to kenize our data before training recaser.
</nextsent>
<nextsent>we then lowercase all data and use all unique headlines in the training data to train language model with the srilm toolkit (stolcke, 2002).
</nextsent>
<nextsent>then we invoke the giza++ aligner using the 7m training paraphrase pairs.
</nextsent>
<nextsent>we run giza++ with standard settings and we perform no optimization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH678">
<title id=" W10-4223.xml">paraphrase generation as monolingual translation data and evaluation </title>
<section> paraphrasing methods.  </section>
<citcontext>
<prevsection>
<prevsent>we compare the pbmt results with simple word substitution baseline.
</prevsent>
<prevsent>for each noun, adjective and verb in the sentence this model takes that word andits part of speech tag and retrieves from wordnet its most frequent synonym from the most frequent synset containing the input word.
</prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
we use the memory based tagger (daelemans et al, 1996) <papid> W96-0102 </papid>system headline source florida executes notorious serial killer pbmt serial killer executed in florida word sub.</citsent>
<aftsection>
<nextsent>florida executes ill-famed series slayer source dublin evacuates airport due to bomb scare pbmt dublin airport evacuated after bomb threat word sub.
</nextsent>
<nextsent>dublin evacuates airdrome due to bomb panic source n. korea blasts nuclear sanctions pbmt n. korea nuclear blast of sanctions word sub.
</nextsent>
<nextsent>n. korea blasts atomic sanction stable 1: examples of generated paraphrased head line strained on the brown corpus to generate the pos tags.
</nextsent>
<nextsent>the wordnet::querydata2 perl module isused to query wordnet (fellbaum, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH679">
<title id=" W11-0117.xml">a representation framework for crosslingualinterlingual lexical semantic correspondences </title>
<section> semantifying bilingual lexical resource entries.  </section>
<citcontext>
<prevsection>
<prevsent>however, to extract the information from an additional description, we need to analyze the presented translation equivalent appropriately.
</prevsent>
<prevsent>this process would be highly resource-dependent,owning to lack of standardized presentation format.
</prevsent>
</prevsection>
<citsent citstr=" W04-2116 ">
nevertheless, technique to extract differ entia (ohara and wiebe 2004) <papid> W04-2116 </papid>can be applied, as some of the translation equivalents are given in the so-called genus-differentia expression pattern.</citsent>
<aftsection>
<nextsent>4.
</nextsent>
<nextsent>although it is not depicted in fig.
</nextsent>
<nextsent>5, if necessary, two underspecified tl senses, will eventu-.
</nextsent>
<nextsent>ally be grounded to the corresponding sense nodes in french lexical semantic resource.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH680">
<title id=" W11-0117.xml">a representation framework for crosslingualinterlingual lexical semantic correspondences </title>
<section> semantifying bilingual lexical resource entries.  </section>
<citcontext>
<prevsection>
<prevsent>this line of work is expected to improve further and increase, as web-based linguistic service infrastructures evolve and gain popularity.obviously, another related area of research is lexicon modeling.
</prevsent>
<prevsent>although the iso lmf will undoubtedly be used as solid and shared framework, requirements to its revisions/extensions continue to emerge.
</prevsent>
</prevsection>
<citsent citstr=" L08-1432 ">
among them, maks et al (2008) <papid> L08-1432 </papid>pointed out that lmf should more explicitly represent language-dependent usage and contrasts, and they proposes model that compromises between the mrd extension and the multilingual extension.</citsent>
<aftsection>
<nextsent>this solution might be reasonable, if we are to represent an existing bilingual dictionary precisely.
</nextsent>
<nextsent>nevertheless, the solution may not be sufficient to model and represent an evolving distributed lexical semantic network, which is prerequisite for this paper.
</nextsent>
<nextsent>the problem raised up by maks et al (2008) <papid> L08-1432 </papid>is closely related to the issue posed by trippel (2010), in which he states: lmf provides the container for combining such resources of different types, but does not merge them into one formalism.</nextsent>
<nextsent>given this motivation, he presented formal lexicon model called lexicon graph, arguing that the loss less combination of lexical resources could be accomplished.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH684">
<title id=" W11-1203.xml">learning the optimal use of dependency parsing information for finding translations with comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we define the optimal transformation matrix by using bayesian probabilistic model, and show that it is feasible to find an approximate solution using markov chain monte carlo methods.
</prevsent>
<prevsent>our experiments demonstrate that our proposed method constantly improves translation accuracy.
</prevsent>
</prevsection>
<citsent citstr=" C10-1070 ">
using comparable corpora to automatically extend bilingual dictionaries is becoming increasingly popular (laroche and langlais, 2010; <papid> C10-1070 </papid>andrade et al, 2010; <papid> C10-1003 </papid>ismail and manandhar, 2010; <papid> C10-2055 </papid>laws et al, 2010; <papid> C10-2070 </papid>garera et al, 2009).<papid> W09-1117 </papid></citsent>
<aftsection>
<nextsent>the general idea is based on the assumption that similar words have similar contexts across languages.
</nextsent>
<nextsent>the context of word can be described by the sentence in whichit occurs (laroche and langlais, 2010) <papid> C10-1070 </papid>or surrounding word-window (rapp, 1999; <papid> P99-1067 </papid>haghighi et al., 2008).<papid> P08-1088 </papid></nextsent>
<nextsent>a few previous studies, like (garera etal., 2009), <papid> W09-1117 </papid>suggested to use the predecessor and successors from the dependency-parse tree, instead of word window.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH686">
<title id=" W11-1203.xml">learning the optimal use of dependency parsing information for finding translations with comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we define the optimal transformation matrix by using bayesian probabilistic model, and show that it is feasible to find an approximate solution using markov chain monte carlo methods.
</prevsent>
<prevsent>our experiments demonstrate that our proposed method constantly improves translation accuracy.
</prevsent>
</prevsection>
<citsent citstr=" C10-1003 ">
using comparable corpora to automatically extend bilingual dictionaries is becoming increasingly popular (laroche and langlais, 2010; <papid> C10-1070 </papid>andrade et al, 2010; <papid> C10-1003 </papid>ismail and manandhar, 2010; <papid> C10-2055 </papid>laws et al, 2010; <papid> C10-2070 </papid>garera et al, 2009).<papid> W09-1117 </papid></citsent>
<aftsection>
<nextsent>the general idea is based on the assumption that similar words have similar contexts across languages.
</nextsent>
<nextsent>the context of word can be described by the sentence in whichit occurs (laroche and langlais, 2010) <papid> C10-1070 </papid>or surrounding word-window (rapp, 1999; <papid> P99-1067 </papid>haghighi et al., 2008).<papid> P08-1088 </papid></nextsent>
<nextsent>a few previous studies, like (garera etal., 2009), <papid> W09-1117 </papid>suggested to use the predecessor and successors from the dependency-parse tree, instead of word window.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH687">
<title id=" W11-1203.xml">learning the optimal use of dependency parsing information for finding translations with comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we define the optimal transformation matrix by using bayesian probabilistic model, and show that it is feasible to find an approximate solution using markov chain monte carlo methods.
</prevsent>
<prevsent>our experiments demonstrate that our proposed method constantly improves translation accuracy.
</prevsent>
</prevsection>
<citsent citstr=" C10-2055 ">
using comparable corpora to automatically extend bilingual dictionaries is becoming increasingly popular (laroche and langlais, 2010; <papid> C10-1070 </papid>andrade et al, 2010; <papid> C10-1003 </papid>ismail and manandhar, 2010; <papid> C10-2055 </papid>laws et al, 2010; <papid> C10-2070 </papid>garera et al, 2009).<papid> W09-1117 </papid></citsent>
<aftsection>
<nextsent>the general idea is based on the assumption that similar words have similar contexts across languages.
</nextsent>
<nextsent>the context of word can be described by the sentence in whichit occurs (laroche and langlais, 2010) <papid> C10-1070 </papid>or surrounding word-window (rapp, 1999; <papid> P99-1067 </papid>haghighi et al., 2008).<papid> P08-1088 </papid></nextsent>
<nextsent>a few previous studies, like (garera etal., 2009), <papid> W09-1117 </papid>suggested to use the predecessor and successors from the dependency-parse tree, instead of word window.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH688">
<title id=" W11-1203.xml">learning the optimal use of dependency parsing information for finding translations with comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we define the optimal transformation matrix by using bayesian probabilistic model, and show that it is feasible to find an approximate solution using markov chain monte carlo methods.
</prevsent>
<prevsent>our experiments demonstrate that our proposed method constantly improves translation accuracy.
</prevsent>
</prevsection>
<citsent citstr=" C10-2070 ">
using comparable corpora to automatically extend bilingual dictionaries is becoming increasingly popular (laroche and langlais, 2010; <papid> C10-1070 </papid>andrade et al, 2010; <papid> C10-1003 </papid>ismail and manandhar, 2010; <papid> C10-2055 </papid>laws et al, 2010; <papid> C10-2070 </papid>garera et al, 2009).<papid> W09-1117 </papid></citsent>
<aftsection>
<nextsent>the general idea is based on the assumption that similar words have similar contexts across languages.
</nextsent>
<nextsent>the context of word can be described by the sentence in whichit occurs (laroche and langlais, 2010) <papid> C10-1070 </papid>or surrounding word-window (rapp, 1999; <papid> P99-1067 </papid>haghighi et al., 2008).<papid> P08-1088 </papid></nextsent>
<nextsent>a few previous studies, like (garera etal., 2009), <papid> W09-1117 </papid>suggested to use the predecessor and successors from the dependency-parse tree, instead of word window.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH689">
<title id=" W11-1203.xml">learning the optimal use of dependency parsing information for finding translations with comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we define the optimal transformation matrix by using bayesian probabilistic model, and show that it is feasible to find an approximate solution using markov chain monte carlo methods.
</prevsent>
<prevsent>our experiments demonstrate that our proposed method constantly improves translation accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W09-1117 ">
using comparable corpora to automatically extend bilingual dictionaries is becoming increasingly popular (laroche and langlais, 2010; <papid> C10-1070 </papid>andrade et al, 2010; <papid> C10-1003 </papid>ismail and manandhar, 2010; <papid> C10-2055 </papid>laws et al, 2010; <papid> C10-2070 </papid>garera et al, 2009).<papid> W09-1117 </papid></citsent>
<aftsection>
<nextsent>the general idea is based on the assumption that similar words have similar contexts across languages.
</nextsent>
<nextsent>the context of word can be described by the sentence in whichit occurs (laroche and langlais, 2010) <papid> C10-1070 </papid>or surrounding word-window (rapp, 1999; <papid> P99-1067 </papid>haghighi et al., 2008).<papid> P08-1088 </papid></nextsent>
<nextsent>a few previous studies, like (garera etal., 2009), <papid> W09-1117 </papid>suggested to use the predecessor and successors from the dependency-parse tree, instead of word window.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH691">
<title id=" W11-1203.xml">learning the optimal use of dependency parsing information for finding translations with comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using comparable corpora to automatically extend bilingual dictionaries is becoming increasingly popular (laroche and langlais, 2010; <papid> C10-1070 </papid>andrade et al, 2010; <papid> C10-1003 </papid>ismail and manandhar, 2010; <papid> C10-2055 </papid>laws et al, 2010; <papid> C10-2070 </papid>garera et al, 2009).<papid> W09-1117 </papid></prevsent>
<prevsent>the general idea is based on the assumption that similar words have similar contexts across languages.</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
the context of word can be described by the sentence in whichit occurs (laroche and langlais, 2010) <papid> C10-1070 </papid>or surrounding word-window (rapp, 1999; <papid> P99-1067 </papid>haghighi et al., 2008).<papid> P08-1088 </papid></citsent>
<aftsection>
<nextsent>a few previous studies, like (garera etal., 2009), <papid> W09-1117 </papid>suggested to use the predecessor and successors from the dependency-parse tree, instead of word window.</nextsent>
<nextsent>in (andrade et al, 2011), we showed that including dependency-parse tree context positions together with sentence bag-of-words context can improve word translation accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH693">
<title id=" W11-1203.xml">learning the optimal use of dependency parsing information for finding translations with comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using comparable corpora to automatically extend bilingual dictionaries is becoming increasingly popular (laroche and langlais, 2010; <papid> C10-1070 </papid>andrade et al, 2010; <papid> C10-1003 </papid>ismail and manandhar, 2010; <papid> C10-2055 </papid>laws et al, 2010; <papid> C10-2070 </papid>garera et al, 2009).<papid> W09-1117 </papid></prevsent>
<prevsent>the general idea is based on the assumption that similar words have similar contexts across languages.</prevsent>
</prevsection>
<citsent citstr=" P08-1088 ">
the context of word can be described by the sentence in whichit occurs (laroche and langlais, 2010) <papid> C10-1070 </papid>or surrounding word-window (rapp, 1999; <papid> P99-1067 </papid>haghighi et al., 2008).<papid> P08-1088 </papid></citsent>
<aftsection>
<nextsent>a few previous studies, like (garera etal., 2009), <papid> W09-1117 </papid>suggested to use the predecessor and successors from the dependency-parse tree, instead of word window.</nextsent>
<nextsent>in (andrade et al, 2011), we showed that including dependency-parse tree context positions together with sentence bag-of-words context can improve word translation accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH700">
<title id=" W11-1203.xml">learning the optimal use of dependency parsing information for finding translations with comparable corpora </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the partof mlit and nhtsa which we used for our experiments, contains 24090 and 47613 sentences, respectively.
</prevsent>
<prevsent>the japanese mlit corpus was morphologically analyzed and dependency parsed using juman and knp4.
</prevsent>
</prevsection>
<citsent citstr=" D08-1047 ">
the english corpus nhtsa waspos-tagged and stemmed with stepp tagger (tsuruoka et al, 2005; okazaki et al, 2008) <papid> D08-1047 </papid>and dependency parsed using the mst parser (mcdonald et al., 2005).<papid> P05-1012 </papid></citsent>
<aftsection>
<nextsent>using the japanese-english dictionary jmdic5, we found 1796 content words in japanese which have translation which is in the english corpus.
</nextsent>
<nextsent>these content words and their translations correspond to our pivot words in japanese and english, respectively.6 2http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html 3http://www-odi.nhtsa.dot.gov/downloads/index.cfm4http://www-lab25.kuee.kyoto-u.ac.jp/nlresource/juman.html and http://www-lab25.kuee.kyoto u.ac.jp/nl-resource/knp.html 5http://www.csse.monash.edu.au/ jwb/edict doc.html6recall that we assume one-to-one correspondence between pivot in japanese and english.
</nextsent>
<nextsent>if japanese pivot word as more than one english translation, we select the translation for which the relative frequency in the target corpus is closest to the pivot in the source corpus.
</nextsent>
<nextsent>14 5.1 evaluation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH701">
<title id=" W11-1203.xml">learning the optimal use of dependency parsing information for finding translations with comparable corpora </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the partof mlit and nhtsa which we used for our experiments, contains 24090 and 47613 sentences, respectively.
</prevsent>
<prevsent>the japanese mlit corpus was morphologically analyzed and dependency parsed using juman and knp4.
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
the english corpus nhtsa waspos-tagged and stemmed with stepp tagger (tsuruoka et al, 2005; okazaki et al, 2008) <papid> D08-1047 </papid>and dependency parsed using the mst parser (mcdonald et al., 2005).<papid> P05-1012 </papid></citsent>
<aftsection>
<nextsent>using the japanese-english dictionary jmdic5, we found 1796 content words in japanese which have translation which is in the english corpus.
</nextsent>
<nextsent>these content words and their translations correspond to our pivot words in japanese and english, respectively.6 2http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html 3http://www-odi.nhtsa.dot.gov/downloads/index.cfm4http://www-lab25.kuee.kyoto-u.ac.jp/nlresource/juman.html and http://www-lab25.kuee.kyoto u.ac.jp/nl-resource/knp.html 5http://www.csse.monash.edu.au/ jwb/edict doc.html6recall that we assume one-to-one correspondence between pivot in japanese and english.
</nextsent>
<nextsent>if japanese pivot word as more than one english translation, we select the translation for which the relative frequency in the target corpus is closest to the pivot in the source corpus.
</nextsent>
<nextsent>14 5.1 evaluation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH706">
<title id=" W11-0604.xml">a statistical test for grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and floor?
</prevsent>
<prevsent>are frequently attested in both child and adult speech from childes.
</prevsent>
</prevsection>
<citsent citstr=" P04-1002 ">
30 the computational linguistics literature has seenthe influence of usage-based approach: computational models have been proposed to proceed from an initial stage of lexicalized constructions toward more general grammatical system (felman 2004, steels 2004, <papid> P04-1002 </papid>cf.</citsent>
<aftsection>
<nextsent>wintner 2009).
</nextsent>
<nextsent>however, as far as we can tell, the evidence for an unproductive stage of grammar as discussed above was established onthe basis of intuition rather than rigorous assessments.
</nextsent>
<nextsent>we are not aware of statistical test against which the predictions of usage-based learning can be verified.
</nextsent>
<nextsent>nor are we of any demonstration thatthe child language data described above is inconsistent with the expectation of fully productive grammar, the position rejected in usage-based learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH707">
<title id=" W11-0604.xml">a statistical test for grammar </title>
<section> quantifying productivity.  </section>
<citcontext>
<prevsection>
<prevsent>i=1 i ) = 1 rhn , hn = n?
</prevsent>
<prevsent>i=1 1 (1)empirical tests show that zipfs law provides an excellent fit of word frequency distributions across languages and genres (baroni 2008).
</prevsent>
</prevsection>
<citsent citstr=" C02-1117 ">
it has been noted that the linguistic combinations such as n-grams show zipf-like power law distributions as well (teahna 1997, ha et al 2002), <papid> C02-1117 </papid>which contributes to the familiar sparse data problem in computational linguistics.</citsent>
<aftsection>
<nextsent>these observations generalize the combination of morphemes (chan 2008) and grammatical rules.
</nextsent>
<nextsent>figure 1 plots the ranks and frequencies syntactic rules (on log-log scale) from the penn treebank (marcus et al 1993); <papid> J93-2004 </papid>certain rules headed by specific functional words have been merged.</nextsent>
<nextsent>claims of usage-based learning build on the 31premise that linguistic productivity entails diversity of usage: the unevenness?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH708">
<title id=" W11-0604.xml">a statistical test for grammar </title>
<section> quantifying productivity.  </section>
<citcontext>
<prevsection>
<prevsent>it has been noted that the linguistic combinations such as n-grams show zipf-like power law distributions as well (teahna 1997, ha et al 2002), <papid> C02-1117 </papid>which contributes to the familiar sparse data problem in computational linguistics.</prevsent>
<prevsent>these observations generalize the combination of morphemes (chan 2008) and grammatical rules.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
figure 1 plots the ranks and frequencies syntactic rules (on log-log scale) from the penn treebank (marcus et al 1993); <papid> J93-2004 </papid>certain rules headed by specific functional words have been merged.</citsent>
<aftsection>
<nextsent>claims of usage-based learning build on the 31premise that linguistic productivity entails diversity of usage: the unevenness?
</nextsent>
<nextsent>in usage distribution such as the low overlap in d(eterminer)-n(oun)combinations is taken to be evidence against systematic grammar.
</nextsent>
<nextsent>paradoxically, however, valian etal.
</nextsent>
<nextsent>(2008) find that the d-n overlap values in moth ers?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH709">
<title id=" W11-0604.xml">a statistical test for grammar </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>a grammar based approach is supported (section 3.1) these results do not resolve the innate ness debate in language acquisition: they only point to the very early availability of an abstract and productive grammar.
</prevsent>
<prevsent>the simulation results on the inadequacy of the memory-and-retrieval approach to child language(section 3.2) show the limitations of lexically specific approach to language learning.
</prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
these results are congruent with the work in statistical parsing that also demonstrates the diminishing returns of lexicalization (gildea 2001, <papid> W01-0521 </papid>klein &amp; manning 2003, <papid> P03-1054 </papid>bikel2004).<papid> J04-4004 </papid></citsent>
<aftsection>
<nextsent>they are also consistent with previous statistical studies (buttery &amp; korhonen 2005) that child directed language data appear to be even more limited in syntactic usage diversity.
</nextsent>
<nextsent>the uneveness?
</nextsent>
<nextsent>inverb islands (tomasello 1992) is to be expected especially when the language sample is small as in the case of most child language acquisition studies.
</nextsent>
<nextsent>it thus seems necessary for the child learner to derive syntactic rules with overarching generality in relatively short period of time (and with few million utterances).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH710">
<title id=" W11-0604.xml">a statistical test for grammar </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>a grammar based approach is supported (section 3.1) these results do not resolve the innate ness debate in language acquisition: they only point to the very early availability of an abstract and productive grammar.
</prevsent>
<prevsent>the simulation results on the inadequacy of the memory-and-retrieval approach to child language(section 3.2) show the limitations of lexically specific approach to language learning.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
these results are congruent with the work in statistical parsing that also demonstrates the diminishing returns of lexicalization (gildea 2001, <papid> W01-0521 </papid>klein &amp; manning 2003, <papid> P03-1054 </papid>bikel2004).<papid> J04-4004 </papid></citsent>
<aftsection>
<nextsent>they are also consistent with previous statistical studies (buttery &amp; korhonen 2005) that child directed language data appear to be even more limited in syntactic usage diversity.
</nextsent>
<nextsent>the uneveness?
</nextsent>
<nextsent>inverb islands (tomasello 1992) is to be expected especially when the language sample is small as in the case of most child language acquisition studies.
</nextsent>
<nextsent>it thus seems necessary for the child learner to derive syntactic rules with overarching generality in relatively short period of time (and with few million utterances).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH711">
<title id=" W11-0604.xml">a statistical test for grammar </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>a grammar based approach is supported (section 3.1) these results do not resolve the innate ness debate in language acquisition: they only point to the very early availability of an abstract and productive grammar.
</prevsent>
<prevsent>the simulation results on the inadequacy of the memory-and-retrieval approach to child language(section 3.2) show the limitations of lexically specific approach to language learning.
</prevsent>
</prevsection>
<citsent citstr=" J04-4004 ">
these results are congruent with the work in statistical parsing that also demonstrates the diminishing returns of lexicalization (gildea 2001, <papid> W01-0521 </papid>klein &amp; manning 2003, <papid> P03-1054 </papid>bikel2004).<papid> J04-4004 </papid></citsent>
<aftsection>
<nextsent>they are also consistent with previous statistical studies (buttery &amp; korhonen 2005) that child directed language data appear to be even more limited in syntactic usage diversity.
</nextsent>
<nextsent>the uneveness?
</nextsent>
<nextsent>inverb islands (tomasello 1992) is to be expected especially when the language sample is small as in the case of most child language acquisition studies.
</nextsent>
<nextsent>it thus seems necessary for the child learner to derive syntactic rules with overarching generality in relatively short period of time (and with few million utterances).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH712">
<title id=" W11-0408.xml">reducing the need for double annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a corpus where each instance is annotated by single annotator unavoidably contains errors.
</prevsent>
<prevsent>to improve the quality of the data, onemay choose to annotate each instance twice and adjudicate the disagreements thus producing the gold standard.
</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
for example, the ontonotes (hovy et al , 2006) <papid> N06-2015 </papid>project opted for this approach.however, is it absolutely necessary to double annotate every example?</citsent>
<aftsection>
<nextsent>in this paper, we demonstrate that it is possible to double annotate only subset of the single annotated data and still achieve the same level of performance as with full double annotation.we accomplish this task by using the single annotated data to guide the selection of the instances to be double annotated.we propose several algorithms that accept single annotated data as input.
</nextsent>
<nextsent>the algorithms selecta subset of this data that they recommend for another round of annotation and adjudication.
</nextsent>
<nextsent>the single annotated data our algorithms work with can potentially come from any source.
</nextsent>
<nextsent>for example, it can be the single annotated output of active learning or the data that had been randomly sampled from some corpus and single annotated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH713">
<title id=" W11-0408.xml">reducing the need for double annotation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>active learning (settles, 2009; olsson, 2009) has been the traditional avenue for reducing the amount of annotation.
</prevsent>
<prevsent>however, in practice, serial active learning is difficult in multi-tagger environment (settles, 2009) when many annotators are working in parallel (e.g. ontonotes employs tens of taggers).
</prevsent>
</prevsection>
<citsent citstr=" N06-1016 ">
at the same time, several papers recently appeared that used ontonotes data for active learning experiments (chen et al , 2006; <papid> N06-1016 </papid>zhu, 2007; zhong et al ., 2008).<papid> D08-1105 </papid></citsent>
<aftsection>
<nextsent>these works all utilized ontonotes gold standard labels, which were obtained via double annotation and adjudication.
</nextsent>
<nextsent>the implicit assumption,therefore, was that the same process of double anno 65 tation and adjudication could be reproduced in the process of active learning.
</nextsent>
<nextsent>however, this assumption is not very realistic and in practice, these approaches may not bring about the kind of annotation cost reduction that they report.
</nextsent>
<nextsent>for example, an instance would have to be annotated by two taggers (and each disagreement adjudicated) on each iteration before the system can be retrained and the next instance selected.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH715">
<title id=" W11-0408.xml">reducing the need for double annotation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>active learning (settles, 2009; olsson, 2009) has been the traditional avenue for reducing the amount of annotation.
</prevsent>
<prevsent>however, in practice, serial active learning is difficult in multi-tagger environment (settles, 2009) when many annotators are working in parallel (e.g. ontonotes employs tens of taggers).
</prevsent>
</prevsection>
<citsent citstr=" D08-1105 ">
at the same time, several papers recently appeared that used ontonotes data for active learning experiments (chen et al , 2006; <papid> N06-1016 </papid>zhu, 2007; zhong et al ., 2008).<papid> D08-1105 </papid></citsent>
<aftsection>
<nextsent>these works all utilized ontonotes gold standard labels, which were obtained via double annotation and adjudication.
</nextsent>
<nextsent>the implicit assumption,therefore, was that the same process of double anno 65 tation and adjudication could be reproduced in the process of active learning.
</nextsent>
<nextsent>however, this assumption is not very realistic and in practice, these approaches may not bring about the kind of annotation cost reduction that they report.
</nextsent>
<nextsent>for example, an instance would have to be annotated by two taggers (and each disagreement adjudicated) on each iteration before the system can be retrained and the next instance selected.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH719">
<title id=" W11-0408.xml">reducing the need for double annotation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this should greatly simplify the application of our approach in real life annotation project.
</prevsent>
<prevsent>our work also borrows from the error detectionliterature.
</prevsent>
</prevsection>
<citsent citstr=" A00-2020 ">
researchers have explored error detection for manually tagged corpora in the context of pos-tagging (eskin, 2000; <papid> A00-2020 </papid>kveton?</citsent>
<aftsection>
<nextsent>and oliva, 2002; novak and razmova?, 2009), dependency parsing (dickinson, 2009), <papid> E09-1023 </papid>and text-classification (fukumoto and suzuki, 2004).<papid> C04-1125 </papid></nextsent>
<nextsent>the approaches to error detection include anomaly detection (eskin,2000), <papid> A00-2020 </papid>finding inconsistent annotations (van halteren, 2000; kveton?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH722">
<title id=" W11-0408.xml">reducing the need for double annotation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our work also borrows from the error detectionliterature.
</prevsent>
<prevsent>researchers have explored error detection for manually tagged corpora in the context of pos-tagging (eskin, 2000; <papid> A00-2020 </papid>kveton?</prevsent>
</prevsection>
<citsent citstr=" E09-1023 ">
and oliva, 2002; novak and razmova?, 2009), dependency parsing (dickinson, 2009), <papid> E09-1023 </papid>and text-classification (fukumoto and suzuki, 2004).<papid> C04-1125 </papid></citsent>
<aftsection>
<nextsent>the approaches to error detection include anomaly detection (eskin,2000), <papid> A00-2020 </papid>finding inconsistent annotations (van halteren, 2000; kveton?</nextsent>
<nextsent>and oliva, 2002; novak and razmova?, 2009), and using the weights assigned by learning algorithms such as boosting (abney et al ., 1999; luo et al , 2005) and svm (nakagawa and matsumoto, 2002; <papid> C02-1101 </papid>fukumoto and suzuki, 2004) <papid> C04-1125 </papid>by exploiting the fact that errors tend to concentrate among the examples with large weights.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH723">
<title id=" W11-0408.xml">reducing the need for double annotation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our work also borrows from the error detectionliterature.
</prevsent>
<prevsent>researchers have explored error detection for manually tagged corpora in the context of pos-tagging (eskin, 2000; <papid> A00-2020 </papid>kveton?</prevsent>
</prevsection>
<citsent citstr=" C04-1125 ">
and oliva, 2002; novak and razmova?, 2009), dependency parsing (dickinson, 2009), <papid> E09-1023 </papid>and text-classification (fukumoto and suzuki, 2004).<papid> C04-1125 </papid></citsent>
<aftsection>
<nextsent>the approaches to error detection include anomaly detection (eskin,2000), <papid> A00-2020 </papid>finding inconsistent annotations (van halteren, 2000; kveton?</nextsent>
<nextsent>and oliva, 2002; novak and razmova?, 2009), and using the weights assigned by learning algorithms such as boosting (abney et al ., 1999; luo et al , 2005) and svm (nakagawa and matsumoto, 2002; <papid> C02-1101 </papid>fukumoto and suzuki, 2004) <papid> C04-1125 </papid>by exploiting the fact that errors tend to concentrate among the examples with large weights.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH725">
<title id=" W11-0408.xml">reducing the need for double annotation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and oliva, 2002; novak and razmova?, 2009), dependency parsing (dickinson, 2009), <papid> E09-1023 </papid>and text-classification (fukumoto and suzuki, 2004).<papid> C04-1125 </papid></prevsent>
<prevsent>the approaches to error detection include anomaly detection (eskin,2000), <papid> A00-2020 </papid>finding inconsistent annotations (van halteren, 2000; kveton?</prevsent>
</prevsection>
<citsent citstr=" C02-1101 ">
and oliva, 2002; novak and razmova?, 2009), and using the weights assigned by learning algorithms such as boosting (abney et al ., 1999; luo et al , 2005) and svm (nakagawa and matsumoto, 2002; <papid> C02-1101 </papid>fukumoto and suzuki, 2004) <papid> C04-1125 </papid>by exploiting the fact that errors tend to concentrate among the examples with large weights.</citsent>
<aftsection>
<nextsent>some of these works eliminate the errors (luo et al , 2005).
</nextsent>
<nextsent>others correct them automatically (eskin, 2000; <papid> A00-2020 </papid>kveton?</nextsent>
<nextsent>and oliva, 2002; fukumoto and suzuki, 2004; <papid> C04-1125 </papid>dickinson, 2009) <papid> E09-1023 </papid>or manually (kveton?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH736">
<title id=" W11-0408.xml">reducing the need for double annotation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and oliva, 2002).
</prevsent>
<prevsent>several authors also demonstrate ensuing performance improvements (fukumoto and suzuki, 2004; <papid> C04-1125 </papid>luo et al , 2005; dickinson, 2009).<papid> E09-1023 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
all of these researchers experimented with single annotated data such as penn treebank (marcus et al , 1993) <papid> J93-2004 </papid>and they were often unable to hand-examineall the data their algorithms marked as errors be cause of the large size of their data sets.</citsent>
<aftsection>
<nextsent>instead, to demonstrate the effectiveness of their approaches,they examined selected subset of the detected examples (e.g.
</nextsent>
<nextsent>(abney et al , 1999; eskin, 2000; <papid> A00-2020 </papid>nakagawa and matsumoto, 2002; <papid> C02-1101 </papid>novak and razmova?,2009)).</nextsent>
<nextsent>in this paper, we experiment with fully double annotated and adjudicated data, which allows us to evaluate the effectiveness of our approach moreprecisely.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH741">
<title id=" W11-0408.xml">reducing the need for double annotation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(abney et al , 1999; eskin, 2000; <papid> A00-2020 </papid>nakagawa and matsumoto, 2002; <papid> C02-1101 </papid>novak and razmova?,2009)).</prevsent>
<prevsent>in this paper, we experiment with fully double annotated and adjudicated data, which allows us to evaluate the effectiveness of our approach moreprecisely.</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
a sizable body of work exists on using noisy labeling obtained from low-cost annotation services such as amazons mechanical turk (snow et al , 2008; <papid> D08-1027 </papid>sheng et al , 2008; hsueh etal., 2009).<papid> W09-1904 </papid></citsent>
<aftsection>
<nextsent>hsueh et al  (2009) <papid> W09-1904 </papid>identify several criteria for selecting high-quality annotations such asnoise level, sentiment ambiguity, and lexical uncer tainty.</nextsent>
<nextsent>(sheng et al , 2008) address the relationships between various repeated labeling strategies and the quality of the resulting models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH742">
<title id=" W11-0408.xml">reducing the need for double annotation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(abney et al , 1999; eskin, 2000; <papid> A00-2020 </papid>nakagawa and matsumoto, 2002; <papid> C02-1101 </papid>novak and razmova?,2009)).</prevsent>
<prevsent>in this paper, we experiment with fully double annotated and adjudicated data, which allows us to evaluate the effectiveness of our approach moreprecisely.</prevsent>
</prevsection>
<citsent citstr=" W09-1904 ">
a sizable body of work exists on using noisy labeling obtained from low-cost annotation services such as amazons mechanical turk (snow et al , 2008; <papid> D08-1027 </papid>sheng et al , 2008; hsueh etal., 2009).<papid> W09-1904 </papid></citsent>
<aftsection>
<nextsent>hsueh et al  (2009) <papid> W09-1904 </papid>identify several criteria for selecting high-quality annotations such asnoise level, sentiment ambiguity, and lexical uncer tainty.</nextsent>
<nextsent>(sheng et al , 2008) address the relationships between various repeated labeling strategies and the quality of the resulting models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH744">
<title id=" W11-0408.xml">reducing the need for double annotation </title>
<section> algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>this algorithm operates by training discriminative classifier and making prediction for each instance in the pool.
</prevsent>
<prevsent>whenever this prediction disagrees with the human-assigned label, the instance is selected for repeated labeling.for classification we choose support vector machine (svm) classifier because we need high accuracy classifier.
</prevsent>
</prevsection>
<citsent citstr=" P08-2008 ">
the state-of-the art system we use for our experiments is svm-based (dligach and palmer, 2008).<papid> P08-2008 </papid></citsent>
<aftsection>
<nextsent>the specific classification software we utilize is libsvm (chang and lin, 2001).
</nextsent>
<nextsent>we accept the default settings (c = 1 and linear ker nel).
</nextsent>
<nextsent>3.3 ambiguity detector algorithm.
</nextsent>
<nextsent>the ambiguity detector algorithm trains probabilistic classifier and makes prediction for each instance in the pool.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH749">
<title id=" W11-0214.xml">from pathways to biomolecular events opportunities and challenges </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as further practical contribution, we introduce the first pathway-to-event conversion software for sbml/celldesigner pathways and discuss the opportunities arising from the ability to convert the substantial existing pathway resources to events.
</prevsent>
<prevsent>for most of the previous decade of biomedical information extraction (ie), efforts have focused on foundational tasks such as named entity detection and their database normalization (krallinger et al,2008) and simple ie targets, most commonly binary entity relations representing associations such as protein-protein interactions (pyysalo et al, 2008; tikk et al, 2010).
</prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
in recent years, an increasing number of resources and methods pursuing more detailed representations of extracted information are becoming available (pyysalo et al, 2007; kim et al, 2008; thompson et al, 2009; bjorne et al, 2010).the main thrust of this move toward structured, fine grained information extraction falls under the heading of event extraction (ananiadou et al, 2010), an approach popularized and represented in particular by the bionlp shared task (bionlp st) (kim et al., 2009<papid> W09-1401 </papid>a; kim et al, 2011).<papid> W11-1801 </papid>while detailed representation of extracted information on biomolecular events has several potential applications ranging from semantic search to database cur ation support (ananiadou et al, 2010), the number of practical applications making use ofthis technology has arguably so far been rather limited.</citsent>
<aftsection>
<nextsent>in this study, we pursue in particular the opportunities that event extraction holds for pathway annotation support,1 arguing that the match between 1throughout this paper, we call the projected task pathway annotation support.
</nextsent>
<nextsent>there is no established task with this label, and we do not envision this to be specific single task.
</nextsent>
<nextsent>rather, we intend the term to refer to set of tasks where informationextraction/text mining methods are applied in some role to contribute directly to pathway cur ation, including, for example, the identification of specific texts in the literature relevant to annotated reactions, the automatic suggestion of further entities or reactions to add to pathway, or even the fully automatic generation of entire pathways from scratch.
</nextsent>
<nextsent>105representations that biologists employ to capture reactions between bio molecules in pathways and the event representation of the bionlp st task makespathway-oriented applications potential killer ap plication?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH750">
<title id=" W11-0214.xml">from pathways to biomolecular events opportunities and challenges </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as further practical contribution, we introduce the first pathway-to-event conversion software for sbml/celldesigner pathways and discuss the opportunities arising from the ability to convert the substantial existing pathway resources to events.
</prevsent>
<prevsent>for most of the previous decade of biomedical information extraction (ie), efforts have focused on foundational tasks such as named entity detection and their database normalization (krallinger et al,2008) and simple ie targets, most commonly binary entity relations representing associations such as protein-protein interactions (pyysalo et al, 2008; tikk et al, 2010).
</prevsent>
</prevsection>
<citsent citstr=" W11-1801 ">
in recent years, an increasing number of resources and methods pursuing more detailed representations of extracted information are becoming available (pyysalo et al, 2007; kim et al, 2008; thompson et al, 2009; bjorne et al, 2010).the main thrust of this move toward structured, fine grained information extraction falls under the heading of event extraction (ananiadou et al, 2010), an approach popularized and represented in particular by the bionlp shared task (bionlp st) (kim et al., 2009<papid> W09-1401 </papid>a; kim et al, 2011).<papid> W11-1801 </papid>while detailed representation of extracted information on biomolecular events has several potential applications ranging from semantic search to database cur ation support (ananiadou et al, 2010), the number of practical applications making use ofthis technology has arguably so far been rather limited.</citsent>
<aftsection>
<nextsent>in this study, we pursue in particular the opportunities that event extraction holds for pathway annotation support,1 arguing that the match between 1throughout this paper, we call the projected task pathway annotation support.
</nextsent>
<nextsent>there is no established task with this label, and we do not envision this to be specific single task.
</nextsent>
<nextsent>rather, we intend the term to refer to set of tasks where informationextraction/text mining methods are applied in some role to contribute directly to pathway cur ation, including, for example, the identification of specific texts in the literature relevant to annotated reactions, the automatic suggestion of further entities or reactions to add to pathway, or even the fully automatic generation of entire pathways from scratch.
</nextsent>
<nextsent>105representations that biologists employ to capture reactions between bio molecules in pathways and the event representation of the bionlp st task makespathway-oriented applications potential killer ap plication?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH758">
<title id=" W11-0214.xml">from pathways to biomolecular events opportunities and challenges </title>
<section> analysis of pathway-event mapping.  </section>
<citcontext>
<prevsection>
<prevsent>the same holds(with somewhat less specificity) for genia inorganic compound.
</prevsent>
<prevsent>finally, although annotated in genia, the category of protein complexes has no correspondence among the entities considered in the bionlp st representation.
</prevsent>
</prevsection>
<citsent citstr=" W09-1313 ">
thus, information extraction systems applying the core bionlp st entity types will entirely lack coverage for protein complexes and will not be able 6while the term protein appears to suggest that the class consists only of protein forms, these entities are in fact annotated in the bionlp st data according to the genia gene/gene product guidelines (ohta et al, 2009) <papid> W09-1313 </papid>and thus include also dna and rna forms.</citsent>
<aftsection>
<nextsent>the type could arguably more accurately be named gene or gene product.
</nextsent>
<nextsent>108 pathway event cell designer biopax st09 st11 genia state transition biochemical reaction (see table 3) truncation biochemical reaction catabolism catabolism catabolism transcription biochemical reaction transcription transcription transcription translation biochemical reaction - - translation association complex assembly binding binding binding dissociation complex assembly - - transport transport w/reaction localization localization localization degradation degradation catabolism catabolism catabolism catalysis catalysis positive regulation positive regulation positive regulation physical stimulation control positive regulation positive regulation positive regulation modulation control regulation regulation regulation trigger control positive regulation positive regulation positive regulation inhibition control negative regulation negative regulation negative regulation table 2: reaction type comparison between pathways and events.
</nextsent>
<nextsent>to fully resolve the detailed type of gene and gene product types applied in the pathway representations.
</nextsent>
<nextsent>while these distinctions exist in the full genia corpus, it has not been frequently applied inevent extraction in its complete form and is unlikely to be adopted over the widely applied stresources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH762">
<title id=" W11-0214.xml">from pathways to biomolecular events opportunities and challenges </title>
<section> protein association and dissociation.  </section>
<citcontext>
<prevsection>
<prevsent>for event annotation, we adapted the genia event corpus annotation guidelines (kim et al, 2008), further developing specific representation and guidelines for annotating dissociation events based on an early iteration of exploratory annotation.
</prevsent>
<prevsent>annotation was performed by single biology phd with extensive experience in event annotation(to).
</prevsent>
</prevsection>
<citsent citstr=" W11-1804 ">
while we could thus not directly assess inter annotator consistency, we note that our recent comparable efforts have been evaluated by comparing independently created annotations at approximately90% f-score for entity annotations and approximately 80% f-score for event annotations (bionlp shared task primary evaluation criteria) (pyysalo et al., 2011; <papid> W11-1804 </papid>ohta et al, 2011).<papid> W11-1803 </papid></citsent>
<aftsection>
<nextsent>4.3 representing association and dissociation.
</nextsent>
<nextsent>based on our analysis of 107 protein dissociation statements annotated in the corpus and corresponding study of the reverse?, statements of protein association in the corpus, we propose the following extensions for the bionlp st event representation.first, the introduction of the event type dissociation, taking as its primary argument single theme identifying participating entity of the type complex.
</nextsent>
<nextsent>second, we propose the new role type product, in the annotation of dissociation events an optional (secondary) argument identifying the protein entities that are released in the dissociationevent.
</nextsent>
<nextsent>this argument should be annotated (or ex tracted) only when explicitly stated in text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH763">
<title id=" W11-0214.xml">from pathways to biomolecular events opportunities and challenges </title>
<section> protein association and dissociation.  </section>
<citcontext>
<prevsection>
<prevsent>for event annotation, we adapted the genia event corpus annotation guidelines (kim et al, 2008), further developing specific representation and guidelines for annotating dissociation events based on an early iteration of exploratory annotation.
</prevsent>
<prevsent>annotation was performed by single biology phd with extensive experience in event annotation(to).
</prevsent>
</prevsection>
<citsent citstr=" W11-1803 ">
while we could thus not directly assess inter annotator consistency, we note that our recent comparable efforts have been evaluated by comparing independently created annotations at approximately90% f-score for entity annotations and approximately 80% f-score for event annotations (bionlp shared task primary evaluation criteria) (pyysalo et al., 2011; <papid> W11-1804 </papid>ohta et al, 2011).<papid> W11-1803 </papid></citsent>
<aftsection>
<nextsent>4.3 representing association and dissociation.
</nextsent>
<nextsent>based on our analysis of 107 protein dissociation statements annotated in the corpus and corresponding study of the reverse?, statements of protein association in the corpus, we propose the following extensions for the bionlp st event representation.first, the introduction of the event type dissociation, taking as its primary argument single theme identifying participating entity of the type complex.
</nextsent>
<nextsent>second, we propose the new role type product, in the annotation of dissociation events an optional (secondary) argument identifying the protein entities that are released in the dissociationevent.
</nextsent>
<nextsent>this argument should be annotated (or ex tracted) only when explicitly stated in text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH764">
<title id=" W11-0214.xml">from pathways to biomolecular events opportunities and challenges </title>
<section> pathway-to-event conversion.  </section>
<citcontext>
<prevsection>
<prevsent>(bjorne et al, 2010)) for specific pathway reactions.
</prevsent>
<prevsent>for pathways where reactions are marked with literature references, conversion further allows event annotations relevant to specific documents to be created automatically, sparing manual annotation costs.
</prevsent>
</prevsection>
<citsent citstr=" P09-1113 ">
while such event annotations will not be bound to specific text expressions, they could be used through the application of techniques such as distant supervision (mintz et al, 2009).<papid> P09-1113 </papid></citsent>
<aftsection>
<nextsent>as first attempt, the conversion introduced in this work is limited in number of ways, but we hope it can serve as starting point for both wider adoption of pathway resources for event extraction and further research into accurate conversions between the two.
</nextsent>
<nextsent>the conversion software, sbml-to-event, is freely available for research purposes.
</nextsent>
<nextsent>over the last decade, the bio-community has invested enormous efforts in the construction of detailed models of the function of large variety of biological systems in the form of pathways.
</nextsent>
<nextsent>these efforts toward building systemic understanding of the functioning of organisms remain central focus ofpresent-day biology, and their support through information extraction and text mining perhaps the greatest potential contribution that the biomedical natural language processing community could make toward the broader bio-community.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH765">
<title id=" W11-0811.xml">a machine learning approach to relational noun mining in german </title>
<section> mining relational nouns: almost mwe.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast to verbs, how ever, complements of nouns are almost exclusively optional.
</prevsent>
<prevsent>the identification of relational nouns is of great importance for variety of content-oriented applications: first, precise hpsg parsing for german can not really be achieved, if high number of noun complements is systematically analysed as modifiers.
</prevsent>
</prevsection>
<citsent citstr=" W04-2705 ">
second, recent extension of semantic role labeling to the argument structure of nouns (meyers et al, 2004) <papid> W04-2705 </papid>increases the interest in lexicographic methods for the extraction of noun subcategorisation information.</citsent>
<aftsection>
<nextsent>third, relational nouns are alsoa valuable resource for machine translation, separating the more semantic task of translating modifying prepositions from the more syntactic task of translating sub categorised for prepositions.
</nextsent>
<nextsent>despite its relevance for accurate deep parsing, the german hpsg grammar developed at dfki (muller and kasper, 2000; crysmann, 2003; crysmann, 2005) currently only includes 107 entries for proposition taking nouns, and lacks entries for pp-taking nouns entirely.in terms of subcategorisation properties, relational nouns in german can be divided up into 3 classes:?
</nextsent>
<nextsent>nouns taking genitival complements (e.g., be ginn der vorlesung beginning of the lecture?, zerstorung der stadt destruction of the city?
</nextsent>
<nextsent>nouns taking propositional complements, either complementiser-introduced finite clause (der glaube, da?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH766">
<title id=" W11-0811.xml">a machine learning approach to relational noun mining in german </title>
<section> mining relational nouns: almost mwe.  </section>
<citcontext>
<prevsection>
<prevsent>owing to the lack of alternation, the preposition by itself does not compositionally contribute to sentence meaning, its only function being the encoding of thematic property of the noun.
</prevsent>
<prevsent>thus, in syntacto-semantic terms, we are again dealing with prototypical mwes.the fact that pp complements of nouns, like modifiers, are syntactically optional, together with the fact that their surface form is indistinguishable from adjunct pps, makes the extraction task far from trivial.
</prevsent>
</prevsection>
<citsent citstr=" W08-1708 ">
it is clear that grammar-based error mining techniques (van noord, 2004; cholakov et al, 2008) <papid> W08-1708 </papid>that have been highly successful in other areas ofdeep lexical acquisition (e.g., verb subcategorisation) cannot be applied here: first, given that an alternative analysis as modifier is readily available in the grammar, missing entries for relational noun swill never incur any coverage problems.</citsent>
<aftsection>
<nextsent>further more, since pp modifiers are highly common we cannot expect decrease in tree probability either.
</nextsent>
<nextsent>instead, shall exploit the mwe-like properties of relational nouns, building on the expectation that the presence of subcategorisation requirement towards fixed, albeit optional, prepositional head should leave trace infrequency distributions.
</nextsent>
<nextsent>thus, building on previous work in mwe extraction, shall pursue data-driven approach that builds on variety of association metrics combined in probabilistic classifier.
</nextsent>
<nextsent>despite the difference of the task, da?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH767">
<title id=" W11-0811.xml">a machine learning approach to relational noun mining in german </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>the approach suggested here shares some significant similarity to previous classifier-based approaches to mwe (pecina, 2008).
</prevsent>
<prevsent>2.1 data preparation.
</prevsent>
</prevsection>
<citsent citstr=" E06-2001 ">
as primary data for relational noun extraction, used the dewac corpus (baroni and kilgariff, 2006), <papid> E06-2001 </papid>1.6 billion token corpus of german crawled from the web.</citsent>
<aftsection>
<nextsent>the corpus is automatically tagged and lemmatised by tree tagger (schmid, 1995).
</nextsent>
<nextsent>from this corpus, extracted all noun (nn) and preposition (appr) unigrams and noun preposition bi grams.
</nextsent>
<nextsent>noun unigrams occuring less than ten times in the entire corpus were subsequently removed.
</nextsent>
<nextsent>in addition to the removal of hapaxes, also filtered out any abbreviations.frequency counts were lemma-based, decision that was motivated by the intended application, namely mining of relational noun entries for lemma-based hpsg lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH768">
<title id=" W11-0811.xml">a machine learning approach to relational noun mining in german </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for nbtree, our best-performing classifier, we subsequently carried out number of experiments to assess the influence and predictive power of individual association measures and to study their interactions.
</prevsent>
<prevsent>67 essentially, we make use of two basic types offeatures: string features, like the form of the preposition or the prefixes and suffixes of the noun, and association measures.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
as for the latter, we drew on the set of measures successfully used in previous studies on collocation extraction:mutual information (mi) an information theoretic measure proposed by (church and hanks, 1990) <papid> J90-1003 </papid>which measures the joint probability of the bigram in relation to the product of the marginal probabilities, i.e., the expected proba bility.</citsent>
<aftsection>
<nextsent>mi = p(noun, prep) p(noun) ? p(prep)mi2 squared variant of mutal information, previously suggested by (daille, 1994).
</nextsent>
<nextsent>essentially, the idea behind squaring the joint probability is to counter the negative effect of extremely low marginal probabilities yielding high mi scores.
</nextsent>
<nextsent>mi2 = (p(noun, prep))2 p(noun) ? p(prep)likelihood ratios measure suggested by (dun ning, 1993) <papid> J93-1003 </papid>that indicates how much more likely the co occurence is than mere coinci dence.</nextsent>
<nextsent>lr = logl(pi, k1, n1) + logl(p2, k2, n2) ? logl(p, k1, n1) ? logl(p, k2, n2) where logl(p, n, k) = log p+ (n? k) log(1 ? p) and p1 = k1 n1 , p2 = k2 n2 , = k1 + k2 n1 + n2 t-score the score of fishers t-test.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH769">
<title id=" W11-0811.xml">a machine learning approach to relational noun mining in german </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>mi = p(noun, prep) p(noun) ? p(prep)mi2 squared variant of mutal information, previously suggested by (daille, 1994).
</prevsent>
<prevsent>essentially, the idea behind squaring the joint probability is to counter the negative effect of extremely low marginal probabilities yielding high mi scores.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
mi2 = (p(noun, prep))2 p(noun) ? p(prep)likelihood ratios measure suggested by (dun ning, 1993) <papid> J93-1003 </papid>that indicates how much more likely the co occurence is than mere coinci dence.</citsent>
<aftsection>
<nextsent>lr = logl(pi, k1, n1) + logl(p2, k2, n2) ? logl(p, k1, n1) ? logl(p, k2, n2) where logl(p, n, k) = log p+ (n? k) log(1 ? p) and p1 = k1 n1 , p2 = k2 n2 , = k1 + k2 n1 + n2 t-score the score of fishers t-test.
</nextsent>
<nextsent>although the underlying assumption regarding normal distribution is incorrect (church and mercer, 1993),<papid> J93-1001 </papid>the score has nevertheless been used with repeated success in collocation extraction tasks (krenn, 2000; krenn and evert, 2001; evert and krenn, 2001).<papid> P01-1025 </papid></nextsent>
<nextsent>tscore = p(noun, prep) ?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH770">
<title id=" W11-0811.xml">a machine learning approach to relational noun mining in german </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>mi2 = (p(noun, prep))2 p(noun) ? p(prep)likelihood ratios measure suggested by (dun ning, 1993) <papid> J93-1003 </papid>that indicates how much more likely the co occurence is than mere coinci dence.</prevsent>
<prevsent>lr = logl(pi, k1, n1) + logl(p2, k2, n2) ? logl(p, k1, n1) ? logl(p, k2, n2) where logl(p, n, k) = log p+ (n? k) log(1 ? p) and p1 = k1 n1 , p2 = k2 n2 , = k1 + k2 n1 + n2 t-score the score of fishers t-test.</prevsent>
</prevsection>
<citsent citstr=" J93-1001 ">
although the underlying assumption regarding normal distribution is incorrect (church and mercer, 1993),<papid> J93-1001 </papid>the score has nevertheless been used with repeated success in collocation extraction tasks (krenn, 2000; krenn and evert, 2001; evert and krenn, 2001).<papid> P01-1025 </papid></citsent>
<aftsection>
<nextsent>tscore = p(noun, prep) ?
</nextsent>
<nextsent>(p(noun) ? p(prep)) ? 2 as suggested by (manning and schutze, 1999) we use as an approximation of 2.
</nextsent>
<nextsent>association strength (smadja, 1993)<papid> J93-1007 </papid>a factor indicating how many times the standard deviation bigram frequency differs from the average.</nextsent>
<nextsent>strength = freqi ? f? best indicates whether bigram is the most frequent one for the given noun or not.best-ratio relative version of the previous feature indicating the frequency ratio between the current noun preposition bigram and the best bigram for the given noun.in addition to the for,m of the preposition, we included information about the nouns suffixes or pre fixes: noun suffix we included common string suffixes that may be clues as to the relational nature ofthe noun, as, e.g., the common derviational suffixes -ion, -schaft, -heit, -keit as well as the endings -en, which are found inter alia with nom inal ised infiniti ves, and -er, which are found,inter alia with agentive nominals.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH771">
<title id=" W11-0811.xml">a machine learning approach to relational noun mining in german </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>mi2 = (p(noun, prep))2 p(noun) ? p(prep)likelihood ratios measure suggested by (dun ning, 1993) <papid> J93-1003 </papid>that indicates how much more likely the co occurence is than mere coinci dence.</prevsent>
<prevsent>lr = logl(pi, k1, n1) + logl(p2, k2, n2) ? logl(p, k1, n1) ? logl(p, k2, n2) where logl(p, n, k) = log p+ (n? k) log(1 ? p) and p1 = k1 n1 , p2 = k2 n2 , = k1 + k2 n1 + n2 t-score the score of fishers t-test.</prevsent>
</prevsection>
<citsent citstr=" P01-1025 ">
although the underlying assumption regarding normal distribution is incorrect (church and mercer, 1993),<papid> J93-1001 </papid>the score has nevertheless been used with repeated success in collocation extraction tasks (krenn, 2000; krenn and evert, 2001; evert and krenn, 2001).<papid> P01-1025 </papid></citsent>
<aftsection>
<nextsent>tscore = p(noun, prep) ?
</nextsent>
<nextsent>(p(noun) ? p(prep)) ? 2 as suggested by (manning and schutze, 1999) we use as an approximation of 2.
</nextsent>
<nextsent>association strength (smadja, 1993)<papid> J93-1007 </papid>a factor indicating how many times the standard deviation bigram frequency differs from the average.</nextsent>
<nextsent>strength = freqi ? f? best indicates whether bigram is the most frequent one for the given noun or not.best-ratio relative version of the previous feature indicating the frequency ratio between the current noun preposition bigram and the best bigram for the given noun.in addition to the for,m of the preposition, we included information about the nouns suffixes or pre fixes: noun suffix we included common string suffixes that may be clues as to the relational nature ofthe noun, as, e.g., the common derviational suffixes -ion, -schaft, -heit, -keit as well as the endings -en, which are found inter alia with nom inal ised infiniti ves, and -er, which are found,inter alia with agentive nominals.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH772">
<title id=" W11-0811.xml">a machine learning approach to relational noun mining in german </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>tscore = p(noun, prep) ?
</prevsent>
<prevsent>(p(noun) ? p(prep)) ? 2 as suggested by (manning and schutze, 1999) we use as an approximation of 2.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
association strength (smadja, 1993)<papid> J93-1007 </papid>a factor indicating how many times the standard deviation bigram frequency differs from the average.</citsent>
<aftsection>
<nextsent>strength = freqi ? f? best indicates whether bigram is the most frequent one for the given noun or not.best-ratio relative version of the previous feature indicating the frequency ratio between the current noun preposition bigram and the best bigram for the given noun.in addition to the for,m of the preposition, we included information about the nouns suffixes or pre fixes: noun suffix we included common string suffixes that may be clues as to the relational nature ofthe noun, as, e.g., the common derviational suffixes -ion, -schaft, -heit, -keit as well as the endings -en, which are found inter alia with nom inal ised infiniti ves, and -er, which are found,inter alia with agentive nominals.
</nextsent>
<nextsent>all other suffixes were mapped to the none class.
</nextsent>
<nextsent>noun prefix included were prefixes that commonly appear as verb prefixes.
</nextsent>
<nextsent>again, this was used as shortcut for true lexical relatedness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH774">
<title id=" W11-1715.xml">mining subjective knowledge from customer reviews a specific case of irony detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one ofthe communicative phenomena which better represents this problem is irony.
</prevsent>
<prevsent>according to wilson and sperber (2007), irony is essentially communicative act which expresses an opposite meaning of what was literally said.due to irony is common in texts that express subjective and deeply-felt opinions, its presence represents significant obstacle to the accurate analysis of sentiment in such texts (cf.
</prevsent>
</prevsection>
<citsent citstr=" W10-3110 ">
councill et al (2010)).<papid> W10-3110 </papid></citsent>
<aftsection>
<nextsent>in this research work we aim at gathering set of discriminating elements to represent irony.
</nextsent>
<nextsent>in particular, we focus on analyzing set of customer reviews (posted on the basis of an online viral effect) in order to obtain set of key components to face the task of irony detection.this paper is organized as follows.
</nextsent>
<nextsent>section 2 introduces the theoretical problem of irony.
</nextsent>
<nextsent>section 3 presents the related work as well as the evaluationcorpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH775">
<title id=" W11-1715.xml">mining subjective knowledge from customer reviews a specific case of irony detection </title>
<section> pragmatic theories of irony.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, grice (1975) con 118 siders that an utterance is ironic if it intentionally violates some conversational maxims.
</prevsent>
<prevsent>wilson and sperber (2007) assume that verbal irony must be understood as echoic; i.e. as distinction between use and mention.
</prevsent>
</prevsection>
<citsent citstr=" C96-2162 ">
utsumi (1996), <papid> C96-2162 </papid>in contrast, suggests an ironic environment which causes negative emotional attitude.</citsent>
<aftsection>
<nextsent>according to these points of view, the elements to conceive verbal expression as ironic point to different ways of explaining the same underlying concept of opposition, but specially note, however, that most of them relyon literary studies (attardo, 2007); thus, their computational formalization is quite challenging.
</nextsent>
<nextsent>further more, consider that people have their own concept of irony, which often does not match with the rules suggested by the experts.
</nextsent>
<nextsent>for instance, consider the following expressions retrieved from the web: 1.
</nextsent>
<nextsent>if you find it hard to laugh at yourself, would be happy to do it for you.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH777">
<title id=" W11-1715.xml">mining subjective knowledge from customer reviews a specific case of irony detection </title>
<section> approaching irony detection.  </section>
<citcontext>
<prevsection>
<prevsent>further more, there are others approaches which are focused on particular devices such as sarcasm and satire,rather than on the whole concept of irony.
</prevsent>
<prevsent>for instance, tsur et al (2010) and davidov et al (2010) address the problem of finding linguistic elements that mark the use of sarcasm in online product reviews and tweets, respectively.
</prevsent>
</prevsection>
<citsent citstr=" P09-2041 ">
finally, burfoot and baldwin (2009) <papid> P09-2041 </papid>explore the task of automatic satire 119 detection by evaluating features related to headline elements, offensive language and slang.</citsent>
<aftsection>
<nextsent>3.1 evaluation corpus.
</nextsent>
<nextsent>due to the scarce work on automatic irony processing, and to the intrinsic features of irony, it is quite difficult and subjective to obtain corpus with ironic data.
</nextsent>
<nextsent>therefore, we decided to relyon the wisdom of the crowd and use collection of customer reviews from the amazon web site.
</nextsent>
<nextsent>these reviews are considered as ironic by customers, as well as bymany journalists, both in mass and social media.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH778">
<title id=" W11-1715.xml">mining subjective knowledge from customer reviews a specific case of irony detection </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>in order to represent this category, we selected some of the best humor features reported in the literature: stylistic features, human centered ness, and keyness.
</prevsent>
<prevsent>the stylistic features, according to the experiments reported in (mihalcea and strapparava, 2006), were obtained by collecting all the words labeled with the tag sexuality?
</prevsent>
</prevsection>
<citsent citstr=" W04-2214 ">
in wordnet domains (bentivogli etal., 2004).<papid> W04-2214 </papid></citsent>
<aftsection>
<nextsent>the second feature focuses on social relationships.
</nextsent>
<nextsent>in order to retrieve these words, the elements registered in wordnet (miller, 1995), which belong to the synsets relation, relationship and relative, were retrieved.
</nextsent>
<nextsent>the last feature is represented by obtaining the keyness value of the words (cf.
</nextsent>
<nextsent>(reyes et al, 2009)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH779">
<title id=" W11-0413.xml">subjectivity and sentiment annotation of modern standard arabic newswire </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>throughout the paper,we discuss expression of subjectivity in natural language, combining various previously scattered insights belonging to many branches of linguistics.
</prevsent>
<prevsent>as the volume of web data continues to phenomenally increase, researchers are becoming more interested in mining that data and making the information therein accessible to end-users in various innovative ways.
</prevsent>
</prevsection>
<citsent citstr=" W09-3012 ">
as result, searches and processing of data beyond the limiting level of surface words are becoming increasingly important (diab et al,2009).<papid> W09-3012 </papid></citsent>
<aftsection>
<nextsent>the sentiment expressed in web data specifically continues to be of high interest and value to internet users, businesses, and governmental bodies.
</nextsent>
<nextsent>thus, the area of subjectivity and sentiment analysis (ssa) has been witnessing flurry of novel research.
</nextsent>
<nextsent>subjectivity in natural language refers to aspects of language used to express opinions, feelings, evaluations, and speculations (banfield, 1982; wiebe,1994) <papid> J94-2004 </papid>and it, thus, incorporates sentiment.</nextsent>
<nextsent>the process of subjectivity classification refers to the task of classifying texts into either objective (e.g., more than 1000 tourists have visited tahrir square, in downtown cairo, last week.)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH780">
<title id=" W11-0413.xml">subjectivity and sentiment annotation of modern standard arabic newswire </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the sentiment expressed in web data specifically continues to be of high interest and value to internet users, businesses, and governmental bodies.
</prevsent>
<prevsent>thus, the area of subjectivity and sentiment analysis (ssa) has been witnessing flurry of novel research.
</prevsent>
</prevsection>
<citsent citstr=" J94-2004 ">
subjectivity in natural language refers to aspects of language used to express opinions, feelings, evaluations, and speculations (banfield, 1982; wiebe,1994) <papid> J94-2004 </papid>and it, thus, incorporates sentiment.</citsent>
<aftsection>
<nextsent>the process of subjectivity classification refers to the task of classifying texts into either objective (e.g., more than 1000 tourists have visited tahrir square, in downtown cairo, last week.)
</nextsent>
<nextsent>or subjective.
</nextsent>
<nextsent>subjective text is further classified with sentiment or polarity.
</nextsent>
<nextsent>for sentiment classification, the task refers to identifying whether subjective text is positive(e.g., the egyptian revolution was really impres sive!), negative (e.g., the bloodbaths that took placein tripoli were horrifying!), neutral (e.g., the company may release the software next month.), and, sometimes, mixed (e.g., really like this labtop, butit is prohibitively expensive.).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH781">
<title id=" W11-0413.xml">subjectivity and sentiment annotation of modern standard arabic newswire </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for sentiment classification, the task refers to identifying whether subjective text is positive(e.g., the egyptian revolution was really impres sive!), negative (e.g., the bloodbaths that took placein tripoli were horrifying!), neutral (e.g., the company may release the software next month.), and, sometimes, mixed (e.g., really like this labtop, butit is prohibitively expensive.).
</prevsent>
<prevsent>ssa sometimes incorporates identifying the holder(s), target(s), and strength (e.g., low, medium, high) of the expressed sentiment.
</prevsent>
</prevsection>
<citsent citstr=" W10-1401 ">
inspite of the great interest in ssa, only few studies have been conducted on morphologically rich languages (mrl) (i.e., languages in which significant information concerning syntactic units and relations are expressed at the word-level (tsarfaty et al, 2010)).<papid> W10-1401 </papid></citsent>
<aftsection>
<nextsent>arabic, hebrew, turkish, czech, and basque are examples of mrls.
</nextsent>
<nextsent>ssa work on mrls has been hampered by lack of annotated data.
</nextsent>
<nextsent>in the current paper we report efforts to manually annotate corpus of modern standard arabic (msa), morphologically-rich variety of arabic, e.g., (diab et al, 2007; habash et al, 2009).
</nextsent>
<nextsent>the corpus is collection of documents from the newswire genre covering several domains such as politics and sports.we label the data at the sentence level.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH782">
<title id=" W11-0413.xml">subjectivity and sentiment annotation of modern standard arabic newswire </title>
<section> subjectivity and sentiment annotation.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 ssa categories.
</prevsent>
<prevsent>for each sentence, each annotator assigned one of 4possible labels: (1) objective (obj), (2) subjective positive (s-pos), (3) subjective-negative (s-neg), and (4) subjective-neutral (s-neut).
</prevsent>
</prevsection>
<citsent citstr=" P99-1032 ">
we followed(wiebe et al, 1999) <papid> P99-1032 </papid>in ope rationalizing the subjective vs. the objective categories.</citsent>
<aftsection>
<nextsent>in other words, if the primary goal of sentence is perceived to be the objective reporting of information, it was labeled obj.
</nextsent>
<nextsent>otherwise, sentence would be candidate for one of the three subjective classes.2 table 1 shows the contingency table for the two annotators judgments.
</nextsent>
<nextsent>overall agreement is 88.06%, with kappa (k) value of 0.38.to illustrate, sentence such as the prime minister announced that he will visit the city, saying that he will be glad to see the injured?, has two authors (the story writer and the prime minister indirectly quoted).
</nextsent>
<nextsent>accordingly to our guidelines, this sentence should be annotated s-pos tag since thepart related to the person quoted (the prime minis 2it is worth noting that even though some ssa researchers include subjective mixed categories, we only saw such categories attested in less than   0.005% which is expected since our granularity level is the sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH783">
<title id=" W11-0413.xml">subjectivity and sentiment annotation of modern standard arabic newswire </title>
<section> subjectivity and sentiment annotation.  </section>
<citcontext>
<prevsection>
<prevsent>thus, bad news andgood news can be obj as is the case in both examples.
</prevsent>
<prevsent>3.3 perspective.
</prevsent>
</prevsection>
<citsent citstr=" W06-2915 ">
some sentences are written from certain perspective (lin et al, 2006) <papid> W06-2915 </papid>or point of view.</citsent>
<aftsection>
<nextsent>consider the two sentences (1) israeli soldiers, our heroes, are keen on protecting settlers?
</nextsent>
<nextsent>and (2) palestinian freedom fighters are willing to attack these israelitargets?.
</nextsent>
<nextsent>sentence (1) is written from an israeli perspective, while sentence (2) is written from palestinian perspective.
</nextsent>
<nextsent>the perspective from which sentence is written inter plays with how sentiment is assigned.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH785">
<title id=" W11-0302.xml">the effect of automatic tokenization vocal ization stemming and  </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we define word to be whit espace delimited unit and token to be (part of) word that has syntactic function.
</prevsent>
<prevsent>for example, the word wsytzwjha (?????????)(english: and he will marry her) consists of 4 tokens: conjunction w, future marker s, verb inflected for the singular masculine in the perfect ive form ytzwj, and feminine singular 3rd person object pronoun.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
parsing such word requires tokenization, and performing dependency parsing in the tradition of the conll-x (buchholz and marsi, 2006) <papid> W06-2920 </papid>and conll 2007 shared task (nivre et al  2007) <papid> D07-1096 </papid>also requires part of speech tagging, lemmatization, linguistic features, and vocal ization, all of which were in the human annotated gold standard form in the shared task.</citsent>
<aftsection>
<nextsent>the current study aims at measuring the effect of pipeline of non gold standard tokenization, lemmatization, vocal ization, linguistic features and pos tagging on the quality of arabic dependency parsing.
</nextsent>
<nextsent>we only assume that we have gold standard sentence boundaries since we do not agree with the sentence boundaries in the data, and introducing our own will have complicating effect on evaluation.
</nextsent>
<nextsent>the conll shared tasks of 2006 and 2007 used gold standard components in all fields, which is not realistic for arabic, or for any other language.
</nextsent>
<nextsent>for arabic and other morphologically rich languages, it may be more unrealistic than it is for english, for example, since the conll 2007 arabic dataset has tokens, rather than white space delimited words, as entries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH786">
<title id=" W11-0302.xml">the effect of automatic tokenization vocal ization stemming and  </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we define word to be whit espace delimited unit and token to be (part of) word that has syntactic function.
</prevsent>
<prevsent>for example, the word wsytzwjha (?????????)(english: and he will marry her) consists of 4 tokens: conjunction w, future marker s, verb inflected for the singular masculine in the perfect ive form ytzwj, and feminine singular 3rd person object pronoun.
</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
parsing such word requires tokenization, and performing dependency parsing in the tradition of the conll-x (buchholz and marsi, 2006) <papid> W06-2920 </papid>and conll 2007 shared task (nivre et al  2007) <papid> D07-1096 </papid>also requires part of speech tagging, lemmatization, linguistic features, and vocal ization, all of which were in the human annotated gold standard form in the shared task.</citsent>
<aftsection>
<nextsent>the current study aims at measuring the effect of pipeline of non gold standard tokenization, lemmatization, vocal ization, linguistic features and pos tagging on the quality of arabic dependency parsing.
</nextsent>
<nextsent>we only assume that we have gold standard sentence boundaries since we do not agree with the sentence boundaries in the data, and introducing our own will have complicating effect on evaluation.
</nextsent>
<nextsent>the conll shared tasks of 2006 and 2007 used gold standard components in all fields, which is not realistic for arabic, or for any other language.
</nextsent>
<nextsent>for arabic and other morphologically rich languages, it may be more unrealistic than it is for english, for example, since the conll 2007 arabic dataset has tokens, rather than white space delimited words, as entries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH787">
<title id=" W11-0302.xml">the effect of automatic tokenization vocal ization stemming and  </title>
<section> related studies.  </section>
<citcontext>
<prevsection>
<prevsent>in conll-x (buchholz and 10 marsi, 2006), the average labeled attachment score on arabic across all results presented by the 19 participating teams was 59.9% with standard deviation of 6.5.
</prevsent>
<prevsent>the best results were obtained by mcdonald et al (2006) with score of 66.9% followed by nivre et al (2006) with 66.7%.
</prevsent>
</prevsection>
<citsent citstr=" D07-1097 ">
the best results on arabic in the conll 2007 shared task were obtained by hallet al (2007) <papid> D07-1097 </papid>as they obtained labeled attachment score of 76.52%, 9.6 percentage points above the highest score of the 2006 shared task.</citsent>
<aftsection>
<nextsent>hallet al used an ensemble system, based on the malt parser dependency parser that extrapolates from single malt parser system.
</nextsent>
<nextsent>the settings with the single malt parser led to labeled accuracy score of 74.75% on arabic.
</nextsent>
<nextsent>the single malt parser is the one used in the current paper.
</nextsent>
<nextsent>all the papers in both shared tasks used gold standard tokenization, vocal ization, lemmatization, pos tags, and linguistic features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH788">
<title id=" W11-0302.xml">the effect of automatic tokenization vocal ization stemming and  </title>
<section> related studies.  </section>
<citcontext>
<prevsection>
<prevsent>the single malt parser is the one used in the current paper.
</prevsent>
<prevsent>all the papers in both shared tasks used gold standard tokenization, vocal ization, lemmatization, pos tags, and linguistic features.
</prevsent>
</prevsection>
<citsent citstr=" W10-1402 ">
a more recent study is that by marton et al  (2010).<papid> W10-1402 </papid></citsent>
<aftsection>
<nextsent>although marton et al varied the pos distribution and linguistic features, they still used gold standard tokenization.
</nextsent>
<nextsent>they also used the columbia arabic treebank, which makes both the methods and data different from those presented here.
</nextsent>
<nextsent>3.1.data the data used for the current study is the same dataset used for the conll (2007) shared task, with the same division into training set, and test set.
</nextsent>
<nextsent>this design helps in comparing results in way that enables us to measure the effect of automatic pre-processing on parsing accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH789">
<title id=" W11-0302.xml">the effect of automatic tokenization vocal ization stemming and  </title>
<section> data, methods, and evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>linguistic features are an unordered set of syntactic and/or morphological features, separated by vertical bar (|), or an underscore if not available.
</prevsent>
<prevsent>the features in the conll 2007 arabic dataset represent case, mood, definite ness, voice, number, gender and person.
</prevsent>
</prevsection>
<citsent citstr=" W04-1602 ">
the data used for training the stemmer/tokenizer is taken from the arabic treebank (maamouri and bies, 2004).<papid> W04-1602 </papid></citsent>
<aftsection>
<nextsent>care has been taken not to use the parts of the atb that are also used in the prague arabic dependency treebank (haijc et al 2004) since the padt and the atb share material.
</nextsent>
<nextsent>3.2.
</nextsent>
<nextsent>methods.
</nextsent>
<nextsent>we implement pipeline as follows (1) we build memory-based word segmenter using timbl (daelemans et al  2007) which treats segmentation as per letter classification in which each word segment is delimited by + sign whether it is syntactic or inflectional.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH791">
<title id=" W11-0145.xml">extracting contextual evaluativity </title>
<section> data gathering and annotation.  </section>
<citcontext>
<prevsection>
<prevsent>these together suggest that such constructions could be simultaneously high-precision sources for evaluativity inference and easily obtainable from large corpora.
</prevsent>
<prevsent>we developed corpus of assessment-grounding excerpts from documents across the web to evaluate the potential of the framework in 2.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
73 positive and 120 negative assessment predicates (like, adore, hate, loathe, etc.) were selected from the mpqa subjectivity lexicon (wilson et al, 2005).<papid> H05-1044 </papid></citsent>
<aftsection>
<nextsent>these were expanded accross inflectional variants to produce 826 assessment templates, half with explicit because, half without (e.g. terrified by because he).
</nextsent>
<nextsent>these templates were filled with personal pronouns and the names of 26 prominent political figures and issued as web search queries to the yahoo!
</nextsent>
<nextsent>search api.2 total of 440,000 web document results were downloaded and processed using an 1152 core sun microsystems blade cluster.
</nextsent>
<nextsent>the relevant sentences from each document were extracted, and those under 80 characters in length were parsed using the stanford dependency parser.3 this produced 60,000 parsed assessment-grounding sentences, 6,000 of which (excluding duplicates) passed the additional criterion that the grounding clause should contain verb with direct object.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH792">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>further, by improving the method for integrating swsd into contextual opinion analysis, even greater benefits from swsd are achieved than in previous work.
</prevsent>
<prevsent>we thus more firmly demonstrate the potential of swsd to improve contextual opinion analysis.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
often, methods for opinion, sentiment, and subjectivity analysis relyon lexicons of subjective (opinion-carrying) words (e.g., (turney, 2002; <papid> P02-1053 </papid>whitelaw et al, 2005; riloff and wiebe, 2003; <papid> W03-1014 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>kim and hovy, 2004; <papid> C04-1200 </papid>bloom et al, 2007; <papid> N07-1039 </papid>andreevskaia and bergler, 2008; <papid> P08-1034 </papid>agarwal et al, 2009)).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>examples of such words are the following (in bold): (1) he is disease to every team he has gone to.
</nextsent>
<nextsent>converting to smf is headache.
</nextsent>
<nextsent>the concert left me cold.
</nextsent>
<nextsent>that guy is such pain.however, even manually developed subjectivity lexicons have significant degrees of subjectivity sense ambiguity (su and markert, 2008; <papid> C08-1104 </papid>gyamfi et al., 2009).<papid> N09-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH793">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>further, by improving the method for integrating swsd into contextual opinion analysis, even greater benefits from swsd are achieved than in previous work.
</prevsent>
<prevsent>we thus more firmly demonstrate the potential of swsd to improve contextual opinion analysis.
</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
often, methods for opinion, sentiment, and subjectivity analysis relyon lexicons of subjective (opinion-carrying) words (e.g., (turney, 2002; <papid> P02-1053 </papid>whitelaw et al, 2005; riloff and wiebe, 2003; <papid> W03-1014 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>kim and hovy, 2004; <papid> C04-1200 </papid>bloom et al, 2007; <papid> N07-1039 </papid>andreevskaia and bergler, 2008; <papid> P08-1034 </papid>agarwal et al, 2009)).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>examples of such words are the following (in bold): (1) he is disease to every team he has gone to.
</nextsent>
<nextsent>converting to smf is headache.
</nextsent>
<nextsent>the concert left me cold.
</nextsent>
<nextsent>that guy is such pain.however, even manually developed subjectivity lexicons have significant degrees of subjectivity sense ambiguity (su and markert, 2008; <papid> C08-1104 </papid>gyamfi et al., 2009).<papid> N09-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH794">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>further, by improving the method for integrating swsd into contextual opinion analysis, even greater benefits from swsd are achieved than in previous work.
</prevsent>
<prevsent>we thus more firmly demonstrate the potential of swsd to improve contextual opinion analysis.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
often, methods for opinion, sentiment, and subjectivity analysis relyon lexicons of subjective (opinion-carrying) words (e.g., (turney, 2002; <papid> P02-1053 </papid>whitelaw et al, 2005; riloff and wiebe, 2003; <papid> W03-1014 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>kim and hovy, 2004; <papid> C04-1200 </papid>bloom et al, 2007; <papid> N07-1039 </papid>andreevskaia and bergler, 2008; <papid> P08-1034 </papid>agarwal et al, 2009)).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>examples of such words are the following (in bold): (1) he is disease to every team he has gone to.
</nextsent>
<nextsent>converting to smf is headache.
</nextsent>
<nextsent>the concert left me cold.
</nextsent>
<nextsent>that guy is such pain.however, even manually developed subjectivity lexicons have significant degrees of subjectivity sense ambiguity (su and markert, 2008; <papid> C08-1104 </papid>gyamfi et al., 2009).<papid> N09-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH795">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>further, by improving the method for integrating swsd into contextual opinion analysis, even greater benefits from swsd are achieved than in previous work.
</prevsent>
<prevsent>we thus more firmly demonstrate the potential of swsd to improve contextual opinion analysis.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
often, methods for opinion, sentiment, and subjectivity analysis relyon lexicons of subjective (opinion-carrying) words (e.g., (turney, 2002; <papid> P02-1053 </papid>whitelaw et al, 2005; riloff and wiebe, 2003; <papid> W03-1014 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>kim and hovy, 2004; <papid> C04-1200 </papid>bloom et al, 2007; <papid> N07-1039 </papid>andreevskaia and bergler, 2008; <papid> P08-1034 </papid>agarwal et al, 2009)).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>examples of such words are the following (in bold): (1) he is disease to every team he has gone to.
</nextsent>
<nextsent>converting to smf is headache.
</nextsent>
<nextsent>the concert left me cold.
</nextsent>
<nextsent>that guy is such pain.however, even manually developed subjectivity lexicons have significant degrees of subjectivity sense ambiguity (su and markert, 2008; <papid> C08-1104 </papid>gyamfi et al., 2009).<papid> N09-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH796">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>further, by improving the method for integrating swsd into contextual opinion analysis, even greater benefits from swsd are achieved than in previous work.
</prevsent>
<prevsent>we thus more firmly demonstrate the potential of swsd to improve contextual opinion analysis.
</prevsent>
</prevsection>
<citsent citstr=" N07-1039 ">
often, methods for opinion, sentiment, and subjectivity analysis relyon lexicons of subjective (opinion-carrying) words (e.g., (turney, 2002; <papid> P02-1053 </papid>whitelaw et al, 2005; riloff and wiebe, 2003; <papid> W03-1014 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>kim and hovy, 2004; <papid> C04-1200 </papid>bloom et al, 2007; <papid> N07-1039 </papid>andreevskaia and bergler, 2008; <papid> P08-1034 </papid>agarwal et al, 2009)).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>examples of such words are the following (in bold): (1) he is disease to every team he has gone to.
</nextsent>
<nextsent>converting to smf is headache.
</nextsent>
<nextsent>the concert left me cold.
</nextsent>
<nextsent>that guy is such pain.however, even manually developed subjectivity lexicons have significant degrees of subjectivity sense ambiguity (su and markert, 2008; <papid> C08-1104 </papid>gyamfi et al., 2009).<papid> N09-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH797">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>further, by improving the method for integrating swsd into contextual opinion analysis, even greater benefits from swsd are achieved than in previous work.
</prevsent>
<prevsent>we thus more firmly demonstrate the potential of swsd to improve contextual opinion analysis.
</prevsent>
</prevsection>
<citsent citstr=" P08-1034 ">
often, methods for opinion, sentiment, and subjectivity analysis relyon lexicons of subjective (opinion-carrying) words (e.g., (turney, 2002; <papid> P02-1053 </papid>whitelaw et al, 2005; riloff and wiebe, 2003; <papid> W03-1014 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>kim and hovy, 2004; <papid> C04-1200 </papid>bloom et al, 2007; <papid> N07-1039 </papid>andreevskaia and bergler, 2008; <papid> P08-1034 </papid>agarwal et al, 2009)).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>examples of such words are the following (in bold): (1) he is disease to every team he has gone to.
</nextsent>
<nextsent>converting to smf is headache.
</nextsent>
<nextsent>the concert left me cold.
</nextsent>
<nextsent>that guy is such pain.however, even manually developed subjectivity lexicons have significant degrees of subjectivity sense ambiguity (su and markert, 2008; <papid> C08-1104 </papid>gyamfi et al., 2009).<papid> N09-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH798">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>further, by improving the method for integrating swsd into contextual opinion analysis, even greater benefits from swsd are achieved than in previous work.
</prevsent>
<prevsent>we thus more firmly demonstrate the potential of swsd to improve contextual opinion analysis.
</prevsent>
</prevsection>
<citsent citstr=" E09-1004 ">
often, methods for opinion, sentiment, and subjectivity analysis relyon lexicons of subjective (opinion-carrying) words (e.g., (turney, 2002; <papid> P02-1053 </papid>whitelaw et al, 2005; riloff and wiebe, 2003; <papid> W03-1014 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>kim and hovy, 2004; <papid> C04-1200 </papid>bloom et al, 2007; <papid> N07-1039 </papid>andreevskaia and bergler, 2008; <papid> P08-1034 </papid>agarwal et al, 2009)).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>examples of such words are the following (in bold): (1) he is disease to every team he has gone to.
</nextsent>
<nextsent>converting to smf is headache.
</nextsent>
<nextsent>the concert left me cold.
</nextsent>
<nextsent>that guy is such pain.however, even manually developed subjectivity lexicons have significant degrees of subjectivity sense ambiguity (su and markert, 2008; <papid> C08-1104 </papid>gyamfi et al., 2009).<papid> N09-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH799">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>converting to smf is headache.
</prevsent>
<prevsent>the concert left me cold.
</prevsent>
</prevsection>
<citsent citstr=" C08-1104 ">
that guy is such pain.however, even manually developed subjectivity lexicons have significant degrees of subjectivity sense ambiguity (su and markert, 2008; <papid> C08-1104 </papid>gyamfi et al., 2009).<papid> N09-1002 </papid></citsent>
<aftsection>
<nextsent>that is, many clues in these lexicons have both subjective and objective senses.
</nextsent>
<nextsent>this ambiguity leads to errors in opinion and sentiment analysis, because objective instances represent false hits of subjectivity clues.
</nextsent>
<nextsent>for example, the following sentence contains the keywords from (1) used with objective senses: (2) early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting.
</nextsent>
<nextsent>recently, in (akkaya et al, 2009), <papid> D09-1020 </papid>we introduced the task of subjectivity word sense disambiguation (swsd), which is to automatically determine which word instances in corpus are being used with subjective senses, and which are being used with objective senses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH800">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>converting to smf is headache.
</prevsent>
<prevsent>the concert left me cold.
</prevsent>
</prevsection>
<citsent citstr=" N09-1002 ">
that guy is such pain.however, even manually developed subjectivity lexicons have significant degrees of subjectivity sense ambiguity (su and markert, 2008; <papid> C08-1104 </papid>gyamfi et al., 2009).<papid> N09-1002 </papid></citsent>
<aftsection>
<nextsent>that is, many clues in these lexicons have both subjective and objective senses.
</nextsent>
<nextsent>this ambiguity leads to errors in opinion and sentiment analysis, because objective instances represent false hits of subjectivity clues.
</nextsent>
<nextsent>for example, the following sentence contains the keywords from (1) used with objective senses: (2) early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting.
</nextsent>
<nextsent>recently, in (akkaya et al, 2009), <papid> D09-1020 </papid>we introduced the task of subjectivity word sense disambiguation (swsd), which is to automatically determine which word instances in corpus are being used with subjective senses, and which are being used with objective senses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH801">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this ambiguity leads to errors in opinion and sentiment analysis, because objective instances represent false hits of subjectivity clues.
</prevsent>
<prevsent>for example, the following sentence contains the keywords from (1) used with objective senses: (2) early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting.
</prevsent>
</prevsection>
<citsent citstr=" D09-1020 ">
recently, in (akkaya et al, 2009), <papid> D09-1020 </papid>we introduced the task of subjectivity word sense disambiguation (swsd), which is to automatically determine which word instances in corpus are being used with subjective senses, and which are being used with objective senses.</citsent>
<aftsection>
<nextsent>we developed supervised system for swsd, and exploited the swsd output to improve the performance of multiple contextual opinion analysis tasks.
</nextsent>
<nextsent>although the reported results are promising, there are three obvious shortcomings.
</nextsent>
<nextsent>first, we were able to apply swsd to contextual opinion analysis onlyon very small scale, due to shortage of annotated data.
</nextsent>
<nextsent>while the experiments show that swsd improves contextual opinion analysis, this was only on the small amount of opinion-annotated data that was in the coverage of our system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH825">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> subjectivity word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, we explore better methods for applying swsd to contextual opinion analysis.
</prevsent>
<prevsent>2.1 annotation tasks.
</prevsent>
</prevsection>
<citsent citstr=" P06-1134 ">
we adopt the definitions of subjective (s) and objective (o) from (wiebe et al, 2005; wiebe and mihalcea, 2006; <papid> P06-1134 </papid>wilson, 2007).</citsent>
<aftsection>
<nextsent>subjective expressions are words and phrases being used to express ment aland emotional states, such as speculations, evaluations, sentiments, and beliefs.
</nextsent>
<nextsent>a general covering term for such states is private state (quirk et al,1985), an internal state that cannot be directly observed or verified by others.
</nextsent>
<nextsent>objective expressions instead are words and phrases that lack subjectivity.the contextual opinion analysis experiments described in section 3 include both s/o and polarity (positive,negative, neutral) classifications.
</nextsent>
<nextsent>the opinion-annotated data used in those experiments is from the mpqa corpus (wiebe et al, 2005; wilson, 2007),1 which consists of news articles annotated for subjective expressions, including polarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH890">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> subjectivity word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1: sense sets for target word attack?
</prevsent>
<prevsent>(abridged).
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
of the subjectivity lexicon of (wilson et al, 2005; <papid> H05-1044 </papid>wilson, 2007).3 there are 39 such words.</citsent>
<aftsection>
<nextsent>(akkaya et al, 2009) <papid> D09-1020 </papid>chose words from subjectivity lexicon because such words are known to have subjective usages.</nextsent>
<nextsent>for this paper, subjectivity sense-tagged data was obtained from the mturk workers using the annotation scheme of (akkaya et al, 2010).<papid> W10-0731 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH911">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> subjectivity word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>of the subjectivity lexicon of (wilson et al, 2005; <papid> H05-1044 </papid>wilson, 2007).3 there are 39 such words.</prevsent>
<prevsent>(akkaya et al, 2009) <papid> D09-1020 </papid>chose words from subjectivity lexicon because such words are known to have subjective usages.</prevsent>
</prevsection>
<citsent citstr=" W10-0731 ">
for this paper, subjectivity sense-tagged data was obtained from the mturk workers using the annotation scheme of (akkaya et al, 2010).<papid> W10-0731 </papid></citsent>
<aftsection>
<nextsent>a goal is to keep the annotation task as simple as possible.
</nextsent>
<nextsent>thus, the workers are not directly asked if the instance of target word has subjective or an objective sense,because the concept of subjectivity would be difficult to explain in this setting.
</nextsent>
<nextsent>instead the workers are shown two sets of senses ? one subjective set and one objective set ? for specific target word and text passage in which the target word appears.
</nextsent>
<nextsent>their job is to select the set that best reflects the meaning of the target word in the text passage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1254">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this indicates that non-expert mturk annotation scan replace expert annotations for our end-goal ? improving contextual opinion analysis ? while reducing time and cost requirements by large margin.
</prevsent>
<prevsent>moreover, we see that the improvements in (akkaya et al, 2009) <papid> D09-1020 </papid>scale up to new subjectivity clues.</prevsent>
</prevsection>
<citsent citstr=" E06-1027 ">
one related line of research is to automatically assign subjectivity and/or polarity labels to word senses in dictionary (valitutti et al, 2004; andreevskaia and bergler, 2006; <papid> E06-1027 </papid>wiebe and mihalcea, 2006; <papid> P06-1134 </papid>esuli and sebastiani, 2007; <papid> P07-1054 </papid>su and markert,2009).<papid> N09-1001 </papid></citsent>
<aftsection>
<nextsent>in contrast, the task in our paper is to automatically assign labels to word instances in corpus.
</nextsent>
<nextsent>recently, some researchers have exploited full word sense disambiguation in methods for opinion related tasks.
</nextsent>
<nextsent>for example, (martn-wanton et al,2010) exploit wsd for recognizing quotation polarities, and (rentoumi et al, 2009; martn-wanton etal., 2010) exploit wsd for recognizing headline polarities.
</nextsent>
<nextsent>none of this previous work investigates performing coarse-grained variation of wsd such as swsd to improve their application results, as we do in this work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1256">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this indicates that non-expert mturk annotation scan replace expert annotations for our end-goal ? improving contextual opinion analysis ? while reducing time and cost requirements by large margin.
</prevsent>
<prevsent>moreover, we see that the improvements in (akkaya et al, 2009) <papid> D09-1020 </papid>scale up to new subjectivity clues.</prevsent>
</prevsection>
<citsent citstr=" P07-1054 ">
one related line of research is to automatically assign subjectivity and/or polarity labels to word senses in dictionary (valitutti et al, 2004; andreevskaia and bergler, 2006; <papid> E06-1027 </papid>wiebe and mihalcea, 2006; <papid> P06-1134 </papid>esuli and sebastiani, 2007; <papid> P07-1054 </papid>su and markert,2009).<papid> N09-1001 </papid></citsent>
<aftsection>
<nextsent>in contrast, the task in our paper is to automatically assign labels to word instances in corpus.
</nextsent>
<nextsent>recently, some researchers have exploited full word sense disambiguation in methods for opinion related tasks.
</nextsent>
<nextsent>for example, (martn-wanton et al,2010) exploit wsd for recognizing quotation polarities, and (rentoumi et al, 2009; martn-wanton etal., 2010) exploit wsd for recognizing headline polarities.
</nextsent>
<nextsent>none of this previous work investigates performing coarse-grained variation of wsd such as swsd to improve their application results, as we do in this work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1257">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this indicates that non-expert mturk annotation scan replace expert annotations for our end-goal ? improving contextual opinion analysis ? while reducing time and cost requirements by large margin.
</prevsent>
<prevsent>moreover, we see that the improvements in (akkaya et al, 2009) <papid> D09-1020 </papid>scale up to new subjectivity clues.</prevsent>
</prevsection>
<citsent citstr=" N09-1001 ">
one related line of research is to automatically assign subjectivity and/or polarity labels to word senses in dictionary (valitutti et al, 2004; andreevskaia and bergler, 2006; <papid> E06-1027 </papid>wiebe and mihalcea, 2006; <papid> P06-1134 </papid>esuli and sebastiani, 2007; <papid> P07-1054 </papid>su and markert,2009).<papid> N09-1001 </papid></citsent>
<aftsection>
<nextsent>in contrast, the task in our paper is to automatically assign labels to word instances in corpus.
</nextsent>
<nextsent>recently, some researchers have exploited full word sense disambiguation in methods for opinion related tasks.
</nextsent>
<nextsent>for example, (martn-wanton et al,2010) exploit wsd for recognizing quotation polarities, and (rentoumi et al, 2009; martn-wanton etal., 2010) exploit wsd for recognizing headline polarities.
</nextsent>
<nextsent>none of this previous work investigates performing coarse-grained variation of wsd such as swsd to improve their application results, as we do in this work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1258">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, (martn-wanton et al,2010) exploit wsd for recognizing quotation polarities, and (rentoumi et al, 2009; martn-wanton etal., 2010) exploit wsd for recognizing headline polarities.
</prevsent>
<prevsent>none of this previous work investigates performing coarse-grained variation of wsd such as swsd to improve their application results, as we do in this work.
</prevsent>
</prevsection>
<citsent citstr=" N10-1054 ">
a notable exception is (su and markert, 2010), <papid> N10-1054 </papid>who exploit swsd to improve the performance on contextual nlp task, as we do.</citsent>
<aftsection>
<nextsent>while the taskin our paper is subjectivity and sentiment analysis, their task is english-chinese lexical substitution.
</nextsent>
<nextsent>as (akkaya et al, 2009) <papid> D09-1020 </papid>did, they annotated word senses, and exploited senseval dataas training data for swsd.</nextsent>
<nextsent>they did not directly annotate words in context with s/o labels, as we do in our work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1279">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as (akkaya et al, 2009) <papid> D09-1020 </papid>did, they annotated word senses, and exploited senseval dataas training data for swsd.</prevsent>
<prevsent>they did not directly annotate words in context with s/o labels, as we do in our work.</prevsent>
</prevsection>
<citsent citstr=" W04-2807 ">
further, they did not separately evaluate swsd system component.many researchers work on reducing the granular ity of sense inventories for wsd (e.g., (palmer et al, 2004; <papid> W04-2807 </papid>navigli, 2006; <papid> P06-1014 </papid>snow et al, 2007; <papid> D07-1107 </papid>hovy et al,2006)).<papid> N06-2015 </papid></citsent>
<aftsection>
<nextsent>their criteria for grouping senses are syntactic and semantic similarities, while the groupings in work on swsd are driven by the goals to improve contextual subjectivity and sentiment analysis.
</nextsent>
<nextsent>in this paper, we utilized large pool of non-expertannotators (mturk) to collect subjectivity sense tagged data for swsd.
</nextsent>
<nextsent>we showed that non-expert annotations are as good as expert annotations for training swsd classifiers.
</nextsent>
<nextsent>moreover, we demonstrated that swsd classifiers trained on non-expert annotations can be exploited to improve contextual opinion analysis.the additional subjectivity sense-tagged data enabled us to evaluate the benefits of swsd on contextual opinion analysis on corpus of opinion annotated data that is five times larger.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1280">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as (akkaya et al, 2009) <papid> D09-1020 </papid>did, they annotated word senses, and exploited senseval dataas training data for swsd.</prevsent>
<prevsent>they did not directly annotate words in context with s/o labels, as we do in our work.</prevsent>
</prevsection>
<citsent citstr=" P06-1014 ">
further, they did not separately evaluate swsd system component.many researchers work on reducing the granular ity of sense inventories for wsd (e.g., (palmer et al, 2004; <papid> W04-2807 </papid>navigli, 2006; <papid> P06-1014 </papid>snow et al, 2007; <papid> D07-1107 </papid>hovy et al,2006)).<papid> N06-2015 </papid></citsent>
<aftsection>
<nextsent>their criteria for grouping senses are syntactic and semantic similarities, while the groupings in work on swsd are driven by the goals to improve contextual subjectivity and sentiment analysis.
</nextsent>
<nextsent>in this paper, we utilized large pool of non-expertannotators (mturk) to collect subjectivity sense tagged data for swsd.
</nextsent>
<nextsent>we showed that non-expert annotations are as good as expert annotations for training swsd classifiers.
</nextsent>
<nextsent>moreover, we demonstrated that swsd classifiers trained on non-expert annotations can be exploited to improve contextual opinion analysis.the additional subjectivity sense-tagged data enabled us to evaluate the benefits of swsd on contextual opinion analysis on corpus of opinion annotated data that is five times larger.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1281">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as (akkaya et al, 2009) <papid> D09-1020 </papid>did, they annotated word senses, and exploited senseval dataas training data for swsd.</prevsent>
<prevsent>they did not directly annotate words in context with s/o labels, as we do in our work.</prevsent>
</prevsection>
<citsent citstr=" D07-1107 ">
further, they did not separately evaluate swsd system component.many researchers work on reducing the granular ity of sense inventories for wsd (e.g., (palmer et al, 2004; <papid> W04-2807 </papid>navigli, 2006; <papid> P06-1014 </papid>snow et al, 2007; <papid> D07-1107 </papid>hovy et al,2006)).<papid> N06-2015 </papid></citsent>
<aftsection>
<nextsent>their criteria for grouping senses are syntactic and semantic similarities, while the groupings in work on swsd are driven by the goals to improve contextual subjectivity and sentiment analysis.
</nextsent>
<nextsent>in this paper, we utilized large pool of non-expertannotators (mturk) to collect subjectivity sense tagged data for swsd.
</nextsent>
<nextsent>we showed that non-expert annotations are as good as expert annotations for training swsd classifiers.
</nextsent>
<nextsent>moreover, we demonstrated that swsd classifiers trained on non-expert annotations can be exploited to improve contextual opinion analysis.the additional subjectivity sense-tagged data enabled us to evaluate the benefits of swsd on contextual opinion analysis on corpus of opinion annotated data that is five times larger.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1282">
<title id=" W11-0311.xml">improving the impact of subjectivity word sense disambiguation on contextual opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as (akkaya et al, 2009) <papid> D09-1020 </papid>did, they annotated word senses, and exploited senseval dataas training data for swsd.</prevsent>
<prevsent>they did not directly annotate words in context with s/o labels, as we do in our work.</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
further, they did not separately evaluate swsd system component.many researchers work on reducing the granular ity of sense inventories for wsd (e.g., (palmer et al, 2004; <papid> W04-2807 </papid>navigli, 2006; <papid> P06-1014 </papid>snow et al, 2007; <papid> D07-1107 </papid>hovy et al,2006)).<papid> N06-2015 </papid></citsent>
<aftsection>
<nextsent>their criteria for grouping senses are syntactic and semantic similarities, while the groupings in work on swsd are driven by the goals to improve contextual subjectivity and sentiment analysis.
</nextsent>
<nextsent>in this paper, we utilized large pool of non-expertannotators (mturk) to collect subjectivity sense tagged data for swsd.
</nextsent>
<nextsent>we showed that non-expert annotations are as good as expert annotations for training swsd classifiers.
</nextsent>
<nextsent>moreover, we demonstrated that swsd classifiers trained on non-expert annotations can be exploited to improve contextual opinion analysis.the additional subjectivity sense-tagged data enabled us to evaluate the benefits of swsd on contextual opinion analysis on corpus of opinion annotated data that is five times larger.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1284">
<title id=" W11-0108.xml">modular graph rewriting to compute semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one may observe that our syntactic representation of sentences involves plain graphs and not trees.
</prevsent>
<prevsent>indeed, these relations can give rise to multiple governors and dependency cycles.
</prevsent>
</prevsection>
<citsent citstr=" E09-1001 ">
on the semantic side, 65 we have also chosen graphs, which are widely used in different formalisms and theories, such as dmrs (copestake (2009)) <papid> E09-1001 </papid>or mtt (melcuk (1988)) . the principles being fixed, our problem was then to choose model of computation well suited to transforming syntactic graphs into semantic graphs.</citsent>
<aftsection>
<nextsent>the ?-calculus, which is widely used informal semantics, is not good candidate because it is appropriate for computing on trees but not on graphs.
</nextsent>
<nextsent>our choice naturally went to graph rewriting.
</nextsent>
<nextsent>graph rewriting is barely used in computational linguistics; it could be due to the difficulty to manage large sets of rules.
</nextsent>
<nextsent>among the pioneers in the use of graph rewriting, we mention hyvonen (1984); <papid> P84-1110 </papid>bohnet and wanner (2001); <papid> W01-0807 </papid>crouch (2005); jijkoun and de rijke (2007); <papid> W07-0208 </papid>bedaride and gardent (2009); <papid> W09-3744 </papid>chaumartin and kahane (2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1285">
<title id=" W11-0108.xml">modular graph rewriting to compute semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our choice naturally went to graph rewriting.
</prevsent>
<prevsent>graph rewriting is barely used in computational linguistics; it could be due to the difficulty to manage large sets of rules.
</prevsent>
</prevsection>
<citsent citstr=" P84-1110 ">
among the pioneers in the use of graph rewriting, we mention hyvonen (1984); <papid> P84-1110 </papid>bohnet and wanner (2001); <papid> W01-0807 </papid>crouch (2005); jijkoun and de rijke (2007); <papid> W07-0208 </papid>bedaride and gardent (2009); <papid> W09-3744 </papid>chaumartin and kahane (2010).</citsent>
<aftsection>
<nextsent>a graph rewriting system is defined as set of graph rewrite rules and computation is sequence of rewrite rule applications to given graph.
</nextsent>
<nextsent>the application of rule is triggered via mechanism of pattern matching, hence sub-graph is isolated from its context and the result is local modification of the input.
</nextsent>
<nextsent>this allows linguistic phenomenon to be easily isolated for applying transformation.
</nextsent>
<nextsent>since each step of computation is fired by some local conditions in the whole graph, it is well known that one has no grip on the sequence of rewriting steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1286">
<title id=" W11-0108.xml">modular graph rewriting to compute semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our choice naturally went to graph rewriting.
</prevsent>
<prevsent>graph rewriting is barely used in computational linguistics; it could be due to the difficulty to manage large sets of rules.
</prevsent>
</prevsection>
<citsent citstr=" W01-0807 ">
among the pioneers in the use of graph rewriting, we mention hyvonen (1984); <papid> P84-1110 </papid>bohnet and wanner (2001); <papid> W01-0807 </papid>crouch (2005); jijkoun and de rijke (2007); <papid> W07-0208 </papid>bedaride and gardent (2009); <papid> W09-3744 </papid>chaumartin and kahane (2010).</citsent>
<aftsection>
<nextsent>a graph rewriting system is defined as set of graph rewrite rules and computation is sequence of rewrite rule applications to given graph.
</nextsent>
<nextsent>the application of rule is triggered via mechanism of pattern matching, hence sub-graph is isolated from its context and the result is local modification of the input.
</nextsent>
<nextsent>this allows linguistic phenomenon to be easily isolated for applying transformation.
</nextsent>
<nextsent>since each step of computation is fired by some local conditions in the whole graph, it is well known that one has no grip on the sequence of rewriting steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1287">
<title id=" W11-0108.xml">modular graph rewriting to compute semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our choice naturally went to graph rewriting.
</prevsent>
<prevsent>graph rewriting is barely used in computational linguistics; it could be due to the difficulty to manage large sets of rules.
</prevsent>
</prevsection>
<citsent citstr=" W07-0208 ">
among the pioneers in the use of graph rewriting, we mention hyvonen (1984); <papid> P84-1110 </papid>bohnet and wanner (2001); <papid> W01-0807 </papid>crouch (2005); jijkoun and de rijke (2007); <papid> W07-0208 </papid>bedaride and gardent (2009); <papid> W09-3744 </papid>chaumartin and kahane (2010).</citsent>
<aftsection>
<nextsent>a graph rewriting system is defined as set of graph rewrite rules and computation is sequence of rewrite rule applications to given graph.
</nextsent>
<nextsent>the application of rule is triggered via mechanism of pattern matching, hence sub-graph is isolated from its context and the result is local modification of the input.
</nextsent>
<nextsent>this allows linguistic phenomenon to be easily isolated for applying transformation.
</nextsent>
<nextsent>since each step of computation is fired by some local conditions in the whole graph, it is well known that one has no grip on the sequence of rewriting steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1288">
<title id=" W11-0108.xml">modular graph rewriting to compute semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our choice naturally went to graph rewriting.
</prevsent>
<prevsent>graph rewriting is barely used in computational linguistics; it could be due to the difficulty to manage large sets of rules.
</prevsent>
</prevsection>
<citsent citstr=" W09-3744 ">
among the pioneers in the use of graph rewriting, we mention hyvonen (1984); <papid> P84-1110 </papid>bohnet and wanner (2001); <papid> W01-0807 </papid>crouch (2005); jijkoun and de rijke (2007); <papid> W07-0208 </papid>bedaride and gardent (2009); <papid> W09-3744 </papid>chaumartin and kahane (2010).</citsent>
<aftsection>
<nextsent>a graph rewriting system is defined as set of graph rewrite rules and computation is sequence of rewrite rule applications to given graph.
</nextsent>
<nextsent>the application of rule is triggered via mechanism of pattern matching, hence sub-graph is isolated from its context and the result is local modification of the input.
</nextsent>
<nextsent>this allows linguistic phenomenon to be easily isolated for applying transformation.
</nextsent>
<nextsent>since each step of computation is fired by some local conditions in the whole graph, it is well known that one has no grip on the sequence of rewriting steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1293">
<title id=" W11-0108.xml">modular graph rewriting to compute semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and barrier (2004)) with surface syntactic dependencies (candito et al (2010)).
</prevsent>
<prevsent>the enrichment is twofold: ? if they are present in the sentence, the deep arguments of infiniti ves and participles (from participial subordinate clauses) are marked with the usual labels of syntactic functions, ? the anaphora relations that are predictable from the syntax (i.e. the antecedents of relative, reflexive and repeated pronouns) are marked with special label ant.this additional information can already be provided by many syntactic parsers and is particularly interesting to compute semantics.
</prevsent>
</prevsection>
<citsent citstr=" W07-1210 ">
the semantic format is dependency minimal recur sion semantics (dmrs) which was introduced by copestake (2009) <papid> E09-1001 </papid>as compact and easily readable equivalent to robust minimal recur sion semantics (rmrs), which was defined by copestake (2007).<papid> W07-1210 </papid></citsent>
<aftsection>
<nextsent>this underspecified semantic formalism was designed for large scale experiments without committing to fine-grained semantic choices.
</nextsent>
<nextsent>dmrs graphs contain the predicate-argument relations, the restriction of generalized quantifiers and the mode of combination between predicates.
</nextsent>
<nextsent>predicate-argument relations are labelled argi, where is an integer following fixed order of oblique ness suj, obj, ats, ato, a-obj, de-obj.
</nextsent>
<nextsent>naturally, the lexicon must be consistent with this ordering.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1294">
<title id=" W10-4231.xml">udel refining a method of named entity generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the grec named entity challenge 2010 (neg) is an nlg shared task whereby submitted systems must select referring expression from list of options for each mention of each person in text.
</prevsent>
<prevsent>the corpus is collection of 2,000 introductory sections from wikipedia articles about individual people in which all mentions of person entities have been annotated.
</prevsent>
</prevsection>
<citsent citstr=" W09-2817 ">
an in-depth description of the task, along with the evaluation results from the previous year, is provided by belz et al (2009).<papid> W09-2817 </papid>our 2009 submission (greenbacker and mccoy, 2009<papid> W09-2821 </papid>a) was an extension of the system we developed for the grec main subject reference generation challenge (msr) (greenbackerand mccoy, 2009<papid> W09-2821 </papid>b).</citsent>
<aftsection>
<nextsent>although our system performed reasonably-well in predicting reg08 type in the neg task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the msr task.
</nextsent>
<nextsent>as suggested by the evaluators (belz et al, 2009), <papid> W09-2817 </papid>this was due in large part to our reliance on the list of res being in particular order, which had changed for the neg task.</nextsent>
<nextsent>the first improvement we made to our existing methods related to the manner by which we selected the specific re to employ.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1295">
<title id=" W10-4231.xml">udel refining a method of named entity generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the grec named entity challenge 2010 (neg) is an nlg shared task whereby submitted systems must select referring expression from list of options for each mention of each person in text.
</prevsent>
<prevsent>the corpus is collection of 2,000 introductory sections from wikipedia articles about individual people in which all mentions of person entities have been annotated.
</prevsent>
</prevsection>
<citsent citstr=" W09-2821 ">
an in-depth description of the task, along with the evaluation results from the previous year, is provided by belz et al (2009).<papid> W09-2817 </papid>our 2009 submission (greenbacker and mccoy, 2009<papid> W09-2821 </papid>a) was an extension of the system we developed for the grec main subject reference generation challenge (msr) (greenbackerand mccoy, 2009<papid> W09-2821 </papid>b).</citsent>
<aftsection>
<nextsent>although our system performed reasonably-well in predicting reg08 type in the neg task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the msr task.
</nextsent>
<nextsent>as suggested by the evaluators (belz et al, 2009), <papid> W09-2817 </papid>this was due in large part to our reliance on the list of res being in particular order, which had changed for the neg task.</nextsent>
<nextsent>the first improvement we made to our existing methods related to the manner by which we selected the specific re to employ.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1302">
<title id=" W10-4231.xml">udel refining a method of named entity generation </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>reg08-type precision and recall were equal in all four sets.
</prevsent>
<prevsent>the fact that our string accuracy scores improve dover our 2009 submission far more than reg08 type prediction is hardly surprising.
</prevsent>
</prevsection>
<citsent citstr=" W09-2818 ">
our efforts during this iteration of the neg task were primarily focused on enhancing our methods of choosing the best re once the reference type was selected.we remain several points below the best performing team from 2009 (icsi-berkeley), possibly due to the inclusion of additional items intheir feature set, or the use of conditional random fields as their learning technique (favre and bohnet, 2009).<papid> W09-2818 </papid></citsent>
<aftsection>
<nextsent>moving forward, we hope to expand our feature set by including the morphology of words immediately surrounding the reference, as well as more extensive reference history, as suggested by (favre and bohnet, 2009).<papid> W09-2818 </papid></nextsent>
<nextsent>we suspect that these features may play significant role in determining the type of referenced used, the prediction of which acts as bottleneck?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1306">
<title id=" W11-0908.xml">desperately seeking implicit arguments in text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the difficulties todeal with lexically unexpressed items or implicit entities are related on the one hand to recall problems,i.e. the problem of deciding whether an item is implicit or not, and on the other hand to precision problems, i.e. if an implicit entity is accessible to the reader from the discourse or its context, an appropriate antecedent has to be found.
</prevsent>
<prevsent>however, system able to derive the presence of ies may be determining factor in improving performance of qa systems and, in general, in informations retrieval and extraction systems.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
the current computational scene has witnessed an increased interest in the creation and use of semantically annotated computational lexica and their associated annotated corpora, like propbank (palmer etal., 2005), <papid> J05-1004 </papid>framenet (baker et al, 1998) <papid> P98-1013 </papid>and nombank (meyers, 2007), where the proposed annotation scheme has been applied in real contexts.</citsent>
<aftsection>
<nextsent>in all these cases, what has been addressed is basic semantic issue, i.e. labeling pas associated to semantic predicates like adjectives, verbs and nouns.
</nextsent>
<nextsent>how ever, what these corpora have not made available is information related to ies.
</nextsent>
<nextsent>for example, in the case of even tive deverbal nominals, information about thesubject/object of the nominal predicate is often implicit and has to be understood from the previous 54discourse or text, e.g. the development of prototype [?
</nextsent>
<nextsent>implicit subject]?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1307">
<title id=" W11-0908.xml">desperately seeking implicit arguments in text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the difficulties todeal with lexically unexpressed items or implicit entities are related on the one hand to recall problems,i.e. the problem of deciding whether an item is implicit or not, and on the other hand to precision problems, i.e. if an implicit entity is accessible to the reader from the discourse or its context, an appropriate antecedent has to be found.
</prevsent>
<prevsent>however, system able to derive the presence of ies may be determining factor in improving performance of qa systems and, in general, in informations retrieval and extraction systems.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the current computational scene has witnessed an increased interest in the creation and use of semantically annotated computational lexica and their associated annotated corpora, like propbank (palmer etal., 2005), <papid> J05-1004 </papid>framenet (baker et al, 1998) <papid> P98-1013 </papid>and nombank (meyers, 2007), where the proposed annotation scheme has been applied in real contexts.</citsent>
<aftsection>
<nextsent>in all these cases, what has been addressed is basic semantic issue, i.e. labeling pas associated to semantic predicates like adjectives, verbs and nouns.
</nextsent>
<nextsent>how ever, what these corpora have not made available is information related to ies.
</nextsent>
<nextsent>for example, in the case of even tive deverbal nominals, information about thesubject/object of the nominal predicate is often implicit and has to be understood from the previous 54discourse or text, e.g. the development of prototype [?
</nextsent>
<nextsent>implicit subject]?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1308">
<title id=" W11-0908.xml">desperately seeking implicit arguments in text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, in the case of even tive deverbal nominals, information about thesubject/object of the nominal predicate is often implicit and has to be understood from the previous 54discourse or text, e.g. the development of prototype [?
</prevsent>
<prevsent>implicit subject]?.
</prevsent>
</prevsection>
<citsent citstr=" P10-1160 ">
as reported by gerber and chai (2010), <papid> P10-1160 </papid>introducing implicit arguments to nominal predicates in nombank would increase the resource coverage of 65%.other ies can be found in agent less passive constructions ( e.g.our little problem will soon be solved ? [?</citsent>
<aftsection>
<nextsent>unexpressed agent ]1) or as unexpressed arguments such as addressee with verbs of commitment, for example can promise ? that one of you will be troubled [?
</nextsent>
<nextsent>unexpressed addressee]?
</nextsent>
<nextsent>and dare swear ? that before tomorrow night hewill be fluttering in our net [?
</nextsent>
<nextsent>unexpressed ad dressee]?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1309">
<title id=" W11-0908.xml">desperately seeking implicit arguments in text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we discuss the issues related to the identification of implicit entities in text, focussing in particular on omissions of core arguments of predicates.
</prevsent>
<prevsent>we investigate the topic from the perspective proposed by (fillmore, 1986) and base our observations on null instantiated arguments annotated for the semeval 2010 task 10, linking events and their participants in discourse?
</prevsent>
</prevsection>
<citsent citstr=" S10-1008 ">
(ruppenhofer et al,2010)<papid> S10-1008 </papid>2.</citsent>
<aftsection>
<nextsent>the paper is structured as follows: in section 2 we detail the task of identifying null instan tiated arguments from theoretical perspective and describe related work.
</nextsent>
<nextsent>in section 3 we briefly introduce the semeval task 10 for identifying implicit arguments in text, while in section 4 we detail our proposal for ni identification and binding.
</nextsent>
<nextsent>in section 5 we give thorough description of the types of null instantiations annotated in the semeval dataset and we explain the behavior of our algorithm w.r.t. such cases.
</nextsent>
<nextsent>we also compare our results with the output of the systems participating to the semeval task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1315">
<title id=" W11-0908.xml">desperately seeking implicit arguments in text </title>
<section> semeval 2010 task 10.  </section>
<citcontext>
<prevsection>
<prevsent>note that overt fes are the explicit frame elements annotated in the dataset.
</prevsent>
<prevsent>although 26 teams downloaded the data sets,there were only two submissions, probably depending on the intrinsic difficulties of the task (see discussion in section 5).
</prevsent>
</prevsection>
<citsent citstr=" S10-1059 ">
the best performing system(chen et al, 2010) <papid> S10-1059 </papid>is based on supervised learning approach using, among others, distributional semantic similarity between the heads of candidate referents and role fillers in the training data, butits performance is strongly affected by data sparse ness.</citsent>
<aftsection>
<nextsent>indeed, only 438 sentences with annotated nis were made available in the training set, which is clearly insufficient to capture such multifaceted phenomenon with supervised approach.
</nextsent>
<nextsent>the second system participating in the task (tonelli and delmonte, 2010) <papid> S10-1065 </papid>was an adaptation of an existing lfg-based system for deep semantic analysis (delmonte, 2009), whose output was mapped to framenet-style annotation.</nextsent>
<nextsent>in this case, the major challenge was to cope with the classification of some ni phenomena which are very much dependent onframe specific information, and can hardly be generalized in the lfg framework.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1316">
<title id=" W11-0908.xml">desperately seeking implicit arguments in text </title>
<section> semeval 2010 task 10.  </section>
<citcontext>
<prevsection>
<prevsent>the best performing system(chen et al, 2010) <papid> S10-1059 </papid>is based on supervised learning approach using, among others, distributional semantic similarity between the heads of candidate referents and role fillers in the training data, butits performance is strongly affected by data sparse ness.</prevsent>
<prevsent>indeed, only 438 sentences with annotated nis were made available in the training set, which is clearly insufficient to capture such multifaceted phenomenon with supervised approach.</prevsent>
</prevsection>
<citsent citstr=" S10-1065 ">
the second system participating in the task (tonelli and delmonte, 2010) <papid> S10-1065 </papid>was an adaptation of an existing lfg-based system for deep semantic analysis (delmonte, 2009), whose output was mapped to framenet-style annotation.</citsent>
<aftsection>
<nextsent>in this case, the major challenge was to cope with the classification of some ni phenomena which are very much dependent onframe specific information, and can hardly be generalized in the lfg framework.
</nextsent>
<nextsent>ni identification and binding in this section, we describe our proposal for dealing with ini/dni identification and evaluate our output against semeval gold standard data.
</nextsent>
<nextsent>as discussed in the previous section, existing systems dealing with this task suffer on the one hand from lack of training data and on the other hand from the dependence of the task on frame annotation, which makes it difficult to adapt existing unsupervised approaches.
</nextsent>
<nextsent>we show that, given this state of the art, better results can be achieved in the task by simply developing an algorithm that reflects as much as possible the linguistic motivations behind ni identification in the framenet paradigm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1322">
<title id=" W11-0908.xml">desperately seeking implicit arguments in text </title>
<section> evaluation and error analysis.  </section>
<citcontext>
<prevsection>
<prevsent>in this case, strategy based on some kind of history list may be very helpful.
</prevsent>
<prevsent>this could contain, for example, all subjects and direct objects previously mentioned in text and selected according to some relevance criteria, as in (tonelli and delmonte, 2010).<papid> S10-1065 </papid></prevsent>
</prevsection>
<citsent citstr=" P86-1004 ">
a further improvement may derive from the integration of an anaphora resolution step, as first proposed by palmer et al (1986) <papid> P86-1004 </papid>and more recently by gerber and chai (2010).<papid> P10-1160 </papid></citsent>
<aftsection>
<nextsent>60 5.3 open issues related to the task.
</nextsent>
<nextsent>other open issues are related to the specification of the task and to the nature of implicit entities, which make it difficult to account for this phenomenon from computational point of view.
</nextsent>
<nextsent>we report be low the main issues that need to be tackled: ini linking: table 1 shows that 28% of dnis in the test set are not linked to any referent.
</nextsent>
<nextsent>this puts into question one of the main assumptions of the task, that is the connection between definite instantiation and referent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1325">
<title id=" W11-1501.xml">extending the tool or how to annotate historical language varieties </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results of thorough evaluation over the extended tool show that using this method an almost state-of-the-art performance is obtained, adequate to carry out quantitative studies in the humanities: 94.5% accuracy for the main part of speech and 92.6% for lemma.
</prevsent>
<prevsent>to our knowledge, this is the first time that sucha strategy is adopted to annotate historical language varieties and we believe that it could be used as well to deal with other non-standard varieties of languages.
</prevsent>
</prevsection>
<citsent citstr=" W09-0214 ">
in the last few years, there has been growing interest in all disciplines of the humanities to study historical varieties of languages using quantitative methods (sagi et al, 2009; <papid> W09-0214 </papid>ludeling et al, to ap pear).</citsent>
<aftsection>
<nextsent>large corpora are necessary to conduct this type of studies, so as to smooth the great data sparseness problem affecting non-standard varieties of languages, and thus guarantee the validity of the generalizations based on these data.
</nextsent>
<nextsent>historical language varieties bear similarities to standard varieties, but they also exhibit remarkable differences in number of respects, such as their morphology, syntax, and semantics.
</nextsent>
<nextsent>in addition, as orthographic rules were not established until later centuries, great amount of graphemic variation is found in historical texts, such that one word can be written using many different graphemic variants.
</nextsent>
<nextsent>this variation increases considerably the number of different words and therefore the lexicon of the corresponding language variety.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1326">
<title id=" W11-1501.xml">extending the tool or how to annotate historical language varieties </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>this module has also been adapted, incorporating old spanish clitics (-gela, li) and other variants of derivation affixes (adverbs in -mientre or -mjentre).
</prevsent>
<prevsent>5.3 retraining the tagger.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
free ling includes 2 different modules able to perform pos tagging: hybrid tagger (relax), integrating statistical and hand-coded grammatical rules, and hidden markov model tagger (hmm), which is classical trigram markov ian tagger, based on tnt (brants, 2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>as mentioned in section 4, the tagger for standard spanish has been used to pre-annotate the gold standard corpus, which has 5 subsequently been corrected to be able to carry out the retraining.
</nextsent>
<nextsent>the effort of correcting the corpus is much lower compared to annotating from scratch.in this paper we present the evaluation of the performance of the extended resource using the hmm tagger with the probabilities generated automatically from the trigrams in the gold standard corpus.
</nextsent>
<nextsent>in this section we evaluate the dictionary (section 6.1) and present the overall tagging results (section 6.2).
</nextsent>
<nextsent>the resources for standard spanish have been used as baseline.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1327">
<title id=" W11-1405.xml">detecting structural events for assessing nonnative speech </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>there have been previous efforts in using nlp 1a t-unit is defined as essentially main clause plus any other clauses which are dependent upon it (hunt, 1970).technology to automatically calculate syntactic complexity metrics on learners?
</prevsent>
<prevsent>writing data.
</prevsent>
</prevsection>
<citsent citstr=" P05-1025 ">
for example, lu (2009) and sagae et al (2005) <papid> P05-1025 </papid>used parsing to get structural information on written texts; however, such efforts have not been undertaken in assessing speech data.</citsent>
<aftsection>
<nextsent>chen et al (2010) <papid> W10-1010 </papid>annotated structural events(such as clause structure and disfluencies) on english language learners?</nextsent>
<nextsent>speech transcriptions and extracted features based on the structural event profile.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1328">
<title id=" W11-1405.xml">detecting structural events for assessing nonnative speech </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>writing data.
</prevsent>
<prevsent>for example, lu (2009) and sagae et al (2005) <papid> P05-1025 </papid>used parsing to get structural information on written texts; however, such efforts have not been undertaken in assessing speech data.</prevsent>
</prevsection>
<citsent citstr=" W10-1010 ">
chen et al (2010) <papid> W10-1010 </papid>annotated structural events(such as clause structure and disfluencies) on english language learners?</citsent>
<aftsection>
<nextsent>speech transcriptions and extracted features based on the structural event profile.
</nextsent>
<nextsent>they found that the features derived from structural event profile show promising correlation to human holistic scores.
</nextsent>
<nextsent>ber stein et al (2010) also computed the features related to sentence lengths andthe counts of syntactic entities.
</nextsent>
<nextsent>they found the extracted features were highly correlated to holistic scores measuring test-takers?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1329">
<title id=" W11-1405.xml">detecting structural events for assessing nonnative speech </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>in the speech research domain, large amount of research has been conducted to detect structural events in speech transcriptions and recognized words using lexical and prosodic cues.
</prevsent>
<prevsent>using language model (lm) trained on words combined withthe events of interest is popular technique for using textual information for structural event detection.
</prevsent>
</prevsection>
<citsent citstr=" J99-4003 ">
for example, heeman and allen (1999) <papid> J99-4003 </papid>developed lm including part of speech (pos) tags, discourse markers (e.g., right, anyway), speech repairs,and into national phrases.</citsent>
<aftsection>
<nextsent>in this way, structural information (e.g., speech repairs), could be predicted using traditional speech recognition approach.prosodic information has been widely used to further improve textual models.
</nextsent>
<nextsent>for example, simple prosodic feature, pause duration between words,was used in gotoh and renals (2000) to detect sentence boundaries.
</nextsent>
<nextsent>it was found that the pause duration model alone was better than using an lm alone,and the combination of the two models further improved the performance.
</nextsent>
<nextsent>more advanced prosody models were used inother research on sentence boundary and speech repair detections (shriberg et al, 2000; shriberg and stolcke, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1330">
<title id=" W11-1405.xml">detecting structural events for assessing nonnative speech </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>research on structural event detection has been strongly affected by the darpa ears program (ears, 2002).
</prevsent>
<prevsent>as in shriberg et al (2000), the structural event detection (e.g., sentence units (sus)and speech repairs) investigated in ears was classification task utilizing both prosodic and textual knowledge sources.
</prevsent>
</prevsection>
<citsent citstr=" W04-3209 ">
new approaches for combining the two knowledge sources, including maximum entropy (maxent) and conditional random fields (crfs), were studied to address the weaknesses of the generative hmm approach (liu et al, 2004).<papid> W04-3209 </papid></citsent>
<aftsection>
<nextsent>liuet al (2005) concluded that adding textual information, building more robust prosodic model, using conditional modeling approaches (maxent andcrf), and system combination all yield performance gains.?
</nextsent>
<nextsent>non-native speech data were collected from the toefl practice test online (tpo) (ets, 2006).in each tpo test, test-takers were required to respond to six speaking test items, in which they were required to provide information or opinions on familiar topics, based on their personal experience or background knowledge.
</nextsent>
<nextsent>for example, the test-takers were asked to describe their opinions about living on or off campus.a total of 1066 responses were collected from examinees.
</nextsent>
<nextsent>then, group of experienced human raters scored these items based on the scoring rubrics designed for scoring the tpo test.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1331">
<title id=" W11-1405.xml">detecting structural events for assessing nonnative speech </title>
<section> methods of structural event detection.  </section>
<citcontext>
<prevsection>
<prevsent>= argmax p (e|w )given that denotes the between-word event sequence and denotes the corresponding lexical cues, the goal is to find the event sequence that has the greatest probability, given the observed features.
</prevsent>
<prevsent>recently, conditional modeling approaches were successfully used in sentence units (sus) and speech repairs detection (liu, 2004).
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
hence, we use the maximum entropy (maxent) (berger et al, 1996) <papid> J96-1002 </papid>and conditional random fields (crfs) (lafferty et al., 2001) approaches to build statistical models for structural event detection.</citsent>
<aftsection>
<nextsent>5.1 setup.
</nextsent>
<nextsent>in our experiment, the whole corpus described in section 3 was split into training set (train), development test set (dev), and testing set (test), without speaker overlap between any pair of sets.
</nextsent>
<nextsent>table 1 summarizes the numbers of items and words, as well as structural events of each dataset.
</nextsent>
<nextsent>2pause durations were obtained by running forced alignment using speech and transcriptions on tri-phone hmm speech recognizer train dev test # item 664 101 301 # word 71523 10509 33754 # cb 6121 918 2852 # ip 1767 267 1112 table 1: the number of items, words, and structural events of the three sets in the tpo corpus on average, each item contains about 108.6words, 9.3 cbs, and 3.0 ips.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1332">
<title id=" W10-4102.xml">textual emotion processing from event analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ortony et al, (1987) conducted an empirical study for structure of affective lexicon based on the ~500 words used in previous emotion studies.
</prevsent>
<prevsent>however, most of the emotion corpora in nlp try to avoid the emotion definition problem.
</prevsent>
</prevsection>
<citsent citstr=" D09-1150 ">
instead, they choose to relyon the intuition of annotators (rens blog emotion corpus, rbec, quan and ren, 2009) <papid> D09-1150 </papid>or authors (mishnes blog emotion corpus, mishne, 2005).</citsent>
<aftsection>
<nextsent>therefore, one of the crucial drawbacks of emotion corpora is the problem of poor quality.
</nextsent>
<nextsent>in this paper, we explore emotion annotation from different perspective.
</nextsent>
<nextsent>we concentrate on explicit emotions, and utilize their contextual information for emotion recognition.
</nextsent>
<nextsent>in terms of emotion representation, textual emotion corpora are basically annotated using either the enumerative representation or the compositional representation (chen et al, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1333">
<title id=" W10-4102.xml">textual emotion processing from event analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>according to turner (2000).
</prevsent>
<prevsent>with regard to emotion recognition technologies, there are two kinds of classification models.
</prevsent>
</prevsection>
<citsent citstr=" C08-1111 ">
one is based on an independent view (mishne, 2005; mihalcea and liu, 2006; aman and szpakowicz, 2007; tokuhisa et al, 2008; <papid> C08-1111 </papid>strapparava and mihalcea, 2008), and the other is dependent view (abbasi et al 2008; keshtkar and inkpen, 2009).</citsent>
<aftsection>
<nextsent>the independent view treats emotions separately, and often chooses single-label classification approach to identify emotions.
</nextsent>
<nextsent>in contrast, the dependent view takes into account complicated emotion expressions, such as emotion interaction and emotion co-occurrences, and thus requires more complicated models.
</nextsent>
<nextsent>abbasi et al (2008) adopt an ensemble classifier to detect the cooccurrences of different emotions; keshtkar and inkpen (2009) use iteratively single-label classifiers in the top-down order of given emotion hierarchy.
</nextsent>
<nextsent>in this paper, we examine emotion recognition as multi-label problem and investigate several multi-label classification approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1334">
<title id=" W10-4102.xml">textual emotion processing from event analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in other words, emotion is envisioned as an event type which triggers another event, i.e. cause event.
</prevsent>
<prevsent>we attempt to examine emotion cause relations for open domains.
</prevsent>
</prevsection>
<citsent citstr=" P02-1047 ">
however, not much work (marcu and echihabi, 2002; <papid> P02-1047 </papid>girju, 2003; chang and choi, 2006) has been done on this kind of general causal relation for open do mains.</citsent>
<aftsection>
<nextsent>most existing causal relation detection systems contain two steps: 1) cause candidate identifica tion; 2) causal relation detection.
</nextsent>
<nextsent>however, step 1) is often oversimplified in real systems.
</nextsent>
<nextsent>for example, the cause-effect pairs are limited to two noun phrases (chang and choi, 2005; girju, 2003), or two clauses connected with selected conjunction words (marcu and echihabi, 2002).<papid> P02-1047 </papid></nextsent>
<nextsent>moreover, the task of step 2) often is considered as binary classification problem, i.e. causal?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1339">
<title id=" W11-1718.xml">sentiment classification using semantic features extracted from wordnet based resources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this heterogeneity has motivated that dealing with the identification and extraction of opinions and sentiments in texts require special attention.
</prevsent>
<prevsent>in fact, the development of different tools to help government information analysts, companies, political parties, economists, etc to automatically get feelings from news and forums is challenging task (wiebe et al, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W06-0301 ">
many researchers such as balahur et al, (2010), hatzivassiloglou et al(2000), kim and hovy (2006), <papid> W06-0301 </papid>wiebe et al (2005) and many others have been working in this way and related areas.</citsent>
<aftsection>
<nextsent>moreover, in the course of years we find long tradition on developing question answering (qa) systems.
</nextsent>
<nextsent>however, in recent years, researchers have concentrated on the development of opinion questions answering (oqa) systems (balahur et al., 2010).
</nextsent>
<nextsent>this new task has to deal with different problems such as sentiment analysis where documents must be classified according to sentiments and subjectivity features.
</nextsent>
<nextsent>therefore, new kind of evaluation that takes into account this new issue is needed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1343">
<title id=" W11-1418.xml">using semantic distance to automatically suggest transfer course equivalencies </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>semantic distance measures have been used in applications such as automatic annotation, keyword extraction, and social network extraction (matsuo et al ., 2007).
</prevsent>
<prevsent>it is important to note that there are two kinds of semantic distance: semantic similarity and semantic relatedness.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
semantic relatedness is more generic than semantic similarity in that it includes all classical and non-classical semantic relations such as holonymy1, meronymy2, and antonymy3, where semantic similarity is limited to relations such as hyponymy4 and hypernymy5 (budanitsky and hirst,2006).<papid> J06-1003 </papid></citsent>
<aftsection>
<nextsent>the terms semantic distance, semantic relatedness, and semantic similarity are sometimes used interchangeably by different authors in the literature related to this topic.
</nextsent>
<nextsent>the relative generality of the three terms is illustrated in figure 2.
</nextsent>
<nextsent>semantic distance semantic relatedness semantic similarity figure 2.
</nextsent>
<nextsent>the relations of semantic distance, semantic relatedness, and semantic similarity as described by budanitsky and hirst (2006).<papid> J06-1003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1345">
<title id=" W11-1418.xml">using semantic distance to automatically suggest transfer course equivalencies </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>a fragment of wordnets taxonomy.
</prevsent>
<prevsent>lexicographic resource based methods typically calculate semantic distance based on wordnet6.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
in related work (rada et al , 1989; wu and palmer, 1994; <papid> P94-1019 </papid>leacock and chodorow, 1998; hirst andst-onge, 1998; yang and powers, 2005), lexicographic resource based methods use one or moreedge-counting (also known as shortest-path) techniques in the wordnet taxonomy (figure 3).</citsent>
<aftsection>
<nextsent>in this technique, concept nodes are constructed in hierarchical network and the minimum number of hops between any two nodes represents their semantic distance (collins and quill ian, 1969).
</nextsent>
<nextsent>the measure by hirst and st-onge (1998) is based on the fact that the target concepts are likely more distant if the target path consists of edges that belong to many different relations.
</nextsent>
<nextsent>the approach by leacock and chodorow (1998) combines the shortest path with maximum depth so that edges lower down in the is-a hierarchy correspond to smaller semantic distances than the ones higher up.
</nextsent>
<nextsent>yang and powers (2005) further suggest that it is necessary to consider relations such as holonymy and meronymy.a corpus-based method typically calculates cooccurrence on one or more corpora to deduce semantic closeness (sahami and heilman, 2006; cilibrasiand vitanyi, 2007; islam and inkpen, 2006; mihalcea et al , 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1346">
<title id=" W11-1418.xml">using semantic distance to automatically suggest transfer course equivalencies </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>the approach by leacock and chodorow (1998) combines the shortest path with maximum depth so that edges lower down in the is-a hierarchy correspond to smaller semantic distances than the ones higher up.
</prevsent>
<prevsent>yang and powers (2005) further suggest that it is necessary to consider relations such as holonymy and meronymy.a corpus-based method typically calculates cooccurrence on one or more corpora to deduce semantic closeness (sahami and heilman, 2006; cilibrasiand vitanyi, 2007; islam and inkpen, 2006; mihalcea et al , 2006).
</prevsent>
</prevsection>
<citsent citstr=" E06-1016 ">
using this technique, two words 6http://wordnet.princeton.edu/ 143 are likely to have short semantic distance if they co-occur within similar contexts (lin, 1998).hybrid methods (including distributional mea sures) combine lexicographic resources with corpus statistics (jiang and conrath, 1997; mohammad and hirst, 2006; <papid> E06-1016 </papid>li et al , 2003; li et al , 2006).</citsent>
<aftsection>
<nextsent>related work shows that hybrid methods generally outperform lexicographic resource based and corpus based methods (budanitsky and hirst, 2006; <papid> J06-1003 </papid>curran, 2004; mohammad and hirst, 2006; <papid> E06-1016 </papid>mohammad, 2008).</nextsent>
<nextsent>li et al  (2006) proposed hybrid method basedonwordnet and the brown corpus to incorporate semantic similarity between words, semantic similarity between sentences, and word order similarity to measure overall sentence similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1352">
<title id=" W11-1418.xml">using semantic distance to automatically suggest transfer course equivalencies </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, mapping between word and certain sense must be provided.
</prevsent>
<prevsent>such mapping is called word sense disambiguation (wsd), which is the ability to identify the meaning of words in context in computational manner (navigli, 2009).
</prevsent>
</prevsection>
<citsent citstr=" W04-0837 ">
we consider two strategies to perform the wsd: (1)compare all senses of two words and select the maximum score, and (2) apply the first sense heuristic (mccarthy et al , 2004).<papid> W04-0837 </papid></citsent>
<aftsection>
<nextsent>we will show that the overall performance of the two strategies is about the same.
</nextsent>
<nextsent>to improve accuracy, the parts of speech7 (pos) of two words have to be the same before visiting the wordnet taxonomy to determine their semantic distance.
</nextsent>
<nextsent>therefore, book?
</nextsent>
<nextsent>as in read book?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1353">
<title id=" W11-1417.xml">predicting change in student motivation by measuring cohesion between tutor and student </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, pedagogic ally significant behaviors such as dictionary lookup in the reap (brown and eskenazi, 2004) vocabulary tutor have been shown to be positively correlated with motivation assessments (delarosa and eskenazi, 2011).
</prevsent>
<prevsent>also, in separate study with the reap tutor, attempts to manipulate reading motivation by presenting more interesting stories were shown to improve vocabulary learning (heilman et al, 2010).in addition to influencing learning outcomes, motivational state may also affect which interventions will be effective during tutoring.
</prevsent>
</prevsection>
<citsent citstr=" N04-3002 ">
for example, ward and litman (2011) have shown that motivation can significantly affect which students benefit from reflective reading following interactive tutoring with the itspoke (litman and silliman, 2004) <papid> N04-3002 </papid>tutor.</citsent>
<aftsection>
<nextsent>an accurate way to measure student motivation during tutoring could therefore be valuable to intelligent tutoring system (its) researchers.
</nextsent>
<nextsent>several self-report instruments have been developed which measure various aspects of motivation (e.g.
</nextsent>
<nextsent>(pintrich and degroot, 1990; mckenna and kear, 1990)).
</nextsent>
<nextsent>however, these instruments are too intrusive to be administered during tutoring, for fear of fatally disrupting learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1354">
<title id=" W11-1417.xml">predicting change in student motivation by measuring cohesion between tutor and student </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1 in addition, ward and litman (2006), ward and litman (2008) investigated measure of lexical similarity between 1in this experiment, the task was to watch series of videos.
</prevsent>
<prevsent>136 the tutor and student partners in tutoring dialog which was shown to be correlated with task success in several corpora of tutorial dialogs.measures of cohesion have also been used in variety of nlp tasks such as measuring text readability(e.g.
</prevsent>
</prevsection>
<citsent citstr=" D08-1020 ">
(pitler and nenkova, 2008)), <papid> D08-1020 </papid>measuring stylistic differences in text (mccarthy et al, 2006), and for topic segmentation in tutorial dialog (olney and cai, 2005).<papid> H05-1122 </papid></citsent>
<aftsection>
<nextsent>given the previously mentioned results relating motivation to educational task success, these links between task success and cohesion lead us to hypothesize direct correlation between motivation and cohesion when using the itspoke tutor.
</nextsent>
<nextsent>we will first briefly describe the itspoke tutor, and the corpus of tutoring dialogs used in this study.
</nextsent>
<nextsent>wewill then describe the instrument we used to measure motivation both before and immediately after tutoring, then we will describe the algorithm used to measure cohesion in the tutoring dialogs.
</nextsent>
<nextsent>finally, we show results of correlations between the measure of motivation and the measure of cohesion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1355">
<title id=" W11-1417.xml">predicting change in student motivation by measuring cohesion between tutor and student </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1 in addition, ward and litman (2006), ward and litman (2008) investigated measure of lexical similarity between 1in this experiment, the task was to watch series of videos.
</prevsent>
<prevsent>136 the tutor and student partners in tutoring dialog which was shown to be correlated with task success in several corpora of tutorial dialogs.measures of cohesion have also been used in variety of nlp tasks such as measuring text readability(e.g.
</prevsent>
</prevsection>
<citsent citstr=" H05-1122 ">
(pitler and nenkova, 2008)), <papid> D08-1020 </papid>measuring stylistic differences in text (mccarthy et al, 2006), and for topic segmentation in tutorial dialog (olney and cai, 2005).<papid> H05-1122 </papid></citsent>
<aftsection>
<nextsent>given the previously mentioned results relating motivation to educational task success, these links between task success and cohesion lead us to hypothesize direct correlation between motivation and cohesion when using the itspoke tutor.
</nextsent>
<nextsent>we will first briefly describe the itspoke tutor, and the corpus of tutoring dialogs used in this study.
</nextsent>
<nextsent>wewill then describe the instrument we used to measure motivation both before and immediately after tutoring, then we will describe the algorithm used to measure cohesion in the tutoring dialogs.
</nextsent>
<nextsent>finally, we show results of correlations between the measure of motivation and the measure of cohesion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1356">
<title id=" W11-1417.xml">predicting change in student motivation by measuring cohesion between tutor and student </title>
<section> semantic cohesion measure.  </section>
<citcontext>
<prevsection>
<prevsent>as described more completely in (ward and litman, 2008), semantic similarity cohesive ties are counted by measuring two words?
</prevsent>
<prevsent>proximity in the wordnet (miller et al, 1990) hierarchy.
</prevsent>
</prevsection>
<citsent citstr=" W02-0109 ">
we use simple path distance similarity measure, as implemented in nltk (loper and bird, 2002).<papid> W02-0109 </papid></citsent>
<aftsection>
<nextsent>this measure counts the number of edges in the shortest path between two words in wordnet, and calculates similarity as 1/ (1 + n).
</nextsent>
<nextsent>our implementation of this semantic similarity measure allows setting threshold ?, such that only word pairs with stronger-than-threshold similarity are counted.
</nextsent>
<nextsent>table 3 shows some semantic similarity pairs counted with threshold of 0.3.
</nextsent>
<nextsent>motion-contact man-person decrease-acceleration acceleration-change travel-flyingtable 3: example semantic ties: ? = 0.3we obtain normalized cohesion score for each dialog by dividing the tie count by the number of turns in the dialog.we then sum the line normalized counts over allthe dialogs for each student, resulting in per student cohesion measure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1357">
<title id=" W11-1417.xml">predicting change in student motivation by measuring cohesion between tutor and student </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>4this definition conflates studies of priming, alignment, convergence and accommodation.
</prevsent>
<prevsent>139as with measures of dialog similarity, dialog entrainment has been found to be related to satisfaction and success in task oriented dialogs.
</prevsent>
</prevsection>
<citsent citstr=" P07-1102 ">
for example, reitter and moore (2007) <papid> P07-1102 </papid>found that lexical and syntactic repetition predicted task succes sin the maptask corpus.</citsent>
<aftsection>
<nextsent>similarly, ward and litman (2007) found that lexical and acoustic-prosodic entrainment are correlated with task success in the itspoke dialog system.
</nextsent>
<nextsent>interestingly, in that work entrainment was more strongly correlated with task success than measure of dialog cohesion similar to the one used in the current paper.
</nextsent>
<nextsent>this raises the question of whether such measure of dialog entrainment might also be better predictor of motivation than the current measure of cohesion.
</nextsent>
<nextsent>we hope in future work to further investigate this possibility.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1358">
<title id=" W10-4153.xml">a multistage clustering framework for chinese personal name disambiguation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>(chicago bulls) sports, basketball table 1: six examples defined in the neukd in the domain knowledge based topic identification algorithm, all domain associated terms occurring in the given document are first mapped into domain features such as football, basketball or cycling.
</prevsent>
<prevsent>the most frequent do main feature is considered as the most likely topic.
</prevsent>
</prevsection>
<citsent citstr=" I05-3015 ">
see zhu and chen (2005) <papid> I05-3015 </papid>for details.</citsent>
<aftsection>
<nextsent>two documents with the same topic can be grouped into the same cluster.
</nextsent>
<nextsent>table 2: examples of pnd on sportsman class 2.4.3 multi-stage clustering framework we proposed multi-stage clustering framework for pnd on common person name class, as shown in figure 1.
</nextsent>
<nextsent>in the multi-stage clustering framework, the first-stage is to adopt strict rule-based hard clustering algorithm using the feature set of personal attributes.
</nextsent>
<nextsent>the second-stage is to implement constrained hierarchical agglomerative clustering using ne-type local features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1359">
<title id=" W10-4006.xml">multiword expression sensitive word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even though this paper doesnot directly address the issues in cross lingual information retrieval (clir), it discusses an approach of direct relevance to the field.
</prevsent>
<prevsent>this approach could be viewed as the opposite of current trend sin clir on semantic space that incorporate notion of order in the bag-of-words model (e.g. co-occurences).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
word alignment (brown et al , 1993; <papid> J93-2003 </papid>vogel et al ., 1996; <papid> C96-2141 </papid>och and ney, 2003<papid> J03-1002 </papid>a; graca et al ,2007) remains key to providing high-quality translations as all subsequent training stages relyon itsperformance.</citsent>
<aftsection>
<nextsent>it alone does not effectively capture many-to-many word correspondences, but instead relies on the ability of subsequent heuristic phrase extraction algorithms, such as grow-diag final (koehn et al , 2003), <papid> N03-1017 </papid>to resolve them.</nextsent>
<nextsent>some aligned corpora include implicit partial alignment annotation, while for other corpora apart ial alignment can be extracted by state-of the-art techniques.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1360">
<title id=" W10-4006.xml">multiword expression sensitive word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even though this paper doesnot directly address the issues in cross lingual information retrieval (clir), it discusses an approach of direct relevance to the field.
</prevsent>
<prevsent>this approach could be viewed as the opposite of current trend sin clir on semantic space that incorporate notion of order in the bag-of-words model (e.g. co-occurences).
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
word alignment (brown et al , 1993; <papid> J93-2003 </papid>vogel et al ., 1996; <papid> C96-2141 </papid>och and ney, 2003<papid> J03-1002 </papid>a; graca et al ,2007) remains key to providing high-quality translations as all subsequent training stages relyon itsperformance.</citsent>
<aftsection>
<nextsent>it alone does not effectively capture many-to-many word correspondences, but instead relies on the ability of subsequent heuristic phrase extraction algorithms, such as grow-diag final (koehn et al , 2003), <papid> N03-1017 </papid>to resolve them.</nextsent>
<nextsent>some aligned corpora include implicit partial alignment annotation, while for other corpora apart ial alignment can be extracted by state-of the-art techniques.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1361">
<title id=" W10-4006.xml">multiword expression sensitive word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even though this paper doesnot directly address the issues in cross lingual information retrieval (clir), it discusses an approach of direct relevance to the field.
</prevsent>
<prevsent>this approach could be viewed as the opposite of current trend sin clir on semantic space that incorporate notion of order in the bag-of-words model (e.g. co-occurences).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
word alignment (brown et al , 1993; <papid> J93-2003 </papid>vogel et al ., 1996; <papid> C96-2141 </papid>och and ney, 2003<papid> J03-1002 </papid>a; graca et al ,2007) remains key to providing high-quality translations as all subsequent training stages relyon itsperformance.</citsent>
<aftsection>
<nextsent>it alone does not effectively capture many-to-many word correspondences, but instead relies on the ability of subsequent heuristic phrase extraction algorithms, such as grow-diag final (koehn et al , 2003), <papid> N03-1017 </papid>to resolve them.</nextsent>
<nextsent>some aligned corpora include implicit partial alignment annotation, while for other corpora apart ial alignment can be extracted by state-of the-art techniques.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1363">
<title id=" W10-4006.xml">multiword expression sensitive word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this approach could be viewed as the opposite of current trend sin clir on semantic space that incorporate notion of order in the bag-of-words model (e.g. co-occurences).
</prevsent>
<prevsent>word alignment (brown et al , 1993; <papid> J93-2003 </papid>vogel et al ., 1996; <papid> C96-2141 </papid>och and ney, 2003<papid> J03-1002 </papid>a; graca et al ,2007) remains key to providing high-quality translations as all subsequent training stages relyon itsperformance.</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
it alone does not effectively capture many-to-many word correspondences, but instead relies on the ability of subsequent heuristic phrase extraction algorithms, such as grow-diag final (koehn et al , 2003), <papid> N03-1017 </papid>to resolve them.</citsent>
<aftsection>
<nextsent>some aligned corpora include implicit partial alignment annotation, while for other corpora apart ial alignment can be extracted by state-of the-art techniques.
</nextsent>
<nextsent>for example, implicit tags such as reference number within the patent corpus of fujii et al  (2010) provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting partial annotation, like kupiec et al  (1993),extract terminology pairs using linguistically predefined pos patterns.
</nextsent>
<nextsent>gale and church (1991)<papid> P91-1023 </papid>extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information.</nextsent>
<nextsent>resnik and melamed (1997) <papid> A97-1050 </papid>automatically extract domain specific lexica.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1364">
<title id=" W10-4006.xml">multiword expression sensitive word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some aligned corpora include implicit partial alignment annotation, while for other corpora apart ial alignment can be extracted by state-of the-art techniques.
</prevsent>
<prevsent>for example, implicit tags such as reference number within the patent corpus of fujii et al  (2010) provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting partial annotation, like kupiec et al  (1993),extract terminology pairs using linguistically predefined pos patterns.
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
gale and church (1991)<papid> P91-1023 </papid>extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information.</citsent>
<aftsection>
<nextsent>resnik and melamed (1997) <papid> A97-1050 </papid>automatically extract domain specific lexica.</nextsent>
<nextsent>moore (2003) <papid> E03-1035 </papid>extracts named entities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1365">
<title id=" W10-4006.xml">multiword expression sensitive word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, implicit tags such as reference number within the patent corpus of fujii et al  (2010) provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting partial annotation, like kupiec et al  (1993),extract terminology pairs using linguistically predefined pos patterns.
</prevsent>
<prevsent>gale and church (1991)<papid> P91-1023 </papid>extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information.</prevsent>
</prevsection>
<citsent citstr=" A97-1050 ">
resnik and melamed (1997) <papid> A97-1050 </papid>automatically extract domain specific lexica.</citsent>
<aftsection>
<nextsent>moore (2003) <papid> E03-1035 </papid>extracts named entities.</nextsent>
<nextsent>in machine translation, lambert andbanchs (2006) <papid> W06-2402 </papid>extract bmwes from phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process.this paper introduces new method of incorporating previously known many-to-many word correspondences into word alignment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1366">
<title id=" W10-4006.xml">multiword expression sensitive word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>gale and church (1991)<papid> P91-1023 </papid>extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information.</prevsent>
<prevsent>resnik and melamed (1997) <papid> A97-1050 </papid>automatically extract domain specific lexica.</prevsent>
</prevsection>
<citsent citstr=" E03-1035 ">
moore (2003) <papid> E03-1035 </papid>extracts named entities.</citsent>
<aftsection>
<nextsent>in machine translation, lambert andbanchs (2006) <papid> W06-2402 </papid>extract bmwes from phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process.this paper introduces new method of incorporating previously known many-to-many word correspondences into word alignment.</nextsent>
<nextsent>a well-known method of incorporating such prior knowledge in machine learning is to replace the likelihood maximization in the m-step of the em algorithm with either the map estimate or the maximum penalized likelihood (mpl) estimate (mclach 26 lan and krishnan, 1997; bishop, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1367">
<title id=" W10-4006.xml">multiword expression sensitive word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>resnik and melamed (1997) <papid> A97-1050 </papid>automatically extract domain specific lexica.</prevsent>
<prevsent>moore (2003) <papid> E03-1035 </papid>extracts named entities.</prevsent>
</prevsection>
<citsent citstr=" W06-2402 ">
in machine translation, lambert andbanchs (2006) <papid> W06-2402 </papid>extract bmwes from phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process.this paper introduces new method of incorporating previously known many-to-many word correspondences into word alignment.</citsent>
<aftsection>
<nextsent>a well-known method of incorporating such prior knowledge in machine learning is to replace the likelihood maximization in the m-step of the em algorithm with either the map estimate or the maximum penalized likelihood (mpl) estimate (mclach 26 lan and krishnan, 1997; bishop, 2006).
</nextsent>
<nextsent>then, the map estimate allows us to incorporate the prior,a probability used to reflect the degree of prior belief about the occurrences of the events.
</nextsent>
<nextsent>a small number of studies have been carried out that use partial alignment annotation for word alignment.
</nextsent>
<nextsent>firstly, graca et al  (2007) introduce posterior regularization to employ the prior that cannot be easily expressed over model parameters such as stochastic constraints and agreement constraints.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1370">
<title id=" W10-4006.xml">multiword expression sensitive word alignment </title>
<section> obtain alignment probabilities..  </section>
<citcontext>
<prevsection>
<prevsent>a typical example of the latter case is when asample of only few hundred lines from the corpus have been hand-annotated.
</prevsent>
<prevsent>4.1 mwe extraction.
</prevsent>
</prevsection>
<citsent citstr=" P93-1003 ">
our algorithm of extracting mwes is statistical method which is bidirectional version of ku piec (1993).<papid> P93-1003 </papid></citsent>
<aftsection>
<nextsent>firstly, kupiec presents method to extract bilingual mwe pairs in unidirectional manner based on the knowledge about typical pos patterns of noun phrases, which is language dependent but can be written down with some ease by linguistic expert.
</nextsent>
<nextsent>for example in french they are n, prep n, and adj.
</nextsent>
<nextsent>secondly, we take the intersection (or union) of extracted bilingual mwe pairs.22in word alignment, bidirectional word alignment by taking the intersection or union is standard method which improves its quality compared to unidirectional word alignment.
</nextsent>
<nextsent>29 algorithm 2 mwe extraction algorithm given: parallel corpus and set of anchor word alignment links: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1372">
<title id=" W10-4006.xml">multiword expression sensitive word alignment </title>
<section> obtain alignment probabilities..  </section>
<citcontext>
<prevsection>
<prevsent>from both sides.
</prevsent>
<prevsent>this is due to the fact that terminology is often labelled with unique reference number, which is labelled on both the sl and tl sides.
</prevsent>
</prevsection>
<citsent citstr=" P03-1010 ">
4.2 prior model i. prior for exhaustive alignment space ibm models 1 and 2 implement prior for all possible 3unlike other language pairs, the availability ofjapaneseenglish parallel corpora is quite limited: the nt cir patent corpus (fujii et al , 2010) of 3 million sentence pairs (the latest ntcir-8 version) for the patent domain andjenaad corpus (utiyama and isahara, 2003) <papid> P03-1010 </papid>of 150k sentence pairs for the news domain.</citsent>
<aftsection>
<nextsent>in this regard, the patent domain is particularly important for this particular language pair.
</nextsent>
<nextsent>algorithm 3 prior model for ibm model 1 given: parallel corpus e?, f?
</nextsent>
<nextsent>, anchor words biterm initialize t(e|f ) uniformly do until convergence set count(e|f ) to 0 for all e,f set total(f) to 0 for all for all sentence pairs (es,fs) prior(e|f)s = getpriormodeli(e?, f?
</nextsent>
<nextsent>, bit erm) for all words in es totals(e) = 0 for all words in fs totals(e) += t(e|f ) for all words in es for all words in fs count(e|f )+=t(e|f)/totals(e)?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1376">
<title id=" W11-0140.xml">an ontology based architecture for translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a key point in semantic interpretation is that the syntax-semantics interface used in the analysis is based on an ontology.
</prevsent>
<prevsent>the knowledge in the ontology concerns an application domain, i.e. weather forecasts, as well as more general information about the world: the latter information is used to compute the sentence meaning.
</prevsent>
</prevsection>
<citsent citstr=" W09-3726 ">
indeed, the sentence meaning consists of complex fragment of the ontology: predicate-argument structures and semantic roles are contained in this fragment and could be extracted by translating this fragment into usual first order logic predicates.3 the idea to use the onto logical paradigm to represent world knowledge as well as sentence meaning is similar to the work by nirenburg and raskin (2004) and buitelaar et al (2009), but in contrast to these approaches (1) we use syntactic parser to account for syntactic analysis; and (2) we use recursive semantic interpretation function similar to cimiano (2009).<papid> W09-3726 </papid></citsent>
<aftsection>
<nextsent>the onto logical knowledge base is formal (partial) description of the domain of application.
</nextsent>
<nextsent>it is formal, since its primitives are formally defined, and it is partial, since it does not include all axioms that provide details about the relationships between the involved concepts.
</nextsent>
<nextsent>the top level of the domain ontology is illustrated in fig.
</nextsent>
<nextsent>1.4 the classes most relevant to weather forecasts are meteo-status-situation , 1http://www.atlas.polito.it/ 2lis, as all the signed languages do not have natural writing form.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1380">
<title id=" W11-0103.xml">deterministic statistical mapping of sentences to underspecified semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it was later recognized that in order to support some tasks, fully specifying certain aspects of logic representation, such as quantifier scope, or reference resolution, is often not necessary.
</prevsent>
<prevsent>for example, for semantic translation, most ambiguities of quantifier scope can be carried over from the source language to the target language without being resolved.
</prevsent>
</prevsection>
<citsent citstr=" P92-1005 ">
this led to the development of underspecified semantic representations (e.g. qlf, alshawi and crouch (1992) <papid> P92-1005 </papid>and mrs, copestake et al(2005)) which are easier to produce from text without contextual inference but which can be further specified as necessary for the task being performed.</citsent>
<aftsection>
<nextsent>while traditionally mapping text to formal representations was predominantly rule-based, for both the syntactic and semantic components (montague (1973), pereira and shieber (1987), alshawi (1992)), good progress in statistical syntactic parsing (e.g. collins (1999), charniak (2000)) <papid> A00-2018 </papid>led to systems that applied rules for semantic interpretation to the output of statistical syntactic parser (e.g. bos et al (2004)).<papid> C04-1180 </papid></nextsent>
<nextsent>more recently researchers have looked at statistical methods to provide robust and trainable methods for mapping text to formal representations of meaning (zettlemoyer and collins, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1381">
<title id=" W11-0103.xml">deterministic statistical mapping of sentences to underspecified semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, for semantic translation, most ambiguities of quantifier scope can be carried over from the source language to the target language without being resolved.
</prevsent>
<prevsent>this led to the development of underspecified semantic representations (e.g. qlf, alshawi and crouch (1992) <papid> P92-1005 </papid>and mrs, copestake et al(2005)) which are easier to produce from text without contextual inference but which can be further specified as necessary for the task being performed.</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
while traditionally mapping text to formal representations was predominantly rule-based, for both the syntactic and semantic components (montague (1973), pereira and shieber (1987), alshawi (1992)), good progress in statistical syntactic parsing (e.g. collins (1999), charniak (2000)) <papid> A00-2018 </papid>led to systems that applied rules for semantic interpretation to the output of statistical syntactic parser (e.g. bos et al (2004)).<papid> C04-1180 </papid></citsent>
<aftsection>
<nextsent>more recently researchers have looked at statistical methods to provide robust and trainable methods for mapping text to formal representations of meaning (zettlemoyer and collins, 2005).
</nextsent>
<nextsent>in this paper we further develop the two strands of work mentioned above, i.e. mapping text to underspecified semantic representations and using statistical parsing methods to perform the analysis.
</nextsent>
<nextsent>15here we take more direct route, starting from scratch by designing an underspecified semantic representation (natural logical form, or nlf) that is purpose-built for statistical text-to-semantics mapping.
</nextsent>
<nextsent>an underspecified logic whose constructs are motivated by natural language and that is amenable to trainable direct semantic mapping from text without an intervening layer of syntactic representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1382">
<title id=" W11-0103.xml">deterministic statistical mapping of sentences to underspecified semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, for semantic translation, most ambiguities of quantifier scope can be carried over from the source language to the target language without being resolved.
</prevsent>
<prevsent>this led to the development of underspecified semantic representations (e.g. qlf, alshawi and crouch (1992) <papid> P92-1005 </papid>and mrs, copestake et al(2005)) which are easier to produce from text without contextual inference but which can be further specified as necessary for the task being performed.</prevsent>
</prevsection>
<citsent citstr=" C04-1180 ">
while traditionally mapping text to formal representations was predominantly rule-based, for both the syntactic and semantic components (montague (1973), pereira and shieber (1987), alshawi (1992)), good progress in statistical syntactic parsing (e.g. collins (1999), charniak (2000)) <papid> A00-2018 </papid>led to systems that applied rules for semantic interpretation to the output of statistical syntactic parser (e.g. bos et al (2004)).<papid> C04-1180 </papid></citsent>
<aftsection>
<nextsent>more recently researchers have looked at statistical methods to provide robust and trainable methods for mapping text to formal representations of meaning (zettlemoyer and collins, 2005).
</nextsent>
<nextsent>in this paper we further develop the two strands of work mentioned above, i.e. mapping text to underspecified semantic representations and using statistical parsing methods to perform the analysis.
</nextsent>
<nextsent>15here we take more direct route, starting from scratch by designing an underspecified semantic representation (natural logical form, or nlf) that is purpose-built for statistical text-to-semantics mapping.
</nextsent>
<nextsent>an underspecified logic whose constructs are motivated by natural language and that is amenable to trainable direct semantic mapping from text without an intervening layer of syntactic representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1383">
<title id=" W11-0103.xml">deterministic statistical mapping of sentences to underspecified semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>15here we take more direct route, starting from scratch by designing an underspecified semantic representation (natural logical form, or nlf) that is purpose-built for statistical text-to-semantics mapping.
</prevsent>
<prevsent>an underspecified logic whose constructs are motivated by natural language and that is amenable to trainable direct semantic mapping from text without an intervening layer of syntactic representation.
</prevsent>
</prevsection>
<citsent citstr=" D09-1001 ">
in contrast, the approach taken by (zettlemoyer and collins, 2005), for example, maps into traditional logic via lambda expressions, and the approach taken by (poon and domingos, 2009) <papid> D09-1001 </papid>depends on an initial step of syntactic parsing.</citsent>
<aftsection>
<nextsent>in this paper, we describe supervised training method for mapping text to nlf, that is, produc inga statistical model for this mapping starting from training pairs consisting of sentences and their corresponding nlf expressions.
</nextsent>
<nextsent>this method makes use of an encoding of nlf expressions into dependency trees in which the set of labels is automatically generated from the encoding process (rather than being pre-supplied by linguistically motivated dependency grammar).
</nextsent>
<nextsent>this encoding allows us to perform the text-to-nlf mapping using any existing statistical methods for labeled dependency parsing (e.g. eisner (1996), <papid> C96-1058 </papid>yamada and matsumoto (2003), mcdonald, crammer, pereira (2005)).</nextsent>
<nextsent>a side benefit of the encoding is that it leads to natural per-word measure for semantic mapping accuracy which we use for evaluation purposes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1384">
<title id=" W11-0103.xml">deterministic statistical mapping of sentences to underspecified semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we describe supervised training method for mapping text to nlf, that is, produc inga statistical model for this mapping starting from training pairs consisting of sentences and their corresponding nlf expressions.
</prevsent>
<prevsent>this method makes use of an encoding of nlf expressions into dependency trees in which the set of labels is automatically generated from the encoding process (rather than being pre-supplied by linguistically motivated dependency grammar).
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
this encoding allows us to perform the text-to-nlf mapping using any existing statistical methods for labeled dependency parsing (e.g. eisner (1996), <papid> C96-1058 </papid>yamada and matsumoto (2003), mcdonald, crammer, pereira (2005)).</citsent>
<aftsection>
<nextsent>a side benefit of the encoding is that it leads to natural per-word measure for semantic mapping accuracy which we use for evaluation purposes.
</nextsent>
<nextsent>by combing our method with deterministic statistical dependency models together with deterministic (hard) clusters instead of parts of speech, we obtain deterministic statistical text-to semantics mapper, opening the way to feasible mapping of text-to-semantics at large scale, for example the entire web.
</nextsent>
<nextsent>this paper concentrates on the text-to-semantics mapping which depends, in part, on some properties of nlf.
</nextsent>
<nextsent>we will not attempt to defend the semantic representation choices for specific constructions illustrated here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1386">
<title id=" W11-0103.xml">deterministic statistical mapping of sentences to underspecified semantics </title>
<section> semantic expressions.  </section>
<citcontext>
<prevsection>
<prevsent>note that some common logical operators are not built-in since they will appear directly as words such as not.2 we currently use the unknown/unspecified operator, %, mainly for linguistic constructions that are beyond the coverage of particular semantic mapping model.
</prevsent>
<prevsent>a simple example that includes % in our converted wsj corpus is other analysts are nearly as pessimistic for which the nlf expression is [are, analysts.other, pessimistic%nearly%as] in section 5 we give some statistics on the number of semantic expressions containing % in the data used for our experiments and explain how it affects our accruracy results.
</prevsent>
</prevsection>
<citsent citstr=" C02-1105 ">
1the term qlf is now sometimes used informally (e.g. liakata and pulman (2002), <papid> C02-1105 </papid>poon and domingos (2009)) <papid> D09-1001 </papid>for any logic-like semantic representation without explicit quantifier scope.</citsent>
<aftsection>
<nextsent>2nlf does include horn clauses, which implictly encode negation, but since horn clauses are not part of the experiments reported in this paper, we will not discuss them further here.
</nextsent>
<nextsent>17 [acquired /stealthily :[in, ?, 2002], chirpy+systems, companies.two :profitable :[producing, ?, pet+accessories]] figure 1: example of an nlf semantic expression.
</nextsent>
<nextsent>operator example denot ation language constructs [...]
</nextsent>
<nextsent>[sold, chirpy, growler] predication tuple clauses, prepositions, ...
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1388">
<title id=" W11-0103.xml">deterministic statistical mapping of sentences to underspecified semantics </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>since our main goal is to investigate trainable mappings from text strings to semantic expressions, we only use the wsj phrase structure trees in data preparation: the phrase structure trees are not used as inputs when training semantic mapping model, or when applying such model.
</prevsent>
<prevsent>for the same reason, in these experiments, we do not use the part-of-speech information associated with the phrase structure trees in training or applying semantic mapping model.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
instead of parts-of-speech we use word cluster features from hierarchical clustering produced with the unsupervised brown clustering method (brown et al 1992); <papid> J92-4003 </papid>specifically we use the publicly available clusters reported in koo et al (2008).<papid> P08-1068 </papid></citsent>
<aftsection>
<nextsent>constructions in the wsj that are beyond the explicit coverage of the conversion rules used for data preparation result in expressions that include the unknown/unspecified (or null?)
</nextsent>
<nextsent>operator %.
</nextsent>
<nextsent>we report on different experimental settings in which we vary how we treat training or testing expressions with %.
</nextsent>
<nextsent>this gives rise to the datasets in table 4 which have +null (i.e., including %), and -null (i.e., not including %) in the dataset names.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1389">
<title id=" W11-0103.xml">deterministic statistical mapping of sentences to underspecified semantics </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>since our main goal is to investigate trainable mappings from text strings to semantic expressions, we only use the wsj phrase structure trees in data preparation: the phrase structure trees are not used as inputs when training semantic mapping model, or when applying such model.
</prevsent>
<prevsent>for the same reason, in these experiments, we do not use the part-of-speech information associated with the phrase structure trees in training or applying semantic mapping model.
</prevsent>
</prevsection>
<citsent citstr=" P08-1068 ">
instead of parts-of-speech we use word cluster features from hierarchical clustering produced with the unsupervised brown clustering method (brown et al 1992); <papid> J92-4003 </papid>specifically we use the publicly available clusters reported in koo et al (2008).<papid> P08-1068 </papid></citsent>
<aftsection>
<nextsent>constructions in the wsj that are beyond the explicit coverage of the conversion rules used for data preparation result in expressions that include the unknown/unspecified (or null?)
</nextsent>
<nextsent>operator %.
</nextsent>
<nextsent>we report on different experimental settings in which we vary how we treat training or testing expressions with %.
</nextsent>
<nextsent>this gives rise to the datasets in table 4 which have +null (i.e., including %), and -null (i.e., not including %) in the dataset names.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1391">
<title id=" W11-0706.xml">detecting forum authority claims in online discussions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in multi-party discussions, language is used to establish identity, status, authority and connections with others in addition to communicating information and opinions.
</prevsent>
<prevsent>automatically extracting this type of social information in language from discussions is useful for understanding group interactions and relationships.the aspect of social communication most explored so far is the detection of participant role, particularly in spoken genres such as broadcast news, broadcast conversations, and meetings.
</prevsent>
</prevsection>
<citsent citstr=" N06-2021 ">
several studies have explored different types of features (lexical, prosodic, and turn-taking) in variety of statistical modeling frameworks (barzilay et al., 2000; maskey and hirschberg, 2006; liu, 2006; <papid> N06-2021 </papid>liu and liu, 2007; vinciarelli, 2007; laskowski et al., 2008; <papid> W08-0124 </papid>hutchinson et al, 2010).</citsent>
<aftsection>
<nextsent>typically, these studies assume that speaker inhabits role for the duration of the discussion, so multiple turns contribute to the decision.
</nextsent>
<nextsent>participant status is similar although the language of others is often more relevant than that of the participant in question.communication of other types of social information can be more localized.
</nextsent>
<nextsent>for example, an attempt to establish authority frequently occurs withina single sentence or turn when entering discussion, though authority bids may involve multiple turns when the participant is challenged.
</nextsent>
<nextsent>similarly, discussion participants may align with or distance themselves from other participants with single statement, or someone could agree with one person at particular point in the conversation and disagree with them at different point.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1392">
<title id=" W11-0706.xml">detecting forum authority claims in online discussions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in multi-party discussions, language is used to establish identity, status, authority and connections with others in addition to communicating information and opinions.
</prevsent>
<prevsent>automatically extracting this type of social information in language from discussions is useful for understanding group interactions and relationships.the aspect of social communication most explored so far is the detection of participant role, particularly in spoken genres such as broadcast news, broadcast conversations, and meetings.
</prevsent>
</prevsection>
<citsent citstr=" W08-0124 ">
several studies have explored different types of features (lexical, prosodic, and turn-taking) in variety of statistical modeling frameworks (barzilay et al., 2000; maskey and hirschberg, 2006; liu, 2006; <papid> N06-2021 </papid>liu and liu, 2007; vinciarelli, 2007; laskowski et al., 2008; <papid> W08-0124 </papid>hutchinson et al, 2010).</citsent>
<aftsection>
<nextsent>typically, these studies assume that speaker inhabits role for the duration of the discussion, so multiple turns contribute to the decision.
</nextsent>
<nextsent>participant status is similar although the language of others is often more relevant than that of the participant in question.communication of other types of social information can be more localized.
</nextsent>
<nextsent>for example, an attempt to establish authority frequently occurs withina single sentence or turn when entering discussion, though authority bids may involve multiple turns when the participant is challenged.
</nextsent>
<nextsent>similarly, discussion participants may align with or distance themselves from other participants with single statement, or someone could agree with one person at particular point in the conversation and disagree with them at different point.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1393">
<title id=" W11-0706.xml">detecting forum authority claims in online discussions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, an attempt to establish authority frequently occurs withina single sentence or turn when entering discussion, though authority bids may involve multiple turns when the participant is challenged.
</prevsent>
<prevsent>similarly, discussion participants may align with or distance themselves from other participants with single statement, or someone could agree with one person at particular point in the conversation and disagree with them at different point.
</prevsent>
</prevsection>
<citsent citstr=" W11-0707 ">
such localized phenomena are also important for understanding the broader context of that participants influence or role in the conversation (bunderson, 2003).in this paper, we focus on particular type of authority claim, namely forum claims, as defined in companion paper (bender et al, 2011).<papid> W11-0707 </papid></citsent>
<aftsection>
<nextsent>forum claims are based on policy, norms, or contextual rules of behavior in the interaction.
</nextsent>
<nextsent>in our experiments, we explore the phenomenon using wikipedia discussion (talk?)
</nextsent>
<nextsent>pages, which are discussions associated with wikipedia article in which changes to the article are debated by the editors in series of discussion threads.
</nextsent>
<nextsent>examples of such forum claims are: ? do think my understanding of wikipedia and policy is better than yours.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1399">
<title id=" W11-0706.xml">detecting forum authority claims in online discussions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, the annotation was performed at the discussion participant level, with evidence marked at the turn level with out distinguishing the different types of claims as in (bender et al, 2011).<papid> W11-0707 </papid></prevsent>
<prevsent>treating the problem of detecting forum claims as sentence-level classification problem is similar toother natural language processing tasks, such as sentiment classification.</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
early work in sentiment analysis used unigram features (pang and lee, 2004; <papid> P04-1035 </papid>pang and lee, 2005).<papid> P05-1015 </papid></citsent>
<aftsection>
<nextsent>however, error analyses suggested that highly accurate sentiment classification requires deeper understanding of the text, or at least higher order n-gram features.
</nextsent>
<nextsent>kim and hovy (2006) <papid> P06-2063 </papid>used unigrams, bigrams, and trigrams for extracting the polarity of online reviews.</nextsent>
<nextsent>gilbert et al (2009) employed weighted n-grams together with additional features to classify blog comments based on agreement polarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1400">
<title id=" W11-0706.xml">detecting forum authority claims in online discussions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, the annotation was performed at the discussion participant level, with evidence marked at the turn level with out distinguishing the different types of claims as in (bender et al, 2011).<papid> W11-0707 </papid></prevsent>
<prevsent>treating the problem of detecting forum claims as sentence-level classification problem is similar toother natural language processing tasks, such as sentiment classification.</prevsent>
</prevsection>
<citsent citstr=" P05-1015 ">
early work in sentiment analysis used unigram features (pang and lee, 2004; <papid> P04-1035 </papid>pang and lee, 2005).<papid> P05-1015 </papid></citsent>
<aftsection>
<nextsent>however, error analyses suggested that highly accurate sentiment classification requires deeper understanding of the text, or at least higher order n-gram features.
</nextsent>
<nextsent>kim and hovy (2006) <papid> P06-2063 </papid>used unigrams, bigrams, and trigrams for extracting the polarity of online reviews.</nextsent>
<nextsent>gilbert et al (2009) employed weighted n-grams together with additional features to classify blog comments based on agreement polarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1401">
<title id=" W11-0706.xml">detecting forum authority claims in online discussions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>early work in sentiment analysis used unigram features (pang and lee, 2004; <papid> P04-1035 </papid>pang and lee, 2005).<papid> P05-1015 </papid></prevsent>
<prevsent>however, error analyses suggested that highly accurate sentiment classification requires deeper understanding of the text, or at least higher order n-gram features.</prevsent>
</prevsection>
<citsent citstr=" P06-2063 ">
kim and hovy (2006) <papid> P06-2063 </papid>used unigrams, bigrams, and trigrams for extracting the polarity of online reviews.</citsent>
<aftsection>
<nextsent>gilbert et al (2009) employed weighted n-grams together with additional features to classify blog comments based on agreement polarity.
</nextsent>
<nextsent>we conjecture that authority claim detection will also benefit from moving beyond uni gram features.the focus of the paper is on two questions in feature extraction: ? can we exploit domain knowledge to address over training issues in sparse data conditions?
</nextsent>
<nextsent>is parse context more effective than n-gram context?
</nextsent>
<nextsent>our experiments compare the performance obtained using multiple methods for incorporating linguistic or data-driven knowledge and context into the feature space, relative to the baseline n-gram features.section 2 describes the general classification architecture.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1402">
<title id=" W11-0706.xml">detecting forum authority claims in online discussions </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>given the low amount of available training data, learning n-gram features we believe is likely to over train, due to the combinatorial explosion in the feature space.
</prevsent>
<prevsent>on the other hand, adding parse tree context information to each feature results in much smaller increase in feature space, due to the smaller number of non-terminal tokens as compared to the vocabulary size.
</prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
to extract such features, the data was run through version of the berkeley parser(petrov et al, 2006) <papid> P06-1055 </papid>trained on the wall street journal portion of the penn treebank.</citsent>
<aftsection>
<nextsent>for each sentence, the one-best parse was used to extract the list of non-terminals above each word in the sequence.
</nextsent>
<nextsent>the list was then filtered to shorter subset of non-terminal tags.
</nextsent>
<nextsent>the words augmented with non-terminal parse tree tags were treated as individual features and used in the usual way.
</nextsent>
<nextsent>we used context of at most three non-terminal tags (i.e. the pos tag and two additional levels if present.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1406">
<title id=" W11-0815.xml">identification and treatment of multiword expressions applied to information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the nature of mwes can be quite heterogeneous and each of the different classes has specific characteristics, posing challenge to the implementation of mechanisms that provide unified treatment for them.
</prevsent>
<prevsent>for instance, even if standard system capable of identifying boundaries between words, i.e.a tokenizer, may nevertheless be incapable of recognizing sequence of words as an mwes and treating them as single unit if necessary (e.g. to kick the bucket meaning to die).
</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
for an nlp application to be effective, it requires mechanisms that are able to identify mwes, handle them and make use of them in meaningful way (sag et al, 2002; baldwin et al., 2003).<papid> W03-1812 </papid></citsent>
<aftsection>
<nextsent>it is estimated that the number of mwes in the lexicon of native speaker of language hasthe same order of magnitude as the number of single words (jackendoff, 1997).
</nextsent>
<nextsent>however, these ratios are probably underestimated when considering domain-specific language, in which the specialized vocabulary and terminology are composed mostly by mwes.
</nextsent>
<nextsent>in this paper, we perform an application-oriented evaluation of the inclusion of mwe treatment into an information retrieval (ir) system.
</nextsent>
<nextsent>ir systems aim to provide users with quick access to data they are interested (baeza-yates and ribeiro-neto,1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1407">
<title id=" W11-0815.xml">identification and treatment of multiword expressions applied to information retrieval </title>
<section> multiword expressions.  </section>
<citcontext>
<prevsection>
<prevsent>the automatic discovery of specific types ofmwes has attracted the attention of many researchers in nlp over the past years.
</prevsent>
<prevsent>with the recent increase inefficiency and accuracy of techniques for preprocessing texts, such as tagging and parsing,these can become an aid in improving the performance of mwe detection techniques.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
in terms of practical mwe identification systems, well known approach is that of smadja (1993), <papid> J93-1007 </papid>who uses setof techniques based on statistical methods, calculated from word frequencies, to identify mwes incorpora.</citsent>
<aftsection>
<nextsent>this approach is implemented in lexicographic tool called xtract.
</nextsent>
<nextsent>more recently there has been the release of the mwe toolkit (ramisch et al, 2010) <papid> C10-3015 </papid>for the automatic extraction of mwes from monolingual corpora, that both generates and validates mwe candidates.</nextsent>
<nextsent>as generation is based on surface forms, for the validation, series of criteria for removing noise are provided, including some (language independent) association measures suchas mutual information, dice coefficient and maximum likelihood.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1408">
<title id=" W11-0815.xml">identification and treatment of multiword expressions applied to information retrieval </title>
<section> multiword expressions.  </section>
<citcontext>
<prevsection>
<prevsent>in terms of practical mwe identification systems, well known approach is that of smadja (1993), <papid> J93-1007 </papid>who uses setof techniques based on statistical methods, calculated from word frequencies, to identify mwes incorpora.</prevsent>
<prevsent>this approach is implemented in lexicographic tool called xtract.</prevsent>
</prevsection>
<citsent citstr=" C10-3015 ">
more recently there has been the release of the mwe toolkit (ramisch et al, 2010) <papid> C10-3015 </papid>for the automatic extraction of mwes from monolingual corpora, that both generates and validates mwe candidates.</citsent>
<aftsection>
<nextsent>as generation is based on surface forms, for the validation, series of criteria for removing noise are provided, including some (language independent) association measures suchas mutual information, dice coefficient and maximum likelihood.
</nextsent>
<nextsent>several other researchers have proposed number of computational techniques that deal with the discovery of mwes: baldwin and villavicencio (2002) <papid> W02-2001 </papid>for verb-particle constructions,pearce (2002) and evert and krenn (2005) for collocations, nicholson and baldwin (2006) <papid> W06-1208 </papid>for compound nouns and many others.for our experiments, we used some standard statistical measures such as mutual information, pointwise mutual information, chi-square, permutation entropy (zhang et al, 2006), <papid> W06-1206 </papid>dice coefficient, andt-test to extract mwes from collection of documents (i.e. we consider the collection of documents indexed by their system as our corpus).</nextsent>
<nextsent>based on the hypothesis that the mwes can improve the results of ir systems, we carried out an evaluation experiment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1409">
<title id=" W11-0815.xml">identification and treatment of multiword expressions applied to information retrieval </title>
<section> multiword expressions.  </section>
<citcontext>
<prevsection>
<prevsent>more recently there has been the release of the mwe toolkit (ramisch et al, 2010) <papid> C10-3015 </papid>for the automatic extraction of mwes from monolingual corpora, that both generates and validates mwe candidates.</prevsent>
<prevsent>as generation is based on surface forms, for the validation, series of criteria for removing noise are provided, including some (language independent) association measures suchas mutual information, dice coefficient and maximum likelihood.</prevsent>
</prevsection>
<citsent citstr=" W02-2001 ">
several other researchers have proposed number of computational techniques that deal with the discovery of mwes: baldwin and villavicencio (2002) <papid> W02-2001 </papid>for verb-particle constructions,pearce (2002) and evert and krenn (2005) for collocations, nicholson and baldwin (2006) <papid> W06-1208 </papid>for compound nouns and many others.for our experiments, we used some standard statistical measures such as mutual information, pointwise mutual information, chi-square, permutation entropy (zhang et al, 2006), <papid> W06-1206 </papid>dice coefficient, andt-test to extract mwes from collection of documents (i.e. we consider the collection of documents indexed by their system as our corpus).</citsent>
<aftsection>
<nextsent>based on the hypothesis that the mwes can improve the results of ir systems, we carried out an evaluation experiment.
</nextsent>
<nextsent>the goal of our evaluation is to detect differences between the quality of the standard ir system, without any treatment for mwes,and the same system improved with the identification of mwes in the queries and in the documents.
</nextsent>
<nextsent>in this section we describe the different resources and methods used in the experiments.
</nextsent>
<nextsent>3.1 resources and tools.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1410">
<title id=" W11-0815.xml">identification and treatment of multiword expressions applied to information retrieval </title>
<section> multiword expressions.  </section>
<citcontext>
<prevsection>
<prevsent>more recently there has been the release of the mwe toolkit (ramisch et al, 2010) <papid> C10-3015 </papid>for the automatic extraction of mwes from monolingual corpora, that both generates and validates mwe candidates.</prevsent>
<prevsent>as generation is based on surface forms, for the validation, series of criteria for removing noise are provided, including some (language independent) association measures suchas mutual information, dice coefficient and maximum likelihood.</prevsent>
</prevsection>
<citsent citstr=" W06-1208 ">
several other researchers have proposed number of computational techniques that deal with the discovery of mwes: baldwin and villavicencio (2002) <papid> W02-2001 </papid>for verb-particle constructions,pearce (2002) and evert and krenn (2005) for collocations, nicholson and baldwin (2006) <papid> W06-1208 </papid>for compound nouns and many others.for our experiments, we used some standard statistical measures such as mutual information, pointwise mutual information, chi-square, permutation entropy (zhang et al, 2006), <papid> W06-1206 </papid>dice coefficient, andt-test to extract mwes from collection of documents (i.e. we consider the collection of documents indexed by their system as our corpus).</citsent>
<aftsection>
<nextsent>based on the hypothesis that the mwes can improve the results of ir systems, we carried out an evaluation experiment.
</nextsent>
<nextsent>the goal of our evaluation is to detect differences between the quality of the standard ir system, without any treatment for mwes,and the same system improved with the identification of mwes in the queries and in the documents.
</nextsent>
<nextsent>in this section we describe the different resources and methods used in the experiments.
</nextsent>
<nextsent>3.1 resources and tools.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1411">
<title id=" W11-0815.xml">identification and treatment of multiword expressions applied to information retrieval </title>
<section> multiword expressions.  </section>
<citcontext>
<prevsection>
<prevsent>more recently there has been the release of the mwe toolkit (ramisch et al, 2010) <papid> C10-3015 </papid>for the automatic extraction of mwes from monolingual corpora, that both generates and validates mwe candidates.</prevsent>
<prevsent>as generation is based on surface forms, for the validation, series of criteria for removing noise are provided, including some (language independent) association measures suchas mutual information, dice coefficient and maximum likelihood.</prevsent>
</prevsection>
<citsent citstr=" W06-1206 ">
several other researchers have proposed number of computational techniques that deal with the discovery of mwes: baldwin and villavicencio (2002) <papid> W02-2001 </papid>for verb-particle constructions,pearce (2002) and evert and krenn (2005) for collocations, nicholson and baldwin (2006) <papid> W06-1208 </papid>for compound nouns and many others.for our experiments, we used some standard statistical measures such as mutual information, pointwise mutual information, chi-square, permutation entropy (zhang et al, 2006), <papid> W06-1206 </papid>dice coefficient, andt-test to extract mwes from collection of documents (i.e. we consider the collection of documents indexed by their system as our corpus).</citsent>
<aftsection>
<nextsent>based on the hypothesis that the mwes can improve the results of ir systems, we carried out an evaluation experiment.
</nextsent>
<nextsent>the goal of our evaluation is to detect differences between the quality of the standard ir system, without any treatment for mwes,and the same system improved with the identification of mwes in the queries and in the documents.
</nextsent>
<nextsent>in this section we describe the different resources and methods used in the experiments.
</nextsent>
<nextsent>3.1 resources and tools.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1412">
<title id=" W11-0412.xml">assessing the practical usability of an automatically annotated corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this is because human experts are asked to follow specific annotation guidelines.rebholz-schuhmann and hahn (2010c) did an intrinsic evaluation of the ssc where they created an 4http://jura.wi.mit.edu/entrez gene/ 5http://www.uniprot.org/6see proceedings of the 1st calbc work shop, 2010, editors: dietrich rebholz-schuhmannand udo hahn (http://www.ebi.ac.uk/rebholz srv/calbc/docs/firstproceedings.pdf) for details.ssc and gsc on dataset of 3,236 medline7 abstracts.
</prevsent>
<prevsent>they were not able to make any specific conclusion whether the ssc is approaching to the gsc.
</prevsent>
</prevsection>
<citsent citstr=" W10-1838 ">
they were of the opinion that ssc annotations are more similar to terminological resources.hahn et al (2010) <papid> W10-1838 </papid>proposed policy where silver standards can be dynamically optimized and customized on demand (given specific goal function)using gold standard as an oracle.</citsent>
<aftsection>
<nextsent>the gold standard is used for optimization only, not for training for the purpose of ssc annotation.
</nextsent>
<nextsent>they argued thatthe nature of diverging tasks to be solved, the levels of specificity to be reached, the sort of guidelines being preferred, etc should allow prospective users of an ssc to customize one on their own and not stick to something that is already prefabricated without concrete application in mind.self-training and co-training are two of the existing approaches that have been used for compensating the lack of training gsc with adequate sizein several different tasks such as word sense disambiguation, semantic role labelling, parsing, etc (ngand cardie, 2003; <papid> N03-1023 </papid>pierce and cardie, 2004; mcclosky et al, 2006; <papid> P06-1043 </papid>he and gildea, 2006).</nextsent>
<nextsent>according to ng and cardie (2003), <papid> N03-1023 </papid>self-training is the procedure where committee of classifiers are trained on the (gold) annotated examples to tag unannotated examples independently.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1413">
<title id=" W11-0412.xml">assessing the practical usability of an automatically annotated corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they were of the opinion that ssc annotations are more similar to terminological resources.hahn et al (2010) <papid> W10-1838 </papid>proposed policy where silver standards can be dynamically optimized and customized on demand (given specific goal function)using gold standard as an oracle.</prevsent>
<prevsent>the gold standard is used for optimization only, not for training for the purpose of ssc annotation.</prevsent>
</prevsection>
<citsent citstr=" N03-1023 ">
they argued thatthe nature of diverging tasks to be solved, the levels of specificity to be reached, the sort of guidelines being preferred, etc should allow prospective users of an ssc to customize one on their own and not stick to something that is already prefabricated without concrete application in mind.self-training and co-training are two of the existing approaches that have been used for compensating the lack of training gsc with adequate sizein several different tasks such as word sense disambiguation, semantic role labelling, parsing, etc (ngand cardie, 2003; <papid> N03-1023 </papid>pierce and cardie, 2004; mcclosky et al, 2006; <papid> P06-1043 </papid>he and gildea, 2006).</citsent>
<aftsection>
<nextsent>according to ng and cardie (2003), <papid> N03-1023 </papid>self-training is the procedure where committee of classifiers are trained on the (gold) annotated examples to tag unannotated examples independently.</nextsent>
<nextsent>only those new annotations to which all the classifiers agree are added to the training set and classifiers are retrained.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1414">
<title id=" W11-0412.xml">assessing the practical usability of an automatically annotated corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they were of the opinion that ssc annotations are more similar to terminological resources.hahn et al (2010) <papid> W10-1838 </papid>proposed policy where silver standards can be dynamically optimized and customized on demand (given specific goal function)using gold standard as an oracle.</prevsent>
<prevsent>the gold standard is used for optimization only, not for training for the purpose of ssc annotation.</prevsent>
</prevsection>
<citsent citstr=" P06-1043 ">
they argued thatthe nature of diverging tasks to be solved, the levels of specificity to be reached, the sort of guidelines being preferred, etc should allow prospective users of an ssc to customize one on their own and not stick to something that is already prefabricated without concrete application in mind.self-training and co-training are two of the existing approaches that have been used for compensating the lack of training gsc with adequate sizein several different tasks such as word sense disambiguation, semantic role labelling, parsing, etc (ngand cardie, 2003; <papid> N03-1023 </papid>pierce and cardie, 2004; mcclosky et al, 2006; <papid> P06-1043 </papid>he and gildea, 2006).</citsent>
<aftsection>
<nextsent>according to ng and cardie (2003), <papid> N03-1023 </papid>self-training is the procedure where committee of classifiers are trained on the (gold) annotated examples to tag unannotated examples independently.</nextsent>
<nextsent>only those new annotations to which all the classifiers agree are added to the training set and classifiers are retrained.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1416">
<title id=" W11-0412.xml">assessing the practical usability of an automatically annotated corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>only those new annotations to which all the classifiers agree are added to the training set and classifiers are retrained.
</prevsent>
<prevsent>this procedure repeats until stop condition is met.
</prevsent>
</prevsection>
<citsent citstr=" W03-0407 ">
according to clark et al (2003), <papid> W03-0407 </papid>self-training is procedure in which tagger is retrained on its own labeled cache at each round?.</citsent>
<aftsection>
<nextsent>in other words, single classifier is trained on the initially (gold) annotated data and then applied on set of unannotated data.
</nextsent>
<nextsent>those examples meeting selection criterion are added to the annotated dataset and the classifier is retrained on this new dataset.
</nextsent>
<nextsent>this procedure can continue for several rounds as required.co-training is another weakly supervised approach (blum and mitchell, 1998).
</nextsent>
<nextsent>it applies for those tasks where each of the two (or more) sets of features from the initially (gold) annotated training data is sufficient to classify/annotate the unannotated data (pierce and cardie, 2001; <papid> W01-0501 </papid>pierce and cardie, 7http://www.nlm.nih.gov/databases/databases medline.html 1032004; he and gildea, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1417">
<title id=" W11-0412.xml">assessing the practical usability of an automatically annotated corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>those examples meeting selection criterion are added to the annotated dataset and the classifier is retrained on this new dataset.
</prevsent>
<prevsent>this procedure can continue for several rounds as required.co-training is another weakly supervised approach (blum and mitchell, 1998).
</prevsent>
</prevsection>
<citsent citstr=" W01-0501 ">
it applies for those tasks where each of the two (or more) sets of features from the initially (gold) annotated training data is sufficient to classify/annotate the unannotated data (pierce and cardie, 2001; <papid> W01-0501 </papid>pierce and cardie, 7http://www.nlm.nih.gov/databases/databases medline.html 1032004; he and gildea, 2006).</citsent>
<aftsection>
<nextsent>as with ssc annotation and self-training, it also attempts to increase the amount of annotated data by making use of unannotated data.
</nextsent>
<nextsent>the main idea of co-training is to represent the initially annotated data using two (or more) separate feature sets, each called view?.
</nextsent>
<nextsent>then, two (or more) classifiers are trained on those viewsof the data which are then used to tag new unannotated data.
</nextsent>
<nextsent>from this newly annotated data, the most confident predictions are added to the previously annotated data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1418">
<title id=" W11-0412.xml">assessing the practical usability of an automatically annotated corpus </title>
<section> experimental settings.  </section>
<citcontext>
<prevsection>
<prevsent>the ssc has in total 137,610 gene annotations in 316,869 sentences of 50,000 abstracts.generally, using customized dictionary of entity names along with annotated corpus boosts nerperformance.
</prevsent>
<prevsent>however, since our objective is to observe to what extent ml system can learn from ssc, we avoid the use of any dictionary.
</prevsent>
</prevsection>
<citsent citstr=" W10-1911 ">
we use an open source ml based bner system named bioenex8 (chowdhury and lavelli, 2010).<papid> W10-1911 </papid></citsent>
<aftsection>
<nextsent>the system uses conditional random fields (crfs), and achieves comparable results (f1 score of 86.22% on the bio creative ii gm test corpus) to that of the other state-of-the-art systems without using any dictionary or lexicon.
</nextsent>
<nextsent>one of the complex issues inner is to come to an agreement regarding the boundaries of entity mentions.
</nextsent>
<nextsent>different annotation guidelines have differentpreferences.
</nextsent>
<nextsent>there may be tasks where longer entity mention such as human il-7 protein?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1420">
<title id=" W11-0406.xml">an annotation scheme for automated bias detection in wikipedia </title>
<section> motivation and background.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 background.
</prevsent>
<prevsent>sentiment analysis efforts usually relyon the prior polarity of words (their polarity out ofcontext).
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
for example, turney (2002) <papid> P02-1053 </papid>proposes method to classify reviews as rec ommended?/not recommended?, based on the average semantic orientation of the review.semantic orientation is the mutual information measure of selected phrases with the word excellent minus their mutual information with the word poor.</citsent>
<aftsection>
<nextsent>however, as wilson etal.
</nextsent>
<nextsent>(2005) point out, even using lexicon of positive/negative words marked for their prior polarity is merely starting point, since words polarity in context might differ from its prior polarity.
</nextsent>
<nextsent>the distinction between prior and contextual polarity is crucial for detecting bias, since words with prior positive/negative polarity may ormay not convey bias, depending on their context.
</nextsent>
<nextsent>notably, the inverse is also true - generally neutral words can be used to create favorable tone towards sentences topic, thereby expressing bias.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1421">
<title id=" W11-0406.xml">an annotation scheme for automated bias detection in wikipedia </title>
<section> motivation and background.  </section>
<citcontext>
<prevsection>
<prevsent>clearly, this phrase does not have contextual polarity, but it does introduce bias.
</prevsent>
<prevsent>within the realm of wikipedia, phrases that create an impression that something specific and meaningful has been said when only vague or ambiguous claim has been communicated, such as it is widely believed, are referred to as weasels (wikipedia, 2011e).
</prevsent>
</prevsection>
<citsent citstr=" W10-3001 ">
the recent conll-2010shared task (farkas et al , 2010), <papid> W10-3001 </papid>aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information.</citsent>
<aftsection>
<nextsent>in the same vein, we include weasel words as part of our annotation scheme to detect bias.finally, as blitzer et al  (2007) <papid> P07-1056 </papid>point out, although the typical word-level analysis captures the finer-grained aspects of sentiment language, it falls short in capturing broader structurally orcontextually-based bias.</nextsent>
<nextsent>bias can also be introduced by repetitive usage of words that in typical usage do not have prior polarity, but when used in repetitive manner, create favorable depiction of sentences topic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1422">
<title id=" W11-0406.xml">an annotation scheme for automated bias detection in wikipedia </title>
<section> motivation and background.  </section>
<citcontext>
<prevsection>
<prevsent>within the realm of wikipedia, phrases that create an impression that something specific and meaningful has been said when only vague or ambiguous claim has been communicated, such as it is widely believed, are referred to as weasels (wikipedia, 2011e).
</prevsent>
<prevsent>the recent conll-2010shared task (farkas et al , 2010), <papid> W10-3001 </papid>aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information.</prevsent>
</prevsection>
<citsent citstr=" P07-1056 ">
in the same vein, we include weasel words as part of our annotation scheme to detect bias.finally, as blitzer et al  (2007) <papid> P07-1056 </papid>point out, although the typical word-level analysis captures the finer-grained aspects of sentiment language, it falls short in capturing broader structurally orcontextually-based bias.</citsent>
<aftsection>
<nextsent>bias can also be introduced by repetitive usage of words that in typical usage do not have prior polarity, but when used in repetitive manner, create favorable depiction of sentences topic.
</nextsent>
<nextsent>this cannot be captured by approaches such as those of wilson et al  or turney.
</nextsent>
<nextsent>48 to tackle cases like those described above, our annotation scheme extends beyond lexical tags,and includes tags that capture dependencies between word and its context, as well as tags that are aimed at capturing subtle expressions of bias.
</nextsent>
<nextsent>3.1 corpus selection and preparation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1425">
<title id=" W11-0707.xml">annotating social acts authority claims and alignment moves in wikipedia talk pages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>authority claims and alignment moves, on the other hand, are examples of communicative moves aimed at social positioning of discus sant within group of participants, which may be specialized dialog acts but are referred to here as social acts.?
</prevsent>
<prevsent>we distinguish social acts from social events?
</prevsent>
</prevsection>
<citsent citstr=" D10-1100 ">
as described in (agarwal and rambow, 2010): <papid> D10-1100 </papid>social events correspond to types of interactions among people, whereas social actis associated with fine-grained social goal and reflected in the specific choices of words and orthographic or prosodic cues at the level of turn.the primary value of this new dataset is in facilitating computational modeling of new task type, i.e. the identification of fine-grained social acts in linguistic interaction.</citsent>
<aftsection>
<nextsent>while there has been some prior work on detecting agreements and disagree 48 ments in multiparty discussions (hillard et al, 2003; <papid> N03-2012 </papid>galley et al, 2004), <papid> P04-1085 </papid>which is related to detecting positive/negative alignment moves, most previous work on authority bids has involved descriptive studies, e.g.</nextsent>
<nextsent>(galegher et al, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1426">
<title id=" W11-0707.xml">annotating social acts authority claims and alignment moves in wikipedia talk pages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we distinguish social acts from social events?
</prevsent>
<prevsent>as described in (agarwal and rambow, 2010): <papid> D10-1100 </papid>social events correspond to types of interactions among people, whereas social actis associated with fine-grained social goal and reflected in the specific choices of words and orthographic or prosodic cues at the level of turn.the primary value of this new dataset is in facilitating computational modeling of new task type, i.e. the identification of fine-grained social acts in linguistic interaction.</prevsent>
</prevsection>
<citsent citstr=" N03-2012 ">
while there has been some prior work on detecting agreements and disagree 48 ments in multiparty discussions (hillard et al, 2003; <papid> N03-2012 </papid>galley et al, 2004), <papid> P04-1085 </papid>which is related to detecting positive/negative alignment moves, most previous work on authority bids has involved descriptive studies, e.g.</citsent>
<aftsection>
<nextsent>(galegher et al, 1998).
</nextsent>
<nextsent>computational modeling of these phenomena and automatic detection will help with understanding effective argumentation strategies in online discussions and automatic identification of divisive or controversial discussions and online trolls.
</nextsent>
<nextsent>we believe that these tasks also provide an interesting arena in which to study linguistic feature engineering and feature selection.
</nextsent>
<nextsent>aswith tasks such as sentiment analysis, simple bag of-words?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1427">
<title id=" W11-0707.xml">annotating social acts authority claims and alignment moves in wikipedia talk pages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we distinguish social acts from social events?
</prevsent>
<prevsent>as described in (agarwal and rambow, 2010): <papid> D10-1100 </papid>social events correspond to types of interactions among people, whereas social actis associated with fine-grained social goal and reflected in the specific choices of words and orthographic or prosodic cues at the level of turn.the primary value of this new dataset is in facilitating computational modeling of new task type, i.e. the identification of fine-grained social acts in linguistic interaction.</prevsent>
</prevsection>
<citsent citstr=" P04-1085 ">
while there has been some prior work on detecting agreements and disagree 48 ments in multiparty discussions (hillard et al, 2003; <papid> N03-2012 </papid>galley et al, 2004), <papid> P04-1085 </papid>which is related to detecting positive/negative alignment moves, most previous work on authority bids has involved descriptive studies, e.g.</citsent>
<aftsection>
<nextsent>(galegher et al, 1998).
</nextsent>
<nextsent>computational modeling of these phenomena and automatic detection will help with understanding effective argumentation strategies in online discussions and automatic identification of divisive or controversial discussions and online trolls.
</nextsent>
<nextsent>we believe that these tasks also provide an interesting arena in which to study linguistic feature engineering and feature selection.
</nextsent>
<nextsent>aswith tasks such as sentiment analysis, simple bag of-words?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1428">
<title id=" W11-0707.xml">annotating social acts authority claims and alignment moves in wikipedia talk pages </title>
<section> the corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the most popular approach to measuring annotation quality is via the surrogate of annotation consistency.
</prevsent>
<prevsent>this assumes that when annotators working independently arrive at the same decisions they have correctly carried outthe task specified by the annotation guidelines.
</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
several quantitative measures of annotator consistency have been proposed and debated over the years (art stein and poesio, 2008).<papid> J08-4004 </papid></citsent>
<aftsection>
<nextsent>we use the well-knowncohens kappa coefficient ?, which accounts for uneven class priors, so one may obtain low agreement score even when high percentage of tokens havethe same label.
</nextsent>
<nextsent>we also report the percentage of instances on which the annotators agreed, a, which includes agreement on the absence of particular label.
</nextsent>
<nextsent>when set of instances have been labeled by more than two annotators, we compute the average of pairwise agreement.
</nextsent>
<nextsent>scores for authority claim and alignment move agreement are presented in tables 2 and 3.2 for2institutional claims are exceedingly rare in our data, appearing in only three labels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1430">
<title id=" W11-0326.xml">composing simple image descriptions using web scale ngrams </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of animage, while permitting creativity in the description ? making for more human-like annotations than previous approaches.
</prevsent>
<prevsent>gaining better understanding of natural language,and especially natural language associated with images helps drive research in both computervision and natural language processing (e.g., barnard et al.
</prevsent>
</prevsection>
<citsent citstr=" P10-1126 ">
(2003), pastra et al (2003), feng and lapata (2010<papid> P10-1126 </papid>b)).</citsent>
<aftsection>
<nextsent>in this paper, we look at how to exploit the enormous amount of textual data electronically available today, web-scale n-gram data in particular,in simple yet highly effective approach to compose image descriptions in natural language.
</nextsent>
<nextsent>automatic generation of image descriptions differs from automatic image tagging (e.g., leong et al (2010))<papid> C10-2074 </papid>in that we aim to generate complex phrases or sentences describing images rather than predicting individual words.</nextsent>
<nextsent>these natural language description scan be useful for variety of applications, including image retrieval, automatic video surveillance,and providing image interpretations for visually impaired people.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1436">
<title id=" W11-0326.xml">composing simple image descriptions using web scale ngrams </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(2003), pastra et al (2003), feng and lapata (2010<papid> P10-1126 </papid>b)).</prevsent>
<prevsent>in this paper, we look at how to exploit the enormous amount of textual data electronically available today, web-scale n-gram data in particular,in simple yet highly effective approach to compose image descriptions in natural language.</prevsent>
</prevsection>
<citsent citstr=" C10-2074 ">
automatic generation of image descriptions differs from automatic image tagging (e.g., leong et al (2010))<papid> C10-2074 </papid>in that we aim to generate complex phrases or sentences describing images rather than predicting individual words.</citsent>
<aftsection>
<nextsent>these natural language description scan be useful for variety of applications, including image retrieval, automatic video surveillance,and providing image interpretations for visually impaired people.
</nextsent>
<nextsent>our work contrasts to most previous approaches in four key aspects: first, we compose fresh sentences from scratch, instead of retrieving (farhadi et al.
</nextsent>
<nextsent>(2010)), or summarizing existing text fragments associated with an image (e.g., aker and gaizauskas (2010), <papid> P10-1127 </papid>feng and lapata (2010<papid> P10-1126 </papid>a)).</nextsent>
<nextsent>second, we aim to generate textual descriptions that are truthful to the specific content of the image, whereas related(but subtly different) work in automatic caption generation creates news-worthy text (feng and lapata (2010<papid> P10-1126 </papid>a)) or encyclopedic text (aker and gaizauskas (2010)) <papid> P10-1127 </papid>that is contextually relevant to the image, but not closely pertinent to the specific content of theimage.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1437">
<title id=" W11-0326.xml">composing simple image descriptions using web scale ngrams </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these natural language description scan be useful for variety of applications, including image retrieval, automatic video surveillance,and providing image interpretations for visually impaired people.
</prevsent>
<prevsent>our work contrasts to most previous approaches in four key aspects: first, we compose fresh sentences from scratch, instead of retrieving (farhadi et al.
</prevsent>
</prevsection>
<citsent citstr=" P10-1127 ">
(2010)), or summarizing existing text fragments associated with an image (e.g., aker and gaizauskas (2010), <papid> P10-1127 </papid>feng and lapata (2010<papid> P10-1126 </papid>a)).</citsent>
<aftsection>
<nextsent>second, we aim to generate textual descriptions that are truthful to the specific content of the image, whereas related(but subtly different) work in automatic caption generation creates news-worthy text (feng and lapata (2010<papid> P10-1126 </papid>a)) or encyclopedic text (aker and gaizauskas (2010)) <papid> P10-1127 </papid>that is contextually relevant to the image, but not closely pertinent to the specific content of theimage.</nextsent>
<nextsent>third, we aim to build general image description method as compared to work that requires domain specific hand-written grammar rules (yao et al.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1451">
<title id=" W11-0326.xml">composing simple image descriptions using web scale ngrams </title>
<section> baseline approaches to surface.  </section>
<citcontext>
<prevsection>
<prevsent>however, optimizing both language-model-based probabilities and parser-based probabilities is intractable.
</prevsent>
<prevsent>therefore, we explore randomized local search approach that makes greedy revisions using both language model sand parsers.
</prevsent>
</prevsection>
<citsent citstr=" W02-0402 ">
randomized local search has been successfully applied to intractable optimization problems in ai (e.g., chisholm and tadepalli (2002)) and nlp (e.g., white and cardie (2002)).<papid> W02-0402 </papid></citsent>
<aftsection>
<nextsent>table 1 shows the skeleton of the algorithm in our study.
</nextsent>
<nextsent>ite rating through loop, it chooses an edit location and an edit operation (insert, delete, or re place) at random.
</nextsent>
<nextsent>if the edit yields better score, then we commit the edit, otherwise we jump to the next iteration of the loop.
</nextsent>
<nextsent>we define the score as score(x) = plm (x)ppcfg(x) where is given sentence (image description), plm (x) is the length normalized probability of based on the language model, and ppcfg(x) is the length normalized probability of based on the probabilistic context free grammar (pcfg) model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1452">
<title id=" W11-0326.xml">composing simple image descriptions using web scale ngrams </title>
<section> baseline approaches to surface.  </section>
<citcontext>
<prevsection>
<prevsent>the loop is repeated until convergence or fixed number of iterations is reached.
</prevsent>
<prevsent>note that this approach can be extended to simulated annealing to allow temporary downward steps to escape from local maxima.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we use the pcfg implementation of klein and manning (2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>3.3 template based approach.
</nextsent>
<nextsent>the third approach is template-based approach with linguistic constraints, technique that has of ten been used for various practical applications suchas summarization (zhou and hovy, 2004) <papid> W04-1010 </papid>and dia 222 blue, bike [2669] blue, bicycle [1365] bike, blue [1184] blue, cycle [324] cycle, of, the, blue [172] cycle, blue [158] bicycle, blue [154] bike, in, blue [98] cycle, of, blue [64] bike, with, blue [43]     blue , bicycle  , near,   shiny , person     bright, boy [8092] bright, child [7840] bright, girl [6191] bright, kid [5873] bright, person [5461 ] bright, man [4936] bright, woman [2726] bright, women [1684] lady, bright [1360] bright, men [1050] person, operating, a, bicycle [3409] man, on, a, bicycle [2842] cycle, of, child [2507] bike, for, men [2485] person, riding, a, bicycle [2118] cycle, in, women [1853] bike, for, women [1442] boy, on, a, bicycle [1378] cycle, of, women [1288] man, on, a, bike [1283] bright person operating blue bicycle [2541 158 938 5] bright man on blue bicycle [1914 83 72 88 0] bright man on blue bike [1690 24 78 07 2] bright person riding blue bicycle [157 881 332 70] bright boy on blue bicycle [1522 08 09 24 0] blue bike for bright men [6964 08 82 50 ] blue bike for bright women [648120 743 2] blue cycle of bright child [6368 18 11 20 ] blue cycle in bright women [1011 02 64 48 ] figure 2: illustration of phrase fusion compositional gorithm using web-scale n-grams.</nextsent>
<nextsent>numbers in square brackets are n-gram frequencies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1453">
<title id=" W11-0326.xml">composing simple image descriptions using web scale ngrams </title>
<section> baseline approaches to surface.  </section>
<citcontext>
<prevsection>
<prevsent>we use the pcfg implementation of klein and manning (2003).<papid> P03-1054 </papid></prevsent>
<prevsent>3.3 template based approach.</prevsent>
</prevsection>
<citsent citstr=" W04-1010 ">
the third approach is template-based approach with linguistic constraints, technique that has of ten been used for various practical applications suchas summarization (zhou and hovy, 2004) <papid> W04-1010 </papid>and dia 222 blue, bike [2669] blue, bicycle [1365] bike, blue [1184] blue, cycle [324] cycle, of, the, blue [172] cycle, blue [158] bicycle, blue [154] bike, in, blue [98] cycle, of, blue [64] bike, with, blue [43]     blue , bicycle  , near,   shiny , person     bright, boy [8092] bright, child [7840] bright, girl [6191] bright, kid [5873] bright, person [5461 ] bright, man [4936] bright, woman [2726] bright, women [1684] lady, bright [1360] bright, men [1050] person, operating, a, bicycle [3409] man, on, a, bicycle [2842] cycle, of, child [2507] bike, for, men [2485] person, riding, a, bicycle [2118] cycle, in, women [1853] bike, for, women [1442] boy, on, a, bicycle [1378] cycle, of, women [1288] man, on, a, bike [1283] bright person operating blue bicycle [2541 158 938 5] bright man on blue bicycle [1914 83 72 88 0] bright man on blue bike [1690 24 78 07 2] bright person riding blue bicycle [157 881 332 70] bright boy on blue bicycle [1522 08 09 24 0] blue bike for bright men [6964 08 82 50 ] blue bike for bright women [648120 743 2] blue cycle of bright child [6368 18 11 20 ] blue cycle in bright women [1011 02 64 48 ] figure 2: illustration of phrase fusion compositional gorithm using web-scale n-grams.</citsent>
<aftsection>
<nextsent>numbers in square brackets are n-gram frequencies.
</nextsent>
<nextsent>logue systems (channarukul et al, 2003).<papid> N03-4003 </papid></nextsent>
<nextsent>because the meaning representation produced by the image recognition system has fixed pattern of   adj1, obj1 , prep,  adj2, obj2  , it can be tem plated as there is [adj1] [obj1] [prep] the [adj2] [obj2].we also include templates that encode basic discourse constraints.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1454">
<title id=" W11-0326.xml">composing simple image descriptions using web scale ngrams </title>
<section> baseline approaches to surface.  </section>
<citcontext>
<prevsection>
<prevsent>the third approach is template-based approach with linguistic constraints, technique that has of ten been used for various practical applications suchas summarization (zhou and hovy, 2004) <papid> W04-1010 </papid>and dia 222 blue, bike [2669] blue, bicycle [1365] bike, blue [1184] blue, cycle [324] cycle, of, the, blue [172] cycle, blue [158] bicycle, blue [154] bike, in, blue [98] cycle, of, blue [64] bike, with, blue [43]     blue , bicycle  , near,   shiny , person     bright, boy [8092] bright, child [7840] bright, girl [6191] bright, kid [5873] bright, person [5461 ] bright, man [4936] bright, woman [2726] bright, women [1684] lady, bright [1360] bright, men [1050] person, operating, a, bicycle [3409] man, on, a, bicycle [2842] cycle, of, child [2507] bike, for, men [2485] person, riding, a, bicycle [2118] cycle, in, women [1853] bike, for, women [1442] boy, on, a, bicycle [1378] cycle, of, women [1288] man, on, a, bike [1283] bright person operating blue bicycle [2541 158 938 5] bright man on blue bicycle [1914 83 72 88 0] bright man on blue bike [1690 24 78 07 2] bright person riding blue bicycle [157 881 332 70] bright boy on blue bicycle [1522 08 09 24 0] blue bike for bright men [6964 08 82 50 ] blue bike for bright women [648120 743 2] blue cycle of bright child [6368 18 11 20 ] blue cycle in bright women [1011 02 64 48 ] figure 2: illustration of phrase fusion compositional gorithm using web-scale n-grams.</prevsent>
<prevsent>numbers in square brackets are n-gram frequencies.</prevsent>
</prevsection>
<citsent citstr=" N03-4003 ">
logue systems (channarukul et al, 2003).<papid> N03-4003 </papid></citsent>
<aftsection>
<nextsent>because the meaning representation produced by the image recognition system has fixed pattern of   adj1, obj1 , prep,  adj2, obj2  , it can be tem plated as there is [adj1] [obj1] [prep] the [adj2] [obj2].we also include templates that encode basic discourse constraints.
</nextsent>
<nextsent>for instance, the template that generated the first sentences in figure 3 and 4 is: [prefix] [#(x1)] [x1], [#(x2)] [x2], ... and [#(xk)] [xk], where xi is the name of an object (e.g. cow?), #(xi) is the number of instances of xi (e.g. one?),and prefix ? {this picture shows?, this is picture of?, etc}.
</nextsent>
<nextsent>although this approach can produce good looking sentences in limited domain, there are many limitations.
</nextsent>
<nextsent>first, template-based approach does not allow creative writing and produces somewhat stilted prose.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1455">
<title id=" W11-0326.xml">composing simple image descriptions using web scale ngrams </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>note that the choice of these action verbs is based only on the co-occurrence statistics encoded in n-grams, without relying on the vision component that specializes in action recognition.
</prevsent>
<prevsent>these examples therefore demonstrate that world knowledge implicitly encoded in natural language can help enhance image content recognition.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
automatic evaluation: bleu (papineni et al,2002) <papid> P02-1040 </papid>is widely used metric for automatic evaluation of machine translation that measures the gram precision of machine generated sentences with respect to human generated sentences.</citsent>
<aftsection>
<nextsent>because ourtask can be viewed as machine translation from images to text, bleu (papineni et al, 2002) <papid> P02-1040 </papid>may seem 4this limitation does not apply to template.</nextsent>
<nextsent>w/o w/ syn language model 0.094 0.106 template 0.087 0.096 local search 0.100 0.111 phrase fusion (any best) 0.149 0.153 phrase fusion (best w/ gerund) 0.146 0.149 human 0.500 0.510 table 2: automatic evaluation: bleu measured at 1 creativ.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1457">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they mainly differ with respect to the kind of selection algorithm they exploit.
</prevsent>
<prevsent>depending on whether training data, machine learning classifiers or external parsers are exploited, existing algorithms can be classified into i) supervised based, ii) ensemble based and iii) unsupervised based methods.the first is the case of the construction of machine learning classifier to predict the reliability of parses on the basis of different feature types.
</prevsent>
</prevsection>
<citsent citstr=" W06-1604 ">
yates et al (2006) <papid> W06-1604 </papid>exploited semantic features derived from the web to create statistical model to detect unreliable parses produced by constituency parser.</citsent>
<aftsection>
<nextsent>kawahara and uchimoto (2008) <papid> I08-2097 </papid>relied on features derived from the output of supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas ravi et al (2008) <papid> D08-1093 </papid>exploited an external constituency parser to extract text based features (e.g. sentence length, unknown words, etc.)as well as syntactic features to develop supervised predictor of the target parser accuracy.</nextsent>
<nextsent>the approaches proposed by reichart and rappoport(2007<papid> P07-1078 </papid>a) and sagae and tsujii (2007) <papid> D07-1111 </papid>can be classified as ensemble based methods.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1458">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>depending on whether training data, machine learning classifiers or external parsers are exploited, existing algorithms can be classified into i) supervised based, ii) ensemble based and iii) unsupervised based methods.the first is the case of the construction of machine learning classifier to predict the reliability of parses on the basis of different feature types.
</prevsent>
<prevsent>yates et al (2006) <papid> W06-1604 </papid>exploited semantic features derived from the web to create statistical model to detect unreliable parses produced by constituency parser.</prevsent>
</prevsection>
<citsent citstr=" I08-2097 ">
kawahara and uchimoto (2008) <papid> I08-2097 </papid>relied on features derived from the output of supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas ravi et al (2008) <papid> D08-1093 </papid>exploited an external constituency parser to extract text based features (e.g. sentence length, unknown words, etc.)as well as syntactic features to develop supervised predictor of the target parser accuracy.</citsent>
<aftsection>
<nextsent>the approaches proposed by reichart and rappoport(2007<papid> P07-1078 </papid>a) and sagae and tsujii (2007) <papid> D07-1111 </papid>can be classified as ensemble based methods.</nextsent>
<nextsent>both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of constituency parser, each trained on different sample from the training data,the latter uses the parses produced by different dependency parsing algorithms trained on the same data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1459">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>depending on whether training data, machine learning classifiers or external parsers are exploited, existing algorithms can be classified into i) supervised based, ii) ensemble based and iii) unsupervised based methods.the first is the case of the construction of machine learning classifier to predict the reliability of parses on the basis of different feature types.
</prevsent>
<prevsent>yates et al (2006) <papid> W06-1604 </papid>exploited semantic features derived from the web to create statistical model to detect unreliable parses produced by constituency parser.</prevsent>
</prevsection>
<citsent citstr=" D08-1093 ">
kawahara and uchimoto (2008) <papid> I08-2097 </papid>relied on features derived from the output of supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas ravi et al (2008) <papid> D08-1093 </papid>exploited an external constituency parser to extract text based features (e.g. sentence length, unknown words, etc.)as well as syntactic features to develop supervised predictor of the target parser accuracy.</citsent>
<aftsection>
<nextsent>the approaches proposed by reichart and rappoport(2007<papid> P07-1078 </papid>a) and sagae and tsujii (2007) <papid> D07-1111 </papid>can be classified as ensemble based methods.</nextsent>
<nextsent>both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of constituency parser, each trained on different sample from the training data,the latter uses the parses produced by different dependency parsing algorithms trained on the same data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1460">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>yates et al (2006) <papid> W06-1604 </papid>exploited semantic features derived from the web to create statistical model to detect unreliable parses produced by constituency parser.</prevsent>
<prevsent>kawahara and uchimoto (2008) <papid> I08-2097 </papid>relied on features derived from the output of supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas ravi et al (2008) <papid> D08-1093 </papid>exploited an external constituency parser to extract text based features (e.g. sentence length, unknown words, etc.)as well as syntactic features to develop supervised predictor of the target parser accuracy.</prevsent>
</prevsection>
<citsent citstr=" P07-1078 ">
the approaches proposed by reichart and rappoport(2007<papid> P07-1078 </papid>a) and sagae and tsujii (2007) <papid> D07-1111 </papid>can be classified as ensemble based methods.</citsent>
<aftsection>
<nextsent>both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of constituency parser, each trained on different sample from the training data,the latter uses the parses produced by different dependency parsing algorithms trained on the same data.
</nextsent>
<nextsent>however, widely acknowledged problem ofboth supervised based and ensemble based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy and the typology of errors of the used parser.
</nextsent>
<nextsent>to our knowledge, reichart and rappoport(2009<papid> W09-1120 </papid>a) are the first to address the task of high qual 115 ity parse selection by resorting to an unsupervised?</nextsent>
<nextsent>based method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1462">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>yates et al (2006) <papid> W06-1604 </papid>exploited semantic features derived from the web to create statistical model to detect unreliable parses produced by constituency parser.</prevsent>
<prevsent>kawahara and uchimoto (2008) <papid> I08-2097 </papid>relied on features derived from the output of supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas ravi et al (2008) <papid> D08-1093 </papid>exploited an external constituency parser to extract text based features (e.g. sentence length, unknown words, etc.)as well as syntactic features to develop supervised predictor of the target parser accuracy.</prevsent>
</prevsection>
<citsent citstr=" D07-1111 ">
the approaches proposed by reichart and rappoport(2007<papid> P07-1078 </papid>a) and sagae and tsujii (2007) <papid> D07-1111 </papid>can be classified as ensemble based methods.</citsent>
<aftsection>
<nextsent>both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of constituency parser, each trained on different sample from the training data,the latter uses the parses produced by different dependency parsing algorithms trained on the same data.
</nextsent>
<nextsent>however, widely acknowledged problem ofboth supervised based and ensemble based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy and the typology of errors of the used parser.
</nextsent>
<nextsent>to our knowledge, reichart and rappoport(2009<papid> W09-1120 </papid>a) are the first to address the task of high qual 115 ity parse selection by resorting to an unsupervised?</nextsent>
<nextsent>based method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1463">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of constituency parser, each trained on different sample from the training data,the latter uses the parses produced by different dependency parsing algorithms trained on the same data.
</prevsent>
<prevsent>however, widely acknowledged problem ofboth supervised based and ensemble based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy and the typology of errors of the used parser.
</prevsent>
</prevsection>
<citsent citstr=" W09-1120 ">
to our knowledge, reichart and rappoport(2009<papid> W09-1120 </papid>a) are the first to address the task of high qual 115 ity parse selection by resorting to an unsupervised?</citsent>
<aftsection>
<nextsent>based method.
</nextsent>
<nextsent>the underlying idea is that syntactic structures that are frequently created by parser are more likely to be correct than structures produced less frequently.
</nextsent>
<nextsent>for this purpose, their pupa (pos?
</nextsent>
<nextsent>based unsupervised parse assessment algorithm) uses statistics about pos tag sequences of parsed sentences produced by an unsupervised constituency parser.in this paper, we address this unsupervised scenario with two main novelties: unlike reichart and rappoport (2009<papid> W09-1120 </papid>a), a) we address the reliable parses selection task using an unsupervised method in supervised parsing scenario, and b) we operate ondependencybased representations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1473">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> the ulisse algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>the features exploited by ulisse are all linguistically motivated and relyon the dependency tree structure.
</prevsent>
<prevsent>different criteria guided their selection.
</prevsent>
</prevsection>
<citsent citstr=" W07-1001 ">
first, as pointed out in roark et al (2007), <papid> W07-1001 </papid>we needed features which could be reliably identified within the automatic output of parser.</citsent>
<aftsection>
<nextsent>second, we focused on dependency structures that are widely agreed in the literature a) to reflect sentences?
</nextsent>
<nextsent>syntactic and thus parsing complexity and b) to impose high cognitive load on the parsing of complete sentence.here follows the list of features used in the experiments reported in this paper, which turned out to be the most effective ones for the task at hand.
</nextsent>
<nextsent>parse tree depth: this feature is reliable indicator of sentence complexity due to the fact that, with sentences of approximately the same length, parse tree depth can be indicative of increased sentence complexity (yngve, 1960; frazier, 1985; gibson, 1998; nenkova, 2010).
</nextsent>
<nextsent>depth of embedded complement chains?: this feature is subtype of the previous one, focusing onthe depth of chains of embedded complements, either prepositional complements or nominal and adjectival modifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1474">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> the ulisse algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>here, there is no obvious relation between the number of dependents and sentence complexity:both small number and high number of dependents can make the sentence processing quite complex, although for different reasons (elliptical constructions in the former case, high number of modifiers in the latter).verbal roots: this feature counts the number of verbal roots with respect to number of all sentence roots in the target corpus.subordinate vs main clauses: subordination is generally considered to be an index of structural complexity in language.
</prevsent>
<prevsent>two distinct features are considered for monitoring this aspect: one measuring the ratio between main and subordinate clauses and the other one focusing on the relative ordering of subordinate clauses with respect to the main clause.it is widely acknowledged fact that highly complex sentences contain deeply embedded subordinate clauses; however, subordinate clauses are easier to process if they occur in post verbal rather than in pre verbal position (miller, 1998).
</prevsent>
</prevsection>
<citsent citstr=" D07-1013 ">
length of dependency links: mcdonald and nivre (2007) <papid> D07-1013 </papid>report that statistical parsers have drop in 116accuracy when analysing long distance dependen cies.</citsent>
<aftsection>
<nextsent>this is in line with lin (1996) <papid> C96-2123 </papid>and gibson (1998) who claim that the syntactic complexity of sentences can be predicted with measures based on the length of dependency links, given the memory overhead of very long distance dependencies.</nextsent>
<nextsent>here, the dependency length is measured in terms of the words occurring between the syntactic head and the dependent.dependency link plausibility (henceforth, arcposfeat): this feature is used to calculate the plausibility of dependency link given the partofspeech of the dependent and the head, by also considering the pos of the head father and the dependency linking the two.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1477">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> the ulisse algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>two distinct features are considered for monitoring this aspect: one measuring the ratio between main and subordinate clauses and the other one focusing on the relative ordering of subordinate clauses with respect to the main clause.it is widely acknowledged fact that highly complex sentences contain deeply embedded subordinate clauses; however, subordinate clauses are easier to process if they occur in post verbal rather than in pre verbal position (miller, 1998).
</prevsent>
<prevsent>length of dependency links: mcdonald and nivre (2007) <papid> D07-1013 </papid>report that statistical parsers have drop in 116accuracy when analysing long distance dependen cies.</prevsent>
</prevsection>
<citsent citstr=" C96-2123 ">
this is in line with lin (1996) <papid> C96-2123 </papid>and gibson (1998) who claim that the syntactic complexity of sentences can be predicted with measures based on the length of dependency links, given the memory overhead of very long distance dependencies.</citsent>
<aftsection>
<nextsent>here, the dependency length is measured in terms of the words occurring between the syntactic head and the dependent.dependency link plausibility (henceforth, arcposfeat): this feature is used to calculate the plausibility of dependency link given the partofspeech of the dependent and the head, by also considering the pos of the head father and the dependency linking the two.
</nextsent>
<nextsent>2.2 computation score.
</nextsent>
<nextsent>the quality score (henceforth, qs) of parsed sentences results from combination of the weights associated with the monitored features.
</nextsent>
<nextsent>ulisse is modular and can use several weights combination strategies, which may be customised with respect to the specific task exploiting the output of ulisse.for this study, qs is computed as simple product of the individual feature weights.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1478">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> the parsers.  </section>
<citcontext>
<prevsection>
<prevsent>f (((pd, ph, t)(ph, ph2, t2)))f ((ph, ph2, t2)) ? ?
</prevsent>
<prevsent>f (((pd, ph, t)(ph, ph2, t2)))f ((((pd,x, t))(x,ph2, t2))) ,where (x) is the frequency of in , is variable and (arc1 arc2) represent two consecutive arcs in the tree.
</prevsent>
</prevsection>
<citsent citstr=" W06-2932 ">
ulisse was tested against the output of two really different data driven parsers: the first order maximum spanning tree (mst) parser (mcdonald et al., 2006) <papid> W06-2932 </papid>and the desr parser (attardi, 2006) <papid> W06-2922 </papid>using support vector machine as learning algorithm.</citsent>
<aftsection>
<nextsent>the 1we set r=0 in the in domain experiments and r=2 in the outofdomain experiment reported in sec 5.3.
</nextsent>
<nextsent>117 former is graph based parser (following the so?
</nextsent>
<nextsent>called allpairs?
</nextsent>
<nextsent>approach buchholz et al (2006))where every possible arc is considered in the construction of the optimal parse tree and where dependency parsing is represented as the search for maximum spanning tree in directed graph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1479">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> the parsers.  </section>
<citcontext>
<prevsection>
<prevsent>f (((pd, ph, t)(ph, ph2, t2)))f ((ph, ph2, t2)) ? ?
</prevsent>
<prevsent>f (((pd, ph, t)(ph, ph2, t2)))f ((((pd,x, t))(x,ph2, t2))) ,where (x) is the frequency of in , is variable and (arc1 arc2) represent two consecutive arcs in the tree.
</prevsent>
</prevsection>
<citsent citstr=" W06-2922 ">
ulisse was tested against the output of two really different data driven parsers: the first order maximum spanning tree (mst) parser (mcdonald et al., 2006) <papid> W06-2932 </papid>and the desr parser (attardi, 2006) <papid> W06-2922 </papid>using support vector machine as learning algorithm.</citsent>
<aftsection>
<nextsent>the 1we set r=0 in the in domain experiments and r=2 in the outofdomain experiment reported in sec 5.3.
</nextsent>
<nextsent>117 former is graph based parser (following the so?
</nextsent>
<nextsent>called allpairs?
</nextsent>
<nextsent>approach buchholz et al (2006))where every possible arc is considered in the construction of the optimal parse tree and where dependency parsing is represented as the search for maximum spanning tree in directed graph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1491">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>first of all, they pose different challenges to parser since they are characterised by quite different syntactic features.for instance, italian, as opposed to english, is characterised by relatively free word order (especially for what concerns subject and object relations with respect to the verb) and by the possible absence of an overt subject.
</prevsent>
<prevsent>secondly, as it is shown in section 5.1, italian is less resourced language with respect to english.
</prevsent>
</prevsection>
<citsent citstr=" C08-1071 ">
this is key issue, since as demonstrated by reichart and rappoport (2007<papid> P07-1078 </papid>b) and mcclosky et al (2008), <papid> C08-1071 </papid>small and big treebanks pose different problems in the reliable parses selection.</citsent>
<aftsection>
<nextsent>last but not least, we aimed at demonstrating that ulisse can be successfully used not only with texts belonging to the same domain as the parser training corpus.
</nextsent>
<nextsent>for this purpose, ulisse was tested on target corpus of italian legislative texts, whose automatic linguistic analysis poses domainspecificchallenges (venturi, 2010).
</nextsent>
<nextsent>outofdomain experiments are being carried out also for english.
</nextsent>
<nextsent>5.1 the corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1492">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>two different target corpora were used for the in domain and outofdomain experiments.
</prevsent>
<prevsent>forthe former, we used corpus of 1,104,237 sentences (22,830,739 wordtokens) of newspapers texts which was extracted from the clic-ilc corpus (marinelli et al, 2003); for the legal domain, we used collection of italian legal texts (2,697,262wordtokens; 97,564 sentences) regulating variety of domains, ranging from environment, human rights, disability rights, freedom of expression to privacy, age disclaimer, etc. in the two experiments, the test sets were represented respectively by: a) the test set used in the evalita09 evaluation campaign, constituted by 260 sentences and 5,011 tokens fromnewpapers text; b) set of 102 sentences (corre spon ding to 5,691 tokens) from legal texts.
</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
the english corpora for the training of parsers we used the dependency based version of sections 211 of the wall street journal partition of thepenn treebank (marcus et al, 2003), which was developed for the conll 2007 shared task on dependency parsing (nivre et al, 2007): <papid> D07-1096 </papid>it includes 447,000 word tokens and about 18,600 sentences.as target data we took corpus of news, specifically the whole wall street journal section of the 2http://medialab.di.unipi.it/wiki/semawiki 3http://evalita.fbk.eu/index.html 119penn treebank4, from which the portion of text corresponding to the training corpus was removed; the english target corpus thus includes 39,285,425 tokens (1,625,606 sentences).</citsent>
<aftsection>
<nextsent>for testing we usedthe test set of the conll 2007 shared task, corresponding to subset of section 23 of the wall street journal partition of the penn treebank (5,003 tokens, 214 sentences).
</nextsent>
<nextsent>5.2 evaluation methodology.
</nextsent>
<nextsent>performances of the ulisse algorithm have been evaluated i) with respect to the accuracy of ranked parses and ii) in terms of precision and recall.
</nextsent>
<nextsent>first, for each experiment we evaluated how the ulisse algorithm and the baselines classify the sentences in the test set with respect to the labelled attachmentscore?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1493">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>ulisse shows promising performance against the output of two supervised parsers selected for their behavioral differences.
</prevsent>
<prevsent>in all experiments, ulisse outperforms all baselines, includingdpupa and sentence length (sl), the latter representing very strong baseline selection method in supervised scenario, where parsers have very high performance with short sentences.
</prevsent>
</prevsection>
<citsent citstr=" D10-1067 ">
the fact of carrying out the task of reliable parse selection in supervised scenario represents an important novelty: however, the unsupervised nature of ulisse could also be used in an unsupervised scenario (reichart and rappoport, 2010).<papid> D10-1067 </papid></citsent>
<aftsection>
<nextsent>current direction of research include careful study of a) the quality score function, in particular for what concerns the combination of individual feature weights, and b) the role and ef fectivess of the set of linguistic features.
</nextsent>
<nextsent>this study is being carried out with specific view to nlp tasks which might benefit from the ulisse algorithm.this is the case, for instance, of the domain adaptation task in self training scenario (mcclosky et al., 2006), <papid> P06-1043 </papid>of the treebank construction process by minimizing the human annotators?</nextsent>
<nextsent>efforts (reichart and rappoport, 2009<papid> W09-1120 </papid>b), of nbest ranking methods for machine translation (zhang, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1494">
<title id=" W11-0314.xml">ulisse an unsupervised algorithm for detecting reliable dependency parses </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>the fact of carrying out the task of reliable parse selection in supervised scenario represents an important novelty: however, the unsupervised nature of ulisse could also be used in an unsupervised scenario (reichart and rappoport, 2010).<papid> D10-1067 </papid></prevsent>
<prevsent>current direction of research include careful study of a) the quality score function, in particular for what concerns the combination of individual feature weights, and b) the role and ef fectivess of the set of linguistic features.</prevsent>
</prevsection>
<citsent citstr=" P06-1043 ">
this study is being carried out with specific view to nlp tasks which might benefit from the ulisse algorithm.this is the case, for instance, of the domain adaptation task in self training scenario (mcclosky et al., 2006), <papid> P06-1043 </papid>of the treebank construction process by minimizing the human annotators?</citsent>
<aftsection>
<nextsent>efforts (reichart and rappoport, 2009<papid> W09-1120 </papid>b), of nbest ranking methods for machine translation (zhang, 2006).</nextsent>
<nextsent>122</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1497">
<title id=" W11-0129.xml">towards semiautomatic methods for improving wordnet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the methods we propose test the coherence of two sources of knowledge, exploiting onto logical principles and semantic constraints.
</prevsent>
<prevsent>wordnet (princeton wordnet (fellbaum, 1998), henceforth wn) is lexical resource widely used ina host of applications in which language or linguistic concepts play role.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
for instance, it is central resource for the quantification of semantic relatedness (budanitsky and hirst, 2006), <papid> J06-1003 </papid>in turn often exploited in applications.</citsent>
<aftsection>
<nextsent>the quality of this resource therefore is very important for nlp as whole, and beyond, in several ai applications.
</nextsent>
<nextsent>neel and garzon (2010) show that the quality of knowledge resource like wn affects the performance in recognizing textual entailment (rte) and word-sense disambiguation (wsd) tasks.
</nextsent>
<nextsent>they observe that the new version of wn induced improvements in recent rte challenges, but conclude that wn currently is not rich enough to resolve such task.
</nextsent>
<nextsent>what is more, its quality may be too low to even be useful at all.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1498">
<title id=" W11-1202.xml">the copiale cipher </title>
<section> first decipher ment approach.  </section>
<citcontext>
<prevsection>
<prevsent>the revealed sequence of roman letters is itself nonsensical, so we posited simple substitution cipher.
</prevsent>
<prevsent>we carried out automatic computer attacks against the revealed roman letter sequence, first assuming german source, then english, then latin, then forty other candidate european and non-european languages.
</prevsent>
</prevsection>
<citsent citstr=" P06-2065 ">
the attack method is given in [knight et al 2006].<papid> P06-2065 </papid></citsent>
<aftsection>
<nextsent>that method automatically combines plaintext-language identification with decipherment.
</nextsent>
<nextsent>unfortunately, this failed, as no 5 language identified itself as more likely plain text candidate than the others.
</nextsent>
<nextsent>we then gave up our theory regarding nulls and posited homophonic cipher, with each plain text letter being encipherable by any of several distinct cipher letters.
</nextsent>
<nextsent>while well executed homophonic cipher will employ flat letter frequency distribution, to confound analysis, we guessed that the copiale cipher is not optimized in this regard.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1499">
<title id=" W10-4112.xml">automatic identification of predicate heads in chinese sentences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, predicate head splitting long sentence into two shorter parts can alleviate the complexity of syntactic analysis to certain degree.
</prevsent>
<prevsent>this is particularly useful when long dependency relations are involved.
</prevsent>
</prevsection>
<citsent citstr=" I05-3003 ">
without doubt, this is also difficult task in chinese dependency parsing (cheng et al, 2005).<papid> I05-3003 </papid></citsent>
<aftsection>
<nextsent>predicate head identification also plays an important role in facilitating various tasks of natural language processing.
</nextsent>
<nextsent>for example, it enhances shallow parsing (sun et al, 2000) and head driven parsing (collins, 1999), and also improves the precision of sentence similarity computation (sui et al, 1998a).
</nextsent>
<nextsent>there is reason to expect it to be more widely applicable to other tasks, e.g. machine translation, information extraction, and question answering.
</nextsent>
<nextsent>in this paper, we propose an effective approach to automatically recognize predicate heads of chinese sentences based on preprocessing step for maximal noun phrases 1(mnps).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1500">
<title id=" W11-1704.xml">creating sentiment dictionaries via triangulation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most of the work in obtaining subjectivity lexicons was done for english.
</prevsent>
<prevsent>however, there were some authors who developed methods for the mapping of subjectivity lexicons to other languages.
</prevsent>
</prevsection>
<citsent citstr=" W06-0301 ">
kim and hovy (2006) <papid> W06-0301 </papid>use machine translation system and subsequently use subjectivity analysis system that was developed for english.</citsent>
<aftsection>
<nextsent>mihalcea et al (2007) <papid> P07-1123 </papid>propose method to learn multilingual subjective language via cross-language projections.</nextsent>
<nextsent>they use the opinion finder lexicon (wilson et al, 2005) <papid> H05-1044 </papid>and two bilingual english-romanian dictionaries to translate the words in the lexicon.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1501">
<title id=" W11-1704.xml">creating sentiment dictionaries via triangulation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, there were some authors who developed methods for the mapping of subjectivity lexicons to other languages.
</prevsent>
<prevsent>kim and hovy (2006) <papid> W06-0301 </papid>use machine translation system and subsequently use subjectivity analysis system that was developed for english.</prevsent>
</prevsection>
<citsent citstr=" P07-1123 ">
mihalcea et al (2007) <papid> P07-1123 </papid>propose method to learn multilingual subjective language via cross-language projections.</citsent>
<aftsection>
<nextsent>they use the opinion finder lexicon (wilson et al, 2005) <papid> H05-1044 </papid>and two bilingual english-romanian dictionaries to translate the words in the lexicon.</nextsent>
<nextsent>since word ambiguity can appear (opinion finder does not mark word senses), they filter as correct translations onlythe most frequent words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1502">
<title id=" W11-1704.xml">creating sentiment dictionaries via triangulation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kim and hovy (2006) <papid> W06-0301 </papid>use machine translation system and subsequently use subjectivity analysis system that was developed for english.</prevsent>
<prevsent>mihalcea et al (2007) <papid> P07-1123 </papid>propose method to learn multilingual subjective language via cross-language projections.</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
they use the opinion finder lexicon (wilson et al, 2005) <papid> H05-1044 </papid>and two bilingual english-romanian dictionaries to translate the words in the lexicon.</citsent>
<aftsection>
<nextsent>since word ambiguity can appear (opinion finder does not mark word senses), they filter as correct translations onlythe most frequent words.
</nextsent>
<nextsent>the problem of translating multi-word expressions is solved by translating word-by-word and filtering those translations that occur at least three times on the web.
</nextsent>
<nextsent>another approach in obtaining subjectivity lexicons for other languages than english was explored in banea et al(2008<papid> L08-1086 </papid>b).</nextsent>
<nextsent>to this aim, the authors perform three different experiments, with good results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1503">
<title id=" W11-1704.xml">creating sentiment dictionaries via triangulation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>since word ambiguity can appear (opinion finder does not mark word senses), they filter as correct translations onlythe most frequent words.
</prevsent>
<prevsent>the problem of translating multi-word expressions is solved by translating word-by-word and filtering those translations that occur at least three times on the web.
</prevsent>
</prevsection>
<citsent citstr=" L08-1086 ">
another approach in obtaining subjectivity lexicons for other languages than english was explored in banea et al(2008<papid> L08-1086 </papid>b).</citsent>
<aftsection>
<nextsent>to this aim, the authors perform three different experiments, with good results.
</nextsent>
<nextsent>in the first one, they automatically translate the annotations ofthe mpqa corpus and thus obtain subjectivity an 29notated sentences in romanian.
</nextsent>
<nextsent>in the second approach, they use the automatically translated entries in the opinion finder lexicon to annotate set of sentences in romanian.
</nextsent>
<nextsent>in the last experiment, they reverse the direction of translation and verify the assumption that subjective language can be translated and thus new subjectivity lexicons can be obtained for languages with no such resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1511">
<title id=" W11-1704.xml">creating sentiment dictionaries via triangulation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>he first translates the english reviews into chinese and subsequently back to english.
</prevsent>
<prevsent>he then performs co-training using all generated corpora.
</prevsent>
</prevsection>
<citsent citstr=" C10-1004 ">
banea et al (2010) <papid> C10-1004 </papid>translate the mpqa corpus into five other languages (somewith similar ethimology, others with very different structure).</citsent>
<aftsection>
<nextsent>subsequently, they expand the feature space used in naive bayes classifier using the same data translated to 2 or 3 other languages.
</nextsent>
<nextsent>their conclusion is that expanding the feature space with data from other languages performs almost as well as training classifier for just one language on large set of training data.
</nextsent>
<nextsent>our approach to dictionary creation starts with semiautomatic way of colle ting subjective terms in english and spanish.
</nextsent>
<nextsent>these pivot language dictionaries are then projected to other languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1512">
<title id=" W11-1704.xml">creating sentiment dictionaries via triangulation </title>
<section> approach overview.  </section>
<citcontext>
<prevsection>
<prevsent>we extended all english and spanish lists with the missing morphological variants of the terms.
</prevsent>
<prevsent>3.2 automatic learning of subjective terms.
</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
we decided to expand our subjective term lists by using automatic term extraction, inspired by (riloffand wiebe, 2003).<papid> W03-1014 </papid></citsent>
<aftsection>
<nextsent>we look at the problem of acquisition of subjective terms as learning of semantic classes.
</nextsent>
<nextsent>since we wanted to do this for two different languages, namely english and spanish, the multilingual term extraction algorithm ontopopulis (tanev et al, 2010) was natural choice.
</nextsent>
<nextsent>ontopopulis performs weakly supervised learning of semantic dictionaries using distributional similarity.
</nextsent>
<nextsent>the algorithm takes on its input small set of seed terms for each semantic class, which is to be learnt, and an unannotated text corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1513">
<title id=" W10-4103.xml">classical chinese sentence segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>like classical chinese sentence segmentation, the task of sbd in speech is to determine which of the inter-word boundaries in the stream of words should be marked as end-of-sentence, and then to divide the entire word sequence into individual sentences.
</prevsent>
<prevsent>empirical methods are commonly employed to deal with this problem.
</prevsent>
</prevsection>
<citsent citstr=" W04-3209 ">
such methods involve many different sequence labeling models including hmms (shriberg et al, 2000), maximum entropy (maxent) models (liu et al, 2004), <papid> W04-3209 </papid>and crfs (liu et al, 2005).<papid> P05-1056 </papid></citsent>
<aftsection>
<nextsent>among these, crf model used in liu et al (2005) <papid> P05-1056 </papid>offered the lowest error rate.</nextsent>
<nextsent>chinese word segmentation is problem closely related to classical chinese sentence segmentation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1514">
<title id=" W10-4103.xml">classical chinese sentence segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>like classical chinese sentence segmentation, the task of sbd in speech is to determine which of the inter-word boundaries in the stream of words should be marked as end-of-sentence, and then to divide the entire word sequence into individual sentences.
</prevsent>
<prevsent>empirical methods are commonly employed to deal with this problem.
</prevsent>
</prevsection>
<citsent citstr=" P05-1056 ">
such methods involve many different sequence labeling models including hmms (shriberg et al, 2000), maximum entropy (maxent) models (liu et al, 2004), <papid> W04-3209 </papid>and crfs (liu et al, 2005).<papid> P05-1056 </papid></citsent>
<aftsection>
<nextsent>among these, crf model used in liu et al (2005) <papid> P05-1056 </papid>offered the lowest error rate.</nextsent>
<nextsent>chinese word segmentation is problem closely related to classical chinese sentence segmentation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1516">
<title id=" W10-4103.xml">classical chinese sentence segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for these reasons, the classical chinese sentence segmentation problem is more challenging.
</prevsent>
<prevsent>methods of chinese word segmentation can be mainly classified into heuristic rule-based approaches, statistical machine learning approaches, and hybrid approaches.
</prevsent>
</prevsection>
<citsent citstr=" P03-1035 ">
hybrid approaches combine the advantages of heuristic and statistical approaches to achieve better results (gao et al, 2003; <papid> P03-1035 </papid>xue, 2003; peng et al, 2004).<papid> C04-1081 </papid></citsent>
<aftsection>
<nextsent>xue (2003) transformed the chinese word segmentation problem into tagging problem.
</nextsent>
<nextsent>forgiven sequence of chinese characters, the author applies maxent tagger to assign each character one of four positions-of-character (poc) tags, and then coverts the tagged sequence into segmented sequence.
</nextsent>
<nextsent>the four poc tags used in xue (2003) denote the positions of characters within word.
</nextsent>
<nextsent>for example, the first character of word is tagged left boundary?, the last character of word is tagged right boundary?, the middle character of word is tagged middle?, and single character that forms word by itself is tagged single character-word?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1517">
<title id=" W10-4103.xml">classical chinese sentence segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for these reasons, the classical chinese sentence segmentation problem is more challenging.
</prevsent>
<prevsent>methods of chinese word segmentation can be mainly classified into heuristic rule-based approaches, statistical machine learning approaches, and hybrid approaches.
</prevsent>
</prevsection>
<citsent citstr=" C04-1081 ">
hybrid approaches combine the advantages of heuristic and statistical approaches to achieve better results (gao et al, 2003; <papid> P03-1035 </papid>xue, 2003; peng et al, 2004).<papid> C04-1081 </papid></citsent>
<aftsection>
<nextsent>xue (2003) transformed the chinese word segmentation problem into tagging problem.
</nextsent>
<nextsent>forgiven sequence of chinese characters, the author applies maxent tagger to assign each character one of four positions-of-character (poc) tags, and then coverts the tagged sequence into segmented sequence.
</nextsent>
<nextsent>the four poc tags used in xue (2003) denote the positions of characters within word.
</nextsent>
<nextsent>for example, the first character of word is tagged left boundary?, the last character of word is tagged right boundary?, the middle character of word is tagged middle?, and single character that forms word by itself is tagged single character-word?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1525">
<title id=" W11-0403.xml">analysis of the hindi proposition bank using dependency structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these preliminary results are promising; they show how well these two frameworks are correlated.
</prevsent>
<prevsent>this can also be used to speed up our annotations.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
proposition bank (from now on, propbank) iscor pus in which the arguments of each verb predicate are annotated with their semantic roles (palmer et al., 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>propbank annotation has been carried out in several languages; most of them are annotated on top of penn treebank style phrase structure (xue and palmer, 2003; <papid> W03-1707 </papid>palmer et al, 2008).<papid> L08-1461 </papid></nextsent>
<nextsent>however, different grammatical analysis has been used for the hindi propbank annotation, dependency structure, which may be particularly suited for the analysis of flexible word order languages such as hindi.as syntactic corpus, we use the hindi dependency treebank (bhatt et al, 2009).<papid> W09-3036 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1526">
<title id=" W11-0403.xml">analysis of the hindi proposition bank using dependency structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this can also be used to speed up our annotations.
</prevsent>
<prevsent>proposition bank (from now on, propbank) iscor pus in which the arguments of each verb predicate are annotated with their semantic roles (palmer et al., 2005).<papid> J05-1004 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1707 ">
propbank annotation has been carried out in several languages; most of them are annotated on top of penn treebank style phrase structure (xue and palmer, 2003; <papid> W03-1707 </papid>palmer et al, 2008).<papid> L08-1461 </papid></citsent>
<aftsection>
<nextsent>however, different grammatical analysis has been used for the hindi propbank annotation, dependency structure, which may be particularly suited for the analysis of flexible word order languages such as hindi.as syntactic corpus, we use the hindi dependency treebank (bhatt et al, 2009).<papid> W09-3036 </papid></nextsent>
<nextsent>using dependency structure has some advantages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1527">
<title id=" W11-0403.xml">analysis of the hindi proposition bank using dependency structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this can also be used to speed up our annotations.
</prevsent>
<prevsent>proposition bank (from now on, propbank) iscor pus in which the arguments of each verb predicate are annotated with their semantic roles (palmer et al., 2005).<papid> J05-1004 </papid></prevsent>
</prevsection>
<citsent citstr=" L08-1461 ">
propbank annotation has been carried out in several languages; most of them are annotated on top of penn treebank style phrase structure (xue and palmer, 2003; <papid> W03-1707 </papid>palmer et al, 2008).<papid> L08-1461 </papid></citsent>
<aftsection>
<nextsent>however, different grammatical analysis has been used for the hindi propbank annotation, dependency structure, which may be particularly suited for the analysis of flexible word order languages such as hindi.as syntactic corpus, we use the hindi dependency treebank (bhatt et al, 2009).<papid> W09-3036 </papid></nextsent>
<nextsent>using dependency structure has some advantages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1528">
<title id=" W11-0403.xml">analysis of the hindi proposition bank using dependency structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>proposition bank (from now on, propbank) iscor pus in which the arguments of each verb predicate are annotated with their semantic roles (palmer et al., 2005).<papid> J05-1004 </papid></prevsent>
<prevsent>propbank annotation has been carried out in several languages; most of them are annotated on top of penn treebank style phrase structure (xue and palmer, 2003; <papid> W03-1707 </papid>palmer et al, 2008).<papid> L08-1461 </papid></prevsent>
</prevsection>
<citsent citstr=" W09-3036 ">
however, different grammatical analysis has been used for the hindi propbank annotation, dependency structure, which may be particularly suited for the analysis of flexible word order languages such as hindi.as syntactic corpus, we use the hindi dependency treebank (bhatt et al, 2009).<papid> W09-3036 </papid></citsent>
<aftsection>
<nextsent>using dependency structure has some advantages.
</nextsent>
<nextsent>first, semantic arguments1 can be marked explicitly on the syntactic trees, so annotations of the predicate argument structure can be more consistent with the dependency structure.
</nextsent>
<nextsent>second, the hindi dependency treebank provides rich set of dependency relations that capture the syntactic-semantic information.
</nextsent>
<nextsent>this facilitates mappings between syntactic dependents and semantic arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1532">
<title id=" W11-0403.xml">analysis of the hindi proposition bank using dependency structure </title>
<section> description of the hindi propbank.  </section>
<citcontext>
<prevsection>
<prevsent>is used to indicate all numbered arguments as well as modifiers in propbank.
</prevsent>
<prevsent>21pendency structure.
</prevsent>
</prevsection>
<citsent citstr=" I08-2099 ">
the hindi dependency tree bank has created an annotation scheme for hindiby adapting labels from paninis sanskrit grammar (also known as cpg: computational panini an grammar; see begum et al (2008)).<papid> I08-2099 </papid></citsent>
<aftsection>
<nextsent>previous work has demonstrated that the english propbank tagsetis quite similar to english dependency trees annotated with the panini an labels (vaidya et al, 2009).propbank has also been mapped to other dependency schemes such as functional generative description (cinkova, 2006).
</nextsent>
<nextsent>2.2 hindi dependency treebank.
</nextsent>
<nextsent>the hindi dependency treebank (hdt) includes morphological, part-of-speech and chunking information as well as dependency relations.
</nextsent>
<nextsent>these are represented in the shakti standard format (ssf; see bharati et al (2007)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1533">
<title id=" W11-0403.xml">analysis of the hindi proposition bank using dependency structure </title>
<section> comparisons between syntactic and.  </section>
<citcontext>
<prevsection>
<prevsent>hpb distinguishes annotations between simple and complex predicates.
</prevsent>
<prevsent>simple predicates consist ofonly single verb whereas complex predicates consist of light verb and pre-verbal element.
</prevsent>
</prevsection>
<citsent citstr=" W10-1810 ">
the complex predicates are identified with special label argm-prx (argument-predicating expresstion), which is being used for all light verb annotations in propbank (hwang et al, 2010).<papid> W10-1810 </papid></citsent>
<aftsection>
<nextsent>figure 2 showsan example of the predicating noun mention annotated as argm-prx, used with come.
</nextsent>
<nextsent>the predicating noun also has its own argument, matter of, indicated with the hdt label r6-k1.
</nextsent>
<nextsent>the hdt has two labels, r6-k1 and r6-k2, for the arguments of the predicating noun.
</nextsent>
<nextsent>hence, the argument span for complex predicates includes not only direct dependents of the verb but also dependents of the noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1534">
<title id=" W11-0403.xml">analysis of the hindi proposition bank using dependency structure </title>
<section> automatic mapping of hdt to hpb.  </section>
<citcontext>
<prevsection>
<prevsent>this_work_loc important_progress be_pres figure 3: hdtvs. hpb on complex predicates.
</prevsent>
<prevsent>mapping between syntactic and semantic structures has been attempted in other languages.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
the penn english and chinese treebanks consist of several semantic roles (e.g., locative, temporal) annotated on top of penn treebank style phrase structure (marcus et al, 1994; <papid> H94-1020 </papid>xue and palmer, 2009).</citsent>
<aftsection>
<nextsent>the chinese propbank specifies mappings between syntactic and semantic arguments in frame set files (e.g., sbj ? arg0) that can be used for automatic mapping (xueand palmer, 2003).<papid> W03-1707 </papid></nextsent>
<nextsent>however, these chinese mappings are limited to certain types of syntactic arguments (mostly subjects and objects).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1537">
<title id=" W10-4105.xml">semantic computing and language knowledge bases </title>
<section> related work on semantic computing.  </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation (wsd) is also very important research subject and lot of work has been done in this regard, such as lesk (1986), gale et al (1998), jin et l.
</prevsent>
<prevsent>(2007) and qu et al (2007) as the chinese counterpart.
</prevsent>
</prevsection>
<citsent citstr=" P93-1022 ">
as to the research on computing word sense relatedness, dagan et al(1993) <papid> P93-1022 </papid>did some pilot work and lee (1997) and resnik (1999) contributed to the research on semantic similarity.</citsent>
<aftsection>
<nextsent>in recent years, semantics-based analysis such as data and web mining, analysis of social networks and semantic system design and synthesis have begun to draw more attention from researchers.
</nextsent>
<nextsent>applications using semantics such as search engines and question answering (li et al, 2002), content-based multimedia retrieval and editing, natural language interfaces (yokoi et al, 2005) based on semantics have also been attracting attentions.
</nextsent>
<nextsent>even semantic computing has been applied to areas like music description, medicine and biology and gis systems and architecture.
</nextsent>
<nextsent>the whole idea is how to realize human-centered computing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1538">
<title id=" W11-0137.xml">question classification for email </title>
<section> the data.  </section>
<citcontext>
<prevsection>
<prevsent>to this end questions were automatically extracted from cmus deduplicated copy of the enron corpus (klimt &amp; yang 2004).
</prevsent>
<prevsent>of the 39,530 unique question strings identified in enron out boxes, random sample of 1147 were manually examined and annotated with the expected question type.
</prevsent>
</prevsection>
<citsent citstr=" C02-1150 ">
a number of taxonomies have been proposed for classifying answer types, of which li &amp; roths (2002) <papid> C02-1150 </papid>two-tier hierarchy is reasonably comprehensive and widely-adopted example.</citsent>
<aftsection>
<nextsent>their coarse classes are abbreviation (abbr), description (desc), entity (enty), human (hum), location (loc),and numeric (num), and they then define set of 50 subclasses.
</nextsent>
<nextsent>table 1 shows how li &amp; roths taxonomy was mapped to the category labels adopted for the current work.
</nextsent>
<nextsent>cotterill 2010 li &amp; roth 2002 % <papid> C02-1150 </papid>person(s) hum{individual,title} 2.53 group or organisation hum{group} 0.17 descriptive text hum{description} 11.51 desc{manner, definition, description} reason desc{reason} 1.57 date or time num{date, period} 3.57 numeric num{weight, volume/size, ordinal, percentage, count, speed, money, temperature, distance, other} 1.92 phone num{code}1 0.40 url 0.17 email 0.17 place loc{country, state, city, mountain, other} 0.96 animal enty{animal} 0.00 physical object enty{instrument, plant, body part, vehicle, food, product, substance} 0.30concept enty{language, religion, letter, color, creative/artwork, dis ease/medical, currency} 0.40 event or activity enty{event, sport, technique/method} 0.87 other enty{symbol, term, word, other} 0.00 abbr{abbreviation, expression} yes/no 41.33 action request 8.98 rhetorical 5.23 multiple 3.23 non-question 16.74 table 1: the new dialogue taxonomy, with mappings to li &amp; roth where applicable, and percentage distribution in the enron sample 1phone number is actually subset of the num:code category, but it accounts for all instances in the enron sample.</nextsent>
<nextsent>331 are you guys still thinking of maybe joining us skiing??</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1540">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results show the community has made significant advancement in terms of both performance improvement and generalization.
</prevsent>
<prevsent>the bionlp shared task (bionlp-st, hereafter)is series of efforts to promote community wide collaboration towards fine-grained information extraction (ie) in biomedical domain.
</prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
the first event, bionlp-st 2009, introducing biomolecular event (bio-event) extraction task to the community, attracted wide attention, with 42 teams being registered for participation and 24 teams submitting final results (kim et al, 2009).<papid> W09-1401 </papid></citsent>
<aftsection>
<nextsent>to establish community effort, the organizers provided the task definition, benchmark data, an devaluations, and the participants competed in developing systems to perform the task.
</nextsent>
<nextsent>meanwhile, participants and organizers communicated to develop better setup of evaluation, and some provided their tools and resources for other participants, making it collaborative competition.the final results enabled to observe the state-ofthe-art performance of the community on the bio event extraction task, which showed that the automatic extraction of simple events - those with unaryarguments, e.g. gene expression, localization, phos phorylation - could be achieved at the performance level of 70% in f-score, but the extraction of complex events, e.g. binding and regulation, was lotmore challenging, having achieved 40% of performance level.
</nextsent>
<nextsent>after bionlp-st 2009, all the resources from the event were released to the public, to encourage continuous efforts for further advancement.
</nextsent>
<nextsent>since then, several improvements have been reported (miwa et al., 2010<papid> W10-1905 </papid>b; poon and vanderwende, 2010; vlachos, 2010; <papid> W10-1901 </papid>miwa et al, 2010<papid> W10-1905 </papid>a; bjorne et al, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1542">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>meanwhile, participants and organizers communicated to develop better setup of evaluation, and some provided their tools and resources for other participants, making it collaborative competition.the final results enabled to observe the state-ofthe-art performance of the community on the bio event extraction task, which showed that the automatic extraction of simple events - those with unaryarguments, e.g. gene expression, localization, phos phorylation - could be achieved at the performance level of 70% in f-score, but the extraction of complex events, e.g. binding and regulation, was lotmore challenging, having achieved 40% of performance level.
</prevsent>
<prevsent>after bionlp-st 2009, all the resources from the event were released to the public, to encourage continuous efforts for further advancement.
</prevsent>
</prevsection>
<citsent citstr=" W10-1905 ">
since then, several improvements have been reported (miwa et al., 2010<papid> W10-1905 </papid>b; poon and vanderwende, 2010; vlachos, 2010; <papid> W10-1901 </papid>miwa et al, 2010<papid> W10-1905 </papid>a; bjorne et al, 2010).</citsent>
<aftsection>
<nextsent>for example, miwa et al (miwa et al, 2010<papid> W10-1905 </papid>b) reported significant improvement with binding events, achieving 50% of performance level.the task introduced in bionlp-st 2009 was renamed to genia event (ge) task, and was hosted again in bionlp-st 2011, which also hosted four other ie tasks and three supporting tasks (kim et al, 2011).<papid> W11-1801 </papid></nextsent>
<nextsent>as the sole task that was repeated in the two events, the ge task was referenced during the development of other tasks, and took the role of connecting the results of the 2009 event to the main tasks of2011.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1543">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>meanwhile, participants and organizers communicated to develop better setup of evaluation, and some provided their tools and resources for other participants, making it collaborative competition.the final results enabled to observe the state-ofthe-art performance of the community on the bio event extraction task, which showed that the automatic extraction of simple events - those with unaryarguments, e.g. gene expression, localization, phos phorylation - could be achieved at the performance level of 70% in f-score, but the extraction of complex events, e.g. binding and regulation, was lotmore challenging, having achieved 40% of performance level.
</prevsent>
<prevsent>after bionlp-st 2009, all the resources from the event were released to the public, to encourage continuous efforts for further advancement.
</prevsent>
</prevsection>
<citsent citstr=" W10-1901 ">
since then, several improvements have been reported (miwa et al., 2010<papid> W10-1905 </papid>b; poon and vanderwende, 2010; vlachos, 2010; <papid> W10-1901 </papid>miwa et al, 2010<papid> W10-1905 </papid>a; bjorne et al, 2010).</citsent>
<aftsection>
<nextsent>for example, miwa et al (miwa et al, 2010<papid> W10-1905 </papid>b) reported significant improvement with binding events, achieving 50% of performance level.the task introduced in bionlp-st 2009 was renamed to genia event (ge) task, and was hosted again in bionlp-st 2011, which also hosted four other ie tasks and three supporting tasks (kim et al, 2011).<papid> W11-1801 </papid></nextsent>
<nextsent>as the sole task that was repeated in the two events, the ge task was referenced during the development of other tasks, and took the role of connecting the results of the 2009 event to the main tasks of2011.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1550">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after bionlp-st 2009, all the resources from the event were released to the public, to encourage continuous efforts for further advancement.
</prevsent>
<prevsent>since then, several improvements have been reported (miwa et al., 2010<papid> W10-1905 </papid>b; poon and vanderwende, 2010; vlachos, 2010; <papid> W10-1901 </papid>miwa et al, 2010<papid> W10-1905 </papid>a; bjorne et al, 2010).</prevsent>
</prevsection>
<citsent citstr=" W11-1801 ">
for example, miwa et al (miwa et al, 2010<papid> W10-1905 </papid>b) reported significant improvement with binding events, achieving 50% of performance level.the task introduced in bionlp-st 2009 was renamed to genia event (ge) task, and was hosted again in bionlp-st 2011, which also hosted four other ie tasks and three supporting tasks (kim et al, 2011).<papid> W11-1801 </papid></citsent>
<aftsection>
<nextsent>as the sole task that was repeated in the two events, the ge task was referenced during the development of other tasks, and took the role of connecting the results of the 2009 event to the main tasks of2011.
</nextsent>
<nextsent>the ge task in 2011 received final submissions from 15 teams.
</nextsent>
<nextsent>the results show the community made significant progress with the task, and also show the technology can be generalized to full papers at moderate cost of performance.
</nextsent>
<nextsent>this paper presents the task setup, preparation, and discusses the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1553">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> participation.  </section>
<citcontext>
<prevsection>
<prevsent>a notable observation is that four teams developed their systems based on the model of uturku09(bjorne et al, 2009) which was the winning system of bionlp-st 2009.
</prevsent>
<prevsent>it may show an influence of the bionlp-st series in the task.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
for syntactic analyses, the prevailing use of charniak johnson re-ranking parser (charniak and johnson, 2005)<papid> P05-1022 </papid>using the self-trained biomedical model from mcclosky (2008) (mcccj) which is converted to stanford dependency (de marneffe et al, 2006) is notable, which may also be an influence from there sults of bionlp-st 2009.</citsent>
<aftsection>
<nextsent>the last two teams, xabionlp and hcmus, who did not use syntactic analyses could not get performance comparable tothe others, which may suggest the importance of using syntactic analyses for complex ie task like ge task.
</nextsent>
<nextsent>5.1 task 1.
</nextsent>
<nextsent>table 6 shows the final evaluation results of task 1.
</nextsent>
<nextsent>for reference, the reported performance of the two systems, uturku09 and miwa10 is listed in the top.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1557">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>for reference, the reported performance of the two systems, uturku09 and miwa10 is listed in the top.
</prevsent>
<prevsent>uturku09 was the winning system of task 1 in 2009 (bjorne et al, 2009), and miwa10 was the best system reported after bionlp-st 2009 (miwa et al, 2010<papid> W10-1905 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W11-1808 ">
particularly, the latter made 10 team 09 task people reference faust ? 12- 3c (riedel et al, 2011) <papid> W11-1808 </papid>umass ? 12- 1c (riedel and mccallum, 2011) <papid> W11-1807 </papid>uturku ? 123 1bi (bjrne and salakoski, 2011) msr-nlp 1-- 4c (quirk et al, 2011) <papid> W11-1825 </papid>concordu ? 1-3 2c (kilicoglu and bergler, 2011) <papid> W11-1827 </papid>uwmadison ? 1-- 2c (vlachos and craven, 2011) <papid> W11-1805 </papid>stanford 1-- 3c+1.5l (mcclosky et al, 2011) <papid> W11-1806 </papid>bmi@asu ? 12- 3c (emadzadeh et al, 2011) <papid> W11-1824 </papid>ccp-btmg ? 1-- 3bi (liu et al, 2011) <papid> W11-1826 </papid>tm-scs 1-- 1c (bui and sloot, 2011) <papid> W11-1820 </papid>xabionlp 1-- 4c (casillas et al, 2011) hcmus 1-- 6l (minh et al, 2011) table 4: team profiles: the 09 column indicates whether at least one team member participated in bionlp-st 2009.</citsent>
<aftsection>
<nextsent>in people column, c=computer scientist, bi=bioinformatician, b=biologist, l=linguist nlp task other resources team lexical proc.
</nextsent>
<nextsent>syntactic proc.
</nextsent>
<nextsent>trig.
</nextsent>
<nextsent>arg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1558">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>for reference, the reported performance of the two systems, uturku09 and miwa10 is listed in the top.
</prevsent>
<prevsent>uturku09 was the winning system of task 1 in 2009 (bjorne et al, 2009), and miwa10 was the best system reported after bionlp-st 2009 (miwa et al, 2010<papid> W10-1905 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W11-1807 ">
particularly, the latter made 10 team 09 task people reference faust ? 12- 3c (riedel et al, 2011) <papid> W11-1808 </papid>umass ? 12- 1c (riedel and mccallum, 2011) <papid> W11-1807 </papid>uturku ? 123 1bi (bjrne and salakoski, 2011) msr-nlp 1-- 4c (quirk et al, 2011) <papid> W11-1825 </papid>concordu ? 1-3 2c (kilicoglu and bergler, 2011) <papid> W11-1827 </papid>uwmadison ? 1-- 2c (vlachos and craven, 2011) <papid> W11-1805 </papid>stanford 1-- 3c+1.5l (mcclosky et al, 2011) <papid> W11-1806 </papid>bmi@asu ? 12- 3c (emadzadeh et al, 2011) <papid> W11-1824 </papid>ccp-btmg ? 1-- 3bi (liu et al, 2011) <papid> W11-1826 </papid>tm-scs 1-- 1c (bui and sloot, 2011) <papid> W11-1820 </papid>xabionlp 1-- 4c (casillas et al, 2011) hcmus 1-- 6l (minh et al, 2011) table 4: team profiles: the 09 column indicates whether at least one team member participated in bionlp-st 2009.</citsent>
<aftsection>
<nextsent>in people column, c=computer scientist, bi=bioinformatician, b=biologist, l=linguist nlp task other resources team lexical proc.
</nextsent>
<nextsent>syntactic proc.
</nextsent>
<nextsent>trig.
</nextsent>
<nextsent>arg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1559">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>for reference, the reported performance of the two systems, uturku09 and miwa10 is listed in the top.
</prevsent>
<prevsent>uturku09 was the winning system of task 1 in 2009 (bjorne et al, 2009), and miwa10 was the best system reported after bionlp-st 2009 (miwa et al, 2010<papid> W10-1905 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W11-1825 ">
particularly, the latter made 10 team 09 task people reference faust ? 12- 3c (riedel et al, 2011) <papid> W11-1808 </papid>umass ? 12- 1c (riedel and mccallum, 2011) <papid> W11-1807 </papid>uturku ? 123 1bi (bjrne and salakoski, 2011) msr-nlp 1-- 4c (quirk et al, 2011) <papid> W11-1825 </papid>concordu ? 1-3 2c (kilicoglu and bergler, 2011) <papid> W11-1827 </papid>uwmadison ? 1-- 2c (vlachos and craven, 2011) <papid> W11-1805 </papid>stanford 1-- 3c+1.5l (mcclosky et al, 2011) <papid> W11-1806 </papid>bmi@asu ? 12- 3c (emadzadeh et al, 2011) <papid> W11-1824 </papid>ccp-btmg ? 1-- 3bi (liu et al, 2011) <papid> W11-1826 </papid>tm-scs 1-- 1c (bui and sloot, 2011) <papid> W11-1820 </papid>xabionlp 1-- 4c (casillas et al, 2011) hcmus 1-- 6l (minh et al, 2011) table 4: team profiles: the 09 column indicates whether at least one team member participated in bionlp-st 2009.</citsent>
<aftsection>
<nextsent>in people column, c=computer scientist, bi=bioinformatician, b=biologist, l=linguist nlp task other resources team lexical proc.
</nextsent>
<nextsent>syntactic proc.
</nextsent>
<nextsent>trig.
</nextsent>
<nextsent>arg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1560">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>for reference, the reported performance of the two systems, uturku09 and miwa10 is listed in the top.
</prevsent>
<prevsent>uturku09 was the winning system of task 1 in 2009 (bjorne et al, 2009), and miwa10 was the best system reported after bionlp-st 2009 (miwa et al, 2010<papid> W10-1905 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W11-1827 ">
particularly, the latter made 10 team 09 task people reference faust ? 12- 3c (riedel et al, 2011) <papid> W11-1808 </papid>umass ? 12- 1c (riedel and mccallum, 2011) <papid> W11-1807 </papid>uturku ? 123 1bi (bjrne and salakoski, 2011) msr-nlp 1-- 4c (quirk et al, 2011) <papid> W11-1825 </papid>concordu ? 1-3 2c (kilicoglu and bergler, 2011) <papid> W11-1827 </papid>uwmadison ? 1-- 2c (vlachos and craven, 2011) <papid> W11-1805 </papid>stanford 1-- 3c+1.5l (mcclosky et al, 2011) <papid> W11-1806 </papid>bmi@asu ? 12- 3c (emadzadeh et al, 2011) <papid> W11-1824 </papid>ccp-btmg ? 1-- 3bi (liu et al, 2011) <papid> W11-1826 </papid>tm-scs 1-- 1c (bui and sloot, 2011) <papid> W11-1820 </papid>xabionlp 1-- 4c (casillas et al, 2011) hcmus 1-- 6l (minh et al, 2011) table 4: team profiles: the 09 column indicates whether at least one team member participated in bionlp-st 2009.</citsent>
<aftsection>
<nextsent>in people column, c=computer scientist, bi=bioinformatician, b=biologist, l=linguist nlp task other resources team lexical proc.
</nextsent>
<nextsent>syntactic proc.
</nextsent>
<nextsent>trig.
</nextsent>
<nextsent>arg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1561">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>for reference, the reported performance of the two systems, uturku09 and miwa10 is listed in the top.
</prevsent>
<prevsent>uturku09 was the winning system of task 1 in 2009 (bjorne et al, 2009), and miwa10 was the best system reported after bionlp-st 2009 (miwa et al, 2010<papid> W10-1905 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W11-1805 ">
particularly, the latter made 10 team 09 task people reference faust ? 12- 3c (riedel et al, 2011) <papid> W11-1808 </papid>umass ? 12- 1c (riedel and mccallum, 2011) <papid> W11-1807 </papid>uturku ? 123 1bi (bjrne and salakoski, 2011) msr-nlp 1-- 4c (quirk et al, 2011) <papid> W11-1825 </papid>concordu ? 1-3 2c (kilicoglu and bergler, 2011) <papid> W11-1827 </papid>uwmadison ? 1-- 2c (vlachos and craven, 2011) <papid> W11-1805 </papid>stanford 1-- 3c+1.5l (mcclosky et al, 2011) <papid> W11-1806 </papid>bmi@asu ? 12- 3c (emadzadeh et al, 2011) <papid> W11-1824 </papid>ccp-btmg ? 1-- 3bi (liu et al, 2011) <papid> W11-1826 </papid>tm-scs 1-- 1c (bui and sloot, 2011) <papid> W11-1820 </papid>xabionlp 1-- 4c (casillas et al, 2011) hcmus 1-- 6l (minh et al, 2011) table 4: team profiles: the 09 column indicates whether at least one team member participated in bionlp-st 2009.</citsent>
<aftsection>
<nextsent>in people column, c=computer scientist, bi=bioinformatician, b=biologist, l=linguist nlp task other resources team lexical proc.
</nextsent>
<nextsent>syntactic proc.
</nextsent>
<nextsent>trig.
</nextsent>
<nextsent>arg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1562">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>for reference, the reported performance of the two systems, uturku09 and miwa10 is listed in the top.
</prevsent>
<prevsent>uturku09 was the winning system of task 1 in 2009 (bjorne et al, 2009), and miwa10 was the best system reported after bionlp-st 2009 (miwa et al, 2010<papid> W10-1905 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W11-1806 ">
particularly, the latter made 10 team 09 task people reference faust ? 12- 3c (riedel et al, 2011) <papid> W11-1808 </papid>umass ? 12- 1c (riedel and mccallum, 2011) <papid> W11-1807 </papid>uturku ? 123 1bi (bjrne and salakoski, 2011) msr-nlp 1-- 4c (quirk et al, 2011) <papid> W11-1825 </papid>concordu ? 1-3 2c (kilicoglu and bergler, 2011) <papid> W11-1827 </papid>uwmadison ? 1-- 2c (vlachos and craven, 2011) <papid> W11-1805 </papid>stanford 1-- 3c+1.5l (mcclosky et al, 2011) <papid> W11-1806 </papid>bmi@asu ? 12- 3c (emadzadeh et al, 2011) <papid> W11-1824 </papid>ccp-btmg ? 1-- 3bi (liu et al, 2011) <papid> W11-1826 </papid>tm-scs 1-- 1c (bui and sloot, 2011) <papid> W11-1820 </papid>xabionlp 1-- 4c (casillas et al, 2011) hcmus 1-- 6l (minh et al, 2011) table 4: team profiles: the 09 column indicates whether at least one team member participated in bionlp-st 2009.</citsent>
<aftsection>
<nextsent>in people column, c=computer scientist, bi=bioinformatician, b=biologist, l=linguist nlp task other resources team lexical proc.
</nextsent>
<nextsent>syntactic proc.
</nextsent>
<nextsent>trig.
</nextsent>
<nextsent>arg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1563">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>for reference, the reported performance of the two systems, uturku09 and miwa10 is listed in the top.
</prevsent>
<prevsent>uturku09 was the winning system of task 1 in 2009 (bjorne et al, 2009), and miwa10 was the best system reported after bionlp-st 2009 (miwa et al, 2010<papid> W10-1905 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W11-1824 ">
particularly, the latter made 10 team 09 task people reference faust ? 12- 3c (riedel et al, 2011) <papid> W11-1808 </papid>umass ? 12- 1c (riedel and mccallum, 2011) <papid> W11-1807 </papid>uturku ? 123 1bi (bjrne and salakoski, 2011) msr-nlp 1-- 4c (quirk et al, 2011) <papid> W11-1825 </papid>concordu ? 1-3 2c (kilicoglu and bergler, 2011) <papid> W11-1827 </papid>uwmadison ? 1-- 2c (vlachos and craven, 2011) <papid> W11-1805 </papid>stanford 1-- 3c+1.5l (mcclosky et al, 2011) <papid> W11-1806 </papid>bmi@asu ? 12- 3c (emadzadeh et al, 2011) <papid> W11-1824 </papid>ccp-btmg ? 1-- 3bi (liu et al, 2011) <papid> W11-1826 </papid>tm-scs 1-- 1c (bui and sloot, 2011) <papid> W11-1820 </papid>xabionlp 1-- 4c (casillas et al, 2011) hcmus 1-- 6l (minh et al, 2011) table 4: team profiles: the 09 column indicates whether at least one team member participated in bionlp-st 2009.</citsent>
<aftsection>
<nextsent>in people column, c=computer scientist, bi=bioinformatician, b=biologist, l=linguist nlp task other resources team lexical proc.
</nextsent>
<nextsent>syntactic proc.
</nextsent>
<nextsent>trig.
</nextsent>
<nextsent>arg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1564">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>for reference, the reported performance of the two systems, uturku09 and miwa10 is listed in the top.
</prevsent>
<prevsent>uturku09 was the winning system of task 1 in 2009 (bjorne et al, 2009), and miwa10 was the best system reported after bionlp-st 2009 (miwa et al, 2010<papid> W10-1905 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W11-1826 ">
particularly, the latter made 10 team 09 task people reference faust ? 12- 3c (riedel et al, 2011) <papid> W11-1808 </papid>umass ? 12- 1c (riedel and mccallum, 2011) <papid> W11-1807 </papid>uturku ? 123 1bi (bjrne and salakoski, 2011) msr-nlp 1-- 4c (quirk et al, 2011) <papid> W11-1825 </papid>concordu ? 1-3 2c (kilicoglu and bergler, 2011) <papid> W11-1827 </papid>uwmadison ? 1-- 2c (vlachos and craven, 2011) <papid> W11-1805 </papid>stanford 1-- 3c+1.5l (mcclosky et al, 2011) <papid> W11-1806 </papid>bmi@asu ? 12- 3c (emadzadeh et al, 2011) <papid> W11-1824 </papid>ccp-btmg ? 1-- 3bi (liu et al, 2011) <papid> W11-1826 </papid>tm-scs 1-- 1c (bui and sloot, 2011) <papid> W11-1820 </papid>xabionlp 1-- 4c (casillas et al, 2011) hcmus 1-- 6l (minh et al, 2011) table 4: team profiles: the 09 column indicates whether at least one team member participated in bionlp-st 2009.</citsent>
<aftsection>
<nextsent>in people column, c=computer scientist, bi=bioinformatician, b=biologist, l=linguist nlp task other resources team lexical proc.
</nextsent>
<nextsent>syntactic proc.
</nextsent>
<nextsent>trig.
</nextsent>
<nextsent>arg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1565">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>for reference, the reported performance of the two systems, uturku09 and miwa10 is listed in the top.
</prevsent>
<prevsent>uturku09 was the winning system of task 1 in 2009 (bjorne et al, 2009), and miwa10 was the best system reported after bionlp-st 2009 (miwa et al, 2010<papid> W10-1905 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W11-1820 ">
particularly, the latter made 10 team 09 task people reference faust ? 12- 3c (riedel et al, 2011) <papid> W11-1808 </papid>umass ? 12- 1c (riedel and mccallum, 2011) <papid> W11-1807 </papid>uturku ? 123 1bi (bjrne and salakoski, 2011) msr-nlp 1-- 4c (quirk et al, 2011) <papid> W11-1825 </papid>concordu ? 1-3 2c (kilicoglu and bergler, 2011) <papid> W11-1827 </papid>uwmadison ? 1-- 2c (vlachos and craven, 2011) <papid> W11-1805 </papid>stanford 1-- 3c+1.5l (mcclosky et al, 2011) <papid> W11-1806 </papid>bmi@asu ? 12- 3c (emadzadeh et al, 2011) <papid> W11-1824 </papid>ccp-btmg ? 1-- 3bi (liu et al, 2011) <papid> W11-1826 </papid>tm-scs 1-- 1c (bui and sloot, 2011) <papid> W11-1820 </papid>xabionlp 1-- 4c (casillas et al, 2011) hcmus 1-- 6l (minh et al, 2011) table 4: team profiles: the 09 column indicates whether at least one team member participated in bionlp-st 2009.</citsent>
<aftsection>
<nextsent>in people column, c=computer scientist, bi=bioinformatician, b=biologist, l=linguist nlp task other resources team lexical proc.
</nextsent>
<nextsent>syntactic proc.
</nextsent>
<nextsent>trig.
</nextsent>
<nextsent>arg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1566">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>this time, three teams achieved better results thanmiwa10, which indicates some role of focused efforts like bionlp-st. the comparison between the 11 performance on abstract and full paper collections shows that generalization to full papers is feasible with very modest loss in performance.
</prevsent>
<prevsent>5.2 task 2.
</prevsent>
</prevsection>
<citsent citstr=" W09-1406 ">
tables 7 shows final evaluation results of task 2.for reference, the reported performance of the task winning system in 2009, ut+dbcls09 (riedel et al., 2009), <papid> W09-1406 </papid>is shown in the top.</citsent>
<aftsection>
<nextsent>the first and second ranked system, faust and umass, which share same author with riedel09, made significant improvement over riedel09 in the abstract collection.
</nextsent>
<nextsent>uturku achieved the best performance in finding sites arguments but did not produce location arguments.
</nextsent>
<nextsent>in table 7, the performance of all the systems in full text collection suggests that finding secondary arguments in full text is much more challenging.
</nextsent>
<nextsent>in detail, significant improvement was made for location arguments (36.59%50.00%).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1567">
<title id=" W11-1802.xml">overview of genia event task in bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>a further breakdown of the results of site extraction, shown in table 8, shows that finding site arguments for phosphorylation, binding and regulation events are all significantly improved, but in different ways.the extraction of protein sites to be phosphory lated is approaching practical level of performance (84.21%), while protein sites to be bound or to be regulated remains challenging to be extracted.
</prevsent>
<prevsent>5.3 task 3.
</prevsent>
</prevsection>
<citsent citstr=" W09-1418 ">
table 9 shows final evaluation results of task 3.for reference, the reported performance of the task winning system in 2009, kilicoglu09(kilicoglu and bergler, 2009), <papid> W09-1418 </papid>is shown in the top.</citsent>
<aftsection>
<nextsent>among the two teams participated in the task, uturku showed better performance in extracting negated events, while concordu showed better performance in extracting speculated events.
</nextsent>
<nextsent>the genia event task which was repeated forbionlp-st 2009 and 2011 took role of measuring the progress of the community and generalization ie technology to full papers.
</nextsent>
<nextsent>the results from 15 teams who made their final submissions to the task show that clear advance of the community in terms of the performance on focused domain andalso generalization to full papers.
</nextsent>
<nextsent>to our disappointment, however, an effective use of supporting task results was not observed, which thus remains as future work for further improvement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1568">
<title id=" W11-0208.xml">automatic acquisition of huge training data for biomedical named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, this approach might suffer from an ambiguity problem in which dictionary includes an expression as entries for multiple semantic types.
</prevsent>
<prevsent>for this reason, we must use the context information of an expression to make sure that the expression stands for the target semantic type.
</prevsent>
</prevsection>
<citsent citstr=" W02-0301 ">
nadeau and sekine (2007) reported that strong trend exists recently in applying machine learning (ml) techniques such as support vector machine (svm) (kazama et al, 2002; <papid> W02-0301 </papid>isozaki and kazawa,2002) <papid> C02-1054 </papid>and conditional random field (crf) (set tles, 2004) <papid> W04-1221 </papid>toner, which can address these issues.in this approach, ner is formalized as classification problem in which given expression is classified into semantic class or other (non-ne) expressions.</citsent>
<aftsection>
<nextsent>because the classification problem is usually modeled using supervised learning methods, we need manually annotated corpus for training ner classifier.
</nextsent>
<nextsent>however, preparing manually annotated corpus for target domain of text and semantic typesis cost-intensive and time-consuming because human experts are needed to reliably annotate nes in text.
</nextsent>
<nextsent>for this reason, manually annotated corpora for ner are often limited to specific domain and covers small amount of text.in this paper we propose novel method for automatically acquiring training data for ner from comprehensible lexical database and huge amounts of unlabeled text.
</nextsent>
<nextsent>this paper presents four contribu 65 gene or protein name official name aliases
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1569">
<title id=" W11-0208.xml">automatic acquisition of huge training data for biomedical named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, this approach might suffer from an ambiguity problem in which dictionary includes an expression as entries for multiple semantic types.
</prevsent>
<prevsent>for this reason, we must use the context information of an expression to make sure that the expression stands for the target semantic type.
</prevsent>
</prevsection>
<citsent citstr=" C02-1054 ">
nadeau and sekine (2007) reported that strong trend exists recently in applying machine learning (ml) techniques such as support vector machine (svm) (kazama et al, 2002; <papid> W02-0301 </papid>isozaki and kazawa,2002) <papid> C02-1054 </papid>and conditional random field (crf) (set tles, 2004) <papid> W04-1221 </papid>toner, which can address these issues.in this approach, ner is formalized as classification problem in which given expression is classified into semantic class or other (non-ne) expressions.</citsent>
<aftsection>
<nextsent>because the classification problem is usually modeled using supervised learning methods, we need manually annotated corpus for training ner classifier.
</nextsent>
<nextsent>however, preparing manually annotated corpus for target domain of text and semantic typesis cost-intensive and time-consuming because human experts are needed to reliably annotate nes in text.
</nextsent>
<nextsent>for this reason, manually annotated corpora for ner are often limited to specific domain and covers small amount of text.in this paper we propose novel method for automatically acquiring training data for ner from comprehensible lexical database and huge amounts of unlabeled text.
</nextsent>
<nextsent>this paper presents four contribu 65 gene or protein name official name aliases
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1570">
<title id=" W11-0208.xml">automatic acquisition of huge training data for biomedical named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, this approach might suffer from an ambiguity problem in which dictionary includes an expression as entries for multiple semantic types.
</prevsent>
<prevsent>for this reason, we must use the context information of an expression to make sure that the expression stands for the target semantic type.
</prevsent>
</prevsection>
<citsent citstr=" W04-1221 ">
nadeau and sekine (2007) reported that strong trend exists recently in applying machine learning (ml) techniques such as support vector machine (svm) (kazama et al, 2002; <papid> W02-0301 </papid>isozaki and kazawa,2002) <papid> C02-1054 </papid>and conditional random field (crf) (set tles, 2004) <papid> W04-1221 </papid>toner, which can address these issues.in this approach, ner is formalized as classification problem in which given expression is classified into semantic class or other (non-ne) expressions.</citsent>
<aftsection>
<nextsent>because the classification problem is usually modeled using supervised learning methods, we need manually annotated corpus for training ner classifier.
</nextsent>
<nextsent>however, preparing manually annotated corpus for target domain of text and semantic typesis cost-intensive and time-consuming because human experts are needed to reliably annotate nes in text.
</nextsent>
<nextsent>for this reason, manually annotated corpora for ner are often limited to specific domain and covers small amount of text.in this paper we propose novel method for automatically acquiring training data for ner from comprehensible lexical database and huge amounts of unlabeled text.
</nextsent>
<nextsent>this paper presents four contribu 65 gene or protein name official name aliases
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1571">
<title id=" W11-0208.xml">automatic acquisition of huge training data for biomedical named entity recognition </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>part of speech tags present grammatical roles of tokens, e.g. verbs, nouns, and prepositions.
</prevsent>
<prevsent>chunk tags compose tokens into syntactically correlated segments, e.g. verb phrases, noun phrases, and prepositional phrases.
</prevsent>
</prevsection>
<citsent citstr=" W09-1119 ">
we use the iobes notation (ratinov and roth, 2009) <papid> W09-1119 </papid>to represent ne mentions with label sequences, thereby ner is formalized as multiclass classification problem in which given token is classified into iobes labels.</citsent>
<aftsection>
<nextsent>to classify labels of tokens, we use linear kernel svm which applies the one-vs.-the-rest method (weston and watkins, 1999) to extend binary classification to multi-class classification.
</nextsent>
<nextsent>given the t-th token xt in sentence, we predict the label yt, yt = argmax s(y|xt, yt1).
</nextsent>
<nextsent>in this equation, s(y|xt, yt1) presents the score(sum of feature weights) when the token xt is labeled y. we use yt1 (the label of the previous to ken) to predict yt, expecting that this feature behave sas label bigram feature (also called translation fea ture) in crf.
</nextsent>
<nextsent>if the sentence consists of x1 to xt , we repeat prediction of labels sequentially from the beginning (y1) to the end (yt ) of sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1572">
<title id=" W11-0208.xml">automatic acquisition of huge training data for biomedical named entity recognition </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>for evaluation, we used the epi genetics and post-translational modification (epi) corpus bionlp 2011 shared task (sigbiomed, 2011).
</prevsent>
<prevsent>only development data and training data are released as the epi corpus at present, we used both of the datasets for evaluation in this experiment.
</prevsent>
</prevsection>
<citsent citstr=" W09-1313 ">
named entities in the corpus are annotated exhaustively and belong to single semantic class, gene or gene product (ggp) (ohta et al, 2009).<papid> W09-1313 </papid></citsent>
<aftsection>
<nextsent>we evaluated the performance of the method p f1 dictionary matching 92.09 39.03 42.69 40.78 trained on acquired data 85.76 10.18 23.83 14.27 table 2: results of the preliminary experiment.
</nextsent>
<nextsent>(a) it is clear that in culture media of am, cystatin and cathepsin are present as proteinaseantiproteinase complexes.
</nextsent>
<nextsent>(b) temperature in the puerperium is higher in am, and lower in pm.figure 2: dictionary-based gene name annotating example (annotated words are shown in italic typeface).
</nextsent>
<nextsent>ner on four measures: accuracy (a), precision (p), recall (r), and f1-measure (f1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1573">
<title id=" W11-0208.xml">automatic acquisition of huge training data for biomedical named entity recognition </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>in each step, 227,000 tokens were sampled from the remaining set (u ).
</prevsent>
<prevsent>because the remaining set has high precision and low recall, we need not revise nes that were annotated in section 2.3.
</prevsent>
</prevsection>
<citsent citstr=" P10-1029 ">
it might lower the quality of the training data to merge annotated entities, thus we used confidence values (huang and riloff, 2010) <papid> P10-1029 </papid>to revise annotations.</citsent>
<aftsection>
<nextsent>therefore, we retain the ne annotations of the remaining setu and over write span of non-ne annotation only if the current model predicts the span as an ne with high confidence.
</nextsent>
<nextsent>we compute the confidence of the prediction (f(x)) which token is predicted as label as, f(x) = s(x, y)max(z 6=ys(x, z)).
</nextsent>
<nextsent>here, s(x, y) denotes the score (the sum of feature weights) computed using the svm model described in the beginning of section 2.
</nextsent>
<nextsent>a confidence sco represents the difference of scores between the predicted (the best) label and the second-best label.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1574">
<title id=" W11-0208.xml">automatic acquisition of huge training data for biomedical named entity recognition </title>
<section> experiment.  </section>
<citcontext>
<prevsection>
<prevsent>here, the first word tna?
</prevsent>
<prevsent>shown in italic typeface in this example is not annotated, although its second mention is annotated at the annotation expansion step.
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
we might apply the one sense per discourse (gale et al, 1992) <papid> H92-1045 </papid>heuristic to label this case.second, the improvement of self-training techniques elicited less than 1.0 f1-measure.</citsent>
<aftsection>
<nextsent>to as certain the reason for this small improvement, we analyzed the distribution of entity length both origi 71 original added 0% 25% 50% 75% 100% length 1 length 2 length 3 more than 4 figure 9: distribution of entity length.nally included entities and newly added entities during self-training, as shown in figure 9.
</nextsent>
<nextsent>they represent the ratio of entity length to the number of total entities.
</nextsent>
<nextsent>figure 9 shows the added distribution of entity length (added) differs from the original one(original).
</nextsent>
<nextsent>results of this analysis show that self training mainly annotates entities of the length one and barely recognizes entities of the length two ormore.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1575">
<title id=" W11-0208.xml">automatic acquisition of huge training data for biomedical named entity recognition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our study focuses mainly on achieving high performance ner without manual annotation.
</prevsent>
<prevsent>several previous studies aimed at reducing the cost of manual annotations.
</prevsent>
</prevsection>
<citsent citstr=" W06-3328 ">
vlachos and gasperin (2006) <papid> W06-3328 </papid>obtained noisy training data from flybase1 with few manually annotated abstracts from flybase.</citsent>
<aftsection>
<nextsent>this study suggested the possibility of acquiring high-quality training data from noisy training data.
</nextsent>
<nextsent>it used bootstrapping method and highly context-based classifiers to increase the number of ne mentions in the training data.
</nextsent>
<nextsent>even though the method achieved high-performance ner in the biomedical domain, it requires curated seed data.whitelaw et al (2008) attempted to create extremely huge training data from the web using aseed set of entities and relations.
</nextsent>
<nextsent>in generating training data automatically, this study used context-basedtagging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1576">
<title id=" W11-0122.xml">the peoples web meets linguistic knowledge automatic sense alignment of wikipedia and wordnet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wikipedia has the advantage of being constantly updated by thousands of voluntary contributors.
</prevsent>
<prevsent>it is multilingual and freely available containing tremendous amount of encyclopedic knowledge enriched with hyper link information.
</prevsent>
</prevsection>
<citsent citstr=" L08-1052 ">
in the past, researchers have explored the alignment of wikipedia categories and wordnet synsets (e.g., toral et al (2008); <papid> L08-1052 </papid>ponzetto and navigli (2009)).</citsent>
<aftsection>
<nextsent>however, using the categories instead of the articles causes three limitations: first, the number of wikipedia categories (about 0.5 million in the english edition) is much smaller compared to the number of articles (about 3.35 million).
</nextsent>
<nextsent>secondly, the category system in wikipedia is not structured consistently (ponzetto and navigli, 2009).
</nextsent>
<nextsent>and finally, disregarding the article level neglects the huge amount of textual content provided by the articles.
</nextsent>
<nextsent>therefore, attempts to align wordnet synsets and wikipedia articles (instead of categories) have been recently made.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1577">
<title id=" W11-0122.xml">the peoples web meets linguistic knowledge automatic sense alignment of wikipedia and wordnet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, word senses contained in both resources can then be represented by relational information from wordnet and encyclopedic information from wikipedia in multilingual manner yielding an enriched knowledge representation.
</prevsent>
<prevsent>and finally, the third major benefit of the alignment is the ability to automatically acquire sense-tagged corpora in mono- and multilingual fashion.
</prevsent>
</prevsection>
<citsent citstr=" N07-1025 ">
for each wordnet synset, the text of the aligned wikipedia article (or all sentences or paragraphs in wikipedia that contain link to the article) can be automatically extracted similar to the approach proposed by mihalcea (2007).<papid> N07-1025 </papid></citsent>
<aftsection>
<nextsent>automatically generated sense-tagged corpora can be used to, e.g., counter the bottleneck of supervised wsd methods that relyon such sense-tagged text collections, which are rare.
</nextsent>
<nextsent>further, due to the cross-lingual links in wikipedia, also corpora in different languages can be constructed easily.
</nextsent>
<nextsent>our contribution to this paper is two-fold.
</nextsent>
<nextsent>first, we propose novel two-step approach to align wordnet synsets and wikipedia articles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1578">
<title id=" W11-0122.xml">the peoples web meets linguistic knowledge automatic sense alignment of wikipedia and wordnet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our contribution to this paper is two-fold.
</prevsent>
<prevsent>first, we propose novel two-step approach to align wordnet synsets and wikipedia articles.
</prevsent>
</prevsection>
<citsent citstr=" E09-1005 ">
we model the task as word sense disambiguation problem applying the personalized page rank algorithm proposed by agirre and soroa (2009) <papid> E09-1005 </papid>as it is state-of-the-art in wsd and combine it with word overlap measure, which increases the overall performance.</citsent>
<aftsection>
<nextsent>second, we generate and introduce well-balanced reference dataset for evaluation consisting of 1,815 manually annotated sense alignment candidates.
</nextsent>
<nextsent>wordnet synsets and their corresponding wikipedia article candidates are sampled along their distinctive properties such as synset size, domain, or the location in the wordnet taxonomy.
</nextsent>
<nextsent>an evaluation on this dataset let us generalize the performance to full alignment between wordnet and wikipedia, which is publicly available for further research activities.
</nextsent>
<nextsent>the alignment of wordnet and wikipedia has been an active area of research for several years with thegoal of creating an enriched ontology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1581">
<title id=" W11-0122.xml">the peoples web meets linguistic knowledge automatic sense alignment of wikipedia and wordnet </title>
<section> well-balanced reference dataset.  </section>
<citcontext>
<prevsection>
<prevsent>to assess the reliability of the annotators?
</prevsent>
<prevsent>decision, we computed the pairwise observed inter-annotator agreement ao and the chance-corrected agreement ?
</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
(artstein and poesio, 2008)<papid> J08-4004 </papid>5.</citsent>
<aftsection>
<nextsent>the agreement values are shown in table 3.
</nextsent>
<nextsent>the average observed agreement ao is 0.9721, while the multi-?
</nextsent>
<nextsent>is 0.8727 indicating high reliability.
</nextsent>
<nextsent>the final dataset was compiled by means of majority decision.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1583">
<title id=" W11-0122.xml">the peoples web meets linguistic knowledge automatic sense alignment of wikipedia and wordnet </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this might be due to the fact that wikipedia article often contains more general terms, i.e. hypernym concepts, especially within the first paragraph of wikipedia article.
</prevsent>
<prevsent>all combinations yield higher performance compared to the stand-alone classifiers.
</prevsent>
</prevsection>
<citsent citstr=" L08-1139 ">
for example, for the setting syn+hyper and p+t+c, cos yields 0.738, ppr 0.765, and the combination of both 0.781 6extracted with jwpl (zesch et al, 2008) <papid> L08-1139 </papid>and some additional post-processing steps.</citsent>
<aftsection>
<nextsent>7we have not optimized this value for this task.
</nextsent>
<nextsent>8as all experimental settings, in which the wikipedia article was represented with its first paragraph instead of the whole article text, yield higher performance, we report only these numbers here.
</nextsent>
<nextsent>211 measure b cos .688 .692 .676 pprd .711 .711 .690 pprd + cos .724 . 714 .716 ppr .737 .718 .716 ppr + cos .740 .730 .728 table 5: agreement (?)
</nextsent>
<nextsent>between automatic and human annotators automatic alignment non-alignment manual alignment 178 49 non-alignment 51 1,537 table 6: confusion matrix (setting: ppr + cos , syn+hyper, p+t+c) performance, which is an improvement of 5.8% and 2.1% compared to the cos and ppr approach, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1584">
<title id=" W11-0711.xml">email formality in the workplace a case study on the enron corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we believe that the methodology proposed in the paper can be applied to other social media domains and be used to test other linguistic or social theories.
</prevsent>
<prevsent>email has become an important way of communication in our daily life.
</prevsent>
</prevsection>
<citsent citstr=" H05-1056 ">
because of its wide usage, it has been the subject of various studies such as social network analysis (e.g., (leuski, 2004; dies ner et al, 2005; carvalho et al, 2007)), deception detection (e.g., (zhou et al, 2004; keila and skill corn, 2005)), information extraction (e.g., (culottaet al, 2004; minkov et al, 2005)), <papid> H05-1056 </papid>and topic discovery (e.g., (mccallum et al, 2007)).</citsent>
<aftsection>
<nextsent>in this study, we focus on email formality in various social settings; that is, we want to determine whether the choice of formality in email communication is affected by factors such as the social distance and relative power between the senders and the recipients.while an early perspective of email communication held that email is lean medium which lacks vital social cues (daft and lengel, 1986), other work has shown that senders of email exhibit wide rangeof language and form choices which vary in different social contexts (orlikowski and yates, 1994).
</nextsent>
<nextsent>through various theories of socio linguistics, it is proposed that these changes take place in predictable manner.
</nextsent>
<nextsent>brown and levinson (1987) have proposed model where in order to save the face?
</nextsent>
<nextsent>or public self image of the hearer of message, speaker can employ range of verbal strategies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1585">
<title id=" W11-0711.xml">email formality in the workplace a case study on the enron corpus </title>
<section> overview of the enron email corpus.  </section>
<citcontext>
<prevsection>
<prevsent>in 2006, jabbari and his colleagues at the university of sheffield manually annotated subset of the emails in the cmu dataset with business?
</prevsent>
<prevsent>or personal?
</prevsent>
</prevsection>
<citsent citstr=" P06-2053 ">
categories (jabbari et al, 2006).<papid> P06-2053 </papid></citsent>
<aftsection>
<nextsent>the subset contains 14,818 emails and 3,598 of them (24.2%) are labeled as personal?.3 we use this dataset in the personal vs. business experiment 1our data including annotations and results can be found at http://students.washington.edu/kellypet/enron-formality/ 2the dataset can be downloaded from http://www.isi.edu/adibi/enron/enron.htm 3the dataset is available at http://staffwww.dcs.shef.ac.uk/people/l.guthrie/nlp/research.htm.
</nextsent>
<nextsent>as described in section 5.1.4 2.3 the isi enron employee position table.
</nextsent>
<nextsent>in addition to the isi database, isi also provided table of 161 employees and their positions in the company.5 in section 5.3, we study the effect of seniority on the formality of message, and we use this table to determine the relative seniority between senders and recipients of given email.
</nextsent>
<nextsent>in this study, we build two classifiers: formality classifier that determines whether an email is formal, and request classifier that determines whether anemail contains request.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1586">
<title id=" W11-0711.xml">email formality in the workplace a case study on the enron corpus </title>
<section> building classifiers.  </section>
<citcontext>
<prevsection>
<prevsent>89 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1-gram 2-gram 3-gram 4-gram 5-gram top 5000 top 1000 at least 5 at least 10 baseline figure 1: accuracy of the request classifier with different feature sets 4.3.1 features for request there has been considerable research into categorizing email messages by function.
</prevsent>
<prevsent>cohen, carvalho, and mitchell (2004) described the classification of email into email speech acts?, building on the speech act theory of searle (1975).
</prevsent>
</prevsection>
<citsent citstr=" W06-3406 ">
carvalho and cohen (2006) <papid> W06-3406 </papid>achieved high-precision results categorizing messages into categories such as request and proposal?</citsent>
<aftsection>
<nextsent>when preprocessing the text in certain ways and using unigram, bigram, and trigram features only.
</nextsent>
<nextsent>unlike formality, which is more about the style ofthe messages (e.g., whether the email is all in lowercase), the content words are more relevant for identifying requests.
</nextsent>
<nextsent>following the work in (carvalho and cohen, 2006), <papid> W06-3406 </papid>we used word ngrams as features.</nextsent>
<nextsent>to prevent the features from being too specific to the small training data, we experimented with two ways of feature selection: by feature counts and by chi-square scores.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1588">
<title id=" W10-4222.xml">grouping axioms for more coherent ontology descriptions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we show how all axiom patterns in el++, dl commonly used in the semantic web, can be aggregated without further domain knowledge, and describe prototype system that performs such aggregations.
</prevsent>
<prevsent>our method aggregates axioms while they are still in logical form,i.e., as part of sentence planning but before converting to linguistic representation and realising as english sentences.
</prevsent>
</prevsection>
<citsent citstr=" E09-2005 ">
this approach is somewhat different from that proposed by other researchers who convert ontology axioms to linguistic structures before aggregating (hielkema, 2009; galanis et al, 2009; <papid> E09-2005 </papid>dongilli, 2008).</citsent>
<aftsection>
<nextsent>we present results from testing our algorithm on over fifty ontologies from the tones repository3.
</nextsent>
<nextsent>in this section we analyse which kinds of axioms might be grouped together.
</nextsent>
<nextsent>power (2010) <papid> P10-2024 </papid>anal 2for brevity we use logic notation rather than e.g., owl functional syntax: subclassof(class(ns:cat) class(ns:animal)) where ns is any valid namespace.</nextsent>
<nextsent>the operatorv denotes the subclass relation, denotes class intersection, and p.c the class of individuals bearing the relation to one or more members of class c. 3http://owl.cs.manchester.ac.uk/ no.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1589">
<title id=" W10-4222.xml">grouping axioms for more coherent ontology descriptions </title>
<section> analysis of axiom groupings.  </section>
<citcontext>
<prevsection>
<prevsent>we present results from testing our algorithm on over fifty ontologies from the tones repository3.
</prevsent>
<prevsent>in this section we analyse which kinds of axioms might be grouped together.
</prevsent>
</prevsection>
<citsent citstr=" P10-2024 ">
power (2010) <papid> P10-2024 </papid>anal 2for brevity we use logic notation rather than e.g., owl functional syntax: subclassof(class(ns:cat) class(ns:animal)) where ns is any valid namespace.</citsent>
<aftsection>
<nextsent>the operatorv denotes the subclass relation, denotes class intersection, and p.c the class of individuals bearing the relation to one or more members of class c. 3http://owl.cs.manchester.ac.uk/ no.
</nextsent>
<nextsent>logic owl % 1 v subclassof(a b) 51 2 v p.b subclassof(a somevaluesfrom(p b)) 33 3 [a, b] ? propertyassertion(p b) 8 4 ? classassertion(a a) 4 table 1: the four most common axiom patterns.ysed axiom patterns present in the same fifty ontologies.
</nextsent>
<nextsent>inspite of the richness of owl, the surprising result was that only four relatively simple patterns dominated, accounting for 96% of all patterns found in more than 35,000 axioms.
</nextsent>
<nextsent>overall there were few unique patterns, typically only 10to 20, and up to 34 in an unusually complex ontology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1590">
<title id=" W10-4122.xml">improving chinese word segmentation by adopting self organized maps of character ngram </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as result, chinese word segmentation (cws) becomes one of the most fundamental technologies in chinese natural language process.
</prevsent>
<prevsent>in the last decade, many statistics-based methods for automatic cws have been proposed with development of machine learning and statistical method (huang and zhao, 2007).
</prevsent>
</prevsection>
<citsent citstr=" I05-3025 ">
especially, the character-based tagging method which was proposed by nianwen xue (2003) achieves great success in the second international chinese word segmentation bakeoff in 2005 (low et al, 2005).<papid> I05-3025 </papid></citsent>
<aftsection>
<nextsent>the character-based tagging method formulates the cws problem as task of predicting tag for each character in the sentence, i.e. every character is considered as one of four different types in 4-tag set: (begin of word), (middle of word), (end of word), and (single character word).
</nextsent>
<nextsent>most of these works train tagging models only on limited labeled training sets, without using any unsupervised learning outcomes from in numerous unlabeled text.
</nextsent>
<nextsent>but in recent years, researchers begin to exploit the value of enormous unlabeled corpus for cws.
</nextsent>
<nextsent>some statistics information on co-occurrence of sub sequences in the whole text has been extracted from unlabeled data and been employed as input features for tagging model training (zhao and kit , 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1591">
<title id=" W10-4122.xml">improving chinese word segmentation by adopting self organized maps of character ngram </title>
<section> applications and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>n1??.
</prevsent>
<prevsent>4.3 character-based tagging method.
</prevsent>
</prevsection>
<citsent citstr=" W06-0127 ">
for cws previous works show that 6-tag set achieved better cws performance (zhao et al, 2006).<papid> W06-0127 </papid></citsent>
<aftsection>
<nextsent>thus, we opt for this tag set.
</nextsent>
<nextsent>this 6 tag set adds b2?
</nextsent>
<nextsent>and b3?
</nextsent>
<nextsent>to 4-tag set which stand for the type of the second and the third character in chinese word respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1592">
<title id=" W11-0120.xml">measuring the semantic relatedness between words and images </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another related line of work by barnard and forsyth (2001) used generative hierarchical model to learn the associative semantics of words and images for improving information retrieval tasks.
</prevsent>
<prevsent>their approach was supervised and evaluated again only on the corel dataset.
</prevsent>
</prevsection>
<citsent citstr=" N10-1011 ">
more recently, feng and lapata (2010) <papid> N10-1011 </papid>showed that it is possible to combine visual representations of word meanings into joint bimodal representation constructed by using latent topics.</citsent>
<aftsection>
<nextsent>while their work focused on unifying meanings from visual and textual data via supervised techniques, no effort was made to compare the semantic relatedness between arbitrary pairs of word and image.
</nextsent>
<nextsent>inspired by the bag-of-words approach employed in information retrieval, the bag of visual codewords?
</nextsent>
<nextsent>is similar technique used mainly for scene classification (yang et al, 2007).
</nextsent>
<nextsent>starting with an image collection, visual features are first extracted as data points from each image, characterizing its appearance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1593">
<title id=" W11-0120.xml">measuring the semantic relatedness between words and images </title>
<section> semantic vector models.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, various studies (kanerva, 1998) have shown that by using collaborative, distributive memory units to represent semantic vectors, closer correspondence to human cognition can be achieved.
</prevsent>
<prevsent>while vector-space models typically require non trivial algebraic machinery, reducing dimensions is often key to uncover the hidden (latent) features of the terms distribution in the corpus, and to circumvent the sparseness issue.
</prevsent>
</prevsection>
<citsent citstr=" L08-1028 ">
there are number of methods that have been developed to reduce dimensions ? see e.g., widdows and ferraro (2008) <papid> L08-1028 </papid>for an overview.</citsent>
<aftsection>
<nextsent>here, we briefly describe one commonly used 187 technique, namely the latent semantic analysis (lsa), noted for its effectiveness in previous works for reducing dimensions.in lsa, term co-occurrences in corpus are captured by means of dimensionality reduction operated by singular value decomposition (svd) on the term-by-document matrix representing the corpus.
</nextsent>
<nextsent>svd is well-known operation in linear algebra, which can be applied to any rectangular matrix in order to find correlations among its rows and columns.
</nextsent>
<nextsent>svd decomposes the term-by-document matrix into three matrices = ukvt where is the diagonal ? matrix containing the singular values of t, 1 ? 2 ? ...
</nextsent>
<nextsent>k and and are column-orthogonal matrices.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1594">
<title id=" W11-0120.xml">measuring the semantic relatedness between words and images </title>
<section> semantic relatedness between words and images.  </section>
<citcontext>
<prevsection>
<prevsent>6we only allowed annotators with an approval rating of 97% or higher.
</prevsent>
<prevsent>here, we expect some variance in the degree of relatedness between the candidate words and images, hence annotations marked with all 10s or 0s are discarded due to lack of distinctions in similarity relatedness 7annotation guidelines and dataset can be downloaded at http://lit.csci.unt.edu/index.php/downloads 190 5.3 experiments.
</prevsent>
</prevsection>
<citsent citstr=" D09-1046 ">
following erk and mccarthy (2009), <papid> D09-1046 </papid>who argued that word meanings are graded over their senses, we believe that the meaning of an image is not limited to set of best fitting?</citsent>
<aftsection>
<nextsent>tags, but rather it exists as distribution over arbitrary words with varying degrees of association.
</nextsent>
<nextsent>specifically, the focus of our experiments is to investigate the correlation between automatic measures of such relatedness scores with respect to human judgments.
</nextsent>
<nextsent>to construct the joint semantic space of words and images, we use the svd described in section 4 to reduce the number of dimensions.
</nextsent>
<nextsent>to build each model, we use the 167 synsets from ima genet and their associated images (minus the held out test data), hence accounting for 167 latent dimensions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1595">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to investigate the context necessary to understand referring expression in situated collaborative dialog, we carried out an experiment with 33evaluators and japanese referring expression corpus.
</prevsent>
<prevsent>the results contribute to finding the proper contexts for extrinsic evalu tion in dynamic domains.
</prevsent>
</prevsection>
<citsent citstr=" W09-0629 ">
in recent years, the nlg community has paid significant attention to the task of generating referring expressions, reflected in the seting-up of several competitive events such as the tuna and give challenges at enlg 2009 (gatt et al, 2009; <papid> W09-0629 </papid>byron et al, 2009).<papid> W09-0628 </papid></citsent>
<aftsection>
<nextsent>with the development of increasingly complex generation systems, there has been heightened interest in and an ongoing significant discussion on different evaluation measures for referring expressions.
</nextsent>
<nextsent>this discussion is carried out broadly in the field of generation, including in the multi-modal domain, e.g.
</nextsent>
<nextsent>(stent et al, 2005; foster, 2008).<papid> W08-1113 </papid></nextsent>
<nextsent>! #$%&amp; ! #$%&amp; ()$)&amp;   ($)*$+% &amp;,#-+. / *+),&amp;#(&amp;  &amp;#),&amp;#(&amp;  -$,$./#&amp;0! #$%1 234456 78$#0! #$%10 234496 :$#0!*,0; =&amp;(0! #$%1 2344 6 ;)/&amp;$0! #$%1 234456 ?/,!$#0@a$ b*, 2344c6 d* e0@0f$))0 2344g6 h8&amp;(0i$i*, 234j46 k/()*,#! #$%&amp;# 234496 figure 1: overview of recent work on evaluation of referring expressions figure 1 shows schematic overview of recent work on evaluation of referring expressions along the two axes of evaluation method and domain in which referring expressions are used.there are two different evaluation methods corresponding to the bottom and the top of the vertical axis in figure 1: intrinsic and extrinsic evaluations (sparck jones and galli ers, 1996).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1596">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to investigate the context necessary to understand referring expression in situated collaborative dialog, we carried out an experiment with 33evaluators and japanese referring expression corpus.
</prevsent>
<prevsent>the results contribute to finding the proper contexts for extrinsic evalu tion in dynamic domains.
</prevsent>
</prevsection>
<citsent citstr=" W09-0628 ">
in recent years, the nlg community has paid significant attention to the task of generating referring expressions, reflected in the seting-up of several competitive events such as the tuna and give challenges at enlg 2009 (gatt et al, 2009; <papid> W09-0629 </papid>byron et al, 2009).<papid> W09-0628 </papid></citsent>
<aftsection>
<nextsent>with the development of increasingly complex generation systems, there has been heightened interest in and an ongoing significant discussion on different evaluation measures for referring expressions.
</nextsent>
<nextsent>this discussion is carried out broadly in the field of generation, including in the multi-modal domain, e.g.
</nextsent>
<nextsent>(stent et al, 2005; foster, 2008).<papid> W08-1113 </papid></nextsent>
<nextsent>! #$%&amp; ! #$%&amp; ()$)&amp;   ($)*$+% &amp;,#-+. / *+),&amp;#(&amp;  &amp;#),&amp;#(&amp;  -$,$./#&amp;0! #$%1 234456 78$#0! #$%10 234496 :$#0!*,0; =&amp;(0! #$%1 2344 6 ;)/&amp;$0! #$%1 234456 ?/,!$#0@a$ b*, 2344c6 d* e0@0f$))0 2344g6 h8&amp;(0i$i*, 234j46 k/()*,#! #$%&amp;# 234496 figure 1: overview of recent work on evaluation of referring expressions figure 1 shows schematic overview of recent work on evaluation of referring expressions along the two axes of evaluation method and domain in which referring expressions are used.there are two different evaluation methods corresponding to the bottom and the top of the vertical axis in figure 1: intrinsic and extrinsic evaluations (sparck jones and galli ers, 1996).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1597">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with the development of increasingly complex generation systems, there has been heightened interest in and an ongoing significant discussion on different evaluation measures for referring expressions.
</prevsent>
<prevsent>this discussion is carried out broadly in the field of generation, including in the multi-modal domain, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W08-1113 ">
(stent et al, 2005; foster, 2008).<papid> W08-1113 </papid></citsent>
<aftsection>
<nextsent>! #$%&amp; ! #$%&amp; ()$)&amp;   ($)*$+% &amp;,#-+. / *+),&amp;#(&amp;  &amp;#),&amp;#(&amp;  -$,$./#&amp;0! #$%1 234456 78$#0! #$%10 234496 :$#0!*,0; =&amp;(0! #$%1 2344 6 ;)/&amp;$0! #$%1 234456 ?/,!$#0@a$ b*, 2344c6 d* e0@0f$))0 2344g6 h8&amp;(0i$i*, 234j46 k/()*,#! #$%&amp;# 234496 figure 1: overview of recent work on evaluation of referring expressions figure 1 shows schematic overview of recent work on evaluation of referring expressions along the two axes of evaluation method and domain in which referring expressions are used.there are two different evaluation methods corresponding to the bottom and the top of the vertical axis in figure 1: intrinsic and extrinsic evaluations (sparck jones and galli ers, 1996).
</nextsent>
<nextsent>intrinsic methods often measure similarity between the system output and the gold standard corpora using metrics such as tree similarity, string-editdistance and bleu (papineni et al, 2002).<papid> P02-1040 </papid></nextsent>
<nextsent>intrinsic methods have recently become popular in the nlg community.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1598">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(stent et al, 2005; foster, 2008).<papid> W08-1113 </papid></prevsent>
<prevsent>! #$%&amp; ! #$%&amp; ()$)&amp;   ($)*$+% &amp;,#-+. / *+),&amp;#(&amp;  &amp;#),&amp;#(&amp;  -$,$./#&amp;0! #$%1 234456 78$#0! #$%10 234496 :$#0!*,0; =&amp;(0! #$%1 2344 6 ;)/&amp;$0! #$%1 234456 ?/,!$#0@a$ b*, 2344c6 d* e0@0f$))0 2344g6 h8&amp;(0i$i*, 234j46 k/()*,#! #$%&amp;# 234496 figure 1: overview of recent work on evaluation of referring expressions figure 1 shows schematic overview of recent work on evaluation of referring expressions along the two axes of evaluation method and domain in which referring expressions are used.there are two different evaluation methods corresponding to the bottom and the top of the vertical axis in figure 1: intrinsic and extrinsic evaluations (sparck jones and galli ers, 1996).</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
intrinsic methods often measure similarity between the system output and the gold standard corpora using metrics such as tree similarity, string-editdistance and bleu (papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>intrinsic methods have recently become popular in the nlg community.
</nextsent>
<nextsent>in contrast, extrinsic methods evaluate generated expressions based on an external metric, such as its impact on human task performance.
</nextsent>
<nextsent>while intrinsic evaluations have been widely used in nlg, e.g.
</nextsent>
<nextsent>(reiter et al, 2005), (cahill and van genabith, 2006) <papid> P06-1130 </papid>and the competitive 2009 tuna-challenge, there have been number of criticisms against this type of evaluation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1599">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in contrast, extrinsic methods evaluate generated expressions based on an external metric, such as its impact on human task performance.
</prevsent>
<prevsent>while intrinsic evaluations have been widely used in nlg, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P06-1130 ">
(reiter et al, 2005), (cahill and van genabith, 2006) <papid> P06-1130 </papid>and the competitive 2009 tuna-challenge, there have been number of criticisms against this type of evaluation.</citsent>
<aftsection>
<nextsent>(reiterand sripada, 2002) argue, for example, that generated text might be very different from corpus but still achieve the specific communicative goal.
</nextsent>
<nextsent>an additional problem is that corpus-similarity metrics measure how well system reproduces what speakers (or writers) do, while for most nlg systems ultimately the most important consideration is its effect on the human user (i.e. listener or reader).
</nextsent>
<nextsent>thus (khan et al, 2009) <papid> W09-0615 </papid>argues that measuring human-likeness disregards effectiveness of these expressions?.</nextsent>
<nextsent>furthermore, as (belz and gatt, 2008) <papid> P08-2050 </papid>state there are no significant correlations between intrinsic and extrinsic evaluation measures?, concluding that similarity to human-produced reference texts is not necessarily indicative of quality as measured by human task performance?.from early on in the nlg community, task based extrinsic evaluations have been considered as the most meaningful evaluation, especially when having to convince people in other communities of the usefulness of system (reiter andbelz, 2009).<papid> J09-4008 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1600">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(reiterand sripada, 2002) argue, for example, that generated text might be very different from corpus but still achieve the specific communicative goal.
</prevsent>
<prevsent>an additional problem is that corpus-similarity metrics measure how well system reproduces what speakers (or writers) do, while for most nlg systems ultimately the most important consideration is its effect on the human user (i.e. listener or reader).
</prevsent>
</prevsection>
<citsent citstr=" W09-0615 ">
thus (khan et al, 2009) <papid> W09-0615 </papid>argues that measuring human-likeness disregards effectiveness of these expressions?.</citsent>
<aftsection>
<nextsent>furthermore, as (belz and gatt, 2008) <papid> P08-2050 </papid>state there are no significant correlations between intrinsic and extrinsic evaluation measures?, concluding that similarity to human-produced reference texts is not necessarily indicative of quality as measured by human task performance?.from early on in the nlg community, task based extrinsic evaluations have been considered as the most meaningful evaluation, especially when having to convince people in other communities of the usefulness of system (reiter andbelz, 2009).<papid> J09-4008 </papid></nextsent>
<nextsent>task performance evaluation is recognized as the only known way to measure the effectiveness of nlg systems with real users?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1601">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an additional problem is that corpus-similarity metrics measure how well system reproduces what speakers (or writers) do, while for most nlg systems ultimately the most important consideration is its effect on the human user (i.e. listener or reader).
</prevsent>
<prevsent>thus (khan et al, 2009) <papid> W09-0615 </papid>argues that measuring human-likeness disregards effectiveness of these expressions?.</prevsent>
</prevsection>
<citsent citstr=" P08-2050 ">
furthermore, as (belz and gatt, 2008) <papid> P08-2050 </papid>state there are no significant correlations between intrinsic and extrinsic evaluation measures?, concluding that similarity to human-produced reference texts is not necessarily indicative of quality as measured by human task performance?.from early on in the nlg community, task based extrinsic evaluations have been considered as the most meaningful evaluation, especially when having to convince people in other communities of the usefulness of system (reiter andbelz, 2009).<papid> J09-4008 </papid></citsent>
<aftsection>
<nextsent>task performance evaluation is recognized as the only known way to measure the effectiveness of nlg systems with real users?
</nextsent>
<nextsent>(re iter et al, 2003).
</nextsent>
<nextsent>following this direction, the give-challenges (koller et al, 2009) <papid> P09-2076 </papid>at inlg2010 (instruction generation) also include task performance evaluation.</nextsent>
<nextsent>in contrast to the vertical axis of figure 1, thereis the horizontal axis of the domain in which refer ring expressions are used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1602">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an additional problem is that corpus-similarity metrics measure how well system reproduces what speakers (or writers) do, while for most nlg systems ultimately the most important consideration is its effect on the human user (i.e. listener or reader).
</prevsent>
<prevsent>thus (khan et al, 2009) <papid> W09-0615 </papid>argues that measuring human-likeness disregards effectiveness of these expressions?.</prevsent>
</prevsection>
<citsent citstr=" J09-4008 ">
furthermore, as (belz and gatt, 2008) <papid> P08-2050 </papid>state there are no significant correlations between intrinsic and extrinsic evaluation measures?, concluding that similarity to human-produced reference texts is not necessarily indicative of quality as measured by human task performance?.from early on in the nlg community, task based extrinsic evaluations have been considered as the most meaningful evaluation, especially when having to convince people in other communities of the usefulness of system (reiter andbelz, 2009).<papid> J09-4008 </papid></citsent>
<aftsection>
<nextsent>task performance evaluation is recognized as the only known way to measure the effectiveness of nlg systems with real users?
</nextsent>
<nextsent>(re iter et al, 2003).
</nextsent>
<nextsent>following this direction, the give-challenges (koller et al, 2009) <papid> P09-2076 </papid>at inlg2010 (instruction generation) also include task performance evaluation.</nextsent>
<nextsent>in contrast to the vertical axis of figure 1, thereis the horizontal axis of the domain in which refer ring expressions are used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1603">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>task performance evaluation is recognized as the only known way to measure the effectiveness of nlg systems with real users?
</prevsent>
<prevsent>(re iter et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" P09-2076 ">
following this direction, the give-challenges (koller et al, 2009) <papid> P09-2076 </papid>at inlg2010 (instruction generation) also include task performance evaluation.</citsent>
<aftsection>
<nextsent>in contrast to the vertical axis of figure 1, thereis the horizontal axis of the domain in which refer ring expressions are used.
</nextsent>
<nextsent>referring expressions can thus be distinguished according to whether they are used in static or dynamic domain, corresponding to the left and right of the horizontal axis of figure 1.
</nextsent>
<nextsent>a static domain is one such as thetuna corpus (van deemter, 2007), which collects referring expressions based on motionless image.
</nextsent>
<nextsent>in contrast, dynamic domain comprises constantly changing situation where humans need context information to identify the referent of referring expression.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1604">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>referring expressions in the static domain have been evaluated relatively extensively.
</prevsent>
<prevsent>a recent example of an intrinsic evaluation is (van der sluis et al, 2007), who employed the dice-coefficient measuring corpus-similarity.
</prevsent>
</prevsection>
<citsent citstr=" W06-1409 ">
there have been number of extrinsic evaluations as well, such as(paraboni et al, 2006) <papid> W06-1409 </papid>and (khan et al, 2009), <papid> W09-0615 </papid>respectively measuring the effect of over specification on task performance and the impact of generated text on accuracy as well as processing speed.they belong thus in the top-left quadrant of figure 1.</citsent>
<aftsection>
<nextsent>over recent period, research in the generation of referring expressions has moved to dynamic domains such as situated dialog, e.g.
</nextsent>
<nextsent>(jordan and walker, 2005) and (stoia et al, 2006).<papid> W06-1412 </papid></nextsent>
<nextsent>however, both of them carried out an intrinsic evaluation measuring corpus-similarity or asking evaluators to compare system output to expressions used by human (the right bottom quadrant in figure 1).the construction of effective generation systems in the dynamic domain requires the implementation of an extrinsic task performance evaluation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1606">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there have been number of extrinsic evaluations as well, such as(paraboni et al, 2006) <papid> W06-1409 </papid>and (khan et al, 2009), <papid> W09-0615 </papid>respectively measuring the effect of over specification on task performance and the impact of generated text on accuracy as well as processing speed.they belong thus in the top-left quadrant of figure 1.</prevsent>
<prevsent>over recent period, research in the generation of referring expressions has moved to dynamic domains such as situated dialog, e.g.</prevsent>
</prevsection>
<citsent citstr=" W06-1412 ">
(jordan and walker, 2005) and (stoia et al, 2006).<papid> W06-1412 </papid></citsent>
<aftsection>
<nextsent>however, both of them carried out an intrinsic evaluation measuring corpus-similarity or asking evaluators to compare system output to expressions used by human (the right bottom quadrant in figure 1).the construction of effective generation systems in the dynamic domain requires the implementation of an extrinsic task performance evaluation.
</nextsent>
<nextsent>there has been work on extrinsic evaluation of instructions in the dynamic domain on the give-2 challenge (byron et al, 2009), <papid> W09-0628 </papid>which is task to generate instructions in virtual world.</nextsent>
<nextsent>it is based on the give-corpus (gargett et al, 2010), which is collected through keyboard interaction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1609">
<title id=" W10-4214.xml">towards an extrinsic evaluation of referring expressions in situated dialogs </title>
<section> referring expressions in the rex-j.  </section>
<citcontext>
<prevsection>
<prevsent>our results indicate that, at least in this domain, extrinsic evaluation results in dynamic domains can depend on the specific amount of context shown to the evaluator.
</prevsent>
<prevsent>based on the results from our evaluation experiments, we discuss the broader conclusions to be drawn and directions for future work.
</prevsent>
</prevsection>
<citsent citstr=" W09-0618 ">
corpus we utilize the rex-j corpus, japanese corpus of referring expressions in situated collaborative task (spanger et al, 2009<papid> W09-0618 </papid>a).</citsent>
<aftsection>
<nextsent>it was collected by recording the interaction of pair of dialog participants solving the tangram puzzle cooperatively.
</nextsent>
<nextsent>the goal of the tangram puzzle is to construct given shape by arranging seven pieces of simple figures as shown in figure 2 ! #$%&amp; #() * +,-.!%#+)# figure 2: screen shot of the tangram simulator in order to record the precise position of every piece and every action by the participants, we implemented simulator.
</nextsent>
<nextsent>the simulator displays two areas: goal shape area, and working area where pieces are shown and can be manipulated.we assigned different roles to the two participants of pair: solver and operator.
</nextsent>
<nextsent>the solver can see the goal shape but cannot manipulate the pieces and hence gives instructions to the opera tor; by contrast, the operator can manipulate the pieces but can not see the goal shape.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1634">
<title id=" W11-0118.xml">formalising and specifying under quantification </title>
<section> automatic quantification: first attempts.  </section>
<citcontext>
<prevsection>
<prevsent>in order to cater for proper nouns, we also indicate whether the head of the noun phrase is capitalised or not.
</prevsent>
<prevsent>article, number and capitalisation information is similarly provided for the object of the verb.
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
all features are automatically extracted from the robust minimal recur sion semantics (rmrs, copestake, 2004) representation of the sentence in which the noun phrase appears (obtained via rasp parse, briscoe et al, 2006).<papid> P06-4020 </papid></citsent>
<aftsection>
<nextsent>the following shows an example of feature line for particular noun phrase (the sentence in which the noun phrase appears is also given): original: [his early blues influences] included artists such as robert johnson, bukka white, skip james and sleepy john estes.
</nextsent>
<nextsent>features: past,possessive,plural,nocap,bare,plural,nocap note that articles belonging to the same class are labelled according to that class: all possessive articles, for instance, are simply marked as possessive?.
</nextsent>
<nextsent>this is the same for demonstrative articles.
</nextsent>
<nextsent>5.4 experiments and results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1635">
<title id=" W11-1402.xml">understanding differences in perceived peer review helpfulness using natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to address this issue, we propose to add peer-review helpfulness model to current peer-review systems, to automatically predict peer-review helpfulness based on features mined from textual reviews using natural language processing (nlp) techniques.
</prevsent>
<prevsent>such an intelligent component could enable peer-review systems to 1) control the quality of peer reviews that are sent back to authors, so authors can focus on the helpful ones; and 2) provide feedback to reviewers with respect to their reviewing performance, so students can learn to write better reviews.
</prevsent>
</prevsection>
<citsent citstr=" P11-2088 ">
in our prior work (xiong and litman, 2011), <papid> P11-2088 </papid>we examined whether techniques used for predicting the helpfulness of product reviews (kim et al, 2006) <papid> W06-1650 </papid>could be tailored to our peer-review domain, where the definition of helpfulness is largely influenced bythe educational context of peer review.</citsent>
<aftsection>
<nextsent>while previously we used the average of two expert-providedratings as our gold standard of peer-review help fulness1, there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model.
</nextsent>
<nextsent>in fact, we observe that peer-review helpfulness seems to differ not only between students and experts (exam ple 1), but also between types of experts (example 2).in the following examples, students judge helpfulness with discrete ratings from one to seven; experts judge it using one to five scale.
</nextsent>
<nextsent>higher ratings on both scales correspond to the most helpful reviews.
</nextsent>
<nextsent>example 1: student rating = 7, average expert rating = 2 the 1averaged ratings are considered more reliable since they are less noisy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1636">
<title id=" W11-1402.xml">understanding differences in perceived peer review helpfulness using natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to address this issue, we propose to add peer-review helpfulness model to current peer-review systems, to automatically predict peer-review helpfulness based on features mined from textual reviews using natural language processing (nlp) techniques.
</prevsent>
<prevsent>such an intelligent component could enable peer-review systems to 1) control the quality of peer reviews that are sent back to authors, so authors can focus on the helpful ones; and 2) provide feedback to reviewers with respect to their reviewing performance, so students can learn to write better reviews.
</prevsent>
</prevsection>
<citsent citstr=" W06-1650 ">
in our prior work (xiong and litman, 2011), <papid> P11-2088 </papid>we examined whether techniques used for predicting the helpfulness of product reviews (kim et al, 2006) <papid> W06-1650 </papid>could be tailored to our peer-review domain, where the definition of helpfulness is largely influenced bythe educational context of peer review.</citsent>
<aftsection>
<nextsent>while previously we used the average of two expert-providedratings as our gold standard of peer-review help fulness1, there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model.
</nextsent>
<nextsent>in fact, we observe that peer-review helpfulness seems to differ not only between students and experts (exam ple 1), but also between types of experts (example 2).in the following examples, students judge helpfulness with discrete ratings from one to seven; experts judge it using one to five scale.
</nextsent>
<nextsent>higher ratings on both scales correspond to the most helpful reviews.
</nextsent>
<nextsent>example 1: student rating = 7, average expert rating = 2 the 1averaged ratings are considered more reliable since they are less noisy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1643">
<title id=" W11-1402.xml">understanding differences in perceived peer review helpfulness using natural language processing </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>lexical features are counts of ten lexical categories (table 1), where the categories were learned in semi-supervised way from review lexicons in pilot study.
</prevsent>
<prevsent>we first manually created list of words that were specified as signal words for annotating feedback type and problem localization in the coding manual; then we supplemented the list with words selected by decision tree model learned using bag of-words representation of the peer reviews.
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
3we used mst parser (mcdonald et al, 2005) <papid> P05-1012 </papid>for syntactic analysis.</citsent>
<aftsection>
<nextsent>12 figure 1: distribution of peer-review helpfulness when rated by students and experts tag meaning word list sug suggestion should, must, might, could, need, needs, maybe, try, revision, want loc location page, paragraph, sentence err problem error, mistakes, typo, problem, difficulties, conclusion ide idea verb consider, mention lnk transition however, but neg negative words fail, hard, difficult, bad, short, little, bit, poor, few, unclear, only, more pos positive words great, good, well, clearly, easily, effective, effectively, helpful, very sum summarization main, overall, also, how, job not negation not, doesnt, dont sol solution revision specify correction table 1: ten lexical categories compared with commonly used lexical unigrams and bigrams (kim et al, 2006), <papid> W06-1650 </papid>these lexical categories are equally useful in modeling peer-review helpfulness, and significantly reduce the feature space.4 ? syntactic features mainly focus on nouns and verbs, and include percentage of tokens that are nouns, verbs, verbs conjugated in the first person (1stpverb%), adjectives/adverbs, and open classes, respectively.?</nextsent>
<nextsent>semantic features capture two important peer 4lexical categories help avoid the risk of over-fitting, given only 189 peer reviews in our case compared to more than ten thousand amazon.com reviews used for predicting product review helpfulness (kim et al, 2006).<papid> W06-1650 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1649">
<title id=" W11-1402.xml">understanding differences in perceived peer review helpfulness using natural language processing </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>kim et al (2006) <papid> W06-1650 </papid>extracted product property keywords from external resources based on their hypothesis that helpful product reviews refer frequently to certain product properties.</prevsent>
<prevsent>similarly, we hypothesize that helpful peer reviews are closely related to domain topics that are shared by all students papers in an assignment.</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
our domain topic set contains 288 words extracted from the collection of student papers using topic-lexicon extraction software5; our feature (domainword)5the software extracts topic words based on topic signatures (lin and hovy, 2000), <papid> C00-1072 </papid>and was kindly provided by annie louis.</citsent>
<aftsection>
<nextsent>13 feature description regtag% the percentage of problems in reviews that could be matched with localization pattern.
</nextsent>
<nextsent>sodomain% the percentage of sentences where any domain word appears between the subject and the object.
</nextsent>
<nextsent>ddeterminer the number of demonstrative determiners.
</nextsent>
<nextsent>window size for each review sentence, we search for the most likely referred window of words in the related paper, and window size is the average number of words of all windows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1653">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, in sentence 2, have look only means look and the meaning of the verb have is impoverished and is thus light.
</prevsent>
<prevsent>in this paper, we propose an in-depth case studyon lvc recognition, in which we investigate machine learning techniques for automatically identifying the impoverished meaning of verb given sentence.
</prevsent>
</prevsection>
<citsent citstr=" W06-2407 ">
unlike the earlier work that has viewed all verbs as possible light verbs (tan et al, 2006), <papid> W06-2407 </papid>we focus on half dozen of broadly documented and most frequently used english light verbs among the small set of them in english.</citsent>
<aftsection>
<nextsent>we construct token-based dataset with total of 2, 162 sentences extracted from british national corpus (bnc)1 and build learner with l2-loss svm.
</nextsent>
<nextsent>our system achieves 86.3% accuracy with baseline (chance) performance of 52.2%.
</nextsent>
<nextsent>we also extract automatically two groups of features, statistical and contextual features and present detailed ablation analysis of the interaction of these features.interestingly, the results show that the system performs similarly when trained independently with either groups of these features.
</nextsent>
<nextsent>and the integration of these two types of features does not improve theperformance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1654">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>lvcs have been well-studied in linguistics since early days (jespersen, 1965; butt, 2003; kearns, 2002).
</prevsent>
<prevsent>recent computational research on lvcsmainly focuses on type-based classification, i.e., statistically aggregated properties of lvcs.
</prevsent>
</prevsection>
<citsent citstr=" H05-1113 ">
for example, many works are about direct measuring of the compositionality (venkatapathy and joshi, 2005),<papid> H05-1113 </papid>compatibility (barrett and davis, 2003), acceptability (north, 2005) and productivity (stevenson et al,2004) <papid> W04-0401 </papid>of lvcs.</citsent>
<aftsection>
<nextsent>other works, if related to token based identification, i.e., identifying idiomatic expressions within context, only consider lvcs as one small subtype of other idiomatic expressions (cook et al, 2007; <papid> W07-1106 </papid>fazly and stevenson, 2006).<papid> E06-1043 </papid></nextsent>
<nextsent>previous computational works on token-basedidentification differs from our work in one key aspect.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1655">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>lvcs have been well-studied in linguistics since early days (jespersen, 1965; butt, 2003; kearns, 2002).
</prevsent>
<prevsent>recent computational research on lvcsmainly focuses on type-based classification, i.e., statistically aggregated properties of lvcs.
</prevsent>
</prevsection>
<citsent citstr=" W04-0401 ">
for example, many works are about direct measuring of the compositionality (venkatapathy and joshi, 2005),<papid> H05-1113 </papid>compatibility (barrett and davis, 2003), acceptability (north, 2005) and productivity (stevenson et al,2004) <papid> W04-0401 </papid>of lvcs.</citsent>
<aftsection>
<nextsent>other works, if related to token based identification, i.e., identifying idiomatic expressions within context, only consider lvcs as one small subtype of other idiomatic expressions (cook et al, 2007; <papid> W07-1106 </papid>fazly and stevenson, 2006).<papid> E06-1043 </papid></nextsent>
<nextsent>previous computational works on token-basedidentification differs from our work in one key aspect.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1656">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recent computational research on lvcsmainly focuses on type-based classification, i.e., statistically aggregated properties of lvcs.
</prevsent>
<prevsent>for example, many works are about direct measuring of the compositionality (venkatapathy and joshi, 2005),<papid> H05-1113 </papid>compatibility (barrett and davis, 2003), acceptability (north, 2005) and productivity (stevenson et al,2004) <papid> W04-0401 </papid>of lvcs.</prevsent>
</prevsection>
<citsent citstr=" W07-1106 ">
other works, if related to token based identification, i.e., identifying idiomatic expressions within context, only consider lvcs as one small subtype of other idiomatic expressions (cook et al, 2007; <papid> W07-1106 </papid>fazly and stevenson, 2006).<papid> E06-1043 </papid></citsent>
<aftsection>
<nextsent>previous computational works on token-basedidentification differs from our work in one key aspect.
</nextsent>
<nextsent>our work builds learning system which systematically incorporates both informative statistical measures and specific local contexts and does indepth analysis on both of them while many previous works, either totally relyon or only emphasize on one of them.
</nextsent>
<nextsent>for example, the method used in (katz and giesbrecht, 2006) <papid> W06-1203 </papid>relies primarily onlocal co-occurrence lexicon to construct feature vectors for each target token.</nextsent>
<nextsent>on the other hand, some other works (fazly and stevenson, 2007; <papid> W07-1102 </papid>fazly and stevenson, 2006; <papid> E06-1043 </papid>stevenson et al, 2004), <papid> W04-0401 </papid>argue that linguistic properties, such as canonical syntactic patterns of specific types of idioms, are more informative than local context.tan et.al.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1657">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recent computational research on lvcsmainly focuses on type-based classification, i.e., statistically aggregated properties of lvcs.
</prevsent>
<prevsent>for example, many works are about direct measuring of the compositionality (venkatapathy and joshi, 2005),<papid> H05-1113 </papid>compatibility (barrett and davis, 2003), acceptability (north, 2005) and productivity (stevenson et al,2004) <papid> W04-0401 </papid>of lvcs.</prevsent>
</prevsection>
<citsent citstr=" E06-1043 ">
other works, if related to token based identification, i.e., identifying idiomatic expressions within context, only consider lvcs as one small subtype of other idiomatic expressions (cook et al, 2007; <papid> W07-1106 </papid>fazly and stevenson, 2006).<papid> E06-1043 </papid></citsent>
<aftsection>
<nextsent>previous computational works on token-basedidentification differs from our work in one key aspect.
</nextsent>
<nextsent>our work builds learning system which systematically incorporates both informative statistical measures and specific local contexts and does indepth analysis on both of them while many previous works, either totally relyon or only emphasize on one of them.
</nextsent>
<nextsent>for example, the method used in (katz and giesbrecht, 2006) <papid> W06-1203 </papid>relies primarily onlocal co-occurrence lexicon to construct feature vectors for each target token.</nextsent>
<nextsent>on the other hand, some other works (fazly and stevenson, 2007; <papid> W07-1102 </papid>fazly and stevenson, 2006; <papid> E06-1043 </papid>stevenson et al, 2004), <papid> W04-0401 </papid>argue that linguistic properties, such as canonical syntactic patterns of specific types of idioms, are more informative than local context.tan et.al.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1658">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous computational works on token-basedidentification differs from our work in one key aspect.
</prevsent>
<prevsent>our work builds learning system which systematically incorporates both informative statistical measures and specific local contexts and does indepth analysis on both of them while many previous works, either totally relyon or only emphasize on one of them.
</prevsent>
</prevsection>
<citsent citstr=" W06-1203 ">
for example, the method used in (katz and giesbrecht, 2006) <papid> W06-1203 </papid>relies primarily onlocal co-occurrence lexicon to construct feature vectors for each target token.</citsent>
<aftsection>
<nextsent>on the other hand, some other works (fazly and stevenson, 2007; <papid> W07-1102 </papid>fazly and stevenson, 2006; <papid> E06-1043 </papid>stevenson et al, 2004), <papid> W04-0401 </papid>argue that linguistic properties, such as canonical syntactic patterns of specific types of idioms, are more informative than local context.tan et.al.</nextsent>
<nextsent>(tan et al, 2006) <papid> W06-2407 </papid>propose learning approach to identify token-based lvcs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1659">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our work builds learning system which systematically incorporates both informative statistical measures and specific local contexts and does indepth analysis on both of them while many previous works, either totally relyon or only emphasize on one of them.
</prevsent>
<prevsent>for example, the method used in (katz and giesbrecht, 2006) <papid> W06-1203 </papid>relies primarily onlocal co-occurrence lexicon to construct feature vectors for each target token.</prevsent>
</prevsection>
<citsent citstr=" W07-1102 ">
on the other hand, some other works (fazly and stevenson, 2007; <papid> W07-1102 </papid>fazly and stevenson, 2006; <papid> E06-1043 </papid>stevenson et al, 2004), <papid> W04-0401 </papid>argue that linguistic properties, such as canonical syntactic patterns of specific types of idioms, are more informative than local context.tan et.al.</citsent>
<aftsection>
<nextsent>(tan et al, 2006) <papid> W06-2407 </papid>propose learning approach to identify token-based lvcs.</nextsent>
<nextsent>the method isonly similar to ours in that it is supervised frame work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1667">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> learning english lvcs.  </section>
<citcontext>
<prevsection>
<prevsent>we use frequency counts gathered from british national corpus (bnc) and then calculate the ratio since bnc encodes the lexeme for each word and is also tagged with parts of speech.
</prevsent>
<prevsent>in addition, it is large corpus with 100 million words, thus, an ideal corpus to calculate the verb-noun usage for each candidate word in the object position.
</prevsent>
</prevsection>
<citsent citstr=" W98-0604 ">
two other lexical resources, wordnet (fellbaum, 1998) and nomlex (meyers et al, 1998), <papid> W98-0604 </papid>are used to identify words which can directly be used as anoun and verb and those that are derivational re lated.</citsent>
<aftsection>
<nextsent>specifically, wordnet is used to identify the words which can be used as both noun and verband nomlex is used to recognize those derivation ally related words.
</nextsent>
<nextsent>and the verb usage counts ofthese nouns are the frequencies of their corresponding derivational verbs.
</nextsent>
<nextsent>for example, for the word transmission?, its verb usage frequency is the countin bnc with its derivation ally related verb trans mit?.phrase size: the third statistical feature is the actual size of the candidate lvc phrase.
</nextsent>
<nextsent>many modifiers can be inserted inside the candidate phrases to generate new candidates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1668">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> learning english lvcs.  </section>
<citcontext>
<prevsection>
<prevsent>lv-nounobj: this is the bigram of the light verband the head of the noun phrase.
</prevsent>
<prevsent>this feature encodes the collocation information between the candidate light verb and the head noun of its object.
</prevsent>
</prevsection>
<citsent citstr=" W05-1005 ">
levins class: it is observed that members within certain groups of verb classes are legitimate candidates to form acceptable lvcs (fazly et al, 2005).<papid> W05-1005 </papid>for example, many sound emission verbs according to levin (levin, 1993), such as clap, whistle, and plop, can be used to generate legitimate lvcs.</citsent>
<aftsection>
<nextsent>phrases such as make clap/plop/whistle are all highly acceptable lvcs by humans even though some of them, such as make plop rarely occur within corpora.
</nextsent>
<nextsent>we formulate vector for all the 256 levins verb classes and turn the correspond-.
</nextsent>
<nextsent>ing class-bits on when the verb usage of the head noun in candidate lvc belongs to these classes.
</nextsent>
<nextsent>we add one extra class, other, to be mapped to those verbs which are not included in any one of these 256 levins verb classes.other features: we construct other local contextual features, for example, the part of speech of the word immediately before the light verb (titled posbefore) and after the whole phrase (posafter).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1671">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> experiments and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>webegin our sentence selection process with the examination of handful of previously investigated verbs (fazly and stevenson, 2007; <papid> W07-1102 </papid>butt, 2003).</prevsent>
<prevsent>among them, we pick the 6 most frequently used english light verbs: do, get, give, have, make and take.</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
to identify potential lvcs within sentences, we first extract all sentences where one or more of the six verbs occur from bnc (xml edition) and then parse these sentences with charniaks parser (char niak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>we focus on the verb + noun object?
</nextsent>
<nextsent>pattern and choose all the sentences which have direct np object for the target verbs.
</nextsent>
<nextsent>we then collect total of 207, 789 sentences.
</nextsent>
<nextsent>we observe that within all these chosen sentences,the distribution of true lvcs is still low.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1673">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> experiments and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>for each experiment, we evaluate the performance with three sets of metrics.
</prevsent>
<prevsent>we first report the standard accuracy on the test dataset.
</prevsent>
</prevsection>
<citsent citstr=" J09-1005 ">
since accuracy is argued not to be sufficient measure of the evaluation of binary classifier (fazly et al, 2009) <papid> J09-1005 </papid>and some previous works also report f1 values for the positive classes, we therefore choose to report the precision, recall and f1 value for both positive and negative classes.</citsent>
<aftsection>
<nextsent>true class+ predicted class + tp fp - fn tn table 1: confusion matrix to define true positive (tp), true negative (tn), false positive (fp) and false negative (fn).
</nextsent>
<nextsent>6http://cogcomp.cs.illinois.edu/ytu/test/lvcmain.html 35 based on the classic confusion matrix as shown in table 1, we calculate the precision and recall for the positive class in equation 1: p+ = tptp + fp + = tptp + fn (1) and similarly, we use equation 2 for negative class.and the f1 value is the harmonic mean of the precision and recall of each class.
</nextsent>
<nextsent>p? = tntn + fn ? = tntn + fp (2) 4.3 experiments with contextual features.
</nextsent>
<nextsent>in our experiments, we aim to build high performance lvc classifier as well as to analyze the interaction between contextual and statistical features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1674">
<title id=" W11-0807.xml">learning english light verb constructions contextual or statistical </title>
<section> experiments and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>we then train the classifier with them together and observe that the classifier exhibits similar performance as the one trained with them independently as shown in table 7.
</prevsent>
<prevsent>this result indicates that these two types of features actually provide similar knowledge to the system and therefore combining them together does not provide any additional new information.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
this observation also agrees with the intuition that point-wise mutual information basically provides information on word collocations (church and hanks, 1990).<papid> J90-1003 </papid>feature accuracy f1+ f1 lv-nounobj 83.817 82.028 85.283 cpmi 83.402 81.481 84.962 cpmi+lv-nounobj 83.817 82.028 85.283 table 7: the classifier achieves similar performance trained jointly with cpmi and lv-nounobj features, comparing with the performance trained independently.</citsent>
<aftsection>
<nextsent>37 4.5.2 situation when they are different token-based lvc identification is difficult task on the basis of surface structures since they always exhibit identical surface properties.
</nextsent>
<nextsent>however, candidate lvcs with identical surface structures in both positive and negative examples provide an ideal test bed for the functionality of local contextual features.for example, consider again these two aforementioned sentences which are repeated here for refer ence: 1.
</nextsent>
<nextsent>he had look of childish bewilderment on his.
</nextsent>
<nextsent>face.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1675">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results look promising, given the minimal supervision of the approach, which can be further scaled up.
</prevsent>
<prevsent>paraphrase is an important linguistic phenomenon which occurs widely in human languages.
</prevsent>
</prevsection>
<citsent citstr=" D09-1040 ">
since paraphrases capture the variations of linguistic expressions while preserving the meaning, they are very useful in many applications, such as machine translation (marton et al, 2009), <papid> D09-1040 </papid>document summarization (barzilay et al, 1999), <papid> P99-1071 </papid>and recognizing textual entailment (rte) (dagan et al, 2005).however, such resources are not trivial to obtain.</citsent>
<aftsection>
<nextsent>if we make comparison between paraphrase and mt, the latter has large parallel bilin gual/multilingual corpora to acquire translation pairs in different granularity; while it is difficult to find naturally?
</nextsent>
<nextsent>occurred paraphrase parallel?
</nextsent>
<nextsent>corpora.
</nextsent>
<nextsent>furthermore, in mt, certain words can be translated into (rather) small set of candidate words in the target language; while in principle, each paraphrase can have infinite number of target?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1676">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results look promising, given the minimal supervision of the approach, which can be further scaled up.
</prevsent>
<prevsent>paraphrase is an important linguistic phenomenon which occurs widely in human languages.
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
since paraphrases capture the variations of linguistic expressions while preserving the meaning, they are very useful in many applications, such as machine translation (marton et al, 2009), <papid> D09-1040 </papid>document summarization (barzilay et al, 1999), <papid> P99-1071 </papid>and recognizing textual entailment (rte) (dagan et al, 2005).however, such resources are not trivial to obtain.</citsent>
<aftsection>
<nextsent>if we make comparison between paraphrase and mt, the latter has large parallel bilin gual/multilingual corpora to acquire translation pairs in different granularity; while it is difficult to find naturally?
</nextsent>
<nextsent>occurred paraphrase parallel?
</nextsent>
<nextsent>corpora.
</nextsent>
<nextsent>furthermore, in mt, certain words can be translated into (rather) small set of candidate words in the target language; while in principle, each paraphrase can have infinite number of target?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1677">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>expressions, which reflects the variety of each human language.
</prevsent>
<prevsent>a variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data.
</prevsent>
</prevsection>
<citsent citstr=" P08-1089 ">
some require bilingual parallel corpora (callison-burch, 2008; zhao et al, 2008), <papid> P08-1089 </papid>others require monolingual parallel corpora (barzilay and mckeown, 2001; <papid> P01-1008 </papid>ibrahim et al, 2003) <papid> W03-1608 </papid>or monolingual comparable corpora (dolan et al, 2004).<papid> C04-1051 </papid></citsent>
<aftsection>
<nextsent>in this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data.
</nextsent>
<nextsent>additionally, this would potentially allow us to extract paraphrases fora variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora.
</nextsent>
<nextsent>this paper makes the following contributions: 1.
</nextsent>
<nextsent>we adapt translation fragment pair extrac-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1678">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>expressions, which reflects the variety of each human language.
</prevsent>
<prevsent>a variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
some require bilingual parallel corpora (callison-burch, 2008; zhao et al, 2008), <papid> P08-1089 </papid>others require monolingual parallel corpora (barzilay and mckeown, 2001; <papid> P01-1008 </papid>ibrahim et al, 2003) <papid> W03-1608 </papid>or monolingual comparable corpora (dolan et al, 2004).<papid> C04-1051 </papid></citsent>
<aftsection>
<nextsent>in this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data.
</nextsent>
<nextsent>additionally, this would potentially allow us to extract paraphrases fora variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora.
</nextsent>
<nextsent>this paper makes the following contributions: 1.
</nextsent>
<nextsent>we adapt translation fragment pair extrac-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1680">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>expressions, which reflects the variety of each human language.
</prevsent>
<prevsent>a variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data.
</prevsent>
</prevsection>
<citsent citstr=" W03-1608 ">
some require bilingual parallel corpora (callison-burch, 2008; zhao et al, 2008), <papid> P08-1089 </papid>others require monolingual parallel corpora (barzilay and mckeown, 2001; <papid> P01-1008 </papid>ibrahim et al, 2003) <papid> W03-1608 </papid>or monolingual comparable corpora (dolan et al, 2004).<papid> C04-1051 </papid></citsent>
<aftsection>
<nextsent>in this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data.
</nextsent>
<nextsent>additionally, this would potentially allow us to extract paraphrases fora variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora.
</nextsent>
<nextsent>this paper makes the following contributions: 1.
</nextsent>
<nextsent>we adapt translation fragment pair extrac-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1681">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>expressions, which reflects the variety of each human language.
</prevsent>
<prevsent>a variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
some require bilingual parallel corpora (callison-burch, 2008; zhao et al, 2008), <papid> P08-1089 </papid>others require monolingual parallel corpora (barzilay and mckeown, 2001; <papid> P01-1008 </papid>ibrahim et al, 2003) <papid> W03-1608 </papid>or monolingual comparable corpora (dolan et al, 2004).<papid> C04-1051 </papid></citsent>
<aftsection>
<nextsent>in this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data.
</nextsent>
<nextsent>additionally, this would potentially allow us to extract paraphrases fora variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora.
</nextsent>
<nextsent>this paper makes the following contributions: 1.
</nextsent>
<nextsent>we adapt translation fragment pair extrac-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1683">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>of the paraphrase collection, using the crowdsourcing technique, which is effective, fast, and cheap.
</prevsent>
<prevsent>52 proceedings of the 4th workshop on building and using comparable corpora, pages 5260, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.
</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
c2011 association for computational linguistics corpora sentence level sub-sentential level paraphrase acquisition monolingual parallel e.g., barzilay and mckeown (2001) <papid> P01-1008 </papid>this paper comparable e.g., quirk et al (2004) <papid> W04-3219 </papid>e.g., shinyama et al (2002) &amp; this paper bilingual parallel n/a e.g., bannard and callison-burch (2005) statistical machine translation bilingual parallel most smt systems smt phrase tables comparable e.g., fung and lo (1998) <papid> P98-1069 </papid>e.g., munteanu and marcu (2006) table 1: previous work in paraphrase acquisition and machine translation.</citsent>
<aftsection>
<nextsent>roughly speaking, there are three dimensions to characterize the previous work in paraphrase acquisition and machine translation, whether the data comes from monolingual or bilingual corpora, whether the corpora are parallel or comparable, and whether the output is at the sentence level or at the sub-sentential level.
</nextsent>
<nextsent>table 1 gives one example in each category.
</nextsent>
<nextsent>paraphrase acquisition is mostly done at the sentence-level, e.g., (barzilay and mckeown, 2001; <papid> P01-1008 </papid>barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al, 2004), <papid> C04-1051 </papid>which is not straightforward to be used as resource for other nlp applications.</nextsent>
<nextsent>quirk et al (2004) <papid> W04-3219 </papid>adopted themt approach to translate?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1685">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>of the paraphrase collection, using the crowdsourcing technique, which is effective, fast, and cheap.
</prevsent>
<prevsent>52 proceedings of the 4th workshop on building and using comparable corpora, pages 5260, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
c2011 association for computational linguistics corpora sentence level sub-sentential level paraphrase acquisition monolingual parallel e.g., barzilay and mckeown (2001) <papid> P01-1008 </papid>this paper comparable e.g., quirk et al (2004) <papid> W04-3219 </papid>e.g., shinyama et al (2002) &amp; this paper bilingual parallel n/a e.g., bannard and callison-burch (2005) statistical machine translation bilingual parallel most smt systems smt phrase tables comparable e.g., fung and lo (1998) <papid> P98-1069 </papid>e.g., munteanu and marcu (2006) table 1: previous work in paraphrase acquisition and machine translation.</citsent>
<aftsection>
<nextsent>roughly speaking, there are three dimensions to characterize the previous work in paraphrase acquisition and machine translation, whether the data comes from monolingual or bilingual corpora, whether the corpora are parallel or comparable, and whether the output is at the sentence level or at the sub-sentential level.
</nextsent>
<nextsent>table 1 gives one example in each category.
</nextsent>
<nextsent>paraphrase acquisition is mostly done at the sentence-level, e.g., (barzilay and mckeown, 2001; <papid> P01-1008 </papid>barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al, 2004), <papid> C04-1051 </papid>which is not straightforward to be used as resource for other nlp applications.</nextsent>
<nextsent>quirk et al (2004) <papid> W04-3219 </papid>adopted themt approach to translate?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1688">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>roughly speaking, there are three dimensions to characterize the previous work in paraphrase acquisition and machine translation, whether the data comes from monolingual or bilingual corpora, whether the corpora are parallel or comparable, and whether the output is at the sentence level or at the sub-sentential level.
</prevsent>
<prevsent>table 1 gives one example in each category.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
paraphrase acquisition is mostly done at the sentence-level, e.g., (barzilay and mckeown, 2001; <papid> P01-1008 </papid>barzilay and lee, 2003; <papid> N03-1003 </papid>dolan et al, 2004), <papid> C04-1051 </papid>which is not straightforward to be used as resource for other nlp applications.</citsent>
<aftsection>
<nextsent>quirk et al (2004) <papid> W04-3219 </papid>adopted themt approach to translate?</nextsent>
<nextsent>one sentence into paraphrased one.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1693">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>quirk et al (2004) <papid> W04-3219 </papid>adopted themt approach to translate?</prevsent>
<prevsent>one sentence into paraphrased one.</prevsent>
</prevsection>
<citsent citstr=" W03-1609 ">
as for the corpora, barzilay and mckeown (2001) <papid> P01-1008 </papid>took different english translations of the same novels (i.e., monolingual parallel corpora), while the others experimented on multiple sources of the same news/events, i.e., monolingual comparable corpora.at the sub-sentential level, interchangeable patterns (shinyama et al, 2002; shinyama and sekine, 2003) <papid> W03-1609 </papid>or inference rules (lin and pantel, 2001)are extracted, which are quite successful in named entity-centered tasks, like information extraction, while they are not generalized enough to be applied to other tasks or they have rather small coverage, e.g. rte (dinu and wang, 2009).<papid> E09-1025 </papid></citsent>
<aftsection>
<nextsent>to our best knowledge, there is few focused study on general paraphrase fragments extraction at the sub-sentential level, from comparable corpora.
</nextsent>
<nextsent>a recent study by belz and kow (2010) <papid> W10-4217 </papid>mainly aimed at natural language generation, which they performed small scale experiment on specific topic, i.e., british hills.</nextsent>
<nextsent>given the available parallel corpora from the mt community, there are studies focusing on extracting paraphrases from bilingual corpora (bannard and callison-burch, 2005; callison-burch, 2008; zhaoet al, 2008).<papid> P08-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1694">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>quirk et al (2004) <papid> W04-3219 </papid>adopted themt approach to translate?</prevsent>
<prevsent>one sentence into paraphrased one.</prevsent>
</prevsection>
<citsent citstr=" E09-1025 ">
as for the corpora, barzilay and mckeown (2001) <papid> P01-1008 </papid>took different english translations of the same novels (i.e., monolingual parallel corpora), while the others experimented on multiple sources of the same news/events, i.e., monolingual comparable corpora.at the sub-sentential level, interchangeable patterns (shinyama et al, 2002; shinyama and sekine, 2003) <papid> W03-1609 </papid>or inference rules (lin and pantel, 2001)are extracted, which are quite successful in named entity-centered tasks, like information extraction, while they are not generalized enough to be applied to other tasks or they have rather small coverage, e.g. rte (dinu and wang, 2009).<papid> E09-1025 </papid></citsent>
<aftsection>
<nextsent>to our best knowledge, there is few focused study on general paraphrase fragments extraction at the sub-sentential level, from comparable corpora.
</nextsent>
<nextsent>a recent study by belz and kow (2010) <papid> W10-4217 </papid>mainly aimed at natural language generation, which they performed small scale experiment on specific topic, i.e., british hills.</nextsent>
<nextsent>given the available parallel corpora from the mt community, there are studies focusing on extracting paraphrases from bilingual corpora (bannard and callison-burch, 2005; callison-burch, 2008; zhaoet al, 2008).<papid> P08-1089 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1695">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as for the corpora, barzilay and mckeown (2001) <papid> P01-1008 </papid>took different english translations of the same novels (i.e., monolingual parallel corpora), while the others experimented on multiple sources of the same news/events, i.e., monolingual comparable corpora.at the sub-sentential level, interchangeable patterns (shinyama et al, 2002; shinyama and sekine, 2003) <papid> W03-1609 </papid>or inference rules (lin and pantel, 2001)are extracted, which are quite successful in named entity-centered tasks, like information extraction, while they are not generalized enough to be applied to other tasks or they have rather small coverage, e.g. rte (dinu and wang, 2009).<papid> E09-1025 </papid></prevsent>
<prevsent>to our best knowledge, there is few focused study on general paraphrase fragments extraction at the sub-sentential level, from comparable corpora.</prevsent>
</prevsection>
<citsent citstr=" W10-4217 ">
a recent study by belz and kow (2010) <papid> W10-4217 </papid>mainly aimed at natural language generation, which they performed small scale experiment on specific topic, i.e., british hills.</citsent>
<aftsection>
<nextsent>given the available parallel corpora from the mt community, there are studies focusing on extracting paraphrases from bilingual corpora (bannard and callison-burch, 2005; callison-burch, 2008; zhaoet al, 2008).<papid> P08-1089 </papid></nextsent>
<nextsent>the way they do is to treat one language as an pivot and equate two phrases in the other languages as paraphrases if they share common pivot phrase.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1697">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted.
</prevsent>
<prevsent>as for the mt research, the standard statistical mt systems require large size of parallel corpora for training and then extract sub-sentential translation phrases.
</prevsent>
</prevsection>
<citsent citstr=" W04-3208 ">
apart from the limited parallel corpora,comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (fung and lo, 1998; koehn and knight, 2000; vogel, 2003; fung and cheung, 2004<papid> W04-3208 </papid>a; fung and cheung, 2004<papid> W04-3208 </papid>b; munteanu and marcu, 2005; <papid> J05-4003 </papid>wu and fung, 2005).<papid> I05-1023 </papid></citsent>
<aftsection>
<nextsent>a recent study by smith et al (2010) <papid> N10-1063 </papid>extracted parallel sentences from comparable corpora to extend the existing resources.</nextsent>
<nextsent>at the sub-sentential level, munteanu and marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the log likelihood-ratio of word translation probability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1705">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted.
</prevsent>
<prevsent>as for the mt research, the standard statistical mt systems require large size of parallel corpora for training and then extract sub-sentential translation phrases.
</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
apart from the limited parallel corpora,comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (fung and lo, 1998; koehn and knight, 2000; vogel, 2003; fung and cheung, 2004<papid> W04-3208 </papid>a; fung and cheung, 2004<papid> W04-3208 </papid>b; munteanu and marcu, 2005; <papid> J05-4003 </papid>wu and fung, 2005).<papid> I05-1023 </papid></citsent>
<aftsection>
<nextsent>a recent study by smith et al (2010) <papid> N10-1063 </papid>extracted parallel sentences from comparable corpora to extend the existing resources.</nextsent>
<nextsent>at the sub-sentential level, munteanu and marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the log likelihood-ratio of word translation probability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1706">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted.
</prevsent>
<prevsent>as for the mt research, the standard statistical mt systems require large size of parallel corpora for training and then extract sub-sentential translation phrases.
</prevsent>
</prevsection>
<citsent citstr=" I05-1023 ">
apart from the limited parallel corpora,comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (fung and lo, 1998; koehn and knight, 2000; vogel, 2003; fung and cheung, 2004<papid> W04-3208 </papid>a; fung and cheung, 2004<papid> W04-3208 </papid>b; munteanu and marcu, 2005; <papid> J05-4003 </papid>wu and fung, 2005).<papid> I05-1023 </papid></citsent>
<aftsection>
<nextsent>a recent study by smith et al (2010) <papid> N10-1063 </papid>extracted parallel sentences from comparable corpora to extend the existing resources.</nextsent>
<nextsent>at the sub-sentential level, munteanu and marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the log likelihood-ratio of word translation probability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1707">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as for the mt research, the standard statistical mt systems require large size of parallel corpora for training and then extract sub-sentential translation phrases.
</prevsent>
<prevsent>apart from the limited parallel corpora,comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (fung and lo, 1998; koehn and knight, 2000; vogel, 2003; fung and cheung, 2004<papid> W04-3208 </papid>a; fung and cheung, 2004<papid> W04-3208 </papid>b; munteanu and marcu, 2005; <papid> J05-4003 </papid>wu and fung, 2005).<papid> I05-1023 </papid></prevsent>
</prevsection>
<citsent citstr=" N10-1063 ">
a recent study by smith et al (2010) <papid> N10-1063 </papid>extracted parallel sentences from comparable corpora to extend the existing resources.</citsent>
<aftsection>
<nextsent>at the sub-sentential level, munteanu and marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the log likelihood-ratio of word translation probability.
</nextsent>
<nextsent>they exploit the possibility of making use of reports within limited time window, which are about the same event or having overlapping contents, but in different languages.
</nextsent>
<nextsent>quirk et al (2007) extracted fragments using generative model of noisy translations.
</nextsent>
<nextsent>they show that even in non-parallel corpora, useful parallel words or phrases can still be found and the size of such data is much larger than that of 53 document pair extraction sentence pair extraction fragment pair extraction corpora (gigaword)  doc  . ..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1708">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in parcitular, amazons mechanical turk1 (mturk) provides way to pay people small amounts of money to perform tasks that are simple for humans but difficult for computers.
</prevsent>
<prevsent>examples of these human intelligence tasks (or hits) range from labeling images to moderating blog comments to providing feedback on relevance of results for search query.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
using mturk for nlp task evaluation has been shown to be significantly cheaper and faster, and there is high agreement between aggregatenon-expert annotations and gold-standard annotations provided by the experts (snow et al, 2008).<papid> D08-1027 </papid></citsent>
<aftsection>
<nextsent>1http://www.mturk.com/
</nextsent>
<nextsent>figure 1 shows the pipeline of our paraphrase acquisition method.
</nextsent>
<nextsent>we evaluate quality at each stage using amazons mechanical turk.
</nextsent>
<nextsent>in order to ensure that the non-expert annotators complete the task accurately, we used both positive and negative controls.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1710">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> fragment pair acquisition.  </section>
<citcontext>
<prevsection>
<prevsent>one is to change the common substring alignment problem from string to word sequence and we extend the longest common substring (lcs) extraction algorithm to multiple common n-grams.
</prevsent>
<prevsent>an alternative way is to use normal word aligner(widely used as the first step in mt systems) to accomplish the job.
</prevsent>
</prevsection>
<citsent citstr=" N06-1014 ">
for our experiments, we use the berkeleyaligner4 (liang et al, 2006) <papid> N06-1014 </papid>by feeding it dictionary of pairs of identical words along with the paired sentences.</citsent>
<aftsection>
<nextsent>we can also combine these two methods by performing the lcs alignment first and adding additional word alignments from the aligner.
</nextsent>
<nextsent>these form the three configurations of our system (table 2).
</nextsent>
<nextsent>following munteanu and marcu (2006), we use both positive and negative lexical associations for the alignment.
</nextsent>
<nextsent>the positive association measures 4http://code.google.com/p/ berkeleyaligner/ how likely one word will be aligned to another (value from 0 to 1); and the negative associations indicates how unlikely an alignment exists between word pair (from -1 to 0).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1711">
<title id=" W11-1208.xml">paraphrase fragment extraction from monolingual comparable corpora </title>
<section> fragment pair acquisition.  </section>
<citcontext>
<prevsection>
<prevsent>the fourth example in table 5).
</prevsent>
<prevsent>an alternative way is to follow the linguistic definition of phrase, e.g. noun phrase (np), verb phrase (vp), etc. in this case, we need to use (at least) chunker to prepro cess the text and obtain the proper boundary of each fragment and we used the opennlp chunker.we finalize our paraphrase collection by filtering out identical fragment pairs, subsumed fragment pairs (one fragment is fully contained in the other),and fragment having only one word.
</prevsent>
</prevsection>
<citsent citstr=" I05-5002 ">
apart from sentence pairs collected from the comparable corpora,we also did experiments on the existing msr paraphrase corpus (dolan and brockett, 2005), <papid> I05-5002 </papid>which isa collection of manually annotated sentential para phrases.</citsent>
<aftsection>
<nextsent>the evaluation on both collections is done by the mturk.
</nextsent>
<nextsent>each task contains 8 pairs of fragments tobe evaluated, plus one positive control using identical fragment pairs, and one negative control using pair of random fragments.
</nextsent>
<nextsent>all the fragments are shown with the corresponding sentences from where they are extracted5.
</nextsent>
<nextsent>the question being asked is 5we thought about evaluating pairs of isolated fragments, how are the two highlighted phrases related??, and the possible answers are, these phrases refer to the same thing as each other?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1713">
<title id=" W11-0404.xml">how good is the crowd at real wsd </title>
<section> defining the task.  </section>
<citcontext>
<prevsection>
<prevsent>the oklahoma was ripped apart when seven torpedoes hit her.
</prevsent>
<prevsent>strictly speaking, the ship is caused to fragment, but the military purpose is to damage her beyond repair, if possible.
</prevsent>
</prevsection>
<citsent citstr=" D09-1046 ">
and there are fairly often examples where the sentence in isolation is ambiguous: rain ripped an other piece of croissant, the sky ripped and hung in tatters , revealing plasterboard and lath behind.such cases are pushing us toward trying to incorporate blending of senses into our paradigm, along the lines of (erk and mccarthy, 2009).<papid> D09-1046 </papid></citsent>
<aftsection>
<nextsent>36 3 conclusion.
</nextsent>
<nextsent>we have shown that it is possible to set up hits on amazon mechanical turk to discriminate the fairly fine sense distinctions used in framenet, if the right approach is taken, and that the results reach level of accuracy that can be useful for further processing, as well as serving as cross-check on the expert data and an invitation to re-think the task itself.
</nextsent>
<nextsent>although the total amount of data collected may not be large by some standards, it has been sufficient to give good sense of which techniques work for the type of wsd problems we are facing.
</nextsent>
<nextsent>we intend to continue investigating the general applicability of this system for frame disambiguation, including further analysis of our data to better understand the factors that make disambiguation task more or less difficult for crowd workers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1714">
<title id=" W11-0202.xml">unsupervised entailment detection between dependency graph fragments </title>
<section> applications.  </section>
<citcontext>
<prevsection>
<prevsent>pattern (2) entails pattern (1) and would also return results matching the information need.
</prevsent>
<prevsent>(2) the over expression of in the larval fat bodya system for entailment detection can automatically extract database of entailing fragments from large corpus and use them to modify any query given by the user.
</prevsent>
</prevsection>
<citsent citstr=" P10-1122 ">
recent studies have also investigated how complex sentence-level entailment relations can be broken down into smaller consecutive steps involving fragment-level entailment (sammons et al, 2010; <papid> P10-1122 </papid>bentivogli et al, 2010).</citsent>
<aftsection>
<nextsent>forex ample: (3) text: the mitogenic effects of the beta chain of fibrinogen are mediated through cell surface calreticulin.
</nextsent>
<nextsent>hypothesis: fibrinogen beta chain interacts with crp55.
</nextsent>
<nextsent>to recognise that the hypothesis is entailed by the text, it can be decomposed into five separate steps involving text fragments: 1.
</nextsent>
<nextsent>b beta chain of fibrinogen?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1715">
<title id=" W11-0202.xml">unsupervised entailment detection between dependency graph fragments </title>
<section> modelling entailment between graph.  </section>
<citcontext>
<prevsection>
<prevsent>is word connected to it through dependency relation,r is the label of that relation and shows the direction of the relation.
</prevsent>
<prevsent>the tuple : (d, r, a?)
</prevsent>
</prevsection>
<citsent citstr=" W05-1202 ">
is referred to as feature of a. to calculate the distributional similarity between two fragments, we adopt an approach similar to weeds et al (2005).<papid> W05-1202 </papid></citsent>
<aftsection>
<nextsent>using the previous notation, (d, r, a?)
</nextsent>
<nextsent>is feature of fragment if (d, r, a?)
</nextsent>
<nextsent>is afeature for word which is contained ina.
</nextsent>
<nextsent>the general algorithm for feature collection is as follows: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1716">
<title id=" W11-0202.xml">unsupervised entailment detection between dependency graph fragments </title>
<section> modelling entailment between graph.  </section>
<citcontext>
<prevsection>
<prevsent>the biomed central2 corpus of full papers was used to collect distributional similarity features for each fragment.
</prevsent>
<prevsent>1000 papers were randomly selected and separated for constructing the test set, leaving 70821 biomedical full papers.
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
these were token ised and parsed using the rasp system (briscoe et al, 2006) <papid> P06-4020 </papid>in order to extract dependency relations.we experimented with various schemes for feature weighting and found the best one to be variation of dices coefficient (dice, 1945), described by curran (2003): wa(f) = 2p (a, f) (a, ?) + (?, f) where wa(f) is the weight of feature for fragmenta, (?, f) is the probability of the feature appearing in the corpus with any fragment, (a, ?) is the probability of the fragment appearing with any feature, and (a, f) is the probability of the fragment and the feature appearing together.</citsent>
<aftsection>
<nextsent>different measures of distributional similarity, both symmetrical and direc tonal, were also tested and clark ede (clarke, 2009) <papid> W09-0215 </papid>was used for the final system as it achieved the highest performance on graph fragments: clarkede(a? b) = ? ffafb min(wa(f), wb(f)) ? ffa wa(f) where fa is the set of features for fragmenta and wa(f) is the weight of feature for fragment a. it quantifies the weighted coverage of the features ofa by the features of by finding the sum of minimum weights.</nextsent>
<nextsent>2http://www.biomedcentral.com/info/about/datamining/ 13 the clark ede similarity measure is designed to detect whether the features of are proper subset of the features of b. this works well for finding more general versions of fragments, but not when comparing fragments which are roughly equal paraphrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1717">
<title id=" W11-0202.xml">unsupervised entailment detection between dependency graph fragments </title>
<section> modelling entailment between graph.  </section>
<citcontext>
<prevsection>
<prevsent>1000 papers were randomly selected and separated for constructing the test set, leaving 70821 biomedical full papers.
</prevsent>
<prevsent>these were token ised and parsed using the rasp system (briscoe et al, 2006) <papid> P06-4020 </papid>in order to extract dependency relations.we experimented with various schemes for feature weighting and found the best one to be variation of dices coefficient (dice, 1945), described by curran (2003): wa(f) = 2p (a, f) (a, ?) + (?, f) where wa(f) is the weight of feature for fragmenta, (?, f) is the probability of the feature appearing in the corpus with any fragment, (a, ?) is the probability of the fragment appearing with any feature, and (a, f) is the probability of the fragment and the feature appearing together.</prevsent>
</prevsection>
<citsent citstr=" W09-0215 ">
different measures of distributional similarity, both symmetrical and direc tonal, were also tested and clark ede (clarke, 2009) <papid> W09-0215 </papid>was used for the final system as it achieved the highest performance on graph fragments: clarkede(a? b) = ? ffafb min(wa(f), wb(f)) ? ffa wa(f) where fa is the set of features for fragmenta and wa(f) is the weight of feature for fragment a. it quantifies the weighted coverage of the features ofa by the features of by finding the sum of minimum weights.</citsent>
<aftsection>
<nextsent>2http://www.biomedcentral.com/info/about/datamining/ 13 the clark ede similarity measure is designed to detect whether the features of are proper subset of the features of b. this works well for finding more general versions of fragments, but not when comparing fragments which are roughly equal paraphrases.
</nextsent>
<nextsent>as solution we constructed new measure based on the symmetrical lin measure (lin, 1998).<papid> P98-2127 </papid></nextsent>
<nextsent>lind(a? b) = ? ffafb [wa(f) + wb(f)] ? ffa wa(f) + ? ffafb wb(f) compared to the original, the features ofb which are not found in are excluded from the score calculation, making the score non-symmetrical butmore balanced compared to clarkede.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1718">
<title id=" W11-0202.xml">unsupervised entailment detection between dependency graph fragments </title>
<section> modelling entailment between graph.  </section>
<citcontext>
<prevsection>
<prevsent>different measures of distributional similarity, both symmetrical and direc tonal, were also tested and clark ede (clarke, 2009) <papid> W09-0215 </papid>was used for the final system as it achieved the highest performance on graph fragments: clarkede(a? b) = ? ffafb min(wa(f), wb(f)) ? ffa wa(f) where fa is the set of features for fragmenta and wa(f) is the weight of feature for fragment a. it quantifies the weighted coverage of the features ofa by the features of by finding the sum of minimum weights.</prevsent>
<prevsent>2http://www.biomedcentral.com/info/about/datamining/ 13 the clark ede similarity measure is designed to detect whether the features of are proper subset of the features of b. this works well for finding more general versions of fragments, but not when comparing fragments which are roughly equal paraphrases.</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
as solution we constructed new measure based on the symmetrical lin measure (lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>lind(a? b) = ? ffafb [wa(f) + wb(f)] ? ffa wa(f) + ? ffafb wb(f) compared to the original, the features ofb which are not found in are excluded from the score calculation, making the score non-symmetrical butmore balanced compared to clarkede.
</nextsent>
<nextsent>we applied this for word-level distributional similarity and achieved better results than with other common similarity measures.
</nextsent>
<nextsent>the lind similarity is also calculated between fragment levels to help detect possible paraphrases.
</nextsent>
<nextsent>if the similarity is very high (greater than 85%), then symmetric relationship between the fragments is assumed and the value of lind is used as extsim.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1719">
<title id=" W11-0202.xml">unsupervised entailment detection between dependency graph fragments </title>
<section> modelling entailment between graph.  </section>
<citcontext>
<prevsection>
<prevsent>is affected by).
</prevsent>
<prevsent>such cases are handled by inverting the direction of the score calculation if fragment is found to contain special cue word that commonly indicates hedged language or negation.
</prevsent>
</prevsection>
<citsent citstr=" W10-3001 ">
in order to find thelist of indicative hedge cues, we analysed the training corpus of conll 2010 shared task (farkas et al., 2010) <papid> W10-3001 </papid>which is annotated for speculation cues and scopes.</citsent>
<aftsection>
<nextsent>any cues that occurred less than 5 times or occurred more often as normal text than as cue words were filtered out, resulting in the following list: (4) suggest, may, might, indicate that, appear, likely, could, possible, whether, would, think, seem, probably, assume, putative, unclear, propose, imply, possibly for negation cues we used the list collected by morante (2009): (5) absence, absent, cannot, could not, either, except, exclude, fail, failure, favor over, impossible, instead of, lack, loss, miss, negative, neither, nor, never, no, no longer, none, not, rather than, rule out, unable, with the exception of, without this is fast and basic method for estimating the presence of hedging and negation in fragment.
</nextsent>
<nextsent>when dealing with longer texts, the exact scope of the cue word should be detected, but for relatively short fragments the presence of keyword acts as good indication of hedging and negation.
</nextsent>
<nextsent>a pilot?
</nextsent>
<nextsent>dataset was created to evaluate different entailment detection methods between fragments3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1720">
<title id=" W11-0202.xml">unsupervised entailment detection between dependency graph fragments </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, the optimal values of ? are close to 0.5, indicating that all four components (intrinsic and distributional similarities between words and fragments) are all contributing to the performance of the final system.
</prevsent>
<prevsent>most work on entailment has focused on comparing sentences or paragraphs.
</prevsent>
</prevsection>
<citsent citstr=" H05-1049 ">
for example, haghighi et al (2005) <papid> H05-1049 </papid>represent sentences as directed dependency graphs and use graph matching to measure semantic overlap.</citsent>
<aftsection>
<nextsent>this method also compares the dependencies when calculating similarity, which supports incorporation of extra syntactic information.
</nextsent>
<nextsent>hickl et al (2006) combine lexico-syntactic features and automatically acquired paraphrases to classify entailing sentences.
</nextsent>
<nextsent>lintean andrus (2009) make use of weighted dependencies and word semantics to detect paraphrases.
</nextsent>
<nextsent>in addition to similarity they look at dissimilarity between two sentences and use their ratio as the confidence score for paraphrasing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1721">
<title id=" W11-0202.xml">unsupervised entailment detection between dependency graph fragments </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to similarity they look at dissimilarity between two sentences and use their ratio as the confidence score for paraphrasing.
</prevsent>
<prevsent>lin and pantel (2001) were one of the first to extend the distributional hypothesis to dependency paths to detect entailment between relations.
</prevsent>
</prevsection>
<citsent citstr=" W04-3206 ">
szpektor et al (2004) <papid> W04-3206 </papid>describe the tease method forex tracting entailing relation templates from the web.</citsent>
<aftsection>
<nextsent>szpektor and dagan (2008) <papid> C08-1107 </papid>use the distributional similarity of arguments to detect unary template entailment, whilst berant et al (2010) <papid> P10-1124 </papid>apply it to binary relations in focused entailment graphs.</nextsent>
<nextsent>snow et al (2005) described basic method of syntactic pattern matching to automatically discover word-level hypernym relations from text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1722">
<title id=" W11-0202.xml">unsupervised entailment detection between dependency graph fragments </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>lin and pantel (2001) were one of the first to extend the distributional hypothesis to dependency paths to detect entailment between relations.
</prevsent>
<prevsent>szpektor et al (2004) <papid> W04-3206 </papid>describe the tease method forex tracting entailing relation templates from the web.</prevsent>
</prevsection>
<citsent citstr=" C08-1107 ">
szpektor and dagan (2008) <papid> C08-1107 </papid>use the distributional similarity of arguments to detect unary template entailment, whilst berant et al (2010) <papid> P10-1124 </papid>apply it to binary relations in focused entailment graphs.</citsent>
<aftsection>
<nextsent>snow et al (2005) described basic method of syntactic pattern matching to automatically discover word-level hypernym relations from text.
</nextsent>
<nextsent>the use of directional distributional similarity measures to find inference relations between single words is explored by kotlerman et al (2010).
</nextsent>
<nextsent>they propose new measures based on feature ranks and compare them with existing ones for the tasks of lexical expansion and text categorisation.in contrast to current work, each of the approaches described above only focuses on detecting entailment between specific sub types of fragments(either sentences, relations or words) and optimising the system for single scenario.
</nextsent>
<nextsent>this means only limited types of entailment relations are found and they cannot be used for entailment generation or compositional entailment detection as described in section 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1723">
<title id=" W11-0202.xml">unsupervised entailment detection between dependency graph fragments </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>lin and pantel (2001) were one of the first to extend the distributional hypothesis to dependency paths to detect entailment between relations.
</prevsent>
<prevsent>szpektor et al (2004) <papid> W04-3206 </papid>describe the tease method forex tracting entailing relation templates from the web.</prevsent>
</prevsection>
<citsent citstr=" P10-1124 ">
szpektor and dagan (2008) <papid> C08-1107 </papid>use the distributional similarity of arguments to detect unary template entailment, whilst berant et al (2010) <papid> P10-1124 </papid>apply it to binary relations in focused entailment graphs.</citsent>
<aftsection>
<nextsent>snow et al (2005) described basic method of syntactic pattern matching to automatically discover word-level hypernym relations from text.
</nextsent>
<nextsent>the use of directional distributional similarity measures to find inference relations between single words is explored by kotlerman et al (2010).
</nextsent>
<nextsent>they propose new measures based on feature ranks and compare them with existing ones for the tasks of lexical expansion and text categorisation.in contrast to current work, each of the approaches described above only focuses on detecting entailment between specific sub types of fragments(either sentences, relations or words) and optimising the system for single scenario.
</nextsent>
<nextsent>this means only limited types of entailment relations are found and they cannot be used for entailment generation or compositional entailment detection as described in section 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1724">
<title id=" W11-0202.xml">unsupervised entailment detection between dependency graph fragments </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they propose new measures based on feature ranks and compare them with existing ones for the tasks of lexical expansion and text categorisation.in contrast to current work, each of the approaches described above only focuses on detecting entailment between specific sub types of fragments(either sentences, relations or words) and optimising the system for single scenario.
</prevsent>
<prevsent>this means only limited types of entailment relations are found and they cannot be used for entailment generation or compositional entailment detection as described in section 2.
</prevsent>
</prevsection>
<citsent citstr=" C08-1066 ">
maccartney and manning (2008) <papid> C08-1066 </papid>approach sentence-level entailment detection by breaking the problem into sequence of atomic edits linking the premise to the hypothesis.</citsent>
<aftsection>
<nextsent>entailment relations are then predicted for each edit, propagated up through 16a syntax tree and then used to compose the resulting entailment decision.
</nextsent>
<nextsent>however, their system focuses more on natural logic and uses predefined set of compositional rules to capture subset of valid inferences with high precision but low recall.
</nextsent>
<nextsent>it also relies on supervised classifier and information from wordnet to reach the final entailment decision.
</nextsent>
<nextsent>androutsopoulos and malakasiotis (2010) have assembled survey of different tasks and approaches related to paraphrasing and entailment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1725">
<title id=" W11-0822.xml">fast and flexible mwe candidate generation with the mwe toolkit </title>
<section> project description.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" C10-3015 ">
the mwe toolkit was presented and demonstrated in ramisch et al (2010<papid> C10-3015 </papid>b) and in ramisch et al (2010<papid> C10-3015 </papid>a), and applied to several languages (linardaki et al, 2010) and domains (ramisch et al, 2010<papid> C10-3015 </papid>c).it is downloadable open-source1 set of command line tools mostly written in python.</citsent>
<aftsection>
<nextsent>our target users are researchers with background in computational linguistics.
</nextsent>
<nextsent>the system performs language- and type-independent candidate extraction in two steps2: 1.
</nextsent>
<nextsent>candidate generation.
</nextsent>
<nextsent>pattern matching3 ? n-gram counting
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1735">
<title id=" W11-0115.xml">computing semantic compositionality in distributional semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this work uses combination of distributional semantics and machine learning techniques.
</prevsent>
<prevsent>the starting data in the experiments reported below are multidimensional vector ial semantic representations extracted from electronic corpora.
</prevsent>
</prevsection>
<citsent citstr=" W10-2805 ">
this work extends the basic methodology presented in guevara (2010) <papid> W10-2805 </papid>with new data collection techniques, improved evaluation metrics and new case studies.</citsent>
<aftsection>
<nextsent>compositionality is probably one of the defining properties of human language and, perhaps, nearly uncontroversial notion among linguists.
</nextsent>
<nextsent>one of the best-known formulations of compositionality is: (1) the principle of compositionality: the meaning of complex expression is function of the meaning of its parts and of the syntactic rules by which they are combined.
</nextsent>
<nextsent>(partee, ter meulen and wall, 1990: 318) the principle of compositionality is standard notion in many different fields of research, notably in logic, in philosophy of language, in linguistics and in computer science; this intrinsic multi-disciplinarity makes tracing back its recent history somewhat difficult.
</nextsent>
<nextsent>the recent years have witnessed an ever-increasing interest in techniques that enable computers to automatically extract semantic information from linguistic corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1739">
<title id=" W11-0115.xml">computing semantic compositionality in distributional semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>distributional semantics, in short, extracts spatial representations of meaning from electronic corpora by using distributional (i.e. statistical) patterns of word usage.
</prevsent>
<prevsent>the main hypothesis in distributional semantics is the so-called distributional hypothesis of meaning, expressing the fact that words that occur in the same contexts tend to have similar meanings?
</prevsent>
</prevsection>
<citsent citstr=" P05-1016 ">
(pantel, 2005).<papid> P05-1016 </papid></citsent>
<aftsection>
<nextsent>the distributional hypothesis of meaning is ascribed to zellig harris, who proposed general distributional methodology for linguistics.
</nextsent>
<nextsent>since representations in distributional semantics are spatial in nature (e.g. vectors representing points in multidimensional space), differences in meaning are captured through differences in location: 135 in the multidimensional space, two semantically (i.e. distributionally) similar words are closer than two words that are dissimilar.
</nextsent>
<nextsent>see sahlgren (2006) and turney and pantel (2010) for detailed overviews of the methodology and applications of distributional semantics.
</nextsent>
<nextsent>2 compositionality in distributional semantics: state-of-the-art.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1740">
<title id=" W11-0115.xml">computing semantic compositionality in distributional semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>5).
</prevsent>
<prevsent>more schematically: (2) vector addition: v1i + v2i = v3i given two independent vectors v1 and v2, the compositional meaning of v3 consists of the sum of the corresponding components of the original vectors.
</prevsent>
</prevsection>
<citsent citstr=" P08-1028 ">
mitchell and lapata (2008) <papid> P08-1028 </papid>introduce whole family of models of compositionality based on vector addition and pointwise-multiplication (and weighted combination of both), evaluated on sentence similarity task inspired by kintsch (2001).</citsent>
<aftsection>
<nextsent>while the additive model captures the compositionality of meaning by considering all available components, multiplicative models only operate on subset of them, i.e. non-zero components.
</nextsent>
<nextsent>they claim that when we pointwise-multiply the vectors representing two words, we obtain an output that captures their composition; actually, this operation is keeping in the output only the components which had corresponding non-zero values: whether this operation has any relation with semantics is still unclear.
</nextsent>
<nextsent>however, in their experiments, mitchell and lapata prove that the pointwise-multiplicative model and the weighted combination of the additive and the multiplicative models perform equally well.
</nextsent>
<nextsent>of these, only the simple multiplicative model will be tested in the experiments present in the following section.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1758">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>instead of simply using the best action output by the classifier, we determine the best action by looking into possible sequences of future actions and evaluating the final states realized by those action sequences.we present perceptron-based parameter optimization method for this learning framework and show its convergence properties.the proposed framework is evaluated on part of-speech tagging, chunking, named entity recognition and dependency parsing, using standard datasets and features.
</prevsent>
<prevsent>experimental results demonstrate that history-based models with look ahead are as competitive as globally optimized models including conditional random fields (crfs) and structured perceptrons.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
history-based models have been popular approach in variety of natural language processing (nlp) tasks including part-of-speech (pos) tagging, named entity recognition, and syntactic parsing (ratnaparkhi, 1996; <papid> W96-0213 </papid>mccallum et al, 2000; yamada and matsumoto, 2003; nivre et al, 2004).<papid> W04-2407 </papid></citsent>
<aftsection>
<nextsent>the idea is to decompose the complex structured prediction problem into series of simple classification problems and use machine learning-basedclassifier to make each decision using the information about the past decisions and partially completed structures as features.although history-based models have many practical merits, their accuracy is often surpassed by globally optimized models such as crfs (lafferty et al, 2001) and structured perceptrons (collins,2002), <papid> W02-1001 </papid>mainly due to the label bias problem.</nextsent>
<nextsent>to day, vanilla history-based models such as maximum entropy markov models (memms) are probably notthe first choice for those who are looking for machine learning model that can deliver the state-ofthe-art accuracy for their nlp task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1759">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>instead of simply using the best action output by the classifier, we determine the best action by looking into possible sequences of future actions and evaluating the final states realized by those action sequences.we present perceptron-based parameter optimization method for this learning framework and show its convergence properties.the proposed framework is evaluated on part of-speech tagging, chunking, named entity recognition and dependency parsing, using standard datasets and features.
</prevsent>
<prevsent>experimental results demonstrate that history-based models with look ahead are as competitive as globally optimized models including conditional random fields (crfs) and structured perceptrons.
</prevsent>
</prevsection>
<citsent citstr=" W04-2407 ">
history-based models have been popular approach in variety of natural language processing (nlp) tasks including part-of-speech (pos) tagging, named entity recognition, and syntactic parsing (ratnaparkhi, 1996; <papid> W96-0213 </papid>mccallum et al, 2000; yamada and matsumoto, 2003; nivre et al, 2004).<papid> W04-2407 </papid></citsent>
<aftsection>
<nextsent>the idea is to decompose the complex structured prediction problem into series of simple classification problems and use machine learning-basedclassifier to make each decision using the information about the past decisions and partially completed structures as features.although history-based models have many practical merits, their accuracy is often surpassed by globally optimized models such as crfs (lafferty et al, 2001) and structured perceptrons (collins,2002), <papid> W02-1001 </papid>mainly due to the label bias problem.</nextsent>
<nextsent>to day, vanilla history-based models such as maximum entropy markov models (memms) are probably notthe first choice for those who are looking for machine learning model that can deliver the state-ofthe-art accuracy for their nlp task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1760">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results demonstrate that history-based models with look ahead are as competitive as globally optimized models including conditional random fields (crfs) and structured perceptrons.
</prevsent>
<prevsent>history-based models have been popular approach in variety of natural language processing (nlp) tasks including part-of-speech (pos) tagging, named entity recognition, and syntactic parsing (ratnaparkhi, 1996; <papid> W96-0213 </papid>mccallum et al, 2000; yamada and matsumoto, 2003; nivre et al, 2004).<papid> W04-2407 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
the idea is to decompose the complex structured prediction problem into series of simple classification problems and use machine learning-basedclassifier to make each decision using the information about the past decisions and partially completed structures as features.although history-based models have many practical merits, their accuracy is often surpassed by globally optimized models such as crfs (lafferty et al, 2001) and structured perceptrons (collins,2002), <papid> W02-1001 </papid>mainly due to the label bias problem.</citsent>
<aftsection>
<nextsent>to day, vanilla history-based models such as maximum entropy markov models (memms) are probably notthe first choice for those who are looking for machine learning model that can deliver the state-ofthe-art accuracy for their nlp task.
</nextsent>
<nextsent>globally optimized models, by contrast, are gaining popularity inthe community despite their relatively high computational cost.in this paper, we argue that history-based models are not something that should be left behind in research history, by demonstrating that their accuracy can be significantly improved by incorporating look ahead mechanism into their decisionmaking process.
</nextsent>
<nextsent>it should be emphasized that weuse the word lookahead?
</nextsent>
<nextsent>differently from some literature on syntactic parsing in which look ahead simply means looking at the succeeding words to choose the right parsing actions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1762">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>first, we evaluate our framework with three sequence prediction tasks: pos tagging, chunking, and named entity recognition.
</prevsent>
<prevsent>we compare our method with the crf model, which is one of the defacto standard machine learning models for such sequence prediction tasks.
</prevsent>
</prevsection>
<citsent citstr=" P09-1054 ">
we trained l1-regularized first-order crf models using the efficient stochastic gradient descent (sgd)-based training method presented in tsuruoka et al (2009).<papid> P09-1054 </papid></citsent>
<aftsection>
<nextsent>since our main in 241 terest is not in achieving the state-of-the-art results for those tasks, we did not conduct feature engineering to come up with elaborate feature swe simply adopted the feature sets described in their paper (with an exception being tag trigram features tested in the pos tagging experiments).
</nextsent>
<nextsent>the experiments for these sequence prediction tasks were carried out using one core of 3.33ghz intel xeon w5590 processor.
</nextsent>
<nextsent>the first set of experiments is about pos tagging.
</nextsent>
<nextsent>the training and test data were created from thewall street journal corpus of the penn treebank (marcus et al, 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1763">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>it is clearly seen that the look ahead improves tagging accuracy, and our history based models with look ahead is as accurate as the crf model.
</prevsent>
<prevsent>we also created another set of models by simply adding tag trigram features, which can not be employed by first-order crf models.
</prevsent>
</prevsection>
<citsent citstr=" P07-1096 ">
these features have slightly improved the tagging accuracy, and the final accuracy achieved by search depth of 3 was comparable to some of the best results achieved by pure supervised learning in this task (shen et al, 2007; <papid> P07-1096 </papid>lavergne et al, 2010).<papid> P10-1052 </papid></citsent>
<aftsection>
<nextsent>the second set of experiments is about chunking.
</nextsent>
<nextsent>we used the dataset for the conll 2000 shared task, which contains 8,936 sentences where each token is annotated with the iob?
</nextsent>
<nextsent>tags representing text chunks.
</nextsent>
<nextsent>the experimental results are shown in table 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1764">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>it is clearly seen that the look ahead improves tagging accuracy, and our history based models with look ahead is as accurate as the crf model.
</prevsent>
<prevsent>we also created another set of models by simply adding tag trigram features, which can not be employed by first-order crf models.
</prevsent>
</prevsection>
<citsent citstr=" P10-1052 ">
these features have slightly improved the tagging accuracy, and the final accuracy achieved by search depth of 3 was comparable to some of the best results achieved by pure supervised learning in this task (shen et al, 2007; <papid> P07-1096 </papid>lavergne et al, 2010).<papid> P10-1052 </papid></citsent>
<aftsection>
<nextsent>the second set of experiments is about chunking.
</nextsent>
<nextsent>we used the dataset for the conll 2000 shared task, which contains 8,936 sentences where each token is annotated with the iob?
</nextsent>
<nextsent>tags representing text chunks.
</nextsent>
<nextsent>the experimental results are shown in table 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1765">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the experimental results are shown in table 2.
</prevsent>
<prevsent>again, our history-based models with look ahead were slightly more accurate than the crf model using exactly the same set of features.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
the accuracy achieved by the look ahead model with search depth of 2 was comparable to the accuracy achieved by computationally heavy combination of max-margin classifiers (kudo and matsumoto, 2001).<papid> N01-1025 </papid></citsent>
<aftsection>
<nextsent>we also tested the effectiveness of additional features of tag trigrams using the development data, but there was no improvement in the accuracy.the third set of experiments is about named entity recognition.
</nextsent>
<nextsent>we used the data provided for the bionlp/nlpba 2004 shared task (kim et al, 2004), which contains 18,546 sentences where each token is annotated with the iob?
</nextsent>
<nextsent>tags representing biomedical named entities.
</nextsent>
<nextsent>we performed the tagging in the right-to-left fashion because it is known that backward tagging is more accurate than forward tagging on this dataset (yoshida and tsujii, 2007).<papid> W07-1033 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1766">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we used the data provided for the bionlp/nlpba 2004 shared task (kim et al, 2004), which contains 18,546 sentences where each token is annotated with the iob?
</prevsent>
<prevsent>tags representing biomedical named entities.
</prevsent>
</prevsection>
<citsent citstr=" W07-1033 ">
we performed the tagging in the right-to-left fashion because it is known that backward tagging is more accurate than forward tagging on this dataset (yoshida and tsujii, 2007).<papid> W07-1033 </papid></citsent>
<aftsection>
<nextsent>table 3 shows the experimental results, together with some previous performance reports achieved by pure machine leaning methods (i.e. without rule based postprocessing or external resources such asgazetteers).
</nextsent>
<nextsent>our history-based model with no looka head was considerably worse than the crf model using the same set of features, but it was significantly improved by the introduction of look ahead and resulted inaccuracy figures better than that of the crf model.
</nextsent>
<nextsent>4.2 dependency parsing.
</nextsent>
<nextsent>we also evaluate our method in dependency parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1767">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>unlabeled attachment scores for all words excluding punctuations are reported.the development set is used for tuning the meta parameters, while the test set is used for evaluating the final accuracy.
</prevsent>
<prevsent>the parsing algorithm is the arc-standard?
</prevsent>
</prevsection>
<citsent citstr=" W04-0308 ">
method (nivre, 2004), <papid> W04-0308 </papid>which is briefly described in section 2.</citsent>
<aftsection>
<nextsent>with this algorithm, state corresponds to parser configuration, i.e., the stack and the queue, and action corresponds to shift, reducel, and reducer.
</nextsent>
<nextsent>in this experiment, we use the same set of feature templates as huang and sagae (2010).<papid> P10-1110 </papid></nextsent>
<nextsent>table 4 shows training time, test time, and parsing accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1768">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>method (nivre, 2004), <papid> W04-0308 </papid>which is briefly described in section 2.</prevsent>
<prevsent>with this algorithm, state corresponds to parser configuration, i.e., the stack and the queue, and action corresponds to shift, reducel, and reducer.</prevsent>
</prevsection>
<citsent citstr=" P10-1110 ">
in this experiment, we use the same set of feature templates as huang and sagae (2010).<papid> P10-1110 </papid></citsent>
<aftsection>
<nextsent>table 4 shows training time, test time, and parsing accuracy.
</nextsent>
<nextsent>in this table, no look ahead (depth = 0)?
</nextsent>
<nextsent>corresponds to conventional shift-reduce parsing method without any look ahead search.
</nextsent>
<nextsent>the results3penn2malt is applied for this conversion, while dependency labels are removed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1773">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>as expected, training and test speed decreases, almost by factor of three, which is the branching factor of the dependency parser.
</prevsent>
<prevsent>the table also lists accuracy figures reported in the literature on shift-reduce dependency parsing.most of the latest studies on shift-reduce dependency parsing employ dynamic programing or beam search, which implies that deterministic methods were not as competitive as those methods.
</prevsent>
</prevsection>
<citsent citstr=" P04-1015 ">
it should also be noted that all of the listed studies learn structured perceptrons (collins and roark, 2004), <papid> P04-1015 </papid>while our parser learns locally optimized perceptrons.</citsent>
<aftsection>
<nextsent>in this table, our parser without look ahead search (i.e. depth = 0) resulted insignificantly lower accuracy than the previous studies.
</nextsent>
<nextsent>in fact, it is worse than the deterministic parser of huang et al (2009), <papid> D09-1127 </papid>which uses (almost) the same set of features.</nextsent>
<nextsent>this is presumably due to the difference between locally optimized perceptrons and globally optimized structured perceptrons.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1774">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>it should also be noted that all of the listed studies learn structured perceptrons (collins and roark, 2004), <papid> P04-1015 </papid>while our parser learns locally optimized perceptrons.</prevsent>
<prevsent>in this table, our parser without look ahead search (i.e. depth = 0) resulted insignificantly lower accuracy than the previous studies.</prevsent>
</prevsection>
<citsent citstr=" D09-1127 ">
in fact, it is worse than the deterministic parser of huang et al (2009), <papid> D09-1127 </papid>which uses (almost) the same set of features.</citsent>
<aftsection>
<nextsent>this is presumably due to the difference between locally optimized perceptrons and globally optimized structured perceptrons.
</nextsent>
<nextsent>however, our parser with lookaheadsearch is significantly better than their deterministic parser, and its accuracy is close to the levels of the parsers with beam search.
</nextsent>
<nextsent>the reason why we introduced look ahead mechanism into history-based models is that we wanted the model to be able to avoid making such mistakes that can be detected only in later stages.
</nextsent>
<nextsent>probabilistic history-based models such as memms should beable to avoid (at least some of) such mistakes by performing viterbi search to find the highest probability path of the actions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1775">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>iii and marcu (2005).
</prevsent>
<prevsent>this framework, called laso (learning as search optimization), can include many variations of search strategies such asbeam search and a* search as special case.
</prevsent>
</prevsection>
<citsent citstr=" P06-1059 ">
indeed, our look ahead framework could be regard edas special case in which each search node con 243 training time (sec) test time (sec) f-measure crf (l1 regularization &amp; sgd training) 235 4 71.63 no look ahead (depth = 0) 66 4 70.17 look ahead (depth = 1) 91 4 72.28 look ahead (depth = 2) 302 7 72.00 look ahead (depth = 3) 2,419 33 72.21 semi-markov crf (okanohara et al, 2006) <papid> P06-1059 </papid>n/a n/a 71.48 reranking (yoshida and tsujii, 2007) <papid> W07-1033 </papid>n/a n/a 72.65 table 3: performance of biomedical named entity recognition (training times and accuracy scores on test data).</citsent>
<aftsection>
<nextsent>training time (sec) test time (sec) accuracy no look ahead (depth = 0) 1,937 4 89.73 look ahead (depth = 1) 4,907 13 91.00 look ahead (depth = 2) 12,800 31 91.10 look ahead (depth = 3) 31,684 79 91.24 beam search (k = 64) (zhang and clark, 2008) <papid> D08-1059 </papid>n/a n/a 91.4 deterministic (huang et al, 2009) <papid> D09-1127 </papid>n/a n/a 90.2 beam search (k = 16) (huang et al, 2009) <papid> D09-1127 </papid>n/a n/a 91.3 dynamic programming (huang and sagae, 2010) <papid> P10-1110 </papid>n/a n/a 92.1 table 4: performance of english dependency parsing (training times and accuracy scores on test data).</nextsent>
<nextsent>sists of the next and look ahead actions4, although the weight updating procedure differs in several minor points.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1777">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this framework, called laso (learning as search optimization), can include many variations of search strategies such asbeam search and a* search as special case.
</prevsent>
<prevsent>indeed, our look ahead framework could be regard edas special case in which each search node con 243 training time (sec) test time (sec) f-measure crf (l1 regularization &amp; sgd training) 235 4 71.63 no look ahead (depth = 0) 66 4 70.17 look ahead (depth = 1) 91 4 72.28 look ahead (depth = 2) 302 7 72.00 look ahead (depth = 3) 2,419 33 72.21 semi-markov crf (okanohara et al, 2006) <papid> P06-1059 </papid>n/a n/a 71.48 reranking (yoshida and tsujii, 2007) <papid> W07-1033 </papid>n/a n/a 72.65 table 3: performance of biomedical named entity recognition (training times and accuracy scores on test data).</prevsent>
</prevsection>
<citsent citstr=" D08-1059 ">
training time (sec) test time (sec) accuracy no look ahead (depth = 0) 1,937 4 89.73 look ahead (depth = 1) 4,907 13 91.00 look ahead (depth = 2) 12,800 31 91.10 look ahead (depth = 3) 31,684 79 91.24 beam search (k = 64) (zhang and clark, 2008) <papid> D08-1059 </papid>n/a n/a 91.4 deterministic (huang et al, 2009) <papid> D09-1127 </papid>n/a n/a 90.2 beam search (k = 16) (huang et al, 2009) <papid> D09-1127 </papid>n/a n/a 91.3 dynamic programming (huang and sagae, 2010) <papid> P10-1110 </papid>n/a n/a 92.1 table 4: performance of english dependency parsing (training times and accuracy scores on test data).</citsent>
<aftsection>
<nextsent>sists of the next and look ahead actions4, although the weight updating procedure differs in several minor points.
</nextsent>
<nextsent>daume?
</nextsent>
<nextsent>iii and marcu (2005) did not try look ahead search strategy, and to the best of our knowledge, this paper is the first that demonstrates that look ahead actually works well for various nlp tasks.performing look ahead is very common technique for variety of decision-making problems in the field of artificial intelligence.
</nextsent>
<nextsent>in computer chess, for example, programs usually need to perform very deep search in the game tree to find agood move.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1783">
<title id=" W11-0328.xml">learning with look ahead can history based models rival globally optimized models </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>(tesauro, 2001; hoki, 2006) in that the parameters are optimized based on the differences of the feature vectors realized by the correct and incorrect actions.in history-based models, the order of actions is of ten very important.
</prevsent>
<prevsent>for example, backward tagging is considerably more accurate than forward tagging in biomedical named entity recognition.
</prevsent>
</prevsection>
<citsent citstr=" H05-1059 ">
our look ahead method is orthogonal to more elaborate techniques for determining the order of actions such as easy-first tagging/parsing strategies (tsuruoka andtsujii, 2005; <papid> H05-1059 </papid>elhadad, 2010).</citsent>
<aftsection>
<nextsent>we expect that incorporating such elaborate techniques in our framework will lead to improved accuracy, but we leave it for future work.
</nextsent>
<nextsent>we have presented simple and general framework for incorporating look ahead process in history based models and perceptron-based training algorithm for the framework.
</nextsent>
<nextsent>we have conducted experiments using standard datasets for pos tagging, chunking, named entity recognition and dependency parsing, and obtained very promising resultstheaccuracy achieved by the history-based models en 244hanced with look ahead was as competitive as globally optimized models including crfs.in most of the experimental results, steady improvement inaccuracy has been observed as the depth of the search is increased.
</nextsent>
<nextsent>although it is not very practical to perform deeper searches with our current implementation we naively explored all possible sequences of actions, future work should encompass extending the depths of search space by introducing elaborate pruning/search extension techniques.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1785">
<title id=" W11-0121.xml">elaborating a knowledge base for deep lexical semantics </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>are treated as expressing predications about possible eventualities.)
</prevsent>
<prevsent>we have developed software4 to translate penn treebank-style trees (as well as other syntactic formalisms) into this notation.
</prevsent>
</prevsection>
<citsent citstr=" P85-1008 ">
the underlying core theories are expressed as axioms in this notation (hobbs, 1985).<papid> P85-1008 </papid></citsent>
<aftsection>
<nextsent>the interpretation of text is taken to be the lowest-cost abductive proof of the logical form of the text, given the knowledge base.
</nextsent>
<nextsent>that is, to interpret text we prove the logical form, allowing for assumptions at cost, and pick the lowest-cost proof.
</nextsent>
<nextsent>factors involved in computing costs include, besides the number of assumptions, the salience of axioms, the plausibility of axioms expressing defeasible knowledge, and consiliance or the degree to which the pervasive implicit redundancy of natural language texts is exploited.
</nextsent>
<nextsent>we have demonstrated that many interpretation problems are solved as by-product of finding the lowest-cost proof.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1786">
<title id=" W11-0121.xml">elaborating a knowledge base for deep lexical semantics </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>we have demonstrated that many interpretation problems are solved as by-product of finding the lowest-cost proof.
</prevsent>
<prevsent>this method has been implemented in an abductive theorem-prover called mini-tacitus5 that has been used in number of applications (hobbs et al, 1993; mulkar et al, 2007), and is used in the textual entailment problems described here.
</prevsent>
</prevsection>
<citsent citstr=" W11-0107 ">
we are also working toward probabilistic semantics for the cost of proofs (blythe et al, 2011).<papid> W11-0107 </papid></citsent>
<aftsection>
<nextsent>abductive interpretation accounts for script-like understanding of texta script predicate provides the most economical interpretation (hobbs et al, 1993)but also enables interpretation of novel texts.
</nextsent>
<nextsent>most commonsense knowledge is defeasible, i.e., it can be defeated.
</nextsent>
<nextsent>this is represented in our framework by having unique et cetera?
</nextsent>
<nextsent>proposition in the antecedent of horn clauses that cannot be proved but can be assumed at cost corresponding to the likeliehood that the conclusion is true.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1787">
<title id=" W11-0909.xml">a joint model of implicit arguments for nominal predicates </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>because most of this work has been done for verbal srl, nominal srl has lagged behind somewhat.
</prevsent>
<prevsent>in particular, the implicit?
</prevsent>
</prevsection>
<citsent citstr=" P10-1160 ">
nominal srl model created by gerber and chai (2010) <papid> P10-1160 </papid>does not address joint argument structures.</citsent>
<aftsection>
<nextsent>implicit arguments are similar to standard srl arguments, primary difference being their ability to cross sentence boundaries.in the model created by gerber and chai, implicit argument candidates are classified independently anda heuristic post-processing method is applied to derive the final structure.
</nextsent>
<nextsent>this paper presents preliminary joint implicit argument model.
</nextsent>
<nextsent>consider the following sentences:1 1we will use the notation of gerber and chai (2010), <papid> P10-1160 </papid>where (1) [c1 the president] is currently struggling to manage [c2 the countrys economy].</nextsent>
<nextsent>(2) if he cannot get it under control, [p loss] of [arg1 the next election] might result.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1790">
<title id=" W11-0909.xml">a joint model of implicit arguments for nominal predicates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude in section 8.
</prevsent>
<prevsent>a number of recent studies have shown that semantic arguments are not independent and that system performance can be improved by taking argument dependencies into account.
</prevsent>
</prevsection>
<citsent citstr=" J08-2002 ">
consider the following examples, due to toutanova et al  (2008): (<papid> J08-2002 </papid>3) [temporal the day] that [arg0 the ogre] [predicate cooked] [arg1 the children] is still remembered.</citsent>
<aftsection>
<nextsent>(4) [arg1 the meal] that [arg0 the ogre] [predicate cooked] [beneficiary the children] is still remembered.
</nextsent>
<nextsent>these examples demonstrate the importance of inter-argument dependencies.
</nextsent>
<nextsent>the change from day in example 3 to meal in example 4 affects more than just the temporal label: additionally, the arg1 changes to beneficiary, even though the underlying text (the children) does not change.
</nextsent>
<nextsent>to capture this dependency, toutanova el al. first generate an best list of argument labels for predicate instance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1791">
<title id=" W11-0909.xml">a joint model of implicit arguments for nominal predicates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they then re-rank this list using joint features that describe multiple arguments simultaneously.
</prevsent>
<prevsent>the features help prevent globally invalid argument configurations (e.g., ones with multiple arg0 labels).
</prevsent>
</prevsection>
<citsent citstr=" J08-2005 ">
punyakanok et al  (2008) <papid> J08-2005 </papid>formulate variety of constraints on argument configurations.</citsent>
<aftsection>
<nextsent>for example, arguments are not allowed to overlap the predicate, nor are they allowed to overlap each other.
</nextsent>
<nextsent>the authors treat these constraints as binary variables within an integer linear program, which is optimized to produce the final labeling.
</nextsent>
<nextsent>ritter et al  (2010) <papid> P10-1044 </papid>investigated joint selectional preferences.</nextsent>
<nextsent>traditionally, selectional preference model provides the strength of association between predicate-argument position and specific textual expression.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1792">
<title id=" W11-0909.xml">a joint model of implicit arguments for nominal predicates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, arguments are not allowed to overlap the predicate, nor are they allowed to overlap each other.
</prevsent>
<prevsent>the authors treat these constraints as binary variables within an integer linear program, which is optimized to produce the final labeling.
</prevsent>
</prevsection>
<citsent citstr=" P10-1044 ">
ritter et al  (2010) <papid> P10-1044 </papid>investigated joint selectional preferences.</citsent>
<aftsection>
<nextsent>traditionally, selectional preference model provides the strength of association between predicate-argument position and specific textual expression.
</nextsent>
<nextsent>returning to examples 1 and 2, one sees that the selectional preference for president and economy in the iarg0 position of loss should be high.
</nextsent>
<nextsent>ritter et al  extended this single-argument model using joint formulation of latent dirichlet al location (lda) (blei et al , 2003).
</nextsent>
<nextsent>in the generative version of joint lda, text for the argument positions is generated from common hidden variable.this approach reflects the intuition behind examples 1 and 2 and would help identify president as the iarg0.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1793">
<title id=" W11-0909.xml">a joint model of implicit arguments for nominal predicates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>one key difference between text runner and other information extraction systems is that text runner does not use closed set of relations (compare to the work described by ace (2008)).
</prevsent>
<prevsent>instead, the relation setis left open, leading to the notion of open information extraction (oie).
</prevsent>
</prevsection>
<citsent citstr=" P08-1004 ">
although oie often has lower precision than traditional information extraction, itis able to extract wider variety of relations at precision levels that are often useful (banko and etzioni, 2008).<papid> P08-1004 </papid></citsent>
<aftsection>
<nextsent>argument assignments returning again to examples 1 and 2, one can query text runner in the following way: arg0 : ? predicate : lose2 arg1 : election in the text runner system, arg0 typically indicates the agent and arg1 typically indicates the theme.
</nextsent>
<nextsent>text runner provides many tuples in response to this query, two of which are shown below: (5) usually, [arg0 the presidents party] [predicate loses] [arg1 seats in the mid-term election].
</nextsent>
<nextsent>(6) [arg0 the president] [predicate lost] [arg1 the election].the tuples present in these sentences give strong indicators about the type of entity that loses elections.
</nextsent>
<nextsent>2nominal predicates are mapped to their verbal forms using information provided by the nombank lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1797">
<title id=" W11-0909.xml">a joint model of implicit arguments for nominal predicates </title>
<section> joint model features.  </section>
<citcontext>
<prevsection>
<prevsent>election additionally, path between more specific synsets (i.e., those lower in the hierarchy) indicates astronger relationship than path between more general synsets (i.e., those higher in the hierarchy).
</prevsent>
<prevsent>these two situations are depicted in figure 1.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
the synset similarity metric defined by wu and palmer (1994) <papid> P94-1019 </papid>combines the path length and synset depth intuitions into single numeric score that is defined as follows: 2 ? depth(lca(synset1, synset2)) depth(synset1) + depth(synset2) (12)in equation 12, lca returns the lowest common ancestor of the two synsets within the wordnet is-a hierarchy.</citsent>
<aftsection>
<nextsent>to summarize, equation 12 indicates the strength of association between synsetcj (e.g., primary election) and ranked synset synseta from that answers question such as what might president lose??.
</nextsent>
<nextsent>if the association between synsetcj and synseta is small, then the assignment of cj to iarg1is unlikely.
</nextsent>
<nextsent>the process works similarly for assessing ci as the filler of iarg0.
</nextsent>
<nextsent>in what follows, we quantify this intuition with features used to represent the conditioning information in equation 8.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1803">
<title id=" W11-0112.xml">integrating logical representations with probabilistic information using markov logic </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>representing the meaning of language in first-order logical form is appealing because it provides powerful and flexible way to express even complex propositions.
</prevsent>
<prevsent>however, systems built solely using first-order logical forms tend to be very brittle as they have no way of integrating uncertain knowledge.
</prevsent>
</prevsection>
<citsent citstr=" H05-1079 ">
they, therefore, tend to have high precision at the cost of low recall (bos and markert, 2005).<papid> H05-1079 </papid>recent advances in computational linguistics have yielded robust methods that use weighted or probabilistic models.</citsent>
<aftsection>
<nextsent>for example, distributional models of word meaning have been used successfully to judge paraphrase appropriateness.
</nextsent>
<nextsent>this has been done by representing the word meaning in context as point in high-dimensional semantics space (erk and pado?, 2008; thater et al, 2010; erk and pado?,2010).
</nextsent>
<nextsent>however, these models typically handle only individual phenomena instead of providing meaning representation for complete sentences.
</nextsent>
<nextsent>it is long-standing open question how best to integrate the weighted or probabilistic information coming from such modules with logic-based representations in way that allows for reasoning over both.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1804">
<title id=" W11-0112.xml">integrating logical representations with probabilistic information using markov logic </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our framework parses natural language into logical form, adds rule weights computed by external nlp modules, and performs inferences using an mln.
</prevsent>
<prevsent>our end-to-end approach integrates multiple existing tools.
</prevsent>
</prevsection>
<citsent citstr=" C04-1180 ">
we use boxer (bos et al, 2004) <papid> C04-1180 </papid>to parse natural 105 language into logical form.</citsent>
<aftsection>
<nextsent>we use alchemy (kok et al, 2005) for mln inference.
</nextsent>
<nextsent>finally, we use the exemplar-based distributional model of erk and pado?
</nextsent>
<nextsent>(2010) to produce rule weights.
</nextsent>
<nextsent>logic-based semantics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1808">
<title id=" W11-0112.xml">integrating logical representations with probabilistic information using markov logic </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>logic-based semantics.
</prevsent>
<prevsent>boxer (bos et al, 2004) <papid> C04-1180 </papid>is software package for wide-coverage semantic analysis that provides semantic representations in the form of discourse representation structures (kamp and reyle, 1993).</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
it builds on the c&c; ccg parser (clark and curran, 2004).<papid> P04-1014 </papid></citsent>
<aftsection>
<nextsent>bos and markert (2005) <papid> H05-1079 </papid>describe system for recognizing textual entailment (rte) that uses boxer to convert both the premise and hypothesis of an rte pair into first-order logical semantic representations and then uses theorem prover to check for logical entailment.</nextsent>
<nextsent>distributional models for lexical meaning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1810">
<title id=" W11-0112.xml">integrating logical representations with probabilistic information using markov logic </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>distributional models describe the meaning of word through the context in which it appears (landauer and dumais, 1997; lund and burgess, 1996), where contexts can be documents, other words, or snippets of syntactic structure.
</prevsent>
<prevsent>distributional models are able to predict semantic similarity between words based on distributional similarity and they can be learned in an unsupervised fashion.
</prevsent>
</prevsection>
<citsent citstr=" P08-1028 ">
recently distributional models have been used to predict the applicability of paraphrases in context (mitchell and lapata, 2008; <papid> P08-1028 </papid>erk and pado?, 2008; thater et al, 2010; erk and pado?, 2010).</citsent>
<aftsection>
<nextsent>for example, in the wine left stain?, result in?
</nextsent>
<nextsent>is better paraphrase for leave?
</nextsent>
<nextsent>than is entrust?, while the opposite is true in he left the children with the nurse?.
</nextsent>
<nextsent>usually, the distributional representation for word mixes all its usages (senses).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1811">
<title id=" W11-0112.xml">integrating logical representations with probabilistic information using markov logic </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>fif wini(x) ? ?
</prevsent>
<prevsent>(1) where g(x) is 1 if is satisfied and 0 otherwise, and ni(x) = ? ggfi g(x) is the number of groundings of fi that are satisfied given the current truth assignment to the variables in . this means that the probability of truth assignment rises exponentially with the number of groundings that are satisfied.
</prevsent>
</prevsection>
<citsent citstr=" D09-1001 ">
markov logic has been used previously in other nlp application (e.g. poon and domingos (2009)).<papid> D09-1001 </papid></citsent>
<aftsection>
<nextsent>however, this paper marks the first attempt at representing deep logical semantics in an mln.
</nextsent>
<nextsent>while it is possible learn rule weights in anmln directly from training data, our approach at this time focuses on incorporating weights computed by external knowledge sources.
</nextsent>
<nextsent>weights for word meaning rules are computed from the distributional model of lexical meaning and then injected into the mln.
</nextsent>
<nextsent>rules governing implicativity and coreference are given infinite weight (hard constraints).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1813">
<title id=" W11-0112.xml">integrating logical representations with probabilistic information using markov logic </title>
<section> evaluation and phenomena.  </section>
<citcontext>
<prevsection>
<prevsent>event e2.
</prevsent>
<prevsent>5 handling the phenomena.
</prevsent>
</prevsection>
<citsent citstr=" W06-3907 ">
implicatives and factivesnairn et al (2006) <papid> W06-3907 </papid>presented an approach to the treatment of inferences involving implicatives and fac tives.</citsent>
<aftsection>
<nextsent>their approach identifies an implication signature?
</nextsent>
<nextsent>for every implicative or factive verb that determines the truth conditions for the verbs nested proposition, whether in positive or negative environment.
</nextsent>
<nextsent>implication signatures take the form x/y? where represents the implicativity in the the positive environment and represents the implicativity in the negative environment.
</nextsent>
<nextsent>both and have three possible values: ?+?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1815">
<title id=" W11-0112.xml">integrating logical representations with probabilistic information using markov logic </title>
<section> evaluation and phenomena.  </section>
<citcontext>
<prevsection>
<prevsent>(11) p: ed does not have convertible h: ed does not own car our inference rules governing the interaction of hypernymy and polarity are given in (12).
</prevsent>
<prevsent>the rule in (12a) states that in positive environment, the hyponym entails the hypernym while the rule in (12b) states that in negative environment, the opposite is true: the hypernym entails the hyponym.
</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
(12) (a) ? p1 p2 x.[(hypernym(p1, p2) ? true(l) ? pred(l, p1, x)) ? pred(l, p2, x)]] (b) ? p1 p2 x.[(hypernym(p1, p2) ? false(l) ? pred(l, p2, x)) ? pred(l, p1, x)]] making use of coreference information as test case for incorporating additional resources into boxers logical form, we used the coreference data in ontonotes (hovy et al, 2006).<papid> N06-2015 </papid></citsent>
<aftsection>
<nextsent>however, the same mechanism would allow us to transfer information into boxer output from arbitrary additional nlp tools such as automatic coreference analysis tools or semantic role labelers.
</nextsent>
<nextsent>our system uses coreference information into two distinct ways.
</nextsent>
<nextsent>the first way we make use of coreference data is to copy atoms describing particular variable to those variables that corefer.
</nextsent>
<nextsent>consider again example (4) which has two-sentence premise.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1819">
<title id=" W11-1502.xml">a low budget tagger for old czech </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>the paper describes tagger for old czech (1200-1500 ad), fus ional language with rich morphology.
</prevsent>
</prevsection>
<citsent citstr=" W04-3229 ">
the practical restrictions(no native speakers, limited corpora and lexicons, limited funding) make old czech anideal candidate for resource-light cross lingual method that we have been developing (e.g. hana et al, 2004; <papid> W04-3229 </papid>feldman and hana, 2010).we use traditional supervised tagger.</citsent>
<aftsection>
<nextsent>however, instead of spending years of effort to create large annotated corpus of old czech, we approximate it by corpus of modern czech.
</nextsent>
<nextsent>we perform series of simple transformations to make modern text look more like text in old czech and vice versa.
</nextsent>
<nextsent>we also use aresource-light morphological analyzer to provide candidate tags.
</nextsent>
<nextsent>the results are worse than the results of traditional taggers, but the amount of language-specific work needed is minimal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1823">
<title id=" W11-1502.xml">a low budget tagger for old czech </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the best set of features was selected after hundreds of experiments were performed.
</prevsent>
<prevsent>in 10 contrast, the resource-light system we developed is not as accurate, but the amount of language-specificwork needed is incomparable to that of the state of-the-art systems.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
language specific work on our oc tagger, for example, was completed in about 20 hours, instead of several years.research in resource-light learning of morphosyntactic properties of languages is not new.some have assumed only partially tagged training corpora (merialdo, 1994); <papid> J94-2001 </papid>some have begun with small tagged seed word lists (cucerzan and yarowsky, 2002) <papid> W02-2006 </papid>for named-entity tagging, while others have exploited the automatic transfer of an already existing annotated resource in different genre or different language (e.g. cross-languageprojection of morphological and syntactic information as in (cucerzan and yarowsky, 2000; <papid> P00-1035 </papid>yarowsky et al, 2001), <papid> H01-1035 </papid>requiring no direct supervision in the target language).</citsent>
<aftsection>
<nextsent>the performance of our system is comparable to the results cited by these researchers.in our work we wanted to connect to preexisting knowledge that has been acquired and systematized by traditional linguists, e.g. morphological paradigms, sound changes, and other well established facts about mc and oc.
</nextsent>
<nextsent>czech is west slavic language with significant influences from german, latin and (in modern times) english.
</nextsent>
<nextsent>it is fus ional (flective) language with rich morphology and high degree of homonymy of endings.
</nextsent>
<nextsent>3.1 old czech.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1824">
<title id=" W11-1502.xml">a low budget tagger for old czech </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the best set of features was selected after hundreds of experiments were performed.
</prevsent>
<prevsent>in 10 contrast, the resource-light system we developed is not as accurate, but the amount of language-specificwork needed is incomparable to that of the state of-the-art systems.
</prevsent>
</prevsection>
<citsent citstr=" W02-2006 ">
language specific work on our oc tagger, for example, was completed in about 20 hours, instead of several years.research in resource-light learning of morphosyntactic properties of languages is not new.some have assumed only partially tagged training corpora (merialdo, 1994); <papid> J94-2001 </papid>some have begun with small tagged seed word lists (cucerzan and yarowsky, 2002) <papid> W02-2006 </papid>for named-entity tagging, while others have exploited the automatic transfer of an already existing annotated resource in different genre or different language (e.g. cross-languageprojection of morphological and syntactic information as in (cucerzan and yarowsky, 2000; <papid> P00-1035 </papid>yarowsky et al, 2001), <papid> H01-1035 </papid>requiring no direct supervision in the target language).</citsent>
<aftsection>
<nextsent>the performance of our system is comparable to the results cited by these researchers.in our work we wanted to connect to preexisting knowledge that has been acquired and systematized by traditional linguists, e.g. morphological paradigms, sound changes, and other well established facts about mc and oc.
</nextsent>
<nextsent>czech is west slavic language with significant influences from german, latin and (in modern times) english.
</nextsent>
<nextsent>it is fus ional (flective) language with rich morphology and high degree of homonymy of endings.
</nextsent>
<nextsent>3.1 old czech.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1825">
<title id=" W11-1502.xml">a low budget tagger for old czech </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the best set of features was selected after hundreds of experiments were performed.
</prevsent>
<prevsent>in 10 contrast, the resource-light system we developed is not as accurate, but the amount of language-specificwork needed is incomparable to that of the state of-the-art systems.
</prevsent>
</prevsection>
<citsent citstr=" P00-1035 ">
language specific work on our oc tagger, for example, was completed in about 20 hours, instead of several years.research in resource-light learning of morphosyntactic properties of languages is not new.some have assumed only partially tagged training corpora (merialdo, 1994); <papid> J94-2001 </papid>some have begun with small tagged seed word lists (cucerzan and yarowsky, 2002) <papid> W02-2006 </papid>for named-entity tagging, while others have exploited the automatic transfer of an already existing annotated resource in different genre or different language (e.g. cross-languageprojection of morphological and syntactic information as in (cucerzan and yarowsky, 2000; <papid> P00-1035 </papid>yarowsky et al, 2001), <papid> H01-1035 </papid>requiring no direct supervision in the target language).</citsent>
<aftsection>
<nextsent>the performance of our system is comparable to the results cited by these researchers.in our work we wanted to connect to preexisting knowledge that has been acquired and systematized by traditional linguists, e.g. morphological paradigms, sound changes, and other well established facts about mc and oc.
</nextsent>
<nextsent>czech is west slavic language with significant influences from german, latin and (in modern times) english.
</nextsent>
<nextsent>it is fus ional (flective) language with rich morphology and high degree of homonymy of endings.
</nextsent>
<nextsent>3.1 old czech.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1826">
<title id=" W11-1502.xml">a low budget tagger for old czech </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the best set of features was selected after hundreds of experiments were performed.
</prevsent>
<prevsent>in 10 contrast, the resource-light system we developed is not as accurate, but the amount of language-specificwork needed is incomparable to that of the state of-the-art systems.
</prevsent>
</prevsection>
<citsent citstr=" H01-1035 ">
language specific work on our oc tagger, for example, was completed in about 20 hours, instead of several years.research in resource-light learning of morphosyntactic properties of languages is not new.some have assumed only partially tagged training corpora (merialdo, 1994); <papid> J94-2001 </papid>some have begun with small tagged seed word lists (cucerzan and yarowsky, 2002) <papid> W02-2006 </papid>for named-entity tagging, while others have exploited the automatic transfer of an already existing annotated resource in different genre or different language (e.g. cross-languageprojection of morphological and syntactic information as in (cucerzan and yarowsky, 2000; <papid> P00-1035 </papid>yarowsky et al, 2001), <papid> H01-1035 </papid>requiring no direct supervision in the target language).</citsent>
<aftsection>
<nextsent>the performance of our system is comparable to the results cited by these researchers.in our work we wanted to connect to preexisting knowledge that has been acquired and systematized by traditional linguists, e.g. morphological paradigms, sound changes, and other well established facts about mc and oc.
</nextsent>
<nextsent>czech is west slavic language with significant influences from german, latin and (in modern times) english.
</nextsent>
<nextsent>it is fus ional (flective) language with rich morphology and high degree of homonymy of endings.
</nextsent>
<nextsent>3.1 old czech.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1827">
<title id=" W11-1502.xml">a low budget tagger for old czech </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>again this is much less than what we would like to have, and we plan to increase the size in the near future.the tagset is modification of the modern tagset using the same categories.
</prevsent>
<prevsent>the main assumption of our method (feldman and hana, 2010) is that model for the target language can be approximated by language models from one or more related source languages and that inclusion of limited amount of high-impact and/or low-cost manual resources is greatly beneficial and desirable.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
we use tnt (brants, 2000), <papid> A00-1031 </papid>second order markov model tagger.</citsent>
<aftsection>
<nextsent>the language model of sucha tagger consists of emission probabilities (corresponding to lexicon with usage frequency information) and transition probabilities (roughly corresponding to syntax rules with strong emphasis on local word-order).
</nextsent>
<nextsent>we approximate the emission and transition probabilities by those trained on modified corpus of related language.
</nextsent>
<nextsent>below, we describe our approach in more detail.
</nextsent>
<nextsent>12
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1828">
<title id=" W10-4163.xml">overview of the chinese word sense induction task at clp2010 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation (wsd) is an important task in natural language proceeding research and is critical to many applications which require language understanding.
</prevsent>
<prevsent>in traditional evaluations, the supervised methods usually can achieve better wsd performance than the unsupervised methods.
</prevsent>
</prevsection>
<citsent citstr=" W07-2002 ">
but the supervised wsd methods have some drawbacks: firstly, they need large annotated dataset which is expensive to manually annotate (agirre and aitor, 2007).<papid> W07-2002 </papid></citsent>
<aftsection>
<nextsent>secondly, the supervised wsd methods are based on the fixed-list of senses?
</nextsent>
<nextsent>paradigm, i.e., the senses of target word are represented as closed list coming from manually constructed dictionary (agirre et al, 2006).<papid> W06-1669 </papid></nextsent>
<nextsent>such fixed-list of senses?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1829">
<title id=" W10-4163.xml">overview of the chinese word sense induction task at clp2010 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but the supervised wsd methods have some drawbacks: firstly, they need large annotated dataset which is expensive to manually annotate (agirre and aitor, 2007).<papid> W07-2002 </papid></prevsent>
<prevsent>secondly, the supervised wsd methods are based on the fixed-list of senses?</prevsent>
</prevsection>
<citsent citstr=" W06-1669 ">
paradigm, i.e., the senses of target word are represented as closed list coming from manually constructed dictionary (agirre et al, 2006).<papid> W06-1669 </papid></citsent>
<aftsection>
<nextsent>such fixed-list of senses?
</nextsent>
<nextsent>paradigm suffers from the lack of explicit and topic relations between word senses, are usually cannot reflect the exact context of the target word (veronis, 2004).
</nextsent>
<nextsent>furthermore, because the fixed-list of senses?
</nextsent>
<nextsent>paradigm make the fix granularity assumption of the senses distinction, it may not be suitable in different situations (samuel and mirella, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1831">
<title id=" W10-4163.xml">overview of the chinese word sense induction task at clp2010 </title>
<section> evaluation metric.  </section>
<citcontext>
<prevsection>
<prevsent>so we use fscore measure to assess clustering solution.
</prevsent>
<prevsent>for the sake of completeness, we also employ the v-measure to assess different clustering solutions.
</prevsent>
</prevsection>
<citsent citstr=" D07-1043 ">
v-measure assesses cluster solution by considering its homogeneity and its completeness (rosenberg and hirschberg, 2007).<papid> D07-1043 </papid></citsent>
<aftsection>
<nextsent>homogeneity measures the degree that each cluster contains data points which belong to single gold standard class.
</nextsent>
<nextsent>and completeness measures the degree that each gold standard class contains data points assigned to single cluster (rosenberg and hirschberg, 2007).<papid> D07-1043 </papid></nextsent>
<nextsent>in general, the larger the v-measure, the better the clustering performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1841">
<title id=" W11-0808.xml">two types of korean light verb constructions in a typed feature structure grammar </title>
<section> similar examples in english (pustejovsky, 1991):.  </section>
<citcontext>
<prevsection>
<prevsent>the second idiosyncratic property is about what predicate is associated with what common noun.
</prevsent>
<prevsent>for instance, in (4a) the cn-lvc has only one reading,  he cooked the rice , not other interpretations like  he ate the rice,  although  cook  and  eat  are (at least semantically and maybe also statistically) plausible candidates for the associated predicates of the common noun pap  rice .
</prevsent>
</prevsection>
<citsent citstr=" N01-1009 ">
lapata (2001) <papid> N01-1009 </papid>uses large corpus to acquire the meanings of polysemous adjectives (e.g., fast).</citsent>
<aftsection>
<nextsent>however, such corpus findings only tell us the possible interpretations, but not impossible interpretations.
</nextsent>
<nextsent>it seems intuitive that common nouns have such information about their related predicates since without specific predicate given, we can normally guess what predicate might come after common noun object in an incomplete sentence (at least in korean whose word order is sov) (see similar combinatoric information related with korean vn of vn-lvcs in cho and sells, 1991 and japanese vn in manning, 1993).
</nextsent>
<nextsent>in short, only some common nouns have such information about certain related predicates.
</nextsent>
<nextsent>pustejovsky (1991) <papid> J91-4003 </papid>refers to this kind of relation as cospecification: i.e. like verb can select for its argument type, an argument also can select its associated predicates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1842">
<title id=" W11-0808.xml">two types of korean light verb constructions in a typed feature structure grammar </title>
<section> similar examples in english (pustejovsky, 1991):.  </section>
<citcontext>
<prevsection>
<prevsent>it seems intuitive that common nouns have such information about their related predicates since without specific predicate given, we can normally guess what predicate might come after common noun object in an incomplete sentence (at least in korean whose word order is sov) (see similar combinatoric information related with korean vn of vn-lvcs in cho and sells, 1991 and japanese vn in manning, 1993).
</prevsent>
<prevsent>in short, only some common nouns have such information about certain related predicates.
</prevsent>
</prevsection>
<citsent citstr=" J91-4003 ">
pustejovsky (1991) <papid> J91-4003 </papid>refers to this kind of relation as cospecification: i.e. like verb can select for its argument type, an argument also can select its associated predicates.</citsent>
<aftsection>
<nextsent>the associated predicate information is included in the qualia structure of lexical item (pustejovsky, 1991).<papid> J91-4003 </papid></nextsent>
<nextsent>among the four basic roles in qualia structure, the telic role has values about purpose and function of the object (e.g., read for novel), and the agentive role has values on factors involved in the origin or bringing about?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1845">
<title id=" W11-0808.xml">two types of korean light verb constructions in a typed feature structure grammar </title>
<section> typed-feature structure grammar.  </section>
<citcontext>
<prevsection>
<prevsent>the type cn also inherits its constraints from its supertypes: for instance, nominal properties from the type n-stem (see kim et al, 2004).
</prevsent>
<prevsent>briscoe et al.
</prevsent>
</prevsection>
<citsent citstr=" E89-1000 ">
(1990) and copestake (1993) illustrate some lexical entries with the qualia structure following pustejovsky and aniek (1988), pustejovsky (1989),  <papid> E89-1000 </papid>pustejovsky (1991).<papid> J91-4003 </papid></citsent>
<aftsection>
<nextsent>for example, autobiography has its associated predicates, write (the value of the agentive role) and read (the value of the telic role).
</nextsent>
<nextsent>they are represented in the lexical entry of autobiography.
</nextsent>
<nextsent>i declare that korean common nouns have both the restr(iction) for normal semantics and the qualia-st(ructure), which in turn has the agentive and telic attributes, adopting the basic idea from pustejovsky (1991) <papid> J91-4003 </papid>and adapting the feature structure from copestake (1993).</nextsent>
<nextsent>moreover, posit the qualia attribute whose value is the sum of the values of the agentive and telic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1850">
<title id=" W11-0603.xml">unsupervised syntactic chunking with acoustic cues computational models for prosodic bootstrapping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results support the hypothesis that acoustic-prosodic cues provide useful evidence about syntactic phrases for language learning infants.
</prevsent>
<prevsent>young children routinely learn to group words into phrases, yet computational methods have so far struggled to accomplish this task without supervision.
</prevsent>
</prevsection>
<citsent citstr=" P04-1061 ">
previous work on unsupervised grammar induction has made progress by exploiting information such as gold-standard part of speech tags (e.g. klein and manning (2004)) <papid> P04-1061 </papid>or punctuation (e.g. seginer (2007)).<papid> P07-1049 </papid></citsent>
<aftsection>
<nextsent>while this information may be available in some nlp contexts, our focus here is on the computational problem facing language-learning infants, who do not have access to either part of speech tags or punctuation.
</nextsent>
<nextsent>however, infants do have access to certain cues that have not been well explored by nlp researchers focused on grammar induction from text.
</nextsent>
<nextsent>in particular, we consider the cues to syntactic structure that might be available from prosody (roughly, the structure of speech conveyed through rhythm and intonation) and its acoustic realization.the idea that prosody provides important initial cues for grammar acquisition is known as the prosodic bootstrapping hypothesis, and is well established in the field of language acquisition (gleitman and wanner, 1982).
</nextsent>
<nextsent>experimental work has provided strong support for this hypothesis, for example by showing that infants begin learning basic rhythmic properties of their language prenatally (mehler et al, 1988) and that 9-month-olds use prosodic cues to distinguish verb phrases from non constituents (soderstrom et al, 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1851">
<title id=" W11-0603.xml">unsupervised syntactic chunking with acoustic cues computational models for prosodic bootstrapping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results support the hypothesis that acoustic-prosodic cues provide useful evidence about syntactic phrases for language learning infants.
</prevsent>
<prevsent>young children routinely learn to group words into phrases, yet computational methods have so far struggled to accomplish this task without supervision.
</prevsent>
</prevsection>
<citsent citstr=" P07-1049 ">
previous work on unsupervised grammar induction has made progress by exploiting information such as gold-standard part of speech tags (e.g. klein and manning (2004)) <papid> P04-1061 </papid>or punctuation (e.g. seginer (2007)).<papid> P07-1049 </papid></citsent>
<aftsection>
<nextsent>while this information may be available in some nlp contexts, our focus here is on the computational problem facing language-learning infants, who do not have access to either part of speech tags or punctuation.
</nextsent>
<nextsent>however, infants do have access to certain cues that have not been well explored by nlp researchers focused on grammar induction from text.
</nextsent>
<nextsent>in particular, we consider the cues to syntactic structure that might be available from prosody (roughly, the structure of speech conveyed through rhythm and intonation) and its acoustic realization.the idea that prosody provides important initial cues for grammar acquisition is known as the prosodic bootstrapping hypothesis, and is well established in the field of language acquisition (gleitman and wanner, 1982).
</nextsent>
<nextsent>experimental work has provided strong support for this hypothesis, for example by showing that infants begin learning basic rhythmic properties of their language prenatally (mehler et al, 1988) and that 9-month-olds use prosodic cues to distinguish verb phrases from non constituents (soderstrom et al, 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1852">
<title id=" W11-0603.xml">unsupervised syntactic chunking with acoustic cues computational models for prosodic bootstrapping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental work has provided strong support for this hypothesis, for example by showing that infants begin learning basic rhythmic properties of their language prenatally (mehler et al, 1988) and that 9-month-olds use prosodic cues to distinguish verb phrases from non constituents (soderstrom et al, 2003).
</prevsent>
<prevsent>however, asfar as we know, there has so far been no direct computational evaluation of the prosodic bootstrapping hypothesis.
</prevsent>
</prevsection>
<citsent citstr=" D07-1072 ">
in this paper, we provide the first such evaluation by exploring the utility of acoustic cues for unsupervised syntactic chunking, i.e., grouping words into non-hierarchical syntactic phrases.nearly all previous work on unsupervised grammar induction has focused on learning hierarchical phrase structure (lari and young, 1990; liang et al, 2007) <papid> D07-1072 </papid>or dependency structure (klein and manning, 2004); <papid> P04-1061 </papid>we are aware of only one previous paper on unsupervised syntactic chunking (ponvert et al, 2010).</citsent>
<aftsection>
<nextsent>ponvert et al describe simple method for chunking that uses only bigram counts and punctuation; when the chunks are combined using right branching structure, the resulting trees achieve unlabeled bracketing precision and recall that is competitive with other unsupervised parsers.
</nextsent>
<nextsent>the sys 20tems dependence on punctuation renders it inappropriate for addressing the questions we are interested in here, but its good performance rec commends syntactic chunking as profitable approach to the problem of grammar induction, especially since chunks can be learned using much simpler models than are needed for hierarchical structure.
</nextsent>
<nextsent>the models used in this paper are all variants of hmms.
</nextsent>
<nextsent>our baseline models are standard hmmsthat learn from either lexical or prosodic observations only; we also consider three types of models (including coupled hmm) that incorporate both lexical and prosodic observations, but vary the degree to which syntactic and prosodic variables are tied together in the latent structure of the models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1855">
<title id=" W11-0603.xml">unsupervised syntactic chunking with acoustic cues computational models for prosodic bootstrapping </title>
<section> prosody and syntax.  </section>
<citcontext>
<prevsection>
<prevsent>prosody is theoretical linguistic concept positing an abstract organizational structure for speech.2while it is often closely associated with such measurable phenomena as movement in fundamental frequency or variation in spectral tilt, these are merely observable acoustic correlates that provide evidence of varying quality about the hidden prosodic structure, which specifies such hidden variables as contrastive stress or question intonation.
</prevsent>
<prevsent>prosody has been hypothesized to be useful for learning syntax because it imposes grouping structure on word sequences that sometimes coincides with traditional constituency analyses (ladd, 1996; shattuck-hufnagel and turk, 1996).
</prevsent>
</prevsection>
<citsent citstr=" H91-1073 ">
moreover, laboratory experiments have shown that adults use prosody both for syntactic disambiguation (millotte et al, 2007; price et al, 1991) <papid> H91-1073 </papid>and, crucially, in learning the syntax of an artificial language (morgan et al, 1987).</citsent>
<aftsection>
<nextsent>accordingly, if prosodic structure is sufficiently prominent in the acoustic signal, and coincides often enough with syntactic structure, then it may provide children with useful information about how to combine words into phrases.although there are several theories of how to represent and annotate prosodic structure, one of themost influential is the tobi (tones and break in dices) theory (beckman et al, 2005), which wewill use in some of our experiments.
</nextsent>
<nextsent>tobi proposes, among other things, that the prosodic phrasing of languages can be represented in terms of sequences of break indices indicating the strength of 2signed languages also exhibit prosodic phenomena, but they are not addressed here.
</nextsent>
<nextsent>21 word boundaries.
</nextsent>
<nextsent>in mainstream american english tobi, for example, the boundary between clitic and its base word (e.g. do?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1856">
<title id=" W11-0603.xml">unsupervised syntactic chunking with acoustic cues computational models for prosodic bootstrapping </title>
<section> prosody and syntax.  </section>
<citcontext>
<prevsection>
<prevsent>and nt? of dont?) is 0, representing very weak boundary, while the boundary following word at the end of an intona tional phrase is 4, indicating very strong boundary.
</prevsent>
<prevsent>below we examine how useful these break indices are for identifying syntactic boundaries.finally, we note that our work is not the first computational approach to using prosody for identifying syntactic structure.
</prevsent>
</prevsection>
<citsent citstr=" N04-1011 ">
however, previous work (gre gory et al, 2004; <papid> N04-1011 </papid>kahn et al, 2005; <papid> H05-1030 </papid>dreyer and shafran, 2007; noth et al, 2000) has focused on supervised parsing rather than unsupervised chunking, and also makes different assumptions about prosody.</citsent>
<aftsection>
<nextsent>for example, gregory et al (2004) <papid> N04-1011 </papid>assume that prosody is an acoustically-realized substitute forpunctuation; our own treatment is much less con strained.</nextsent>
<nextsent>kahn et al (2005) <papid> H05-1030 </papid>and dreyer and shafran(2007) use tobi labels to represent prosodic information, whereas we explore both tobi and direct acoustic measures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1857">
<title id=" W11-0603.xml">unsupervised syntactic chunking with acoustic cues computational models for prosodic bootstrapping </title>
<section> prosody and syntax.  </section>
<citcontext>
<prevsection>
<prevsent>and nt? of dont?) is 0, representing very weak boundary, while the boundary following word at the end of an intona tional phrase is 4, indicating very strong boundary.
</prevsent>
<prevsent>below we examine how useful these break indices are for identifying syntactic boundaries.finally, we note that our work is not the first computational approach to using prosody for identifying syntactic structure.
</prevsent>
</prevsection>
<citsent citstr=" H05-1030 ">
however, previous work (gre gory et al, 2004; <papid> N04-1011 </papid>kahn et al, 2005; <papid> H05-1030 </papid>dreyer and shafran, 2007; noth et al, 2000) has focused on supervised parsing rather than unsupervised chunking, and also makes different assumptions about prosody.</citsent>
<aftsection>
<nextsent>for example, gregory et al (2004) <papid> N04-1011 </papid>assume that prosody is an acoustically-realized substitute forpunctuation; our own treatment is much less con strained.</nextsent>
<nextsent>kahn et al (2005) <papid> H05-1030 </papid>and dreyer and shafran(2007) use tobi labels to represent prosodic information, whereas we explore both tobi and direct acoustic measures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1860">
<title id=" W11-0603.xml">unsupervised syntactic chunking with acoustic cues computational models for prosodic bootstrapping </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>finally, noth et al (2000) do notuse tobi, instead developing novel prosodic annotation system designed specifically to provide cues to syntax and for annotation efficiency.
</prevsent>
<prevsent>however, their system is supervised and focuses on improving parse speed rather than accuracy.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
following previous work (e.g. molina and pla (2002) sha and pereira (2003)), <papid> N03-1028 </papid>we formulate chunking as tagging task.</citsent>
<aftsection>
<nextsent>we use hidden markov models (hmms) and their variants to perform the tagging, with carefully specified tags and constrained transition distributions to allow us to interpret the results as bracketing of the input.
</nextsent>
<nextsent>specifically, we use four chunk tags: (begin?)
</nextsent>
<nextsent>and (end?)
</nextsent>
<nextsent>tags are interpreted as the first and last words of chunk, respectively, with (inside?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1862">
<title id=" W11-1508.xml">structure preserving pipelines for digital libraries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most existing hlt pipelines assume the inputis pure text or, at most, html and either ignore (logical) document structure or remove it.
</prevsent>
<prevsent>we argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of document is preserved.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
many off-the-shelf human language technology (hlt) pipelines are now freely available (examplesinclude lingpipe,1 opennlp,2 gate3 (cunning ham et al, 2002), <papid> P02-1022 </papid>textpro4 (pianta et al, 2008)),<papid> L08-1408 </papid>and although they support variety of document formats as input, actual processing (mostly) takes no advantage of structural information, i.e. structural information is not used, or stripped off during pre processing.</citsent>
<aftsection>
<nextsent>such processing can be considered safe, e.g. in case of news wire snippets, when processing does not need to be aware of sentence or paragraph boundaries, or of text being part of table or figure caption.
</nextsent>
<nextsent>however, when processing large documents, section or chapter boundaries may be considered an important segmentation to use, and when working with the type of data typically found in digital libraries or historical archives, such as whole 1 http://alias-i.com/lingpipe/ 2 http://incubator.apache.org/opennlp/ 3 http://http://gate.ac.uk/ 4 http://textpro.fbk.eu/books, exhibition catalogs, scientific articles, contracts we should keep the structure.
</nextsent>
<nextsent>at least three types of problems can be observed when trying to use standard hlt pipeline for documents whose structure cannot be easily ignored: ? techniques for extracting content from plain text do not work on, say, bibliographic references, or lists; ? simply removing the parts of document that do not contain plain text may not be the right thing to do for all applications, as sometimes the information contained in them may also be useful (e.g., keywords are often useful for classification, bibliographic references are useful ina variety of applications) or even the most important parts of text (e.g., in topic classification information provided by titles and other types of document structure is often the most important part of document); ? even for parts of document that still can be considered as containing basically texte.g., titles knowing that we are dealing with what we will call here non-paragraph text can be useful to achieve good - or improve - performance as e.g., the syntactic conventions usedin those type of document elements may be different - e.g., the syntax of nps in titles can be pretty different from that in other sections of text.
</nextsent>
<nextsent>in this paper we summarize several years of workon developing structure-preserving pipelines for different applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1863">
<title id=" W11-1508.xml">structure preserving pipelines for digital libraries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most existing hlt pipelines assume the inputis pure text or, at most, html and either ignore (logical) document structure or remove it.
</prevsent>
<prevsent>we argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of document is preserved.
</prevsent>
</prevsection>
<citsent citstr=" L08-1408 ">
many off-the-shelf human language technology (hlt) pipelines are now freely available (examplesinclude lingpipe,1 opennlp,2 gate3 (cunning ham et al, 2002), <papid> P02-1022 </papid>textpro4 (pianta et al, 2008)),<papid> L08-1408 </papid>and although they support variety of document formats as input, actual processing (mostly) takes no advantage of structural information, i.e. structural information is not used, or stripped off during pre processing.</citsent>
<aftsection>
<nextsent>such processing can be considered safe, e.g. in case of news wire snippets, when processing does not need to be aware of sentence or paragraph boundaries, or of text being part of table or figure caption.
</nextsent>
<nextsent>however, when processing large documents, section or chapter boundaries may be considered an important segmentation to use, and when working with the type of data typically found in digital libraries or historical archives, such as whole 1 http://alias-i.com/lingpipe/ 2 http://incubator.apache.org/opennlp/ 3 http://http://gate.ac.uk/ 4 http://textpro.fbk.eu/books, exhibition catalogs, scientific articles, contracts we should keep the structure.
</nextsent>
<nextsent>at least three types of problems can be observed when trying to use standard hlt pipeline for documents whose structure cannot be easily ignored: ? techniques for extracting content from plain text do not work on, say, bibliographic references, or lists; ? simply removing the parts of document that do not contain plain text may not be the right thing to do for all applications, as sometimes the information contained in them may also be useful (e.g., keywords are often useful for classification, bibliographic references are useful ina variety of applications) or even the most important parts of text (e.g., in topic classification information provided by titles and other types of document structure is often the most important part of document); ? even for parts of document that still can be considered as containing basically texte.g., titles knowing that we are dealing with what we will call here non-paragraph text can be useful to achieve good - or improve - performance as e.g., the syntactic conventions usedin those type of document elements may be different - e.g., the syntax of nps in titles can be pretty different from that in other sections of text.
</nextsent>
<nextsent>in this paper we summarize several years of workon developing structure-preserving pipelines for different applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1864">
<title id=" W11-1508.xml">structure preserving pipelines for digital libraries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at least three types of problems can be observed when trying to use standard hlt pipeline for documents whose structure cannot be easily ignored: ? techniques for extracting content from plain text do not work on, say, bibliographic references, or lists; ? simply removing the parts of document that do not contain plain text may not be the right thing to do for all applications, as sometimes the information contained in them may also be useful (e.g., keywords are often useful for classification, bibliographic references are useful ina variety of applications) or even the most important parts of text (e.g., in topic classification information provided by titles and other types of document structure is often the most important part of document); ? even for parts of document that still can be considered as containing basically texte.g., titles knowing that we are dealing with what we will call here non-paragraph text can be useful to achieve good - or improve - performance as e.g., the syntactic conventions usedin those type of document elements may be different - e.g., the syntax of nps in titles can be pretty different from that in other sections of text.
</prevsent>
<prevsent>in this paper we summarize several years of workon developing structure-preserving pipelines for different applications.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
we discuss the incorporation of 54 document structure parsers both in pipelines which the information is passed in boi format (ramshaw and marcus, 1995), <papid> W95-0107 </papid>such as the textpro pipeline (pianta et al, 2008), <papid> L08-1408 </papid>and in pipelines based on astandoff xml (ide, 1998).</citsent>
<aftsection>
<nextsent>we also present several distinct applications that require preserving document structure.
</nextsent>
<nextsent>the structure of the paper is as follows.
</nextsent>
<nextsent>we first discuss the notion of document structure and previous work in extracting it.
</nextsent>
<nextsent>we then introduce our architecture for structure-preserving pipeline.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1866">
<title id=" W11-1508.xml">structure preserving pipelines for digital libraries </title>
<section> the logical structure of document.  </section>
<citcontext>
<prevsection>
<prevsent>the most common logical elements in suchproposalschapters, sections, paragraphs, footnotes, etc.can all be found in html, latex, or any other modern text processor.
</prevsent>
<prevsent>it should be pointed out however that many modern types of documents found on the web do not fit this pattern: e.g. blog posts with comments, and the structure of reply threads and inner-linkings to other comments cannot be captured; or much of wikipedias non paragraph text.
</prevsent>
</prevsection>
<citsent citstr=" J03-2003 ">
(for an indepth comparison and discussion of logical formats, and formal characterizations thereof we suggest (power et al, 2003; <papid> J03-2003 </papid>summers, 1998).)</citsent>
<aftsection>
<nextsent>2.2 extracting logical structure.
</nextsent>
<nextsent>two families of methods have been developed to extract document structure.
</nextsent>
<nextsent>older systems tend to follow the template-matching paradigm.
</nextsent>
<nextsent>in this approach the assignment of the categories to parts of the string is done by matching sequence of handcrafted templates against the input string s. an instance of this kind of systems is delos (deriva tion of logical structure) (niyogi and srihari, 1995)which uses control rules, strategy rules and knowl 6 http://www.tei-c.org 55 edge rules to derive the logical document structure from scanned image of the document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1867">
<title id=" W11-1508.xml">structure preserving pipelines for digital libraries </title>
<section> the logical structure of document.  </section>
<citcontext>
<prevsection>
<prevsent>the system obtains an average f1 measure of 93 for the cora dataset.a better performance for sequence labeling is obtained if crf replaces the traditional hmm.
</prevsent>
<prevsent>the reason for this is that crf systems better tolerate errors and they have good performance even when richer features are not available.
</prevsent>
</prevsection>
<citsent citstr=" L08-1291 ">
a system which uses crf and series of post-processing rules for both document logical structure identification and reference string parsing is parscit (councill et al,2008).<papid> L08-1291 </papid></citsent>
<aftsection>
<nextsent>parscit comprises three sub-modules: sect label and parse head for document logical structure identification and parscit for reference string parsing.
</nextsent>
<nextsent>the system is built on top of the well known crf++ package.
</nextsent>
<nextsent>the linguistic surface level, i.e. the linear orderof words, sentences, and paragraphs, and the hierarchical, tree-like, logical structure also lends itself to parsing-like methods for the structure analysis.
</nextsent>
<nextsent>however, the complexity of fostering, maintaining, and augmenting document structure grammars is challenging, and the notorious uncertainty of the input demands for the whole set of stochastic techniques the field has to offer ? this comes at high computing price; cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1869">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>notonly is there real choice between range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. natural language generation (nlg) has not so far developed generic tools and methods for comparing them to the same extent as natural language analysis (nla) has.
</prevsent>
<prevsent>the sub field of nlg that has perhaps come closest to developing generic tools is surface realisation.
</prevsent>
</prevsection>
<citsent citstr=" W96-0501 ">
wide-coverage surface realisers such as penman/nigel (mann and mathiesen, 1983), fuf/surge (elhadad and robin, 1996) <papid> W96-0501 </papid>and realpro (lavoie and rambow, 1997) <papid> A97-1039 </papid>were intended to be more or less off the-shelf plug-and-play modules.</citsent>
<aftsection>
<nextsent>but they tended to require significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set.
</nextsent>
<nextsent>with the advent of statistical techniques in nlg surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood.
</nextsent>
<nextsent>an early example, the japan-gloss system (knight et al , 1995) replaced penmans default settings with statistical decisions.
</nextsent>
<nextsent>the halo gen/nitrogen developers (langkilde and knight,1998<papid> P98-1116 </papid>a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiserwas decided simply by highest likelihood according to language model, automatically trainable from raw corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1870">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>notonly is there real choice between range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. natural language generation (nlg) has not so far developed generic tools and methods for comparing them to the same extent as natural language analysis (nla) has.
</prevsent>
<prevsent>the sub field of nlg that has perhaps come closest to developing generic tools is surface realisation.
</prevsent>
</prevsection>
<citsent citstr=" A97-1039 ">
wide-coverage surface realisers such as penman/nigel (mann and mathiesen, 1983), fuf/surge (elhadad and robin, 1996) <papid> W96-0501 </papid>and realpro (lavoie and rambow, 1997) <papid> A97-1039 </papid>were intended to be more or less off the-shelf plug-and-play modules.</citsent>
<aftsection>
<nextsent>but they tended to require significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set.
</nextsent>
<nextsent>with the advent of statistical techniques in nlg surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood.
</nextsent>
<nextsent>an early example, the japan-gloss system (knight et al , 1995) replaced penmans default settings with statistical decisions.
</nextsent>
<nextsent>the halo gen/nitrogen developers (langkilde and knight,1998<papid> P98-1116 </papid>a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiserwas decided simply by highest likelihood according to language model, automatically trainable from raw corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1871">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>with the advent of statistical techniques in nlg surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood.
</prevsent>
<prevsent>an early example, the japan-gloss system (knight et al , 1995) replaced penmans default settings with statistical decisions.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
the halo gen/nitrogen developers (langkilde and knight,1998<papid> P98-1116 </papid>a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiserwas decided simply by highest likelihood according to language model, automatically trainable from raw corpora.</citsent>
<aftsection>
<nextsent>the halogen/nitrogen work sparked an interest in statistical nlg which led to range of surface realisation methods that used corpus frequencies in one way or another (varges and mellish, 2001; <papid> N01-1001 </papid>white, 2004; velldal et al , 2004; paiva and evans, 2005).<papid> P05-1008 </papid></nextsent>
<nextsent>some surface realisation work looked at directly applying statistical models during linguistically informed generation process to prune the search space (white, 2004; carroll and oepen, 2005).<papid> I05-1015 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1872">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>an early example, the japan-gloss system (knight et al , 1995) replaced penmans default settings with statistical decisions.
</prevsent>
<prevsent>the halo gen/nitrogen developers (langkilde and knight,1998<papid> P98-1116 </papid>a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiserwas decided simply by highest likelihood according to language model, automatically trainable from raw corpora.</prevsent>
</prevsection>
<citsent citstr=" N01-1001 ">
the halogen/nitrogen work sparked an interest in statistical nlg which led to range of surface realisation methods that used corpus frequencies in one way or another (varges and mellish, 2001; <papid> N01-1001 </papid>white, 2004; velldal et al , 2004; paiva and evans, 2005).<papid> P05-1008 </papid></citsent>
<aftsection>
<nextsent>some surface realisation work looked at directly applying statistical models during linguistically informed generation process to prune the search space (white, 2004; carroll and oepen, 2005).<papid> I05-1015 </papid></nextsent>
<nextsent>while statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. significant subset of statistical realisation work (langkilde, 2002; callaway, 2003; nakanishi et al , 2005; <papid> W05-1510 </papid>zhong and stent, 2005; cahill and van genabith, 2006; <papid> P06-1130 </papid>white and rajkumar, 2009) <papid> D09-1043 </papid>has recently produced results for regenerating the penn treebank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1873">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>an early example, the japan-gloss system (knight et al , 1995) replaced penmans default settings with statistical decisions.
</prevsent>
<prevsent>the halo gen/nitrogen developers (langkilde and knight,1998<papid> P98-1116 </papid>a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiserwas decided simply by highest likelihood according to language model, automatically trainable from raw corpora.</prevsent>
</prevsection>
<citsent citstr=" P05-1008 ">
the halogen/nitrogen work sparked an interest in statistical nlg which led to range of surface realisation methods that used corpus frequencies in one way or another (varges and mellish, 2001; <papid> N01-1001 </papid>white, 2004; velldal et al , 2004; paiva and evans, 2005).<papid> P05-1008 </papid></citsent>
<aftsection>
<nextsent>some surface realisation work looked at directly applying statistical models during linguistically informed generation process to prune the search space (white, 2004; carroll and oepen, 2005).<papid> I05-1015 </papid></nextsent>
<nextsent>while statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. significant subset of statistical realisation work (langkilde, 2002; callaway, 2003; nakanishi et al , 2005; <papid> W05-1510 </papid>zhong and stent, 2005; cahill and van genabith, 2006; <papid> P06-1130 </papid>white and rajkumar, 2009) <papid> D09-1043 </papid>has recently produced results for regenerating the penn treebank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1874">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the halo gen/nitrogen developers (langkilde and knight,1998<papid> P98-1116 </papid>a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiserwas decided simply by highest likelihood according to language model, automatically trainable from raw corpora.</prevsent>
<prevsent>the halogen/nitrogen work sparked an interest in statistical nlg which led to range of surface realisation methods that used corpus frequencies in one way or another (varges and mellish, 2001; <papid> N01-1001 </papid>white, 2004; velldal et al , 2004; paiva and evans, 2005).<papid> P05-1008 </papid></prevsent>
</prevsection>
<citsent citstr=" I05-1015 ">
some surface realisation work looked at directly applying statistical models during linguistically informed generation process to prune the search space (white, 2004; carroll and oepen, 2005).<papid> I05-1015 </papid></citsent>
<aftsection>
<nextsent>while statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. significant subset of statistical realisation work (langkilde, 2002; callaway, 2003; nakanishi et al , 2005; <papid> W05-1510 </papid>zhong and stent, 2005; cahill and van genabith, 2006; <papid> P06-1130 </papid>white and rajkumar, 2009) <papid> D09-1043 </papid>has recently produced results for regenerating the penn treebank.</nextsent>
<nextsent>the basic approach in all this work is to remove information from the penn tree bank parses (the word strings themselves as wellas some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1875">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the halogen/nitrogen work sparked an interest in statistical nlg which led to range of surface realisation methods that used corpus frequencies in one way or another (varges and mellish, 2001; <papid> N01-1001 </papid>white, 2004; velldal et al , 2004; paiva and evans, 2005).<papid> P05-1008 </papid></prevsent>
<prevsent>some surface realisation work looked at directly applying statistical models during linguistically informed generation process to prune the search space (white, 2004; carroll and oepen, 2005).<papid> I05-1015 </papid></prevsent>
</prevsection>
<citsent citstr=" W05-1510 ">
while statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. significant subset of statistical realisation work (langkilde, 2002; callaway, 2003; nakanishi et al , 2005; <papid> W05-1510 </papid>zhong and stent, 2005; cahill and van genabith, 2006; <papid> P06-1130 </papid>white and rajkumar, 2009) <papid> D09-1043 </papid>has recently produced results for regenerating the penn treebank.</citsent>
<aftsection>
<nextsent>the basic approach in all this work is to remove information from the penn tree bank parses (the word strings themselves as wellas some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence.
</nextsent>
<nextsent>results are typically evaluated using bleu, and, roughly speaking, bleu scores go down as more information is removed.
</nextsent>
<nextsent>while publications of work along these lines do refer to each other and (tentatively) compare bleu scores, the results are not in fact directly comparable, because of the differences in the input representations automatically derived from penn tree bank annotations.
</nextsent>
<nextsent>in particular, the extent to which they are underspecified varies from one system to the next.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1876">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the halogen/nitrogen work sparked an interest in statistical nlg which led to range of surface realisation methods that used corpus frequencies in one way or another (varges and mellish, 2001; <papid> N01-1001 </papid>white, 2004; velldal et al , 2004; paiva and evans, 2005).<papid> P05-1008 </papid></prevsent>
<prevsent>some surface realisation work looked at directly applying statistical models during linguistically informed generation process to prune the search space (white, 2004; carroll and oepen, 2005).<papid> I05-1015 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1130 ">
while statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. significant subset of statistical realisation work (langkilde, 2002; callaway, 2003; nakanishi et al , 2005; <papid> W05-1510 </papid>zhong and stent, 2005; cahill and van genabith, 2006; <papid> P06-1130 </papid>white and rajkumar, 2009) <papid> D09-1043 </papid>has recently produced results for regenerating the penn treebank.</citsent>
<aftsection>
<nextsent>the basic approach in all this work is to remove information from the penn tree bank parses (the word strings themselves as wellas some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence.
</nextsent>
<nextsent>results are typically evaluated using bleu, and, roughly speaking, bleu scores go down as more information is removed.
</nextsent>
<nextsent>while publications of work along these lines do refer to each other and (tentatively) compare bleu scores, the results are not in fact directly comparable, because of the differences in the input representations automatically derived from penn tree bank annotations.
</nextsent>
<nextsent>in particular, the extent to which they are underspecified varies from one system to the next.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1877">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the halogen/nitrogen work sparked an interest in statistical nlg which led to range of surface realisation methods that used corpus frequencies in one way or another (varges and mellish, 2001; <papid> N01-1001 </papid>white, 2004; velldal et al , 2004; paiva and evans, 2005).<papid> P05-1008 </papid></prevsent>
<prevsent>some surface realisation work looked at directly applying statistical models during linguistically informed generation process to prune the search space (white, 2004; carroll and oepen, 2005).<papid> I05-1015 </papid></prevsent>
</prevsection>
<citsent citstr=" D09-1043 ">
while statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. significant subset of statistical realisation work (langkilde, 2002; callaway, 2003; nakanishi et al , 2005; <papid> W05-1510 </papid>zhong and stent, 2005; cahill and van genabith, 2006; <papid> P06-1130 </papid>white and rajkumar, 2009) <papid> D09-1043 </papid>has recently produced results for regenerating the penn treebank.</citsent>
<aftsection>
<nextsent>the basic approach in all this work is to remove information from the penn tree bank parses (the word strings themselves as wellas some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence.
</nextsent>
<nextsent>results are typically evaluated using bleu, and, roughly speaking, bleu scores go down as more information is removed.
</nextsent>
<nextsent>while publications of work along these lines do refer to each other and (tentatively) compare bleu scores, the results are not in fact directly comparable, because of the differences in the input representations automatically derived from penn tree bank annotations.
</nextsent>
<nextsent>in particular, the extent to which they are underspecified varies from one system to the next.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1878">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to treebank-1 material, treebank-3 contains documents from the switchboard and brown corpora.
</prevsent>
<prevsent>2.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
propbank (palmer et al , 2005): <papid> J05-1004 </papid>this is se-.</citsent>
<aftsection>
<nextsent>mantic annotation of the wall street journal section of penn treebank-2.
</nextsent>
<nextsent>more specifically, each verb occurring in the treebank has been treated as semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate.
</nextsent>
<nextsent>the verbs have also been tagged with coarse grained senses and with inflectional information.
</nextsent>
<nextsent>3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1879">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>the verbs have also been tagged with coarse grained senses and with inflectional information.
</prevsent>
<prevsent>3.
</prevsent>
</prevsection>
<citsent citstr=" W04-0413 ">
nombank 1.0 (meyers et al , 2004): <papid> W04-0413 </papid>nom-.</citsent>
<aftsection>
<nextsent>bank is an annotation project at new york university that provides argument structure for common nouns in the penn treebank.
</nextsent>
<nextsent>nombank marks the sets of arguments that occur with nouns in propbank i, just as the latter records such information for verbs.
</nextsent>
<nextsent>4.
</nextsent>
<nextsent>bbn pronoun coreference and entity type.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1880">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> ontonotes 2.0 (weischedel et al , 2008):.  </section>
<citcontext>
<prevsection>
<prevsent>zettelmoyer and collins (2009) have manually converted the original sql meaning annotations of the atis corpus (et al , 1994)some 4,637 sentences into lambda-calculus expressions which were used for training and testing their semantic parser.
</prevsent>
<prevsent>this resource might make good out-of-domain test set for generation systems trained on wsj data.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
framenet, used for semantic parsing, see for example gildea and jurafsky (2002), <papid> J02-3001 </papid>identifies sentences frame elements and assigns semantic roles to the frame elements.</citsent>
<aftsection>
<nextsent>framenet data (baker and sato, 2003) <papid> P03-2030 </papid>was used for training and test setsin one of the senseval-3 shared tasks in 2004 (au tomatic labeling of semantic roles).</nextsent>
<nextsent>there has been some work combining framenet with other lexical resources.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1881">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> ontonotes 2.0 (weischedel et al , 2008):.  </section>
<citcontext>
<prevsection>
<prevsent>this resource might make good out-of-domain test set for generation systems trained on wsj data.
</prevsent>
<prevsent>framenet, used for semantic parsing, see for example gildea and jurafsky (2002), <papid> J02-3001 </papid>identifies sentences frame elements and assigns semantic roles to the frame elements.</prevsent>
</prevsection>
<citsent citstr=" P03-2030 ">
framenet data (baker and sato, 2003) <papid> P03-2030 </papid>was used for training and test setsin one of the senseval-3 shared tasks in 2004 (au tomatic labeling of semantic roles).</citsent>
<aftsection>
<nextsent>there has been some work combining framenet with other lexical resources.
</nextsent>
<nextsent>for example, shi and mihalcea (2005) integrated framenet with verbnet and wordnet for the purpose of enabling more robust semantic parsing.
</nextsent>
<nextsent>the semlink project (http://verbs.colorado.
</nextsent>
<nextsent>edu/semlink/) aims to integrate propbank, framenet, wordnet and verbnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1882">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> ontonotes 2.0 (weischedel et al , 2008):.  </section>
<citcontext>
<prevsection>
<prevsent>the semlink project (http://verbs.colorado.
</prevsent>
<prevsent>edu/semlink/) aims to integrate propbank, framenet, wordnet and verbnet.
</prevsent>
</prevsection>
<citsent citstr=" P01-1052 ">
other relevant work includes moldovan andrus (moldovan andrus, 2001; <papid> P01-1052 </papid>rus, 2002) who developed technique for parsing into logical forms and used this to transform wordnet concept definitions into logical forms.</citsent>
<aftsection>
<nextsent>the same method (with additional manual correction) was used to produce the test set for another senseval-3 shared task (identification of logic forms in english).
</nextsent>
<nextsent>4.1 conll 2008 shared task data.
</nextsent>
<nextsent>perhaps the most immediately promising resource is is the conll shared task data from 2008 (surdeanu et al , 2008) <papid> W08-2121 </papid>which has syntactic dependency annotations, named-entity boundaries and the semantic dependencies model roles of both verbal and nominal predicates.</nextsent>
<nextsent>the data consist of excerpts from penn treebank-3, bbn pronoun coreference and entity type corpus, propbank and nombank 1.0.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1883">
<title id=" W10-4237.xml">finding common ground towards a surface realisation shared task </title>
<section> ontonotes 2.0 (weischedel et al , 2008):.  </section>
<citcontext>
<prevsection>
<prevsent>the same method (with additional manual correction) was used to produce the test set for another senseval-3 shared task (identification of logic forms in english).
</prevsent>
<prevsent>4.1 conll 2008 shared task data.
</prevsent>
</prevsection>
<citsent citstr=" W08-2121 ">
perhaps the most immediately promising resource is is the conll shared task data from 2008 (surdeanu et al , 2008) <papid> W08-2121 </papid>which has syntactic dependency annotations, named-entity boundaries and the semantic dependencies model roles of both verbal and nominal predicates.</citsent>
<aftsection>
<nextsent>the data consist of excerpts from penn treebank-3, bbn pronoun coreference and entity type corpus, propbank and nombank 1.0.
</nextsent>
<nextsent>in conll 08, the data wasused to train and test systems for the task of producing joint semantic and syntactic dependency analysis of english sentences (the 2009 conll shared task extended this to multi-lingual data).
</nextsent>
<nextsent>it seems feasible that we could reuse the conll data for prototype surface realisation task, adapting it and inversing the direction of the task, i.e. mapping from syntactic-semantic dependency representations to word strings.
</nextsent>
<nextsent>5 developing the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1884">
<title id=" W11-1416.xml">story assembly in a dyslexia fluency tutor </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>in addition we are greatly expanding our rule-base as we instantiate more of the prototype story.
</prevsent>
<prevsent>in the course of this work we will also evaluate moving to more expressive story formalism, such as graessers conceptual graph structures (graesser et al, 1991) which can represent additional relationships between nodes.in addition, we will evaluate improved ways to select the best text from the many options output by the system.
</prevsent>
</prevsection>
<citsent citstr=" N04-1024 ">
rather than simply comparing the number of targeted decoding patterns (as we do now) we will experiment with other evaluation metrics such as cohesion (graesser et al, 2004), or methods which have been useful in essay evaluation (e.g. : (higgins et al, 2004)).<papid> N04-1024 </papid></citsent>
<aftsection>
<nextsent>after sufficient story development, we intend to evaluate the effect of interactive text on students?
</nextsent>
<nextsent>motivation to read.
</nextsent>
<nextsent>this evaluation will collect motivational survey results and voluntary?
</nextsent>
<nextsent>reading times, and compare them between students using interactive and non-interactive versions of the system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1885">
<title id=" W11-1510.xml">crowdsourcing syntactic relatedness judgements for opinion mining in the study of information technology adoption </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>use of these techniques is growing in the areas of computational linguistics and information retrieval, particularly since these fields nowrely on the collection of large datasets for use in machine learning.
</prevsent>
<prevsent>considering the variety of applications, variety of datasets is needed, but trained, known workers are an expense in principle that mustbe furnished for each one.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
consequently, crowdsourcing offers way to collect this data cheaply and quickly (snow et al, 2008; <papid> D08-1027 </papid>sayeed et al, 2010<papid> N10-1051 </papid>a).we applied crowdsourcing to perform the fine grained annotation of domain-specific corpus.</citsent>
<aftsection>
<nextsent>ouruser interface design and our annotator quality control process allows these anonymous workers to perform highly subjective task in manner that correlates their collective understanding of the task to our own expert judgements about it.
</nextsent>
<nextsent>the path to success provides some illustration of the pitfalls inherent in opinion annotation.
</nextsent>
<nextsent>our task is: domain and application-specific sentiment classification at the sub-sentence levelat the word level.
</nextsent>
<nextsent>1.1 opinions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1886">
<title id=" W11-1510.xml">crowdsourcing syntactic relatedness judgements for opinion mining in the study of information technology adoption </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>use of these techniques is growing in the areas of computational linguistics and information retrieval, particularly since these fields nowrely on the collection of large datasets for use in machine learning.
</prevsent>
<prevsent>considering the variety of applications, variety of datasets is needed, but trained, known workers are an expense in principle that mustbe furnished for each one.
</prevsent>
</prevsection>
<citsent citstr=" N10-1051 ">
consequently, crowdsourcing offers way to collect this data cheaply and quickly (snow et al, 2008; <papid> D08-1027 </papid>sayeed et al, 2010<papid> N10-1051 </papid>a).we applied crowdsourcing to perform the fine grained annotation of domain-specific corpus.</citsent>
<aftsection>
<nextsent>ouruser interface design and our annotator quality control process allows these anonymous workers to perform highly subjective task in manner that correlates their collective understanding of the task to our own expert judgements about it.
</nextsent>
<nextsent>the path to success provides some illustration of the pitfalls inherent in opinion annotation.
</nextsent>
<nextsent>our task is: domain and application-specific sentiment classification at the sub-sentence levelat the word level.
</nextsent>
<nextsent>1.1 opinions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1894">
<title id=" W11-1510.xml">crowdsourcing syntactic relatedness judgements for opinion mining in the study of information technology adoption </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our task is: domain and application-specific sentiment classification at the sub-sentence levelat the word level.
</prevsent>
<prevsent>1.1 opinions.
</prevsent>
</prevsection>
<citsent citstr=" W06-0301 ">
for our purposes, we define opinion mining (sometimes known as sentiment analysis) to be there trieval of triple {source, target, opinion} (sayeed et al, 2010<papid> N10-1051 </papid>b; pang and lee, 2008; kim and hovy,2006) <papid> W06-0301 </papid>in which the source is the entity that originated the opinionated language, the target is mention of the entity or concept that is the opinions topic, and the opinion is value (possibly struc ture) that reflects some kind of emotional orientation expressed by the source towards the target.</citsent>
<aftsection>
<nextsent>in much of the recent literature on automatic opinion mining, opinion is at best gradient between positive and negative or binary classification thereof; further complexity affects the reliability of machine-learning techniques (koppel and schler, 2006).
</nextsent>
<nextsent>we call opinion mining fine-grained?
</nextsent>
<nextsent>when we are attempting to retrieve potentially many different 69 {source, target, opinion} triples per document.
</nextsent>
<nextsent>this is particularly challenging when there are multiple triples even at sentence level.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1895">
<title id=" W11-1510.xml">crowdsourcing syntactic relatedness judgements for opinion mining in the study of information technology adoption </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since the goal of our exercise is to ascertain the correlation between the sources behaviour and that of others, then it may be more appropriate to look at opinion analysis with the view that what we are attempting to discover are the views of an aggregate reader who may otherwise have an interest in the it concept in question.
</prevsent>
<prevsent>we thus define an expression of opinion in the following manner:a expresses opinion about if an interested third party cs actions towards may be affected by as textually recorded actions, in context where actions have positive or negative weight.
</prevsent>
</prevsection>
<citsent citstr=" L08-1087 ">
this perspective runs counter to widespread view (ruppenhofer et al, 2008) <papid> L08-1087 </papid>which has assumed treatment of opinionated language as an observation of latent private state?</citsent>
<aftsection>
<nextsent>held by the source.
</nextsent>
<nextsent>this definition reflects the relationship of sentiment and opinion with the study of social impact and marketprediction.
</nextsent>
<nextsent>we return to the question of how to define opinion in section 6.2.
</nextsent>
<nextsent>1.3 crowdsourcing in sentiment analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1896">
<title id=" W11-1510.xml">crowdsourcing syntactic relatedness judgements for opinion mining in the study of information technology adoption </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.3 crowdsourcing in sentiment analysis.
</prevsent>
<prevsent>paid crowdsourcing is relatively new trend in computational linguistics.
</prevsent>
</prevsection>
<citsent citstr=" W09-1904 ">
work exists at the paragraph and document level, and it exists for the twitter and blog genres (hsueh et al, 2009).<papid> W09-1904 </papid>a key problem in crowdsourcing sentiment analysis is the matter of quality control.</citsent>
<aftsection>
<nextsent>a crowd sourced opinion mining task is an attempt to use untrained annotators over task that is inherently very subjective.
</nextsent>
<nextsent>it is doubly difficult for specialized domains,since crowdsourcing platforms have no way of directly recruiting domain experts.hsueh et al (2009) <papid> W09-1904 </papid>present results in quality control over snippets of political blog posts in task classifying them by sentiment and political align ment.</nextsent>
<nextsent>they find that they can use measurement of annotator noise to eliminate low-quality annotation sat this coarse level by re weighting snippet ambiguity scores with noise scores.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1898">
<title id=" W11-1510.xml">crowdsourcing syntactic relatedness judgements for opinion mining in the study of information technology adoption </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, we want to acquire the syntactic relationships between opinion-bearing words and within-sentence targets.
</prevsent>
<prevsent>supervised ml techniques require gold standard data annotated in advance.
</prevsent>
</prevsection>
<citsent citstr=" W05-0308 ">
the multi-perspective question-answering (mpqa) newswire corpus (wilson and wiebe, 2005) <papid> W05-0308 </papid>and the j. d. power &amp; associates (jdpa) automotive review blog post (kessler et al, 2010)corpus are appropriate because both contain sub sentence annotations of sentiment-bearing language as text spans.</citsent>
<aftsection>
<nextsent>in some cases, they also include links to within-sentence targets.
</nextsent>
<nextsent>this is an example of an mpqa annotation: that was the moment at which the fabric of compassion tore, and worlds cracked apart; when the contrast and conflict of civilisational values became so great asto remove any sense of common ground even on which to do battle.
</nextsent>
<nextsent>the italicized portion is intended to reflect negative sentiment about the bolded portion.
</nextsent>
<nextsent>however, whileit is the case that the whole italicized phrase represents negative sentiment, remove?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1899">
<title id=" W11-1510.xml">crowdsourcing syntactic relatedness judgements for opinion mining in the study of information technology adoption </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>74 workers excluded no.
</prevsent>
<prevsent>of words lost (of 48) prec/rec/f acc cohens ? score threshold (prior polarity) n/a 0.87 / 0.38 / 0.53 0.79 -0.26 n/a 0 0 0.64 / 0.71 / 0.67 0.79 0.48 0.333 1 0 0.64 / 0.71 / 0.67 0.79 0.48 0.476 3 0 0.66 / 0.73 / 0.69 0.80 0.51 0.560 5 0 0.69 / 0.73 / 0.71 0.81 0.53 0.674 7 2 0.81 / 0.76 / 0.79 0.86 0.65 0.714 10 9 0.85 / 0.74 / 0.79 0.88 0.54 0.776 12 11 0.68 / 0.68 / 0.68 0.82 0.20 0.820 table 1: results by number of workers excluded from the task.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
the prior polarity baseline comes from lexicon by wilson et al (2005) <papid> H05-1044 </papid>that is not specific to the it domain.</citsent>
<aftsection>
<nextsent>we have been able to show that crowdsourcing avery fine-grained, domain-specific sentiment analysis task with nonstandard, application-specific definition of sentiment is possible with careful user interface design and mutliple layers of quality control.our techniques succeed on two different interpretations of the evaluation measure, and we can reclaim any lost words by re-running the task.
</nextsent>
<nextsent>we used an elaborate processing pipeline before and after annotation in order to accomplish this.
</nextsent>
<nextsent>in this section, we discuss some aspects of the pipeline that led to the success of this technique.
</nextsent>
<nextsent>6.1 quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1900">
<title id=" W11-0903.xml">extraction of semantic word relations in turkish from dictionary definitions </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>over 15,000 authors enter sentences to contribute to the project.
</prevsent>
<prevsent>users can answer questions via the web interface, which aim to fill the gaps in the project.
</prevsent>
</prevsection>
<citsent citstr=" P08-1052 ">
however, in the study of nakov and hearts (2008), <papid> P08-1052 </papid>the whole web is treated like corpus and the occurrences of the noun pairs together are converted into feature vectors to perform classification for semantic relations.</citsent>
<aftsection>
<nextsent>there are various methods under the subject of string or structural patterns that represent specific semantic relations.
</nextsent>
<nextsent>barriere(1997) investigates some syntactical rules in her study and matches the dictionary definitions to these rules for figuring out the relations.
</nextsent>
<nextsent>also, in some languages in which prepositions are used frequently, some relations can be extracted depending on the prepositions, like in the study of celli and nessim (2009).<papid> W09-3707 </papid></nextsent>
<nextsent>in addition, there are some studies which aim to extract some patterns for each relation for the purpose of finding new instances.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1901">
<title id=" W11-0903.xml">extraction of semantic word relations in turkish from dictionary definitions </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>there are various methods under the subject of string or structural patterns that represent specific semantic relations.
</prevsent>
<prevsent>barriere(1997) investigates some syntactical rules in her study and matches the dictionary definitions to these rules for figuring out the relations.
</prevsent>
</prevsection>
<citsent citstr=" W09-3707 ">
also, in some languages in which prepositions are used frequently, some relations can be extracted depending on the prepositions, like in the study of celli and nessim (2009).<papid> W09-3707 </papid></citsent>
<aftsection>
<nextsent>in addition, there are some studies which aim to extract some patterns for each relation for the purpose of finding new instances.
</nextsent>
<nextsent>turneys study (2006) is good example, which uses corpus based method for finding high quality patterns.
</nextsent>
<nextsent>it searches the noun pairs through the corpus to extract some row patterns.
</nextsent>
<nextsent>the patterns are ranked by ranking algorithm in order to determine the most qualified patterns for the further steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1902">
<title id=" W11-0903.xml">extraction of semantic word relations in turkish from dictionary definitions </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>it searches the noun pairs through the corpus to extract some row patterns.
</prevsent>
<prevsent>the patterns are ranked by ranking algorithm in order to determine the most qualified patterns for the further steps.
</prevsent>
</prevsection>
<citsent citstr=" P06-1015 ">
espresso (pantel and pennacchiotti, 2006) <papid> P06-1015 </papid>is also concerned in finding patterns to represent relations.</citsent>
<aftsection>
<nextsent>it starts with few reliable seed of relations and iteratively learns the surface patterns in given corpus.
</nextsent>
<nextsent>there is lot of work to be done for turkish in this area.
</nextsent>
<nextsent>except one project (bilgin et al, 2004), 12 which was performed and limited within the scope of balkan et project, there is no significant work in this area for turkish.
</nextsent>
<nextsent>balkan et project is valuable in the sense of being one of the first attempts for developing turkish wordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1903">
<title id=" W11-0903.xml">extraction of semantic word relations in turkish from dictionary definitions </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>the existing connections can be verified or ranked in terms of their reliability by using such information.
</prevsent>
<prevsent>in addition, to remove erroneous sense determination, word sense disambiguation method can be improved.
</prevsent>
</prevsection>
<citsent citstr=" P06-1040 ">
after obtaining reliable, small network, which will serve as seed, new patterns can be extracted by following turney (2006) <papid> P06-1040 </papid>and by using these patterns more instances can be extracted from larger corpora.</citsent>
<aftsection>
<nextsent>as an alternative, the words can be first tagged with concrete or abstract labels automatically.
</nextsent>
<nextsent>this information can limit the types of connections word can contribute.
</nextsent>
<nextsent>for example, an abstract word cannot connect to another word with part-whole relation.
</nextsent>
<nextsent>for this task, pre-processing step should be applied to classify the words as concrete or abstract.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1904">
<title id=" W11-0818.xml">jmwe a java toolkit for detecting multiword expressions </title>
<section> detection algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>jmwe does not include facilities to do this; tokenization must be done via an external library.
</prevsent>
<prevsent>most detection strategies also require tokens to be tagged with part of speech and lemmatized.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
this information is also not provided directly by jmwe, but there are bindings in the library for using jwi and the stanford pos tagger (toutanova et al, 2003) <papid> N03-1033 </papid>to tag and lemmatize set of texts, provided those texts can be accessed via the jsemcor library.</citsent>
<aftsection>
<nextsent>2.1 detector types.
</nextsent>
<nextsent>mwe token detectors can be split into at least threetypes: basic detectors, filters, and resolvers.
</nextsent>
<nextsent>performance of selected combinations of these detectors are given in table 1.
</nextsent>
<nextsent>basic detectors that fall into this category use anmwe index, or other source of information, to detect mwe tokens in stream of tokens.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1905">
<title id=" W10-4234.xml">the first question generation shared task evaluation challenge </title>
<section> task a: question generation from paragraphs.  </section>
<citcontext>
<prevsection>
<prevsent>paragraphs were selected such that they are self-contained (no need for previous context to be interpreted, e.g. will have no unresolved pronouns) and contain around 5-7 sentences for total of 100-200 tokens (excluding punctuation).
</prevsent>
<prevsent>in addition, we aimed for diversity of topics of general interest.
</prevsent>
</prevsection>
<citsent citstr=" P09-1075 ">
we also provided discourse relations based on hilda, freely available automatic discourse parser (duverle &amp; prendinger, 2009).<papid> P09-1075 </papid></citsent>
<aftsection>
<nextsent>2 task b: question generation from sentences.
</nextsent>
<nextsent>2.1 task definition.
</nextsent>
<nextsent>participants were given set of inputs, with each input consisting of: ? single sentence and ? specific target question type (e.g., who?, why?, how?, when?; see below for the complete list of types used in the challenge).
</nextsent>
<nextsent>for each input, the task was to generate 2 questions of the specified target question type.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1906">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluation isa difficult problem for nlg, and many of the problems identified in this work are relevant for other generation tasks.
</prevsent>
<prevsent>shared tasks are popular in many areas as way to compare system performance in an unbiased manner.
</prevsent>
</prevsection>
<citsent citstr=" W06-1421 ">
unlike other tasks, such as machine translation, there is no shared-task evaluation for compression, even though some compression systems are indirectly evaluated as part of duc.the benefits of shared-task evaluation have been discussed before (e.g., belz and kilgarriff (2006) <papid> W06-1421 </papid>and reiter and belz (2006)), <papid> W06-1422 </papid>and they include comparing systems fairly under the same conditions.</citsent>
<aftsection>
<nextsent>one difficulty in evaluating compression systems fairly is that an unbiased automatic metric is hardto define.
</nextsent>
<nextsent>automatic evaluation relies on comparison to single gold standard at predetermined length, which greatly limits the types of compres sions that can be fairly judged.
</nextsent>
<nextsent>as we will discuss in section 2.1.1, automatic evaluation assumes that deletions are independent, considers only single gold standard, and cannot handle compress ions with paraphrasing.
</nextsent>
<nextsent>like for most areas in nlg, human evaluation is preferable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1907">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluation isa difficult problem for nlg, and many of the problems identified in this work are relevant for other generation tasks.
</prevsent>
<prevsent>shared tasks are popular in many areas as way to compare system performance in an unbiased manner.
</prevsent>
</prevsection>
<citsent citstr=" W06-1422 ">
unlike other tasks, such as machine translation, there is no shared-task evaluation for compression, even though some compression systems are indirectly evaluated as part of duc.the benefits of shared-task evaluation have been discussed before (e.g., belz and kilgarriff (2006) <papid> W06-1421 </papid>and reiter and belz (2006)), <papid> W06-1422 </papid>and they include comparing systems fairly under the same conditions.</citsent>
<aftsection>
<nextsent>one difficulty in evaluating compression systems fairly is that an unbiased automatic metric is hardto define.
</nextsent>
<nextsent>automatic evaluation relies on comparison to single gold standard at predetermined length, which greatly limits the types of compres sions that can be fairly judged.
</nextsent>
<nextsent>as we will discuss in section 2.1.1, automatic evaluation assumes that deletions are independent, considers only single gold standard, and cannot handle compress ions with paraphrasing.
</nextsent>
<nextsent>like for most areas in nlg, human evaluation is preferable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1908">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>argue that comparisons in many previous publications are invalid, ? provide suggestions for unbiased evaluation.
</prevsent>
<prevsent>while many may find this discussion intuitive, these points are not addressed in much of the existing research, and therefore it is crucial to enumerate them in order to improve the scientific validity of the task.
</prevsent>
</prevsection>
<citsent citstr=" E06-1038 ">
because it was developed in support of extractive summarization (knight and marcu, 2000), compression has mostly been framed as deletion task(e.g., mcdonald (2006), <papid> E06-1038 </papid>galanis and androutsopoulos (2010), <papid> N10-1131 </papid>clarke and lapata (2008), and galley 91 words sentence 31 kaczynski faces charges contained in 10-count federal indictment naming him as the person responsible for trans-.</citsent>
<aftsection>
<nextsent>porting bombs and bomb parts from montana to california and mailing them to victims . 17 kaczynski faces charges naming him responsible for transporting bombs to california and mailing them to victims ..
</nextsent>
<nextsent>18 kaczynski faces charges naming him responsible for transporting bombs and bomb parts and mailing them to victims ..
</nextsent>
<nextsent>18 kaczynski faces 10-count federal indictment for transporting bombs and bomb parts and mailing them to victims ..
</nextsent>
<nextsent>table 1: three acceptable compress ions of sentence created by different annotators (the first is the original).and mckeown (2007)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1910">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>argue that comparisons in many previous publications are invalid, ? provide suggestions for unbiased evaluation.
</prevsent>
<prevsent>while many may find this discussion intuitive, these points are not addressed in much of the existing research, and therefore it is crucial to enumerate them in order to improve the scientific validity of the task.
</prevsent>
</prevsection>
<citsent citstr=" N10-1131 ">
because it was developed in support of extractive summarization (knight and marcu, 2000), compression has mostly been framed as deletion task(e.g., mcdonald (2006), <papid> E06-1038 </papid>galanis and androutsopoulos (2010), <papid> N10-1131 </papid>clarke and lapata (2008), and galley 91 words sentence 31 kaczynski faces charges contained in 10-count federal indictment naming him as the person responsible for trans-.</citsent>
<aftsection>
<nextsent>porting bombs and bomb parts from montana to california and mailing them to victims . 17 kaczynski faces charges naming him responsible for transporting bombs to california and mailing them to victims ..
</nextsent>
<nextsent>18 kaczynski faces charges naming him responsible for transporting bombs and bomb parts and mailing them to victims ..
</nextsent>
<nextsent>18 kaczynski faces 10-count federal indictment for transporting bombs and bomb parts and mailing them to victims ..
</nextsent>
<nextsent>table 1: three acceptable compress ions of sentence created by different annotators (the first is the original).and mckeown (2007)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1911">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>these corpora contain one gold standard for each sentence.
</prevsent>
<prevsent>2.1 automatic techniques.
</prevsent>
</prevsection>
<citsent citstr=" N03-1026 ">
one of the most widely used automatic metrics is thef1 measure over grammatical relations of the gold standard compress ions (riezler et al, 2003).<papid> N03-1026 </papid></citsent>
<aftsection>
<nextsent>this metric correlates significantly with human judgments and is better than simple string accuracy (bangalore et al, 2000) <papid> W00-1401 </papid>for judging compression quality (clarke and lapata, 2006).<papid> P06-1048 </papid></nextsent>
<nextsent>f1 has also been used over unigrams (martins and smith, 2009) <papid> W09-1801 </papid>and bigrams (unno et al, 2006).<papid> P06-2109 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1912">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 automatic techniques.
</prevsent>
<prevsent>one of the most widely used automatic metrics is thef1 measure over grammatical relations of the gold standard compress ions (riezler et al, 2003).<papid> N03-1026 </papid></prevsent>
</prevsection>
<citsent citstr=" W00-1401 ">
this metric correlates significantly with human judgments and is better than simple string accuracy (bangalore et al, 2000) <papid> W00-1401 </papid>for judging compression quality (clarke and lapata, 2006).<papid> P06-1048 </papid></citsent>
<aftsection>
<nextsent>f1 has also been used over unigrams (martins and smith, 2009) <papid> W09-1801 </papid>and bigrams (unno et al, 2006).<papid> P06-2109 </papid></nextsent>
<nextsent>unno et al (2006) <papid> P06-2109 </papid>compared the f1 measures to bleu scores (usingthe gold standard as single reference) over varying compression rates, and found that bleu behaves similarly to both f1 measures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1913">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 automatic techniques.
</prevsent>
<prevsent>one of the most widely used automatic metrics is thef1 measure over grammatical relations of the gold standard compress ions (riezler et al, 2003).<papid> N03-1026 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1048 ">
this metric correlates significantly with human judgments and is better than simple string accuracy (bangalore et al, 2000) <papid> W00-1401 </papid>for judging compression quality (clarke and lapata, 2006).<papid> P06-1048 </papid></citsent>
<aftsection>
<nextsent>f1 has also been used over unigrams (martins and smith, 2009) <papid> W09-1801 </papid>and bigrams (unno et al, 2006).<papid> P06-2109 </papid></nextsent>
<nextsent>unno et al (2006) <papid> P06-2109 </papid>compared the f1 measures to bleu scores (usingthe gold standard as single reference) over varying compression rates, and found that bleu behaves similarly to both f1 measures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1914">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>one of the most widely used automatic metrics is thef1 measure over grammatical relations of the gold standard compress ions (riezler et al, 2003).<papid> N03-1026 </papid></prevsent>
<prevsent>this metric correlates significantly with human judgments and is better than simple string accuracy (bangalore et al, 2000) <papid> W00-1401 </papid>for judging compression quality (clarke and lapata, 2006).<papid> P06-1048 </papid></prevsent>
</prevsection>
<citsent citstr=" W09-1801 ">
f1 has also been used over unigrams (martins and smith, 2009) <papid> W09-1801 </papid>and bigrams (unno et al, 2006).<papid> P06-2109 </papid></citsent>
<aftsection>
<nextsent>unno et al (2006) <papid> P06-2109 </papid>compared the f1 measures to bleu scores (usingthe gold standard as single reference) over varying compression rates, and found that bleu behaves similarly to both f1 measures.</nextsent>
<nextsent>a syntactic approach considers the alignment over parse trees (jing, 2000), <papid> A00-1043 </papid>and similar technique has been used with dependency trees to evaluate the quality of sentence fusions (marsi and krahmer, 2005).<papid> W05-1612 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1915">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>one of the most widely used automatic metrics is thef1 measure over grammatical relations of the gold standard compress ions (riezler et al, 2003).<papid> N03-1026 </papid></prevsent>
<prevsent>this metric correlates significantly with human judgments and is better than simple string accuracy (bangalore et al, 2000) <papid> W00-1401 </papid>for judging compression quality (clarke and lapata, 2006).<papid> P06-1048 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-2109 ">
f1 has also been used over unigrams (martins and smith, 2009) <papid> W09-1801 </papid>and bigrams (unno et al, 2006).<papid> P06-2109 </papid></citsent>
<aftsection>
<nextsent>unno et al (2006) <papid> P06-2109 </papid>compared the f1 measures to bleu scores (usingthe gold standard as single reference) over varying compression rates, and found that bleu behaves similarly to both f1 measures.</nextsent>
<nextsent>a syntactic approach considers the alignment over parse trees (jing, 2000), <papid> A00-1043 </papid>and similar technique has been used with dependency trees to evaluate the quality of sentence fusions (marsi and krahmer, 2005).<papid> W05-1612 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1917">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>f1 has also been used over unigrams (martins and smith, 2009) <papid> W09-1801 </papid>and bigrams (unno et al, 2006).<papid> P06-2109 </papid></prevsent>
<prevsent>unno et al (2006) <papid> P06-2109 </papid>compared the f1 measures to bleu scores (usingthe gold standard as single reference) over varying compression rates, and found that bleu behaves similarly to both f1 measures.</prevsent>
</prevsection>
<citsent citstr=" A00-1043 ">
a syntactic approach considers the alignment over parse trees (jing, 2000), <papid> A00-1043 </papid>and similar technique has been used with dependency trees to evaluate the quality of sentence fusions (marsi and krahmer, 2005).<papid> W05-1612 </papid></citsent>
<aftsection>
<nextsent>the only metric that has been shown to correlate with human judgments is f1 (clarke and lapata, 2006), <papid> P06-1048 </papid>but even this is not entirely reliable.</nextsent>
<nextsent>f1 over grammatical relations also depends on parser accuracy and the type of dependency relations used.11for example, the rasp parser uses 16 grammatical depen 2.1.1 pitfalls of automatic evaluation automatic evaluation operates under three often incorrect assumptions: deletions are independent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1918">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>f1 has also been used over unigrams (martins and smith, 2009) <papid> W09-1801 </papid>and bigrams (unno et al, 2006).<papid> P06-2109 </papid></prevsent>
<prevsent>unno et al (2006) <papid> P06-2109 </papid>compared the f1 measures to bleu scores (usingthe gold standard as single reference) over varying compression rates, and found that bleu behaves similarly to both f1 measures.</prevsent>
</prevsection>
<citsent citstr=" W05-1612 ">
a syntactic approach considers the alignment over parse trees (jing, 2000), <papid> A00-1043 </papid>and similar technique has been used with dependency trees to evaluate the quality of sentence fusions (marsi and krahmer, 2005).<papid> W05-1612 </papid></citsent>
<aftsection>
<nextsent>the only metric that has been shown to correlate with human judgments is f1 (clarke and lapata, 2006), <papid> P06-1048 </papid>but even this is not entirely reliable.</nextsent>
<nextsent>f1 over grammatical relations also depends on parser accuracy and the type of dependency relations used.11for example, the rasp parser uses 16 grammatical depen 2.1.1 pitfalls of automatic evaluation automatic evaluation operates under three often incorrect assumptions: deletions are independent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1920">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>for an example, see table 1.
</prevsent>
<prevsent>having multiple gold standards would provide references at different compression lengths andre flect different deletion choices (see section 3).
</prevsent>
</prevsection>
<citsent citstr=" P08-1035 ">
since no large corpus with multiple gold standards exists to our knowledge, systems could instead report thequality of compress ions at several different compression rates, as nomoto (2008) <papid> P08-1035 </papid>did.</citsent>
<aftsection>
<nextsent>alternatively, systems could evaluate compress ions that are of similar length as the gold standard compression, to fix length for the purpose of evaluation.
</nextsent>
<nextsent>output length is controlled for evaluation in some other areas, notably duc.systems compress by deletion and not substitution.
</nextsent>
<nextsent>more recent approaches to compression introduce reordering and paraphrase operations (e.g.,dencies (briscoe, 2006) while there are over 50 stanford dependencies (de marneffe and manning, 2008).
</nextsent>
<nextsent>92 cohn and lapata (2008), <papid> C08-1018 </papid>woodsend et al (2010),<papid> D10-1050 </papid>and napoles et al (2011)).<papid> W11-1610 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1922">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>output length is controlled for evaluation in some other areas, notably duc.systems compress by deletion and not substitution.
</prevsent>
<prevsent>more recent approaches to compression introduce reordering and paraphrase operations (e.g.,dencies (briscoe, 2006) while there are over 50 stanford dependencies (de marneffe and manning, 2008).
</prevsent>
</prevsection>
<citsent citstr=" C08-1018 ">
92 cohn and lapata (2008), <papid> C08-1018 </papid>woodsend et al (2010),<papid> D10-1050 </papid>and napoles et al (2011)).<papid> W11-1610 </papid></citsent>
<aftsection>
<nextsent>for paraphrastic compress ions, manual evaluation alone reliably determines the compression quality.
</nextsent>
<nextsent>because automatic evaluation metrics compare shortened sentences to extractive gold standards, they cannot be applied to paraphrastic compression.to apply automatic techniques to substitution based compression, one would need gold-standard set of paraphrastic compressions.
</nextsent>
<nextsent>these are rare.cohn and lapata (2008) <papid> C08-1018 </papid>created an abs tractive corpus, which contains word reordering and paraphrasing in addition to deletion.</nextsent>
<nextsent>unfortunately, this corpus is small (575 sentences) and only includes one possible compression for each sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1923">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>output length is controlled for evaluation in some other areas, notably duc.systems compress by deletion and not substitution.
</prevsent>
<prevsent>more recent approaches to compression introduce reordering and paraphrase operations (e.g.,dencies (briscoe, 2006) while there are over 50 stanford dependencies (de marneffe and manning, 2008).
</prevsent>
</prevsection>
<citsent citstr=" D10-1050 ">
92 cohn and lapata (2008), <papid> C08-1018 </papid>woodsend et al (2010),<papid> D10-1050 </papid>and napoles et al (2011)).<papid> W11-1610 </papid></citsent>
<aftsection>
<nextsent>for paraphrastic compress ions, manual evaluation alone reliably determines the compression quality.
</nextsent>
<nextsent>because automatic evaluation metrics compare shortened sentences to extractive gold standards, they cannot be applied to paraphrastic compression.to apply automatic techniques to substitution based compression, one would need gold-standard set of paraphrastic compressions.
</nextsent>
<nextsent>these are rare.cohn and lapata (2008) <papid> C08-1018 </papid>created an abs tractive corpus, which contains word reordering and paraphrasing in addition to deletion.</nextsent>
<nextsent>unfortunately, this corpus is small (575 sentences) and only includes one possible compression for each sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1924">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>output length is controlled for evaluation in some other areas, notably duc.systems compress by deletion and not substitution.
</prevsent>
<prevsent>more recent approaches to compression introduce reordering and paraphrase operations (e.g.,dencies (briscoe, 2006) while there are over 50 stanford dependencies (de marneffe and manning, 2008).
</prevsent>
</prevsection>
<citsent citstr=" W11-1610 ">
92 cohn and lapata (2008), <papid> C08-1018 </papid>woodsend et al (2010),<papid> D10-1050 </papid>and napoles et al (2011)).<papid> W11-1610 </papid></citsent>
<aftsection>
<nextsent>for paraphrastic compress ions, manual evaluation alone reliably determines the compression quality.
</nextsent>
<nextsent>because automatic evaluation metrics compare shortened sentences to extractive gold standards, they cannot be applied to paraphrastic compression.to apply automatic techniques to substitution based compression, one would need gold-standard set of paraphrastic compressions.
</nextsent>
<nextsent>these are rare.cohn and lapata (2008) <papid> C08-1018 </papid>created an abs tractive corpus, which contains word reordering and paraphrasing in addition to deletion.</nextsent>
<nextsent>unfortunately, this corpus is small (575 sentences) and only includes one possible compression for each sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1928">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>decisions are rated along 5-point scale (ldc, 2005).
</prevsent>
<prevsent>most compression systems consider sentences out of context (a few exceptions exist, e.g., daume?
</prevsent>
</prevsection>
<citsent citstr=" W03-1101 ">
iii and marcu (2002), martins and smith (2009), <papid> W09-1801 </papid>andlin (2003)).<papid> W03-1101 </papid></citsent>
<aftsection>
<nextsent>contextual cues and discourse structure may not be factor to consider if the sentences are generated for use out of context.
</nextsent>
<nextsent>an example of context-aware approach considered the summaries formed by shortened sentences and evaluated the compression systems based on how well people could answer questions about the original document from the summaries (clarke and lapata, 2007).<papid> D07-1001 </papid></nextsent>
<nextsent>this technique has been used before for evaluating summarization and text comprehension (mani et al, 2002; morris et al, 1992).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1929">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> current practices.  </section>
<citcontext>
<prevsection>
<prevsent>iii and marcu (2002), martins and smith (2009), <papid> W09-1801 </papid>andlin (2003)).<papid> W03-1101 </papid></prevsent>
<prevsent>contextual cues and discourse structure may not be factor to consider if the sentences are generated for use out of context.</prevsent>
</prevsection>
<citsent citstr=" D07-1001 ">
an example of context-aware approach considered the summaries formed by shortened sentences and evaluated the compression systems based on how well people could answer questions about the original document from the summaries (clarke and lapata, 2007).<papid> D07-1001 </papid></citsent>
<aftsection>
<nextsent>this technique has been used before for evaluating summarization and text comprehension (mani et al, 2002; morris et al, 1992).
</nextsent>
<nextsent>2.2.1 pitfalls of manual evaluation grammar judgments decrease when the compression is presented alongside the original sentence.
</nextsent>
<nextsent>figure 1 shows that the mean grammar rating for the same compress ions is on average about 0.3 points higher when the compression is judged in isolation.researchers should be careful to state when grammar is judged on compress ions lacking reference sentences.another factor is the group of judges.
</nextsent>
<nextsent>obviously different studies will relyon different judges,so whenever possible the sentences from an existing model should be re-evaluated alongside the new model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1930">
<title id=" W11-1611.xml">evaluating sentence compression pitfalls and suggested remedies </title>
<section> mismatched comparisons.  </section>
<citcontext>
<prevsection>
<prevsent>we have observed that difference in compression rates as small as 5 percentage points can influence the quality ratings by as much as 0.1 points andconclude: systems must be compared using similar levels of compression.
</prevsent>
<prevsent>in particular, if system as output is higher quality, but longer than system bs, then it is not necessarily the case that is better than b. conversely, if has results at least as good as system a, one can claim that is better, since bs output is shorter.here are some examples in the literature of mismatched comparisons:?
</prevsent>
</prevsection>
<citsent citstr=" D09-1041 ">
nomoto (2009) <papid> D09-1041 </papid>concluded their system significantly outperformed that of cohn and lapata (2008).<papid> C08-1018 </papid></citsent>
<aftsection>
<nextsent>however, the compression rate of their system ranged from 45 to 74, while the compression rate of cohn and lapata (2008) <papid> C08-1018 </papid>was 35.</nextsent>
<nextsent>this claim is unverifiable without further.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1943">
<title id=" W11-1210.xml">active learning with multiple annotations for comparable data classification task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such corpora needs to be carefully created by language experts or speakers, which makes building mt systems feasible only for those language pairs with sufficient public interest or financial support.
</prevsent>
<prevsent>with the increasing rate of social media creation and the quick growth of web media in languages other than english makes it relevant for language research community to explore the feasibility of internet as source for parallel data.
</prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
(resnik and smith, 2003) <papid> J03-3002 </papid>show that parallel corpora for variety of languages can be harvested on the internet.</citsent>
<aftsection>
<nextsent>it is to be observed that major portion of the multilingual web documents are created independent of one an other and so are only mildly parallel at the document level.there are multiple challenges in building comparable corpora for consumption by the mt systems.the first challenge is to identify the parallelism between documents of different languages which has been reliably done using cross lingual information retrieval techniques.
</nextsent>
<nextsent>once we have identified subset of documents that are potentially parallel, the second challenge is to identify comparable sentencepairs.
</nextsent>
<nextsent>this is an interesting challenge as the availability of completely parallel sentences on the internet is quite low in most language-pairs, but one can observe very few comparable sentences among comparable documents forgiven language-pair.
</nextsent>
<nextsent>our work tries to address this problem by posing the identification of comparable sentences from comparable data as supervised classification problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1944">
<title id=" W11-1210.xml">active learning with multiple annotations for comparable data classification task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is an interesting challenge as the availability of completely parallel sentences on the internet is quite low in most language-pairs, but one can observe very few comparable sentences among comparable documents forgiven language-pair.
</prevsent>
<prevsent>our work tries to address this problem by posing the identification of comparable sentences from comparable data as supervised classification problem.
</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
unlike earlier research (munteanu and marcu, 2005) <papid> J05-4003 </papid>where the authors try to identify parallel sentences among pool of comparable documents, we try to first identify comparable sentences in pool with dominantly non-parallel sentences.</citsent>
<aftsection>
<nextsent>we then builda supervised classifier that learns from user annotations for comparable corpora identification.
</nextsent>
<nextsent>training such classifier requires reliably annotated data that may be unavailable for low-resource language pairs.
</nextsent>
<nextsent>involving human expert to perform such annotations is expensive for low-resource language sand so we propose active learning as suitable technique to reduce the labeling effort.
</nextsent>
<nextsent>there is yet one other issue that needs to be solved in order for our classification based approach to work for truly low-resource language pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1948">
<title id=" W11-1210.xml">active learning with multiple annotations for comparable data classification task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is yet one other issue that needs to be solved in order for our classification based approach to work for truly low-resource language pairs.
</prevsent>
<prevsent>as wewill describe later in the paper, our comparable sentence classifier relies on the availability of an ini 69 proceedings of the 4th workshop on building and using comparable corpora, pages 6977, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
c2011 association for computational linguistics tial seed lexicon that can either be provided by human or can be statistically trained from parallel corpora (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>experiments show that abroad coverage lexicon provides us with better coverage for effective identification of comparable corpora.
</nextsent>
<nextsent>however, availability of such resource can not be expected in very low-resource language pairs, or even if present may not be of good quality.
</nextsent>
<nextsent>this opens an interesting research question - can we also elicit such information effectively at low costs?
</nextsent>
<nextsent>we propose active learning strategies for identifying the most informative comparable sentence pairs which human can then extract parallel segments from.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1949">
<title id=" W11-1210.xml">active learning with multiple annotations for comparable data classification task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>section 6 presents experimental results and the effectiveness of the active learning strategies.
</prevsent>
<prevsent>we conclude with further discussion and future work.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
there has been lot of interest in using comparable corpora for mt, primarily on extracting parallel sentence pairs from comparable sources (zhao and vogel, 2002; fung and yee, 1998).<papid> P98-1069 </papid></citsent>
<aftsection>
<nextsent>some workhas gone beyond this focussing on extracting sub sentential fragments from noisier comparable data (munteanu and marcu, 2006; <papid> P06-1011 </papid>quirk et al, 2007).the research conducted in this paper has two primary contributions and so we will discuss the related work as relevant to each of them.our first contribution in this paper is the application of active learning for acquiring comparable data in the low-resource scenario, especially relevant when working with low-resource languages.</nextsent>
<nextsent>there is some earlier work highlighting the needfor techniques to deal with low-resource scenarios.(munteanu and marcu, 2005) <papid> J05-4003 </papid>propose bootstrapping using an existing classifier for collecting new data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1950">
<title id=" W11-1210.xml">active learning with multiple annotations for comparable data classification task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude with further discussion and future work.
</prevsent>
<prevsent>there has been lot of interest in using comparable corpora for mt, primarily on extracting parallel sentence pairs from comparable sources (zhao and vogel, 2002; fung and yee, 1998).<papid> P98-1069 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1011 ">
some workhas gone beyond this focussing on extracting sub sentential fragments from noisier comparable data (munteanu and marcu, 2006; <papid> P06-1011 </papid>quirk et al, 2007).the research conducted in this paper has two primary contributions and so we will discuss the related work as relevant to each of them.our first contribution in this paper is the application of active learning for acquiring comparable data in the low-resource scenario, especially relevant when working with low-resource languages.</citsent>
<aftsection>
<nextsent>there is some earlier work highlighting the needfor techniques to deal with low-resource scenarios.(munteanu and marcu, 2005) <papid> J05-4003 </papid>propose bootstrapping using an existing classifier for collecting new data.</nextsent>
<nextsent>however, this approach works when there isa classifier of reasonable performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1955">
<title id=" W11-1210.xml">active learning with multiple annotations for comparable data classification task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a relevant setup is multi task learning (caruana, 1997) which is increasingly becoming popular in natural language processing for learning from multiple learning tasks.
</prevsent>
<prevsent>there has been very less work in the area of multi task active learning.
</prevsent>
</prevsection>
<citsent citstr=" P08-1098 ">
(reichart et al, 2008) <papid> P08-1098 </papid>proposes an extension of the single-sided active elicitation task to multi-task scenario, where data elicitation is performed for two or more independent tasks at the same time.</citsent>
<aftsection>
<nextsent>(settles et al, 2008) propose elicitation of annotations for image segmentation under multi-instance learning framework.
</nextsent>
<nextsent>active learning with multiple annotations also has similarities to the recent body of work in learning from instance feedback and feature feedback(melville et al, 2005).
</nextsent>
<nextsent>(druck et al, 2009) <papid> D09-1009 </papid>propose active learning extensions to the gradient approach of learning from feature and instance feed back.</nextsent>
<nextsent>however, in the comparable corpora problem although the second annotation is geared towards learning better features by enhancing the coverage of the lexicon, the annotation itself is not on the features but for extracting training data that is then used to train the lexicon.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1956">
<title id=" W11-1210.xml">active learning with multiple annotations for comparable data classification task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(settles et al, 2008) propose elicitation of annotations for image segmentation under multi-instance learning framework.
</prevsent>
<prevsent>active learning with multiple annotations also has similarities to the recent body of work in learning from instance feedback and feature feedback(melville et al, 2005).
</prevsent>
</prevsection>
<citsent citstr=" D09-1009 ">
(druck et al, 2009) <papid> D09-1009 </papid>propose active learning extensions to the gradient approach of learning from feature and instance feed back.</citsent>
<aftsection>
<nextsent>however, in the comparable corpora problem although the second annotation is geared towards learning better features by enhancing the coverage of the lexicon, the annotation itself is not on the features but for extracting training data that is then used to train the lexicon.
</nextsent>
<nextsent>70
</nextsent>
<nextsent>classification in this section we discuss our supervised training setup and the classification algorithm.
</nextsent>
<nextsent>our classifier tries to identify comparable sentences from among alarge pool of noisy comparable sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1966">
<title id=" W11-0119.xml">the exploitation of spatial information in narrative discourse </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>rhetorical relations describe the role that one clause plays with respect to another in text and contributes to texts coherence (hobbs, 1985).
</prevsent>
<prevsent>as such, these relations are pragmatic features of text.
</prevsent>
</prevsection>
<citsent citstr=" J00-3005 ">
in nlp generally, classifying rhetorical relations has been an important area of research (marcu, 2000; <papid> J00-3005 </papid>sporleder and lascarides, 2005) and has been shown to be useful for tasks such as text summarization (marcu, 1998).</citsent>
<aftsection>
<nextsent>the inventory of rhetorical relations in segmented discourse representation theory (sdrt) (asher and lascarides, 2003) is widely used in these applications.
</nextsent>
<nextsent>this inventory includes the following relations, illustrated by example: narration: klose got up.
</nextsent>
<nextsent>he entered the game.
</nextsent>
<nextsent>elaboration: klose pushed the serbian midfielder.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1967">
<title id=" W11-0119.xml">the exploitation of spatial information in narrative discourse </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>continuation: klose received red card.
</prevsent>
<prevsent>ronaldo received yellow card.
</prevsent>
</prevsection>
<citsent citstr=" P02-1047 ">
in previous work, rhetorical relations have been predicted based on range of features including discourse connectives, relation location, clause length, part-of-speech, content and function words, and syntactic features (marcu and echihabi, 2002; <papid> P02-1047 </papid>lapata and lascarides, 2004).<papid> N04-1020 </papid></citsent>
<aftsection>
<nextsent>these systems have wide range of average accuracies for all relations sought to be predicted - e.g. 33.96% (marcu and echihabi, 2002) <papid> P02-1047 </papid>to 70.70% (lapata and lascarides, 2004) - <papid> N04-1020 </papid>and individual relations - e.g. result - 16.21% and explanation - 75.39% (marcu and echihabi, 2002) <papid> P02-1047 </papid>and contrast - 43.64% and continuation 83.35% (sporleder and lascarides, 2005).</nextsent>
<nextsent>our focus is on the narration, background and elaboration relations, which account for over 90% of the discourses in our corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1968">
<title id=" W11-0119.xml">the exploitation of spatial information in narrative discourse </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>continuation: klose received red card.
</prevsent>
<prevsent>ronaldo received yellow card.
</prevsent>
</prevsection>
<citsent citstr=" N04-1020 ">
in previous work, rhetorical relations have been predicted based on range of features including discourse connectives, relation location, clause length, part-of-speech, content and function words, and syntactic features (marcu and echihabi, 2002; <papid> P02-1047 </papid>lapata and lascarides, 2004).<papid> N04-1020 </papid></citsent>
<aftsection>
<nextsent>these systems have wide range of average accuracies for all relations sought to be predicted - e.g. 33.96% (marcu and echihabi, 2002) <papid> P02-1047 </papid>to 70.70% (lapata and lascarides, 2004) - <papid> N04-1020 </papid>and individual relations - e.g. result - 16.21% and explanation - 75.39% (marcu and echihabi, 2002) <papid> P02-1047 </papid>and contrast - 43.64% and continuation 83.35% (sporleder and lascarides, 2005).</nextsent>
<nextsent>our focus is on the narration, background and elaboration relations, which account for over 90% of the discourses in our corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1974">
<title id=" W11-0119.xml">the exploitation of spatial information in narrative discourse </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>we follow asher and sablayrolles (1995) who classify prepositions based on the position (position - at, initial direction - from, medial position - through, final position - to) and contact (inner - in, contact - against, outer 176- along, and outer-most - beyond) of two regions (figure and ground).
</prevsent>
<prevsent>for verbs, muller (2002) proposes six mereotopological classes: reach, leave, internal, external, hit, and cross.
</prevsent>
</prevsection>
<citsent citstr=" C08-2024 ">
pustejovsky and moszkowicz (2008) <papid> C08-2024 </papid>mapped mullers classes to framenet and verbnet and propose ten general classes of motion (move, move-external, move-internal, leave, reach, detach, hit, follow, deviate, stay).</citsent>
<aftsection>
<nextsent>third, figure and ground relationships vary by the perspective used to describe the relationship.
</nextsent>
<nextsent>for this discussion, perspective takes two forms, granularity of spatial description (following mon tello (1993)) and frames of reference (following levinson (1996)).
</nextsent>
<nextsent>granularity refers to the level of detail in given spatial description.
</nextsent>
<nextsent>mon tello (1993, p. 315) indicates four spatial granularities based on the cognitive organization of spatial knowledge (summarized in (4)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1978">
<title id=" W11-1807.xml">robust biomedical event extraction with dual decomposition and minimal domain adaptation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results are very competitive: with minimal adaptation of our model we come in second for two of the tasks right behind version of the system presented here that includes predictions of the stanford event extractor as features.
</prevsent>
<prevsent>we also show that for the infectious diseases task using data from the genia track is very effective way to improve accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W11-1802 ">
this paper presents the umass entry to the bionlp 2011 shared task (kim et al, 2011<papid> W11-1802 </papid>a).</citsent>
<aftsection>
<nextsent>we introduce simple joint model for the extraction of biomedical events, and show competitive results for four tracks of the competition.
</nextsent>
<nextsent>our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings.
</nextsent>
<nextsent>fast and accurate joint inference is provided by combining optimizing methods for these three sub models via dual decomposition (komodakis et al,2007; rush et al, 2010).<papid> D10-1001 </papid></nextsent>
<nextsent>notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same bindingevent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1982">
<title id=" W11-1807.xml">robust biomedical event extraction with dual decomposition and minimal domain adaptation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we introduce simple joint model for the extraction of biomedical events, and show competitive results for four tracks of the competition.
</prevsent>
<prevsent>our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings.
</prevsent>
</prevsection>
<citsent citstr=" D10-1001 ">
fast and accurate joint inference is provided by combining optimizing methods for these three sub models via dual decomposition (komodakis et al,2007; rush et al, 2010).<papid> D10-1001 </papid></citsent>
<aftsection>
<nextsent>notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same bindingevent.
</nextsent>
<nextsent>so far this has either been done through postprocessing heuristics (bjrne et al, 2009; riedel et al., 2009; <papid> W09-1406 </papid>poon and vanderwende, 2010), <papid> N10-1123 </papid>or through local classifier at the end of pipeline (miwa et al, 2010).</nextsent>
<nextsent>our model is very competitive.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1983">
<title id=" W11-1807.xml">robust biomedical event extraction with dual decomposition and minimal domain adaptation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fast and accurate joint inference is provided by combining optimizing methods for these three sub models via dual decomposition (komodakis et al,2007; rush et al, 2010).<papid> D10-1001 </papid></prevsent>
<prevsent>notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same bindingevent.</prevsent>
</prevsection>
<citsent citstr=" W09-1406 ">
so far this has either been done through postprocessing heuristics (bjrne et al, 2009; riedel et al., 2009; <papid> W09-1406 </papid>poon and vanderwende, 2010), <papid> N10-1123 </papid>or through local classifier at the end of pipeline (miwa et al, 2010).</citsent>
<aftsection>
<nextsent>our model is very competitive.
</nextsent>
<nextsent>for genia (ge)task 1 (kim et al, 2011<papid> W11-1802 </papid>b) we achieve the second best results.</nextsent>
<nextsent>in addition, the best-performing faust system (riedel et al, 2011) <papid> W11-1808 </papid>is variant of the model presented here.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1984">
<title id=" W11-1807.xml">robust biomedical event extraction with dual decomposition and minimal domain adaptation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fast and accurate joint inference is provided by combining optimizing methods for these three sub models via dual decomposition (komodakis et al,2007; rush et al, 2010).<papid> D10-1001 </papid></prevsent>
<prevsent>notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same bindingevent.</prevsent>
</prevsection>
<citsent citstr=" N10-1123 ">
so far this has either been done through postprocessing heuristics (bjrne et al, 2009; riedel et al., 2009; <papid> W09-1406 </papid>poon and vanderwende, 2010), <papid> N10-1123 </papid>or through local classifier at the end of pipeline (miwa et al, 2010).</citsent>
<aftsection>
<nextsent>our model is very competitive.
</nextsent>
<nextsent>for genia (ge)task 1 (kim et al, 2011<papid> W11-1802 </papid>b) we achieve the second best results.</nextsent>
<nextsent>in addition, the best-performing faust system (riedel et al, 2011) <papid> W11-1808 </papid>is variant of the model presented here.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1989">
<title id=" W11-1807.xml">robust biomedical event extraction with dual decomposition and minimal domain adaptation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our model is very competitive.
</prevsent>
<prevsent>for genia (ge)task 1 (kim et al, 2011<papid> W11-1802 </papid>b) we achieve the second best results.</prevsent>
</prevsection>
<citsent citstr=" W11-1808 ">
in addition, the best-performing faust system (riedel et al, 2011) <papid> W11-1808 </papid>is variant of the model presented here.</citsent>
<aftsection>
<nextsent>its advantage stems from the fact that it uses predictions of the stanford system (mc closky et al, 2011<papid> P11-1163 </papid>a; mcclosky et al, 2011<papid> P11-1163 </papid>b), and hence performs model combination.</nextsent>
<nextsent>the same holds for the infectious diseases (id) track (pyysalo et al, 2011), <papid> W11-1804 </papid>where we come in as second right behind the faust system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1990">
<title id=" W11-1807.xml">robust biomedical event extraction with dual decomposition and minimal domain adaptation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for genia (ge)task 1 (kim et al, 2011<papid> W11-1802 </papid>b) we achieve the second best results.</prevsent>
<prevsent>in addition, the best-performing faust system (riedel et al, 2011) <papid> W11-1808 </papid>is variant of the model presented here.</prevsent>
</prevsection>
<citsent citstr=" P11-1163 ">
its advantage stems from the fact that it uses predictions of the stanford system (mc closky et al, 2011<papid> P11-1163 </papid>a; mcclosky et al, 2011<papid> P11-1163 </papid>b), and hence performs model combination.</citsent>
<aftsection>
<nextsent>the same holds for the infectious diseases (id) track (pyysalo et al, 2011), <papid> W11-1804 </papid>where we come in as second right behind the faust system.</nextsent>
<nextsent>for the epi genetics and post translational modifications (epi) track (ohta et al, 2011) <papid> W11-1803 </papid>we achieve the 4th rank, partly because we did not aim to extract speculations, neg ations or cellular locations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1992">
<title id=" W11-1807.xml">robust biomedical event extraction with dual decomposition and minimal domain adaptation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, the best-performing faust system (riedel et al, 2011) <papid> W11-1808 </papid>is variant of the model presented here.</prevsent>
<prevsent>its advantage stems from the fact that it uses predictions of the stanford system (mc closky et al, 2011<papid> P11-1163 </papid>a; mcclosky et al, 2011<papid> P11-1163 </papid>b), and hence performs model combination.</prevsent>
</prevsection>
<citsent citstr=" W11-1804 ">
the same holds for the infectious diseases (id) track (pyysalo et al, 2011), <papid> W11-1804 </papid>where we come in as second right behind the faust system.</citsent>
<aftsection>
<nextsent>for the epi genetics and post translational modifications (epi) track (ohta et al, 2011) <papid> W11-1803 </papid>we achieve the 4th rank, partly because we did not aim to extract speculations, neg ations or cellular locations.</nextsent>
<nextsent>finally, for genia task 2 we rank 3rd?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1993">
<title id=" W11-1807.xml">robust biomedical event extraction with dual decomposition and minimal domain adaptation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>its advantage stems from the fact that it uses predictions of the stanford system (mc closky et al, 2011<papid> P11-1163 </papid>a; mcclosky et al, 2011<papid> P11-1163 </papid>b), and hence performs model combination.</prevsent>
<prevsent>the same holds for the infectious diseases (id) track (pyysalo et al, 2011), <papid> W11-1804 </papid>where we come in as second right behind the faust system.</prevsent>
</prevsection>
<citsent citstr=" W11-1803 ">
for the epi genetics and post translational modifications (epi) track (ohta et al, 2011) <papid> W11-1803 </papid>we achieve the 4th rank, partly because we did not aim to extract speculations, neg ations or cellular locations.</citsent>
<aftsection>
<nextsent>finally, for genia task 2 we rank 3rd?
</nextsent>
<nextsent>with the 1st rank achieved by the faust system.
</nextsent>
<nextsent>in the following we will briefly describe our model and inference algorithm, as far as this is possible in limited space.
</nextsent>
<nextsent>then we show our results on the three tasks and conclude.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1998">
<title id=" W11-1807.xml">robust biomedical event extraction with dual decomposition and minimal domain adaptation </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>after basic tokenization and sentence segmentation, we generate set of protein head tokens prot (x)for each sentence based on protein span definitions from the shared task.
</prevsent>
<prevsent>to ensure tokens contain not more than one protein we split them at protein boundaries.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
parsing is performed using the charniak-johnson parser (charniak and johnson, 2005) <papid> P05-1022 </papid>with the self-trained biomedical parsing 1we refer to koo et al (2010) for details on how to set t. 48 svt bind reg tot task 1 73.5 48.8 43.8 55.2 task 1 (abst.)</citsent>
<aftsection>
<nextsent>71.5 50.8 45.5 56.1 task 1 (full) 79.2 44.4 40.1 53.1 task 2 71.4 38.6 39.1 51.0 table 1: results for the ge track, task 1 and 2; abst.=abstract; full=full text.
</nextsent>
<nextsent>model of mcclosky and charniak (2008).<papid> P08-2026 </papid></nextsent>
<nextsent>finally, based on the set of trigger words in the training data, we generate set of candidate triggers trig (x).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH1999">
<title id=" W11-1807.xml">robust biomedical event extraction with dual decomposition and minimal domain adaptation </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>parsing is performed using the charniak-johnson parser (charniak and johnson, 2005) <papid> P05-1022 </papid>with the self-trained biomedical parsing 1we refer to koo et al (2010) for details on how to set t. 48 svt bind reg tot task 1 73.5 48.8 43.8 55.2 task 1 (abst.)</prevsent>
<prevsent>71.5 50.8 45.5 56.1 task 1 (full) 79.2 44.4 40.1 53.1 task 2 71.4 38.6 39.1 51.0 table 1: results for the ge track, task 1 and 2; abst.=abstract; full=full text.</prevsent>
</prevsection>
<citsent citstr=" P08-2026 ">
model of mcclosky and charniak (2008).<papid> P08-2026 </papid></citsent>
<aftsection>
<nextsent>finally, based on the set of trigger words in the training data, we generate set of candidate triggers trig (x).
</nextsent>
<nextsent>we apply the same model to the ge, id and epi tracks, with minor modifications in order to deal with the different event type sets and role sets rof each track.
</nextsent>
<nextsent>training and testing together took between 30 (epi) to 120 (ge) minutes using single core implementation.
</nextsent>
<nextsent>4.1 genia.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2000">
<title id=" W11-1410.xml">developing methodology for korean particle error detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our evaluation shows that, while the data selection is effective, there is much work to be done with preprocessing and system optimization.
</prevsent>
<prevsent>a growing area of research in analyzing learner language is to detect errors in function words, namely categories such as prepositions and articles (see leacock et al, 2010, and references therein).
</prevsent>
</prevsection>
<citsent citstr=" W10-1502 ">
this work has mostly been for english, and there are issues,such as greater morphological complexity, in moving to other languages (see, e.g., de ilarraza et al, 2008; dickinson et al, 2010).<papid> W10-1502 </papid></citsent>
<aftsection>
<nextsent>our goal is to build ama chine learning system for detecting errors in post positional particles in korean, significant source of learner errors (ko et al, 2004; lee et al, 2009b).
</nextsent>
<nextsent>korean post positional particles are morphemes that attach to preceding nominal to indicate rangeof linguistic functions, including grammatical functions, e.g., subject and object; semantic roles; and discourse functions.
</nextsent>
<nextsent>in (1), for instance, ka marks the subject (function) and agent (semantic role).1 similar to english prepositions, particles can also have modifier functions, adding meanings of time, location, instrument, possession, and so forth.
</nextsent>
<nextsent>1we use the yale roman ization scheme for writing korean.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2003">
<title id=" W11-1410.xml">developing methodology for korean particle error detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1) sumi-ka sumi-sbj john-uy john-gen cip-eyse house-loc ku-lul he-obj twu two sikan-ul hours-obj kitaly-ess-ta.
</prevsent>
<prevsent>wait-past-end sumi waited for john for (the whole) two hours in his house.?
</prevsent>
</prevsection>
<citsent citstr=" W07-1604 ">
we treat the task of particle error detection asone of particle selection, and we use machine learning because it has proven effective in similar tasks for other languages (e.g., chodorow et al, 2007; <papid> W07-1604 </papid>oyama, 2010).</citsent>
<aftsection>
<nextsent>training on corpus of well-formedkorean, we predict which particle should appear after given nominal; if this is different from the learners, we have detected an error.
</nextsent>
<nextsent>using machine learner has the advantage of being able to perform well without researcher having to specify rules, especially with the complex set of linguistic relationships motivating particle selection.2 we build from dickinson et al (2010) <papid> W10-1502 </papid>in two main ways: first, we implement presence-selectionpipeline that has proven effective for english preposition error detection (cf.</nextsent>
<nextsent>gamon et al, 2008).<papid> I08-1059 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2006">
<title id=" W11-1410.xml">developing methodology for korean particle error detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>training on corpus of well-formedkorean, we predict which particle should appear after given nominal; if this is different from the learners, we have detected an error.
</prevsent>
<prevsent>using machine learner has the advantage of being able to perform well without researcher having to specify rules, especially with the complex set of linguistic relationships motivating particle selection.2 we build from dickinson et al (2010) <papid> W10-1502 </papid>in two main ways: first, we implement presence-selectionpipeline that has proven effective for english preposition error detection (cf.</prevsent>
</prevsection>
<citsent citstr=" I08-1059 ">
gamon et al, 2008).<papid> I08-1059 </papid></citsent>
<aftsection>
<nextsent>as the task is under studied, the work is preliminary, butit nonetheless is able to highlight the primary areas of focus for future work.
</nextsent>
<nextsent>secondly, we improve upon the training data, in particular doing better job of selecting relevant instances for the machine learner.
</nextsent>
<nextsent>obtaining better-quality training data is major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (gamon, 2010).<papid> N10-1019 </papid></nextsent>
<nextsent>2see dickinson and lee (2009); de ilarraza et al (2008); oyama (2010) for related work in other languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2008">
<title id=" W11-1410.xml">developing methodology for korean particle error detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as the task is under studied, the work is preliminary, butit nonetheless is able to highlight the primary areas of focus for future work.
</prevsent>
<prevsent>secondly, we improve upon the training data, in particular doing better job of selecting relevant instances for the machine learner.
</prevsent>
</prevsection>
<citsent citstr=" N10-1019 ">
obtaining better-quality training data is major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (gamon, 2010).<papid> N10-1019 </papid></citsent>
<aftsection>
<nextsent>2see dickinson and lee (2009); de ilarraza et al (2008); oyama (2010) for related work in other languages.
</nextsent>
<nextsent>81
</nextsent>
<nextsent>2.1 pre-processing.
</nextsent>
<nextsent>korean is an agglutinative language: korean words (referred to as ecels) are usually composed of root with number of functional affixes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2009">
<title id=" W11-1410.xml">developing methodology for korean particle error detection </title>
<section> particle error detection.  </section>
<citcontext>
<prevsection>
<prevsent>to evaluate our parameters for obtaining the most.
</prevsent>
<prevsent>relevant instances, we keep the task simple and perform only step 1, as this step provides information about the usability of the training data.
</prevsent>
</prevsection>
<citsent citstr=" W10-1406 ">
for actual system performance, we evaluate both steps.in selecting features for korean, we have to account for relatively free word order (chung et al, 2010).<papid> W10-1406 </papid></citsent>
<aftsection>
<nextsent>we follow our previous work (dickinsonet al, 2010) <papid> W10-1502 </papid>in our feature choices, using five word window that includes the target stem and two words on either side for context (see also tetreault and chodorow, 2008).<papid> C08-1109 </papid></nextsent>
<nextsent>each word is broken down into: stem, affixes, stem pos, and affixes pos.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2011">
<title id=" W11-1410.xml">developing methodology for korean particle error detection </title>
<section> particle error detection.  </section>
<citcontext>
<prevsection>
<prevsent>relevant instances, we keep the task simple and perform only step 1, as this step provides information about the usability of the training data.
</prevsent>
<prevsent>for actual system performance, we evaluate both steps.in selecting features for korean, we have to account for relatively free word order (chung et al, 2010).<papid> W10-1406 </papid></prevsent>
</prevsection>
<citsent citstr=" C08-1109 ">
we follow our previous work (dickinsonet al, 2010) <papid> W10-1502 </papid>in our feature choices, using five word window that includes the target stem and two words on either side for context (see also tetreault and chodorow, 2008).<papid> C08-1109 </papid></citsent>
<aftsection>
<nextsent>each word is broken down into: stem, affixes, stem pos, and affixes pos.
</nextsent>
<nextsent>we also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties.
</nextsent>
<nextsent>although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide basis for preliminary system (section 4).
</nextsent>
<nextsent>we need well-formed korean data in order to traina machine learner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2026">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>syntax-based machine translation systems, regard less of the underlying formalism they use, depend on method for acquiring bilingual rules in that formalism to build the systems translation model.
</prevsent>
<prevsent>in modern syntax-based mt, this formalism is often synchronous context-free grammar (scfg), and the scfg rules are obtained automatically from parallel data through large variety of methods.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
some scfg rule extraction techniques require only viterbi word alignment links between the source and target sides of the input corpus (chiang, 2005), <papid> P05-1033 </papid>while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed.</citsent>
<aftsection>
<nextsent>among such techniques, most retain the dependency on viterbi word alignments for each sentence (galley et al, 2004; <papid> N04-1035 </papid>zollmann and venugopal, 2006; <papid> W06-3119 </papid>lavie et al, 2008; <papid> W08-0411 </papid>chiang, 2010) <papid> P10-1146 </papid>while others make use of general, corpus-level statistical lexicon instead of individual alignment links (zhechev and way, 2008).<papid> C08-1139 </papid></nextsent>
<nextsent>each method may also place constraints on the size, format, or structure of the rules it returns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2028">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in modern syntax-based mt, this formalism is often synchronous context-free grammar (scfg), and the scfg rules are obtained automatically from parallel data through large variety of methods.
</prevsent>
<prevsent>some scfg rule extraction techniques require only viterbi word alignment links between the source and target sides of the input corpus (chiang, 2005), <papid> P05-1033 </papid>while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed.</prevsent>
</prevsection>
<citsent citstr=" N04-1035 ">
among such techniques, most retain the dependency on viterbi word alignments for each sentence (galley et al, 2004; <papid> N04-1035 </papid>zollmann and venugopal, 2006; <papid> W06-3119 </papid>lavie et al, 2008; <papid> W08-0411 </papid>chiang, 2010) <papid> P10-1146 </papid>while others make use of general, corpus-level statistical lexicon instead of individual alignment links (zhechev and way, 2008).<papid> C08-1139 </papid></citsent>
<aftsection>
<nextsent>each method may also place constraints on the size, format, or structure of the rules it returns.
</nextsent>
<nextsent>this paper describes new, general-purpose rule extractor intended for cases in which two parse trees and viterbi word alignment links are provided foreach sentence, although compatibility with single parse-tree extraction methods can be achieved by supplying flat dummy?
</nextsent>
<nextsent>parse for the missing tree.our framework for rule extraction is thus most similar to the stat-xfer system (lavie et al, 2008; <papid> W08-0411 </papid>ambati et al, 2009) and the tree-to-tree situation considered by chiang (2010).<papid> P10-1146 </papid></nextsent>
<nextsent>however, we significantly broaden the scope of allowable rules compared to the stat-xfer heuristics, and our approach differs from chiangs system in its respect of the linguistic constituency constraints expressed in the in put tree structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2029">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in modern syntax-based mt, this formalism is often synchronous context-free grammar (scfg), and the scfg rules are obtained automatically from parallel data through large variety of methods.
</prevsent>
<prevsent>some scfg rule extraction techniques require only viterbi word alignment links between the source and target sides of the input corpus (chiang, 2005), <papid> P05-1033 </papid>while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed.</prevsent>
</prevsection>
<citsent citstr=" W06-3119 ">
among such techniques, most retain the dependency on viterbi word alignments for each sentence (galley et al, 2004; <papid> N04-1035 </papid>zollmann and venugopal, 2006; <papid> W06-3119 </papid>lavie et al, 2008; <papid> W08-0411 </papid>chiang, 2010) <papid> P10-1146 </papid>while others make use of general, corpus-level statistical lexicon instead of individual alignment links (zhechev and way, 2008).<papid> C08-1139 </papid></citsent>
<aftsection>
<nextsent>each method may also place constraints on the size, format, or structure of the rules it returns.
</nextsent>
<nextsent>this paper describes new, general-purpose rule extractor intended for cases in which two parse trees and viterbi word alignment links are provided foreach sentence, although compatibility with single parse-tree extraction methods can be achieved by supplying flat dummy?
</nextsent>
<nextsent>parse for the missing tree.our framework for rule extraction is thus most similar to the stat-xfer system (lavie et al, 2008; <papid> W08-0411 </papid>ambati et al, 2009) and the tree-to-tree situation considered by chiang (2010).<papid> P10-1146 </papid></nextsent>
<nextsent>however, we significantly broaden the scope of allowable rules compared to the stat-xfer heuristics, and our approach differs from chiangs system in its respect of the linguistic constituency constraints expressed in the in put tree structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2030">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in modern syntax-based mt, this formalism is often synchronous context-free grammar (scfg), and the scfg rules are obtained automatically from parallel data through large variety of methods.
</prevsent>
<prevsent>some scfg rule extraction techniques require only viterbi word alignment links between the source and target sides of the input corpus (chiang, 2005), <papid> P05-1033 </papid>while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed.</prevsent>
</prevsection>
<citsent citstr=" W08-0411 ">
among such techniques, most retain the dependency on viterbi word alignments for each sentence (galley et al, 2004; <papid> N04-1035 </papid>zollmann and venugopal, 2006; <papid> W06-3119 </papid>lavie et al, 2008; <papid> W08-0411 </papid>chiang, 2010) <papid> P10-1146 </papid>while others make use of general, corpus-level statistical lexicon instead of individual alignment links (zhechev and way, 2008).<papid> C08-1139 </papid></citsent>
<aftsection>
<nextsent>each method may also place constraints on the size, format, or structure of the rules it returns.
</nextsent>
<nextsent>this paper describes new, general-purpose rule extractor intended for cases in which two parse trees and viterbi word alignment links are provided foreach sentence, although compatibility with single parse-tree extraction methods can be achieved by supplying flat dummy?
</nextsent>
<nextsent>parse for the missing tree.our framework for rule extraction is thus most similar to the stat-xfer system (lavie et al, 2008; <papid> W08-0411 </papid>ambati et al, 2009) and the tree-to-tree situation considered by chiang (2010).<papid> P10-1146 </papid></nextsent>
<nextsent>however, we significantly broaden the scope of allowable rules compared to the stat-xfer heuristics, and our approach differs from chiangs system in its respect of the linguistic constituency constraints expressed in the in put tree structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2032">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in modern syntax-based mt, this formalism is often synchronous context-free grammar (scfg), and the scfg rules are obtained automatically from parallel data through large variety of methods.
</prevsent>
<prevsent>some scfg rule extraction techniques require only viterbi word alignment links between the source and target sides of the input corpus (chiang, 2005), <papid> P05-1033 </papid>while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed.</prevsent>
</prevsection>
<citsent citstr=" P10-1146 ">
among such techniques, most retain the dependency on viterbi word alignments for each sentence (galley et al, 2004; <papid> N04-1035 </papid>zollmann and venugopal, 2006; <papid> W06-3119 </papid>lavie et al, 2008; <papid> W08-0411 </papid>chiang, 2010) <papid> P10-1146 </papid>while others make use of general, corpus-level statistical lexicon instead of individual alignment links (zhechev and way, 2008).<papid> C08-1139 </papid></citsent>
<aftsection>
<nextsent>each method may also place constraints on the size, format, or structure of the rules it returns.
</nextsent>
<nextsent>this paper describes new, general-purpose rule extractor intended for cases in which two parse trees and viterbi word alignment links are provided foreach sentence, although compatibility with single parse-tree extraction methods can be achieved by supplying flat dummy?
</nextsent>
<nextsent>parse for the missing tree.our framework for rule extraction is thus most similar to the stat-xfer system (lavie et al, 2008; <papid> W08-0411 </papid>ambati et al, 2009) and the tree-to-tree situation considered by chiang (2010).<papid> P10-1146 </papid></nextsent>
<nextsent>however, we significantly broaden the scope of allowable rules compared to the stat-xfer heuristics, and our approach differs from chiangs system in its respect of the linguistic constituency constraints expressed in the in put tree structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2033">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in modern syntax-based mt, this formalism is often synchronous context-free grammar (scfg), and the scfg rules are obtained automatically from parallel data through large variety of methods.
</prevsent>
<prevsent>some scfg rule extraction techniques require only viterbi word alignment links between the source and target sides of the input corpus (chiang, 2005), <papid> P05-1033 </papid>while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed.</prevsent>
</prevsection>
<citsent citstr=" C08-1139 ">
among such techniques, most retain the dependency on viterbi word alignments for each sentence (galley et al, 2004; <papid> N04-1035 </papid>zollmann and venugopal, 2006; <papid> W06-3119 </papid>lavie et al, 2008; <papid> W08-0411 </papid>chiang, 2010) <papid> P10-1146 </papid>while others make use of general, corpus-level statistical lexicon instead of individual alignment links (zhechev and way, 2008).<papid> C08-1139 </papid></citsent>
<aftsection>
<nextsent>each method may also place constraints on the size, format, or structure of the rules it returns.
</nextsent>
<nextsent>this paper describes new, general-purpose rule extractor intended for cases in which two parse trees and viterbi word alignment links are provided foreach sentence, although compatibility with single parse-tree extraction methods can be achieved by supplying flat dummy?
</nextsent>
<nextsent>parse for the missing tree.our framework for rule extraction is thus most similar to the stat-xfer system (lavie et al, 2008; <papid> W08-0411 </papid>ambati et al, 2009) and the tree-to-tree situation considered by chiang (2010).<papid> P10-1146 </papid></nextsent>
<nextsent>however, we significantly broaden the scope of allowable rules compared to the stat-xfer heuristics, and our approach differs from chiangs system in its respect of the linguistic constituency constraints expressed in the in put tree structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2038">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> rule extraction algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>first, all words in the yield of must either be aligned to words within the yield of t, or they must be unaligned.
</prevsent>
<prevsent>second, the reverse must also hold: all words in the yield of must be aligned to words within the yield of or again be unaligned.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
this is analogous to the word-alignment consistency constraint of phrase-based smt phrase extraction (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>in figure 1, for example, the np dominating the french words les voit ures bleuesis aligned to the equivalent english np node dominating blue cars.
</nextsent>
<nextsent>as in phrase-based smt, where phrase in one language may be consistent with multiple possible phrases in the other language, we allow parse nodes in both trees to have multiple node alignments.
</nextsent>
<nextsent>this is in contrast to one-derivation rule extractors such as that of lavie et al (2008), <papid> W08-0411 </papid>in which each node 136 in may only be aligned to single node in andvice versa.</nextsent>
<nextsent>the french np node ma me`re, for example, aligns to both the nnp and np nodes in english producing mother.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2043">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> comparison to other methods.  </section>
<citcontext>
<prevsection>
<prevsent>yes stat-xfer yes no some no ghkm yes no no yes samt no no yes yes chiang (2010) <papid> P10-1146 </papid>no no yes yes this work yes yes yes yes table 1: comparisons between the rule extractor described in this paper and other scfg rule extraction methods.</prevsent>
<prevsent>table 1 compares the rule extractor described in section 2 to other scfg extraction methods described in the literature.</prevsent>
</prevsection>
<citsent citstr=" P06-1121 ">
we include comparisons of our work against the hiero system (chiang, 2005), <papid> P05-1033 </papid>thestat-xfer system rule learner most recently described by ambati et al (2009), the composed version of ghkm rule extraction (galley et al, 2006),<papid> P06-1121 </papid>the so-called syntax-augmented mt (samt) system (zollmann and venugopal, 2006), <papid> W06-3119 </papid>and hiero?</citsent>
<aftsection>
<nextsent>samt extension with source- and target-side syntax described by chiang (2010).<papid> P10-1146 </papid></nextsent>
<nextsent>note that some of these methods make use of only target-side parse trees ? or no parse trees at all, in the case of hiero ? but our primary interest in comparison is the constraints placed on the rule extraction process rather than the final output form of the rules themselves.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2050">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our constraint achieves the goal of controlling the size of the rule set while remaining flexi bile in terms of depth depending on the shape of the parse trees.
</prevsent>
<prevsent>we conducted experiments with our rule extractor on the fbis corpus, made up of approximately 302,000 chinese english sentence pairs.
</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
we parsed the corpus with the chinese and english grammars of the berkeley parser (petrov and klein, 2007) <papid> N07-1051 </papid>and word-aligned it with giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>the parsed and word-aligned fbis corpus served as the input to our rule extractor, which we ran with number of different settings.
</nextsent>
<nextsent>first, we acquired baseline rule extraction(xfer-orig?)
</nextsent>
<nextsent>from our corpus using an implementation of the basic stat-xfer rule learner (lavie et al, 2008), <papid> W08-0411 </papid>which decomposes each input tree pair into asingle set of minimal scfg rules2 using only original nodes in the parse trees.</nextsent>
<nextsent>next, we tested the effect of allowing multiple decompositions by running our own rule learner, but restricting its rules to also only make use of original nodes (compatible?).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2051">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our constraint achieves the goal of controlling the size of the rule set while remaining flexi bile in terms of depth depending on the shape of the parse trees.
</prevsent>
<prevsent>we conducted experiments with our rule extractor on the fbis corpus, made up of approximately 302,000 chinese english sentence pairs.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we parsed the corpus with the chinese and english grammars of the berkeley parser (petrov and klein, 2007) <papid> N07-1051 </papid>and word-aligned it with giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>the parsed and word-aligned fbis corpus served as the input to our rule extractor, which we ran with number of different settings.
</nextsent>
<nextsent>first, we acquired baseline rule extraction(xfer-orig?)
</nextsent>
<nextsent>from our corpus using an implementation of the basic stat-xfer rule learner (lavie et al, 2008), <papid> W08-0411 </papid>which decomposes each input tree pair into asingle set of minimal scfg rules2 using only original nodes in the parse trees.</nextsent>
<nextsent>next, we tested the effect of allowing multiple decompositions by running our own rule learner, but restricting its rules to also only make use of original nodes (compatible?).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2053">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 translation results.
</prevsent>
<prevsent>the grammars obtained from our rule extractor can be filtered and formatted for use with variety ofscfg-based decoders and rule formats.
</prevsent>
</prevsection>
<citsent citstr=" W09-0424 ">
we carried out end-to-end translation experiments with the various extracted rule sets from the fbis corpus using the open-source decoder joshua (li et al, 2009).<papid> W09-0424 </papid></citsent>
<aftsection>
<nextsent>given source-language string, joshua translates by producing synchronous parse of it according to scored scfg and target-side language model.
</nextsent>
<nextsent>a significant engineering challenge in building realmt system of this type is selecting more moderate sized subset of all extracted rules to retain in the final translation model.
</nextsent>
<nextsent>this is an especially important consideration when dealing with expanded rule sets derived from virtual nodes and multiple decomposi tions in each input tree.
</nextsent>
<nextsent>in our experiments, we pass all grammars through 3the compatible configuration is somewhat of an outlier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2054">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, before rule extraction, we globally correct the nodel labels of all numeral terminals in english and certain punctuation marks in both english and chinese.
</prevsent>
<prevsent>second, we attempt to reduce derivational ambiguity in cases where the same scfg right-hand side appears in the grammar after extraction with large number of possible left-hand-side labels.
</prevsent>
</prevsection>
<citsent citstr=" W10-1709 ">
to this end, we sort the possible left-hand sides by frequency for each unique right-hand side, and we remove the least frequent 10 percent of the label distribution.our translation model scoring is based on the feature set of hanneman et al (2010).<papid> W10-1709 </papid></citsent>
<aftsection>
<nextsent>this includes the standard bidirectional conditional maximum likelihood scores at both the word and phrase level on the right-hand side of rules.
</nextsent>
<nextsent>we also include maximum-likelihood scores for the left-hand-side label given all or part of the right-hand side.
</nextsent>
<nextsent>using statistics local to each rule, we set binary indicator features for rules whose frequencies are ? 3, plus five additional indicator features according to the format of the rules right-hand side, such as whether it is fully abstract.
</nextsent>
<nextsent>since the system in this paper is not constructed using any non-syntactic rules, wedo not include the hanneman et al (2010) <papid> W10-1709 </papid>not labelable?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2056">
<title id=" W11-1015.xml">a general purpose rule extractor for scfgbased machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we also separate fully abstract hierarchical rules from partially lexicalized hierarchical rules, and in further selection technique we retain the 5,000 most frequent abstract and 100,000 most frequent partially lexicalized rules.given these final rule sets, we tune our mt systems on the nist mt 2006 dataset using the minimum error-rate training package z-mert (zaidan, 2009), and we test on nist mt 2003.
</prevsent>
<prevsent>both sets have four reference translations.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
table 4 presentscase-insensitive evaluation results on the test set according to the automatic metrics bleu (papineni et al., 2002), <papid> P02-1040 </papid>ter (snover et al, 2006), and meteor (lavie and denkowski, 2009).4 the trend in the results is that including larger grammar is generally better for performance, but filtering techniques also play substantial role in determining how well given grammar will perform at run time.we first compare the results in table 4 for different rule sets all filtered the same way at decoding time.</citsent>
<aftsection>
<nextsent>with only 10,000 hierarchical rules in use (10k?), the improvements in scores indicate that an important contribution is being made by the additional phrase pair coverage provided by each suc 4for meteor scoring we use version 1.0 of the metric,tuned to hter with the exact, stemming, and synonymy modules enabled.cessive rule set.
</nextsent>
<nextsent>the original stat-xfer rule extraction provides 244,988 phrase pairs that match the mt 2003 test set.
</nextsent>
<nextsent>this is already increased to520,995 in the compatible system using multiple de compositions.
</nextsent>
<nextsent>with virtual nodes enabled, the full system produces 766,379 matching phrase pairs up to length 5 or 776,707 up to length 7.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2058">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tl], by finding the most likely translation given by: ? = arg max p(t |s) (1) in most of the existing approaches, following (brown et al, 1993), eq.
</prevsent>
<prevsent>(1) is factored using the source-channel model into ? = arg max p(s|t )p?(t ), (2) where the two models, the translation model,p(s|t ), and the language model (lm), p(t ), are estimated separately: the former using parallel corpus and hidden alignment model and the latter using typically much larger monolingual corpus.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the weighting factor ? is typically tuned on development test set by optimizing translation accuracy criterion such as bleu (papineni et al, 2002).<papid> P02-1040 </papid>in recent years, among all the proposed approaches, the phrase-based method has become the widely adopted one in smt due to its capability of capturing local context information from adjacent words.</citsent>
<aftsection>
<nextsent>word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (koehn et al, 2003), <papid> N03-1017 </papid>among allthe features utilized in maximum-entropy (loglinear) model (och and ney, 2002).<papid> P02-1038 </papid></nextsent>
<nextsent>the distortion cost utilized during the decoding is usually penalty linearly proportional to the number of wordsin the source sentence that are skipped in translation path.in this paper, we propose several novel approaches to improve reordering in the phrase-basedtranslation with maximum-entropy model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2059">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1) is factored using the source-channel model into ? = arg max p(s|t )p?(t ), (2) where the two models, the translation model,p(s|t ), and the language model (lm), p(t ), are estimated separately: the former using parallel corpus and hidden alignment model and the latter using typically much larger monolingual corpus.
</prevsent>
<prevsent>the weighting factor ? is typically tuned on development test set by optimizing translation accuracy criterion such as bleu (papineni et al, 2002).<papid> P02-1040 </papid>in recent years, among all the proposed approaches, the phrase-based method has become the widely adopted one in smt due to its capability of capturing local context information from adjacent words.</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (koehn et al, 2003), <papid> N03-1017 </papid>among allthe features utilized in maximum-entropy (loglinear) model (och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>the distortion cost utilized during the decoding is usually penalty linearly proportional to the number of wordsin the source sentence that are skipped in translation path.in this paper, we propose several novel approaches to improve reordering in the phrase-basedtranslation with maximum-entropy model.
</nextsent>
<nextsent>in section 2, we review the previous work that focused onthe distortion and phrase reordering in smt.
</nextsent>
<nextsent>in section 3, we briefly review the baseline of this work.in section 4, we introduce smoothed prior probability by taking into account the distortions in thepriors.
</nextsent>
<nextsent>in section 5, we present multiple novel distortion features based on syntactic parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2060">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1) is factored using the source-channel model into ? = arg max p(s|t )p?(t ), (2) where the two models, the translation model,p(s|t ), and the language model (lm), p(t ), are estimated separately: the former using parallel corpus and hidden alignment model and the latter using typically much larger monolingual corpus.
</prevsent>
<prevsent>the weighting factor ? is typically tuned on development test set by optimizing translation accuracy criterion such as bleu (papineni et al, 2002).<papid> P02-1040 </papid>in recent years, among all the proposed approaches, the phrase-based method has become the widely adopted one in smt due to its capability of capturing local context information from adjacent words.</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (koehn et al, 2003), <papid> N03-1017 </papid>among allthe features utilized in maximum-entropy (loglinear) model (och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>the distortion cost utilized during the decoding is usually penalty linearly proportional to the number of wordsin the source sentence that are skipped in translation path.in this paper, we propose several novel approaches to improve reordering in the phrase-basedtranslation with maximum-entropy model.
</nextsent>
<nextsent>in section 2, we review the previous work that focused onthe distortion and phrase reordering in smt.
</nextsent>
<nextsent>in section 3, we briefly review the baseline of this work.in section 4, we introduce smoothed prior probability by taking into account the distortions in thepriors.
</nextsent>
<nextsent>in section 5, we present multiple novel distortion features based on syntactic parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2063">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations.
</prevsent>
<prevsent>orientations typically apply to the adjacent phrases.two adjacent phrases can be either placed mono tonic ally (sometimes called straight) or swapped (non-monotonically or inverted).
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
in (och and ney,2004; <papid> J04-4002 </papid>tillmann, 2004; <papid> N04-4026 </papid>kumar and byrne, 2005; <papid> H05-1021 </papid>al onaizan and papineni, 2006; xiong et al, 2006;<papid> P06-1066 </papid>zens and ney, 2006; <papid> W06-3108 </papid>ni et al, 2009), <papid> P09-2061 </papid>people presented models that use lexical features from the phrases to predict their orientations.</citsent>
<aftsection>
<nextsent>these models are very powerful in predicting local phrase placements.
</nextsent>
<nextsent>in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</nextsent>
<nextsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2064">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations.
</prevsent>
<prevsent>orientations typically apply to the adjacent phrases.two adjacent phrases can be either placed mono tonic ally (sometimes called straight) or swapped (non-monotonically or inverted).
</prevsent>
</prevsection>
<citsent citstr=" N04-4026 ">
in (och and ney,2004; <papid> J04-4002 </papid>tillmann, 2004; <papid> N04-4026 </papid>kumar and byrne, 2005; <papid> H05-1021 </papid>al onaizan and papineni, 2006; xiong et al, 2006;<papid> P06-1066 </papid>zens and ney, 2006; <papid> W06-3108 </papid>ni et al, 2009), <papid> P09-2061 </papid>people presented models that use lexical features from the phrases to predict their orientations.</citsent>
<aftsection>
<nextsent>these models are very powerful in predicting local phrase placements.
</nextsent>
<nextsent>in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</nextsent>
<nextsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2065">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations.
</prevsent>
<prevsent>orientations typically apply to the adjacent phrases.two adjacent phrases can be either placed mono tonic ally (sometimes called straight) or swapped (non-monotonically or inverted).
</prevsent>
</prevsection>
<citsent citstr=" H05-1021 ">
in (och and ney,2004; <papid> J04-4002 </papid>tillmann, 2004; <papid> N04-4026 </papid>kumar and byrne, 2005; <papid> H05-1021 </papid>al onaizan and papineni, 2006; xiong et al, 2006;<papid> P06-1066 </papid>zens and ney, 2006; <papid> W06-3108 </papid>ni et al, 2009), <papid> P09-2061 </papid>people presented models that use lexical features from the phrases to predict their orientations.</citsent>
<aftsection>
<nextsent>these models are very powerful in predicting local phrase placements.
</nextsent>
<nextsent>in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</nextsent>
<nextsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2066">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations.
</prevsent>
<prevsent>orientations typically apply to the adjacent phrases.two adjacent phrases can be either placed mono tonic ally (sometimes called straight) or swapped (non-monotonically or inverted).
</prevsent>
</prevsection>
<citsent citstr=" P06-1066 ">
in (och and ney,2004; <papid> J04-4002 </papid>tillmann, 2004; <papid> N04-4026 </papid>kumar and byrne, 2005; <papid> H05-1021 </papid>al onaizan and papineni, 2006; xiong et al, 2006;<papid> P06-1066 </papid>zens and ney, 2006; <papid> W06-3108 </papid>ni et al, 2009), <papid> P09-2061 </papid>people presented models that use lexical features from the phrases to predict their orientations.</citsent>
<aftsection>
<nextsent>these models are very powerful in predicting local phrase placements.
</nextsent>
<nextsent>in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</nextsent>
<nextsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2067">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations.
</prevsent>
<prevsent>orientations typically apply to the adjacent phrases.two adjacent phrases can be either placed mono tonic ally (sometimes called straight) or swapped (non-monotonically or inverted).
</prevsent>
</prevsection>
<citsent citstr=" W06-3108 ">
in (och and ney,2004; <papid> J04-4002 </papid>tillmann, 2004; <papid> N04-4026 </papid>kumar and byrne, 2005; <papid> H05-1021 </papid>al onaizan and papineni, 2006; xiong et al, 2006;<papid> P06-1066 </papid>zens and ney, 2006; <papid> W06-3108 </papid>ni et al, 2009), <papid> P09-2061 </papid>people presented models that use lexical features from the phrases to predict their orientations.</citsent>
<aftsection>
<nextsent>these models are very powerful in predicting local phrase placements.
</nextsent>
<nextsent>in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</nextsent>
<nextsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2068">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations.
</prevsent>
<prevsent>orientations typically apply to the adjacent phrases.two adjacent phrases can be either placed mono tonic ally (sometimes called straight) or swapped (non-monotonically or inverted).
</prevsent>
</prevsection>
<citsent citstr=" P09-2061 ">
in (och and ney,2004; <papid> J04-4002 </papid>tillmann, 2004; <papid> N04-4026 </papid>kumar and byrne, 2005; <papid> H05-1021 </papid>al onaizan and papineni, 2006; xiong et al, 2006;<papid> P06-1066 </papid>zens and ney, 2006; <papid> W06-3108 </papid>ni et al, 2009), <papid> P09-2061 </papid>people presented models that use lexical features from the phrases to predict their orientations.</citsent>
<aftsection>
<nextsent>these models are very powerful in predicting local phrase placements.
</nextsent>
<nextsent>in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</nextsent>
<nextsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2069">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in (och and ney,2004; <papid> J04-4002 </papid>tillmann, 2004; <papid> N04-4026 </papid>kumar and byrne, 2005; <papid> H05-1021 </papid>al onaizan and papineni, 2006; xiong et al, 2006;<papid> P06-1066 </papid>zens and ney, 2006; <papid> W06-3108 </papid>ni et al, 2009), <papid> P09-2061 </papid>people presented models that use lexical features from the phrases to predict their orientations.</prevsent>
<prevsent>these models are very powerful in predicting local phrase placements.</prevsent>
</prevsection>
<citsent citstr=" D08-1089 ">
in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</citsent>
<aftsection>
<nextsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.
</nextsent>
<nextsent>syntax information has been used for reordering, such as in (xia and mccord, 2004; <papid> C04-1073 </papid>collins et al, 2005; <papid> P05-1066 </papid>wang et al, 2007; <papid> D07-1077 </papid>li et al, 2007; <papid> P07-1091 </papid>chang etal., 2009).<papid> W09-2307 </papid></nextsent>
<nextsent>more recently, in (ge, 2010) <papid> N10-1127 </papid>probabilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2070">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</prevsent>
<prevsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.</prevsent>
</prevsection>
<citsent citstr=" C04-1073 ">
syntax information has been used for reordering, such as in (xia and mccord, 2004; <papid> C04-1073 </papid>collins et al, 2005; <papid> P05-1066 </papid>wang et al, 2007; <papid> D07-1077 </papid>li et al, 2007; <papid> P07-1091 </papid>chang etal., 2009).<papid> W09-2307 </papid></citsent>
<aftsection>
<nextsent>more recently, in (ge, 2010) <papid> N10-1127 </papid>probabilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency.</nextsent>
<nextsent>the reordering model is used to generate reordering lattice which encodes many reordering and their costs (negative log probability).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2071">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</prevsent>
<prevsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
syntax information has been used for reordering, such as in (xia and mccord, 2004; <papid> C04-1073 </papid>collins et al, 2005; <papid> P05-1066 </papid>wang et al, 2007; <papid> D07-1077 </papid>li et al, 2007; <papid> P07-1091 </papid>chang etal., 2009).<papid> W09-2307 </papid></citsent>
<aftsection>
<nextsent>more recently, in (ge, 2010) <papid> N10-1127 </papid>probabilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency.</nextsent>
<nextsent>the reordering model is used to generate reordering lattice which encodes many reordering and their costs (negative log probability).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2072">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</prevsent>
<prevsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.</prevsent>
</prevsection>
<citsent citstr=" D07-1077 ">
syntax information has been used for reordering, such as in (xia and mccord, 2004; <papid> C04-1073 </papid>collins et al, 2005; <papid> P05-1066 </papid>wang et al, 2007; <papid> D07-1077 </papid>li et al, 2007; <papid> P07-1091 </papid>chang etal., 2009).<papid> W09-2307 </papid></citsent>
<aftsection>
<nextsent>more recently, in (ge, 2010) <papid> N10-1127 </papid>probabilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency.</nextsent>
<nextsent>the reordering model is used to generate reordering lattice which encodes many reordering and their costs (negative log probability).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2073">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</prevsent>
<prevsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.</prevsent>
</prevsection>
<citsent citstr=" P07-1091 ">
syntax information has been used for reordering, such as in (xia and mccord, 2004; <papid> C04-1073 </papid>collins et al, 2005; <papid> P05-1066 </papid>wang et al, 2007; <papid> D07-1077 </papid>li et al, 2007; <papid> P07-1091 </papid>chang etal., 2009).<papid> W09-2307 </papid></citsent>
<aftsection>
<nextsent>more recently, in (ge, 2010) <papid> N10-1127 </papid>probabilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency.</nextsent>
<nextsent>the reordering model is used to generate reordering lattice which encodes many reordering and their costs (negative log probability).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2074">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in (galley and manning, 2008) <papid> D08-1089 </papid>hierarchical orientation model is introduced that captures some non-local phrase reordering by shift reducealgorithm.</prevsent>
<prevsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.</prevsent>
</prevsection>
<citsent citstr=" W09-2307 ">
syntax information has been used for reordering, such as in (xia and mccord, 2004; <papid> C04-1073 </papid>collins et al, 2005; <papid> P05-1066 </papid>wang et al, 2007; <papid> D07-1077 </papid>li et al, 2007; <papid> P07-1091 </papid>chang etal., 2009).<papid> W09-2307 </papid></citsent>
<aftsection>
<nextsent>more recently, in (ge, 2010) <papid> N10-1127 </papid>probabilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency.</nextsent>
<nextsent>the reordering model is used to generate reordering lattice which encodes many reordering and their costs (negative log probability).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2075">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>because of the heavy use of lexical features, these models tend to suffer from data sparseness problems.
</prevsent>
<prevsent>syntax information has been used for reordering, such as in (xia and mccord, 2004; <papid> C04-1073 </papid>collins et al, 2005; <papid> P05-1066 </papid>wang et al, 2007; <papid> D07-1077 </papid>li et al, 2007; <papid> P07-1091 </papid>chang etal., 2009).<papid> W09-2307 </papid></prevsent>
</prevsection>
<citsent citstr=" N10-1127 ">
more recently, in (ge, 2010) <papid> N10-1127 </papid>probabilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency.</citsent>
<aftsection>
<nextsent>the reordering model is used to generate reordering lattice which encodes many reordering and their costs (negative log probability).
</nextsent>
<nextsent>another recent work is (green et al, 2010), <papid> N10-1129 </papid>which estimates future linear distortion cost and presents discriminative distortion model that predicts word movement during translation based on multiple features.this work differentiates itself from all the previous work on the phrase reordering as the following.firstly, we propose smoothed distortion prior probability in the maximum-entropy-based mt frame work.</nextsent>
<nextsent>it not only takes into account the distortion in the prior, but also alleviates the data sparseness problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2076">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>more recently, in (ge, 2010) <papid> N10-1127 </papid>probabilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency.</prevsent>
<prevsent>the reordering model is used to generate reordering lattice which encodes many reordering and their costs (negative log probability).</prevsent>
</prevsection>
<citsent citstr=" N10-1129 ">
another recent work is (green et al, 2010), <papid> N10-1129 </papid>which estimates future linear distortion cost and presents discriminative distortion model that predicts word movement during translation based on multiple features.this work differentiates itself from all the previous work on the phrase reordering as the following.firstly, we propose smoothed distortion prior probability in the maximum-entropy-based mt frame work.</citsent>
<aftsection>
<nextsent>it not only takes into account the distortion in the prior, but also alleviates the data sparseness problem.
</nextsent>
<nextsent>secondly, we propose multiple syntactic features based on the source-side parse tree to capture the reordering phenomena between two different languages.
</nextsent>
<nextsent>the correct reordering patterns will be automatically favored during the decoding, due to the higher weights obtained through the maximum entropy training on the parallel data.
</nextsent>
<nextsent>finally, we also introduce new metric to quantify the effect on the distortions in different systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2077">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> maximum-entropy model for mt.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we also introduce new metric to quantify the effect on the distortions in different systems.
</prevsent>
<prevsent>the experiment son chinese-english mt task show that these proposed approaches additively improve both the distortion and translation performance significantly.
</prevsent>
</prevsection>
<citsent citstr=" N07-1008 ">
in this section we give brief review of specialmaximum-entropy (me) model as introduced in (it tycheriah and roukos, 2007).<papid> N07-1008 </papid></citsent>
<aftsection>
<nextsent>the model has the following form, p(t, j|s) = p0(t, j|s) exp ? ii(t, j, s), (3) where is source phrase, and is target phrase.
</nextsent>
<nextsent>j is the jump distance from the previously translated source word to the current source word.
</nextsent>
<nextsent>during training can vary widely due to automatic word alignment in the parallel corpus.
</nextsent>
<nextsent>to limit the sparseness created by long jumps, is capped to window of source words (-5 to 5 words) around the last translated source word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2079">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> a new distortion evaluation metric </section>
<citcontext>
<prevsection>
<prevsent>rn} the pair-wise distortion metric pdscore can be computed as follows: pdscore( ??
</prevsent>
<prevsent>h ) = ? i=1 i(hi = rj ? hi1 = rj1) (8) it measures how often the translation output gets the pair-wise source visit order correct.
</prevsent>
</prevsection>
<citsent citstr=" W10-1749 ">
we notice that an mt metric named lrscore was proposed in (birch and osborne, 2010).<papid> W10-1749 </papid></citsent>
<aftsection>
<nextsent>it computes the distance between two word order sequences, which is different from the metric we proposed here.
</nextsent>
<nextsent>7.1 data and baseline.
</nextsent>
<nextsent>we conduct set of experiments on chinese-to english mt task.
</nextsent>
<nextsent>the training data includes theun parallel corpus and ldc-released parallel corpora,with about 11m sentence pairs, 320m words in total (counted at the english side).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2080">
<title id=" W11-1007.xml">improving reordering for statistical machine translation with smoothed priors and syntactic features </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for the given test set, we obtain the first 20 instances of n-grams (length from1 to 15) from the test that occur in the training universe and the resulting sentences then form the training sample.
</prevsent>
<prevsent>in the end, 1m sentence pairs are selected for the sampled training for each genre of the mt08 test set.a 5-gram language model is trained from the english gigaword corpus and the english portion of the parallel corpus used in the translation model training.
</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
the chinese parse trees are produced by maximum entropy based parser (ratnaparkhi,1997).<papid> W97-0301 </papid></citsent>
<aftsection>
<nextsent>the baseline decoder is phrase-based decoder that employs both normal phrases and also non-contiguous phrases.
</nextsent>
<nextsent>the value of maximum skip is set to 9 in all the experiments.
</nextsent>
<nextsent>the smoothing parameter ? for distortion prior is set to 0.9 empirically based on the results on the development set.
</nextsent>
<nextsent>7.2 distortion evaluation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2084">
<title id=" W10-4132.xml">adaptive chinese word segmentation with online passive aggressive algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when we use the machine learning methods for 1available at http://code.google.com/p/ fudannlp/chinese word segmentation, we assume that training and test data are drawn from the same distribution.
</prevsent>
<prevsent>this assumption underlies both theoretical analysis and experimental evaluations of learning algorithms.
</prevsent>
</prevsection>
<citsent citstr=" W06-1615 ">
however, the assumption does not hold for domain adaptation(ben-david et al, 2007; blitzer et al, 2006).<papid> W06-1615 </papid></citsent>
<aftsection>
<nextsent>the challenge is the difference of distribution between the source and target domains.in this paper, we use online margin maximization algorithm and domain in variant features for domain adaptive cws.
</nextsent>
<nextsent>the online learning algorithm is passive-aggressive (pa) algorithm(crammer et al, 2006), which passively accepts solution whose loss is zero, while it aggressively forces the new prototype vector to stay as close as possible to the one previously learned.
</nextsent>
<nextsent>the rest of the paper is organized as follows.
</nextsent>
<nextsent>section 2 introduces the related works.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2085">
<title id=" W10-4226.xml">the grec challenges 2010 overview and evaluation results </title>
<section> evaluation procedures.  </section>
<citcontext>
<prevsection>
<prevsent>word strings contained in the outermost refex i.e. embeddedrefex word strings were not considered separately.
</prevsent>
<prevsent>we also computed bleu-3, nist, string-editdistance and length-normalised string-edit distance, all on word strings defined as for string accuracy.
</prevsent>
</prevsection>
<citsent citstr=" H05-1004 ">
bleu and nist are designed for multiple output versions, and for the string-edit metrics we computed the mean of means over the three text level scores (computed against the three versions of text).to measure accuracy in the ner task, we applied three commonly used performance measures for coreference resolution: muc-6 (vilain et al, 1995), ceaf (luo, 2005), <papid> H05-1004 </papid>and b-cubed (bagga and baldwin, 1998).</citsent>
<aftsection>
<nextsent>3.2 human-assessed evaluations.
</nextsent>
<nextsent>we designed the human-assessed intrinsic evaluation as preference-judgement test where subjects expressed their preference, in terms of two criteria, for either the original wikipedia text or the version of it with system-generated referring expressions in it.
</nextsent>
<nextsent>for the grec-neg systems, the intrinsic human evaluation involved system outputs for 30 randomly selected items from the test set.
</nextsent>
<nextsent>we used repeated latin squares design which ensures that each subject sees the same number of outputs from each system and for each test set item.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2086">
<title id=" W10-4158.xml">jumping distance based chinese person name disambiguation </title>
<section> systems.  </section>
<citcontext>
<prevsection>
<prevsent>after that, we combine the weights of the same features and normalized the value by dividing that by the average weight in the specific class of features.
</prevsent>
<prevsent>based on the two classes of features, given target string and the document where it occurs, the detection component firstly generate the jumping tree of the document, and then determines whether the string is person name or things by measuring the similarity of the tree to the classes of features.
</prevsent>
</prevsection>
<citsent citstr=" P98-1012 ">
here, we simply use the vsm and cosine metric bagga and baldwin, 1998?<papid> P98-1012 </papid></citsent>
<aftsection>
<nextsent>to obtain the similarity.
</nextsent>
<nextsent>the second component, viz.
</nextsent>
<nextsent>dj-based person name disambiguation, firstly generates the jumping trees for all documents that involve specific person name.
</nextsent>
<nextsent>and two-stage clustering algorithm is adopted to divide the documents and refer each cluster to person.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2087">
<title id=" W10-4158.xml">jumping distance based chinese person name disambiguation </title>
<section> systems.  </section>
<citcontext>
<prevsection>
<prevsent>the two-stage clustering algorithm(ikeda et al,2009) initially obtains the optimal parameters that respectively refer to the maximum precision and recall based on training data, and then regards statistical tradeoff as the final value of the parameters.
</prevsent>
<prevsent>here, the affinity propagation clustering tools (frey bj and dueck d, 2007) is in use.
</prevsent>
</prevsection>
<citsent citstr=" D09-1056 ">
z system2 the system is similar to the system1 except that it additionally involve named entity identification (artiles et.al,2009<papid> D09-1056 </papid>b; popescu,o. and magnini, b.,2007)before the two-stage clustering in the component of person name disam biguation.</citsent>
<aftsection>
<nextsent>in detail, given person name and the documents that it occurs in, the disambiguation component of system2 firstly adopt ner crf++ toolkit3 provided by msra to identify named entities(chen et al, 2006) <papid> W06-0116 </papid>that involve the given name string, such as the entity ???</nextsent>
<nextsent>(viz.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2088">
<title id=" W10-4158.xml">jumping distance based chinese person name disambiguation </title>
<section> systems.  </section>
<citcontext>
<prevsection>
<prevsent>here, the affinity propagation clustering tools (frey bj and dueck d, 2007) is in use.
</prevsent>
<prevsent>z system2 the system is similar to the system1 except that it additionally involve named entity identification (artiles et.al,2009<papid> D09-1056 </papid>b; popescu,o. and magnini, b.,2007)before the two-stage clustering in the component of person name disam biguation.</prevsent>
</prevsection>
<citsent citstr=" W06-0116 ">
in detail, given person name and the documents that it occurs in, the disambiguation component of system2 firstly adopt ner crf++ toolkit3 provided by msra to identify named entities(chen et al, 2006) <papid> W06-0116 </papid>that involve the given name string, such as the entity ???</citsent>
<aftsection>
<nextsent>(viz.
</nextsent>
<nextsent>gao-ming li in english) when given the target name string ????(viz.
</nextsent>
<nextsent>ming gao in english).
</nextsent>
<nextsent>thus the documents can be roughly divided into different clusters of named entities without name segmentation errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2089">
<title id=" W10-4158.xml">jumping distance based chinese person name disambiguation </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>thus we can deal with the issue of disambiguation without the interruption of name segmentation errors.
</prevsent>
<prevsent>z training dataset: they contain about 30 chinese personal names, and document set of about 100-300 news articles from collection of xinhua news documents in timespan of fourteen years are provided for each personal name.
</prevsent>
</prevsection>
<citsent citstr=" D07-1074 ">
z external dataset: chinese wikipedia2 personal attribution (cucerzan, 2007; <papid> D07-1074 </papid>nguyen and cao,2008).</citsent>
<aftsection>
<nextsent>z test dataset: there are about 26 chinese personal names, which are similar to train datasets.
</nextsent>
<nextsent>the systems that run on test dataset are evaluated by both b-cubed (bagga and baldwin, 1998; <papid> P98-1012 </papid>artiles et al,2009<papid> D09-1056 </papid>a) and p-ip (artiles et al., 2007 ;<papid> W07-2012 </papid>artiles et al,2009<papid> D09-1056 </papid>a).</nextsent>
<nextsent>and the systems that run on training dataset were only evaluated by b-cubed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2093">
<title id=" W10-4158.xml">jumping distance based chinese person name disambiguation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>z external dataset: chinese wikipedia2 personal attribution (cucerzan, 2007; <papid> D07-1074 </papid>nguyen and cao,2008).</prevsent>
<prevsent>z test dataset: there are about 26 chinese personal names, which are similar to train data sets.</prevsent>
</prevsection>
<citsent citstr=" W07-2012 ">
the systems that run on test dataset are evaluated by both b-cubed (bagga and baldwin, 1998; <papid> P98-1012 </papid>artiles et al,2009<papid> D09-1056 </papid>a) and p-ip (artiles et al., 2007 ;<papid> W07-2012 </papid>artiles et al,2009<papid> D09-1056 </papid>a).</citsent>
<aftsection>
<nextsent>and the systems that run on training dataset were only evaluated by b-cubed.
</nextsent>
<nextsent>in experiments, we firstly evaluate the performance of name segmentation error detection on the training dataset.
</nextsent>
<nextsent>for comparison, we additionally perform another detection method which only using name entity identifcation (ner crf++ tools) to distinguish name-strings from the discarded ones.
</nextsent>
<nextsent>the results are shown in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2096">
<title id=" W11-0217.xml">hypothesis and evidence extraction from full text scientific journal articles </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>the craft corpus is set of 97 full-text papers describing the function of genes in the mouse genome informatics database (blake et al, 2011).
</prevsent>
<prevsent>these documents have already been annotated with syntactic information (parse trees and part-of-speechtags), linguistic phenomena (coreference), and semantic entities (genes, chemicals, cell lines, biological functions and molecular processes), making the corpus rich resource for extracting or inferring information from full scientific papers.
</prevsent>
</prevsection>
<citsent citstr=" W09-1325 ">
the cisp schema (soldatova and liakata, 2007;liakata et al, 2009) <papid> W09-1325 </papid>contains 11 categories, and several of the categories describe the intentions of the authors, making it well suited for markup of argumentation.</citsent>
<aftsection>
<nextsent>we chose to narrow these down to 9 categories (excluding model and object) during annotation training; our guidelines are shown in figure 1.
</nextsent>
<nextsent>we expect this schema to describe the pragmat-.
</nextsent>
<nextsent>ics in the text well, while still offering the potential for high inter annotator agreement due to manageable number of categories.
</nextsent>
<nextsent>the process of marking the sentences in the craft corpus according to the cisp guidelines took one annotator about four months.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2097">
<title id=" W11-0705.xml">sentiment analysis of twitter data </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude and give future directions of research in section 9.
</prevsent>
<prevsent>sentiment analysis has been handled as natural language processing task at many levels of granularity.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
starting from being document level classification task (turney, 2002; <papid> P02-1053 </papid>pang and lee, 2004), <papid> P04-1035 </papid>it has been handled at the sentence level (hu and liu, 2004; kim and hovy, 2004) <papid> C04-1200 </papid>and more recently at the phrase level (wilson et al, 2005; <papid> H05-1044 </papid>agarwal et al, 2009).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>microblog data like twitter, on which users post real time reactions to and opinions about every thing?, poses newer and different challenges.
</nextsent>
<nextsent>some of the early and recent results on sentiment analysis of twitter data are by go et al (2009), (bermingham and smeaton, 2010) and pak and paroubek (2010).go et al (2009) use distant learning to acquire sentiment data.
</nextsent>
<nextsent>they use tweets ending in positive emoticons like ?:)?
</nextsent>
<nextsent>as positive and negative emoticons like ?:(?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2098">
<title id=" W11-0705.xml">sentiment analysis of twitter data </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude and give future directions of research in section 9.
</prevsent>
<prevsent>sentiment analysis has been handled as natural language processing task at many levels of granularity.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
starting from being document level classification task (turney, 2002; <papid> P02-1053 </papid>pang and lee, 2004), <papid> P04-1035 </papid>it has been handled at the sentence level (hu and liu, 2004; kim and hovy, 2004) <papid> C04-1200 </papid>and more recently at the phrase level (wilson et al, 2005; <papid> H05-1044 </papid>agarwal et al, 2009).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>microblog data like twitter, on which users post real time reactions to and opinions about every thing?, poses newer and different challenges.
</nextsent>
<nextsent>some of the early and recent results on sentiment analysis of twitter data are by go et al (2009), (bermingham and smeaton, 2010) and pak and paroubek (2010).go et al (2009) use distant learning to acquire sentiment data.
</nextsent>
<nextsent>they use tweets ending in positive emoticons like ?:)?
</nextsent>
<nextsent>as positive and negative emoticons like ?:(?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2099">
<title id=" W11-0705.xml">sentiment analysis of twitter data </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude and give future directions of research in section 9.
</prevsent>
<prevsent>sentiment analysis has been handled as natural language processing task at many levels of granularity.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
starting from being document level classification task (turney, 2002; <papid> P02-1053 </papid>pang and lee, 2004), <papid> P04-1035 </papid>it has been handled at the sentence level (hu and liu, 2004; kim and hovy, 2004) <papid> C04-1200 </papid>and more recently at the phrase level (wilson et al, 2005; <papid> H05-1044 </papid>agarwal et al, 2009).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>microblog data like twitter, on which users post real time reactions to and opinions about every thing?, poses newer and different challenges.
</nextsent>
<nextsent>some of the early and recent results on sentiment analysis of twitter data are by go et al (2009), (bermingham and smeaton, 2010) and pak and paroubek (2010).go et al (2009) use distant learning to acquire sentiment data.
</nextsent>
<nextsent>they use tweets ending in positive emoticons like ?:)?
</nextsent>
<nextsent>as positive and negative emoticons like ?:(?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2100">
<title id=" W11-0705.xml">sentiment analysis of twitter data </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude and give future directions of research in section 9.
</prevsent>
<prevsent>sentiment analysis has been handled as natural language processing task at many levels of granularity.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
starting from being document level classification task (turney, 2002; <papid> P02-1053 </papid>pang and lee, 2004), <papid> P04-1035 </papid>it has been handled at the sentence level (hu and liu, 2004; kim and hovy, 2004) <papid> C04-1200 </papid>and more recently at the phrase level (wilson et al, 2005; <papid> H05-1044 </papid>agarwal et al, 2009).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>microblog data like twitter, on which users post real time reactions to and opinions about every thing?, poses newer and different challenges.
</nextsent>
<nextsent>some of the early and recent results on sentiment analysis of twitter data are by go et al (2009), (bermingham and smeaton, 2010) and pak and paroubek (2010).go et al (2009) use distant learning to acquire sentiment data.
</nextsent>
<nextsent>they use tweets ending in positive emoticons like ?:)?
</nextsent>
<nextsent>as positive and negative emoticons like ?:(?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2101">
<title id=" W11-0705.xml">sentiment analysis of twitter data </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude and give future directions of research in section 9.
</prevsent>
<prevsent>sentiment analysis has been handled as natural language processing task at many levels of granularity.
</prevsent>
</prevsection>
<citsent citstr=" E09-1004 ">
starting from being document level classification task (turney, 2002; <papid> P02-1053 </papid>pang and lee, 2004), <papid> P04-1035 </papid>it has been handled at the sentence level (hu and liu, 2004; kim and hovy, 2004) <papid> C04-1200 </papid>and more recently at the phrase level (wilson et al, 2005; <papid> H05-1044 </papid>agarwal et al, 2009).<papid> E09-1004 </papid></citsent>
<aftsection>
<nextsent>microblog data like twitter, on which users post real time reactions to and opinions about every thing?, poses newer and different challenges.
</nextsent>
<nextsent>some of the early and recent results on sentiment analysis of twitter data are by go et al (2009), (bermingham and smeaton, 2010) and pak and paroubek (2010).go et al (2009) use distant learning to acquire sentiment data.
</nextsent>
<nextsent>they use tweets ending in positive emoticons like ?:)?
</nextsent>
<nextsent>as positive and negative emoticons like ?:(?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2102">
<title id=" W11-0705.xml">sentiment analysis of twitter data </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>another contribution of this paper is that we report results on manually annotated data that does not suffer from any known biases.
</prevsent>
<prevsent>our data is random sample of streaming tweets unlike data collected by using specific queries.
</prevsent>
</prevsection>
<citsent citstr=" C10-2005 ">
the sizeof our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds.another significant effort for sentiment classification on twitter data is by barbosa and feng (2010).<papid> C10-2005 </papid></citsent>
<aftsection>
<nextsent>they use polarity predictions from three web sites as noisy labels to train model and use 1000 manually labeled tweets for tuning and another 1000 manually labeled tweets for testing.
</nextsent>
<nextsent>they however do not mention how they collect their test data.
</nextsent>
<nextsent>they propose the use of syntax features of tweets like retweet, hash tags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and pos of words.
</nextsent>
<nextsent>we extend their approach by using real valued prior polarity, and by combining prior polarity with pos.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2103">
<title id=" W11-0705.xml">sentiment analysis of twitter data </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>our results show that the features that enhance the performance of our classifiers the most are features that combine prior polarity of words with their parts of speech.
</prevsent>
<prevsent>the tweet syntax features help but only marginally.
</prevsent>
</prevsection>
<citsent citstr=" C04-1121 ">
gamon (2004) <papid> C04-1121 </papid>perform sentiment analysis onfeeadback data from global support services sur vey.</citsent>
<aftsection>
<nextsent>one aim of their paper is to analyze the role 31 of linguistic features like pos tags.
</nextsent>
<nextsent>they perform extensive feature analysis and feature selection and demonstrate that abstract linguistic analysis features contributes to the classifier accuracy.
</nextsent>
<nextsent>in this paper we perform extensive feature analysis and show thatthe use of only 100 abstract linguistic features performs as well as hard unigram baseline.
</nextsent>
<nextsent>twitter is social networking and microblogging service that allows users to post real time messages, called tweets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2104">
<title id=" W11-0705.xml">sentiment analysis of twitter data </title>
<section> resources and pre-processing of data.  </section>
<citcontext>
<prevsection>
<prevsent>acronym english expansion gr8, gr8t great lol laughing out loud rotf rolling on the floor bff best friend forever table 1: example acrynom and their expansion in the acronym dictionary.
</prevsent>
<prevsent>we present some preliminary statistics about the data in table 3.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we use the stanford tokenizer (klein and manning, 2003) <papid> P03-1054 </papid>to tokenize the tweets.</citsent>
<aftsection>
<nextsent>we use stop word dictionary3 to identify stop words.
</nextsent>
<nextsent>allthe other words which are found in wordnet (fell baum, 1998) are counted as english words.
</nextsent>
<nextsent>we use 1http://en.wikipedia.org/wiki/list of emoticons 2http://www.noslang.com/ 3http://www.webconfs.com/stop-words.php 32 emoticon polarity :-) :) :o) :] :3 :c) positive :d c: extremely-positive :-( :( :c :[ negative d8 d; d= dx v.v extremely-negative : | neutral table 2: part of the dictionary of emoticons the standard tagset defined by the penn treebank for identifying punctuation.
</nextsent>
<nextsent>we record the occurrence of three standard twitter tags: emoticons, urls and targets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2106">
<title id=" W11-0304.xml">modeling infant word segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this problem has created challenge for researchers modeling language acquisition to suggest more sophisticated strategies that infants might use.
</prevsent>
<prevsent>these approaches have fallen into two primary categories: optimization-based and bootstrapping algorithm strategies.
</prevsent>
</prevsection>
<citsent citstr=" J01-3002 ">
optimization-based strategies have focused on techniques that learner might use to arrive at an optimal segmentation, either through dynamic programming approach (brent, 1999), online learning (venkataraman, 2001), <papid> J01-3002 </papid>or non parametric bayesian inference (goldwater et al, 2009; johnson and goldwater, 2009).<papid> N09-1036 </papid></citsent>
<aftsection>
<nextsent>these approaches fit within standard statistical approaches to natural language processing, defining statistical objectives and inference strategies, with the learners trying to optimize some combination of the quality of its lexicon and representations of the corpus.
</nextsent>
<nextsent>in contrast, bootstrapping approaches (gambell and yang, 2004; <papid> W04-1307 </papid>lignos and yang, 2010) <papid> W10-2912 </papid>to word segmentation have focused on simple heuristics for populating lexicon and strategies for using the contents of the lexicon to segment utterances.</nextsent>
<nextsent>these approaches have focused on procedure for segmentation rather than defining an optimal segmentation explicitly, and do not define formal objective that is to be optimized.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2107">
<title id=" W11-0304.xml">modeling infant word segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this problem has created challenge for researchers modeling language acquisition to suggest more sophisticated strategies that infants might use.
</prevsent>
<prevsent>these approaches have fallen into two primary categories: optimization-based and bootstrapping algorithm strategies.
</prevsent>
</prevsection>
<citsent citstr=" N09-1036 ">
optimization-based strategies have focused on techniques that learner might use to arrive at an optimal segmentation, either through dynamic programming approach (brent, 1999), online learning (venkataraman, 2001), <papid> J01-3002 </papid>or non parametric bayesian inference (goldwater et al, 2009; johnson and goldwater, 2009).<papid> N09-1036 </papid></citsent>
<aftsection>
<nextsent>these approaches fit within standard statistical approaches to natural language processing, defining statistical objectives and inference strategies, with the learners trying to optimize some combination of the quality of its lexicon and representations of the corpus.
</nextsent>
<nextsent>in contrast, bootstrapping approaches (gambell and yang, 2004; <papid> W04-1307 </papid>lignos and yang, 2010) <papid> W10-2912 </papid>to word segmentation have focused on simple heuristics for populating lexicon and strategies for using the contents of the lexicon to segment utterances.</nextsent>
<nextsent>these approaches have focused on procedure for segmentation rather than defining an optimal segmentation explicitly, and do not define formal objective that is to be optimized.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2108">
<title id=" W11-0304.xml">modeling infant word segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>optimization-based strategies have focused on techniques that learner might use to arrive at an optimal segmentation, either through dynamic programming approach (brent, 1999), online learning (venkataraman, 2001), <papid> J01-3002 </papid>or non parametric bayesian inference (goldwater et al, 2009; johnson and goldwater, 2009).<papid> N09-1036 </papid></prevsent>
<prevsent>these approaches fit within standard statistical approaches to natural language processing, defining statistical objectives and inference strategies, with the learners trying to optimize some combination of the quality of its lexicon and representations of the corpus.</prevsent>
</prevsection>
<citsent citstr=" W04-1307 ">
in contrast, bootstrapping approaches (gambell and yang, 2004; <papid> W04-1307 </papid>lignos and yang, 2010) <papid> W10-2912 </papid>to word segmentation have focused on simple heuristics for populating lexicon and strategies for using the contents of the lexicon to segment utterances.</citsent>
<aftsection>
<nextsent>these approaches have focused on procedure for segmentation rather than defining an optimal segmentation explicitly, and do not define formal objective that is to be optimized.
</nextsent>
<nextsent>while bootstrapping approaches have generally made stronger attempts to align with infants abilities to process the speech signal (gambell and yang, 2004) <papid> W04-1307 </papid>than other approaches, little effort has been made to connect the details of an implemented segmentation strategy with childrens learning patterns since the earliest computational models of the task(olivier, 1968).</nextsent>
<nextsent>it is important to draw contrast here between attempts to match patterns of human development with regard to word segmentation with attempts to model performance in artificial language learning experiments whose goal is to probe word segmentation abilities in humans (frank et al, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2111">
<title id=" W11-0304.xml">modeling infant word segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>optimization-based strategies have focused on techniques that learner might use to arrive at an optimal segmentation, either through dynamic programming approach (brent, 1999), online learning (venkataraman, 2001), <papid> J01-3002 </papid>or non parametric bayesian inference (goldwater et al, 2009; johnson and goldwater, 2009).<papid> N09-1036 </papid></prevsent>
<prevsent>these approaches fit within standard statistical approaches to natural language processing, defining statistical objectives and inference strategies, with the learners trying to optimize some combination of the quality of its lexicon and representations of the corpus.</prevsent>
</prevsection>
<citsent citstr=" W10-2912 ">
in contrast, bootstrapping approaches (gambell and yang, 2004; <papid> W04-1307 </papid>lignos and yang, 2010) <papid> W10-2912 </papid>to word segmentation have focused on simple heuristics for populating lexicon and strategies for using the contents of the lexicon to segment utterances.</citsent>
<aftsection>
<nextsent>these approaches have focused on procedure for segmentation rather than defining an optimal segmentation explicitly, and do not define formal objective that is to be optimized.
</nextsent>
<nextsent>while bootstrapping approaches have generally made stronger attempts to align with infants abilities to process the speech signal (gambell and yang, 2004) <papid> W04-1307 </papid>than other approaches, little effort has been made to connect the details of an implemented segmentation strategy with childrens learning patterns since the earliest computational models of the task(olivier, 1968).</nextsent>
<nextsent>it is important to draw contrast here between attempts to match patterns of human development with regard to word segmentation with attempts to model performance in artificial language learning experiments whose goal is to probe word segmentation abilities in humans (frank et al, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2115">
<title id=" W11-0304.xml">modeling infant word segmentation </title>
<section> infant performance in word.  </section>
<citcontext>
<prevsection>
<prevsent>that infants should be able to operate on syllables is unsurprising; infants as young as 4-days-old are able to discriminate words based on syllable length (bijeljac-babicet al, 1993) and phonotactic cues to syllable boundaries seem to be rapidly acquired by infants (onishi et al, 2002).
</prevsent>
<prevsent>the use of the syllable in experimental work on word segmentation stands in contrast to many computational models that have operated at the phoneme level (brent, 1999; venkataraman, 2001; <papid> J01-3002 </papid>goldwater et al, 2009).</prevsent>
</prevsection>
<citsent citstr=" P08-1046 ">
an exception to the focus on phoneme-based segmentation is the joint learning model proposed by johnson (2008)<papid> P08-1046 </papid>that learns syllabification and other levels of representation jointly with word segmentation, but that model poses problems as developmentally relevant approach in that it predicts un attested joint syllabifi cation/segmentation errors by infants and problem sas linguistically relevant approach due to its non phonotactic approach to learning syllabification.from this survey, we see some relevant phenomena that good model of infant word segmentation should replicate.</citsent>
<aftsection>
<nextsent>(1) the learner should operate onsyllables.
</nextsent>
<nextsent>(2) at some stage of learning, underseg men tation function word collocations (e.g., that-ashould occur.
</nextsent>
<nextsent>(3) at some stage of learning, over segmentation of function words that may begin other words (e.g., be-have) should occur.
</nextsent>
<nextsent>(4) the learner should attend to the ends of utterances as use them to help identify novel words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2122">
<title id=" W11-0304.xml">modeling infant word segmentation </title>
<section> an algorithm for segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>new hypotheses are added to support multiple possible sub tractive segmentations.for example, using the utterance above, at the beginning of segmentation either part or partof couldbe subtracted from the utterance, and both possible segment ations can be evaluated.
</prevsent>
<prevsent>the learner scores these hypotheses in fashion similar to the greedy segmentation, but using function based on the score of all words used in the utterance.
</prevsent>
</prevsection>
<citsent citstr=" E03-1076 ">
the geometric mean has been used in compound splitting (koehn and knight, 2003), <papid> E03-1076 </papid>task in many ways similar to word segmentation, so we adopt it as the criterion for selecting the best hypothesis.</citsent>
<aftsection>
<nextsent>for hypothesized segmentation comprised of words wi . . .
</nextsent>
<nextsent>wn, hypothesis is chosen as follows: argmax ( ? wih score(wi)) 1 for any not found in the lexicon we must assign score; we assign it score of one as that would be its value assuming it had just been added to the lexicon, an approach similar to laplace smoothing.
</nextsent>
<nextsent>returning to the previous example, while the score of partof is greater than that of part, the score of of is much higher than either, so if both partof an apple and part of an apple are considered, the high score of of causes the latter to be chosen.
</nextsent>
<nextsent>when beam search is employed, only words used in the winning hypothesis are rewarded, similar to the greedy case where there are no other hypotheses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2140">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to alleviate this problem, paraphrase-enriched smt system shave been proposed to show the effectiveness of incorporating paraphrase information.
</prevsent>
<prevsent>in terms of the position at which paraphrases are incorporated in the mt-pipeline, previous work can be organized into three different categories:?
</prevsent>
</prevsection>
<citsent citstr=" D09-1040 ">
translation model augmentation with paraphrases (callison-burch et al, 2006; marton et al., 2009).<papid> D09-1040 </papid></citsent>
<aftsection>
<nextsent>here the focus is on the translation of unknown source words or phrases in the in put sentences by enriching the translation table with paraphrases.?
</nextsent>
<nextsent>training corpus augmentation with paraphrases (bond et al, 2008; nakov, 2008<papid> W08-0320 </papid>a; nakov, 2008<papid> W08-0320 </papid>b).</nextsent>
<nextsent>paraphrases are incorporated into the mt systems by expanding the training data.?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2141">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>translation model augmentation with paraphrases (callison-burch et al, 2006; marton et al., 2009).<papid> D09-1040 </papid></prevsent>
<prevsent>here the focus is on the translation of unknown source words or phrases in the in put sentences by enriching the translation table with paraphrases.?</prevsent>
</prevsection>
<citsent citstr=" W08-0320 ">
training corpus augmentation with paraphrases (bond et al, 2008; nakov, 2008<papid> W08-0320 </papid>a; nakov, 2008<papid> W08-0320 </papid>b).</citsent>
<aftsection>
<nextsent>paraphrases are incorporated into the mt systems by expanding the training data.?
</nextsent>
<nextsent>word-lattice-based method with paraphrases (du et al, 2010; <papid> D10-1041 </papid>onishi et al, 312010).</nextsent>
<nextsent>instead of augmenting the translation table, source-language paraphrases are constructed to enrich the inputs to the smt system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2145">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>training corpus augmentation with paraphrases (bond et al, 2008; nakov, 2008<papid> W08-0320 </papid>a; nakov, 2008<papid> W08-0320 </papid>b).</prevsent>
<prevsent>paraphrases are incorporated into the mt systems by expanding the training data.?</prevsent>
</prevsection>
<citsent citstr=" D10-1041 ">
word-lattice-based method with paraphrases (du et al, 2010; <papid> D10-1041 </papid>onishi et al, 312010).</citsent>
<aftsection>
<nextsent>instead of augmenting the translation table, source-language paraphrases are constructed to enrich the inputs to the smt system.
</nextsent>
<nextsent>another directly related work is to use word lattices to deal with multi-source translation (schroeder et al, 2009), <papid> E09-1082 </papid>in which paraphrases are actually generated from the alignments of difference source sentences.comparing these three methods, the word-lattice based method has the least overheads because: ? the translation model augmentation method has to re-run the whole mt pipeline oncethe inputs are changed, while the word-lattice based method only need to transform the new input sentences into word lattices.?</nextsent>
<nextsent>the training corpus augmentation method requires corpus-scale expansion, which drastically increases the computational complexity on large corpora, while the word-lattice-based method only deals with the development set and test set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2148">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word-lattice-based method with paraphrases (du et al, 2010; <papid> D10-1041 </papid>onishi et al, 312010).</prevsent>
<prevsent>instead of augmenting the translation table, source-language paraphrases are constructed to enrich the inputs to the smt system.</prevsent>
</prevsection>
<citsent citstr=" E09-1082 ">
another directly related work is to use word lattices to deal with multi-source translation (schroeder et al, 2009), <papid> E09-1082 </papid>in which paraphrases are actually generated from the alignments of difference source sentences.comparing these three methods, the word-lattice based method has the least overheads because: ? the translation model augmentation method has to re-run the whole mt pipeline oncethe inputs are changed, while the word-lattice based method only need to transform the new input sentences into word lattices.?</citsent>
<aftsection>
<nextsent>the training corpus augmentation method requires corpus-scale expansion, which drastically increases the computational complexity on large corpora, while the word-lattice-based method only deals with the development set and test set.
</nextsent>
<nextsent>in (du et al, 2010; <papid> D10-1041 </papid>onishi et al, 2010), <papid> P10-2001 </papid>it is also observed that the word-lattice-based method performed better than the translation model augmentation method on different scales and two different language pairs in several translation tasks.</nextsent>
<nextsent>thus they concluded that the word-lattice-based method is preferable for this task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2152">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another directly related work is to use word lattices to deal with multi-source translation (schroeder et al, 2009), <papid> E09-1082 </papid>in which paraphrases are actually generated from the alignments of difference source sentences.comparing these three methods, the word-lattice based method has the least overheads because: ? the translation model augmentation method has to re-run the whole mt pipeline oncethe inputs are changed, while the word-lattice based method only need to transform the new input sentences into word lattices.?</prevsent>
<prevsent>the training corpus augmentation method requires corpus-scale expansion, which drastically increases the computational complexity on large corpora, while the word-lattice-based method only deals with the development set and test set.</prevsent>
</prevsection>
<citsent citstr=" P10-2001 ">
in (du et al, 2010; <papid> D10-1041 </papid>onishi et al, 2010), <papid> P10-2001 </papid>it is also observed that the word-lattice-based method performed better than the translation model augmentation method on different scales and two different language pairs in several translation tasks.</citsent>
<aftsection>
<nextsent>thus they concluded that the word-lattice-based method is preferable for this task.
</nextsent>
<nextsent>however, there are still some drawbacks for the word-lattice-based method:?
</nextsent>
<nextsent>in the lattice construction processing, duplicated paths are created and fed into smt decoders.
</nextsent>
<nextsent>this decreases the paraphrase capacity in the word lattices.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2155">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main contribution of this paper is to show that this compromise also works for smt systems incorporating source language paraphrases in the inputs.
</prevsent>
<prevsent>regarding the use of paraphrases smt system, there are still other two categories of work that are related to this paper:?
</prevsent>
</prevsection>
<citsent citstr=" W07-0716 ">
using paraphrases to improve system optimization (madnani et al, 2007).<papid> W07-0716 </papid></citsent>
<aftsection>
<nextsent>with an english english mt system, this work utilises paraphrases to reduce the number of manually translated references that are needed in the parameter tuning process of smt, while preserved similar translation quality.?
</nextsent>
<nextsent>using paraphrases to smooth translation models (kuhn et al, 2010; <papid> C10-1069 </papid>max, 2010).</nextsent>
<nextsent>either cluster-based or example-based methods are 32 proposed to obtain better estimation on phrase translation probabilities with paraphrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2156">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using paraphrases to improve system optimization (madnani et al, 2007).<papid> W07-0716 </papid></prevsent>
<prevsent>with an english english mt system, this work utilises paraphrases to reduce the number of manually translated references that are needed in the parameter tuning process of smt, while preserved similar translation quality.?</prevsent>
</prevsection>
<citsent citstr=" C10-1069 ">
using paraphrases to smooth translation models (kuhn et al, 2010; <papid> C10-1069 </papid>max, 2010).</citsent>
<aftsection>
<nextsent>either cluster-based or example-based methods are 32 proposed to obtain better estimation on phrase translation probabilities with paraphrases.
</nextsent>
<nextsent>the rest of this paper is organized as follows:in section 2, we present an overview of the word lattice-based method and its drawbacks.
</nextsent>
<nextsent>section 3proposes the cn-based method, including the building process and its application on paraphrases in smt.
</nextsent>
<nextsent>section 4 presents the experiments and resultsof the proposed method as well as discussions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2167">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> word-lattice-based method.  </section>
<citcontext>
<prevsection>
<prevsent>this is because the aforementioned straightforward construction process does not track path duplications from different spans on the source side.
</prevsent>
<prevsent>since the number of admitted paraphrases is restricted by parameter in formula (4), the path duplication will decrease the paraphrase capacity to certain extend.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
moreover, state of the art pbsmt decoders (e.g. moses (koehn et al, 2007)) <papid> P07-2045 </papid>have much higher computational complexity for lattice structures thanfor sentences.</citsent>
<aftsection>
<nextsent>thus even though only the test sentences need to be transformed into word lattices, decoding time is still too slow for real-time applications.
</nextsent>
<nextsent>motivated by transforming asr word-graphs into cns (bertoldi et al, 2008), we adopt cn as the trade-off between efficiency and quality.
</nextsent>
<nextsent>we aim to merge duplicate paths in the word lattices to increase paraphrase capacity, and to speed up the decoding process via cn decoding.
</nextsent>
<nextsent>details of the proposed method are presented in the following section.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2168">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> confusion-network-based method.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 modified mt pipeline.
</prevsent>
<prevsent>by transforming word lattices into cns, duplicate paths are merged.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
furthermore the new features on the edges are introduced by formula (6), which is then tuned on the development set using mert (och, 2003) <papid> P03-1021 </papid>in the log-linear model (och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>since the smt decoders are able to perform cn decoding (bertoldi et al, 2008) in an efficient multi-stack decoding way, decoding time is drastically reduced compared to lattice decoding.the training steps are then modified as fol lows: 1) extract phrase table, reordering table, and build target-side language models from parallel and monolingual corpora respectively for the pbsmtmodel; 2) transform source sentences in the development set into word lattices, and then transform them into cns using the method proposed in section 3.1; 3) tune the pbsmt model on the cns via the development set.
</nextsent>
<nextsent>note that the overhead of the evaluation steps are: transform each test set sentence into word lattice, and also transform them into cn, then feed them into the smt decoder to obtain decoding results.
</nextsent>
<nextsent>4.1 experimental setup.
</nextsent>
<nextsent>experiments were carried out on three english chinese translation tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2169">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> confusion-network-based method.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 modified mt pipeline.
</prevsent>
<prevsent>by transforming word lattices into cns, duplicate paths are merged.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
furthermore the new features on the edges are introduced by formula (6), which is then tuned on the development set using mert (och, 2003) <papid> P03-1021 </papid>in the log-linear model (och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>since the smt decoders are able to perform cn decoding (bertoldi et al, 2008) in an efficient multi-stack decoding way, decoding time is drastically reduced compared to lattice decoding.the training steps are then modified as fol lows: 1) extract phrase table, reordering table, and build target-side language models from parallel and monolingual corpora respectively for the pbsmtmodel; 2) transform source sentences in the development set into word lattices, and then transform them into cns using the method proposed in section 3.1; 3) tune the pbsmt model on the cns via the development set.
</nextsent>
<nextsent>note that the overhead of the evaluation steps are: transform each test set sentence into word lattice, and also transform them into cn, then feed them into the smt decoder to obtain decoding results.
</nextsent>
<nextsent>4.1 experimental setup.
</nextsent>
<nextsent>experiments were carried out on three english chinese translation tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2174">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for the 2.1 million corpus, the nist 2005 chinese english current set (1,082 sentences) with one reference is used as the development set, and nist 2003 english chinese current set (1,859 sentences) with four references is used as the test set.
</prevsent>
<prevsent>three baseline systems are built for comparison: moses pbsmt baseline system (koehn et al, 2007), <papid> P07-2045 </papid>realization of the translation model augmentation system described in (callison-burch et al, 2006) (named para-sub?</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
hereafter), and the word-lattice based system proposed in (du et al, 2010).<papid> D10-1041 </papid>word alignments on the parallel corpus are performed using giza++ (och and ney, 2003) <papid> J03-1002 </papid>with the grow-diag-final?</citsent>
<aftsection>
<nextsent>refinement.
</nextsent>
<nextsent>maximum phrase length is set to 10 words and the parameters in the log-linear model are tuned by mert (och, 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>all the language models are 5-gram built with the srilm toolkit (stolcke, 2002) on the monolingual part of the parallel corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2176">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>3ldc number: ldc2007t09.
</prevsent>
<prevsent>36 figure 3: an example of real cn converted from paraphrase lattice.
</prevsent>
</prevsection>
<citsent citstr=" W09-0441 ">
note that it is subsection of the whole cn that is converted from the word lattice in figure 2.ble4 of ter-plus (snover et al, 2009).<papid> W09-0441 </papid></citsent>
<aftsection>
<nextsent>further more, the following two steps are taken to filter out noise paraphrases as described in (du et al, 2010): <papid> D10-1041 </papid>1.</nextsent>
<nextsent>filter out paraphrases with probabilities lower.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2178">
<title id=" W11-1004.xml">incorporating source language paraphrases into phrase based smt with confusion networks </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the filtered paraphrase table is then used to generate word lattices and cns.
</prevsent>
<prevsent>4.3 experimental results.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the results are reported in bleu (papineni et al, 2002) <papid> P02-1040 </papid>and ter (snover et al, 2006) scores.table 1 compares the performance of four systems on three translation tasks.</citsent>
<aftsection>
<nextsent>as can be observed from the table, for 20k and 200k corpora, theword-lattice-based system accomplished the best results.
</nextsent>
<nextsent>for the 20k corpus, the cn outperformed the baseline pbsmt by 0.31 absolute (2.15% relative) bleu points and 1.5 absolute (1.99% relative) ter points.
</nextsent>
<nextsent>for the 200k corpus, it still outperformed the para-sub?
</nextsent>
<nextsent>by 0.06 absolute (0.26%relative) bleu points and 0.15 absolute (0.23% rel ative) ter points.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2180">
<title id=" W11-0106.xml">a model for composing semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in computational linguistics, there have been several proposals to detect semantic relations.
</prevsent>
<prevsent>current approaches focus on particular set of relations and given text they output relations.
</prevsent>
</prevsection>
<citsent citstr=" W09-2415 ">
there have been competitions aiming at detecting semantic roles (i.e., relations between verb and its arguments) (carreras and ma`rquez, 2005), and between nominals (girju et al, 2007; hendrickx et al, 2009).<papid> W09-2415 </papid>in this paper, we propose model to compose semantic relations to extract previously ignored rela tions.</citsent>
<aftsection>
<nextsent>the model allows us to automatically obtain inference axioms given set of relations and is not coupled to any particular set.
</nextsent>
<nextsent>axioms take as their input semantic relations and yield new semantic relation as their conclusion.
</nextsent>
<nextsent>consider the sentence john went to the shop to buy flowers.
</nextsent>
<nextsent>figure 1 shows semantic role annotation with solid arrows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2182">
<title id=" W11-0106.xml">a model for composing semantic relations </title>
<section> case study: propbank.  </section>
<citcontext>
<prevsection>
<prevsent>two values for the same primitive are compatible unless they have different opposites or either value is ???
</prevsent>
<prevsent>(i.e., prohibited).
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
propbank (palmer et al, 2005) <papid> J05-1004 </papid>adds layer of predicate-argument information, or semantic role labels, on top of the syntactic trees provided by the penn treebank.</citsent>
<aftsection>
<nextsent>along with framenet, it is the resource most widely used for semantic role annotation.
</nextsent>
<nextsent>propbank uses series of numeric core roles (arg0 - arg5) and set of more general roles, argms (e.g. mtmp, mloc, mmnr).
</nextsent>
<nextsent>the interpretation of the numeric roles is determined by verb-specific frame sets, although arg0 and arg1 usually correspond to the prototypical agent and theme.
</nextsent>
<nextsent>on the other hand, the meaning of agrms generalize across verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2184">
<title id=" W11-0106.xml">a model for composing semantic relations </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>all these approaches, regardless of their particular details, take as their input text and output the relations found in it.
</prevsent>
<prevsent>in contrast, the framework proposed in this article obtains axioms that take as their input relations found in text and output more relations previously ignored.
</prevsent>
</prevsection>
<citsent citstr=" P08-2045 ">
generally, efforts to extract semantic relations have concentrated on particular sets of relations or single relation, e.g. cause (bethard and martin, 2008; <papid> P08-2045 </papid>chang and choi, 2006) and part-whole (girju et al, 2006).</citsent>
<aftsection>
<nextsent>automatic detection of semantic roles has received lot of attention lately (ma`rquez et al, 2008; carreras and ma`rquez, 2005).
</nextsent>
<nextsent>the semeval-2007 task 04 (girju et al, 2007) and semeval-2010 task 08 (hendrickx et al, 2009) <papid> W09-2415 </papid>aimed at relations between nominals.</nextsent>
<nextsent>there has been work on detecting relations within noun phrases (moldovan et al, 2004; <papid> W04-2609 </papid>nulty, 2007), <papid> P07-3014 </papid>clauses (szpakowicz et al, 1995) and syntax-based comma resolution (srikumar et al, 2008).<papid> P08-1117 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2186">
<title id=" W11-0106.xml">a model for composing semantic relations </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>automatic detection of semantic roles has received lot of attention lately (ma`rquez et al, 2008; carreras and ma`rquez, 2005).
</prevsent>
<prevsent>the semeval-2007 task 04 (girju et al, 2007) and semeval-2010 task 08 (hendrickx et al, 2009) <papid> W09-2415 </papid>aimed at relations between nominals.</prevsent>
</prevsection>
<citsent citstr=" W04-2609 ">
there has been work on detecting relations within noun phrases (moldovan et al, 2004; <papid> W04-2609 </papid>nulty, 2007), <papid> P07-3014 </papid>clauses (szpakowicz et al, 1995) and syntax-based comma resolution (srikumar et al, 2008).<papid> P08-1117 </papid></citsent>
<aftsection>
<nextsent>previous research has exploited the idea of using semantic primitives to define and classify semantic relations under different names.
</nextsent>
<nextsent>among others, the literature uses relation elements, deep structure, aspects and primitives.
</nextsent>
<nextsent>to the best of our knowledge, the first effort on describing semantic relations 52 using primitives was made by chaffin and herrmann (1987).
</nextsent>
<nextsent>they introduce relation element theory, and differentiate relations by relation elements.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2187">
<title id=" W11-0106.xml">a model for composing semantic relations </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>automatic detection of semantic roles has received lot of attention lately (ma`rquez et al, 2008; carreras and ma`rquez, 2005).
</prevsent>
<prevsent>the semeval-2007 task 04 (girju et al, 2007) and semeval-2010 task 08 (hendrickx et al, 2009) <papid> W09-2415 </papid>aimed at relations between nominals.</prevsent>
</prevsection>
<citsent citstr=" P07-3014 ">
there has been work on detecting relations within noun phrases (moldovan et al, 2004; <papid> W04-2609 </papid>nulty, 2007), <papid> P07-3014 </papid>clauses (szpakowicz et al, 1995) and syntax-based comma resolution (srikumar et al, 2008).<papid> P08-1117 </papid></citsent>
<aftsection>
<nextsent>previous research has exploited the idea of using semantic primitives to define and classify semantic relations under different names.
</nextsent>
<nextsent>among others, the literature uses relation elements, deep structure, aspects and primitives.
</nextsent>
<nextsent>to the best of our knowledge, the first effort on describing semantic relations 52 using primitives was made by chaffin and herrmann (1987).
</nextsent>
<nextsent>they introduce relation element theory, and differentiate relations by relation elements.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2188">
<title id=" W11-0106.xml">a model for composing semantic relations </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>automatic detection of semantic roles has received lot of attention lately (ma`rquez et al, 2008; carreras and ma`rquez, 2005).
</prevsent>
<prevsent>the semeval-2007 task 04 (girju et al, 2007) and semeval-2010 task 08 (hendrickx et al, 2009) <papid> W09-2415 </papid>aimed at relations between nominals.</prevsent>
</prevsection>
<citsent citstr=" P08-1117 ">
there has been work on detecting relations within noun phrases (moldovan et al, 2004; <papid> W04-2609 </papid>nulty, 2007), <papid> P07-3014 </papid>clauses (szpakowicz et al, 1995) and syntax-based comma resolution (srikumar et al, 2008).<papid> P08-1117 </papid></citsent>
<aftsection>
<nextsent>previous research has exploited the idea of using semantic primitives to define and classify semantic relations under different names.
</nextsent>
<nextsent>among others, the literature uses relation elements, deep structure, aspects and primitives.
</nextsent>
<nextsent>to the best of our knowledge, the first effort on describing semantic relations 52 using primitives was made by chaffin and herrmann (1987).
</nextsent>
<nextsent>they introduce relation element theory, and differentiate relations by relation elements.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2189">
<title id=" W10-4107.xml">reducing the false alarm rate of chinese character error detection and correction </title>
<section> data in experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the general n-gram formula is: )|()( 1 1 ? +?= nnn wwpsp (1) where was set to two for bigram and was set to one for unigram.
</prevsent>
<prevsent>the maximum likelihood estimation (mle) was used to train the n-gram model.
</prevsent>
</prevsection>
<citsent citstr=" P96-1041 ">
we adopted the interpolated kneser-ney smoothing method as suggested by chen &amp; goodman (1996).<papid> P96-1041 </papid></citsent>
<aftsection>
<nextsent>as following: )()1()|( )|( 1 1int wpwwp wwp unigramibigram ierpolate ??
</nextsent>
<nextsent>(2) to determine whether replacement is good or not, our system use the modified perplexity: nsp perplexity /))(log(2?= (3) where is the length of sentence and p(s) is the bigram probability of sentence after smoothing.
</nextsent>
<nextsent>3.3 dictionary and test set.
</nextsent>
<nextsent>we used free online dictionary provided by taiwans ministry of education, moe (2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2190">
<title id=" W11-0411.xml">proposal for an extension of traditional named entities from guidelines to evaluation an overview </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 named entity definitions.
</prevsent>
<prevsent>named entity recognition was first defined as recognizing proper names (coates-stephens, 1992).
</prevsent>
</prevsection>
<citsent citstr=" C96-1079 ">
since muc-6 (grishman and sundheim, 1996; <papid> C96-1079 </papid>saic, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations.</citsent>
<aftsection>
<nextsent>proposals were made to sub-divide these entities into finer-grained classes.
</nextsent>
<nextsent>the politicians?
</nextsent>
<nextsent>subclass was proposed for the person?
</nextsent>
<nextsent>class by (fleis chman and hovy, 2002) <papid> C02-1130 </papid>while the cities?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2192">
<title id=" W11-0411.xml">proposal for an extension of traditional named entities from guidelines to evaluation an overview </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the politicians?
</prevsent>
<prevsent>subclass was proposed for the person?
</prevsent>
</prevsection>
<citsent citstr=" C02-1130 ">
class by (fleis chman and hovy, 2002) <papid> C02-1130 </papid>while the cities?</citsent>
<aftsection>
<nextsent>subclass was added to the location?
</nextsent>
<nextsent>class by (fleischman, 2001; lee and lee, 2005).<papid> I05-1058 </papid></nextsent>
<nextsent>the conll conference added miscellaneous type that includes proper names falling outside the previous classes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2193">
<title id=" W11-0411.xml">proposal for an extension of traditional named entities from guidelines to evaluation an overview </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>class by (fleis chman and hovy, 2002) <papid> C02-1130 </papid>while the cities?</prevsent>
<prevsent>subclass was added to the location?</prevsent>
</prevsection>
<citsent citstr=" I05-1058 ">
class by (fleischman, 2001; lee and lee, 2005).<papid> I05-1058 </papid></citsent>
<aftsection>
<nextsent>the conll conference added miscellaneous type that includes proper names falling outside the previous classes.
</nextsent>
<nextsent>some classes have thus sometimes been added, e.g. the product?
</nextsent>
<nextsent>class by (bick, 2004; galliano et al, 2009).
</nextsent>
<nextsent>92 specific entities are proposed and handled insome tasks: language?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2194">
<title id=" W11-0411.xml">proposal for an extension of traditional named entities from guidelines to evaluation an overview </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>at the same time, extensions of named entities have beenproposed: (sekine, 2004) defined complete hierarchy of named entities containing about 200 types.
</prevsent>
<prevsent>2.2 named entities and annotation.
</prevsent>
</prevsection>
<citsent citstr=" W09-3025 ">
as for any other kind of annotation, some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (ehrmann, 2008;fort et al, 2009).<papid> W09-3025 </papid></citsent>
<aftsection>
<nextsent>three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context(paris?
</nextsent>
<nextsent>can be town or person name); (2) detecting the boundaries (in person designation, is only the proper name to be annotated or the trigger mr? too?)
</nextsent>
<nextsent>and (3) annotating metonymies (france?
</nextsent>
<nextsent>can be sports team, country, etc.).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2196">
<title id=" W11-0411.xml">proposal for an extension of traditional named entities from guidelines to evaluation an overview </title>
<section> taxonomy.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 guidelines production.
</prevsent>
<prevsent>having in mind the objective of building fact database through the extraction of named entities from texts, we defined richer taxonomy than those used in other information extraction works.
</prevsent>
</prevsection>
<citsent citstr=" W10-1804 ">
following (bonneau-maynard et al, 2005; alex et al, 2010), <papid> W10-1804 </papid>the annotation guidelines were first written from december 2009 to may 2010 by three researchers managing the manual annotation cam paign.</citsent>
<aftsection>
<nextsent>during guidelines production, we evaluated the feasibility of this specific annotation task and the usefulness of the guidelines by annotating small part of the target corpus.
</nextsent>
<nextsent>then, these guidelines were delivered to the annotators.
</nextsent>
<nextsent>they consist of adescription of the objects to annotate, general annotation rules and principles, and more than 250 prototypical and real examples extracted from the corpus (rosset et al, 2010).
</nextsent>
<nextsent>rules are important to setthe general way annotations must be produced.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2201">
<title id=" W11-0411.xml">proposal for an extension of traditional named entities from guidelines to evaluation an overview </title>
<section> inter-annotator agreement.  </section>
<citcontext>
<prevsection>
<prevsent>it is therefore impossible to really evaluate the validity of an annotation.
</prevsent>
<prevsent>all we can and should do is to evaluate its reliability, i.e. the consistency of the annotation across annotators, which is achieved through computation of the inter-annotator agreement (iaa).
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
97 the best way to compute it is to use one of the kappa family coefficients, namely cohens kappa (cohen, 1960) or scotts pi (scott, 1955), also known as carlettas kappa (carletta, 1996),<papid> J96-2004 </papid>11as they take chance into account (artstein and poe sio, 2008).<papid> J08-4004 </papid></citsent>
<aftsection>
<nextsent>however, these coefficients imply comparison with random baseline?
</nextsent>
<nextsent>to establish whether the correlation between annotations is statistically significant.
</nextsent>
<nextsent>this baseline depends on the number of markables?, i.e. all the units that could be annotated.
</nextsent>
<nextsent>in the case of named entities, as in many others, this random baseline?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2202">
<title id=" W11-0411.xml">proposal for an extension of traditional named entities from guidelines to evaluation an overview </title>
<section> inter-annotator agreement.  </section>
<citcontext>
<prevsection>
<prevsent>it is therefore impossible to really evaluate the validity of an annotation.
</prevsent>
<prevsent>all we can and should do is to evaluate its reliability, i.e. the consistency of the annotation across annotators, which is achieved through computation of the inter-annotator agreement (iaa).
</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
97 the best way to compute it is to use one of the kappa family coefficients, namely cohens kappa (cohen, 1960) or scotts pi (scott, 1955), also known as carlettas kappa (carletta, 1996),<papid> J96-2004 </papid>11as they take chance into account (artstein and poe sio, 2008).<papid> J08-4004 </papid></citsent>
<aftsection>
<nextsent>however, these coefficients imply comparison with random baseline?
</nextsent>
<nextsent>to establish whether the correlation between annotations is statistically significant.
</nextsent>
<nextsent>this baseline depends on the number of markables?, i.e. all the units that could be annotated.
</nextsent>
<nextsent>in the case of named entities, as in many others, this random baseline?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2205">
<title id=" W11-0411.xml">proposal for an extension of traditional named entities from guidelines to evaluation an overview </title>
<section> conclusion and perspectives.  </section>
<citcontext>
<prevsection>
<prevsent>the organizers also annotated small part of the corpus to build mini reference corpus.
</prevsent>
<prevsent>we evaluated the human annotations with our mini-reference corpus: the actual computed ? is between 0.71 et 0.85 which, given the complexity of the task, seems to indicate good annotation quality.
</prevsent>
</prevsection>
<citsent citstr=" W09-3002 ">
our results are consistent with other studies (dandapat et al, 2009) <papid> W09-3002 </papid>in demonstrating that human annotators?</citsent>
<aftsection>
<nextsent>training is key asset to produce quality annotations.
</nextsent>
<nextsent>we also saw that guidelines are never fixed, but evolve all along the annotation process due to feedback between annotators and organizers; the relationship between guidelines producers and human annotators evolved from parent?
</nextsent>
<nextsent>to peer?
</nextsent>
<nextsent>(akrich and boullier, 1991).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2206">
<title id=" W11-0317.xml">using second order vectors in a knowledge based method for acronym disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>acronyms also tend to be noun phrases, therefore syntactic features do not provide relevant information for the purposes of disambiguation.
</prevsent>
<prevsent>identifying the correct long-form of an acronym is important not only for the retrieval of information but the understanding of the information by there cipient.
</prevsent>
</prevsection>
<citsent citstr=" W01-0516 ">
in general english, park and byrd (2001) <papid> W01-0516 </papid>note that acronym disambiguation is not widely studied because acronyms are not as prevalent in literature and newspaper articles as they are in specific domains such as government, law, and biomedicine.</citsent>
<aftsection>
<nextsent>in the biomedical sublanguage domain, acronym disambiguation is an extensively studied problem.pakhomov (2002) <papid> P02-1021 </papid>note acronyms in biomedical literature tend to be used much more frequently than in news media or general english literature, and tendto be highly ambiguous.</nextsent>
<nextsent>for example, the unified medical language system (umls), which includes one of the largest terminology resources inthe biomedical domain, contains 11 possible long forms of the acronym ms in addition to the four examples used above.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2207">
<title id=" W11-0317.xml">using second order vectors in a knowledge based method for acronym disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>identifying the correct long-form of an acronym is important not only for the retrieval of information but the understanding of the information by there cipient.
</prevsent>
<prevsent>in general english, park and byrd (2001) <papid> W01-0516 </papid>note that acronym disambiguation is not widely studied because acronyms are not as prevalent in literature and newspaper articles as they are in specific domains such as government, law, and biomedicine.</prevsent>
</prevsection>
<citsent citstr=" P02-1021 ">
in the biomedical sublanguage domain, acronym disambiguation is an extensively studied problem.pakhomov (2002) <papid> P02-1021 </papid>note acronyms in biomedical literature tend to be used much more frequently than in news media or general english literature, and tendto be highly ambiguous.</citsent>
<aftsection>
<nextsent>for example, the unified medical language system (umls), which includes one of the largest terminology resources inthe biomedical domain, contains 11 possible long forms of the acronym ms in addition to the four examples used above.
</nextsent>
<nextsent>liu et al  (2001) show that 33% of acronyms are ambiguous in the umls.
</nextsent>
<nextsent>in subsequent study, liu et al  (2002a) found that 80% of all acronyms found in medline, large repository of abstracts from biomedical journals, are ambiguous.
</nextsent>
<nextsent>wren and garner (2002) found that there exist 174,000 unique acronyms in the medline abstracts in which 36% of them are ambiguous.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2209">
<title id=" W11-0317.xml">using second order vectors in a knowledge based method for acronym disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>supervised and semi-supervised methods have been used successfully for acronym disambiguation 145 but are limited in scope due to the need for sufficient training data.
</prevsent>
<prevsent>liu et al  (2004) state that an acronym could have approximately 16 possible long-forms in medline but could not obtain sufficient number of instances for each of the acronym-long-form pairs for their experiments.
</prevsent>
</prevsection>
<citsent citstr=" W09-1309 ">
stevenson et al  (2009) <papid> W09-1309 </papid>citea similar problem indicating that acronym disambiguation methods that do not require training data, regardless if it is created manually or automatically, are needed.in this paper, we introduce novel knowledge based method to disambiguate acronyms using second-order co-occurrence vectors.</citsent>
<aftsection>
<nextsent>this method does not relyon training data, and therefore, is not limited to disambiguating only commonly occurring possible long-forms.
</nextsent>
<nextsent>these vectors are created using the first-order features obtained from the umls about the acronyms long-forms and second-orderfeatures obtained from medline.
</nextsent>
<nextsent>we show that using second-order features provide distinct representation of the long-form for the purposes of disambiguation and obtains significantly higher disambiguation accuracy than using first order features.
</nextsent>
<nextsent>the unified medical language system (umls) is data warehouse that stores number of distinct biomedical and clinical resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2225">
<title id=" W11-0317.xml">using second order vectors in a knowledge based method for acronym disambiguation </title>
<section> acronym disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>joshi (2006) expands on liu, et al work.
</prevsent>
<prevsent>they evaluate additional machine learning algorithms using unigrams, bigrams and trigrams as features.they found that given their feature set, svms obtain the highest accuracy (97%).stevenson et al  (2009) <papid> W09-1309 </papid>re-recreate this dataset using the method described in liu et al  (2001) to automatically create training data for their method which uses mixture of linguistics features (e.g., collocations, unigrams, bigrams and trigrams) in combination with the biomedical features cuis and medical subject headings, which are terms manually assigned to medline abstracts for indexing purposes.</prevsent>
</prevsection>
<citsent citstr=" W04-0813 ">
the authors evaluate the naive bayes, svm and vector space model (vsm) described by agirre and martinez (2004), <papid> W04-0813 </papid>and report that vsm obtained the highest accuracy (99%).pakhomov (2002) <papid> P02-1021 </papid>also developed semi supervised method in which training data was automatically created by first identifying the long form found in the text of clinical reports, replacing the long-form with the acronym to use as training data.</citsent>
<aftsection>
<nextsent>a maximum entropy model trained and tested on corpus of 10,000 clinical notes achieved an accuracy of 89%.
</nextsent>
<nextsent>in subsequent study, pakhomov et al  (2005) evaluate obtaining training data from three sources: medline, clinical records and the world wide web finding using combination of instances from clinical records and the web obtained the highest accuracy.
</nextsent>
<nextsent>joshi et al  (2006) compare using the naive bayes, decision trees and svm on ambiguous acronyms found in clinical reports.
</nextsent>
<nextsent>the authors use the part-of-speech, the unigrams and the bi grams of the context surrounding the acronym as features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2228">
<title id=" W11-0317.xml">using second order vectors in a knowledge based method for acronym disambiguation </title>
<section> word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>many knowledge-based wsd methods have been developed to disambiguate terms which are closely related to the work presented in this paper.
</prevsent>
<prevsent>lesk (1986) proposes definition overlap method in which the appropriate sense of an ambiguous term was determined based on the overlap between its definition in machine readable dictionary (mrd).
</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
ide and veronis (1998) <papid> J98-1001 </papid>note that this work provideda basis for most future mrd disambiguation meth ods; including the one presented in this paper.</citsent>
<aftsection>
<nextsent>banerjee and pedersen (2002) use the lesksoverlap method to determine the relatedness between two concepts (synsets) in wordnet.
</nextsent>
<nextsent>they extend the method to not only include the definition (gloss) of the two synsets in the overlap but also the glosses of related synsets.
</nextsent>
<nextsent>wilks et al  (1990) expand upon lesks method by calculating the number of times the words in the definition co-occur with the ambiguous words.
</nextsent>
<nextsent>in their method, vector is created using the co-occurrence information for the ambiguous word and each of its possible senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2229">
<title id=" W11-0317.xml">using second order vectors in a knowledge based method for acronym disambiguation </title>
<section> word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>the similarity is then calculated between the ambiguous words vector and each of the sense vectors.
</prevsent>
<prevsent>the sense whose vector is most similar is assigned to the ambiguous word.
</prevsent>
</prevsection>
<citsent citstr=" W06-2501 ">
147 0 .3 0 0 0 0 0 0 dis phosphoric glucose fructose phosphoric esters changed effect 0 0 0 0 0 glycolyte en zym es co bined decreases intensity acid 0 etabolites features 0 0 0 0 .2 0 acid 0 0 0 .1 0 0 0 0 0 0 .5 0 0 esters 0 0 0 0 0 0 0 .1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 fructose 0 0 0 0 0 0 0 0 0 0 0 0 0 diphosphate 0 0 0 0 0 0 isomer 0 0 0 0 0 0 0 prevalent 0 0 0 0 0 0 0 .1 0 .3 .5 .2 0 2nd order vector for fructose diphosphate 0 0 0 .1 0 0 ex te nd ed ef in ition fo f ru ct os di ph os ph at figure 1: 2nd order vector for fructose diphosphate (fdp)patwardhan and pedersen (2006) <papid> W06-2501 </papid>introduce vector measure to determine the relatedness between pairs of concepts.</citsent>
<aftsection>
<nextsent>in this measure, second orderco-occurrence vector is created for each concept using the words in each of the concepts definition and calculating the cosine between the two vectors.
</nextsent>
<nextsent>this method has been used in the task of wsd by calculating the relatedness between each possible sense of the ambiguous word and its surrounding context.the context whose sum is the most similar is assigned to the ambiguous word.second-order co-occurrence vectors were first introduced by schutze (1992) for the task of word sense discrimination and later extended by purandare and pedersen (2004).<papid> W04-2406 </papid></nextsent>
<nextsent>as noted by pedersen (2010), disambiguation requires sense-inventory in which the long-forms are known ahead of time, where as in discrimination this information is not known priori.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2230">
<title id=" W11-0317.xml">using second order vectors in a knowledge based method for acronym disambiguation </title>
<section> word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>147 0 .3 0 0 0 0 0 0 dis phosphoric glucose fructose phosphoric esters changed effect 0 0 0 0 0 glycolyte en zym es co bined decreases intensity acid 0 etabolites features 0 0 0 0 .2 0 acid 0 0 0 .1 0 0 0 0 0 0 .5 0 0 esters 0 0 0 0 0 0 0 .1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 fructose 0 0 0 0 0 0 0 0 0 0 0 0 0 diphosphate 0 0 0 0 0 0 isomer 0 0 0 0 0 0 0 prevalent 0 0 0 0 0 0 0 .1 0 .3 .5 .2 0 2nd order vector for fructose diphosphate 0 0 0 .1 0 0 ex te nd ed ef in ition fo f ru ct os di ph os ph at figure 1: 2nd order vector for fructose diphosphate (fdp)patwardhan and pedersen (2006) <papid> W06-2501 </papid>introduce vector measure to determine the relatedness between pairs of concepts.</prevsent>
<prevsent>in this measure, second orderco-occurrence vector is created for each concept using the words in each of the concepts definition and calculating the cosine between the two vectors.</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
this method has been used in the task of wsd by calculating the relatedness between each possible sense of the ambiguous word and its surrounding context.the context whose sum is the most similar is assigned to the ambiguous word.second-order co-occurrence vectors were first introduced by schutze (1992) for the task of word sense discrimination and later extended by purandare and pedersen (2004).<papid> W04-2406 </papid></citsent>
<aftsection>
<nextsent>as noted by pedersen (2010), disambiguation requires sense-inventory in which the long-forms are known ahead of time, where as in discrimination this information is not known priori.
</nextsent>
<nextsent>in our method, second-order co-occurrence vector is created for each possible long-form of the acronym, and the acronym itself.
</nextsent>
<nextsent>the appropriate long-form of the acronym is then determined by computing cosine between the vector representing the ambiguous acronym and each of the vectors representing the long-forms.
</nextsent>
<nextsent>the long-form whose vector has the smallest angle between it and the acronym vector is chosen as the most likely long form of the acronym.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2231">
<title id=" W11-0317.xml">using second order vectors in a knowledge based method for acronym disambiguation </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>the fructose diphosphate isomer is most prevalent.
</prevsent>
<prevsent>fructose diphosphate.after the extended definition is obtained, we create the second-order vector by first creating word by word co-occurrence matrix in which the rows represent the content words in the long-forms, extended definition, and the columns represent words that co-occur in medline abstracts with the words in the definition.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
each cell in this matrix contains the log likelihood ratio (dunning (1993)) <papid> J93-1003 </papid>of the word found in the row and the word in the column.</citsent>
<aftsection>
<nextsent>second, each word in the long-forms, extended definition is replaced by its corresponding vector, as given in the co-occurrence matrix.
</nextsent>
<nextsent>the centro id of these vectors constitutes the second order co-occurrence vector used to represent the long-form.for example, given the example corpus containing two instances: 1) the metabolites, glucose fructose and their phosphoric acid esters are changed due to the effect of glycolytic enzymes, and 2)the phosphoric acid combined with metabolites decreases the intensity.
</nextsent>
<nextsent>figure 1 shows how the second-order co-occurrence vector is created for the long-form fructose diphosphate using the extended definition and features from our given corpus above.
</nextsent>
<nextsent>the second-order co-occurrence vector for the ambiguous acronym is created in similar fashion,only rather than using words in the extended definition, we use the words surrounding the acronym in the instance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2287">
<title id=" W11-0134.xml">using topic salience and connotational drifts to detect candidates to semantic change </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to measure semantic change, one has to evaluate the semantics of lexical item at given point.
</prevsent>
<prevsent>to do so, semantic similarity measures in vector spaces or geometrical spaces may be used to compare the 315 item with its own occurrences at later points.
</prevsent>
</prevsection>
<citsent citstr=" W09-0214 ">
this method has been applied in sagi et al (2009), <papid> W09-0214 </papid>where semantic density was calculated as the average angle between vectors in semantic space.</citsent>
<aftsection>
<nextsent>the variability of that density was observed for the same lexical item at different points in time.
</nextsent>
<nextsent>density measures were applied to series of acknowledged semantic change cases, in the project gutenberg corpus, historical corpus of english organized by documents.
</nextsent>
<nextsent>results mostly include broadening and narrowing cases.
</nextsent>
<nextsent>the same method yielded results on the difference between nominal and verbal types of change, showing that verbs were more likely to change than nouns (sagi (2010)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2288">
<title id=" W10-4201.xml">comparing rating scales and preference judgements in language evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>individual evaluators have different tendencies in using rating scales, e.g. what is known as end-aversion?
</prevsent>
<prevsent>tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. choi and pak, (2005).
</prevsent>
</prevsection>
<citsent citstr=" C04-1072 ">
it is not surprising then that stable averages of quality judgements, let al ne high levels of agreement, are hard to achieve, as has been observed for mt (turian et al , 2003; lin and och, 2004), <papid> C04-1072 </papid>text summarisation (trang dang, 2006), <papid> W06-0707 </papid>and language generation (belz and reiter, 2006).<papid> E06-1040 </papid></citsent>
<aftsection>
<nextsent>it has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (duc literature).the result of rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale).
</nextsent>
<nextsent>the means-based ranks and statistical significance tests that are commonly presented with the results of rses are not generally considered appropriate for ordinal data in the statistics literature (siegel, 1957).
</nextsent>
<nextsent>at the least, test on the means imposes the requirement that the measures must be additive, i.e. numerical?
</nextsent>
<nextsent>(siegel, 1957, p. 14).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2289">
<title id=" W10-4201.xml">comparing rating scales and preference judgements in language evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>individual evaluators have different tendencies in using rating scales, e.g. what is known as end-aversion?
</prevsent>
<prevsent>tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. choi and pak, (2005).
</prevsent>
</prevsection>
<citsent citstr=" W06-0707 ">
it is not surprising then that stable averages of quality judgements, let al ne high levels of agreement, are hard to achieve, as has been observed for mt (turian et al , 2003; lin and och, 2004), <papid> C04-1072 </papid>text summarisation (trang dang, 2006), <papid> W06-0707 </papid>and language generation (belz and reiter, 2006).<papid> E06-1040 </papid></citsent>
<aftsection>
<nextsent>it has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (duc literature).the result of rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale).
</nextsent>
<nextsent>the means-based ranks and statistical significance tests that are commonly presented with the results of rses are not generally considered appropriate for ordinal data in the statistics literature (siegel, 1957).
</nextsent>
<nextsent>at the least, test on the means imposes the requirement that the measures must be additive, i.e. numerical?
</nextsent>
<nextsent>(siegel, 1957, p. 14).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2290">
<title id=" W10-4201.xml">comparing rating scales and preference judgements in language evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>individual evaluators have different tendencies in using rating scales, e.g. what is known as end-aversion?
</prevsent>
<prevsent>tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. choi and pak, (2005).
</prevsent>
</prevsection>
<citsent citstr=" E06-1040 ">
it is not surprising then that stable averages of quality judgements, let al ne high levels of agreement, are hard to achieve, as has been observed for mt (turian et al , 2003; lin and och, 2004), <papid> C04-1072 </papid>text summarisation (trang dang, 2006), <papid> W06-0707 </papid>and language generation (belz and reiter, 2006).<papid> E06-1040 </papid></citsent>
<aftsection>
<nextsent>it has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (duc literature).the result of rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale).
</nextsent>
<nextsent>the means-based ranks and statistical significance tests that are commonly presented with the results of rses are not generally considered appropriate for ordinal data in the statistics literature (siegel, 1957).
</nextsent>
<nextsent>at the least, test on the means imposes the requirement that the measures must be additive, i.e. numerical?
</nextsent>
<nextsent>(siegel, 1957, p. 14).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2291">
<title id=" W10-4201.xml">comparing rating scales and preference judgements in language evaluation </title>
<section> grec-neg rse/pje: named entity.  </section>
<citcontext>
<prevsection>
<prevsent>the grec-neg data2 consists of introduction sections from wikipedia articles about people in which all mentions of people have been annotated by marking up the word strings that function as referential expressions (res) and annotating them with coreference information as well as syntactic and semantic features.
</prevsent>
<prevsent>the following is an example of an annotated re from the corpus:  ref entity= 0  mention= 1  semcat= person  syncat= np  synfunc= subj   refex entity= 0  reg08-type= name 2the grec-neg data and documentation is available for download from http://www.nltg.brighton.ac.uk/home/anja.belz case= plain  sir alexander fleming /refex   /ref (6 august 1881 - 11 march 1955) was scottish biologist and pharmacologist.
</prevsent>
</prevsection>
<citsent citstr=" W09-2817 ">
this data was used in the grec-neg09 shared-task competition (belz et al , 2009), <papid> W09-2817 </papid>where the task was to create systems which automatically select suitable res for all references to all person entities in text.the evaluation experiments use clarity and fluency as quality criteria which were explained in the introduction as follows (the wording of the first is from duc): 1.</citsent>
<aftsection>
<nextsent>referential clarity: it should be easy to identify who.
</nextsent>
<nextsent>the referring expressions are referring to.
</nextsent>
<nextsent>if person is mentioned, it should be clear what their role in the story is. so, reference would be unclear if person is referenced, but their identity or relation to the story remains unclear.
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2292">
<title id=" W10-4201.xml">comparing rating scales and preference judgements in language evaluation </title>
<section> meteo rse/pje: weather forecasts.  </section>
<citcontext>
<prevsection>
<prevsent>the wind forecast texts were taken from comprehensive maritime weather forecasts produced by the professional meteorologists employed by commercial weather forecasting company for clients who run offshore oilrigs.
</prevsent>
<prevsent>there were two evaluation criteria; clarity was explained as indicating how understandable forecast was, and readability as indicating how fluent and readable it was.
</prevsent>
</prevsection>
<citsent citstr=" W09-0603 ">
the experiment involved 22forecast dates and outputs from the 10 systems described in (belz and kow, 2009) <papid> W09-0603 </papid>also included in the corpus release) for those dates (as well as the corresponding forecasts in the corpus) in the evaluation, i.e. total of 242 forecast texts.</citsent>
<aftsection>
<nextsent>4.2 rating scale experiment.
</nextsent>
<nextsent>we used the results of previous experiment (belz and kow, 2009) <papid> W09-0603 </papid>in which participants were asked to rate forecast texts for clarity and readability, each on scale of 17.</nextsent>
<nextsent>the 22 participants were all university of brighton staff whose first language was english and who had no experience of nlp.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2296">
<title id=" W11-0817.xml">detecting noun compounds and light verb constructions a contrastive study </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, they are less frequent in language use (only 9.5% of multiword expressions were light verb constructions in the wikipedia database) and they are syntactically flexible, that is, they can manifest in various forms: the verb can be inflected, the noun can occur in its plural form and the noun can be modified.
</prevsent>
<prevsent>the nominal and the verbal component may not even be adjacent in e.g. passive sentences.our goal being to compare how different approaches perform in the case of the different types of multiword expressions, we have chosen these two types of mwes that are dissimilar in several aspects.
</prevsent>
</prevsection>
<citsent citstr=" W03-1807 ">
there are several applications developed for identifying mwes, which can be classified according to the methods they make use of (piao et al, 2003).<papid> W03-1807 </papid></citsent>
<aftsection>
<nextsent>first, statistical models relyon word frequencies, co-occurrence data and contextual information in deciding whether bigram or trigram (or even ann-gram) of words can be labeled as multiword expression or not.
</nextsent>
<nextsent>such systems are used for several 116languages and several types of multiword expressions, see e.g. bouma (2010).<papid> P10-2020 </papid></nextsent>
<nextsent>the advantage of statistical systems is that they can be easily adapted to other languages and other types of multiword expressions, however, they are not able to identify rare multiword expressions (as piao et al (2003) <papid> W03-1807 </papid>emphasize, 68% of multiword expressions occur at most twice in their corpus).some hybrid systems make use of both statistical and linguistic information as well, that is, rules based on syntactic or semantic regularities are also incorporated into the system (evert and kermes, 2003; <papid> E03-1080 </papid>bannard, 2007; <papid> W07-1101 </papid>cook et al, 2007; <papid> W07-1106 </papid>al-haj and wintner, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2297">
<title id=" W11-0817.xml">detecting noun compounds and light verb constructions a contrastive study </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there are several applications developed for identifying mwes, which can be classified according to the methods they make use of (piao et al, 2003).<papid> W03-1807 </papid></prevsent>
<prevsent>first, statistical models relyon word frequencies, co-occurrence data and contextual information in deciding whether bigram or trigram (or even ann-gram) of words can be labeled as multiword expression or not.</prevsent>
</prevsection>
<citsent citstr=" P10-2020 ">
such systems are used for several 116languages and several types of multiword expressions, see e.g. bouma (2010).<papid> P10-2020 </papid></citsent>
<aftsection>
<nextsent>the advantage of statistical systems is that they can be easily adapted to other languages and other types of multiword expressions, however, they are not able to identify rare multiword expressions (as piao et al (2003) <papid> W03-1807 </papid>emphasize, 68% of multiword expressions occur at most twice in their corpus).some hybrid systems make use of both statistical and linguistic information as well, that is, rules based on syntactic or semantic regularities are also incorporated into the system (evert and kermes, 2003; <papid> E03-1080 </papid>bannard, 2007; <papid> W07-1101 </papid>cook et al, 2007; <papid> W07-1106 </papid>al-haj and wintner, 2010).</nextsent>
<nextsent>this results in better coverage of multiword expressions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2299">
<title id=" W11-0817.xml">detecting noun compounds and light verb constructions a contrastive study </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>first, statistical models relyon word frequencies, co-occurrence data and contextual information in deciding whether bigram or trigram (or even ann-gram) of words can be labeled as multiword expression or not.
</prevsent>
<prevsent>such systems are used for several 116languages and several types of multiword expressions, see e.g. bouma (2010).<papid> P10-2020 </papid></prevsent>
</prevsection>
<citsent citstr=" E03-1080 ">
the advantage of statistical systems is that they can be easily adapted to other languages and other types of multiword expressions, however, they are not able to identify rare multiword expressions (as piao et al (2003) <papid> W03-1807 </papid>emphasize, 68% of multiword expressions occur at most twice in their corpus).some hybrid systems make use of both statistical and linguistic information as well, that is, rules based on syntactic or semantic regularities are also incorporated into the system (evert and kermes, 2003; <papid> E03-1080 </papid>bannard, 2007; <papid> W07-1101 </papid>cook et al, 2007; <papid> W07-1106 </papid>al-haj and wintner, 2010).</citsent>
<aftsection>
<nextsent>this results in better coverage of multiword expressions.
</nextsent>
<nextsent>on the other hand, these methods are highly language-dependent because ofthe amount of linguistic rules encoded, thus, it requires much effort to adapt them to different languages or even to different types of multiword expressions.
</nextsent>
<nextsent>however, the combination of different methods may improve the performance of mwe extracting systems (pecina, 2010).several features are used in identifying multiword expressions, which are applicable to different types of multiword expressions to various degrees.
</nextsent>
<nextsent>co-occurrence statistics and pos-tags seem to be useful for all types of multiword expressions, for instance the tool mwe toolkit (ramisch et al,2010<papid> C10-3015 </papid>a) makes use of such features, which is illustrated through the example of identifying english compound nouns (ramisch et al, 2010<papid> C10-3015 </papid>b).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2300">
<title id=" W11-0817.xml">detecting noun compounds and light verb constructions a contrastive study </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>first, statistical models relyon word frequencies, co-occurrence data and contextual information in deciding whether bigram or trigram (or even ann-gram) of words can be labeled as multiword expression or not.
</prevsent>
<prevsent>such systems are used for several 116languages and several types of multiword expressions, see e.g. bouma (2010).<papid> P10-2020 </papid></prevsent>
</prevsection>
<citsent citstr=" W07-1101 ">
the advantage of statistical systems is that they can be easily adapted to other languages and other types of multiword expressions, however, they are not able to identify rare multiword expressions (as piao et al (2003) <papid> W03-1807 </papid>emphasize, 68% of multiword expressions occur at most twice in their corpus).some hybrid systems make use of both statistical and linguistic information as well, that is, rules based on syntactic or semantic regularities are also incorporated into the system (evert and kermes, 2003; <papid> E03-1080 </papid>bannard, 2007; <papid> W07-1101 </papid>cook et al, 2007; <papid> W07-1106 </papid>al-haj and wintner, 2010).</citsent>
<aftsection>
<nextsent>this results in better coverage of multiword expressions.
</nextsent>
<nextsent>on the other hand, these methods are highly language-dependent because ofthe amount of linguistic rules encoded, thus, it requires much effort to adapt them to different languages or even to different types of multiword expressions.
</nextsent>
<nextsent>however, the combination of different methods may improve the performance of mwe extracting systems (pecina, 2010).several features are used in identifying multiword expressions, which are applicable to different types of multiword expressions to various degrees.
</nextsent>
<nextsent>co-occurrence statistics and pos-tags seem to be useful for all types of multiword expressions, for instance the tool mwe toolkit (ramisch et al,2010<papid> C10-3015 </papid>a) makes use of such features, which is illustrated through the example of identifying english compound nouns (ramisch et al, 2010<papid> C10-3015 </papid>b).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2303">
<title id=" W11-0817.xml">detecting noun compounds and light verb constructions a contrastive study </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>first, statistical models relyon word frequencies, co-occurrence data and contextual information in deciding whether bigram or trigram (or even ann-gram) of words can be labeled as multiword expression or not.
</prevsent>
<prevsent>such systems are used for several 116languages and several types of multiword expressions, see e.g. bouma (2010).<papid> P10-2020 </papid></prevsent>
</prevsection>
<citsent citstr=" W07-1106 ">
the advantage of statistical systems is that they can be easily adapted to other languages and other types of multiword expressions, however, they are not able to identify rare multiword expressions (as piao et al (2003) <papid> W03-1807 </papid>emphasize, 68% of multiword expressions occur at most twice in their corpus).some hybrid systems make use of both statistical and linguistic information as well, that is, rules based on syntactic or semantic regularities are also incorporated into the system (evert and kermes, 2003; <papid> E03-1080 </papid>bannard, 2007; <papid> W07-1101 </papid>cook et al, 2007; <papid> W07-1106 </papid>al-haj and wintner, 2010).</citsent>
<aftsection>
<nextsent>this results in better coverage of multiword expressions.
</nextsent>
<nextsent>on the other hand, these methods are highly language-dependent because ofthe amount of linguistic rules encoded, thus, it requires much effort to adapt them to different languages or even to different types of multiword expressions.
</nextsent>
<nextsent>however, the combination of different methods may improve the performance of mwe extracting systems (pecina, 2010).several features are used in identifying multiword expressions, which are applicable to different types of multiword expressions to various degrees.
</nextsent>
<nextsent>co-occurrence statistics and pos-tags seem to be useful for all types of multiword expressions, for instance the tool mwe toolkit (ramisch et al,2010<papid> C10-3015 </papid>a) makes use of such features, which is illustrated through the example of identifying english compound nouns (ramisch et al, 2010<papid> C10-3015 </papid>b).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2306">
<title id=" W11-0817.xml">detecting noun compounds and light verb constructions a contrastive study </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, these methods are highly language-dependent because ofthe amount of linguistic rules encoded, thus, it requires much effort to adapt them to different languages or even to different types of multiword expressions.
</prevsent>
<prevsent>however, the combination of different methods may improve the performance of mwe extracting systems (pecina, 2010).several features are used in identifying multiword expressions, which are applicable to different types of multiword expressions to various degrees.
</prevsent>
</prevsection>
<citsent citstr=" C10-3015 ">
co-occurrence statistics and pos-tags seem to be useful for all types of multiword expressions, for instance the tool mwe toolkit (ramisch et al,2010<papid> C10-3015 </papid>a) makes use of such features, which is illustrated through the example of identifying english compound nouns (ramisch et al, 2010<papid> C10-3015 </papid>b).</citsent>
<aftsection>
<nextsent>caseli et al (2010) developed an alignment-based method for extracting multiword expressions from parallel corpora.
</nextsent>
<nextsent>this method is also applied to the pediatrics domain (caseli et al, 2009).<papid> W09-2901 </papid></nextsent>
<nextsent>zarrie?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2310">
<title id=" W11-0817.xml">detecting noun compounds and light verb constructions a contrastive study </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>co-occurrence statistics and pos-tags seem to be useful for all types of multiword expressions, for instance the tool mwe toolkit (ramisch et al,2010<papid> C10-3015 </papid>a) makes use of such features, which is illustrated through the example of identifying english compound nouns (ramisch et al, 2010<papid> C10-3015 </papid>b).</prevsent>
<prevsent>caseli et al (2010) developed an alignment-based method for extracting multiword expressions from parallel corpora.</prevsent>
</prevsection>
<citsent citstr=" W09-2901 ">
this method is also applied to the pediatrics domain (caseli et al, 2009).<papid> W09-2901 </papid></citsent>
<aftsection>
<nextsent>zarrie?
</nextsent>
<nextsent>and kuhn (2009) argue that multiword expressions can be reliably detected in parallel corpora by using dependency-parsed, word-aligned sentences.
</nextsent>
<nextsent>sinha(2009) <papid> W09-2906 </papid>detects hindi complex predicates (i.e. combination of light verb and noun, verb or an adjective) in hindi english parallel corpus by identifying mismatch of the hindi light verb meaning in the aligned english sentence.</nextsent>
<nextsent>van de cruys and moiron (2007) describe semantic-based method for identifying verb-preposition-noun combinations in dutch, which relies on selectional preferences forboth the noun and the verb.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2311">
<title id=" W11-0817.xml">detecting noun compounds and light verb constructions a contrastive study </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>zarrie?
</prevsent>
<prevsent>and kuhn (2009) argue that multiword expressions can be reliably detected in parallel corpora by using dependency-parsed, word-aligned sentences.
</prevsent>
</prevsection>
<citsent citstr=" W09-2906 ">
sinha(2009) <papid> W09-2906 </papid>detects hindi complex predicates (i.e. combination of light verb and noun, verb or an adjective) in hindi english parallel corpus by identifying mismatch of the hindi light verb meaning in the aligned english sentence.</citsent>
<aftsection>
<nextsent>van de cruys and moiron (2007) describe semantic-based method for identifying verb-preposition-noun combinations in dutch, which relies on selectional preferences forboth the noun and the verb.
</nextsent>
<nextsent>cook et al (2007) <papid> W07-1106 </papid>differentiate between literal and idiomatic usages of verb and noun constructions in english.</nextsent>
<nextsent>they makeuse of syntactic fixed ness of idioms when developing their unsupervised method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2320">
<title id=" W11-0817.xml">detecting noun compounds and light verb constructions a contrastive study </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>they found that linguistic features (i.e. the degree of composi tionality) and the frequency of the construction both have an effect on aligning the constructions.
</prevsent>
<prevsent>in order to identify multiword expressions, simple methods are worth examining, which can serve as basis for implementing more complex systems andcan be used as features in machine learning settings.
</prevsent>
</prevsection>
<citsent citstr=" W06-2407 ">
our aim being to compare the effect of different methods on the identification of noun compounds and light verb constructions, we considered it important to develop methods for both mwe types that make use of their characteristics and to adapt those methods to the other type of mwe ? in this way, the efficacy and the mwe-(in)dependence of the methods can be empirically evaluated, which can later have impact on developing statistical mwe detectors.earlier studies on the detection of light verb constructions generally take syntactic information as starting point (cook et al, 2007; <papid> W07-1106 </papid>bannard, 2007; <papid> W07-1101 </papid>tan et al, 2006), <papid> W06-2407 </papid>that is, their goal is to classify verb+ object constructions selected on the basis of syntactic pattern as literal or idiomatic.</citsent>
<aftsection>
<nextsent>however, we do not aim at classifying lvc candidates filtered by syntactic patterns but at identifying them in running text without assuming that syntactic information is necessarily available.
</nextsent>
<nextsent>in our investigations, we willpay distinctive attention to the added value of syntactic features on the systems performance.
</nextsent>
<nextsent>3.1 methods for mwe identification.
</nextsent>
<nextsent>for identifying noun compounds, we made use of alist constructed from the english wikipedia.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2321">
<title id=" W11-0817.xml">detecting noun compounds and light verb constructions a contrastive study </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for light verb constructions, the pos-rule method meant that each n-gram for which the pre-defined patterns(e.g. vb.?
</prevsent>
<prevsent>(nn|nns)) could be applied was accepted as light verb constructions.
</prevsent>
</prevsection>
<citsent citstr=" W00-1308 ">
for pos-tagging, we used the stanford pos tagger (toutanova and manning, 2000).<papid> W00-1308 </papid></citsent>
<aftsection>
<nextsent>since the methods to follow relyon morphological information (i.e. it is required to know which element is noun), matching the pos-rules is prerequisite to apply those methods to identify mwes.
</nextsent>
<nextsent>the suffix?
</nextsent>
<nextsent>method exploited the fact that many nominal components in light verb constructions are derived from verbs.
</nextsent>
<nextsent>thus, in this case only constructions that contained nouns ending in certain derivational suffixes were allowed and for nominal compounds the last noun had to have this ending.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2322">
<title id=" W11-0817.xml">detecting noun compounds and light verb constructions a contrastive study </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>in this case, we accepted 1as listed at http://en.wikipedia.org/wiki/ most\_common\_words\_in\_english only candidates that had the nominal component / the last noun whose stem was of verbal nature, i.e. coincided with stem of verb.
</prevsent>
<prevsent>syntactic information can also be exploited in identifying mwes.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
typically, the syntactic relation between the verb and the nominal component in light verb construction is dobj or prep ? using stanford parser (klein and manning, 2003)).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>the relation between the members of typical noun compound is nn or amod in attributive constructions.
</nextsent>
<nextsent>the syntax?
</nextsent>
<nextsent>method accepts candidates among whose members these syntactic relations hold.
</nextsent>
<nextsent>we also combined the above methods to identify noun compounds and light verb constructions in our databases (the union of candidates yielded by the methods is denoted by ? while the intersection is denoted by ? in the respective tables).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2330">
<title id=" W11-1301.xml">linear maps of the impossible capturing semantic anomalies in distributional space </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we present first attempt to characterize the semantic deviance of composite expressions in distributional semantics.
</prevsent>
<prevsent>specifically, we look for properties ofadjective-noun combinations within vector based semantic space that might cue their lackof meaning.
</prevsent>
</prevsection>
<citsent citstr=" P08-1028 ">
we evaluate four different com positionality models shown to have various levels of success in representing the meaning of an pairs: the simple additive and multiplicative models of mitchell and lap ata (2008), <papid> P08-1028 </papid>and the linear-map-based models of guevara (2010) <papid> W10-2805 </papid>and baroni and zamparelli(2010).<papid> D10-1115 </papid></citsent>
<aftsection>
<nextsent>for each model, we generate composite vectors for set of an combinations un attested in the source corpus and which have been deemed either acceptable or semantically deviant.
</nextsent>
<nextsent>we then compute measures that might cue semantic anomaly, and compare each models results for the two classes ofans.
</nextsent>
<nextsent>our study shows that simple, unsupervised cues can indeed significantly tell un attested but acceptable ans apart from impossible, or deviant, ans, and that the simple additive and multiplicative models are the most effective in this task.
</nextsent>
<nextsent>statistical approaches to describe, represent and understand natural language have been criticized as failing to account for linguistic creativity?, property which has been accredited to the compositional nature of natural language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2331">
<title id=" W11-1301.xml">linear maps of the impossible capturing semantic anomalies in distributional space </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we present first attempt to characterize the semantic deviance of composite expressions in distributional semantics.
</prevsent>
<prevsent>specifically, we look for properties ofadjective-noun combinations within vector based semantic space that might cue their lackof meaning.
</prevsent>
</prevsection>
<citsent citstr=" W10-2805 ">
we evaluate four different com positionality models shown to have various levels of success in representing the meaning of an pairs: the simple additive and multiplicative models of mitchell and lap ata (2008), <papid> P08-1028 </papid>and the linear-map-based models of guevara (2010) <papid> W10-2805 </papid>and baroni and zamparelli(2010).<papid> D10-1115 </papid></citsent>
<aftsection>
<nextsent>for each model, we generate composite vectors for set of an combinations un attested in the source corpus and which have been deemed either acceptable or semantically deviant.
</nextsent>
<nextsent>we then compute measures that might cue semantic anomaly, and compare each models results for the two classes ofans.
</nextsent>
<nextsent>our study shows that simple, unsupervised cues can indeed significantly tell un attested but acceptable ans apart from impossible, or deviant, ans, and that the simple additive and multiplicative models are the most effective in this task.
</nextsent>
<nextsent>statistical approaches to describe, represent and understand natural language have been criticized as failing to account for linguistic creativity?, property which has been accredited to the compositional nature of natural language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2332">
<title id=" W11-1301.xml">linear maps of the impossible capturing semantic anomalies in distributional space </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we present first attempt to characterize the semantic deviance of composite expressions in distributional semantics.
</prevsent>
<prevsent>specifically, we look for properties ofadjective-noun combinations within vector based semantic space that might cue their lackof meaning.
</prevsent>
</prevsection>
<citsent citstr=" D10-1115 ">
we evaluate four different com positionality models shown to have various levels of success in representing the meaning of an pairs: the simple additive and multiplicative models of mitchell and lap ata (2008), <papid> P08-1028 </papid>and the linear-map-based models of guevara (2010) <papid> W10-2805 </papid>and baroni and zamparelli(2010).<papid> D10-1115 </papid></citsent>
<aftsection>
<nextsent>for each model, we generate composite vectors for set of an combinations un attested in the source corpus and which have been deemed either acceptable or semantically deviant.
</nextsent>
<nextsent>we then compute measures that might cue semantic anomaly, and compare each models results for the two classes ofans.
</nextsent>
<nextsent>our study shows that simple, unsupervised cues can indeed significantly tell un attested but acceptable ans apart from impossible, or deviant, ans, and that the simple additive and multiplicative models are the most effective in this task.
</nextsent>
<nextsent>statistical approaches to describe, represent and understand natural language have been criticized as failing to account for linguistic creativity?, property which has been accredited to the compositional nature of natural language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2342">
<title id=" W11-1301.xml">linear maps of the impossible capturing semantic anomalies in distributional space </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 semantic deviance.
</prevsent>
<prevsent>as far as we know, we are the first to try to model semantic deviance using distributional methods, but the issue of when complex linguistic expression is semantically deviant has been addressed since the1950s in various areas of linguistics.
</prevsent>
</prevsection>
<citsent citstr=" J83-3004 ">
in computational linguistics, the possibility of detecting semantic deviance has been seen as prerequisite to access metaphorical/non-literal semantic interpretations (fass and wilks, 1983; <papid> J83-3004 </papid>zhou et al, 2007).</citsent>
<aftsection>
<nextsent>in psycho linguistics, it has been part of wide debate on the point at which context can make us perceive literal?
</nextsent>
<nextsent>vs. figurative?
</nextsent>
<nextsent>meaning (giora, 2002).
</nextsent>
<nextsent>in theoretical generative linguistics, the issue was originally part of discussion on the boundaries between syntax and semantics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2344">
<title id=" W11-1301.xml">linear maps of the impossible capturing semantic anomalies in distributional space </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>whatever the nature of coercion, we do not want it to run so smoothly that any combination of and (or and its arguments) becomes meaningful and completely acceptable.
</prevsent>
<prevsent>2 2.2 distributional approaches to meaning.
</prevsent>
</prevsection>
<citsent citstr=" D09-1045 ">
composition although the issue of how to compose meaning has attracted interest since the early days of distributional semantics (landauer and dumais, 1997), recently very general framework for modeling compositionality has been proposed by mitchell and lapata (mitchell and lapata, 2008; <papid> P08-1028 </papid>mitchell and la pata, 2009; <papid> D09-1045 </papid>mitchell and lapata, 2010).</citsent>
<aftsection>
<nextsent>given two vectors and v, they identify two general classes of composition models, (linear) additive models: = au + bv (1)where and are weight matrices, and multiplicative models: = cuvwhere is weight tensor projecting the uv tensor product onto the space of p. mitchell and lapata derive two simplified models from these general forms: the simplified additive model given byp = + v, and simplified multiplicative approach that reduces to component-wise multiplication, where the i-th component of the composed vector is given by: pi = uivi.
</nextsent>
<nextsent>mitchell and lapata evaluate the simplified models on wide range oftasks ranging from paraphrasing to statistical language modeling to predicting similarity intuitions.
</nextsent>
<nextsent>both simple models fare quite well across tasks and alternative semantic representations, also when compared to more complex methods derived fromthe equations above.
</nextsent>
<nextsent>given their overall simplicity, good performance and the fact that they have also been extensively tested in other studies (baroni and zamparelli, 2010; <papid> D10-1115 </papid>erk and pado?, 2008; guevara, 2010; <papid> W10-2805 </papid>kintsch, 2001; landauer and dumais, 1997), we re-implement here both the simplified additive and simplified multiplicative methods (we do not, however, attempt to tune the weights of the additive model, although we do apply scalar normalization constant to the adjective and noun vectors).mitchell and lapata (as well as earlier re searchers) do not exploit corpus evidence about the vectors that result from composition, despite the fact that it is straightforward (at least for shortconstructions) to extract direct distributional evidence about the composite items from the corpus(just collect co-occurrence information for the composite item from windows around the contexts in which it occurs).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2392">
<title id=" W11-0201.xml">not all links are equal exploiting dependency types for the extraction of protein protein interactions from text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the second class is based on machine learning.
</prevsent>
<prevsent>here, statistical model is learned from aset of positive and negative examples and then applied to unseen texts.
</prevsent>
</prevsection>
<citsent citstr=" W02-0301 ">
in general, machine learning based methods to relation extraction perform very well for any task where sufficient, representative and high quality training data is available (kazama etal., 2002).<papid> W02-0301 </papid></citsent>
<aftsection>
<nextsent>this need for training data is their major drawback, as annotated texts are, especially inthe life sciences, rather costly to produce.
</nextsent>
<nextsent>furthermore, they are prone to over-fit to the training corpus, which renders evaluation results less infer able to real applications.
</nextsent>
<nextsent>a third class of methods is based on pattern matching.
</nextsent>
<nextsent>such methods work with patterns constructed from linguistically anno 1 tated text, which are matched against unseen textto detect relationships.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2393">
<title id=" W11-0201.xml">not all links are equal exploiting dependency types for the extraction of protein protein interactions from text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>e.g., spies (hao et al, 2005) filters patterns using the minimum description length (mdl) method which improves its f1 by 6.72%.
</prevsent>
<prevsent>another classification of ppi extraction method sis based on the sentence representation that is applied.
</prevsent>
</prevsection>
<citsent citstr=" E06-1051 ">
the simplest such representation is the bag ofwords (bow) that occur in the sentence; more complex representations are constituent trees, capturing the syntactic structure of the sentence, and dependency trees (dts), which represent the main grammatical entities and their relationships to each other.ppi extraction methods use various sentence representation, e.g., are based only on bow (bunescu and mooney, 2006; giuliano et al, 2006), <papid> E06-1051 </papid>use onlydts (erkan et al, 2007), <papid> D07-1024 </papid>or combine representations (airola et al, 2008; miwa et al, 2008).</citsent>
<aftsection>
<nextsent>in the last years, dependency trees have become the most popular representation for relation extraction.
</nextsent>
<nextsent>dts characterize, via their dependency links, grammatical relationships among words.
</nextsent>
<nextsent>they are particularly favored by kernel-based learning approaches, see e.g.
</nextsent>
<nextsent>(culotta and sorensen, 2004; <papid> P04-1054 </papid>erkan et al, 2007; <papid> D07-1024 </papid>airola et al, 2008; miwa et al,2008; kim et al, 2010) but also graph matching approaches using dts have been proposed (liu et al, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2394">
<title id=" W11-0201.xml">not all links are equal exploiting dependency types for the extraction of protein protein interactions from text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>e.g., spies (hao et al, 2005) filters patterns using the minimum description length (mdl) method which improves its f1 by 6.72%.
</prevsent>
<prevsent>another classification of ppi extraction method sis based on the sentence representation that is applied.
</prevsent>
</prevsection>
<citsent citstr=" D07-1024 ">
the simplest such representation is the bag ofwords (bow) that occur in the sentence; more complex representations are constituent trees, capturing the syntactic structure of the sentence, and dependency trees (dts), which represent the main grammatical entities and their relationships to each other.ppi extraction methods use various sentence representation, e.g., are based only on bow (bunescu and mooney, 2006; giuliano et al, 2006), <papid> E06-1051 </papid>use onlydts (erkan et al, 2007), <papid> D07-1024 </papid>or combine representations (airola et al, 2008; miwa et al, 2008).</citsent>
<aftsection>
<nextsent>in the last years, dependency trees have become the most popular representation for relation extraction.
</nextsent>
<nextsent>dts characterize, via their dependency links, grammatical relationships among words.
</nextsent>
<nextsent>they are particularly favored by kernel-based learning approaches, see e.g.
</nextsent>
<nextsent>(culotta and sorensen, 2004; <papid> P04-1054 </papid>erkan et al, 2007; <papid> D07-1024 </papid>airola et al, 2008; miwa et al,2008; kim et al, 2010) but also graph matching approaches using dts have been proposed (liu et al, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2395">
<title id=" W11-0201.xml">not all links are equal exploiting dependency types for the extraction of protein protein interactions from text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dts characterize, via their dependency links, grammatical relationships among words.
</prevsent>
<prevsent>they are particularly favored by kernel-based learning approaches, see e.g.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
(culotta and sorensen, 2004; <papid> P04-1054 </papid>erkan et al, 2007; <papid> D07-1024 </papid>airola et al, 2008; miwa et al,2008; kim et al, 2010) but also graph matching approaches using dts have been proposed (liu et al, 2010).</citsent>
<aftsection>
<nextsent>however, these methods do not further utilize the grammatical information encoded in the dependency types (edge labels).
</nextsent>
<nextsent>recently proposed methods like (buyko et al, 2009; <papid> W09-1403 </papid>rinaldi et al, 2010)modify the dts by e.g. trimming irrelevant depen dencies.</nextsent>
<nextsent>in contrast to these approaches, our method exploits the dependency types of dts and performs basic transformations on dts; we use stanford dependencies, which are presumably the most often used dt representation in ppi extraction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2397">
<title id=" W11-0201.xml">not all links are equal exploiting dependency types for the extraction of protein protein interactions from text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(culotta and sorensen, 2004; <papid> P04-1054 </papid>erkan et al, 2007; <papid> D07-1024 </papid>airola et al, 2008; miwa et al,2008; kim et al, 2010) but also graph matching approaches using dts have been proposed (liu et al, 2010).</prevsent>
<prevsent>however, these methods do not further utilize the grammatical information encoded in the dependency types (edge labels).</prevsent>
</prevsection>
<citsent citstr=" W09-1403 ">
recently proposed methods like (buyko et al, 2009; <papid> W09-1403 </papid>rinaldi et al, 2010)modify the dts by e.g. trimming irrelevant depen dencies.</citsent>
<aftsection>
<nextsent>in contrast to these approaches, our method exploits the dependency types of dts and performs basic transformations on dts; we use stanford dependencies, which are presumably the most often used dt representation in ppi extraction.
</nextsent>
<nextsent>the rest of this paper is organized as follows.
</nextsent>
<nextsent>we describe our novel method for extracting ppis from text that is based on pattern matching in dependency graphs.
</nextsent>
<nextsent>we evaluate our method against benchmark ppi corpora, and discuss results with focus on dependency type information based methods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2398">
<title id=" W11-0201.xml">not all links are equal exploiting dependency types for the extraction of protein protein interactions from text </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in terms of f1 we achieve on bio infer, the corpus with most protein pairs, the second-bestresult.
</prevsent>
<prevsent>on iepa and lll we achieve mid-range results and on aimed and hprd50 we yield results below average.
</prevsent>
</prevsection>
<citsent citstr=" D09-1013 ">
over fitting remains severe problem in ml based methods as these results are inferior to those measured in cross-validation (tikk et al., 2010), though there are suggestions to overcome this issue even in ml setting (miwa et al, 2009).<papid> D09-1013 </papid></citsent>
<aftsection>
<nextsent>4.3 error analysis.
</nextsent>
<nextsent>we randomly picked 30 gold standard sentences (allcorpora) containing false negatives pairs and investigated all 72 false negative pairs included therein.
</nextsent>
<nextsent>for29 positive pairs, possibly matching pattern were removed by cdc, as the corresponding dependency combination was not covered in our rule set.
</nextsent>
<nextsent>further 16 graphs passed the filtering, but our set of sentences contained no adequate pattern.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2404">
<title id=" W11-1206.xml">bilingual lexicon extraction from comparable corpora as meta search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then reinterpret the extended method, and motivate anovel model to reformulate this approach inspired by the meta search engines in information retrieval.
</prevsent>
<prevsent>the empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s (rapp, 1995; <papid> P95-1050 </papid>fung, 1998; fung and lo, 1998; peters and picchi, 1998; rapp, 1999; <papid> P99-1067 </papid>chiao and zweigenbaum, 2002<papid> C02-2020 </papid>a; dejean et al, 2002; <papid> C02-1166 </papid>gaussier et al, 2004; <papid> P04-1067 </papid>morin et al, 2007; <papid> P07-1084 </papid>laroche and langlais, 2010, <papid> C10-1070 </papid>among others).</citsent>
<aftsection>
<nextsent>this attention has been motivated by the scarcity of parallel corpora, especially for countries with only one official language and for language pairs not involving english.
</nextsent>
<nextsent>furthermore, as parallel corpus is comprised of pair of texts (a source text and translated text), the vocabulary appearing in the translated text is highly influenced by the source text, especially in technical domains.
</nextsent>
<nextsent>consequently, comparable corpora are considered by human translators to be more trustworthy than parallel corpora (bowker and pearson, 2002).
</nextsent>
<nextsent>comparable corpora are clearly of usein the enrichment of bilingual dictionaries and the sauri (chiao and zweigenbaum, 2002<papid> C02-2020 </papid>b; dejean et al., 2002), <papid> C02-1166 </papid>and in the improvement of cross-language information retrieval (peters and picchi, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2406">
<title id=" W11-1206.xml">bilingual lexicon extraction from comparable corpora as meta search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then reinterpret the extended method, and motivate anovel model to reformulate this approach inspired by the meta search engines in information retrieval.
</prevsent>
<prevsent>the empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s (rapp, 1995; <papid> P95-1050 </papid>fung, 1998; fung and lo, 1998; peters and picchi, 1998; rapp, 1999; <papid> P99-1067 </papid>chiao and zweigenbaum, 2002<papid> C02-2020 </papid>a; dejean et al, 2002; <papid> C02-1166 </papid>gaussier et al, 2004; <papid> P04-1067 </papid>morin et al, 2007; <papid> P07-1084 </papid>laroche and langlais, 2010, <papid> C10-1070 </papid>among others).</citsent>
<aftsection>
<nextsent>this attention has been motivated by the scarcity of parallel corpora, especially for countries with only one official language and for language pairs not involving english.
</nextsent>
<nextsent>furthermore, as parallel corpus is comprised of pair of texts (a source text and translated text), the vocabulary appearing in the translated text is highly influenced by the source text, especially in technical domains.
</nextsent>
<nextsent>consequently, comparable corpora are considered by human translators to be more trustworthy than parallel corpora (bowker and pearson, 2002).
</nextsent>
<nextsent>comparable corpora are clearly of usein the enrichment of bilingual dictionaries and the sauri (chiao and zweigenbaum, 2002<papid> C02-2020 </papid>b; dejean et al., 2002), <papid> C02-1166 </papid>and in the improvement of cross-language information retrieval (peters and picchi, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2407">
<title id=" W11-1206.xml">bilingual lexicon extraction from comparable corpora as meta search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then reinterpret the extended method, and motivate anovel model to reformulate this approach inspired by the meta search engines in information retrieval.
</prevsent>
<prevsent>the empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach.
</prevsent>
</prevsection>
<citsent citstr=" C02-2020 ">
bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s (rapp, 1995; <papid> P95-1050 </papid>fung, 1998; fung and lo, 1998; peters and picchi, 1998; rapp, 1999; <papid> P99-1067 </papid>chiao and zweigenbaum, 2002<papid> C02-2020 </papid>a; dejean et al, 2002; <papid> C02-1166 </papid>gaussier et al, 2004; <papid> P04-1067 </papid>morin et al, 2007; <papid> P07-1084 </papid>laroche and langlais, 2010, <papid> C10-1070 </papid>among others).</citsent>
<aftsection>
<nextsent>this attention has been motivated by the scarcity of parallel corpora, especially for countries with only one official language and for language pairs not involving english.
</nextsent>
<nextsent>furthermore, as parallel corpus is comprised of pair of texts (a source text and translated text), the vocabulary appearing in the translated text is highly influenced by the source text, especially in technical domains.
</nextsent>
<nextsent>consequently, comparable corpora are considered by human translators to be more trustworthy than parallel corpora (bowker and pearson, 2002).
</nextsent>
<nextsent>comparable corpora are clearly of usein the enrichment of bilingual dictionaries and the sauri (chiao and zweigenbaum, 2002<papid> C02-2020 </papid>b; dejean et al., 2002), <papid> C02-1166 </papid>and in the improvement of cross-language information retrieval (peters and picchi, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2411">
<title id=" W11-1206.xml">bilingual lexicon extraction from comparable corpora as meta search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then reinterpret the extended method, and motivate anovel model to reformulate this approach inspired by the meta search engines in information retrieval.
</prevsent>
<prevsent>the empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach.
</prevsent>
</prevsection>
<citsent citstr=" C02-1166 ">
bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s (rapp, 1995; <papid> P95-1050 </papid>fung, 1998; fung and lo, 1998; peters and picchi, 1998; rapp, 1999; <papid> P99-1067 </papid>chiao and zweigenbaum, 2002<papid> C02-2020 </papid>a; dejean et al, 2002; <papid> C02-1166 </papid>gaussier et al, 2004; <papid> P04-1067 </papid>morin et al, 2007; <papid> P07-1084 </papid>laroche and langlais, 2010, <papid> C10-1070 </papid>among others).</citsent>
<aftsection>
<nextsent>this attention has been motivated by the scarcity of parallel corpora, especially for countries with only one official language and for language pairs not involving english.
</nextsent>
<nextsent>furthermore, as parallel corpus is comprised of pair of texts (a source text and translated text), the vocabulary appearing in the translated text is highly influenced by the source text, especially in technical domains.
</nextsent>
<nextsent>consequently, comparable corpora are considered by human translators to be more trustworthy than parallel corpora (bowker and pearson, 2002).
</nextsent>
<nextsent>comparable corpora are clearly of usein the enrichment of bilingual dictionaries and the sauri (chiao and zweigenbaum, 2002<papid> C02-2020 </papid>b; dejean et al., 2002), <papid> C02-1166 </papid>and in the improvement of cross-language information retrieval (peters and picchi, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2413">
<title id=" W11-1206.xml">bilingual lexicon extraction from comparable corpora as meta search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then reinterpret the extended method, and motivate anovel model to reformulate this approach inspired by the meta search engines in information retrieval.
</prevsent>
<prevsent>the empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach.
</prevsent>
</prevsection>
<citsent citstr=" P04-1067 ">
bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s (rapp, 1995; <papid> P95-1050 </papid>fung, 1998; fung and lo, 1998; peters and picchi, 1998; rapp, 1999; <papid> P99-1067 </papid>chiao and zweigenbaum, 2002<papid> C02-2020 </papid>a; dejean et al, 2002; <papid> C02-1166 </papid>gaussier et al, 2004; <papid> P04-1067 </papid>morin et al, 2007; <papid> P07-1084 </papid>laroche and langlais, 2010, <papid> C10-1070 </papid>among others).</citsent>
<aftsection>
<nextsent>this attention has been motivated by the scarcity of parallel corpora, especially for countries with only one official language and for language pairs not involving english.
</nextsent>
<nextsent>furthermore, as parallel corpus is comprised of pair of texts (a source text and translated text), the vocabulary appearing in the translated text is highly influenced by the source text, especially in technical domains.
</nextsent>
<nextsent>consequently, comparable corpora are considered by human translators to be more trustworthy than parallel corpora (bowker and pearson, 2002).
</nextsent>
<nextsent>comparable corpora are clearly of usein the enrichment of bilingual dictionaries and the sauri (chiao and zweigenbaum, 2002<papid> C02-2020 </papid>b; dejean et al., 2002), <papid> C02-1166 </papid>and in the improvement of cross-language information retrieval (peters and picchi, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2414">
<title id=" W11-1206.xml">bilingual lexicon extraction from comparable corpora as meta search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then reinterpret the extended method, and motivate anovel model to reformulate this approach inspired by the meta search engines in information retrieval.
</prevsent>
<prevsent>the empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach.
</prevsent>
</prevsection>
<citsent citstr=" P07-1084 ">
bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s (rapp, 1995; <papid> P95-1050 </papid>fung, 1998; fung and lo, 1998; peters and picchi, 1998; rapp, 1999; <papid> P99-1067 </papid>chiao and zweigenbaum, 2002<papid> C02-2020 </papid>a; dejean et al, 2002; <papid> C02-1166 </papid>gaussier et al, 2004; <papid> P04-1067 </papid>morin et al, 2007; <papid> P07-1084 </papid>laroche and langlais, 2010, <papid> C10-1070 </papid>among others).</citsent>
<aftsection>
<nextsent>this attention has been motivated by the scarcity of parallel corpora, especially for countries with only one official language and for language pairs not involving english.
</nextsent>
<nextsent>furthermore, as parallel corpus is comprised of pair of texts (a source text and translated text), the vocabulary appearing in the translated text is highly influenced by the source text, especially in technical domains.
</nextsent>
<nextsent>consequently, comparable corpora are considered by human translators to be more trustworthy than parallel corpora (bowker and pearson, 2002).
</nextsent>
<nextsent>comparable corpora are clearly of usein the enrichment of bilingual dictionaries and the sauri (chiao and zweigenbaum, 2002<papid> C02-2020 </papid>b; dejean et al., 2002), <papid> C02-1166 </papid>and in the improvement of cross-language information retrieval (peters and picchi, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2415">
<title id=" W11-1206.xml">bilingual lexicon extraction from comparable corpora as meta search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then reinterpret the extended method, and motivate anovel model to reformulate this approach inspired by the meta search engines in information retrieval.
</prevsent>
<prevsent>the empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach.
</prevsent>
</prevsection>
<citsent citstr=" C10-1070 ">
bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s (rapp, 1995; <papid> P95-1050 </papid>fung, 1998; fung and lo, 1998; peters and picchi, 1998; rapp, 1999; <papid> P99-1067 </papid>chiao and zweigenbaum, 2002<papid> C02-2020 </papid>a; dejean et al, 2002; <papid> C02-1166 </papid>gaussier et al, 2004; <papid> P04-1067 </papid>morin et al, 2007; <papid> P07-1084 </papid>laroche and langlais, 2010, <papid> C10-1070 </papid>among others).</citsent>
<aftsection>
<nextsent>this attention has been motivated by the scarcity of parallel corpora, especially for countries with only one official language and for language pairs not involving english.
</nextsent>
<nextsent>furthermore, as parallel corpus is comprised of pair of texts (a source text and translated text), the vocabulary appearing in the translated text is highly influenced by the source text, especially in technical domains.
</nextsent>
<nextsent>consequently, comparable corpora are considered by human translators to be more trustworthy than parallel corpora (bowker and pearson, 2002).
</nextsent>
<nextsent>comparable corpora are clearly of usein the enrichment of bilingual dictionaries and the sauri (chiao and zweigenbaum, 2002<papid> C02-2020 </papid>b; dejean et al., 2002), <papid> C02-1166 </papid>and in the improvement of cross-language information retrieval (peters and picchi, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2421">
<title id=" W11-1206.xml">bilingual lexicon extraction from comparable corpora as meta search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more precisely, in the standard approach dedicated to bilingual lexicon extraction from comparable corpora, word to be translated is represented by vector context composed of the words that appear in its lexical context.
</prevsent>
<prevsent>the candidate translations for word are obtained by comparing the translated source context vector with the target context vectors through general bilingual dictionary.
</prevsent>
</prevsection>
<citsent citstr=" W97-0119 ">
using this approach, good results on single word terms (swts) can be obtained from large corpora of several million words, with an accuracy of about 80% for the top 10 20 proposed candidates (fung and mckeown, 1997; <papid> W97-0119 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>cao and li (2002) <papid> C02-1011 </papid>have achieved 91% accuracy for the top three candidates using the web as comparable corpus.</nextsent>
<nextsent>results drop to 60% forswts using specialized small size language corpora (chiao and zweigenbaum, 2002<papid> C02-2020 </papid>a; dejean and gaussier, 2002; morin et al, 2007).<papid> P07-1084 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2423">
<title id=" W11-1206.xml">bilingual lexicon extraction from comparable corpora as meta search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the candidate translations for word are obtained by comparing the translated source context vector with the target context vectors through general bilingual dictionary.
</prevsent>
<prevsent>using this approach, good results on single word terms (swts) can be obtained from large corpora of several million words, with an accuracy of about 80% for the top 10 20 proposed candidates (fung and mckeown, 1997; <papid> W97-0119 </papid>rapp, 1999).<papid> P99-1067 </papid></prevsent>
</prevsection>
<citsent citstr=" C02-1011 ">
cao and li (2002) <papid> C02-1011 </papid>have achieved 91% accuracy for the top three candidates using the web as comparable corpus.</citsent>
<aftsection>
<nextsent>results drop to 60% forswts using specialized small size language corpora (chiao and zweigenbaum, 2002<papid> C02-2020 </papid>a; dejean and gaussier, 2002; morin et al, 2007).<papid> P07-1084 </papid></nextsent>
<nextsent>in order to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors, an extended approach has 35 proceedings of the 4th workshop on building and using comparable corpora, pages 3543, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2435">
<title id=" W11-1206.xml">bilingual lexicon extraction from comparable corpora as meta search </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the basis of this observation consists in the identification of first-order affinities for each source and target language: first-order affinities describe what other words are likely to be found in the immediate vicinity of given word (grefenstette, 1994a, p. 279).
</prevsent>
<prevsent>these affinities can be represented by context vectors, and each vector element represents word which occurs within the window of the word to be translated (for instance seven-word window approximates syntactical dependencies).the implementation of this approach can be carried out by applying the following four steps (rapp, 1995; <papid> P95-1050 </papid>fung and mckeown, 1997): <papid> W97-0119 </papid>context characterization all the lexical units in the context of each lexical unit are collected, and their frequency in window of words around extracted.</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
for each lexical unit of the source and the target languages, we obtain context vector where each entry, ij , of the vector is given by function of the co-occurrences of units jand i. usually, association measures such as the mutual information (fano, 1961) or the log-likelihood (dunning, 1993) <papid> J93-1003 </papid>are used to define vector entries.</citsent>
<aftsection>
<nextsent>vector transfer the lexical units of the context vector are translated using bilingual dictionary.
</nextsent>
<nextsent>whenever the bilingual dictionary provides several translations for lexical unit, all the entries are considered but weighted according to their frequency in the targetlanguage.
</nextsent>
<nextsent>lexical units with no entry in the dictionary are discarded.
</nextsent>
<nextsent>target language vector matching similarity measure, sim(i, t), is used to score each lexical unit, t, in the target language with respect to the translated context vector, i. usual measures of vector similarity include the cosine similarity (salton and lesk, 1968) or the weighted jaccard index (wj) (grefenstette, 1994b) for instance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2437">
<title id=" W11-1206.xml">bilingual lexicon extraction from comparable corpora as meta search </title>
<section> the meta search approach.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we can not ensure result stability within particular ranges of 37 values.
</prevsent>
<prevsent>therefore, the value of should be carefully tuned.starting from the intuition that each nearest lexical unit (nlu) contributes to the characterization of lexical unit to be translated, our proposition aims at providing an algorithm that gives better precision while ensuring higher stability with respect to the number of nlu.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
pushing the analogy of ir style approaches (fung and lo, 1998) <papid> P98-1069 </papid>step further, we propose novel way of looking at the problem of word translation from comparable corpora that is conceptually simple: meta search problem.in information retrieval, meta search is the problem of combining different ranked lists, returned by multiple search engines in response to given query, in such way as to optimize the performance of the combined ranking (aslam and montague, 2001).</citsent>
<aftsection>
<nextsent>since the nlu result indistinct rankings, meta search provides an appropriate framework for exploiting information conveyed by the rankings.
</nextsent>
<nextsent>in our model, we consider each list of given nlu as response of search engine independently fromthe others.
</nextsent>
<nextsent>after collecting all the lists of these lected nlus, we combine them to obtain the final similarity score.
</nextsent>
<nextsent>it is worth noting that all the lists are normalized to maximize in such way the contribution of each nlu.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2444">
<title id=" W11-0806.xml">tree rewriting models of multiword expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or next to?
</prevsent>
<prevsent>(compare to beside?)
</prevsent>
</prevsection>
<citsent citstr=" T75-2013 ">
suggest fundamentality of expressions, as opposed to words, as basic unit of meaning in language (becker, 1975;<papid> T75-2013 </papid>fillmore, 2003).</citsent>
<aftsection>
<nextsent>other examples of mwes are idioms such as kick the bucket?
</nextsent>
<nextsent>or spill the beans?, which have figurative meanings as expressions that sometimes even allow modification (spill some of the beans?)
</nextsent>
<nextsent>and variation in sentence forms (which beans were spilled??), but are not available when the component words of the mwe occur independently.
</nextsent>
<nextsent>a formal system that is flexible enough to model these large and often syntactically-rich non-compositional chunks as single units in naturally occurring text could considerably simplify large-scale semantic annotation projects, in which it would be undesirable to have to develop internal compositional analyses of common technical expressions that have specific idiosyncratic meanings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2445">
<title id=" W11-0806.xml">tree rewriting models of multiword expressions </title>
<section> reduction of string-rewriting.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, it is precisely these non-string-rewriting-equivalentelementary trees that are needed to model the local non-compositionality of larger multi-word expressions like threw to the lions?
</prevsent>
<prevsent>(see figure 5), because only downward branches with multiple non 3recognition and parsing of feature-based grammars, andof tree-rewriting systems whose elementary trees contain multiple foot nodes, are both exponential in the worst case.
</prevsent>
</prevsection>
<citsent citstr=" P94-1022 ">
however, both types of grammars are amenable to regular-from restrictions which prohibit recursive adjunction at internal (non root, non-foot) tree nodes, and thereby constrain recognition and parsing complexity to cubic time for most kinds of natural language grammars (rogers, 1994).<papid> P94-1022 </papid></citsent>
<aftsection>
<nextsent>s np?
</nextsent>
<nextsent>vp vp threw?
</nextsent>
<nextsent>np?
</nextsent>
<nextsent>pp to?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2447">
<title id=" W11-0806.xml">tree rewriting models of multiword expressions </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>although neither the tree-rewriting nor thestring-rewriting analyses defined above can be generated in guaranteed polynomial time (since they may require the construction of unbounded stacks of unrecognized structure during bottom-up recognition), they can both be made polynomial (indeed, cu bic) by the introduction of regular form?
</prevsent>
<prevsent>constraints (rogers, 1994), <papid> P94-1022 </papid>which limit this stack in the same way in both cases.</prevsent>
</prevsection>
<citsent citstr=" W04-0411 ">
in contrast with representations like that of(villavicencio et al, 2004), <papid> W04-0411 </papid>in which concepts are distributed over several lexical entries, tree-rewritingrepresentation such as the one described in this paper allows only single lexical entry to be listed for each concept.</citsent>
<aftsection>
<nextsent>for example: ... throw ... to the lions: (s(np0!)(vp(v)(np1!)(pp(p)(np(d)(n))))) ... to the ... power: (vp(vp0*)(pp(p)(np(d)(n(a1!)(n))))) (using the notation ?!?
</nextsent>
<nextsent>and ?*?
</nextsent>
<nextsent>for substitution sites and foot nodes, respectively).
</nextsent>
<nextsent>it is anticipated that this will simplify the organization of lexical resources for multi-word expressions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2448">
<title id=" W10-4118.xml">treebank of chinese bible translations </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluated the segmentation of our cuv trees with the scoring script used in the first 1 the segmented cuv text was provided by asia.
</prevsent>
<prevsent>bible society.
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
international chinese segmentation bakeoff (sproat &amp; emerson 2003).<papid> W03-1719 </papid></citsent>
<aftsection>
<nextsent>here are the results: recall: 99.844% precision: 99.826% f-score: 99.845% we don show the oov numbers as they are not relevant here, because all the words have been exhaustively listed in our dictionary.
</nextsent>
<nextsent>of total of 31151 verses in the bible, 30568 verses (98.13%) do not contain single error (whole verses segmented correctly).
</nextsent>
<nextsent>of course, segmentation accuracy does not imply parsing accuracy, though wrong segmentation necessarily implies wrong parse.
</nextsent>
<nextsent>since we do not have separate word segmenter and segmentation is an output of the parsing process, the high segmentation accuracy does serve as reflection of the quality of the trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2449">
<title id=" W10-4151.xml">complete syntactic analysis bases on multilevel chunking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>parsing by chunks is proved to be feasible (abney, 1994).
</prevsent>
<prevsent>the concept of chunking was first proposed by abney in 1991, who defined chunks in terms of major heads, and parsed by chunks in 1994 (ab ney, 1994).
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
an additional chunk tag set {b, i, o} was added to chunking (ramshaw and marcus, 1995), <papid> W95-0107 </papid>which limited dependencies between elements in chunk, changed chunking into question of sequenced tags, to promote the development of chunking.</citsent>
<aftsection>
<nextsent>chunking algorithm was extended to the bottom-up parser, which is trained and tested on the wall street journal (wsj) part of the penn treebank (marcus, santorini and marcinkiewicz 1993), and achieved performance of 80.49% f-measure, the results show that it performed better than standard probabilistic con text-free grammar, and can improve performance by adding the information of parent node (sang, 2000).
</nextsent>
<nextsent>on chinese parsing, maximum entropy model was first used to have pos tagging and chunking, and then full parsing tree was generated (fung, 2004), training and testing in the penn chinese treebank, which achieved 79.56% f-measure.
</nextsent>
<nextsent>the parsing process was divided into pos tagging, base chunking and complex chunking, having pos tagging and chunking on given sentence, and then looping the process of complex chunking up to identify the root node (li and zhou, 2009).
</nextsent>
<nextsent>this parsing method is the basis of this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2450">
<title id=" W11-1413.xml">generating example contexts to illustrate a target word sense </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes what we believe is the first system to generate example contexts forgiven target sense of polysemous word.
</prevsent>
<prevsent>liu et al  (2009) characterized good contexts for helping children learn vocabulary and generated them for target part of speech, but not given word sense.
</prevsent>
</prevsection>
<citsent citstr=" W09-2106 ">
pino and eskenazi (2009) <papid> W09-2106 </papid>addressed the polysemy issue, but in system for selecting contexts rather than for generating them.</citsent>
<aftsection>
<nextsent>generation can supply more contexts forgiven purpose, e.g. teaching children, than wordnet or fixed corpus contains.
</nextsent>
<nextsent>section 2 describes method to generate sense targeted contexts.
</nextsent>
<nextsent>section 3 compares them to wordnet examples.
</nextsent>
<nextsent>section 4 concludes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2451">
<title id=" W11-1413.xml">generating example contexts to illustrate a target word sense </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>section 2.2 describes the context generation phase, which generates contexts that contain the target word and indicators for the target sense.
</prevsent>
<prevsent>figure 1: overall work flow diagram 2.1 sense indicator extraction.
</prevsent>
</prevsection>
<citsent citstr=" N10-1088 ">
kulkarni and pedersen (2005) and duan and yates (2010) <papid> N10-1088 </papid>performed sense indicator extraction, but the indicators they extracted are not sense targeted.</citsent>
<aftsection>
<nextsent>content words in the definition and examples for each sense are often good indicators for that sense, but we found that on their own they did poorly.
</nextsent>
<nextsent>one reason is that such indicators sometimes co occur with different sense.
</nextsent>
<nextsent>but the main reason is that there are so few of them that the word sense often appears without any of them.
</nextsent>
<nextsent>thus we need more (and if possible better) sense indicators.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2452">
<title id=" W11-1413.xml">generating example contexts to illustrate a target word sense </title>
<section> for each word      in    where.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, our variant allows all topics for each document, because it may use more than one sense of the target word.
</prevsent>
<prevsent>allowing other senses provides additional flexibility to discover appropriate sense indicators.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
the lda method we use to obtain sense indicators fits naturally into the framework of bootstrapping wsd (yarowsky 1995; <papid> P95-1026 </papid>mihalcea 2002; martinez et al  2008; duan and yates 2010), <papid> N10-1088 </papid>in which seeds are given for each target word, and the goal is to disambiguate the target word by bootstrapping good sense indicators that can identify the sense.</citsent>
<aftsection>
<nextsent>in contrast to wsd, our goal is to generate contexts for each sense of the target word.
</nextsent>
<nextsent>2.2 context generation.
</nextsent>
<nextsent>to generate sense-targeted contexts, we extend the vegematic context generation system (liu et al . 2009).
</nextsent>
<nextsent>vegematic generates contexts forgiven target word using the google n-gram corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2454">
<title id=" W11-1413.xml">generating example contexts to illustrate a target word sense </title>
<section> for each word      in    where.  </section>
<citcontext>
<prevsection>
<prevsent>coarse-grained precision was 67% for the 23 wordnet examples, 40% for the examples generated for those 23 syn sets, and 33% for all 67 generated examples.
</prevsent>
<prevsent>coarse-grained precision is important because fine-grained semantic distinctions do not matter in illustrating core sense of word.
</prevsent>
</prevsection>
<citsent citstr=" W07-2006 ">
the problem of how to cluster fine-grained senses into coarse senses is hard, especially if consensus is required (navigli et al  2007).<papid> W07-2006 </papid></citsent>
<aftsection>
<nextsent>rather than attempt to identify single definitive partition of target words synsets into coarse senses, we implicitly define coarse sense as the subset of synsets rated by judge as fitting given example.
</nextsent>
<nextsent>thus the clustering into coarse senses is not only judge-specific but example-specific: different, possibly overlapping sets of synsets may fit different examples.
</nextsent>
<nextsent>recall is the percentage of synsets that fit their generated examples.
</nextsent>
<nextsent>algebraic ally it is the product of precision and yield.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2455">
<title id=" W11-1705.xml">generating semantic orientation lexicon using large data and thesaurus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we evaluate these lexicons intrinsically and extrinsic ally, and they perform comparable when compared to other existing lexicons.
</prevsent>
<prevsent>in recent years, the field of natural language processing (nlp) has seen tremendous growth and interest in the computational analysis of emotions, sentiments, and opinions.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
this work has focused onmany application areas, such as sentiment analysis of consumer reviews e.g., (pang et al, 2002;<papid> W02-1011 </papid>nasukawa and yi, 2003), product reputation analysis e.g., (morinaga et al, 2002; nasukawa and yi, 2003), tracking sentiments toward events e.g., (das and chen, 2001; tong, 2001), and automatically producing plot unit representations e.g., (goyal et al., 2010<papid> W10-1503 </papid>b).</citsent>
<aftsection>
<nextsent>an important resource in accomplishing the above tasks is list of words with semantic orientation (so): positive or negative.
</nextsent>
<nextsent>the goal of this work is to automatically create such list of words using large data and thesaurus structure.
</nextsent>
<nextsent>for this purpose, we store exact counts of all the words in hash table and use count-min (cm) sketch (cormode and muthukrishnan, 2004; goyal et al, 2010) <papid> W10-1503 </papid>to store the approximate counts of all word pairs for large corpus in bounded space of8gb.</nextsent>
<nextsent>(storing the counts of all word pairs is computationally expensive and memory intensive on large data (agirre et al, 2009; <papid> N09-1003 </papid>pantel et al, 2009)).<papid> D09-1098 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2456">
<title id=" W11-1705.xml">generating semantic orientation lexicon using large data and thesaurus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we evaluate these lexicons intrinsically and extrinsic ally, and they perform comparable when compared to other existing lexicons.
</prevsent>
<prevsent>in recent years, the field of natural language processing (nlp) has seen tremendous growth and interest in the computational analysis of emotions, sentiments, and opinions.
</prevsent>
</prevsection>
<citsent citstr=" W10-1503 ">
this work has focused onmany application areas, such as sentiment analysis of consumer reviews e.g., (pang et al, 2002;<papid> W02-1011 </papid>nasukawa and yi, 2003), product reputation analysis e.g., (morinaga et al, 2002; nasukawa and yi, 2003), tracking sentiments toward events e.g., (das and chen, 2001; tong, 2001), and automatically producing plot unit representations e.g., (goyal et al., 2010<papid> W10-1503 </papid>b).</citsent>
<aftsection>
<nextsent>an important resource in accomplishing the above tasks is list of words with semantic orientation (so): positive or negative.
</nextsent>
<nextsent>the goal of this work is to automatically create such list of words using large data and thesaurus structure.
</nextsent>
<nextsent>for this purpose, we store exact counts of all the words in hash table and use count-min (cm) sketch (cormode and muthukrishnan, 2004; goyal et al, 2010) <papid> W10-1503 </papid>to store the approximate counts of all word pairs for large corpus in bounded space of8gb.</nextsent>
<nextsent>(storing the counts of all word pairs is computationally expensive and memory intensive on large data (agirre et al, 2009; <papid> N09-1003 </papid>pantel et al, 2009)).<papid> D09-1098 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2466">
<title id=" W11-1705.xml">generating semantic orientation lexicon using large data and thesaurus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of this work is to automatically create such list of words using large data and thesaurus structure.
</prevsent>
<prevsent>for this purpose, we store exact counts of all the words in hash table and use count-min (cm) sketch (cormode and muthukrishnan, 2004; goyal et al, 2010) <papid> W10-1503 </papid>to store the approximate counts of all word pairs for large corpus in bounded space of8gb.</prevsent>
</prevsection>
<citsent citstr=" N09-1003 ">
(storing the counts of all word pairs is computationally expensive and memory intensive on large data (agirre et al, 2009; <papid> N09-1003 </papid>pantel et al, 2009)).<papid> D09-1098 </papid></citsent>
<aftsection>
<nextsent>storage space saving in cm sketch is achieved by approximating the frequency of word pairs in the corpus without explicitly storing the word pairs themselves.
</nextsent>
<nextsent>both updating (adding new word pair or increasing the frequency of existing word pair) and querying (finding the frequency of given wordpair) are constant time operations making it an efficient online storage data structure for large data.
</nextsent>
<nextsent>once we have these counts, we find semantic orientation (so) (turney and littman, 2003) of word using its association strength with positive (e.g. good, and nice) and negative (e.g., bad and nasty) seeds.
</nextsent>
<nextsent>next, we make use of thesaurus (like roget) structure in which near-synonymous words appear in single group.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2467">
<title id=" W11-1705.xml">generating semantic orientation lexicon using large data and thesaurus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of this work is to automatically create such list of words using large data and thesaurus structure.
</prevsent>
<prevsent>for this purpose, we store exact counts of all the words in hash table and use count-min (cm) sketch (cormode and muthukrishnan, 2004; goyal et al, 2010) <papid> W10-1503 </papid>to store the approximate counts of all word pairs for large corpus in bounded space of8gb.</prevsent>
</prevsection>
<citsent citstr=" D09-1098 ">
(storing the counts of all word pairs is computationally expensive and memory intensive on large data (agirre et al, 2009; <papid> N09-1003 </papid>pantel et al, 2009)).<papid> D09-1098 </papid></citsent>
<aftsection>
<nextsent>storage space saving in cm sketch is achieved by approximating the frequency of word pairs in the corpus without explicitly storing the word pairs themselves.
</nextsent>
<nextsent>both updating (adding new word pair or increasing the frequency of existing word pair) and querying (finding the frequency of given wordpair) are constant time operations making it an efficient online storage data structure for large data.
</nextsent>
<nextsent>once we have these counts, we find semantic orientation (so) (turney and littman, 2003) of word using its association strength with positive (e.g. good, and nice) and negative (e.g., bad and nasty) seeds.
</nextsent>
<nextsent>next, we make use of thesaurus (like roget) structure in which near-synonymous words appear in single group.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2468">
<title id=" W11-1705.xml">generating semantic orientation lexicon using large data and thesaurus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one can argue that, first two words have positive connotation andlast two have negative.
</prevsent>
<prevsent>to remove these ambiguous words errors from the lexicon, we discard those words which have conflicting so compared to their group so.
</prevsent>
</prevsection>
<citsent citstr=" D09-1063 ">
the idea behind using thesaurus structure is motivated from the idea of using number of positive and negative seed words (mohammad et al, 2009) <papid> D09-1063 </papid>in thesaurus group to determine the polarity of words in the group.</citsent>
<aftsection>
<nextsent>in our experiments, we show the effectiveness ofthe lexicons created using large data and freely avail 37 able thesaurus both intrinsically and extrinsically.
</nextsent>
<nextsent>2.1 related work.
</nextsent>
<nextsent>the literature on sentiment lexicon induction can be broadly classified into three categories: (1) corpora based, (2) using thesaurus structure, and (3) combination of (1) and (2).
</nextsent>
<nextsent>pang and lee (2008) provide an excellent survey on the literature of sentiment analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2470">
<title id=" W11-1705.xml">generating semantic orientation lexicon using large data and thesaurus </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>pang and lee (2008) provide an excellent survey on the literature of sentiment analysis.
</prevsent>
<prevsent>we briefly discuss some of the works which have motivated our research for this work.
</prevsent>
</prevsection>
<citsent citstr=" N10-1119 ">
a web-derived lexicon (velikovich et al,2010) <papid> N10-1119 </papid>was constructed for all words and phrases using graph propagation algorithm which propagates polarity from seed words to all other words.</citsent>
<aftsection>
<nextsent>the graph was constructed using distributional similarity between the words.
</nextsent>
<nextsent>the goal of their work wasto create high coverage lexicon.
</nextsent>
<nextsent>in similar direction (rao and ravichandran, 2009), <papid> E09-1077 </papid>word-net was used to construct the graph for label propagation.</nextsent>
<nextsent>our work is most closely related to mohammad etal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2471">
<title id=" W11-1705.xml">generating semantic orientation lexicon using large data and thesaurus </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the graph was constructed using distributional similarity between the words.
</prevsent>
<prevsent>the goal of their work wasto create high coverage lexicon.
</prevsent>
</prevsection>
<citsent citstr=" E09-1077 ">
in similar direction (rao and ravichandran, 2009), <papid> E09-1077 </papid>word-net was used to construct the graph for label propagation.</citsent>
<aftsection>
<nextsent>our work is most closely related to mohammad etal.
</nextsent>
<nextsent>(2009) which exploits thesaurus structure to determine the polarity of words in the thesaurus group.
</nextsent>
<nextsent>2.2 semantic orientation.
</nextsent>
<nextsent>we use (turney and littman, 2003) framework to infer the semantic orientation (so) of word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2488">
<title id=" W11-1705.xml">generating semantic orientation lexicon using large data and thesaurus </title>
<section> evaluating so computed using sketch.  </section>
<citcontext>
<prevsection>
<prevsent>3we do not assign polarity to phrases and stop words.
</prevsent>
<prevsent>39 4.1 data.
</prevsent>
</prevsection>
<citsent citstr=" P05-1077 ">
we use gigaword corpus (graff, 2003) and 66%portion of copy of web crawled by (ravichan dran et al, 2005).<papid> P05-1077 </papid></citsent>
<aftsection>
<nextsent>for both the corpora, we split the text into sentences, tokenize and convert into lower-case.
</nextsent>
<nextsent>we generate words and word pairs over sliding window of size 7.
</nextsent>
<nextsent>we use four different sized corpora: gigaword (gw), gigaword + 16% of web data (gwb16), gigaword + 50% of web data (gwb50), and gigaword + 66% of web data (gwb66).
</nextsent>
<nextsent>corpus statistics are shown in table 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2505">
<title id=" W11-1705.xml">generating semantic orientation lexicon using large data and thesaurus </title>
<section> lexicon evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in all other cases, do not assign any tag.
</prevsent>
<prevsent>the only difference with respect to mohammad et al.
</prevsent>
</prevsection>
<citsent citstr=" H05-2018 ">
(2009) is that we use list of 58 negation words used in opinionfinder6 (wilson et al, 2005<papid> H05-2018 </papid>b) (ver sion 1.4) to flip the polarity of phrase if it contains odd number of negation words.</citsent>
<aftsection>
<nextsent>we can get better 5http://www.cs.pitt.edu/mpqa/databaserelease/ 6www.cs.pitt.edu/mpqa/opinionfinderrelease lexicon # of positives # of negatives # of all gi 1597 1980 3577 mpqa 2666 4888 7554 asl 2320 2616 4936 roget (asl) 21637 6161 27798 roget (gi) 10804 16319 27123 roget (asl+gi) 16168 12530 28698 msol 22088 32712 54800 so 16620 15582 32202 so-tp 22959 10117 33076 so-wtp 14357 8257 22614 so+gi 8629 9936 18565 so-tp+gi 12049 9317 21366 table 4: summarizes all lexicons size accuracies on phrase polarity identification using supervised classifiers (wilson et al, 2005<papid> H05-2018 </papid>a).</nextsent>
<nextsent>however,the goal of this work is only to show the effectiveness of large data and thesaurus learned lexicons.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2529">
<title id=" W11-0602.xml">a  </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>netic categories based on experience in rational way.
</prevsent>
<prevsent>we propose that bayesian belief updating can provide principled computational framework for understanding rapid adaptation of phonetic categories as optimal inference under uncertainty.
</prevsent>
</prevsection>
<citsent citstr=" W10-2003 ">
sucha framework has the appeal of being successfully applied in other domains (brenner et al, 2000; fine et al, 2010).<papid> W10-2003 </papid></citsent>
<aftsection>
<nextsent>in addition, rational models have also been used within the domain of speech perception to model acquisition of phonetic categories (vallabha et al, 2007; feldman et al, 2009a; mcmurray et al, 2009), the perceptual magnet effect (feldman et al, 2009b), and how various cues to the same phonetic contrast can be combined (toscano and mcmurray, 2010).
</nextsent>
<nextsent>recalibration and selective adaptation the flexibility of phonetic categories has been demonstrated through studies which manipulate the distribution of acoustic cues associated with particular category.
</nextsent>
<nextsent>these studies take advantage of the natural variability of acoustic cues.
</nextsent>
<nextsent>take, for example, the consonants /b/ and /d/.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2531">
<title id=" W10-4169.xml">chinese word sense induction based on hierarchical clustering algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we used the hierarchical clustering algorithm as the classifier for word sense induction.
</prevsent>
<prevsent>experiments show the system can achieve 72% f-score about train-corpus and 65% f-score about test-corpus.
</prevsent>
</prevsection>
<citsent citstr=" H05-1097 ">
word sense induction is central problem in many natural language processing tasks such as information extraction, information retrieval, and machine translation [vickrey et al, 2005].<papid> H05-1097 </papid></citsent>
<aftsection>
<nextsent>clp 2010 launches totally 4 tasks for evaluation exercise, these are: chinese word segmentation, chinese parsing, chinese personal name disambiguation and chinese word sense induction.
</nextsent>
<nextsent>we participated in task 4, which is chinese word sense induction..
</nextsent>
<nextsent>because the contents surround an ambiguous word is related to its meaning, we solve the sense problem by grouping the instances of the target word into the supposed number of clusters according to the similarity of contexts of the instance.
</nextsent>
<nextsent>in this paper we used the hierarchical clustering algorithm to accomplish the problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2532">
<title id=" W10-4169.xml">chinese word sense induction based on hierarchical clustering algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>researchers have proposed much approach to the sense induction task which involved the use of basic word co-occurrence features and application of classical clustering algorithms.
</prevsent>
<prevsent>because the meanings of unknown words can be inferred from the contexts in which they appear, pantel and lin (2002) map the senses to wordnet.
</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
more recently, the mapping has been used to test the system on publicly available benchmarks (purandare and pedersen, 2004; <papid> W04-2406 </papid>niu et al, 2005).<papid> P05-1049 </papid></citsent>
<aftsection>
<nextsent>however, this approach does not generalize to multiple-sense words.
</nextsent>
<nextsent>each sense of polysemous word can appear in different context, there have been many attempts in recent years to apply classical clustering algorithms to this problem.
</nextsent>
<nextsent>clustering algorithms have been employed ranging from k-means (purandare and pedersen, 2004), <papid> W04-2406 </papid>to agglomerative clustering (schutze, 1998), and the information bottleneck (niu et al., 2007).<papid> W07-2037 </papid></nextsent>
<nextsent>senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (veronis, 2004).the sib algorithm was used to estimate cluster structure, which measures the similarity of contexts of instances according to the similarity of their feature conditional distribution(slonim, et al,2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2533">
<title id=" W10-4169.xml">chinese word sense induction based on hierarchical clustering algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>researchers have proposed much approach to the sense induction task which involved the use of basic word co-occurrence features and application of classical clustering algorithms.
</prevsent>
<prevsent>because the meanings of unknown words can be inferred from the contexts in which they appear, pantel and lin (2002) map the senses to wordnet.
</prevsent>
</prevsection>
<citsent citstr=" P05-1049 ">
more recently, the mapping has been used to test the system on publicly available benchmarks (purandare and pedersen, 2004; <papid> W04-2406 </papid>niu et al, 2005).<papid> P05-1049 </papid></citsent>
<aftsection>
<nextsent>however, this approach does not generalize to multiple-sense words.
</nextsent>
<nextsent>each sense of polysemous word can appear in different context, there have been many attempts in recent years to apply classical clustering algorithms to this problem.
</nextsent>
<nextsent>clustering algorithms have been employed ranging from k-means (purandare and pedersen, 2004), <papid> W04-2406 </papid>to agglomerative clustering (schutze, 1998), and the information bottleneck (niu et al., 2007).<papid> W07-2037 </papid></nextsent>
<nextsent>senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (veronis, 2004).the sib algorithm was used to estimate cluster structure, which measures the similarity of contexts of instances according to the similarity of their feature conditional distribution(slonim, et al,2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2535">
<title id=" W10-4169.xml">chinese word sense induction based on hierarchical clustering algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, this approach does not generalize to multiple-sense words.
</prevsent>
<prevsent>each sense of polysemous word can appear in different context, there have been many attempts in recent years to apply classical clustering algorithms to this problem.
</prevsent>
</prevsection>
<citsent citstr=" W07-2037 ">
clustering algorithms have been employed ranging from k-means (purandare and pedersen, 2004), <papid> W04-2406 </papid>to agglomerative clustering (schutze, 1998), and the information bottleneck (niu et al., 2007).<papid> W07-2037 </papid></citsent>
<aftsection>
<nextsent>senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (veronis, 2004).the sib algorithm was used to estimate cluster structure, which measures the similarity of contexts of instances according to the similarity of their feature conditional distribution(slonim, et al,2002).
</nextsent>
<nextsent>each algorithm treats words as feature vectors, using the same similarity function based on context information.
</nextsent>
<nextsent>the remainder of this paper is organized as follows.
</nextsent>
<nextsent>in section 2 the featured set and word similarity definition is introduced.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2536">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>two of the most studied error types in learner english are preposition and article errors since they make up large percentage of errors in learner writing (16% and 13% respectively in the cambridge learner corpus, without considering spelling and punctuation errors).
</prevsent>
<prevsent>the most widely used approach for detecting and correcting these errors is classification, with lexical and pos features gleaned from window around the potential preposition/article site in sentence.
</prevsent>
</prevsection>
<citsent citstr=" W07-1604 ">
some recent work includes chodorow et al (2007), <papid> W07-1604 </papid>de felice and pulman (2008), gamon (2010), <papid> N10-1019 </papid>han et al (2010), izumi et al (2004), tetreault and chodorow (2008), <papid> C08-1109 </papid>rozovskaya and roth (2010<papid> N10-1018 </papid>a), zovskaya and roth (2010b).</citsent>
<aftsection>
<nextsent>gamon et al (2008) <papid> I08-1059 </papid>and gamon (2010) <papid> N10-1019 </papid>used language model in addition to classifier and combined the classifier output and language model scores in meta classifier.</nextsent>
<nextsent>these error-specific methods achieve high precision (up to 80-90% on some corpora) but only capture highly constrained error types such as preposition and determiner errors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2537">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>two of the most studied error types in learner english are preposition and article errors since they make up large percentage of errors in learner writing (16% and 13% respectively in the cambridge learner corpus, without considering spelling and punctuation errors).
</prevsent>
<prevsent>the most widely used approach for detecting and correcting these errors is classification, with lexical and pos features gleaned from window around the potential preposition/article site in sentence.
</prevsent>
</prevsection>
<citsent citstr=" N10-1019 ">
some recent work includes chodorow et al (2007), <papid> W07-1604 </papid>de felice and pulman (2008), gamon (2010), <papid> N10-1019 </papid>han et al (2010), izumi et al (2004), tetreault and chodorow (2008), <papid> C08-1109 </papid>rozovskaya and roth (2010<papid> N10-1018 </papid>a), zovskaya and roth (2010b).</citsent>
<aftsection>
<nextsent>gamon et al (2008) <papid> I08-1059 </papid>and gamon (2010) <papid> N10-1019 </papid>used language model in addition to classifier and combined the classifier output and language model scores in meta classifier.</nextsent>
<nextsent>these error-specific methods achieve high precision (up to 80-90% on some corpora) but only capture highly constrained error types such as preposition and determiner errors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2538">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>two of the most studied error types in learner english are preposition and article errors since they make up large percentage of errors in learner writing (16% and 13% respectively in the cambridge learner corpus, without considering spelling and punctuation errors).
</prevsent>
<prevsent>the most widely used approach for detecting and correcting these errors is classification, with lexical and pos features gleaned from window around the potential preposition/article site in sentence.
</prevsent>
</prevsection>
<citsent citstr=" C08-1109 ">
some recent work includes chodorow et al (2007), <papid> W07-1604 </papid>de felice and pulman (2008), gamon (2010), <papid> N10-1019 </papid>han et al (2010), izumi et al (2004), tetreault and chodorow (2008), <papid> C08-1109 </papid>rozovskaya and roth (2010<papid> N10-1018 </papid>a), zovskaya and roth (2010b).</citsent>
<aftsection>
<nextsent>gamon et al (2008) <papid> I08-1059 </papid>and gamon (2010) <papid> N10-1019 </papid>used language model in addition to classifier and combined the classifier output and language model scores in meta classifier.</nextsent>
<nextsent>these error-specific methods achieve high precision (up to 80-90% on some corpora) but only capture highly constrained error types such as preposition and determiner errors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2539">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>two of the most studied error types in learner english are preposition and article errors since they make up large percentage of errors in learner writing (16% and 13% respectively in the cambridge learner corpus, without considering spelling and punctuation errors).
</prevsent>
<prevsent>the most widely used approach for detecting and correcting these errors is classification, with lexical and pos features gleaned from window around the potential preposition/article site in sentence.
</prevsent>
</prevsection>
<citsent citstr=" N10-1018 ">
some recent work includes chodorow et al (2007), <papid> W07-1604 </papid>de felice and pulman (2008), gamon (2010), <papid> N10-1019 </papid>han et al (2010), izumi et al (2004), tetreault and chodorow (2008), <papid> C08-1109 </papid>rozovskaya and roth (2010<papid> N10-1018 </papid>a), zovskaya and roth (2010b).</citsent>
<aftsection>
<nextsent>gamon et al (2008) <papid> I08-1059 </papid>and gamon (2010) <papid> N10-1019 </papid>used language model in addition to classifier and combined the classifier output and language model scores in meta classifier.</nextsent>
<nextsent>these error-specific methods achieve high precision (up to 80-90% on some corpora) but only capture highly constrained error types such as preposition and determiner errors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2541">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the most widely used approach for detecting and correcting these errors is classification, with lexical and pos features gleaned from window around the potential preposition/article site in sentence.
</prevsent>
<prevsent>some recent work includes chodorow et al (2007), <papid> W07-1604 </papid>de felice and pulman (2008), gamon (2010), <papid> N10-1019 </papid>han et al (2010), izumi et al (2004), tetreault and chodorow (2008), <papid> C08-1109 </papid>rozovskaya and roth (2010<papid> N10-1018 </papid>a), zovskaya and roth (2010b).</prevsent>
</prevsection>
<citsent citstr=" I08-1059 ">
gamon et al (2008) <papid> I08-1059 </papid>and gamon (2010) <papid> N10-1019 </papid>used language model in addition to classifier and combined the classifier output and language model scores in meta classifier.</citsent>
<aftsection>
<nextsent>these error-specific methods achieve high precision (up to 80-90% on some corpora) but only capture highly constrained error types such as preposition and determiner errors.
</nextsent>
<nextsent>there has also been research on error-detection methods that are not designed to identify specific error type.
</nextsent>
<nextsent>the basic idea behind these error agnostic approaches is to identify an error where there is particularly unlikely sequence compared to the patterns found in large well-formed corpus.
</nextsent>
<nextsent>atwell (1986) used low-likelihood sequences of pos tags as indicators for the presence of an error.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2547">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>bigert and knutsson (2002) employed statistical method to identify variety of errors in swedish writing as rare sequences of morpho-syntactic tags.
</prevsent>
<prevsent>they significantly reduced false positives by using additional methods to determine whether the unexpected sequence is due to phrase or sentence boundaries or due to rare single tags.
</prevsent>
</prevsection>
<citsent citstr=" A00-2019 ">
chodorow and leacock (2000) <papid> A00-2019 </papid>utilized mutual information and chi-square statistics to identify typical contexts for small set of targeted words from large well-formed corpus.</citsent>
<aftsection>
<nextsent>comparing these statistics to the ones found in novel sentence, they could identify unlikely contexts for the targeted words that were often good indicators of the presence of an error.
</nextsent>
<nextsent>sun et al (2007) <papid> P07-1011 </papid>mined for patterns that consist of pos tags and function words.</nextsent>
<nextsent>the patterns are of variable length and can also contain gaps.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2548">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>chodorow and leacock (2000) <papid> A00-2019 </papid>utilized mutual information and chi-square statistics to identify typical contexts for small set of targeted words from large well-formed corpus.</prevsent>
<prevsent>comparing these statistics to the ones found in novel sentence, they could identify unlikely contexts for the targeted words that were often good indicators of the presence of an error.</prevsent>
</prevsection>
<citsent citstr=" P07-1011 ">
sun et al (2007) <papid> P07-1011 </papid>mined for patterns that consist of pos tags and function words.</citsent>
<aftsection>
<nextsent>the patterns are of variable length and can also contain gaps.
</nextsent>
<nextsent>patterns were then combined in classifier to distinguish correct from erroneous sentences.
</nextsent>
<nextsent>wagner et al (2007) combined parse probabilities from set of statistical parsers and pos tag n-gram probabilities in classifier to detect ungrammatical sentences.
</nextsent>
<nextsent>okanohara and tsu jii (2007) <papid> P07-1010 </papid>differed from the previous approaches in that they directly used discriminative language models to distinguish correct from incorrect sentences, without the direct modeling of error indicating patterns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2549">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>patterns were then combined in classifier to distinguish correct from erroneous sentences.
</prevsent>
<prevsent>wagner et al (2007) combined parse probabilities from set of statistical parsers and pos tag n-gram probabilities in classifier to detect ungrammatical sentences.
</prevsent>
</prevsection>
<citsent citstr=" P07-1010 ">
okanohara and tsu jii (2007) <papid> P07-1010 </papid>differed from the previous approaches in that they directly used discriminative language models to distinguish correct from incorrect sentences, without the direct modeling of error indicating patterns.</citsent>
<aftsection>
<nextsent>park and levy (2011) <papid> P11-1094 </papid>use noisy channel model with base language model and set of error-specific noise models for error detection and correction.</nextsent>
<nextsent>in contrast to previous work, we cast the task as sequence modeling problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2550">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>wagner et al (2007) combined parse probabilities from set of statistical parsers and pos tag n-gram probabilities in classifier to detect ungrammatical sentences.
</prevsent>
<prevsent>okanohara and tsu jii (2007) <papid> P07-1010 </papid>differed from the previous approaches in that they directly used discriminative language models to distinguish correct from incorrect sentences, without the direct modeling of error indicating patterns.</prevsent>
</prevsection>
<citsent citstr=" P11-1094 ">
park and levy (2011) <papid> P11-1094 </papid>use noisy channel model with base language model and set of error-specific noise models for error detection and correction.</citsent>
<aftsection>
<nextsent>in contrast to previous work, we cast the task as sequence modeling problem.
</nextsent>
<nextsent>this provides flexible framework in which multiple statistical and linguistic signals can be combined and calibrated by supervised learning.
</nextsent>
<nextsent>the approach is er ror-agnostic and can easily be extended with additional statistical or linguistic features.
</nextsent>
<nextsent>errors consist of sub-sequence of tokens in longer token sequence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2551">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> error detection by sequence modeling.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1 shows the structure of our memm with markov order of five (the diagram only shows the complete set of arcs for the last state).
</prevsent>
<prevsent>the in put sentence contains the token sequence the past year was stayed ? with the error was stayed.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
instead of using the tokens themselves as observations, we chose to use pos tags assigned by an automatic tagger (toutanova et al 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>this choice was motivated by data sparseness.
</nextsent>
<nextsent>learning model that observes individual lexical items and predicts sequence of error/non-error tags would be ideal, but given the many different error types and triggering contexts for an error, such model would require much more training data.
</nextsent>
<nextsent>a large set of features that serve as constraints on the state transition models are extracted for each state.
</nextsent>
<nextsent>these features are described in section 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2555">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>two binary features indicate whether token is capitalized (initial capitalization or all capitalized), one feature indicates the token length in characters and one feature measures the number of tokens in the sentence.
</prevsent>
<prevsent>5.3 linguistic analysis features.
</prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
each sentence is linguistically analyzed by pcfg-la parser (petrov et al, 2006) <papid> P06-1055 </papid>trained on the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>a number of features are extracted from the constituency tree to assess the syntactic complexity of the whole sentence, the syntactic complexity of the local environment of token, and simple constituency information for each token.
</nextsent>
<nextsent>these features are: label of the parent and grandparent node, number of sibling nodes, number of siblings of the parent, presence of governing head node, label of the governing head node, and length of path to the root.
</nextsent>
<nextsent>an additional feature indicates whether the pos tag assigned by the parser does not match the tag assigned by the pos tagger, which may indicate tagging error.
</nextsent>
<nextsent>6.1 design.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2556">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>two binary features indicate whether token is capitalized (initial capitalization or all capitalized), one feature indicates the token length in characters and one feature measures the number of tokens in the sentence.
</prevsent>
<prevsent>5.3 linguistic analysis features.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
each sentence is linguistically analyzed by pcfg-la parser (petrov et al, 2006) <papid> P06-1055 </papid>trained on the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>a number of features are extracted from the constituency tree to assess the syntactic complexity of the whole sentence, the syntactic complexity of the local environment of token, and simple constituency information for each token.
</nextsent>
<nextsent>these features are: label of the parent and grandparent node, number of sibling nodes, number of siblings of the parent, presence of governing head node, label of the governing head node, and length of path to the root.
</nextsent>
<nextsent>an additional feature indicates whether the pos tag assigned by the parser does not match the tag assigned by the pos tagger, which may indicate tagging error.
</nextsent>
<nextsent>6.1 design.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2569">
<title id=" W11-1422.xml">high order sequence modeling for language learner error detection </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>two possibilities that we would like to high light are the model structure and the feature set.
</prevsent>
<prevsent>as mentioned in section 3, instead of using separate pos tagger we could follow mccallum et al (2003) and design model that jointly predicts two sequences: pos tags and error tags.
</prevsent>
</prevsection>
<citsent citstr=" N10-2012 ">
as for feature sets, we conducted some preliminary additional experiments where we added second set of language model features, based on different language model, namely the microsoft web n-gram model (wang et al 2010).<papid> N10-2012 </papid></citsent>
<aftsection>
<nextsent>the addition of these features raised both precision and recall.
</nextsent>
<nextsent>finally, an error detection system is only of practical use if it is combined with component that suggests possible corrections.
</nextsent>
<nextsent>for future work, we envision combination of generic error detection with corpus-based lookup system that finds alternative strings that have been observed in similar contexts.
</nextsent>
<nextsent>all these alternatives can then be scored by language model in the original context of the user input, allowing only those suggestions to be shown to the user that achieve better language model score than the original input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2570">
<title id=" W11-1805.xml">biomedical event extraction from abstracts and full papers using search based structured prediction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, we report on experiments using simple domain adaptation method.
</prevsent>
<prevsent>the term biomedical event extraction is used to refer to the task of extracting descriptions of actions and relations among one or more entities from the biomedical literature.
</prevsent>
</prevsection>
<citsent citstr=" W11-1802 ">
the bionlp 2011 shared task genia task1 (bionlp11st-ge1) (kim et al, 2011) <papid> W11-1802 </papid>focuses on extracting events from abstracts and full papers.</citsent>
<aftsection>
<nextsent>the inclusion of full papers in the datasets is the only difference from task1 of the bionlp 2009 shared task (bionlp09st1) (kim et al., 2009), <papid> W09-1401 </papid>which used the same task definition and abstracts dataset.</nextsent>
<nextsent>each event consists of trigger and one or more arguments, the latter being proteins or other events.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2571">
<title id=" W11-1805.xml">biomedical event extraction from abstracts and full papers using search based structured prediction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the term biomedical event extraction is used to refer to the task of extracting descriptions of actions and relations among one or more entities from the biomedical literature.
</prevsent>
<prevsent>the bionlp 2011 shared task genia task1 (bionlp11st-ge1) (kim et al, 2011) <papid> W11-1802 </papid>focuses on extracting events from abstracts and full papers.</prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
the inclusion of full papers in the datasets is the only difference from task1 of the bionlp 2009 shared task (bionlp09st1) (kim et al., 2009), <papid> W09-1401 </papid>which used the same task definition and abstracts dataset.</citsent>
<aftsection>
<nextsent>each event consists of trigger and one or more arguments, the latter being proteins or other events.
</nextsent>
<nextsent>the protein names are annotated in advance and any token in sentence can be trigger for one of the nine event types.
</nextsent>
<nextsent>in an example demonstrating the complexity of the task, given the passage ?.
</nextsent>
<nextsent>sq 22536 suppressed gp41-inducedil-10 production in monocytes?, systems should extract the three nested events shown in fig.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2572">
<title id=" W11-1805.xml">biomedical event extraction from abstracts and full papers using search based structured prediction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>searnconverts the problem of learning model for structured prediction into learning set of models for cost-sensitive classification (csc).
</prevsent>
<prevsent>in csc, each training instance has vector of misclassification costs associated with it, thus rendering some mistakes in some instances to be more expensive than others.
</prevsent>
</prevsection>
<citsent citstr=" N10-1123 ">
compared to other structured prediction frameworks such as markov logic networks (poon and vanderwende, 2010), <papid> N10-1123 </papid>searn provides high modeling flexibility but it does not requiring task dependent approximate inference.in this work, we show that searn is more accurate than pipeline using the same features and it is better able to handle the domain shift from abstract sto full papers.</citsent>
<aftsection>
<nextsent>furthermore, we report on experiments with the simple domain adaptation method proposed by daume?
</nextsent>
<nextsent>iii (2007), which creates version of each feature for each domain.
</nextsent>
<nextsent>while the results were mixed, this method improves our performance on full papers of the test set, for which little training data is available.
</nextsent>
<nextsent>figure 1 describes the event extraction decomposition that is used throughout the paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2573">
<title id=" W11-1805.xml">biomedical event extraction from abstracts and full papers using search based structured prediction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the aggressiveness parameter and the number of rounds in parameter learning are set by tuning on 10% of the training dataand we use the variant named pa-ii with prediction based updates.
</prevsent>
<prevsent>for searn, we set the interpolation parameter ? to 0.3.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
for syntactic parsing, we use the output of the parser of charniak and johnson(2005) <papid> P05-1022 </papid>adapted to the biomedical domain by mcclosky (2010), as provided by the shared task organizers in the stanford collapsed dependencies with conjunct dependency propagation (stenetorp et al, 2011).<papid> W11-1816 </papid></citsent>
<aftsection>
<nextsent>lemmatization is performed using morpha (minnen et al, 2001).
</nextsent>
<nextsent>no other knowledge sources or tools are used.in order to assess the benefits of joint learning under searn, we compare it against pipeline of independently learned classifiers using the same features and task decomposition.
</nextsent>
<nextsent>table 1 reports the recall/precision/f-score achieved in each stage, as well as the overall performance.
</nextsent>
<nextsent>searn obtains better performance on the development set by 8.5 f-score points.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2574">
<title id=" W11-1805.xml">biomedical event extraction from abstracts and full papers using search based structured prediction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the aggressiveness parameter and the number of rounds in parameter learning are set by tuning on 10% of the training dataand we use the variant named pa-ii with prediction based updates.
</prevsent>
<prevsent>for searn, we set the interpolation parameter ? to 0.3.
</prevsent>
</prevsection>
<citsent citstr=" W11-1816 ">
for syntactic parsing, we use the output of the parser of charniak and johnson(2005) <papid> P05-1022 </papid>adapted to the biomedical domain by mcclosky (2010), as provided by the shared task organizers in the stanford collapsed dependencies with conjunct dependency propagation (stenetorp et al, 2011).<papid> W11-1816 </papid></citsent>
<aftsection>
<nextsent>lemmatization is performed using morpha (minnen et al, 2001).
</nextsent>
<nextsent>no other knowledge sources or tools are used.in order to assess the benefits of joint learning under searn, we compare it against pipeline of independently learned classifiers using the same features and task decomposition.
</nextsent>
<nextsent>table 1 reports the recall/precision/f-score achieved in each stage, as well as the overall performance.
</nextsent>
<nextsent>searn obtains better performance on the development set by 8.5 f-score points.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2575">
<title id=" W10-4232.xml">udel named entity recognition and reference regeneration from surface text </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>entities found by the ner tool are marked as names, and the additional res we identified are marked as either pronouns or common nouns.case values are determined by analyzing the assigned type and any type dependency representation (provided by the parser) involving the entity.at this stage we also note the gender of each pronoun and common noun, the plurality of each reference, and begin to deal with embedded entities.
</prevsent>
<prevsent>the next step identifies which tagged mentionscorefer.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
we implemented coreference resolution tool using shallow rule-based approach inspired by lappin and leass (1994) <papid> J94-4002 </papid>and bontcheva et al (2002).</citsent>
<aftsection>
<nextsent>each mention is compared to allpreviously-seen entities on the basis of case, gender, synfunc, plurality, and type.
</nextsent>
<nextsent>each entity is then evaluated in order of appearance and compared to all previous entities starting with the most recent and working back to the first in the text.
</nextsent>
<nextsent>we apply rules to each of these pairs basedon the reg08-type attribute of the current entity.
</nextsent>
<nextsent>names and common nouns are analyzed using string and word token matching.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2576">
<title id=" W10-4232.xml">udel named entity recognition and reference regeneration from surface text </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>as post-processing step, we remove all extra (non-grec) tags used in previous steps, re-order the remaining attributes in the proper sequence, add the list of res (alt-refex), and write the final output following the grec format.
</prevsent>
<prevsent>at this point, the grec-ner task is concluded and its output is used as input for the grec-full task.
</prevsent>
</prevsection>
<citsent citstr=" W09-2821 ">
to improve the fluency and clarity of the text by regenerating the referring expressions, we relyon the system we developed for the grec named entity challenge 2010 (neg), refined version of our 2009 submission (greenbacker and mccoy, 2009<papid> W09-2821 </papid>a).</citsent>
<aftsection>
<nextsent>this system trains decision treeson psycholinguistically-inspired feature set (described by greenbacker and mccoy (2009<papid> W09-2821 </papid>b)) extracted from training corpus.</nextsent>
<nextsent>it predicts the most appropriate reference type and case for the given context, and selects the best match from the list of available res.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2578">
<title id=" W11-0124.xml">abductive reasoning with a large knowledge base for discourse processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as the reader will see in the following sections, inferences carried out by mini-tacitus are fairly general and not tuned for particular application.
</prevsent>
<prevsent>we decided to test our approach on rte because this is well-defined task that captures major semantic inference needs across many natural language 1http://www.rutumulkar.com/download/tacitus/tacitus.php 225 processing applications, such as question answering, information retrieval, information extraction, and document summarization.
</prevsent>
</prevsection>
<citsent citstr=" L08-1038 ">
for evaluation, we have chosen the rte-2 dataset (bar-haim et al , 2006), because besides providing text-hypothesis pairs and gold standard this dataset has been annotated with framenet frame and role labels (burchardt and pennacchiotti, 2008) <papid> L08-1038 </papid>which gives us the possibility of evaluating our frame and role labeling based on the axioms extracted from framenet.</citsent>
<aftsection>
<nextsent>our natural language pipeline produces interpretations of texts given the appropriate knowledge base.
</nextsent>
<nextsent>a text is first input to the english slot grammar (esg) parser (mccord, 1990, 2010).
</nextsent>
<nextsent>for each segment, the parse produced by esg is dependency tree that shows both surface and deep structure.
</nextsent>
<nextsent>the deep structure is exhibited via word sense predication for each node, with logical arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2579">
<title id=" W11-0124.xml">abductive reasoning with a large knowledge base for discourse processing </title>
<section> nl pipeline and abductive reasoning.  </section>
<citcontext>
<prevsection>
<prevsent>the deep structure is exhibited via word sense predication for each node, with logical arguments.
</prevsent>
<prevsent>these logical predications form good start on logical form (lf) for the whole segment.
</prevsent>
</prevsection>
<citsent citstr=" P85-1008 ">
an add-on to esg converts the parse tree into lf in the style of hobbs (1985).<papid> P85-1008 </papid></citsent>
<aftsection>
<nextsent>the lf is conjunction of predications, which have generalized entity arguments that can be used for showing relationships among the predications.
</nextsent>
<nextsent>these lfs are used by the downstream components.
</nextsent>
<nextsent>the interpretation of the text is carried out by an inference system called mini-tacitus using weighted abduction as described in detail in hobbs et al  (1993).
</nextsent>
<nextsent>mini-tacitus tries to prove the logical form of the text, allowing assumptions where necessary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2580">
<title id=" W11-0124.xml">abductive reasoning with a large knowledge base for discourse processing </title>
<section> knowledge base.  </section>
<citcontext>
<prevsection>
<prevsent>these axioms have the total weight of 100.
</prevsent>
<prevsent>on(e2,e1,x2):25 &amp; back(e3,x2):25 &amp; of (e4,x2,x1):25 &amp; horse(e5,x1):25 ? synset-x(e0,x0)the second resource which we have used as source of axioms is framenet, release 1.5, see ruppenhofer et al  (2006).
</prevsent>
</prevsection>
<citsent citstr=" D07-1002 ">
framenet has shorter history in nlp applications than wordnet, but lately more and more researchers have been demonstrating its potential to improve the quality of question answering(shen and lapata, 2007) <papid> D07-1002 </papid>and recognizing textual entailment (burchardt et al , 2009).</citsent>
<aftsection>
<nextsent>the lexical meaning of predicates in framenet is represented in terms of frames which describe prototypical situations spoken about in natural language.
</nextsent>
<nextsent>every frame contains set of roles corresponding to the participants of the described situation.
</nextsent>
<nextsent>predicates with similar semantics are assigned to the same frame; e.g. both give and hand over refer to the giving frame.
</nextsent>
<nextsent>for most of the lexical elements framenet provides syntactic patterns showing the surface realization of these lexical elements and their arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2582">
<title id=" W11-1724.xml">instance level transfer learning for cross lingual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>accordingly, automated opinion analysis has attracted growing attentions.
</prevsent>
<prevsent>opinion analysis, also known as sentiment analysis, sentiment classification, and opinion mining, aims to identify opinions in text and classify their sentiment polarity (pang and lee, 2008).
</prevsent>
</prevsection>
<citsent citstr=" P07-1056 ">
many sentiment resources such as sentiment lexicons (e.g., sentiwordnet (esuli and sebastiani, 2006))and opinion corpora (e.g., mpqa (blitzer et al , 2007)) <papid> P07-1056 </papid>have been developed on different languages in which most of them are for english.</citsent>
<aftsection>
<nextsent>the lack of reliably sentiment resources is one of the core issues in opinion analysis for other languages.
</nextsent>
<nextsent>meanwhile, the manually annotation is costly, thus the amount of available annotated opinion corpora are still insufficient for supporting supervised learning, even for english.
</nextsent>
<nextsent>these facts motivate to borrow?
</nextsent>
<nextsent>the opinion resources in one language (source language, sl) to another language (target language, tl) for improving the opinion analysis on the target language.cross lingual opinion analysis (cloa) techniques are investigated to improve opinion analysis in tl through leveraging the opinion-related resources, such as dictionaries and annotated corpus in sl.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2583">
<title id=" W11-1724.xml">instance level transfer learning for cross lingual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these facts motivate to borrow?
</prevsent>
<prevsent>the opinion resources in one language (source language, sl) to another language (target language, tl) for improving the opinion analysis on the target language.cross lingual opinion analysis (cloa) techniques are investigated to improve opinion analysis in tl through leveraging the opinion-related resources, such as dictionaries and annotated corpus in sl.
</prevsent>
</prevsection>
<citsent citstr=" P07-1123 ">
some cloa works used bilingual dictionaries (mihalcea et al , 2007), <papid> P07-1123 </papid>or aligned corpus (kim and hovy, 2006) <papid> N06-1026 </papid>to align the expressions between source and target languages.</citsent>
<aftsection>
<nextsent>these works are puzzled by the limited aligned opinion resources.
</nextsent>
<nextsent>alternatively, some works used machine translation system to do the opinion expression alignment.
</nextsent>
<nextsent>banea et al  (2008) <papid> D08-1014 </papid>proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language.</nextsent>
<nextsent>wan (2009) <papid> P09-1027 </papid>combined the annotated english reviews, unannotated chinese reviews and their translations to co-train two separate classifiers for each language, respectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2584">
<title id=" W11-1724.xml">instance level transfer learning for cross lingual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these facts motivate to borrow?
</prevsent>
<prevsent>the opinion resources in one language (source language, sl) to another language (target language, tl) for improving the opinion analysis on the target language.cross lingual opinion analysis (cloa) techniques are investigated to improve opinion analysis in tl through leveraging the opinion-related resources, such as dictionaries and annotated corpus in sl.
</prevsent>
</prevsection>
<citsent citstr=" N06-1026 ">
some cloa works used bilingual dictionaries (mihalcea et al , 2007), <papid> P07-1123 </papid>or aligned corpus (kim and hovy, 2006) <papid> N06-1026 </papid>to align the expressions between source and target languages.</citsent>
<aftsection>
<nextsent>these works are puzzled by the limited aligned opinion resources.
</nextsent>
<nextsent>alternatively, some works used machine translation system to do the opinion expression alignment.
</nextsent>
<nextsent>banea et al  (2008) <papid> D08-1014 </papid>proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language.</nextsent>
<nextsent>wan (2009) <papid> P09-1027 </papid>combined the annotated english reviews, unannotated chinese reviews and their translations to co-train two separate classifiers for each language, respectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2585">
<title id=" W11-1724.xml">instance level transfer learning for cross lingual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these works are puzzled by the limited aligned opinion resources.
</prevsent>
<prevsent>alternatively, some works used machine translation system to do the opinion expression alignment.
</prevsent>
</prevsection>
<citsent citstr=" D08-1014 ">
banea et al  (2008) <papid> D08-1014 </papid>proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language.</citsent>
<aftsection>
<nextsent>wan (2009) <papid> P09-1027 </papid>combined the annotated english reviews, unannotated chinese reviews and their translations to co-train two separate classifiers for each language, respectively.</nextsent>
<nextsent>182 these works directly used all of the translation of annotated corpus in source language as the training data for target language without considering the following two problems: (1) the machine translation errors propagate to following cloa procedure; (2) the annotated corpora from different languages are collected from different domains and different writing styles which lead the training and testing data having different feature spaces and distributions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2586">
<title id=" W11-1724.xml">instance level transfer learning for cross lingual opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>alternatively, some works used machine translation system to do the opinion expression alignment.
</prevsent>
<prevsent>banea et al  (2008) <papid> D08-1014 </papid>proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language.</prevsent>
</prevsection>
<citsent citstr=" P09-1027 ">
wan (2009) <papid> P09-1027 </papid>combined the annotated english reviews, unannotated chinese reviews and their translations to co-train two separate classifiers for each language, respectively.</citsent>
<aftsection>
<nextsent>182 these works directly used all of the translation of annotated corpus in source language as the training data for target language without considering the following two problems: (1) the machine translation errors propagate to following cloa procedure; (2) the annotated corpora from different languages are collected from different domains and different writing styles which lead the training and testing data having different feature spaces and distributions.
</nextsent>
<nextsent>therefore, the performances of these supervised learning algorithms are affected.
</nextsent>
<nextsent>to address these problems, we propose two instance level transfer learning based algorithm sto estimate the confidence of translated sl examples and to transfer the promising ones as the supplementary tl training data.
</nextsent>
<nextsent>we firstly apply transfer ada boost (tradaboost) (dai et al ., 2007) to improve the overall performance with the union of target and translated source language training corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2592">
<title id=" W11-0419.xml">increasing informative ness in temporal annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the narrative container is the default interval containing the events being discussed in the text, when no explicit temporal anchor is given.
</prevsent>
<prevsent>by exploiting this notion in the creation of new temporal annotation over time bank, we were able to reduce inconsistencies and increase informative ness when compared to existing tlinks in timebank.
</prevsent>
</prevsection>
<citsent citstr=" W10-1840 ">
in linguistic annotation projects, there is often gap between what the annotation schema is designed to capture and how the guidelines are interpreted by the annotators and adjudicators given specific corpus and task (ide and bunt, 2010; <papid> W10-1840 </papid>ide, 2007).</citsent>
<aftsection>
<nextsent>the difficulty in resolving these two aspects of annotation is compounded when tasks are looked at in potentially incomplete annotation task; namely, where the guideline is following specification to point, but in fact human annotation is not even suggested as complete because it would be infeasible.
</nextsent>
<nextsent>creating temporal links to represent the timeline of events ina document is an example of this: human annotation of every possible temporal relationship between events and times in narrative would be an overwhelming task.in this paper, we discuss how temporal relation annotation must be sensitive to two aspects of the task that were not mentioned in the time bank guideline (pustejovsky et al, 2005): (a) sensitivity to the genre and style of the text; and (b) the interaction with discourse relations that explicitly reference the flow of the narrative in the text.
</nextsent>
<nextsent>we believe that making reference to both these aspects in the text during the annotation process will increase overall informative ness and accuracy of the annotation.
</nextsent>
<nextsent>in the present paper, we focus primarily on the first ofthese points, and introduce document level information structure we call narrative container (nc).because of the impossibility of humans capturing every relationship, it is vital that the annotation guidelines describe an approach that will result in maximally informative temporal links without relying on standards that are too difficult to apply.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2593">
<title id=" W11-0419.xml">increasing informative ness in temporal annotation </title>
<section> identifying temporal relations.  </section>
<citcontext>
<prevsection>
<prevsent>temporal ordering relations in text are of three kinds: (2) a. relation between two events; b. relation between two times; c. relation between time and an event.
</prevsent>
<prevsent>timeml, as formal specification of the temporal information conveyed in language, makes no distinction between these ordering types.
</prevsent>
</prevsection>
<citsent citstr=" W04-0210 ">
but human reader of text does make distinction, based on the discourse relations established by the author of the narrative (miltsakaki et al, 2004; poesio, 2004).<papid> W04-0210 </papid></citsent>
<aftsection>
<nextsent>temporal expressions denoting the local narrative container in the text act as embedding intervals within which events occur.
</nextsent>
<nextsent>within timeml, these are event-time anchoring relations (tlinks).
</nextsent>
<nextsent>discourse relations establish how events relate to one another in the narrative, and hence should constrain temporal relations between two events.
</nextsent>
<nextsent>thus, one of the most significant constraints we can impose is to take advantage of the discourse structure in the document before event-event ordering relations are identified.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2594">
<title id=" W11-0419.xml">increasing informative ness in temporal annotation </title>
<section> identifying temporal relations.  </section>
<citcontext>
<prevsection>
<prevsent>consider first external informativeness.
</prevsent>
<prevsent>this is information derived from relations outside the temporal relation constraint set, e.g., as coming from explicit discourse relations between events (and hence is associated with the relations in (2a) above).
</prevsent>
</prevsection>
<citsent citstr=" L08-1093 ">
for example, we will assume that, for two events, e1 and e2, in text, the temporal relation between them is more informative if they are also linked through discourse relation,e.g., pdtb relation (prasad et al, 2008).<papid> L08-1093 </papid></citsent>
<aftsection>
<nextsent>making such an assumption will allow us to focus in on the temporal relations that are most valuable without having to exhaustively annotate all event pairs.
</nextsent>
<nextsent>now consider internal informativeness.
</nextsent>
<nextsent>this is information derived from the nature of the relation itself, as defined largely by the algebra of relations (allen, 1984; vilain et al, 1986).
</nextsent>
<nextsent>first, we assume that, for two events, e1 and e2, temporal relation r1 is more informative than r2 if r1 entails r2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2595">
<title id=" W11-0710.xml">language use as a reflection of socialization in online communities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>analyses were computed using word frequencies of common classes (such as self references) and 77 table 1: statistics dataset.
</prevsent>
<prevsent>posts 1,562,590 threads 68,226 users (at least one post) 31,307 time-span oct 2002 - jan 2011 manual coding.
</prevsent>
</prevsection>
<citsent citstr=" W06-3403 ">
huffaker et al (2006) <papid> W06-3403 </papid>examined asubset of the same data.</citsent>
<aftsection>
<nextsent>when comparing consecutive weeks over 6 week time period, they found that the language diverged.
</nextsent>
<nextsent>they hypothesized that this was caused by external events leading to the introduction of new words.our research differs from the research by cassell and tversky (2005), huffaker et al (2006) <papid> W06-3403 </papid>andpostmes et al (2000) in several respects.</nextsent>
<nextsent>forex ample, in all of this work, participants joined the community simultaneously at the inception of the community.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2601">
<title id=" W11-0710.xml">language use as a reflection of socialization in online communities </title>
<section> predicting membership duration.  </section>
<citcontext>
<prevsection>
<prevsent>as result, this method delivers sparse models.
</prevsent>
<prevsent>we useorthant-wise limited-memory quasi-newton optimizer (andrew and gao, 2007) as our optimization method.
</prevsent>
</prevsection>
<citsent citstr=" P07-1104 ">
this method has proven to establish competitive performances with other optimization methods, while producing sparse models (gao et al, 2007).<papid> P07-1104 </papid></citsent>
<aftsection>
<nextsent>because our observations suggest that language change decreases as members have been active longer, we also experimented with applying log transformation on the number of weeks.
</nextsent>
<nextsent>5.3 features.
</nextsent>
<nextsent>for all features, we only use information that has been available for that particular week.
</nextsent>
<nextsent>we explore different types of features related to the qualitative differences in language we discussed in section 3: textual, behavioral, sub forum and meta-features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2602">
<title id=" W11-0710.xml">language use as a reflection of socialization in online communities </title>
<section> predicting membership duration.  </section>
<citcontext>
<prevsection>
<prevsent>5.3.1 textual features we explore the following textual features: ? unigrams and bigrams.
</prevsent>
<prevsent>part of speech (pos) bigrams.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
text was tagged using the stanford pos tagger (toutanova et al., 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>81?
</nextsent>
<nextsent>liwc (pennebaker et al, 2001), word counting program that captures word classes and stylistic features.
</nextsent>
<nextsent>usernames.
</nextsent>
<nextsent>because some of the usernamesare common words, we only consider user names of users active in the same thread.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2603">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>c2011 association for computational linguistics provides substantial improvement over each individual baseline system and even state-of-the art monolingual slot filling systems.
</prevsent>
<prevsent>compared to previous methods of exploiting comparable corpora, our approach is novel in multiple aspects because it exploits knowledge from: (1)both local and global statistics; (2) both lan guages; and (3) both shallow and deep analysis.
</prevsent>
</prevsection>
<citsent citstr=" C04-1127 ">
sudo et al (2004) <papid> C04-1127 </papid>found that for cross lin gual single-document ie task, source language extraction and fact translation performed notably better than machine translation and target language extraction.</citsent>
<aftsection>
<nextsent>we observed the same results.
</nextsent>
<nextsent>in addition we also demonstrate that these two approaches are complementary and can be used to boost each others results in statistical rescoring model with global evidence from large comparable corpora.
</nextsent>
<nextsent>hakkani-tur et al (2007) described filtering mechanism using two cross lingual ie systems for improving cross lingual document retrieval.many previous validation methods for cross lingual qa, such as those organized by cross language evaluation forum (vallin et al, 2005), focused on local information which involves only the query and answer (e.g.
</nextsent>
<nextsent>(kwork and deng, 2006)), keyword translation (e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2604">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(harabagiu et al, 2005)).
</prevsent>
<prevsent>how ever, all of these approaches made limited efforts at disambiguating entities in queries and limited use of fact extraction in answer search and validation.
</prevsent>
</prevsection>
<citsent citstr=" D09-1016 ">
several recent ie studies have stressed the benefits of using information redundancy on estimating the correctness of the ie out put (downey et al, 2005; yangarber, 2006;patwardhan and riloff, 2009; <papid> D09-1016 </papid>ji and grishman, 2008).<papid> P08-1030 </papid></citsent>
<aftsection>
<nextsent>some recent research used comparable corpora to re-score name translitera tions (sproat et al, 2006; <papid> P06-1010 </papid>klementiev and roth, 2006) <papid> N06-1011 </papid>or mine new word translations (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>shao and ng, 2004; tao and zhai, 2005; hassan et al, 2007; udupa etal., 2009; <papid> E09-1091 </papid>ji, 2009).<papid> W09-3107 </papid></nextsent>
<nextsent>to the best of our knowledge, this is the first work on mining facts from comparable corpora for answer validation in new cross lingual entity profiling task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2605">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(harabagiu et al, 2005)).
</prevsent>
<prevsent>how ever, all of these approaches made limited efforts at disambiguating entities in queries and limited use of fact extraction in answer search and validation.
</prevsent>
</prevsection>
<citsent citstr=" P08-1030 ">
several recent ie studies have stressed the benefits of using information redundancy on estimating the correctness of the ie out put (downey et al, 2005; yangarber, 2006;patwardhan and riloff, 2009; <papid> D09-1016 </papid>ji and grishman, 2008).<papid> P08-1030 </papid></citsent>
<aftsection>
<nextsent>some recent research used comparable corpora to re-score name translitera tions (sproat et al, 2006; <papid> P06-1010 </papid>klementiev and roth, 2006) <papid> N06-1011 </papid>or mine new word translations (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>shao and ng, 2004; tao and zhai, 2005; hassan et al, 2007; udupa etal., 2009; <papid> E09-1091 </papid>ji, 2009).<papid> W09-3107 </papid></nextsent>
<nextsent>to the best of our knowledge, this is the first work on mining facts from comparable corpora for answer validation in new cross lingual entity profiling task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2606">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, all of these approaches made limited efforts at disambiguating entities in queries and limited use of fact extraction in answer search and validation.
</prevsent>
<prevsent>several recent ie studies have stressed the benefits of using information redundancy on estimating the correctness of the ie out put (downey et al, 2005; yangarber, 2006;patwardhan and riloff, 2009; <papid> D09-1016 </papid>ji and grishman, 2008).<papid> P08-1030 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1010 ">
some recent research used comparable corpora to re-score name translitera tions (sproat et al, 2006; <papid> P06-1010 </papid>klementiev and roth, 2006) <papid> N06-1011 </papid>or mine new word translations (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>shao and ng, 2004; tao and zhai, 2005; hassan et al, 2007; udupa etal., 2009; <papid> E09-1091 </papid>ji, 2009).<papid> W09-3107 </papid></citsent>
<aftsection>
<nextsent>to the best of our knowledge, this is the first work on mining facts from comparable corpora for answer validation in new cross lingual entity profiling task.
</nextsent>
<nextsent>3.1 task definition.
</nextsent>
<nextsent>the goal of the kbp slot filling task is to extract facts from large source corpus regarding certain attributes (slots?)
</nextsent>
<nextsent>of an entity, which may be person or organization, and use these facts to augment an existing knowledge base (kb).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2607">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, all of these approaches made limited efforts at disambiguating entities in queries and limited use of fact extraction in answer search and validation.
</prevsent>
<prevsent>several recent ie studies have stressed the benefits of using information redundancy on estimating the correctness of the ie out put (downey et al, 2005; yangarber, 2006;patwardhan and riloff, 2009; <papid> D09-1016 </papid>ji and grishman, 2008).<papid> P08-1030 </papid></prevsent>
</prevsection>
<citsent citstr=" N06-1011 ">
some recent research used comparable corpora to re-score name translitera tions (sproat et al, 2006; <papid> P06-1010 </papid>klementiev and roth, 2006) <papid> N06-1011 </papid>or mine new word translations (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>shao and ng, 2004; tao and zhai, 2005; hassan et al, 2007; udupa etal., 2009; <papid> E09-1091 </papid>ji, 2009).<papid> W09-3107 </papid></citsent>
<aftsection>
<nextsent>to the best of our knowledge, this is the first work on mining facts from comparable corpora for answer validation in new cross lingual entity profiling task.
</nextsent>
<nextsent>3.1 task definition.
</nextsent>
<nextsent>the goal of the kbp slot filling task is to extract facts from large source corpus regarding certain attributes (slots?)
</nextsent>
<nextsent>of an entity, which may be person or organization, and use these facts to augment an existing knowledge base (kb).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2608">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, all of these approaches made limited efforts at disambiguating entities in queries and limited use of fact extraction in answer search and validation.
</prevsent>
<prevsent>several recent ie studies have stressed the benefits of using information redundancy on estimating the correctness of the ie out put (downey et al, 2005; yangarber, 2006;patwardhan and riloff, 2009; <papid> D09-1016 </papid>ji and grishman, 2008).<papid> P08-1030 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
some recent research used comparable corpora to re-score name translitera tions (sproat et al, 2006; <papid> P06-1010 </papid>klementiev and roth, 2006) <papid> N06-1011 </papid>or mine new word translations (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>shao and ng, 2004; tao and zhai, 2005; hassan et al, 2007; udupa etal., 2009; <papid> E09-1091 </papid>ji, 2009).<papid> W09-3107 </papid></citsent>
<aftsection>
<nextsent>to the best of our knowledge, this is the first work on mining facts from comparable corpora for answer validation in new cross lingual entity profiling task.
</nextsent>
<nextsent>3.1 task definition.
</nextsent>
<nextsent>the goal of the kbp slot filling task is to extract facts from large source corpus regarding certain attributes (slots?)
</nextsent>
<nextsent>of an entity, which may be person or organization, and use these facts to augment an existing knowledge base (kb).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2609">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, all of these approaches made limited efforts at disambiguating entities in queries and limited use of fact extraction in answer search and validation.
</prevsent>
<prevsent>several recent ie studies have stressed the benefits of using information redundancy on estimating the correctness of the ie out put (downey et al, 2005; yangarber, 2006;patwardhan and riloff, 2009; <papid> D09-1016 </papid>ji and grishman, 2008).<papid> P08-1030 </papid></prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
some recent research used comparable corpora to re-score name translitera tions (sproat et al, 2006; <papid> P06-1010 </papid>klementiev and roth, 2006) <papid> N06-1011 </papid>or mine new word translations (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>shao and ng, 2004; tao and zhai, 2005; hassan et al, 2007; udupa etal., 2009; <papid> E09-1091 </papid>ji, 2009).<papid> W09-3107 </papid></citsent>
<aftsection>
<nextsent>to the best of our knowledge, this is the first work on mining facts from comparable corpora for answer validation in new cross lingual entity profiling task.
</nextsent>
<nextsent>3.1 task definition.
</nextsent>
<nextsent>the goal of the kbp slot filling task is to extract facts from large source corpus regarding certain attributes (slots?)
</nextsent>
<nextsent>of an entity, which may be person or organization, and use these facts to augment an existing knowledge base (kb).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2610">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, all of these approaches made limited efforts at disambiguating entities in queries and limited use of fact extraction in answer search and validation.
</prevsent>
<prevsent>several recent ie studies have stressed the benefits of using information redundancy on estimating the correctness of the ie out put (downey et al, 2005; yangarber, 2006;patwardhan and riloff, 2009; <papid> D09-1016 </papid>ji and grishman, 2008).<papid> P08-1030 </papid></prevsent>
</prevsection>
<citsent citstr=" E09-1091 ">
some recent research used comparable corpora to re-score name translitera tions (sproat et al, 2006; <papid> P06-1010 </papid>klementiev and roth, 2006) <papid> N06-1011 </papid>or mine new word translations (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>shao and ng, 2004; tao and zhai, 2005; hassan et al, 2007; udupa etal., 2009; <papid> E09-1091 </papid>ji, 2009).<papid> W09-3107 </papid></citsent>
<aftsection>
<nextsent>to the best of our knowledge, this is the first work on mining facts from comparable corpora for answer validation in new cross lingual entity profiling task.
</nextsent>
<nextsent>3.1 task definition.
</nextsent>
<nextsent>the goal of the kbp slot filling task is to extract facts from large source corpus regarding certain attributes (slots?)
</nextsent>
<nextsent>of an entity, which may be person or organization, and use these facts to augment an existing knowledge base (kb).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2611">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, all of these approaches made limited efforts at disambiguating entities in queries and limited use of fact extraction in answer search and validation.
</prevsent>
<prevsent>several recent ie studies have stressed the benefits of using information redundancy on estimating the correctness of the ie out put (downey et al, 2005; yangarber, 2006;patwardhan and riloff, 2009; <papid> D09-1016 </papid>ji and grishman, 2008).<papid> P08-1030 </papid></prevsent>
</prevsection>
<citsent citstr=" W09-3107 ">
some recent research used comparable corpora to re-score name translitera tions (sproat et al, 2006; <papid> P06-1010 </papid>klementiev and roth, 2006) <papid> N06-1011 </papid>or mine new word translations (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>shao and ng, 2004; tao and zhai, 2005; hassan et al, 2007; udupa etal., 2009; <papid> E09-1091 </papid>ji, 2009).<papid> W09-3107 </papid></citsent>
<aftsection>
<nextsent>to the best of our knowledge, this is the first work on mining facts from comparable corpora for answer validation in new cross lingual entity profiling task.
</nextsent>
<nextsent>3.1 task definition.
</nextsent>
<nextsent>the goal of the kbp slot filling task is to extract facts from large source corpus regarding certain attributes (slots?)
</nextsent>
<nextsent>of an entity, which may be person or organization, and use these facts to augment an existing knowledge base (kb).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2612">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>the ie pipeline includes relation extraction and event extraction based on maximum entropy models that incorporate diverse lexical, syntactic, semantic and onto logical knowledge.
</prevsent>
<prevsent>the extracted ace relations and events are then mapped to kbp slot fills.
</prevsent>
</prevsection>
<citsent citstr=" P09-1113 ">
in pattern matching,we extract and rank patterns based on distant supervision approach (mintz et al, 2009) <papid> P09-1113 </papid>that uses entity-attribute pairs from wikipedia info boxes and free base (bollacker et al, 2008).</citsent>
<aftsection>
<nextsent>we set low threshold to include more answer candidates, and then series of filtering stepsto refine and improve the overall pipeline results.
</nextsent>
<nextsent>the filtering steps include removing answers which have inappropriate entity types orhave inappropriate dependency paths to the entities.
</nextsent>
<nextsent>3.3.3 document and name translation we use statistical, phrase-based mt system (zens and ney, 2004) <papid> N04-1033 </papid>to translate chinese documents into english for type approaches.</nextsent>
<nextsent>the best translation is computed by using aweighted log-linear combination of various statistical models: an n-gram language model, aphrase translation model and word-based lex 112 icon model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2613">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>we set low threshold to include more answer candidates, and then series of filtering stepsto refine and improve the overall pipeline results.
</prevsent>
<prevsent>the filtering steps include removing answers which have inappropriate entity types orhave inappropriate dependency paths to the entities.
</prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
3.3.3 document and name translation we use statistical, phrase-based mt system (zens and ney, 2004) <papid> N04-1033 </papid>to translate chinese documents into english for type approaches.</citsent>
<aftsection>
<nextsent>the best translation is computed by using aweighted log-linear combination of various statistical models: an n-gram language model, aphrase translation model and word-based lex 112 icon model.
</nextsent>
<nextsent>the latter two models are used in source-to-target and target-to-source directions.the model scaling factors are optimized with respect to the bleu score similar to (och, 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>the training data includes 200 million running words in each language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2614">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>3.3.3 document and name translation we use statistical, phrase-based mt system (zens and ney, 2004) <papid> N04-1033 </papid>to translate chinese documents into english for type approaches.</prevsent>
<prevsent>the best translation is computed by using aweighted log-linear combination of various statistical models: an n-gram language model, aphrase translation model and word-based lex 112 icon model.</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the latter two models are used in source-to-target and target-to-source directions.the model scaling factors are optimized with respect to the bleu score similar to (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>the training data includes 200 million running words in each language.
</nextsent>
<nextsent>the total language model training data consists of about 600 million running words.
</nextsent>
<nextsent>we applied various name mining approaches from comparable corpora and parallel corpora, as described in (ji et al, 2009) to extract and translate names in queries and answers in type approaches.
</nextsent>
<nextsent>the accuracy of name translation is about 88%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2615">
<title id=" W11-1215.xml">cross lingual slot filling from comparable corpora </title>
<section> analysis of baseline pipelines.  </section>
<citcontext>
<prevsection>
<prevsent>table 3 shows the distribution of spurious errors.
</prevsent>
<prevsent>pipeline spurious errors distribution content translation + extraction 85% query translation 13% type answer translation 2% word segmentation 34% relation extraction 33% coreference 17% semantic type 13% type slot type 3% table 3: distribution of spurious errors table 3 indicates majority (85%) of spurious errors from type pipelines were due to applying monolingual slot filling methods to mt output which preserves chinese structure.as demonstrated in previous work (e.g.
</prevsent>
</prevsection>
<citsent citstr=" C10-2109 ">
(par ton and mckeown, 2010; <papid> C10-2109 </papid>ji et al, 2009)), we also found that many (14.6%) errors were caused by the low quality of name translation for queries and answers.for example, ?????/mcginty?</citsent>
<aftsection>
<nextsent>was mistakenly translated into the query name kim jong-il?, which led to many incorrect answers such as the british royal joint military research institute?
</nextsent>
<nextsent>for per:employee of ?.
</nextsent>
<nextsent>in contrast, the spurious errors from type bpipelines were more diverse.
</nextsent>
<nextsent>chinese ie components severely suffered from word segmentation errors (34%), which were then directly propagated into chinese document retrieval and slot filling.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2616">
<title id=" W10-4213.xml">complex lexico syntactic reformulation of sentences using typed dependency representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are many reasons why writer might want to choose one formulation of discourse relation over another; for example, maintaining thread of discourse, avoiding shifts in focus and issues of salience and end weight.
</prevsent>
<prevsent>there are also reasons to use different formulations for different audiences; for example, to account for differences in reading skills and domain knowledge.
</prevsent>
</prevsection>
<citsent citstr=" N10-1144 ">
in recent work, siddharthan and katsos (2010) <papid> N10-1144 </papid>demonstrated through psycho linguistic experiments that domain experts and lay readers show significant differences inwhich formulations of causation they find acceptable.</citsent>
<aftsection>
<nextsent>they further showed that the most appropriate formulation depends both on the domain expertise of the user and the propositional content of the sentence, and that these preferences canbe learnt in supervised machine learning framework.
</nextsent>
<nextsent>that work, as does much of the related comprehension and literacy literature, used manually reformulated sentences.
</nextsent>
<nextsent>in this paper, we present an approach to automate such complex reformulation.
</nextsent>
<nextsent>we consider the four lexico-syntacticdiscourse markers for causation studied by siddharthan and katsos (2010); <papid> N10-1144 </papid>consider 1a.d. below (from their corpus, but simplified to aid pre sentation): (1) a. an incendiary device caused the explosion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2621">
<title id=" W10-4213.xml">complex lexico syntactic reformulation of sentences using typed dependency representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the pset project focused mainly on lexical simplification (replacing difficult words with easier ones), but there has been work on syntactic simplification and, in particular, the way syntactic rewrites interact with discourse structure and text cohesion (siddharthan, 2006).
</prevsent>
<prevsent>these were restricted to string substitution and sentence splitting based on pattern matching over chunked text.
</prevsent>
</prevsection>
<citsent citstr=" P02-1028 ">
our work aims to extend these strands of research by allowing for more sophisticated insertion, deletion and substitution operations that can involve substantial reorganisation and modification of content within sentence.elsewhere, there has been interest in paraphrasing, including the replacement of words (espe cially verbs) with their dictionary definitions (kaji et al, 2002) <papid> P02-1028 </papid>and the replacement of idiomatic or otherwise troublesome expressions with simpler ones.</citsent>
<aftsection>
<nextsent>the emphasis has been on automatically learning paraphrases from comparable or aligned corpora (barzilay and lee, 2003; <papid> N03-1003 </papid>ibrahim et al, 2003).</nextsent>
<nextsent>the text simplification and paraphrasing literature does not address paraphrasing that requires syntactic alterations such as those in example 1 or the question of appropriateness of different formulations of discourse relation.some natural language generation systems incorporate results from psycho linguistic studies tomake principled choices between alternative for mulations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2622">
<title id=" W10-4213.xml">complex lexico syntactic reformulation of sentences using typed dependency representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>these were restricted to string substitution and sentence splitting based on pattern matching over chunked text.
</prevsent>
<prevsent>our work aims to extend these strands of research by allowing for more sophisticated insertion, deletion and substitution operations that can involve substantial reorganisation and modification of content within sentence.elsewhere, there has been interest in paraphrasing, including the replacement of words (espe cially verbs) with their dictionary definitions (kaji et al, 2002) <papid> P02-1028 </papid>and the replacement of idiomatic or otherwise troublesome expressions with simpler ones.</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
the emphasis has been on automatically learning paraphrases from comparable or aligned corpora (barzilay and lee, 2003; <papid> N03-1003 </papid>ibrahim et al, 2003).</citsent>
<aftsection>
<nextsent>the text simplification and paraphrasing literature does not address paraphrasing that requires syntactic alterations such as those in example 1 or the question of appropriateness of different formulations of discourse relation.some natural language generation systems incorporate results from psycho linguistic studies tomake principled choices between alternative formulations.
</nextsent>
<nextsent>for example, skill sum (williams and reiter, 2008) and iconoclast (power et al, 2003) are two contemporary generation systems that allow for specifying aspects of style such as choice of discourse marker, clause order, repetition and sentence and paragraph lengths in theform of constraints that can be optimised.
</nextsent>
<nextsent>however, to date, these systems do not consider syntactic reformulations of the type we are interestedin.
</nextsent>
<nextsent>our research is directly relevant to such generation systems as it can help such systems make decisions in principled manner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2623">
<title id=" W10-4213.xml">complex lexico syntactic reformulation of sentences using typed dependency representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there are similarities between our interest in reformulation and existing work in sentence compression.
</prevsent>
<prevsent>sentence compression has usually been addressed in generative framework, where transformation rules are learnt from parsed corpora of sentences aligned with manually compressed versions.
</prevsent>
</prevsection>
<citsent citstr=" N07-1023 ">
the compression rules learnt are therefore tree-tree transformations (knight and marcu, 2000; galley and mckeown, 2007; <papid> N07-1023 </papid>riezler et al, 2003) <papid> N03-1026 </papid>of some variety.</citsent>
<aftsection>
<nextsent>these approaches focus on deletion operations, mostly performed low down in the parse tree to remove modifiers.
</nextsent>
<nextsent>further they make assumptions about isomorphism between the aligned tree, which means they cannot be readily applied to more complex reformulation operations such as insertion and reordering that are essential to perform reformulations such as those in example 1.
</nextsent>
<nextsent>cohn and lapata (2009) provide an approach.
</nextsent>
<nextsent>based on synchronous tree substitution grammar (stsg) that in principle can handle the range of reformulation operations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2624">
<title id=" W10-4213.xml">complex lexico syntactic reformulation of sentences using typed dependency representations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there are similarities between our interest in reformulation and existing work in sentence compression.
</prevsent>
<prevsent>sentence compression has usually been addressed in generative framework, where transformation rules are learnt from parsed corpora of sentences aligned with manually compressed versions.
</prevsent>
</prevsection>
<citsent citstr=" N03-1026 ">
the compression rules learnt are therefore tree-tree transformations (knight and marcu, 2000; galley and mckeown, 2007; <papid> N07-1023 </papid>riezler et al, 2003) <papid> N03-1026 </papid>of some variety.</citsent>
<aftsection>
<nextsent>these approaches focus on deletion operations, mostly performed low down in the parse tree to remove modifiers.
</nextsent>
<nextsent>further they make assumptions about isomorphism between the aligned tree, which means they cannot be readily applied to more complex reformulation operations such as insertion and reordering that are essential to perform reformulations such as those in example 1.
</nextsent>
<nextsent>cohn and lapata (2009) provide an approach.
</nextsent>
<nextsent>based on synchronous tree substitution grammar (stsg) that in principle can handle the range of reformulation operations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2627">
<title id=" W10-4213.xml">complex lexico syntactic reformulation of sentences using typed dependency representations </title>
<section> regeneration using transfer rules.  </section>
<citcontext>
<prevsection>
<prevsent>as described above, we have access to corpus that contains aligned sentences for each pair of types (a type is combination of discourse marker and an information order; thus we have 8types).
</prevsent>
<prevsent>in principle it should be easy to learn transfer rules between parse trees of aligned sentences.
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
figure 1 shows parse trees ( using the rasp parser(briscoe et al, 2006)) <papid> P06-4020 </papid>for the active and the passive voice with cause?</citsent>
<aftsection>
<nextsent>as verb.
</nextsent>
<nextsent>a transfer rule is derived by aligning nodes between two parse trees so that the rule only contains the difference sin structure between the trees.
</nextsent>
<nextsent>in the representation in figure 1, the variable x0[np] maps 1pubmed url: http://www.ncbi.nlm.nih.gov/pubmed/2the british national corpus, version 3 (bnc xml edi tion).
</nextsent>
<nextsent>2007.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2630">
<title id=" W11-1218.xml">building a web based parallel corpus and filtering out machine translated text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we address the tasks of verification of parallel documents, extraction of the best parallel fragments and filtering out automatically translated texts.
</prevsent>
<prevsent>mining parallel texts from big document collection usually involves three phases: ? detecting set of potential parallel document pairs with fast but low-precision algorithms ? pairwise verification procedure ? further filtering of unwanted texts, e.g. automatically translated texts finding potential parallel texts in collection of web documents is challenging task that does not yet have universal solution.
</prevsent>
</prevsection>
<citsent citstr=" L08-1060 ">
there exist methods based on the analysis of meta-information (ma and liberman, 1999; resnik, 2003; mohler and mihalcea, 2008, <papid> L08-1060 </papid>nadeau and foster 2004), such as url similarity, html markup, publication date and time.</citsent>
<aftsection>
<nextsent>more complicated methods are aimed at 136 proceedings of the 4th workshop on building and using comparable corpora, pages 136144, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.
</nextsent>
<nextsent>c2011 association for computational linguistics detecting potential parallel texts by their content.
</nextsent>
<nextsent>in this case mining of parallel documents in the internet can be regarded as the task of near duplicate detection (uszkoreit et al, 2010).<papid> C10-1124 </papid></nextsent>
<nextsent>all of the abovementioned approaches are useful as each of them is able to provide some document pairs that are not found by other methods.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2631">
<title id=" W11-1218.xml">building a web based parallel corpus and filtering out machine translated text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more complicated methods are aimed at 136 proceedings of the 4th workshop on building and using comparable corpora, pages 136144, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.
</prevsent>
<prevsent>c2011 association for computational linguistics detecting potential parallel texts by their content.
</prevsent>
</prevsection>
<citsent citstr=" C10-1124 ">
in this case mining of parallel documents in the internet can be regarded as the task of near duplicate detection (uszkoreit et al, 2010).<papid> C10-1124 </papid></citsent>
<aftsection>
<nextsent>all of the abovementioned approaches are useful as each of them is able to provide some document pairs that are not found by other methods.
</nextsent>
<nextsent>in our experiments, fast algorithms of the first phase classify every pair of documents as parallel with very low precision, from 20% to 0.001%.
</nextsent>
<nextsent>that results in huge set of candidate pairs of documents, for which we must decide if they are actually parallel or not.
</nextsent>
<nextsent>for example, if we need to get 100 000 really parallel documents we should check from 500 thousand to 100 million pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2633">
<title id=" W11-1218.xml">building a web based parallel corpus and filtering out machine translated text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, if we need to get 100 000 really parallel documents we should check from 500 thousand to 100 million pairs.
</prevsent>
<prevsent>the large number of pairwise comparisons to be made implies that the verification procedure must be fast and scalable.
</prevsent>
</prevsection>
<citsent citstr=" P91-1022 ">
our approach is based on sentence alignment algorithm similar to (brown et al, 1991; <papid> P91-1022 </papid>gale and church, 1993; <papid> J93-1004 </papid>chen, 1993; <papid> P93-1002 </papid>moore 2002; ma, 2006) but it is mainly aimed at achieving high precision rather than high recall.</citsent>
<aftsection>
<nextsent>the algorithm is able to extract parallel fragments from comparable documents, as web documents often are not exactly parallel.
</nextsent>
<nextsent>the similarity estimate relies on probabilistic dictionary trained on initial parallel corpus and may improve when the corpus grows.
</nextsent>
<nextsent>due to growing popularity of machine translation systems, russian web sites are being increasingly filled with texts that are translated automatically.
</nextsent>
<nextsent>according to selective manual annotation the share of machine translation among the texts that have passed the verification procedure is 25-35%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2634">
<title id=" W11-1218.xml">building a web based parallel corpus and filtering out machine translated text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, if we need to get 100 000 really parallel documents we should check from 500 thousand to 100 million pairs.
</prevsent>
<prevsent>the large number of pairwise comparisons to be made implies that the verification procedure must be fast and scalable.
</prevsent>
</prevsection>
<citsent citstr=" J93-1004 ">
our approach is based on sentence alignment algorithm similar to (brown et al, 1991; <papid> P91-1022 </papid>gale and church, 1993; <papid> J93-1004 </papid>chen, 1993; <papid> P93-1002 </papid>moore 2002; ma, 2006) but it is mainly aimed at achieving high precision rather than high recall.</citsent>
<aftsection>
<nextsent>the algorithm is able to extract parallel fragments from comparable documents, as web documents often are not exactly parallel.
</nextsent>
<nextsent>the similarity estimate relies on probabilistic dictionary trained on initial parallel corpus and may improve when the corpus grows.
</nextsent>
<nextsent>due to growing popularity of machine translation systems, russian web sites are being increasingly filled with texts that are translated automatically.
</nextsent>
<nextsent>according to selective manual annotation the share of machine translation among the texts that have passed the verification procedure is 25-35%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2635">
<title id=" W11-1218.xml">building a web based parallel corpus and filtering out machine translated text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, if we need to get 100 000 really parallel documents we should check from 500 thousand to 100 million pairs.
</prevsent>
<prevsent>the large number of pairwise comparisons to be made implies that the verification procedure must be fast and scalable.
</prevsent>
</prevsection>
<citsent citstr=" P93-1002 ">
our approach is based on sentence alignment algorithm similar to (brown et al, 1991; <papid> P91-1022 </papid>gale and church, 1993; <papid> J93-1004 </papid>chen, 1993; <papid> P93-1002 </papid>moore 2002; ma, 2006) but it is mainly aimed at achieving high precision rather than high recall.</citsent>
<aftsection>
<nextsent>the algorithm is able to extract parallel fragments from comparable documents, as web documents often are not exactly parallel.
</nextsent>
<nextsent>the similarity estimate relies on probabilistic dictionary trained on initial parallel corpus and may improve when the corpus grows.
</nextsent>
<nextsent>due to growing popularity of machine translation systems, russian web sites are being increasingly filled with texts that are translated automatically.
</nextsent>
<nextsent>according to selective manual annotation the share of machine translation among the texts that have passed the verification procedure is 25-35%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2636">
<title id=" W11-1218.xml">building a web based parallel corpus and filtering out machine translated text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this method allows reducing the number of automatically translated texts to 5% in the final corpus.
</prevsent>
<prevsent>our final goal is to build quality corpus of parallel sentences appropriate for training statistical machine translation system.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
we evaluate the 1-million-sentence part of our corpus by training phrase-based translation system (koehn et al, 2007) <papid> P07-2045 </papid>on these sentences and compare the results with the results of training on noisy data, containing automatically translated texts as its part.</citsent>
<aftsection>
<nextsent>the rest of the paper is organized as follows: section 2 provides an overview of the system architecture and addresses specific problems at the preparatory stage.
</nextsent>
<nextsent>section 3 describes the sentence-alignment algorithm and the pairwise verification procedure.
</nextsent>
<nextsent>the algorithm makes use of statistical dictionaries trained beforehand.
</nextsent>
<nextsent>in section 4 we discuss the problem of filtering out automatically translated texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2640">
<title id=" W11-1218.xml">building a web based parallel corpus and filtering out machine translated text </title>
<section> filtering out machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>prior to translating each sentence, special language model is built consisting of n-grams from the reference sentence.
</prevsent>
<prevsent>that model serves as sort of soft constraint on the result of translation.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the decoder output is scored against reference translation with the bleu metric (papineni et al, 2002) - <papid> P02-1040 </papid>we shall call it r-bleu for the rest of this section.</citsent>
<aftsection>
<nextsent>the idea is that the higher is r-bleu, the more likely the reference is statistical translation itself.
</nextsent>
<nextsent>the program was implemented based on the decoder of the statistical phrase-based translation system.
</nextsent>
<nextsent>the phrase table and the factor weights were not modified.
</nextsent>
<nextsent>phrase reordering was not allowed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2642">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>raw word streams, such as utterances transcribed by speech recognizers, are often difficult even for humans (kim and woodland,2002).
</prevsent>
<prevsent>therefore, one would expect grammar induc ers to exploit any available linguistic meta-data.
</prevsent>
</prevsection>
<citsent citstr=" P04-1061 ">
andyet in unsupervised dependency parsing, sentence internal punctuation has long been ignored (carrolland charniak, 1992; paskin, 2001; klein and manning, 2004; <papid> P04-1061 </papid>blunsom and cohn, 2010, <papid> D10-1117 </papid>inter alia).html is another kind of meta-data that is ordinarily stripped out in pre-processing.</citsent>
<aftsection>
<nextsent>however, recently spitkovsky et al  (2010<papid> W10-2902 </papid>b) demonstrated that web markup can successfully guide hierarchical syntactic structure discovery, observing, for example, that anchors often match linguistic constituents: ..., whereas mccain is secure on the topic, obama  [vp worries about winning the pro-israel vote] /a .</nextsent>
<nextsent>we propose exploring punctuations potential toaid grammar induction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2643">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>raw word streams, such as utterances transcribed by speech recognizers, are often difficult even for humans (kim and woodland,2002).
</prevsent>
<prevsent>therefore, one would expect grammar induc ers to exploit any available linguistic meta-data.
</prevsent>
</prevsection>
<citsent citstr=" D10-1117 ">
andyet in unsupervised dependency parsing, sentence internal punctuation has long been ignored (carrolland charniak, 1992; paskin, 2001; klein and manning, 2004; <papid> P04-1061 </papid>blunsom and cohn, 2010, <papid> D10-1117 </papid>inter alia).html is another kind of meta-data that is ordinarily stripped out in pre-processing.</citsent>
<aftsection>
<nextsent>however, recently spitkovsky et al  (2010<papid> W10-2902 </papid>b) demonstrated that web markup can successfully guide hierarchical syntactic structure discovery, observing, for example, that anchors often match linguistic constituents: ..., whereas mccain is secure on the topic, obama  [vp worries about winning the pro-israel vote] /a .</nextsent>
<nextsent>we propose exploring punctuations potential toaid grammar induction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2644">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, one would expect grammar induc ers to exploit any available linguistic meta-data.
</prevsent>
<prevsent>andyet in unsupervised dependency parsing, sentence internal punctuation has long been ignored (carrolland charniak, 1992; paskin, 2001; klein and manning, 2004; <papid> P04-1061 </papid>blunsom and cohn, 2010, <papid> D10-1117 </papid>inter alia).html is another kind of meta-data that is ordinarily stripped out in pre-processing.</prevsent>
</prevsection>
<citsent citstr=" W10-2902 ">
however, recently spitkovsky et al  (2010<papid> W10-2902 </papid>b) demonstrated that web markup can successfully guide hierarchical syntactic structure discovery, observing, for example, that anchors often match linguistic constituents: ..., whereas mccain is secure on the topic, obama  [vp worries about winning the pro-israel vote] /a .</citsent>
<aftsection>
<nextsent>we propose exploring punctuations potential toaid grammar induction.
</nextsent>
<nextsent>consider motivating example (all of our examples are from wsj), in which all (six) marks align with constituent boundaries: [sbar although it probably has reduced the level of expenditures for some purchasers], [np utilization management] ? [pp like most other cost containment strate gies] ? [vp doesnt appear to have altered the long-term rate of increase in health-care costs], [np the institute of medicine], [np an affiliate of the national academy of sciences], [vp concluded after two-year study].
</nextsent>
<nextsent>this link between punctuation and constituent boundaries suggests that we could approximate parsing by treating inter-punctuation fragments independently.
</nextsent>
<nextsent>in training, our algorithm first parses each fragment separately, then parses the sequence of the resulting headwords.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2664">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> definitions, analyses and constraints.  </section>
<citcontext>
<prevsection>
<prevsent>19 count pos sequence frac cum 1 3,492 nnp 2.8% 3 2,519 nnp nnp 2.0 7.1 4 2,512 rb 2.0 9.1 5 1,495 cd 1.2 10.3 6 1,025 nn 0.8 11.1 7 1,023 nnp nnp nnp 0.8 11.9 8 916 in nn 0.7 12.7 9 795 vbz nnp nnp 0.6 13.3 10 748 cc 0.6 13.9 11 730 cd dt nn 0.6 14.5 12 705 prp vbd 0.6 15.1 13 652 jj nn 0.5 15.6 14 648 dt nn 0.5 16.1 15 627 in dt nn 0.5 16.6 wsj +103,148 more with count ? 621 83.4% table 1: top 15 fragments of pos tag sequences in wsj.
</prevsent>
<prevsent>count non-terminal frac cum 1 40,223 32.5% 3 16,413 vp 13.3 72.9 4 12,441 pp 10.1 83.0 5 8,350 sbar 6.7 89.7 6 4,085 advp 3.3 93.0 7 3,080 qp 2.5 95.5 8 2,480 sinv 2.0 97.5 9 1,257 adjp 1.0 98.5 10 369 prn 0.3 98.8 wsj +1,446 more with count ? 356 1.2% table 2: top 99% of the lowest dominating non-terminals deriving complete inter-punctuation fragments in wsj.
</prevsent>
</prevsection>
<citsent citstr=" C94-1069 ">
punctuation and syntax are related (nunberg, 1990; briscoe, 1994; jones, 1994; <papid> C94-1069 </papid>doran, 1998, inter alia).</citsent>
<aftsection>
<nextsent>but are there simple enough connections between the two to aid in grammar induction?
</nextsent>
<nextsent>this section explores the regularities.
</nextsent>
<nextsent>our study of punctuation in wsj (marcus et al , 1993) parallels spitkovsky et al (2010<papid> W10-2902 </papid>b), parallels spitkovsky et al (5) analysis of markup from web log, since their proposed constraints turn out to be useful.</nextsent>
<nextsent>throughout, we define an inter-punctuationfragment as maximal (non-empty) consecutive sequence of words that does not cross punctuation boundaries and is shorter than its source sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2689">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> definitions, analyses and constraints.  </section>
<citcontext>
<prevsection>
<prevsent>ordinarily, the probability of [i, j]his computed by multiplying the probability of the associated unsealed span by two stopping probabilities ? that of the word at on the left (adjacent if = h; non-adjacent if   h) and on the right (adjacent ifh = j; non-adjacent if   j).
</prevsent>
<prevsent>to impose constraint, we ran through all of the annotations [x, y]associated with sentence and zeroed out this probability if any of them satisfied disallowed conditions.
</prevsent>
</prevsection>
<citsent citstr=" P99-1059 ">
there are faster ? e.g., o(l4), and even o(l3) recognizers for split head automaton grammars (eis ner and satta, 1999).<papid> P99-1059 </papid></citsent>
<aftsection>
<nextsent>perhaps more practical, but still clear, approach would be to generate n-best lists using more efficient unconstrained algorithm, then apply the constraints as post-filtering step.
</nextsent>
<nextsent>relaxed constraints disallow joining adjacent subtrees, e.g., preventing the seal [i, j]h from merging below the unsealed span [j +1, ]h , on the left: h j + 1 j 21 tear ? prevents . . .
</nextsent>
<nextsent>y from being torn apart by external heads from opposite sides.
</nextsent>
<nextsent>it holds for94.7% of fragments (97.9% of markup), and is violated when (x ? ?   ?   x), in this case.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2797">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> the model, methods and metrics.  </section>
<citcontext>
<prevsection>
<prevsent>our primary baseline is the basic system without constraints (standard training).
</prevsent>
<prevsent>it ignores punctuation, as is standard, scoring 52.0% against wsj45.a secondary (punctuation as words) baseline in 3note that wsj{15, 45} overlap with section 23 ? training on the test set is standard practice in unsupervised learning.
</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
corporates punctuation into the grammar as if it were words, as in supervised dependency parsing (nivre et al , 2007<papid> D07-1096 </papid>b; lin, 1998; sleator and temperley, 1993, inter alia).</citsent>
<aftsection>
<nextsent>it is worse, scoring only 41.0%.4,5
</nextsent>
<nextsent>our first experiment compares punctuation as constraints?
</nextsent>
<nextsent>to the baseline systems.
</nextsent>
<nextsent>we use default settings, as recommended by spitkovsky et al  (2010<papid> W10-2902 </papid>b):loose in training; and sprawl in inference.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2869">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> more advanced methods.  </section>
<citcontext>
<prevsection>
<prevsent>but would it help modern system?
</prevsent>
<prevsent>our next two experiments employ slightly more complicated set-up, compared with the one used upuntil now (3.1).
</prevsent>
</prevsection>
<citsent citstr=" N09-1012 ">
the key difference is that this system is lexicalized, as is standard among the more accurate grammar inducers (blunsom and cohn, 2010; <papid> D10-1117 </papid>gillenwater et al , 2010; headden et al , 2009).<papid> N09-1012 </papid></citsent>
<aftsection>
<nextsent>lexicalization we lexicalize only in the second (full data) stage, using the method of headden et al  (2009).<papid> N09-1012 </papid></nextsent>
<nextsent>for words seen at least 100 times in the training corpus, we augment their gold pos tag with the lexical item.the first (data poor) stage remains entirely unlexi calized, with gold pos tags for word classes, as in the earlier systems (klein and manning, 2004).<papid> P04-1061 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2874">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> experiment #3: state-of-the-art.  </section>
<citcontext>
<prevsection>
<prevsent>the purpose of these experiments is to compare thepunctuation-enhanced dmv with other, recent state of-the-art systems.
</prevsent>
<prevsent>we find that, lexicalized (6), our approach performs better, by wide margin; without lexicalization (3.1), it was already better for longer, but not for shorter, sentences (see tables 5 and 4).
</prevsent>
</prevsection>
<citsent citstr=" W00-0717 ">
we trained variant of our system without gold part-of-speech tags, using the unsupervised word clusters (clark, 2000) <papid> W00-0717 </papid>computed by finkel and manning (2009).<papid> N09-1037 </papid>6 accuracy decreased slightly, to 58.2% on section 23 of wsj (down only 0.2%).</citsent>
<aftsection>
<nextsent>this result improves over substantial performance degradations previously observed for unsupervised dependency parsing with induced word categories (klein and manning, 2004; <papid> P04-1061 </papid>headden et al , 2008, <papid> C08-1042 </papid>inter alia).</nextsent>
<nextsent>6available from http://nlp.stanford.edu/software/ stanford-postagger-2008-09-28.tar.gz: models/egw.bnc.200 brown wsj?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2875">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> experiment #3: state-of-the-art.  </section>
<citcontext>
<prevsection>
<prevsent>the purpose of these experiments is to compare thepunctuation-enhanced dmv with other, recent state of-the-art systems.
</prevsent>
<prevsent>we find that, lexicalized (6), our approach performs better, by wide margin; without lexicalization (3.1), it was already better for longer, but not for shorter, sentences (see tables 5 and 4).
</prevsent>
</prevsection>
<citsent citstr=" N09-1037 ">
we trained variant of our system without gold part-of-speech tags, using the unsupervised word clusters (clark, 2000) <papid> W00-0717 </papid>computed by finkel and manning (2009).<papid> N09-1037 </papid>6 accuracy decreased slightly, to 58.2% on section 23 of wsj (down only 0.2%).</citsent>
<aftsection>
<nextsent>this result improves over substantial performance degradations previously observed for unsupervised dependency parsing with induced word categories (klein and manning, 2004; <papid> P04-1061 </papid>headden et al , 2008, <papid> C08-1042 </papid>inter alia).</nextsent>
<nextsent>6available from http://nlp.stanford.edu/software/ stanford-postagger-2008-09-28.tar.gz: models/egw.bnc.200 brown wsj?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2877">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> experiment #3: state-of-the-art.  </section>
<citcontext>
<prevsection>
<prevsent>we find that, lexicalized (6), our approach performs better, by wide margin; without lexicalization (3.1), it was already better for longer, but not for shorter, sentences (see tables 5 and 4).
</prevsent>
<prevsent>we trained variant of our system without gold part-of-speech tags, using the unsupervised word clusters (clark, 2000) <papid> W00-0717 </papid>computed by finkel and manning (2009).<papid> N09-1037 </papid>6 accuracy decreased slightly, to 58.2% on section 23 of wsj (down only 0.2%).</prevsent>
</prevsection>
<citsent citstr=" C08-1042 ">
this result improves over substantial performance degradations previously observed for unsupervised dependency parsing with induced word categories (klein and manning, 2004; <papid> P04-1061 </papid>headden et al , 2008, <papid> C08-1042 </papid>inter alia).</citsent>
<aftsection>
<nextsent>6available from http://nlp.stanford.edu/software/ stanford-postagger-2008-09-28.tar.gz: models/egw.bnc.200 brown wsj?
</nextsent>
<nextsent>wsj10 (headden et al , 2009) ? ?<papid> N09-1012 </papid></nextsent>
<nextsent>68.8 (spitkovsky et al , 2010<papid> W10-2902 </papid>b) 53.3 50.4 69.3 (gillenwater et al , 2010) ? 53.3 64.3 (blunsom and cohn, 2010) <papid> D10-1117 </papid>55.7 67.7 constrained training 58.4 58.0 69.3 w/constrained inference 59.5 58.4 69.5 table 5: accuracies on the out-of-domain brown100 set and section 23 of wsj?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2883">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> experiment #4: multi-lingual testing.  </section>
<citcontext>
<prevsection>
<prevsent>w/inference gain arabic 2006 23.3 23.6 (+0.3) 32.8 33.1 (+0.4) 31.5 31.6 (+0.1) 32.1 32.6 (+0.5) +1.1 7 25.6 26.4 (+0.8) 33.7 34.2 (+0.5) 32.7 33.6 (+0.9) 34.9 35.3 (+0.4) +2.6 basque 7 19.3 20.8 (+1.5) 29.9 30.9 (+1.0) 29.3 30.1 (+0.8) 29.3 29.9 (+0.6) +0.6 bulgarian 6 23.7 24.7 (+1.0) 39.3 40.7 (+1.4) 38.8 39.9 (+1.1) 39.9 40.5 (+0.6) +1.6 catalan 7 33.2 34.1 (+0.8) 54.8 55.5 (+0.7) 54.3 55.1 (+0.8) 54.3 55.2 (+0.9) +0.9 czech 6 18.6 19.6 (+1.0) 34.6 35.8 (+1.2) 34.8 35.7 (+0.9) 37.0 37.8 (+0.8) +3.0 7 17.6 18.4 (+0.8) 33.5 35.4 (+1.9) 33.4 34.4 (+1.0) 35.2 36.2 (+1.0) +2.7 danish 6 22.9 24.0 (+1.1) 35.6 36.7 (+1.2) 36.9 37.8 (+0.9) 36.5 37.1 (+0.6) +0.2 dutch 6 15.8 16.5 (+0.7) 11.2 12.5 (+1.3) 11.0 11.9 (+1.0) 13.7 14.0 (+0.3) +3.0 english 7 25.0 25.4 (+0.5) 47.2 49.5 (+2.3) 47.5 48.8 (+1.3) 49.3 50.3 (+0.9) +2.8 german 6 19.2 19.6 (+0.4) 27.4 28.0 (+0.7) 27.0 27.8 (+0.8) 28.2 28.6 (+0.4) +1.6 greek 7 18.5 18.8 (+0.3) 20.7 21.4 (+0.7) 20.5 21.0 (+0.5) 20.9 21.2 (+0.3) +0.7 hungarian 7 17.4 17.7 (+0.3) 6.7 7.2 (+0.5) 6.6 7.0 (+0.4) 7.8 8.0 (+0.2) +1.4 italian 7 25.0 26.3 (+1.2) 29.6 29.9 (+0.3) 29.7 29.7 (+0.1) 28.3 28.8 (+0.5) -0.8 japanese 6 30.0 30.0 (+0.0) 27.3 27.3 (+0.0) 27.4 27.4 (+0.0) 27.5 27.5 (+0.0) +0.1 portuguese 6 27.3 27.5 (+0.2) 32.8 33.7 (+0.9) 32.7 33.4 (+0.7) 33.3 33.5 (+0.3) +0.8 slovenian 6 21.8 21.9 (+0.2) 28.3 30.4 (+2.1) 28.4 30.4 (+2.0) 29.8 31.2 (+1.4) +2.8 spanish 6 25.3 26.2 (+0.9) 31.7 32.4 (+0.7) 31.6 32.3 (+0.8) 31.9 32.3 (+0.5) +0.8 swedish 6 31.0 31.5 (+0.6) 44.1 45.2 (+1.1) 45.6 46.1 (+0.5) 46.1 46.4 (+0.3) +0.8 turkish 6 22.3 22.9 (+0.6) 39.1 39.5 (+0.4) 39.9 39.9 (+0.1) 40.6 40.9 (+0.3) +1.0 7 22.7 23.3 (+0.6) 41.7 42.3 (+0.6) 41.9 42.1 (+0.2) 41.6 42.0 (+0.4) +0.1 average: 23.4 24.0 (+0.7) 31.9 32.9 (+1.0) 31.9 32.6 (+0.7) 32.6 33.2 (+0.5) +1.3 table 6: multi-lingual evaluation for conll sets, measured at all three stages of training, with and without constraints.
</prevsent>
<prevsent>this final batch of experiments probes the generalization of our approach (6) across languages.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
thedata are from 2006/7 conll shared tasks (buch holz and marsi, 2006; <papid> W06-2920 </papid>nivre et al , 2007<papid> D07-1096 </papid>a), where punctuation was identified by the organizers, who also furnished disjoint train/test splits.</citsent>
<aftsection>
<nextsent>we tested against all sentences in their evaluation sets.7,8 the gains are not english-specific (see table 6).every language improves with constrained decoding (more so without constrained training); and all but italian benefit in combination.
</nextsent>
<nextsent>averaged across all eighteen languages, the net change inaccuracy is 1.3%.
</nextsent>
<nextsent>after standard training, constrained decoding alone delivers 0.7% gain, on average, never causing harm in any of our experiments.
</nextsent>
<nextsent>these gains are statistically significant: ? 1.59 ? 105 for constrained training; and ? 4.27107 for inference.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2887">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, after constrained training, we detected no evidence of benefits to additional retraining: not with the relaxed sprawl constraint, nor unconstrained.
</prevsent>
<prevsent>punctuation has been used to improve parsing since rule-based systems (jones, 1994).<papid> C94-1069 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1007 ">
statistical parsers reap dramatic gains from punctuation (engel et al , 2002; <papid> W02-1007 </papid>roark, 2001; charniak, 2000; <papid> A00-2018 </papid>johnson, 1998; <papid> J98-4004 </papid>collins, 1997, <papid> P97-1003 </papid>inter alia).</citsent>
<aftsection>
<nextsent>and it is even known to help in unsupervised constituent parsing (seginer, 2007).<papid> P07-1049 </papid></nextsent>
<nextsent>but for dependency grammar induction, until now, punctuation remained unexploited.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2888">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, after constrained training, we detected no evidence of benefits to additional retraining: not with the relaxed sprawl constraint, nor unconstrained.
</prevsent>
<prevsent>punctuation has been used to improve parsing since rule-based systems (jones, 1994).<papid> C94-1069 </papid></prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
statistical parsers reap dramatic gains from punctuation (engel et al , 2002; <papid> W02-1007 </papid>roark, 2001; charniak, 2000; <papid> A00-2018 </papid>johnson, 1998; <papid> J98-4004 </papid>collins, 1997, <papid> P97-1003 </papid>inter alia).</citsent>
<aftsection>
<nextsent>and it is even known to help in unsupervised constituent parsing (seginer, 2007).<papid> P07-1049 </papid></nextsent>
<nextsent>but for dependency grammar induction, until now, punctuation remained unexploited.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2889">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, after constrained training, we detected no evidence of benefits to additional retraining: not with the relaxed sprawl constraint, nor unconstrained.
</prevsent>
<prevsent>punctuation has been used to improve parsing since rule-based systems (jones, 1994).<papid> C94-1069 </papid></prevsent>
</prevsection>
<citsent citstr=" J98-4004 ">
statistical parsers reap dramatic gains from punctuation (engel et al , 2002; <papid> W02-1007 </papid>roark, 2001; charniak, 2000; <papid> A00-2018 </papid>johnson, 1998; <papid> J98-4004 </papid>collins, 1997, <papid> P97-1003 </papid>inter alia).</citsent>
<aftsection>
<nextsent>and it is even known to help in unsupervised constituent parsing (seginer, 2007).<papid> P07-1049 </papid></nextsent>
<nextsent>but for dependency grammar induction, until now, punctuation remained unexploited.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2890">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, after constrained training, we detected no evidence of benefits to additional retraining: not with the relaxed sprawl constraint, nor unconstrained.
</prevsent>
<prevsent>punctuation has been used to improve parsing since rule-based systems (jones, 1994).<papid> C94-1069 </papid></prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
statistical parsers reap dramatic gains from punctuation (engel et al , 2002; <papid> W02-1007 </papid>roark, 2001; charniak, 2000; <papid> A00-2018 </papid>johnson, 1998; <papid> J98-4004 </papid>collins, 1997, <papid> P97-1003 </papid>inter alia).</citsent>
<aftsection>
<nextsent>and it is even known to help in unsupervised constituent parsing (seginer, 2007).<papid> P07-1049 </papid></nextsent>
<nextsent>but for dependency grammar induction, until now, punctuation remained unexploited.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2891">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>punctuation has been used to improve parsing since rule-based systems (jones, 1994).<papid> C94-1069 </papid></prevsent>
<prevsent>statistical parsers reap dramatic gains from punctuation (engel et al , 2002; <papid> W02-1007 </papid>roark, 2001; charniak, 2000; <papid> A00-2018 </papid>johnson, 1998; <papid> J98-4004 </papid>collins, 1997, <papid> P97-1003 </papid>inter alia).</prevsent>
</prevsection>
<citsent citstr=" P07-1049 ">
and it is even known to help in unsupervised constituent parsing (seginer, 2007).<papid> P07-1049 </papid></citsent>
<aftsection>
<nextsent>but for dependency grammar induction, until now, punctuation remained unexploited.
</nextsent>
<nextsent>parsing techniques most-similar to constrain tsa divide-and-rule?
</nextsent>
<nextsent>strategy that relies on punctuation has been used in supervised constituent parsing of long chinese sentences (li et al , 2005).<papid> I05-2002 </papid></nextsent>
<nextsent>for english, there has been interest in balanced punctuation (briscoe, 1994), more recently using rule-based filters (white and rajkumar, 2008) <papid> W08-1703 </papid>in combinatory categorial grammar (ccg).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2892">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>but for dependency grammar induction, until now, punctuation remained unexploited.
</prevsent>
<prevsent>parsing techniques most-similar to constrain tsa divide-and-rule?
</prevsent>
</prevsection>
<citsent citstr=" I05-2002 ">
strategy that relies on punctuation has been used in supervised constituent parsing of long chinese sentences (li et al , 2005).<papid> I05-2002 </papid></citsent>
<aftsection>
<nextsent>for english, there has been interest in balanced punctuation (briscoe, 1994), more recently using rule-based filters (white and rajkumar, 2008) <papid> W08-1703 </papid>in combinatory categorial grammar (ccg).</nextsent>
<nextsent>our focus is specifically 25 on unsupervised learning of dependency grammars and is similar, in spirit, to eisner and smiths (2005) <papid> W05-1504 </papid>vine grammar?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2893">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>parsing techniques most-similar to constrain tsa divide-and-rule?
</prevsent>
<prevsent>strategy that relies on punctuation has been used in supervised constituent parsing of long chinese sentences (li et al , 2005).<papid> I05-2002 </papid></prevsent>
</prevsection>
<citsent citstr=" W08-1703 ">
for english, there has been interest in balanced punctuation (briscoe, 1994), more recently using rule-based filters (white and rajkumar, 2008) <papid> W08-1703 </papid>in combinatory categorial grammar (ccg).</citsent>
<aftsection>
<nextsent>our focus is specifically 25 on unsupervised learning of dependency grammars and is similar, in spirit, to eisner and smiths (2005) <papid> W05-1504 </papid>vine grammar?</nextsent>
<nextsent>formalism.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2894">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>strategy that relies on punctuation has been used in supervised constituent parsing of long chinese sentences (li et al , 2005).<papid> I05-2002 </papid></prevsent>
<prevsent>for english, there has been interest in balanced punctuation (briscoe, 1994), more recently using rule-based filters (white and rajkumar, 2008) <papid> W08-1703 </papid>in combinatory categorial grammar (ccg).</prevsent>
</prevsection>
<citsent citstr=" W05-1504 ">
our focus is specifically 25 on unsupervised learning of dependency grammars and is similar, in spirit, to eisner and smiths (2005) <papid> W05-1504 </papid>vine grammar?</citsent>
<aftsection>
<nextsent>formalism.
</nextsent>
<nextsent>an important difference is that instead of imposing static limits on allowed dependency lengths, our restrictions are dynamic ? they disallow some long (and some short) arcs that would have otherwise crossed nearby punctuation.
</nextsent>
<nextsent>incorporating partial bracketings into grammar induction is an idea tracing back to pereira and schabes (1992).<papid> P92-1017 </papid></nextsent>
<nextsent>it inspired spitkovsky et al  (2010<papid> W10-2902 </papid>b) to mine parsing constraints from the web.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2895">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>formalism.
</prevsent>
<prevsent>an important difference is that instead of imposing static limits on allowed dependency lengths, our restrictions are dynamic ? they disallow some long (and some short) arcs that would have otherwise crossed nearby punctuation.
</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
incorporating partial bracketings into grammar induction is an idea tracing back to pereira and schabes (1992).<papid> P92-1017 </papid></citsent>
<aftsection>
<nextsent>it inspired spitkovsky et al  (2010<papid> W10-2902 </papid>b) to mine parsing constraints from the web.</nextsent>
<nextsent>in that same vein, we prospected more abundant and naturallanguage-resource ? punctuation, using constraint based techniques they developed for web markup.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2920">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>various other uses of punctuation in nlp punctuation is hard to predict,9 partly because it can signal long-range dependences (lu and ng, 2010).
</prevsent>
<prevsent>it often provides valuable cues to nlp tasks such as part-of-speech tagging and named-entityrecognition (hillard et al , 2006), information extraction (favre et al , 2008) and machine translation (lee et al , 2006; matusov et al , 2006).
</prevsent>
</prevsection>
<citsent citstr=" P86-1024 ">
other applications have included japanese sentence analysis (ohyama et al , 1986), <papid> P86-1024 </papid>genre detection (stamatatos et al , 2000), <papid> C00-2117 </papid>bilingual sentence alignment (yeh, 2003), semantic role labeling (pradhan et al ., 2005), <papid> W05-0634 </papid>chinese creation-title recognition (chen and chen, 2005) <papid> I05-1073 </papid>and word segmentation (li andsun, 2009), <papid> J09-4006 </papid>plus, recently, automatic vandalism de 9punctuation has high semantic entropy (melamed, 1997); <papid> W97-0207 </papid>for an analysis of the many roles played in the wsj by the comma ? the most frequent and unpredictable punctuation mark in that dataset ? see beeferman et al  (1998, table 2).</citsent>
<aftsection>
<nextsent>tection in wikipedia (wang and mckeown, 2010).<papid> C10-1129 </papid></nextsent>
<nextsent>punctuation improves dependency grammar induc tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2921">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>various other uses of punctuation in nlp punctuation is hard to predict,9 partly because it can signal long-range dependences (lu and ng, 2010).
</prevsent>
<prevsent>it often provides valuable cues to nlp tasks such as part-of-speech tagging and named-entityrecognition (hillard et al , 2006), information extraction (favre et al , 2008) and machine translation (lee et al , 2006; matusov et al , 2006).
</prevsent>
</prevsection>
<citsent citstr=" C00-2117 ">
other applications have included japanese sentence analysis (ohyama et al , 1986), <papid> P86-1024 </papid>genre detection (stamatatos et al , 2000), <papid> C00-2117 </papid>bilingual sentence alignment (yeh, 2003), semantic role labeling (pradhan et al ., 2005), <papid> W05-0634 </papid>chinese creation-title recognition (chen and chen, 2005) <papid> I05-1073 </papid>and word segmentation (li andsun, 2009), <papid> J09-4006 </papid>plus, recently, automatic vandalism de 9punctuation has high semantic entropy (melamed, 1997); <papid> W97-0207 </papid>for an analysis of the many roles played in the wsj by the comma ? the most frequent and unpredictable punctuation mark in that dataset ? see beeferman et al  (1998, table 2).</citsent>
<aftsection>
<nextsent>tection in wikipedia (wang and mckeown, 2010).<papid> C10-1129 </papid></nextsent>
<nextsent>punctuation improves dependency grammar induc tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2922">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>various other uses of punctuation in nlp punctuation is hard to predict,9 partly because it can signal long-range dependences (lu and ng, 2010).
</prevsent>
<prevsent>it often provides valuable cues to nlp tasks such as part-of-speech tagging and named-entityrecognition (hillard et al , 2006), information extraction (favre et al , 2008) and machine translation (lee et al , 2006; matusov et al , 2006).
</prevsent>
</prevsection>
<citsent citstr=" W05-0634 ">
other applications have included japanese sentence analysis (ohyama et al , 1986), <papid> P86-1024 </papid>genre detection (stamatatos et al , 2000), <papid> C00-2117 </papid>bilingual sentence alignment (yeh, 2003), semantic role labeling (pradhan et al ., 2005), <papid> W05-0634 </papid>chinese creation-title recognition (chen and chen, 2005) <papid> I05-1073 </papid>and word segmentation (li andsun, 2009), <papid> J09-4006 </papid>plus, recently, automatic vandalism de 9punctuation has high semantic entropy (melamed, 1997); <papid> W97-0207 </papid>for an analysis of the many roles played in the wsj by the comma ? the most frequent and unpredictable punctuation mark in that dataset ? see beeferman et al  (1998, table 2).</citsent>
<aftsection>
<nextsent>tection in wikipedia (wang and mckeown, 2010).<papid> C10-1129 </papid></nextsent>
<nextsent>punctuation improves dependency grammar induc tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2923">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>various other uses of punctuation in nlp punctuation is hard to predict,9 partly because it can signal long-range dependences (lu and ng, 2010).
</prevsent>
<prevsent>it often provides valuable cues to nlp tasks such as part-of-speech tagging and named-entityrecognition (hillard et al , 2006), information extraction (favre et al , 2008) and machine translation (lee et al , 2006; matusov et al , 2006).
</prevsent>
</prevsection>
<citsent citstr=" I05-1073 ">
other applications have included japanese sentence analysis (ohyama et al , 1986), <papid> P86-1024 </papid>genre detection (stamatatos et al , 2000), <papid> C00-2117 </papid>bilingual sentence alignment (yeh, 2003), semantic role labeling (pradhan et al ., 2005), <papid> W05-0634 </papid>chinese creation-title recognition (chen and chen, 2005) <papid> I05-1073 </papid>and word segmentation (li andsun, 2009), <papid> J09-4006 </papid>plus, recently, automatic vandalism de 9punctuation has high semantic entropy (melamed, 1997); <papid> W97-0207 </papid>for an analysis of the many roles played in the wsj by the comma ? the most frequent and unpredictable punctuation mark in that dataset ? see beeferman et al  (1998, table 2).</citsent>
<aftsection>
<nextsent>tection in wikipedia (wang and mckeown, 2010).<papid> C10-1129 </papid></nextsent>
<nextsent>punctuation improves dependency grammar induc tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2924">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>various other uses of punctuation in nlp punctuation is hard to predict,9 partly because it can signal long-range dependences (lu and ng, 2010).
</prevsent>
<prevsent>it often provides valuable cues to nlp tasks such as part-of-speech tagging and named-entityrecognition (hillard et al , 2006), information extraction (favre et al , 2008) and machine translation (lee et al , 2006; matusov et al , 2006).
</prevsent>
</prevsection>
<citsent citstr=" J09-4006 ">
other applications have included japanese sentence analysis (ohyama et al , 1986), <papid> P86-1024 </papid>genre detection (stamatatos et al , 2000), <papid> C00-2117 </papid>bilingual sentence alignment (yeh, 2003), semantic role labeling (pradhan et al ., 2005), <papid> W05-0634 </papid>chinese creation-title recognition (chen and chen, 2005) <papid> I05-1073 </papid>and word segmentation (li andsun, 2009), <papid> J09-4006 </papid>plus, recently, automatic vandalism de 9punctuation has high semantic entropy (melamed, 1997); <papid> W97-0207 </papid>for an analysis of the many roles played in the wsj by the comma ? the most frequent and unpredictable punctuation mark in that dataset ? see beeferman et al  (1998, table 2).</citsent>
<aftsection>
<nextsent>tection in wikipedia (wang and mckeown, 2010).<papid> C10-1129 </papid></nextsent>
<nextsent>punctuation improves dependency grammar induc tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2925">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>various other uses of punctuation in nlp punctuation is hard to predict,9 partly because it can signal long-range dependences (lu and ng, 2010).
</prevsent>
<prevsent>it often provides valuable cues to nlp tasks such as part-of-speech tagging and named-entityrecognition (hillard et al , 2006), information extraction (favre et al , 2008) and machine translation (lee et al , 2006; matusov et al , 2006).
</prevsent>
</prevsection>
<citsent citstr=" W97-0207 ">
other applications have included japanese sentence analysis (ohyama et al , 1986), <papid> P86-1024 </papid>genre detection (stamatatos et al , 2000), <papid> C00-2117 </papid>bilingual sentence alignment (yeh, 2003), semantic role labeling (pradhan et al ., 2005), <papid> W05-0634 </papid>chinese creation-title recognition (chen and chen, 2005) <papid> I05-1073 </papid>and word segmentation (li andsun, 2009), <papid> J09-4006 </papid>plus, recently, automatic vandalism de 9punctuation has high semantic entropy (melamed, 1997); <papid> W97-0207 </papid>for an analysis of the many roles played in the wsj by the comma ? the most frequent and unpredictable punctuation mark in that dataset ? see beeferman et al  (1998, table 2).</citsent>
<aftsection>
<nextsent>tection in wikipedia (wang and mckeown, 2010).<papid> C10-1129 </papid></nextsent>
<nextsent>punctuation improves dependency grammar induc tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2926">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it often provides valuable cues to nlp tasks such as part-of-speech tagging and named-entityrecognition (hillard et al , 2006), information extraction (favre et al , 2008) and machine translation (lee et al , 2006; matusov et al , 2006).
</prevsent>
<prevsent>other applications have included japanese sentence analysis (ohyama et al , 1986), <papid> P86-1024 </papid>genre detection (stamatatos et al , 2000), <papid> C00-2117 </papid>bilingual sentence alignment (yeh, 2003), semantic role labeling (pradhan et al ., 2005), <papid> W05-0634 </papid>chinese creation-title recognition (chen and chen, 2005) <papid> I05-1073 </papid>and word segmentation (li andsun, 2009), <papid> J09-4006 </papid>plus, recently, automatic vandalism de 9punctuation has high semantic entropy (melamed, 1997); <papid> W97-0207 </papid>for an analysis of the many roles played in the wsj by the comma ? the most frequent and unpredictable punctuation mark in that dataset ? see beeferman et al  (1998, table 2).</prevsent>
</prevsection>
<citsent citstr=" C10-1129 ">
tection in wikipedia (wang and mckeown, 2010).<papid> C10-1129 </papid></citsent>
<aftsection>
<nextsent>punctuation improves dependency grammar induction.
</nextsent>
<nextsent>many unsupervised (and supervised) parsers could be easily modified to use sprawl-constraineddecoding in inference.
</nextsent>
<nextsent>it applies to pre-trained models and, so far, helped every dataset and language.tightly interwoven into the fabric of writing systems, punctuation frames most unannotated plain text.
</nextsent>
<nextsent>we showed that rules for converting markup into accurate parsing constraints are still optimal for inter-punctuation fragments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2927">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>we showed that rules for converting markup into accurate parsing constraints are still optimal for inter-punctuation fragments.
</prevsent>
<prevsent>punctuation marks are more ubiquitous and natural than web markup: what little punctuation-induced constraints lack in precision, they more than make up in recall ? perhaps both types of constraints would work better yet intandem.
</prevsent>
</prevsection>
<citsent citstr=" H05-1030 ">
for language acquisition, natural question is whether prosody could similarly aid grammar induction from speech (kahn et al , 2005).<papid> H05-1030 </papid>our results underscore the power of simple models and algorithms, combined with common-senseconstraints.</citsent>
<aftsection>
<nextsent>they reinforce insights from joint modeling in supervised learning, where simplified, independent models, viterbi decoding and expressive constraints excel at sequence labeling tasks (rothand yih, 2005).
</nextsent>
<nextsent>such evidence is particularly welcome in unsupervised settings (punyakanok et al ,2005), where it is crucial that systems scale gracefully to volumes of data, on top of the usual desiderata ? ease of implementation, extension, understanding and debugging.
</nextsent>
<nextsent>future work could explore softening constraints (hayes and mouradian, 1980; <papid> P80-1026 </papid>chang et al , 2007), <papid> P07-1036 </papid>perhaps using features (eisner and smith, 2005; <papid> W05-1504 </papid>berg-kirkpatrick et al , 2010) orby learning to associate different settings with various marks: simply adding hidden tag for ordi nary?</nextsent>
<nextsent>versus divide?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2928">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>they reinforce insights from joint modeling in supervised learning, where simplified, independent models, viterbi decoding and expressive constraints excel at sequence labeling tasks (rothand yih, 2005).
</prevsent>
<prevsent>such evidence is particularly welcome in unsupervised settings (punyakanok et al ,2005), where it is crucial that systems scale gracefully to volumes of data, on top of the usual desiderata ? ease of implementation, extension, understanding and debugging.
</prevsent>
</prevsection>
<citsent citstr=" P80-1026 ">
future work could explore softening constraints (hayes and mouradian, 1980; <papid> P80-1026 </papid>chang et al , 2007), <papid> P07-1036 </papid>perhaps using features (eisner and smith, 2005; <papid> W05-1504 </papid>berg-kirkpatrick et al , 2010) orby learning to associate different settings with various marks: simply adding hidden tag for ordi nary?</citsent>
<aftsection>
<nextsent>versus divide?
</nextsent>
<nextsent>types of punctuation (li et al , 2005) <papid> I05-2002 </papid>may already usefully extend our model.</nextsent>
<nextsent>acknowledgments partially funded by the air force research laboratory (afrl), under prime contract no.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2929">
<title id=" W11-0303.xml">punctuation making a point in unsupervised dependency parsing </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>they reinforce insights from joint modeling in supervised learning, where simplified, independent models, viterbi decoding and expressive constraints excel at sequence labeling tasks (rothand yih, 2005).
</prevsent>
<prevsent>such evidence is particularly welcome in unsupervised settings (punyakanok et al ,2005), where it is crucial that systems scale gracefully to volumes of data, on top of the usual desiderata ? ease of implementation, extension, understanding and debugging.
</prevsent>
</prevsection>
<citsent citstr=" P07-1036 ">
future work could explore softening constraints (hayes and mouradian, 1980; <papid> P80-1026 </papid>chang et al , 2007), <papid> P07-1036 </papid>perhaps using features (eisner and smith, 2005; <papid> W05-1504 </papid>berg-kirkpatrick et al , 2010) orby learning to associate different settings with various marks: simply adding hidden tag for ordi nary?</citsent>
<aftsection>
<nextsent>versus divide?
</nextsent>
<nextsent>types of punctuation (li et al , 2005) <papid> I05-2002 </papid>may already usefully extend our model.</nextsent>
<nextsent>acknowledgments partially funded by the air force research laboratory (afrl), under prime contract no.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2932">
<title id=" W11-0149.xml">semantic relatedness from automatically generated semantic networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present promising first results that indicate potential competitiveness with approaches based on manually created resources.
</prevsent>
<prevsent>the quantification of semantic similarity and relatedness of terms is an important problem of lexicalsemantics.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
its applications include word sense disambiguation, text summarization and information retrieval (budanitsky and hirst, 2006).<papid> J06-1003 </papid></citsent>
<aftsection>
<nextsent>most approaches to measuring semantic relatedness fall into one of two categories.
</nextsent>
<nextsent>they either look at distributional properties based on corpora (finkelstein et al, 2002;agirre et al, 2009) <papid> N09-1003 </papid>or make use of pre-existing knowledge resources such as wordnet or rogets thesaurus (hughes and ramage, 2007; <papid> D07-1061 </papid>jarmasz, 2003).</nextsent>
<nextsent>the latter approaches achieve good results, but theyare inherently restricted in coverage and domain adaptation due to their reliance on costly manual acquisition of the resource.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2935">
<title id=" W11-0149.xml">semantic relatedness from automatically generated semantic networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>its applications include word sense disambiguation, text summarization and information retrieval (budanitsky and hirst, 2006).<papid> J06-1003 </papid></prevsent>
<prevsent>most approaches to measuring semantic relatedness fall into one of two categories.</prevsent>
</prevsection>
<citsent citstr=" N09-1003 ">
they either look at distributional properties based on corpora (finkelstein et al, 2002;agirre et al, 2009) <papid> N09-1003 </papid>or make use of pre-existing knowledge resources such as wordnet or rogets thesaurus (hughes and ramage, 2007; <papid> D07-1061 </papid>jarmasz, 2003).</citsent>
<aftsection>
<nextsent>the latter approaches achieve good results, but theyare inherently restricted in coverage and domain adaptation due to their reliance on costly manual acquisition of the resource.
</nextsent>
<nextsent>in addition, those methods that are based on hierarchical, taxonomic ally structured resources are generally better suited for measuring semantic similarity than relatedness (budanitsky and hirst, 2006).<papid> J06-1003 </papid></nextsent>
<nextsent>in this paper, we introduce novel technique that measures semantic relatedness based on an automatically generated semantic network.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2939">
<title id=" W11-0149.xml">semantic relatedness from automatically generated semantic networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>its applications include word sense disambiguation, text summarization and information retrieval (budanitsky and hirst, 2006).<papid> J06-1003 </papid></prevsent>
<prevsent>most approaches to measuring semantic relatedness fall into one of two categories.</prevsent>
</prevsection>
<citsent citstr=" D07-1061 ">
they either look at distributional properties based on corpora (finkelstein et al, 2002;agirre et al, 2009) <papid> N09-1003 </papid>or make use of pre-existing knowledge resources such as wordnet or rogets thesaurus (hughes and ramage, 2007; <papid> D07-1061 </papid>jarmasz, 2003).</citsent>
<aftsection>
<nextsent>the latter approaches achieve good results, but theyare inherently restricted in coverage and domain adaptation due to their reliance on costly manual acquisition of the resource.
</nextsent>
<nextsent>in addition, those methods that are based on hierarchical, taxonomic ally structured resources are generally better suited for measuring semantic similarity than relatedness (budanitsky and hirst, 2006).<papid> J06-1003 </papid></nextsent>
<nextsent>in this paper, we introduce novel technique that measures semantic relatedness based on an automatically generated semantic network.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2943">
<title id=" W11-0149.xml">semantic relatedness from automatically generated semantic networks </title>
<section> similarity and relatedness from semantic networks.  </section>
<citcontext>
<prevsection>
<prevsent>we build the network incrementally by parsing every sentence, translating it into small network fragment and then mapping that fragment onto the main network generated from all previous sentences.
</prevsent>
<prevsent>our translation of sentences from text to network is based on the one used in the asknet system (harrington and clark, 2007).
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
it makes use of two nlp tools, the clark and curran parser (clark and curran, 2004) <papid> P04-1014 </papid>and the semantic analysis tool boxer (bos et al, 2004), <papid> C04-1180 </papid>both of which are part of the c&c; toolkit1.the parser is based on combinatory categorial grammar (ccg) and has been trained on 40,000 manually annotated sentences of the wsj.</citsent>
<aftsection>
<nextsent>it is both robust and efficient.
</nextsent>
<nextsent>boxer is designed to convert the ccg parsed text into logical representation based on discourse representation theory (drt).
</nextsent>
<nextsent>this intermediate logical form representation presents an abstraction from syntactic details to semantic core information.
</nextsent>
<nextsent>for example, the syntactical forms progress of student and students progress have the same boxer representation as well as the student who attends the lecture and the student attending the lecture.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2944">
<title id=" W11-0149.xml">semantic relatedness from automatically generated semantic networks </title>
<section> similarity and relatedness from semantic networks.  </section>
<citcontext>
<prevsection>
<prevsent>we build the network incrementally by parsing every sentence, translating it into small network fragment and then mapping that fragment onto the main network generated from all previous sentences.
</prevsent>
<prevsent>our translation of sentences from text to network is based on the one used in the asknet system (harrington and clark, 2007).
</prevsent>
</prevsection>
<citsent citstr=" C04-1180 ">
it makes use of two nlp tools, the clark and curran parser (clark and curran, 2004) <papid> P04-1014 </papid>and the semantic analysis tool boxer (bos et al, 2004), <papid> C04-1180 </papid>both of which are part of the c&c; toolkit1.the parser is based on combinatory categorial grammar (ccg) and has been trained on 40,000 manually annotated sentences of the wsj.</citsent>
<aftsection>
<nextsent>it is both robust and efficient.
</nextsent>
<nextsent>boxer is designed to convert the ccg parsed text into logical representation based on discourse representation theory (drt).
</nextsent>
<nextsent>this intermediate logical form representation presents an abstraction from syntactic details to semantic core information.
</nextsent>
<nextsent>for example, the syntactical forms progress of student and students progress have the same boxer representation as well as the student who attends the lecture and the student attending the lecture.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2968">
<title id=" W11-1716.xml">automatic expansion of feature level opinion lexicons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as key in solving most of these problems, the semantic orientation of some opinion expressions should be com puted: numeric value, usually between 1 and 1,referring to the negative or positive affective implications of given word or prhase.
</prevsent>
<prevsent>these values can be collected in an opinion lexicon, so this resource can be accessed when needed.
</prevsent>
</prevsection>
<citsent citstr=" H05-1043 ">
many recent works (popescu and etzioni, 2005; <papid> H05-1043 </papid>kanayama and nasukawa, 2006; <papid> W06-1642 </papid>cruz et al, 2010;qiu et al, 2011) suggest the need for domain specific opinion lexicons, containing semantic orientations of opinion expressions when used in particular domain (e.g., the word predictable?</citsent>
<aftsection>
<nextsent>has opposite semantic orientations when used to define the driving experience of car or the plot of movie).
</nextsent>
<nextsent>moreover, within given domain, the specific target of the opinion is also important to induce the polarity and the intensity of the affective implications of some opinion expressions ( consider for example the word cheap?
</nextsent>
<nextsent>when referring to the price or tothe appearance of an electronic device).
</nextsent>
<nextsent>this is especially important to extract opinions from product reviews, where users write their opinions about individual features of product.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2969">
<title id=" W11-1716.xml">automatic expansion of feature level opinion lexicons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as key in solving most of these problems, the semantic orientation of some opinion expressions should be com puted: numeric value, usually between 1 and 1,referring to the negative or positive affective implications of given word or prhase.
</prevsent>
<prevsent>these values can be collected in an opinion lexicon, so this resource can be accessed when needed.
</prevsent>
</prevsection>
<citsent citstr=" W06-1642 ">
many recent works (popescu and etzioni, 2005; <papid> H05-1043 </papid>kanayama and nasukawa, 2006; <papid> W06-1642 </papid>cruz et al, 2010;qiu et al, 2011) suggest the need for domain specific opinion lexicons, containing semantic orientations of opinion expressions when used in particular domain (e.g., the word predictable?</citsent>
<aftsection>
<nextsent>has opposite semantic orientations when used to define the driving experience of car or the plot of movie).
</nextsent>
<nextsent>moreover, within given domain, the specific target of the opinion is also important to induce the polarity and the intensity of the affective implications of some opinion expressions ( consider for example the word cheap?
</nextsent>
<nextsent>when referring to the price or tothe appearance of an electronic device).
</nextsent>
<nextsent>this is especially important to extract opinions from product reviews, where users write their opinions about individual features of product.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2970">
<title id=" W11-1716.xml">automatic expansion of feature level opinion lexicons </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 semantic orientation induction.
</prevsent>
<prevsent>many methods for computing semantic orientations of words or phrases have been proposed over the last years.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
some of them relyon large set of text documents to compute semantic orientations of words in an unsupervised manner (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>turney and littman, 2003; yu and hatzivassiloglou, 2003).<papid> W03-1017 </papid></citsent>
<aftsection>
<nextsent>they all start from afew positive and negative seeds, and calculate the semantic orientation of target words based on conjunc tive constructions (hatzivassiloglou and mckeown, 1997) <papid> P97-1023 </papid>or co-occurrences (turney and littman, 2003; yu and hatzivassiloglou, 2003) <papid> W03-1017 </papid>of target words andseeds.</nextsent>
<nextsent>these methods allow computing domain specific semantic orientations, just using set of documents of the selected domain, but they obtain modest values of recall and precision.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2971">
<title id=" W11-1716.xml">automatic expansion of feature level opinion lexicons </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 semantic orientation induction.
</prevsent>
<prevsent>many methods for computing semantic orientations of words or phrases have been proposed over the last years.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
some of them relyon large set of text documents to compute semantic orientations of words in an unsupervised manner (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>turney and littman, 2003; yu and hatzivassiloglou, 2003).<papid> W03-1017 </papid></citsent>
<aftsection>
<nextsent>they all start from afew positive and negative seeds, and calculate the semantic orientation of target words based on conjunc tive constructions (hatzivassiloglou and mckeown, 1997) <papid> P97-1023 </papid>or co-occurrences (turney and littman, 2003; yu and hatzivassiloglou, 2003) <papid> W03-1017 </papid>of target words andseeds.</nextsent>
<nextsent>these methods allow computing domain specific semantic orientations, just using set of documents of the selected domain, but they obtain modest values of recall and precision.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2977">
<title id=" W11-1716.xml">automatic expansion of feature level opinion lexicons </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we are using the observations about conjunctive constructions from (hatzivassiloglou and mckeown, 1997) <papid> P97-1023 </papid>in our approach.other works use the lexical resource wordnet(fellbaum, 1998) to compute the semantic orientation of given word or phrase.</prevsent>
<prevsent>for example, in (kamps et al, 2004), distance function between words is defined using wordnet synonymy relations, so the semantic orientation of word is calculated from the distance to positive seed (good?)and negative seed (bad?).</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
other works use bigger set of seeds and the synonyms/antonyms sets from wordnet to build an opinion lexicon incrementally (hu and liu, 2004a; kim and hovy, 2004).<papid> C04-1200 </papid>in other works (esuli and sebastiani, 2006; <papid> E06-1025 </papid>bac cianella et al, 2010; esuli and sebastiani, 2005), the basic assumption is that if word is semantically oriented in one direction, then the words in its gloss (i.e. textual definitions) tend to be oriented in thesame direction.</citsent>
<aftsection>
<nextsent>two big sets of positive and negative words are built, starting from two initial sets of seed words and growing them using the synonymy and antonymy relations in wordnet.
</nextsent>
<nextsent>for every word in those sets, textual representation is obtained by collecting all the glosses of that word.
</nextsent>
<nextsent>these textual representations are transformed into vectors by standard text indexing techniques, and binary classifier is trained using these vectors.
</nextsent>
<nextsent>the same assumption about words and their glosses is made by esuli and sebastiani (2007), <papid> P07-1054 </papid>but the relation between words and glosses are used to build graph representation of wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2978">
<title id=" W11-1716.xml">automatic expansion of feature level opinion lexicons </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we are using the observations about conjunctive constructions from (hatzivassiloglou and mckeown, 1997) <papid> P97-1023 </papid>in our approach.other works use the lexical resource wordnet(fellbaum, 1998) to compute the semantic orientation of given word or phrase.</prevsent>
<prevsent>for example, in (kamps et al, 2004), distance function between words is defined using wordnet synonymy relations, so the semantic orientation of word is calculated from the distance to positive seed (good?)and negative seed (bad?).</prevsent>
</prevsection>
<citsent citstr=" E06-1025 ">
other works use bigger set of seeds and the synonyms/antonyms sets from wordnet to build an opinion lexicon incrementally (hu and liu, 2004a; kim and hovy, 2004).<papid> C04-1200 </papid>in other works (esuli and sebastiani, 2006; <papid> E06-1025 </papid>bac cianella et al, 2010; esuli and sebastiani, 2005), the basic assumption is that if word is semantically oriented in one direction, then the words in its gloss (i.e. textual definitions) tend to be oriented in thesame direction.</citsent>
<aftsection>
<nextsent>two big sets of positive and negative words are built, starting from two initial sets of seed words and growing them using the synonymy and antonymy relations in wordnet.
</nextsent>
<nextsent>for every word in those sets, textual representation is obtained by collecting all the glosses of that word.
</nextsent>
<nextsent>these textual representations are transformed into vectors by standard text indexing techniques, and binary classifier is trained using these vectors.
</nextsent>
<nextsent>the same assumption about words and their glosses is made by esuli and sebastiani (2007), <papid> P07-1054 </papid>but the relation between words and glosses are used to build graph representation of wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2979">
<title id=" W11-1716.xml">automatic expansion of feature level opinion lexicons </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for every word in those sets, textual representation is obtained by collecting all the glosses of that word.
</prevsent>
<prevsent>these textual representations are transformed into vectors by standard text indexing techniques, and binary classifier is trained using these vectors.
</prevsent>
</prevsection>
<citsent citstr=" P07-1054 ">
the same assumption about words and their glosses is made by esuli and sebastiani (2007), <papid> P07-1054 </papid>but the relation between words and glosses are used to build graph representation of wordnet.</citsent>
<aftsection>
<nextsent>given few seeds as input, two scores of positivity and negativity are computed, using arandom-walk ranking algorithm similar to pager ank (page et al, 1998).
</nextsent>
<nextsent>as result of these works, an opinion lexicon named sentiwordnet (baccianellaet al, 2010) is publicly available.
</nextsent>
<nextsent>we are also using ranking algorithm in our expansion method, but applying it to differently built, domain-specific graph of terms.the main weakness of the dictionary-based approaches is that they compute domain-independentsemantic orientations.
</nextsent>
<nextsent>there are some manually collected lexicons (stone, 1966; cerini et al, 2007), with semantic orientations of terms set by humans.however, they are also domain-independent resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2983">
<title id=" W11-0416.xml">mae and mai lightweight annotation and adjudication tools </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>both programs are written in java and use stand alone sqlite database1 for storage and retrieval of annotation data, and output standoff xml that is compliant with the abstract laf model.
</prevsent>
<prevsent>both of these tools are available from http: //pages.cs.brandeis.edu/astubbs/
</prevsent>
</prevsection>
<citsent citstr=" N06-4006 ">
as previously mentioned, there are already number of annotation tools in use dipper et al examined five different programs; additionally knowta tor (ogren, 2006), <papid> N06-4006 </papid>gate (cunningham et al, 2010), callisto (mitre, 2002), and bat (verhagen, 2010) have been used for various annotation tasks; and the list goes on2.</citsent>
<aftsection>
<nextsent>however, as kaplan et al noted in paper about their own annotation software, slat 2.0 (2010), much annotation software is not generic,either because it was designed for specific annotation task, or designed to be used in particularway.
</nextsent>
<nextsent>bat, for example, utilizes layered annotation framework, which allows for adjudication at each step of the annotation process, but this makes 1http://www.zentus.com/sqlitejdbc/ 2see http://annotation.exmaralda.org/index.php/tools for reasonably up-to-date list of annotation software 129 tasks difficult to modify and is best suited for use when the schema is not likely to change.
</nextsent>
<nextsent>gate was built primarily as tool for automated annotation,and callisto, while excellent for annotating contiguous portions of texts, cannot easily create links?
</nextsent>
<nextsent>it requires the user to create an entire task-specificplug-in.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2984">
<title id=" W10-4208.xml">towards a programmable instrumented generator </title>
<section> controlling an nlg system: examples.  </section>
<citcontext>
<prevsection>
<prevsent>often conformance can only be tested at the end of the nlg pipeline, when whole number of complex strategic and tactical decisions have been made, resulting in complete text.
</prevsent>
<prevsent>a number of recent pieces of work have begun to address the question of how to tune systems in order to make the decisions that lead to the most stylistically preferred outputs.
</prevsent>
</prevsection>
<citsent citstr=" P05-1008 ">
paiva and evans (2005) (<papid> P05-1008 </papid>henceforth pe) investigate controlling generator decisions for achieving stylistic goals, e.g. choices between: the patient takes the two gram dose of the patients medicine twice day.</citsent>
<aftsection>
<nextsent>and the dose of the patients medicine is taken twice day.
</nextsent>
<nextsent>it is two grams.
</nextsent>
<nextsent>in this case, stylistic goal of the system is expressed as goal values for features ssi, where each ssi expresses something that can be measured in the output text, e.g. counting the number of pronouns or passives.
</nextsent>
<nextsent>the system learns to control the number of times specific binary generator deci figure 1: example personage rule sions are made (gdj), where these decisions involve things like whether to split the input into 2 sentences or whether to generate an pp clause.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2985">
<title id=" W10-4208.xml">towards a programmable instrumented generator </title>
<section> experiment 1: matching human texts.  </section>
<citcontext>
<prevsection>
<prevsent>it is important to note that for these systems the instrument ing was done by someone (the author) with limited knowledge of the underlying nlg system and with notion of text quality different from that used by the original system.
</prevsent>
<prevsent>also, in both cases the limited availability of example data meant that testing had to be performed on the training data (and so any positive results may be partly due to overfitting).
</prevsent>
</prevsection>
<citsent citstr=" E06-2020 ">
for this experiment, we took an nlg system that produces pollen forecasts and was written by ross turner (turner et al 2006).<papid> E06-2020 </papid></citsent>
<aftsection>
<nextsent>turner collected 68 examples of pollen prediction data for scotland (each consisting of 6 small inte gers and characterisation of the previous trend) with human written forecasts, which we took as both our training and test data.
</nextsent>
<nextsent>we evaluated text quality by similarity to the human text, as measured by the meteor metric (lavie and denkowski 2009).
</nextsent>
<nextsent>note that the human forecasters had access to more background knowledge than the system, and so this is not task that the system would be expected to do particularly well on.
</nextsent>
<nextsent>the notion of program state?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2986">
<title id=" W10-4208.xml">towards a programmable instrumented generator </title>
<section> experiment 2: text length control.  </section>
<citcontext>
<prevsection>
<prevsent>following the learned policy, the system also obtained an average of 0.279.
</prevsent>
<prevsent>the difference between the learned behaviour and random generation is significant (p =0.002) according to t test.
</prevsent>
</prevsection>
<citsent citstr=" J00-2005 ">
a challenging stylistic requirement for nlg is that of producing text satisfying precise length requirements (reiter 2000).<papid> J00-2005 </papid></citsent>
<aftsection>
<nextsent>for this experiment, we took the eleonplus nlg system developed by hien nguyen.
</nextsent>
<nextsent>this combines the existing eleon user interface for domain authoring (bilidas et al  2007) with new nlg system that incorporates the simple nlg realiser (gatt and reiter 2009).<papid> W09-0613 </papid></nextsent>
<nextsent>figure 3: pig panel interface the system was used for simple domain of texts about university buildings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2987">
<title id=" W10-4208.xml">towards a programmable instrumented generator </title>
<section> experiment 2: text length control.  </section>
<citcontext>
<prevsection>
<prevsent>a challenging stylistic requirement for nlg is that of producing text satisfying precise length requirements (reiter 2000).<papid> J00-2005 </papid></prevsent>
<prevsent>for this experiment, we took the eleonplus nlg system developed by hien nguyen.</prevsent>
</prevsection>
<citsent citstr=" W09-0613 ">
this combines the existing eleon user interface for domain authoring (bilidas et al  2007) with new nlg system that incorporates the simple nlg realiser (gatt and reiter 2009).<papid> W09-0613 </papid></citsent>
<aftsection>
<nextsent>figure 3: pig panel interface the system was used for simple domain of texts about university buildings.
</nextsent>
<nextsent>the data used was the authored information about 7 university buildings and associated objects.
</nextsent>
<nextsent>we evaluated texts using simple (character) length criterion, where the ideal text was 250 characters, with steeply increasing penalty for texts longer than this and slowly increasing penalty for texts that are shorter.
</nextsent>
<nextsent>the notion of state?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2988">
<title id=" W10-4208.xml">towards a programmable instrumented generator </title>
<section> experiment 2: text length control.  </section>
<citcontext>
<prevsection>
<prevsent>the notion of state?
</prevsent>
<prevsent>that was logged took account of the depth of the traversal of the domain data, the maximum number of facts per sentence and an aggregation decision.
</prevsent>
</prevsection>
<citsent citstr=" P06-2085 ">
following the previous successful demonstration of reinforcement learning for nlg decisions (rie ser and lemon 2006), <papid> P06-2085 </papid>we decided to use the sarsa approach (though without function approximation) for the training.</citsent>
<aftsection>
<nextsent>this involves rewarding individual states for their (direct or indirect) influence on outcome quality as the system actually performs.
</nextsent>
<nextsent>the policy is mixture of random exploration and the choosing of the currently most promising states, according to the value of numerical parameter ?.
</nextsent>
<nextsent>running the system on the 7 examples with 3 random generations for each produced an average text quality of -2514.
</nextsent>
<nextsent>we tried sarsa training regime with 3000 random examples at ?=0.1, followed by 2000 random examples at ?=0.001.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2989">
<title id=" W11-1305.xml">shared task system description frustratingly hard compositionality prediction </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>is not very different from that of age?.we used coals (rohde et al, 2009) to calculate word distributions.
</prevsent>
<prevsent>the coals algorithm builds word-to-word semantic space from corpus.
</prevsent>
</prevsection>
<citsent citstr=" P10-4006 ">
we used the implementation by jurgens and stevens (2010), <papid> P10-4006 </papid>generating the semantic space fromthe wacky corpora for english and german with duplicate sentences removed and low-frequency words substituted by dummy symbols.</citsent>
<aftsection>
<nextsent>the word pairs have been fed to coals as compounds that have to be treated as single tokens, and the semantic space has been generated and reduced using singular value decompositon.
</nextsent>
<nextsent>the vectors for w1, w2 and w1 w2?
</nextsent>
<nextsent>are calculated, and we compute the cosine distance between the semantic space vectors for the word pair and its parts, and between the parts themselves, namely for w1 w2?
</nextsent>
<nextsent>and w1, for w1 w2?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2990">
<title id=" W11-1103.xml">in variants and variability of synonymy networks self mediated agreement by confluence </title>
<section> analysis of the disagreement between.  </section>
<citcontext>
<prevsection>
<prevsent>this divergence in judgements can be explained by editorial policies and choices (regarding, for example printed size constraints, targeted audiences...).
</prevsent>
<prevsent>furthermore, lexicographers also have their subjectivity.
</prevsent>
</prevsection>
<citsent citstr=" J02-2001 ">
sincesynonymy is more continuous gradient than discrete choice (edmonds and hirst, 2002), <papid> J02-2001 </papid>an alternative limited to synonym/not synonym leaves ample room for subjective interpretation.</citsent>
<aftsection>
<nextsent>however, these justifications do not account for such discrepancies between resources describing the semantic relations of words of the same language.
</nextsent>
<nextsent>therefore, we expect that, if two words are deemed not synonyms in one resource g1, but synonyms in another g2, they will nevertheless share many neighbours in g1 and g2.
</nextsent>
<nextsent>in other words they will belong to the same dense zones.
</nextsent>
<nextsent>consequently the dense zones (or clus ters) found in g1 will be similar to those found in g2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2991">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>subjectivity identification is to identify whether an expression contains opinion or sentiment.
</prevsent>
<prevsent>automatic subjectivity identification can benefit many natural language processing (nlp) tasks.
</prevsent>
</prevsection>
<citsent citstr=" C08-2019 ">
for example, information retrieval systems can provide affective or informative articles separately (pang and lee, 2008).<papid> C08-2019 </papid></citsent>
<aftsection>
<nextsent>summarization systems may want to summarize factual and opinionated content differently (murray and carenini, 2008).<papid> D08-1081 </papid></nextsent>
<nextsent>in this paper, we perform subjectivity detection at sentence level,which is more appropriate for some subsequent processing such as opinion summarization.previous work has shown that when enough labeled data is available, supervised classification methods can achieve high accuracy for subjectivity detection in some domains.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2992">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatic subjectivity identification can benefit many natural language processing (nlp) tasks.
</prevsent>
<prevsent>for example, information retrieval systems can provide affective or informative articles separately (pang and lee, 2008).<papid> C08-2019 </papid></prevsent>
</prevsection>
<citsent citstr=" D08-1081 ">
summarization systems may want to summarize factual and opinionated content differently (murray and carenini, 2008).<papid> D08-1081 </papid></citsent>
<aftsection>
<nextsent>in this paper, we perform subjectivity detection at sentence level,which is more appropriate for some subsequent processing such as opinion summarization.previous work has shown that when enough labeled data is available, supervised classification methods can achieve high accuracy for subjectivity detection in some domains.
</nextsent>
<nextsent>however, it is often expensive to create such training data.
</nextsent>
<nextsent>on the other hand, lot of unannotated data is readily available in various domains.
</nextsent>
<nextsent>therefore an interesting and important problem is to develop semi-supervised or unsupervised learning methods that can learn froman unannotated corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2993">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the early age, knowledge-based methods were widely used for subjectivity detection.
</prevsent>
<prevsent>they used lexicon or patterns and rules to predict whether target is subjective or not.
</prevsent>
</prevsection>
<citsent citstr=" I05-2011 ">
these methods tended to yield high precision and low recall, or low precision and high recall (kim and hovy, 2005).<papid> I05-2011 </papid></citsent>
<aftsection>
<nextsent>recently, machine learning approaches have been adopted more often (ng et al, 2006).<papid> P06-2079 </papid></nextsent>
<nextsent>there are limitations in both methods.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2994">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they used lexicon or patterns and rules to predict whether target is subjective or not.
</prevsent>
<prevsent>these methods tended to yield high precision and low recall, or low precision and high recall (kim and hovy, 2005).<papid> I05-2011 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-2079 ">
recently, machine learning approaches have been adopted more often (ng et al, 2006).<papid> P06-2079 </papid></citsent>
<aftsection>
<nextsent>there are limitations in both methods.
</nextsent>
<nextsent>in knowledge-based approaches, predefined subjectivity lexicon maynot adapt well to different domains.
</nextsent>
<nextsent>while in machine learning approach, human labeling efforts are required to create large training set.
</nextsent>
<nextsent>to overcome the above drawbacks, unsupervised or semi-supervised methods have been explored in sentiment analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2995">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while in machine learning approach, human labeling efforts are required to create large training set.
</prevsent>
<prevsent>to overcome the above drawbacks, unsupervised or semi-supervised methods have been explored in sentiment analysis.
</prevsent>
</prevsection>
<citsent citstr=" P09-1079 ">
for polarity classification, some previous work used spectral techniques (dasgupta and ng, 2009) <papid> P09-1079 </papid>or co-training (li et al, 2010) <papid> P10-1043 </papid>to mine the reviews in semi-supervised manner.</citsent>
<aftsection>
<nextsent>for subjectivity identification, wiebe and riloff (wiebe and riloff, 2005) applied rule-based method to create training set first and then used it to train naive bayes classifier.
</nextsent>
<nextsent>melville et al (melville et al, 2009) used pooling multinomial method to combine lexicon derived probability and statistical probability.
</nextsent>
<nextsent>our work is similar to the study in (wiebe and riloff, 2005) in that we both use rule-based method to create an initial training set and learn from 161unannotated corpus.
</nextsent>
<nextsent>however, there are two key differences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2996">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while in machine learning approach, human labeling efforts are required to create large training set.
</prevsent>
<prevsent>to overcome the above drawbacks, unsupervised or semi-supervised methods have been explored in sentiment analysis.
</prevsent>
</prevsection>
<citsent citstr=" P10-1043 ">
for polarity classification, some previous work used spectral techniques (dasgupta and ng, 2009) <papid> P09-1079 </papid>or co-training (li et al, 2010) <papid> P10-1043 </papid>to mine the reviews in semi-supervised manner.</citsent>
<aftsection>
<nextsent>for subjectivity identification, wiebe and riloff (wiebe and riloff, 2005) applied rule-based method to create training set first and then used it to train naive bayes classifier.
</nextsent>
<nextsent>melville et al (melville et al, 2009) used pooling multinomial method to combine lexicon derived probability and statistical probability.
</nextsent>
<nextsent>our work is similar to the study in (wiebe and riloff, 2005) in that we both use rule-based method to create an initial training set and learn from 161unannotated corpus.
</nextsent>
<nextsent>however, there are two key differences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2997">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>our cross corpus study shows how the unsupervised learning approach performs in different domains and helps us understand what are the factors impacting the learning methods.
</prevsent>
<prevsent>we use three datasets from different domains: movie, news resource, and meeting conversations.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
the first two are from written text domain and have been widely used in many previous studies for sentiment analysis (pang and lee, 2004; <papid> P04-1035 </papid>raaijmakers and kraaij, 2008).</citsent>
<aftsection>
<nextsent>the third one is from speechtranscripts.
</nextsent>
<nextsent>it has been used in few recent studies (raaijmakers et al, 2008; <papid> D08-1049 </papid>murray and carenini,2009), but not as much as those text data.</nextsent>
<nextsent>the following provides more details of the data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH2998">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>the first two are from written text domain and have been widely used in many previous studies for sentiment analysis (pang and lee, 2004; <papid> P04-1035 </papid>raaijmakers and kraaij, 2008).</prevsent>
<prevsent>the third one is from speechtranscripts.</prevsent>
</prevsection>
<citsent citstr=" D08-1049 ">
it has been used in few recent studies (raaijmakers et al, 2008; <papid> D08-1049 </papid>murray and carenini,2009), but not as much as those text data.</citsent>
<aftsection>
<nextsent>the following provides more details of the data.
</nextsent>
<nextsent>the first corpus is movie data (pang and lee, 2004).<papid> P04-1035 </papid></nextsent>
<nextsent>it contains 5,000 subjective sentences collected from movie reviews and 5,000 objective sentences collected from movie plot sum maries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3000">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>it contains 5,000 subjective sentences collected from movie reviews and 5,000 objective sentences collected from movie plot summaries.
</prevsent>
<prevsent>the sentences in each collection are randomly ordered.?
</prevsent>
</prevsection>
<citsent citstr=" W03-2102 ">
the second one is extracted from mpqa corpus (version 2.0) (wilson and wiebe, 2003), <papid> W03-2102 </papid>which is collected from news articles.</citsent>
<aftsection>
<nextsent>this data has been annotated with subjective information at phrase level.
</nextsent>
<nextsent>we adopted the same rules as in (riloff and wiebe, 2003) <papid> W03-1014 </papid>to create the sentence level label: if sentence has at least one private state of strength medium or higher, thenthe sentence is labeled subjective, otherwise it is labeled objective.</nextsent>
<nextsent>we randomly extracted 5,000 subjective and 5,000 objective sentences from this corpus to make it comparable with the movie data.?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3001">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>the second one is extracted from mpqa corpus (version 2.0) (wilson and wiebe, 2003), <papid> W03-2102 </papid>which is collected from news articles.</prevsent>
<prevsent>this data has been annotated with subjective information at phrase level.</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
we adopted the same rules as in (riloff and wiebe, 2003) <papid> W03-1014 </papid>to create the sentence level label: if sentence has at least one private state of strength medium or higher, thenthe sentence is labeled subjective, otherwise it is labeled objective.</citsent>
<aftsection>
<nextsent>we randomly extracted 5,000 subjective and 5,000 objective sentences from this corpus to make it comparable with the movie data.?
</nextsent>
<nextsent>the third dataset is from ami meeting corpus.
</nextsent>
<nextsent>it has been annotated using the scheme described in (wilson, 2008).<papid> L08-1475 </papid></nextsent>
<nextsent>there are 3 main categories of annotations regarding sentiments: subjective utterances, subjective questions, and objective polar utterances.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3002">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>we randomly extracted 5,000 subjective and 5,000 objective sentences from this corpus to make it comparable with the movie data.?
</prevsent>
<prevsent>the third dataset is from ami meeting corpus.
</prevsent>
</prevsection>
<citsent citstr=" L08-1475 ">
it has been annotated using the scheme described in (wilson, 2008).<papid> L08-1475 </papid></citsent>
<aftsection>
<nextsent>there are 3 main categories of annotations regarding sentiments: subjective utterances, subjective questions, and objective polar utterances.
</nextsent>
<nextsent>we consider the union of subjective utterance and subjective question as subjective and the rest as objective.
</nextsent>
<nextsent>the subjectivity classification task is done at the dialog act (da) levels.
</nextsent>
<nextsent>we label each dausing the label of the utterance that has overlap with it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3003">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> unsupervised subjectivity detection.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 create initial training set.
</prevsent>
<prevsent>a lexicon-based method is used to create an initial training set, since it can often achieve high precision rate (though low recall) for subjectivity detection.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
we use subjectivity lexicon (wilson et al, 2005) <papid> H05-1044 </papid>to calculate the subjectivity score for each sentence.</citsent>
<aftsection>
<nextsent>162this lexicon contains 8,221 entries that are categorized into strong and weak subjective clues.
</nextsent>
<nextsent>for each word w, we assign subjectivity score sub(w): 1 to strong subjective clues, 0.5 to weak clues, and 0 for any other word.
</nextsent>
<nextsent>then the subjectivity score of sentence is the sum of the values ofall the words in the sentence, normalized by the sentence length.
</nextsent>
<nextsent>we noticed that for sentences labeled as subjective in the three corpora, the subjective clues appear more frequently in movie data than the other two corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3004">
<title id=" W11-1721.xml">a cross corpus study of unsupervised subjectivity identification based on calibrated em </title>
<section> unsupervised subjectivity detection.  </section>
<citcontext>
<prevsection>
<prevsent>in the first iteration, we assign (cj |si) using the pseudo training data generated based on lexicon information.
</prevsent>
<prevsent>if sentence is labeled subjective,then (sub|si) is 1 and (obj|si) is 0; for the sentences with objective labels, (sub|si) is 0 and (obj|si) is 1.
</prevsent>
</prevsection>
<citsent citstr=" W03-0417 ">
in our work, we use variant of standard em: calibrated em, introduced by (tsuruoka and tsujii, 2003).<papid> W03-0417 </papid></citsent>
<aftsection>
<nextsent>the basic idea of this approach is to shift the probability values of unlabeled data to the extent such that the class distribution of unlabeled datais identical to the distribution in labeled data (bal anced class in our case).
</nextsent>
<nextsent>in our approach, before model training (m-step?)
</nextsent>
<nextsent>in each iteration, we adjust the posterior probability of each sentence in the following steps: ? transform the posterior probabilities through the inverse function of the sigmoid function.
</nextsent>
<nextsent>the outputs are real values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3005">
<title id=" W11-0612.xml">a survival analysis of fixation times in reading </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>it is also worth pointing out that although we believe that 111 table 1: results of cox proportional hazards model of fixation times in the dundee corpus section 01-16: hazard ratios (hr) and significance levels (p) for all covariates in the model, and for each individual model of reader a-j.
</prevsent>
<prevsent>a c e g i variable hr hr hr hr hr hr hr hr hr hr word length 1.015   .001 0.983   .001 0.979   .001 0.988   .001 0.992   .05 0.992   .01 0.992   .01 0.985   .001 0.990   .01 0.987   .001 word frequency 1.055   .001 1.042   .001 1.036   .001 1.051   .001 1.051   .001 1.014   .001 1.031   .001 1.028   .001 1.040   .001 1.044   .001 bigram probability 1.108   .001 1.196   .1 1.092   .05 1.006   .01 1.013   .05 1.014   .001 0.953 1.011   .001 1.003 1.005   .05 surprisal 1.001 0.986   .001 0.994   .01 0.984   .001 0.998   .01 0.991   .05 1.002 0.994 0.993   .05 0.996   .01 entropy 0.966   .001 0.986   .01 0.980   .001 0.988   .01 0.963   .001 1.002 0.990   .05 0.992   .05 0.969   .001 0.978   .001careful comparison of the results obtained using survival analysis to those reported for other regression methods would be useful and interesting, it is nevertheless beyond the scope of this paper.
</prevsent>
</prevsection>
<citsent citstr=" D09-1034 ">
most of the stimulus variables included in the analysis have been shown to correlate with reading times in other regression studies: the number of letters in the word, the logarithm of the words relative frequency (based on occurrences in the british national corpus), the logarithm of the conditional(bigram) probability of the word (based on occurrences in the google web 1t 5-gram corpus (brantsand franz, 2006)), the syntactic surprisal and entropy scores3 (computed here using the probabilistic pcfg parser by roark et al (2009)).<papid> D09-1034 </papid></citsent>
<aftsection>
<nextsent>the surprisal (hale, 2001) <papid> N01-1021 </papid>at word wi refers to the negative log probability of wi given the preceding words, computed using the prefix probabilities of the parser.</nextsent>
<nextsent>a number of studies have previously established positive relation between surprisal and word-reading times (boston et al, 2008; demberg and keller, 2008; roark et al, 2009).<papid> D09-1034 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3006">
<title id=" W11-0612.xml">a survival analysis of fixation times in reading </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>a c e g i variable hr hr hr hr hr hr hr hr hr hr word length 1.015   .001 0.983   .001 0.979   .001 0.988   .001 0.992   .05 0.992   .01 0.992   .01 0.985   .001 0.990   .01 0.987   .001 word frequency 1.055   .001 1.042   .001 1.036   .001 1.051   .001 1.051   .001 1.014   .001 1.031   .001 1.028   .001 1.040   .001 1.044   .001 bigram probability 1.108   .001 1.196   .1 1.092   .05 1.006   .01 1.013   .05 1.014   .001 0.953 1.011   .001 1.003 1.005   .05 surprisal 1.001 0.986   .001 0.994   .01 0.984   .001 0.998   .01 0.991   .05 1.002 0.994 0.993   .05 0.996   .01 entropy 0.966   .001 0.986   .01 0.980   .001 0.988   .01 0.963   .001 1.002 0.990   .05 0.992   .05 0.969   .001 0.978   .001careful comparison of the results obtained using survival analysis to those reported for other regression methods would be useful and interesting, it is nevertheless beyond the scope of this paper.
</prevsent>
<prevsent>most of the stimulus variables included in the analysis have been shown to correlate with reading times in other regression studies: the number of letters in the word, the logarithm of the words relative frequency (based on occurrences in the british national corpus), the logarithm of the conditional(bigram) probability of the word (based on occurrences in the google web 1t 5-gram corpus (brantsand franz, 2006)), the syntactic surprisal and entropy scores3 (computed here using the probabilistic pcfg parser by roark et al (2009)).<papid> D09-1034 </papid></prevsent>
</prevsection>
<citsent citstr=" N01-1021 ">
the surprisal (hale, 2001) <papid> N01-1021 </papid>at word wi refers to the negative log probability of wi given the preceding words, computed using the prefix probabilities of the parser.</citsent>
<aftsection>
<nextsent>a number of studies have previously established positive relation between surprisal and word-reading times (boston et al, 2008; demberg and keller, 2008; roark et al, 2009).<papid> D09-1034 </papid></nextsent>
<nextsent>the entropy, as quantified here, approximates the structural uncertainty associated with the rest of the sentence, or what is yet to be computed (roark et al, 2009).<papid> D09-1034 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3011">
<title id=" W11-0612.xml">a survival analysis of fixation times in reading </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>0.077 0.077 0.171 0.176 0.226 0.241 0.136 0.143 0.063 0.065 table 2: prediction error on held-out data between the observed survival status and the predicted survival probability at different times t, for kaplan-meier and cox-model adjusted survival, and for all models of readers a-j.
</prevsent>
<prevsent>creased risk by 1-4% per additional unit increase,after adjusting for the effects of the other predictors.
</prevsent>
</prevsection>
<citsent citstr=" W10-2010 ">
while frank (2010) <papid> W10-2010 </papid>recently showed that sentence entropy, i.e. non-structural entropy, accounts for significant fraction of the variance in reading times, our results provide additional support for the influence of structural sentence entropy on reading times.</citsent>
<aftsection>
<nextsent>moreover, it is noteworthy that the effect of entropy appears reliably robust in individual first fixation times, suggesting that the effects of structural processing demands can be immediate rather than delayed in the eye movement record.
</nextsent>
<nextsent>6.1.2 adjusted survival we summarize the results of the evaluation of the adjusted survival function on held-out data in table2 and in table 3.
</nextsent>
<nextsent>table 2 shows the brier score computed at different points in time in the interval 100to 300 ms. results are reported both for the kaplan meier estimate of the survival function and for the fitted cox-models.
</nextsent>
<nextsent>we present the results for each individual model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3012">
<title id=" W10-4010.xml">towards multilingual summarization a comparative analysis of sentence extraction methods on english and hebrew corpora </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>only some of the automated summarization methods proposed in the literature, however, can be defined as language independent?, as they are not based onany morphological analysis of the summarized text.
</prevsent>
<prevsent>in this paper, we perform an in-depth comparative analysis of language-independent sentence scoring methods for extractive single-documentsummarization.
</prevsent>
</prevsection>
<citsent citstr=" P10-1095 ">
we evaluate 15 published summarization methods proposed in the literature and 16 methods introduced in (litvak et al, 2010)<papid> P10-1095 </papid></citsent>
<aftsection>
<nextsent>the evaluation is performed on english and hebrew corpora.
</nextsent>
<nextsent>the results suggest thatthe performance ranking of the compared methods is quite similar in both languages.
</nextsent>
<nextsent>the top ten bilingual scoring methods include six methods introduced in (litvak et al, 2010)<papid> P10-1095 </papid></nextsent>
<nextsent>automatically generated summaries can significantly reduce the information overload on professionals in variety of fields, could prove beneficial for the automated classification and filtering of documents, the search for information over the internet and applications that utilize large textual databases.document summarization methodologies include statistic-based, using either the classic vector space model or graph representation, andsemantic-based, using ontologies and language specific knowledge (mani &amp; maybury, 1999).although the use of language-specific knowledge can potentially improve the quality of automated summaries generated in particular language, its language specificity ultimately restricts the use of such summarizer to single language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3026">
<title id=" W10-4010.xml">towards multilingual summarization a comparative analysis of sentence extraction methods on english and hebrew corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>gong and liu (2001) and steinberger and jezek (2004) showed that singular value decomposition (svd) can be applied to generate extracts.among text representation models, graph based text representations have gained popularity in automated summarization, as they enable the model to be enriched with syntactic and semantic relations.
</prevsent>
<prevsent>salton et al (1997) were among the first to attempt graph-based ranking methods for single document extractive summarization by generating similarity links between document paragraphs.
</prevsent>
</prevsection>
<citsent citstr=" P05-3013 ">
the important paragraphs of text were extracted using degree scores.erkan and radev (2004) and mihalcea (2005) <papid> P05-3013 </papid>introduced approaches for unsupervised extractive summarization that relyon the application of iterative graph based ranking algorithms.</citsent>
<aftsection>
<nextsent>in their approaches, each document is represented as graph of sentences interconnected by similarity relations.
</nextsent>
<nextsent>methods for sentence extraction various language dependent and independent sentence scoring methods have been introduced in the literature.
</nextsent>
<nextsent>we selected the 15 most prominent language independent methods for evaluation.
</nextsent>
<nextsent>most of them can be categorized as frequency, position, length, or title-based, and they utilize vector representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3047">
<title id=" W10-4010.xml">towards multilingual summarization a comparative analysis of sentence extraction methods on english and hebrew corpora </title>
<section> language-independent scoring.  </section>
<citcontext>
<prevsection>
<prevsent>pr average page rank for all sentence nodes: score(s) = ? i?{words(s)} pri |s| .?
</prevsent>
<prevsent>similarity-based.
</prevsent>
</prevsection>
<citsent citstr=" W06-3805 ">
edge matching techniques similar to those of nastase and szpakowicz (2006) <papid> W06-3805 </papid>are used.</citsent>
<aftsection>
<nextsent>edge matching is an alternative approach to measure the similarity between graphs based on the number of common edges:title o graph-based extension of title ? overlap-based edge matching between title and sentence graphs.
</nextsent>
<nextsent>title j graph-based extension of title ? jaccard-based edge matching between title and sentence graphs.
</nextsent>
<nextsent>d cov o graph-based extension of cov ? overlap-based edge matching between sentence and document complement (the rest of document sentences) graphs.
</nextsent>
<nextsent>d cov j graph-based extension of cov ? jaccard-based edge matching 5using undirected word graphs with page rank does not make sense, since for an undirected graph node pagerankscore is known to be proportional to its degree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3056">
<title id=" W11-1302.xml">distributed structures and distributional meaning </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>we propose pure distributed tree (dt) and distributional distributed tree (ddt).
</prevsent>
<prevsent>dtsand ddts are exploited for defining distributed tree kernels (dtks) and distributional distributed tree kernels (ddtks).
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
we compare dtks and ddtks in two tasks: approximating tree kernels tk (collins and duffy,2002); <papid> P02-1034 </papid>performing textual entailment recognition (rte).</citsent>
<aftsection>
<nextsent>results show that dtks correlate with tks and perform in rte better thanddtks.
</nextsent>
<nextsent>then, including distributional vectors in distributed structures is very difficult task.
</nextsent>
<nextsent>demonstrating that distributional semantics is semantic model of natural language is real research challenge in natural language processing.
</nextsent>
<nextsent>fregesprinciple of compositionality (frege, 1884), naturally taken into account in logic-based semantic models of natural language (montague, 1974), is hardly effectively included in distributional semantics models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3057">
<title id=" W11-1302.xml">distributed structures and distributional meaning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fregesprinciple of compositionality (frege, 1884), naturally taken into account in logic-based semantic models of natural language (montague, 1974), is hardly effectively included in distributional semantics models.
</prevsent>
<prevsent>these models should compositionally derive distributional vectors for sentences and phrases from the distributional vectors of the composing words.
</prevsent>
</prevsection>
<citsent citstr=" W10-2805 ">
besides vector averaging (landauer and dumais,1997; foltz et al, 1998), that can model distributional meaning of sentences, recent distributional compositional models focus on finding distributional vectors of word pairs (mitchell and lapata, 2010; guevara, 2010; <papid> W10-2805 </papid>baroni and zamparelli, 2010;<papid> D10-1115 </papid>zanzotto et al, 2010).<papid> C10-1142 </papid></citsent>
<aftsection>
<nextsent>scaling up these 2-word sequence models to the sentence level is not trivial as syntactic structure of sentences plays very important role.
</nextsent>
<nextsent>understanding the relation between the structure and the meaning is needed for building distributional compositional models for sentences.
</nextsent>
<nextsent>research in distributed representations (dr) (hinton et al, 1986) proposed models and methods for encoding data structures in vectors, matrices, or high-order tensors.
</nextsent>
<nextsent>distributed representations are oriented to preserve the structural information in the final representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3058">
<title id=" W11-1302.xml">distributed structures and distributional meaning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fregesprinciple of compositionality (frege, 1884), naturally taken into account in logic-based semantic models of natural language (montague, 1974), is hardly effectively included in distributional semantics models.
</prevsent>
<prevsent>these models should compositionally derive distributional vectors for sentences and phrases from the distributional vectors of the composing words.
</prevsent>
</prevsection>
<citsent citstr=" D10-1115 ">
besides vector averaging (landauer and dumais,1997; foltz et al, 1998), that can model distributional meaning of sentences, recent distributional compositional models focus on finding distributional vectors of word pairs (mitchell and lapata, 2010; guevara, 2010; <papid> W10-2805 </papid>baroni and zamparelli, 2010;<papid> D10-1115 </papid>zanzotto et al, 2010).<papid> C10-1142 </papid></citsent>
<aftsection>
<nextsent>scaling up these 2-word sequence models to the sentence level is not trivial as syntactic structure of sentences plays very important role.
</nextsent>
<nextsent>understanding the relation between the structure and the meaning is needed for building distributional compositional models for sentences.
</nextsent>
<nextsent>research in distributed representations (dr) (hinton et al, 1986) proposed models and methods for encoding data structures in vectors, matrices, or high-order tensors.
</nextsent>
<nextsent>distributed representations are oriented to preserve the structural information in the final representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3059">
<title id=" W11-1302.xml">distributed structures and distributional meaning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fregesprinciple of compositionality (frege, 1884), naturally taken into account in logic-based semantic models of natural language (montague, 1974), is hardly effectively included in distributional semantics models.
</prevsent>
<prevsent>these models should compositionally derive distributional vectors for sentences and phrases from the distributional vectors of the composing words.
</prevsent>
</prevsection>
<citsent citstr=" C10-1142 ">
besides vector averaging (landauer and dumais,1997; foltz et al, 1998), that can model distributional meaning of sentences, recent distributional compositional models focus on finding distributional vectors of word pairs (mitchell and lapata, 2010; guevara, 2010; <papid> W10-2805 </papid>baroni and zamparelli, 2010;<papid> D10-1115 </papid>zanzotto et al, 2010).<papid> C10-1142 </papid></citsent>
<aftsection>
<nextsent>scaling up these 2-word sequence models to the sentence level is not trivial as syntactic structure of sentences plays very important role.
</nextsent>
<nextsent>understanding the relation between the structure and the meaning is needed for building distributional compositional models for sentences.
</nextsent>
<nextsent>research in distributed representations (dr) (hinton et al, 1986) proposed models and methods for encoding data structures in vectors, matrices, or high-order tensors.
</nextsent>
<nextsent>distributed representations are oriented to preserve the structural information in the final representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3062">
<title id=" W11-1302.xml">distributed structures and distributional meaning </title>
<section> distributed trees and distributional.  </section>
<citcontext>
<prevsection>
<prevsent>3.2.1 entailment-specific kernels recognizing textual entailment (rte) is acomplex semantic task often interpreted as classification task.
</prevsent>
<prevsent>given the text and the hypothesis determine whether or not entails . for applying the previous kernels to this classification task, we need to define specific class of kernels.
</prevsent>
</prevsection>
<citsent citstr=" P06-1051 ">
as in (zanzotto and moschitti, 2006; <papid> P06-1051 </papid>wang and neumann, 2007; zanzotto et al, 2009), we encode the text and the hypothesis in two separate syntactic feature spaces.</citsent>
<aftsection>
<nextsent>then, given two pairs of text-hypothesis p1 = (t1, h1) and p2 = (t2, h2), the prototypical kernel pk is written as follows: pk(p1, p2) = k(t1, t2) +k(h1, h2) (5)where k(?, ?) is generic kernel.
</nextsent>
<nextsent>we will then experiment with different pk kernels obtained using: the original tree kernel function (tk) (collins and duffy, 2002), <papid> P02-1034 </papid>dtk, and ddtk.</nextsent>
<nextsent>along with the previous task specific kernels, weuse simpler feature (lex) that is extremely effective in determining the entailment between andh . this simple feature is the lexical similarity between andh computed using wordnet-based metrics asin (corley and mihalcea, 2005).<papid> W05-1203 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3065">
<title id=" W11-1302.xml">distributed structures and distributional meaning </title>
<section> distributed trees and distributional.  </section>
<citcontext>
<prevsection>
<prevsent>then, given two pairs of text-hypothesis p1 = (t1, h1) and p2 = (t2, h2), the prototypical kernel pk is written as follows: pk(p1, p2) = k(t1, t2) +k(h1, h2) (5)where k(?, ?) is generic kernel.
</prevsent>
<prevsent>we will then experiment with different pk kernels obtained using: the original tree kernel function (tk) (collins and duffy, 2002), <papid> P02-1034 </papid>dtk, and ddtk.</prevsent>
</prevsection>
<citsent citstr=" W05-1203 ">
along with the previous task specific kernels, weuse simpler feature (lex) that is extremely effective in determining the entailment between andh . this simple feature is the lexical similarity between andh computed using wordnet-based metrics asin (corley and mihalcea, 2005).<papid> W05-1203 </papid></citsent>
<aftsection>
<nextsent>this feature, here after called lex, encodes the similarity between andh , i.e., sim(t,h).
</nextsent>
<nextsent>this feature is used alone or in combination with the previous kernels and it gives an important boost to their performances.
</nextsent>
<nextsent>in the task experiment, we will then also have: lex+tk, lex+dtk, and lex+ddtk.
</nextsent>
<nextsent>in this section, we experiment with the distributed tree kernels (dtk) and the distributional distributed tree kernels (ddtk) in order to understand whether or not the syntactic structure and the distributional meaning can be easily encoded in the distributed trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3067">
<title id=" W11-1302.xml">distributed structures and distributional meaning </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>12 4.1 experimental set-up.
</prevsent>
<prevsent>we have the double aim of producing direct comparison of how the distributed tree kernel (dtk) is approximating the original tree kernel (tk) and atask based comparison for assessing if the approximation is enough effective to similarly solve the task that is textual entailment recognition.
</prevsent>
</prevsection>
<citsent citstr=" W07-1401 ">
for both experimental settings, we take the recognizing textual entailment sets ranging from the first challenge (rte-1) to the fifth (rte-5) (dagan et al, 2006; bar-haim et al, 2006; giampiccolo et al, 2007; <papid> W07-1401 </papid>bentivogli et al, 2009).</citsent>
<aftsection>
<nextsent>the distributional vectors used for ddtk have been obtained by an lsa reduction of the word-by word cooccurrence matrix generated on the ukwaccorpus (ferraresi et al, 2008), using context window of size 3.
</nextsent>
<nextsent>an appropriate size for the lsa reduction was deemed to be 250.
</nextsent>
<nextsent>thus, in the experiments we used 250 dimensions both for distributional and random vectors, to allow correct comparison between dtk and ddtk models.for the direct comparison, we used tree pairs derived from the rte sets.
</nextsent>
<nextsent>each pair is derived from t-h pair where and are syntactically analyzed and each rte set produces the corresponding set oftree pairs, e.g., the development set of rte1 produces set of 567 tree pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3068">
<title id=" W11-1302.xml">distributed structures and distributional meaning </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>higher correlation corresponds to better approximation of tk.
</prevsent>
<prevsent>for the task driven comparison, we experimented with the datasets in the classical learning setting: the development set is used as training set and the final classifier is tested on the testing set.
</prevsent>
</prevsection>
<citsent citstr=" E06-1015 ">
we used support vector machine (joachims, 1999) with an implementation of the original tree kernel (moschitti, 2006).<papid> E06-1015 </papid></citsent>
<aftsection>
<nextsent>the classifiers are evaluated according to the accuracy of the classification decision on the testing set, i.e., the ratio of the correct decisions over all the decisions to take.
</nextsent>
<nextsent>average spear mans correlation dtk 0.8335 ddtk 0.7641table 1: average spear mans correlations of the tree kernel (tk) with the distributed tree kernel (dtk) and the distributed distributional tree kernel (ddtk) in vector space with 250 dimensions avg rte1 rte2 rte3 rte5 tk 55.02% 55.50% 53.38% 55.88% 55.33% dtk 55.63% 57.25% 54.88% 54.38% 56.00% ddtk 55.11% 54.00% 53.88% 55.38% 57.17% lex+tk 62.11% 59.75% 61.25% 66.62% 60.83% lex+dtk 63.25% 61.12% 62.12% 66.25% 63.50% lex+ddtk 62.90% 60.62% 61.25% 66.38% 63.33%table 2: accuracies of the different methods on the textual entailment recognition task 4.2 experimental results.
</nextsent>
<nextsent>in the first experiment of this set, we want to investigate which one between dtk and ddtk correlates better with original tk.
</nextsent>
<nextsent>table 1 reports the spear mans correlations of tree kernels with dtk and ddtk in vector space with 250 dimensions.these correlations are obtained averaging the correlations over the 9 rte sets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3069">
<title id=" W11-1302.xml">distributed structures and distributional meaning </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>compositional distributional semantics poses the same issue.
</prevsent>
<prevsent>we empirically showed that methodology for including distributional meaning in distributed representation is possible, but it must be furtherly developed to be an added value.
</prevsent>
</prevsection>
<citsent citstr=" N10-1146 ">
distributional semantics has been positively added in traditional tree kernels (mehdad et al, 2010).<papid> N10-1146 </papid></citsent>
<aftsection>
<nextsent>yet, the specific requirement of distributed tree kernels (i.e., the orthogonality of the vectors) reduces this positive effect.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3070">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another very valuable resource for smt studies, especially for under-resource languages, are comparable corpora, made of pairs of monolingual corpora that contain texts of similar genres, from similar periods, and/or about similar topics.
</prevsent>
<prevsent>the potential of comparable corpora has long been established as useful source from which to extract bilingual word dictionaries (see eg.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
(rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998)) <papid> P98-1069 </papid>or to learn multilingual terms (see e.g.</citsent>
<aftsection>
<nextsent>(lange?, 1995; smadja et al, 1996)).<papid> J96-1001 </papid></nextsent>
<nextsent>more recently, the relative corpus has caused the usefulness of comparable corpora be reevaluated as potential source of parallel fragments, be they paragraphs, sentences, phrases, terms, chunks, or isolated words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3071">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another very valuable resource for smt studies, especially for under-resource languages, are comparable corpora, made of pairs of monolingual corpora that contain texts of similar genres, from similar periods, and/or about similar topics.
</prevsent>
<prevsent>the potential of comparable corpora has long been established as useful source from which to extract bilingual word dictionaries (see eg.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
(rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998)) <papid> P98-1069 </papid>or to learn multilingual terms (see e.g.</citsent>
<aftsection>
<nextsent>(lange?, 1995; smadja et al, 1996)).<papid> J96-1001 </papid></nextsent>
<nextsent>more recently, the relative corpus has caused the usefulness of comparable corpora be reevaluated as potential source of parallel fragments, be they paragraphs, sentences, phrases, terms, chunks, or isolated words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3072">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the potential of comparable corpora has long been established as useful source from which to extract bilingual word dictionaries (see eg.
</prevsent>
<prevsent>(rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998)) <papid> P98-1069 </papid>or to learn multilingual terms (see e.g.</prevsent>
</prevsection>
<citsent citstr=" J96-1001 ">
(lange?, 1995; smadja et al, 1996)).<papid> J96-1001 </papid></citsent>
<aftsection>
<nextsent>more recently, the relative corpus has caused the usefulness of comparable corpora be reevaluated as potential source of parallel fragments, be they paragraphs, sentences, phrases, terms, chunks, or isolated words.
</nextsent>
<nextsent>this tendency is illustrated by the work of e.g.
</nextsent>
<nextsent>(resnik and smith, 2003; <papid> J03-3002 </papid>munteanu and marcu, 2005), <papid> J05-4003 </papid>which combines information retrieval techniques (to identify parallel documents) and sentence similarity detection to detect parallel sentences.there are many other ways to improve smt models with comparable or monolingual data.</nextsent>
<nextsent>for instance, the work reported in (schwenk, 2008) draws inspiration from recent advances in unsupervised training of acoustic models for speech recognition and proposes to use self-training on in-domain data to adapt and improve baseline system trained mostly with out-of-domain data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3073">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more recently, the relative corpus has caused the usefulness of comparable corpora be reevaluated as potential source of parallel fragments, be they paragraphs, sentences, phrases, terms, chunks, or isolated words.
</prevsent>
<prevsent>this tendency is illustrated by the work of e.g.
</prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
(resnik and smith, 2003; <papid> J03-3002 </papid>munteanu and marcu, 2005), <papid> J05-4003 </papid>which combines information retrieval techniques (to identify parallel documents) and sentence similarity detection to detect parallel sentences.there are many other ways to improve smt models with comparable or monolingual data.</citsent>
<aftsection>
<nextsent>for instance, the work reported in (schwenk, 2008) draws inspiration from recent advances in unsupervised training of acoustic models for speech recognition and proposes to use self-training on in-domain data to adapt and improve baseline system trained mostly with out-of-domain data.
</nextsent>
<nextsent>as discussed e.g. in (fung and cheung, 2004),<papid> C04-1151 </papid>comparable corpora are of various nature: there exists continuum between truly parallel and completely unrelated texts.</nextsent>
<nextsent>algorithms for exploiting comparable corpora should thus be tailored to the peculiarities of the data on which they are applied.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3074">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more recently, the relative corpus has caused the usefulness of comparable corpora be reevaluated as potential source of parallel fragments, be they paragraphs, sentences, phrases, terms, chunks, or isolated words.
</prevsent>
<prevsent>this tendency is illustrated by the work of e.g.
</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
(resnik and smith, 2003; <papid> J03-3002 </papid>munteanu and marcu, 2005), <papid> J05-4003 </papid>which combines information retrieval techniques (to identify parallel documents) and sentence similarity detection to detect parallel sentences.there are many other ways to improve smt models with comparable or monolingual data.</citsent>
<aftsection>
<nextsent>for instance, the work reported in (schwenk, 2008) draws inspiration from recent advances in unsupervised training of acoustic models for speech recognition and proposes to use self-training on in-domain data to adapt and improve baseline system trained mostly with out-of-domain data.
</nextsent>
<nextsent>as discussed e.g. in (fung and cheung, 2004),<papid> C04-1151 </papid>comparable corpora are of various nature: there exists continuum between truly parallel and completely unrelated texts.</nextsent>
<nextsent>algorithms for exploiting comparable corpora should thus be tailored to the peculiarities of the data on which they are applied.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3075">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(resnik and smith, 2003; <papid> J03-3002 </papid>munteanu and marcu, 2005), <papid> J05-4003 </papid>which combines information retrieval techniques (to identify parallel documents) and sentence similarity detection to detect parallel sentences.there are many other ways to improve smt models with comparable or monolingual data.</prevsent>
<prevsent>for instance, the work reported in (schwenk, 2008) draws inspiration from recent advances in unsupervised training of acoustic models for speech recognition and proposes to use self-training on in-domain data to adapt and improve baseline system trained mostly with out-of-domain data.</prevsent>
</prevsection>
<citsent citstr=" C04-1151 ">
as discussed e.g. in (fung and cheung, 2004),<papid> C04-1151 </papid>comparable corpora are of various nature: there exists continuum between truly parallel and completely unrelated texts.</citsent>
<aftsection>
<nextsent>algorithms for exploiting comparable corpora should thus be tailored to the peculiarities of the data on which they are applied.
</nextsent>
<nextsent>in this paper, we report on experiments aimed atusing noisy parallel corpus made out of news stories in french and arabic in two different ways: first,to extract new, in-domain, parallel sentences; second, to adapt our translation and language models.this approach is made possible due to the specificities of our corpus.
</nextsent>
<nextsent>in fact, our work is part of aproject aiming at developing platform for processing multimedia news documents (texts, interviews, images and videos) in arabic, so as to streamline the 44 proceedings of the 4th workshop on building and using comparable corpora, pages 4451, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.
</nextsent>
<nextsent>c2011 association for computational linguistics work of major international newsagency.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3076">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>from birds eye view, attempts to use comparable corpora in smt fall into two main categories: first, approaches aimed at extracting parallel fragments;second, approaches aimed at adapting existing resources to new domain.
</prevsent>
<prevsent>2.1 extracting parallel fragments.
</prevsent>
</prevsection>
<citsent citstr=" N09-2024 ">
most attempts at automatically extracting parallel fragments use two step process (see (tillmann and xu, 2009) <papid> N09-2024 </papid>for counter-example): set of candidate parallel texts is first identified; within this short list of possibly paired texts, parallel sentences are then identified based on some similarity score.the work reported in (zhao and vogel, 2002) concentrates on finding parallel sentences in set of comparable stories pairs in chinese/english.</citsent>
<aftsection>
<nextsent>sentence similarity derives from probabilistic alignment model for documents, which enables to recognize parallel sentences based on their length ratio,as well as on the ibm 1 model score of their word to-word alignment.
</nextsent>
<nextsent>to account for various levels of parallelism, the model allows some sentences in the source or target language to remain unaligned.
</nextsent>
<nextsent>the work of (resnik and smith, 2003) <papid> J03-3002 </papid>considers mining much larger corpora?</nextsent>
<nextsent>consisting of documents collected on the internet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3080">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>matched document sand sentences are primarily detected based on surface and/or formal similarity of the web addresses or of the page internal structure.
</prevsent>
<prevsent>this line of work is developed notably in (munteanu and marcu, 2005)<papid> J05-4003 </papid>: <papid> J05-4003 </papid>candidate parallel texts are found using cross-lingual information retrieval (clir) techniques; sentence similarity is indirectly computed using logistic regression model aimed at detecting parallel sentences.</prevsent>
</prevsection>
<citsent citstr=" N10-1063 ">
this formalism allows to enrich baseline features such as the length ratio, the word-to-word (ibm 1) alignment scores with supplementary scores aimed at rewarding sentences containing identical words, etc. more recently, (smith et al, 2010) <papid> N10-1063 </papid>reported significant improvements mining parallel wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating richer set of features and cross-sentence dependencies within conditional random fields (crfs) model.</citsent>
<aftsection>
<nextsent>for lack of find enough parallel sentences, (munteanu and marcu, 2006; <papid> P06-1011 </papid>kumano and tokunaga, 2007) consider the more difficult issue of mining parallel phrases.</nextsent>
<nextsent>in (abdul-rauf and schwenk, 2009), the authors, rather than computing similarity score between asource and target sentence, propose to use an existing translation engine to process the source side of the corpus, thus enabling sentence comparison to be performed in the target language, using the edit distance or variants thereof (wer or ter).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3081">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this line of work is developed notably in (munteanu and marcu, 2005)<papid> J05-4003 </papid>: <papid> J05-4003 </papid>candidate parallel texts are found using cross-lingual information retrieval (clir) techniques; sentence similarity is indirectly computed using logistic regression model aimed at detecting parallel sentences.</prevsent>
<prevsent>this formalism allows to enrich baseline features such as the length ratio, the word-to-word (ibm 1) alignment scores with supplementary scores aimed at rewarding sentences containing identical words, etc. more recently, (smith et al, 2010) <papid> N10-1063 </papid>reported significant improvements mining parallel wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating richer set of features and cross-sentence dependencies within conditional random fields (crfs) model.</prevsent>
</prevsection>
<citsent citstr=" P06-1011 ">
for lack of find enough parallel sentences, (munteanu and marcu, 2006; <papid> P06-1011 </papid>kumano and tokunaga, 2007) consider the more difficult issue of mining parallel phrases.</citsent>
<aftsection>
<nextsent>in (abdul-rauf and schwenk, 2009), the authors, rather than computing similarity score between asource and target sentence, propose to use an existing translation engine to process the source side of the corpus, thus enabling sentence comparison to be performed in the target language, using the edit distance or variants thereof (wer or ter).
</nextsent>
<nextsent>this approach is generalized to much larger collections in (uszkoreit et al, 2010), <papid> C10-1124 </papid>which draw advanta geof working in one language to adopt efficient parallelism detection techniques (broder, 2000).</nextsent>
<nextsent>2.2 comparable corpora for adaptation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3082">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for lack of find enough parallel sentences, (munteanu and marcu, 2006; <papid> P06-1011 </papid>kumano and tokunaga, 2007) consider the more difficult issue of mining parallel phrases.</prevsent>
<prevsent>in (abdul-rauf and schwenk, 2009), the authors, rather than computing similarity score between asource and target sentence, propose to use an existing translation engine to process the source side of the corpus, thus enabling sentence comparison to be performed in the target language, using the edit distance or variants thereof (wer or ter).</prevsent>
</prevsection>
<citsent citstr=" C10-1124 ">
this approach is generalized to much larger collections in (uszkoreit et al, 2010), <papid> C10-1124 </papid>which draw advanta geof working in one language to adopt efficient parallelism detection techniques (broder, 2000).</citsent>
<aftsection>
<nextsent>2.2 comparable corpora for adaptation.
</nextsent>
<nextsent>another very productive use of comparable corpor ais to adaptor specialize existing resources (dictionaries, translation models, language models) to specific domains and/or genres.
</nextsent>
<nextsent>we will only focus here on adapting the translation model; review of the literature on language model adaptation is in (bella garda, 2001) and the references cited therein.
</nextsent>
<nextsent>45 figure 1: extraction of parallel corpora the work in (snover et al, 2008) <papid> D08-1090 </papid>is first step towards augmenting the translation model with new translation rules: these rules associate, with tiny probability, every phrase in source document withthe most frequent target phrases found in comparable corpus specifically built for this document.the study in (schwenk, 2008) considers self training, which allows to adapt an existing system to new domains using monolingual (source) data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3083">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another very productive use of comparable corpor ais to adaptor specialize existing resources (dictionaries, translation models, language models) to specific domains and/or genres.
</prevsent>
<prevsent>we will only focus here on adapting the translation model; review of the literature on language model adaptation is in (bella garda, 2001) and the references cited therein.
</prevsent>
</prevsection>
<citsent citstr=" D08-1090 ">
45 figure 1: extraction of parallel corpora the work in (snover et al, 2008) <papid> D08-1090 </papid>is first step towards augmenting the translation model with new translation rules: these rules associate, with tiny probability, every phrase in source document withthe most frequent target phrases found in comparable corpus specifically built for this document.the study in (schwenk, 2008) considers self training, which allows to adapt an existing system to new domains using monolingual (source) data.</citsent>
<aftsection>
<nextsent>the idea is to automatically translate the source side of an in-domain corpus using reference translation system.
</nextsent>
<nextsent>then, according to some confidence score,the best translations are selected to form an adaptation corpus, which can serve to retrain the translation model.
</nextsent>
<nextsent>the authors of (cettolo et al, 2010) follow similar goals with different means: here, the baseline translation model is used to obtain phrase alignment between source and target sentences incomparable corpus.
</nextsent>
<nextsent>these phrase alignments are further refined, before new phrases not in the original phrase-table, can be collected.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3085">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> domain adaptation.  </section>
<citcontext>
<prevsection>
<prevsent>this means that we have three parallel corpora at our disposal: ? the baseline training corpus, which is large(a hundred million words), delivering reasonable translation performance quality of translation, but out-of-domain; ? the extracted in-domain corpus, which is much smaller, and potentially noisy; ? the translated in-domain corpus, which is of medium-size, and much worse in quality than the others.considering these three corpora, different adaptation methods of the translation models are explored.
</prevsent>
<prevsent>the first approach is to concatenate the baseline andin-domain training data (either extracted or trans lated) to train new translation model.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
given the difference in size between the two corpus, this approach may introduce bias in the translation model in favor of out-of-domain.the second approach is to train separate translation models with baseline on the one hand, and within-domain on the other data and to weight their combination with mert (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>this alleviates the former problem but increases the number of features that need to be trained, running the risk to make mert less stable.
</nextsent>
<nextsent>a last approach is also considered, which consist sin using only the in-domain data to train the translation model.
</nextsent>
<nextsent>in that case, the question is the small size of the in-domain data.the comparative experiments on the three approaches, using the three corpora are described in next section.
</nextsent>
<nextsent>5.1 context and data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3086">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>then, the arabic data are segmented into sentences.
</prevsent>
<prevsent>a crf-based sentence segmenter for arabic was built with the wapiti3 (lavergne et al, 2010) package.
</prevsent>
</prevsection>
<citsent citstr=" N06-2013 ">
a morphological analysis of the arabic text is then done using the arabic morphological analyzer and disambiguation tool mada (nizar habash and roth, 2009), with the mada-d2 since it seems to be the most efficient scheme for large data (habash and sadat, 2006).<papid> N06-2013 </papid></citsent>
<aftsection>
<nextsent>the preprocessed arabic and french data were aligned using mgiza++4 (gao and vogel, 2008).<papid> W08-0509 </papid></nextsent>
<nextsent>the moses toolkit (koehn et al, 2007) <papid> P07-2045 </papid>is then usedto make the alignments symmetric using the grow diag-final-and heuristic and to extract phrases with maximum length of 7 words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3087">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>a crf-based sentence segmenter for arabic was built with the wapiti3 (lavergne et al, 2010) package.
</prevsent>
<prevsent>a morphological analysis of the arabic text is then done using the arabic morphological analyzer and disambiguation tool mada (nizar habash and roth, 2009), with the mada-d2 since it seems to be the most efficient scheme for large data (habash and sadat, 2006).<papid> N06-2013 </papid></prevsent>
</prevsection>
<citsent citstr=" W08-0509 ">
the preprocessed arabic and french data were aligned using mgiza++4 (gao and vogel, 2008).<papid> W08-0509 </papid></citsent>
<aftsection>
<nextsent>the moses toolkit (koehn et al, 2007) <papid> P07-2045 </papid>is then usedto make the alignments symmetric using the grow diag-final-and heuristic and to extract phrases with maximum length of 7 words.</nextsent>
<nextsent>a distortion model lexically conditioned on both the arabic phrases and french phrases is then trained.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3088">
<title id=" W11-1207.xml">two ways to use a noisy parallel news corpus for improving statistical machine translation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>a morphological analysis of the arabic text is then done using the arabic morphological analyzer and disambiguation tool mada (nizar habash and roth, 2009), with the mada-d2 since it seems to be the most efficient scheme for large data (habash and sadat, 2006).<papid> N06-2013 </papid></prevsent>
<prevsent>the preprocessed arabic and french data were aligned using mgiza++4 (gao and vogel, 2008).<papid> W08-0509 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the moses toolkit (koehn et al, 2007) <papid> P07-2045 </papid>is then usedto make the alignments symmetric using the grow diag-final-and heuristic and to extract phrases with maximum length of 7 words.</citsent>
<aftsection>
<nextsent>a distortion model lexically conditioned on both the arabic phrases and french phrases is then trained.
</nextsent>
<nextsent>feature weights wereset by running mert (och, 2003) <papid> P03-1021 </papid>on the development set.</nextsent>
<nextsent>5.3 extraction of the in-domain parallel corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3090">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unsupervised dependency parsers do not achieve the same quality as supervised or semi-supervised parsers, but in some situations precision may be less important compared to the cost of producing manually annotated data.
</prevsent>
<prevsent>moreover, unsupervised dependency parsing is attractive from theoretical pointof view as it does not relyon particular style of annotation and may potentially provide insights about the difficulties of human language learning.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
unsupervised dependency parsing has seen rapid progress recently, with error reductions on english (marcus et al, 1993) <papid> J93-2004 </papid>of about 15% in six years (klein and manning, 2004; <papid> P04-1061 </papid>spitkovsky et al, 2010), <papid> W10-2902 </papid>and better and better results for other languages (gillenwater et al, 2010; <papid> P10-2036 </papid>naseem et al, 2010), <papid> D10-1120 </papid>but results are still far from what can be achieved with small seeds, language-specific rules (druck et al., 2009) <papid> P09-1041 </papid>or using cross-language adaptation (smith and eisner, 2009; <papid> D09-1086 </papid>spreyer et al, 2010).</citsent>
<aftsection>
<nextsent>the standard method in unsupervised dependency parsing is to optimize the overall probability of the corpus by assigning trees to its sentences that capture general patterns in the distribution of part-of speech (pos).
</nextsent>
<nextsent>this happens in several iterations over the corpus.
</nextsent>
<nextsent>this method requires clever initialization, which can be seen as kind of minimal supervision.
</nextsent>
<nextsent>state-of-the-art unsupervised dependency parsers, except seginer (2007), <papid> P07-1049 </papid>also relyon manually annotated text or text processed by supervised pos taggers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3091">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unsupervised dependency parsers do not achieve the same quality as supervised or semi-supervised parsers, but in some situations precision may be less important compared to the cost of producing manually annotated data.
</prevsent>
<prevsent>moreover, unsupervised dependency parsing is attractive from theoretical pointof view as it does not relyon particular style of annotation and may potentially provide insights about the difficulties of human language learning.
</prevsent>
</prevsection>
<citsent citstr=" P04-1061 ">
unsupervised dependency parsing has seen rapid progress recently, with error reductions on english (marcus et al, 1993) <papid> J93-2004 </papid>of about 15% in six years (klein and manning, 2004; <papid> P04-1061 </papid>spitkovsky et al, 2010), <papid> W10-2902 </papid>and better and better results for other languages (gillenwater et al, 2010; <papid> P10-2036 </papid>naseem et al, 2010), <papid> D10-1120 </papid>but results are still far from what can be achieved with small seeds, language-specific rules (druck et al., 2009) <papid> P09-1041 </papid>or using cross-language adaptation (smith and eisner, 2009; <papid> D09-1086 </papid>spreyer et al, 2010).</citsent>
<aftsection>
<nextsent>the standard method in unsupervised dependency parsing is to optimize the overall probability of the corpus by assigning trees to its sentences that capture general patterns in the distribution of part-of speech (pos).
</nextsent>
<nextsent>this happens in several iterations over the corpus.
</nextsent>
<nextsent>this method requires clever initialization, which can be seen as kind of minimal supervision.
</nextsent>
<nextsent>state-of-the-art unsupervised dependency parsers, except seginer (2007), <papid> P07-1049 </papid>also relyon manually annotated text or text processed by supervised pos taggers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3092">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unsupervised dependency parsers do not achieve the same quality as supervised or semi-supervised parsers, but in some situations precision may be less important compared to the cost of producing manually annotated data.
</prevsent>
<prevsent>moreover, unsupervised dependency parsing is attractive from theoretical pointof view as it does not relyon particular style of annotation and may potentially provide insights about the difficulties of human language learning.
</prevsent>
</prevsection>
<citsent citstr=" W10-2902 ">
unsupervised dependency parsing has seen rapid progress recently, with error reductions on english (marcus et al, 1993) <papid> J93-2004 </papid>of about 15% in six years (klein and manning, 2004; <papid> P04-1061 </papid>spitkovsky et al, 2010), <papid> W10-2902 </papid>and better and better results for other languages (gillenwater et al, 2010; <papid> P10-2036 </papid>naseem et al, 2010), <papid> D10-1120 </papid>but results are still far from what can be achieved with small seeds, language-specific rules (druck et al., 2009) <papid> P09-1041 </papid>or using cross-language adaptation (smith and eisner, 2009; <papid> D09-1086 </papid>spreyer et al, 2010).</citsent>
<aftsection>
<nextsent>the standard method in unsupervised dependency parsing is to optimize the overall probability of the corpus by assigning trees to its sentences that capture general patterns in the distribution of part-of speech (pos).
</nextsent>
<nextsent>this happens in several iterations over the corpus.
</nextsent>
<nextsent>this method requires clever initialization, which can be seen as kind of minimal supervision.
</nextsent>
<nextsent>state-of-the-art unsupervised dependency parsers, except seginer (2007), <papid> P07-1049 </papid>also relyon manually annotated text or text processed by supervised pos taggers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3093">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unsupervised dependency parsers do not achieve the same quality as supervised or semi-supervised parsers, but in some situations precision may be less important compared to the cost of producing manually annotated data.
</prevsent>
<prevsent>moreover, unsupervised dependency parsing is attractive from theoretical pointof view as it does not relyon particular style of annotation and may potentially provide insights about the difficulties of human language learning.
</prevsent>
</prevsection>
<citsent citstr=" P10-2036 ">
unsupervised dependency parsing has seen rapid progress recently, with error reductions on english (marcus et al, 1993) <papid> J93-2004 </papid>of about 15% in six years (klein and manning, 2004; <papid> P04-1061 </papid>spitkovsky et al, 2010), <papid> W10-2902 </papid>and better and better results for other languages (gillenwater et al, 2010; <papid> P10-2036 </papid>naseem et al, 2010), <papid> D10-1120 </papid>but results are still far from what can be achieved with small seeds, language-specific rules (druck et al., 2009) <papid> P09-1041 </papid>or using cross-language adaptation (smith and eisner, 2009; <papid> D09-1086 </papid>spreyer et al, 2010).</citsent>
<aftsection>
<nextsent>the standard method in unsupervised dependency parsing is to optimize the overall probability of the corpus by assigning trees to its sentences that capture general patterns in the distribution of part-of speech (pos).
</nextsent>
<nextsent>this happens in several iterations over the corpus.
</nextsent>
<nextsent>this method requires clever initialization, which can be seen as kind of minimal supervision.
</nextsent>
<nextsent>state-of-the-art unsupervised dependency parsers, except seginer (2007), <papid> P07-1049 </papid>also relyon manually annotated text or text processed by supervised pos taggers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3094">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unsupervised dependency parsers do not achieve the same quality as supervised or semi-supervised parsers, but in some situations precision may be less important compared to the cost of producing manually annotated data.
</prevsent>
<prevsent>moreover, unsupervised dependency parsing is attractive from theoretical pointof view as it does not relyon particular style of annotation and may potentially provide insights about the difficulties of human language learning.
</prevsent>
</prevsection>
<citsent citstr=" D10-1120 ">
unsupervised dependency parsing has seen rapid progress recently, with error reductions on english (marcus et al, 1993) <papid> J93-2004 </papid>of about 15% in six years (klein and manning, 2004; <papid> P04-1061 </papid>spitkovsky et al, 2010), <papid> W10-2902 </papid>and better and better results for other languages (gillenwater et al, 2010; <papid> P10-2036 </papid>naseem et al, 2010), <papid> D10-1120 </papid>but results are still far from what can be achieved with small seeds, language-specific rules (druck et al., 2009) <papid> P09-1041 </papid>or using cross-language adaptation (smith and eisner, 2009; <papid> D09-1086 </papid>spreyer et al, 2010).</citsent>
<aftsection>
<nextsent>the standard method in unsupervised dependency parsing is to optimize the overall probability of the corpus by assigning trees to its sentences that capture general patterns in the distribution of part-of speech (pos).
</nextsent>
<nextsent>this happens in several iterations over the corpus.
</nextsent>
<nextsent>this method requires clever initialization, which can be seen as kind of minimal supervision.
</nextsent>
<nextsent>state-of-the-art unsupervised dependency parsers, except seginer (2007), <papid> P07-1049 </papid>also relyon manually annotated text or text processed by supervised pos taggers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3095">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unsupervised dependency parsers do not achieve the same quality as supervised or semi-supervised parsers, but in some situations precision may be less important compared to the cost of producing manually annotated data.
</prevsent>
<prevsent>moreover, unsupervised dependency parsing is attractive from theoretical pointof view as it does not relyon particular style of annotation and may potentially provide insights about the difficulties of human language learning.
</prevsent>
</prevsection>
<citsent citstr=" P09-1041 ">
unsupervised dependency parsing has seen rapid progress recently, with error reductions on english (marcus et al, 1993) <papid> J93-2004 </papid>of about 15% in six years (klein and manning, 2004; <papid> P04-1061 </papid>spitkovsky et al, 2010), <papid> W10-2902 </papid>and better and better results for other languages (gillenwater et al, 2010; <papid> P10-2036 </papid>naseem et al, 2010), <papid> D10-1120 </papid>but results are still far from what can be achieved with small seeds, language-specific rules (druck et al., 2009) <papid> P09-1041 </papid>or using cross-language adaptation (smith and eisner, 2009; <papid> D09-1086 </papid>spreyer et al, 2010).</citsent>
<aftsection>
<nextsent>the standard method in unsupervised dependency parsing is to optimize the overall probability of the corpus by assigning trees to its sentences that capture general patterns in the distribution of part-of speech (pos).
</nextsent>
<nextsent>this happens in several iterations over the corpus.
</nextsent>
<nextsent>this method requires clever initialization, which can be seen as kind of minimal supervision.
</nextsent>
<nextsent>state-of-the-art unsupervised dependency parsers, except seginer (2007), <papid> P07-1049 </papid>also relyon manually annotated text or text processed by supervised pos taggers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3096">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unsupervised dependency parsers do not achieve the same quality as supervised or semi-supervised parsers, but in some situations precision may be less important compared to the cost of producing manually annotated data.
</prevsent>
<prevsent>moreover, unsupervised dependency parsing is attractive from theoretical pointof view as it does not relyon particular style of annotation and may potentially provide insights about the difficulties of human language learning.
</prevsent>
</prevsection>
<citsent citstr=" D09-1086 ">
unsupervised dependency parsing has seen rapid progress recently, with error reductions on english (marcus et al, 1993) <papid> J93-2004 </papid>of about 15% in six years (klein and manning, 2004; <papid> P04-1061 </papid>spitkovsky et al, 2010), <papid> W10-2902 </papid>and better and better results for other languages (gillenwater et al, 2010; <papid> P10-2036 </papid>naseem et al, 2010), <papid> D10-1120 </papid>but results are still far from what can be achieved with small seeds, language-specific rules (druck et al., 2009) <papid> P09-1041 </papid>or using cross-language adaptation (smith and eisner, 2009; <papid> D09-1086 </papid>spreyer et al, 2010).</citsent>
<aftsection>
<nextsent>the standard method in unsupervised dependency parsing is to optimize the overall probability of the corpus by assigning trees to its sentences that capture general patterns in the distribution of part-of speech (pos).
</nextsent>
<nextsent>this happens in several iterations over the corpus.
</nextsent>
<nextsent>this method requires clever initialization, which can be seen as kind of minimal supervision.
</nextsent>
<nextsent>state-of-the-art unsupervised dependency parsers, except seginer (2007), <papid> P07-1049 </papid>also relyon manually annotated text or text processed by supervised pos taggers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3097">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this happens in several iterations over the corpus.
</prevsent>
<prevsent>this method requires clever initialization, which can be seen as kind of minimal supervision.
</prevsent>
</prevsection>
<citsent citstr=" P07-1049 ">
state-of-the-art unsupervised dependency parsers, except seginer (2007), <papid> P07-1049 </papid>also relyon manually annotated text or text processed by supervised pos taggers.</citsent>
<aftsection>
<nextsent>since there is an intimate relationship between pos tagging and dependency parsing, thepos tags can also be seen as seed or as partial annotation.
</nextsent>
<nextsent>inducing model from the corpus is typically very slow process.this paper presents new and very different approach to unsupervised dependency parsing.
</nextsent>
<nextsent>the parser does not induce model from big corpus,but with few exceptions only considers the sentence in question.
</nextsent>
<nextsent>it does use larger corpus to induce distributional clusters and ranking of keywords in terms of frequency and centrality, but thisis computationally efficient and is only indirectly related to the subsequent assignment of dependency structures to sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3101">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most if notall previous work in unsupervised dependency parsing has focused on projective dependency parsing, building on work in context-free parsing, but our parser is guaranteed to produce well-formed non projective dependency trees.
</prevsent>
<prevsent>non-projective parsing algorithms for supervised dependency parsing have, for example, been presented in mcdonald et al.
</prevsent>
</prevsection>
<citsent citstr=" P09-1040 ">
(2005) and nivre (2009).<papid> P09-1040 </papid></citsent>
<aftsection>
<nextsent>1.2 related work.
</nextsent>
<nextsent>dependency model with valence (dmv) by klein and manning (2004) <papid> P04-1061 </papid>was the first unsupervised dependency parser to achieve an accuracy for manually 61pos-tagged english above right-branching base line.</nextsent>
<nextsent>dmv is generative model in which the sentence root is generated and then each head recursively generates its left and right dependents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3105">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dependency model with valence (dmv) by klein and manning (2004) <papid> P04-1061 </papid>was the first unsupervised dependency parser to achieve an accuracy for manually 61pos-tagged english above right-branching base line.</prevsent>
<prevsent>dmv is generative model in which the sentence root is generated and then each head recursively generates its left and right dependents.</prevsent>
</prevsection>
<citsent citstr=" P05-1044 ">
for each si ? s, ti is assumed to have been built the following way:the arguments of head in direction are generated one after another with the probability that nomore arguments of should be generated in direction conditioned on h, and whether this would be the first argument of in direction d. the pos tag of the argument of is generated given and d. klein and manning (2004) <papid> P04-1061 </papid>use expectation maximization (em) to estimate probabilities with manually tuned linguistically-biased priors.smith and eisner (2005) <papid> P05-1044 </papid>use contrastive estimation instead of em, while smith and eisner (2006) <papid> P06-1072 </papid>use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training.</citsent>
<aftsection>
<nextsent>cohen etal.
</nextsent>
<nextsent>(2008) use bayesian priors (dirichlet and logistic normal) with dmv.
</nextsent>
<nextsent>all of the above approaches to unsupervised dependency parsing build on the linguistically-biased priors introduced by klein and manning (2004).<papid> P04-1061 </papid></nextsent>
<nextsent>in similar way gillenwater et al (2010) <papid> P10-2036 </papid>tryto penalize models with large number of distinct dependency types by using sparse posteriors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3106">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dependency model with valence (dmv) by klein and manning (2004) <papid> P04-1061 </papid>was the first unsupervised dependency parser to achieve an accuracy for manually 61pos-tagged english above right-branching base line.</prevsent>
<prevsent>dmv is generative model in which the sentence root is generated and then each head recursively generates its left and right dependents.</prevsent>
</prevsection>
<citsent citstr=" P06-1072 ">
for each si ? s, ti is assumed to have been built the following way:the arguments of head in direction are generated one after another with the probability that nomore arguments of should be generated in direction conditioned on h, and whether this would be the first argument of in direction d. the pos tag of the argument of is generated given and d. klein and manning (2004) <papid> P04-1061 </papid>use expectation maximization (em) to estimate probabilities with manually tuned linguistically-biased priors.smith and eisner (2005) <papid> P05-1044 </papid>use contrastive estimation instead of em, while smith and eisner (2006) <papid> P06-1072 </papid>use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training.</citsent>
<aftsection>
<nextsent>cohen etal.
</nextsent>
<nextsent>(2008) use bayesian priors (dirichlet and logistic normal) with dmv.
</nextsent>
<nextsent>all of the above approaches to unsupervised dependency parsing build on the linguistically-biased priors introduced by klein and manning (2004).<papid> P04-1061 </papid></nextsent>
<nextsent>in similar way gillenwater et al (2010) <papid> P10-2036 </papid>tryto penalize models with large number of distinct dependency types by using sparse posteriors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3112">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an exception to using linguistically-biased priors is spitkovsky et al (2009) who use predictions on sentences of length to initialize search on sentences of length n+ 1.
</prevsent>
<prevsent>in other words, their method requires no manual tuning and bootstraps itself on increasingly longer sentences.
</prevsent>
</prevsection>
<citsent citstr=" D10-1118 ">
a very different, but interesting, approach is takenin brody (2010) <papid> D10-1118 </papid>who use methods from unsupervised word alignment for unsupervised dependency parsing.</citsent>
<aftsection>
<nextsent>in particular, he sees dependency parsing as directional alignment from sentence (possibledependents) to itself (possible heads) with the modification that words cannot align to themselves; following klein and manning (2004) <papid> P04-1061 </papid>and the subsequent papers mentioned above, brody (2010) <papid> D10-1118 </papid>considers sequences of pos tags rather than raw text.</nextsent>
<nextsent>results are below state-of-the-art, but in some cases better than the dmv model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3120">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> ranking dependency tree nodes.  </section>
<citcontext>
<prevsection>
<prevsent>semantically, the nodes near the root typically express the main predicate and its arguments.
</prevsent>
<prevsent>iterative graph-based ranking (page and brin, 1998) was first used to rank web pages according to their centrality, but the technique has found wide application in natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
variations of the algorithm presented in pageand brin (1998) have been used in keyword extraction and extractive summarization (mihalcea and ta rau, 2004), <papid> W04-3252 </papid>word sense disambiguation (agirre andsoroa, 2009), <papid> E09-1005 </papid>and abs tractive summarization (gane san et al, 2010).</citsent>
<aftsection>
<nextsent>in this paper, we use it as the first step in two-step unsupervised dependency parsing procedure.the parser assigns dependency structure to sequence of words in two stages.
</nextsent>
<nextsent>it first decorates the nodes of what will become our dependency structure with word forms and distributional clusters, constructs directed acyclic graph from the nodes in o(n2), and ranks the nodes using iterative graph-based ranking.
</nextsent>
<nextsent>subsequently, it construc tsa tree from the ranked list of words using simple o(n log n) parsing algorithm.
</nextsent>
<nextsent>this section describes the graph construction step in some detail and briefly describes the iterative graph-based ranking algorithm used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3121">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> ranking dependency tree nodes.  </section>
<citcontext>
<prevsection>
<prevsent>semantically, the nodes near the root typically express the main predicate and its arguments.
</prevsent>
<prevsent>iterative graph-based ranking (page and brin, 1998) was first used to rank web pages according to their centrality, but the technique has found wide application in natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" E09-1005 ">
variations of the algorithm presented in pageand brin (1998) have been used in keyword extraction and extractive summarization (mihalcea and ta rau, 2004), <papid> W04-3252 </papid>word sense disambiguation (agirre andsoroa, 2009), <papid> E09-1005 </papid>and abs tractive summarization (gane san et al, 2010).</citsent>
<aftsection>
<nextsent>in this paper, we use it as the first step in two-step unsupervised dependency parsing procedure.the parser assigns dependency structure to sequence of words in two stages.
</nextsent>
<nextsent>it first decorates the nodes of what will become our dependency structure with word forms and distributional clusters, constructs directed acyclic graph from the nodes in o(n2), and ranks the nodes using iterative graph-based ranking.
</nextsent>
<nextsent>subsequently, it construc tsa tree from the ranked list of words using simple o(n log n) parsing algorithm.
</nextsent>
<nextsent>this section describes the graph construction step in some detail and briefly describes the iterative graph-based ranking algorithm used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3123">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> ranking dependency tree nodes.  </section>
<citcontext>
<prevsection>
<prevsent>our edge selection was incremental considering first an extended set of candidate edges with arbitrary parameters and then considering each edge type at time.if the edge type was helpful, we optimized any possible parameters (say context windows) and went on to the next edge type: otherwise we disregarded it.3 following dataset et al (2010), we apply the best setting for english to all other languages.
</prevsent>
<prevsent>vine edges.
</prevsent>
</prevsection>
<citsent citstr=" W05-1504 ">
eisner and smith (2005) <papid> W05-1504 </papid>motivate vine parsing approach to supervised dependency parsing arguing that language users have strong preference for short dependencies.</citsent>
<aftsection>
<nextsent>reflecting preference for short dependencies, we first add links between all words and their neighbors and neighbors?
</nextsent>
<nextsent>neighbors.
</nextsent>
<nextsent>this also guarantees that the final graph is con nected.keywords and closed class words.
</nextsent>
<nextsent>we use key word extraction algorithm without stop word lists to extract non-content words and the most important content words, typically nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3128">
<title id=" W11-1109.xml">from ranked words to dependency trees two stage unsupervised non projective dependency parsing </title>
<section> ranking dependency tree nodes.  </section>
<citcontext>
<prevsection>
<prevsent>we use exactly the same experimental set-up as gillenwater et al (2010).<papid> P10-2036 </papid></prevsent>
<prevsent>the edge model was developed on development data from the englishpenn-iii treebank (marcus et al, 1993), <papid> J93-2004 </papid>and we evaluate on sect.</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
23 of the english treebanks and the test sections of the remaining 11 treebanks, which were all used in the conll-x shared task (buchholz and marsi, 2006).<papid> W06-2920 </papid></citsent>
<aftsection>
<nextsent>gillenwater et al (2010) <papid> P10-2036 </papid>for some reason did not evaluate on the arabic and chinese treebanks also used in the shared task.</nextsent>
<nextsent>we also follow gillenwater et al (2010) <papid> P10-2036 </papid>in only evaluating our parser on sentences of at most 10 non-punctuation words and in reporting unlabeled attachment scores excluding punctuation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3143">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in several nlp tasks, systems trained on annotated data from one domain perform well when tested on the same domain but adapt poorly to other domains.
</prevsent>
<prevsent>for example, all systems of conll 2005shared task (carreras and mrquez, 2005) on semantic role labeling showed performance degradation of almost 10% or more when tested on different domain.
</prevsent>
</prevsection>
<citsent citstr=" W06-1615 ">
most works in domain adaptation have focused on learning common representation across training and test domains (blitzer et al, 2006; <papid> W06-1615 </papid>daumiii,2007; huang and yates, 2009).<papid> P09-1056 </papid></citsent>
<aftsection>
<nextsent>using this representation, they retrain the model for every new domain.
</nextsent>
<nextsent>but these are not open domain systems since the model needs to be retrained for every new domain.
</nextsent>
<nextsent>this is very difficult for pipeline systems like srl where syntactic parser, shallow parser, pos tagger and then srl need to be retrained.
</nextsent>
<nextsent>moreover, these methods need to have lot of unlabeled data that istaken from the same domain, in order to learn meaningful feature correspondences across training and test domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3145">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in several nlp tasks, systems trained on annotated data from one domain perform well when tested on the same domain but adapt poorly to other domains.
</prevsent>
<prevsent>for example, all systems of conll 2005shared task (carreras and mrquez, 2005) on semantic role labeling showed performance degradation of almost 10% or more when tested on different domain.
</prevsent>
</prevsection>
<citsent citstr=" P09-1056 ">
most works in domain adaptation have focused on learning common representation across training and test domains (blitzer et al, 2006; <papid> W06-1615 </papid>daumiii,2007; huang and yates, 2009).<papid> P09-1056 </papid></citsent>
<aftsection>
<nextsent>using this representation, they retrain the model for every new domain.
</nextsent>
<nextsent>but these are not open domain systems since the model needs to be retrained for every new domain.
</nextsent>
<nextsent>this is very difficult for pipeline systems like srl where syntactic parser, shallow parser, pos tagger and then srl need to be retrained.
</nextsent>
<nextsent>moreover, these methods need to have lot of unlabeled data that istaken from the same domain, in order to learn meaningful feature correspondences across training and test domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3146">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> transformation functions.  </section>
<citcontext>
<prevsection>
<prevsent>the second is learned transformations that uses transformation rules that have been learned from training data.
</prevsent>
<prevsent>4.1 transformation from list.
</prevsent>
</prevsection>
<citsent citstr=" P10-1099 ">
i. replacement of predicate: as noted in (huang and yates, 2010), <papid> P10-1099 </papid>6.1% of the predicates in the brown test set do not appear in wsj training set and 11.8% appear at most twice.</citsent>
<aftsection>
<nextsent>since the semantic roles of sentence depend on the predicate, these infrequent predicates hurt srl performance on new domains.
</nextsent>
<nextsent>note that since all predicates in propbank are verbs, we will use the words predicate and verb interchangeably.
</nextsent>
<nextsent>we count the frequency of each predicate and its accuracy in terms of f1 score over the training data.
</nextsent>
<nextsent>if the frequency or the f1 score of the predicate in the test sentence is below threshold, we perturb that predicate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3148">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> transformation functions.  </section>
<citcontext>
<prevsection>
<prevsent>in the brown test set, 5% of total words were never seen in the wsj training set.
</prevsent>
<prevsent>given an unseen word which is not verb, we replace it with wordnet synonyms and hypernyms that were seen in the training data.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
we used the clusters obtained in (liang, 2005) from running the brown algorithm (brown et al, 1992) <papid> J92-4003 </papid>on reuters 1996 dataset.</citsent>
<aftsection>
<nextsent>but since this cluster was generated automatically, it is noisy.
</nextsent>
<nextsent>so we chose replacements from the brown clusters selectively.
</nextsent>
<nextsent>we only replace those words for which the pos tagger and the syntactic parser predicted different tags.
</nextsent>
<nextsent>for each such word, we find its cluster and select the set of words from the cluster.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3149">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> combination by joint inference.  </section>
<citcontext>
<prevsection>
<prevsent>for each argument arg in s?, we calculate scores over possible labels as thesum over the probability distribution (over output la bels) of all arguments in that have the same span as arg divided by the number of sentences in that contained arg.
</prevsent>
<prevsent>this results in set of arguments with distinct spans and for each argument, set of scores over possible labels.
</prevsent>
</prevsection>
<citsent citstr=" J08-2005 ">
following the joint inference procedure in (punyakanok et al, 2008), <papid> J08-2005 </papid>we want to select label for each argument such that the total score is maximized subject to some constraints.</citsent>
<aftsection>
<nextsent>let us index the set s?
</nextsent>
<nextsent>as s1:m where = |s?|.
</nextsent>
<nextsent>also assume that each argument can take label from set . the set of arguments in s1:m can take set of labels c1:m ? 1:m . given some constraints, the resulting solution space is limited to feasible set f; the inference task is: c1:m = arg maxc1:mf (p 1:m ) i=1 score(s = ci).
</nextsent>
<nextsent>the constraints used are: 1) no overlapping or embedding argument.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3150">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>similar to the conll 2005 shared tasks, we train our system using sections 02-21 of the wall street journal portion of penn treebank labeled with propbank.
</prevsent>
<prevsent>we test our system on an annotated brown corpus consisting of three sections (ck01 - ck03).
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
since we need to annotate new sentences with syntactic parse, pos tags and shallow parses, we donot use annotations in the conll distribution; instead, we re-annotate the data using publicly available part of speech tagger and shallow parser1, charniak 2005 parser (charniak and johnson, 2005) <papid> P05-1022 </papid>and stanford parser (klein and manning, 2003).</citsent>
<aftsection>
<nextsent>our baseline srl model is an implementation of(punyakanok et al, 2008) <papid> J08-2005 </papid>which was the topper forming system in conll 2005 shared task.</nextsent>
<nextsent>due to space constraints, we omit the details of the system and refer readers to (punyakanok et al, 2008).<papid> J08-2005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3158">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>so we combinedadut-stanford, adut-charniak and another system adut-charniak-2 based on 2nd best charniak parse using joint inference.
</prevsent>
<prevsent>in table 3, we compare with (punyakanok et al, 2008) <papid> J08-2005 </papid>which was the top performing system in conll 2005 shared task.</prevsent>
</prevsection>
<citsent citstr=" J08-2002 ">
we also compare with the multi parse system of (toutanova et al, 2008) <papid> J08-2002 </papid>which uses global joint model using multiple parse trees.</citsent>
<aftsection>
<nextsent>in (surdeanu et al,2007), the authors experimented with several combination strategies.
</nextsent>
<nextsent>their first combination strategy was similar to ours where they directly combined the outputs of different systems using constraints (de noted as cons in table 3).
</nextsent>
<nextsent>but their best result onbrown set was obtained by treating the combination of multiple systems as meta-learning problem.they trained new model to score candidate arguments produced by individual systems before combining them through constraints (denoted as lbi in table 3).
</nextsent>
<nextsent>we also compare with (huang and yates,2010) <papid> P10-1099 </papid>where the authors retrained srl model using hmm features learned over unlabeled data of wsj and brown.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3167">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the times are calculated basedon machine with 2x 6-core xeon x5650 processor with 48g memory.
</prevsent>
<prevsent>235 transformation r f1 baseline 69.6 61.8 65.5 replacement of unknown words 70.6 62.1 66.1 replacement of predicate 71.2 62.8 66.8 replacement of quotes 71.0 63.4 67.0 simplification 70.3 62.9 66.4 rule transformation 70.9 62.2 66.2 sentence split 70.8 62.1 66.2 together 72.75 66.1 69.3 table 4: ablation study for adut-charniak frequency baseline replacement of predicate 0 64.2 67.8 less than 3 59.7 65.1 less than 7 58.9 64.8 all predicates 65.5 66.78table 5: performance on infrequent verbs for the transformation of replacement of predicate
</prevsent>
</prevsection>
<citsent citstr=" N09-1068 ">
traditional adaptation techniques like (daumiii,2007; chelba and acero, 2004; finkel and manning, 2009; <papid> N09-1068 </papid>jiang and zhai, 2007; <papid> P07-1034 </papid>blitzer et al, 2006; <papid> W06-1615 </papid>huang and yates, 2009; <papid> P09-1056 </papid>ando and zhang,2005; ming-wei chang and roth, 2010) need to retrain the model for every new domain.</citsent>
<aftsection>
<nextsent>in (umansky pesin et al, 2010), there was no retraining; instead, pos tag was predicted for every unknown word in the new domain by considering contexts of that word collected by web search queries.
</nextsent>
<nextsent>we differ from them in that our transformations are label preserving; moreover, our transformations aim at making the target text resemble the training text.
</nextsent>
<nextsent>we also present an algorithm to learn transformation rules from training data.
</nextsent>
<nextsent>our application domain, srl, is also more complex and structured than pos tagging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3168">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the times are calculated basedon machine with 2x 6-core xeon x5650 processor with 48g memory.
</prevsent>
<prevsent>235 transformation r f1 baseline 69.6 61.8 65.5 replacement of unknown words 70.6 62.1 66.1 replacement of predicate 71.2 62.8 66.8 replacement of quotes 71.0 63.4 67.0 simplification 70.3 62.9 66.4 rule transformation 70.9 62.2 66.2 sentence split 70.8 62.1 66.2 together 72.75 66.1 69.3 table 4: ablation study for adut-charniak frequency baseline replacement of predicate 0 64.2 67.8 less than 3 59.7 65.1 less than 7 58.9 64.8 all predicates 65.5 66.78table 5: performance on infrequent verbs for the transformation of replacement of predicate
</prevsent>
</prevsection>
<citsent citstr=" P07-1034 ">
traditional adaptation techniques like (daumiii,2007; chelba and acero, 2004; finkel and manning, 2009; <papid> N09-1068 </papid>jiang and zhai, 2007; <papid> P07-1034 </papid>blitzer et al, 2006; <papid> W06-1615 </papid>huang and yates, 2009; <papid> P09-1056 </papid>ando and zhang,2005; ming-wei chang and roth, 2010) need to retrain the model for every new domain.</citsent>
<aftsection>
<nextsent>in (umansky pesin et al, 2010), there was no retraining; instead, pos tag was predicted for every unknown word in the new domain by considering contexts of that word collected by web search queries.
</nextsent>
<nextsent>we differ from them in that our transformations are label preserving; moreover, our transformations aim at making the target text resemble the training text.
</nextsent>
<nextsent>we also present an algorithm to learn transformation rules from training data.
</nextsent>
<nextsent>our application domain, srl, is also more complex and structured than pos tagging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3172">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we also present an algorithm to learn transformation rules from training data.
</prevsent>
<prevsent>our application domain, srl, is also more complex and structured than pos tagging.
</prevsent>
</prevsection>
<citsent citstr=" N10-1004 ">
in (mcclosky et al, 2010), <papid> N10-1004 </papid>the task of multiple source parser adaptation was introduced.</citsent>
<aftsection>
<nextsent>the authors trained parsing models on corpora from different domains and given new text, used linear combination of trained models.
</nextsent>
<nextsent>their approach requires annotated data from multiple domains as well as unlabeled data for the new domain, which is not needed in our framework.
</nextsent>
<nextsent>in (huang and yates, 2010), <papid> P10-1099 </papid>the authors trained hmm over the brown test set and the wsj unlabeled data.</nextsent>
<nextsent>they derived features from viterbi optimal states of single word sand spans of words and retrained their models using these features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3175">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in (huang and yates, 2010), <papid> P10-1099 </papid>the authors trained hmm over the brown test set and the wsj unlabeled data.</prevsent>
<prevsent>they derived features from viterbi optimal states of single word sand spans of words and retrained their models using these features.</prevsent>
</prevsection>
<citsent citstr=" P08-1040 ">
in (vickrey and koller, 2008), <papid> P08-1040 </papid>large number of hand-written rules were used to simplify the parse trees and reduce syntactic variation to overcome feature sparsity.</citsent>
<aftsection>
<nextsent>we have several types of transformations, and use less than 10 simplification heuristics, based on replacing larger phrases with smaller phrases and deleting unnecessary parse tree nodes.
</nextsent>
<nextsent>there are also some methods for unsupervised semantic role labeling (swier and stevenson, 2004), (<papid> W04-3213 </papid>abend et al, 2009) <papid> P09-1004 </papid>that easily adapt across domains but their performances are not comparable to supervised systems.</nextsent>
<nextsent>we presented framework for adapt ating natural language text so that models can be used across domains without modification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3176">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in (vickrey and koller, 2008), <papid> P08-1040 </papid>large number of hand-written rules were used to simplify the parse trees and reduce syntactic variation to overcome feature sparsity.</prevsent>
<prevsent>we have several types of transformations, and use less than 10 simplification heuristics, based on replacing larger phrases with smaller phrases and deleting unnecessary parse tree nodes.</prevsent>
</prevsection>
<citsent citstr=" W04-3213 ">
there are also some methods for unsupervised semantic role labeling (swier and stevenson, 2004), (<papid> W04-3213 </papid>abend et al, 2009) <papid> P09-1004 </papid>that easily adapt across domains but their performances are not comparable to supervised systems.</citsent>
<aftsection>
<nextsent>we presented framework for adapt ating natural language text so that models can be used across domains without modification.
</nextsent>
<nextsent>our framework supports adapting to new domains without any data or knowledge of the target domain.
</nextsent>
<nextsent>we showed that our approach significantly improves srl performance over the state-of-the-art single parse based system on brown set.
</nextsent>
<nextsent>in the future, we would like to extend this approach to other nlp problems and study how combining multiple systems can further improve its performance and robustness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3177">
<title id=" W11-0327.xml">adapting text instead of the model an open domain approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in (vickrey and koller, 2008), <papid> P08-1040 </papid>large number of hand-written rules were used to simplify the parse trees and reduce syntactic variation to overcome feature sparsity.</prevsent>
<prevsent>we have several types of transformations, and use less than 10 simplification heuristics, based on replacing larger phrases with smaller phrases and deleting unnecessary parse tree nodes.</prevsent>
</prevsection>
<citsent citstr=" P09-1004 ">
there are also some methods for unsupervised semantic role labeling (swier and stevenson, 2004), (<papid> W04-3213 </papid>abend et al, 2009) <papid> P09-1004 </papid>that easily adapt across domains but their performances are not comparable to supervised systems.</citsent>
<aftsection>
<nextsent>we presented framework for adapt ating natural language text so that models can be used across domains without modification.
</nextsent>
<nextsent>our framework supports adapting to new domains without any data or knowledge of the target domain.
</nextsent>
<nextsent>we showed that our approach significantly improves srl performance over the state-of-the-art single parse based system on brown set.
</nextsent>
<nextsent>in the future, we would like to extend this approach to other nlp problems and study how combining multiple systems can further improve its performance and robustness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3178">
<title id=" W10-4233.xml">report on the second nlg challenge on generating instructions in virtual environments give2 </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 software infrastructure.
</prevsent>
<prevsent>we adapted the give-1 software to the give-2 setting.
</prevsent>
</prevsection>
<citsent citstr=" P09-2076 ">
the give software infrastructure (kolleret al, 2009<papid> P09-2076 </papid>a) consists of three different mod ules: the client, which is the program which the user runs on their machine to interact with the virtual world (see fig.</citsent>
<aftsection>
<nextsent>1); collection of nlg servers, which generate instructions in real-time and send them to the client; and matchmaker, which chooses random nlg server and virtual world for each incoming connection from client and stores the game results in database.
</nextsent>
<nextsent>the most visible change compared to give-1was to modify the client so it permitted free movement in the virtual world.
</nextsent>
<nextsent>this change further necessitated number of modifications to the internal representation of the world.
</nextsent>
<nextsent>to support the development of virtual worlds for give, we changed the file format for world descriptions to be much more readable, and provided an automatic tool for displaying virtual worlds graphically (see the screen shots in fig.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3180">
<title id=" W11-0211.xml">towards morphologically annotated corpus of hospital discharge reports in polish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>medical corpora are also collected for lesser spoken languages, e.g. medlex ? swedish medical corpus (kokkinakis, 2006); iatrolexi project for greek (tsalidis et al., 2007); or norwegian corpus of patients?
</prevsent>
<prevsent>histories (rst et al, 2008).
</prevsent>
</prevsection>
<citsent citstr=" W05-1306 ">
the paper (cohen et al, 2005)<papid> W05-1306 </papid>contains survey of 6 biomedical corpora.</citsent>
<aftsection>
<nextsent>the authors emphasize the importance of standard format and give guidelines for careful annotation and evaluation of corpora.the immediate goal of the paper is to establish and test method of annotating polish clinical data with low level linguistic information, i.e. token and morpheme descriptions.
</nextsent>
<nextsent>the research is done on relatively small set of data (more than450,000 tokens) but to gain the experience necessary to create much larger annotated corpus of polish medical texts.
</nextsent>
<nextsent>we would like to use our corpus to refine and test domain tools for: tagging, named entity recognition or annotation of nominal phrases.
</nextsent>
<nextsent>we have already annotated the corpus with semantic information (marciniak and mykowiecka,2011) using an existing rule based extraction system (mykowiecka et al, 2009) and performed experiments with machine learning approaches to semantic labeling (mykowiecka and marciniak, 2011).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3181">
<title id=" W11-0211.xml">towards morphologically annotated corpus of hospital discharge reports in polish </title>
<section> corpus description.  </section>
<citcontext>
<prevsection>
<prevsent>only the first word is not in polish.
</prevsent>
<prevsent>the corpus is annotated with morphological and semantic information.
</prevsent>
</prevsection>
<citsent citstr=" W03-1313 ">
the standard of annotation fol 93 lows the tei p5 guidelines advised for annotation of biomedical corpora, see (erjavec et al, 2003).<papid> W03-1313 </papid></citsent>
<aftsection>
<nextsent>our corpus format is based on the one accepted for the nkjp corpus (przepirkowski and banski, 2009).according to this scheme, every annotation is described in separate file.
</nextsent>
<nextsent>each discharge document is represented by catalog containing the following five files: ? xxx.txt ? plain text of the original annonymized document; ? xxx.xml ? text of the document (in the form as in xxx.txt file) divided into numbered sections which are in turn divided into paragraphs; ? xxx_segm.xml ? token limits and types (29 classes); ? xxx_morph.xml ? morphological information (lemmas and morphological feature values); ? xxx_sem.xml ? semantic labels and limits.
</nextsent>
<nextsent>the first level of text analysis is its segmentation into tokens.
</nextsent>
<nextsent>in general, most tokens in texts are lowercase words, words beginning with capital letter and punctuation marks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3182">
<title id=" W11-0211.xml">towards morphologically annotated corpus of hospital discharge reports in polish </title>
<section> conclusions and further work.  </section>
<citcontext>
<prevsection>
<prevsent>in the case of abbreviations and acronyms, the best method is to use dictionaries, but some heuristics can be useful too.
</prevsent>
<prevsent>electronic dictionaries of acronyms and abbreviations are not available for polish, but on the basis of annotated data, do main specific lexicon can be created.
</prevsent>
</prevsection>
<citsent citstr=" L08-1237 ">
moreover, wewant to test ideas from (kokkinakis, 2008), <papid> L08-1237 </papid>the author presents method for the application of the mesh lexicon (that contains english and latin data) to swedish medical corpus annotation.</citsent>
<aftsection>
<nextsent>we will usea similar approach for acronyms and complex medication name recognition.
</nextsent>
<nextsent>99
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3183">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our approach achieved an average bleu score improvement of 0.49 as well as 1.21 point reduction inter.
</prevsent>
<prevsent>syntax-based machine translation methods have achieved comparable performance to phrase-basedsystems.
</prevsent>
</prevsection>
<citsent citstr=" J07-2003 ">
hierarchical phrase-based machine translation, proposed by chiang (chiang, 2007), <papid> J07-2003 </papid>uses agen eral non-terminal label but does not use linguistic information from the source or the target language.</citsent>
<aftsection>
<nextsent>there have been efforts to include linguistic information into machine translation.
</nextsent>
<nextsent>liu et al  (2006) <papid> P06-1077 </papid>experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the packed forest data structure to reduce the impact of parsing errors (liu and huang, 2010).<papid> P10-5002 </papid></nextsent>
<nextsent>the string-to-tree (galleyet al  2006) <papid> P06-1121 </papid>and tree-to-tree (chiang, 2010) <papid> P10-1146 </papid>methods have also been the subject of experimentation, as well as other formalisms such as dependency trees (shen et al , 2008).<papid> P08-1066 </papid>one problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3184">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hierarchical phrase-based machine translation, proposed by chiang (chiang, 2007), <papid> J07-2003 </papid>uses agen eral non-terminal label but does not use linguistic information from the source or the target language.</prevsent>
<prevsent>there have been efforts to include linguistic information into machine translation.</prevsent>
</prevsection>
<citsent citstr=" P06-1077 ">
liu et al  (2006) <papid> P06-1077 </papid>experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the packed forest data structure to reduce the impact of parsing errors (liu and huang, 2010).<papid> P10-5002 </papid></citsent>
<aftsection>
<nextsent>the string-to-tree (galleyet al  2006) <papid> P06-1121 </papid>and tree-to-tree (chiang, 2010) <papid> P10-1146 </papid>methods have also been the subject of experimentation, as well as other formalisms such as dependency trees (shen et al , 2008).<papid> P08-1066 </papid>one problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules.</nextsent>
<nextsent>samt (zollmannand venugopal, 2006) <papid> W06-3119 </papid>and tree sequence alignment (zhang et al , 2008) <papid> P08-1064 </papid>are proposed to amend this problem by allowing non-constituent phrases to beextracted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3185">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hierarchical phrase-based machine translation, proposed by chiang (chiang, 2007), <papid> J07-2003 </papid>uses agen eral non-terminal label but does not use linguistic information from the source or the target language.</prevsent>
<prevsent>there have been efforts to include linguistic information into machine translation.</prevsent>
</prevsection>
<citsent citstr=" P10-5002 ">
liu et al  (2006) <papid> P06-1077 </papid>experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the packed forest data structure to reduce the impact of parsing errors (liu and huang, 2010).<papid> P10-5002 </papid></citsent>
<aftsection>
<nextsent>the string-to-tree (galleyet al  2006) <papid> P06-1121 </papid>and tree-to-tree (chiang, 2010) <papid> P10-1146 </papid>methods have also been the subject of experimentation, as well as other formalisms such as dependency trees (shen et al , 2008).<papid> P08-1066 </papid>one problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules.</nextsent>
<nextsent>samt (zollmannand venugopal, 2006) <papid> W06-3119 </papid>and tree sequence alignment (zhang et al , 2008) <papid> P08-1064 </papid>are proposed to amend this problem by allowing non-constituent phrases to beextracted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3186">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there have been efforts to include linguistic information into machine translation.
</prevsent>
<prevsent>liu et al  (2006) <papid> P06-1077 </papid>experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the packed forest data structure to reduce the impact of parsing errors (liu and huang, 2010).<papid> P10-5002 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1121 ">
the string-to-tree (galleyet al  2006) <papid> P06-1121 </papid>and tree-to-tree (chiang, 2010) <papid> P10-1146 </papid>methods have also been the subject of experimentation, as well as other formalisms such as dependency trees (shen et al , 2008).<papid> P08-1066 </papid>one problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules.</citsent>
<aftsection>
<nextsent>samt (zollmannand venugopal, 2006) <papid> W06-3119 </papid>and tree sequence alignment (zhang et al , 2008) <papid> P08-1064 </papid>are proposed to amend this problem by allowing non-constituent phrases to beextracted.</nextsent>
<nextsent>the reported results show that while utilizing linguistic information helps, the coverage is more important (chiang, 2010).<papid> P10-1146 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3187">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there have been efforts to include linguistic information into machine translation.
</prevsent>
<prevsent>liu et al  (2006) <papid> P06-1077 </papid>experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the packed forest data structure to reduce the impact of parsing errors (liu and huang, 2010).<papid> P10-5002 </papid></prevsent>
</prevsection>
<citsent citstr=" P10-1146 ">
the string-to-tree (galleyet al  2006) <papid> P06-1121 </papid>and tree-to-tree (chiang, 2010) <papid> P10-1146 </papid>methods have also been the subject of experimentation, as well as other formalisms such as dependency trees (shen et al , 2008).<papid> P08-1066 </papid>one problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules.</citsent>
<aftsection>
<nextsent>samt (zollmannand venugopal, 2006) <papid> W06-3119 </papid>and tree sequence alignment (zhang et al , 2008) <papid> P08-1064 </papid>are proposed to amend this problem by allowing non-constituent phrases to beextracted.</nextsent>
<nextsent>the reported results show that while utilizing linguistic information helps, the coverage is more important (chiang, 2010).<papid> P10-1146 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3188">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there have been efforts to include linguistic information into machine translation.
</prevsent>
<prevsent>liu et al  (2006) <papid> P06-1077 </papid>experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the packed forest data structure to reduce the impact of parsing errors (liu and huang, 2010).<papid> P10-5002 </papid></prevsent>
</prevsection>
<citsent citstr=" P08-1066 ">
the string-to-tree (galleyet al  2006) <papid> P06-1121 </papid>and tree-to-tree (chiang, 2010) <papid> P10-1146 </papid>methods have also been the subject of experimentation, as well as other formalisms such as dependency trees (shen et al , 2008).<papid> P08-1066 </papid>one problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules.</citsent>
<aftsection>
<nextsent>samt (zollmannand venugopal, 2006) <papid> W06-3119 </papid>and tree sequence alignment (zhang et al , 2008) <papid> P08-1064 </papid>are proposed to amend this problem by allowing non-constituent phrases to beextracted.</nextsent>
<nextsent>the reported results show that while utilizing linguistic information helps, the coverage is more important (chiang, 2010).<papid> P10-1146 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3189">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>liu et al  (2006) <papid> P06-1077 </papid>experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the packed forest data structure to reduce the impact of parsing errors (liu and huang, 2010).<papid> P10-5002 </papid></prevsent>
<prevsent>the string-to-tree (galleyet al  2006) <papid> P06-1121 </papid>and tree-to-tree (chiang, 2010) <papid> P10-1146 </papid>methods have also been the subject of experimentation, as well as other formalisms such as dependency trees (shen et al , 2008).<papid> P08-1066 </papid>one problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules.</prevsent>
</prevsection>
<citsent citstr=" W06-3119 ">
samt (zollmannand venugopal, 2006) <papid> W06-3119 </papid>and tree sequence alignment (zhang et al , 2008) <papid> P08-1064 </papid>are proposed to amend this problem by allowing non-constituent phrases to beextracted.</citsent>
<aftsection>
<nextsent>the reported results show that while utilizing linguistic information helps, the coverage is more important (chiang, 2010).<papid> P10-1146 </papid></nextsent>
<nextsent>when dealing with formalisms such as semantic role labeling, the coverage problem is also critical.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3190">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>liu et al  (2006) <papid> P06-1077 </papid>experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the packed forest data structure to reduce the impact of parsing errors (liu and huang, 2010).<papid> P10-5002 </papid></prevsent>
<prevsent>the string-to-tree (galleyet al  2006) <papid> P06-1121 </papid>and tree-to-tree (chiang, 2010) <papid> P10-1146 </papid>methods have also been the subject of experimentation, as well as other formalisms such as dependency trees (shen et al , 2008).<papid> P08-1066 </papid>one problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules.</prevsent>
</prevsection>
<citsent citstr=" P08-1064 ">
samt (zollmannand venugopal, 2006) <papid> W06-3119 </papid>and tree sequence alignment (zhang et al , 2008) <papid> P08-1064 </papid>are proposed to amend this problem by allowing non-constituent phrases to beextracted.</citsent>
<aftsection>
<nextsent>the reported results show that while utilizing linguistic information helps, the coverage is more important (chiang, 2010).<papid> P10-1146 </papid></nextsent>
<nextsent>when dealing with formalisms such as semantic role labeling, the coverage problem is also critical.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3192">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this preserves the coverage of rules.
</prevsent>
<prevsent>recently there has been increased attention to use semantic information in machine translation.
</prevsent>
</prevsection>
<citsent citstr=" W08-0308 ">
liu and gildea (2008), <papid> W08-0308 </papid>liu and gildea (2010) <papid> C10-1081 </papid>proposed using semantic role labels (srl) in their tree-to-string machine translation system and demonstrated improvement over conventional tree-to-string methods.</citsent>
<aftsection>
<nextsent>wu and fung (2009) <papid> N09-2004 </papid>developed framework to reorder the output using information from both the source and the target srl labels.</nextsent>
<nextsent>in this paper, we explore an approach of using the target side srl information in addition to hierarchical phrase-based machine translation framework.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3193">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this preserves the coverage of rules.
</prevsent>
<prevsent>recently there has been increased attention to use semantic information in machine translation.
</prevsent>
</prevsection>
<citsent citstr=" C10-1081 ">
liu and gildea (2008), <papid> W08-0308 </papid>liu and gildea (2010) <papid> C10-1081 </papid>proposed using semantic role labels (srl) in their tree-to-string machine translation system and demonstrated improvement over conventional tree-to-string methods.</citsent>
<aftsection>
<nextsent>wu and fung (2009) <papid> N09-2004 </papid>developed framework to reorder the output using information from both the source and the target srl labels.</nextsent>
<nextsent>in this paper, we explore an approach of using the target side srl information in addition to hierarchical phrase-based machine translation framework.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3195">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently there has been increased attention to use semantic information in machine translation.
</prevsent>
<prevsent>liu and gildea (2008), <papid> W08-0308 </papid>liu and gildea (2010) <papid> C10-1081 </papid>proposed using semantic role labels (srl) in their tree-to-string machine translation system and demonstrated improvement over conventional tree-to-string methods.</prevsent>
</prevsection>
<citsent citstr=" N09-2004 ">
wu and fung (2009) <papid> N09-2004 </papid>developed framework to reorder the output using information from both the source and the target srl labels.</citsent>
<aftsection>
<nextsent>in this paper, we explore an approach of using the target side srl information in addition to hierarchical phrase-based machine translation framework.
</nextsent>
<nextsent>the proposed method extracts initial phrases with two different heuristics: the first heuristic is used to extract rules that have general left-hand-side (lhs) non-terminal tag , 107 second we must build flood prevention system , strengthen pre-flood inspections and implement flood prevention measures arg0 mod arg0 mod arg0 mod pred pred arg1 arg1 pred arg1 figure 1: example of predicate-argument structure in sentence i.e., hiero rules.
</nextsent>
<nextsent>the second will extract phrases that contain information of srl structures.
</nextsent>
<nextsent>the predicate and arguments that the phrase covers will be represented in the lhs non-terminal tags.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3197">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the experimental results are given in section 6, followed by analysis and conclusion in section 7.
</prevsent>
<prevsent>2.1 hierarchical phrase-based machine.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
translation proposed by chiang (2005), <papid> P05-1033 </papid>the hierarchicalphrase-based machine translation model (commonly known as the hiero model) has achieved results comparable, if not superior, to conventional phrase-based approaches.</citsent>
<aftsection>
<nextsent>the basic idea is to treat the translation as synchronous parsing problem.
</nextsent>
<nextsent>using the source side terminals as input, the decoder tries to build parse tree and synchronously generate target side terminals.
</nextsent>
<nextsent>the rules that generates such synchronous parse trees are in the following form: ?
</nextsent>
<nextsent>(f1 x1 f2 x2 f3, e1 x2 e2 x1 e3)where x1 and x2 are non-terminals, and the sub scripts represents the correspondence between thenon-terminals.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3203">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the tags will restrict the derivation of the target side parse tree to accept only srl structures we have seen in the training corpus.
</prevsent>
<prevsent>the mapping from srl structures to non-terminal tags can be defined according to the srl annotation set.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
in this paper we adopt the propbank (palmer et al ., 2005) <papid> J05-1004 </papid>annotation set of semantic labels, because the annotation set is relatively simple and easy to parse.</citsent>
<aftsection>
<nextsent>the small set of argument tags also makes the number of lhs non-terminal tags small, which alleviates the problem of data scarcity.
</nextsent>
<nextsent>however the methodology of this paper is not limited to propbank tags.
</nextsent>
<nextsent>by defining appropriate mapping, it is also possible to use other annotation sets, such as framenet (baker et al , 2002).
</nextsent>
<nextsent>the srl-aware scfg rules are scfg rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3204">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> srl-aware scfg rules.  </section>
<citcontext>
<prevsection>
<prevsent>1the translation is xinjiangs yili holds propaganda drive and the pinyin transliteration is xinjiang daguimo kaizhan mianduimian xuan jiang huodong
</prevsent>
<prevsent>a) sample of valid derivation b) sample of invalid derivation figure 3: example of derivations of sentence we can see from the example in figure 3a), that the srl-aware scfg rules fit perfectly in the scfg framework.
</prevsent>
</prevsection>
<citsent citstr=" W08-0510 ">
therefore no modification need to be made on decoder, such as moses chart decoder,forinstance (hoang and koehn, 2008).<papid> W08-0510 </papid></citsent>
<aftsection>
<nextsent>the main problem is how to extract the srl-aware scfg rules from the corpus and estimate the feature values so that it works together with the conventional hiero rules.
</nextsent>
<nextsent>in the next two sections we will present therule extraction algorithm and two alternative methods for comparison.
</nextsent>
<nextsent>the hiero rule extraction algorithm uses the following steps: 1.
</nextsent>
<nextsent>extract the initial phrases with the commonly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3207">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> decoder integration.  </section>
<citcontext>
<prevsection>
<prevsent>tokens including terminals and non-terminals.
</prevsent>
<prevsent>the extracted scfg rules, both srl-aware and , will go through the feature estimation process to produce the rule table.
</prevsent>
</prevsection>
<citsent citstr=" P10-4002 ">
integrated with the conversion rules, most chart-based decoders such as moses chart (hoang and koehn, 2008), <papid> W08-0510 </papid>cdec (dyer et al  2010) <papid> P10-4002 </papid>and joshua (li et al  2009) <papid> W09-0424 </papid>can use these rules in decoding.</citsent>
<aftsection>
<nextsent>we applied moses chart for tuning and decoding.while the srl-aware scfg rules are used to constrain the search space and derivation, we do not in 112 mt02 mt03 mt04 mt05 mt08 bl-nw bl-wb dv-nw dv-wb avg bleu 29.56 27.02 30.28 26.80 21.16 21.96 20.10 24.26 20.13 n/a baseline ter 68.87 70.19 67.18 70.60 69.93 64.44 64.74 63.21 66.61 n/a (t-b)/2 19.66 21.59 18.45 21.90 24.39 21.24 22.32 19.48 23.24 n/a bleu +0.33 0.50 +0.20 +0.47 0.16 +1.24 +1.13 +0.39 +1.35 +0.49 srl ter 1.58 1.77 1.93 1.68 0.71 0.29 0.22 1.36 1.34 1.21 (t-b)/2 0.95 0.63 1.07 1.08 0.28 0.76 0.68 0.88 1.35 0.85 table 1: experiment results on chinese-english translation tasks, bl-nw and bl-wb are newswire and weblog parts for dev07-blind, dv-nw and dv-wb are newswire and weblog parts for dev07-dev.
</nextsent>
<nextsent>we present the bleu scores, ter scores and (ter-bleu)/2.
</nextsent>
<nextsent>troduce new features into the system.
</nextsent>
<nextsent>the feature swe used in the decoder are commonly used, including source and target rule translation probabilities, the lexical translation probabilities, and the language model probability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3208">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> decoder integration.  </section>
<citcontext>
<prevsection>
<prevsent>tokens including terminals and non-terminals.
</prevsent>
<prevsent>the extracted scfg rules, both srl-aware and , will go through the feature estimation process to produce the rule table.
</prevsent>
</prevsection>
<citsent citstr=" W09-0424 ">
integrated with the conversion rules, most chart-based decoders such as moses chart (hoang and koehn, 2008), <papid> W08-0510 </papid>cdec (dyer et al  2010) <papid> P10-4002 </papid>and joshua (li et al  2009) <papid> W09-0424 </papid>can use these rules in decoding.</citsent>
<aftsection>
<nextsent>we applied moses chart for tuning and decoding.while the srl-aware scfg rules are used to constrain the search space and derivation, we do not in 112 mt02 mt03 mt04 mt05 mt08 bl-nw bl-wb dv-nw dv-wb avg bleu 29.56 27.02 30.28 26.80 21.16 21.96 20.10 24.26 20.13 n/a baseline ter 68.87 70.19 67.18 70.60 69.93 64.44 64.74 63.21 66.61 n/a (t-b)/2 19.66 21.59 18.45 21.90 24.39 21.24 22.32 19.48 23.24 n/a bleu +0.33 0.50 +0.20 +0.47 0.16 +1.24 +1.13 +0.39 +1.35 +0.49 srl ter 1.58 1.77 1.93 1.68 0.71 0.29 0.22 1.36 1.34 1.21 (t-b)/2 0.95 0.63 1.07 1.08 0.28 0.76 0.68 0.88 1.35 0.85 table 1: experiment results on chinese-english translation tasks, bl-nw and bl-wb are newswire and weblog parts for dev07-blind, dv-nw and dv-wb are newswire and weblog parts for dev07-dev.
</nextsent>
<nextsent>we present the bleu scores, ter scores and (ter-bleu)/2.
</nextsent>
<nextsent>troduce new features into the system.
</nextsent>
<nextsent>the feature swe used in the decoder are commonly used, including source and target rule translation probabilities, the lexical translation probabilities, and the language model probability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3209">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>we filter the corpus with maximum sentence length be 30.
</prevsent>
<prevsent>the corpus has 2.5 million words in chinese side and 3.1 million on english side.
</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
we adopted the assert semantic role labeler(pradhan et al , 2004) <papid> N04-1030 </papid>to label the english side sen tences.</citsent>
<aftsection>
<nextsent>the parallel sentences are aligned using mgiza++ (gao and vogel, 2008) <papid> W08-0509 </papid>and then the proposed rule extraction algorithm was used in extracting the srl-aware scfg rules.</nextsent>
<nextsent>we used the moses chart decoder (hoang and koehn, 2008) <papid> W08-0510 </papid>and the moses toolkit (koehn et al  2007) <papid> P07-2045 </papid>for tuning and decoding.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3210">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the corpus has 2.5 million words in chinese side and 3.1 million on english side.
</prevsent>
<prevsent>we adopted the assert semantic role labeler(pradhan et al , 2004) <papid> N04-1030 </papid>to label the english side sen tences.</prevsent>
</prevsection>
<citsent citstr=" W08-0509 ">
the parallel sentences are aligned using mgiza++ (gao and vogel, 2008) <papid> W08-0509 </papid>and then the proposed rule extraction algorithm was used in extracting the srl-aware scfg rules.</citsent>
<aftsection>
<nextsent>we used the moses chart decoder (hoang and koehn, 2008) <papid> W08-0510 </papid>and the moses toolkit (koehn et al  2007) <papid> P07-2045 </papid>for tuning and decoding.</nextsent>
<nextsent>the language model is trigram language model trained on english gigaword corpus (v1 v3) using the srilm toolkit.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3212">
<title id=" W11-1012.xml">utilizing target side semantic role labels to assist hierarchical phrase based machine translation </title>
<section> experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>we adopted the assert semantic role labeler(pradhan et al , 2004) <papid> N04-1030 </papid>to label the english side sen tences.</prevsent>
<prevsent>the parallel sentences are aligned using mgiza++ (gao and vogel, 2008) <papid> W08-0509 </papid>and then the proposed rule extraction algorithm was used in extracting the srl-aware scfg rules.</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
we used the moses chart decoder (hoang and koehn, 2008) <papid> W08-0510 </papid>and the moses toolkit (koehn et al  2007) <papid> P07-2045 </papid>for tuning and decoding.</citsent>
<aftsection>
<nextsent>the language model is trigram language model trained on english gigaword corpus (v1 v3) using the srilm toolkit.
</nextsent>
<nextsent>we used the nist mt06 test set for tuning, and experimented with an additional 9 test sets, including mt02, 03, 04, 05, 08, and gale test sets dev07-dev and dev07-blind.
</nextsent>
<nextsent>dev07-dev and dev07-blind are further divided into newswire and weblog parts.
</nextsent>
<nextsent>we experimented with the proposed method and the alternative methods presented in section 4, and the results of nine test sets are listed in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3213">
<title id=" W11-0704.xml">contextual bearing on linguistic variation in social media </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>although our work is primarily focused on analyzing the lexical variation in language found in on line social media, our analysis methodology makes strong use of techniques for normalizing noisy text?
</prevsent>
<prevsent>such as sms-messages and twitter messages into standard english.
</prevsent>
</prevsection>
<citsent citstr=" C08-1056 ">
normalizing text can traditionally be approached using three well-known nlp metaphors, namely that of spell-checking, machine translation (mt) and automatic speech recognition (asr) (kobus et al, 2008).<papid> C08-1056 </papid></citsent>
<aftsection>
<nextsent>in the spell-checking approach, corrections fromnoisy?
</nextsent>
<nextsent>words to clean?
</nextsent>
<nextsent>words proceed on word by-word basis.
</nextsent>
<nextsent>choudhury (2007) implements the noisy channel model (shannon and weaver, 1948) using hidden markov model to handle both graphemic and phonemic variations, and cook and stevenson (2009) <papid> W09-2010 </papid>improve on this model by adapting the channel noise according to several predefined word formations such as stylistic variation, word clipping, etc. however, spelling correction is traditionally conducted in media with relatively high percentages of well-formed text where one can perform word boundary detection and thus tokenization to high degree of accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3215">
<title id=" W11-0704.xml">contextual bearing on linguistic variation in social media </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>words to clean?
</prevsent>
<prevsent>words proceed on word by-word basis.
</prevsent>
</prevsection>
<citsent citstr=" W09-2010 ">
choudhury (2007) implements the noisy channel model (shannon and weaver, 1948) using hidden markov model to handle both graphemic and phonemic variations, and cook and stevenson (2009) <papid> W09-2010 </papid>improve on this model by adapting the channel noise according to several predefined word formations such as stylistic variation, word clipping, etc. however, spelling correction is traditionally conducted in media with relatively high percentages of well-formed text where one can perform word boundary detection and thus tokenization to high degree of accuracy.</citsent>
<aftsection>
<nextsent>the main drawback is the strong confidence this approach places on word boundaries (beaufort et al, 2010), <papid> P10-1079 </papid>since detecting word boundaries in noisy text is not trivial prob lem.</nextsent>
<nextsent>in the machine translation approach (bangalore et al, 2002; <papid> C02-1134 </papid>aw et al, 2006), <papid> P06-2005 </papid>normalizing noisy text is considered as translation task from source language (the noisy text) to target language (the cleansed text).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3216">
<title id=" W11-0704.xml">contextual bearing on linguistic variation in social media </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>words proceed on word by-word basis.
</prevsent>
<prevsent>choudhury (2007) implements the noisy channel model (shannon and weaver, 1948) using hidden markov model to handle both graphemic and phonemic variations, and cook and stevenson (2009) <papid> W09-2010 </papid>improve on this model by adapting the channel noise according to several predefined word formations such as stylistic variation, word clipping, etc. however, spelling correction is traditionally conducted in media with relatively high percentages of well-formed text where one can perform word boundary detection and thus tokenization to high degree of accuracy.</prevsent>
</prevsection>
<citsent citstr=" P10-1079 ">
the main drawback is the strong confidence this approach places on word boundaries (beaufort et al, 2010), <papid> P10-1079 </papid>since detecting word boundaries in noisy text is not trivial prob lem.</citsent>
<aftsection>
<nextsent>in the machine translation approach (bangalore et al, 2002; <papid> C02-1134 </papid>aw et al, 2006), <papid> P06-2005 </papid>normalizing noisy text is considered as translation task from source language (the noisy text) to target language (the cleansed text).</nextsent>
<nextsent>since noisy- and clean text typically vary wildly, it satisfies the notion of translating between two languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3217">
<title id=" W11-0704.xml">contextual bearing on linguistic variation in social media </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>choudhury (2007) implements the noisy channel model (shannon and weaver, 1948) using hidden markov model to handle both graphemic and phonemic variations, and cook and stevenson (2009) <papid> W09-2010 </papid>improve on this model by adapting the channel noise according to several predefined word formations such as stylistic variation, word clipping, etc. however, spelling correction is traditionally conducted in media with relatively high percentages of well-formed text where one can perform word boundary detection and thus tokenization to high degree of accuracy.</prevsent>
<prevsent>the main drawback is the strong confidence this approach places on word boundaries (beaufort et al, 2010), <papid> P10-1079 </papid>since detecting word boundaries in noisy text is not trivial prob lem.</prevsent>
</prevsection>
<citsent citstr=" C02-1134 ">
in the machine translation approach (bangalore et al, 2002; <papid> C02-1134 </papid>aw et al, 2006), <papid> P06-2005 </papid>normalizing noisy text is considered as translation task from source language (the noisy text) to target language (the cleansed text).</citsent>
<aftsection>
<nextsent>since noisy- and clean text typically vary wildly, it satisfies the notion of translating between two languages.
</nextsent>
<nextsent>however, since these transformations can be highly creative, they usually need wide context (more than one word) to be resolvedadequately.
</nextsent>
<nextsent>kobus (2008) also points out that despite the fairly good results achieved with this system, such purely phrase-based translation model cannot adequately handle the wide level of lexical creativity found in these media.finally, the asr approach is based on the observation that many noisy word forms in smses or other noisy text are based on phonetic plays ofthe clean word.
</nextsent>
<nextsent>this approach starts by converting the input message into phone lattice, whichis converted to word lattice using phonemegrapheme dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3218">
<title id=" W11-0704.xml">contextual bearing on linguistic variation in social media </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>choudhury (2007) implements the noisy channel model (shannon and weaver, 1948) using hidden markov model to handle both graphemic and phonemic variations, and cook and stevenson (2009) <papid> W09-2010 </papid>improve on this model by adapting the channel noise according to several predefined word formations such as stylistic variation, word clipping, etc. however, spelling correction is traditionally conducted in media with relatively high percentages of well-formed text where one can perform word boundary detection and thus tokenization to high degree of accuracy.</prevsent>
<prevsent>the main drawback is the strong confidence this approach places on word boundaries (beaufort et al, 2010), <papid> P10-1079 </papid>since detecting word boundaries in noisy text is not trivial prob lem.</prevsent>
</prevsection>
<citsent citstr=" P06-2005 ">
in the machine translation approach (bangalore et al, 2002; <papid> C02-1134 </papid>aw et al, 2006), <papid> P06-2005 </papid>normalizing noisy text is considered as translation task from source language (the noisy text) to target language (the cleansed text).</citsent>
<aftsection>
<nextsent>since noisy- and clean text typically vary wildly, it satisfies the notion of translating between two languages.
</nextsent>
<nextsent>however, since these transformations can be highly creative, they usually need wide context (more than one word) to be resolvedadequately.
</nextsent>
<nextsent>kobus (2008) also points out that despite the fairly good results achieved with this system, such purely phrase-based translation model cannot adequately handle the wide level of lexical creativity found in these media.finally, the asr approach is based on the observation that many noisy word forms in smses or other noisy text are based on phonetic plays ofthe clean word.
</nextsent>
<nextsent>this approach starts by converting the input message into phone lattice, whichis converted to word lattice using phonemegrapheme dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3222">
<title id=" W11-0704.xml">contextual bearing on linguistic variation in social media </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>#wasted?).
</prevsent>
<prevsent>following kaufmann (2010) we remove hash tags at the end of messages when they are preceded by typical end-of-sentence punctuation marks.
</prevsent>
</prevsection>
<citsent citstr=" W02-0109 ">
hash tags in the middle of messages are retained, and the hash sign removed.next we tokenize this preprocessed message using the nltk tokenizer (loper and bird, 2002).<papid> W02-0109 </papid></citsent>
<aftsection>
<nextsent>as noted earlier, standard nlp tools do not perform well on noisy text out-of-the-box.
</nextsent>
<nextsent>based on inspection of incorrectly tokenized output, we therefore include post-tokenization phase where we split all tokens that include punctuation symbol into the individual one or two alphanumeric tokens (on either side of the punctuation symbol), and the punctuation symbol1.
</nextsent>
<nextsent>this heuristic catches most cases of run-on sentences.
</nextsent>
<nextsent>given set of input tokens, we process these one by one, by comparing each token to the words in the lexicon and constructing confusion network cn.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3223">
<title id=" W11-0704.xml">contextual bearing on linguistic variation in social media </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>this expands, for example, t0day??
</prevsent>
<prevsent>[t0day?, today?], and also 2day??
</prevsent>
</prevsection>
<citsent citstr=" C10-2022 ">
[2day?, twoday?, today?], etc.each transliterated candidate word in each confusion set produced this way is then scored with the original word and ranked using the heuristic function (sim()) described in (contractor et al,2010)<papid> C10-2022 </papid>2.</citsent>
<aftsection>
<nextsent>we also evaluated purely phonetic edit distance similarity function, based on the double meta phone algorithm (philips, 2000), but found thestring-similarity-based function to give more reliable results.each confusion set produced this way (see algorithm 2) is joined to its previous set to form agr owing confusion lattice.
</nextsent>
<nextsent>finally this lattice is decoded by converting it into the probabilistic finite state grammar format, and by using the sri-lmtoolkits (stolcke, 2002) lattice-tool command to find the best path through the lattice by 2the longest common sub sequence between the two words,normalized by the edit distances between their consonant skeletons.
</nextsent>
<nextsent>23 transformation type rel % single char (see??
</nextsent>
<nextsent>c?) 29.1% suffix (why??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3224">
<title id=" W10-4138.xml">term contributed boundary tagging by conditional random fields for sighan 2010 chinese word segmentation bakeoff </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>character-based tagging of.
</prevsent>
<prevsent>conditional random field as baseline the character-based obi?
</prevsent>
</prevsection>
<citsent citstr=" W03-1728 ">
tagging of conditional random field (lafferty et al, 2001) has been widely used in chinese word segmentation recently (xue and shen, 2003; <papid> W03-1728 </papid>peng and mccallum, 2004; tseng et al, 2005).<papid> I05-3027 </papid></citsent>
<aftsection>
<nextsent>under the scheme, each character of word is labeled as b? if it is the first character of multiple-character word, or i? otherwise.
</nextsent>
<nextsent>if the character is single-character word itself, o? will be its label.
</nextsent>
<nextsent>as table 1 shows, the lost of performance is about 1% by replacing o? with b? for character-based crf tagging on the dataset of cips-sighan-2010 bakeoff task of chinese word segmentation, thus we choose bi?
</nextsent>
<nextsent>as our baseline for simplicity, with this 1% lost bearing in mind.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3225">
<title id=" W10-4138.xml">term contributed boundary tagging by conditional random fields for sighan 2010 chinese word segmentation bakeoff </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>character-based tagging of.
</prevsent>
<prevsent>conditional random field as baseline the character-based obi?
</prevsent>
</prevsection>
<citsent citstr=" I05-3027 ">
tagging of conditional random field (lafferty et al, 2001) has been widely used in chinese word segmentation recently (xue and shen, 2003; <papid> W03-1728 </papid>peng and mccallum, 2004; tseng et al, 2005).<papid> I05-3027 </papid></citsent>
<aftsection>
<nextsent>under the scheme, each character of word is labeled as b? if it is the first character of multiple-character word, or i? otherwise.
</nextsent>
<nextsent>if the character is single-character word itself, o? will be its label.
</nextsent>
<nextsent>as table 1 shows, the lost of performance is about 1% by replacing o? with b? for character-based crf tagging on the dataset of cips-sighan-2010 bakeoff task of chinese word segmentation, thus we choose bi?
</nextsent>
<nextsent>as our baseline for simplicity, with this 1% lost bearing in mind.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3226">
<title id=" W10-4138.xml">term contributed boundary tagging by conditional random fields for sighan 2010 chinese word segmentation bakeoff </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>is followed by en quiries?
</prevsent>
<prevsent>irrespective of what precedes it.
</prevsent>
</prevsection>
<citsent citstr=" N06-2049 ">
this problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping ngrams still appear, therefore corresponding solutions such as those of zhang et al (2006) <papid> N06-2049 </papid>were proposed.</citsent>
<aftsection>
<nextsent>we uses suffix array algorithm to calculate exact boundaries of phrase and their frequencies (sung et al, 2008), called term contributed boundaries (tcb) and term contributed frequencies (tcf), respectively, to analogize similarities and differences with the term frequencies (tf).
</nextsent>
<nextsent>for example, in vodis corpus, the original tf of the term rail enquiries?
</nextsent>
<nextsent>is 73.
</nextsent>
<nextsent>however, the actual tcf of rail enqui ries?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3227">
<title id=" W11-1722.xml">towards a unified approach for opinion question answering and summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>research focused on building factoid qa systems has long tradition, however, it is only recently that studies have started to focus on the creation and development of opinion qa systems.
</prevsent>
<prevsent>example of this can be (stoyanov et al, 2004) who took advantage of opinion summarization to support multi-perspective qa system, aiming at extracting opinion-oriented information of question.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
(yu and hatzivassiloglou,2003) <papid> W03-1017 </papid>separated opinions from facts and summarized them as answer to opinion questions.</citsent>
<aftsection>
<nextsent>apart from these studies, specialized competitions for systems dealing with opinion retrieval and qa have been organized in the past few years.
</nextsent>
<nextsent>the tac 2008 opinion summarization pilot track proposed.
</nextsent>
<nextsent>a mixed setting of factoid and opinion questions.
</nextsent>
<nextsent>1http://www.nist.gov/tac/2008/summarization/ 168it is interesting to note that most of the participating systems only adapted their factual qa system sto overcome the newly introduced difficulties related to opinion mining and polarity classification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3228">
<title id=" W11-1722.xml">towards a unified approach for opinion question answering and summarization </title>
<section> an opinion question answering and.  </section>
<citcontext>
<prevsection>
<prevsent>having created the extended set of nuggets, we groupe dall of them pertaining to the same topic, and considered it gold-standard summary.
</prevsent>
<prevsent>now, the average number of nuggets per topic is 53, which we have increased by twice the number of original nuggets provided at tac.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
further on, our summaries are compared against this new gold-standard using rouge (lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>this tool computes the number of different kinds of overlap n-grams between an automatic summary and human-made summary.
</nextsent>
<nextsent>for our evaluation,we compute rouge-1 (unigrams), rouge-2 (bi grams), rouge-su4 (it measures the overlap of skip-bigrams between candidate summary and set of reference summaries with maximum skip distance of 4), and rouge-l (longest common sub sequence between two texts).
</nextsent>
<nextsent>the results and discussion are next provided.
</nextsent>
<nextsent>4.4 results and discussion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3229">
<title id=" W11-0135.xml">towards component based textual entailment </title>
<section> component-based te framework.  </section>
<citcontext>
<prevsection>
<prevsent>the definition presented above provides strong interpretation of the compositional framework forte, that can be described as continuum that tends towards systems developed combining identifiable and separable components addressing specific inference types.
</prevsent>
<prevsent>a number of works in the literature can be placed along this continuum, according to how much they get closer to this interpretation.
</prevsent>
</prevsection>
<citsent citstr=" W07-1412 ">
systems addressing te exploiting machine learning techniques with variety of features, including lexical-syntactic and semantic features (e.g. kozareva and montoyo (2006), zanzotto et al (2007)) <papid> W07-1412 </papid>tend towards the opposite extreme of this framework, since even if linguistic features are used, they bring information about specific aspect relevant to the inference task but they do not provide an independent judgment on it.</citsent>
<aftsection>
<nextsent>these systems are not modular, and it is difficult to assess the contribution of certain feature in providing the correct overall judgment for pair.
</nextsent>
<nextsent>a step closer towards the direction of component-based te is done by bar-haim et al (2008), that model semantic inference as application of entailment rules specifying the generation of entailed sentences from source sentence.
</nextsent>
<nextsent>such rules capture semantic knowledge about linguistic phenomena (e.g. paraphrases, synonyms), and are applied in transformation-based framework.
</nextsent>
<nextsent>even if these rules are clearly identifiable, their application perse does not provide any judgment about an existing entailment relation between and h.a component-based system has been developed by wang and neumann (2008), based on three specialized rte-modules: (i) to tackle temporal expressions; (ii) to deal with other types of nes; (iii) to deal with cases with two arguments for each event.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3230">
<title id=" W11-0135.xml">towards component based textual entailment </title>
<section> component-based te framework.  </section>
<citcontext>
<prevsection>
<prevsent>our definition abstracts from the different theories underlying the categorization of linguistic phenomena, so straightforward relation between te component and linguistic phenomena cannot be defined priori.
</prevsent>
<prevsent>some work has already been done in investigating indepth sub-aspects of entailment, and in developing ad hoc resources to assess the impact of systems components created to address specific inference types.
</prevsent>
</prevsection>
<citsent citstr=" W07-1409 ">
earlier works in the field (e.g. vanderwende et al (2005), clark et al (2007)) <papid> W07-1409 </papid>carried out partial analysis of the datasets in order to evaluate how many entailment examples could be accurately predicted relying only on lexical,syntactic or world knowledge.</citsent>
<aftsection>
<nextsent>bar-haim et al (2005) defined two intermediate models of textual entailment, corresponding to lexical and lexical-syntactic levels of representation, and sample from rte-1 dataset was annotated according to each model.
</nextsent>
<nextsent>a step further, other rte groups have developed focused datasets with the aim of investigating and experimenting on specific phenomena underlying language variability.
</nextsent>
<nextsent>for instance, to evaluate contradiction detection module marneffe et al (2008) created corpus where contradictions arise from negation, by adding negative markers to the rte-2 test data.
</nextsent>
<nextsent>kirk (2009) describes his work of building an inference corpus for spatial inference about motion, while akhmatova and dras (2009) experiment current approaches on hypernymy acquisition to improve entailment classification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3231">
<title id=" W11-0407.xml">a collaborative annotation between human annotators and a statistical parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition,we implemented prototype system and conducted manual annotation experiments on small test set.
</prevsent>
<prevsent>on the basis of the success of the research on the corpus-based development in nlp, the demand for variety of corpora has increased, for use as both at raining resource and an evaluation data-set.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
however, the development of richly annotated corpus such as an hpsg treebank is not an easy task, since the traditional two-step annotation, in which parser first generates the candidates and then an annotator checks each candidate, needs intensive efforts even for well-trained annotators (marcus et al, 1994; <papid> H94-1020 </papid>kurohashi and nagao, 1998).</citsent>
<aftsection>
<nextsent>among many nlp problems, adapting parser for out-domaintexts, which is usually referred to as domain adaptation problem, is one of the most remarkable problems.
</nextsent>
<nextsent>the main cause of this problem is the lack of corpora in that domain.
</nextsent>
<nextsent>because it is difficult to prepare sufficient corpus for each domain without reducing the annotation cost, research on annotation methodologies has been intensively studied.
</nextsent>
<nextsent>there has been number of research projects to efficiently develop richly annotated corpora with the help of parsers, one of which is called discriminant-based tree banking (carter, 1997).<papid> W97-1502 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3232">
<title id=" W11-0407.xml">a collaborative annotation between human annotators and a statistical parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main cause of this problem is the lack of corpora in that domain.
</prevsent>
<prevsent>because it is difficult to prepare sufficient corpus for each domain without reducing the annotation cost, research on annotation methodologies has been intensively studied.
</prevsent>
</prevsection>
<citsent citstr=" W97-1502 ">
there has been number of research projects to efficiently develop richly annotated corpora with the help of parsers, one of which is called discriminant-based tree banking (carter, 1997).<papid> W97-1502 </papid></citsent>
<aftsection>
<nextsent>indiscriminant-based tree banking, the annotation process consists of two steps: parser first generates the parse trees, which are annotation candidates,and then human annotator selects the most plausible one.
</nextsent>
<nextsent>one of the most important characteristics of this methodology is to use easily-understandablequestions called discriminants for picking up the final annotation results.
</nextsent>
<nextsent>human annotators can perform annotations simply by answering those questions without closely examining the whole tree.
</nextsent>
<nextsent>although this approach has been successful in breaking down the difficult annotations into set of easy questions, specific knowledge about the grammar,especially in the case of deep grammar, is still required for an annotator.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3233">
<title id=" W11-0407.xml">a collaborative annotation between human annotators and a statistical parser </title>
<section> statistical deep parser.  </section>
<citcontext>
<prevsection>
<prevsent>in our system, we aim at building hpsg treebanks with low-cost in which even non experts can perform annotations.
</prevsent>
<prevsent>2.2 hpsg deep parser.
</prevsent>
</prevsection>
<citsent citstr=" W07-2208 ">
the enju parser (ninomiya et al, 2007) <papid> W07-2208 </papid>is statistical deep parser based on the hpsg formalism.</citsent>
<aftsection>
<nextsent>it produces an analysis of sentence that includes the 2 64 head noun subj    comps    3 75 dogs 2 64 head verb subj   noun   comps    3 75 drung ? 2 64 head verb subj    comps    3 75 subject 1 2 64 head noun subj    comps    3 75 headj 2 664 head verb subj   1   comps    3 775 figure 1: example of hpsg parsing for dogs run.?
</nextsent>
<nextsent>syntactic structure (i.e., parse tree) and the semantic structure represented as set of predicate-argument dependencies.
</nextsent>
<nextsent>the grammar design is based on the standard hpsg analysis of english (pollard and sag, 1994).
</nextsent>
<nextsent>the parser finds best parse tree scored by maxent disambiguation model using cky-style algorithm and beam search.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3234">
<title id=" W11-0407.xml">a collaborative annotation between human annotators and a statistical parser </title>
<section> validation of cfg-based annotation.  </section>
<citcontext>
<prevsection>
<prevsent>tion is that the stochastic model of the hpsg parser properly resolves the remaining ambiguities in the hpsg annotation within the constraints given by part of the cfg trees.
</prevsent>
<prevsent>in order to check the validity of this expectation and to measure to what extent the cfg-based annotations can achieve correct hpsg annotations, we performed pseudo-annotation experiment.
</prevsent>
</prevsection>
<citsent citstr=" P10-2013 ">
in this experiment, we used bracketed sentences in the brown corpus (kucera and francis, 1967),and court transcript portion of the manually annotated sub-corpus (masc) (ide et al, 2010).<papid> P10-2013 </papid></citsent>
<aftsection>
<nextsent>we automatically created hpsg annotations that mimic the annotation results by an ideal annotator in the following four steps.
</nextsent>
<nextsent>first, hpsg treebanks for these sentences are created by the treebank conversion program distributed with the enju parser.
</nextsent>
<nextsent>this program converts syntactic tree annotated by penn treebank style into an hpsg tree.
</nextsent>
<nextsent>since this program cannot convert the sentences that are not covered by the basic design of the grammar, we used only those that are successfully converted by the program throughout the experiments and considered this converted treebank as the gold-standard tree bank for evaluation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3235">
<title id=" W11-0407.xml">a collaborative annotation between human annotators and a statistical parser </title>
<section> validation of cfg-based annotation.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 domain adaptation.
</prevsent>
<prevsent>we evaluated the parser accuracy adapted with the automatically created treebank on the brown corpus.
</prevsent>
</prevsection>
<citsent citstr=" W07-2202 ">
in this experiment, we used the adaptation algorithm by (hara et al, 2007), <papid> W07-2202 </papid>with the same hyper parameters used there.</citsent>
<aftsection>
<nextsent>table 5 shows the result of the adapted parser.
</nextsent>
<nextsent>each line of this table stands for the parser adapted with different data.
</nextsent>
<nextsent>gold?
</nextsent>
<nextsent>is the result adapted on the gold-standard annotations, and gold (only covered)?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3236">
<title id=" W11-0407.xml">a collaborative annotation between human annotators and a statistical parser </title>
<section> interactive annotation on a.  </section>
<citcontext>
<prevsection>
<prevsent>all of the sentences were annotated twice by two annotators.
</prevsent>
<prevsent>both of the annotators has background in computer science and lin guistics.table 6 shows the statistics of the annotation procedures.
</prevsent>
</prevsection>
<citsent citstr=" C10-2166 ">
this table indicates that human annotators strongly prefer s? operation to others, and that the manual annotation on the prototype system is at least comparable to the recent discriminant-based annotation system by (zhang and kordoni, 2010), <papid> C10-2166 </papid>although the comparison is not strict because of the difference of the text.</citsent>
<aftsection>
<nextsent>table 7 shows the automatic evaluation results.
</nextsent>
<nextsent>we can see that the interactive annotation gave slight improvements in all accuracy metrics.
</nextsent>
<nextsent>the improvements were however not as much as we desired.by classifying the remaining errors in the annotation results, we identified several classes of major errors: 1.
</nextsent>
<nextsent>truly ambiguous structures, which require the.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3237">
<title id=" W11-1309.xml">measuring the compositionality of collocations via word cooccurrence vectors shared task system description </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>collocations or multiword expressions vary in the degree to which native speaker is able to understand them based on the interaction of their constituents?
</prevsent>
<prevsent>individual meanings.
</prevsent>
</prevsection>
<citsent citstr=" W11-1304 ">
the concept of com positionality of collocation captures this notion.the shared task of the disco 2011 workshop (bie mann and giesbrecht, 2011) <papid> W11-1304 </papid>consists in comparingsystems?</citsent>
<aftsection>
<nextsent>compositionality scores against compositionality scores based on human judgements.
</nextsent>
<nextsent>systems were evaluated on the match of the compositional scores generated by the system and those based on human judgements ? specifically taking themean of the absolute difference of these scores.
</nextsent>
<nextsent>additionally the organisers also classified the human derived scores into three coarse categories of com positionality: non-compositional (low), somewhat compositional (medium) and compositional (high).
</nextsent>
<nextsent>systems were required to produce an additional compositionality labelling into these three coarse categories and were evaluated on the precision of this labelling.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3238">
<title id=" W11-1309.xml">measuring the compositionality of collocations via word cooccurrence vectors shared task system description </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>additionally the organisers also classified the human derived scores into three coarse categories of com positionality: non-compositional (low), somewhat compositional (medium) and compositional (high).
</prevsent>
<prevsent>systems were required to produce an additional compositionality labelling into these three coarse categories and were evaluated on the precision of this labelling.
</prevsent>
</prevsection>
<citsent citstr=" W03-1810 ">
the methods used by our system for measuring compositionality take inspiration from the work of mccarthy et al (2003), <papid> W03-1810 </papid>who measured the similarity between phrasal verb (a main verb and preposition like blow up) and its main verb (blow)by comparing the words that are closely semantically related to each, and use this similarity as an indicator of compositionality.</citsent>
<aftsection>
<nextsent>our method for measuring compositionality is considerably different asit instead directly compares the semantic similarity between the headword and the collocation and between the modifier and the collocation by computing cosine similarity score between word cooccurrence vectors that represent the headword, the modifier and the collocation (see 3.2).
</nextsent>
<nextsent>our system can be regarded as fully unsupervised as it does not employ any parsers in its processing or any external data other than the corpus and the collocation lists provided by the organisers.the rest of the paper is organised as follows: section 2 describes the corpora and the collocation list provided by the task organisers.
</nextsent>
<nextsent>section 3 introduces some definitions and describes the three configurations in detail.
</nextsent>
<nextsent>section 4 presents the results and concludes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3239">
<title id=" W11-1309.xml">measuring the compositionality of collocations via word cooccurrence vectors shared task system description </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>all three rely in representing words and collocations as word co-occurrence vectors and measure semantic similarity using the cosine measure.
</prevsent>
<prevsent>3.1 preliminary definitions.
</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
these definitions are largely based on the construction of first-order context vectors, word cooccurrence vectors and second-order context vectors via global selection as described in schtze (1998)and in purandare and pedersen (2004) <papid> W04-2406 </papid>by considering context windows of 20 words centred at target word.the first-order context vector is vector representing token of word, or equivalently position in document.</citsent>
<aftsection>
<nextsent>dimensions of the vector are word types w, and the value on dimension is count of the frequency with which occurs in specified window around in given document doc.
</nextsent>
<nextsent>c1(p)(w) = ? p? 6=p p10p? pp+10 (1 if = doc(p?), else 0) (1)in this work the dimensions are the 2,000 non function words that are most frequent in the corpus1.
</nextsent>
<nextsent>the word co-occurrence vector (or simply wordvector) is vector recording the co-occurrence behaviour of particular word type in corpus.
</nextsent>
<nextsent>as 1we employ modified version of the stop word list supplied with ted pedersens text-nsp package (http://www.d.umn.edu/~tpederse/nsp.html) such it can be defined by summation over first-order context vectors: w(w) =?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3240">
<title id=" W11-1309.xml">measuring the compositionality of collocations via word cooccurrence vectors shared task system description </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the tsunami caused great deal of damage to.
</prevsent>
<prevsent>the countrys infrastructure.
</prevsent>
</prevsection>
<citsent citstr=" S10-1081 ">
in word sense induction, clustering is used to group occurrences of target word according to its sense or usage in context (see e.g. pedersen (2010)) <papid> S10-1081 </papid>as it is expected that each cluster will represent different sense or usage of the target word.</citsent>
<aftsection>
<nextsent>however, since the contexts that human annotators referred to when judging the compositionality of the collocations was not provided, our system employs work around that uses weighted average when measuring compositionality.
</nextsent>
<nextsent>this work around is explained in what follows.
</nextsent>
<nextsent>in this configuration, the system first builds word vectors for the 20,000 most frequent words in the corpus (equation 2), and then uses these to compute the second-order context vectors for each occurrence of the collocation and its constituents in the corpus(equation 3).
</nextsent>
<nextsent>after context vectors for all occurrences have been computed, they are clustered usingclutos repeated bisect ions algorithm2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3241">
<title id=" W11-1503.xml">evaluating an offtheshelf pos tagger on early modern german text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, as spelling variants in our corpus have been manually normalised to modern standard, this paper also aims to explore the extent to which tagger performance is affected by spelling variation, and to what degree performance can be improved by using normalised?
</prevsent>
<prevsent>input.
</prevsent>
</prevsection>
<citsent citstr=" W10-2209 ">
our findings promise to be of considerable interest to other current corpus-based projects of earlier periods of german (jurish, 2010; <papid> W10-2209 </papid>fass hauer, 2011; dipper, 2010).before presenting the results in section 4, we describe the corpus design (section 2), and the preprocessing steps necessary to create the gold standard annotations, including adaptations to the pos tagset (section 3).</citsent>
<aftsection>
<nextsent>in order to be as representative of early modern german as possible, the germanc corpus design considers three different levels.
</nextsent>
<nextsent>first, the corpus includes range of text types: four orally-oriented 19 genres (dramas, newspapers, letters, and sermons), and four print-oriented ones (narrative prose, and humanities, scientific, and legal texts).
</nextsent>
<nextsent>secondly, in order to enable historical developments to be traced, the period is divided into three fifty year sections (1650-1700, 1700-1750, and 1750-1800).
</nextsent>
<nextsent>finally,the corpus also aims to be representative with respect to region, including five broad areas: north german, west central, east central, west upper (including switzerland), and east upper german (including austria).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3242">
<title id=" W11-1503.xml">evaluating an offtheshelf pos tagger on early modern german text </title>
<section> creating the gold standard annotations.  </section>
<citcontext>
<prevsection>
<prevsent>the corpus contains 57,845 tokens in total, and was annotated with gold standard pos tags, lemmas, and normalised word forms (scheible et al, to appear).
</prevsent>
<prevsent>this section provides an overview of the preprocessing work necessary to obtain the gold standard annotations in germanc-gs.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
we used the gate platform to produce the initial annotations, which facilitates automatic as well as manual annotation (cunningham et al, 2002).<papid> P02-1022 </papid></citsent>
<aftsection>
<nextsent>first, gates german language plugin1 was used to obtain word tokens and sentence boundaries.
</nextsent>
<nextsent>the output was manually inspected and corrected by one annotator, who further added layer of normalised spelling variants.
</nextsent>
<nextsent>this annotation layer was then used as input for the tree tagger (schmid, 1994), obtaining annotations in terms of pos tags and lemmas.
</nextsent>
<nextsent>all annotations 1http://gate.ac.uk/sale/tao/splitch15.html were subsequently corrected by two annotators, and disagreements were reconciled to produce the gold standard.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3243">
<title id=" W11-1503.xml">evaluating an offtheshelf pos tagger on early modern german text </title>
<section> pwrel: interrogative pronoun used as rel-.  </section>
<citcontext>
<prevsection>
<prevsent>97%; schmid, (1995)), the current results seem relatively low.
</prevsent>
<prevsent>however, two issues should be taken into account when interpreting these findings: first, the modern accuracy figures result from an evaluation of the tagger on the text type it was developed on (newspaper text), while germanc-gs includes variety of genres, which is bound to result in lower performance.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
secondly,inter-annotator agreement was also found to be considerably lower in the present task (91.6%) than in one reported for modern german (98.6%; brants, 2000<papid> A00-1031 </papid>a).</citsent>
<aftsection>
<nextsent>this is likely to be due to the large number of unfamiliar word forms and variants in the corpus, which represent problem for human annotators.
</nextsent>
<nextsent>finally, figure 3 provides more detailed overview of the effects of spelling variation on pos tagger performance.
</nextsent>
<nextsent>of 12,744 normalised tokens in the corpus, almost half (5981; 47%) are only tagged correctly when using the normalised variants as in put.
</nextsent>
<nextsent>using the original word form as input resultsin false pos tag in these cases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3248">
<title id=" W11-1306.xml">identifying collocations to measure compositionality shared task system description </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>identifying collocations is another key capability of nlp systems.
</prevsent>
<prevsent>collocations are generally considered to be units of text that occur with some regularity and may have some non-compositional meaning.
</prevsent>
</prevsection>
<citsent citstr=" W11-1304 ">
the duluth systems that participated in the disco 2011 shared task (biemann and giesbrecht, 2011) <papid> W11-1304 </papid>seek to determine the degree to which collocation identification techniques can be used to measure semantic compositionality.</citsent>
<aftsection>
<nextsent>in particular, these systems are based on the following hypothesis:an ngram that has high score according to measure of association (for identifying collocations) will be less compositional (and less literal) than those that have lower scores.
</nextsent>
<nextsent>the intuition underlying this hypothesis is high score from measure of association shows that the words in the ngram are occurring together more of ten than would be expected by chance, and that non-compositional phrase is unlikely to occur in such way that it looks like chance event.
</nextsent>
<nextsent>the duluth systems were developed by identifying collocations based on frequency counts obtained from the wacky english corpus (baroni et al, 2009), hereafter referred to as the corpus.
</nextsent>
<nextsent>the part of speech tags were removed from the corpus, and the text was converted to lowercase.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3249">
<title id=" W11-1306.xml">identifying collocations to measure compositionality shared task system description </title>
<section> system development.  </section>
<citcontext>
<prevsection>
<prevsent>33 2.1 collocation discovery.
</prevsent>
<prevsent>the ngram statistics package (text::nsp) (baner jee and pedersen, 2003) was used to measure the association between the training pairs based on frequency count data collected from the corpus.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
all thirteen measures in the ngram statistics package were employed, including the log-likelihood ratio (ll) (dunning, 1993), <papid> J93-1003 </papid>pointwise mutual information (pmi) (church and hanks, 1990), <papid> J90-1003 </papid>mutual information (tmi) (church and hanks, 1990), <papid> J90-1003 </papid>poisson stirling (ps) (church, 2000), <papid> C00-1027 </papid>fishers exact test (leftfisher, right fisher, and twotailed) (pedersen etal., 1996), jaccard coefficient (jaccard), dice coefficient (dice), phi coefficient (phi), t-score (tscore) (church and hanks, 1990), <papid> J90-1003 </papid>pearsons chi-squared test (x2), and the odds ratio (odds).</citsent>
<aftsection>
<nextsent>these measure the co-occurrence of word pairs (bigrams) relative to their individual frequencies and assess how likely it is that the word pair is occurring together by chance (and is therefore likely composi tional) or has some significant pattern of occurrence as pair (in which case it is non-compositional).
</nextsent>
<nextsent>more formally, many of these methods compare the observed empirical data with model that casts the words in the bigram as independent statistical events.
</nextsent>
<nextsent>the measures determine the degree to which the observed data deviates from what would be expected under the model of independence.
</nextsent>
<nextsent>if the observed data differs significantly from that, then thereis no evidence to support the hypothesis that the bigram is chance event, and we assume that there is some interesting or significant pattern that implies non-compositionality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3250">
<title id=" W11-1306.xml">identifying collocations to measure compositionality shared task system description </title>
<section> system development.  </section>
<citcontext>
<prevsection>
<prevsent>33 2.1 collocation discovery.
</prevsent>
<prevsent>the ngram statistics package (text::nsp) (baner jee and pedersen, 2003) was used to measure the association between the training pairs based on frequency count data collected from the corpus.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
all thirteen measures in the ngram statistics package were employed, including the log-likelihood ratio (ll) (dunning, 1993), <papid> J93-1003 </papid>pointwise mutual information (pmi) (church and hanks, 1990), <papid> J90-1003 </papid>mutual information (tmi) (church and hanks, 1990), <papid> J90-1003 </papid>poisson stirling (ps) (church, 2000), <papid> C00-1027 </papid>fishers exact test (leftfisher, right fisher, and twotailed) (pedersen etal., 1996), jaccard coefficient (jaccard), dice coefficient (dice), phi coefficient (phi), t-score (tscore) (church and hanks, 1990), <papid> J90-1003 </papid>pearsons chi-squared test (x2), and the odds ratio (odds).</citsent>
<aftsection>
<nextsent>these measure the co-occurrence of word pairs (bigrams) relative to their individual frequencies and assess how likely it is that the word pair is occurring together by chance (and is therefore likely composi tional) or has some significant pattern of occurrence as pair (in which case it is non-compositional).
</nextsent>
<nextsent>more formally, many of these methods compare the observed empirical data with model that casts the words in the bigram as independent statistical events.
</nextsent>
<nextsent>the measures determine the degree to which the observed data deviates from what would be expected under the model of independence.
</nextsent>
<nextsent>if the observed data differs significantly from that, then thereis no evidence to support the hypothesis that the bigram is chance event, and we assume that there is some interesting or significant pattern that implies non-compositionality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3256">
<title id=" W11-1306.xml">identifying collocations to measure compositionality shared task system description </title>
<section> system development.  </section>
<citcontext>
<prevsection>
<prevsent>33 2.1 collocation discovery.
</prevsent>
<prevsent>the ngram statistics package (text::nsp) (baner jee and pedersen, 2003) was used to measure the association between the training pairs based on frequency count data collected from the corpus.
</prevsent>
</prevsection>
<citsent citstr=" C00-1027 ">
all thirteen measures in the ngram statistics package were employed, including the log-likelihood ratio (ll) (dunning, 1993), <papid> J93-1003 </papid>pointwise mutual information (pmi) (church and hanks, 1990), <papid> J90-1003 </papid>mutual information (tmi) (church and hanks, 1990), <papid> J90-1003 </papid>poisson stirling (ps) (church, 2000), <papid> C00-1027 </papid>fishers exact test (leftfisher, right fisher, and twotailed) (pedersen etal., 1996), jaccard coefficient (jaccard), dice coefficient (dice), phi coefficient (phi), t-score (tscore) (church and hanks, 1990), <papid> J90-1003 </papid>pearsons chi-squared test (x2), and the odds ratio (odds).</citsent>
<aftsection>
<nextsent>these measure the co-occurrence of word pairs (bigrams) relative to their individual frequencies and assess how likely it is that the word pair is occurring together by chance (and is therefore likely composi tional) or has some significant pattern of occurrence as pair (in which case it is non-compositional).
</nextsent>
<nextsent>more formally, many of these methods compare the observed empirical data with model that casts the words in the bigram as independent statistical events.
</nextsent>
<nextsent>the measures determine the degree to which the observed data deviates from what would be expected under the model of independence.
</nextsent>
<nextsent>if the observed data differs significantly from that, then thereis no evidence to support the hypothesis that the bigram is chance event, and we assume that there is some interesting or significant pattern that implies non-compositionality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3260">
<title id=" W11-0607.xml">composing and updating verb argument expectations a distributional semantic model </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>(elman 2009: 21).
</prevsent>
<prevsent>thematic fit judgments have been successfully modeled in distributional semantics.
</prevsent>
</prevsection>
<citsent citstr=" P07-1028 ">
erk et al(2010) propose the exemplar-based model of selectional preferences, in turn based on erk (2007).<papid> P07-1028 </papid></citsent>
<aftsection>
<nextsent>the thematic fit of noun as an argument of verb is measured with the similarity in vector space between and set of noun exemplars occurring in the same argument role of v. related approach is adopted by baroni and lenci (2010), the main difference being that the thematic fit of nis measured by comparing its vector with proto type?
</nextsent>
<nextsent>vector obtaining by averaging over the vectors of the most typical arguments of v. in both cases, the distributional measure of thematic fit is shown tobe highly correlated with human plausibility judgements.
</nextsent>
<nextsent>their success notwithstanding, these models fall short of accounting for the dynamical and context-sensitive nature of thematic fit.
</nextsent>
<nextsent>in the next section, we will extend the baroni and lenci (2010)approach with model for verb-argument composition, which is able to account for the argument interdependency phenomena shown by the experiments 59 in bicknell et al(2010).if verb argument expectations are likely to be dynamically computed integrating knowledge of theverb with information about its fillers, modeling thematic fit with dsm requires us to address compositional representations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3261">
<title id=" W11-0607.xml">composing and updating verb argument expectations a distributional semantic model </title>
<section> distributional memory.  </section>
<citcontext>
<prevsection>
<prevsent>the score ? is some function of the co-occurrence frequency of the tuple in corpus and is used to characterize its statistical salience.
</prevsent>
<prevsent>distributional memory belongs to the family of so-called structured dsms, which take into account the crucial role played by syntactic structures in shaping the distributional properties of words.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
to qualify as context of target item, word must be 60linked to it by some (interesting) lexico-syntactic relation, which is also typically used to distinguish thetype of this co-occurrence (lin, 1998; <papid> P98-2127 </papid>pado?</citsent>
<aftsection>
<nextsent>and lap ata, 2007).
</nextsent>
<nextsent>differently from other structured dsms, the tuple structure is formally represented as a3-way geometrical object, namely third order labeled tensor.
</nextsent>
<nextsent>a tensor is multi-way array (turney, 2007; kolda and bader, 2009), i.e. generalization of vectors (first order tensors) and matrices (second order tensors).
</nextsent>
<nextsent>different semantic spaces are then generated on demand?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3262">
<title id=" W11-0607.xml">composing and updating verb argument expectations a distributional semantic model </title>
<section> distributional memory.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, the space w1lw2 is formed by vectors for words and the dimensions represent the attributes of these words in terms of lexico-syntactic relations with lexical collocates, such as obj, read?,or use, pen?.
</prevsent>
<prevsent>consistently, this space is most suit able to address tasks involving the measurement ofthe attributional similarity?
</prevsent>
</prevsection>
<citsent citstr=" J06-3003 ">
between words (tur ney, 2006), <papid> J06-3003 </papid>such as synonym detection or modeling selectional preferences.</citsent>
<aftsection>
<nextsent>instead, the space w1w2l contains vectors associated with word pairs, whose dimensions are links between these pairs.
</nextsent>
<nextsent>this spaceis thus suitable to address tasks involving the measurement of so-called relational similarity?
</nextsent>
<nextsent>(tur ney, 2006), <papid> J06-3003 </papid>such as analogy detection or relation classification (cf.</nextsent>
<nextsent>baroni and lenci 2010 for more details about the distributional memory spaces and tasks).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3267">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments shows that acceptable parsing andhead tagging results are obtained on our approaches.
</prevsent>
<prevsent>constituent parsing is challenging but useful task aiming at analyzing the constituent structure of sentence.
</prevsent>
</prevsection>
<citsent citstr=" P05-1067 ">
recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and lexical resource augmentation (snow et al, 2004).</citsent>
<aftsection>
<nextsent>a great deal of researches have been conducted on this topic with promising progress (magerman, 1995; <papid> P95-1037 </papid>collins, 1999; charniak, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>sagae and lavie, 2006; <papid> P06-2089 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>finkel et al, 2008; <papid> P08-1109 </papid>huang, 2008).<papid> P08-1067 </papid>recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (mcdonald,2006; nivre and scholz, 2004).<papid> C04-1010 </papid></nextsent>
<nextsent>since graph structures of dependency and constituent parsing over sentence are strongly related, they should be benefited from each other.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3268">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments shows that acceptable parsing andhead tagging results are obtained on our approaches.
</prevsent>
<prevsent>constituent parsing is challenging but useful task aiming at analyzing the constituent structure of sentence.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and lexical resource augmentation (snow et al, 2004).</citsent>
<aftsection>
<nextsent>a great deal of researches have been conducted on this topic with promising progress (magerman, 1995; <papid> P95-1037 </papid>collins, 1999; charniak, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>sagae and lavie, 2006; <papid> P06-2089 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>finkel et al, 2008; <papid> P08-1109 </papid>huang, 2008).<papid> P08-1067 </papid>recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (mcdonald,2006; nivre and scholz, 2004).<papid> C04-1010 </papid></nextsent>
<nextsent>since graph structures of dependency and constituent parsing over sentence are strongly related, they should be benefited from each other.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3269">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>constituent parsing is challenging but useful task aiming at analyzing the constituent structure of sentence.
</prevsent>
<prevsent>recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and lexical resource augmentation (snow et al, 2004).</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
a great deal of researches have been conducted on this topic with promising progress (magerman, 1995; <papid> P95-1037 </papid>collins, 1999; charniak, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>sagae and lavie, 2006; <papid> P06-2089 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>finkel et al, 2008; <papid> P08-1109 </papid>huang, 2008).<papid> P08-1067 </papid>recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (mcdonald,2006; nivre and scholz, 2004).<papid> C04-1010 </papid></citsent>
<aftsection>
<nextsent>since graph structures of dependency and constituent parsing over sentence are strongly related, they should be benefited from each other.
</nextsent>
<nextsent>it is true that constituent parsing maybe smoothly altered to fit dependency parsing.
</nextsent>
<nextsent>how ever, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter this work is partially supported by the national natural science foundation of china (grant no. 60903119, grantno.
</nextsent>
<nextsent>60773090 and grant no. 90820018), the national basic research program of china (grant no. 2009cb320901), and the national high-tech research program of china (grant no.2008aa02z315).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3270">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>constituent parsing is challenging but useful task aiming at analyzing the constituent structure of sentence.
</prevsent>
<prevsent>recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and lexical resource augmentation (snow et al, 2004).</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
a great deal of researches have been conducted on this topic with promising progress (magerman, 1995; <papid> P95-1037 </papid>collins, 1999; charniak, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>sagae and lavie, 2006; <papid> P06-2089 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>finkel et al, 2008; <papid> P08-1109 </papid>huang, 2008).<papid> P08-1067 </papid>recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (mcdonald,2006; nivre and scholz, 2004).<papid> C04-1010 </papid></citsent>
<aftsection>
<nextsent>since graph structures of dependency and constituent parsing over sentence are strongly related, they should be benefited from each other.
</nextsent>
<nextsent>it is true that constituent parsing maybe smoothly altered to fit dependency parsing.
</nextsent>
<nextsent>how ever, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter this work is partially supported by the national natural science foundation of china (grant no. 60903119, grantno.
</nextsent>
<nextsent>60773090 and grant no. 90820018), the national basic research program of china (grant no. 2009cb320901), and the national high-tech research program of china (grant no.2008aa02z315).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3271">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>constituent parsing is challenging but useful task aiming at analyzing the constituent structure of sentence.
</prevsent>
<prevsent>recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and lexical resource augmentation (snow et al, 2004).</prevsent>
</prevsection>
<citsent citstr=" P06-2089 ">
a great deal of researches have been conducted on this topic with promising progress (magerman, 1995; <papid> P95-1037 </papid>collins, 1999; charniak, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>sagae and lavie, 2006; <papid> P06-2089 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>finkel et al, 2008; <papid> P08-1109 </papid>huang, 2008).<papid> P08-1067 </papid>recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (mcdonald,2006; nivre and scholz, 2004).<papid> C04-1010 </papid></citsent>
<aftsection>
<nextsent>since graph structures of dependency and constituent parsing over sentence are strongly related, they should be benefited from each other.
</nextsent>
<nextsent>it is true that constituent parsing maybe smoothly altered to fit dependency parsing.
</nextsent>
<nextsent>how ever, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter this work is partially supported by the national natural science foundation of china (grant no. 60903119, grantno.
</nextsent>
<nextsent>60773090 and grant no. 90820018), the national basic research program of china (grant no. 2009cb320901), and the national high-tech research program of china (grant no.2008aa02z315).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3272">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>constituent parsing is challenging but useful task aiming at analyzing the constituent structure of sentence.
</prevsent>
<prevsent>recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and lexical resource augmentation (snow et al, 2004).</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
a great deal of researches have been conducted on this topic with promising progress (magerman, 1995; <papid> P95-1037 </papid>collins, 1999; charniak, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>sagae and lavie, 2006; <papid> P06-2089 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>finkel et al, 2008; <papid> P08-1109 </papid>huang, 2008).<papid> P08-1067 </papid>recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (mcdonald,2006; nivre and scholz, 2004).<papid> C04-1010 </papid></citsent>
<aftsection>
<nextsent>since graph structures of dependency and constituent parsing over sentence are strongly related, they should be benefited from each other.
</nextsent>
<nextsent>it is true that constituent parsing maybe smoothly altered to fit dependency parsing.
</nextsent>
<nextsent>how ever, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter this work is partially supported by the national natural science foundation of china (grant no. 60903119, grantno.
</nextsent>
<nextsent>60773090 and grant no. 90820018), the national basic research program of china (grant no. 2009cb320901), and the national high-tech research program of china (grant no.2008aa02z315).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3273">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>constituent parsing is challenging but useful task aiming at analyzing the constituent structure of sentence.
</prevsent>
<prevsent>recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and lexical resource augmentation (snow et al, 2004).</prevsent>
</prevsection>
<citsent citstr=" P08-1109 ">
a great deal of researches have been conducted on this topic with promising progress (magerman, 1995; <papid> P95-1037 </papid>collins, 1999; charniak, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>sagae and lavie, 2006; <papid> P06-2089 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>finkel et al, 2008; <papid> P08-1109 </papid>huang, 2008).<papid> P08-1067 </papid>recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (mcdonald,2006; nivre and scholz, 2004).<papid> C04-1010 </papid></citsent>
<aftsection>
<nextsent>since graph structures of dependency and constituent parsing over sentence are strongly related, they should be benefited from each other.
</nextsent>
<nextsent>it is true that constituent parsing maybe smoothly altered to fit dependency parsing.
</nextsent>
<nextsent>how ever, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter this work is partially supported by the national natural science foundation of china (grant no. 60903119, grantno.
</nextsent>
<nextsent>60773090 and grant no. 90820018), the national basic research program of china (grant no. 2009cb320901), and the national high-tech research program of china (grant no.2008aa02z315).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3274">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>constituent parsing is challenging but useful task aiming at analyzing the constituent structure of sentence.
</prevsent>
<prevsent>recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and lexical resource augmentation (snow et al, 2004).</prevsent>
</prevsection>
<citsent citstr=" P08-1067 ">
a great deal of researches have been conducted on this topic with promising progress (magerman, 1995; <papid> P95-1037 </papid>collins, 1999; charniak, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>sagae and lavie, 2006; <papid> P06-2089 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>finkel et al, 2008; <papid> P08-1109 </papid>huang, 2008).<papid> P08-1067 </papid>recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (mcdonald,2006; nivre and scholz, 2004).<papid> C04-1010 </papid></citsent>
<aftsection>
<nextsent>since graph structures of dependency and constituent parsing over sentence are strongly related, they should be benefited from each other.
</nextsent>
<nextsent>it is true that constituent parsing maybe smoothly altered to fit dependency parsing.
</nextsent>
<nextsent>how ever, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter this work is partially supported by the national natural science foundation of china (grant no. 60903119, grantno.
</nextsent>
<nextsent>60773090 and grant no. 90820018), the national basic research program of china (grant no. 2009cb320901), and the national high-tech research program of china (grant no.2008aa02z315).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3275">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>constituent parsing is challenging but useful task aiming at analyzing the constituent structure of sentence.
</prevsent>
<prevsent>recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and lexical resource augmentation (snow et al, 2004).</prevsent>
</prevsection>
<citsent citstr=" C04-1010 ">
a great deal of researches have been conducted on this topic with promising progress (magerman, 1995; <papid> P95-1037 </papid>collins, 1999; charniak, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>sagae and lavie, 2006; <papid> P06-2089 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>finkel et al, 2008; <papid> P08-1109 </papid>huang, 2008).<papid> P08-1067 </papid>recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (mcdonald,2006; nivre and scholz, 2004).<papid> C04-1010 </papid></citsent>
<aftsection>
<nextsent>since graph structures of dependency and constituent parsing over sentence are strongly related, they should be benefited from each other.
</nextsent>
<nextsent>it is true that constituent parsing maybe smoothly altered to fit dependency parsing.
</nextsent>
<nextsent>how ever, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter this work is partially supported by the national natural science foundation of china (grant no. 60903119, grantno.
</nextsent>
<nextsent>60773090 and grant no. 90820018), the national basic research program of china (grant no. 2009cb320901), and the national high-tech research program of china (grant no.2008aa02z315).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3276">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> parsing constituents using.  </section>
<citcontext>
<prevsection>
<prevsent>however, there is not an effective method which can accurately accomplish the opposite transformation, from the dependency structures back into constituent ones due to the existence of ambiguity introduced by the former transformation.aimed at the above difficulty, our solution is to introduce formal dependency structure and machine learning method so that the ambiguity from dependency structures to constituent structures can be dealt with automatically.
</prevsent>
<prevsent>3.1.1 binarization we first transform constituent trees into the form that all productions for all subtrees are either unary or binary, before converting them to dependency structures.
</prevsent>
</prevsection>
<citsent citstr=" W98-1115 ">
due to the binarization, the target constituent trees of the conversion from dependency back to constituent structures are binary branching.this binarization is done by the left-factoring approach described in (charniak et al, 1998; <papid> W98-1115 </papid>petrov and klein, 2008), which converts each production with children, where   2, into n?</citsent>
<aftsection>
<nextsent>1 binary productions.additional non-terminal nodes introduced in this conversion must be clearly marked.
</nextsent>
<nextsent>transforming the binary branching trees into arbitrary branching trees is accomplished by using the reverse process.
</nextsent>
<nextsent>3.1.2 using binary classifier we train classifier to decide which dependency edges should be transformed first at each step of conversion automatically.
</nextsent>
<nextsent>after the binarization described in the previous section, only one dependency edge should be transformed at each step.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3277">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> parsing constituents using.  </section>
<citcontext>
<prevsection>
<prevsent>the label for each constituent is just used as the label dependency type for each dependency edge.the conversion method is tested on the development, too.
</prevsent>
<prevsent>constituent trees are firstly converted into dependency structures using the head rules described in (li and zhou, 2009).
</prevsent>
</prevsection>
<citsent citstr=" H91-1060 ">
then, we transform those trees back to constituent structure using our conversion method and use the parseval (black et al, 1991)<papid> H91-1060 </papid>measures to evaluate the performance of the conversion method.</citsent>
<aftsection>
<nextsent>our conversion method obtains 99.76%precision and 99.76% recall, which is great performance.
</nextsent>
<nextsent>3.2 dependency parser for constituent parsing.
</nextsent>
<nextsent>based on the proposed conversion method, dependency parsing algorithms can be used for constituentparsing.
</nextsent>
<nextsent>this can be done by firstly transforming training data from constituents into dependencies and extract training instances to train binary classifier fordependency-constituent conversion, then training dependency parser using the transformed training data.on the test step, parse the test data using the dependency parser and convert output dependencies to constituents using the binary classifier trained in advance.in addition, since our conversion method needs dependency types, labeled dependency parsing algorithms are always required.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3278">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we compare our method with several main parsers on the official dataset of 2009 cips-parseval shared task.
</prevsent>
<prevsent>all these results are evaluated with official evaluation tool by the 2009 cips-parseval shared task.
</prevsent>
</prevsection>
<citsent citstr=" J04-4004 ">
bikels parser4 (bikel, 2004) <papid> J04-4004 </papid>in table 6 is implementation of collins?</citsent>
<aftsection>
<nextsent>head-driven statistical model (collins, 2003).<papid> J03-4003 </papid></nextsent>
<nextsent>the stanford parser5 is basedon the factored model described in (klein and manning, 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3279">
<title id=" W10-4146.xml">dependency parser for chinese constituent parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>all these results are evaluated with official evaluation tool by the 2009 cips-parseval shared task.
</prevsent>
<prevsent>bikels parser4 (bikel, 2004) <papid> J04-4004 </papid>in table 6 is implementation of collins?</prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
head-driven statistical model (collins, 2003).<papid> J03-4003 </papid></citsent>
<aftsection>
<nextsent>the stanford parser5 is basedon the factored model described in (klein and manning, 2002).
</nextsent>
<nextsent>the charniaks parser6 is based on the parsing model described in (charniak, 2000).
</nextsent>
<nextsent>berkeley parser7 is based on un lexicalized parsing model described in (petrov and klein, 2007).<papid> N07-1051 </papid></nextsent>
<nextsent>according to table 6, the performance of our method is better than all the four parsers described above.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3281">
<title id=" W11-0309.xml">subword and spatiotemporal models for identifying actionable information in  </title>
<section> features (f ).  </section>
<citcontext>
<prevsection>
<prevsent>there is also substantial variation due to lack of spelling conventions, geographic variation, varying literacy, more-or-less french spellings for cognates, and character sets/accents (thank you?: mesi, mesi, meci meci, merci).
</prevsent>
<prevsent>see table 2 for further examples and common subword patterns that were discovered across very different surface forms.
</prevsent>
</prevsection>
<citsent citstr=" N10-1075 ">
the approach here builds on the earlier work ofmunro and manning (2010), <papid> N10-1075 </papid>adapted from gold water et al (2009), where unsupervised methods were used to segment and phonologically normalize the words.</citsent>
<aftsection>
<nextsent>for example, the process might turn all variants of thank you?
</nextsent>
<nextsent>into mesi?
</nextsent>
<nextsent>and all variants of my family?
</nextsent>
<nextsent>into fan mwen?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3283">
<title id=" W11-0309.xml">subword and spatiotemporal models for identifying actionable information in  </title>
<section> features (f ).  </section>
<citcontext>
<prevsection>
<prevsent>intuitively, svms with non-linear kernels could more accurately model geographic divisions inthe latitude and longitude dimensions and discover different combinations of features like has location=true and category=emergency.
</prevsent>
<prevsent>however, we were not able to find combination of kernels and parameter settings that demonstrated this.
</prevsent>
</prevsection>
<citsent citstr=" D09-1026 ">
it is possible that we could not avoid over-fitting or thatthe composite features had already sufficiently captured the combinations.munro and manning (2010) <papid> N10-1075 </papid>also found gains using supervised lda (ramage et al, 2009), <papid> D09-1026 </papid>which has also previously been used for disaster response clustering (kireyev et al, 2009).</citsent>
<aftsection>
<nextsent>we implemented supervised lda and unsupervised lda topic models, but they showed modest improvements over the baseline model only.
</nextsent>
<nextsent>we presume that this is be cause when we add the predicted categories from our (supervised) category learning task, they already contained enough information about topic divisions.
</nextsent>
<nextsent>we looked at several methods for spatio-temporalclustering including cliques (zhang et al, 2004), kmeans (wagstaff et al, 2001), and targeted low frequency clusters (huang et al, 2003).
</nextsent>
<nextsent>the change inaccuracy was negligible but the exploration of methods was by no means exhaustive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3284">
<title id=" W11-0309.xml">subword and spatiotemporal models for identifying actionable information in  </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however,there are similarities relating to problems of message brevity and the ability to extend the feature space.
</prevsent>
<prevsent>for example, sriram et al (2010) also found that modeling the source of message improved accuracy.
</prevsent>
</prevsection>
<citsent citstr=" D10-1124 ">
eisenstein et al (eisenstein et al, 2010) <papid> D10-1124 </papid>show promising results in identifying an authors geographic location from micro-blogs, but the locations are course-grained and relyon substantial message history per-source.</citsent>
<aftsection>
<nextsent>in recent work with medical text messages in the chichewa language, we compared the accuracy ofrule-based and unsupervised phonological normalization and morphological segmentation when applied to classification task over medical labels, 74 showing substantial gains from subword models (munro and manning, 2010).<papid> N10-1075 </papid></nextsent>
<nextsent>a cluster of earlier work looked at sms-spam in english (healy et al, 2005; hidalgo et al, 2006; cormack et al, 2007) and beaufort et al (2010)<papid> P10-1079 </papid>used similar preprocessing method for normalizing text-messages in french, combining rule-based models with finite-state framework.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3286">
<title id=" W11-0309.xml">subword and spatiotemporal models for identifying actionable information in  </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>eisenstein et al (eisenstein et al, 2010) <papid> D10-1124 </papid>show promising results in identifying an authors geographic location from micro-blogs, but the locations are course-grained and relyon substantial message history per-source.</prevsent>
<prevsent>in recent work with medical text messages in the chichewa language, we compared the accuracy ofrule-based and unsupervised phonological normalization and morphological segmentation when applied to classification task over medical labels, 74 showing substantial gains from subword models (munro and manning, 2010).<papid> N10-1075 </papid></prevsent>
</prevsection>
<citsent citstr=" P10-1079 ">
a cluster of earlier work looked at sms-spam in english (healy et al, 2005; hidalgo et al, 2006; cormack et al, 2007) and beaufort et al (2010)<papid> P10-1079 </papid>used similar preprocessing method for normalizing text-messages in french, combining rule-based models with finite-state framework.</citsent>
<aftsection>
<nextsent>the accuracy was calculated relative to bleu scores for correct?
</nextsent>
<nextsent>french, not as classification task.machine-translation into more well-spoken language can extend the potential workforce.
</nextsent>
<nextsent>early results are promising (lewis, 2010) but still leave some latency in deployment.
</nextsent>
<nextsent>for streaming architectures, zhang et al (2008) proposed similar method for calculating per epoch weights as an alternative to discounting function with significant gains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3287">
<title id=" W11-0309.xml">subword and spatiotemporal models for identifying actionable information in  </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>wang et al (2007) also looked at multiple parallel streams of text from different newspapers reporting the same events but we couldnt apply their method here as there were few instances of the same pairs of people independently reporting two distinct events.
</prevsent>
<prevsent>the two-tiered architecture used here is similar to hierarchical model, the main difference being epoch-based retraining and the temporal offset of the base models feeding into the final one.
</prevsent>
</prevsection>
<citsent citstr=" P10-1074 ">
joint learning over hierarchical models has been successful in nlp (finkel and manning, 2010) <papid> P10-1074 </papid>but to our best knowledge no onehas published work on joint learning over hierarchical streaming models, in nlp or otherwise.</citsent>
<aftsection>
<nextsent>from models optimized over words and ngrams to one including temporal clustering and subword models the accuracy rises from f=0.207 to f=0.326.clearly, the words that someone has chosen to express message is just one small aspect of the context in which that message is understood and by combining different learning models with richer features we can prioritize actionable reports with some accuracy.
</nextsent>
<nextsent>with spatial clustering this rises to f=0.885, indicating that geographic location was the single most important factor for prioritizing action able messages.
</nextsent>
<nextsent>these results are only first step as there is great potential for research identifying more accurate and efficient learning paradigms.
</nextsent>
<nextsent>a growing number of our communications are real-time text with frequent spelling variants and spatial component (tweets, location-based check-ins?, instant messaging, etc) so there will be increasingly more data available in an increasing variety of languages.it is easy to imagine many humanitarian applications for classifying text-messages with spatiotemporal information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3288">
<title id=" W10-4130.xml">chinese word segmentation with conditional support vector inspired markov models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our system achieved good performance, especially in the open track on the domain of medicine, our system got the highest score among 18 systems.
</prevsent>
<prevsent>in the last decade, many statistics-based methods for automatic chinese word segmentation (cws) have been proposed with development of machine learning and statistical method (huang and zhao, 2007).
</prevsent>
</prevsection>
<citsent citstr=" I05-3025 ">
especially, character-based tagging method which was proposed by nianwen xue (2003) achieves great success in the second international chinese word segmentation bakeoff in 2005 (low et al, 2005).<papid> I05-3025 </papid></citsent>
<aftsection>
<nextsent>the character-based tagging method formulates the cws problem as task of predicting tag for each character in the sentence, i.e. every character is considered as one of four different types in 4-tag set: (begin of word), (middle of word), (end of word), and (single-character word).
</nextsent>
<nextsent>most of these works train tagging models only on limited labeled training sets, without using any unsupervised learning outcomes from unlabeled text.
</nextsent>
<nextsent>but in recent years, researchers begin to exploit the value of enormous unlabeled corpus for cws, such as some statistics information on co-occurrence of sub sequences in the whole text has been extracted from unlabeled data and been employed as input features for tagging model training (zhao and kit , 2007).
</nextsent>
<nextsent>word clustering is common method to utilize unlabeled corpus in language processing research to enhance the generalization ability, such as part-of-speech clustering and semantic clustering (lee et al, 1999 and wang and wang 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3289">
<title id=" W10-4130.xml">chinese word segmentation with conditional support vector inspired markov models </title>
<section> applications and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>n1??.
</prevsent>
<prevsent>4.2 character-based tagging method.
</prevsent>
</prevsection>
<citsent citstr=" W06-0127 ">
for cws previous works show that 6-tag set achieved better cws performance (zhao et al, 2006).<papid> W06-0127 </papid></citsent>
<aftsection>
<nextsent>thus, we opt for this tag set.
</nextsent>
<nextsent>this 6 tag set adds b2?
</nextsent>
<nextsent>and b3?
</nextsent>
<nextsent>to 4-tag set which stand for the type of the second and the third character in chinese word respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3290">
<title id=" W11-0132.xml">discovering semantic classes for urdu nv complex predicates </title>
<section> corpus study.  </section>
<citcontext>
<prevsection>
<prevsent>our main aim is therefore to identify semantic classes of nouns on the basis of their syntactic patterns with respect to complex predicates.
</prevsent>
<prevsent>306
</prevsent>
</prevsection>
<citsent citstr=" W10-1810 ">
according to the best of our knowledge there is no systematic inventory of which types of nouns are allowed to combine with which types of light verbs in urdu, though the basic problem has been recognized for hindi by hwang et al (2010), <papid> W10-1810 </papid>who are developing annotation guidelines for complex predicate constructions.</citsent>
<aftsection>
<nextsent>we used small part-of-speech (pos) tagged corpus to extract number of n-v complex predicates and then used native speaker judgements to further manually explore their ability to appear with each of the light verbs kar do?, ho be?, hu- become?.1 the manual exploration was necessary due to data sparseness problem, since the available tagged corpora for urdu are of limited size.
</nextsent>
<nextsent>3.1 corpus.
</nextsent>
<nextsent>we used an urdu pos tagged corpus compiled by the center for research in urdu language processing(crulp) in lahore, pakistan (available at http://www.crulp.org/software/ling resources/urdunepali englishparallelcorpus.htm).
</nextsent>
<nextsent>the corpus consists of 100 000 words from the english penn treebank that have been (manually) translated into urdu.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3291">
<title id=" W11-0703.xml">what pushes their buttons predicting comment polarity from the content of political blog posts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we consider only predicting polarity (positive and negative feeling).
</prevsent>
<prevsent>in contrast to work done traditionally in sentiment analysis which focuses on determining the sentiment expressed in text, in this work, we focus on the task of predicting the sentiment that block of text will evoke in readers, expressed in the comment section, as response to the blog post.
</prevsent>
</prevsection>
<citsent citstr=" N09-1054 ">
this task is related to, but distinct from, several other studies that have been made using comment sand discussions in political communities, or analysis of sentiment in comments - (yano et al, 2009), (<papid> N09-1054 </papid>oconnor et al, 2010), (tumasjan et al, 2010).</citsent>
<aftsection>
<nextsent>below we discuss the methods used to address the various parts of this task.
</nextsent>
<nextsent>first, we evaluate two methods to automatically determine the comment polarity: sentiwordnet (baccianella and sebastiani,2010) general purpose resource that assigns sentiment scores to entries in wordnet, and an auto 12 mated corpus-specific technique based on pointwise mutual information.
</nextsent>
<nextsent>the quality of the polarity assessments by these techniques are made by comparing them to hand annotated assessments on small number of blog posts.
</nextsent>
<nextsent>second, we consider two methods for predicting comment polarity from post content: support vector machine classification, and slda, topic-modeling-based approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3293">
<title id=" W11-0703.xml">what pushes their buttons predicting comment polarity from the content of political blog posts </title>
<section> labelling comments with sentiment.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, each blog post is now associated with binary response variable indicating the polarity of the sentiment expressed in the comments.
</prevsent>
<prevsent>3.2 using pointwise mutual information.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
a second technique to determine the sentiment polarity of comments uses the principle of pointwise mutual information (pmi)(turney, 2002).<papid> P02-1053 </papid></citsent>
<aftsection>
<nextsent>we first construct seed list of positive and negative words by choosing the 100 topmost positive and negative words from sentiwordnet and manually eliminating words from this list that dont pertain to sentiment in our context.
</nextsent>
<nextsent>(appendix has the list of seed words used.)
</nextsent>
<nextsent>this seed list is used to constructa larger set of positive and negative words by computing the pmi of the words in the seed lists with every other word in the vocabulary.
</nextsent>
<nextsent>its important to note that this list is constructed for the specific corpus that we work with.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3294">
<title id=" W11-0503.xml">summarizing decisions in spoken meetings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for those who didnot participate in the earlier meetings, decision summaries might provide one type of efficient overview of the meeting contents.
</prevsent>
<prevsent>for managers, decision summaries could act as concise record of the idea generation process.
</prevsent>
</prevsection>
<citsent citstr=" W09-3934 ">
while there has been some previous work in summarizing meetings and conversations, very little work has focused on decision summarization:fernandez et al  (2008a) and bui et al  (2009) <papid> W09-3934 </papid>investigate the use of semantic parser and machine learning methods for phrase- and token-level decision summarization.</citsent>
<aftsection>
<nextsent>we believe our work is the first to explore and compare token-level and dialogue act-level approaches ? using both unsupervised and supervised learning methods ? for summarizing decisions in meetings.
</nextsent>
<nextsent>c: just spinning and not scrolling , would say .
</nextsent>
<nextsent>(1) c: but if youve got [disfmarker] if if youve got flipped thing , effectively its something thats curved on one side and flat on the other side , but you folded it in half .
</nextsent>
<nextsent>(2) d: the case would be rubber and the the buttons , (3) b: think the spinning wheel is definitely very now .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3298">
<title id=" W11-0503.xml">summarizing decisions in spoken meetings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our goal is to eliminate redundant and less informative utterances.
</prevsent>
<prevsent>the1these are similar, but not completely equivalent, to the decision dialogue acts (ddas) of bui et al  (2009), <papid> W09-3934 </papid>fernandez et al .</prevsent>
</prevsection>
<citsent citstr=" D09-1118 ">
(2008a), frampton et al  (2009).<papid> D09-1118 </papid></citsent>
<aftsection>
<nextsent>the latter refer to all das that appear in decision discussion even if they do not support any particular decision.
</nextsent>
<nextsent>selected drdas are then concatenated to form the decision summary.
</nextsent>
<nextsent>optional token-level summarization of the selecteddrdas.
</nextsent>
<nextsent>methods are employed to capture concisely the gist of each decision, discarding any distracting text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3299">
<title id=" W11-0503.xml">summarizing decisions in spoken meetings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also investigate unsupervised methods and supervised learning for decision summarization at both theda and token level, with and without the incorporation of discourse context.
</prevsent>
<prevsent>during training, the supervised decision summarizers are told which drdas for each decision are the most informative for constructing the decision abstract.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
our experiments employ the aforementioned ami meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the rouge-1 (lin and hovy, 2003) <papid> N03-1020 </papid>text summarization evaluation metric.in the supervised summarization setting, our experiments demonstrate that with true clusterings of decision-related das, token-level summaries that employ limited discourse context can approach an upper bound for summaries extracted directly from drdas2 ? 0.4387 rouge-f1 vs. 0.5333.</citsent>
<aftsection>
<nextsent>when using system-generated drda clusterings, the dalevel summaries always dominate token-level methods in terms of performance.
</nextsent>
<nextsent>for the unsupervised summarization setting, we investigate the use of both unsupervised and supervised methods for the initial drda clustering step.
</nextsent>
<nextsent>we find that summaries based on unsupervised clusterings perform comparably to those generated using supervised techniques (0.2214 rouge-f1 using lda-based topic models vs. 0.2349 using svms).
</nextsent>
<nextsent>as in the supervised summarization setting,we observe that including additional discourse context boosts performance only for token-level summaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3301">
<title id=" W11-0503.xml">summarizing decisions in spoken meetings </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there exists much previous research on automatic text summarization using corpus-based, knowledge based or statistical methods (mani, 1999; marcu,2000).
</prevsent>
<prevsent>dialogue summarization methods, however, generally try to account for the special characteristics of speech.
</prevsent>
</prevsection>
<citsent citstr=" J02-4003 ">
among early work in this subarea, zechner (2002) <papid> J02-4003 </papid>investigates speech summarization based on maximal marginal relevance (mmr) and cross-speaker linking of information.</citsent>
<aftsection>
<nextsent>popular supervised methods for summarizing speech ? including maximum entropy, conditional random fields (crfs), and support vector machines (svms) ? are investigated in buist et al (2004), xie et al  (2008) and galley (2006).<papid> W06-1643 </papid></nextsent>
<nextsent>techniques for determining semantic similarity are used for selecting relevant utterances in gurevych and strube (2004).<papid> C04-1110 </papid>studies in banerjee et al  (2005) show that decisions are considered to be one of the most important outputs of meetings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3302">
<title id=" W11-0503.xml">summarizing decisions in spoken meetings </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>dialogue summarization methods, however, generally try to account for the special characteristics of speech.
</prevsent>
<prevsent>among early work in this subarea, zechner (2002) <papid> J02-4003 </papid>investigates speech summarization based on maximal marginal relevance (mmr) and cross-speaker linking of information.</prevsent>
</prevsection>
<citsent citstr=" W06-1643 ">
popular supervised methods for summarizing speech ? including maximum entropy, conditional random fields (crfs), and support vector machines (svms) ? are investigated in buist et al (2004), xie et al  (2008) and galley (2006).<papid> W06-1643 </papid></citsent>
<aftsection>
<nextsent>techniques for determining semantic similarity are used for selecting relevant utterances in gurevych and strube (2004).<papid> C04-1110 </papid>studies in banerjee et al  (2005) show that decisions are considered to be one of the most important outputs of meetings.</nextsent>
<nextsent>and in recent years,there has been much research on detecting decision related das.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3303">
<title id=" W11-0503.xml">summarizing decisions in spoken meetings </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>among early work in this subarea, zechner (2002) <papid> J02-4003 </papid>investigates speech summarization based on maximal marginal relevance (mmr) and cross-speaker linking of information.</prevsent>
<prevsent>popular supervised methods for summarizing speech ? including maximum entropy, conditional random fields (crfs), and support vector machines (svms) ? are investigated in buist et al (2004), xie et al  (2008) and galley (2006).<papid> W06-1643 </papid></prevsent>
</prevsection>
<citsent citstr=" C04-1110 ">
techniques for determining semantic similarity are used for selecting relevant utterances in gurevych and strube (2004).<papid> C04-1110 </papid>studies in banerjee et al  (2005) show that decisions are considered to be one of the most important outputs of meetings.</citsent>
<aftsection>
<nextsent>and in recent years,there has been much research on detecting decision related das.
</nextsent>
<nextsent>hsueh and moore (2008), for example, propose maximum entropy classification techniques to identify drdas in meetings; fernandez et al  (2008b) develop model of decision-making dialogue structure and detect decision das based on it; and frampton et al  (2009) <papid> D09-1118 </papid>implement real-time decision detection system.</nextsent>
<nextsent>fernandez et al  (2008a) and bui et al  (2009), <papid> W09-3934 </papid>however, might be the most relevant previous workto ours.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3307">
<title id=" W11-0503.xml">summarizing decisions in spoken meetings </title>
<section> clustering decision-related dialogue.  </section>
<citcontext>
<prevsection>
<prevsent>5here is the definition for the relative position of pairwise das.
</prevsent>
<prevsent>suppose there are das in one meeting ordered by time, have the same da type.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
we employ support vector machines (svms) and maximum entropy (maxent) as our learning methods, because svms are shown to be effective in text categorization (joachims, 1998) and max ent has been applied in many natural language processing tasks (berger et al , 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>given an ???
</nextsent>
<nextsent>fvij , for svms, we utilize the decision value of wt ? ???
</nextsent>
<nextsent>fvij + as the similarity, where is the weight vector and is the bias.
</nextsent>
<nextsent>for maxent, we make use of the probability of (samedecision | ???
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3308">
<title id=" W11-0503.xml">summarizing decisions in spoken meetings </title>
<section> clustering decision-related dialogue.  </section>
<citcontext>
<prevsection>
<prevsent>two clustering baselines are utilized for comparison.
</prevsent>
<prevsent>one baseline places all decision related das for the meeting into single partition(allinonegroup).
</prevsent>
</prevsection>
<citsent citstr=" A00-2004 ">
the second uses the text segmentation software of choi (2000) <papid> A00-2004 </papid>to partition the decision-related das (ordered according to time) into several topic-based groups (choisegment).</citsent>
<aftsection>
<nextsent>experimental setup and evaluation.
</nextsent>
<nextsent>results for pairwise supervised clustering were obtained using3-fold cross-validation.
</nextsent>
<nextsent>in the current work, stopping conditions for hierarchical agglomerative clustering are selected manually: for the tf-idf and topic model approaches, we stop when the similarity measure reaches 0.035 and 0.015, respectively; for the svm and maxent versions, we use 0 and0.45, respectively.
</nextsent>
<nextsent>we use the mallet implementation for maxent and the svm light implementation of svms.our evaluation metrics include b3 (also called bcubed) (bagga and baldwin, 1998), which is com dai is the ith da and daj is positioned at j. so the relative position of dai and daj is |ij| . 19 b-cubed pairwise voi precision recall f1 precision recall f1 baselines allinonegroup 0.2854 1.0000 0.4441 0.1823 1.0000 0.3083 2.2279 choi segment 0.4235 0.9657 0.5888 0.2390 0.8493 0.3730 1.8061 unsupervised methods tfidf 0.6840 0.6686 0.6762 0.3281 0.3004 0.3137 1.6604 lda topic models 0.8265 0.6432 0.7235 0.4588 0.2980 0.3613 1.4203 pairwise supervised methods svm 0.7593 0.7466 0.7529 0.5474 0.4821 0.5127 1.2239 maxent 0.6999 0.7948 0.7443 0.4858 0.5704 0.5247 1.2726 table 3: results for clustering decision-related das according to the decision each supports mon measure employed in noun phrase coreference resolution research; pairwise scorer that measures correctness for every pair of drdas; and variation of information (voi) scorer (meila?, 2007), which measures the difference between the distributions of the true clustering and system generated clustering.as space is limited, we refer the readers to the original papers for more details.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3316">
<title id=" W11-1515.xml">author age prediction from text using linear regression </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>iii (2007), we train model on all these corpora together and separate the global features from corpus-specific features that are associated with age.
</prevsent>
<prevsent>a second contribution is the investigation of age prediction with age modeled as continuous variable rather than as categorical variable.
</prevsent>
</prevsection>
<citsent citstr=" P09-1080 ">
most prior research on age prediction has framed this as two-class or three-class classification problem (e.g., schler et al, 2006 and garera and yarowsky, 2009).<papid> P09-1080 </papid></citsent>
<aftsection>
<nextsent>in our work, modeling age as continuous variable is interesting not only as more realistic representation of age, but also for practical benefits of joint modeling of age across corpora since the bound 115 aries for discretizing age into categorical variable in prior work have been chosen heuristic ally and in corpus-dependent way, making it hard to compare performance across different kinds of data.in the remainder of the paper, we first discuss related work and present and compare the different datasets.
</nextsent>
<nextsent>we then outline our approach and results.
</nextsent>
<nextsent>we conclude with discussion and future work.
</nextsent>
<nextsent>time is an important factor in socio linguistic analysis of language variation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3319">
<title id=" W11-1515.xml">author age prediction from text using linear regression </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>is constant and can be found by optimizing over the developmentdata.
</prevsent>
<prevsent>as result, this method delivers sparse models.
</prevsent>
</prevsection>
<citsent citstr=" P07-1104 ">
we use owlqn to optimize the regularized empirical risk (andrew and gao, 2007; gao et al,2007).<papid> P07-1104 </papid></citsent>
<aftsection>
<nextsent>we evaluate the models by reporting the correlation and mean absolute error (mae).
</nextsent>
<nextsent>4.2 joint model.
</nextsent>
<nextsent>to discover which features are important across datasets and which are corpus-specific, we train model on the data of all corpora using the feature representation proposed by daume?
</nextsent>
<nextsent>iii (2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3320">
<title id=" W11-1515.xml">author age prediction from text using linear regression </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>unigrams.?
</prevsent>
<prevsent>pos unigrams and bigrams.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
text is tagged using the stanford pos tagger (toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>liwc (pennebaker et al, 2001).
</nextsent>
<nextsent>this is word counting program that captures word classes such as inclusion words (liwc-incl: with,and,?
</nextsent>
<nextsent>include,?
</nextsent>
<nextsent>etc.), causation words (liwccause: because,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3322">
<title id=" W11-1408.xml">exploring effective dialogue act sequences in oneonone computer science tutoring dialogues </title>
<section> a tutor can provide feedback: </section>
<citcontext>
<prevsection>
<prevsent>during each iteration, two human annotators independently annotated several dialogues for one da at time, compared outcomes, discussed disagreements, and fine-tuned the scheme accordingly.
</prevsent>
<prevsent>this process was repeated until sufficiently high inter-coder agreement wasreached.
</prevsent>
</prevsection>
<citsent citstr=" J04-1005 ">
the kappa values we obtained in the final iteration of this process are listed in table 2 (di eugenio and glass, 2004; <papid> J04-1005 </papid>artstein and poesio, 2008).<papid> J08-4004 </papid></citsent>
<aftsection>
<nextsent>in table 2, the double coded*?
</nextsent>
<nextsent>column refers to the sessions that we double coded to calculate the inter-coder agreement.
</nextsent>
<nextsent>this number does not include the sessions which were double coded when coders were developing the coding manual.
</nextsent>
<nextsent>the numbers of double-coded sessions differ by da since it depends on the frequency on the particular da (recall that we coded for one da at time).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3323">
<title id=" W11-1408.xml">exploring effective dialogue act sequences in oneonone computer science tutoring dialogues </title>
<section> a tutor can provide feedback: </section>
<citcontext>
<prevsection>
<prevsent>during each iteration, two human annotators independently annotated several dialogues for one da at time, compared outcomes, discussed disagreements, and fine-tuned the scheme accordingly.
</prevsent>
<prevsent>this process was repeated until sufficiently high inter-coder agreement wasreached.
</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
the kappa values we obtained in the final iteration of this process are listed in table 2 (di eugenio and glass, 2004; <papid> J04-1005 </papid>artstein and poesio, 2008).<papid> J08-4004 </papid></citsent>
<aftsection>
<nextsent>in table 2, the double coded*?
</nextsent>
<nextsent>column refers to the sessions that we double coded to calculate the inter-coder agreement.
</nextsent>
<nextsent>this number does not include the sessions which were double coded when coders were developing the coding manual.
</nextsent>
<nextsent>the numbers of double-coded sessions differ by da since it depends on the frequency on the particular da (recall that we coded for one da at time).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3324">
<title id=" W11-1408.xml">exploring effective dialogue act sequences in oneonone computer science tutoring dialogues </title>
<section> a tutor can provide feedback: </section>
<citcontext>
<prevsection>
<prevsent>the tool that allows us to leave gaps in sequences is part of apache lucene,1 an open source full text search library.
</prevsent>
<prevsent>it provides strong capabilities to match and count efficiently.
</prevsent>
</prevsection>
<citsent citstr=" W10-3016 ">
our counting method is based on two important features provided by lucene, that we already used in other work (chenand di eugenio, 2010) <papid> W10-3016 </papid>to detect uncertainty in different types of corpora.</citsent>
<aftsection>
<nextsent>synonym matching: we can specify several different tokens at the same position in field of document, so that each of them can be used to match the query.
</nextsent>
<nextsent>precise gaps: with lucene, we can precisely specify the gap between the matched query and the indexed documents (sequences of das in our case) using special type of query called spannearquery.
</nextsent>
<nextsent>to take advantage of lucene as described above, we use the following algorithm to index our corpus.
</nextsent>
<nextsent>1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3325">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>performance in this setting for two reasons.
</prevsent>
<prevsent>first, feature sparsity prevents systems from generalizing accurately to words and features not seen during training.
</prevsent>
</prevsection>
<citsent citstr=" J04-4004 ">
because word frequencies are zipf distributed, this often means that there is little relevant training data for substantial fraction of parameters (bikel, 2004), <papid> J04-4004 </papid>especially innew domains (huang and yates, 2009).<papid> P09-1056 </papid></citsent>
<aftsection>
<nextsent>for example, word-type features form the backbone of most pos-tagging systems, but types like gene?
</nextsent>
<nextsent>andpathway?
</nextsent>
<nextsent>show up frequently in biomedical literature, and rarely in newswire text.
</nextsent>
<nextsent>thus, classifier trained on newswire data and tested on biomedical data will have seen few training examples related to sentences with features gene?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3326">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>performance in this setting for two reasons.
</prevsent>
<prevsent>first, feature sparsity prevents systems from generalizing accurately to words and features not seen during training.
</prevsent>
</prevsection>
<citsent citstr=" P09-1056 ">
because word frequencies are zipf distributed, this often means that there is little relevant training data for substantial fraction of parameters (bikel, 2004), <papid> J04-4004 </papid>especially innew domains (huang and yates, 2009).<papid> P09-1056 </papid></citsent>
<aftsection>
<nextsent>for example, word-type features form the backbone of most pos-tagging systems, but types like gene?
</nextsent>
<nextsent>andpathway?
</nextsent>
<nextsent>show up frequently in biomedical literature, and rarely in newswire text.
</nextsent>
<nextsent>thus, classifier trained on newswire data and tested on biomedical data will have seen few training examples related to sentences with features gene?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3327">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, classifier trained on newswire data and tested on biomedical data will have seen few training examples related to sentences with features gene?
</prevsent>
<prevsent>and pathway?
</prevsent>
</prevsection>
<citsent citstr=" W06-1615 ">
(ben david et al , 2009; blitzer et al , 2006).<papid> W06-1615 </papid>further, because words are polysemous, word type features prevent systems from generalizing to situations in which words have different meanings.for instance, the word type signaling?</citsent>
<aftsection>
<nextsent>appears primarily as present participle (vbg) in wall street journal (wsj) text, as in, interest rates rose, signaling that . . .
</nextsent>
<nextsent>(marcus et al , 1993).<papid> J93-2004 </papid></nextsent>
<nextsent>in biomedical text, however, signaling?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3328">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(ben david et al , 2009; blitzer et al , 2006).<papid> W06-1615 </papid>further, because words are polysemous, word type features prevent systems from generalizing to situations in which words have different meanings.for instance, the word type signaling?</prevsent>
<prevsent>appears primarily as present participle (vbg) in wall street journal (wsj) text, as in, interest rates rose, signaling that . . .</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
(marcus et al , 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>in biomedical text, however, signaling?
</nextsent>
<nextsent>appears primarily in the phrase signaling pathway,?
</nextsent>
<nextsent>where it is considered noun (nn) (pennbioie, 2005); this phrase never appears in the wsj portion of the penn treebank (huang and yates, 2010<papid> W10-2604 </papid>a).</nextsent>
<nextsent>our response to these problems with traditional nlp representations is to seek new representations that allow systems to generalize more accurately to previously unseen examples.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3329">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in biomedical text, however, signaling?
</prevsent>
<prevsent>appears primarily in the phrase signaling pathway,?
</prevsent>
</prevsection>
<citsent citstr=" W10-2604 ">
where it is considered noun (nn) (pennbioie, 2005); this phrase never appears in the wsj portion of the penn treebank (huang and yates, 2010<papid> W10-2604 </papid>a).</citsent>
<aftsection>
<nextsent>our response to these problems with traditional nlp representations is to seek new representations that allow systems to generalize more accurately to previously unseen examples.
</nextsent>
<nextsent>our approach depends on the well-known distributional hypothesis, which states that words meaning is identified with the contexts in which it appears (harris, 1954; hindle, 1990).<papid> P90-1034 </papid></nextsent>
<nextsent>our goal is to develop probabilistic lan 125guage models that describe the contexts of individual words accurately.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3337">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>where it is considered noun (nn) (pennbioie, 2005); this phrase never appears in the wsj portion of the penn treebank (huang and yates, 2010<papid> W10-2604 </papid>a).</prevsent>
<prevsent>our response to these problems with traditional nlp representations is to seek new representations that allow systems to generalize more accurately to previously unseen examples.</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
our approach depends on the well-known distributional hypothesis, which states that words meaning is identified with the contexts in which it appears (harris, 1954; hindle, 1990).<papid> P90-1034 </papid></citsent>
<aftsection>
<nextsent>our goal is to develop probabilistic lan 125guage models that describe the contexts of individual words accurately.
</nextsent>
<nextsent>we then construct representations, or mappings from word tokens and types to real-valued vectors, from these language models.
</nextsent>
<nextsent>since the language models are designed to model words?
</nextsent>
<nextsent>contexts, the features they produce can be used to combat problems with polysemy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3338">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sections 5 and 6 describe our two tasks and the resultsof using our representations on each of them.
</prevsent>
<prevsent>section 7 concludes.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
there is long tradition of nlp research on representations, mostly falling into one of four cate gories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (salton and mcgill, 1983; turney and pantel, 2010;sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (deerwester et al , 1990; honkela, 1997; kaski, 1998; sahlgren, 2005; blei et al , 2003; vayrynen et al , 2007); 3) using clusters that are induced from distributional similarity (brown et al , 1992; <papid> J92-4003 </papid>pereira et al , 1993; <papid> P93-1024 </papid>martin et al , 1998) as non-sparse features (lin and wu, 2009; <papid> P09-1116 </papid>candito and crabbe, 2009; koo et al , 2008; <papid> P08-1068 </papid>zhao et al , 2009); <papid> W09-1208 </papid>4) and recently, language models(bengio, 2008; mnih and hinton, 2009) as representations (weston et al , 2008; collobert and weston,2008; bengio et al , 2009), some of which have already yielded state of the art performance on domain adaptation tasks (huang and yates, 2009; <papid> P09-1056 </papid>huang and yates, 2010<papid> W10-2604 </papid>a; huang and yates, 2010<papid> W10-2604 </papid>b; turian et al , 2010) <papid> P10-1040 </papid>and ie (ahuja and downey, 2010; <papid> N10-1026 </papid>downey etal., 2007<papid> P07-1088 </papid>b).</citsent>
<aftsection>
<nextsent>in contrast to this previous work, we develop novel partial lattice mrf language model that incorporates facto rial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in pos tagging in domain adaptation setting.
</nextsent>
<nextsent>we also analyze the novel plmrf representation on an ie task, and several representations along the key dimensions of sparsity and polysemy.most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (daume?
</nextsent>
<nextsent>iii, 2007; jiang and zhai, 2007; <papid> P07-1034 </papid>daume?</nextsent>
<nextsent>iii and marcu, 2006; finkel and manning, 2009; <papid> N09-1068 </papid>dredzeet al , 2010; dredze and crammer, 2008).<papid> D08-1072 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3339">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sections 5 and 6 describe our two tasks and the resultsof using our representations on each of them.
</prevsent>
<prevsent>section 7 concludes.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
there is long tradition of nlp research on representations, mostly falling into one of four cate gories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (salton and mcgill, 1983; turney and pantel, 2010;sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (deerwester et al , 1990; honkela, 1997; kaski, 1998; sahlgren, 2005; blei et al , 2003; vayrynen et al , 2007); 3) using clusters that are induced from distributional similarity (brown et al , 1992; <papid> J92-4003 </papid>pereira et al , 1993; <papid> P93-1024 </papid>martin et al , 1998) as non-sparse features (lin and wu, 2009; <papid> P09-1116 </papid>candito and crabbe, 2009; koo et al , 2008; <papid> P08-1068 </papid>zhao et al , 2009); <papid> W09-1208 </papid>4) and recently, language models(bengio, 2008; mnih and hinton, 2009) as representations (weston et al , 2008; collobert and weston,2008; bengio et al , 2009), some of which have already yielded state of the art performance on domain adaptation tasks (huang and yates, 2009; <papid> P09-1056 </papid>huang and yates, 2010<papid> W10-2604 </papid>a; huang and yates, 2010<papid> W10-2604 </papid>b; turian et al , 2010) <papid> P10-1040 </papid>and ie (ahuja and downey, 2010; <papid> N10-1026 </papid>downey etal., 2007<papid> P07-1088 </papid>b).</citsent>
<aftsection>
<nextsent>in contrast to this previous work, we develop novel partial lattice mrf language model that incorporates facto rial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in pos tagging in domain adaptation setting.
</nextsent>
<nextsent>we also analyze the novel plmrf representation on an ie task, and several representations along the key dimensions of sparsity and polysemy.most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (daume?
</nextsent>
<nextsent>iii, 2007; jiang and zhai, 2007; <papid> P07-1034 </papid>daume?</nextsent>
<nextsent>iii and marcu, 2006; finkel and manning, 2009; <papid> N09-1068 </papid>dredzeet al , 2010; dredze and crammer, 2008).<papid> D08-1072 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3340">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sections 5 and 6 describe our two tasks and the resultsof using our representations on each of them.
</prevsent>
<prevsent>section 7 concludes.
</prevsent>
</prevsection>
<citsent citstr=" P09-1116 ">
there is long tradition of nlp research on representations, mostly falling into one of four cate gories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (salton and mcgill, 1983; turney and pantel, 2010;sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (deerwester et al , 1990; honkela, 1997; kaski, 1998; sahlgren, 2005; blei et al , 2003; vayrynen et al , 2007); 3) using clusters that are induced from distributional similarity (brown et al , 1992; <papid> J92-4003 </papid>pereira et al , 1993; <papid> P93-1024 </papid>martin et al , 1998) as non-sparse features (lin and wu, 2009; <papid> P09-1116 </papid>candito and crabbe, 2009; koo et al , 2008; <papid> P08-1068 </papid>zhao et al , 2009); <papid> W09-1208 </papid>4) and recently, language models(bengio, 2008; mnih and hinton, 2009) as representations (weston et al , 2008; collobert and weston,2008; bengio et al , 2009), some of which have already yielded state of the art performance on domain adaptation tasks (huang and yates, 2009; <papid> P09-1056 </papid>huang and yates, 2010<papid> W10-2604 </papid>a; huang and yates, 2010<papid> W10-2604 </papid>b; turian et al , 2010) <papid> P10-1040 </papid>and ie (ahuja and downey, 2010; <papid> N10-1026 </papid>downey etal., 2007<papid> P07-1088 </papid>b).</citsent>
<aftsection>
<nextsent>in contrast to this previous work, we develop novel partial lattice mrf language model that incorporates facto rial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in pos tagging in domain adaptation setting.
</nextsent>
<nextsent>we also analyze the novel plmrf representation on an ie task, and several representations along the key dimensions of sparsity and polysemy.most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (daume?
</nextsent>
<nextsent>iii, 2007; jiang and zhai, 2007; <papid> P07-1034 </papid>daume?</nextsent>
<nextsent>iii and marcu, 2006; finkel and manning, 2009; <papid> N09-1068 </papid>dredzeet al , 2010; dredze and crammer, 2008).<papid> D08-1072 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3341">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sections 5 and 6 describe our two tasks and the resultsof using our representations on each of them.
</prevsent>
<prevsent>section 7 concludes.
</prevsent>
</prevsection>
<citsent citstr=" P08-1068 ">
there is long tradition of nlp research on representations, mostly falling into one of four cate gories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (salton and mcgill, 1983; turney and pantel, 2010;sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (deerwester et al , 1990; honkela, 1997; kaski, 1998; sahlgren, 2005; blei et al , 2003; vayrynen et al , 2007); 3) using clusters that are induced from distributional similarity (brown et al , 1992; <papid> J92-4003 </papid>pereira et al , 1993; <papid> P93-1024 </papid>martin et al , 1998) as non-sparse features (lin and wu, 2009; <papid> P09-1116 </papid>candito and crabbe, 2009; koo et al , 2008; <papid> P08-1068 </papid>zhao et al , 2009); <papid> W09-1208 </papid>4) and recently, language models(bengio, 2008; mnih and hinton, 2009) as representations (weston et al , 2008; collobert and weston,2008; bengio et al , 2009), some of which have already yielded state of the art performance on domain adaptation tasks (huang and yates, 2009; <papid> P09-1056 </papid>huang and yates, 2010<papid> W10-2604 </papid>a; huang and yates, 2010<papid> W10-2604 </papid>b; turian et al , 2010) <papid> P10-1040 </papid>and ie (ahuja and downey, 2010; <papid> N10-1026 </papid>downey etal., 2007<papid> P07-1088 </papid>b).</citsent>
<aftsection>
<nextsent>in contrast to this previous work, we develop novel partial lattice mrf language model that incorporates facto rial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in pos tagging in domain adaptation setting.
</nextsent>
<nextsent>we also analyze the novel plmrf representation on an ie task, and several representations along the key dimensions of sparsity and polysemy.most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (daume?
</nextsent>
<nextsent>iii, 2007; jiang and zhai, 2007; <papid> P07-1034 </papid>daume?</nextsent>
<nextsent>iii and marcu, 2006; finkel and manning, 2009; <papid> N09-1068 </papid>dredzeet al , 2010; dredze and crammer, 2008).<papid> D08-1072 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3342">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sections 5 and 6 describe our two tasks and the resultsof using our representations on each of them.
</prevsent>
<prevsent>section 7 concludes.
</prevsent>
</prevsection>
<citsent citstr=" W09-1208 ">
there is long tradition of nlp research on representations, mostly falling into one of four cate gories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (salton and mcgill, 1983; turney and pantel, 2010;sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (deerwester et al , 1990; honkela, 1997; kaski, 1998; sahlgren, 2005; blei et al , 2003; vayrynen et al , 2007); 3) using clusters that are induced from distributional similarity (brown et al , 1992; <papid> J92-4003 </papid>pereira et al , 1993; <papid> P93-1024 </papid>martin et al , 1998) as non-sparse features (lin and wu, 2009; <papid> P09-1116 </papid>candito and crabbe, 2009; koo et al , 2008; <papid> P08-1068 </papid>zhao et al , 2009); <papid> W09-1208 </papid>4) and recently, language models(bengio, 2008; mnih and hinton, 2009) as representations (weston et al , 2008; collobert and weston,2008; bengio et al , 2009), some of which have already yielded state of the art performance on domain adaptation tasks (huang and yates, 2009; <papid> P09-1056 </papid>huang and yates, 2010<papid> W10-2604 </papid>a; huang and yates, 2010<papid> W10-2604 </papid>b; turian et al , 2010) <papid> P10-1040 </papid>and ie (ahuja and downey, 2010; <papid> N10-1026 </papid>downey etal., 2007<papid> P07-1088 </papid>b).</citsent>
<aftsection>
<nextsent>in contrast to this previous work, we develop novel partial lattice mrf language model that incorporates facto rial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in pos tagging in domain adaptation setting.
</nextsent>
<nextsent>we also analyze the novel plmrf representation on an ie task, and several representations along the key dimensions of sparsity and polysemy.most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (daume?
</nextsent>
<nextsent>iii, 2007; jiang and zhai, 2007; <papid> P07-1034 </papid>daume?</nextsent>
<nextsent>iii and marcu, 2006; finkel and manning, 2009; <papid> N09-1068 </papid>dredzeet al , 2010; dredze and crammer, 2008).<papid> D08-1072 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3360">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sections 5 and 6 describe our two tasks and the resultsof using our representations on each of them.
</prevsent>
<prevsent>section 7 concludes.
</prevsent>
</prevsection>
<citsent citstr=" P10-1040 ">
there is long tradition of nlp research on representations, mostly falling into one of four cate gories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (salton and mcgill, 1983; turney and pantel, 2010;sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (deerwester et al , 1990; honkela, 1997; kaski, 1998; sahlgren, 2005; blei et al , 2003; vayrynen et al , 2007); 3) using clusters that are induced from distributional similarity (brown et al , 1992; <papid> J92-4003 </papid>pereira et al , 1993; <papid> P93-1024 </papid>martin et al , 1998) as non-sparse features (lin and wu, 2009; <papid> P09-1116 </papid>candito and crabbe, 2009; koo et al , 2008; <papid> P08-1068 </papid>zhao et al , 2009); <papid> W09-1208 </papid>4) and recently, language models(bengio, 2008; mnih and hinton, 2009) as representations (weston et al , 2008; collobert and weston,2008; bengio et al , 2009), some of which have already yielded state of the art performance on domain adaptation tasks (huang and yates, 2009; <papid> P09-1056 </papid>huang and yates, 2010<papid> W10-2604 </papid>a; huang and yates, 2010<papid> W10-2604 </papid>b; turian et al , 2010) <papid> P10-1040 </papid>and ie (ahuja and downey, 2010; <papid> N10-1026 </papid>downey etal., 2007<papid> P07-1088 </papid>b).</citsent>
<aftsection>
<nextsent>in contrast to this previous work, we develop novel partial lattice mrf language model that incorporates facto rial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in pos tagging in domain adaptation setting.
</nextsent>
<nextsent>we also analyze the novel plmrf representation on an ie task, and several representations along the key dimensions of sparsity and polysemy.most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (daume?
</nextsent>
<nextsent>iii, 2007; jiang and zhai, 2007; <papid> P07-1034 </papid>daume?</nextsent>
<nextsent>iii and marcu, 2006; finkel and manning, 2009; <papid> N09-1068 </papid>dredzeet al , 2010; dredze and crammer, 2008).<papid> D08-1072 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3361">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sections 5 and 6 describe our two tasks and the resultsof using our representations on each of them.
</prevsent>
<prevsent>section 7 concludes.
</prevsent>
</prevsection>
<citsent citstr=" N10-1026 ">
there is long tradition of nlp research on representations, mostly falling into one of four cate gories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (salton and mcgill, 1983; turney and pantel, 2010;sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (deerwester et al , 1990; honkela, 1997; kaski, 1998; sahlgren, 2005; blei et al , 2003; vayrynen et al , 2007); 3) using clusters that are induced from distributional similarity (brown et al , 1992; <papid> J92-4003 </papid>pereira et al , 1993; <papid> P93-1024 </papid>martin et al , 1998) as non-sparse features (lin and wu, 2009; <papid> P09-1116 </papid>candito and crabbe, 2009; koo et al , 2008; <papid> P08-1068 </papid>zhao et al , 2009); <papid> W09-1208 </papid>4) and recently, language models(bengio, 2008; mnih and hinton, 2009) as representations (weston et al , 2008; collobert and weston,2008; bengio et al , 2009), some of which have already yielded state of the art performance on domain adaptation tasks (huang and yates, 2009; <papid> P09-1056 </papid>huang and yates, 2010<papid> W10-2604 </papid>a; huang and yates, 2010<papid> W10-2604 </papid>b; turian et al , 2010) <papid> P10-1040 </papid>and ie (ahuja and downey, 2010; <papid> N10-1026 </papid>downey etal., 2007<papid> P07-1088 </papid>b).</citsent>
<aftsection>
<nextsent>in contrast to this previous work, we develop novel partial lattice mrf language model that incorporates facto rial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in pos tagging in domain adaptation setting.
</nextsent>
<nextsent>we also analyze the novel plmrf representation on an ie task, and several representations along the key dimensions of sparsity and polysemy.most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (daume?
</nextsent>
<nextsent>iii, 2007; jiang and zhai, 2007; <papid> P07-1034 </papid>daume?</nextsent>
<nextsent>iii and marcu, 2006; finkel and manning, 2009; <papid> N09-1068 </papid>dredzeet al , 2010; dredze and crammer, 2008).<papid> D08-1072 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3362">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sections 5 and 6 describe our two tasks and the resultsof using our representations on each of them.
</prevsent>
<prevsent>section 7 concludes.
</prevsent>
</prevsection>
<citsent citstr=" P07-1088 ">
there is long tradition of nlp research on representations, mostly falling into one of four cate gories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (salton and mcgill, 1983; turney and pantel, 2010;sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (deerwester et al , 1990; honkela, 1997; kaski, 1998; sahlgren, 2005; blei et al , 2003; vayrynen et al , 2007); 3) using clusters that are induced from distributional similarity (brown et al , 1992; <papid> J92-4003 </papid>pereira et al , 1993; <papid> P93-1024 </papid>martin et al , 1998) as non-sparse features (lin and wu, 2009; <papid> P09-1116 </papid>candito and crabbe, 2009; koo et al , 2008; <papid> P08-1068 </papid>zhao et al , 2009); <papid> W09-1208 </papid>4) and recently, language models(bengio, 2008; mnih and hinton, 2009) as representations (weston et al , 2008; collobert and weston,2008; bengio et al , 2009), some of which have already yielded state of the art performance on domain adaptation tasks (huang and yates, 2009; <papid> P09-1056 </papid>huang and yates, 2010<papid> W10-2604 </papid>a; huang and yates, 2010<papid> W10-2604 </papid>b; turian et al , 2010) <papid> P10-1040 </papid>and ie (ahuja and downey, 2010; <papid> N10-1026 </papid>downey etal., 2007<papid> P07-1088 </papid>b).</citsent>
<aftsection>
<nextsent>in contrast to this previous work, we develop novel partial lattice mrf language model that incorporates facto rial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in pos tagging in domain adaptation setting.
</nextsent>
<nextsent>we also analyze the novel plmrf representation on an ie task, and several representations along the key dimensions of sparsity and polysemy.most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (daume?
</nextsent>
<nextsent>iii, 2007; jiang and zhai, 2007; <papid> P07-1034 </papid>daume?</nextsent>
<nextsent>iii and marcu, 2006; finkel and manning, 2009; <papid> N09-1068 </papid>dredzeet al , 2010; dredze and crammer, 2008).<papid> D08-1072 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3363">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast to this previous work, we develop novel partial lattice mrf language model that incorporates facto rial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in pos tagging in domain adaptation setting.
</prevsent>
<prevsent>we also analyze the novel plmrf representation on an ie task, and several representations along the key dimensions of sparsity and polysemy.most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (daume?
</prevsent>
</prevsection>
<citsent citstr=" P07-1034 ">
iii, 2007; jiang and zhai, 2007; <papid> P07-1034 </papid>daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2006; finkel and manning, 2009; <papid> N09-1068 </papid>dredzeet al , 2010; dredze and crammer, 2008).<papid> D08-1072 </papid></nextsent>
<nextsent>learning bounds are known (blitzer et al , 2007; mansour et al , 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3364">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>we also analyze the novel plmrf representation on an ie task, and several representations along the key dimensions of sparsity and polysemy.most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (daume?
</prevsent>
<prevsent>iii, 2007; jiang and zhai, 2007; <papid> P07-1034 </papid>daume?</prevsent>
</prevsection>
<citsent citstr=" N09-1068 ">
iii and marcu, 2006; finkel and manning, 2009; <papid> N09-1068 </papid>dredzeet al , 2010; dredze and crammer, 2008).<papid> D08-1072 </papid></citsent>
<aftsection>
<nextsent>learning bounds are known (blitzer et al , 2007; mansour et al , 2009).
</nextsent>
<nextsent>daume?
</nextsent>
<nextsent>iii et al  (2010) use semi supervised learning to incorporate labeled and unlabeled data from the target domain.
</nextsent>
<nextsent>in contrast, we investigate domain adaptation setting where no labeled data is available for the target domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3365">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>we also analyze the novel plmrf representation on an ie task, and several representations along the key dimensions of sparsity and polysemy.most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (daume?
</prevsent>
<prevsent>iii, 2007; jiang and zhai, 2007; <papid> P07-1034 </papid>daume?</prevsent>
</prevsection>
<citsent citstr=" D08-1072 ">
iii and marcu, 2006; finkel and manning, 2009; <papid> N09-1068 </papid>dredzeet al , 2010; dredze and crammer, 2008).<papid> D08-1072 </papid></citsent>
<aftsection>
<nextsent>learning bounds are known (blitzer et al , 2007; mansour et al , 2009).
</nextsent>
<nextsent>daume?
</nextsent>
<nextsent>iii et al  (2010) use semi supervised learning to incorporate labeled and unlabeled data from the target domain.
</nextsent>
<nextsent>in contrast, we investigate domain adaptation setting where no labeled data is available for the target domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3379">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> a novel lattice language model </section>
<citcontext>
<prevsection>
<prevsent>the values of the layers of latent variables for single word can be used as distinct features in our representation.
</prevsent>
<prevsent>the i-hmm attempts to mode lthe same intuition, but unlike lattice model the ihmm layers are entirely independent, and as result there is no mechanism to enforce that the layers model different dimensions.
</prevsent>
</prevsection>
<citsent citstr=" P05-2004 ">
duh (2005) <papid> P05-2004 </papid>previously used 2-layer lattice for tagging and chunking, but in supervised setting rather than for representation learning.</citsent>
<aftsection>
<nextsent>let cliq(x,y) represent the set of all maximal cliques in the graph of the mrf model for and y. 1percy liangs implementation is available at http://metaoptimize.com/projects/wordreprs/.
</nextsent>
<nextsent>turian et al  also tested run with 3200 clusters in their experiments, which we have been training for months, but which has not finished in time for publication.
</nextsent>
<nextsent>y4,1 y3,1 y4,2 y3,2 y4,3 y3,3 y4,4 y3,4 y4,5 y3,5 x1 2,1 y1,1 x2 2,2 y1,2 x3 2,3 y1,3 x4 2,4 y1,4 x5 2,5 y1,5 figure 1: the partial lattice mrf (pl-mrf) model for 5-word sentence and 4-layer lattice.
</nextsent>
<nextsent>dashed gray edges are part of full lattice, but not the pl-mrf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3380">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> a novel lattice language model </section>
<citcontext>
<prevsection>
<prevsent>3c.f. tree-width of min(m ,n ) for the un pruned model where (x), the neighborhood of x, indicates set of perturbed variations of the original sentence x. contrastive estimation seeks to move probability mass away from the perturbed neighborhood sentences and onto the original sentence.
</prevsent>
<prevsent>we use neighborhood function that includes all sentences which can be obtained from the original sentence by swapping the order of consecutive pair of words.
</prevsent>
</prevsection>
<citsent citstr=" P05-1044 ">
training uses gradient descent over this non-convex objective function with standard software package (liu and nocedal, 1989) and converges to local maximum (smith and eisner, 2005).<papid> P05-1044 </papid></citsent>
<aftsection>
<nextsent>for tract ability, we modify the training procedure to train the pl-mrf one layer at time.
</nextsent>
<nextsent>let represent the set of parameters relating to features of layer i, and let represent all other parameters.
</nextsent>
<nextsent>we fix 0 = 0, and optimize 0 using contrastiveestimation.
</nextsent>
<nextsent>after convergence, we fix 1, and optimize 1, and so on.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3382">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> domain adaptation for pos tagger.  </section>
<citcontext>
<prevsection>
<prevsent>the novel pl-mrf model outperforms the previous state of the art, the i-hmm model, and much of the performance increase comes from a11.3% relative reduction in error on words that appear in biomedical texts but not in newswire texts.
</prevsent>
<prevsent>both graphical model representations significantly outperform the ngram model, which is trained on farmore text.
</prevsent>
</prevsection>
<citsent citstr=" P07-1096 ">
for comparison, our best model, the plmrf, achieved 96.8% in-domain accuracy on sections 22-24 of the penn treebank, about 0.5% shy of state-of-the-art in-domain system (shen et al , 2007) <papid> P07-1096 </papid>with more sophisticated supervised learning.</citsent>
<aftsection>
<nextsent>we expected that language model representations perform well in part because they provide meaningful features for sparse and polysemous words.
</nextsent>
<nextsent>to test this, we selected 109 polysemous word types model % error oov % error trad-r 11.7 32.7 trad-r+self-training 11.5 29.6scl 11.1 brown-token-r 10.8 25.4 hmm-token-r 9.5 24.8 ngram-r 6.9 24.4 i-hmm-token-r 6.7 24 lattice-token-r 6.2 21.3scl+500bio 3.9 table 2: pl-mrf representations reduce error by 7.5%relative to the previous state-of-the-art i-hmm, and approach within 2.3% absolute error scl+500bio model with access to 500 labeled sentences from the target do main.
</nextsent>
<nextsent>1.8% of the tags in the test set are new tags that do not occur in the wsj training data, so an error rate of 3.9+1.8 = 5.7% error is reasonable bound for the best possible performance of model that has seen no examples from the target domain.
</nextsent>
<nextsent>from our test data, along with 296 non-polysemous word types, chosen based on pos tags and manual inspection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3386">
<title id=" W11-0315.xml">language models as representations for weakly supervised nlp tasks </title>
<section> information extraction experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we developed all word type representations 131 model auc hmm-type-r 0.18 brown-type-r 0.16 lattice-type-r 0.11 ngram-r 0.10 random baseline 0.10 table 4: hmm-type-r outperforms the other methods, improving performance by 12.5% over brown clusters, and by 80% over the traditional ngram-r.
</prevsent>
<prevsent>using this corpus.to obtain examples of multiple semantic categories, we utilized selected wikipedia listof?
</prevsent>
</prevsection>
<citsent citstr=" D09-1098 ">
pages from (pantel et al , 2009) <papid> D09-1098 </papid>and augmented these with our own manually defined categories, such tha teach list contained at least ten distinct examples occurring in our corpus.</citsent>
<aftsection>
<nextsent>in all, we had 432 examples across 16 distinct categories such as countries, greek islands, and police tv dramas.
</nextsent>
<nextsent>6.3 results.
</nextsent>
<nextsent>for each semantic category, we tested five different random selections of five seed examples, treating the unselected members of the category as positive examples, and all other candidate phrases as negative examples.
</nextsent>
<nextsent>we evaluate using the area under the precision-recall curve (auc) metric.the results are shown in table 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3387">
<title id=" W10-4133.xml">a character based joint model for cipssighan word segmentation bakeoff 2010 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems.
</prevsent>
<prevsent>the character-based tagging approach (xue, 2003) has become the dominant technique for chinese word segmentation (cws) as it can tolerate out-of-vocabulary (oov) words.
</prevsent>
</prevsection>
<citsent citstr=" I05-3027 ">
in the last few years, this method has been widely adopted and further improved in many previous works (tseng et al, 2005; <papid> I05-3027 </papid>zhang et al, 2006; <papid> P06-2123 </papid>jiang et al., 2008).<papid> P08-1102 </papid></citsent>
<aftsection>
<nextsent>among various character-based tagging approaches, the character-based joint model (wang et al, 2010) achieves good balance between in-vocabulary (iv) words recognition and oov words identification.
</nextsent>
<nextsent>in this work, we adopt the character-based joint model as our basic system, which combines character-based discriminative model and character-based generative model.
</nextsent>
<nextsent>the generative module holds robust performance on iv words, while the discriminative module can handle the extra features easily and enhance the oov words segmentation.
</nextsent>
<nextsent>however, the performance of out-of-domain text is still not satisfactory as that of in-domain text, while few previous works have paid attention to this problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3388">
<title id=" W10-4133.xml">a character based joint model for cipssighan word segmentation bakeoff 2010 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems.
</prevsent>
<prevsent>the character-based tagging approach (xue, 2003) has become the dominant technique for chinese word segmentation (cws) as it can tolerate out-of-vocabulary (oov) words.
</prevsent>
</prevsection>
<citsent citstr=" P06-2123 ">
in the last few years, this method has been widely adopted and further improved in many previous works (tseng et al, 2005; <papid> I05-3027 </papid>zhang et al, 2006; <papid> P06-2123 </papid>jiang et al., 2008).<papid> P08-1102 </papid></citsent>
<aftsection>
<nextsent>among various character-based tagging approaches, the character-based joint model (wang et al, 2010) achieves good balance between in-vocabulary (iv) words recognition and oov words identification.
</nextsent>
<nextsent>in this work, we adopt the character-based joint model as our basic system, which combines character-based discriminative model and character-based generative model.
</nextsent>
<nextsent>the generative module holds robust performance on iv words, while the discriminative module can handle the extra features easily and enhance the oov words segmentation.
</nextsent>
<nextsent>however, the performance of out-of-domain text is still not satisfactory as that of in-domain text, while few previous works have paid attention to this problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3389">
<title id=" W10-4133.xml">a character based joint model for cipssighan word segmentation bakeoff 2010 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems.
</prevsent>
<prevsent>the character-based tagging approach (xue, 2003) has become the dominant technique for chinese word segmentation (cws) as it can tolerate out-of-vocabulary (oov) words.
</prevsent>
</prevsection>
<citsent citstr=" P08-1102 ">
in the last few years, this method has been widely adopted and further improved in many previous works (tseng et al, 2005; <papid> I05-3027 </papid>zhang et al, 2006; <papid> P06-2123 </papid>jiang et al., 2008).<papid> P08-1102 </papid></citsent>
<aftsection>
<nextsent>among various character-based tagging approaches, the character-based joint model (wang et al, 2010) achieves good balance between in-vocabulary (iv) words recognition and oov words identification.
</nextsent>
<nextsent>in this work, we adopt the character-based joint model as our basic system, which combines character-based discriminative model and character-based generative model.
</nextsent>
<nextsent>the generative module holds robust performance on iv words, while the discriminative module can handle the extra features easily and enhance the oov words segmentation.
</nextsent>
<nextsent>however, the performance of out-of-domain text is still not satisfactory as that of in-domain text, while few previous works have paid attention to this problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3390">
<title id=" W11-0308.xml">using sequence kernels to identify opinion entities in urdu </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the morphological richness of the urdu language enables us to extract features based on noun and verb inflections that effectively contribute to the opinion entity extraction task.
</prevsent>
<prevsent>most importantly, these features can be generalized to other indic languages (hindi, bengali etc.) owing to the grammatical similarity between the languages.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
58english has seen extensive use of sequence kernels (string and tree kernels) for tasks such as relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and semantic role labeling (moschitti et al, 2008).<papid> J08-2003 </papid></citsent>
<aftsection>
<nextsent>but, the application of these kernels to task like opinion entity detection is scarcely explored (weigand and klalow, 2010).
</nextsent>
<nextsent>moreover, existing works in english perform only opinion holder identification using these kernels.
</nextsent>
<nextsent>what makes our approach unique is that we use the power of sequence kernels to simultaneously identify opinion holders and targets in the urdu language.
</nextsent>
<nextsent>sequence kernels allow efficient use of the learning algorithm exploiting massive number of features without the traditional explicit feature representation (such as, bag of words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3391">
<title id=" W11-0308.xml">using sequence kernels to identify opinion entities in urdu </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the morphological richness of the urdu language enables us to extract features based on noun and verb inflections that effectively contribute to the opinion entity extraction task.
</prevsent>
<prevsent>most importantly, these features can be generalized to other indic languages (hindi, bengali etc.) owing to the grammatical similarity between the languages.
</prevsent>
</prevsection>
<citsent citstr=" J08-2003 ">
58english has seen extensive use of sequence kernels (string and tree kernels) for tasks such as relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and semantic role labeling (moschitti et al, 2008).<papid> J08-2003 </papid></citsent>
<aftsection>
<nextsent>but, the application of these kernels to task like opinion entity detection is scarcely explored (weigand and klalow, 2010).
</nextsent>
<nextsent>moreover, existing works in english perform only opinion holder identification using these kernels.
</nextsent>
<nextsent>what makes our approach unique is that we use the power of sequence kernels to simultaneously identify opinion holders and targets in the urdu language.
</nextsent>
<nextsent>sequence kernels allow efficient use of the learning algorithm exploiting massive number of features without the traditional explicit feature representation (such as, bag of words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3392">
<title id=" W11-0308.xml">using sequence kernels to identify opinion entities in urdu </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our approach shows encouraging performance.
</prevsent>
<prevsent>choi et al, (2005) consider opinion entity identification as an information extraction task and the opinion holders are identified using conditional random field (lafferty et al, 2001) based se quence-labeling approach.
</prevsent>
</prevsection>
<citsent citstr=" W03-0404 ">
patterns are extracted using auto slog (riloff et al, 2003).<papid> W03-0404 </papid></citsent>
<aftsection>
<nextsent>bloom et al, (2006) use hand built lexicons for opinion entity identification.
</nextsent>
<nextsent>their method is dependent on combination of heuristic shallow parsing and dependency parsing information.
</nextsent>
<nextsent>kim and hovy 59 (2006) map the semantic frames of framenet (baker et al, 1998) into opinion holder and target for adjectives and verbs to identify these components.
</nextsent>
<nextsent>stoyanov and cardie (2008) <papid> L08-1088 </papid>treat the task of identifying opinion holders and targets as coreference resolution problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3393">
<title id=" W11-0308.xml">using sequence kernels to identify opinion entities in urdu </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their method is dependent on combination of heuristic shallow parsing and dependency parsing information.
</prevsent>
<prevsent>kim and hovy 59 (2006) map the semantic frames of framenet (baker et al, 1998) into opinion holder and target for adjectives and verbs to identify these components.
</prevsent>
</prevsection>
<citsent citstr=" L08-1088 ">
stoyanov and cardie (2008) <papid> L08-1088 </papid>treat the task of identifying opinion holders and targets as coreference resolution problem.</citsent>
<aftsection>
<nextsent>kim et al, (2008) used set of communication words, appraisal words from senti-wordnet (esuli and sebastiani, 2006) and nlp tools such as ne taggers and syntactic parsers to identify opinion holders accurately.
</nextsent>
<nextsent>kim and hovy (2006) <papid> W06-0301 </papid>use structural features of the language to identify opinion enti ties.</nextsent>
<nextsent>their technique is based on syntactic path and dependency features along with heuristic features such as topic words and named entities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3394">
<title id=" W11-0308.xml">using sequence kernels to identify opinion entities in urdu </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>stoyanov and cardie (2008) <papid> L08-1088 </papid>treat the task of identifying opinion holders and targets as coreference resolution problem.</prevsent>
<prevsent>kim et al, (2008) used set of communication words, appraisal words from senti-wordnet (esuli and sebastiani, 2006) and nlp tools such as ne taggers and syntactic parsers to identify opinion holders accu rately.</prevsent>
</prevsection>
<citsent citstr=" W06-0301 ">
kim and hovy (2006) <papid> W06-0301 </papid>use structural features of the language to identify opinion enti ties.</citsent>
<aftsection>
<nextsent>their technique is based on syntactic path and dependency features along with heuristic features such as topic words and named entities.
</nextsent>
<nextsent>weigand and klalow (2010) use convolution kernels that use predicate argument structure and parse trees.
</nextsent>
<nextsent>for urdu specifically, work in the area of classifying subjective and objective sentences is attempted by mukund and srihari, (2010) <papid> C10-2099 </papid>using vector space model.</nextsent>
<nextsent>nlp tools that include pos taggers, shallow parser, ne tagger and morphological analyzer for urdu is provided by mukund et al, (2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3395">
<title id=" W11-0308.xml">using sequence kernels to identify opinion entities in urdu </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their technique is based on syntactic path and dependency features along with heuristic features such as topic words and named entities.
</prevsent>
<prevsent>weigand and klalow (2010) use convolution kernels that use predicate argument structure and parse trees.
</prevsent>
</prevsection>
<citsent citstr=" C10-2099 ">
for urdu specifically, work in the area of classifying subjective and objective sentences is attempted by mukund and srihari, (2010) <papid> C10-2099 </papid>using vector space model.</citsent>
<aftsection>
<nextsent>nlp tools that include pos taggers, shallow parser, ne tagger and morphological analyzer for urdu is provided by mukund et al, (2010).
</nextsent>
<nextsent>this is the only extensive work done for automating urdu nlp, although other efforts to generate semantic role labels and dependency parsers are underway.
</nextsent>
<nextsent>in this section we introduce the different cues used to capture the contextual information for creating candidate sequences in urdu by exploiting the morphological richness of the language.
</nextsent>
<nextsent>table 2: case inflections on nouns urdu is head final language with post positional case markers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3397">
<title id=" W11-0308.xml">using sequence kernels to identify opinion entities in urdu </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>gaps between words are penalized with suitable use of decay factor to compensate for matches between lengthy word sequences.
</prevsent>
<prevsent>formally, let be the feature space over words.
</prevsent>
</prevsection>
<citsent citstr=" H05-1091 ">
consequently, we declare other disjoint feature spaces (stem words, pos, chunks, gender inflections, etc.) and . for any two-feature vectors let compute the number of common features between and t. table 5 lists the features used to compute . given two sequences, and and the kernel function that calculates the number of 61 weighted sparse sub sequences of length (say, =2: bigram) common to both and t, then is as shown in eq 1 (bunescu and mooney, 2005).<papid> H05-1091 </papid></citsent>
<aftsection>
<nextsent>(i,j,k are dimensions) ------ eq 1.
</nextsent>
<nextsent>generating correct sequences is prior requirement for sequence kernels.
</nextsent>
<nextsent>for example, in the task of relation extraction, features included in the shortest path between the mentions of the two sequences (which hold the relation) play decisive role (bunescu and mooney, 2005).<papid> H05-1091 </papid></nextsent>
<nextsent>similarly, in the task of role labeling (srl - moschitti et al, 2008), <papid> J08-2003 </papid>syntactic sub-trees containing the arguments are crucial in finding the correct associations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3402">
<title id=" W11-0308.xml">using sequence kernels to identify opinion entities in urdu </title>
<section> pos sequence of predicate words.  </section>
<citcontext>
<prevsection>
<prevsent>this shows the robustness of sk and its capability to learn complex substructures with only words.
</prevsent>
<prevsent>a sequence kernel considers all possible sub sequence matching and therefore implements concept of partial (fuzzy) matching.
</prevsent>
</prevsection>
<citsent citstr=" I08-2119 ">
because of its tendency to learn all fuzzy matches while penalizing the gaps between words intelligently, the performance of sk in general has better recall (wang, 2008).<papid> I08-2119 </papid></citsent>
<aftsection>
<nextsent>to explain the recall situation, consider set 2 of table 8.
</nextsent>
<nextsent>this illustrates the effect of disjoint feature.
</nextsent>
<nextsent>scopes of each feature (pos, chunk, gender).
</nextsent>
<nextsent>each feature adds up and expands the feature space of sequence kernel and allows fuzzy matching there by improving the recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3403">
<title id=" W11-0307.xml">search based structured prediction applied to biomedical event extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>additionally, we consider the issue of cost estimation during learning and present an approach called focused costing that improves improves efficiency and predictive accuracy.
</prevsent>
<prevsent>the term biomedical event extraction is used to refer to the task of extracting descriptions of actions and relations involving one or more entities from the biomedical literature.
</prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
the recent bionlp 2009 shared task (bionlp09st) on event extraction (kimet al, 2009) <papid> W09-1401 </papid>focused on event types of varying com plexity.</citsent>
<aftsection>
<nextsent>each event consists of trigger and one or more arguments, the latter being proteins or other events.
</nextsent>
<nextsent>any token in sentence can be trigger for one of the nine event types and, depending on their associated event types, triggers are assigned appropriate arguments.
</nextsent>
<nextsent>thus, the task can be viewed as structured prediction problem in which the output for given instance is (possibly disconnected) directed acyclic graph (not necessarily tree) in which vertices correspond to triggers or protein arguments, and edges represent relations between them.
</nextsent>
<nextsent>despite being structured prediction task, most of the systems that have been applied to bionlp09stto date are pipelines that decompose event extraction into set of simpler classification tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3404">
<title id=" W11-0307.xml">search based structured prediction applied to biomedical event extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>classifiers for these tasks are typically learned independently, thereby ignoring event structure during training.
</prevsent>
<prevsent>typically in such systems, the relationships among these tasks are taken into account by incorporating post-processing rules that enforce certain constraints when combining their predictions, andby tuning classification thresholds to improve the accuracy of joint predictions.
</prevsent>
</prevsection>
<citsent citstr=" C10-1088 ">
pipelines are appealing as they are relatively easy to implement and they of ten achieve state-of-the-art performance (bjorne et al., 2009; miwa et al, 2010).<papid> C10-1088 </papid></citsent>
<aftsection>
<nextsent>because of the nature of the output space, the taskis not amenable to sequential or grammar-based approaches (e.g. linear crfs, hmms, pcfgs) which employ dynamic programming in order to do efficient inference.
</nextsent>
<nextsent>the only joint inference framework that has been applied to bionlp09st to date is markov logic networks (mlns) (riedel et al, 2009; <papid> W09-1406 </papid>poon and vanderwende, 2010).<papid> N10-1123 </papid></nextsent>
<nextsent>however,mlns require task-dependent approximate inference and substantial computational resources in order to achieve state-of-the-art performance.in this work we explore an alternative joint inference approach to biomedical event extraction using search-based structured prediction framework, searn (daume?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3405">
<title id=" W11-0307.xml">search based structured prediction applied to biomedical event extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pipelines are appealing as they are relatively easy to implement and they of ten achieve state-of-the-art performance (bjorne et al., 2009; miwa et al, 2010).<papid> C10-1088 </papid></prevsent>
<prevsent>because of the nature of the output space, the taskis not amenable to sequential or grammar-based approaches (e.g. linear crfs, hmms, pcfgs) which employ dynamic programming in order to do efficient inference.</prevsent>
</prevsection>
<citsent citstr=" W09-1406 ">
the only joint inference framework that has been applied to bionlp09st to date is markov logic networks (mlns) (riedel et al, 2009; <papid> W09-1406 </papid>poon and vanderwende, 2010).<papid> N10-1123 </papid></citsent>
<aftsection>
<nextsent>however,mlns require task-dependent approximate inference and substantial computational resources in order to achieve state-of-the-art performance.in this work we explore an alternative joint inference approach to biomedical event extraction using search-based structured prediction framework, searn (daume?
</nextsent>
<nextsent>iii et al, 2009).
</nextsent>
<nextsent>searn is an algorithm that converts the problem of learning model for structured prediction into learning set of models for cost-sensitive classification (csc).
</nextsent>
<nextsent>csc is task in which each training instance has vector of misclassification costs associated with it, thus rendering some mistakes on some instances to be more expensive than others (domingos, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3406">
<title id=" W11-0307.xml">search based structured prediction applied to biomedical event extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pipelines are appealing as they are relatively easy to implement and they of ten achieve state-of-the-art performance (bjorne et al., 2009; miwa et al, 2010).<papid> C10-1088 </papid></prevsent>
<prevsent>because of the nature of the output space, the taskis not amenable to sequential or grammar-based approaches (e.g. linear crfs, hmms, pcfgs) which employ dynamic programming in order to do efficient inference.</prevsent>
</prevsection>
<citsent citstr=" N10-1123 ">
the only joint inference framework that has been applied to bionlp09st to date is markov logic networks (mlns) (riedel et al, 2009; <papid> W09-1406 </papid>poon and vanderwende, 2010).<papid> N10-1123 </papid></citsent>
<aftsection>
<nextsent>however,mlns require task-dependent approximate inference and substantial computational resources in order to achieve state-of-the-art performance.in this work we explore an alternative joint inference approach to biomedical event extraction using search-based structured prediction framework, searn (daume?
</nextsent>
<nextsent>iii et al, 2009).
</nextsent>
<nextsent>searn is an algorithm that converts the problem of learning model for structured prediction into learning set of models for cost-sensitive classification (csc).
</nextsent>
<nextsent>csc is task in which each training instance has vector of misclassification costs associated with it, thus rendering some mistakes on some instances to be more expensive than others (domingos, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3407">
<title id=" W11-0307.xml">search based structured prediction applied to biomedical event extraction </title>
<section> bionlp 2009 shared task description.  </section>
<citcontext>
<prevsection>
<prevsent>we believe that these contributions are likely to be relevant to applications of searn to other natural language processing tasks that involve structured prediction in complex output spaces.
</prevsent>
<prevsent>bionlp09st focused on the extraction of events involving proteins whose names are annotated in advance.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
each event has two types of arguments, theme and cause, which correspond respectively tothe agent and patient roles in semantic role labeling (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>nine event types are defined which can be broadly classified in three categories, namely simple, binding and regulation.simple events include gene expression, transcription, protein catabolism, phosphorylation, and localization events.
</nextsent>
<nextsent>these have only one theme argument which is protein.
</nextsent>
<nextsent>binding events have one or more protein themes.
</nextsent>
<nextsent>finally, regulation events, which include positive regulation, negative regulation and regulation, have one obligatory theme and one optional cause, each of which can be either protein or another event.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3408">
<title id=" W11-0307.xml">search based structured prediction applied to biomedical event extraction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5.a pre-processing step we perform on the training data is to reduce the multi-token triggers in thegold standard to their syntactic heads.
</prevsent>
<prevsent>this procedure simplifies the task of assigning arguments to triggers and, as the evaluation variant used allows approximate trigger matching, it does not result in performance loss.
</prevsent>
</prevsection>
<citsent citstr=" P08-2026 ">
for syntactic parsing, we use the output of the bllip re-ranking parser adapted to the biomedical domain by mcclosky and charniak (2008), <papid> P08-2026 </papid>as provided by the shared task organizers in the stanford collapsed dependency format with conjunct dependency propagation.</citsent>
<aftsection>
<nextsent>lemmatization is performed using morpha (minnen et al, 2001).
</nextsent>
<nextsent>in all our experiments, for csc learning with pa,the parameter is set by tuning on 10% of the training data and the number of rounds is fixed to 10.
</nextsent>
<nextsent>for searn, we set the interpolation parameter ? to 0.3 and the number of iterations to 12.
</nextsent>
<nextsent>the costs for each action are obtained by averaging three sample sas described in sec.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3413">
<title id=" W11-0307.xml">search based structured prediction applied to biomedical event extraction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>finally, using one instead of three samples per action decreases the f-score by 1.3 points on the development data.
</prevsent>
<prevsent>compared with the mln approaches applied to bionlp09st, our predictive accuracy is better than that of poon and vanderwende (2010) <papid> N10-1123 </papid>which is thebest joint inference performance to date and substantially better than that of riedel et al (2009) (<papid> W09-1406 </papid>50 and 44.4 in f-score respectively).</prevsent>
</prevsection>
<citsent citstr=" P11-1163 ">
recently, mcclosky et al (2011) <papid> P11-1163 </papid>combined multiple decoders forde pendency parser with reranker, achieving 48.6 inf-score.</citsent>
<aftsection>
<nextsent>while they also extracted structure features for theme and cause assignment, their model is restricted to trees (ours can output directed acyclicgraphs) and their trigger recognizer is learned independently.
</nextsent>
<nextsent>when we train searn combining the training and the development sets, we reach 52.3 in f-score, which is better than the performance of the top system in bionlp09st (51.95) by bjorne et al (2009) which was trained in the same way.
</nextsent>
<nextsent>the best performance to date is reported by miwa et al (2010) (<papid> C10-1088 </papid>56.3 in f-score), who experimented with six parsers, three dependency representations and various combinations of these.</nextsent>
<nextsent>they found that different parser/dependency combinations provided the best results on the development and test sets.a direct comparison between learning frameworks is difficult due to the differences in task decomposition and feature extraction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3417">
<title id=" W10-4114.xml">a statistical nlp approach for feature and sentiment identification from chinese reviews </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>grammatically incorrect.
</prevsent>
<prevsent>to address the issue, our approach employs an additional step to prune candidates with low topical relevance, which is statistical measure of how frequently term appears in one review and across different reviews.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
pang et al (2002) <papid> W02-1011 </papid>examined the effectiveness of using supervised learning methods to identify document level sentiments.</citsent>
<aftsection>
<nextsent>but the technique requires large amount of training data, and must be re-trained whenever it is applied to new domain.
</nextsent>
<nextsent>furthermore, it does not perform well at the sentence level.
</nextsent>
<nextsent>zhou et al (2008) and qiu et al (2008) proposed dictionary-based approaches to infer contextual sentiments from chinese sentences.
</nextsent>
<nextsent>however, it is difficult to maintain an up-to-date dictionary, as new expressions emerge frequently online.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3418">
<title id=" W10-4114.xml">a statistical nlp approach for feature and sentiment identification from chinese reviews </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our idea of topical relevance is related but different; we only focus on the relevance of candidate feature with respect to review topic, so as to extract the features on which sentiments are expressed.
</prevsent>
<prevsent>2.2 polarity inference for opinion word.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
turney (2002) <papid> P02-1053 </papid>used point-wise mutual information (pmi) to predict the polarity of an opinion word o, which is calculated as mi1-mi2, where mi1 is the mutual information between word and positive word excellent?, and mi2 denotes the mutual information between and negative word poor?.</citsent>
<aftsection>
<nextsent>instead of pmi, our method uses the likelihood ratio test (lrt) to compute the semantic association between an opinion word and each seed word, since lrt leads to better 2a review region is sentence or clause which contains one and only feature.
</nextsent>
<nextsent>results in practice.
</nextsent>
<nextsent>finally, the polarity is calculated as the weighted sum of the polarity values of all seed words, where the weights are determined by the semantic association.
</nextsent>
<nextsent>2.3 feature-sentiment pair identification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3422">
<title id=" W10-4114.xml">a statistical nlp approach for feature and sentiment identification from chinese reviews </title>
<section> contextual sentiment identification.  </section>
<citcontext>
<prevsection>
<prevsent>(common) has score of 0, and ????
</prevsent>
<prevsent>(lousy) has score of -10.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
to measure the semantic association aij between an opinion word oi and each seed word sj, we propose formula based on the likelihood ratio test (dunning, 1993), <papid> J93-1003 </papid>as follows: 1 1 1 2 2 2 1 1 2 2 2[ log ( , , ) log ( , , ) log ( , , ) log ( , , ) ] ija p n p n p n p n + ? ?</citsent>
<aftsection>
<nextsent>= (6) where ( , , ) (1 )k nl k p ?= ? ; 1 2 1 2 k n += + , 1 1 1 p = , 22 2 p = ; 1 1n k= + 3 2 2n k= +, . 4 the variable k1(o, s) in table 4 refers to the count of documents containing both opinion word and seed word s, k2(o,s) indicates the number of documents containing but not s, k3(o, s) counts the number of documents containing but not o, while k4(o,?
</nextsent>
<nextsent>s) tallies the count of documents containing neither nor s. table 4: document counts.
</nextsent>
<nextsent>s ? o k1 (o, s) k2 (o,?
</nextsent>
<nextsent>s) k3 (o, s) k4 (o,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3423">
<title id=" W11-1513.xml">topic modeling on historical newspapers </title>
<section> topic modeling.  </section>
<citcontext>
<prevsection>
<prevsent>the former used the probabilistic latent semantic analysis (plsa) model, and the latter used the latent dirichlet al ocation (lda) model, method introduced by blei et al  (2003).
</prevsent>
<prevsent>lda has also been used by griffiths and steyvers (2004) to 1http://americanpast.richmond.edu/dispatch/ find research topic trends by looking at abstracts of scientific papers.
</prevsent>
</prevsection>
<citsent citstr=" D08-1038 ">
hallet al  (2008) <papid> D08-1038 </papid>have similarly applied lda to discover trends in the computational linguistics field.</citsent>
<aftsection>
<nextsent>both plsa and lda models are probabilistic models that look at each document as mixture of multinomials or topics.
</nextsent>
<nextsent>the models decompose the document collection into groups of words representing the main topics.
</nextsent>
<nextsent>see for instance table 1, which shows two topics extracted from our collection.
</nextsent>
<nextsent>topic worth price black white goods yard silk made ladies wool lot inch week sale prices pair suits fine quality state states bill united people men general law government party made president today washington war committee country public york table 1: example of two topic groups boyd-graber et al  (2009) compared several topic models, including lda, correlated topic model (ctm), and probabilistic latent semantic indexing(plsi), and found that lda generally worked comparably well or better than the other two at predicting topics that match topics picked by the human annotators.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3424">
<title id=" W11-1513.xml">topic modeling on historical newspapers </title>
<section> topic modeling.  </section>
<citcontext>
<prevsection>
<prevsent>topic worth price black white goods yard silk made ladies wool lot inch week sale prices pair suits fine quality state states bill united people men general law government party made president today washington war committee country public york table 1: example of two topic groups boyd-graber et al  (2009) compared several topic models, including lda, correlated topic model (ctm), and probabilistic latent semantic indexing(plsi), and found that lda generally worked comparably well or better than the other two at predicting topics that match topics picked by the human annotators.
</prevsent>
<prevsent>we therefore chose to use parallel threaded sparse lda implementation to conduct the topic modeling, namely umass amherstsmachine learning for language toolkit (mallet)2 (mccallum, 2002).
</prevsent>
</prevsection>
<citsent citstr=" D10-1024 ">
mallets topic modeling toolkit has been used by walker et al  (2010)<papid> D10-1024 </papid>to test the effects of noisy optical character recognition (ocr) data on lda.</citsent>
<aftsection>
<nextsent>it has been used by nelson (2010) to mine topics from the civil war era newspaper dispatch, and it has also been used by blevins (2010) to examine general topics and to identify emotional moments from martha balla rds diary.3
</nextsent>
<nextsent>our sample data comes from collection of digitized historical newspapers, consisting of newspapers published in texas from 1829 to 2008.
</nextsent>
<nextsent>issues are segmented by pages with continuous text containing articles and advertisements.
</nextsent>
<nextsent>table 2 provides more information about the dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3425">
<title id=" W11-1513.xml">topic modeling on historical newspapers </title>
<section> dataset.  </section>
<citcontext>
<prevsection>
<prevsent>suggested replacements are used if they are within the length normalized edit distance of the originals.
</prevsent>
<prevsent>an extra dictionary list of location names is used with aspell.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
next, the spelling corrected dataset is run through the stanford named entity recognizer (ner).4 stanford ner system first detects sentences in thedata then labels four classes of named entities: person, organization, location, and miscellaneous (finkel et al , 2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>the model used in conjunction with the tagger is provided by the software and was trained on the conll 2003 training data using distributional similarity features.
</nextsent>
<nextsent>the output is then massaged so that entities with multiple words would stay together in the topic modeling phase.
</nextsent>
<nextsent>property # of unique # of total loc entities 1,508,432 8,620,856 org entities 6,497,111 14,263,391 per entities 2,846,906 12,260,535 misc entities 1,182,845 3,594,916 named entities 12,035,294 38,739,698 table 3: properties of the newspaper collection after named entity recognition lastly, the words that are not tagged as named entities pass through an english stemmer while the named entities stay unchanged.
</nextsent>
<nextsent>we are using the snowball stemmer.5 at the end of each of the pre-processing stage, we extract subsets from the data corresponding to the sample years mentioned earlier (1865-1901, 1892,1893, and 1929-1930), which are then used for further processing in the topic modeling phase.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3426">
<title id=" W11-0128.xml">ontology based distinction between polysemy and homonymy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the distinction between polysemy and homonymy also has important potential ramifications for computational linguistics, in particular for word sense disambiguation (wsd).
</prevsent>
<prevsent>notably, ide and wilks (2006) argue that wsd should focus on modeling homonymous sense distinctions, which are easy to make and provide most benefit.
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
another case in point is the one-sense-per-discourse hypothesis (gale et al, 1992), <papid> H92-1045 </papid>which claims that within discourse, instances of word will strongly tend towards realizing the same sense.</citsent>
<aftsection>
<nextsent>this hypothesis seems to apply primarily to homonyms, as pointed out by krovetz (1998).
</nextsent>
<nextsent>unfortunately, the distinction between polysemy and homonymy is still very much an unsolved question.
</nextsent>
<nextsent>the discussion in the theoretical literature focuses mostly on clear-cut examples and avoids the broader issue.
</nextsent>
<nextsent>work on wsd, and in computational linguistics more generally, almost exclusively builds on the wordnet (fellbaum, 1998) word sense inventory, which lists an unstructured set of senses for each word and does not indicate in which way these senses are semantically related.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3428">
<title id=" W11-0128.xml">ontology based distinction between polysemy and homonymy </title>
<section> the one-sense-per-discourse hypothesis.  </section>
<citcontext>
<prevsection>
<prevsent>if word such as sentence appears two or more times in well-written discourse, it is extremely likely that they will all share the same sense?.
</prevsent>
<prevsent>the authors verified their hypothesis on small experiment with encouraging results (only 4% of discourses broke the hypothesis).
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
indeed, if this hypothesis were unreservedly true, then it would represent very strong global constraint that could serve to improve word sense disambiguation ? and in fact, follow-up paper by yarowsky (1995) <papid> P95-1026 </papid>exploited the hypothesis for this benefit.</citsent>
<aftsection>
<nextsent>unfortunately, it seems that 1spd does not apply universally.
</nextsent>
<nextsent>at the time (1992), wordnet had not yet emerged as widely used sense inventory, and the sense labels used by gale et al were fairly coarse-grained ones, motivated by translation pairs (e.g., english duty translated as french droit (tax) vs. devoir (obligation)), which correspond mostly to homonymous sense distinctions.5 current wsd, in contrast, uses the much more fine-grained wordnet sense inventory which conflates homonymous and polysemous sense distinctions.
</nextsent>
<nextsent>now, 1spd seems intuitively plausible for homonyms, where the senses describe different entities that are unlikely to occur in the same discourse (or if they do, different words will be used).
</nextsent>
<nextsent>however, the situation is different for polysemous words: in discourse about party, bottle might felicitously occur both as an object and measure word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3429">
<title id=" W11-0128.xml">ontology based distinction between polysemy and homonymy </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>we will use this information as global prior on an all-words?
</prevsent>
<prevsent>wsd task, where all occurrences of word in discourse need to be disambiguated.
</prevsent>
</prevsection>
<citsent citstr=" H05-1051 ">
finally, stokoe (2005) <papid> H05-1051 </papid>demonstrates the chances for improvement in information retrieval systems if we can reliably distinguish between homonymous and polysemous senses of word.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3430">
<title id=" W10-4005.xml">the noisier the better identifying multilingual word translations using a single monolingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also com-pare the results for two different types of corpora, namely new sticker texts and web corpora.
</prevsent>
<prevsent>our findings show that re-sults are best if english is the source language, and that noisy web corpora are better suited for this task than well edited new sticker texts.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
established methods for the identification of word translations are based on parallel (brown et al, 1990) <papid> J90-2002 </papid>or comparable corpora (fung &amp; mckeown, 1997; <papid> W97-0119 </papid>fung &amp; yee, 1998; <papid> P98-1069 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp 1999; <papid> P99-1067 </papid>chiao et al, 2004).</citsent>
<aftsection>
<nextsent>the work using parallel corpora such as europarl (koehn, 2005; armstrong et al, 1998) or jrc acquis (steinberger et al, 2006) typically performs length-based sentence alignment of the trans-lated texts, and then tries to conduct word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible.
</nextsent>
<nextsent>this approach works very well and can easily be put into prac-tice using number of freely available open source tools such as moses (koehn et al, 2007) <papid> P07-2045 </papid>and giza++ (och &amp; ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>however, parallel texts are scarce resource for many language pairs (rapp &amp; martn vide, 2007), which is why methods based on compa-rable corpora have come into focus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3431">
<title id=" W10-4005.xml">the noisier the better identifying multilingual word translations using a single monolingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also com-pare the results for two different types of corpora, namely new sticker texts and web corpora.
</prevsent>
<prevsent>our findings show that re-sults are best if english is the source language, and that noisy web corpora are better suited for this task than well edited new sticker texts.
</prevsent>
</prevsection>
<citsent citstr=" W97-0119 ">
established methods for the identification of word translations are based on parallel (brown et al, 1990) <papid> J90-2002 </papid>or comparable corpora (fung &amp; mckeown, 1997; <papid> W97-0119 </papid>fung &amp; yee, 1998; <papid> P98-1069 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp 1999; <papid> P99-1067 </papid>chiao et al, 2004).</citsent>
<aftsection>
<nextsent>the work using parallel corpora such as europarl (koehn, 2005; armstrong et al, 1998) or jrc acquis (steinberger et al, 2006) typically performs length-based sentence alignment of the trans-lated texts, and then tries to conduct word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible.
</nextsent>
<nextsent>this approach works very well and can easily be put into prac-tice using number of freely available open source tools such as moses (koehn et al, 2007) <papid> P07-2045 </papid>and giza++ (och &amp; ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>however, parallel texts are scarce resource for many language pairs (rapp &amp; martn vide, 2007), which is why methods based on compa-rable corpora have come into focus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3432">
<title id=" W10-4005.xml">the noisier the better identifying multilingual word translations using a single monolingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also com-pare the results for two different types of corpora, namely new sticker texts and web corpora.
</prevsent>
<prevsent>our findings show that re-sults are best if english is the source language, and that noisy web corpora are better suited for this task than well edited new sticker texts.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
established methods for the identification of word translations are based on parallel (brown et al, 1990) <papid> J90-2002 </papid>or comparable corpora (fung &amp; mckeown, 1997; <papid> W97-0119 </papid>fung &amp; yee, 1998; <papid> P98-1069 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp 1999; <papid> P99-1067 </papid>chiao et al, 2004).</citsent>
<aftsection>
<nextsent>the work using parallel corpora such as europarl (koehn, 2005; armstrong et al, 1998) or jrc acquis (steinberger et al, 2006) typically performs length-based sentence alignment of the trans-lated texts, and then tries to conduct word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible.
</nextsent>
<nextsent>this approach works very well and can easily be put into prac-tice using number of freely available open source tools such as moses (koehn et al, 2007) <papid> P07-2045 </papid>and giza++ (och &amp; ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>however, parallel texts are scarce resource for many language pairs (rapp &amp; martn vide, 2007), which is why methods based on compa-rable corpora have come into focus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3433">
<title id=" W10-4005.xml">the noisier the better identifying multilingual word translations using a single monolingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also com-pare the results for two different types of corpora, namely new sticker texts and web corpora.
</prevsent>
<prevsent>our findings show that re-sults are best if english is the source language, and that noisy web corpora are better suited for this task than well edited new sticker texts.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
established methods for the identification of word translations are based on parallel (brown et al, 1990) <papid> J90-2002 </papid>or comparable corpora (fung &amp; mckeown, 1997; <papid> W97-0119 </papid>fung &amp; yee, 1998; <papid> P98-1069 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp 1999; <papid> P99-1067 </papid>chiao et al, 2004).</citsent>
<aftsection>
<nextsent>the work using parallel corpora such as europarl (koehn, 2005; armstrong et al, 1998) or jrc acquis (steinberger et al, 2006) typically performs length-based sentence alignment of the trans-lated texts, and then tries to conduct word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible.
</nextsent>
<nextsent>this approach works very well and can easily be put into prac-tice using number of freely available open source tools such as moses (koehn et al, 2007) <papid> P07-2045 </papid>and giza++ (och &amp; ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>however, parallel texts are scarce resource for many language pairs (rapp &amp; martn vide, 2007), which is why methods based on compa-rable corpora have come into focus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3434">
<title id=" W10-4005.xml">the noisier the better identifying multilingual word translations using a single monolingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also com-pare the results for two different types of corpora, namely new sticker texts and web corpora.
</prevsent>
<prevsent>our findings show that re-sults are best if english is the source language, and that noisy web corpora are better suited for this task than well edited new sticker texts.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
established methods for the identification of word translations are based on parallel (brown et al, 1990) <papid> J90-2002 </papid>or comparable corpora (fung &amp; mckeown, 1997; <papid> W97-0119 </papid>fung &amp; yee, 1998; <papid> P98-1069 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp 1999; <papid> P99-1067 </papid>chiao et al, 2004).</citsent>
<aftsection>
<nextsent>the work using parallel corpora such as europarl (koehn, 2005; armstrong et al, 1998) or jrc acquis (steinberger et al, 2006) typically performs length-based sentence alignment of the trans-lated texts, and then tries to conduct word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible.
</nextsent>
<nextsent>this approach works very well and can easily be put into prac-tice using number of freely available open source tools such as moses (koehn et al, 2007) <papid> P07-2045 </papid>and giza++ (och &amp; ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>however, parallel texts are scarce resource for many language pairs (rapp &amp; martn vide, 2007), which is why methods based on compa-rable corpora have come into focus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3435">
<title id=" W10-4005.xml">the noisier the better identifying multilingual word translations using a single monolingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>established methods for the identification of word translations are based on parallel (brown et al, 1990) <papid> J90-2002 </papid>or comparable corpora (fung &amp; mckeown, 1997; <papid> W97-0119 </papid>fung &amp; yee, 1998; <papid> P98-1069 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp 1999; <papid> P99-1067 </papid>chiao et al, 2004).</prevsent>
<prevsent>the work using parallel corpora such as europarl (koehn, 2005; armstrong et al, 1998) or jrc acquis (steinberger et al, 2006) typically performs length-based sentence alignment of the trans-lated texts, and then tries to conduct word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible.</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
this approach works very well and can easily be put into prac-tice using number of freely available open source tools such as moses (koehn et al, 2007) <papid> P07-2045 </papid>and giza++ (och &amp; ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>however, parallel texts are scarce resource for many language pairs (rapp &amp; martn vide, 2007), which is why methods based on compa-rable corpora have come into focus.
</nextsent>
<nextsent>one ap-proach is to extract parallel sentences from comparable corpora (munteanu &amp; marcu, 2005; <papid> J05-4003 </papid>wu &amp; fung, 2005).<papid> I05-1023 </papid></nextsent>
<nextsent>another approach relates co-occurrence patterns between languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3436">
<title id=" W10-4005.xml">the noisier the better identifying multilingual word translations using a single monolingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>established methods for the identification of word translations are based on parallel (brown et al, 1990) <papid> J90-2002 </papid>or comparable corpora (fung &amp; mckeown, 1997; <papid> W97-0119 </papid>fung &amp; yee, 1998; <papid> P98-1069 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp 1999; <papid> P99-1067 </papid>chiao et al, 2004).</prevsent>
<prevsent>the work using parallel corpora such as europarl (koehn, 2005; armstrong et al, 1998) or jrc acquis (steinberger et al, 2006) typically performs length-based sentence alignment of the trans-lated texts, and then tries to conduct word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
this approach works very well and can easily be put into prac-tice using number of freely available open source tools such as moses (koehn et al, 2007) <papid> P07-2045 </papid>and giza++ (och &amp; ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>however, parallel texts are scarce resource for many language pairs (rapp &amp; martn vide, 2007), which is why methods based on compa-rable corpora have come into focus.
</nextsent>
<nextsent>one ap-proach is to extract parallel sentences from comparable corpora (munteanu &amp; marcu, 2005; <papid> J05-4003 </papid>wu &amp; fung, 2005).<papid> I05-1023 </papid></nextsent>
<nextsent>another approach relates co-occurrence patterns between languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3437">
<title id=" W10-4005.xml">the noisier the better identifying multilingual word translations using a single monolingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this approach works very well and can easily be put into prac-tice using number of freely available open source tools such as moses (koehn et al, 2007) <papid> P07-2045 </papid>and giza++ (och &amp; ney, 2003).<papid> J03-1002 </papid></prevsent>
<prevsent>however, parallel texts are scarce resource for many language pairs (rapp &amp; martn vide, 2007), which is why methods based on compa-rable corpora have come into focus.</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
one ap-proach is to extract parallel sentences from comparable corpora (munteanu &amp; marcu, 2005; <papid> J05-4003 </papid>wu &amp; fung, 2005).<papid> I05-1023 </papid></citsent>
<aftsection>
<nextsent>another approach relates co-occurrence patterns between languages.
</nextsent>
<nextsent>hereby the underlying assumption is that across languages there is correlation between the co-occurrences of words which are translations of each other.
</nextsent>
<nextsent>if, for example, in text of one lan-guage two words and co-occur more often than expected by chance, then in text of an-other language those words which are the trans-lations of and should also co-occur more frequently than expected.
</nextsent>
<nextsent>however, to exploit this observation some bridge needs to be built between the two lan-guages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3438">
<title id=" W10-4005.xml">the noisier the better identifying multilingual word translations using a single monolingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this approach works very well and can easily be put into prac-tice using number of freely available open source tools such as moses (koehn et al, 2007) <papid> P07-2045 </papid>and giza++ (och &amp; ney, 2003).<papid> J03-1002 </papid></prevsent>
<prevsent>however, parallel texts are scarce resource for many language pairs (rapp &amp; martn vide, 2007), which is why methods based on compa-rable corpora have come into focus.</prevsent>
</prevsection>
<citsent citstr=" I05-1023 ">
one ap-proach is to extract parallel sentences from comparable corpora (munteanu &amp; marcu, 2005; <papid> J05-4003 </papid>wu &amp; fung, 2005).<papid> I05-1023 </papid></citsent>
<aftsection>
<nextsent>another approach relates co-occurrence patterns between languages.
</nextsent>
<nextsent>hereby the underlying assumption is that across languages there is correlation between the co-occurrences of words which are translations of each other.
</nextsent>
<nextsent>if, for example, in text of one lan-guage two words and co-occur more often than expected by chance, then in text of an-other language those words which are the trans-lations of and should also co-occur more frequently than expected.
</nextsent>
<nextsent>however, to exploit this observation some bridge needs to be built between the two lan-guages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3439">
<title id=" W10-4005.xml">the noisier the better identifying multilingual word translations using a single monolingual corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it turns out that the most similar words between the two corpora effectively identify the translations of words.
</prevsent>
<prevsent>this approach based on comparable corpora considerably relieves the data acquisition bot-tleneck, but has the disadvantage that the results tend to lack accuracy in practice.
</prevsent>
</prevsection>
<citsent citstr=" W02-0902 ">
as an alternative, there is also the approach of identifying ortho graphically similar words (koehn &amp; knight, 2002) <papid> W02-0902 </papid>which has the advan-tage that it does not even require corpus.</citsent>
<aftsection>
<nextsent>a simple word list will suffice.
</nextsent>
<nextsent>however, this ap-proach works only for closely related languages, and has limited potential otherwise.
</nextsent>
<nextsent>we propose here to generate dictionaries on the basis of foreign word occurrences in texts.
</nextsent>
<nextsent>as far as we know, this is method which has not been tried before.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3440">
<title id=" W10-4005.xml">the noisier the better identifying multilingual word translations using a single monolingual corpus </title>
<section> approach and language resources.  </section>
<citcontext>
<prevsection>
<prevsent>associations between words can be com-puted in straightforward manner by counting word co-occurrences followed by the applica-tion of an association measure on the co-occurrence counts.
</prevsent>
<prevsent>co-occurrence counts are based on text window comprising the 20 words on either side of given foreign word.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
on the resulting counts we apply the log-likelihood ratio (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>as explained by dunning, this measure has the advantage to be applicable also on low counts, which is an important characteristic in our setting where the problem of data sparseness is particularly se-vere.
</nextsent>
<nextsent>this is also the reason why we chose window size somewhat larger than the ones used in most other studies.
</nextsent>
<nextsent>despite its simplicity this procedure of com-puting associations to foreign words is well suited for identifying word translations.
</nextsent>
<nextsent>as mentioned above, we assume that the strongest association to foreign word is its best transla-tion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3442">
<title id=" W11-1001.xml">automatic projection of semantic structures an application to pairwise translation ranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>preliminary experiments in pairwise hypothesis ranking on five confidence estimation benchmarks show that the model has the potential to capture salient aspects of translation quality.
</prevsent>
<prevsent>the ability to automatically assess the quality of translation hypotheses is key requirement towards the development of accurate and depend able translation models.
</prevsent>
</prevsection>
<citsent citstr=" W10-3807 ">
while it is largely agreed that proper transfer of predicate-argument structures from source to target is very strong indicator of translation quality, especially in relation to adequacy (lo and wu, 2010<papid> W10-3807 </papid>a; 2010b), the incorporation of this kind of information in the statistical machine translation (smt) evaluation pipeline is still limited to few and isolated cases, e.g., (gimenez and ma`rquez, 2010).</citsent>
<aftsection>
<nextsent>in this paper, we propose general model forthe incorporation of predicate-level semantic annotations in the framework of confidence estimation (ce) for machine translation, with specific focus on the sub-problem of pairwise hypothesis ranking.the model is based on the following underlying as sumption: by observing how automatic alignments project semantic annotations from source to target in parallel corpus, it is possible to isolate features that are characteristic of good translations, such as movements of specific arguments for some classes of predicates.
</nextsent>
<nextsent>the presence (or absence) of these features in automatic translations can then be used as an indicator of their quality.
</nextsent>
<nextsent>it is important to stress that we are not claiming that the projections preserve the meaning of the original annotation.
</nextsent>
<nextsent>still, it should be possible to observe regularities that can be helpful to rank alternative translation hypotheses.the general workflow (which can easily be extended to cope with different annotation layers, such as sequences of meaningful phrase boundaries, named entities or sequences of chunks or pos tags) is exemplified in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3446">
<title id=" W11-1001.xml">automatic projection of semantic structures an application to pairwise translation ranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>within this framework, in this paper we describe our attempt to bridge semantic role labeling (srl) and ce by modeling proposition-level semantics for pairwise translation ranking.
</prevsent>
<prevsent>the extent to which this kind of annotations are transferred from source to target has indeed very high correlation with respect to human quality assessments (lo and wu, 2010<papid> W10-3807 </papid>a; 2010b).</prevsent>
</prevsection>
<citsent citstr=" C04-1046 ">
the measure that we propose is thenan ideal addition to already established ce measures, e.g., (specia et al , 2009; blatz et al , 2004), <papid> C04-1046 </papid>as it attempts to explicitly model the adequacy of translation hypotheses as function of predicate argument structure coverage.</citsent>
<aftsection>
<nextsent>while we are aware of the fact that the current definition of the model can be improved in many different ways, our preliminary investigation, on five english to spanish translation benchmarks, shows promising accuracy on the difficult task of pairwise translation ranking, even for translations with very few distinguishing features.
</nextsent>
<nextsent>to capture different aspects of the projection of srl annotations we employ two instances of the abstract architecture shown in figure 1.
</nextsent>
<nextsent>the first works at the proposition level, and models the correct movement of arguments from source to target.
</nextsent>
<nextsent>the second works at the argument level, and models the fluency and adequacy of individual arguments within each predicate-argument structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3448">
<title id=" W11-1001.xml">automatic projection of semantic structures an application to pairwise translation ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>translations.
</prevsent>
<prevsent>even though the authors show that small set of shallow features and some supervision can produce good results on specific benchmark, we are convinced that more linguistic features are needed for these methods to perform better across wider spectrum of domains and applications.
</prevsent>
</prevsection>
<citsent citstr=" N09-2004 ">
concerning the usage of srl for smt, wu and fung (2009) <papid> N09-2004 </papid>reported first successful application of semantic role labels to improve translation quality.</citsent>
<aftsection>
<nextsent>they note that improvements in translation quality are not reflected by traditional mt evaluation metrics (doddington, 2002; papineni et al , 2002) <papid> P02-1040 </papid>based on n-gram overlaps.</nextsent>
<nextsent>to further investigate the topic,lo and wu (2010<papid> W10-3807 </papid>a), lo and wu (2010<papid> W10-3807 </papid>b) involved human annotators to demonstrate that the quality of semantic role projection on translated sentences is very highly correlated with human assessments.gimenez and ma`rquez (2010) describe framework for mt evaluation and meta-evaluation combining rich set of n-gram-based and linguistic metrics, including several variants of metric based onsrl.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3449">
<title id=" W11-1001.xml">automatic projection of semantic structures an application to pairwise translation ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>even though the authors show that small set of shallow features and some supervision can produce good results on specific benchmark, we are convinced that more linguistic features are needed for these methods to perform better across wider spectrum of domains and applications.
</prevsent>
<prevsent>concerning the usage of srl for smt, wu and fung (2009) <papid> N09-2004 </papid>reported first successful application of semantic role labels to improve translation quality.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
they note that improvements in translation quality are not reflected by traditional mt evaluation metrics (doddington, 2002; papineni et al , 2002) <papid> P02-1040 </papid>based on n-gram overlaps.</citsent>
<aftsection>
<nextsent>to further investigate the topic,lo and wu (2010<papid> W10-3807 </papid>a), lo and wu (2010<papid> W10-3807 </papid>b) involved human annotators to demonstrate that the quality of semantic role projection on translated sentences is very highly correlated with human assessments.gimenez and ma`rquez (2010) describe framework for mt evaluation and meta-evaluation combining rich set of n-gram-based and linguistic metrics, including several variants of metric based onsrl.</nextsent>
<nextsent>automatic and reference translations are annotated independently, and the lexical overlap between corresponding arguments is employed as an indicator of translation quality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3454">
<title id=" W11-1001.xml">automatic projection of semantic structures an application to pairwise translation ranking </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we outline general framework for the inclusion of linguistic processors in ce that has the advantage of requiring resources and software tools only on the source side of the translation, where well-formed input can reasonably be expected.
</prevsent>
<prevsent>the task of semantic role labeling (srl) consist sin recognizing and automatically annotating semantic relations between predicate word (not necessarily verb) and its arguments in natural language texts.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
the resulting predicate-argument structures are commonly referred to as propositions, even though we will also use the more general term anno tations.in propbank (palmer et al , 2005) <papid> J05-1004 </papid>style annotations, which our model is based on, predicates are generally verbs and roles are divided into two classes: core roles (labeled a0, a1, . . .</citsent>
<aftsection>
<nextsent>a5), whose semantic value is defined by the predicate syntactic frame, and adjunct roles (labeled am-*, e.g., amtmp or am-loc) 1 which are closed set of verb independent semantic labels accounting for predicate aspects such as temporal, locative, manner orpurpose.
</nextsent>
<nextsent>for instance, in the sentence the commission met to discuss the problem?
</nextsent>
<nextsent>we can identify two predicates, met and discuss.
</nextsent>
<nextsent>the corresponding annotations are ?[a0 the commission] [pred met] [am-prp to discuss the problem]?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3455">
<title id=" W11-1001.xml">automatic projection of semantic structures an application to pairwise translation ranking </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, here we have an absolute measure of translation accuracy, as opposed to relative rankings; 2)each system involved in the evaluation has very peculiar characteristics, hence they are very likely to generate quite different translations for the same in put sentences.
</prevsent>
<prevsent>4.2 setup.
</prevsent>
</prevsection>
<citsent citstr=" W05-0635 ">
our model consists of four main components: an automatic semantic role labeler (to annotate sourcesentences); lexical translation model (to generate the alignments required to map the annotations onto translation hypothesis); translation model for predicate-argument structures, to assign score to projected annotations; and translation model for role fillers, to assign score to the projection of each argument.to automatically label our training data with semantic roles we used the swirl system2 (surdeanuand turmo, 2005) <papid> W05-0635 </papid>with the bundled english models for syntactic and semantic parsing.</citsent>
<aftsection>
<nextsent>on the conll-2005 benchmark (carreras and ma`rquez, 2005), swirl sports an f1-measure of 76.46.
</nextsent>
<nextsent>this figure drops to 75 for mixed data, and to 65.42 onout-of-domain data, which we can regard as conservative estimate of the accuracy of the labeler on wmt benchmarks.
</nextsent>
<nextsent>for all the translation tasks we employed the moses phrase-based decoder3 in single-factor configuration.
</nextsent>
<nextsent>the -constraint command line parameter is used to force moses to output the desiredtranslation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3456">
<title id=" W11-1205.xml">bilingual lexicon extraction from comparable corpora enhanced with parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bilingual lexicons are important resources of many applications of natural language processing such as cross-language information retrieval or machinetranslation.
</prevsent>
<prevsent>these lexicons are traditionally extracted from bilingual corpora.in this area, the main work involves parallel corpora, i.e. corpus that contains source texts and theirtranslations.
</prevsent>
</prevsection>
<citsent citstr=" W02-1402 ">
from sentence-to-sentence aligned corpora, symbolic (carl and langlais, 2002), <papid> W02-1402 </papid>statistical (daille et al, 1994), <papid> C94-1084 </papid>or hybrid techniques (gaussier and lange?, 1995) are used for word and expression alignments.</citsent>
<aftsection>
<nextsent>however, despite good results in the compilation of bilingual lexicons, parallel corpora are rather scarce resources, especially for technical domains and for language pairs not involving english.
</nextsent>
<nextsent>for instance, current resources of parallel corpora are built from the proceedings of international institutions such as the european union (11 languages) or the united nations (6 languages), bilingual countries such as canada (english and french languages), or bilingual regions such as hongkong (chinese and english languages).
</nextsent>
<nextsent>for these reasons, research in bilingual lexicon extraction is focused on another kind of bilingualcorpora.
</nextsent>
<nextsent>these corpora, known as comparable corpora, are comprised of texts sharing common features such as domain, genre, register, sampling period, etc. without having source text-target text relationship.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3457">
<title id=" W11-1205.xml">bilingual lexicon extraction from comparable corpora enhanced with parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bilingual lexicons are important resources of many applications of natural language processing such as cross-language information retrieval or machinetranslation.
</prevsent>
<prevsent>these lexicons are traditionally extracted from bilingual corpora.in this area, the main work involves parallel corpora, i.e. corpus that contains source texts and theirtranslations.
</prevsent>
</prevsection>
<citsent citstr=" C94-1084 ">
from sentence-to-sentence aligned corpora, symbolic (carl and langlais, 2002), <papid> W02-1402 </papid>statistical (daille et al, 1994), <papid> C94-1084 </papid>or hybrid techniques (gaussier and lange?, 1995) are used for word and expression alignments.</citsent>
<aftsection>
<nextsent>however, despite good results in the compilation of bilingual lexicons, parallel corpora are rather scarce resources, especially for technical domains and for language pairs not involving english.
</nextsent>
<nextsent>for instance, current resources of parallel corpora are built from the proceedings of international institutions such as the european union (11 languages) or the united nations (6 languages), bilingual countries such as canada (english and french languages), or bilingual regions such as hongkong (chinese and english languages).
</nextsent>
<nextsent>for these reasons, research in bilingual lexicon extraction is focused on another kind of bilingualcorpora.
</nextsent>
<nextsent>these corpora, known as comparable corpora, are comprised of texts sharing common features such as domain, genre, register, sampling period, etc. without having source text-target text relationship.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3458">
<title id=" W11-1205.xml">bilingual lexicon extraction from comparable corpora enhanced with parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these corpora, known as comparable corpora, are comprised of texts sharing common features such as domain, genre, register, sampling period, etc. without having source text-target text relationship.
</prevsent>
<prevsent>although the building of comparable corpora is easier than the building of parallel corpora, the results obtained thus far on comparable corpora are contrasted.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
for instance, good results are obtained from large corpora ? several million words ? for which the accuracy of the proposed translation is between 76% (fung, 1998) and 89% (rapp, 1999) <papid> P99-1067 </papid>for the first 20 candidates.</citsent>
<aftsection>
<nextsent>(cao and li, 2002) <papid> C02-1011 </papid>have achieved 91% accuracy for the top three candidates using the web as comparable cor pus.</nextsent>
<nextsent>but for technical domains, for which large corpora are not available, the results obtained, even though encouraging, are not completely satisfactory yet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3459">
<title id=" W11-1205.xml">bilingual lexicon extraction from comparable corpora enhanced with parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although the building of comparable corpora is easier than the building of parallel corpora, the results obtained thus far on comparable corpora are contrasted.
</prevsent>
<prevsent>for instance, good results are obtained from large corpora ? several million words ? for which the accuracy of the proposed translation is between 76% (fung, 1998) and 89% (rapp, 1999) <papid> P99-1067 </papid>for the first 20 candidates.</prevsent>
</prevsection>
<citsent citstr=" C02-1011 ">
(cao and li, 2002) <papid> C02-1011 </papid>have achieved 91% accuracy for the top three candidates using the web as comparable cor pus.</citsent>
<aftsection>
<nextsent>but for technical domains, for which large corpora are not available, the results obtained, even though encouraging, are not completely satisfactory yet.
</nextsent>
<nextsent>for instance, (dejean et al, 2002) <papid> C02-1166 </papid>obtained precision of 44% and 57% for the first 10 and 20 candidates in 100,000-word medical corpus, and 35% and 42% in multi-domain 8 million-word corpus.</nextsent>
<nextsent>for french/english single words, (chiao and zweigenbaum, 2002) <papid> C02-2020 </papid>using medical corpus of 1.2 million words, obtained precision of about 50% and 60% for the top 10 and top 20 candidates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3460">
<title id=" W11-1205.xml">bilingual lexicon extraction from comparable corpora enhanced with parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(cao and li, 2002) <papid> C02-1011 </papid>have achieved 91% accuracy for the top three candidates using the web as comparable cor pus.</prevsent>
<prevsent>but for technical domains, for which large corpora are not available, the results obtained, even though encouraging, are not completely satisfactory yet.</prevsent>
</prevsection>
<citsent citstr=" C02-1166 ">
for instance, (dejean et al, 2002) <papid> C02-1166 </papid>obtained precision of 44% and 57% for the first 10 and 20 candidates in 100,000-word medical corpus, and 35% and 42% in multi-domain 8 million-word corpus.</citsent>
<aftsection>
<nextsent>for french/english single words, (chiao and zweigenbaum, 2002) <papid> C02-2020 </papid>using medical corpus of 1.2 million words, obtained precision of about 50% and 60% for the top 10 and top 20 candidates.</nextsent>
<nextsent>(morin et al, 2007) <papid> P07-1084 </papid>obtained precision of 51% and 60% for the top 10 and 20 candidates in 1.5 27 proceedings of the 4th workshop on building and using comparable corpora, pages 2734, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3461">
<title id=" W11-1205.xml">bilingual lexicon extraction from comparable corpora enhanced with parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but for technical domains, for which large corpora are not available, the results obtained, even though encouraging, are not completely satisfactory yet.
</prevsent>
<prevsent>for instance, (dejean et al, 2002) <papid> C02-1166 </papid>obtained precision of 44% and 57% for the first 10 and 20 candidates in 100,000-word medical corpus, and 35% and 42% in multi-domain 8 million-word corpus.</prevsent>
</prevsection>
<citsent citstr=" C02-2020 ">
for french/english single words, (chiao and zweigenbaum, 2002) <papid> C02-2020 </papid>using medical corpus of 1.2 million words, obtained precision of about 50% and 60% for the top 10 and top 20 candidates.</citsent>
<aftsection>
<nextsent>(morin et al, 2007) <papid> P07-1084 </papid>obtained precision of 51% and 60% for the top 10 and 20 candidates in 1.5 27 proceedings of the 4th workshop on building and using comparable corpora, pages 2734, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.</nextsent>
<nextsent>c2011 association for computational linguistics million-word french-japanese diabetes corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3462">
<title id=" W11-1205.xml">bilingual lexicon extraction from comparable corpora enhanced with parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, (dejean et al, 2002) <papid> C02-1166 </papid>obtained precision of 44% and 57% for the first 10 and 20 candidates in 100,000-word medical corpus, and 35% and 42% in multi-domain 8 million-word corpus.</prevsent>
<prevsent>for french/english single words, (chiao and zweigenbaum, 2002) <papid> C02-2020 </papid>using medical corpus of 1.2 million words, obtained precision of about 50% and 60% for the top 10 and top 20 candidates.</prevsent>
</prevsection>
<citsent citstr=" P07-1084 ">
(morin et al, 2007) <papid> P07-1084 </papid>obtained precision of 51% and 60% for the top 10 and 20 candidates in 1.5 27 proceedings of the 4th workshop on building and using comparable corpora, pages 2734, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.</citsent>
<aftsection>
<nextsent>c2011 association for computational linguistics million-word french-japanese diabetes corpus.
</nextsent>
<nextsent>the above work in bilingual lexicon extraction from comparable corpora relies on the assumption that words which have the same meaning in different languages tend to appear in the same lexical contexts(fung, 1998; rapp, 1999).<papid> P99-1067 </papid></nextsent>
<nextsent>based on this assumption, standard approach consists of building context vectors for each word of the source and targetlanguages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3465">
<title id=" W11-1205.xml">bilingual lexicon extraction from comparable corpora enhanced with parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this approach, the translation of the words of the source context vectors depends on the coverage of the bilingual dictionary vis-a`-vis the corpus.
</prevsent>
<prevsent>this aspect can be potential problem if too few corpus words are found in the bilingual dictionary (chiao and zweigenbaum, 2003; dejean et al, 2002).<papid> C02-1166 </papid>in this article, we want to show how this problem can be partially circumvented by combining agen eral bilingual dictionary with specialized bilingual dictionary based on parallel corpus extracted through mining of the comparable corpus.</prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
in the same way that recent works in statistical machine translation (smt) mines comparable corpora to discover parallel sentences (resnik and smith, 2003; <papid> J03-3002 </papid>yang and li, 2003; munteanu and marcu, 2005; <papid> J05-4003 </papid>abdul-rauf and schwenk, 2009, among others),this work contributes to the bridging of the gap between comparable and parallel corpora by offering framework for bilingual lexicon extraction from comparable corpus with the help of parallel corpus based pairs of terms.the remainder of this article is organized as fol lows.</citsent>
<aftsection>
<nextsent>in section 2, we first present the method for bilingual lexicon extraction from comparable corpora enhanced with parallel corpora and the associated system architecture.
</nextsent>
<nextsent>we then quantify and analyse in section 3 the performance improvement of our method on medical comparable corpora when used to extract specialized bilingual lexicon.
</nextsent>
<nextsent>finally, in section 4, we discuss the present study and present our conclusions.
</nextsent>
<nextsent>the overall architecture of the system for lexical alignment is shown in figure 1 and comprises parallel corpus- and comparable corpus-based alignments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3467">
<title id=" W11-1205.xml">bilingual lexicon extraction from comparable corpora enhanced with parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this approach, the translation of the words of the source context vectors depends on the coverage of the bilingual dictionary vis-a`-vis the corpus.
</prevsent>
<prevsent>this aspect can be potential problem if too few corpus words are found in the bilingual dictionary (chiao and zweigenbaum, 2003; dejean et al, 2002).<papid> C02-1166 </papid>in this article, we want to show how this problem can be partially circumvented by combining agen eral bilingual dictionary with specialized bilingual dictionary based on parallel corpus extracted through mining of the comparable corpus.</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
in the same way that recent works in statistical machine translation (smt) mines comparable corpora to discover parallel sentences (resnik and smith, 2003; <papid> J03-3002 </papid>yang and li, 2003; munteanu and marcu, 2005; <papid> J05-4003 </papid>abdul-rauf and schwenk, 2009, among others),this work contributes to the bridging of the gap between comparable and parallel corpora by offering framework for bilingual lexicon extraction from comparable corpus with the help of parallel corpus based pairs of terms.the remainder of this article is organized as fol lows.</citsent>
<aftsection>
<nextsent>in section 2, we first present the method for bilingual lexicon extraction from comparable corpora enhanced with parallel corpora and the associated system architecture.
</nextsent>
<nextsent>we then quantify and analyse in section 3 the performance improvement of our method on medical comparable corpora when used to extract specialized bilingual lexicon.
</nextsent>
<nextsent>finally, in section 4, we discuss the present study and present our conclusions.
</nextsent>
<nextsent>the overall architecture of the system for lexical alignment is shown in figure 1 and comprises parallel corpus- and comparable corpus-based alignments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3471">
<title id=" W11-1205.xml">bilingual lexicon extraction from comparable corpora enhanced with parallel corpora </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>for word to be translated, the output of the system is ranked list of candidate translations.
</prevsent>
<prevsent>2.1 extracting parallel sentences from.
</prevsent>
</prevsection>
<citsent citstr=" W04-3208 ">
comparable corpora parallel sentence extraction from comparable corpora has been studied by number of researchers (ma and liberman, 1999; chen and nie, 2000; resnik and smith, 2003; <papid> J03-3002 </papid>yang and li, 2003; fung and cheung, 2004; <papid> W04-3208 </papid>munteanu and marcu, 2005; <papid> J05-4003 </papid>abdul-rauf and schwenk, 2009, among others) and several systems have been developed such as bits (bilingual internet test search) (ma and liberman, 1999), ptminer (parallel text miner) (chen and nie, 2000), and strand (structural translation recognition for acquiring natural data) (resnikand smith, 2003).<papid> J03-3002 </papid></citsent>
<aftsection>
<nextsent>their work relies on the observation that collection of texts in different languages composed independently and based on sharing common features such as content, domain, genre, register, sampling period, etc. contains probably some sentences with source text-target text relationship.
</nextsent>
<nextsent>based on this observation, dynamic programming (yang and li, 2003), similarity measures such as cosine (fung and cheung, 2004) <papid> W04-3208 </papid>or word and translation error ratios (abdul-rauf and schwenk, 2009), or maximum entropy classifier (munteanu and marcu, 2005) <papid> J05-4003 </papid>are used for discovering parallel sentences.</nextsent>
<nextsent>although our purpose is similar to these works, the amount of data required by these techniques makes them ineffective when applied to specialized comparable corpora used to discover parallel sen tences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3479">
<title id=" W11-1205.xml">bilingual lexicon extraction from comparable corpora enhanced with parallel corpora </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>t assockt 2 (3) wjvkvl = ? min(assoclt, assockt )?
</prevsent>
<prevsent>t max(assoclt, assockt ) (4)this approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures.
</prevsent>
</prevsection>
<citsent citstr=" C10-1070 ">
the most complete study about the influence of these parameters on the quality of bilingual alignment has been carried out by laroche and langlais (2010).<papid> C10-1070 </papid></citsent>
<aftsection>
<nextsent>in the previous section, we have introduced our comparable corpus and described the method dedicated to bilingual lexicon extraction from comparable corpora enhanced with parallel corpora.
</nextsent>
<nextsent>in this section, we then quantify and analyse the performance improvement of our method on medical comparable corpus when used to extract specialized bilingual lexicon.
</nextsent>
<nextsent>3.1 experimental test bed.
</nextsent>
<nextsent>the documents comprising the french/english specialized comparable corpus have been normalised through the following linguistic pre-processingsteps: token isation, part-of-speech tagging, and lem matisation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3481">
<title id=" W10-4206.xml">textual properties and task based evaluation investigating the role of surface properties structure and content </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the core argument of this paper is that more domain knowledge-based metrics shed more light on the relationship between deep semantic properties of text and task performance.
</prevsent>
<prevsent>evaluation methodology in nlg has generated lot of interest.
</prevsent>
</prevsection>
<citsent citstr=" J09-4008 ">
some recent work suggested thatthe relationship between various intrinsic and extrinsic evaluation methods (sparck-jones and gal liers, 1996) is not straightforward (reiter and belz, 2009; <papid> J09-4008 </papid>gatt and belz, to appear), leading tosome arguments for more domain-specific intrinsic metrics (foster, 2008).<papid> W08-1113 </papid></citsent>
<aftsection>
<nextsent>one reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform systemdevelopment.
</nextsent>
<nextsent>indeed, this is often the stated purpose of evaluation metrics such as bleu (papineni et al, 2002) <papid> P02-1040 </papid>and rouge (lin and hovy, 2003), <papid> N03-1020 </papid>which were originally characterised as evaluation understudies?.</nextsent>
<nextsent>in this paper we take up these questions in the context of knowledge-based nlg system, bt-45 (portet et al, 2009), which summarises medical data for decision support purposes in neonatal intensive care unit (nicu).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3482">
<title id=" W10-4206.xml">textual properties and task based evaluation investigating the role of surface properties structure and content </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the core argument of this paper is that more domain knowledge-based metrics shed more light on the relationship between deep semantic properties of text and task performance.
</prevsent>
<prevsent>evaluation methodology in nlg has generated lot of interest.
</prevsent>
</prevsection>
<citsent citstr=" W08-1113 ">
some recent work suggested thatthe relationship between various intrinsic and extrinsic evaluation methods (sparck-jones and gal liers, 1996) is not straightforward (reiter and belz, 2009; <papid> J09-4008 </papid>gatt and belz, to appear), leading tosome arguments for more domain-specific intrinsic metrics (foster, 2008).<papid> W08-1113 </papid></citsent>
<aftsection>
<nextsent>one reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform systemdevelopment.
</nextsent>
<nextsent>indeed, this is often the stated purpose of evaluation metrics such as bleu (papineni et al, 2002) <papid> P02-1040 </papid>and rouge (lin and hovy, 2003), <papid> N03-1020 </papid>which were originally characterised as evaluation understudies?.</nextsent>
<nextsent>in this paper we take up these questions in the context of knowledge-based nlg system, bt-45 (portet et al, 2009), which summarises medical data for decision support purposes in neonatal intensive care unit (nicu).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3483">
<title id=" W10-4206.xml">textual properties and task based evaluation investigating the role of surface properties structure and content </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some recent work suggested thatthe relationship between various intrinsic and extrinsic evaluation methods (sparck-jones and gal liers, 1996) is not straightforward (reiter and belz, 2009; <papid> J09-4008 </papid>gatt and belz, to appear), leading tosome arguments for more domain-specific intrinsic metrics (foster, 2008).<papid> W08-1113 </papid></prevsent>
<prevsent>one reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform systemdevelopment.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
indeed, this is often the stated purpose of evaluation metrics such as bleu (papineni et al, 2002) <papid> P02-1040 </papid>and rouge (lin and hovy, 2003), <papid> N03-1020 </papid>which were originally characterised as evaluation understudies?.</citsent>
<aftsection>
<nextsent>in this paper we take up these questions in the context of knowledge-based nlg system, bt-45 (portet et al, 2009), which summarises medical data for decision support purposes in neonatal intensive care unit (nicu).
</nextsent>
<nextsent>our extrinsic data comes from an experiment involving complex medical decisionmaking based on automatically generated and human-authored texts (van der meulen et al, 2009).
</nextsent>
<nextsent>this gives us the opportunity to directly compare the textual characteristics of generated and human-written summaries and their relationship to decision-making performance.
</nextsent>
<nextsent>the present work uses data from an earlier study (gatt and portet, 2009), which presented some preliminary results along these lines for the system in question.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3484">
<title id=" W10-4206.xml">textual properties and task based evaluation investigating the role of surface properties structure and content </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some recent work suggested thatthe relationship between various intrinsic and extrinsic evaluation methods (sparck-jones and gal liers, 1996) is not straightforward (reiter and belz, 2009; <papid> J09-4008 </papid>gatt and belz, to appear), leading tosome arguments for more domain-specific intrinsic metrics (foster, 2008).<papid> W08-1113 </papid></prevsent>
<prevsent>one reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform systemdevelopment.</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
indeed, this is often the stated purpose of evaluation metrics such as bleu (papineni et al, 2002) <papid> P02-1040 </papid>and rouge (lin and hovy, 2003), <papid> N03-1020 </papid>which were originally characterised as evaluation understudies?.</citsent>
<aftsection>
<nextsent>in this paper we take up these questions in the context of knowledge-based nlg system, bt-45 (portet et al, 2009), which summarises medical data for decision support purposes in neonatal intensive care unit (nicu).
</nextsent>
<nextsent>our extrinsic data comes from an experiment involving complex medical decisionmaking based on automatically generated and human-authored texts (van der meulen et al, 2009).
</nextsent>
<nextsent>this gives us the opportunity to directly compare the textual characteristics of generated and human-written summaries and their relationship to decision-making performance.
</nextsent>
<nextsent>the present work uses data from an earlier study (gatt and portet, 2009), which presented some preliminary results along these lines for the system in question.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3485">
<title id=" W10-4206.xml">textual properties and task based evaluation investigating the role of surface properties structure and content </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in nlg evaluation, extrinsic, task-based method splay significant role (reiter et al, 2003; karas imos and isard, 2004; stock et al, 2007).
</prevsent>
<prevsent>depending on the study design, these studies often leave open the question of precisely which aspects of system (and of the text it generates) contribute to success or failure.
</prevsent>
</prevsection>
<citsent citstr=" J97-1004 ">
intrinsic nlg evaluations often involve ratings of text quality or responses to questionnaires (lester and porter, 1997; <papid> J97-1004 </papid>callaway and lester, 2002; foster, 2008),<papid> W08-1113 </papid>with some studies using post-editing by human experts (reiter et al, 2005).</citsent>
<aftsection>
<nextsent>automatically computed metrics exploiting corpora, such as bleu,nist and rouge, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (langkilde-geary, 2002; callaway, 2003), though they have recently also been usedfor subtasks such as referring expression generation (gatt and belz, to appear) as well as end-to end weather forecasting systems (reiter and belz, 2009).<papid> J09-4008 </papid></nextsent>
<nextsent>the widespread use of these metrics in nlp partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in mt (calliston-burch et al, 2006) and summarisation (dorr et al, 2005; <papid> W05-0901 </papid>liu and liu, 2008).<papid> P08-2051 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3488">
<title id=" W10-4206.xml">textual properties and task based evaluation investigating the role of surface properties structure and content </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>intrinsic nlg evaluations often involve ratings of text quality or responses to questionnaires (lester and porter, 1997; <papid> J97-1004 </papid>callaway and lester, 2002; foster, 2008),<papid> W08-1113 </papid>with some studies using post-editing by human experts (reiter et al, 2005).</prevsent>
<prevsent>automatically computed metrics exploiting corpora, such as bleu,nist and rouge, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (langkilde-geary, 2002; callaway, 2003), though they have recently also been usedfor subtasks such as referring expression generation (gatt and belz, to appear) as well as end-to end weather forecasting systems (reiter and belz, 2009).<papid> J09-4008 </papid></prevsent>
</prevsection>
<citsent citstr=" W05-0901 ">
the widespread use of these metrics in nlp partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in mt (calliston-burch et al, 2006) and summarisation (dorr et al, 2005; <papid> W05-0901 </papid>liu and liu, 2008).<papid> P08-2051 </papid></citsent>
<aftsection>
<nextsent>as noted in section 1, similar questions have been raised in nlg.
</nextsent>
<nextsent>one of the problems associated with these metrics is that they rely onthe notion of gold standard?, which is not always precisely definable given multiple solution sto the same generation, summarisation or translation task.
</nextsent>
<nextsent>these observations underlie recent developments in summarisation evaluation such as the pyramid method (nenkova and passonneau, 2004), <papid> N04-1019 </papid>which in addition also emphasises content overlap with set of reference summaries, rather than n-gram matches.it is interesting to note that, with some exceptions (foster, 2008),<papid> W08-1113 </papid> most of the methodological studies on intrinsic evaluation cited here have focused on generic?</nextsent>
<nextsent>metrics (corpus-based automatic measures being foremost among them), none of which use domain knowledge to quantify those aspects of text related to its content.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3489">
<title id=" W10-4206.xml">textual properties and task based evaluation investigating the role of surface properties structure and content </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>intrinsic nlg evaluations often involve ratings of text quality or responses to questionnaires (lester and porter, 1997; <papid> J97-1004 </papid>callaway and lester, 2002; foster, 2008),<papid> W08-1113 </papid>with some studies using post-editing by human experts (reiter et al, 2005).</prevsent>
<prevsent>automatically computed metrics exploiting corpora, such as bleu,nist and rouge, have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (langkilde-geary, 2002; callaway, 2003), though they have recently also been usedfor subtasks such as referring expression generation (gatt and belz, to appear) as well as end-to end weather forecasting systems (reiter and belz, 2009).<papid> J09-4008 </papid></prevsent>
</prevsection>
<citsent citstr=" P08-2051 ">
the widespread use of these metrics in nlp partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in mt (calliston-burch et al, 2006) and summarisation (dorr et al, 2005; <papid> W05-0901 </papid>liu and liu, 2008).<papid> P08-2051 </papid></citsent>
<aftsection>
<nextsent>as noted in section 1, similar questions have been raised in nlg.
</nextsent>
<nextsent>one of the problems associated with these metrics is that they rely onthe notion of gold standard?, which is not always precisely definable given multiple solution sto the same generation, summarisation or translation task.
</nextsent>
<nextsent>these observations underlie recent developments in summarisation evaluation such as the pyramid method (nenkova and passonneau, 2004), <papid> N04-1019 </papid>which in addition also emphasises content overlap with set of reference summaries, rather than n-gram matches.it is interesting to note that, with some exceptions (foster, 2008),<papid> W08-1113 </papid> most of the methodological studies on intrinsic evaluation cited here have focused on generic?</nextsent>
<nextsent>metrics (corpus-based automatic measures being foremost among them), none of which use domain knowledge to quantify those aspects of text related to its content.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3490">
<title id=" W10-4206.xml">textual properties and task based evaluation investigating the role of surface properties structure and content </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as noted in section 1, similar questions have been raised in nlg.
</prevsent>
<prevsent>one of the problems associated with these metrics is that they rely onthe notion of gold standard?, which is not always precisely definable given multiple solution sto the same generation, summarisation or translation task.
</prevsent>
</prevsection>
<citsent citstr=" N04-1019 ">
these observations underlie recent developments in summarisation evaluation such as the pyramid method (nenkova and passonneau, 2004), <papid> N04-1019 </papid>which in addition also emphasises content overlap with set of reference summaries, rather than n-gram matches.it is interesting to note that, with some exceptions (foster, 2008),<papid> W08-1113 </papid> most of the methodological studies on intrinsic evaluation cited here have focused on generic?</citsent>
<aftsection>
<nextsent>metrics (corpus-based automatic measures being foremost among them), none of which use domain knowledge to quantify those aspects of text related to its content.
</nextsent>
<nextsent>there is some work in summarisation that suggests that incorporating more knowledge improves results.
</nextsent>
<nextsent>for example, yoo and song (yoo et al, 2007) used the medical subject headings (mesh) to construct graphs representing the high-level content of documents, which are then used to cluster documents by topic, each cluster being used to produce summary.
</nextsent>
<nextsent>in (plaza et al, 2009), the authors have proposed summarisation method based on wordnet concepts and showed that this higher level representation improves the summari sation task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3492">
<title id=" W10-4206.xml">textual properties and task based evaluation investigating the role of surface properties structure and content </title>
<section> the bt-45 system.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, we computed different score spap, defined as the proportion of appropriate actions selected by participant withina scenario out of the total number of actions selected (effectively measure of precision?).
</prevsent>
<prevsent>a comparison between means for these three scores obtained across scenarios showed no significant differences.in the analysis reported in section 6, we compare our textual metrics to both the global scorep as well as to these three other performance indicators.
</prevsent>
</prevsection>
<citsent citstr=" W08-1119 ">
in various follow-up analyses (van der meulen et al, 2009; reiter et al, 2008), <papid> W08-1119 </papid>it was found that the three scenarios in which the target action was no action?</citsent>
<aftsection>
<nextsent>may have misled some participants, insofar as this option was included among set of other actions, some of which were themselves deemed appropriate or at least neutral (in the sense that they could be carried out without harming the patient).
</nextsent>
<nextsent>we shall therefore exclude these scenarios from our analyses.
</nextsent>
<nextsent>  at 14:15 hours  event type= heel_prick  id= e11   heel prick is done.
</nextsent>
<nextsent> /event   event type= trend  source= hr  direction= increasing  id= e12   the hr increases  /event  at this point and for 7 minutes from the start of this procedure  event cardinality= 3  source= sao2  type= artifact  id= e13   there is lot of artefact in the oxygen saturation trace.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3496">
<title id=" W11-0219.xml">building timelines from narrative clinical records initial results basedon deep natural language understanding </title>
<section> status post cholecystectomy..  </section>
<citcontext>
<prevsection>
<prevsent>it can be also dynamically augmented for unknown words by consulting wordnet (miller, 1995).
</prevsent>
<prevsent>to support more robust processing as well as domain configurability, the core system is informed by variety of statistical and symbolic preprocessors.
</prevsent>
</prevsection>
<citsent citstr=" W00-1308 ">
these include several off-the-shelf sta tisical nlp tools such as the stanford pos tagger (toutanova and manning, 2000), <papid> W00-1308 </papid>the stanford named-entity recognizer (ner) (finkel et al, 2005) <papid> P05-1045 </papid>and the stanford parser (klein and manning, 2003).</citsent>
<aftsection>
<nextsent>the output of these and other specialized pre processors (such as street address recognizer) are sent to the parser as advice.
</nextsent>
<nextsent>the parser then can include or not include this advice (e.g., that certain phrase is named entity) as it searches for the optimal parse of the sentence.
</nextsent>
<nextsent>the result of parsing is frame-like semantic representation that we call the logical form (lf).
</nextsent>
<nextsent>the lf representation includes semantic types, semantic roles for predicate arguments, and dependency relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3497">
<title id=" W11-0219.xml">building timelines from narrative clinical records initial results basedon deep natural language understanding </title>
<section> status post cholecystectomy..  </section>
<citcontext>
<prevsection>
<prevsent>it can be also dynamically augmented for unknown words by consulting wordnet (miller, 1995).
</prevsent>
<prevsent>to support more robust processing as well as domain configurability, the core system is informed by variety of statistical and symbolic preprocessors.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
these include several off-the-shelf sta tisical nlp tools such as the stanford pos tagger (toutanova and manning, 2000), <papid> W00-1308 </papid>the stanford named-entity recognizer (ner) (finkel et al, 2005) <papid> P05-1045 </papid>and the stanford parser (klein and manning, 2003).</citsent>
<aftsection>
<nextsent>the output of these and other specialized pre processors (such as street address recognizer) are sent to the parser as advice.
</nextsent>
<nextsent>the parser then can include or not include this advice (e.g., that certain phrase is named entity) as it searches for the optimal parse of the sentence.
</nextsent>
<nextsent>the result of parsing is frame-like semantic representation that we call the logical form (lf).
</nextsent>
<nextsent>the lf representation includes semantic types, semantic roles for predicate arguments, and dependency relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3498">
<title id=" W11-0219.xml">building timelines from narrative clinical records initial results basedon deep natural language understanding </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>since so much clinical information is still residing in unstructured form, in particular as text in the patients health record, the last decade has seen number of serious efforts in medical nlp in general (meystre et al., 2008) and in extracting temporal information from clinical text in particular.
</prevsent>
<prevsent>some of this surge in interest has been spurred by dedicated competitions on extraction of concepts and events from clinical text (such as the i2b2 nlp challenges).
</prevsent>
</prevsection>
<citsent citstr=" P05-3021 ">
at the same time, the evolution of temporal markup languages such as ti meml (sauri et al, 2006), and temporal extraction/inference competitions (such as the two tempe val challenges, verhagen et al., 2009) in the general area of nlp have led to the development of tools such as tarsqi (verhagen et al., 2005) <papid> P05-3021 </papid>that could be adapted to the clinical domain.</citsent>
<aftsection>
<nextsent>although the prevailing paradigm in this area is to use superficial methods for extracting and classifying temporal expressions, it has long been recognized that higher level semantic processing, including discourse-level analysis, would have to be performed to get past the limits of the current approaches (cf.
</nextsent>
<nextsent>zhou and hripcsak, 2007).
</nextsent>
<nextsent>recent attempts to use deeper linguistic features include the work of bethard et al.
</nextsent>
<nextsent>(2007), who used syntactic structure in addition to lexical and some minor semantic features to classify temporal relations of the type we discussed in section 4.3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3499">
<title id=" W10-4011.xml">more languages more map a study of multiple assisting languages in multilingual prf </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, some resource-constrained languages do not have adequate information coverage in their own language.
</prevsent>
<prevsent>for example, languages like hungarian and finnish have meager online content in their own languages.
</prevsent>
</prevsection>
<citsent citstr=" P10-1137 ">
multilingual pseudo-relevance feedback (multiprf) (chinnakotla et al, 2010<papid> P10-1137 </papid>a) is novel framework for prf to overcome the above limitations of prf.</citsent>
<aftsection>
<nextsent>it does so by taking the help of different language called the assisting language.
</nextsent>
<nextsent>thus, the performance of resource-constrained language could be improved by harnessing the good coverage of another language.
</nextsent>
<nextsent>mulitiprf showed significant improvements on standard clef collections (braschler and peters, 2004) over state-of-art prf system.
</nextsent>
<nextsent>on the web, each language has its own exclusive topical coverage besides sharing large number of common topics with other languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3508">
<title id=" W10-4011.xml">more languages more map a study of multiple assisting languages in multilingual prf </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>we use standard evaluation measures like map, p@5 and p@10for evaluation.
</prevsent>
<prevsent>additionally, for assessing robustness, we use the geometric mean average precision (gmap) metric (robertson, 2006) which is also used in the trec robust track (voorhees, 2006).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the probabilistic bi-lingual dictionary used in multi prf was learnt automatically by running giza++: word alignment tool (och and ney, 2003) <papid> J03-1002 </papid>on parallel sentence aligned corpora.for all the above language pairs we used the eu roparl corpus (philipp, 2005).</citsent>
<aftsection>
<nextsent>we use google translate as the query translation system as it has been shown to perform well for the task (wu etal., 2008).
</nextsent>
<nextsent>we use two-stage dirichlet smoothing with the optimal parameters tuned based on the collection (zhai and lafferty, 2004).
</nextsent>
<nextsent>we tune the parameters of mbf, specifically ? and ?, and choose the values which give the optimal performance on given collection.
</nextsent>
<nextsent>we observe that the optimal parameters ? and ? are uniform across collections and vary in the range 0.4-0.48.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3509">
<title id=" W11-0802.xml">automatic extraction of nv expressions in basque basic issues on cooccurrence techniques </title>
<section> mwe: basic definition and extraction.  </section>
<citcontext>
<prevsection>
<prevsent>as preliminary step, we must face the morphosyntactic aspects of basque that might influence the efficiency of the process.
</prevsent>
<prevsent>techniques as basis for our work, we take idiomaticity asthe key feature for the definition and classification of mwe.
</prevsent>
</prevsection>
<citsent citstr=" W07-1102 ">
idiomaticity could be described as non-discrete magnitude, whose value?, according to recent investigations (baldwin and kim, 2010; fazly and stevenson, 2007; <papid> W07-1102 </papid>granger and paquot,2008), has turned to depend on complex combination of features such as institutionalization, non compositionality and lexico-syntactic fixedness.</citsent>
<aftsection>
<nextsent>the idiomaticity of mwes appears rather as acontinuum than as scale of discrete values (sinclair, 1996; wulff, 2010).
</nextsent>
<nextsent>thus, the classification of mwes into discrete categories is difficult task.
</nextsent>
<nextsent>taking cowies classification as an initial basis (cowie, 1998), our work is focused on phrase-like units, aiming, at this stage, to differentiate mwes (idioms and collocations) from free combinations.
</nextsent>
<nextsent>specifically, nv combinations with the following characteristics are considered as mwes: ? idioms: non-compositional combinations, as opaque idioms (adarra jo: to pull somebodysleg?; lit: to play the horn?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3511">
<title id=" W11-0802.xml">automatic extraction of nv expressions in basque basic issues on cooccurrence techniques </title>
<section> mwe: basic definition and extraction.  </section>
<citcontext>
<prevsection>
<prevsent>to tear the law?).
</prevsent>
<prevsent>compositional combinations with lexical restriction, in which it is not possible to substitute the verb with its synonyms, or that present clear statistical idiosyncrasy in favor of given synonym choice (elka rtasuna adierazi: to express solidarity?;konpromisoa berretsi: to confirm com mitment?).
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
among the different techniques that have been proposed to extract and characterize mwes, the cooccurrence of the components is the most used heuristic of institutionalization, and the use of association measures (am) goes back to early research on this field (church and hanks, 1990; <papid> J90-1003 </papid>smadja, 1993).<papid> J93-1007 </papid></citsent>
<aftsection>
<nextsent>in recent years, the comparative analysis of ams has aroused considerable interest, as well asthe possibility of obtaining better results by combining them (pearce, 2002; pecina, 2005).<papid> P05-2003 </papid></nextsent>
<nextsent>cooccurrence techniques are usually used in combination with linguistic techniques, which allow the use oflemmatized and pos-tagged corpora, or even syntactic dependencies (seretan, 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3512">
<title id=" W11-0802.xml">automatic extraction of nv expressions in basque basic issues on cooccurrence techniques </title>
<section> mwe: basic definition and extraction.  </section>
<citcontext>
<prevsection>
<prevsent>to tear the law?).
</prevsent>
<prevsent>compositional combinations with lexical restriction, in which it is not possible to substitute the verb with its synonyms, or that present clear statistical idiosyncrasy in favor of given synonym choice (elka rtasuna adierazi: to express solidarity?;konpromisoa berretsi: to confirm com mitment?).
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
among the different techniques that have been proposed to extract and characterize mwes, the cooccurrence of the components is the most used heuristic of institutionalization, and the use of association measures (am) goes back to early research on this field (church and hanks, 1990; <papid> J90-1003 </papid>smadja, 1993).<papid> J93-1007 </papid></citsent>
<aftsection>
<nextsent>in recent years, the comparative analysis of ams has aroused considerable interest, as well asthe possibility of obtaining better results by combining them (pearce, 2002; pecina, 2005).<papid> P05-2003 </papid></nextsent>
<nextsent>cooccurrence techniques are usually used in combination with linguistic techniques, which allow the use oflemmatized and pos-tagged corpora, or even syntactic dependencies (seretan, 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3513">
<title id=" W11-0802.xml">automatic extraction of nv expressions in basque basic issues on cooccurrence techniques </title>
<section> mwe: basic definition and extraction.  </section>
<citcontext>
<prevsection>
<prevsent>compositional combinations with lexical restriction, in which it is not possible to substitute the verb with its synonyms, or that present clear statistical idiosyncrasy in favor of given synonym choice (elka rtasuna adierazi: to express solidarity?;konpromisoa berretsi: to confirm com mitment?).
</prevsent>
<prevsent>among the different techniques that have been proposed to extract and characterize mwes, the cooccurrence of the components is the most used heuristic of institutionalization, and the use of association measures (am) goes back to early research on this field (church and hanks, 1990; <papid> J90-1003 </papid>smadja, 1993).<papid> J93-1007 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-2003 ">
in recent years, the comparative analysis of ams has aroused considerable interest, as well asthe possibility of obtaining better results by combining them (pearce, 2002; pecina, 2005).<papid> P05-2003 </papid></citsent>
<aftsection>
<nextsent>cooccurrence techniques are usually used in combination with linguistic techniques, which allow the use oflemmatized and pos-tagged corpora, or even syntactic dependencies (seretan, 2008).
</nextsent>
<nextsent>combinations these are some characteristics of the nv combinations in basque to be considered in order to design the extraction process efficiently: ? basque being an agglutinative language, mwe extraction must work on tagged texts, in order to identify different surface forms with their corresponding lemma.
</nextsent>
<nextsent>thus, pure statistical methods working with raw text are not expected to yield acceptable results.
</nextsent>
<nextsent>some combinations with noun as first lemma do not correspond to nv combinations in the sense that is usually understood in english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3516">
<title id=" W11-0802.xml">automatic extraction of nv expressions in basque basic issues on cooccurrence techniques </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>a more accurate view of the behaviour of ams in function of several factors such as the type of combination, corpus size, frequency range, window span,etc. bigram normalization is, in principle, reasonable procedure to formulate representative canonical forms, but requires deeper analysis of the silence that it seems to generate in the results.
</prevsent>
<prevsent>finally,the first evaluation using small gold-standard is encouraging, because it suggests that using ams it is possible to find new expressions that are not published in basque dictionaries.
</prevsent>
</prevsection>
<citsent citstr=" P06-2084 ">
in the near future, we want to carry out more comprehensive evaluation of the ams, and study how to combine them in order to improve there sults (pecina and schlesinger, 2006).<papid> P06-2084 </papid></citsent>
<aftsection>
<nextsent>in addition ofthis, we want to detect lexical, syntactic and semantic features of the expressions, and use this information to characterize them (fazly et al, 2009).<papid> J09-1005 </papid></nextsent>
<nextsent>acknowledgments this research was supported in part by the spanish ministry of education and science (openmt-2,tin2009-14675-c03-01) and by the basque government (berbatek: tools and technologies to promote language industry.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3517">
<title id=" W11-0802.xml">automatic extraction of nv expressions in basque basic issues on cooccurrence techniques </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>finally,the first evaluation using small gold-standard is encouraging, because it suggests that using ams it is possible to find new expressions that are not published in basque dictionaries.
</prevsent>
<prevsent>in the near future, we want to carry out more comprehensive evaluation of the ams, and study how to combine them in order to improve there sults (pecina and schlesinger, 2006).<papid> P06-2084 </papid></prevsent>
</prevsection>
<citsent citstr=" J09-1005 ">
in addition ofthis, we want to detect lexical, syntactic and semantic features of the expressions, and use this information to characterize them (fazly et al, 2009).<papid> J09-1005 </papid></citsent>
<aftsection>
<nextsent>acknowledgments this research was supported in part by the spanish ministry of education and science (openmt-2,tin2009-14675-c03-01) and by the basque government (berbatek: tools and technologies to promote language industry.
</nextsent>
<nextsent>etortek - ie09-262).
</nextsent>
<nextsent>our colleagues ainara estarrona and larraitz uria are kindly acknowledged for providing their expertise as linguists in the manual evaluation process.
</nextsent>
<nextsent>6
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3518">
<title id=" W10-4125.xml">space characters in chinese semi structured texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper shows that space character plays an equal role as the textual content, where it can be used not only to construct certain layout, but also to signal certain syntactic structure.
</prevsent>
<prevsent>some researchers have been seen to make use ofspace characters, but they mainly use spaces to create or recognise certain special layouts.
</prevsent>
</prevsection>
<citsent citstr=" P99-1057 ">
for example,(rus and summers, 1994) used white spaces to reformat documents into somewhat structured styles; (ng et al, 1999) <papid> P99-1057 </papid>and (hurst and nasukawa, 2000) <papid> C00-1049 </papid>used spaces to recognise tables in free text.</citsent>
<aftsection>
<nextsent>wrapper generation is more related to our research since it uses layout to extract structured content from documents(irmak and suel, 2006; chen et al, 2003).
</nextsent>
<nextsent>how ever, wrapper generation is too high level, this paper is aimed at exploring the effects of space characters at lower level.in this paper, we focus on semi-structured documents (in our case, real-world chinese curriculavitae), since these types of documents tend to contain more space layout information.
</nextsent>
<nextsent>this paper is intended to address the importance of space characters not only in layout extraction, but also in information extraction.
</nextsent>
<nextsent>to do so, daxtra technologies?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3519">
<title id=" W10-4125.xml">space characters in chinese semi structured texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper shows that space character plays an equal role as the textual content, where it can be used not only to construct certain layout, but also to signal certain syntactic structure.
</prevsent>
<prevsent>some researchers have been seen to make use ofspace characters, but they mainly use spaces to create or recognise certain special layouts.
</prevsent>
</prevsection>
<citsent citstr=" C00-1049 ">
for example,(rus and summers, 1994) used white spaces to reformat documents into somewhat structured styles; (ng et al, 1999) <papid> P99-1057 </papid>and (hurst and nasukawa, 2000) <papid> C00-1049 </papid>used spaces to recognise tables in free text.</citsent>
<aftsection>
<nextsent>wrapper generation is more related to our research since it uses layout to extract structured content from documents(irmak and suel, 2006; chen et al, 2003).
</nextsent>
<nextsent>how ever, wrapper generation is too high level, this paper is aimed at exploring the effects of space characters at lower level.in this paper, we focus on semi-structured documents (in our case, real-world chinese curriculavitae), since these types of documents tend to contain more space layout information.
</nextsent>
<nextsent>this paper is intended to address the importance of space characters not only in layout extraction, but also in information extraction.
</nextsent>
<nextsent>to do so, daxtra technologies?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3520">
<title id=" W10-4125.xml">space characters in chinese semi structured texts </title>
<section> space characters.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we perform evaluation of the tools on set of real-world cvs and give proposals for future work.
</prevsent>
<prevsent>a space character, when considered as punctuation, is blank area devoid of content, serving to separate words, letters, numbers and other punctuation.
</prevsent>
</prevsection>
<citsent citstr=" C94-1069 ">
(jones, 1994) <papid> C94-1069 </papid>found broadly three types of punctuation marks: delimiting, separating and dis ambiguating.</citsent>
<aftsection>
<nextsent>similarly, space characters have three different functionalities: delimiting, structuring and disambiguating.
</nextsent>
<nextsent>space characters are natural delimiters in some languages.
</nextsent>
<nextsent>in english and many other latin-basedlanguages for example, spaces are used for separating words and certain punctuation marks (e.g. period and colon).
</nextsent>
<nextsent>however, informal chinese typesetting, spaces are not used to delimit words or characters.hence the need for automatic word segmentation systems (zhang et al, 2003).<papid> W03-1730 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3521">
<title id=" W10-4125.xml">space characters in chinese semi structured texts </title>
<section> space characters.  </section>
<citcontext>
<prevsection>
<prevsent>space characters are natural delimiters in some languages.
</prevsent>
<prevsent>in english and many other latin-basedlanguages for example, spaces are used for separating words and certain punctuation marks (e.g. period and colon).
</prevsent>
</prevsection>
<citsent citstr=" W03-1730 ">
however, informal chinese typesetting, spaces are not used to delimit words or characters.hence the need for automatic word segmentation systems (zhang et al, 2003).<papid> W03-1730 </papid></citsent>
<aftsection>
<nextsent>the current segmentation systems mainly focus on resolving ambiguities and detecting new words in segmenting text with no spaces (gao et al, 2005).<papid> J05-4005 </papid></nextsent>
<nextsent>however, ambiguities can be caused not only by characters themselves, but also the spaces and layout around them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3522">
<title id=" W10-4125.xml">space characters in chinese semi structured texts </title>
<section> space characters.  </section>
<citcontext>
<prevsection>
<prevsent>in english and many other latin-basedlanguages for example, spaces are used for separating words and certain punctuation marks (e.g. period and colon).
</prevsent>
<prevsent>however, informal chinese typesetting, spaces are not used to delimit words or characters.hence the need for automatic word segmentation systems (zhang et al, 2003).<papid> W03-1730 </papid></prevsent>
</prevsection>
<citsent citstr=" J05-4005 ">
the current segmentation systems mainly focus on resolving ambiguities and detecting new words in segmenting text with no spaces (gao et al, 2005).<papid> J05-4005 </papid></citsent>
<aftsection>
<nextsent>however, ambiguities can be caused not only by characters themselves, but also the spaces and layout around them.
</nextsent>
<nextsent>the paper will later demonstrate this in terms of recognising entities, but the same should apply to segmentation.
</nextsent>
<nextsent>therefore, chinese documents can have white spaces, it is up to the author of the document to decide when to use spaces, which makes dealing with peoples spacing habits one of the reasons to include treatment of space characters in linguistic systems.
</nextsent>
<nextsent>structuring refers to space characters being used for layout purposes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3523">
<title id=" W11-1011.xml">automatic category label coarsening for syntax based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common modeling choice among syntax-based statistical machine translation systems is the use of synchronous context-free grammar (scfg), where source-language string and target-language string are produced simultaneously by applying series of re-write rules.
</prevsent>
<prevsent>given parallel corpus that has been statistically word-aligned and annotated with constituency structure on one or both sides, scfg models for mt can be learned via variety of methods.
</prevsent>
</prevsection>
<citsent citstr=" P06-1077 ">
parsing may be applied on the source side (liu et al , 2006), <papid> P06-1077 </papid>on the target side (galley et al , 2004), <papid> N04-1035 </papid>or on both sides of the parallel corpus (lavie et al , 2008; <papid> W08-0411 </papid>zhechev and way, 2008).<papid> C08-1139 </papid></citsent>
<aftsection>
<nextsent>in any of these cases, using the raw label set from source and/or target-side parsers can be undesirable.
</nextsent>
<nextsent>label sets used in statistical parsers are usually inherited directly from monolingual treebank projects, where the inventory of category labels was designed by independent teams of human linguists.these labels sets are not necessarily ideal forsta tistical parsing, let al ne for bilingual syntax-based translation models.
</nextsent>
<nextsent>further, the side(s) on which syntax is represented defines the nonterminal label space used by the resulting scfg.
</nextsent>
<nextsent>a pair of aligned adjectives, for example, may be labeled adj if onlysource-side syntax is used, jj if only target-side syntax is used, or adj::jj if syntax from both sides is used in the grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3524">
<title id=" W11-1011.xml">automatic category label coarsening for syntax based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common modeling choice among syntax-based statistical machine translation systems is the use of synchronous context-free grammar (scfg), where source-language string and target-language string are produced simultaneously by applying series of re-write rules.
</prevsent>
<prevsent>given parallel corpus that has been statistically word-aligned and annotated with constituency structure on one or both sides, scfg models for mt can be learned via variety of methods.
</prevsent>
</prevsection>
<citsent citstr=" N04-1035 ">
parsing may be applied on the source side (liu et al , 2006), <papid> P06-1077 </papid>on the target side (galley et al , 2004), <papid> N04-1035 </papid>or on both sides of the parallel corpus (lavie et al , 2008; <papid> W08-0411 </papid>zhechev and way, 2008).<papid> C08-1139 </papid></citsent>
<aftsection>
<nextsent>in any of these cases, using the raw label set from source and/or target-side parsers can be undesirable.
</nextsent>
<nextsent>label sets used in statistical parsers are usually inherited directly from monolingual treebank projects, where the inventory of category labels was designed by independent teams of human linguists.these labels sets are not necessarily ideal forsta tistical parsing, let al ne for bilingual syntax-based translation models.
</nextsent>
<nextsent>further, the side(s) on which syntax is represented defines the nonterminal label space used by the resulting scfg.
</nextsent>
<nextsent>a pair of aligned adjectives, for example, may be labeled adj if onlysource-side syntax is used, jj if only target-side syntax is used, or adj::jj if syntax from both sides is used in the grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3525">
<title id=" W11-1011.xml">automatic category label coarsening for syntax based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common modeling choice among syntax-based statistical machine translation systems is the use of synchronous context-free grammar (scfg), where source-language string and target-language string are produced simultaneously by applying series of re-write rules.
</prevsent>
<prevsent>given parallel corpus that has been statistically word-aligned and annotated with constituency structure on one or both sides, scfg models for mt can be learned via variety of methods.
</prevsent>
</prevsection>
<citsent citstr=" W08-0411 ">
parsing may be applied on the source side (liu et al , 2006), <papid> P06-1077 </papid>on the target side (galley et al , 2004), <papid> N04-1035 </papid>or on both sides of the parallel corpus (lavie et al , 2008; <papid> W08-0411 </papid>zhechev and way, 2008).<papid> C08-1139 </papid></citsent>
<aftsection>
<nextsent>in any of these cases, using the raw label set from source and/or target-side parsers can be undesirable.
</nextsent>
<nextsent>label sets used in statistical parsers are usually inherited directly from monolingual treebank projects, where the inventory of category labels was designed by independent teams of human linguists.these labels sets are not necessarily ideal forsta tistical parsing, let al ne for bilingual syntax-based translation models.
</nextsent>
<nextsent>further, the side(s) on which syntax is represented defines the nonterminal label space used by the resulting scfg.
</nextsent>
<nextsent>a pair of aligned adjectives, for example, may be labeled adj if onlysource-side syntax is used, jj if only target-side syntax is used, or adj::jj if syntax from both sides is used in the grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3526">
<title id=" W11-1011.xml">automatic category label coarsening for syntax based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common modeling choice among syntax-based statistical machine translation systems is the use of synchronous context-free grammar (scfg), where source-language string and target-language string are produced simultaneously by applying series of re-write rules.
</prevsent>
<prevsent>given parallel corpus that has been statistically word-aligned and annotated with constituency structure on one or both sides, scfg models for mt can be learned via variety of methods.
</prevsent>
</prevsection>
<citsent citstr=" C08-1139 ">
parsing may be applied on the source side (liu et al , 2006), <papid> P06-1077 </papid>on the target side (galley et al , 2004), <papid> N04-1035 </papid>or on both sides of the parallel corpus (lavie et al , 2008; <papid> W08-0411 </papid>zhechev and way, 2008).<papid> C08-1139 </papid></citsent>
<aftsection>
<nextsent>in any of these cases, using the raw label set from source and/or target-side parsers can be undesirable.
</nextsent>
<nextsent>label sets used in statistical parsers are usually inherited directly from monolingual treebank projects, where the inventory of category labels was designed by independent teams of human linguists.these labels sets are not necessarily ideal forsta tistical parsing, let al ne for bilingual syntax-based translation models.
</nextsent>
<nextsent>further, the side(s) on which syntax is represented defines the nonterminal label space used by the resulting scfg.
</nextsent>
<nextsent>a pair of aligned adjectives, for example, may be labeled adj if onlysource-side syntax is used, jj if only target-side syntax is used, or adj::jj if syntax from both sides is used in the grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3527">
<title id=" W11-1011.xml">automatic category label coarsening for syntax based machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>increased permissiveness, inturn, directs the decoders search into largely disjoint realm from the search space explored by the baseline system.
</prevsent>
<prevsent>a full summary and ideas for future work are given in section 7.
</prevsent>
</prevsection>
<citsent citstr=" W06-3119 ">
one example of modifying the scfg nonterminal set is seen in the syntax-augmented mt (samt) system of zollmann and venugopal (2006).<papid> W06-3119 </papid></citsent>
<aftsection>
<nextsent>in samt rule extraction, rules whose left-hand sides correspond exactly to target-side parse node retain that label in the grammar.
</nextsent>
<nextsent>additional nonterminal labels of the form t1+ t2 are created for rules spanning two adjacent parse nodes, while categorial grammar style nonterminals t1/t2 and t1\t2 areused for rules spanning partial t1 node that is missing t2 node to its right or left.
</nextsent>
<nextsent>these compound nonterminals in practice lead to very large label set.
</nextsent>
<nextsent>probability estimates for rules with the same structure up to labeling can be combined with the use of preference grammar (venugopal et al , 2009), <papid> N09-1027 </papid>which replaces the variant labelings with single scfg rule using generic x? la bels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3528">
<title id=" W11-1011.xml">automatic category label coarsening for syntax based machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>additional nonterminal labels of the form t1+ t2 are created for rules spanning two adjacent parse nodes, while categorial grammar style nonterminals t1/t2 and t1\t2 areused for rules spanning partial t1 node that is missing t2 node to its right or left.
</prevsent>
<prevsent>these compound nonterminals in practice lead to very large label set.
</prevsent>
</prevsection>
<citsent citstr=" N09-1027 ">
probability estimates for rules with the same structure up to labeling can be combined with the use of preference grammar (venugopal et al , 2009), <papid> N09-1027 </papid>which replaces the variant labelings with single scfg rule using generic x? la bels.</citsent>
<aftsection>
<nextsent>the generic rules preference?
</nextsent>
<nextsent>over possible labelings is stored as probability distribution inside the rule for use at decoding time.
</nextsent>
<nextsent>preference grammars thus reduce the label set size to one for the purposes of some feature calculations ? which avoids the fragmentation of rule scores due to labeling ambiguity ? but the original labels persist for specifying which rules may combine with which others.
</nextsent>
<nextsent>chiang (2010) <papid> P10-1146 </papid>extended samt-style labels to both source- and target-side parses, also introducing mechanism by which scfg rules may apply at runtime even if their labels do not match.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3529">
<title id=" W11-1011.xml">automatic category label coarsening for syntax based machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>over possible labelings is stored as probability distribution inside the rule for use at decoding time.
</prevsent>
<prevsent>preference grammars thus reduce the label set size to one for the purposes of some feature calculations ? which avoids the fragmentation of rule scores due to labeling ambiguity ? but the original labels persist for specifying which rules may combine with which others.
</prevsent>
</prevsection>
<citsent citstr=" P10-1146 ">
chiang (2010) <papid> P10-1146 </papid>extended samt-style labels to both source- and target-side parses, also introducing mechanism by which scfg rules may apply at runtime even if their labels do not match.</citsent>
<aftsection>
<nextsent>under chiangs soft matching constraint, rule headed by label a::z may still plug into substitution site labeled b::y by paying additional model costs substba and substyz . this is an on-the-fly method of coarsening the effective label set on case-by-casebasis.
</nextsent>
<nextsent>unfortunately, it also requires tuning separate decoder feature for each pair of source-side and each pair of target-side labels.
</nextsent>
<nextsent>this tuning can become prohibitively complex when working with standard parser label sets, which typically contain between 30 and 70 labels on each side.
</nextsent>
<nextsent>99 jj jjr jjs figure 1: alignment distributions over french labels for the english adjective labels jj, jjr, and jjs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3530">
<title id=" W11-1011.xml">automatic category label coarsening for syntax based machine translation </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>in section 5, we explore several earlier and more meaningful stopping points.
</prevsent>
<prevsent>experiments are conducted on chinese-to-english translation using approximately 300,000 sentence pairs from the fbis corpus.
</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
to obtain parse trees over both sides of each parallel corpus, we used the english and chinese grammars of the berkeley 100 parser (petrov and klein, 2007).<papid> N07-1051 </papid>given parsed and word-aligned parallel sentence, we extract scfg rules from it following the procedure of lavie et al  (2008).<papid> W08-0411 </papid></citsent>
<aftsection>
<nextsent>the method first identifies node alignments between the two parse trees according to support from the word alignments.
</nextsent>
<nextsent>a node in the source parse tree will be aligned to node in the target parse tree if all the words in the yield of the source node are either all aligned to words within the yield of the target node or have no alignments at all.
</nextsent>
<nextsent>then scfg rules can be extracted from adjacent levels of aligned nodes, which specify points at which the tree pair can be decomposed into minimal scfg rules.
</nextsent>
<nextsent>in addition to produc inga minimal rule, each decomposition point also produces phrase pair rule with the node pairs yields as the right-hand side, as long as the length of the yield is less than specified threshold.following grammar extraction, labels are optionally clustered and collapsed according to the algorithm in section 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3532">
<title id=" W11-1011.xml">automatic category label coarsening for syntax based machine translation </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to produc inga minimal rule, each decomposition point also produces phrase pair rule with the node pairs yields as the right-hand side, as long as the length of the yield is less than specified threshold.following grammar extraction, labels are optionally clustered and collapsed according to the algorithm in section 3.
</prevsent>
<prevsent>the grammar is re-written withthe modified nonterminals, then scored as usual according to our translation model features.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
feature weights themselves are learned via minimum error rate training as implemented in z-mert (zaidan, 2009) with the bleu metric (papineni et al , 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>decoding is carried out with joshua (li et al , 2009), <papid> W09-0424 </papid>an open-source platform for scfg-based mt. due to engineering limitations in decoding witha large grammar, we apply three additional error correction and filtering steps to every system.</nextsent>
<nextsent>first, we observed that the syntactic parsers were most likely to make labeling errors for cardinal numbers in english and punctuation marks in all languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3533">
<title id=" W11-1011.xml">automatic category label coarsening for syntax based machine translation </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>the grammar is re-written withthe modified nonterminals, then scored as usual according to our translation model features.
</prevsent>
<prevsent>feature weights themselves are learned via minimum error rate training as implemented in z-mert (zaidan, 2009) with the bleu metric (papineni et al , 2002).<papid> P02-1040 </papid></prevsent>
</prevsection>
<citsent citstr=" W09-0424 ">
decoding is carried out with joshua (li et al , 2009), <papid> W09-0424 </papid>an open-source platform for scfg-based mt. due to engineering limitations in decoding witha large grammar, we apply three additional error correction and filtering steps to every system.</citsent>
<aftsection>
<nextsent>first, we observed that the syntactic parsers were most likely to make labeling errors for cardinal numbers in english and punctuation marks in all languages.
</nextsent>
<nextsent>we thus post-process the parses of our training data to tag all english cardinal numbers as cd and to over write the labels of various punctuation marks with the correct labels as defined by each languages label set.
</nextsent>
<nextsent>second, after rule extraction, we compute the distribution of left-hand-side labels for each unique labeled right-hand side in the grammar, and we remove the labels in the least frequent 10% of thedistribution.
</nextsent>
<nextsent>this puts general-purpose limit on labeling ambiguity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3535">
<title id=" W11-1011.xml">automatic category label coarsening for syntax based machine translation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>onmt 2008, improvements range from 1.25 to 2.21 points on bleu and from 2.43 to 4.30 points onter.
</prevsent>
<prevsent>improvements on both sets according to meteor, though smaller, are still noticable (up to 0.89points).
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
in the case of bleu, we verified the significance of the improvements by conducting paired bootstrap re sampling (koehn, 2004) <papid> W04-3250 </papid>on themt 2003output.</citsent>
<aftsection>
<nextsent>with = 1000 and   0.05, all five label collapsed systems were statistically significant improvements over the baseline, and all other collapsed systems were significant improvements over the 99 iteration system.
</nextsent>
<nextsent>thus, though the system that provides the highest score changes across metrics and test sets, the overall pattern of scores suggests that over-collapsing labels may start to weaken results.
</nextsent>
<nextsent>a more moderate stopping point is thus preferable, but beyond that we suspect the best result is determined more by the test set, automatic metric choice, and mert instability than systematic changes in the label set.
</nextsent>
<nextsent>table 1 showed strong practical benefit to running the label collapsing algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3536">
<title id=" W11-1415.xml">readability annotation replacing the expert by the crowd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we conclude that crowdsourcing is viable alternative tothe opinions of language experts for readability prediction.
</prevsent>
<prevsent>the task of automatically determining the readability of texts has long and rich tradition.
</prevsent>
</prevsection>
<citsent citstr=" P05-1065 ">
this has notonly resulted in large number of readability formulas (flesch, 1948; brouwer, 1963; dale and chall, 1948; gunning, 1952; mclaughlin, 1969), but also to the more recent tendency of using insights from nlp for automatic readability prediction (schwarm and ostendorf, 2005; <papid> P05-1065 </papid>collins-thompson and callan,2004; pitler and nenkova, 2008).<papid> D08-1020 </papid></citsent>
<aftsection>
<nextsent>potential applications include the selection of reading material for language learners, automatic essay scoring, the selection of online text material for automatic summarization, etc. one of the well-known bottlenecks in data-driven nlp research is the lack of sufficiently large datasets for which annotators provided labels with sufficient agreement.
</nextsent>
<nextsent>also readability research is faced with the crucial obstacle that very few corpora of generic texts exist of which reliable readability information is available (tanaka-ishii et al, 2010).when constructing such corpus, the inherent subjectivity of the concept of readability cannot be ignored.
</nextsent>
<nextsent>the ease with which given reader can correctly identify the message conveyed in text is, among other things, inextricably related to the readers background knowledge of the subject at hand (mcnamara et al, 1993).
</nextsent>
<nextsent>the construction of corpus, which can serve as gold standard against which new scoring or ranking systems can be tested,thus requires multifaceted approach taking into account both the properties of the text under evaluation and those of the readers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3537">
<title id=" W11-1415.xml">readability annotation replacing the expert by the crowd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we conclude that crowdsourcing is viable alternative tothe opinions of language experts for readability prediction.
</prevsent>
<prevsent>the task of automatically determining the readability of texts has long and rich tradition.
</prevsent>
</prevsection>
<citsent citstr=" D08-1020 ">
this has notonly resulted in large number of readability formulas (flesch, 1948; brouwer, 1963; dale and chall, 1948; gunning, 1952; mclaughlin, 1969), but also to the more recent tendency of using insights from nlp for automatic readability prediction (schwarm and ostendorf, 2005; <papid> P05-1065 </papid>collins-thompson and callan,2004; pitler and nenkova, 2008).<papid> D08-1020 </papid></citsent>
<aftsection>
<nextsent>potential applications include the selection of reading material for language learners, automatic essay scoring, the selection of online text material for automatic summarization, etc. one of the well-known bottlenecks in data-driven nlp research is the lack of sufficiently large datasets for which annotators provided labels with sufficient agreement.
</nextsent>
<nextsent>also readability research is faced with the crucial obstacle that very few corpora of generic texts exist of which reliable readability information is available (tanaka-ishii et al, 2010).when constructing such corpus, the inherent subjectivity of the concept of readability cannot be ignored.
</nextsent>
<nextsent>the ease with which given reader can correctly identify the message conveyed in text is, among other things, inextricably related to the readers background knowledge of the subject at hand (mcnamara et al, 1993).
</nextsent>
<nextsent>the construction of corpus, which can serve as gold standard against which new scoring or ranking systems can be tested,thus requires multifaceted approach taking into account both the properties of the text under evaluation and those of the readers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3539">
<title id=" W11-1415.xml">readability annotation replacing the expert by the crowd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, tendency seems to have arisen to also explicitly address this subjective aspect of readability.
</prevsent>
<prevsent>pitler and nenkova(2008), <papid> D08-1020 </papid>for example, base their readability prediction method exclusively on the extent to which readers found text to be well-written?</prevsent>
</prevsection>
<citsent citstr=" C10-1062 ">
and kate et al (2010) <papid> C10-1062 </papid>take the assessments supplied by number of experts as their gold standard, and test their read ability prediction method as well as assessments by novices against these expert opinions.</citsent>
<aftsection>
<nextsent>in this paper, we report on two methodologies to construct corpus of readability assessments, which can serve as gold standard against which new scoring or ranking systems can be tested.
</nextsent>
<nextsent>both methodologies were used for collecting readability assessments of dutch and english texts.
</nextsent>
<nextsent>since these data collection experiments for english only recently started, the focus in this paper will be on 120 dutch.
</nextsent>
<nextsent>by collecting multiple assessments per text, the goal was to level out the readers background knowledge and attitude.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3540">
<title id=" W11-1415.xml">readability annotation replacing the expert by the crowd </title>
<section> readability assessment by the expert.  </section>
<citcontext>
<prevsection>
<prevsent>section 3 outlines simpler crow sourcing application and its correspondences withthe experts.
</prevsent>
<prevsent>finally, in section 4, we draw conclusions and give short summary of future work.
</prevsent>
</prevsection>
<citsent citstr=" W08-0909 ">
reader since readability prediction was initially primarily designed to identify reading material suited to the reading competence of given individual, most of the existing datasets are drawn from textbooks and other sources intended for different compentence levels (francois, 2009; heilman et al, 2008).<papid> W08-0909 </papid></citsent>
<aftsection>
<nextsent>for dutch, for example, the only large-scale experimental readability research (staphorsius and krom,1985; staphorsius, 1994) is limited to texts for elementary school children.1 for english, the situation is similar as for dutch, viz.
</nextsent>
<nextsent>a predominant focus on educational corpora.
</nextsent>
<nextsent>recently, an evaluation was designed by ldc in the framework of the darpa machine reading program (kate et al, 2010).<papid> C10-1062 </papid></nextsent>
<nextsent>for this purpose more general corpus was assembled which was not tailored to specific audience, genre or domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3544">
<title id=" W11-1415.xml">readability annotation replacing the expert by the crowd </title>
<section> readability assessment by the expert.  </section>
<citcontext>
<prevsection>
<prevsent>the text pairs plotted in the middle of the figure will be regarded as assessed with somewhat different readability and those plotted in the upper 123 right part will be interpreted as text pairs with much difference in readability.
</prevsent>
<prevsent>based on the assumption that the readability of text can be conceptualized as the extent to which the text is perceived to be readable by the community of language users, we also investigated whether acrowdsourcing approach could be viable alternative to expert labeling.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
crowdsourcing has already been used with success for nlp applications such as wsd (snow et al, 2008) <papid> D08-1027 </papid>or anaphora resolution3.by redesigning readability assessment as crowdsourcing application, we hypothesize that no background in linguistics is required to judge the readability of given text.</citsent>
<aftsection>
<nextsent>the sort by readability application4 is designed as simple crowdsourcing application to be used by as many users as possible.the site is accessible to anyone having internet access and very inutitive; the users are not required to provide personal data.
</nextsent>
<nextsent>a screen shot of the crowdsourcing application is shown in figure 5.
</nextsent>
<nextsent>two texts are displayed simultaneously and the user is asked to tick one of the following statements left: much more difficult ? right: much easier?, left: somewhat more difficult ? right: somewhat easier?, both equally difficult?, left: somewhat easier ? right: somewhat more difficult?, left:much easier right: much more difficult?.
</nextsent>
<nextsent>the assessments were performed on the same dataset thatwas used for the expert readers application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3545">
<title id=" W11-1415.xml">readability annotation replacing the expert by the crowd </title>
<section> readability assessment by the expert.  </section>
<citcontext>
<prevsection>
<prevsent>28 % of the text pairs in both applications are thus labeled as equally readable, while 18 % of the pairs are labeled with much difference indifficulty.
</prevsent>
<prevsent>those partitions correspond with boundary values of 0.016 and 0.29 for s, respectively.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
we used as proposed by carletta (1996) <papid> J96-2004 </papid>as measure for the agreement between annotators.</citsent>
<aftsection>
<nextsent>k is given by the following formula: = (a)?
</nextsent>
<nextsent>p (e) 1?
</nextsent>
<nextsent>p (e) in which (a) is the probability that two annotators make the same decision and (e) is the probability that the same decision is made coincidently.
</nextsent>
<nextsent>for (a), we take into account the number of times two annotators agree about text pair and the number of times they disagree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3546">
<title id=" W11-1709.xml">tracking sentiment in mail how genders differ on emotional axes </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatic analysis and tracking of emotions in emails has number of benefits including: 1.
</prevsent>
<prevsent>determining risk of repeat attempts by analyz-.
</prevsent>
</prevsection>
<citsent citstr=" W09-1323 ">
ing suicide notes (osgood and walker, 1959; matykiewicz et al, 2009; <papid> W09-1323 </papid>pestian et al, 2008).<papid> W08-0616 </papid>1</citsent>
<aftsection>
<nextsent>through work-place and personal email (boneva et al, 2001).over time.
</nextsent>
<nextsent>for example, did certain managerial course bring about measurable change in ones inter-personal communication?
</nextsent>
<nextsent>4.
</nextsent>
<nextsent>determining if there is correlation between.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3547">
<title id=" W11-1709.xml">tracking sentiment in mail how genders differ on emotional axes </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatic analysis and tracking of emotions in emails has number of benefits including: 1.
</prevsent>
<prevsent>determining risk of repeat attempts by analyz-.
</prevsent>
</prevsection>
<citsent citstr=" W08-0616 ">
ing suicide notes (osgood and walker, 1959; matykiewicz et al, 2009; <papid> W09-1323 </papid>pestian et al, 2008).<papid> W08-0616 </papid>1</citsent>
<aftsection>
<nextsent>through work-place and personal email (boneva et al, 2001).over time.
</nextsent>
<nextsent>for example, did certain managerial course bring about measurable change in ones inter-personal communication?
</nextsent>
<nextsent>4.
</nextsent>
<nextsent>determining if there is correlation between.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3548">
<title id=" W11-1709.xml">tracking sentiment in mail how genders differ on emotional axes </title>
<section> understanding how genders communicate.  </section>
<citcontext>
<prevsection>
<prevsent>it also associates words with joy,sadness, anger, fear, trust, disgust, surprise, anticipation, which are argued to be the eight basic and prototypical emotions (plutchik, 1980).
</prevsent>
<prevsent>2 related work.
</prevsent>
</prevsection>
<citsent citstr=" D09-1063 ">
over the last decade, there has been considerable work in sentiment analysis, especially in determining whether term has positive or negative polarity (lehrer, 1974; turney and littman, 2003; mohammad et al, 2009).<papid> D09-1063 </papid></citsent>
<aftsection>
<nextsent>there is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (bellegarda, 2010; <papid> W10-0201 </papid>mohammad and turney, 2010; <papid> W10-0204 </papid>alm et al, 2005; <papid> H05-1073 </papid>alm et al, 2005).<papid> H05-1073 </papid></nextsent>
<nextsent>the technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (dodds and danforth, 2010; pang and lee, 2008).automatically analyzing affect in emails has primarily been done for automatic gender identification (cheng et al, 2009; corney et al, 2002), but it has relied on mostly on surface features such as exclamations and very small emotion lexicons.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3549">
<title id=" W11-1709.xml">tracking sentiment in mail how genders differ on emotional axes </title>
<section> understanding how genders communicate.  </section>
<citcontext>
<prevsection>
<prevsent>2 related work.
</prevsent>
<prevsent>over the last decade, there has been considerable work in sentiment analysis, especially in determining whether term has positive or negative polarity (lehrer, 1974; turney and littman, 2003; mohammad et al, 2009).<papid> D09-1063 </papid></prevsent>
</prevsection>
<citsent citstr=" W10-0201 ">
there is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (bellegarda, 2010; <papid> W10-0201 </papid>mohammad and turney, 2010; <papid> W10-0204 </papid>alm et al, 2005; <papid> H05-1073 </papid>alm et al, 2005).<papid> H05-1073 </papid></citsent>
<aftsection>
<nextsent>the technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (dodds and danforth, 2010; pang and lee, 2008).automatically analyzing affect in emails has primarily been done for automatic gender identification (cheng et al, 2009; corney et al, 2002), but it has relied on mostly on surface features such as exclamations and very small emotion lexicons.
</nextsent>
<nextsent>the wordnet affect lexicon (wal) (strapparavaand valitutti, 2004) has few hundred words annotated with associations to number of affect categories including the six ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 general inquirer (gi) (stone et al, 1966) has 11,788 words labeled with 182 categories of word tags, including positive and negative polarity.4 affective norms for english words (anew) has pleasure (happy?
</nextsent>
<nextsent>unhappy), arousal (excitedcalm), and dominance(controlledin control) ratings for 1034 words.5 mohammad and turney (2010) <papid> W10-0204 </papid>compiled emotion annotations for about 4000 words with eight emotions (six of ekman, trust, and anticipation).</nextsent>
<nextsent>3.1 emotion lexicon.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3550">
<title id=" W11-1709.xml">tracking sentiment in mail how genders differ on emotional axes </title>
<section> understanding how genders communicate.  </section>
<citcontext>
<prevsection>
<prevsent>2 related work.
</prevsent>
<prevsent>over the last decade, there has been considerable work in sentiment analysis, especially in determining whether term has positive or negative polarity (lehrer, 1974; turney and littman, 2003; mohammad et al, 2009).<papid> D09-1063 </papid></prevsent>
</prevsection>
<citsent citstr=" W10-0204 ">
there is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (bellegarda, 2010; <papid> W10-0201 </papid>mohammad and turney, 2010; <papid> W10-0204 </papid>alm et al, 2005; <papid> H05-1073 </papid>alm et al, 2005).<papid> H05-1073 </papid></citsent>
<aftsection>
<nextsent>the technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (dodds and danforth, 2010; pang and lee, 2008).automatically analyzing affect in emails has primarily been done for automatic gender identification (cheng et al, 2009; corney et al, 2002), but it has relied on mostly on surface features such as exclamations and very small emotion lexicons.
</nextsent>
<nextsent>the wordnet affect lexicon (wal) (strapparavaand valitutti, 2004) has few hundred words annotated with associations to number of affect categories including the six ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 general inquirer (gi) (stone et al, 1966) has 11,788 words labeled with 182 categories of word tags, including positive and negative polarity.4 affective norms for english words (anew) has pleasure (happy?
</nextsent>
<nextsent>unhappy), arousal (excitedcalm), and dominance(controlledin control) ratings for 1034 words.5 mohammad and turney (2010) <papid> W10-0204 </papid>compiled emotion annotations for about 4000 words with eight emotions (six of ekman, trust, and anticipation).</nextsent>
<nextsent>3.1 emotion lexicon.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3551">
<title id=" W11-1709.xml">tracking sentiment in mail how genders differ on emotional axes </title>
<section> understanding how genders communicate.  </section>
<citcontext>
<prevsection>
<prevsent>2 related work.
</prevsent>
<prevsent>over the last decade, there has been considerable work in sentiment analysis, especially in determining whether term has positive or negative polarity (lehrer, 1974; turney and littman, 2003; mohammad et al, 2009).<papid> D09-1063 </papid></prevsent>
</prevsection>
<citsent citstr=" H05-1073 ">
there is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (bellegarda, 2010; <papid> W10-0201 </papid>mohammad and turney, 2010; <papid> W10-0204 </papid>alm et al, 2005; <papid> H05-1073 </papid>alm et al, 2005).<papid> H05-1073 </papid></citsent>
<aftsection>
<nextsent>the technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (dodds and danforth, 2010; pang and lee, 2008).automatically analyzing affect in emails has primarily been done for automatic gender identification (cheng et al, 2009; corney et al, 2002), but it has relied on mostly on surface features such as exclamations and very small emotion lexicons.
</nextsent>
<nextsent>the wordnet affect lexicon (wal) (strapparavaand valitutti, 2004) has few hundred words annotated with associations to number of affect categories including the six ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 general inquirer (gi) (stone et al, 1966) has 11,788 words labeled with 182 categories of word tags, including positive and negative polarity.4 affective norms for english words (anew) has pleasure (happy?
</nextsent>
<nextsent>unhappy), arousal (excitedcalm), and dominance(controlledin control) ratings for 1034 words.5 mohammad and turney (2010) <papid> W10-0204 </papid>compiled emotion annotations for about 4000 words with eight emotions (six of ekman, trust, and anticipation).</nextsent>
<nextsent>3.1 emotion lexicon.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3555">
<title id=" W11-1709.xml">tracking sentiment in mail how genders differ on emotional axes </title>
<section> emotion analysis.  </section>
<citcontext>
<prevsection>
<prevsent>we created large word emotion association lexicon by crowdsourcing to amazons mechanicalturk.6 we follow the method outlined in mohammad and turney (2010).<papid> W10-0204 </papid></prevsent>
<prevsent>unlike mohammad and turney, who used the macquarie thesaurus (bernard, 1986), we use the roget thesaurus as the source for target terms.7 since the 1911 us edition of rogets is available freely in the public domain, it allows us to distribute our emotion lexicon without the burden of restrictive licenses.</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
we annotated only those words that occurred more than 120,000 times in the google n-gram corpus.8 the rogets thesaurus groups related words into about thousand categories, which can be thought of as coarse senses or concepts (yarowsky, 1992).<papid> C92-2070 </papid></citsent>
<aftsection>
<nextsent>if word is ambiguous, then it is listed in more than one category.
</nextsent>
<nextsent>since word may have different emotion associations when used in different senses, we obtained annotations at word-sense level by first asking an automatically generated word-choice question pertaining to the target: q1.
</nextsent>
<nextsent>which word is closest in meaning to shark (target)?
</nextsent>
<nextsent>car ? tree ? fish ? olive the near-synonym is taken from the thesaurus, and the dis tractors are randomly chosen words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3557">
<title id=" W11-1709.xml">tracking sentiment in mail how genders differ on emotional axes </title>
<section> emotion analysis.  </section>
<citcontext>
<prevsection>
<prevsent>this simple approach may not be reliable in determining if particular sentence is expressing certain emotion, but it is reliable in determining if large piece of text has more emotional expressions compared to others in corpus.
</prevsent>
<prevsent>example applications include detecting spikes in anger words in close proximity to mentions of target product in twitter stream (daz and ruz, 2002; dube?
</prevsent>
</prevsection>
<citsent citstr=" P11-2064 ">
andmaute, 1996), and literary analyses of text, forex ample, how novels and fairy tales differ in the use of emotion words (mohammad, 2011<papid> P11-2064 </papid>b).in this section, we quantitatively compare the emotion words in love letters, hate mail, and suicide notes.</citsent>
<aftsection>
<nextsent>we compiled love letters corpus (llc) 0.1 by extracting 348 postings from lovingyou.com.9we created hate mail corpus (hmc) 0.1 by collecting 279 pieces of hate mail sent to the millenium project.10 the suicide notes corpus (snc) 0.1 has 21 notes taken from art klein ers website.11 we will continue to add more data to these corpora as we find them, and all three corpora are freely available.figures 1, 2, and 3 show the percentages of positive and negative words in the love letters corpus,hate mail corpus, and the suicide notes corpus.
</nextsent>
<nextsent>figures 5, 6, and 7 show the percentages of different emotion words in the three corpora.
</nextsent>
<nextsent>emotions are represented by colours as per study on word?
</nextsent>
<nextsent>colour associations (mohammad, 2011<papid> P11-2064 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3561">
<title id=" W11-0205.xml">fast and simple semantic class assignment for biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluation with the structured test suite reveals number of characteristics of the performance of the approach.
</prevsent>
<prevsent>broad semantic class assignment is useful for number of language processing tasks, including coreference resolution (hobbs, 1978), document classification (caporaso et al, 2005), and information extraction (baumgartner jr. et al, 2008).
</prevsent>
</prevsection>
<citsent citstr=" M98-1001 ">
alimited number of semantic classes have been studied extensively, such as assigning text strings to the category gene or protein (yeh et al, 2005;smith et al, 2008), or the person, organization, and location categories introduced in the message understanding conferences (chinchor,1998).<papid> M98-1001 </papid></citsent>
<aftsection>
<nextsent>a larger number of semantic classes have received smaller amounts of attention, e.g. the classe sin the genia ontology (kim et al, 2004), various event types derived from the gene ontology(kim et al, 2009), <papid> W09-1401 </papid>and diseases (leaman and gonzalez, 2008).</nextsent>
<nextsent>however, many semantic types havenot been studied at all.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3562">
<title id=" W11-0205.xml">fast and simple semantic class assignment for biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>broad semantic class assignment is useful for number of language processing tasks, including coreference resolution (hobbs, 1978), document classification (caporaso et al, 2005), and information extraction (baumgartner jr. et al, 2008).
</prevsent>
<prevsent>alimited number of semantic classes have been studied extensively, such as assigning text strings to the category gene or protein (yeh et al, 2005;smith et al, 2008), or the person, organization, and location categories introduced in the message understanding conferences (chinchor,1998).<papid> M98-1001 </papid></prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
a larger number of semantic classes have received smaller amounts of attention, e.g. the classe sin the genia ontology (kim et al, 2004), various event types derived from the gene ontology(kim et al, 2009), <papid> W09-1401 </papid>and diseases (leaman and gonzalez, 2008).</citsent>
<aftsection>
<nextsent>however, many semantic types havenot been studied at all.
</nextsent>
<nextsent>in addition, where ontologies are concerned, although there has been work on finding mentions or evidence of specific terms in text (blaschke et al, 2005; stoica and hearst, 2006; davis et al, 2006; shah et al, 2009), there has been no work specifically addressing assigning multiple very broad semantic classes with potential overlap.in particular, this paper examines the problem of taking set of ontologies and text string (typically,but not necessarily, noun phrase) as input and determining which ontology defines the semantic class that that text string refers to.
</nextsent>
<nextsent>we make an equivalence here between the notion of belonging to the domain of an ontology and belonging to specific semantic class.
</nextsent>
<nextsent>for example, if string in text refers to something in the domain of the gene ontology,we take it as belonging to gene ontology semantic class (using the name of the ontology only for convenience); if string in text refers to something belonging to the domain of the sequence ontology,we take it as belonging to sequence ontology semantic class.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3563">
<title id=" W11-0218.xml">simsem fast approximate string matching in relation to semantic category disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we suggest possible explanations and future research directions.
</prevsent>
<prevsent>our lexical resources and implementation are made freely available for research purposes at: http://github.com/ninjin/ simsem
</prevsent>
</prevsection>
<citsent citstr=" W09-1119 ">
the use of dictionaries for boosting performance has become commonplace for named entity recognition (ner) systems (torii et al, 2009; ratinov and roth, 2009).<papid> W09-1119 </papid></citsent>
<aftsection>
<nextsent>in particular, dictionaries can give an initial improvement when little or no training data is available.
</nextsent>
<nextsent>however, no dictionary is perfect, and all resources lack certain spelling variants and lag behind current vocabulary usage and thus are unable to cover the intended domain in full.
</nextsent>
<nextsent>further,due to varying dictionary cur ation and corpus annotation guidelines, the definition of what constitutes semantic category is highly unlikely to precisely match for any two specific resources (wang et al, 2009).
</nextsent>
<nextsent>ideally, for applying lexical resource to an entity recognition or disambiguation task to serve as definition of semantic category there would bea precise match between the definitions of the lexical resource and target domain, but this is seldom or never the case.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3564">
<title id=" W11-0218.xml">simsem fast approximate string matching in relation to semantic category disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ideally, for applying lexical resource to an entity recognition or disambiguation task to serve as definition of semantic category there would bea precise match between the definitions of the lexical resource and target domain, but this is seldom or never the case.
</prevsent>
<prevsent>most previous work studying the use of dictionary resources in entity mention-related tasks has focused on single-class ner, in particular this is true forbionlp where it has mainly concerned the detection of proteins.
</prevsent>
</prevsection>
<citsent citstr=" W08-0609 ">
these efforts include tsuruoka andtsujii (2003), utilising dictionaries for protein detection by considering each dictionary entry using anovel distance measure, and sasaki et al (2008), <papid> W08-0609 </papid>applying dictionaries to restrain the contexts in which proteins appear in text.</citsent>
<aftsection>
<nextsent>in this work, we do not consider entity mention detection, but instead focus solely on the related task of disambiguating these mantic category forgiven continuous sequence of characters (a textual span), doing so we side-step the issue of boundary detection in favour of focusing on novel aspects of semantic category disambiguation.
</nextsent>
<nextsent>also, we are yet to see high-performing multi-class biomedical ner system, this motivates our desire to include multiple semantic categories.
</nextsent>
<nextsent>136
</nextsent>
<nextsent>in this section we introduce our approach and the structure of our system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3565">
<title id=" W11-0218.xml">simsem fast approximate string matching in relation to semantic category disambiguation </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>and semantic?.
</prevsent>
<prevsent>2.2 simstring.
</prevsent>
</prevsection>
<citsent citstr=" C10-1096 ">
simstring1 is software library utilising the cpmerge algorithm (okazaki and tsujii, 2010) <papid> C10-1096 </papid>to enable fast approximate string matching.</citsent>
<aftsection>
<nextsent>the software makes it possible to find matches in collection with over ten million entries using cosine similarity anda similarity threshold of 0.7 in approximately 1 millisecond with modest modern hardware.
</nextsent>
<nextsent>this makes it useful for querying large collection of strings to 1http://www.chokkan.org/software/simstring/ find entries which may differ from the query string only superficially and may still be members of the same semantic category.
</nextsent>
<nextsent>as an example, if we construct sim string database using an american english wordlist2 and query it using the cosine measure and threshold of 0.7.
</nextsent>
<nextsent>for the query reviewer?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3566">
<title id=" W11-0218.xml">simsem fast approximate string matching in relation to semantic category disambiguation </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, the linnaeus dictionary was converted into single database since it covers the single category species?.table 3 contains the statistics per dictionary resource and the number of sim string databases created for each resource.
</prevsent>
<prevsent>due to space requirements we leave out the full details for go bp, go cc, go mf, umls, entrez gene and turku trig, and instead give the total entries for all the databases generated from these resources.
</prevsent>
</prevsection>
<citsent citstr=" W10-1911 ">
138 name abbreviation semantic categories publication gene ontology go multiple ash burner et al (2000) protein information resource pir proteins wu et al (2003) unified medical language system umls multiple bodenreider (2004) entrez gene ? proteins maglott et al (2005) automatically generated dictionary shi proteins shi and campagne (2005) jochem jochem multiple hettne et al (2009) turku event corpus turku proteins and biomolecular events bjorne et al (2010) arizona disease corpus azdc diseases chowdhury and lavelli (2010) <papid> W10-1911 </papid>linnaeus dictionary linnaeus species gerner et al (2010) websters international dictionary wid multiple ? table 2: lexical resources gathered for our experiments resource unique entries databases go bp 67,411 4 go cc 5,993 4 go mf 55,595 4 pir 691,577 1 umls 5,902,707 135 entrez gene 3,602,757 5 shi 61,676 1 chebi 187,993 1 chemical 1,527,751 1 turku prot 4,745,825 1 turku trig 130,139 10 azdc 1,195 1 linnaeus 3,119,005 1 wid 235,802 1 total: 20, 335, 426 170 table 3: statistics per dictionary resource 3.2 corpora.</citsent>
<aftsection>
<nextsent>to evaluate our approach we need variety of corpora annotated with multiple semantic categories.
</nextsent>
<nextsent>for this purpose we selected the six corpora listed in table 4.
</nextsent>
<nextsent>the majority of our corpora are available in the common stand-off style format introduced for the bionlp 2009 shared task (bionlp09 st) (kim et al, 2009).<papid> W09-1401 </papid></nextsent>
<nextsent>the remaining two, nlpba and calbc cii, were converted into the bionlp09 st format so that we could process all resources in the same manner for our experimental set-up.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3567">
<title id=" W11-0218.xml">simsem fast approximate string matching in relation to semantic category disambiguation </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>to evaluate our approach we need variety of corpora annotated with multiple semantic categories.
</prevsent>
<prevsent>for this purpose we selected the six corpora listed in table 4.
</prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
the majority of our corpora are available in the common stand-off style format introduced for the bionlp 2009 shared task (bionlp09 st) (kim et al, 2009).<papid> W09-1401 </papid></citsent>
<aftsection>
<nextsent>the remaining two, nlpba and calbc cii, were converted into the bionlp09 st format so that we could process all resources in the same manner for our experimental set-up.
</nextsent>
<nextsent>in addition to physical entity annotations, the grec, epi, id and genia corpora incorporate event trigger annotations (e.g. gene regulatory event (gre) for grec).
</nextsent>
<nextsent>these trigger expressions carry with them specific semantic type (e.g. in teract?
</nextsent>
<nextsent>can carry the semantic type binding for genia), allowing us to enrich the datasets with additional semantic categories by including these types in our dataset as distinct semantic categories.this gave us the following increase in semantic cat egories: grec one, epi 15, id ten, genia nine.the original grec corpus contains an exceptionally wide array of semantic categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3568">
<title id=" W11-0218.xml">simsem fast approximate string matching in relation to semantic category disambiguation </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>calbc cii contains 75,000 documents, which is more than enough for our experiments.
</prevsent>
<prevsent>in order to maintain balance in size between the resources inour experiments, we sampled random 5,000 documents and used these as our calbc cii dataset.
</prevsent>
</prevsection>
<citsent citstr=" W11-1803 ">
5http://www.nactem.ac.uk/download.phptarget=grec/ event annotation guidelines.pdf 139 name abbreviation publication bionlp/nlpba 2004 shared task corpus nlpba kim et al (2004) gene regulation event corpus grec thompson et al (2009) collaborative annotation of large biomedical corpus calbc cii rebholz-schuhmann et al (2010) epi genetics and post-translational modifications epi ohta et al (2011) <papid> W11-1803 </papid>infectious diseases corpus id pyysalo et al (2011) <papid> W11-1804 </papid>genia event corpus genia kim et al (2011) <papid> W11-1802 </papid>table 4: corpora used for evaluation 3.3 corpus statistics.</citsent>
<aftsection>
<nextsent>in this section we present statistics for each of ourdatasets.
</nextsent>
<nextsent>for resources with limited number of semantic categories we use pie charts to illustrate their distribution (figure 1).
</nextsent>
<nextsent>for the other corpora we use tables to illustrate this.
</nextsent>
<nextsent>tables for the corpora for which pie charts are given has been left out due to space requirements.the nlpba corpus (figure 1a) with 59,601 tokens annotated, covers five semantic categories, with clear majority of protein annotations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3569">
<title id=" W11-0218.xml">simsem fast approximate string matching in relation to semantic category disambiguation </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>calbc cii contains 75,000 documents, which is more than enough for our experiments.
</prevsent>
<prevsent>in order to maintain balance in size between the resources inour experiments, we sampled random 5,000 documents and used these as our calbc cii dataset.
</prevsent>
</prevsection>
<citsent citstr=" W11-1804 ">
5http://www.nactem.ac.uk/download.phptarget=grec/ event annotation guidelines.pdf 139 name abbreviation publication bionlp/nlpba 2004 shared task corpus nlpba kim et al (2004) gene regulation event corpus grec thompson et al (2009) collaborative annotation of large biomedical corpus calbc cii rebholz-schuhmann et al (2010) epi genetics and post-translational modifications epi ohta et al (2011) <papid> W11-1803 </papid>infectious diseases corpus id pyysalo et al (2011) <papid> W11-1804 </papid>genia event corpus genia kim et al (2011) <papid> W11-1802 </papid>table 4: corpora used for evaluation 3.3 corpus statistics.</citsent>
<aftsection>
<nextsent>in this section we present statistics for each of ourdatasets.
</nextsent>
<nextsent>for resources with limited number of semantic categories we use pie charts to illustrate their distribution (figure 1).
</nextsent>
<nextsent>for the other corpora we use tables to illustrate this.
</nextsent>
<nextsent>tables for the corpora for which pie charts are given has been left out due to space requirements.the nlpba corpus (figure 1a) with 59,601 tokens annotated, covers five semantic categories, with clear majority of protein annotations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3570">
<title id=" W11-0218.xml">simsem fast approximate string matching in relation to semantic category disambiguation </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>calbc cii contains 75,000 documents, which is more than enough for our experiments.
</prevsent>
<prevsent>in order to maintain balance in size between the resources inour experiments, we sampled random 5,000 documents and used these as our calbc cii dataset.
</prevsent>
</prevsection>
<citsent citstr=" W11-1802 ">
5http://www.nactem.ac.uk/download.phptarget=grec/ event annotation guidelines.pdf 139 name abbreviation publication bionlp/nlpba 2004 shared task corpus nlpba kim et al (2004) gene regulation event corpus grec thompson et al (2009) collaborative annotation of large biomedical corpus calbc cii rebholz-schuhmann et al (2010) epi genetics and post-translational modifications epi ohta et al (2011) <papid> W11-1803 </papid>infectious diseases corpus id pyysalo et al (2011) <papid> W11-1804 </papid>genia event corpus genia kim et al (2011) <papid> W11-1802 </papid>table 4: corpora used for evaluation 3.3 corpus statistics.</citsent>
<aftsection>
<nextsent>in this section we present statistics for each of ourdatasets.
</nextsent>
<nextsent>for resources with limited number of semantic categories we use pie charts to illustrate their distribution (figure 1).
</nextsent>
<nextsent>for the other corpora we use tables to illustrate this.
</nextsent>
<nextsent>tables for the corpora for which pie charts are given has been left out due to space requirements.the nlpba corpus (figure 1a) with 59,601 tokens annotated, covers five semantic categories, with clear majority of protein annotations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3571">
<title id=" W10-4216.xml">feature selection for fluency ranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that the often-used frequency-based selection performs badly compared to maximum entropy feature selection, and that models with afew hundred well-picked features are competitive to models with no feature selection applied.
</prevsent>
<prevsent>in the experiments described in this paper, we compressed model of approximately 490.000 features to 1.000 features.
</prevsent>
</prevsection>
<citsent citstr=" W05-1510 ">
as shown previously, maximum entropy model shave proven to be viable for fluency ranking (nakan ishi et al, 2005; <papid> W05-1510 </papid>velldal and oepen, 2006; <papid> W06-1661 </papid>velldal, 2008).</citsent>
<aftsection>
<nextsent>the basic principle of maximum entropy models is to minimize assumptions, while imposing constraints such that the expected feature valueis equal to the observed feature value in the training data.
</nextsent>
<nextsent>in its canonical form, the probability of acer tain event (y) occurring in the context (x) is loglinear combination of features and feature weights,wherez(x) is normalization over all events in context (berger et al, 1996): <papid> J96-1002 </papid>p(y|x) = 1 z(x) exp n?</nextsent>
<nextsent>i=1 ifi (1) the training process estimates optimal feature weights, given the constraints and the principle of maximum entropy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3572">
<title id=" W10-4216.xml">feature selection for fluency ranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that the often-used frequency-based selection performs badly compared to maximum entropy feature selection, and that models with afew hundred well-picked features are competitive to models with no feature selection applied.
</prevsent>
<prevsent>in the experiments described in this paper, we compressed model of approximately 490.000 features to 1.000 features.
</prevsent>
</prevsection>
<citsent citstr=" W06-1661 ">
as shown previously, maximum entropy model shave proven to be viable for fluency ranking (nakan ishi et al, 2005; <papid> W05-1510 </papid>velldal and oepen, 2006; <papid> W06-1661 </papid>velldal, 2008).</citsent>
<aftsection>
<nextsent>the basic principle of maximum entropy models is to minimize assumptions, while imposing constraints such that the expected feature valueis equal to the observed feature value in the training data.
</nextsent>
<nextsent>in its canonical form, the probability of acer tain event (y) occurring in the context (x) is loglinear combination of features and feature weights,wherez(x) is normalization over all events in context (berger et al, 1996): <papid> J96-1002 </papid>p(y|x) = 1 z(x) exp n?</nextsent>
<nextsent>i=1 ifi (1) the training process estimates optimal feature weights, given the constraints and the principle of maximum entropy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3573">
<title id=" W10-4216.xml">feature selection for fluency ranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as shown previously, maximum entropy model shave proven to be viable for fluency ranking (nakan ishi et al, 2005; <papid> W05-1510 </papid>velldal and oepen, 2006; <papid> W06-1661 </papid>velldal, 2008).</prevsent>
<prevsent>the basic principle of maximum entropy models is to minimize assumptions, while imposing constraints such that the expected feature valueis equal to the observed feature value in the training data.</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
in its canonical form, the probability of acer tain event (y) occurring in the context (x) is loglinear combination of features and feature weights,wherez(x) is normalization over all events in context (berger et al, 1996): <papid> J96-1002 </papid>p(y|x) = 1 z(x) exp n?</citsent>
<aftsection>
<nextsent>i=1 ifi (1) the training process estimates optimal feature weights, given the constraints and the principle of maximum entropy.
</nextsent>
<nextsent>in fluency ranking the input (e.g.a dependency structure) is context, and realization of that input is an event within that context.features can be hand-crafted or generated automatically using very general feature templates.
</nextsent>
<nextsent>for example, if we apply template rule that enumerates the rules used to construct derivation tree to the partial tree in figure 1 the rule(max xp(np)) and rule(np det n) features will be created.
</nextsent>
<nextsent>figure 1: partial derivation tree for the noun phrase de adviezen (the advices).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3577">
<title id=" W10-4216.xml">feature selection for fluency ranking </title>
<section> feature selection.  </section>
<citcontext>
<prevsection>
<prevsent>however, optimizing the weights of the features inmodel ps,f for every candidate feature is computationally intractable.
</prevsent>
<prevsent>as simplification, it is assumed that the weights of features that are already in the model are not affected by the addition of feature . as result, the optimal weight ? of can be found using simple line search method.
</prevsent>
</prevsection>
<citsent citstr=" W03-1020 ">
however, as zhou et al (2003) <papid> W03-1020 </papid>note, there is still an inefficiency in that the weight of every candidate feature is recalculated during every selection step.they observe that gains of remaining candidate features rarely increase as the result of adding fea ture.</citsent>
<aftsection>
<nextsent>if it is assumed that this never happens, list of candidate features ordered by gain can be kept.
</nextsent>
<nextsent>to account for the fact that the topmost feature in that list may have lost its effectiveness as the result of previous addition of feature to the model, thegain of the topmost feature is recalculated and reinserted into the list according to its new gain.
</nextsent>
<nextsent>when the topmost feature retains its position, it is selected and added to the model.
</nextsent>
<nextsent>since we use feature selection with features that are not binary, and for ranking task, we modified the recursive forms of the model to: sumsf (y|x) = sums(y|x) ? f(y) (3) zsf (x) = zs(x)?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3578">
<title id=" W10-4216.xml">feature selection for fluency ranking </title>
<section> feature selection.  </section>
<citcontext>
<prevsection>
<prevsent>y sums(y|x) + ? sumsf (y|x) (4) another issue that needs to be dealt with is the calculation of context and event probabilities.
</prevsent>
<prevsent>in the literature two approaches are prevalent.
</prevsent>
</prevsection>
<citsent citstr=" C00-1085 ">
the first approach divides the probability mass uniformly over contexts, and the probability of events within context is proportional to the event score (osborne, 2000): <papid> C00-1085 </papid>p(x) = 1 |x| (5) p(y|x) = p(x) ( score(x,y)p score(x,y) ) (6)where |x| is the number of contexts.</citsent>
<aftsection>
<nextsent>the second approach puts more emphasis on the contexts that contain relatively many events with high scores, by making the context probability dependent on the scores of events within that context (malouf and van noord, 2004): p(x) = ? score(x, y) ? yx score(x, y) (7)in our experiments, the second definition of context probability outperformed the first by such awide margin, that we only used the second definition in the experiments described in this paper.
</nextsent>
<nextsent>2.6 note on overlap detection although maximum-entropy based feature-selection may be worthwhile in itself, the technique can alsobe used during feature engineering to find overlapping features.
</nextsent>
<nextsent>in the selection method of berger etal.
</nextsent>
<nextsent>(1996), the weight and gain of each candidate feature is re-estimated during each selection step.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3579">
<title id=" W10-4216.xml">feature selection for fluency ranking </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>we then use feature templates to extract features from the derivation trees.
</prevsent>
<prevsent>two classes of features (and templates) can be distinguished output features that model the output of process and construction features that model the process that constructs the out put.
</prevsent>
</prevsection>
<citsent citstr=" A00-2021 ">
3.1.1 output features currently, there are two output features, both representing auxiliary distributions (johnson and riezler, 2000): <papid> A00-2021 </papid>word trigram model and part-of speech trigram model.</citsent>
<aftsection>
<nextsent>the part-of-speech tag set consists of the alpino part of speech tags.
</nextsent>
<nextsent>both models are trained on newspaper articles, consisting of 110 million words, from the twente nieuws corpus1.
</nextsent>
<nextsent>the probability of unknown trigrams is estimated using linear interpolation smoothing (brants, 2000).
</nextsent>
<nextsent>unknown word probabilities are determined with laplacian smoothing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3580">
<title id=" W10-4216.xml">feature selection for fluency ranking </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>the resulting derivation trees, including attribute-value structures associated with each node, are compressed and stored in derivation treebank.
</prevsent>
<prevsent>training and testing data was then obtained by extracting features from derivation trees stored in the derivation treebank.
</prevsent>
</prevsection>
<citsent citstr=" N03-2021 ">
at this time, the realizations are also scored using the general text matcher method (gtm) (melamed et al, 2003), <papid> N03-2021 </papid>by comparing them to the original sentence.</citsent>
<aftsection>
<nextsent>we have previously experimented with rouge-n scores, which gave rise to similar results.
</nextsent>
<nextsent>however, it is shown that gtm shows the highest correlation with human judgments (cahill, 2009).<papid> P09-2025 </papid></nextsent>
<nextsent>3.3 methodology.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3581">
<title id=" W10-4216.xml">feature selection for fluency ranking </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>at this time, the realizations are also scored using the general text matcher method (gtm) (melamed et al, 2003), <papid> N03-2021 </papid>by comparing them to the original sentence.</prevsent>
<prevsent>we have previously experimented with rouge-n scores, which gave rise to similar results.</prevsent>
</prevsection>
<citsent citstr=" P09-2025 ">
however, it is shown that gtm shows the highest correlation with human judgments (cahill, 2009).<papid> P09-2025 </papid></citsent>
<aftsection>
<nextsent>3.3 methodology.
</nextsent>
<nextsent>to evaluate the feature selection methods, we first train models for each selection method in three steps: 1.
</nextsent>
<nextsent>for each abstract dependency structure in the training data 100 realizations (and corresponding features) are randomly selected.
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3582">
<title id=" W11-0417.xml">empty categories in hindi dependency treebank analysis and recovery </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this we make use of lexical knowledge along with the parsed output from constraint based parser.through this work we show that it is possible to successfully discover certain types of empty categories while some other types are more difficult to identify.
</prevsent>
<prevsent>this work leads tothe state-of-the-art system for automatic insertion of empty categories in the hindi sentence.
</prevsent>
</prevsection>
<citsent citstr=" I08-2099 ">
empty categories play crucial role in the annotation framework of the hindi dependency treebank1 (begum et al, 2008; <papid> I08-2099 </papid>bharati et al, 2009<papid> W09-3812 </papid>b).</citsent>
<aftsection>
<nextsent>they are inserted in sentence in case the dependency analysis does not lead to fully connected tree.
</nextsent>
<nextsent>in the hindi treebank, an empty category (denoted by null node) always has at least one child.
</nextsent>
<nextsent>these elements have essentially the same properties (e.g.case-marking, agreement, etc.) as an overtly realized element and they provide valuable information(such as predicate-argument structure, etc.).
</nextsent>
<nextsent>a different kind of motivation for postulating empty categories comes from the demands of natural lan-guage processing, in particular parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3583">
<title id=" W11-0417.xml">empty categories in hindi dependency treebank analysis and recovery </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this we make use of lexical knowledge along with the parsed output from constraint based parser.through this work we show that it is possible to successfully discover certain types of empty categories while some other types are more difficult to identify.
</prevsent>
<prevsent>this work leads tothe state-of-the-art system for automatic insertion of empty categories in the hindi sentence.
</prevsent>
</prevsection>
<citsent citstr=" W09-3812 ">
empty categories play crucial role in the annotation framework of the hindi dependency treebank1 (begum et al, 2008; <papid> I08-2099 </papid>bharati et al, 2009<papid> W09-3812 </papid>b).</citsent>
<aftsection>
<nextsent>they are inserted in sentence in case the dependency analysis does not lead to fully connected tree.
</nextsent>
<nextsent>in the hindi treebank, an empty category (denoted by null node) always has at least one child.
</nextsent>
<nextsent>these elements have essentially the same properties (e.g.case-marking, agreement, etc.) as an overtly realized element and they provide valuable information(such as predicate-argument structure, etc.).
</nextsent>
<nextsent>a different kind of motivation for postulating empty categories comes from the demands of natural lan-guage processing, in particular parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3587">
<title id=" W11-0417.xml">empty categories in hindi dependency treebank analysis and recovery </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical parsers like malt parser (nivre, 2003), mst parser (mcdonald, 2005), as well as constraint based hybrid parser (cbhp) (bharati et al, 2009<papid> W09-3812 </papid>a) produce incorrect parse trees once the empty categories are removed from the input data.</prevsent>
<prevsent>hence there is need for automatic detection and insertion of empty categories inthe hindi data.</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
additionally, it is evident that successful detection of such nodes will help the annotation process as well.there have been many approaches for the recovery of empty categories in the treebanks like penn treebank, both ml based (collins, 1997; <papid> P97-1003 </papid>johnson, 2002; dienes and dubey, 2003<papid> W03-1005 </papid>a,b; higgins, 2003)<papid> E03-1049 </papid>and rule based (r campbell, 2004).</citsent>
<aftsection>
<nextsent>some approaches such as yang and xue (2010) <papid> C10-2158 </papid>follow postprocessing step of recovering empty categories after parsing the text.</nextsent>
<nextsent>in this paper we make use of lexical knowledge along with the parsed output from constraint based parser to successfully insert empty category in the input sentence, which may further be given for parsing or other applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3589">
<title id=" W11-0417.xml">empty categories in hindi dependency treebank analysis and recovery </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical parsers like malt parser (nivre, 2003), mst parser (mcdonald, 2005), as well as constraint based hybrid parser (cbhp) (bharati et al, 2009<papid> W09-3812 </papid>a) produce incorrect parse trees once the empty categories are removed from the input data.</prevsent>
<prevsent>hence there is need for automatic detection and insertion of empty categories inthe hindi data.</prevsent>
</prevsection>
<citsent citstr=" W03-1005 ">
additionally, it is evident that successful detection of such nodes will help the annotation process as well.there have been many approaches for the recovery of empty categories in the treebanks like penn treebank, both ml based (collins, 1997; <papid> P97-1003 </papid>johnson, 2002; dienes and dubey, 2003<papid> W03-1005 </papid>a,b; higgins, 2003)<papid> E03-1049 </papid>and rule based (r campbell, 2004).</citsent>
<aftsection>
<nextsent>some approaches such as yang and xue (2010) <papid> C10-2158 </papid>follow postprocessing step of recovering empty categories after parsing the text.</nextsent>
<nextsent>in this paper we make use of lexical knowledge along with the parsed output from constraint based parser to successfully insert empty category in the input sentence, which may further be given for parsing or other applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3591">
<title id=" W11-0417.xml">empty categories in hindi dependency treebank analysis and recovery </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical parsers like malt parser (nivre, 2003), mst parser (mcdonald, 2005), as well as constraint based hybrid parser (cbhp) (bharati et al, 2009<papid> W09-3812 </papid>a) produce incorrect parse trees once the empty categories are removed from the input data.</prevsent>
<prevsent>hence there is need for automatic detection and insertion of empty categories inthe hindi data.</prevsent>
</prevsection>
<citsent citstr=" E03-1049 ">
additionally, it is evident that successful detection of such nodes will help the annotation process as well.there have been many approaches for the recovery of empty categories in the treebanks like penn treebank, both ml based (collins, 1997; <papid> P97-1003 </papid>johnson, 2002; dienes and dubey, 2003<papid> W03-1005 </papid>a,b; higgins, 2003)<papid> E03-1049 </papid>and rule based (r campbell, 2004).</citsent>
<aftsection>
<nextsent>some approaches such as yang and xue (2010) <papid> C10-2158 </papid>follow postprocessing step of recovering empty categories after parsing the text.</nextsent>
<nextsent>in this paper we make use of lexical knowledge along with the parsed output from constraint based parser to successfully insert empty category in the input sentence, which may further be given for parsing or other applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3592">
<title id=" W11-0417.xml">empty categories in hindi dependency treebank analysis and recovery </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence there is need for automatic detection and insertion of empty categories inthe hindi data.
</prevsent>
<prevsent>additionally, it is evident that successful detection of such nodes will help the annotation process as well.there have been many approaches for the recovery of empty categories in the treebanks like penn treebank, both ml based (collins, 1997; <papid> P97-1003 </papid>johnson, 2002; dienes and dubey, 2003<papid> W03-1005 </papid>a,b; higgins, 2003)<papid> E03-1049 </papid>and rule based (r campbell, 2004).</prevsent>
</prevsection>
<citsent citstr=" C10-2158 ">
some approaches such as yang and xue (2010) <papid> C10-2158 </papid>follow postprocessing step of recovering empty categories after parsing the text.</citsent>
<aftsection>
<nextsent>in this paper we make use of lexical knowledge along with the parsed output from constraint based parser to successfully insert empty category in the input sentence, which may further be given for parsing or other applications.
</nextsent>
<nextsent>throughout this paper, we use the term recovery (of empty categories) for the insertion of different types of empty categories into the input sentence.the paper is arranged as follows, section 2 discusses the empty nodes in the treebank and classifies 134 null np tokens 69 null vg tokens 68 null ccp tokens 32 sentences with more than one empty category in them 159 table 1: empty categories in hindi tree bank them based on their syntactic type.
</nextsent>
<nextsent>in section 3 we provide an algorithm to automatically recover these elements.
</nextsent>
<nextsent>section 4 shows the performance of our system and discusses the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3594">
<title id=" W11-0417.xml">empty categories in hindi dependency treebank analysis and recovery </title>
<section> an overview of empty categories in.  </section>
<citcontext>
<prevsection>
<prevsent>hindi dependency treebank begum et al, (2008) <papid> I08-2099 </papid>proposed dependency framework in which an empty node is introduced during the annotation process only if its presence is required to build the dependency tree for the sentence (figures 1, 2, 3) 2.</prevsent>
<prevsent>empty categories such as those discussed in bhatia et al (2010) which would be leaf nodes in the dependency tree are not part of the dependency structure and are added during propbanking3.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
consequently, the empty categories in hindi treebank do not mark displacement as inpenn treebank (marcus et al, 1993) <papid> J93-2004 </papid>rather, they represent un displaced syntactic elements which happen to lack phonological realization.</citsent>
<aftsection>
<nextsent>in the hindi dependency treebank, an empty category is represented by null?
</nextsent>
<nextsent>word.
</nextsent>
<nextsent>sentences can have missing vg or np or ccp 4.
</nextsent>
<nextsent>these are represented by null token and are marked with the appropriate part-of speech tag along with marking the chunk tag suchas null np, null vgf, null ccp, etc. in table 2 2due to space constraints, sentences in all the figures only show chunk heads.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3597">
<title id=" W11-0125.xml">incremental dialogue act understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>of these tasks, lexical analysis, being concerned with local information at word level, can be done for each word as soon as it has been recognized, and is naturally performed as an incremental part of utterance processing, but syntactic, semantic and pragmatic analysis are traditionally performed on complete utterances.
</prevsent>
<prevsent>tomitas pioneering work in left-to-right syntactic parsing has shown that incremental parsing can be much more efficient and of equal quality as the parsing of complete utterances (tomita (1986)).
</prevsent>
</prevsection>
<citsent citstr=" P85-1008 ">
computational approaches to incremental semantic and pragmatic interpretation have 235 been less successful (see e.g. haddock (1989); milward and cooper (2009)), but work in computational semantics on the design of underspecified representation formalisms has shown that such formalisms, developed originally for the underspecified representation of quantifier scopes, can also be applied in situations where incomplete input information is available (see e.g. bos (2002); bunt (2007), hobbs (1985), <papid> P85-1008 </papid>pinkal (1999)) and as such hold promise for incremental semantic interpretation.</citsent>
<aftsection>
<nextsent>pragmatic interpretation, in particular the recognition of speakers intentions in incoming dialogue utterances, is another major aspect of language understanding for dialogue systems.
</nextsent>
<nextsent>computational modelling of dialogue behaviour in terms of dialogue acts aims to capture speaker intentions in the communicative functions of dialogue acts, and offers an effective integration with semantic content analysis through the information state update approach (poesio and traum (1998)).
</nextsent>
<nextsent>in this approach, dialogue act is viewed as having as its main components communicative function and semantic content, where the semantic content is the referential, propositional, or action-related information that the dialogue act addresses, and the communicative function defines how an understand ers information state is to be updated with that information.
</nextsent>
<nextsent>evaluation of non-incremental dialogue system and its incremental counterpart reported in aist et al (2007) showed that the latter is faster overall than the former due to the incorporation of pragmatic information in early stages of the understanding process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3598">
<title id=" W11-0125.xml">incremental dialogue act understanding </title>
<section> incremental understanding experiments.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 concludes.
</prevsent>
<prevsent>2.1 related work.
</prevsent>
</prevsection>
<citsent citstr=" P99-1026 ">
nakano et al (nakano et al (1999)) <papid> P99-1026 </papid>proposed method for the incremental understanding of utterances whose boundaries are not known.</citsent>
<aftsection>
<nextsent>the incremental sentence sequence search (isss) algorithm finds plausible boundaries of utterances, called significant utterances (sus), which can be full sentence or sub sentential phrase, such as noun phrase or verb phrase.
</nextsent>
<nextsent>any phrase that can change the belief state is defined as su.
</nextsent>
<nextsent>in this sense an su corresponds more or less with what we call functional segment?, which is defined as minimal stretch of behaviour that has communicative function (see bunt et al (2010)).
</nextsent>
<nextsent>isss maintains multiple possible belief states, and updates these each time word hypothesis is input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3599">
<title id=" W11-1707.xml">detecting implicit expressions of sentiment in text based on commonsense knowledge </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate that by using this resource, we are 53 able to detect emotion from textual contexts in which no explicit mention of affect is present.
</prevsent>
<prevsent>in artificial intelligence (ai), the term affective computing was first introduced by picard (1995).
</prevsent>
</prevsection>
<citsent citstr=" W03-0404 ">
previous approaches to spot affect in text include the use of models simulating human reactions according to their needs and desires (dyer, 1987), fuzzy logic (subasic and huettner, 2000), lexical affinity based on similarity of contexts ? the basis for the construction of wordnet affect (strapparava and valitutti, 2004) or sentiword net (esuli and sebastiani, 2005), detection of affective keywords (riloff et al, 2003) <papid> W03-0404 </papid>and machine learning using term frequency (pang et al., 2002; <papid> W02-1011 </papid>riloff and wiebe, 2003), <papid> W03-1014 </papid>or term discrimination (danisman and alpkocak, 2008).</citsent>
<aftsection>
<nextsent>other proposed methods include the creation of syntactic patterns and rules for cause-effect modeling (mei lee et al, 2009).
</nextsent>
<nextsent>significantly different proposals for emotion detection in text are given in the work by (liu et al 2003) and the recently proposed framework of sentic computing (cambria et al, 2009), whose scope is to model affective reaction based on commonsense knowledge.
</nextsent>
<nextsent>for survey on the affect models and their affective computing applications, see (calvo and dmello, 2010).
</nextsent>
<nextsent>the tasks of emotion detection and sentiment analysis have been approached by large volume of research in nlp . nevertheless, most of this research has concentrated on developing methods for detecting only explicit mentions of sentiment in text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3600">
<title id=" W11-1707.xml">detecting implicit expressions of sentiment in text based on commonsense knowledge </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate that by using this resource, we are 53 able to detect emotion from textual contexts in which no explicit mention of affect is present.
</prevsent>
<prevsent>in artificial intelligence (ai), the term affective computing was first introduced by picard (1995).
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
previous approaches to spot affect in text include the use of models simulating human reactions according to their needs and desires (dyer, 1987), fuzzy logic (subasic and huettner, 2000), lexical affinity based on similarity of contexts ? the basis for the construction of wordnet affect (strapparava and valitutti, 2004) or sentiword net (esuli and sebastiani, 2005), detection of affective keywords (riloff et al, 2003) <papid> W03-0404 </papid>and machine learning using term frequency (pang et al., 2002; <papid> W02-1011 </papid>riloff and wiebe, 2003), <papid> W03-1014 </papid>or term discrimination (danisman and alpkocak, 2008).</citsent>
<aftsection>
<nextsent>other proposed methods include the creation of syntactic patterns and rules for cause-effect modeling (mei lee et al, 2009).
</nextsent>
<nextsent>significantly different proposals for emotion detection in text are given in the work by (liu et al 2003) and the recently proposed framework of sentic computing (cambria et al, 2009), whose scope is to model affective reaction based on commonsense knowledge.
</nextsent>
<nextsent>for survey on the affect models and their affective computing applications, see (calvo and dmello, 2010).
</nextsent>
<nextsent>the tasks of emotion detection and sentiment analysis have been approached by large volume of research in nlp . nevertheless, most of this research has concentrated on developing methods for detecting only explicit mentions of sentiment in text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3601">
<title id=" W11-1707.xml">detecting implicit expressions of sentiment in text based on commonsense knowledge </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate that by using this resource, we are 53 able to detect emotion from textual contexts in which no explicit mention of affect is present.
</prevsent>
<prevsent>in artificial intelligence (ai), the term affective computing was first introduced by picard (1995).
</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
previous approaches to spot affect in text include the use of models simulating human reactions according to their needs and desires (dyer, 1987), fuzzy logic (subasic and huettner, 2000), lexical affinity based on similarity of contexts ? the basis for the construction of wordnet affect (strapparava and valitutti, 2004) or sentiword net (esuli and sebastiani, 2005), detection of affective keywords (riloff et al, 2003) <papid> W03-0404 </papid>and machine learning using term frequency (pang et al., 2002; <papid> W02-1011 </papid>riloff and wiebe, 2003), <papid> W03-1014 </papid>or term discrimination (danisman and alpkocak, 2008).</citsent>
<aftsection>
<nextsent>other proposed methods include the creation of syntactic patterns and rules for cause-effect modeling (mei lee et al, 2009).
</nextsent>
<nextsent>significantly different proposals for emotion detection in text are given in the work by (liu et al 2003) and the recently proposed framework of sentic computing (cambria et al, 2009), whose scope is to model affective reaction based on commonsense knowledge.
</nextsent>
<nextsent>for survey on the affect models and their affective computing applications, see (calvo and dmello, 2010).
</nextsent>
<nextsent>the tasks of emotion detection and sentiment analysis have been approached by large volume of research in nlp . nevertheless, most of this research has concentrated on developing methods for detecting only explicit mentions of sentiment in text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3602">
<title id=" W11-1707.xml">detecting implicit expressions of sentiment in text based on commonsense knowledge </title>
<section> building knowledge base for.  </section>
<citcontext>
<prevsection>
<prevsent>3.
</prevsent>
<prevsent>the expansion of the ontology using.
</prevsent>
</prevsection>
<citsent citstr=" W04-3205 ">
existing commonsense knowledge bases ? concept net (liu and singh, 2004) and other resources ? verb ocean (chklovski and pantel, 2004).<papid> W04-3205 </papid></citsent>
<aftsection>
<nextsent>5.1 design of the ontology.
</nextsent>
<nextsent>as mentioned before, the process of building the core of the emotinet knowledge base (kb) of action chains started with the design of the core ontology, whose design process was specifically divided in three stages: 1.
</nextsent>
<nextsent>establishing the scope and purpose of the.
</nextsent>
<nextsent>ontology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3603">
<title id=" W11-0611.xml">colourful language measuring word colour associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, we showhow word colour associations manifest themselves in language, and quantify usefulness ofco-occurrence and polarity cues in automatically detecting colour associations.1
</prevsent>
<prevsent>colour is vital component in the successful delivery of information, whether it is in marketing commercial product (sable and akcay, 2010), designing web pages (meier, 1988; pribadi et al, 1990), or visualizing information (christ, 1975; card et al, 1999).
</prevsent>
</prevsection>
<citsent citstr=" P11-2064 ">
since real-world concepts have associations with certain colour categories (for example, danger with red, and softness with pink), complimenting linguistic and non-linguistic information with appropriate colours has number of benefits, including: 1this paper is an extended, non-archival, version of the short paper mohammad (2011).<papid> P11-2064 </papid></citsent>
<aftsection>
<nextsent>it provides additional details on the analysis of crowd sourced data, and experiments on the manifestations of word colour associations in wordnet and in text.
</nextsent>
<nextsent>it also proposes polarity-based automatic method.
</nextsent>
<nextsent>(1) strengthening the message (improving semantic coherence), (2) easing cognitive load on the receiver, (3) conveying the message quickly, and (4) evoking the desired emotional response.
</nextsent>
<nextsent>consider, for example, the use of red in stop signs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3604">
<title id=" W11-0611.xml">colourful language measuring word colour associations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(see also the online study by joe hallock4.)
</prevsent>
<prevsent>in this work, we are interested in identifying words that have strong association with colour due to their meaning; associations that are not affected by age and gender preferences.there is substantial work on inferring the emotions evoked by colour (luscher, 1969; xin et al, 2004; kaya, 2004).
</prevsent>
</prevsection>
<citsent citstr=" W10-3405 ">
strapparava and ozbal (2010) <papid> W10-3405 </papid>compute corpus-based semantic similarity between emotions and colours.</citsent>
<aftsection>
<nextsent>we combine the word colour and word emotion association lexicons to determine the correlation between emotion-associated words and colours.
</nextsent>
<nextsent>berlin and kay (1969), and later kay and maffi (1999), showed that often colour terms appeared in languages in certain groups.
</nextsent>
<nextsent>if language has only two colour terms, then they are white and black.
</nextsent>
<nextsent>if language has three colour terms, then they are white, black, and red.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3605">
<title id=" W11-0611.xml">colourful language measuring word colour associations </title>
<section> crowdsourcing.  </section>
<citcontext>
<prevsection>
<prevsent>it is scale from 100 (very hard to visualize) to 700 (very easy to visualize).
</prevsent>
<prevsent>we use the ratings in our experiments to determine whether there is correlation between image ability and strength of colour association.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
amazons mechanical turk (amt) is an online crowdsourcing platform that is especially well suited for tasks that can be done over the internet through computer or mobile device.7 it is already being used to obtain human annotation on various linguistic tasks (snow et al, 2008; <papid> D08-1027 </papid>callison-burch, 2009).however, one must define the task carefully to obtain annotations of high quality.</citsent>
<aftsection>
<nextsent>several checks mustbe placed to ensure that random and erroneous annotations are discouraged, rejected, and re-annotated.
</nextsent>
<nextsent>we used mechanical turk to obtain word colour association annotations on large-scale.
</nextsent>
<nextsent>each task is broken into small independently solvable units called hits (human intelligence tasks) and up loaded on the mechanical turk website.
</nextsent>
<nextsent>the people who provide responses to these hits are called turkers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3606">
<title id=" W11-0611.xml">colourful language measuring word colour associations </title>
<section> crowdsourcing.  </section>
<citcontext>
<prevsection>
<prevsent>the macquarie has about thousand categories, each having about hundred or so related terms.
</prevsent>
<prevsent>each category has head word that best represents the words in it.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
the categories can be thought of as coarse senses or concepts (yarowsky, 1992).<papid> C92-2070 </papid></citsent>
<aftsection>
<nextsent>if word is ambiguous, then it is listed in more than one category.
</nextsent>
<nextsent>since word may have different colour associations when used in different senses, we obtained annotations at word-sense level.
</nextsent>
<nextsent>we chose to annotate words that 6http://www.psy.uwa.edu.au/mrcdatabase/uwa mrc.htm 7mechanical turk: www.mturk.com had one to five senses in the macquarie thesaurus and occurred frequently in the google n-gram corpus.
</nextsent>
<nextsent>we annotated more than 10,000 of these word?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3607">
<title id=" W11-0611.xml">colourful language measuring word colour associations </title>
<section> the colour of emotion words.  </section>
<citcontext>
<prevsection>
<prevsent>image ability and colour association have pearsons product moment correlation of 0.116, and spearman rank order correlation of 0.102.
</prevsent>
<prevsent>emotions such as joy and anger are abstract concepts dealing with ones psychological state.
</prevsent>
</prevsection>
<citsent citstr=" W10-0204 ">
mohammad and turney (2010) <papid> W10-0204 </papid>created crowd sourced term emotion association lexicon consisting of associations of over 10,000 word-sense pairs with eight emotions joy, sadness, anger, fear, trust, disgust, surprise, and anticipation argued to be the basic and prototypical emotions (plutchik, 1980).</citsent>
<aftsection>
<nextsent>we combine their term emotion association lexicon andour term colour lexicon to determine the colour signature of different emotions the rows in table 3.
</nextsent>
<nextsent>the top two most frequently associated colours with each of the eight emotions are shown in bold.
</nextsent>
<nextsent>for example, the anger?
</nextsent>
<nextsent>row shows the percentage of 101 white black red green yellow blue brown pink purple orange grey anger words 2.1 30.7 32.4 5.0 5.0 2.4 6.6 0.5 2.3 2.5 9.9 anticipation words 16.2 7.5 11.5 16.2 10.7 9.5 5.7 5.9 3.1 4.9 8.4 disgust words 2.0 33.7 24.9 4.8 5.5 1.9 9.7 1.1 1.8 3.5 10.5 fear words 4.5 31.8 25.0 3.5 6.9 3.0 6.1 1.3 2.3 3.3 11.8 joy words 21.8 2.2 7.4 14.1 13.4 11.3 3.1 11.1 6.3 5.8 2.8 sadness words 3.0 36.0 18.6 3.4 5.4 5.8 7.1 0.5 1.4 2.1 16.1 surprise words 11.0 13.4 21.0 8.3 13.5 5.2 3.4 5.2 4.1 5.6 8.8 trust words 22.0 6.3 8.4 14.2 8.3 14.4 5.9 5.5 4.9 3.8 5.8 table 3: colour signature of emotive terms: percentage of terms associated with each colour.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3611">
<title id=" W11-0611.xml">colourful language measuring word colour associations </title>
<section> manifestation of concept colour.  </section>
<citcontext>
<prevsection>
<prevsent>it is plausible that if concept is strongly associated with certain colour, then such concept colour pairs will be close to each other in semantic network such as wordnet.
</prevsent>
<prevsent>if so, the semantic closeness of word with each of the eleven basic colours in wordnet can be used to automatically determine the colour most associated with the 177 thesaurus categories from the gold standard described in section 5 earlier.
</prevsent>
</prevsection>
<citsent citstr=" P97-1009 ">
we determine closeness using two similarity measures jiang and conrath (1997) and lin (1997)<papid> P97-1009 </papid>and two relatedness measureslesk (banerjee and pedersen, 2003) and gloss vector overlap (pedersen et al, 2004)from the wordnet similarity package.</citsent>
<aftsection>
<nextsent>for each thesaurus category colour pair, we summed the wordnet closeness of each of the terms in the category to the colour.
</nextsent>
<nextsent>the colour with the highest sum is chosen as the one closest to the thesaurus category.
</nextsent>
<nextsent>section (c) and section (d) of table 8.2, show how often the closest colours are also the colours most associated with the gold standardcategories.
</nextsent>
<nextsent>section (a) lists some unsupervised baselines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3612">
<title id=" W11-0611.xml">colourful language measuring word colour associations </title>
<section> manifestation of concept colour.  </section>
<citcontext>
<prevsection>
<prevsent>from table 5 in section 7, we know that some 103 automatic method for choosing colour accuracy (a) unsupervised baselines: - randomly choosing colour 9.1 - most frequent colour in bnc (black) 23.2 - most frequent colour in gnc (black) 23.2 - most frequent colour in gbc (white) 33.3 (b) supervised baseline: - colour most often associated with categories (white) 33.3 (c) wordnet similarity measures: - jiang conrath measure 15.7 - lins measure 15.7 (d) wordnet relatedness measures: - lesk measure 24.7 - gloss vector measure 28.6 (e) co-occurrence in text: - p(colour|word) in bnc 31.4 - p(colour|word) in gnc 37.9 - p(colour|word) in gbc 38.3 (f) co-occurrence and polarity: - p(colour|word, polarity) in bnc 51.4 - p(colour|word, polarity) in gnc 47.6 - p(colour|word, polarity) in gbc 60.1table 7: percentage of times the colour chosen by automatic method is also the colour identified by annotators as most associated to thesaurus category.colours tend to be strongly positive and others negative.
</prevsent>
<prevsent>we wanted to determine how useful these polarity cues can be in identifying the colour most associated with category.
</prevsent>
</prevsection>
<citsent citstr=" D09-1063 ">
we used the automatically generated macquarie semantic orientation lexicon (msol) (mohammad et al, 2009) <papid> D09-1063 </papid>to determine if athesaurus category is positive or negative.10 category is marked as negative if it has more negative words than positive, otherwise it is marked as pos itive.</citsent>
<aftsection>
<nextsent>if category is positive, then co-occurrence cues were used to select colour from only the positive colours (white, green, yellow, blue, pink, and orange), whereas if category is negative, then co-occurrence cues select from only the negative colours (black, red, brown, and grey).
</nextsent>
<nextsent>section (f) of table 8.2 provides results with this method.
</nextsent>
<nextsent>observe that these numbers are marked improvement over section (e) numbers, suggesting that polarity cues can be very useful in determining concept colour association.
</nextsent>
<nextsent>10msol is available at http://www.umiacs.umd.edu/saif/ webpages/researchinterests.html#semanticorientation.counts from the gnc yielded poorer results compared to the much smaller bnc, and the somewhat smaller gbc possibly because frequency counts from gnc are available only for those n-grams that occur at least thirty times.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3613">
<title id=" W11-0812.xml">identifying and analyzing brazilian portuguese complex predicates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the remainder of this paper is organized as follows: in 2 we discuss related work, in 3 we present the corpus and the details about our methodology, in 4we present and discuss the resulting lists of candidates, in 5 we envisage further work and draw our conclusions.
</prevsent>
<prevsent>part of the cps focused on here are represented by lvcs and svcs.
</prevsent>
</prevsection>
<citsent citstr=" C90-3043 ">
these cps have been studied in several languages from different points of view: di acronic (ranchhod, 1999; marchello-nizia, 1996), language contrastive (danlos and samvel ian, 1992; athayde, 2001), descriptive (butt, 2003; langer, 2004; langer, 2005) and for nlp purposes (salkoff, 1990; <papid> C90-3043 </papid>stevenson et al, 2004; <papid> W04-0401 </papid>barre iro and cabral, 2009; hwang et al, 2010).<papid> W10-1810 </papid></citsent>
<aftsection>
<nextsent>closer to our study, hendrickx et al (2010) <papid> W10-1812 </papid>annotated treebank of 1m tokens of european portuguese with almost 2,000 cps, which include lvcs and verbal chains.</nextsent>
<nextsent>this lexicon is relevant for many nlp applications, notably for automatic translation, since in any task involving language generation they confer fluency and naturalness to the output of the system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3614">
<title id=" W11-0812.xml">identifying and analyzing brazilian portuguese complex predicates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the remainder of this paper is organized as follows: in 2 we discuss related work, in 3 we present the corpus and the details about our methodology, in 4we present and discuss the resulting lists of candidates, in 5 we envisage further work and draw our conclusions.
</prevsent>
<prevsent>part of the cps focused on here are represented by lvcs and svcs.
</prevsent>
</prevsection>
<citsent citstr=" W04-0401 ">
these cps have been studied in several languages from different points of view: di acronic (ranchhod, 1999; marchello-nizia, 1996), language contrastive (danlos and samvel ian, 1992; athayde, 2001), descriptive (butt, 2003; langer, 2004; langer, 2005) and for nlp purposes (salkoff, 1990; <papid> C90-3043 </papid>stevenson et al, 2004; <papid> W04-0401 </papid>barre iro and cabral, 2009; hwang et al, 2010).<papid> W10-1810 </papid></citsent>
<aftsection>
<nextsent>closer to our study, hendrickx et al (2010) <papid> W10-1812 </papid>annotated treebank of 1m tokens of european portuguese with almost 2,000 cps, which include lvcs and verbal chains.</nextsent>
<nextsent>this lexicon is relevant for many nlp applications, notably for automatic translation, since in any task involving language generation they confer fluency and naturalness to the output of the system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3615">
<title id=" W11-0812.xml">identifying and analyzing brazilian portuguese complex predicates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the remainder of this paper is organized as follows: in 2 we discuss related work, in 3 we present the corpus and the details about our methodology, in 4we present and discuss the resulting lists of candidates, in 5 we envisage further work and draw our conclusions.
</prevsent>
<prevsent>part of the cps focused on here are represented by lvcs and svcs.
</prevsent>
</prevsection>
<citsent citstr=" W10-1810 ">
these cps have been studied in several languages from different points of view: di acronic (ranchhod, 1999; marchello-nizia, 1996), language contrastive (danlos and samvel ian, 1992; athayde, 2001), descriptive (butt, 2003; langer, 2004; langer, 2005) and for nlp purposes (salkoff, 1990; <papid> C90-3043 </papid>stevenson et al, 2004; <papid> W04-0401 </papid>barre iro and cabral, 2009; hwang et al, 2010).<papid> W10-1810 </papid></citsent>
<aftsection>
<nextsent>closer to our study, hendrickx et al (2010) <papid> W10-1812 </papid>annotated treebank of 1m tokens of european portuguese with almost 2,000 cps, which include lvcs and verbal chains.</nextsent>
<nextsent>this lexicon is relevant for many nlp applications, notably for automatic translation, since in any task involving language generation they confer fluency and naturalness to the output of the system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3618">
<title id=" W11-0812.xml">identifying and analyzing brazilian portuguese complex predicates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>part of the cps focused on here are represented by lvcs and svcs.
</prevsent>
<prevsent>these cps have been studied in several languages from different points of view: di acronic (ranchhod, 1999; marchello-nizia, 1996), language contrastive (danlos and samvel ian, 1992; athayde, 2001), descriptive (butt, 2003; langer, 2004; langer, 2005) and for nlp purposes (salkoff, 1990; <papid> C90-3043 </papid>stevenson et al, 2004; <papid> W04-0401 </papid>barre iro and cabral, 2009; hwang et al, 2010).<papid> W10-1810 </papid></prevsent>
</prevsection>
<citsent citstr=" W10-1812 ">
closer to our study, hendrickx et al (2010) <papid> W10-1812 </papid>annotated treebank of 1m tokens of european portuguese with almost 2,000 cps, which include lvcs and verbal chains.</citsent>
<aftsection>
<nextsent>this lexicon is relevant for many nlp applications, notably for automatic translation, since in any task involving language generation they confer fluency and naturalness to the output of the system.
</nextsent>
<nextsent>work focusing on the automatic extraction oflvcs or svcs often take as starting point list of recurrent light verbs (hendrickx et al, 2010) <papid> W10-1812 </papid>or list of nominalizations (teufel and grefenstette, 1995; <papid> E95-1014 </papid>dras, 1995; hwang et al, 2010).<papid> W10-1810 </papid></nextsent>
<nextsent>these approaches are not adopted here because our goal is precisely to identify which are the verbs, the nouns and other lexical elements that take part in cps.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3621">
<title id=" W11-0812.xml">identifying and analyzing brazilian portuguese complex predicates </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>closer to our study, hendrickx et al (2010) <papid> W10-1812 </papid>annotated treebank of 1m tokens of european portuguese with almost 2,000 cps, which include lvcs and verbal chains.</prevsent>
<prevsent>this lexicon is relevant for many nlp applications, notably for automatic translation, since in any task involving language generation they confer fluency and naturalness to the output of the system.</prevsent>
</prevsection>
<citsent citstr=" E95-1014 ">
work focusing on the automatic extraction oflvcs or svcs often take as starting point list of recurrent light verbs (hendrickx et al, 2010) <papid> W10-1812 </papid>or list of nominalizations (teufel and grefenstette, 1995; <papid> E95-1014 </papid>dras, 1995; hwang et al, 2010).<papid> W10-1810 </papid></citsent>
<aftsection>
<nextsent>these approaches are not adopted here because our goal is precisely to identify which are the verbs, the nouns and other lexical elements that take part in cps.
</nextsent>
<nextsent>similar motivation to study lvcs/svcs (for srl) is found within the scope of framenet (atkins et al, 2003) and propbank (hwang et al, 2010).<papid> W10-1810 </papid></nextsent>
<nextsent>these projects have taken different decisions on how to annotate such constructions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3630">
<title id=" W11-0812.xml">identifying and analyzing brazilian portuguese complex predicates </title>
<section> corpus, extraction tool and methods.  </section>
<citcontext>
<prevsection>
<prevsent>this strategy is suitable to extract occurrences from active sentences, both affirmative and negative.
</prevsent>
<prevsent>cases which present intervening material between the verb and the other element of the cp are not captured, but this is not serious problem considering the size of our corpus, although it influences the frequencies used in candidate selection.
</prevsent>
</prevsection>
<citsent citstr=" C10-3015 ">
in order to facilitate human analysis of candidate lists, we used the mwetoolkit4: tool that has been developed specifically to extract mwes from corpora, which encompasses candidate extraction through pattern matching, candidate filtering (e.g. through association measures) and evaluation tools (ramisch et al, 2010).<papid> C10-3015 </papid></citsent>
<aftsection>
<nextsent>after generating separate lists of candidates for each pattern, we filtered out all those occurring less than 10 times in the corpus.
</nextsent>
<nextsent>the entries resulting of automatic identification were classified by their frequency and their annotation is discussed in the following section.
</nextsent>
<nextsent>each pattern of pos tags returned large number of candidates.
</nextsent>
<nextsent>our expectation was to identify cps among the most frequent candidates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3631">
<title id=" W11-0816.xml">stepwise mining of multiword expressions in hindi </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the design of general purpose automated mwe extractor is dominated by using association measures such as point-wise mutual information and other statistical hypothesis tests (church et al 1990; smadja 1993; pecina 2008).
</prevsent>
<prevsent>superior results have been reported when supervised classifier is used with multiple association measures (pecina 2008).
</prevsent>
</prevsection>
<citsent citstr=" P99-1041 ">
the association measure is extended to include substitution to test semantic and statistical idiomaticity (lin 1999).<papid> P99-1041 </papid></citsent>
<aftsection>
<nextsent>moiron et al (2006) use translation ambiguity to determine non compositionality of mwes.
</nextsent>
<nextsent>for hindi, there have been limited investigations on mwe extraction.
</nextsent>
<nextsent>venkatapathy et al (2005) considered n-v collocation extraction problem using maxent classifier with certain syntactic and semantic features.
</nextsent>
<nextsent>mukerjee et al (2006) <papid> W06-1205 </papid>used pos projection from english to hindi with corpus alignment for extracting complex predicates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3632">
<title id=" W11-0816.xml">stepwise mining of multiword expressions in hindi </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for hindi, there have been limited investigations on mwe extraction.
</prevsent>
<prevsent>venkatapathy et al (2005) considered n-v collocation extraction problem using maxent classifier with certain syntactic and semantic features.
</prevsent>
</prevsection>
<citsent citstr=" W06-1205 ">
mukerjee et al (2006) <papid> W06-1205 </papid>used pos projection from english to hindi with corpus alignment for extracting complex predicates.</citsent>
<aftsection>
<nextsent>chakrabarti et al (2008) <papid> C08-2007 </papid>present method for extracting hindi v+v compound verbs using linguistic features.</nextsent>
<nextsent>kunchukuttan et al (2008) present method for extracting compound nouns in hindi using 110statistical co-occurrence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3633">
<title id=" W11-0816.xml">stepwise mining of multiword expressions in hindi </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>venkatapathy et al (2005) considered n-v collocation extraction problem using maxent classifier with certain syntactic and semantic features.
</prevsent>
<prevsent>mukerjee et al (2006) <papid> W06-1205 </papid>used pos projection from english to hindi with corpus alignment for extracting complex predicates.</prevsent>
</prevsection>
<citsent citstr=" C08-2007 ">
chakrabarti et al (2008) <papid> C08-2007 </papid>present method for extracting hindi v+v compound verbs using linguistic features.</citsent>
<aftsection>
<nextsent>kunchukuttan et al (2008) present method for extracting compound nouns in hindi using 110statistical co-occurrence.
</nextsent>
<nextsent>sinha (2009<papid> W09-2906 </papid>b) use linguistic property of light verbs in extracting complex predicates using hindi-english parallel corpus.</nextsent>
<nextsent>all of these works have considered only limited aspects of hindi mwe.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3634">
<title id=" W11-0816.xml">stepwise mining of multiword expressions in hindi </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>chakrabarti et al (2008) <papid> C08-2007 </papid>present method for extracting hindi v+v compound verbs using linguistic features.</prevsent>
<prevsent>kunchukuttan et al (2008) present method for extracting compound nouns in hindi using 110statistical co-occurrence.</prevsent>
</prevsection>
<citsent citstr=" W09-2906 ">
sinha (2009<papid> W09-2906 </papid>b) use linguistic property of light verbs in extracting complex predicates using hindi-english parallel corpus.</citsent>
<aftsection>
<nextsent>all of these works have considered only limited aspects of hindi mwe.
</nextsent>
<nextsent>in this paper, we have considered almost all types of mwes in hindi and present method for their identification using linguistic features.
</nextsent>
<nextsent>multi-word expressions appear in variety of forms in hindi.
</nextsent>
<nextsent>the primary criterion used in defining mwe in this work is non compositionality i.e. the meaning of mwe is not composed purely on the meanings of the constituent words (baldin et al 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3655">
<title id=" W11-0816.xml">stepwise mining of multiword expressions in hindi </title>
<section> identification, extraction and interpre-.  </section>
<citcontext>
<prevsection>
<prevsent>an example (chunks are shown within curly parentheses and english equivalent is enclosed within parentheses):{bhagawaan raam ke haathon}(by lord ram) {mahaabalii raavana}(mighty ravan) {yuddha bhoomi men}(in battlefield) {maara daa laa gayaa thaa}(had been killed).
</prevsent>
<prevsent>in chunking, firstly the verb group is identified.
</prevsent>
</prevsection>
<citsent citstr=" C10-2040 ">
since hindi is verb ending language, finite state machine (fsm) is designed which starts scanning the words from the rear end (right to left) for possible inclusion in the verb group based on the pos tag and the morphemes (gune et al 2010) <papid> C10-2040 </papid>of the words.</citsent>
<aftsection>
<nextsent>a hindi complex verb group may consist of auxiliaries, light verbs, predicate verbs and intensifiers besides the main verb.
</nextsent>
<nextsent>such verb groups make an mwe because of its non-compositionality.
</nextsent>
<nextsent>in the above example, the last chunk which is the verb group chunk, is reproduced with meanings:{maara (kill) daalaa (put) gayaa (went) thaa (was)} (had been killed).
</nextsent>
<nextsent>here main verb is maara (kill), daalaa (put) is light verb making maara daalaa predicate verb, gayaa (went) is an intensifier and thaa (was) is an auxiliary verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3675">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we introduce several ideas that improve the performance of supervised information extraction systems with pipeline architecture, when they are customized for new domains.we show that: (a) combination of sequence tagger with rule-based approach for entity mention extraction yields better performance for both entity and relation mention extraction; (b) improving the identification of syntactic heads of entity mentions helps relation extraction; and (c) deterministic inference engine captures some of the joint domain structure, even when introduced as postprocessing step to pipeline system.
</prevsent>
<prevsent>all in all, our contributions yield 20% relative increas ein f1 score in domain significantly different from the domains used during the development of our information extraction system.
</prevsent>
</prevsection>
<citsent citstr=" D10-1099 ">
information extraction (ie) systems generally consist of multiple interdependent components, e.g., entity mentions predicted by an entity mention detection (emd) model connected by relations via relation mention detection (rmd) component (yao etal., 2010; <papid> D10-1099 </papid>roth and yih, 2007; surdeanu and ciaramita, 2007).</citsent>
<aftsection>
<nextsent>figure 1 shows sentence from asports domain where both entity and relation mentions are annotated.
</nextsent>
<nextsent>when training data exists, thebest performance in ie is generally obtained by supervised machine learning approaches.
</nextsent>
<nextsent>in this scenario, the typical approach for domain customization is apparently straightforward: simply retrain on data from the new domain (and potentially tune model parameters).
</nextsent>
<nextsent>in this paper we argue that, even when considerable training data is available, this is not sufficient to maximize performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3676">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we apply several simple ideas that yield significant performance boost, and can be implemented with minimal effort.
</prevsent>
<prevsent>in particular: ? we show that combination of conditional random field model (lafferty et al, 2001) with rule-based approach that is recall oriented yields better performance for emd and forthe downstream rmd component.
</prevsent>
</prevsection>
<citsent citstr=" E99-1001 ">
the rule based approach includes gazette ers, which have been shown to be important by mikheev et al (1999), <papid> E99-1001 </papid>among others.?</citsent>
<aftsection>
<nextsent>we improve the unification of the predicted semantic annotations with the syntactic analysis of the corresponding text, i.e., finding the syntactic head of given semantic constituent.
</nextsent>
<nextsent>since many features in an ie system depend on syntactic analysis, this leads to more consistent features and better extraction models.?
</nextsent>
<nextsent>we add simple inference engine that generates additional relation mentions based solely on the relation mentions extracted by the rmdmodel.
</nextsent>
<nextsent>this engine mitigates some of the limitations of text-based rmd model, which can not extract relations not explicitly stated in text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3677">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, section 6 concludes the paper.
</prevsent>
<prevsent>other recent works have analyzed the robustness of information extraction systems.
</prevsent>
</prevsection>
<citsent citstr=" D10-1033 ">
for example, florian et al (2010) <papid> D10-1033 </papid>observed that emd systems perform badly on noisy inputs, e.g., automatic speech transcripts, and propose system combination (sim ilar to our first proposal) to increase robustness insuch scenarios.</citsent>
<aftsection>
<nextsent>ratinov and roth (2009) <papid> W09-1119 </papid>also investigate design challenges for named entity recognition, and showed that other design choices, suchas the representation of output labels and using features built on external knowledge, are more important than the learning model itself.</nextsent>
<nextsent>these works are conceptually similar to our paper, but we propose several additional directions to improve robustness,and we investigate their impact in complete ie system instead of just emd.several of our lessons are drawn from the biocre ative challenge1 and the bionlp shared task (kim 1http://biocreative.sourceforge.net/et al, 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3678">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>other recent works have analyzed the robustness of information extraction systems.
</prevsent>
<prevsent>for example, florian et al (2010) <papid> D10-1033 </papid>observed that emd systems perform badly on noisy inputs, e.g., automatic speech transcripts, and propose system combination (sim ilar to our first proposal) to increase robustness insuch scenarios.</prevsent>
</prevsection>
<citsent citstr=" W09-1119 ">
ratinov and roth (2009) <papid> W09-1119 </papid>also investigate design challenges for named entity recognition, and showed that other design choices, suchas the representation of output labels and using features built on external knowledge, are more important than the learning model itself.</citsent>
<aftsection>
<nextsent>these works are conceptually similar to our paper, but we propose several additional directions to improve robustness,and we investigate their impact in complete ie system instead of just emd.several of our lessons are drawn from the biocre ative challenge1 and the bionlp shared task (kim 1http://biocreative.sourceforge.net/et al, 2009).
</nextsent>
<nextsent>these tasks have shown the importance of high quality syntactic annotations and using heuristic fixes to correct systematic errors (schuman and bergler, 2006; <papid> W06-3312 </papid>poon and vanderwende, 2010, <papid> N10-1123 </papid>among others).</nextsent>
<nextsent>systems in the latter task have also shown the importance of high recall in the earlier stages of pipeline system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3679">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ratinov and roth (2009) <papid> W09-1119 </papid>also investigate design challenges for named entity recognition, and showed that other design choices, suchas the representation of output labels and using features built on external knowledge, are more important than the learning model itself.</prevsent>
<prevsent>these works are conceptually similar to our paper, but we propose several additional directions to improve robustness,and we investigate their impact in complete ie system instead of just emd.several of our lessons are drawn from the biocre ative challenge1 and the bionlp shared task (kim 1http://biocreative.sourceforge.net/et al, 2009).</prevsent>
</prevsection>
<citsent citstr=" W06-3312 ">
these tasks have shown the importance of high quality syntactic annotations and using heuristic fixes to correct systematic errors (schuman and bergler, 2006; <papid> W06-3312 </papid>poon and vanderwende, 2010, <papid> N10-1123 </papid>among others).</citsent>
<aftsection>
<nextsent>systems in the latter task have also shown the importance of high recall in the earlier stages of pipeline system.
</nextsent>
<nextsent>we illustrate our proposed ideas using simple ie system that implements pipeline architecture: entity mention extraction followed by relation mention extraction.
</nextsent>
<nextsent>note however that the domain customization discussion in section 5 is independent of the system architecture or classifiers used for emdand rmd, and we expect the proposed ideas to apply to other ie approaches as well.
</nextsent>
<nextsent>we performed all pre-processing (tokenization, part-of-speech (pos) tagging) with the stanford corenlp toolkit.2 for emd we used the stanford named entity recognizer (finkel et al, 2005).<papid> P05-1045 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3680">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ratinov and roth (2009) <papid> W09-1119 </papid>also investigate design challenges for named entity recognition, and showed that other design choices, suchas the representation of output labels and using features built on external knowledge, are more important than the learning model itself.</prevsent>
<prevsent>these works are conceptually similar to our paper, but we propose several additional directions to improve robustness,and we investigate their impact in complete ie system instead of just emd.several of our lessons are drawn from the biocre ative challenge1 and the bionlp shared task (kim 1http://biocreative.sourceforge.net/et al, 2009).</prevsent>
</prevsection>
<citsent citstr=" N10-1123 ">
these tasks have shown the importance of high quality syntactic annotations and using heuristic fixes to correct systematic errors (schuman and bergler, 2006; <papid> W06-3312 </papid>poon and vanderwende, 2010, <papid> N10-1123 </papid>among others).</citsent>
<aftsection>
<nextsent>systems in the latter task have also shown the importance of high recall in the earlier stages of pipeline system.
</nextsent>
<nextsent>we illustrate our proposed ideas using simple ie system that implements pipeline architecture: entity mention extraction followed by relation mention extraction.
</nextsent>
<nextsent>note however that the domain customization discussion in section 5 is independent of the system architecture or classifiers used for emdand rmd, and we expect the proposed ideas to apply to other ie approaches as well.
</nextsent>
<nextsent>we performed all pre-processing (tokenization, part-of-speech (pos) tagging) with the stanford corenlp toolkit.2 for emd we used the stanford named entity recognizer (finkel et al, 2005).<papid> P05-1045 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3681">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> description of the generic ie system.  </section>
<citcontext>
<prevsection>
<prevsent>we illustrate our proposed ideas using simple ie system that implements pipeline architecture: entity mention extraction followed by relation mention extraction.
</prevsent>
<prevsent>note however that the domain customization discussion in section 5 is independent of the system architecture or classifiers used for emdand rmd, and we expect the proposed ideas to apply to other ie approaches as well.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
we performed all pre-processing (tokenization, part-of-speech (pos) tagging) with the stanford corenlp toolkit.2 for emd we used the stanford named entity recognizer (finkel et al, 2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>in all our experiments we used generic set of features(macro?)
</nextsent>
<nextsent>and the io notation3 for entity mention labels (e.g., the labels for the tokens over the seattle seahawks on sunday?
</nextsent>
<nextsent>(from figure 1) are encoded as o nflteam nflteam date?).
</nextsent>
<nextsent>2http://nlp.stanford.edu/software/ corenlp.shtml 3the io notation facilitates faster inference than the iob or iob2 notations with minimal impact on performance, when there are fewer adjacent mentions with the same type.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3682">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> description of the generic ie system.  </section>
<citcontext>
<prevsection>
<prevsent>2http://nlp.stanford.edu/software/ corenlp.shtml 3the io notation facilitates faster inference than the iob or iob2 notations with minimal impact on performance, when there are fewer adjacent mentions with the same type.
</prevsent>
<prevsent>3 argument features ? headwords of the two arguments and their combination ? entity mention labels of the two arguments and their combination syntactic features ? sequence of dependency labels in the dependency path linking the heads of the two arguments?
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
lemmas of all words in the dependency path ? syntactic path in the constituent parse tree between the largest constituents headed by the same words as the two arguments (similar to gildea and jurafsky (2002)) <papid> J02-3001 </papid>surface features?</citsent>
<aftsection>
<nextsent>concatenation of pos tags between arguments ? binary indicators set to true if there is an entity mention with agiven type between the two arguments table 1: feature set used for rmd.
</nextsent>
<nextsent>the rmd model was built from scratch as multi-class classifier that extracts binary relations between entity mentions in the same sentence.
</nextsent>
<nextsent>during training, known relation mentions become positive examples for the corresponding label and all other possible combinations between entity mentions in the same sentence become negative examples.
</nextsent>
<nextsent>we used multiclass logistic regression classifier with l2 regularization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3684">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> description of the generic ie system.  </section>
<citcontext>
<prevsection>
<prevsent>during training, known relation mentions become positive examples for the corresponding label and all other possible combinations between entity mentions in the same sentence become negative examples.
</prevsent>
<prevsent>we used multiclass logistic regression classifier with l2 regularization.
</prevsent>
</prevsection>
<citsent citstr=" P09-1113 ">
our feature set is taken from (yao et al, 2010; <papid> D10-1099 </papid>mintz et al, 2009; <papid> P09-1113 </papid>roth andyih, 2007; surdeanu and ciaramita, 2007) and models the relation arguments, the surface distance between the relation arguments, and the syntactic path between the two arguments, using both constituency and dependency representations.</citsent>
<aftsection>
<nextsent>for syntactic information, we used the stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and the stanford dependency representation (de marneffe et al, 2006).for rmd, we implemented an additive feature selection algorithm similar to the one in (surdeanu et al, 2008), <papid> P08-1082 </papid>which iteratively adds the feature with the highest improvement in f1 score to the current feature set, until no improvement is seen.</nextsent>
<nextsent>the algorithm was configured to select features that yielded the best combined performance on the dataset from roth and yih (2007) and the training partition of ace 2007.4 we used ten-fold cross val 4ldc catalog numbers ldc2006e54 and ldc2007e11 documents words entity relation mentions mentions 110 70,119 2,188 1,629 table 2: summary statistics of the nfl corpus, after our conversion to binary relations.idation on both datasets.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3685">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> description of the generic ie system.  </section>
<citcontext>
<prevsection>
<prevsent>we used multiclass logistic regression classifier with l2 regularization.
</prevsent>
<prevsent>our feature set is taken from (yao et al, 2010; <papid> D10-1099 </papid>mintz et al, 2009; <papid> P09-1113 </papid>roth andyih, 2007; surdeanu and ciaramita, 2007) and models the relation arguments, the surface distance between the relation arguments, and the syntactic path between the two arguments, using both constituency and dependency representations.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
for syntactic information, we used the stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and the stanford dependency representation (de marneffe et al, 2006).for rmd, we implemented an additive feature selection algorithm similar to the one in (surdeanu et al, 2008), <papid> P08-1082 </papid>which iteratively adds the feature with the highest improvement in f1 score to the current feature set, until no improvement is seen.</citsent>
<aftsection>
<nextsent>the algorithm was configured to select features that yielded the best combined performance on the dataset from roth and yih (2007) and the training partition of ace 2007.4 we used ten-fold cross val 4ldc catalog numbers ldc2006e54 and ldc2007e11 documents words entity relation mentions mentions 110 70,119 2,188 1,629 table 2: summary statistics of the nfl corpus, after our conversion to binary relations.idation on both datasets.
</nextsent>
<nextsent>we decided to use standard f1 score to evaluate rmd performance rather than the more complex ace score because we believe that the former is more interpretable.
</nextsent>
<nextsent>we used gold entity mentions for the feature selection process.
</nextsent>
<nextsent>table 1 summarizes the final set of features selected.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3686">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> description of the generic ie system.  </section>
<citcontext>
<prevsection>
<prevsent>we used multiclass logistic regression classifier with l2 regularization.
</prevsent>
<prevsent>our feature set is taken from (yao et al, 2010; <papid> D10-1099 </papid>mintz et al, 2009; <papid> P09-1113 </papid>roth andyih, 2007; surdeanu and ciaramita, 2007) and models the relation arguments, the surface distance between the relation arguments, and the syntactic path between the two arguments, using both constituency and dependency representations.</prevsent>
</prevsection>
<citsent citstr=" P08-1082 ">
for syntactic information, we used the stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and the stanford dependency representation (de marneffe et al, 2006).for rmd, we implemented an additive feature selection algorithm similar to the one in (surdeanu et al, 2008), <papid> P08-1082 </papid>which iteratively adds the feature with the highest improvement in f1 score to the current feature set, until no improvement is seen.</citsent>
<aftsection>
<nextsent>the algorithm was configured to select features that yielded the best combined performance on the dataset from roth and yih (2007) and the training partition of ace 2007.4 we used ten-fold cross val 4ldc catalog numbers ldc2006e54 and ldc2007e11 documents words entity relation mentions mentions 110 70,119 2,188 1,629 table 2: summary statistics of the nfl corpus, after our conversion to binary relations.idation on both datasets.
</nextsent>
<nextsent>we decided to use standard f1 score to evaluate rmd performance rather than the more complex ace score because we believe that the former is more interpretable.
</nextsent>
<nextsent>we used gold entity mentions for the feature selection process.
</nextsent>
<nextsent>table 1 summarizes the final set of features selected.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3687">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> domain customization.  </section>
<citcontext>
<prevsection>
<prevsent>this happens because city 9http://webdocs.cs.ualberta.ca/lindek/ downloads/sim.tgz 6names that are not references to team names are relatively common in this corpus, and the crf model favors the generic city name interpretation.
</prevsent>
<prevsent>however, since the goal is to answer structured queries over the extracted relations, we would prefer model that favors recall for emd, to avoid losing candidates for rmd.
</prevsent>
</prevsection>
<citsent citstr=" N06-2024 ">
while this can be achieved in different ways (minkov et al, 2006), <papid> N06-2024 </papid>in this paper we implement very simple approach: we recognize nflteam mentions with rule-based system that extracts all token sequences that begin, end, or are equal to known team name.</citsent>
<aftsection>
<nextsent>for example, green bay?
</nextsent>
<nextsent>and packers?
</nextsent>
<nextsent>are marked as team mentions,but not bay?.
</nextsent>
<nextsent>note that this approach is prone to introducing false positives, e.g., green?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3689">
<title id=" W11-0902.xml">customizing an information extraction system to a new domain </title>
<section> domain customization.  </section>
<citcontext>
<prevsection>
<prevsent>5.2 improving head identification for entity.
</prevsent>
<prevsent>mention stable 1 indicates that most rmd features (e.g., lexical information on arguments, dependency paths between arguments) depend on the syntactic heads of entity mentions.
</prevsent>
</prevsection>
<citsent citstr=" D09-1120 ">
this observation applies to other natural language processing (nlp) tasks aswell, e.g., semantic role labeling or coreference resolution (gildea and jurafsky, 2002; <papid> J02-3001 </papid>haghighi and klein, 2009).<papid> D09-1120 </papid></citsent>
<aftsection>
<nextsent>it is thus crucial that syntactic heads of mentions be correctly identified.
</nextsent>
<nextsent>originally we employed common heuristic: we first try to find aconstituent with the exact same span as the given entity mention in the parse tree of the entire sentence, and extract its head.
</nextsent>
<nextsent>if no such constituent exists, we parse only the text corresponding to the mention and return the head of the generated tree (haghighi entity mentions r f1 date 69.5 75.9 72.5 final score 90.9 88.8 89.8 nflgame 60.5 51.0 55.4 nflplayoffgame 37.0 26.3 30.8 nflteam 72.4 98.3 83.4 scoretype-1 69.7 62.1 65.7 scoretype-2 76.9 63.3 69.4 scoretype-3 64.3 50.0 56.3 scoretype-4 72.7 57.1 64.0 total 73.2 79.2 76.1 relation mentions r f1 fieldgoalpartialcount 81.2 55.4 65.9 game date 93.9 27.0 41.9 game loser 51.2 17.7 26.3 game winner 50.0 8.9 15.2 teamfinalscore 96.5 47.4 63.6 teaming ame 48.3 33.5 39.5 teamscoringall 86.7 72.9 79.2 touchdownpartialcount 89.1 61.2 72.6 total 78.5 45.9 57.9 table 6: performance with the improved syntactic head identification rules.
</nextsent>
<nextsent>and klein, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3692">
<title id=" W11-0101.xml">the semantics of dialogue acts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>each dimension in this sense constitutes category of communicative activity, and the dialogue acts involved in these activities are concerned with different types of information: feedback acts with the success of processing previous utterances; turn management acts with the allocation of the speaker role, task-related acts with the dialogue task; and so on.
</prevsent>
<prevsent>dimensions thus classify semantic content.
</prevsent>
</prevsection>
<citsent citstr=" N09-2050 ">
1 petukhova &amp; bunt (2009<papid> N09-2050 </papid>a),  bunt (2009b) formulate criteria for distinguishing dimensions, and apply these in the analysis of the dimensions that occur in 18 existing annotation schemes, showing that the 10 dimensions of dit++ form well-founded set of dimensions.</citsent>
<aftsection>
<nextsent>these are the following: (1) 1.
</nextsent>
<nextsent>task/activity: dialogue acts for performing the task or activity underlying the dialogue 2.
</nextsent>
<nextsent>auto-feedback: providing information about the speakers processing of previous utterances..
</nextsent>
<nextsent>3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3693">
<title id=" W11-1717.xml">robust sense based sentiment classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, the movie is directed by nolan?
</prevsent>
<prevsent>is labeled as neutral.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
for the purpose of this work, we follow the definition of pang et al (2002) &amp; <papid> W02-1011 </papid>turney (2002) <papid> P02-1053 </papid>and consider binary classification task for output labels as positive and negative.lexeme-based (bag-of-words) features are commonly used for supervised sentiment classification (pang and lee, 2008).</citsent>
<aftsection>
<nextsent>in addition to this, there also has been work that identifies the roles of different parts-of-speech (pos) like adjectives in sentiment classification (pang et al, 2002; <papid> W02-1011 </papid>whitelaw et al., 2005).</nextsent>
<nextsent>complex features based on parse tree shave been explored for modeling high-accuracy polarity classifiers (matsumoto et al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3694">
<title id=" W11-1717.xml">robust sense based sentiment classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, the movie is directed by nolan?
</prevsent>
<prevsent>is labeled as neutral.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
for the purpose of this work, we follow the definition of pang et al (2002) &amp; <papid> W02-1011 </papid>turney (2002) <papid> P02-1053 </papid>and consider binary classification task for output labels as positive and negative.lexeme-based (bag-of-words) features are commonly used for supervised sentiment classification (pang and lee, 2008).</citsent>
<aftsection>
<nextsent>in addition to this, there also has been work that identifies the roles of different parts-of-speech (pos) like adjectives in sentiment classification (pang et al, 2002; <papid> W02-1011 </papid>whitelaw et al., 2005).</nextsent>
<nextsent>complex features based on parse tree shave been explored for modeling high-accuracy polarity classifiers (matsumoto et al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3700">
<title id=" W11-1717.xml">robust sense based sentiment classification </title>
<section> experimentation.  </section>
<citcontext>
<prevsection>
<prevsent>for automatic sense disambiguation, we used the trained iwsd engine (trained on tourism domain) from khapra et al(2010).
</prevsent>
<prevsent>these synset identifiers along with pos category identifiers are then used as features.
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
for replacement using semantic similarity measures, we used wordnet::similarity 2.05 package by pedersen et al (2004).<papid> N04-3012 </papid></citsent>
<aftsection>
<nextsent>to evaluate the result, we use accuracy, f-score, recall and precision as the metrics.
</nextsent>
<nextsent>classification accuracy defines the ratio of the number of true instances to the total number of instances.
</nextsent>
<nextsent>recall is calculated as ratio of the true instances found tothe total number of false positives and true positives.
</nextsent>
<nextsent>precision is defined as the number of true instances divided by number of true positives and false negatives.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3701">
<title id=" W11-1717.xml">robust sense based sentiment classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a good metric which covers all pos categories can provide substantial improvement in the classification accuracy.
</prevsent>
<prevsent>this work deals with studying benefit of word sense-based feature space to supervised sentiment classification.
</prevsent>
</prevsection>
<citsent citstr=" P06-1134 ">
this work assumes the hypothesis that word sense is associated with the sentiment asshown by wiebe and mihalcea (2006) <papid> P06-1134 </papid>through human inter annotator agreement.</citsent>
<aftsection>
<nextsent>akkaya et al (2009) and martn-wanton et al(2010) study rule-based sentiment classification using word senses where martn-wanton et al (2010) uses combination of sentiment lexical resources.
</nextsent>
<nextsent>instead of rule-based implementation, our work leverages on benefits of statistical learning-based methods by using supervised approach.
</nextsent>
<nextsent>rentoumi et al (2009) suggest an approach to use word senses to detect sentence level polarity using graph-basedsimilarity.
</nextsent>
<nextsent>while rentoumi et al (2009) targets using senses to handle metaphors in sentences, we deal with generating general-purpose classifier.carrillo de albornoz et al (2010) create an emotional intensity classifier using affective class concepts as features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3702">
<title id=" W11-1717.xml">robust sense based sentiment classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>by using wordnet synsets as features, we construct feature vectors that map to larger sense-based space.
</prevsent>
<prevsent>akkaya et al (2009), martn-wanton et al (2010) and carrillo de albornoz et al (2010) deal with sentiment classification of sentences.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
on the other hand, we associate sentiment polarity to document on the whole as opposed to pang and lee (2004)<papid> P04-1035 </papid>which deals with sentiment prediction of subjectivity content only.</citsent>
<aftsection>
<nextsent>carrillo de albornoz et al (2010) suggests expansion using wordnet relations which we perform in our experiments.
</nextsent>
<nextsent>we present an empirical study to show that sense based features work better as compared to word based features.
</nextsent>
<nextsent>we show how the performance impact differs for different automatic and manual techniques.
</nextsent>
<nextsent>we also show the benefit using wordnet based similarity metrics for replacing unknown features in the test set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3703">
<title id=" W11-0146.xml">using mmil for the high level semantic annotation of the french media dialogue corpus </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>in constr ast, when measuring fine-grained features inside components we found eight types of disagreement, namely conjunctions, dis junctions, creation of participant for simple features, groups of features inside entities, features of entities, values of features, relation names and relation among entities.
</prevsent>
<prevsent>the most frequent cases concern the first two, which refer to coordination: conjunctions (20%) and dis junctions (5%).
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
theinter-annotator agreement for the coordinate entities was computed, obtaining the kappa measure, carletta (1996), <papid> J96-2004 </papid>of 0.25 for conjunctions and 0.15 for dis junctions, meaning fair and slight agreement respectively.</citsent>
<aftsection>
<nextsent>although the other cases were less frequent, the inter-annotator agreement was even lower, indicating no agreement.
</nextsent>
<nextsent>inspite of the disagreement, when measuring the global similarity between the mmil components created by both annotators we found high score of 98%.
</nextsent>
<nextsent>this metric measures the graph similarity 378 by computing the similarity between entities and relations, including the fine-grained features inside entities.
</nextsent>
<nextsent>the speech-act, main-event and main arguments are in compliance with the specifications in both annotations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3704">
<title id=" W11-0414.xml">creating an annotated tamil corpus as a discourse resource </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such study is possible in given human language only if there are sufficient discourse annotated resources available for that language.
</prevsent>
<prevsent>the penn discourse treebank (pdtb) is project whose goal is to annotate the discourse relations holding between events described in text.
</prevsent>
</prevsection>
<citsent citstr=" I08-7010 ">
the pdtb is lexically grounded approach where discourse relations are anchored in lexical items wherever they are explicitly realized in the text (miltsakaki et al 2004, prasad et al, 2008).<papid> I08-7010 </papid></citsent>
<aftsection>
<nextsent>to foster cross-linguistic studies in discourse relations, projects similar to the pdtb in discourse annotation were initiated in czech (mladov?
</nextsent>
<nextsent>et al, 2008), chinese (xue, 2005), <papid> W05-0312 </papid>turkish (zeyrek and webber, 2008) <papid> I08-7009 </papid>and hindi (prasad et al, 2008).<papid> I08-7010 </papid></nextsent>
<nextsent>we explore how the underlying framework and annotation guidelines apply to tamil, morphologically rich, agglutinative, free word order language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3706">
<title id=" W11-0414.xml">creating an annotated tamil corpus as a discourse resource </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the pdtb is lexically grounded approach where discourse relations are anchored in lexical items wherever they are explicitly realized in the text (miltsakaki et al 2004, prasad et al, 2008).<papid> I08-7010 </papid></prevsent>
<prevsent>to foster cross-linguistic studies in discourse relations, projects similar to the pdtb in discourse annotation were initiated in czech (mladov?</prevsent>
</prevsection>
<citsent citstr=" W05-0312 ">
et al, 2008), chinese (xue, 2005), <papid> W05-0312 </papid>turkish (zeyrek and webber, 2008) <papid> I08-7009 </papid>and hindi (prasad et al, 2008).<papid> I08-7010 </papid></citsent>
<aftsection>
<nextsent>we explore how the underlying framework and annotation guidelines apply to tamil, morphologically rich, agglutinative, free word order language.
</nextsent>
<nextsent>in this paper, we present how corpus of tamil texts was created on which we performed our pilot experiment.
</nextsent>
<nextsent>next, in section 3 we cover the basics of the pdtb guidelines that we followed during our annotation process.
</nextsent>
<nextsent>in section 4, we show various categories of tamil discourse connectives that we identified after preliminary study on discourse connectives in tamil, illustrating each with examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3707">
<title id=" W11-0414.xml">creating an annotated tamil corpus as a discourse resource </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the pdtb is lexically grounded approach where discourse relations are anchored in lexical items wherever they are explicitly realized in the text (miltsakaki et al 2004, prasad et al, 2008).<papid> I08-7010 </papid></prevsent>
<prevsent>to foster cross-linguistic studies in discourse relations, projects similar to the pdtb in discourse annotation were initiated in czech (mladov?</prevsent>
</prevsection>
<citsent citstr=" I08-7009 ">
et al, 2008), chinese (xue, 2005), <papid> W05-0312 </papid>turkish (zeyrek and webber, 2008) <papid> I08-7009 </papid>and hindi (prasad et al, 2008).<papid> I08-7010 </papid></citsent>
<aftsection>
<nextsent>we explore how the underlying framework and annotation guidelines apply to tamil, morphologically rich, agglutinative, free word order language.
</nextsent>
<nextsent>in this paper, we present how corpus of tamil texts was created on which we performed our pilot experiment.
</nextsent>
<nextsent>next, in section 3 we cover the basics of the pdtb guidelines that we followed during our annotation process.
</nextsent>
<nextsent>in section 4, we show various categories of tamil discourse connectives that we identified after preliminary study on discourse connectives in tamil, illustrating each with examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3710">
<title id=" W11-0414.xml">creating an annotated tamil corpus as a discourse resource </title>
<section> penn discourse treebank guidelines.  </section>
<citcontext>
<prevsection>
<prevsent>the corpus thus created consists of 1 http://static.wikipedia.org/ 119 about 2.2 million words from approximately 200,000 sentences.
</prevsent>
<prevsent>since the texts used in building the corpus were all encyclopedia articles featured in the tamil language version of wikipedia, the corpus covers wide variety of topics including arts, culture, biographies, geography, society, history, etc., written and edited by volunteers from around the world.
</prevsent>
</prevsection>
<citsent citstr=" W98-0315 ">
the pdtb is resource built on discourse structure in (webber and joshi, 1998) <papid> W98-0315 </papid>where discourse connectives are treated as discourse-level predicates that always take exactly two abstract objects such as events, states and propositions as their arguments.</citsent>
<aftsection>
<nextsent>we now describe the types of connectives and their senses from the pdtb framework and provide examples from tamil sentences.
</nextsent>
<nextsent>3.1 annotation process.
</nextsent>
<nextsent>the process of discourse annotation involves identifying discourse connectives in raw text and then annotating their arguments and semantics.
</nextsent>
<nextsent>discourse connectives are identified as being explicit, implicit, altlex, entrel or norel (prasad et al 2008).<papid> I08-7010 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3713">
<title id=" W10-4202.xml">a discourse aware graph based content selection framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this very domain dependent task is extremely important from the perspective ofusers (sripada et al, 2001) who have been observed to be tolerant of realization problems as long as the appropriate content is expressed.
</prevsent>
<prevsent>the nlg community has proposed various content selection approaches since early systems (moore and paris, 1993; mckeown, 1985) which placed emphasis on text structure and adapted planning techniques or schemas to meet discourse goals.
</prevsent>
</prevsection>
<citsent citstr=" J97-1004 ">
this paper proposes domain-independent framework which can be incorporated as content selection component in system whose goal is to deliver descriptive or explanatory texts, such as the ilex (odonnell et al, 2001), knight (lester and porter, 1997), <papid> J97-1004 </papid>and polibox (chiarcos ands tede, 2004) systems.</citsent>
<aftsection>
<nextsent>at the core of our framework lies novel use of graph-based ranking algorithm, which exploits discourse related considerations in determining what content to convey in response to request for information.
</nextsent>
<nextsent>this framework provides the ability to generate successive history-aware texts and the flexibility to generate different texts with different parameter settings.
</nextsent>
<nextsent>one discourse consideration is the tenet that the propositions selected for inclusion in text should be in some way related to one another.
</nextsent>
<nextsent>thus, the selection process should be influenced by the relevance of information to what has already been selected for inclusion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3714">
<title id=" W10-4202.xml">a discourse aware graph based content selection framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many systems (such as match (walker et al,2004) and gea (carenini and moore, 2006)) contain user model which is employed to adapt content selection to the users preferences (reiter and dale, 1997).
</prevsent>
<prevsent>our framework provides facility to model stereotypical user by incorporating the priori importance of propositions.
</prevsent>
</prevsection>
<citsent citstr=" H93-1032 ">
this facility can also be used to capture the preferences of particular user.in dialogue system, utterances that are generated without exploiting the previous discourse seem awkward and unnatural (moore, 1993).<papid> H93-1032 </papid></citsent>
<aftsection>
<nextsent>our framework takes the previous discourse into account so as to omit recently communicated propositions and to determine when repetition of previously communicated proposition is appropriate.
</nextsent>
<nextsent>to our knowledge, our work is the first effort utilizing graph-based ranking algorithm for content selection, while taking into account what information preferably should and shouldnt be conveyed together, the priori importance of information, and the discourse history.
</nextsent>
<nextsent>our framework is domain-independent methodology containingdomain-dependent features that must be inst anti ated when applying the methodology to domain.
</nextsent>
<nextsent>section 2 describes our domain-independentmethodology for determining the content of response.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3715">
<title id=" W10-4202.xml">a discourse aware graph based content selection framework </title>
<section> application in particular domain.  </section>
<citcontext>
<prevsection>
<prevsent>this section illustrates the application of our framework to particular domain and how our framework facilitates flexible content selection.
</prevsent>
<prevsent>our example is content selection in the sight system (elzer et al, 2007), whose goal is to provide visually impaired users with the knowledge that one would gain from viewing information graphics (such as bar charts) that appear in popular media.
</prevsent>
</prevsection>
<citsent citstr=" W08-1103 ">
in the current implementation, sight constructs brief initial summary (demir et al, 2008) <papid> W08-1103 </papid>that conveys the primary message of bar chart along with its salient features.</citsent>
<aftsection>
<nextsent>we enhanced the current sight system to respond to users follow-up requests for more information about the graphic, where the request does not specify the kind of information that is desired.the first step in using our framework is determining the set of propositions that might be conveyed in this domain.
</nextsent>
<nextsent>in our earlier work (demir et al, 2008), <papid> W08-1103 </papid>we identified set of propositions that capture information that could be determined by looking at bar chart, and for each message type defined in sight, specified subset of these propositions that are related to this message type.in our example, we use these propositions as candidates for inclusion in follow-up responses.</nextsent>
<nextsent>figure 2 presents portion of the relation graph,where some of the identified propositions are represented as vertices.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3720">
<title id=" W10-4160.xml">dlut chinese personal name disambiguation with rich features </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>first, the text is segmented by word segmentation system explored by luo and huang (2009).
</prevsent>
<prevsent>the second step is extract all features from segmented text, all features are put into two feature vectors: ne vector and common noun vector.
</prevsent>
</prevsection>
<citsent citstr=" W07-2024 ">
then we will compute the distance between corresponding vectors of each two documents, the standard softtfidf (chen and martin, 2007) <papid> W07-2024 </papid>are employed to compute the distance between two feature vectors.</citsent>
<aftsection>
<nextsent>lastly, we use the hac algorithm for clustering of documents.
</nextsent>
<nextsent>3.1 word segmentation.
</nextsent>
<nextsent>word segmentation is base and difficult work of natural language processing (nlp) and precondition of feature extraction.
</nextsent>
<nextsent>in this paper, the word segmentation system explored by luo and huang (2009) are employed to do this work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3721">
<title id=" W11-0313.xml">assessing benefit from feature feedback inactive learning for text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>alternatively, the user may directly label the features.
</prevsent>
<prevsent>for example, for sentiment classification task, the user may label features, such as words or phrases, as expressing positive or negative sentiment.
</prevsent>
</prevsection>
<citsent citstr=" N07-1033 ">
prior work (raghavan et al, 2006; zaidan et al, 2007)<papid> N07-1033 </papid>has demonstrated that users are able to reliably provide useful feedback on features.</citsent>
<aftsection>
<nextsent>direct feedback on list of features (raghavan etal., 2006; druck et al, 2008) is limited to simple features like unigrams.
</nextsent>
<nextsent>however, unigrams are limited in the linguistic phenomena they can capture.
</nextsent>
<nextsent>structured features such as dependency relations, paths in syntactic parse trees, etc., are often needed for learning the target concept (pradhan et al, 2004; <papid> N04-1030 </papid>joshi and rose?, 2009).</nextsent>
<nextsent>it is not clear how direct feature feedback can be extended straightforwardly to structured features, as they are difficult to present visually for feedback and may require special expertise to comprehend.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3722">
<title id=" W11-0313.xml">assessing benefit from feature feedback inactive learning for text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>direct feedback on list of features (raghavan etal., 2006; druck et al, 2008) is limited to simple features like unigrams.
</prevsent>
<prevsent>however, unigrams are limited in the linguistic phenomena they can capture.
</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
structured features such as dependency relations, paths in syntactic parse trees, etc., are often needed for learning the target concept (pradhan et al, 2004; <papid> N04-1030 </papid>joshi and rose?, 2009).</citsent>
<aftsection>
<nextsent>it is not clear how direct feature feedback can be extended straightforwardly to structured features, as they are difficult to present visually for feedback and may require special expertise to comprehend.
</nextsent>
<nextsent>an alternative approach is to seek indirect feedback on structured features (arora and nyberg, 2009) <papid> N09-3010 </papid>by asking the user to highlight spans of text, called rationales, that support the instance label (zaidan et al, 2007)<papid> N07-1033 </papid>.</nextsent>
<nextsent>for example, when classifying the sentiment of movie review, rationales are spans of text in the review that support the sentiment label for the review.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3723">
<title id=" W11-0313.xml">assessing benefit from feature feedback inactive learning for text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>structured features such as dependency relations, paths in syntactic parse trees, etc., are often needed for learning the target concept (pradhan et al, 2004; <papid> N04-1030 </papid>joshi and rose?, 2009).</prevsent>
<prevsent>it is not clear how direct feature feedback can be extended straightforwardly to structured features, as they are difficult to present visually for feedback and may require special expertise to comprehend.</prevsent>
</prevsection>
<citsent citstr=" N09-3010 ">
an alternative approach is to seek indirect feedback on structured features (arora and nyberg, 2009) <papid> N09-3010 </papid>by asking the user to highlight spans of text, called rationales, that support the instance label (zaidan et al, 2007)<papid> N07-1033 </papid>.</citsent>
<aftsection>
<nextsent>for example, when classifying the sentiment of movie review, rationales are spans of text in the review that support the sentiment label for the review.
</nextsent>
<nextsent>assuming fixed cost per unit of work, it might be cheaper to ask the user to label few features, i.e. identify relevant features and their class association,than to label several instances.
</nextsent>
<nextsent>prior work (ragha van et al, 2006; druck et al, 2008; druck et al,2009; <papid> D09-1009 </papid>zaidan et al, 2007)<papid> N07-1033 </papid> has shown that combination of instance and feature labeling can be used to reduce the total annotation cost required to learn the target concept.</nextsent>
<nextsent>however, the benefit from feature feedback may vary across learning problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3725">
<title id=" W11-0313.xml">assessing benefit from feature feedback inactive learning for text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, when classifying the sentiment of movie review, rationales are spans of text in the review that support the sentiment label for the review.
</prevsent>
<prevsent>assuming fixed cost per unit of work, it might be cheaper to ask the user to label few features, i.e. identify relevant features and their class association,than to label several instances.
</prevsent>
</prevsection>
<citsent citstr=" D09-1009 ">
prior work (ragha van et al, 2006; druck et al, 2008; druck et al,2009; <papid> D09-1009 </papid>zaidan et al, 2007)<papid> N07-1033 </papid> has shown that combination of instance and feature labeling can be used to reduce the total annotation cost required to learn the target concept.</citsent>
<aftsection>
<nextsent>however, the benefit from feature feedback may vary across learning problems.
</nextsent>
<nextsent>if we can estimate the benefit from feature feedback for 106 given problem, we can minimize the total annotation cost for achieving the desired performance by selecting the optimal annotation strategy (feature feedback or not) at every stage in learning.
</nextsent>
<nextsent>in this paper, we present the ground work for this research problem by analyzing how benefit from feature feedback varies across different learning problems and what characteristics of learning problem have significant effect on benefit from feature feedback.
</nextsent>
<nextsent>we define learning problem (p = {d, g, , l,i , s}) as tuple of the domain (d), instance gran ularity (g), feature representation (f ), labeled data units (l), amount of irrelevant text (i) and instance selection strategy (s).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3728">
<title id=" W11-0313.xml">assessing benefit from feature feedback inactive learning for text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>apart from the feature space size, it also matters what types of features are used.
</prevsent>
<prevsent>when handcrafted features from domain expert are used (pradhan etal., 2004) <papid> N04-1030 </papid>we expect to gain less from feature feed back as most of the features will be relevant.</prevsent>
</prevsection>
<citsent citstr=" W10-0216 ">
onthe other hand, when features are extracted automatically as patterns in annotation graphs (arora et al, 2010) <papid> W10-0216 </papid>feature feedback can help to identify relevant features from the large feature space.in active learning, instances to be labeled are selectively sampled in each iteration.</citsent>
<aftsection>
<nextsent>benefit from feature feedback will depend on the instances that were used to train the model in each iteration.
</nextsent>
<nextsent>in the caseof indirect feature feedback through rationales or direct feature feedback in context, instances selected will also determine what features receive feedback.
</nextsent>
<nextsent>hence, instance selection strategy should affect the benefit from feature feedback.
</nextsent>
<nextsent>in text classification, an instance may contain large amount of text, and even simple unigram representation will generate lot of features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3731">
<title id=" W11-0313.xml">assessing benefit from feature feedback inactive learning for text classification </title>
<section> experiments, results and observations.  </section>
<citcontext>
<prevsection>
<prevsent>we used two feature configurations of unigram only?
</prevsent>
<prevsent>and unigram+dependency triples?.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
the unigram and dependency annotations are derived from the stanford dependency parser (klein and manning, 2003).<papid> P03-1054 </papid>rationales by definition are spans of text in review that convey the sentiment of the reviewer and hence are the part of the document most relevant for the classification task.</citsent>
<aftsection>
<nextsent>in order to vary the amount of irrelevant text, we vary the amount of text (mea sured in terms of the number of characters) around the rationales that is included in the instance representation.
</nextsent>
<nextsent>we call this the slack around rationales.
</nextsent>
<nextsent>when using the rationales with or without the slack, only features that overlap with the rationales (and the slack, if used) are used to represent the instance.
</nextsent>
<nextsent>since we only have rationales for the movie review documents, we only studied the effect of varying the amount of irrelevant text on this dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3732">
<title id=" W11-0809.xml">mwuaware partofspeech tagging with a crf model and lexical resources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we showed that our tagger reaches state-of-the-art results for french in the standard evaluation conditions (i.e. each multiword unit is already merged in single token).
</prevsent>
<prevsent>the evaluation of the tagger integrating mwu recognition clearly shows the interest of incorporating features based on mwu resources.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
part-of-speech (pos) tagging reaches excellent results thanks to powerful discriminative multi feature models such as conditional random fields (lafferty et al, 2001), support vector machine (gimenez and marquez, 2004), maximum entropy (ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>some studies like (denis and sagot, 2009) have shown that featuring these models by means of external morphosyntactic resources still improves accuracy.
</nextsent>
<nextsent>nevertheless, current taggers rarely take multiword units such as compound words into account, whereas they form very frequent lexical units with strong syntactic and semantic particularities (sag et al, 2001; copestake et al, 2002) and their identification is crucial for applications requiring semantic processing.
</nextsent>
<nextsent>indeed, taggers are generally evaluated on perfectly tokenized texts where multiword units (mwu) have already been identified.
</nextsent>
<nextsent>our paper presents mwu-aware pos tagger (i.e. pos tagger including mwu recognition1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3733">
<title id=" W11-0809.xml">mwuaware partofspeech tagging with a crf model and lexical resources </title>
<section> statistical pos tagging with linear.  </section>
<citcontext>
<prevsection>
<prevsent>chain conditional random fields linear chain conditional ramdom fields (crf) are discriminative probabilistic models introduced by (lafferty et al, 2001) for sequential labelling.
</prevsent>
<prevsent>givenan input sequence = (x1, x2, ..., xn ) and an out 1this strategy somewhat resembles the popular approach ofjoint word segmentation and part-of-speech tagging for chinese, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P08-1101 ">
(zhang and clark, 2008).<papid> P08-1101 </papid></citsent>
<aftsection>
<nextsent>moreover, other similar experiments on the same task for french are reported in (con stant et al, 2011).
</nextsent>
<nextsent>49 put sequence of labels = (y1, y2, ..., yn ), the model is defined as follows: p?(y|x) = 1 z(x) . ? k ? k.fk(t, yt, yt1, x) where z(x) is normalization factor depending on x. it is based on features each of them being defined by binary function fk depending on the current position in x, the current label yt,the preceding one yt1 and the whole input sequence x. the feature is activated if given configuration between t, yt, yt1 and is satisfied (i.e. fk(t, yt, yt1, x) = 1).
</nextsent>
<nextsent>each feature fk is associated with weight k. the weights are the parameters of the model.
</nextsent>
<nextsent>they are estimated during the training process by maximizing the conditional loglikeli hood on set of examples already labeled (training data).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3735">
<title id=" W11-0809.xml">mwuaware partofspeech tagging with a crf model and lexical resources </title>
<section> statistical pos tagging with linear.  </section>
<citcontext>
<prevsection>
<prevsent>there exist dynamic programming procedures such as viterbi algorithm in order to efficiently explore all labelling possibilities.features are defined by combining different properties of the tokens in the input sequence and the labels at the current position and the preceding one.properties of tokens can be either binary or textual: e.g. token contains digit, token is capitalized (binary property), form of the token, suffix ofsize 2 of the token (textual property).
</prevsent>
<prevsent>most taggers exclusively use language-independent properties ? e.g.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
(ratnaparkhi, 1996; <papid> W96-0213 </papid>toutanova et al, 2003; <papid> N03-1033 </papid>gimenez and marquez, 2004; tsuruoka etal., 2009).<papid> E09-1090 </papid></citsent>
<aftsection>
<nextsent>it is also possible to integrate language dependant properties computed from an external broad-coverage morphosyntactic lexicon, that are pos tags found in the lexicon for the given token (e.g.
</nextsent>
<nextsent>(denis and sagot, 2009)).
</nextsent>
<nextsent>it is of great interest to deal with unknown words2 as most of them are covered by the lexicon, and to somewhat filter the list of candidate tags for each token.
</nextsent>
<nextsent>we therefore added to our system language-dependent property: token is associated with the concatenation of its possible tags in an external lexicon, i.e. the am bibuity class of the token (ac).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3736">
<title id=" W11-0809.xml">mwuaware partofspeech tagging with a crf model and lexical resources </title>
<section> statistical pos tagging with linear.  </section>
<citcontext>
<prevsection>
<prevsent>there exist dynamic programming procedures such as viterbi algorithm in order to efficiently explore all labelling possibilities.features are defined by combining different properties of the tokens in the input sequence and the labels at the current position and the preceding one.properties of tokens can be either binary or textual: e.g. token contains digit, token is capitalized (binary property), form of the token, suffix ofsize 2 of the token (textual property).
</prevsent>
<prevsent>most taggers exclusively use language-independent properties ? e.g.
</prevsent>
</prevsection>
<citsent citstr=" E09-1090 ">
(ratnaparkhi, 1996; <papid> W96-0213 </papid>toutanova et al, 2003; <papid> N03-1033 </papid>gimenez and marquez, 2004; tsuruoka etal., 2009).<papid> E09-1090 </papid></citsent>
<aftsection>
<nextsent>it is also possible to integrate language dependant properties computed from an external broad-coverage morphosyntactic lexicon, that are pos tags found in the lexicon for the given token (e.g.
</nextsent>
<nextsent>(denis and sagot, 2009)).
</nextsent>
<nextsent>it is of great interest to deal with unknown words2 as most of them are covered by the lexicon, and to somewhat filter the list of candidate tags for each token.
</nextsent>
<nextsent>we therefore added to our system language-dependent property: token is associated with the concatenation of its possible tags in an external lexicon, i.e. the am bibuity class of the token (ac).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3737">
<title id=" W11-0809.xml">mwuaware partofspeech tagging with a crf model and lexical resources </title>
<section> mwu-aware pos tagging.  </section>
<citcontext>
<prevsection>
<prevsent>in our case, n=1.
</prevsent>
<prevsent>50 it is somewhat similar to segmentation tasks like chunking or named entity recognition, that identify the limits of chunk or named entity segments and classify these segments.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
by using an iob5 scheme (ramshaw and marcus, 1995), <papid> W95-0107 </papid>this task isthen equivalent to labelling simple tokens.</citsent>
<aftsection>
<nextsent>each token is labeled by tag in the form x+b or x+i,where is the pos labelling the lexical unit the token belongs to.
</nextsent>
<nextsent>suffix indicates that the token is at the beginning of the lexical unit.
</nextsent>
<nextsent>suffix indicates an internal position.
</nextsent>
<nextsent>suffix is useless as the endof lexical unit corresponds to the beginning of an other one (suffix b) or the end of sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3738">
<title id=" W11-0809.xml">mwuaware partofspeech tagging with a crf model and lexical resources </title>
<section> a finite-state framework </section>
<citcontext>
<prevsection>
<prevsent>the decoder is in charge of selecting the most probable one (i.e. the path in the tfst which has the best probability).
</prevsent>
<prevsent>51 4.1 weighted finite-state transducers.
</prevsent>
</prevsection>
<citsent citstr=" J97-2003 ">
finite-state technology is very powerful machinery for natural language processing (mohri, 1997;<papid> J97-2003 </papid>kornai, 1999; karttunen, 2001), and in particular for pos tagging, e.g.</citsent>
<aftsection>
<nextsent>(roche and schabes, 1995).<papid> J95-2004 </papid></nextsent>
<nextsent>it is indeed very convenient because ithas simple factor ized representations and interesting well-defined mathematical operations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3739">
<title id=" W11-0809.xml">mwuaware partofspeech tagging with a crf model and lexical resources </title>
<section> a finite-state framework </section>
<citcontext>
<prevsection>
<prevsent>51 4.1 weighted finite-state transducers.
</prevsent>
<prevsent>finite-state technology is very powerful machinery for natural language processing (mohri, 1997;<papid> J97-2003 </papid>kornai, 1999; karttunen, 2001), and in particular for pos tagging, e.g.</prevsent>
</prevsection>
<citsent citstr=" J95-2004 ">
(roche and schabes, 1995).<papid> J95-2004 </papid></citsent>
<aftsection>
<nextsent>it is indeed very convenient because ithas simple factor ized representations and interesting well-defined mathematical operations.
</nextsent>
<nextsent>for instance, weighted finite-state transducers (wfst) are often used to represent probabilistic models such as hidden markov models.
</nextsent>
<nextsent>in that case, they map in put sequences into output sequences associated with weights following probability semi ring (r+,+,?, 0, 1) or log semi ring (r ? {??,+?},log,+,+?, 0) for numerical stability6.
</nextsent>
<nextsent>a wfst is finite state automaton which each transition is composed of an input symbol, an output symbol and weight.a path in wfst is therefore sequence of consecutive transitions of the wfst going from an initial state to final state, i.e. it puts binary relation between an input sequence and an output sequence with weight that is the product of the weights of the path transitions in probability semi ring (the sumin the log semiring).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3740">
<title id=" W11-0809.xml">mwuaware partofspeech tagging with a crf model and lexical resources </title>
<section> experiments and evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we firstly evaluated our system for standard tagging without mwu segmentation and compare it with other available statistical taggers that we all trained on the ftb-train corpus.
</prevsent>
<prevsent>we tested the 8we excluded local grammars recognizing dates, person names and complex numbers.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
53 well-known tree tagger (schmid, 1994) based on probabilistic decision trees, as well as tnt (brants, 2000) <papid> A00-1031 </papid>implementing second-order hidden markov.</citsent>
<aftsection>
<nextsent>we also compared our system with two existing discriminative taggers: svmtool (gimenez and marquez, 2004) based on support vector models with language-independent features; melt (denis and sagot, 2009) based on maximum entropy model also incorporating language-dependent feature computed from an external lexicon.
</nextsent>
<nextsent>the lexicon used to train and test melt included all lexical re sources9 described in section 5.
</nextsent>
<nextsent>for our crf-based system, we trained two models with crf++10: (a) std using language-independent template features (i.e. excluding ac-based features); (b) lex using all feature templates described in table 2.
</nextsent>
<nextsent>we note crf-std and crf-lex the two related taggers when no preliminary lexical analysis is performed;crf-std+ and crf-lex+ when lexical analysis is performed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3741">
<title id=" W11-1105.xml">using a wikipedia based semantic relatedness measure for document clustering </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>similarity is measured in the new concept space.
</prevsent>
<prevsent>esa does not use the link structure or other structured knowledge from wikipedia.
</prevsent>
</prevsection>
<citsent citstr=" W09-3206 ">
moreover, by walking over content similarity graph, our method benefits from nonlinear distance measure according to the paths consisting of small neighborhoods.in the work of yeh et al  (2009), <papid> W09-3206 </papid>graph of documents and hyper links is computed from wikipedia, then personalized page rank (haveliwala, 2003) is computed for each text fragment, with the teleport vector being the one resulting from the esa algorithm cited above.</citsent>
<aftsection>
<nextsent>to compute semantic relatedness between two texts, yeh et al  (2009) <papid> W09-3206 </papid>simply compare their personalized page rank vectors.</nextsent>
<nextsent>by comparison, in our method, we also consider in addition to hyper links the effect of word co-occurrence between article contents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3743">
<title id=" W11-1105.xml">using a wikipedia based semantic relatedness measure for document clustering </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>hitting time has been used in various studies as distance measure in graphs, e.g. for dimensionality reduction (saerens et al , 2004)or for collaborative filtering in recommender system (brand, 2005).
</prevsent>
<prevsent>hitting time was also used for link prediction in social networks along with other distances (liben-nowell and kleinberg, 2003), or for semantic query suggestion using query/urlbipartite graph (mei et al , 2008).
</prevsent>
</prevsection>
<citsent citstr=" E09-1005 ">
as for personalized page rank, it was used for word sense disambiguation (agirre and soroa, 2009), <papid> E09-1005 </papid>and for measuring lexical relatedness of words in graph built from wordnet (hughes and ramage, 2007).<papid> D07-1061 </papid></citsent>
<aftsection>
<nextsent>we proposed model for measuring text semantic relatedness based on knowledge embodied in wikipedia, seen here as document network with two types of links ? hyper links and lexical similarityones.
</nextsent>
<nextsent>we have used visiting probability to measure proximity between weighted sets of nodes, and have proposed approximation algorithms to make computation efficient for large graphs (more than one million nodes and 40 million links) and large text clustering datasets (20,000 documents in 20 newsgroups).
</nextsent>
<nextsent>results on the document clustering task showed an improvement using both word cooccurrence information and user-defined hyperlinksbetween articles over other methods for text representation.
</nextsent>
<nextsent>acknowledgments the work presented in this paper has been supported by the im2 nccr (interactive multimodal information management) of the swiss national science foundation (http://www.im2.ch).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3744">
<title id=" W11-1105.xml">using a wikipedia based semantic relatedness measure for document clustering </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>hitting time has been used in various studies as distance measure in graphs, e.g. for dimensionality reduction (saerens et al , 2004)or for collaborative filtering in recommender system (brand, 2005).
</prevsent>
<prevsent>hitting time was also used for link prediction in social networks along with other distances (liben-nowell and kleinberg, 2003), or for semantic query suggestion using query/urlbipartite graph (mei et al , 2008).
</prevsent>
</prevsection>
<citsent citstr=" D07-1061 ">
as for personalized page rank, it was used for word sense disambiguation (agirre and soroa, 2009), <papid> E09-1005 </papid>and for measuring lexical relatedness of words in graph built from wordnet (hughes and ramage, 2007).<papid> D07-1061 </papid></citsent>
<aftsection>
<nextsent>we proposed model for measuring text semantic relatedness based on knowledge embodied in wikipedia, seen here as document network with two types of links ? hyper links and lexical similarityones.
</nextsent>
<nextsent>we have used visiting probability to measure proximity between weighted sets of nodes, and have proposed approximation algorithms to make computation efficient for large graphs (more than one million nodes and 40 million links) and large text clustering datasets (20,000 documents in 20 newsgroups).
</nextsent>
<nextsent>results on the document clustering task showed an improvement using both word cooccurrence information and user-defined hyperlinksbetween articles over other methods for text representation.
</nextsent>
<nextsent>acknowledgments the work presented in this paper has been supported by the im2 nccr (interactive multimodal information management) of the swiss national science foundation (http://www.im2.ch).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3745">
<title id=" W11-0609.xml">chameleons in imagined conversations a new approach to understanding coordination of linguistic style in dialogs </title>
<section> related work not already mentioned.  </section>
<citcontext>
<prevsection>
<prevsent>such an understanding would give us insight into how and what kinds of language coordination yield more satisfying interactions ? convergence has been already shown to enhance communication in organizational contexts (bourhis, 1991), psychotherapy (fer rara, 1991), care of the mentally disabled (hamilton, 1991), and police-community interactions (giles et al., 2007).
</prevsent>
<prevsent>moreover, deeper understanding can aidhuman-computer interaction by informing the construction of natural-language generation systems,since people are often more satisfied with encounters exhibiting appropriate linguistic convergence (bradac et al, 1988; van baaren et al, 2003), even when the other conversational participant is known to be computer (nass and lee, 2000; branigan et al., 2010).
</prevsent>
</prevsection>
<citsent citstr=" D10-1021 ">
linguistic style and human characteristics using stylistic (i.e., non-topical) elements like articles and prepositions to characterize the utterer insome way has long history, including in author ship attribution (mosteller and wallace, 1984; juola, 2008), personality-type classification (argamon et al., 2005; oberlander and gill, 2006; mairesse et al, 2007), gender categorization (koppel et al, 2002; mukherjee and liu, 2010; <papid> D10-1021 </papid>herring and paolillo, 2006), identification of interact ional style (jurafsky et al, 2009; <papid> N09-1072 </papid>ranganath et al, 2009), <papid> D09-1035 </papid>and recognizing deceptive language (hancock et al, 2008; mihalcea and strapparava, 2009).<papid> P09-2078 </papid></citsent>
<aftsection>
<nextsent>imagined conversations there has been work inthe nlp community applying computational techniques to fiction, scripts, and other types of text containing imagined conversations.
</nextsent>
<nextsent>for example, one recent project identifies conversational network sin novels, with the goal of evaluating various literary theories (elson et al, 2010; <papid> P10-1015 </papid>elson and mckeown, 2010).</nextsent>
<nextsent>movie scripts were used as word-sense disambiguation evaluation data as part of an effort to generate computer animation from the scripts (ye and baldwin, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3746">
<title id=" W11-0609.xml">chameleons in imagined conversations a new approach to understanding coordination of linguistic style in dialogs </title>
<section> related work not already mentioned.  </section>
<citcontext>
<prevsection>
<prevsent>such an understanding would give us insight into how and what kinds of language coordination yield more satisfying interactions ? convergence has been already shown to enhance communication in organizational contexts (bourhis, 1991), psychotherapy (fer rara, 1991), care of the mentally disabled (hamilton, 1991), and police-community interactions (giles et al., 2007).
</prevsent>
<prevsent>moreover, deeper understanding can aidhuman-computer interaction by informing the construction of natural-language generation systems,since people are often more satisfied with encounters exhibiting appropriate linguistic convergence (bradac et al, 1988; van baaren et al, 2003), even when the other conversational participant is known to be computer (nass and lee, 2000; branigan et al., 2010).
</prevsent>
</prevsection>
<citsent citstr=" N09-1072 ">
linguistic style and human characteristics using stylistic (i.e., non-topical) elements like articles and prepositions to characterize the utterer insome way has long history, including in author ship attribution (mosteller and wallace, 1984; juola, 2008), personality-type classification (argamon et al., 2005; oberlander and gill, 2006; mairesse et al, 2007), gender categorization (koppel et al, 2002; mukherjee and liu, 2010; <papid> D10-1021 </papid>herring and paolillo, 2006), identification of interact ional style (jurafsky et al, 2009; <papid> N09-1072 </papid>ranganath et al, 2009), <papid> D09-1035 </papid>and recognizing deceptive language (hancock et al, 2008; mihalcea and strapparava, 2009).<papid> P09-2078 </papid></citsent>
<aftsection>
<nextsent>imagined conversations there has been work inthe nlp community applying computational techniques to fiction, scripts, and other types of text containing imagined conversations.
</nextsent>
<nextsent>for example, one recent project identifies conversational network sin novels, with the goal of evaluating various literary theories (elson et al, 2010; <papid> P10-1015 </papid>elson and mckeown, 2010).</nextsent>
<nextsent>movie scripts were used as word-sense disambiguation evaluation data as part of an effort to generate computer animation from the scripts (ye and baldwin, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3747">
<title id=" W11-0609.xml">chameleons in imagined conversations a new approach to understanding coordination of linguistic style in dialogs </title>
<section> related work not already mentioned.  </section>
<citcontext>
<prevsection>
<prevsent>such an understanding would give us insight into how and what kinds of language coordination yield more satisfying interactions ? convergence has been already shown to enhance communication in organizational contexts (bourhis, 1991), psychotherapy (fer rara, 1991), care of the mentally disabled (hamilton, 1991), and police-community interactions (giles et al., 2007).
</prevsent>
<prevsent>moreover, deeper understanding can aidhuman-computer interaction by informing the construction of natural-language generation systems,since people are often more satisfied with encounters exhibiting appropriate linguistic convergence (bradac et al, 1988; van baaren et al, 2003), even when the other conversational participant is known to be computer (nass and lee, 2000; branigan et al., 2010).
</prevsent>
</prevsection>
<citsent citstr=" D09-1035 ">
linguistic style and human characteristics using stylistic (i.e., non-topical) elements like articles and prepositions to characterize the utterer insome way has long history, including in author ship attribution (mosteller and wallace, 1984; juola, 2008), personality-type classification (argamon et al., 2005; oberlander and gill, 2006; mairesse et al, 2007), gender categorization (koppel et al, 2002; mukherjee and liu, 2010; <papid> D10-1021 </papid>herring and paolillo, 2006), identification of interact ional style (jurafsky et al, 2009; <papid> N09-1072 </papid>ranganath et al, 2009), <papid> D09-1035 </papid>and recognizing deceptive language (hancock et al, 2008; mihalcea and strapparava, 2009).<papid> P09-2078 </papid></citsent>
<aftsection>
<nextsent>imagined conversations there has been work inthe nlp community applying computational techniques to fiction, scripts, and other types of text containing imagined conversations.
</nextsent>
<nextsent>for example, one recent project identifies conversational network sin novels, with the goal of evaluating various literary theories (elson et al, 2010; <papid> P10-1015 </papid>elson and mckeown, 2010).</nextsent>
<nextsent>movie scripts were used as word-sense disambiguation evaluation data as part of an effort to generate computer animation from the scripts (ye and baldwin, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3748">
<title id=" W11-0609.xml">chameleons in imagined conversations a new approach to understanding coordination of linguistic style in dialogs </title>
<section> related work not already mentioned.  </section>
<citcontext>
<prevsection>
<prevsent>such an understanding would give us insight into how and what kinds of language coordination yield more satisfying interactions ? convergence has been already shown to enhance communication in organizational contexts (bourhis, 1991), psychotherapy (fer rara, 1991), care of the mentally disabled (hamilton, 1991), and police-community interactions (giles et al., 2007).
</prevsent>
<prevsent>moreover, deeper understanding can aidhuman-computer interaction by informing the construction of natural-language generation systems,since people are often more satisfied with encounters exhibiting appropriate linguistic convergence (bradac et al, 1988; van baaren et al, 2003), even when the other conversational participant is known to be computer (nass and lee, 2000; branigan et al., 2010).
</prevsent>
</prevsection>
<citsent citstr=" P09-2078 ">
linguistic style and human characteristics using stylistic (i.e., non-topical) elements like articles and prepositions to characterize the utterer insome way has long history, including in author ship attribution (mosteller and wallace, 1984; juola, 2008), personality-type classification (argamon et al., 2005; oberlander and gill, 2006; mairesse et al, 2007), gender categorization (koppel et al, 2002; mukherjee and liu, 2010; <papid> D10-1021 </papid>herring and paolillo, 2006), identification of interact ional style (jurafsky et al, 2009; <papid> N09-1072 </papid>ranganath et al, 2009), <papid> D09-1035 </papid>and recognizing deceptive language (hancock et al, 2008; mihalcea and strapparava, 2009).<papid> P09-2078 </papid></citsent>
<aftsection>
<nextsent>imagined conversations there has been work inthe nlp community applying computational techniques to fiction, scripts, and other types of text containing imagined conversations.
</nextsent>
<nextsent>for example, one recent project identifies conversational network sin novels, with the goal of evaluating various literary theories (elson et al, 2010; <papid> P10-1015 </papid>elson and mckeown, 2010).</nextsent>
<nextsent>movie scripts were used as word-sense disambiguation evaluation data as part of an effort to generate computer animation from the scripts (ye and baldwin, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3749">
<title id=" W11-0609.xml">chameleons in imagined conversations a new approach to understanding coordination of linguistic style in dialogs </title>
<section> related work not already mentioned.  </section>
<citcontext>
<prevsection>
<prevsent>linguistic style and human characteristics using stylistic (i.e., non-topical) elements like articles and prepositions to characterize the utterer insome way has long history, including in author ship attribution (mosteller and wallace, 1984; juola, 2008), personality-type classification (argamon et al., 2005; oberlander and gill, 2006; mairesse et al, 2007), gender categorization (koppel et al, 2002; mukherjee and liu, 2010; <papid> D10-1021 </papid>herring and paolillo, 2006), identification of interact ional style (jurafsky et al, 2009; <papid> N09-1072 </papid>ranganath et al, 2009), <papid> D09-1035 </papid>and recognizing deceptive language (hancock et al, 2008; mihalcea and strapparava, 2009).<papid> P09-2078 </papid></prevsent>
<prevsent>imagined conversations there has been work inthe nlp community applying computational techniques to fiction, scripts, and other types of text containing imagined conversations.</prevsent>
</prevsection>
<citsent citstr=" P10-1015 ">
for example, one recent project identifies conversational network sin novels, with the goal of evaluating various literary theories (elson et al, 2010; <papid> P10-1015 </papid>elson and mckeown, 2010).</citsent>
<aftsection>
<nextsent>movie scripts were used as word-sense disambiguation evaluation data as part of an effort to generate computer animation from the scripts (ye and baldwin, 2006).
</nextsent>
<nextsent>sonderegger (2010) employ eda corpus of english poetry to study the relationship between pronunciation and network structure.rayson et al (2001) computed part-of-speech frequencies for imaginative writing in the british national corpus, finding typology gradient progressing from conversation to imaginative writing (e.g.,novels) to task-oriented speech to informative writing.
</nextsent>
<nextsent>the data analyzed by oberlander and gill(2006) consisted of emails that participants were instructed to write by imagining that they were going to update good friend on their current goings-on.
</nextsent>
<nextsent>to address the questions raised in the introduction, we created large set of imagined conversations, starting from movie scripts crawled from various sites.8 meta data for conversation analysis and duplicate-script detection involved mostly automatic matching of movie scripts with the imdbmovie database; clean-up resulted in 617 unique titles tagged with genre, release year, cast lists, and 8the source of these scripts and more detail about the corpus are given in the readme associated with the cornell movie dialogs corpus, available at http://www.cs.cornell.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3750">
<title id=" W11-0609.xml">chameleons in imagined conversations a new approach to understanding coordination of linguistic style in dialogs </title>
<section> measuring convergence.  </section>
<citcontext>
<prevsection>
<prevsent>b: eaten??
</prevsent>
<prevsent>a: nope.??
</prevsent>
</prevsection>
<citsent citstr=" C00-1027 ">
has two exchanges, one initiated by as hi?, the other by bs eaten??.10other asymmetric measures based on conditional probability of occurrence have been proposed for adaptation within monologues (church, 2000) <papid> C00-1027 </papid>and between conversations (stenchikova and stent, 2007).</citsent>
<aftsection>
<nextsent>since our focus is different, we control for different factors.
</nextsent>
<nextsent>79 capture an important asymmetry.
</nextsent>
<nextsent>the case whereat = 1 but bta = 0 represents true failure to ac commodate; but the case where at = 0 but bta = 1should not, at least not to the same degree.
</nextsent>
<nextsent>forex ample, may be very short (e.g., what??)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3751">
<title id=" W11-0609.xml">chameleons in imagined conversations a new approach to understanding coordination of linguistic style in dialogs </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>the trigger families are ordered by decreasing convergence.
</prevsent>
<prevsent>all differences are statistically significant (paired t-test).
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
in all figures in this paper, error bars represent standard error, estimated via bootstrap re sampling (koehn, 2004).<papid> W04-3250 </papid></citsent>
<aftsection>
<nextsent>(here, the error bars, in red, are very tight.)
</nextsent>
<nextsent>movies vs. twitter one can ask how our results on movie dialogs correspond to those for real-life conversations.
</nextsent>
<nextsent>to study this, we utilize the resultsof danescu-niculescu-mizil et al (2011) on large scale collection of twitter exchanges as data on real conversational exchanges.
</nextsent>
<nextsent>figure 2 depicts the comparison, revealing two interesting effects.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3752">
<title id=" W11-0609.xml">chameleons in imagined conversations a new approach to understanding coordination of linguistic style in dialogs </title>
<section> summary and future work.  </section>
<citcontext>
<prevsection>
<prevsent>shows convergence calculated on all the conversations ofthe 24 romantic-comedy pairs considered in this experiment.
</prevsent>
<prevsent>be used to further automatic controversy detection (mishne and glance, 2006; gmez et al, 2008).
</prevsent>
</prevsection>
<citsent citstr=" P11-1078 ">
moreover, if we succeeded in linking our results on narrative importance to relative social status, we might further the development of systems that can infer social relationships in online social networks when conversational data is present but other, more explicit cues are absent (wyatt et al, 2008; bramsen et al, 2011).<papid> P11-1078 </papid></citsent>
<aftsection>
<nextsent>such systems could be valuable tothe rapidly expanding field of analyzing social networks.
</nextsent>
<nextsent>acknowledgments many thanks for their help aredue to claire cardie, catalin draghici, susan dumais, shimon edelman, michael gamon, jon kleinberg, magdalena narozniak, alex niculescu-mizil, myle ott, bo pang, morgan sonderegger, plus nlp seminar participants eric baumer, bin lu, chenhao tan, lu wang, bishan yang, ainur yessenalina, and marseille, and the anonymous reviewers (who wentfar beyond the call of duty!).
</nextsent>
<nextsent>supported by nsf iis 0910664, the yahoo!
</nextsent>
<nextsent>frep program, and yahoo!
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3753">
<title id=" W11-1703.xml">experiments with a differential semantics annotation for wordnet 30 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>depending on the selected factor, various facets of subjective meanings come under scrutiny.
</prevsent>
<prevsent>the inspiring work of kamps and marx still has several limitations.
</prevsent>
</prevsection>
<citsent citstr=" E06-1027 ">
the majority of researchers working on subjectivity agree that the subjectivity load of given word is dependent on the senses of the respective word (andreevskaia and bergler, 2006), (<papid> E06-1027 </papid>bentivogli et al, 2004), (<papid> W04-2214 </papid>mihalcea et al, 2007), (<papid> P07-1123 </papid>valiutti et al, 2004) and many others.; yet, in kamps and marxs model (kmm, henceforth), because they work with words and not word senses, the sense distinctions are lost, making it impossible to assign different scores to different senses of the word in case.</citsent>
<aftsection>
<nextsent>going up from the level of word to the level of sentence, paragraph or entire text, the bag of words approach can easily be fooled in the presence of valence shift ers (polanyi and zaenen, 2006).
</nextsent>
<nextsent>in order to cope with this problem, the text under investigation needs minimal level of sentence processing, required for the identification of the structures that could get under the scope of valence shifter (tufi?, 2008).
</nextsent>
<nextsent>for dealing with irony or sarcasm, processing requirements go beyond sentence level, and discourse structure of the text might be necessary.
</nextsent>
<nextsent>on the other hand, although the adjectives make up the obvious class of subjectivity words, the other open class categories have significant potential for expressing subjective meanings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3754">
<title id=" W11-1703.xml">experiments with a differential semantics annotation for wordnet 30 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>depending on the selected factor, various facets of subjective meanings come under scrutiny.
</prevsent>
<prevsent>the inspiring work of kamps and marx still has several limitations.
</prevsent>
</prevsection>
<citsent citstr=" W04-2214 ">
the majority of researchers working on subjectivity agree that the subjectivity load of given word is dependent on the senses of the respective word (andreevskaia and bergler, 2006), (<papid> E06-1027 </papid>bentivogli et al, 2004), (<papid> W04-2214 </papid>mihalcea et al, 2007), (<papid> P07-1123 </papid>valiutti et al, 2004) and many others.; yet, in kamps and marxs model (kmm, henceforth), because they work with words and not word senses, the sense distinctions are lost, making it impossible to assign different scores to different senses of the word in case.</citsent>
<aftsection>
<nextsent>going up from the level of word to the level of sentence, paragraph or entire text, the bag of words approach can easily be fooled in the presence of valence shift ers (polanyi and zaenen, 2006).
</nextsent>
<nextsent>in order to cope with this problem, the text under investigation needs minimal level of sentence processing, required for the identification of the structures that could get under the scope of valence shifter (tufi?, 2008).
</nextsent>
<nextsent>for dealing with irony or sarcasm, processing requirements go beyond sentence level, and discourse structure of the text might be necessary.
</nextsent>
<nextsent>on the other hand, although the adjectives make up the obvious class of subjectivity words, the other open class categories have significant potential for expressing subjective meanings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3755">
<title id=" W11-1703.xml">experiments with a differential semantics annotation for wordnet 30 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>depending on the selected factor, various facets of subjective meanings come under scrutiny.
</prevsent>
<prevsent>the inspiring work of kamps and marx still has several limitations.
</prevsent>
</prevsection>
<citsent citstr=" P07-1123 ">
the majority of researchers working on subjectivity agree that the subjectivity load of given word is dependent on the senses of the respective word (andreevskaia and bergler, 2006), (<papid> E06-1027 </papid>bentivogli et al, 2004), (<papid> W04-2214 </papid>mihalcea et al, 2007), (<papid> P07-1123 </papid>valiutti et al, 2004) and many others.; yet, in kamps and marxs model (kmm, henceforth), because they work with words and not word senses, the sense distinctions are lost, making it impossible to assign different scores to different senses of the word in case.</citsent>
<aftsection>
<nextsent>going up from the level of word to the level of sentence, paragraph or entire text, the bag of words approach can easily be fooled in the presence of valence shift ers (polanyi and zaenen, 2006).
</nextsent>
<nextsent>in order to cope with this problem, the text under investigation needs minimal level of sentence processing, required for the identification of the structures that could get under the scope of valence shifter (tufi?, 2008).
</nextsent>
<nextsent>for dealing with irony or sarcasm, processing requirements go beyond sentence level, and discourse structure of the text might be necessary.
</nextsent>
<nextsent>on the other hand, although the adjectives make up the obvious class of subjectivity words, the other open class categories have significant potential for expressing subjective meanings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3756">
<title id=" W11-1403.xml">generating varied narrative probability exercises </title>
<section> language generation.  </section>
<citcontext>
<prevsection>
<prevsent>since using such marked word order may come across as unnatural in neutral discourse context, this type of variation should be applied with caution.
</prevsent>
<prevsent>ellipsis.
</prevsent>
</prevsection>
<citsent citstr=" W09-0624 ">
this is the removal of duplicate words from sentences, which typically applies to aggregated sentences (harbusch and kempen, 2009).<papid> W09-0624 </papid></citsent>
<aftsection>
<nextsent>genpex can apply different types of ellipsis, such as gapping and conjunction reduction.
</nextsent>
<nextsent>gapping is the removal of all except the first verb in an aggregatedsentence.
</nextsent>
<nextsent>an example from figure 1 is the sentence 70 fahrrader sind bill iger als 500 euro und 30 fahrrader teurer als 500 euro?
</nextsent>
<nextsent>(70 bicycles are.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3757">
<title id=" W11-1212.xml">identifying parallel documents from a large bilingual collection of texts application to parallel article extraction in wikipedia </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>while several recent works on dealing with large bilingual collections of texts, e.g.
</prevsent>
</prevsection>
<citsent citstr=" N10-1063 ">
(smithet al, 2010), <papid> N10-1063 </papid>seek for extracting parallel sentences from comparable corpora, we present para docs, system designed to recognize pairs of parallel documents in (large) bilingual collection of texts.</citsent>
<aftsection>
<nextsent>we show that this system outperforms fair baseline (enrightand kondrak, 2007) <papid> N07-2008 </papid>in number of controlled tasks.</nextsent>
<nextsent>we applied it on the french english cross-language linked article pairs of wikipedia in order see whether parallel articles in this resource are available, and ifour system is able to locate them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3758">
<title id=" W11-1212.xml">identifying parallel documents from a large bilingual collection of texts application to parallel article extraction in wikipedia </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>while several recent works on dealing with large bilingual collections of texts, e.g.
</prevsent>
<prevsent>(smithet al, 2010), <papid> N10-1063 </papid>seek for extracting parallel sentences from comparable corpora, we present para docs, system designed to recognize pairs of parallel documents in (large) bilingual collection of texts.</prevsent>
</prevsection>
<citsent citstr=" N07-2008 ">
we show that this system outperforms fair baseline (enrightand kondrak, 2007) <papid> N07-2008 </papid>in number of controlled tasks.</citsent>
<aftsection>
<nextsent>we applied it on the french english cross-language linked article pairs of wikipedia in order see whether parallel articles in this resource are available, and ifour system is able to locate them.
</nextsent>
<nextsent>according to some manual evaluation we conducted, fourth of the article pairs in wikipedia are indeed in translation relation, and para docs identifies parallel or noisy parallel article pairs with precision of 80%.
</nextsent>
<nextsent>there is growing interest within the machine translation (mt) community to investigate comparable corpora.
</nextsent>
<nextsent>the idea that they are available ina much larger quantity certainly contributes to foster this interest.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3760">
<title id=" W11-1212.xml">identifying parallel documents from a large bilingual collection of texts application to parallel article extraction in wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we describe para docs, system capable of mining parallel documents in collection, based on lightweight content-based features extracted from the documents.
</prevsent>
<prevsent>on the contrary to other systems designed to target parallel corpora (chen 2http://opus.lingfil.uu.se/ 3http://www.statmt.org/europarl/ 4http://fr.wikipedia.org/ 87 proceedings of the 4th workshop on building and using comparable corpora, pages 8795, 49th annual meeting of the association for computational linguistics, portland, oregon, 24 june 2011.
</prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
c2011 association for computational linguistics and nie, 2000; resnik and smith, 2003), <papid> J03-3002 </papid>we donot assume any specific naming conventions on file names or urls.the reminder of this article is organized as follows.</citsent>
<aftsection>
<nextsent>in the next section, we describe our approach to mining parallel documents in bilingual collection of texts.
</nextsent>
<nextsent>we test our approach on theeuroparl corpus in section 3.
</nextsent>
<nextsent>we present in section 4 the application of our system to subpart of the french-english articles of wikipedia.
</nextsent>
<nextsent>we describe related work in section 5, summarize ourwork in section 6 and present future works in section 7.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3770">
<title id=" W11-1212.xml">identifying parallel documents from a large bilingual collection of texts application to parallel article extraction in wikipedia </title>
<section> experiments with wikipedia.  </section>
<citcontext>
<prevsection>
<prevsent>we ended up with 537 067 articles in each language.
</prevsent>
<prevsent>the average length of the english pages is 711 words, while the average for french is 445 words.
</prevsent>
</prevsection>
<citsent citstr=" W09-1605 ">
the difference in length among linked articles has been studied by filatova (2009) <papid> W09-1605 </papid>on small excerpt of bibliographical articles describing 48 persons listed in the biography generation task (task 5) of duc 2004.13 12at least they were at the time of redaction.</citsent>
<aftsection>
<nextsent>13http://duc.nist.gov/duc2004/tasks.html/ 4.2 parallel ness of cross-language linked.
</nextsent>
<nextsent>article pairs in fr-en wikipedia.in this experiment, we wanted to measure the proportion of cross-language linked article pairs in wikipedia that are in translation relation.
</nextsent>
<nextsent>in order to do so, we manually evaluated 200 pairs of articles in our french-english wikipedia repository.a web interface was developed in order to annotate each pair, following the distinction introduced by fung and cheung (2004): <papid> W04-3208 </papid>parallel indicates sentence-aligned texts that are in translation relation;noisy characterizes two documents that are nevertheless mostly bilingual translations of each other;topic corresponds to documents which share similar topics, but that are not translation of each others and very-non that stands for rather unrelated texts.</nextsent>
<nextsent>the results of the manual evaluation are reported in the left column of table 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3771">
<title id=" W11-1212.xml">identifying parallel documents from a large bilingual collection of texts application to parallel article extraction in wikipedia </title>
<section> experiments with wikipedia.  </section>
<citcontext>
<prevsection>
<prevsent>13http://duc.nist.gov/duc2004/tasks.html/ 4.2 parallel ness of cross-language linked.
</prevsent>
<prevsent>article pairs in fr-en wikipedia.in this experiment, we wanted to measure the proportion of cross-language linked article pairs in wikipedia that are in translation relation.
</prevsent>
</prevsection>
<citsent citstr=" W04-3208 ">
in order to do so, we manually evaluated 200 pairs of articles in our french-english wikipedia repository.a web interface was developed in order to annotate each pair, following the distinction introduced by fung and cheung (2004): <papid> W04-3208 </papid>parallel indicates sentence-aligned texts that are in translation relation;noisy characterizes two documents that are nevertheless mostly bilingual translations of each other;topic corresponds to documents which share similar topics, but that are not translation of each others and very-non that stands for rather unrelated texts.</citsent>
<aftsection>
<nextsent>the results of the manual evaluation are reported in the left column of table 1.
</nextsent>
<nextsent>we observe that fourth of the pairs of articles are indeed parallel ornoisy parallel.
</nextsent>
<nextsent>this figure quantifies the observation made by adafre and de rijke (2006) that while some articles in wikipedia tend to be translations of each other, the majority of the articles tend to be written independently of each other.
</nextsent>
<nextsent>to the best ofour knowledge, this is the first time someone is measuring the degree of parallel ness of wikipedia at the article level.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3775">
<title id=" W11-1212.xml">identifying parallel documents from a large bilingual collection of texts application to parallel article extraction in wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this is for instance the case of ptminer (chen and nie, 2000)and strand (resnik and smith, 2003), <papid> J03-3002 </papid>two systems that are intended to mine parallel documents over the web.</prevsent>
<prevsent>since heuristics on url names does not ensure parallel ness, other cues, such as the ratio of the length of the documents paired or their htmlstructure, are further being used.</prevsent>
</prevsection>
<citsent citstr=" P06-1062 ">
others have proposed to use features computed after sentence aligning candidate pair of documents (shi et al, 2006),<papid> P06-1062 </papid>a very time consuming strategy (that we tried without success).</citsent>
<aftsection>
<nextsent>others have tried to use bilingual lexicons in order to compare document pairs; this is for instance the case of the bits system (ma and liberman, 1999).
</nextsent>
<nextsent>also, enright and kondrak (2007) <papid> N07-2008 </papid>propose very lightweight content-based approach to pairing documents, capitalizing on the number of hapax words they share.</nextsent>
<nextsent>we show in this study, that this approach can easily be outperformed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3777">
<title id=" W11-1212.xml">identifying parallel documents from a large bilingual collection of texts application to parallel article extraction in wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>also, enright and kondrak (2007) <papid> N07-2008 </papid>propose very lightweight content-based approach to pairing documents, capitalizing on the number of hapax words they share.</prevsent>
<prevsent>we show in this study, that this approach can easily be outperformed.</prevsent>
</prevsection>
<citsent citstr=" N04-1034 ">
zhao and vogel (2002) were among the first to report experiments on harvesting comparable news collections in order to extract parallel sentences.with similar goal, munteanu et al (2004) <papid> N04-1034 </papid>proposed to train in supervised way (using some parallel data) classifier designed to recognize parallel sentences.</citsent>
<aftsection>
<nextsent>they applied their classifier on two monolingual news corpora in arabic and english,covering similar periods, and showed that the parallel material extracted, when added to an in-domainparallel training corpus of united nation texts, improved significantly an arabic-to-english smt system tested on news data.
</nextsent>
<nextsent>still, they noted that the extracted material does not come close to the quality obtained by adding small out-domain parallel corpus to the in-domain training material.
</nextsent>
<nextsent>different variants of this approach have been tried afterwards, e.g.
</nextsent>
<nextsent>(abdul-rauf and schwenk, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3784">
<title id=" W10-4165.xml">triplet based chinese word sense induction </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>at last we discuss the improvement and the weakness of our system.
</prevsent>
<prevsent>sense induction is typically treated as aclustering problem, by considering their cooccurring contexts, the instances of target word are partitioned into classes.
</prevsent>
</prevsection>
<citsent citstr=" W97-0322 ">
previous methods have used the first or second order co-occurrence (pedersen and bruce, 1997;<papid> W97-0322 </papid>schtze, 1998), parts of speech, and local collocations (niu et al, 2007).<papid> W07-2037 </papid></citsent>
<aftsection>
<nextsent>the size of context window is also various, it can be as small as only two words before and after the targetwords.
</nextsent>
<nextsent>it may be the sentence where the target word is in.
</nextsent>
<nextsent>or it will be 20 surrounding words on either side of the target words and even more words.
</nextsent>
<nextsent>after every instance of the target word is represented as feature vector, it will be the input of the clustering methods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3785">
<title id=" W10-4165.xml">triplet based chinese word sense induction </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>at last we discuss the improvement and the weakness of our system.
</prevsent>
<prevsent>sense induction is typically treated as aclustering problem, by considering their cooccurring contexts, the instances of target word are partitioned into classes.
</prevsent>
</prevsection>
<citsent citstr=" W07-2037 ">
previous methods have used the first or second order co-occurrence (pedersen and bruce, 1997;<papid> W97-0322 </papid>schtze, 1998), parts of speech, and local collocations (niu et al, 2007).<papid> W07-2037 </papid></citsent>
<aftsection>
<nextsent>the size of context window is also various, it can be as small as only two words before and after the targetwords.
</nextsent>
<nextsent>it may be the sentence where the target word is in.
</nextsent>
<nextsent>or it will be 20 surrounding words on either side of the target words and even more words.
</nextsent>
<nextsent>after every instance of the target word is represented as feature vector, it will be the input of the clustering methods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3789">
<title id=" W10-4165.xml">triplet based chinese word sense induction </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>sib (sequential information bottle neck) variation of information bottleneck is applied in (niu et al, 2007).<papid> W07-2037 </papid></prevsent>
<prevsent>in (dorow and widdows, 2003) graph-based clustering algorithm is employed that in graph node represents noun and two nodes have an edge between them if they co-occur in list more than given number of times.</prevsent>
</prevsection>
<citsent citstr=" E09-1013 ">
a generative model based on lda is proposed in (brody and la pata, 2009).<papid> E09-1013 </papid></citsent>
<aftsection>
<nextsent>in our method, we use the triplets (bordag, 2006) <papid> E06-1018 </papid>and their intersections from the internet to construct the feature vectors then sib is used as the clustering method.</nextsent>
<nextsent>our method select the features of the words similar to (bordag, 2006) <papid> E06-1018 </papid>is also using thetriplets.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3790">
<title id=" W10-4165.xml">triplet based chinese word sense induction </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>in (dorow and widdows, 2003) graph-based clustering algorithm is employed that in graph node represents noun and two nodes have an edge between them if they co-occur in list more than given number of times.
</prevsent>
<prevsent>a generative model based on lda is proposed in (brody and la pata, 2009).<papid> E09-1013 </papid></prevsent>
</prevsection>
<citsent citstr=" E06-1018 ">
in our method, we use the triplets (bordag, 2006) <papid> E06-1018 </papid>and their intersections from the internet to construct the feature vectors then sib is used as the clustering method.</citsent>
<aftsection>
<nextsent>our method select the features of the words similar to (bordag, 2006) <papid> E06-1018 </papid>is also using thetriplets.</nextsent>
<nextsent>in chinese there are no natural separators between the words as english, so the first step in chinese language processing is of ten the chinese word segmentation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3796">
<title id=" W11-0409.xml">crowdsourcing word sense definition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the methodology we propose explicitly addresses the question of related word senses and fuzzy boundaries between them,without trying to establish hard divisions where empirically there are none.the proposed method uses amazons mechanical turk for sense annotation.
</prevsent>
<prevsent>over the last several of years, mechanical turk, introduced by amazonas artificial artificial intelligence?, has been used successfully for number of nlp tasks, including robust evaluation of machine translation systems by reading comprehension (callison-burch, 2009), and other tasks explored in the recent naacl workshop (callison-burch and dredze, 2010b).
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
mechanical turk has also been used to create labeled datasets for word sense disambiguation (snow et al, 2008)<papid> D08-1027 </papid>and even to modify sense inventories.</citsent>
<aftsection>
<nextsent>but the original sense inventory construction has always been left to the experts.
</nextsent>
<nextsent>in contrast, in the annotation method we describe, the expert is eliminated from the annotation process.
</nextsent>
<nextsent>as has been the case with using mechanical turk for other nlp tasks, the proposed annotation is quite inexpensive and can be done very quickly, while maintaining expert-level annotation quality.
</nextsent>
<nextsent>the resulting resource will produce several ways to empirically measure distances between senses,and should help to address some open research questions regarding word sense perceptions by nativespeakers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3797">
<title id=" W11-0409.xml">crowdsourcing word sense definition </title>
<section> the problem of sense definition.  </section>
<citcontext>
<prevsection>
<prevsent>the goal is to provide more accurate representation the way speakers of language conceptualize senses, which can be used for training and testing of the automated wsd systems, as well as to automatically induce semantic and syntactic context patterns that represent usage norms and permit native speakers to perform sense disambiguation.
</prevsent>
<prevsent>the quality of the annotated corpora depends directly on the selected sense inventory, so, forex ample, semcor (landes et al, 1998), which used wordnet synsets, inherited all the associated problems, including using senses that are too fine grained and in many cases poorly distinguished.
</prevsent>
</prevsection>
<citsent citstr=" W04-0807 ">
at the senseval competitions (mihalcea et al, 2004; <papid> W04-0807 </papid>snyder and palmer, 2004; <papid> W04-0811 </papid>preiss and yarowsky,2001), the choice of sense inventory also frequently presented problems, spurring the efforts to create coarser-grained sense inventories (navigli,2006; <papid> P06-1014 </papid>hovy et al, 2006; <papid> N06-2015 </papid>palmer et al, 2007).</citsent>
<aftsection>
<nextsent>inventories derived from wordnet by using small-scale corpus analysis and by automatic mapping to top entries in oxford dictionary of english were usedin the recent workshops on semantic evaluation, including semeval-2007 and semeval-2010 (agirre et al., 2007; erk and strapparava, 2010).
</nextsent>
<nextsent>several current resource-oriented projects attempt to formalize the procedure of creating sense inventory.
</nextsent>
<nextsent>framenet (ruppenhofer et al, 2006) attempt sto organize lexical information in terms of script like semantic frames, with semantic and syntactic combinatorial possibilities specified for each frame evoking lexical unit (word/sense pairing).
</nextsent>
<nextsent>corpus pattern analysis (cpa) (hanks and pustejovsky,2005) attempts to catalog norms of usage for individual words, specifying them in terms of context patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3798">
<title id=" W11-0409.xml">crowdsourcing word sense definition </title>
<section> the problem of sense definition.  </section>
<citcontext>
<prevsection>
<prevsent>the goal is to provide more accurate representation the way speakers of language conceptualize senses, which can be used for training and testing of the automated wsd systems, as well as to automatically induce semantic and syntactic context patterns that represent usage norms and permit native speakers to perform sense disambiguation.
</prevsent>
<prevsent>the quality of the annotated corpora depends directly on the selected sense inventory, so, forex ample, semcor (landes et al, 1998), which used wordnet synsets, inherited all the associated problems, including using senses that are too fine grained and in many cases poorly distinguished.
</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
at the senseval competitions (mihalcea et al, 2004; <papid> W04-0807 </papid>snyder and palmer, 2004; <papid> W04-0811 </papid>preiss and yarowsky,2001), the choice of sense inventory also frequently presented problems, spurring the efforts to create coarser-grained sense inventories (navigli,2006; <papid> P06-1014 </papid>hovy et al, 2006; <papid> N06-2015 </papid>palmer et al, 2007).</citsent>
<aftsection>
<nextsent>inventories derived from wordnet by using small-scale corpus analysis and by automatic mapping to top entries in oxford dictionary of english were usedin the recent workshops on semantic evaluation, including semeval-2007 and semeval-2010 (agirre et al., 2007; erk and strapparava, 2010).
</nextsent>
<nextsent>several current resource-oriented projects attempt to formalize the procedure of creating sense inventory.
</nextsent>
<nextsent>framenet (ruppenhofer et al, 2006) attempt sto organize lexical information in terms of script like semantic frames, with semantic and syntactic combinatorial possibilities specified for each frame evoking lexical unit (word/sense pairing).
</nextsent>
<nextsent>corpus pattern analysis (cpa) (hanks and pustejovsky,2005) attempts to catalog norms of usage for individual words, specifying them in terms of context patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3799">
<title id=" W11-0409.xml">crowdsourcing word sense definition </title>
<section> the problem of sense definition.  </section>
<citcontext>
<prevsection>
<prevsent>the goal is to provide more accurate representation the way speakers of language conceptualize senses, which can be used for training and testing of the automated wsd systems, as well as to automatically induce semantic and syntactic context patterns that represent usage norms and permit native speakers to perform sense disambiguation.
</prevsent>
<prevsent>the quality of the annotated corpora depends directly on the selected sense inventory, so, forex ample, semcor (landes et al, 1998), which used wordnet synsets, inherited all the associated problems, including using senses that are too fine grained and in many cases poorly distinguished.
</prevsent>
</prevsection>
<citsent citstr=" P06-1014 ">
at the senseval competitions (mihalcea et al, 2004; <papid> W04-0807 </papid>snyder and palmer, 2004; <papid> W04-0811 </papid>preiss and yarowsky,2001), the choice of sense inventory also frequently presented problems, spurring the efforts to create coarser-grained sense inventories (navigli,2006; <papid> P06-1014 </papid>hovy et al, 2006; <papid> N06-2015 </papid>palmer et al, 2007).</citsent>
<aftsection>
<nextsent>inventories derived from wordnet by using small-scale corpus analysis and by automatic mapping to top entries in oxford dictionary of english were usedin the recent workshops on semantic evaluation, including semeval-2007 and semeval-2010 (agirre et al., 2007; erk and strapparava, 2010).
</nextsent>
<nextsent>several current resource-oriented projects attempt to formalize the procedure of creating sense inventory.
</nextsent>
<nextsent>framenet (ruppenhofer et al, 2006) attempt sto organize lexical information in terms of script like semantic frames, with semantic and syntactic combinatorial possibilities specified for each frame evoking lexical unit (word/sense pairing).
</nextsent>
<nextsent>corpus pattern analysis (cpa) (hanks and pustejovsky,2005) attempts to catalog norms of usage for individual words, specifying them in terms of context patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3800">
<title id=" W11-0409.xml">crowdsourcing word sense definition </title>
<section> the problem of sense definition.  </section>
<citcontext>
<prevsection>
<prevsent>the goal is to provide more accurate representation the way speakers of language conceptualize senses, which can be used for training and testing of the automated wsd systems, as well as to automatically induce semantic and syntactic context patterns that represent usage norms and permit native speakers to perform sense disambiguation.
</prevsent>
<prevsent>the quality of the annotated corpora depends directly on the selected sense inventory, so, forex ample, semcor (landes et al, 1998), which used wordnet synsets, inherited all the associated problems, including using senses that are too fine grained and in many cases poorly distinguished.
</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
at the senseval competitions (mihalcea et al, 2004; <papid> W04-0807 </papid>snyder and palmer, 2004; <papid> W04-0811 </papid>preiss and yarowsky,2001), the choice of sense inventory also frequently presented problems, spurring the efforts to create coarser-grained sense inventories (navigli,2006; <papid> P06-1014 </papid>hovy et al, 2006; <papid> N06-2015 </papid>palmer et al, 2007).</citsent>
<aftsection>
<nextsent>inventories derived from wordnet by using small-scale corpus analysis and by automatic mapping to top entries in oxford dictionary of english were usedin the recent workshops on semantic evaluation, including semeval-2007 and semeval-2010 (agirre et al., 2007; erk and strapparava, 2010).
</nextsent>
<nextsent>several current resource-oriented projects attempt to formalize the procedure of creating sense inventory.
</nextsent>
<nextsent>framenet (ruppenhofer et al, 2006) attempt sto organize lexical information in terms of script like semantic frames, with semantic and syntactic combinatorial possibilities specified for each frame evoking lexical unit (word/sense pairing).
</nextsent>
<nextsent>corpus pattern analysis (cpa) (hanks and pustejovsky,2005) attempts to catalog norms of usage for individual words, specifying them in terms of context patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3801">
<title id=" W11-0409.xml">crowdsourcing word sense definition </title>
<section> the problem of sense definition.  </section>
<citcontext>
<prevsection>
<prevsent>corpus pattern analysis (cpa) (hanks and pustejovsky,2005) attempts to catalog norms of usage for individual words, specifying them in terms of context patterns.
</prevsent>
<prevsent>other large-scale resource-building projects also use corpus analysis techniques.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
in propbank (palmer et al, 2005), <papid> J05-1004 </papid>verb senses we redefined based on their use in wall street journal corpus and specified in terms of frame sets which consist of set of semantic roles for the arguments of particular sense.</citsent>
<aftsection>
<nextsent>in the ontonotes project (hovy etal., 2006), <papid> N06-2015 </papid>annotators use small-scale corpus analysis to create sense inventories derived by grouping together wordnet senses, with the procedure restricted to maintain 90% inter-annotator agreement.importantly, most standard wsd resources contain no information about the clarity of distinctions between different senses in the sense inventory.</nextsent>
<nextsent>for example, ontonotes, which was used for evaluation in the word sense disambiguation and sense induction tasks in the latest semeval competitions contains no information about sense hierarchy, related senses, or difficulty and consistency of given set of senses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3803">
<title id=" W11-0409.xml">crowdsourcing word sense definition </title>
<section> pilot annotations.  </section>
<citcontext>
<prevsection>
<prevsent>we propose the following to be done as part of this pilot study.
</prevsent>
<prevsent>for selected sample of polysemous words: ? extract several hundred instances for each word from the english part of multilingual corpus, such as the europarl (koehn, 2005); 2?
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
use the best mturk annotation procedure as established in sec 5.2 to cluster the extracted in stances; ? obtain translation equivalents for each instance of the target word using word-alignment produced with giza++ (och and ney, 2000); ? <papid> P00-1056 </papid>compute the distances between the obtained clusters by estimating the probability of different lexicalization of the two senses from the word-aligned parallel corpus.the distances would then be computed using multilingual cost function similar to the one used by resnik and yarowsky (1999), shown in figure 5.3.the europarl corpus contains indo-european languages (except for finnish), predominantly of theromanic and germanic family.</citsent>
<aftsection>
<nextsent>these languages of ten have parallel sense distinctions.
</nextsent>
<nextsent>if that proves to be the case, small additional parallel corpus with the data from other non-european languages would need to be used to supplement the data from eu roparl.
</nextsent>
<nextsent>in this paper, we have presented proposal for new annotation strategy for obtaining sense-annotateddata wsd/wsi applications, together with the corresponding sense inventories, using non-expert annotators.
</nextsent>
<nextsent>we have described set of pilot studies that would need to be conducted prior to applying this strategy in large-scale annotation effort.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3804">
<title id=" W11-0418.xml">annotating events temporal expressions and relations in italian the ittimeml experience for the itatimebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, the adaptation of the timeml scheme to italian is described, and special attention is given to the methodology used for the realization of the annotation specifications, which are strategic in order to create good quality annotated resources and to justify the annotated items.
</prevsent>
<prevsent>the reliability of the it-timeml guidelines and specifications is evaluated on the basis of the results of the inter-coder agreement performed during the annotation of the two corpora.
</prevsent>
</prevsection>
<citsent citstr=" S10-1010 ">
in recent years renewed interest in temporal processing has spread in the nlp community, thanks to the success of the timeml annotation scheme (pustejovsky et al, 2003a) and to the availability of annotated resources, such as the english and french time banks (pustejovsky et al., 2003b; bittar, 2010) and the tempe val corpora (verhagen et al, 2010).<papid> S10-1010 </papid></citsent>
<aftsection>
<nextsent>the iso tc 37 / sc 4 initiative (terminology and other language and content resources?)
</nextsent>
<nextsent>and the tempeval-2 contest have contributed to the development of timeml compliant annotation schemes in languages other than english, namely spanish, korean, chinese, french and italian.
</nextsent>
<nextsent>once the corresponding corpora will be completed and made available, the nlp community will benefit from having access to different language resources with common layer of annotation which could boost studies in multilingual temporal processing and improve the performance of complex multilingual nlp systems, such as question-answering and textual entailment.
</nextsent>
<nextsent>this paper focuses on the annotation guidelines and specifications which have been developed for the creation of the italian time bank (hereafter, ita-timebank).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3805">
<title id=" W11-0418.xml">annotating events temporal expressions and relations in italian the ittimeml experience for the itatimebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, in section 5 conclusions and extensions to the current annotation effort will be reported.
</prevsent>
<prevsent>notice that, for clarity sake, in this paper the examples will focus only on the tag (or attribute or link) under discussion.
</prevsent>
</prevsection>
<citsent citstr=" W09-3417 ">
it-timeml: extensions and language specific issues applying an annotation scheme to language other than the one for which it was initially developed, requires careful study of the language specific issues related to the linguistic phenomena taken into account (im et al, 2009; <papid> W09-3417 </papid>bittar, 2008).</citsent>
<aftsection>
<nextsent>timeml focuses on events (i.e. actions, states, and processes -  event  tag), temporal expressions (i.e. durations, calendar dates, times of day and sets of time -  timex3  tag), signals (e.g. temporal prepositions and sub ordinators -  signal  tag) and various kind of dependencies between events and/or temporal expressions (i.e. temporal, aspect ual and subordination relations -  tlink ,  alink  and  slink  tags respectively).
</nextsent>
<nextsent>an iso language-independent specification of timeml is underdevelopment but it is still in the enquiry stage1.
</nextsent>
<nextsent>for this reason, in the following subsections we will mostly compare the italian annotation guidelines with the latest version of the english annotation guidelines (timeml working group, 2010), focusing on the two main tags, i.e  event  and  timex3 , in italian.
</nextsent>
<nextsent>the  event  tag is used to mark-up instances of eventualities (bach, 1986).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3806">
<title id=" W11-0418.xml">annotating events temporal expressions and relations in italian the ittimeml experience for the itatimebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the  event  tag is used to mark-up instances of eventualities (bach, 1986).
</prevsent>
<prevsent>this category comprises all types of actions (punctual or durative) and states as well.
</prevsent>
</prevsection>
<citsent citstr=" W01-1315 ">
with respect to 1 http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalog ue_detail.htmcsnumber=37331 previous annotations schemes (katz and arosio, 2001, <papid> W01-1315 </papid>filatova and hovy, 2001, <papid> W01-1313 </papid>setzer and gaizauskas, 2001 <papid> W01-1311 </papid>among other), timeml allows for annotating as events not only verbs but also nouns, adjectives and prepositional phrases.</citsent>
<aftsection>
<nextsent>in the adaptation to italian, two annotation principles adopted for english, that is an orientation towards surface linguistic phenomena and the notion of minimal chunk for the tag extent, have been preserved without major modifications.
</nextsent>
<nextsent>the main differences with respect to the english version rely i.)
</nextsent>
<nextsent>in the attribute list; and ii.)
</nextsent>
<nextsent>in the attributes values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3807">
<title id=" W11-0418.xml">annotating events temporal expressions and relations in italian the ittimeml experience for the itatimebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the  event  tag is used to mark-up instances of eventualities (bach, 1986).
</prevsent>
<prevsent>this category comprises all types of actions (punctual or durative) and states as well.
</prevsent>
</prevsection>
<citsent citstr=" W01-1313 ">
with respect to 1 http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalog ue_detail.htmcsnumber=37331 previous annotations schemes (katz and arosio, 2001, <papid> W01-1315 </papid>filatova and hovy, 2001, <papid> W01-1313 </papid>setzer and gaizauskas, 2001 <papid> W01-1311 </papid>among other), timeml allows for annotating as events not only verbs but also nouns, adjectives and prepositional phrases.</citsent>
<aftsection>
<nextsent>in the adaptation to italian, two annotation principles adopted for english, that is an orientation towards surface linguistic phenomena and the notion of minimal chunk for the tag extent, have been preserved without major modifications.
</nextsent>
<nextsent>the main differences with respect to the english version rely i.)
</nextsent>
<nextsent>in the attribute list; and ii.)
</nextsent>
<nextsent>in the attributes values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3808">
<title id=" W11-0418.xml">annotating events temporal expressions and relations in italian the ittimeml experience for the itatimebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the  event  tag is used to mark-up instances of eventualities (bach, 1986).
</prevsent>
<prevsent>this category comprises all types of actions (punctual or durative) and states as well.
</prevsent>
</prevsection>
<citsent citstr=" W01-1311 ">
with respect to 1 http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalog ue_detail.htmcsnumber=37331 previous annotations schemes (katz and arosio, 2001, <papid> W01-1315 </papid>filatova and hovy, 2001, <papid> W01-1313 </papid>setzer and gaizauskas, 2001 <papid> W01-1311 </papid>among other), timeml allows for annotating as events not only verbs but also nouns, adjectives and prepositional phrases.</citsent>
<aftsection>
<nextsent>in the adaptation to italian, two annotation principles adopted for english, that is an orientation towards surface linguistic phenomena and the notion of minimal chunk for the tag extent, have been preserved without major modifications.
</nextsent>
<nextsent>the main differences with respect to the english version rely i.)
</nextsent>
<nextsent>in the attribute list; and ii.)
</nextsent>
<nextsent>in the attributes values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3809">
<title id=" W11-0418.xml">annotating events temporal expressions and relations in italian the ittimeml experience for the itatimebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, the identification of reporting events showed to be problematic because of the vague definition adopted in the guidelines.
</prevsent>
<prevsent>a reporting event is giving information speech act in which communicator conveys message to an addressee.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
to help annotators in deciding whether an event is reporting one, the annotation specifications suggest to relyon framenet as starting point (baker, et al 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>more specifically, an italian lexical unit has been classified as reporting if it is the translation equivalent of one of the lexical units assigned to the communication frame, which has message as core element.
</nextsent>
<nextsent>among the frames using and inherited from the communication frame, only the ones having the message as core element and conveying giving information speech act have been selected and the lexical units belonging to them have been classified as reporting events: e.g. urlare [to scream] from the communication_noise frame, sottolineare [to stress] from the convey_importance frame, dichiarare [to declare] from the statement frame.
</nextsent>
<nextsent>similarly, for the identification of tlinks, set of decision trees has been developed to identify the conditions under which temporal relation is to be annotated and method to decide the value of the reltype attribute.
</nextsent>
<nextsent>for instance, the annotation of temporal relations between nominal events and temporal expressions in the same sentence is allowed only when the temporal expression is realized either by an adjective or prepositional phrase of the form   di (of) + temporal expression   e.g.: (12.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3810">
<title id=" W11-0418.xml">annotating events temporal expressions and relations in italian the ittimeml experience for the itatimebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluating annotations two corpora have been developed in parallel following the it-timeml annotation scheme, namely the celct corpus and the ilc corpus.
</prevsent>
<prevsent>once these two corpora will be completed and released, they will form the italian time bank providing the nlp community with the largest resource annotated with temporal and event information (more than 150k tokens).
</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
in this section, the two corpora are briefly described and the results of the inter-coder agreement (artstein and poesio, 2008) <papid> J08-4004 </papid>achieved during their annotation are compared in order to evaluate the quality of the guidelines and of the resources.</citsent>
<aftsection>
<nextsent>the celct corpus has been created within the live memories project2 and it consists of news stories taken from the italian content annotation bank (i-cab, magnini et al, 2006).
</nextsent>
<nextsent>more than 180,000 tokens have been annotated with temporal expressions and more than 90,000 tokens have been annotated also with events, signals and links.
</nextsent>
<nextsent>the brandeis annotation tool3 (bat) has been used for the pilot annotation and for the automatic computation of the inter-coder agreement on the extent and the attributes of temporal expressions, events and signals.
</nextsent>
<nextsent>after the pilot annotation, the first prototype of the celct annotation tool (cat) has been used to perform the annotation and to compute the inter-coder agreement on links.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3811">
<title id=" W10-4127.xml">a multilayer chinese word segmentation system optimized for outofdomain tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by training second layer of large margin classifier on top of the outputs from several conditional random fields classifiers, it can utilize small amount of in-domain training data to improve the performance.experimental results show consistent improvement on f1 scores and oov recall rates by applying the approach.
</prevsent>
<prevsent>the chinese word segmentation problem has been intensively investigated in the past two decades.
</prevsent>
</prevsection>
<citsent citstr=" W03-1709 ">
from lexicon-based methods such as bi-directed maximum match (bdmm) (chen et al, 2005) to statistical models such as hidden markove model (hmm) (zhang et al, 2003), <papid> W03-1709 </papid>broad spectrum of approaches have been experimented.</citsent>
<aftsection>
<nextsent>by casting the problem as character labeling task, sequence labeling models such as conditional random fields can be applied on the problem (xueand shen, 2003).<papid> W03-1728 </papid></nextsent>
<nextsent>state-of-the-art crf-based systems have achieved good performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3812">
<title id=" W10-4127.xml">a multilayer chinese word segmentation system optimized for outofdomain tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the chinese word segmentation problem has been intensively investigated in the past two decades.
</prevsent>
<prevsent>from lexicon-based methods such as bi-directed maximum match (bdmm) (chen et al, 2005) to statistical models such as hidden markove model (hmm) (zhang et al, 2003), <papid> W03-1709 </papid>broad spectrum of approaches have been experimented.</prevsent>
</prevsection>
<citsent citstr=" W03-1728 ">
by casting the problem as character labeling task, sequence labeling models such as conditional random fields can be applied on the problem (xueand shen, 2003).<papid> W03-1728 </papid></citsent>
<aftsection>
<nextsent>state-of-the-art crf-based systems have achieved good performance.
</nextsent>
<nextsent>however,like many machine learning problems, generalizability is crucial for domain-independent segmentation system.
</nextsent>
<nextsent>because the training data usually come from limited domains, when the domain of test data is different from the training data, the results are still not satisfactory.a straight-forward solution is to obtain more labeled data in the domain we want to test.
</nextsent>
<nextsent>however this is not easily achievable because the amount of data needed to train segmentation system are large.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3813">
<title id=" W10-4127.xml">a multilayer chinese word segmentation system optimized for outofdomain tasks </title>
<section> crf-based sub-systems.  </section>
<citcontext>
<prevsection>
<prevsent>the tag set weuse is the 6-tag (b1, b2, b3, m, e, s) set proposed by zhao et al(2006).
</prevsent>
<prevsent>all of the sub-systems use the same tag set, however as we will see later, the second-layer classifier in our system does not require the sub-systems to have common tagset.
</prevsent>
</prevsection>
<citsent citstr=" I08-4017 ">
also, all of the sub-systems include common set of character features proposed in (zhao and kit, 2008).<papid> I08-4017 </papid></citsent>
<aftsection>
<nextsent>the offsets and concatenations of the six n-gram features (the feature template) are: c1, c0, c1, c1c0, c0c1, c1c1.
</nextsent>
<nextsent>in the remaining part of the section we will introduce other features that we employed in different subsystems.
</nextsent>
<nextsent>2.1 character type features.
</nextsent>
<nextsent>by simply classify the characters into four types: punctuation (p), digits (d), roman letters (l)and chinese characters (c), we can assign character type tags to every character.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3814">
<title id=" W11-0131.xml">structured composition of semantic vectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since semantic information is used in syntactic disambiguation (macdonald et al , 1994), we would expect practical improvements in parsing accuracy by accounting for the interactive interpretation process.others have incorporated syntactic information with vector-space semantics, challenging the bag of-words assumption.
</prevsent>
<prevsent>syntax and semantics may be jointly generated with bayesian methods (griffiths et al , 2005); syntactic structure may be coupled to the basis elements of semantic space (pado?
</prevsent>
</prevsection>
<citsent citstr=" P08-1068 ">
and lapata, 2007); clustered semantics may be used as pre-processing step (koo et al , 2008); <papid> P08-1068 </papid>or, semantics may be learned in some defined syntactic context (lin, 1998).</citsent>
<aftsection>
<nextsent>these techniques are interactive, but their semantic models are not syntactically compositional (frege, 1892).
</nextsent>
<nextsent>svs is generative model of sentences that uses variant of the last strategy to incorporate syntax at pre terminal tree nodes, but is inherently compositional.
</nextsent>
<nextsent>mitchell and lapata (2008) <papid> P08-1028 </papid>provide general framework for semantic vector composition: = f(u,v,r,k) (1) 295 where and are the vectors to be composed, is syntactic context, is semantic knowledge base, and is resulting composed vector (or tensor).</nextsent>
<nextsent>in this initial work of theirs, they leave out any notion of syntactic context, focusing on additive and multiplicative vector composition (with some variations): add: p[i] = u[i] + v[i] mult: p[i] = u[i] ? v[i] (2)since the structured vector ial semantics proposed here may be viewed within this framework, our discussion will begin from their definition in section 2.1.erk and pados (2008) model also fits inside mitchell and lapatas framework, and like svs, it includes syntactic context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3815">
<title id=" W11-0131.xml">structured composition of semantic vectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these techniques are interactive, but their semantic models are not syntactically compositional (frege, 1892).
</prevsent>
<prevsent>svs is generative model of sentences that uses variant of the last strategy to incorporate syntax at pre terminal tree nodes, but is inherently compositional.
</prevsent>
</prevsection>
<citsent citstr=" P08-1028 ">
mitchell and lapata (2008) <papid> P08-1028 </papid>provide general framework for semantic vector composition: = f(u,v,r,k) (1) 295 where and are the vectors to be composed, is syntactic context, is semantic knowledge base, and is resulting composed vector (or tensor).</citsent>
<aftsection>
<nextsent>in this initial work of theirs, they leave out any notion of syntactic context, focusing on additive and multiplicative vector composition (with some variations): add: p[i] = u[i] + v[i] mult: p[i] = u[i] ? v[i] (2)since the structured vector ial semantics proposed here may be viewed within this framework, our discussion will begin from their definition in section 2.1.erk and pados (2008) model also fits inside mitchell and lapatas framework, and like svs, it includes syntactic context.
</nextsent>
<nextsent>their semantic vectors use syntactic information as relations between multiple vectors in arriving at final meaning representation.
</nextsent>
<nextsent>the emphasis, however, is on selectional preferences of individual words; resulting representations are similar to word-sense disambiguation output, and do not construct phrase-level meaning from word meaning.
</nextsent>
<nextsent>mitchell and lapatas more recent work (2009) combines syntactic parses with distributional semantics; but the underlying compositional model requires (as other existing models would) an interpolation of the vector composition results with separate parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3817">
<title id=" W11-0131.xml">structured composition of semantic vectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, unlike previous models, unique phrasal vector ial semantic representation is composed during decoding.
</prevsent>
<prevsent>due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
the structured vector ial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed:lexicalized parsing (charniak, 1996; collins, 1997, <papid> P97-1003 </papid>etc.) and relational clustering (akin to latent annotations (matsuzaki et al , 2005; <papid> P05-1010 </papid>petrov et al , 2006; <papid> P06-1055 </papid>gesmundo et al , 2009)).<papid> W09-1205 </papid></citsent>
<aftsection>
<nextsent>because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (klein and manning, 2003), <papid> P03-1054 </papid>syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and in transitive verbs).</nextsent>
<nextsent>this pessimistically isolates the contribution of semantics on parsing accuracy ? it will only show parsing gains where semantic information does not overlap with distributional syntactic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3818">
<title id=" W11-0131.xml">structured composition of semantic vectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, unlike previous models, unique phrasal vector ial semantic representation is composed during decoding.
</prevsent>
<prevsent>due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing.
</prevsent>
</prevsection>
<citsent citstr=" P05-1010 ">
the structured vector ial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed:lexicalized parsing (charniak, 1996; collins, 1997, <papid> P97-1003 </papid>etc.) and relational clustering (akin to latent annotations (matsuzaki et al , 2005; <papid> P05-1010 </papid>petrov et al , 2006; <papid> P06-1055 </papid>gesmundo et al , 2009)).<papid> W09-1205 </papid></citsent>
<aftsection>
<nextsent>because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (klein and manning, 2003), <papid> P03-1054 </papid>syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and in transitive verbs).</nextsent>
<nextsent>this pessimistically isolates the contribution of semantics on parsing accuracy ? it will only show parsing gains where semantic information does not overlap with distributional syntactic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3819">
<title id=" W11-0131.xml">structured composition of semantic vectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, unlike previous models, unique phrasal vector ial semantic representation is composed during decoding.
</prevsent>
<prevsent>due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing.
</prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
the structured vector ial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed:lexicalized parsing (charniak, 1996; collins, 1997, <papid> P97-1003 </papid>etc.) and relational clustering (akin to latent annotations (matsuzaki et al , 2005; <papid> P05-1010 </papid>petrov et al , 2006; <papid> P06-1055 </papid>gesmundo et al , 2009)).<papid> W09-1205 </papid></citsent>
<aftsection>
<nextsent>because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (klein and manning, 2003), <papid> P03-1054 </papid>syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and in transitive verbs).</nextsent>
<nextsent>this pessimistically isolates the contribution of semantics on parsing accuracy ? it will only show parsing gains where semantic information does not overlap with distributional syntactic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3820">
<title id=" W11-0131.xml">structured composition of semantic vectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, unlike previous models, unique phrasal vector ial semantic representation is composed during decoding.
</prevsent>
<prevsent>due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing.
</prevsent>
</prevsection>
<citsent citstr=" W09-1205 ">
the structured vector ial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed:lexicalized parsing (charniak, 1996; collins, 1997, <papid> P97-1003 </papid>etc.) and relational clustering (akin to latent annotations (matsuzaki et al , 2005; <papid> P05-1010 </papid>petrov et al , 2006; <papid> P06-1055 </papid>gesmundo et al , 2009)).<papid> W09-1205 </papid></citsent>
<aftsection>
<nextsent>because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (klein and manning, 2003), <papid> P03-1054 </papid>syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and in transitive verbs).</nextsent>
<nextsent>this pessimistically isolates the contribution of semantics on parsing accuracy ? it will only show parsing gains where semantic information does not overlap with distributional syntactic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3821">
<title id=" W11-0131.xml">structured composition of semantic vectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing.
</prevsent>
<prevsent>the structured vector ial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed:lexicalized parsing (charniak, 1996; collins, 1997, <papid> P97-1003 </papid>etc.) and relational clustering (akin to latent annotations (matsuzaki et al , 2005; <papid> P05-1010 </papid>petrov et al , 2006; <papid> P06-1055 </papid>gesmundo et al , 2009)).<papid> W09-1205 </papid></prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (klein and manning, 2003), <papid> P03-1054 </papid>syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and in transitive verbs).</citsent>
<aftsection>
<nextsent>this pessimistically isolates the contribution of semantics on parsing accuracy ? it will only show parsing gains where semantic information does not overlap with distributional syntactic information.
</nextsent>
<nextsent>evaluations show that interactively considering semantic information with syntax has the predicted positive impact on parsing accuracy over syntax alone; it also lowers per-word perplexity.the remainder of this paper is organized as follows: section 2 describes svs as both vector composition and parsing; section 3 shows how relational-clustering svs subsumes pcfg-las; and section 4 evaluates modeling assumptions and empirical performance.
</nextsent>
<nextsent>2.1 vector composition.
</nextsent>
<nextsent>we begin with some notation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3824">
<title id=" W11-0131.xml">structured composition of semantic vectors </title>
<section> structured vector ial semantics.  </section>
<citcontext>
<prevsection>
<prevsent>of note in this definition of f(?)
</prevsent>
<prevsent>is the presence of matrices that operate on distributed semantic vectors.
</prevsent>
</prevsection>
<citsent citstr=" P10-1093 ">
while it is widely understood that matrices can represent transformations, relatively few have used matrices to represent the distributed, dynamic nature of meaning composition (see rudolph and giesbrecht (2010) <papid> P10-1093 </papid>for counterexample).</citsent>
<aftsection>
<nextsent>2.2 syntax semantics interface.
</nextsent>
<nextsent>this section aims to more thoroughly define the way in which the syntax and semantics interact during structured vector ial semantic composition.
</nextsent>
<nextsent>svs will specify this interface such that the composition of semantic vectors is probabilistically consistent and subsumes parsing under various frameworks.
</nextsent>
<nextsent>parsing has at times added semantic annotations that unwittingly carry some semantic value: headwords (collins, 1997) <papid> P97-1003 </papid>are one-word concepts that subsume the words below them; latent annotations (matsuzaki et al , 2005) <papid> P05-1010 </papid>are clustered concepts that touch on both syntactic and semantic information at node.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3828">
<title id=" W11-0131.xml">structured composition of semantic vectors </title>
<section> structured vector ial semantics.  </section>
<citcontext>
<prevsection>
<prevsent>svs will specify this interface such that the composition of semantic vectors is probabilistically consistent and subsumes parsing under various frameworks.
</prevsent>
<prevsent>parsing has at times added semantic annotations that unwittingly carry some semantic value: headwords (collins, 1997) <papid> P97-1003 </papid>are one-word concepts that subsume the words below them; latent annotations (matsuzaki et al , 2005) <papid> P05-1010 </papid>are clustered concepts that touch on both syntactic and semantic information at node.</prevsent>
</prevsection>
<citsent citstr=" W05-0602 ">
of course, other annotations (ge and mooney, 2005) <papid> W05-0602 </papid>carry more explicit forms of semantics.</citsent>
<aftsection>
<nextsent>in this light, semantic concepts (vector indices i) and relation labels (matrix arguments l) may also be seen as annotations on grammar trees.
</nextsent>
<nextsent>let us introduce notation to make the connection with parsing and syntax explicit.
</nextsent>
<nextsent>this paper will denote syntactic categories as and string yields as x. the location of these variables in phrase structure will be identified using sub scripts that describe the path from the root to the constituent.1 paths consist of left and/or right branches (indicated by 0s and 1s, respectively, as in figure 1a).
</nextsent>
<nextsent>variables ?, ?, and ? stand for whole paths; ? is the path of composed vector; and  is the empty path at the root.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3831">
<title id=" W11-0131.xml">structured composition of semantic vectors </title>
<section> svs with relational clusters.  </section>
<citcontext>
<prevsection>
<prevsent>from parsing perspective, this amounts to latent annotations (matsuzaki et al , 2005) <papid> P05-1010 </papid>in l-context.</prevsent>
<prevsent>299 let us re-notate the headword-lexicalized version of svs (the example in section 2.2.1) using hfor headword semantics, and reserve for relationally-clustered concepts.</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
treebank trees can be deterministically annotated with headwords and relations by using head rules (magerman, 1995).<papid> P95-1037 </papid></citsent>
<aftsection>
<nextsent>the 5 svs models m, l, p-vit(g), pig, and pig can thus be obtained by counting instances and normalizing.
</nextsent>
<nextsent>empirical probabilities of this kind are denoted with tilde, whereas estimated models have hat.
</nextsent>
<nextsent>concepts in distributed semantic representation, however, cannot be found from annotated trees (see example concepts in figure 2).
</nextsent>
<nextsent>therefore, we use expectation maximization (em) in variant ofthe inside-outside algorithm (baker, 1979) to learn distributed-concept behavior.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3835">
<title id=" W11-0131.xml">structured composition of semantic vectors </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>4.5 perplexity.
</prevsent>
<prevsent>finally, per-word perplexities were calculated for syntactic model and for 5-concept relation ally clustered model.
</prevsent>
</prevsection>
<citsent citstr=" D09-1045 ">
specific to this evaluation, following mitchell and lapata (2009), <papid> D09-1045 </papid>only the top 20,000 words in wsj sections 02-21 were kept in training or test sentences, and the rest replaced with unk?; numbers were replaced with num.?</citsent>
<aftsection>
<nextsent>sec.
</nextsent>
<nextsent>23, unk?+num?
</nextsent>
<nextsent>perplexity syntax only baseline 428.94 reln clust.
</nextsent>
<nextsent>1khw005e 371.76 table 2: model fit as measured by perplexity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3836">
<title id=" W10-4117.xml">exploiting social qampa collection in answering complex questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>among the approaches proposed to answer these questions, machine learning techniques have been found more effective in constructing qa components from scratch.
</prevsent>
<prevsent>yet these supervised techniques require certain scale of (question, answer), short for q&a;, pairs as training data.
</prevsent>
</prevsection>
<citsent citstr=" P05-1027 ">
for example,(echihabi et al, 2003) and (sasaki, 2005) <papid> P05-1027 </papid>constructed 90,000 english q&a; pairs and 2,000japanese q&a; pairs, respectively for their fac toid qa systems.</citsent>
<aftsection>
<nextsent>(cui et al, 2004) constructed1complex questions cannot be answered by simply extracting named entities.
</nextsent>
<nextsent>in this paper complex questions do not include definitional questions.
</nextsent>
<nextsent>76 term-definition pairs for their definitional qa systems.
</nextsent>
<nextsent>(stoyanov et al, 2005) <papid> H05-1116 </papid>required known subjective vocabulary for their opinion qa.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3837">
<title id=" W10-4117.xml">exploiting social qampa collection in answering complex questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper complex questions do not include definitional questions.
</prevsent>
<prevsent>76 term-definition pairs for their definitional qa systems.
</prevsent>
</prevsection>
<citsent citstr=" H05-1116 ">
(stoyanov et al, 2005) <papid> H05-1116 </papid>required known subjective vocabulary for their opinion qa.</citsent>
<aftsection>
<nextsent>(higashinaka and isozaki, 2008) used 4,849 positive and 521,177 negative examples in their reason qa system.
</nextsent>
<nextsent>among complex qa systems, many other types of questions have notbeen well studied, apart from reason and definitional questions.
</nextsent>
<nextsent>appendix lists 10 types of complex chinese questions and their examples we discussed in this paper.according to the related studies on qa, supervised machine-learning technique may be effective for answering these questions.
</nextsent>
<nextsent>to employ the supervised approach, we need to reconstruct training q&a; pairs for each type of question, though this is an extremely expensive and labor-intensive task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3838">
<title id=" W10-4117.xml">exploiting social qampa collection in answering complex questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to employ the supervised approach, we need to reconstruct training q&a; pairs for each type of question, though this is an extremely expensive and labor-intensive task.
</prevsent>
<prevsent>to deal with the acquisition problem of training q&a; pairs, we investigate techniques to automatically construct training data by utilizing social q&a; collections crawled from the web, which contains millions of user-generated q&a; pairs.
</prevsent>
</prevsection>
<citsent citstr=" P08-1082 ">
many studies (surdeanu et al, 2008)<papid> P08-1082 </papid>duan et al, 2008) <papid> P08-1019 </papid>have been done on retrieving similar q&a; pairs from social qa web sites as answers to test questions.our study, however, regards social q&a; web sites as knowledge repository and aims to mine knowledge from them for synthesizing answers to questions from multiple documents.</citsent>
<aftsection>
<nextsent>there is very little literature on this aspect.
</nextsent>
<nextsent>our work can be seen as kind of query-based summarization (dang, 2006) (harabagiu et al, 2006) (erkan and radev, 2004), and can also be employed to answer questions that have not been answered in social q&a; websites.
</nextsent>
<nextsent>this paper mainly focuses on the following threesteps: (1) automatically constructing question type-specific training q&a; pairs from the social q&a; collection; (2) extracting cue expressions for each type of question from the collected training data, and (3) building questiontype-specific classifiers to filer out noise sentences before using state-of-the-art ir formula to select answers.we evaluate our system on 10 types of chinese questions by using the pourpre evaluation tool (lin and demner-fushman, 2006).
</nextsent>
<nextsent>the experimental results show the effectiveness of our system, for instance, the f3/nr improvement of our system over the baseline and translation-based model reaches 7.9%/11.1%, and 5.1%/5.6%, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3840">
<title id=" W10-4117.xml">exploiting social qampa collection in answering complex questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to employ the supervised approach, we need to reconstruct training q&a; pairs for each type of question, though this is an extremely expensive and labor-intensive task.
</prevsent>
<prevsent>to deal with the acquisition problem of training q&a; pairs, we investigate techniques to automatically construct training data by utilizing social q&a; collections crawled from the web, which contains millions of user-generated q&a; pairs.
</prevsent>
</prevsection>
<citsent citstr=" P08-1019 ">
many studies (surdeanu et al, 2008)<papid> P08-1082 </papid>duan et al, 2008) <papid> P08-1019 </papid>have been done on retrieving similar q&a; pairs from social qa web sites as answers to test questions.our study, however, regards social q&a; web sites as knowledge repository and aims to mine knowledge from them for synthesizing answers to questions from multiple documents.</citsent>
<aftsection>
<nextsent>there is very little literature on this aspect.
</nextsent>
<nextsent>our work can be seen as kind of query-based summarization (dang, 2006) (harabagiu et al, 2006) (erkan and radev, 2004), and can also be employed to answer questions that have not been answered in social q&a; websites.
</nextsent>
<nextsent>this paper mainly focuses on the following threesteps: (1) automatically constructing question type-specific training q&a; pairs from the social q&a; collection; (2) extracting cue expressions for each type of question from the collected training data, and (3) building questiontype-specific classifiers to filer out noise sentences before using state-of-the-art ir formula to select answers.we evaluate our system on 10 types of chinese questions by using the pourpre evaluation tool (lin and demner-fushman, 2006).
</nextsent>
<nextsent>the experimental results show the effectiveness of our system, for instance, the f3/nr improvement of our system over the baseline and translation-based model reaches 7.9%/11.1%, and 5.1%/5.6%, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3841">
<title id=" W10-4117.xml">exploiting social qampa collection in answering complex questions </title>
<section> our complex qa system.  </section>
<citcontext>
<prevsection>
<prevsent>table 1 lists some questions, which, together with their best answers,are employed as the training data of the corresponding type of questions.
</prevsent>
<prevsent>for each type of question, we also randomly select some q&a; pairs that do not contain informers in questions as negative training data.
</prevsent>
</prevsection>
<citsent citstr=" H05-1054 ">
preprocessing of the training data, including word segmentation, pos tagging, and named entity (ne) tagging (wu et al., 2005), <papid> H05-1054 </papid>is conducted.</citsent>
<aftsection>
<nextsent>we also replace each ne with its tag type.
</nextsent>
<nextsent>qtype questions of q&a; pairs hazard type what are the hazards of the tro jan.psw.misc.kah virus?
</nextsent>
<nextsent>what are the hazards of rmb appreciation on chinas economy?
</nextsent>
<nextsent>hazards of smoke what are the hazards of contact lenses what are the hazards of waste accumula tion casualty type what were the casualties on either side from the u.s.-iraq war?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3842">
<title id=" W10-4117.xml">exploiting social qampa collection in answering complex questions </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>language.
</prevsent>
<prevsent>this model can be expressed by: (q|s) = ? wq ((1 ? ?)pmx(w|s) + pml(w|c)) pmx(w|s) = (1 ? ?)pml(w|s)+ ? ?
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
ts (w|t)pml(t|s) (5) where, is the question, the sentence, (w|t) the probability of translating sentence term to the question term w, which is obtained by using the giza++ toolkit (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>we use six million q&a; pairs to train ibm model 1 for obtaining word-to-word probability (w|t).
</nextsent>
<nextsent>ourserrorrate and ourspre@k denote our models that are based on classifiers optimizing performance measure error-rate and prec@k, respectively.
</nextsent>
<nextsent>ourslin, linear interpolation model, that combines scores of classifiers and the baseline, which is similar to (mori et al, 2008) and can be expressed by the equation: sim(q, s)?
</nextsent>
<nextsent>= sim(q, s) + ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3845">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>preliminary results indicate that the phrase-based approach performs better in terms of nist scores and produces paraphrases at greater distance from the source.
</prevsent>
<prevsent>one of the challenging properties of natural language is that the same semantic content can typically be expressed by many different surface forms.
</prevsent>
</prevsection>
<citsent citstr=" J10-3003 ">
asthe ability to deal with paraphrases holds great potential for improving the coverage of nlp systems,a substantial body of research addressing recognition, extraction and generation of paraphrases has emerged (androutsopoulos and malakasiotis, 2010; madnani and dorr, 2010).<papid> J10-3003 </papid></citsent>
<aftsection>
<nextsent>paraphrase generation can be regarded as translation task in which source and target language are the same.
</nextsent>
<nextsent>both paraphrase generation and machine translation (mt) are instances of text-to-text generation, which involves transforming one text into another, obeying certainrestrictions.
</nextsent>
<nextsent>here these restrictions are that the generated text must be grammatically well-formed and semantically/translationally equivalent to the source text.
</nextsent>
<nextsent>addionally paraphrase generation requires that the output should differ from the input to certain degree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3846">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>addionally paraphrase generation requires that the output should differ from the input to certain degree.
</prevsent>
<prevsent>the similarity between paraphrase generation and mt suggests that methods and tools originally developed for mt could be exploited for paraphrase generation.
</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
one popular approach ? arguably the most successful so far ? is statistical phrase-based machine translation (pbmt), which learns phrase translation rules from aligned bilingual text corpora (och et al, 1999; <papid> W99-0604 </papid>vogel et al, 2000; zens et al, 2002; koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>prior work has explored the use of pbmt for paraphrase generation (quirk etal., 2004; <papid> W04-3219 </papid>bannard and callison-burch, 2005; madnani et al, 2007; <papid> W07-0716 </papid>callison-burch, 2008; zhao et al, 2009; <papid> P09-1094 </papid>wubben et al, 2010) <papid> W10-4223 </papid>however, since many researchers believe thatpbmt has reached performance ceiling, ongoing research looks into more structural approaches to statistical mt (marcu and wong, 2002; <papid> W02-1018 </papid>och andney, 2004; <papid> J04-4002 </papid>khalilov and fonollosa, 2009).</nextsent>
<nextsent>syntax based mt attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3847">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>addionally paraphrase generation requires that the output should differ from the input to certain degree.
</prevsent>
<prevsent>the similarity between paraphrase generation and mt suggests that methods and tools originally developed for mt could be exploited for paraphrase generation.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
one popular approach ? arguably the most successful so far ? is statistical phrase-based machine translation (pbmt), which learns phrase translation rules from aligned bilingual text corpora (och et al, 1999; <papid> W99-0604 </papid>vogel et al, 2000; zens et al, 2002; koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>prior work has explored the use of pbmt for paraphrase generation (quirk etal., 2004; <papid> W04-3219 </papid>bannard and callison-burch, 2005; madnani et al, 2007; <papid> W07-0716 </papid>callison-burch, 2008; zhao et al, 2009; <papid> P09-1094 </papid>wubben et al, 2010) <papid> W10-4223 </papid>however, since many researchers believe thatpbmt has reached performance ceiling, ongoing research looks into more structural approaches to statistical mt (marcu and wong, 2002; <papid> W02-1018 </papid>och andney, 2004; <papid> J04-4002 </papid>khalilov and fonollosa, 2009).</nextsent>
<nextsent>syntax based mt attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3848">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the similarity between paraphrase generation and mt suggests that methods and tools originally developed for mt could be exploited for paraphrase generation.
</prevsent>
<prevsent>one popular approach ? arguably the most successful so far ? is statistical phrase-based machine translation (pbmt), which learns phrase translation rules from aligned bilingual text corpora (och et al, 1999; <papid> W99-0604 </papid>vogel et al, 2000; zens et al, 2002; koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
prior work has explored the use of pbmt for paraphrase generation (quirk etal., 2004; <papid> W04-3219 </papid>bannard and callison-burch, 2005; madnani et al, 2007; <papid> W07-0716 </papid>callison-burch, 2008; zhao et al, 2009; <papid> P09-1094 </papid>wubben et al, 2010) <papid> W10-4223 </papid>however, since many researchers believe thatpbmt has reached performance ceiling, ongoing research looks into more structural approaches to statistical mt (marcu and wong, 2002; <papid> W02-1018 </papid>och andney, 2004; <papid> J04-4002 </papid>khalilov and fonollosa, 2009).</citsent>
<aftsection>
<nextsent>syntax based mt attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages.
</nextsent>
<nextsent>syntactic information might lead to better results in the areaof grammatical well-formedness, and unlike phrase based mt that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns.
</nextsent>
<nextsent>while the verdict on whether or not this approach leads to any significant performance gain is still out, similar line of reasoning would suggest thatsyntax-based paraphrasing may offer similar advantages over phrase-based paraphrasing.
</nextsent>
<nextsent>considering the fact that the success of pbmt can partly be attributed to the abundance of large parallel corpora, 27 and that sufficiently large parallel corpora are still lacking for paraphrase generation, using more linguistically motivated methods might prove beneficial for paraphrase generation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3849">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the similarity between paraphrase generation and mt suggests that methods and tools originally developed for mt could be exploited for paraphrase generation.
</prevsent>
<prevsent>one popular approach ? arguably the most successful so far ? is statistical phrase-based machine translation (pbmt), which learns phrase translation rules from aligned bilingual text corpora (och et al, 1999; <papid> W99-0604 </papid>vogel et al, 2000; zens et al, 2002; koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" W07-0716 ">
prior work has explored the use of pbmt for paraphrase generation (quirk etal., 2004; <papid> W04-3219 </papid>bannard and callison-burch, 2005; madnani et al, 2007; <papid> W07-0716 </papid>callison-burch, 2008; zhao et al, 2009; <papid> P09-1094 </papid>wubben et al, 2010) <papid> W10-4223 </papid>however, since many researchers believe thatpbmt has reached performance ceiling, ongoing research looks into more structural approaches to statistical mt (marcu and wong, 2002; <papid> W02-1018 </papid>och andney, 2004; <papid> J04-4002 </papid>khalilov and fonollosa, 2009).</citsent>
<aftsection>
<nextsent>syntax based mt attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages.
</nextsent>
<nextsent>syntactic information might lead to better results in the areaof grammatical well-formedness, and unlike phrase based mt that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns.
</nextsent>
<nextsent>while the verdict on whether or not this approach leads to any significant performance gain is still out, similar line of reasoning would suggest thatsyntax-based paraphrasing may offer similar advantages over phrase-based paraphrasing.
</nextsent>
<nextsent>considering the fact that the success of pbmt can partly be attributed to the abundance of large parallel corpora, 27 and that sufficiently large parallel corpora are still lacking for paraphrase generation, using more linguistically motivated methods might prove beneficial for paraphrase generation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3850">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the similarity between paraphrase generation and mt suggests that methods and tools originally developed for mt could be exploited for paraphrase generation.
</prevsent>
<prevsent>one popular approach ? arguably the most successful so far ? is statistical phrase-based machine translation (pbmt), which learns phrase translation rules from aligned bilingual text corpora (och et al, 1999; <papid> W99-0604 </papid>vogel et al, 2000; zens et al, 2002; koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" P09-1094 ">
prior work has explored the use of pbmt for paraphrase generation (quirk etal., 2004; <papid> W04-3219 </papid>bannard and callison-burch, 2005; madnani et al, 2007; <papid> W07-0716 </papid>callison-burch, 2008; zhao et al, 2009; <papid> P09-1094 </papid>wubben et al, 2010) <papid> W10-4223 </papid>however, since many researchers believe thatpbmt has reached performance ceiling, ongoing research looks into more structural approaches to statistical mt (marcu and wong, 2002; <papid> W02-1018 </papid>och andney, 2004; <papid> J04-4002 </papid>khalilov and fonollosa, 2009).</citsent>
<aftsection>
<nextsent>syntax based mt attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages.
</nextsent>
<nextsent>syntactic information might lead to better results in the areaof grammatical well-formedness, and unlike phrase based mt that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns.
</nextsent>
<nextsent>while the verdict on whether or not this approach leads to any significant performance gain is still out, similar line of reasoning would suggest thatsyntax-based paraphrasing may offer similar advantages over phrase-based paraphrasing.
</nextsent>
<nextsent>considering the fact that the success of pbmt can partly be attributed to the abundance of large parallel corpora, 27 and that sufficiently large parallel corpora are still lacking for paraphrase generation, using more linguistically motivated methods might prove beneficial for paraphrase generation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3851">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the similarity between paraphrase generation and mt suggests that methods and tools originally developed for mt could be exploited for paraphrase generation.
</prevsent>
<prevsent>one popular approach ? arguably the most successful so far ? is statistical phrase-based machine translation (pbmt), which learns phrase translation rules from aligned bilingual text corpora (och et al, 1999; <papid> W99-0604 </papid>vogel et al, 2000; zens et al, 2002; koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" W10-4223 ">
prior work has explored the use of pbmt for paraphrase generation (quirk etal., 2004; <papid> W04-3219 </papid>bannard and callison-burch, 2005; madnani et al, 2007; <papid> W07-0716 </papid>callison-burch, 2008; zhao et al, 2009; <papid> P09-1094 </papid>wubben et al, 2010) <papid> W10-4223 </papid>however, since many researchers believe thatpbmt has reached performance ceiling, ongoing research looks into more structural approaches to statistical mt (marcu and wong, 2002; <papid> W02-1018 </papid>och andney, 2004; <papid> J04-4002 </papid>khalilov and fonollosa, 2009).</citsent>
<aftsection>
<nextsent>syntax based mt attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages.
</nextsent>
<nextsent>syntactic information might lead to better results in the areaof grammatical well-formedness, and unlike phrase based mt that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns.
</nextsent>
<nextsent>while the verdict on whether or not this approach leads to any significant performance gain is still out, similar line of reasoning would suggest thatsyntax-based paraphrasing may offer similar advantages over phrase-based paraphrasing.
</nextsent>
<nextsent>considering the fact that the success of pbmt can partly be attributed to the abundance of large parallel corpora, 27 and that sufficiently large parallel corpora are still lacking for paraphrase generation, using more linguistically motivated methods might prove beneficial for paraphrase generation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3852">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the similarity between paraphrase generation and mt suggests that methods and tools originally developed for mt could be exploited for paraphrase generation.
</prevsent>
<prevsent>one popular approach ? arguably the most successful so far ? is statistical phrase-based machine translation (pbmt), which learns phrase translation rules from aligned bilingual text corpora (och et al, 1999; <papid> W99-0604 </papid>vogel et al, 2000; zens et al, 2002; koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
prior work has explored the use of pbmt for paraphrase generation (quirk etal., 2004; <papid> W04-3219 </papid>bannard and callison-burch, 2005; madnani et al, 2007; <papid> W07-0716 </papid>callison-burch, 2008; zhao et al, 2009; <papid> P09-1094 </papid>wubben et al, 2010) <papid> W10-4223 </papid>however, since many researchers believe thatpbmt has reached performance ceiling, ongoing research looks into more structural approaches to statistical mt (marcu and wong, 2002; <papid> W02-1018 </papid>och andney, 2004; <papid> J04-4002 </papid>khalilov and fonollosa, 2009).</citsent>
<aftsection>
<nextsent>syntax based mt attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages.
</nextsent>
<nextsent>syntactic information might lead to better results in the areaof grammatical well-formedness, and unlike phrase based mt that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns.
</nextsent>
<nextsent>while the verdict on whether or not this approach leads to any significant performance gain is still out, similar line of reasoning would suggest thatsyntax-based paraphrasing may offer similar advantages over phrase-based paraphrasing.
</nextsent>
<nextsent>considering the fact that the success of pbmt can partly be attributed to the abundance of large parallel corpora, 27 and that sufficiently large parallel corpora are still lacking for paraphrase generation, using more linguistically motivated methods might prove beneficial for paraphrase generation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3853">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the similarity between paraphrase generation and mt suggests that methods and tools originally developed for mt could be exploited for paraphrase generation.
</prevsent>
<prevsent>one popular approach ? arguably the most successful so far ? is statistical phrase-based machine translation (pbmt), which learns phrase translation rules from aligned bilingual text corpora (och et al, 1999; <papid> W99-0604 </papid>vogel et al, 2000; zens et al, 2002; koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
prior work has explored the use of pbmt for paraphrase generation (quirk etal., 2004; <papid> W04-3219 </papid>bannard and callison-burch, 2005; madnani et al, 2007; <papid> W07-0716 </papid>callison-burch, 2008; zhao et al, 2009; <papid> P09-1094 </papid>wubben et al, 2010) <papid> W10-4223 </papid>however, since many researchers believe thatpbmt has reached performance ceiling, ongoing research looks into more structural approaches to statistical mt (marcu and wong, 2002; <papid> W02-1018 </papid>och andney, 2004; <papid> J04-4002 </papid>khalilov and fonollosa, 2009).</citsent>
<aftsection>
<nextsent>syntax based mt attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages.
</nextsent>
<nextsent>syntactic information might lead to better results in the areaof grammatical well-formedness, and unlike phrase based mt that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns.
</nextsent>
<nextsent>while the verdict on whether or not this approach leads to any significant performance gain is still out, similar line of reasoning would suggest thatsyntax-based paraphrasing may offer similar advantages over phrase-based paraphrasing.
</nextsent>
<nextsent>considering the fact that the success of pbmt can partly be attributed to the abundance of large parallel corpora, 27 and that sufficiently large parallel corpora are still lacking for paraphrase generation, using more linguistically motivated methods might prove beneficial for paraphrase generation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3854">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at the same time, automatic syntactic analysis introduces errors in the parse trees, as no syntactic parser is perfect.
</prevsent>
<prevsent>likewise, automatic alignment of syntactic phrases may be prone to errors.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the main contribution of this paper is systematic comparison between phrase-based and syntax-basedparaphrase generation using an off-the-shelf statistical machine translation (smt) decoder, namely moses (koehn et al, 2007) <papid> P07-2045 </papid>and the word-alignment tool giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>training data derives from new, large scale (2.1m tokens) paraphrase corpus for dutch, which has been recently released.the paper is organized as follows.
</nextsent>
<nextsent>section 2 reviews the paraphrase corpus from which provides training and test data.
</nextsent>
<nextsent>next, section 3 describes the paraphrase generation methods and the experimental setup.
</nextsent>
<nextsent>results are presented in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3855">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at the same time, automatic syntactic analysis introduces errors in the parse trees, as no syntactic parser is perfect.
</prevsent>
<prevsent>likewise, automatic alignment of syntactic phrases may be prone to errors.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the main contribution of this paper is systematic comparison between phrase-based and syntax-basedparaphrase generation using an off-the-shelf statistical machine translation (smt) decoder, namely moses (koehn et al, 2007) <papid> P07-2045 </papid>and the word-alignment tool giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>training data derives from new, large scale (2.1m tokens) paraphrase corpus for dutch, which has been recently released.the paper is organized as follows.
</nextsent>
<nextsent>section 2 reviews the paraphrase corpus from which provides training and test data.
</nextsent>
<nextsent>next, section 3 describes the paraphrase generation methods and the experimental setup.
</nextsent>
<nextsent>results are presented in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3856">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the main bottleneck in building smt systems is the need for substantial amount of parallel alignedtext.
</prevsent>
<prevsent>likewise, exploiting smt for paraphrasing requires large amounts of monolingual parallel text.however, paraphrase corpora are scarce; the situation is more dire than in mt, and this has caused some studies to focus on the automatic harvesting of paraphrase corpora.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
the use of monolingual parallel text corpora was first suggested by barzilay and mckeown (2001), <papid> P01-1008 </papid>who built their corpus using various alternative human-produced translationsof literary texts and then applied machine learning or multi-sequence alignment for extracting para phrases.</citsent>
<aftsection>
<nextsent>in similar vein, pang et al (2003) <papid> N03-1024 </papid>used corpus of alternative english translations of chinese news stories in combination with syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified.</nextsent>
<nextsent>so-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (shinyama et al, 2002; barzilay and lee, 2003; <papid> N03-1003 </papid>shen et al, 2006; <papid> P06-2096 </papid>wubben et al, 2009) <papid> W09-0621 </papid>the first manually collected paraphrase corpus is the microsoft research paraphrase (msrp) corpus (dolan et al, 2004), <papid> C04-1051 </papid>consisting of 5,801 sentence pairs, sampled from larger corpus of news articles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3857">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>likewise, exploiting smt for paraphrasing requires large amounts of monolingual parallel text.however, paraphrase corpora are scarce; the situation is more dire than in mt, and this has caused some studies to focus on the automatic harvesting of paraphrase corpora.
</prevsent>
<prevsent>the use of monolingual parallel text corpora was first suggested by barzilay and mckeown (2001), <papid> P01-1008 </papid>who built their corpus using various alternative human-produced translationsof literary texts and then applied machine learning or multi-sequence alignment for extracting para phrases.</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
in similar vein, pang et al (2003) <papid> N03-1024 </papid>used corpus of alternative english translations of chinese news stories in combination with syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified.</citsent>
<aftsection>
<nextsent>so-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (shinyama et al, 2002; barzilay and lee, 2003; <papid> N03-1003 </papid>shen et al, 2006; <papid> P06-2096 </papid>wubben et al, 2009) <papid> W09-0621 </papid>the first manually collected paraphrase corpus is the microsoft research paraphrase (msrp) corpus (dolan et al, 2004), <papid> C04-1051 </papid>consisting of 5,801 sentence pairs, sampled from larger corpus of news articles.</nextsent>
<nextsent>however, it is rather small and contains no sub sentential allignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3858">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the use of monolingual parallel text corpora was first suggested by barzilay and mckeown (2001), <papid> P01-1008 </papid>who built their corpus using various alternative human-produced translationsof literary texts and then applied machine learning or multi-sequence alignment for extracting para phrases.</prevsent>
<prevsent>in similar vein, pang et al (2003) <papid> N03-1024 </papid>used corpus of alternative english translations of chinese news stories in combination with syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified.</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
so-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (shinyama et al, 2002; barzilay and lee, 2003; <papid> N03-1003 </papid>shen et al, 2006; <papid> P06-2096 </papid>wubben et al, 2009) <papid> W09-0621 </papid>the first manually collected paraphrase corpus is the microsoft research paraphrase (msrp) corpus (dolan et al, 2004), <papid> C04-1051 </papid>consisting of 5,801 sentence pairs, sampled from larger corpus of news articles.</citsent>
<aftsection>
<nextsent>however, it is rather small and contains no sub sentential allignments.
</nextsent>
<nextsent>cohn et al (2008) <papid> J08-4005 </papid>developed parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level.</nextsent>
<nextsent>however, all of these corpora are small from an smt perspective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3859">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the use of monolingual parallel text corpora was first suggested by barzilay and mckeown (2001), <papid> P01-1008 </papid>who built their corpus using various alternative human-produced translationsof literary texts and then applied machine learning or multi-sequence alignment for extracting para phrases.</prevsent>
<prevsent>in similar vein, pang et al (2003) <papid> N03-1024 </papid>used corpus of alternative english translations of chinese news stories in combination with syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified.</prevsent>
</prevsection>
<citsent citstr=" P06-2096 ">
so-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (shinyama et al, 2002; barzilay and lee, 2003; <papid> N03-1003 </papid>shen et al, 2006; <papid> P06-2096 </papid>wubben et al, 2009) <papid> W09-0621 </papid>the first manually collected paraphrase corpus is the microsoft research paraphrase (msrp) corpus (dolan et al, 2004), <papid> C04-1051 </papid>consisting of 5,801 sentence pairs, sampled from larger corpus of news articles.</citsent>
<aftsection>
<nextsent>however, it is rather small and contains no sub sentential allignments.
</nextsent>
<nextsent>cohn et al (2008) <papid> J08-4005 </papid>developed parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level.</nextsent>
<nextsent>however, all of these corpora are small from an smt perspective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3860">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the use of monolingual parallel text corpora was first suggested by barzilay and mckeown (2001), <papid> P01-1008 </papid>who built their corpus using various alternative human-produced translationsof literary texts and then applied machine learning or multi-sequence alignment for extracting para phrases.</prevsent>
<prevsent>in similar vein, pang et al (2003) <papid> N03-1024 </papid>used corpus of alternative english translations of chinese news stories in combination with syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified.</prevsent>
</prevsection>
<citsent citstr=" W09-0621 ">
so-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (shinyama et al, 2002; barzilay and lee, 2003; <papid> N03-1003 </papid>shen et al, 2006; <papid> P06-2096 </papid>wubben et al, 2009) <papid> W09-0621 </papid>the first manually collected paraphrase corpus is the microsoft research paraphrase (msrp) corpus (dolan et al, 2004), <papid> C04-1051 </papid>consisting of 5,801 sentence pairs, sampled from larger corpus of news articles.</citsent>
<aftsection>
<nextsent>however, it is rather small and contains no sub sentential allignments.
</nextsent>
<nextsent>cohn et al (2008) <papid> J08-4005 </papid>developed parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level.</nextsent>
<nextsent>however, all of these corpora are small from an smt perspective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3861">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the use of monolingual parallel text corpora was first suggested by barzilay and mckeown (2001), <papid> P01-1008 </papid>who built their corpus using various alternative human-produced translationsof literary texts and then applied machine learning or multi-sequence alignment for extracting para phrases.</prevsent>
<prevsent>in similar vein, pang et al (2003) <papid> N03-1024 </papid>used corpus of alternative english translations of chinese news stories in combination with syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified.</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
so-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (shinyama et al, 2002; barzilay and lee, 2003; <papid> N03-1003 </papid>shen et al, 2006; <papid> P06-2096 </papid>wubben et al, 2009) <papid> W09-0621 </papid>the first manually collected paraphrase corpus is the microsoft research paraphrase (msrp) corpus (dolan et al, 2004), <papid> C04-1051 </papid>consisting of 5,801 sentence pairs, sampled from larger corpus of news articles.</citsent>
<aftsection>
<nextsent>however, it is rather small and contains no sub sentential allignments.
</nextsent>
<nextsent>cohn et al (2008) <papid> J08-4005 </papid>developed parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level.</nextsent>
<nextsent>however, all of these corpora are small from an smt perspective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3862">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>so-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (shinyama et al, 2002; barzilay and lee, 2003; <papid> N03-1003 </papid>shen et al, 2006; <papid> P06-2096 </papid>wubben et al, 2009) <papid> W09-0621 </papid>the first manually collected paraphrase corpus is the microsoft research paraphrase (msrp) corpus (dolan et al, 2004), <papid> C04-1051 </papid>consisting of 5,801 sentence pairs, sampled from larger corpus of news articles.</prevsent>
<prevsent>however, it is rather small and contains no sub sentential allignments.</prevsent>
</prevsection>
<citsent citstr=" J08-4005 ">
cohn et al (2008) <papid> J08-4005 </papid>developed parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level.</citsent>
<aftsection>
<nextsent>however, all of these corpora are small from an smt perspective.
</nextsent>
<nextsent>recently new large-scale paraphrase corpus for dutch, the daeso corpus, was released.
</nextsent>
<nextsent>the corpus contains both samples of parallel and comparable text in which similar sentences, phrases and words are aligned.
</nextsent>
<nextsent>one part of the corpus is manually aligned, whereas another part is automatically aligned using data-driven aligner trained on the first part.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3864">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> paraphrase generation.  </section>
<citcontext>
<prevsection>
<prevsent>one issue with the phrase-based approach is thatrecursion is not handled explicitly.
</prevsent>
<prevsent>it is generally acknowledged that language contains recursive structures up to certain depths.
</prevsent>
</prevsection>
<citsent citstr=" H05-1098 ">
so-called hierarchical models have introduced the inclusion of nonterminals in the mapping rules, to allow for recur sion (chiang et al, 2005).<papid> H05-1098 </papid></citsent>
<aftsection>
<nextsent>however, using generic non-terminal can introduce many substitution sin translations that do not make sense.
</nextsent>
<nextsent>by making the non-terminals explicit, using syntactic categories such as np and p s, this phenomenon is constrained, resulting in syntax-based translation.
</nextsent>
<nextsent>instead of phrase translations, translation rules interms of syntactic constituents or subtrees are extracted, presupposing the availability of syntactic structures for source, target, or both languages.
</nextsent>
<nextsent>incorporating syntax can guide the translation process and unlike phrase-based mt syntax it enables the modeling of long-distance translation patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3865">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> paraphrase generation.  </section>
<citcontext>
<prevsection>
<prevsent>instead of phrase translations, translation rules interms of syntactic constituents or subtrees are extracted, presupposing the availability of syntactic structures for source, target, or both languages.
</prevsent>
<prevsent>incorporating syntax can guide the translation process and unlike phrase-based mt syntax it enables the modeling of long-distance translation patterns.
</prevsent>
</prevsection>
<citsent citstr=" W06-3119 ">
syntax-based systems may parse the data onthe target side (string-to-tree), source side (tree-to string), or both (tree-to-tree).in our experiments we use tree-to-tree syntax based mt. we also experiment with relaxing the parses by method proposed under the label ofsyntax-augmented machine translation (samt), described in (zollmann and venugopal, 2006).<papid> W06-3119 </papid></citsent>
<aftsection>
<nextsent>this method combines any neighboring nodes and labels previously unlabeled nodes, removing the syntactic constraint on the grammar1.
</nextsent>
<nextsent>we train all systems on the daeso data (218,102 lines of aligned sentences) and test on held-outset consisting of manually aligned headlines that ap 1this method is implemented in the moses package in the program relax-parse as option samt 4 29 table 2: examples of output of the phrase-based and syntax-based systems source jongen ( 7 ) zwaargewond na aanrijding boy (7) severely-injured after crash phrase-based 7-jarige gewond na botsing 7-year-old injured after collision syntax-based jongen ( 7 ) zwaar gewond na aanrijding boy (7) severely injured after crash source jeugdwerkloosheid daalt vooral bij voldoende opleiding youth-unemployment drops especially with adequate training phrase-based werkloosheid jong eren daalt , vooral bij voldoende studie unemployment youths drops, especially with sufficient study syntax-based * jeugdwerkloosheid daalt vooral in voldoende opleiding youth-unemployment drops especially inadequate training source kritiek op boetebeleid ns criticism of fining-policy ns phrase-based * kritiek op de omstreden boetebeleid en criticism of the controversial and syntax-based kritiek op omstreden boetebeleid nederlandse spoorwegen criticism of controversial fining-policy dutch railways source weer bestuurders radboud weg again directors radboud [hospital] leaving phrase-based * weer de weg ziekenhuis again the leaving hospital syntax-based alweer bestuurders ziekenhuis weg yet-again directors hospital leaving peared in may 2006.2 we test on 773 headlines that have three or more aligned paraphrasing referenceheadlines.
</nextsent>
<nextsent>we use an srilm (stolcke, 2002) language model trained on the twente news corpus3.
</nextsent>
<nextsent>to investigate the effect of the amount of training data on results, we also train phrase-based modelon more data by adding more aligned headlines originating from data crawled in 2010 and aligned usingtf.idf scores over headline clusters and cosine similarity as described in (wubben et al, 2009), <papid> W09-0621 </papid>resulting in an extra 612,158 aligned headlines.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3867">
<title id=" W11-1604.xml">comparing phrase based and syntax based paraphrase generation </title>
<section> paraphrase generation.  </section>
<citcontext>
<prevsection>
<prevsent>formally, the score is computed as fol lows: nistweightedld = ? ?
</prevsent>
<prevsent>i=ld(1..8) (i ni nisti) ? i=ld(1..8) (i ni) where ? is the percentage of output phrases that havea sentence levenshtein distance higher than 0.
</prevsent>
</prevsection>
<citsent citstr=" W07-0734 ">
instead of nist scores, other mt evaluation scores can be plugged into this formula, such as meteor (lavie and agarwal, 2007) <papid> W07-0734 </papid>for languages for which paraphrase data is available.</citsent>
<aftsection>
<nextsent>figure 1 shows nist scores per levenshtein distance.
</nextsent>
<nextsent>it can be observed that overall the nist score decreases as the distance to the input increases, indicating that more distant paraphrases are of less quality.
</nextsent>
<nextsent>the relaxed syntax-based approach (samt)performs mildly better than the standard syntax based approach, but performs worse than the phrase based approach.
</nextsent>
<nextsent>the distribution of generated paraphrases per levenshtein distance is shown in figure 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3868">
<title id=" W11-0133.xml">discuss a dialogue move taxonomy layered over semantic representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such scheme should account for differences in tutoring style and question scaffolding techniques and should capture the subtle distinctions between different question types.
</prevsent>
<prevsent>to do this, requires representation that connects turns communicative and rhetorical functions to its underlying semantic content.
</prevsent>
</prevsection>
<citsent citstr=" W04-2307 ">
while efforts such as damsl (core and allen, 1997) and dit++ (bunt, 2009) have helped to make dialogue act annotation more uniform and applicable to wider audience, and while tutoring-specific initiatives (tsovaltzi and karagjosova, 2004; <papid> W04-2307 </papid>buckley and wolska, 2008) <papid> C08-1010 </papid>have helped to bring dialogue acts to tutorial dialogue, the move granularity in these schemas is too coarse to capture the differences in tutorial questioning styles exhibited in our corpus of socratic-style tutorial dialogues.</citsent>
<aftsection>
<nextsent>conversely, question type categories (graesser and person, 1994; nielsen et al, 2008) have been designed with education in mind, but they largely ignore how the student and tutor may work together to construct meaning.
</nextsent>
<nextsent>the discount schemes (pilkington, 1999) combination of dialogue acts and rhetorical functions enabled it to better capture tutoring moves, but its omission of shallow semantics prevents it from capturing how content influences behavior.
</nextsent>
<nextsent>our long-term goals of automatic dialogue characterization, tutorial move prediction and question generation led us to design our own dialogue representation called discuss (dialogue scheme for unifying speech and semantics).
</nextsent>
<nextsent>design of this dialogue move taxonomy was based on preliminary observations from our corpus of tutorial dialogues, and was influenced by the aforementioned research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3869">
<title id=" W11-0133.xml">discuss a dialogue move taxonomy layered over semantic representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such scheme should account for differences in tutoring style and question scaffolding techniques and should capture the subtle distinctions between different question types.
</prevsent>
<prevsent>to do this, requires representation that connects turns communicative and rhetorical functions to its underlying semantic content.
</prevsent>
</prevsection>
<citsent citstr=" C08-1010 ">
while efforts such as damsl (core and allen, 1997) and dit++ (bunt, 2009) have helped to make dialogue act annotation more uniform and applicable to wider audience, and while tutoring-specific initiatives (tsovaltzi and karagjosova, 2004; <papid> W04-2307 </papid>buckley and wolska, 2008) <papid> C08-1010 </papid>have helped to bring dialogue acts to tutorial dialogue, the move granularity in these schemas is too coarse to capture the differences in tutorial questioning styles exhibited in our corpus of socratic-style tutorial dialogues.</citsent>
<aftsection>
<nextsent>conversely, question type categories (graesser and person, 1994; nielsen et al, 2008) have been designed with education in mind, but they largely ignore how the student and tutor may work together to construct meaning.
</nextsent>
<nextsent>the discount schemes (pilkington, 1999) combination of dialogue acts and rhetorical functions enabled it to better capture tutoring moves, but its omission of shallow semantics prevents it from capturing how content influences behavior.
</nextsent>
<nextsent>our long-term goals of automatic dialogue characterization, tutorial move prediction and question generation led us to design our own dialogue representation called discuss (dialogue scheme for unifying speech and semantics).
</nextsent>
<nextsent>design of this dialogue move taxonomy was based on preliminary observations from our corpus of tutorial dialogues, and was influenced by the aforementioned research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3870">
<title id=" W11-0305.xml">word segmentation as general chunking </title>
<section> general chunking.  </section>
<citcontext>
<prevsection>
<prevsent>this 1inuktitut is poly synthetic inuit language known for its highly complex morphology.
</prevsent>
<prevsent>40 figure 1: the dfa signature of hub (top) and stretched hub in the hubmorph algorithm.
</prevsent>
</prevsection>
<citsent citstr=" N03-2015 ">
figure from johnson and martin (2003).<papid> N03-2015 </papid></citsent>
<aftsection>
<nextsent>knowledge does not consist in the memorization ofwhole words (chunks), but rather in statistics describing the beginnings and endings of chunks.
</nextsent>
<nextsent>inthe word segmentation domain, these statistics effectively correspond to phonotactic constraints thatare inferred from hypothesized segmentations.
</nextsent>
<nextsent>inferred boundaries are stored in data structure calleda knowledge trie (shown in figure 2), which is essentially generalized prefix or suffix trie.
</nextsent>
<nextsent>a 3 3 3 2 1 1 root . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3871">
<title id=" W11-0305.xml">word segmentation as general chunking </title>
<section> computational models of word.  </section>
<citcontext>
<prevsection>
<prevsent>a fully unsupervised paradigm would include no boundary information at all, meaning that the input is, or is treated as, continuous sequences of phonemes.
</prevsent>
<prevsent>the mbdp-1 algorithm wasnot designed for operation in this continuous condition, as it relies on having at least some true boundary information to generalize.
</prevsent>
</prevsection>
<citsent citstr=" P08-1016 ">
mbdp-1 achieves robust form of bootstrapping through the use of bayesian maximum-likelihood estimation of the parameters of language model.more recent algorithms in the same tradition, including the refined mbdp-1 of venkataraman (2001), the word ends algorithm of fleck (2008), <papid> P08-1016 </papid>and the hierarchical dirichlet process (hdp) algorithm of goldwater (2007), share this limitation.</citsent>
<aftsection>
<nextsent>however, infants are able to discover words in single stream of continuous speech, as shown by the seminal series of studies by saffran et al  (1996; 1998; 2003).
</nextsent>
<nextsent>in these studies, saffran et al  show that both adults and 8-month-old infants quickly learn to extract words of simple artificial language from continuous speech stream containing no pauses.
</nextsent>
<nextsent>the general chunking algorithms ve, bve, and ptm work in either condition.
</nextsent>
<nextsent>the unsupervised, continuous condition is the norm (cohen et al , 2007; hewlett and cohen, 2009; tanaka-ishii and jin, 2006) but these algorithms are easily adapted to the semi-supervised, incremental condition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3877">
<title id=" W11-0322.xml">evaluating a semantic network automatically constructed from lexical cooccurrence on a word sense disambiguation task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the next section (section 2), we discuss related work.
</prevsent>
<prevsent>we then give an overview of the method we use to construct our network (sections 3 and 4).
</prevsent>
</prevsection>
<citsent citstr=" P10-1154 ">
the network is evaluated through its application to wsd task (sections 57), where we compare its performance to wordnet and another automatically acquired semantic network called wordnet++ (ponzetto and navigli, 2010).<papid> P10-1154 </papid></citsent>
<aftsection>
<nextsent>a discussion follows (section 8), 190 and we present our conclusions in section 9.
</nextsent>
<nextsent>our work bears strong relation to wordnet++(henceforth wn++), which is constructed automatically from the semantic annotations in wikipedia (ponzetto and navigli, 2010).<papid> P10-1154 </papid>2 links in wn++ are established between words whose articles link to one another.</nextsent>
<nextsent>for example, the article on astronomy in wikipedia links to the article on celestial navigation, so we find an edge from astronomy#n#1 to celestial navigation#n#1 in wn++.3 the nouns related in wn++ are disambiguated automatically using further semantic annotation data from wikipedia, including sense labels, the titles of other pages linked to by any two related nouns, and the folk sonomic categories to which articles belong.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3881">
<title id=" W11-0322.xml">evaluating a semantic network automatically constructed from lexical cooccurrence on a word sense disambiguation task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the resulting resource contains 1,902,859 unique edges between noun senses.
</prevsent>
<prevsent>augmenting the structure of wikipedia itself has been the subject of research as well, and involves the discovery of relations between articles.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
mihalcea and csomai (2007), for example, added links between wikipedia pages after automatically identifying keywords in each article and disambiguating those words to their appropriate wikipedia concepts (article titles), while ponzetto and navigli (2009)used graph theoretic approaches to augment the taxonomic organization of wikipedia articles.in terms of automatically discovering semantic relations, many pattern-based approaches have been used to extract specific types of relations from large corpora, e.g., hyponymy, meronymy, and synonymy (hearst, 1992; <papid> C92-2082 </papid>pantel and pennacchiotti, 2006).<papid> P06-1015 </papid></citsent>
<aftsection>
<nextsent>approaches based on distributional similarity have been applied toward the same end (harris, 1985;gorman and curran, 2006), <papid> P06-1046 </papid>and there are several approaches that relyon the underlying structure of wordnet or wikipedia to measure the relatedness between two concepts or nouns quantitatively (hughes and ramage, 2007; <papid> D07-1061 </papid>gabrilovich and 2http://lcl.uniroma1.it/wordnetplusplus 3the notation astronomy#n#1 refers to sense 1 (#1) of the noun (#n) astronomy?</nextsent>
<nextsent>in wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3882">
<title id=" W11-0322.xml">evaluating a semantic network automatically constructed from lexical cooccurrence on a word sense disambiguation task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the resulting resource contains 1,902,859 unique edges between noun senses.
</prevsent>
<prevsent>augmenting the structure of wikipedia itself has been the subject of research as well, and involves the discovery of relations between articles.
</prevsent>
</prevsection>
<citsent citstr=" P06-1015 ">
mihalcea and csomai (2007), for example, added links between wikipedia pages after automatically identifying keywords in each article and disambiguating those words to their appropriate wikipedia concepts (article titles), while ponzetto and navigli (2009)used graph theoretic approaches to augment the taxonomic organization of wikipedia articles.in terms of automatically discovering semantic relations, many pattern-based approaches have been used to extract specific types of relations from large corpora, e.g., hyponymy, meronymy, and synonymy (hearst, 1992; <papid> C92-2082 </papid>pantel and pennacchiotti, 2006).<papid> P06-1015 </papid></citsent>
<aftsection>
<nextsent>approaches based on distributional similarity have been applied toward the same end (harris, 1985;gorman and curran, 2006), <papid> P06-1046 </papid>and there are several approaches that relyon the underlying structure of wordnet or wikipedia to measure the relatedness between two concepts or nouns quantitatively (hughes and ramage, 2007; <papid> D07-1061 </papid>gabrilovich and 2http://lcl.uniroma1.it/wordnetplusplus 3the notation astronomy#n#1 refers to sense 1 (#1) of the noun (#n) astronomy?</nextsent>
<nextsent>in wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3883">
<title id=" W11-0322.xml">evaluating a semantic network automatically constructed from lexical cooccurrence on a word sense disambiguation task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>augmenting the structure of wikipedia itself has been the subject of research as well, and involves the discovery of relations between articles.
</prevsent>
<prevsent>mihalcea and csomai (2007), for example, added links between wikipedia pages after automatically identifying keywords in each article and disambiguating those words to their appropriate wikipedia concepts (article titles), while ponzetto and navigli (2009)used graph theoretic approaches to augment the taxonomic organization of wikipedia articles.in terms of automatically discovering semantic relations, many pattern-based approaches have been used to extract specific types of relations from large corpora, e.g., hyponymy, meronymy, and synonymy (hearst, 1992; <papid> C92-2082 </papid>pantel and pennacchiotti, 2006).<papid> P06-1015 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1046 ">
approaches based on distributional similarity have been applied toward the same end (harris, 1985;gorman and curran, 2006), <papid> P06-1046 </papid>and there are several approaches that relyon the underlying structure of wordnet or wikipedia to measure the relatedness between two concepts or nouns quantitatively (hughes and ramage, 2007; <papid> D07-1061 </papid>gabrilovich and 2http://lcl.uniroma1.it/wordnetplusplus 3the notation astronomy#n#1 refers to sense 1 (#1) of the noun (#n) astronomy?</citsent>
<aftsection>
<nextsent>in wordnet.
</nextsent>
<nextsent>other parts of speech are denoted by #v (verbs), #a (adjectives), or #r (adverbs).markovitch, 2007; zaragoza et al, 2007; patwardhan and pedersen, 2006; <papid> W06-2501 </papid>strube and ponzetto, 2006; budanitsky and hirst, 2006; <papid> J06-1003 </papid>resnik, 1995).</nextsent>
<nextsent>other quantitative approaches have leveraged the large amounts of data available on the web to discover relatedness.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3884">
<title id=" W11-0322.xml">evaluating a semantic network automatically constructed from lexical cooccurrence on a word sense disambiguation task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>augmenting the structure of wikipedia itself has been the subject of research as well, and involves the discovery of relations between articles.
</prevsent>
<prevsent>mihalcea and csomai (2007), for example, added links between wikipedia pages after automatically identifying keywords in each article and disambiguating those words to their appropriate wikipedia concepts (article titles), while ponzetto and navigli (2009)used graph theoretic approaches to augment the taxonomic organization of wikipedia articles.in terms of automatically discovering semantic relations, many pattern-based approaches have been used to extract specific types of relations from large corpora, e.g., hyponymy, meronymy, and synonymy (hearst, 1992; <papid> C92-2082 </papid>pantel and pennacchiotti, 2006).<papid> P06-1015 </papid></prevsent>
</prevsection>
<citsent citstr=" D07-1061 ">
approaches based on distributional similarity have been applied toward the same end (harris, 1985;gorman and curran, 2006), <papid> P06-1046 </papid>and there are several approaches that relyon the underlying structure of wordnet or wikipedia to measure the relatedness between two concepts or nouns quantitatively (hughes and ramage, 2007; <papid> D07-1061 </papid>gabrilovich and 2http://lcl.uniroma1.it/wordnetplusplus 3the notation astronomy#n#1 refers to sense 1 (#1) of the noun (#n) astronomy?</citsent>
<aftsection>
<nextsent>in wordnet.
</nextsent>
<nextsent>other parts of speech are denoted by #v (verbs), #a (adjectives), or #r (adverbs).markovitch, 2007; zaragoza et al, 2007; patwardhan and pedersen, 2006; <papid> W06-2501 </papid>strube and ponzetto, 2006; budanitsky and hirst, 2006; <papid> J06-1003 </papid>resnik, 1995).</nextsent>
<nextsent>other quantitative approaches have leveraged the large amounts of data available on the web to discover relatedness.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3885">
<title id=" W11-0322.xml">evaluating a semantic network automatically constructed from lexical cooccurrence on a word sense disambiguation task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>approaches based on distributional similarity have been applied toward the same end (harris, 1985;gorman and curran, 2006), <papid> P06-1046 </papid>and there are several approaches that relyon the underlying structure of wordnet or wikipedia to measure the relatedness between two concepts or nouns quantitatively (hughes and ramage, 2007; <papid> D07-1061 </papid>gabrilovich and 2http://lcl.uniroma1.it/wordnetplusplus 3the notation astronomy#n#1 refers to sense 1 (#1) of the noun (#n) astronomy?</prevsent>
<prevsent>in wordnet.</prevsent>
</prevsection>
<citsent citstr=" W06-2501 ">
other parts of speech are denoted by #v (verbs), #a (adjectives), or #r (adverbs).markovitch, 2007; zaragoza et al, 2007; patwardhan and pedersen, 2006; <papid> W06-2501 </papid>strube and ponzetto, 2006; budanitsky and hirst, 2006; <papid> J06-1003 </papid>resnik, 1995).</citsent>
<aftsection>
<nextsent>other quantitative approaches have leveraged the large amounts of data available on the web to discover relatedness.
</nextsent>
<nextsent>notably, agirre and de lacalle (2004) employed web queries to associate wordnet synsets with representative context words, known as topic signatures.
</nextsent>
<nextsent>cuadros and rigau (2008) <papid> C08-1021 </papid>have used these data to construct four know nets, semantic knowledge bases derived by disambiguating the top 5, 10, 15, and 20 nouns, respectively, from the topic signatures of agirre and de lacalle.</nextsent>
<nextsent>network the semantic network is automatically acquired in three distinct stages (szumlanski and gomez, 2010): (1) quantitative measurement of relatedness between nouns that co-occur in large corpus; (2) categorical determination of whether the quantitative measure indicates strong and mutual semantic relatedness between given pair of nouns; and (3) unsupervised disambiguation of all the nouns that are found to be semantically related.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3886">
<title id=" W11-0322.xml">evaluating a semantic network automatically constructed from lexical cooccurrence on a word sense disambiguation task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>approaches based on distributional similarity have been applied toward the same end (harris, 1985;gorman and curran, 2006), <papid> P06-1046 </papid>and there are several approaches that relyon the underlying structure of wordnet or wikipedia to measure the relatedness between two concepts or nouns quantitatively (hughes and ramage, 2007; <papid> D07-1061 </papid>gabrilovich and 2http://lcl.uniroma1.it/wordnetplusplus 3the notation astronomy#n#1 refers to sense 1 (#1) of the noun (#n) astronomy?</prevsent>
<prevsent>in wordnet.</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
other parts of speech are denoted by #v (verbs), #a (adjectives), or #r (adverbs).markovitch, 2007; zaragoza et al, 2007; patwardhan and pedersen, 2006; <papid> W06-2501 </papid>strube and ponzetto, 2006; budanitsky and hirst, 2006; <papid> J06-1003 </papid>resnik, 1995).</citsent>
<aftsection>
<nextsent>other quantitative approaches have leveraged the large amounts of data available on the web to discover relatedness.
</nextsent>
<nextsent>notably, agirre and de lacalle (2004) employed web queries to associate wordnet synsets with representative context words, known as topic signatures.
</nextsent>
<nextsent>cuadros and rigau (2008) <papid> C08-1021 </papid>have used these data to construct four know nets, semantic knowledge bases derived by disambiguating the top 5, 10, 15, and 20 nouns, respectively, from the topic signatures of agirre and de lacalle.</nextsent>
<nextsent>network the semantic network is automatically acquired in three distinct stages (szumlanski and gomez, 2010): (1) quantitative measurement of relatedness between nouns that co-occur in large corpus; (2) categorical determination of whether the quantitative measure indicates strong and mutual semantic relatedness between given pair of nouns; and (3) unsupervised disambiguation of all the nouns that are found to be semantically related.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3887">
<title id=" W11-0322.xml">evaluating a semantic network automatically constructed from lexical cooccurrence on a word sense disambiguation task </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>other quantitative approaches have leveraged the large amounts of data available on the web to discover relatedness.
</prevsent>
<prevsent>notably, agirre and de lacalle (2004) employed web queries to associate wordnet synsets with representative context words, known as topic signatures.
</prevsent>
</prevsection>
<citsent citstr=" C08-1021 ">
cuadros and rigau (2008) <papid> C08-1021 </papid>have used these data to construct four know nets, semantic knowledge bases derived by disambiguating the top 5, 10, 15, and 20 nouns, respectively, from the topic signatures of agirre and de lacalle.</citsent>
<aftsection>
<nextsent>network the semantic network is automatically acquired in three distinct stages (szumlanski and gomez, 2010): (1) quantitative measurement of relatedness between nouns that co-occur in large corpus; (2) categorical determination of whether the quantitative measure indicates strong and mutual semantic relatedness between given pair of nouns; and (3) unsupervised disambiguation of all the nouns that are found to be semantically related.
</nextsent>
<nextsent>we provide an overview of each of these steps below (sections 3.13.3), andthen discuss how we have expanded this methodology to create more complete semantic network (section 4).
</nextsent>
<nextsent>3.1 quantitatively measuring relatedness from.
</nextsent>
<nextsent>lexical co-occurrencewe first measure the semantic relatedness, or relational strength, of target, t, to one of its cooccurring nouns, or co-targets, c, with the following asymmetric function: srel(t, c) = (t|c)p (c|t)log (c|t) (c) where (c|t) is the relative frequency of among all nouns co-occurring with t, and vice versa for (t|c).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3889">
<title id=" W11-0322.xml">evaluating a semantic network automatically constructed from lexical cooccurrence on a word sense disambiguation task </title>
<section> coarse-grained wsd experiments.  </section>
<citcontext>
<prevsection>
<prevsent>using this method, we have automatically created semantic network that has 208,832 pairs of related noun senses ? the most extensive semantic network between wordnet noun senses to be derived automatically from simple lexical co-occurrence measure.
</prevsent>
<prevsent>for the remainder of this paper, we will refer to our network as the szumlanski-gomez network (sgn).
</prevsent>
</prevsection>
<citsent citstr=" W07-2006 ">
to evaluate our semantic network, and to provide fair comparison to related work, we take our cue from ponzetto and navigli (2010), <papid> P10-1154 </papid>who evaluated the performance of wn++ on the semeval-2007 (navigli et al, 2007) <papid> W07-2006 </papid>coarse-grained all-words wsd task using extended gloss overlaps (banerjee and entity#1 food#1 cake#2 equipment#1 ballplayer#1 pitch#2 cake#3 dessert#1 tempura#1 game_equipment#1 sports_equipment#1 runner#4 fielder#1 hitter#1 fastball#1 strike#5 waffle#1 pancake#1 pudding#2 pudding#3 ball#1 infielder#1 outfielder#1 baseball#2 bat#4 base#3 glove#1 glove#3 shortstop#1 centerfielder#1 figure 1: partial view of the wordnet graph, showing senses of nouns related to batter?</citsent>
<aftsection>
<nextsent>(gray nodes) and intermediary concepts (white nodes) that connect them to the root of the taxonomy through hypernymic relationships.pedersen, 2003) and the graph-based degree centrality algorithm (navigli and lapata, 2010).
</nextsent>
<nextsent>in this particular semeval task, we are presented with 237 sentences in which lemmatized target words have been flagged for disambiguation.
</nextsent>
<nextsent>in our experiments, we disambiguate nouns only (as did ponzetto and navigli), since both sgn (our net work) and wn++ relate only concepts denoted bynouns, and no other parts of speech.
</nextsent>
<nextsent>in our experimental setup, each sentence is considered in isolation from the rest, and all lemmatized content words in sentence are provided to the disambiguationalgorithms; the verbs, adjectives, and adverbs, although we do not resolve their senses, lend additional context to the disambiguation algorithms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3899">
<title id=" W10-4170.xml">iscas a system for chinese word sense induction based on kmeans algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and the hypothesis is also popularized with the phrase word characterized by the company it keeps?
</prevsent>
<prevsent>(john, 1957).
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
this concept shows us method to automatically discover senses of words by clustering the target words with similar contexts (lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>the word sense induction can be regarded as an unsupervised clustering problem.
</nextsent>
<nextsent>first, select some features to be used when comparing similarity between words.
</nextsent>
<nextsent>second, represent disam biguated words as vectors of selected features according to target words?
</nextsent>
<nextsent>contexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3900">
<title id=" W11-1217.xml">an expectation maximization algorithm for textual unit alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>regarding the latter, monolingual corpora is evidently easier to collect than parallel corpora and the truth of this statement is even more obvious when it comes to pairs of languages other than those both widely spoken and computationally well-treated around the world such as english, spanish, french or german.
</prevsent>
<prevsent>comparable corpora came as possible solution to the problem of scarcity of parallel corpora with the promise that it may serve as seed for parallel data extraction.
</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
a general definition of comparability that we find operational is given by munteanu and marcu (2005).<papid> J05-4003 </papid></citsent>
<aftsection>
<nextsent>they say that (bilingual) comparable corpus is set of paired documents that, while not parallel in the strict sense, are related and convey overlapping information.
</nextsent>
<nextsent>current practices of automatically collecting domain-dependent bilingual comparable corpora from the web usually begin with collecting list of terms as seed data in both the source and the target languages.
</nextsent>
<nextsent>each term (in each language) is then queried on the most popular search engine and the first document hits are retained.
</nextsent>
<nextsent>the final corpus will contain ? documents in each language and in subsequent usage the document boundaries are often disregarded.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3901">
<title id=" W11-1217.xml">an expectation maximization algorithm for textual unit alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the final corpus will contain ? documents in each language and in subsequent usage the document boundaries are often disregarded.
</prevsent>
<prevsent>at this point, it is important to stress out the importance of the pairing of documents incomparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
suppose that we want to word align bilingual comparable corpus consisting of documents per language, each with words, using the ibm-1 word alignment algorithm (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>this algorithm searches for each source word, the target words that have maximum translation probability with the source word.
</nextsent>
<nextsent>aligning all the words in our corpus with no regard to document boundaries, would yield time complexity of operations.
</nextsent>
<nextsent>the alternative would be in finding 1:p (with a small positive integer, usually 1, 2 or 3) document assignment (a set of aligned document pairs) that would enforce the no search outside the document boundary?
</nextsent>
<nextsent>condition when doing word alignment with the advantage of reducing the time complexity to operations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3902">
<title id=" W11-1217.xml">an expectation maximization algorithm for textual unit alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we will end the article with thorough evaluation of the performances of this algorithm and the conclusions that arise from these evaluations.
</prevsent>
<prevsent>document alignment and other types of textual unit alignment have been attempted in various situations involving extracting parallel data from comparable corpora.
</prevsent>
</prevsection>
<citsent citstr=" W02-1037 ">
the first case study is offered by munteanu and marcu (2002).<papid> W02-1037 </papid></citsent>
<aftsection>
<nextsent>they align sentences in an english-french comparable corpus of 1.3m of words per language by comparing suffix trees of the sentences.
</nextsent>
<nextsent>each sentence from each part of the corpus is encoded as suffix tree which is tree that stores each possible suffix of string from the last character to the full string.
</nextsent>
<nextsent>using this method, munteanu and marcu are able to detect correct sentence alignments with precision of 95% (out of 100 human-judged and randomly selected sentences from the generated output).
</nextsent>
<nextsent>the running time of their algorithm is approximately 100 hours for 50000 sentences in each of the languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3904">
<title id=" W11-1217.xml">an expectation maximization algorithm for textual unit alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>since the number of negative instances (50002 ? 5000) is far more large than the number of positive ones (5000), the negative training instances were selected randomly out of instances that passed certain word overlap filter (see the paper for details).
</prevsent>
<prevsent>the classifier precision is around 97% with recall of 40% at the chinese-english task and around 95% with recall of 41% for the arabic-english task.
</prevsent>
</prevsection>
<citsent citstr=" P93-1002 ">
another case study of sentence alignment that we will present here is that of chen (1993).<papid> P93-1002 </papid></citsent>
<aftsection>
<nextsent>he employs an em algorithm that will find sentence alignment in parallel corpus which maximizes the translation probability for each sentence bead in the alignment.
</nextsent>
<nextsent>the translation probability to be maximized by the em procedure considering each possible alignment is given by ( ) ( )?
</nextsent>
<nextsent>([ ]) the following notations were used: is the english corpus (a sequence of english sentences), is the french corpus, [ ] is sentence bead (a pairing of sentences in english with sentences in french), ([ ] [ ]) is the sentence alignment (a sequence of sentence beads) and p(l) is the probability that an alignment contains beads.
</nextsent>
<nextsent>the obtained accuracy is around 96% and was computed indirectly by checking disagreement with the brown sentence aligner (brown et al, 1991) <papid> P91-1022 </papid>on randomly selected 500 disagreement cases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3905">
<title id=" W11-1217.xml">an expectation maximization algorithm for textual unit alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the translation probability to be maximized by the em procedure considering each possible alignment is given by ( ) ( )?
</prevsent>
<prevsent>([ ]) the following notations were used: is the english corpus (a sequence of english sentences), is the french corpus, [ ] is sentence bead (a pairing of sentences in english with sentences in french), ([ ] [ ]) is the sentence alignment (a sequence of sentence beads) and p(l) is the probability that an alignment contains beads.
</prevsent>
</prevsection>
<citsent citstr=" P91-1022 ">
the obtained accuracy is around 96% and was computed indirectly by checking disagreement with the brown sentence aligner (brown et al, 1991) <papid> P91-1022 </papid>on randomly selected 500 disagreement cases.</citsent>
<aftsection>
<nextsent>the last case study of document and sentence alignment from very-non-parallel corpora?
</nextsent>
<nextsent>is the work from fung and cheung (2004).<papid> W04-3208 </papid></nextsent>
<nextsent>their contribution to the problem of textual unit alignment resides in devising bootstrapping mechanism in which, after an initial document pairing and consequent sentence alignment using lexical overlapping similarity measure, ibm-4 model (brown et al., 1993) <papid> J93-2003 </papid>is employed to enrich the bilingual dictionary that is used by the similarity measure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3906">
<title id=" W11-1217.xml">an expectation maximization algorithm for textual unit alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the obtained accuracy is around 96% and was computed indirectly by checking disagreement with the brown sentence aligner (brown et al, 1991) <papid> P91-1022 </papid>on randomly selected 500 disagreement cases.</prevsent>
<prevsent>the last case study of document and sentence alignment from very-non-parallel corpora?</prevsent>
</prevsection>
<citsent citstr=" W04-3208 ">
is the work from fung and cheung (2004).<papid> W04-3208 </papid></citsent>
<aftsection>
<nextsent>their contribution to the problem of textual unit alignment resides in devising bootstrapping mechanism in which, after an initial document pairing and consequent sentence alignment using lexical overlapping similarity measure, ibm-4 model (brown et al., 1993) <papid> J93-2003 </papid>is employed to enrich the bilingual dictionary that is used by the similarity measure.</nextsent>
<nextsent>the 129 process is repeated until the set of identified aligned sentences does not grow anymore.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3911">
<title id=" W11-1217.xml">an expectation maximization algorithm for textual unit alignment </title>
<section> emacc.  </section>
<citcontext>
<prevsection>
<prevsent>we suggest to the reader to frequently refer to this section in order to properly understand the next equations: ? is the set of source documents, is the cardinal of this set; ? is the set of target documents with its cardinal; ? is pair of documents, and ; ? is pair of translation equivalents ? ?
</prevsent>
<prevsent>such that is lexical item that belongs to and is lexical item that belongs to ; ? is the set of all existing translation equivalents pairs ? ?.
</prevsent>
</prevsection>
<citsent citstr=" W08-0509 ">
is the translation probability score (as the one given for instance by giza++ (gao and vogel, 2008)).<papid> W08-0509 </papid></citsent>
<aftsection>
<nextsent>we assume that giza++ translation lexicons already exist for the pair of languages of interest.
</nextsent>
<nextsent>in order to tie equation 1 to our problem, we define its variables as follows: ? is the sequence of 1:1 document alignments of the form , { }.
</nextsent>
<nextsent>we call an assignment which is basically sequence of 1:1 document alignments.
</nextsent>
<nextsent>if there are 1:1 document alignments in and if , then the set of all possible assignments has 130 the cardinal equal to ( ) where n!
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3914">
<title id=" W10-4119.xml">using topic sentiment sentences to recognize sentiment polarity in chinese reviews </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>the sp determination can be generally conducted on three language levels: the word level, the sentence level and the discourse level.
</prevsent>
<prevsent>the two main popular approaches, especially in real-world applications, have been based on machine learning techniques and based on semantic analysis techniques.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
research aiming at recognizing the overall sp of discourse is represented by turney (2002), <papid> P02-1053 </papid>pang et al (2002) <papid> W02-1011 </papid>and yi et al (2003).</citsent>
<aftsection>
<nextsent>turney proposed an unsupervised learning algorithm to classify the sentiment orientation of reviews.
</nextsent>
<nextsent>the mutual information difference between the given word or phrase and the words poor?
</nextsent>
<nextsent>and excellent?
</nextsent>
<nextsent>was calculated respectively to measure its semantic orientation; then the average semantic orientation of all the words in given text was regarded as the overall semantic orientation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3915">
<title id=" W10-4119.xml">using topic sentiment sentences to recognize sentiment polarity in chinese reviews </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>the sp determination can be generally conducted on three language levels: the word level, the sentence level and the discourse level.
</prevsent>
<prevsent>the two main popular approaches, especially in real-world applications, have been based on machine learning techniques and based on semantic analysis techniques.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
research aiming at recognizing the overall sp of discourse is represented by turney (2002), <papid> P02-1053 </papid>pang et al (2002) <papid> W02-1011 </papid>and yi et al (2003).</citsent>
<aftsection>
<nextsent>turney proposed an unsupervised learning algorithm to classify the sentiment orientation of reviews.
</nextsent>
<nextsent>the mutual information difference between the given word or phrase and the words poor?
</nextsent>
<nextsent>and excellent?
</nextsent>
<nextsent>was calculated respectively to measure its semantic orientation; then the average semantic orientation of all the words in given text was regarded as the overall semantic orientation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3916">
<title id=" W10-4119.xml">using topic sentiment sentences to recognize sentiment polarity in chinese reviews </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>the semantic orientation of sentence then is determined by referring to sentiment lexicon and sentiment pattern database.
</prevsent>
<prevsent>they applied the approach to classifying the overall sp of document.other related works are concerning the sentiment analysis of sentences and words which underlie recognizing the overall sp of whole text.
</prevsent>
</prevsection>
<citsent citstr=" J04-3002 ">
wiebe et al (2000), wiebe et al (2004) <papid> J04-3002 </papid>proved that the subjectivity of sentence could be judged according to the adjectives in it.</citsent>
<aftsection>
<nextsent>kim &amp; hovy (2004) <papid> C04-1200 </papid>and weibe &amp; riloff (2005) explored the classification of subjective and objective sentences.</nextsent>
<nextsent>yu et al.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3917">
<title id=" W10-4119.xml">using topic sentiment sentences to recognize sentiment polarity in chinese reviews </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>they applied the approach to classifying the overall sp of document.other related works are concerning the sentiment analysis of sentences and words which underlie recognizing the overall sp of whole text.
</prevsent>
<prevsent>wiebe et al (2000), wiebe et al (2004) <papid> J04-3002 </papid>proved that the subjectivity of sentence could be judged according to the adjectives in it.</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
kim &amp; hovy (2004) <papid> C04-1200 </papid>and weibe &amp; riloff (2005) explored the classification of subjective and objective sentences.</citsent>
<aftsection>
<nextsent>yu et al.
</nextsent>
<nextsent>(2003) put forward an approach to extract opinionated sentences in order to serve an automatic question answering system.
</nextsent>
<nextsent>the extracted sentences were classified and the sp of each was determined.
</nextsent>
<nextsent>hu &amp; liu (2004) took advantage of wordnet to obtain sentiment words and their orientations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3920">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the new algorithm shows comparable results to another state-of-the-art system.
</prevsent>
<prevsent>the clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks.
</prevsent>
</prevsection>
<citsent citstr=" D07-1002 ">
semantic role labeling (srl) has sparked much interest in nlp (shen and lapata, 2007; <papid> D07-1002 </papid>liu and gildea, 2010).<papid> C10-1081 </papid></citsent>
<aftsection>
<nextsent>lately, dependency-based srl has shown advantages over constituent-based srl (jo hansson and nugues, 2008).<papid> D08-1008 </papid></nextsent>
<nextsent>two main benefits can be found.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3921">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the new algorithm shows comparable results to another state-of-the-art system.
</prevsent>
<prevsent>the clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks.
</prevsent>
</prevsection>
<citsent citstr=" C10-1081 ">
semantic role labeling (srl) has sparked much interest in nlp (shen and lapata, 2007; <papid> D07-1002 </papid>liu and gildea, 2010).<papid> C10-1081 </papid></citsent>
<aftsection>
<nextsent>lately, dependency-based srl has shown advantages over constituent-based srl (jo hansson and nugues, 2008).<papid> D08-1008 </papid></nextsent>
<nextsent>two main benefits can be found.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3922">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks.
</prevsent>
<prevsent>semantic role labeling (srl) has sparked much interest in nlp (shen and lapata, 2007; <papid> D07-1002 </papid>liu and gildea, 2010).<papid> C10-1081 </papid></prevsent>
</prevsection>
<citsent citstr=" D08-1008 ">
lately, dependency-based srl has shown advantages over constituent-based srl (jo hansson and nugues, 2008).<papid> D08-1008 </papid></citsent>
<aftsection>
<nextsent>two main benefits can be found.
</nextsent>
<nextsent>first, dependency parsing is much faster than constituent parsing, whereas constituent parsing is usually considered to be bottleneck to srl interms of execution time.
</nextsent>
<nextsent>second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between predicate and its arguments with labeled arcs.
</nextsent>
<nextsent>unlike constituent-based srlthat maps phrases to semantic roles, dependency based srl maps headwords to semantic roles because there is no phrasal node in dependency structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3923">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between predicate and its arguments with labeled arcs.
</prevsent>
<prevsent>unlike constituent-based srlthat maps phrases to semantic roles, dependency based srl maps headwords to semantic roles because there is no phrasal node in dependency structure.
</prevsent>
</prevsection>
<citsent citstr=" W10-1811 ">
this may lead to concern about getting the actual semantic chunks back, but choi and palmer (2010) <papid> W10-1811 </papid>have shown that it is possible to recover the original chunks from the headwords with minimal loss, using certain type of dependency structure.traditionally, either constituent or dependency based, semantic role labeling is done in two steps, argument identification and classification (gildeaand jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>this is from general belief that each step requires different set of features (xue and palmer, 2004), <papid> W04-3212 </papid>and training these steps in pipeline takes less time than training them as joint-inference task.</nextsent>
<nextsent>however, recent machine learning algorithms can deal with large scale vector spaces without taking too much training time (hsieh et al, 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3924">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between predicate and its arguments with labeled arcs.
</prevsent>
<prevsent>unlike constituent-based srlthat maps phrases to semantic roles, dependency based srl maps headwords to semantic roles because there is no phrasal node in dependency structure.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
this may lead to concern about getting the actual semantic chunks back, but choi and palmer (2010) <papid> W10-1811 </papid>have shown that it is possible to recover the original chunks from the headwords with minimal loss, using certain type of dependency structure.traditionally, either constituent or dependency based, semantic role labeling is done in two steps, argument identification and classification (gildeaand jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>this is from general belief that each step requires different set of features (xue and palmer, 2004), <papid> W04-3212 </papid>and training these steps in pipeline takes less time than training them as joint-inference task.</nextsent>
<nextsent>however, recent machine learning algorithms can deal with large scale vector spaces without taking too much training time (hsieh et al, 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3925">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unlike constituent-based srlthat maps phrases to semantic roles, dependency based srl maps headwords to semantic roles because there is no phrasal node in dependency structure.
</prevsent>
<prevsent>this may lead to concern about getting the actual semantic chunks back, but choi and palmer (2010) <papid> W10-1811 </papid>have shown that it is possible to recover the original chunks from the headwords with minimal loss, using certain type of dependency structure.traditionally, either constituent or dependency based, semantic role labeling is done in two steps, argument identification and classification (gildeaand jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3212 ">
this is from general belief that each step requires different set of features (xue and palmer, 2004), <papid> W04-3212 </papid>and training these steps in pipeline takes less time than training them as joint-inference task.</citsent>
<aftsection>
<nextsent>however, recent machine learning algorithms can deal with large scale vector spaces without taking too much training time (hsieh et al, 2008).
</nextsent>
<nextsent>furthermore, from our experience in dependency parsing, handling these steps together improves accuracy in identification as well as classification (unlabeled and labeled attachment scoresin dependency parsing).
</nextsent>
<nextsent>this motivates the development of new semantic role labeling algorithm that treats these two steps as joint inference task.our algorithm is inspired by shift-reduce parsing (nivre, 2008).<papid> J08-4003 </papid></nextsent>
<nextsent>the algorithm uses several transitions to identify predicates and their arguments with semantic roles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3926">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, recent machine learning algorithms can deal with large scale vector spaces without taking too much training time (hsieh et al, 2008).
</prevsent>
<prevsent>furthermore, from our experience in dependency parsing, handling these steps together improves accuracy in identification as well as classification (unlabeled and labeled attachment scoresin dependency parsing).
</prevsent>
</prevsection>
<citsent citstr=" J08-4003 ">
this motivates the development of new semantic role labeling algorithm that treats these two steps as joint inference task.our algorithm is inspired by shift-reduce parsing (nivre, 2008).<papid> J08-4003 </papid></citsent>
<aftsection>
<nextsent>the algorithm uses several transitions to identify predicates and their arguments with semantic roles.
</nextsent>
<nextsent>one big advantage of the transition based approach is that it can use previously identified arguments as features to predict the next argument.
</nextsent>
<nextsent>we apply this technique to our approach and achieve comparable results to another state-of-the art system evaluated on the same datasets.
</nextsent>
<nextsent>37 no-pred ( 1 , 2, j, 3, [i|4], )?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3927">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> transition-based semantic role labeling.  </section>
<citcontext>
<prevsection>
<prevsent>first, unlike dependency parsing that tries to find some kind of relation between any word pair, semantic role labeling restricts its search onlyto top-down relations between predicate and argument pairs.
</prevsent>
<prevsent>second, dependency parsing requires one head for each word, so the final output is tree,whereas semantic role labeling allows multiple predicates for each argument.
</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
thus, not all dependency parsing algorithms, such as maximum spanning tree algorithm (mcdonald and pereira, 2006), <papid> E06-1011 </papid>can be naively applied to semantic role labeling.some transition-based dependency parsing algorithms have been adapted to semantic role labeling and shown good results (henderson et al, 2008; <papid> W08-2122 </papid>titov et al, 2009).</citsent>
<aftsection>
<nextsent>however, these algorithms are originally designed for dependency parsing, so arenot necessarily customized for semantic role labeling.
</nextsent>
<nextsent>here, we present novel transition-based algorithm dedicated to semantic role labeling.
</nextsent>
<nextsent>the key difference between this algorithm and most other transition-based algorithms is in its directionality.
</nextsent>
<nextsent>given an identified predicate, this algorithm tries to find top-down relations between the predicate and the words on both left and right-hand sides, whereas other transition-based algorithms would consider words on either the left or the right-hand side, but not both.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3928">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> transition-based semantic role labeling.  </section>
<citcontext>
<prevsection>
<prevsent>first, unlike dependency parsing that tries to find some kind of relation between any word pair, semantic role labeling restricts its search onlyto top-down relations between predicate and argument pairs.
</prevsent>
<prevsent>second, dependency parsing requires one head for each word, so the final output is tree,whereas semantic role labeling allows multiple predicates for each argument.
</prevsent>
</prevsection>
<citsent citstr=" W08-2122 ">
thus, not all dependency parsing algorithms, such as maximum spanning tree algorithm (mcdonald and pereira, 2006), <papid> E06-1011 </papid>can be naively applied to semantic role labeling.some transition-based dependency parsing algorithms have been adapted to semantic role labeling and shown good results (henderson et al, 2008; <papid> W08-2122 </papid>titov et al, 2009).</citsent>
<aftsection>
<nextsent>however, these algorithms are originally designed for dependency parsing, so arenot necessarily customized for semantic role labeling.
</nextsent>
<nextsent>here, we present novel transition-based algorithm dedicated to semantic role labeling.
</nextsent>
<nextsent>the key difference between this algorithm and most other transition-based algorithms is in its directionality.
</nextsent>
<nextsent>given an identified predicate, this algorithm tries to find top-down relations between the predicate and the words on both left and right-hand sides, whereas other transition-based algorithms would consider words on either the left or the right-hand side, but not both.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3929">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> transition-based semantic role labeling.  </section>
<citcontext>
<prevsection>
<prevsent>the upper and lower arcs stand for syntactic and semantic dependencies, respectively.
</prevsent>
<prevsent>sbj, obj, oprd, im, nmod stand for subject, object, object predicative, infinitive marker, and noun-modifier.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
a0, a1 stand for arg0, arg1 in propbank (palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>transition 1 2 3 4 0 [ ] [ ] 1 [ ] [2..6] ? 1 no-pred [1] [ ] 2 [ ] [3..6].
</nextsent>
<nextsent>2 left-arc [ ] [1] 2 [ ] [3..6] ? {1a0?
</nextsent>
<nextsent>2}.
</nextsent>
<nextsent>4 no-arc [ ] [1] 2 [3..4] [5..6].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3930">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> transition-based semantic role labeling.  </section>
<citcontext>
<prevsection>
<prevsent>notice that this algorithm does not take separate steps for argument identification and classification.
</prevsent>
<prevsent>by adding the no-arc transitions, we successfully merge these two steps together without decrease in labeling accuracy.1 since each word can be predicate candidate and each predicate considers all other words as argument candidates, worst-case complexity of the algorithm is o(n2).
</prevsent>
</prevsection>
<citsent citstr=" D09-1004 ">
to reduce the complexity, zhao et al (2009) <papid> D09-1004 </papid>reformulated pruning algorithm introduced by xue and palmer (2004) <papid> W04-3212 </papid>for dependency structure by considering only direct dependents of predicate and its ancestors as argument candidates.</citsent>
<aftsection>
<nextsent>this pruning algorithm can be easily applied to our algorithm: the oracle can pre filter such dependents and uses the information to perform no-arc transitions without consulting statistical models.
</nextsent>
<nextsent>1we also experimented with the traditional approach of building separate classifiers for identification and classification, which did not lead to better performance in our case.
</nextsent>
<nextsent>39table 2 shows parsing states generated by our algorithm.
</nextsent>
<nextsent>our experiments show that this algorithm gives comparable results against another state-of the-art system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3934">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> predicate argument clustering.  </section>
<citcontext>
<prevsection>
<prevsent>39table 2 shows parsing states generated by our algorithm.
</prevsent>
<prevsent>our experiments show that this algorithm gives comparable results against another state-of the-art system.
</prevsent>
</prevsection>
<citsent citstr=" J08-2006 ">
some studies showed that verb clustering information could improve performance in semantic role labeling (gildea and jurafsky, 2002; <papid> J02-3001 </papid>pradhan et al, 2008).<papid> J08-2006 </papid></citsent>
<aftsection>
<nextsent>this is because semantic role label ers usually perform worse on verbs not seen during training, for which the clustering information can provide usefulfeatures.
</nextsent>
<nextsent>most previous studies used either bag-ofwords or syntactic structure to cluster verbs; however, this may or may not capture the nature of predicate argument structure, which is more semantically oriented.
</nextsent>
<nextsent>thus, it is preferable to cluster verbs by their predicate argument structures to get optimized features for semantic role labeling.in this section, we present self-learning clustering technique that effectively improves labeling accuracy in the test domain.
</nextsent>
<nextsent>first, we perform semantic role labeling on the test data using the algorithm in section 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3936">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for in-domain and out-of-domain evaluations, wsj section 23 and the brown corpus are used, also distributed by conll09.
</prevsent>
<prevsent>to retrieve automatically generated dependency trees as input to our semantic role labeler, we train our open source dependency parser, called clearparser3, on the training set and run the parser on the evaluation sets.
</prevsent>
</prevsection>
<citsent citstr=" P11-2121 ">
clearparseruses transition-based dependency parsing algorithm that gives near state-of-the-art results (choi and palmer, 2011), <papid> P11-2121 </papid>and mirrors our srl algorithm.</citsent>
<aftsection>
<nextsent>5.2 statistical models.
</nextsent>
<nextsent>we use liblinear l2-l1 svm for learning; linear classification algorithm using l2 regularization andl1 loss function.
</nextsent>
<nextsent>this algorithm is designed to handle large scale data: it assumes the data to belin early separable so does not use any kind of kernel space (hsieh et al, 2008).
</nextsent>
<nextsent>as result, it significantly reduces training time compared to typical svm, yet performs accurately.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3938">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>the clustering technique shows potential for improving performance in other new domains.
</prevsent>
<prevsent>these preliminary results are promising; however, there is still much room for improvement.
</prevsent>
</prevsection>
<citsent citstr=" D08-1059 ">
since our algorithm is transition-based, many existing techniques such as k-best ranking (zhang and clark, 2008) <papid> D08-1059 </papid>or dynamic programming (huang and sagae, 2010) <papid> P10-1110 </papid>designed to improve transition-based parsing can be applied.</citsent>
<aftsection>
<nextsent>we can also apply different kinds of clustering algorithms to improve the quality of the verb clusters.
</nextsent>
<nextsent>furthermore, more features, such as named entity tags or dependency labels, can be used to form better representation of feature vectors for the clustering.
</nextsent>
<nextsent>one of the strongest motivations for designing ourtransition-based srl system is to develop joint inference system between dependency parsing and semantic role labeling.
</nextsent>
<nextsent>since we have already developed dependency parser, clear parser, based on parallel transition-based approach, it will be straightforward to integrate this srl system with the parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3939">
<title id=" W11-0906.xml">transition based semantic role labeling using predicate argument clustering </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>the clustering technique shows potential for improving performance in other new domains.
</prevsent>
<prevsent>these preliminary results are promising; however, there is still much room for improvement.
</prevsent>
</prevsection>
<citsent citstr=" P10-1110 ">
since our algorithm is transition-based, many existing techniques such as k-best ranking (zhang and clark, 2008) <papid> D08-1059 </papid>or dynamic programming (huang and sagae, 2010) <papid> P10-1110 </papid>designed to improve transition-based parsing can be applied.</citsent>
<aftsection>
<nextsent>we can also apply different kinds of clustering algorithms to improve the quality of the verb clusters.
</nextsent>
<nextsent>furthermore, more features, such as named entity tags or dependency labels, can be used to form better representation of feature vectors for the clustering.
</nextsent>
<nextsent>one of the strongest motivations for designing ourtransition-based srl system is to develop joint inference system between dependency parsing and semantic role labeling.
</nextsent>
<nextsent>since we have already developed dependency parser, clear parser, based on parallel transition-based approach, it will be straightforward to integrate this srl system with the parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3944">
<title id=" W11-1404.xml">elicited imitation for prediction of opi test scores </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one motivation for using ei, as opposed tosome other form of test, is that it is relatively inexpensive to administer.
</prevsent>
<prevsent>an ei test can be effectively scored by non-experts in relatively short amount of time.
</prevsent>
</prevsection>
<citsent citstr=" L08-1385 ">
it is also well suited for automated scoring (graham et al, 2008), <papid> L08-1385 </papid>since correct responses are predictable.</citsent>
<aftsection>
<nextsent>1.2 motivation.
</nextsent>
<nextsent>the language skills directly measured by an ei test are those involved in repeating back what one has just heard.
</nextsent>
<nextsent>in order to directly measure broader set of language skills, other tests must be used.
</nextsent>
<nextsent>one of these is the oral proficiency interview (opi).the opi is face-to-face interview conducted to assess language proficiency.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3947">
<title id=" W11-1516.xml">a study of academic collaborations in computational linguistics using a latent mixture of authors model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we pre senta new frame for thinking about the variation in collaboration types and develop computational metric to characterize the distinct contributions and roles of each collaborator within the scholarly material they produce.the topic of understanding collaborations has attracted much interest in the social sciences over the years.
</prevsent>
<prevsent>recently, it has gained traction in computer science, too, in the form of social network analysis.
</prevsent>
</prevsection>
<citsent citstr=" W09-3607 ">
much work focuses on studying networks formed via citations (radev et al , 2009; <papid> W09-3607 </papid>white and mccain, 1998), as well as co-authorship links (nascimento et al , 2003; liu et al , 2005).</citsent>
<aftsection>
<nextsent>however, these works focus largely on the graphical structure derived from paper citations and author co-occurrences, and less on the textual content of the papers themselves.
</nextsent>
<nextsent>inthis work, we examine the nature of academic collaboration using text as primary component.we propose theoretical framework for determining the types of collaboration present in document, based on factors such as the number of established authors, the presence of unestablished authors and the similarity of the established authors past work to the documents term vector.
</nextsent>
<nextsent>these collaboration types attempt to describe the nature of coauthor ships between students and advisors (e.g. ap prentice?
</nextsent>
<nextsent>versus new blood?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3948">
<title id=" W11-1516.xml">a study of academic collaborations in computational linguistics using a latent mixture of authors model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present decision diagram for classifying papers into these types, as well as description of the intuition behind each collaboration class.
</prevsent>
<prevsent>124 we explore our theory with computational method to categorize collaborative works into their collaboration types using an approach based on topic modeling, where we model every paper as latent mixture of its authors.
</prevsent>
</prevsection>
<citsent citstr=" D09-1026 ">
for our system, we use labeled-lda (llda (ramage et al , 2009)) <papid> D09-1026 </papid>to train models over the acl corpus for every year of the words best attributed to each author in all the papers they write.</citsent>
<aftsection>
<nextsent>we use the resulting author signatures as basis for several metrics that can classify each document by its collaboration type.we qualitatively analyze our results by examining the categorization of several high impact papers.
</nextsent>
<nextsent>with consultation from prominent researchers and textbook writers in the field, we demonstrate that our system is able to differentiate between the various types of collaborations in our suggested taxonomy, based only on words used, at low but statistically significant accuracy.
</nextsent>
<nextsent>we use this same similarity score to analyze the acl community by sub-field, finding significant deviations.
</nextsent>
<nextsent>in recent years, popular topic models such as latent dirichlet allocation (blei et al , 2003) have been increasingly used to study the history of science by observing the changing trends in term based topics (hall et al , 2008), (<papid> D08-1038 </papid>gerrish and blei, 2010).in the case of hallet al , regular lda topic models were trained over the acl anthology on per year basis, and the changing trends in topics were studied from year to year.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3949">
<title id=" W11-1516.xml">a study of academic collaborations in computational linguistics using a latent mixture of authors model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>with consultation from prominent researchers and textbook writers in the field, we demonstrate that our system is able to differentiate between the various types of collaborations in our suggested taxonomy, based only on words used, at low but statistically significant accuracy.
</prevsent>
<prevsent>we use this same similarity score to analyze the acl community by sub-field, finding significant deviations.
</prevsent>
</prevsection>
<citsent citstr=" D08-1038 ">
in recent years, popular topic models such as latent dirichlet allocation (blei et al , 2003) have been increasingly used to study the history of science by observing the changing trends in term based topics (hall et al , 2008), (<papid> D08-1038 </papid>gerrish and blei, 2010).in the case of hallet al , regular lda topic models were trained over the acl anthology on per year basis, and the changing trends in topics were studied from year to year.</citsent>
<aftsection>
<nextsent>gerrish and bleis work computed measure of influence by using dynamic topic models (blei and lafferty, 2006) and studying the change of statistics of the language used in corpus.these models propose interesting ideas for utilizing topic modeling to understand aspects of scientific history.
</nextsent>
<nextsent>however, our primary interest, in this paper, is the study of academic collaboration between different authors; we therefore look to learn models for authors instead of only documents.
</nextsent>
<nextsent>popular topic models for authors include the author topic model (rosen-zvi et al , 2004), simple extension of regular lda that adds an additional author variable over the topics.
</nextsent>
<nextsent>the author-topic model learns distribution over words for each topic, as in regular lda, as well as distribution over topics for each author.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3960">
<title id=" W11-0318.xml">using the mutual knearest neighbor graphs for semi supervised classification on natural language data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semi-supervised classification try to take advantage of large amount of unlabeled data in addition to small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data.
</prevsent>
<prevsent>in particular,graph-based techniques for semi-supervised classification (zhou et al , 2004; zhu et al , 2003; cal lut et al , 2008; wang et al , 2008) are recognized as promising approach.
</prevsent>
</prevsection>
<citsent citstr=" N07-1026 ">
some of these techniques have been successfully applied for nlp tasks: word sense disambiguation (alexandrescu and kirchhoff,2007; <papid> N07-1026 </papid>niu et al , 2005), <papid> P05-1049 </papid>sentiment analysis (goldberg and zhu, 2006), <papid> W06-3808 </papid>and statistical machine translation (alexandrescu and kirchhoff, 2009), <papid> N09-1014 </papid>to name but few.however, the focus of these studies is how to as sign accurate labels to vertices in given graph.</citsent>
<aftsection>
<nextsent>by contrast, there has not been much work on how sucha graph should be built, and graph construction remains more of an art than science?
</nextsent>
<nextsent>(zhu, 2005).yet, it is an essential step for graph-based semi supervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results.
</nextsent>
<nextsent>both for semi-supervised classification and for clustering, the k-nearest neighbor (k-nn) graph construction has been used almost exclusively in the literature.
</nextsent>
<nextsent>however, k-nn graphs often produce hubs, or vertices with extremely high degree (i.e.,the number of edges incident to vertex).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3961">
<title id=" W11-0318.xml">using the mutual knearest neighbor graphs for semi supervised classification on natural language data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semi-supervised classification try to take advantage of large amount of unlabeled data in addition to small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data.
</prevsent>
<prevsent>in particular,graph-based techniques for semi-supervised classification (zhou et al , 2004; zhu et al , 2003; cal lut et al , 2008; wang et al , 2008) are recognized as promising approach.
</prevsent>
</prevsection>
<citsent citstr=" P05-1049 ">
some of these techniques have been successfully applied for nlp tasks: word sense disambiguation (alexandrescu and kirchhoff,2007; <papid> N07-1026 </papid>niu et al , 2005), <papid> P05-1049 </papid>sentiment analysis (goldberg and zhu, 2006), <papid> W06-3808 </papid>and statistical machine translation (alexandrescu and kirchhoff, 2009), <papid> N09-1014 </papid>to name but few.however, the focus of these studies is how to as sign accurate labels to vertices in given graph.</citsent>
<aftsection>
<nextsent>by contrast, there has not been much work on how sucha graph should be built, and graph construction remains more of an art than science?
</nextsent>
<nextsent>(zhu, 2005).yet, it is an essential step for graph-based semi supervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results.
</nextsent>
<nextsent>both for semi-supervised classification and for clustering, the k-nearest neighbor (k-nn) graph construction has been used almost exclusively in the literature.
</nextsent>
<nextsent>however, k-nn graphs often produce hubs, or vertices with extremely high degree (i.e.,the number of edges incident to vertex).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3962">
<title id=" W11-0318.xml">using the mutual knearest neighbor graphs for semi supervised classification on natural language data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semi-supervised classification try to take advantage of large amount of unlabeled data in addition to small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data.
</prevsent>
<prevsent>in particular,graph-based techniques for semi-supervised classification (zhou et al , 2004; zhu et al , 2003; cal lut et al , 2008; wang et al , 2008) are recognized as promising approach.
</prevsent>
</prevsection>
<citsent citstr=" W06-3808 ">
some of these techniques have been successfully applied for nlp tasks: word sense disambiguation (alexandrescu and kirchhoff,2007; <papid> N07-1026 </papid>niu et al , 2005), <papid> P05-1049 </papid>sentiment analysis (goldberg and zhu, 2006), <papid> W06-3808 </papid>and statistical machine translation (alexandrescu and kirchhoff, 2009), <papid> N09-1014 </papid>to name but few.however, the focus of these studies is how to as sign accurate labels to vertices in given graph.</citsent>
<aftsection>
<nextsent>by contrast, there has not been much work on how sucha graph should be built, and graph construction remains more of an art than science?
</nextsent>
<nextsent>(zhu, 2005).yet, it is an essential step for graph-based semi supervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results.
</nextsent>
<nextsent>both for semi-supervised classification and for clustering, the k-nearest neighbor (k-nn) graph construction has been used almost exclusively in the literature.
</nextsent>
<nextsent>however, k-nn graphs often produce hubs, or vertices with extremely high degree (i.e.,the number of edges incident to vertex).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3963">
<title id=" W11-0318.xml">using the mutual knearest neighbor graphs for semi supervised classification on natural language data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semi-supervised classification try to take advantage of large amount of unlabeled data in addition to small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data.
</prevsent>
<prevsent>in particular,graph-based techniques for semi-supervised classification (zhou et al , 2004; zhu et al , 2003; cal lut et al , 2008; wang et al , 2008) are recognized as promising approach.
</prevsent>
</prevsection>
<citsent citstr=" N09-1014 ">
some of these techniques have been successfully applied for nlp tasks: word sense disambiguation (alexandrescu and kirchhoff,2007; <papid> N07-1026 </papid>niu et al , 2005), <papid> P05-1049 </papid>sentiment analysis (goldberg and zhu, 2006), <papid> W06-3808 </papid>and statistical machine translation (alexandrescu and kirchhoff, 2009), <papid> N09-1014 </papid>to name but few.however, the focus of these studies is how to as sign accurate labels to vertices in given graph.</citsent>
<aftsection>
<nextsent>by contrast, there has not been much work on how sucha graph should be built, and graph construction remains more of an art than science?
</nextsent>
<nextsent>(zhu, 2005).yet, it is an essential step for graph-based semi supervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results.
</nextsent>
<nextsent>both for semi-supervised classification and for clustering, the k-nearest neighbor (k-nn) graph construction has been used almost exclusively in the literature.
</nextsent>
<nextsent>however, k-nn graphs often produce hubs, or vertices with extremely high degree (i.e.,the number of edges incident to vertex).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3966">
<title id=" W11-0318.xml">using the mutual knearest neighbor graphs for semi supervised classification on natural language data </title>
<section> problem statement.  </section>
<citcontext>
<prevsection>
<prevsent>to this end, we eliminate such high degree vertices from the graph,and compare the classification accuracy of other vertices before and after the elimination.
</prevsent>
<prevsent>for this preliminary experiment, we used the line?
</prevsent>
</prevsection>
<citsent citstr=" H93-1051 ">
dataset of word sense disambiguation task (leacock et al , 1993).<papid> H93-1051 </papid></citsent>
<aftsection>
<nextsent>for details of the dataset and the task, see section 6.in this experiment, we randomly selected 10 percent of examples as labeled examples.
</nextsent>
<nextsent>the remaining 90 percent makes the set of unlabeled examples, and the goal is to predict the label (word sense) of these unlabeled examples.
</nextsent>
<nextsent>we first built k-nn graph (with = 3) from the dataset, and ran gaussian random fields(grf) (zhu et al , 2003), one of the most widely used graph-based semi-supervised classification algorithms.
</nextsent>
<nextsent>then we removed vertices with degree 156 greater than or equal to 30 from the k-nn graph, and ran grf again on this hub-removed?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3967">
<title id=" W11-0318.xml">using the mutual knearest neighbor graphs for semi supervised classification on natural language data </title>
<section> problem statement.  </section>
<citcontext>
<prevsection>
<prevsent>each instance of the polysemous word interest?
</prevsent>
<prevsent>has been tagged with one of the six senses in longman dictionary of contemporary english.
</prevsent>
</prevsection>
<citsent citstr=" P94-1020 ">
the details of the dataset are described in bruce and wiebe (1994).<papid> P94-1020 </papid>the line?</citsent>
<aftsection>
<nextsent>data is originally used in numerous comparative studies of word sense disambiguation.
</nextsent>
<nextsent>each instance of the word line?
</nextsent>
<nextsent>has been tagged with one of the six senses on the wordnet thesaurus.
</nextsent>
<nextsent>further details can be found in the leacock et al  (1993).<papid> H93-1051 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3970">
<title id=" W11-0318.xml">using the mutual knearest neighbor graphs for semi supervised classification on natural language data </title>
<section> problem statement.  </section>
<citcontext>
<prevsection>
<prevsent>further details can be found in the leacock et al  (1993).<papid> H93-1051 </papid></prevsent>
<prevsent>following niu et al  (2005), <papid> P05-1049 </papid>we used the following context features in the word sense disambiguation tasks: part-of-speech of neighboring words, single words in the surrounding context, and local colloca tion.</prevsent>
</prevsection>
<citsent citstr=" W02-1006 ">
details of these context features can be found in lee and ng (2002).<papid> W02-1006 </papid>the reuters dataset is extracted from rcv1 v2/lyrl2004, text categorization test collection(lewis et al , 2004).</citsent>
<aftsection>
<nextsent>in the same manner as crammer et al  (2009), <papid> D09-1052 </papid>we produced the classification dataset by selecting approximately 4,000 documents from 4 general topics (corporate, economic, government and markets) at random.</nextsent>
<nextsent>the features described in lewis et al  (2004) are used with this dataset.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3971">
<title id=" W11-0318.xml">using the mutual knearest neighbor graphs for semi supervised classification on natural language data </title>
<section> problem statement.  </section>
<citcontext>
<prevsection>
<prevsent>following niu et al  (2005), <papid> P05-1049 </papid>we used the following context features in the word sense disambiguation tasks: part-of-speech of neighboring words, single words in the surrounding context, and local colloca tion.</prevsent>
<prevsent>details of these context features can be found in lee and ng (2002).<papid> W02-1006 </papid>the reuters dataset is extracted from rcv1 v2/lyrl2004, text categorization test collection(lewis et al , 2004).</prevsent>
</prevsection>
<citsent citstr=" D09-1052 ">
in the same manner as crammer et al  (2009), <papid> D09-1052 </papid>we produced the classification dataset by selecting approximately 4,000 documents from 4 general topics (corporate, economic, government and markets) at random.</citsent>
<aftsection>
<nextsent>the features described in lewis et al  (2004) are used with this dataset.
</nextsent>
<nextsent>the 20 news groups dataset is popular dataset frequently used for document classification and clustering.
</nextsent>
<nextsent>the dataset consists of approximately 20,000 messages on news groups and is originally distributed by lang (1995).
</nextsent>
<nextsent>each message is assigned one of the 20 possible labels indicating which news group it has been posted to, and represented as binary bag-of-words features as described in rennie (2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3972">
<title id=" W11-1605.xml">text specificity and impact on quality of news summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of ponce, puerto rico, and 200 miles southeast of santo domingo.
</prevsent>
<prevsent>table 1: general (in italics) and specific sentences prior studies have advocated that the distinction between general and specific content is relevant for text summarization.
</prevsent>
</prevsection>
<citsent citstr=" A00-2024 ">
jing and mckeown (2000) <papid> A00-2024 </papid>studied what edits people use to create summaries from sentences in the source text.</citsent>
<aftsection>
<nextsent>two of the operations they identify are generalization and specification where the source content gets changed in the summary with respect to specificity.
</nextsent>
<nextsent>in more recent work, haghighi and vanderwende (2009) <papid> N09-1041 </papid>built summarization system based on topic models, where both topics at general document level as well as those at specific sub topic levels were learnt.</nextsent>
<nextsent>the underlying idea here is that summaries are generated by combination of content from both these levels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3973">
<title id=" W11-1605.xml">text specificity and impact on quality of news summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>jing and mckeown (2000) <papid> A00-2024 </papid>studied what edits people use to create summaries from sentences in the source text.</prevsent>
<prevsent>two of the operations they identify are generalization and specification where the source content gets changed in the summary with respect to specificity.</prevsent>
</prevsection>
<citsent citstr=" N09-1041 ">
in more recent work, haghighi and vanderwende (2009) <papid> N09-1041 </papid>built summarization system based on topic models, where both topics at general document level as well as those at specific sub topic levels were learnt.</citsent>
<aftsection>
<nextsent>the underlying idea here is that summaries are generated by combination of content from both these levels.
</nextsent>
<nextsent>but since the preference for these two types of content is not known, haghighi and vanderwende (2009) <papid> N09-1041 </papid>use some heuristic proportions.many systems that deal with sentence compression (knight and marcu, 2002; mcdonald, 2006; <papid> E06-1038 </papid>galley and mckeown, 2007; <papid> N07-1023 </papid>clarke and lapata, 2008) and fusion (barzilay and mckeown, 2005;<papid> J05-3002 </papid>filippova and strube, 2008), <papid> D08-1019 </papid>do not take into account the specificity of the original or desired sentence.</nextsent>
<nextsent>however, wan et al (2008) <papid> D08-1057 </papid>introduce generation task where summary sentence is created by combining content from key (general) sentence and its supporting sentences in the source.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3977">
<title id=" W11-1605.xml">text specificity and impact on quality of news summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in more recent work, haghighi and vanderwende (2009) <papid> N09-1041 </papid>built summarization system based on topic models, where both topics at general document level as well as those at specific sub topic levels were learnt.</prevsent>
<prevsent>the underlying idea here is that summaries are generated by combination of content from both these levels.</prevsent>
</prevsection>
<citsent citstr=" E06-1038 ">
but since the preference for these two types of content is not known, haghighi and vanderwende (2009) <papid> N09-1041 </papid>use some heuristic proportions.many systems that deal with sentence compression (knight and marcu, 2002; mcdonald, 2006; <papid> E06-1038 </papid>galley and mckeown, 2007; <papid> N07-1023 </papid>clarke and lapata, 2008) and fusion (barzilay and mckeown, 2005;<papid> J05-3002 </papid>filippova and strube, 2008), <papid> D08-1019 </papid>do not take into account the specificity of the original or desired sentence.</citsent>
<aftsection>
<nextsent>however, wan et al (2008) <papid> D08-1057 </papid>introduce generation task where summary sentence is created by combining content from key (general) sentence and its supporting sentences in the source.</nextsent>
<nextsent>more 34 recently, marsi et al (2010) manually annotated the transformations between source and compressed phrases and observe that generalization is frequent transformation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3979">
<title id=" W11-1605.xml">text specificity and impact on quality of news summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in more recent work, haghighi and vanderwende (2009) <papid> N09-1041 </papid>built summarization system based on topic models, where both topics at general document level as well as those at specific sub topic levels were learnt.</prevsent>
<prevsent>the underlying idea here is that summaries are generated by combination of content from both these levels.</prevsent>
</prevsection>
<citsent citstr=" N07-1023 ">
but since the preference for these two types of content is not known, haghighi and vanderwende (2009) <papid> N09-1041 </papid>use some heuristic proportions.many systems that deal with sentence compression (knight and marcu, 2002; mcdonald, 2006; <papid> E06-1038 </papid>galley and mckeown, 2007; <papid> N07-1023 </papid>clarke and lapata, 2008) and fusion (barzilay and mckeown, 2005;<papid> J05-3002 </papid>filippova and strube, 2008), <papid> D08-1019 </papid>do not take into account the specificity of the original or desired sentence.</citsent>
<aftsection>
<nextsent>however, wan et al (2008) <papid> D08-1057 </papid>introduce generation task where summary sentence is created by combining content from key (general) sentence and its supporting sentences in the source.</nextsent>
<nextsent>more 34 recently, marsi et al (2010) manually annotated the transformations between source and compressed phrases and observe that generalization is frequent transformation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3980">
<title id=" W11-1605.xml">text specificity and impact on quality of news summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in more recent work, haghighi and vanderwende (2009) <papid> N09-1041 </papid>built summarization system based on topic models, where both topics at general document level as well as those at specific sub topic levels were learnt.</prevsent>
<prevsent>the underlying idea here is that summaries are generated by combination of content from both these levels.</prevsent>
</prevsection>
<citsent citstr=" J05-3002 ">
but since the preference for these two types of content is not known, haghighi and vanderwende (2009) <papid> N09-1041 </papid>use some heuristic proportions.many systems that deal with sentence compression (knight and marcu, 2002; mcdonald, 2006; <papid> E06-1038 </papid>galley and mckeown, 2007; <papid> N07-1023 </papid>clarke and lapata, 2008) and fusion (barzilay and mckeown, 2005;<papid> J05-3002 </papid>filippova and strube, 2008), <papid> D08-1019 </papid>do not take into account the specificity of the original or desired sentence.</citsent>
<aftsection>
<nextsent>however, wan et al (2008) <papid> D08-1057 </papid>introduce generation task where summary sentence is created by combining content from key (general) sentence and its supporting sentences in the source.</nextsent>
<nextsent>more 34 recently, marsi et al (2010) manually annotated the transformations between source and compressed phrases and observe that generalization is frequent transformation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3981">
<title id=" W11-1605.xml">text specificity and impact on quality of news summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in more recent work, haghighi and vanderwende (2009) <papid> N09-1041 </papid>built summarization system based on topic models, where both topics at general document level as well as those at specific sub topic levels were learnt.</prevsent>
<prevsent>the underlying idea here is that summaries are generated by combination of content from both these levels.</prevsent>
</prevsection>
<citsent citstr=" D08-1019 ">
but since the preference for these two types of content is not known, haghighi and vanderwende (2009) <papid> N09-1041 </papid>use some heuristic proportions.many systems that deal with sentence compression (knight and marcu, 2002; mcdonald, 2006; <papid> E06-1038 </papid>galley and mckeown, 2007; <papid> N07-1023 </papid>clarke and lapata, 2008) and fusion (barzilay and mckeown, 2005;<papid> J05-3002 </papid>filippova and strube, 2008), <papid> D08-1019 </papid>do not take into account the specificity of the original or desired sentence.</citsent>
<aftsection>
<nextsent>however, wan et al (2008) <papid> D08-1057 </papid>introduce generation task where summary sentence is created by combining content from key (general) sentence and its supporting sentences in the source.</nextsent>
<nextsent>more 34 recently, marsi et al (2010) manually annotated the transformations between source and compressed phrases and observe that generalization is frequent transformation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3982">
<title id=" W11-1605.xml">text specificity and impact on quality of news summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the underlying idea here is that summaries are generated by combination of content from both these levels.
</prevsent>
<prevsent>but since the preference for these two types of content is not known, haghighi and vanderwende (2009) <papid> N09-1041 </papid>use some heuristic proportions.many systems that deal with sentence compression (knight and marcu, 2002; mcdonald, 2006; <papid> E06-1038 </papid>galley and mckeown, 2007; <papid> N07-1023 </papid>clarke and lapata, 2008) and fusion (barzilay and mckeown, 2005;<papid> J05-3002 </papid>filippova and strube, 2008), <papid> D08-1019 </papid>do not take into account the specificity of the original or desired sentence.</prevsent>
</prevsection>
<citsent citstr=" D08-1057 ">
however, wan et al (2008) <papid> D08-1057 </papid>introduce generation task where summary sentence is created by combining content from key (general) sentence and its supporting sentences in the source.</citsent>
<aftsection>
<nextsent>more 34 recently, marsi et al (2010) manually annotated the transformations between source and compressed phrases and observe that generalization is frequent transformation.
</nextsent>
<nextsent>but it is not known what distribution of genera land specific content is natural for summaries.
</nextsent>
<nextsent>in addition, an analysis of whether this aspect is related to quality of the summary has also not been done so far.
</nextsent>
<nextsent>we address this issue in our work, making useof an accurate classifier to identify general and specific sentences that we have developed (louis and nenkova, 2011).we present the first quantitative analysis of general and specific content in large corpus of news documents and human and automatic summaries produced for them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3984">
<title id=" W11-1605.xml">text specificity and impact on quality of news summaries </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>so we performed an analysis to check the contribution of generality to the content scores in addition to the importance factor.
</prevsent>
<prevsent>we combine measure of content importance 37 predictor mean ? stdev.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
t value p-value (intercept) 0.212 0.03 6.87 2.3e-11 * rouge2 1.299 0.11 11.74   2e-16 * avgspec -0.166 0.04 -4.21 3.1e-05 * table 3: results from regression test from the rouge automatic evaluation (lin and hovy, 2003; <papid> N03-1020 </papid>lin, 2004) <papid> W04-1013 </papid>with generality to predict the coverage scores.</citsent>
<aftsection>
<nextsent>we use the same reference as used for the official coverage score evaluation and compute rouge-2 which is the recall of bigrams of the human summary by the system summary.
</nextsent>
<nextsent>next we train regression model on our data using the rouge-2 score and specificity as predictors of the coverage score.
</nextsent>
<nextsent>we then inspected the weights learnt in the regression model to identify the influence of the predictors.
</nextsent>
<nextsent>table 3 shows the mean values and standard deviation of the beta coefficients.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3985">
<title id=" W11-1605.xml">text specificity and impact on quality of news summaries </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>so we performed an analysis to check the contribution of generality to the content scores in addition to the importance factor.
</prevsent>
<prevsent>we combine measure of content importance 37 predictor mean ? stdev.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
t value p-value (intercept) 0.212 0.03 6.87 2.3e-11 * rouge2 1.299 0.11 11.74   2e-16 * avgspec -0.166 0.04 -4.21 3.1e-05 * table 3: results from regression test from the rouge automatic evaluation (lin and hovy, 2003; <papid> N03-1020 </papid>lin, 2004) <papid> W04-1013 </papid>with generality to predict the coverage scores.</citsent>
<aftsection>
<nextsent>we use the same reference as used for the official coverage score evaluation and compute rouge-2 which is the recall of bigrams of the human summary by the system summary.
</nextsent>
<nextsent>next we train regression model on our data using the rouge-2 score and specificity as predictors of the coverage score.
</nextsent>
<nextsent>we then inspected the weights learnt in the regression model to identify the influence of the predictors.
</nextsent>
<nextsent>table 3 shows the mean values and standard deviation of the beta coefficients.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3990">
<title id=" W11-1414.xml">generating concept map exercises from textbooks </title>
<section> computational model.  </section>
<citcontext>
<prevsection>
<prevsent>the next sections outline the techniques and models used for defining key terms and edges, followed by our method of graph extraction.
</prevsent>
<prevsent>3.1 key terms.
</prevsent>
</prevsection>
<citsent citstr=" D09-1137 ">
general purpose key term extraction procedures are the subject of current research (medelyan et al, 2009), <papid> D09-1137 </papid>but they are less relevant in pedagogical context where key terms are often already provided in learning materials.</citsent>
<aftsection>
<nextsent>for example, both glossaries (navigli and velardi, 2008), and textbook indices (larranaga et al, 2004) have previously been usedas resources in constructing domain models and ontologies.
</nextsent>
<nextsent>to develop our key terms, we used the glossary and index from textbook in the domain of biology (miller and levine, 2002) as well as the keywords given in test-prep study guide (cypress curriculum services, 2008).
</nextsent>
<nextsent>thus we can skip the key word extraction step of previous work on concept map extraction (valerio and leake, 2008; zouaq and nkambou, 2009) and the various errors associated with that process.
</nextsent>
<nextsent>3.2 edge relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3991">
<title id=" W11-1414.xml">generating concept map exercises from textbooks </title>
<section> computational model.  </section>
<citcontext>
<prevsection>
<prevsent>additionally, the following process was developed using biology glossary and biology study guide as development dataset, so training and testing data were kept separate in this study.
</prevsent>
<prevsent>we processed high school biology text (miller and levine, 2002), using its index and glossary as sources of key terms as described above, using thelth srl2 parser.
</prevsent>
</prevsection>
<citsent citstr=" W08-2123 ">
the lth srl parser is semantic role labeling parser that outputs dependency parse annotated with propbank and nombank pred icate/argument structures (johansson and nugues, 2008; <papid> W08-2123 </papid>meyers et al, 2004; <papid> W04-2705 </papid>palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>foreach word token in parse, the parser returns in 2the swedish lunds tekniska hogskola?
</nextsent>
<nextsent>translates as faculty of engineering?
</nextsent>
<nextsent>formation about the word tokens part of speech, lemma, head, and relation to the head.
</nextsent>
<nextsent>moreover,it uses propbank and nombank to identify predicates in the parse, either verbal predicates (prop bank) or nominal predicates (nombank), and their associated arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3992">
<title id=" W11-1414.xml">generating concept map exercises from textbooks </title>
<section> computational model.  </section>
<citcontext>
<prevsection>
<prevsent>additionally, the following process was developed using biology glossary and biology study guide as development dataset, so training and testing data were kept separate in this study.
</prevsent>
<prevsent>we processed high school biology text (miller and levine, 2002), using its index and glossary as sources of key terms as described above, using thelth srl2 parser.
</prevsent>
</prevsection>
<citsent citstr=" W04-2705 ">
the lth srl parser is semantic role labeling parser that outputs dependency parse annotated with propbank and nombank pred icate/argument structures (johansson and nugues, 2008; <papid> W08-2123 </papid>meyers et al, 2004; <papid> W04-2705 </papid>palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>foreach word token in parse, the parser returns in 2the swedish lunds tekniska hogskola?
</nextsent>
<nextsent>translates as faculty of engineering?
</nextsent>
<nextsent>formation about the word tokens part of speech, lemma, head, and relation to the head.
</nextsent>
<nextsent>moreover,it uses propbank and nombank to identify predicates in the parse, either verbal predicates (prop bank) or nominal predicates (nombank), and their associated arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3993">
<title id=" W11-1414.xml">generating concept map exercises from textbooks </title>
<section> computational model.  </section>
<citcontext>
<prevsection>
<prevsent>additionally, the following process was developed using biology glossary and biology study guide as development dataset, so training and testing data were kept separate in this study.
</prevsent>
<prevsent>we processed high school biology text (miller and levine, 2002), using its index and glossary as sources of key terms as described above, using thelth srl2 parser.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
the lth srl parser is semantic role labeling parser that outputs dependency parse annotated with propbank and nombank pred icate/argument structures (johansson and nugues, 2008; <papid> W08-2123 </papid>meyers et al, 2004; <papid> W04-2705 </papid>palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>foreach word token in parse, the parser returns in 2the swedish lunds tekniska hogskola?
</nextsent>
<nextsent>translates as faculty of engineering?
</nextsent>
<nextsent>formation about the word tokens part of speech, lemma, head, and relation to the head.
</nextsent>
<nextsent>moreover,it uses propbank and nombank to identify predicates in the parse, either verbal predicates (prop bank) or nominal predicates (nombank), and their associated arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3994">
<title id=" W11-1414.xml">generating concept map exercises from textbooks </title>
<section> computational model.  </section>
<citcontext>
<prevsection>
<prevsent>finally the nominal filter removes all nombankpredicates except has-part predicates, since these of ten have span end nodes and so contain themselves, e.g. light has-property the energy of sunlight.
</prevsent>
<prevsent>the final filter uses likelihood ratios to establish whether the relation between start and end nodes is meaningful, i.e. something not likely to occurby chance.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
this filter measures the association between the start and end node using likelihood ratios (dunning, 1993) <papid> J93-1003 </papid>and 2 significance criterion to remove triples with insignificant association.</citsent>
<aftsection>
<nextsent>as first step in the filter, words from the end node that have low log entropy are removed prior to calculation.
</nextsent>
<nextsent>this penalizes non-distinctive words that occur in many contexts.
</nextsent>
<nextsent>next, the remaining words from start and end nodes are pooled into bags of words, and the likelihood ratio calculated.
</nextsent>
<nextsent>by transforming the likelihood ratio to be 2 distributed (manningand schutze, 1999), and applying statistical significance threshold of .0001, triples with weak association between start and end nodes were filtered out.the likelihood ratio filter helps prevent sentences related to specific examples from being integrated into concept maps for general concept.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3995">
<title id=" W10-4224.xml">anchor progression in spatially situated discourse a production experiment </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>in their seminal work dale and reiter (1995) present the incremental algorithm (ia) forgre.
</prevsent>
<prevsent>recent extensions address some of its shortcomings, such as negated and dis joined properties (van deemter, 2002) and an account of salience for generating contextually appropriate shorter res (krahmer and theune, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P97-1027 ">
other, alternative gre algorithms exist (horacek, 1997; <papid> P97-1027 </papid>bateman,1999; <papid> P99-1017 </papid>krahmer et al, 2003).<papid> J03-1003 </papid></citsent>
<aftsection>
<nextsent>however, all these algorithms relyon given domain of discourse constituting the current context (or focus of attention).
</nextsent>
<nextsent>the task of the gre algorithm is then to single out the intended referent against the other members of the context, which act as potential distractors.
</nextsent>
<nextsent>aslong as the domains are such closed-context scenarios, the intended referent is always in the current focus.
</nextsent>
<nextsent>we address the challenge of producing and understanding of references to entities that are outside the current focus of attention, because they have not been mentioned yet and are beyond the currently observable scene.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3996">
<title id=" W10-4224.xml">anchor progression in spatially situated discourse a production experiment </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>in their seminal work dale and reiter (1995) present the incremental algorithm (ia) forgre.
</prevsent>
<prevsent>recent extensions address some of its shortcomings, such as negated and dis joined properties (van deemter, 2002) and an account of salience for generating contextually appropriate shorter res (krahmer and theune, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P99-1017 ">
other, alternative gre algorithms exist (horacek, 1997; <papid> P97-1027 </papid>bateman,1999; <papid> P99-1017 </papid>krahmer et al, 2003).<papid> J03-1003 </papid></citsent>
<aftsection>
<nextsent>however, all these algorithms relyon given domain of discourse constituting the current context (or focus of attention).
</nextsent>
<nextsent>the task of the gre algorithm is then to single out the intended referent against the other members of the context, which act as potential distractors.
</nextsent>
<nextsent>aslong as the domains are such closed-context scenarios, the intended referent is always in the current focus.
</nextsent>
<nextsent>we address the challenge of producing and understanding of references to entities that are outside the current focus of attention, because they have not been mentioned yet and are beyond the currently observable scene.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3997">
<title id=" W10-4224.xml">anchor progression in spatially situated discourse a production experiment </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>in their seminal work dale and reiter (1995) present the incremental algorithm (ia) forgre.
</prevsent>
<prevsent>recent extensions address some of its shortcomings, such as negated and dis joined properties (van deemter, 2002) and an account of salience for generating contextually appropriate shorter res (krahmer and theune, 2002).
</prevsent>
</prevsection>
<citsent citstr=" J03-1003 ">
other, alternative gre algorithms exist (horacek, 1997; <papid> P97-1027 </papid>bateman,1999; <papid> P99-1017 </papid>krahmer et al, 2003).<papid> J03-1003 </papid></citsent>
<aftsection>
<nextsent>however, all these algorithms relyon given domain of discourse constituting the current context (or focus of attention).
</nextsent>
<nextsent>the task of the gre algorithm is then to single out the intended referent against the other members of the context, which act as potential distractors.
</nextsent>
<nextsent>aslong as the domains are such closed-context scenarios, the intended referent is always in the current focus.
</nextsent>
<nextsent>we address the challenge of producing and understanding of references to entities that are outside the current focus of attention, because they have not been mentioned yet and are beyond the currently observable scene.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3998">
<title id=" W10-4224.xml">anchor progression in spatially situated discourse a production experiment </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>details might be missing, and people might be uncertain about particular things and states of affairs that are known to change frequently.
</prevsent>
<prevsent>still, people regularly engage in conversation about such an environment, making successful references to spatially located entities.it is generally assumed that humans adopt partially hierarchical representation of spatial organization (stevens and coupe, 1978; mcnamara, 1986).
</prevsent>
</prevsection>
<citsent citstr=" J07-2004 ">
the basic units of such representation are topological regions (i.e., more or less clearly bounded spatial areas) (hirtle and jonides, 1985).paraboni et al (2007) <papid> J07-2004 </papid>are among the few to address the issue of generating references to entities outside the immediate environment, and present an algorithm for context determination in hierar ...</citsent>
<aftsection>
<nextsent>... office1 office4 office1 floor1 floor2 building 1a building 3b old campus kitchen office2 help desk office3office5 floor1 floor2 floor1 building 2c building 3b new campus dien stag, 14.
</nextsent>
<nextsent>april 2009 (a) example for hierarchical representation of space.(b) illustration of theta principle: starting from the atten tional anchor (a), the smallest sub-hierarchy containing both and the intended referent (r) is formed incrementally.
</nextsent>
<nextsent>figure 1: ta in spatial hierarchy.
</nextsent>
<nextsent>chic ally ordered domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH3999">
<title id=" W10-4224.xml">anchor progression in spatially situated discourse a production experiment </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>large-scale space can be viewed as hierarchically ordered domain.
</prevsent>
<prevsent>to keep track of the referential context in such domain, in our previous work we propose the principle of topological abstraction (ta, summarized in fig.
</prevsent>
</prevsection>
<citsent citstr=" W09-0622 ">
1) for context extension (zender et al, 2009<papid> W09-0622 </papid>a), similar to ancestral search (paraboni et al, 2007).<papid> J07-2004 </papid></citsent>
<aftsection>
<nextsent>in (zender etal., 2009<papid> W09-0622 </papid>b), we describe the integration of the approach in an nlp system for situated human-robot dialogues and present two algorithms instantiating theta principle for gre and resolving referring expressions (rre), respectively.</nextsent>
<nextsent>it relies on two parameters: the location of the intended referent r, and the attentional anchor a. as discussed inour previous works, for single utterances the anchor is the physical position where it is made (i.e., the utterance situation (devlin, 2006)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4002">
<title id=" W10-4224.xml">anchor progression in spatially situated discourse a production experiment </title>
<section> the experiment.  </section>
<citcontext>
<prevsection>
<prevsent>in order to preclude the specific phenomena of collaborative, task-oriented dialogue (cf., e.g., (garrod and pickering, 2004)),the participants had to instruct an imaginary recipient of orders.
</prevsent>
<prevsent>the choice of robot was made torule out potential social implications when imagining, e.g., talking to child, butler, or friend.
</prevsent>
</prevsection>
<citsent citstr=" C04-1096 ">
the sss scenes show birds-eye view of the table including the robots position (similar to (funakoshi et al, 2004)).<papid> C04-1096 </papid></citsent>
<aftsection>
<nextsent>the way the objects are arranged allows to refer to their location with respect to the corners of the table, with plates as additionallandmarks.
</nextsent>
<nextsent>the lss scenes depict an indoor environment with corridor and, parallel to sss, four rooms with tables as landmarks.
</nextsent>
<nextsent>the scenes show table 1: example from the small-scale (12) and large-scale space (34) scenes in fig.
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4003">
<title id=" W10-4224.xml">anchor progression in spatially situated discourse a production experiment </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>an rre component, on the other hand, should be ableto identify as many referents as possible by treating as few as possible res as under-descriptions.
</prevsent>
<prevsent>the analysis of the sss data with respect to establishes the baseline for comparison with other experiments and gre approaches.
</prevsent>
</prevsection>
<citsent citstr=" W06-1410 ">
13.9% of theres contain redundant information (overg), compared to 21% in (viethen and dale, 2006).<papid> W06-1410 </papid></citsent>
<aftsection>
<nextsent>in contrast, however, our sss scenes did not provide the possibility for producing more-than-minimal res for every target object, which might account for the difference.
</nextsent>
<nextsent>underg res occur with frequency of 7.2% in the sss data.
</nextsent>
<nextsent>because under descriptions result in the the hearer being unable to reliably resolve the reference, this means that the robot in our experiment cannot fulfill its task.
</nextsent>
<nextsent>this might explain the difference to the 16% observed in the task-independent study by viethen and dale (2006).<papid> W06-1410 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4005">
<title id=" W11-0507.xml">extractive multi document summaries should explicitly not contain document specific content </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>unsupervised approaches to multi-documentsummarization consist of two steps: finding content model of the documents to be summarized, and then generating summary that best represents the most salient information of the documents.
</prevsent>
<prevsent>in this paper, we present sentence selection objective forex tractive summarization in which sentences are penalized for containing content that is specific to the documents they were extractedfrom.
</prevsent>
</prevsection>
<citsent citstr=" N09-1041 ">
we modify an existing system, hier sum (haghighi &amp; vanderwende, 2009)<papid> N09-1041 </papid>to use our objective, which significantly outperforms the original hiersum in pairwise user evaluation.</citsent>
<aftsection>
<nextsent>additionally, our rouge scores advance the current state-of-the-art for both supervised and unsupervised systems with statistical significance.
</nextsent>
<nextsent>multi-document summarization is the task of generating single summary from set of documents thatare related to single topic.
</nextsent>
<nextsent>summaries should contain information that is relevant to the main ideas ofthe entire document set, and should not contain information that is too specific to any one document.
</nextsent>
<nextsent>for example, summary of multiple news articles about the star wars movies could contain the words lucas and jedi?, but should not contain the name of fan who was interviewed in one article.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4007">
<title id=" W11-0507.xml">extractive multi document summaries should explicitly not contain document specific content </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most approaches to this problem generate summaries ex tract ively, selecting whole or partial sentences fromthe original text, then attempting to piece them together incoherent manner.
</prevsent>
<prevsent>extracted text is selected based on its relevance to the main ideas of the document set.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
summaries can be evaluated manually, or with automatic metrics such as rouge (lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>the use of structured probabilistic topic model shas made it possible to represent document set content with increasing complexity (daume?
</nextsent>
<nextsent>&amp; marcu,2006; tang et al, 2009; celikyilmaz &amp; hakkani tur, 2010).
</nextsent>
<nextsent>haghighi and vanderwende (2009) <papid> N09-1041 </papid>demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models.</nextsent>
<nextsent>their most complex hier archial model improves summary content by teasing out the words that are not general enough to represent the document set as whole.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4017">
<title id=" W11-0507.xml">extractive multi document summaries should explicitly not contain document specific content </title>
<section> kl selection.  </section>
<citcontext>
<prevsection>
<prevsent>hiersum (haghighi &amp; vanderwende, 2009)<papid> N09-1041 </papid>adds more structure to topic sum by further splitting the content distribution into multiple sub-topics.the content words in each sentence can be generated by either the general content topic or the content sub-topic for that sentence, and the words from the general content distribution are considered when building the summary.</prevsent>
<prevsent>1the original bayesum can also be used without query, in which case, bayesum and topic sum are the exact same model.</prevsent>
</prevsection>
<citsent citstr=" N09-2029 ">
the kl-divergence between two unigram word distributions and is given by kl(p ||q) = ? p (w) log (w)q(w) . this quantity is used for summary sentence selection in several systems including lerman and mcdonald (2009) <papid> N09-2029 </papid>and haghighi and vanderwende (2009), <papid> N09-1041 </papid>and was used as feature in the discrimitive sentence ranking of daume?</citsent>
<aftsection>
<nextsent>and marcu (2006).
</nextsent>
<nextsent>topic sum and hiersum use the following klobjective, which finds s?, the summary that minimizes the kl-divergence between the estimated content distribution and the summary word distribution ps: s?
</nextsent>
<nextsent>= min s:|s|l kl(c||ps)a greedy approximation is used to find s?.
</nextsent>
<nextsent>starting with an empty summary, sentences are greedily added to the summary one at time until the summary has reached the maximum word limit, l. the values of ps are smoothed uniformly in order to ensure finite values of kl(c||ps).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4025">
<title id=" W11-0507.xml">extractive multi document summaries should explicitly not contain document specific content </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>7in order to ensure quality results, we asked participants to write sentence on why they selected their preference for each question.
</prevsent>
<prevsent>we also monitored the time taken to complete each comparison.
</prevsent>
</prevsection>
<citsent citstr=" W10-0722 ">
overall, we rejected about 25% of responses we received, which is similar to the percentage of responses rejected by gillick and liu (2010).<papid> W10-0722 </papid></citsent>
<aftsection>
<nextsent>52 information.
</nextsent>
<nextsent>this leaves more room in the summary for content that is relevant to the main idea of the document set (q1) and keeps out content that is not relevant (q4).
</nextsent>
<nextsent>additionally, although neither criterion explicitly addresses coherence, we found that significant proportion of users found our summaries to be more coherent (q3).
</nextsent>
<nextsent>we believe this may be the case because the presence of document-specific information can distract from the main ideas of the summary, and make it less likely that the extracted sentences will flow together.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4026">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we explore two different methods for mapping words to entities, and look at the effect of mapping various subsets of named entity types.
</prevsent>
<prevsent>thus far, results show no improvement in parsing accuracy over the best base line score; we identify possible problems and outline suggestions for future directions.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
techniques for handling lexical data sparsity in parsers have been important ever since the lexicalisation of parsers led to significant improvements in parser performance (collins, 1999; charniak,2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>the original treebank set of non-terminal labels is too general to give good parsing results.
</nextsent>
<nextsent>to overcome this problem, in lexicalised constituency parsers, non-terminals are enriched with lexical information.
</nextsent>
<nextsent>lexicalisation of the grammar vastly increases the number of parameters in the model,spreading the data over more specific events.
</nextsent>
<nextsent>statistics based on low frequency events are not as reliable as statistics on phenomena which occur regularly inthe data; frequency counts involving words are typically sparse.word statistics are also important in more recent un lexicalised approaches to constituency parsing such as latent variable parsing (matsuzaki et al, 2005; <papid> P05-1010 </papid>petrov et al, 2006).<papid> P06-1055 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4027">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to overcome this problem, in lexicalised constituency parsers, non-terminals are enriched with lexical information.
</prevsent>
<prevsent>lexicalisation of the grammar vastly increases the number of parameters in the model,spreading the data over more specific events.
</prevsent>
</prevsection>
<citsent citstr=" P05-1010 ">
statistics based on low frequency events are not as reliable as statistics on phenomena which occur regularly inthe data; frequency counts involving words are typically sparse.word statistics are also important in more recent un lexicalised approaches to constituency parsing such as latent variable parsing (matsuzaki et al, 2005; <papid> P05-1010 </papid>petrov et al, 2006).<papid> P06-1055 </papid></citsent>
<aftsection>
<nextsent>the basic idea of latent variable parsing is that rather than enrich the nonterminal labels by augmenting them with words, aset of enriched labels which can encapsulate the syntactic behaviour of words is automatically learned via an em training mechanism.parsers need to be able to handle both low frequency words and words occurring in the test set which were unseen in the training set (unknown words).
</nextsent>
<nextsent>the problem of rare and unknown words is particularly significant for languages where the size of the treebank is small.
</nextsent>
<nextsent>lexical sparseness is also critical when running parser on data that is in different domain to the domain upon which the parser was trained.
</nextsent>
<nextsent>as interest in parsing real world data increases, parsers ability to adequately handle out of-domain data is critical.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4028">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to overcome this problem, in lexicalised constituency parsers, non-terminals are enriched with lexical information.
</prevsent>
<prevsent>lexicalisation of the grammar vastly increases the number of parameters in the model,spreading the data over more specific events.
</prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
statistics based on low frequency events are not as reliable as statistics on phenomena which occur regularly inthe data; frequency counts involving words are typically sparse.word statistics are also important in more recent un lexicalised approaches to constituency parsing such as latent variable parsing (matsuzaki et al, 2005; <papid> P05-1010 </papid>petrov et al, 2006).<papid> P06-1055 </papid></citsent>
<aftsection>
<nextsent>the basic idea of latent variable parsing is that rather than enrich the nonterminal labels by augmenting them with words, aset of enriched labels which can encapsulate the syntactic behaviour of words is automatically learned via an em training mechanism.parsers need to be able to handle both low frequency words and words occurring in the test set which were unseen in the training set (unknown words).
</nextsent>
<nextsent>the problem of rare and unknown words is particularly significant for languages where the size of the treebank is small.
</nextsent>
<nextsent>lexical sparseness is also critical when running parser on data that is in different domain to the domain upon which the parser was trained.
</nextsent>
<nextsent>as interest in parsing real world data increases, parsers ability to adequately handle out of-domain data is critical.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4029">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>results are presented in section 4, followed by brief discussion in section 5 indicating possible problems and avenues worth pursuing.
</prevsent>
<prevsent>finally, we conclude.
</prevsent>
</prevsection>
<citsent citstr=" N10-1089 ">
much previous work on parsing and multiword units (mwus) adopts the words-with-spaces approach which treats mwus as one token (by concatenat ing the words together) (nivre and nilsson, 2004;cafferkey et al, 2007; korkontzelos and manandhar, 2010).<papid> N10-1089 </papid></citsent>
<aftsection>
<nextsent>alternative approaches are that of fink eland manning (2009) <papid> N09-1037 </papid>on joint parsing and named entity recognition and the work of (wehrli et al, 2010)<papid> W10-3705 </papid>which uses collocation information to rank competing hypotheses in symbolic parser.</nextsent>
<nextsent>also related is work on mwus and grammar engineering, such as (zhang et al, 2006; <papid> W06-1206 </papid>villavicencio et al, 2007) <papid> D07-1110 </papid>where automatically detected mwus are added tothe lexicon of hpsg grammar to improve cover age.our work is most similar to the words-with spaces approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4030">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we conclude.
</prevsent>
<prevsent>much previous work on parsing and multiword units (mwus) adopts the words-with-spaces approach which treats mwus as one token (by concatenat ing the words together) (nivre and nilsson, 2004;cafferkey et al, 2007; korkontzelos and manandhar, 2010).<papid> N10-1089 </papid></prevsent>
</prevsection>
<citsent citstr=" N09-1037 ">
alternative approaches are that of fink eland manning (2009) <papid> N09-1037 </papid>on joint parsing and named entity recognition and the work of (wehrli et al, 2010)<papid> W10-3705 </papid>which uses collocation information to rank competing hypotheses in symbolic parser.</citsent>
<aftsection>
<nextsent>also related is work on mwus and grammar engineering, such as (zhang et al, 2006; <papid> W06-1206 </papid>villavicencio et al, 2007) <papid> D07-1110 </papid>where automatically detected mwus are added tothe lexicon of hpsg grammar to improve cover age.our work is most similar to the words-with spaces approach.</nextsent>
<nextsent>our many-to-one experiments (see 3.1) in particular are similar to previous work on parsing words-with-spaces, except that we map words to entity types rather than concatenated words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4031">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we conclude.
</prevsent>
<prevsent>much previous work on parsing and multiword units (mwus) adopts the words-with-spaces approach which treats mwus as one token (by concatenat ing the words together) (nivre and nilsson, 2004;cafferkey et al, 2007; korkontzelos and manandhar, 2010).<papid> N10-1089 </papid></prevsent>
</prevsection>
<citsent citstr=" W10-3705 ">
alternative approaches are that of fink eland manning (2009) <papid> N09-1037 </papid>on joint parsing and named entity recognition and the work of (wehrli et al, 2010)<papid> W10-3705 </papid>which uses collocation information to rank competing hypotheses in symbolic parser.</citsent>
<aftsection>
<nextsent>also related is work on mwus and grammar engineering, such as (zhang et al, 2006; <papid> W06-1206 </papid>villavicencio et al, 2007) <papid> D07-1110 </papid>where automatically detected mwus are added tothe lexicon of hpsg grammar to improve cover age.our work is most similar to the words-with spaces approach.</nextsent>
<nextsent>our many-to-one experiments (see 3.1) in particular are similar to previous work on parsing words-with-spaces, except that we map words to entity types rather than concatenated words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4032">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>much previous work on parsing and multiword units (mwus) adopts the words-with-spaces approach which treats mwus as one token (by concatenat ing the words together) (nivre and nilsson, 2004;cafferkey et al, 2007; korkontzelos and manandhar, 2010).<papid> N10-1089 </papid></prevsent>
<prevsent>alternative approaches are that of fink eland manning (2009) <papid> N09-1037 </papid>on joint parsing and named entity recognition and the work of (wehrli et al, 2010)<papid> W10-3705 </papid>which uses collocation information to rank competing hypotheses in symbolic parser.</prevsent>
</prevsection>
<citsent citstr=" W06-1206 ">
also related is work on mwus and grammar engineering, such as (zhang et al, 2006; <papid> W06-1206 </papid>villavicencio et al, 2007) <papid> D07-1110 </papid>where automatically detected mwus are added tothe lexicon of hpsg grammar to improve cover age.our work is most similar to the words-with spaces approach.</citsent>
<aftsection>
<nextsent>our many-to-one experiments (see 3.1) in particular are similar to previous work on parsing words-with-spaces, except that we map words to entity types rather than concatenated words.
</nextsent>
<nextsent>results are difficult to compare however, due to different parsing methodologies, different types of mwus, as well as different evaluation methods.
</nextsent>
<nextsent>other relevant work is the integration of named 1it is true that latent variable parsers automatically induce categories for similar words, and thus might be expected to induce category for say names of people if examples of such words occurred in similar syntactic patterns in the data.nonetheless, the problem of data sparsity remains - it is difficult even for latent variable parsers to learn accurate patterns based on words which only occur say once in the training set.entity types in surface realisation task by rajkumar et al (2009) <papid> N09-2041 </papid>and the french parsing experiments of (candito and crabbe?, 2009; candito and sed dah, 2010) which involve mapping words to clusters based on morphology as well as clusters automatically induced via unsupervised learning on large corpus.</nextsent>
<nextsent>2.1 parsing unknown words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4033">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>much previous work on parsing and multiword units (mwus) adopts the words-with-spaces approach which treats mwus as one token (by concatenat ing the words together) (nivre and nilsson, 2004;cafferkey et al, 2007; korkontzelos and manandhar, 2010).<papid> N10-1089 </papid></prevsent>
<prevsent>alternative approaches are that of fink eland manning (2009) <papid> N09-1037 </papid>on joint parsing and named entity recognition and the work of (wehrli et al, 2010)<papid> W10-3705 </papid>which uses collocation information to rank competing hypotheses in symbolic parser.</prevsent>
</prevsection>
<citsent citstr=" D07-1110 ">
also related is work on mwus and grammar engineering, such as (zhang et al, 2006; <papid> W06-1206 </papid>villavicencio et al, 2007) <papid> D07-1110 </papid>where automatically detected mwus are added tothe lexicon of hpsg grammar to improve cover age.our work is most similar to the words-with spaces approach.</citsent>
<aftsection>
<nextsent>our many-to-one experiments (see 3.1) in particular are similar to previous work on parsing words-with-spaces, except that we map words to entity types rather than concatenated words.
</nextsent>
<nextsent>results are difficult to compare however, due to different parsing methodologies, different types of mwus, as well as different evaluation methods.
</nextsent>
<nextsent>other relevant work is the integration of named 1it is true that latent variable parsers automatically induce categories for similar words, and thus might be expected to induce category for say names of people if examples of such words occurred in similar syntactic patterns in the data.nonetheless, the problem of data sparsity remains - it is difficult even for latent variable parsers to learn accurate patterns based on words which only occur say once in the training set.entity types in surface realisation task by rajkumar et al (2009) <papid> N09-2041 </papid>and the french parsing experiments of (candito and crabbe?, 2009; candito and sed dah, 2010) which involve mapping words to clusters based on morphology as well as clusters automatically induced via unsupervised learning on large corpus.</nextsent>
<nextsent>2.1 parsing unknown words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4034">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our many-to-one experiments (see 3.1) in particular are similar to previous work on parsing words-with-spaces, except that we map words to entity types rather than concatenated words.
</prevsent>
<prevsent>results are difficult to compare however, due to different parsing methodologies, different types of mwus, as well as different evaluation methods.
</prevsent>
</prevsection>
<citsent citstr=" N09-2041 ">
other relevant work is the integration of named 1it is true that latent variable parsers automatically induce categories for similar words, and thus might be expected to induce category for say names of people if examples of such words occurred in similar syntactic patterns in the data.nonetheless, the problem of data sparsity remains - it is difficult even for latent variable parsers to learn accurate patterns based on words which only occur say once in the training set.entity types in surface realisation task by rajkumar et al (2009) <papid> N09-2041 </papid>and the french parsing experiments of (candito and crabbe?, 2009; candito and sed dah, 2010) which involve mapping words to clusters based on morphology as well as clusters automatically induced via unsupervised learning on large corpus.</citsent>
<aftsection>
<nextsent>2.1 parsing unknown words.
</nextsent>
<nextsent>most state-of-the-art constituency parsers (e.g.
</nextsent>
<nextsent>(petrov et al, 2006; <papid> P06-1055 </papid>klein and manning, 2003)) <papid> P03-1054 </papid>take similar approach to rare and unknown words.</nextsent>
<nextsent>at the beginning of the training process very low frequency words in the training set are mapped to special unknown tokens.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4036">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 parsing unknown words.
</prevsent>
<prevsent>most state-of-the-art constituency parsers (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
(petrov et al, 2006; <papid> P06-1055 </papid>klein and manning, 2003)) <papid> P03-1054 </papid>take similar approach to rare and unknown words.</citsent>
<aftsection>
<nextsent>at the beginning of the training process very low frequency words in the training set are mapped to special unknown tokens.
</nextsent>
<nextsent>in this way, some probability mass is reserved for occurrences of unknown tokens and the lexicon contains productions for such tokens (x ? unknown), with associated probabilities.
</nextsent>
<nextsent>when faced with word inthe test set that the parser has not seen in its training set - the unknown word is mapped to the special unknown token.in syntactic parsing, rather than map all low frequency words to one generic unknown type, itis useful to have several different clusters of unknown words, grouped according to morphological and other surfacey?
</nextsent>
<nextsent>clues in the original word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4037">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as well as suffix information, unknown words are commonly grouped based on information on capitalisation and hyphenation.
</prevsent>
<prevsent>similar techniques for handling unknown words have been used for pos tagging (e.g.
</prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
(weischedel et al, 1993; <papid> J93-2006 </papid>tseng etal., 2005)) <papid> I05-3005 </papid>and are used in the charniak (charniak, 2000), <papid> A00-2018 </papid>berkeley (petrov et al, 2006) <papid> P06-1055 </papid>and stanford (klein and manning, 2003) <papid> P03-1054 </papid>parsers, as well as in the parser used for the experiments in this paper, an in-house implementation of the berkeley parser.</citsent>
<aftsection>
<nextsent>the bbn entity type corpus (weischedel and brun stein, 2005) consists of sentences from the penn wsj corpus, manually annotated with namedentities.
</nextsent>
<nextsent>the entity type corpus includes annota 15 type count examples person 11254 kim cattrall per desc 21451 president,chief executive officer, fac 383 office, rockefeller center fac desc 2193 chateau ,stadiums, golf course organization 24239 securities and exchange commission org desc 15765 automaker, college gpe 10323 los angeles,south africa gpe desc 1479 center, nation, country location 907 north america,europe, hudson river norp 3269 far eastern product 667 maxima, 300zx product desc 1156 cars event 296 vietnam war,hugo ,world war ii work of art 561 revitalized classics take..
</nextsent>
<nextsent>law 300 catastrophic care act,bill of rights language 62 latin contact info 30 555 w. 57th st. plant 172 crops, tree animal 355 hawks substance 2205 gold,drugs, oil disease 254 schizophrenia,alcoholism game 74 football senior tennis and golf tours table 1: name expression entity types (sections 02-21)tion for three classes of named entity: name expressions, time expressions and numeric expressions (in this paper we focus on name expressions).
</nextsent>
<nextsent>these are further broken down into types.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4038">
<title id=" W11-0804.xml">decreasing lexical data sparsity in statistical syntactic parsing  experiments with named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as well as suffix information, unknown words are commonly grouped based on information on capitalisation and hyphenation.
</prevsent>
<prevsent>similar techniques for handling unknown words have been used for pos tagging (e.g.
</prevsent>
</prevsection>
<citsent citstr=" I05-3005 ">
(weischedel et al, 1993; <papid> J93-2006 </papid>tseng etal., 2005)) <papid> I05-3005 </papid>and are used in the charniak (charniak, 2000), <papid> A00-2018 </papid>berkeley (petrov et al, 2006) <papid> P06-1055 </papid>and stanford (klein and manning, 2003) <papid> P03-1054 </papid>parsers, as well as in the parser used for the experiments in this paper, an in-house implementation of the berkeley parser.</citsent>
<aftsection>
<nextsent>the bbn entity type corpus (weischedel and brun stein, 2005) consists of sentences from the penn wsj corpus, manually annotated with namedentities.
</nextsent>
<nextsent>the entity type corpus includes annota 15 type count examples person 11254 kim cattrall per desc 21451 president,chief executive officer, fac 383 office, rockefeller center fac desc 2193 chateau ,stadiums, golf course organization 24239 securities and exchange commission org desc 15765 automaker, college gpe 10323 los angeles,south africa gpe desc 1479 center, nation, country location 907 north america,europe, hudson river norp 3269 far eastern product 667 maxima, 300zx product desc 1156 cars event 296 vietnam war,hugo ,world war ii work of art 561 revitalized classics take..
</nextsent>
<nextsent>law 300 catastrophic care act,bill of rights language 62 latin contact info 30 555 w. 57th st. plant 172 crops, tree animal 355 hawks substance 2205 gold,drugs, oil disease 254 schizophrenia,alcoholism game 74 football senior tennis and golf tours table 1: name expression entity types (sections 02-21)tion for three classes of named entity: name expressions, time expressions and numeric expressions (in this paper we focus on name expressions).
</nextsent>
<nextsent>these are further broken down into types.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4042">
<title id=" W10-4235.xml">generation under uncertainty </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the problem here is to generate output that takes these types of uncertainty into account appropriately.
</prevsent>
<prevsent>for example, you may need to choose referring expression for user, even though you are not sure whether they are an expert or novice in the domain.
</prevsent>
</prevsection>
<citsent citstr=" P10-1008 ">
in addition, the next time you speakto that user, you need to adapt to new information you have gained about them (janarthanam and lemon, 2010).<papid> P10-1008 </papid></citsent>
<aftsection>
<nextsent>the issue of uncertainty for referring expression generation has been discussed before by (reiter, 1991; horacek, 2005).
</nextsent>
<nextsent>another example is in planning an information presentation for user, when you cannot know with certainty how they will respond to it (rieser and lemon, 2009; <papid> E09-1078 </papid>rieser et al, 2010).<papid> P10-1103 </papid></nextsent>
<nextsent>in the worst case, you may even be uncertain about the users goals or information needs (as in pomdp?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4043">
<title id=" W10-4235.xml">generation under uncertainty </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, the next time you speakto that user, you need to adapt to new information you have gained about them (janarthanam and lemon, 2010).<papid> P10-1008 </papid></prevsent>
<prevsent>the issue of uncertainty for referring expression generation has been discussed before by (reiter, 1991; horacek, 2005).</prevsent>
</prevsection>
<citsent citstr=" E09-1078 ">
another example is in planning an information presentation for user, when you cannot know with certainty how they will respond to it (rieser and lemon, 2009; <papid> E09-1078 </papid>rieser et al, 2010).<papid> P10-1103 </papid></citsent>
<aftsection>
<nextsent>in the worst case, you may even be uncertain about the users goals or information needs (as in pomdp?
</nextsent>
<nextsent>approaches to dialogue management (young et al, 2009; henderson and lemon, 2008<papid> P08-2019 </papid>a)), but you still need to generate output for them in an appropriate way.</nextsent>
<nextsent>in particular, in interactive applications of nlg: ? each nlg action changes the environment state or context, ? the effect of each nlg action is uncertain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4044">
<title id=" W10-4235.xml">generation under uncertainty </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, the next time you speakto that user, you need to adapt to new information you have gained about them (janarthanam and lemon, 2010).<papid> P10-1008 </papid></prevsent>
<prevsent>the issue of uncertainty for referring expression generation has been discussed before by (reiter, 1991; horacek, 2005).</prevsent>
</prevsection>
<citsent citstr=" P10-1103 ">
another example is in planning an information presentation for user, when you cannot know with certainty how they will respond to it (rieser and lemon, 2009; <papid> E09-1078 </papid>rieser et al, 2010).<papid> P10-1103 </papid></citsent>
<aftsection>
<nextsent>in the worst case, you may even be uncertain about the users goals or information needs (as in pomdp?
</nextsent>
<nextsent>approaches to dialogue management (young et al, 2009; henderson and lemon, 2008<papid> P08-2019 </papid>a)), but you still need to generate output for them in an appropriate way.</nextsent>
<nextsent>in particular, in interactive applications of nlg: ? each nlg action changes the environment state or context, ? the effect of each nlg action is uncertain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4046">
<title id=" W10-4235.xml">generation under uncertainty </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another example is in planning an information presentation for user, when you cannot know with certainty how they will respond to it (rieser and lemon, 2009; <papid> E09-1078 </papid>rieser et al, 2010).<papid> P10-1103 </papid></prevsent>
<prevsent>in the worst case, you may even be uncertain about the users goals or information needs (as in pomdp?</prevsent>
</prevsection>
<citsent citstr=" P08-2019 ">
approaches to dialogue management (young et al, 2009; henderson and lemon, 2008<papid> P08-2019 </papid>a)), but you still need to generate output for them in an appropriate way.</citsent>
<aftsection>
<nextsent>in particular, in interactive applications of nlg: ? each nlg action changes the environment state or context, ? the effect of each nlg action is uncertain.
</nextsent>
<nextsent>several recent approaches describe nlg tasksas different kinds of planning, e.g.
</nextsent>
<nextsent>(koller and petrick, 2008; rieser et al, 2010; <papid> P10-1103 </papid>janarthanam and lemon, 2010), <papid> P10-1008 </papid>or as contextual decisionmaking according to cost function (van deemter, 2009).it would be very interesting to explore how different approaches perform in nlg problems where different types of uncertainty are present in the generation environment.in the following we discuss possible generation challenges arising from such considerations, which we hope will lead to work on an agreed shared challenge in this research community.</nextsent>
<nextsent>in section 2 we briefly review recent work showing that simulated environments can be used to evaluate generation under uncertainty, and in section 3 we discuss some possible metrics for such tasks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4052">
<title id=" W10-4235.xml">generation under uncertainty </title>
<section> generation in uncertain simulated.  </section>
<citcontext>
<prevsection>
<prevsent>reactions to the systems ip structure decisions (p (au,t|ips,t)), and tri-gram (i.e. ip structure + attribute choice) model for predicting user reactions to the systems combined ip structure and attribute selection deci sions: (au,t|ips,t, attributess,t).
</prevsent>
<prevsent>we have evaluated the performance of these models by measuring dialogue similarity to the original data, based on the kullback-leibler (kl) divergence, as also used by e.g.
</prevsent>
</prevsection>
<citsent citstr=" W09-3916 ">
(cuayahuitl et al, 2005; jung et al, 2009; janarthanam and lemon,2009).<papid> W09-3916 </papid></citsent>
<aftsection>
<nextsent>we compared the raw probabilities as observed in the data with the probabilities generated by our n-gram models using different discounting techniques for each context.
</nextsent>
<nextsent>all the models have small divergence from the original data (especiallythe bi-gram model), suggesting that they are reasonable simulations for training and testing nlg policies (rieser et al, 2010).<papid> P10-1103 </papid></nextsent>
<nextsent>2.2 other simulated components.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4055">
<title id=" W10-4235.xml">generation under uncertainty </title>
<section> generation in uncertain simulated.  </section>
<citcontext>
<prevsection>
<prevsent>building simulations for these components to capture their inherent uncertainty, again, is an interesting challenge.for example, one might want to adapt the generated output according to the predicted tts quality.
</prevsent>
<prevsent>therefore, one needs model of the expected/ predicted tts quality for tts engine (boidin et al., 2009).
</prevsent>
</prevsection>
<citsent citstr=" P04-1011 ">
furthermore, nlg decisions might be inputs to stochastic sentence realiser, such as sparky (stent et al, 2004).<papid> P04-1011 </papid></citsent>
<aftsection>
<nextsent>however, one might not have fully trained stochastic sentence realiser for this domain (yet).
</nextsent>
<nextsent>in (rieser et al, 2010) <papid> P10-1103 </papid>we therefore modelled the variance as observed in the top ranking sparky examples.</nextsent>
<nextsent>2.3 generating referring expressions under.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4061">
<title id=" W11-0204.xml">evex a pubmedscale resource for homology based generalization of text mining predictions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, many known genotype-phenotype links are still buried in research articles: the largest biomolecular literature database, pubmed, consists of more than 20 million citations1.
</prevsent>
<prevsent>due to its exponential growth, automated tools have become necessity to uncover all relevant information.
</prevsent>
</prevsection>
<citsent citstr=" P06-4005 ">
there exist several text mining efforts focusing on pairwise interactions and co-occurrence links of genes and proteins (hoffmann and valencia, 2004; ohta et al, 2006; <papid> P06-4005 </papid>szklarczyk et al, 2011).</citsent>
<aftsection>
<nextsent>inthis paper, we present the first large-scale text mining resource which both utilizes detailed event based representation of biological statements and provides homology-based generalization of the text mining predictions.
</nextsent>
<nextsent>this resource results from the integration of text mining predictions from nearly 18m pubmed citations with records from public gene databases (section 2).
</nextsent>
<nextsent>to enable such integration, it is crucial to first produce canonical formsof the automatically tagged biological entities (sec tion 3.1).
</nextsent>
<nextsent>a gene symbol disambiguation algorithm then links these canonical forms to gene families andgene identifiers (section 3.2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4062">
<title id=" W11-0204.xml">evex a pubmedscale resource for homology based generalization of text mining predictions </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>bjorne et al (2010) have applied to all pubmed abstracts an event extraction pipeline comprising of the banner named entity recognizer (leaman and gonzalez, 2008) and the turku event extraction system (bjorne et al, 2009).
</prevsent>
<prevsent>the resulting dataset contains 36.5m occurrences of gene / gene product(ggp) entities and 19.2m occurrences of events pertaining to these entities.
</prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
the file format and information scheme of the resource correspond to the definition of the bionlp09 shared task on event extraction (kim et al, 2009).<papid> W09-1401 </papid></citsent>
<aftsection>
<nextsent>events are defined as typed relations between arguments that are either entity occurrence sor, recursively, other events.
</nextsent>
<nextsent>there are nine possible event types: localization, binding, gene expression, transcription, protein catabolism, phosphory lation, regulation, positive regulation, and negative regulation.
</nextsent>
<nextsent>further, arguments are assigned role: theme or cause for the core arguments and atloc, toloc, site, and csite for auxiliary arguments that define additional information such as cellular location of the event.
</nextsent>
<nextsent>in addition, each event occurrence may be marked as negative and/or speculative.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4064">
<title id=" W11-0204.xml">evex a pubmedscale resource for homology based generalization of text mining predictions </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>secondly, all official gene names are extracted from entrez gene.
</prevsent>
<prevsent>as this latter list also contains common english words such as was and protein, we have only selected those that were likely to be standalone gene symbols.
</prevsent>
</prevsection>
<citsent citstr=" C10-1096 ">
we calculate this likelihood by cs/(cs + cn) where cs 5all string matching steps have been implemented using the sim string string retrieval library (okazaki and tsujii, 2010).<papid> C10-1096 </papid></citsent>
<aftsection>
<nextsent>ggp contexts -org- -ggp- gene -ggp- sequences mutant -ggp- proteins -ggp- homologscytoplasmic wild-type -ggptable 1: this table lists few examples of entity occurrences extracted with banner that are resolved to the embedded minimal gene symbol (marked as -ggp-).
</nextsent>
<nextsent>is the number of times string is tagged standalone and cn is the number of times the string occurs in pubmed but is not tagged (neither as standalone,nor as part of larger entity).
</nextsent>
<nextsent>this likelihood represents the proportion of standalone occurrences of the string that are tagged.
</nextsent>
<nextsent>we experimentally set athreshold on this value to be higher than 0.01, excluding list of 2,865 common english words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4067">
<title id=" W11-1514.xml">from once upon a time to happily ever after tracking emotions in novels and fairy tales </title>
<section> social analysis: identifying how books have.  </section>
<citcontext>
<prevsection>
<prevsent>given search query, the goal is to provide users with relevant plots presented in this paper, as well as ability to search for text snippets from multiple texts that have high emotion word densities.
</prevsent>
<prevsent>2 related work.
</prevsent>
</prevsection>
<citsent citstr=" D09-1063 ">
over the last decade, there has been considerable work in sentiment analysis, especially in determining whether term has positive or negative polarity (lehrer, 1974; turney and littman, 2003; mohammad et al, 2009).<papid> D09-1063 </papid></citsent>
<aftsection>
<nextsent>there is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (bellegarda, 2010; <papid> W10-0201 </papid>mohammad and turney, 2010; <papid> W10-0204 </papid>alm et al, 2005; <papid> H05-1073 </papid>alm et al, 2005).<papid> H05-1073 </papid></nextsent>
<nextsent>the technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (dodds and danforth, 2010; pang and lee, 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4068">
<title id=" W11-1514.xml">from once upon a time to happily ever after tracking emotions in novels and fairy tales </title>
<section> social analysis: identifying how books have.  </section>
<citcontext>
<prevsection>
<prevsent>2 related work.
</prevsent>
<prevsent>over the last decade, there has been considerable work in sentiment analysis, especially in determining whether term has positive or negative polarity (lehrer, 1974; turney and littman, 2003; mohammad et al, 2009).<papid> D09-1063 </papid></prevsent>
</prevsection>
<citsent citstr=" W10-0201 ">
there is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (bellegarda, 2010; <papid> W10-0201 </papid>mohammad and turney, 2010; <papid> W10-0204 </papid>alm et al, 2005; <papid> H05-1073 </papid>alm et al, 2005).<papid> H05-1073 </papid></citsent>
<aftsection>
<nextsent>the technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (dodds and danforth, 2010; pang and lee, 2008).
</nextsent>
<nextsent>automatic analysis of emotions in text has so far had to relyon small emotion lexicons.
</nextsent>
<nextsent>the wordnet affect lexicon (wal) (strapparava and valitutti,2004) has few hundred words annotated with associations to number of affect categories including the six ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 general inquirer (gi) (stoneet al, 1966) has 11,788 words labeled with 182 categories of word tags, including positive and negativepolarity.4 we use the nrc emotion lexicon (mo hammad and yang, 2011; mohammad and turney, 2010), <papid> W10-0204 </papid>large set of human-provided word emotion association ratings, in our experiments.5 empirical assessment of emotions in literary texts has sometimes relied on human annotation of the texts, but this has restricted the number of texts analyzed.</nextsent>
<nextsent>for example, alm and sproat (2005) annotated 22 brothers grimm fairy tales to show that fairy tales often began with neutral sentence and ended with happy sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4069">
<title id=" W11-1514.xml">from once upon a time to happily ever after tracking emotions in novels and fairy tales </title>
<section> social analysis: identifying how books have.  </section>
<citcontext>
<prevsection>
<prevsent>2 related work.
</prevsent>
<prevsent>over the last decade, there has been considerable work in sentiment analysis, especially in determining whether term has positive or negative polarity (lehrer, 1974; turney and littman, 2003; mohammad et al, 2009).<papid> D09-1063 </papid></prevsent>
</prevsection>
<citsent citstr=" W10-0204 ">
there is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (bellegarda, 2010; <papid> W10-0201 </papid>mohammad and turney, 2010; <papid> W10-0204 </papid>alm et al, 2005; <papid> H05-1073 </papid>alm et al, 2005).<papid> H05-1073 </papid></citsent>
<aftsection>
<nextsent>the technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (dodds and danforth, 2010; pang and lee, 2008).
</nextsent>
<nextsent>automatic analysis of emotions in text has so far had to relyon small emotion lexicons.
</nextsent>
<nextsent>the wordnet affect lexicon (wal) (strapparava and valitutti,2004) has few hundred words annotated with associations to number of affect categories including the six ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 general inquirer (gi) (stoneet al, 1966) has 11,788 words labeled with 182 categories of word tags, including positive and negativepolarity.4 we use the nrc emotion lexicon (mo hammad and yang, 2011; mohammad and turney, 2010), <papid> W10-0204 </papid>large set of human-provided word emotion association ratings, in our experiments.5 empirical assessment of emotions in literary texts has sometimes relied on human annotation of the texts, but this has restricted the number of texts analyzed.</nextsent>
<nextsent>for example, alm and sproat (2005) annotated 22 brothers grimm fairy tales to show that fairy tales often began with neutral sentence and ended with happy sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4070">
<title id=" W11-1514.xml">from once upon a time to happily ever after tracking emotions in novels and fairy tales </title>
<section> social analysis: identifying how books have.  </section>
<citcontext>
<prevsection>
<prevsent>2 related work.
</prevsent>
<prevsent>over the last decade, there has been considerable work in sentiment analysis, especially in determining whether term has positive or negative polarity (lehrer, 1974; turney and littman, 2003; mohammad et al, 2009).<papid> D09-1063 </papid></prevsent>
</prevsection>
<citsent citstr=" H05-1073 ">
there is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (bellegarda, 2010; <papid> W10-0201 </papid>mohammad and turney, 2010; <papid> W10-0204 </papid>alm et al, 2005; <papid> H05-1073 </papid>alm et al, 2005).<papid> H05-1073 </papid></citsent>
<aftsection>
<nextsent>the technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (dodds and danforth, 2010; pang and lee, 2008).
</nextsent>
<nextsent>automatic analysis of emotions in text has so far had to relyon small emotion lexicons.
</nextsent>
<nextsent>the wordnet affect lexicon (wal) (strapparava and valitutti,2004) has few hundred words annotated with associations to number of affect categories including the six ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 general inquirer (gi) (stoneet al, 1966) has 11,788 words labeled with 182 categories of word tags, including positive and negativepolarity.4 we use the nrc emotion lexicon (mo hammad and yang, 2011; mohammad and turney, 2010), <papid> W10-0204 </papid>large set of human-provided word emotion association ratings, in our experiments.5 empirical assessment of emotions in literary texts has sometimes relied on human annotation of the texts, but this has restricted the number of texts analyzed.</nextsent>
<nextsent>for example, alm and sproat (2005) annotated 22 brothers grimm fairy tales to show that fairy tales often began with neutral sentence and ended with happy sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4074">
<title id=" W11-1514.xml">from once upon a time to happily ever after tracking emotions in novels and fairy tales </title>
<section> emotion analysis.  </section>
<citcontext>
<prevsection>
<prevsent>the 1911 roget thesaurus was used as the source for target terms.6 only those thesaurus words that occurred more than 120,000 times in the google gram corpus were annotated for version 0.92 of the lexicon which we use for the experiments described in this paper.7 the rogets thesaurus groups related words into about thousand categories, which can be thought of 3wal: http://wndomains.fbk.eu/wnaffect.html 4gi: http://www.wjh.harvard.edu/inquirer 5please send an e-mail to saif.mohammad@nrc-cnrc.gc.ca to obtain the latest version of the nrc emotion lexicon.
</prevsent>
<prevsent>6rogets thesaurus: www.gutenberg.org/ebooks/106817the google n-gram corpus is available through the linguistic data consortium.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
106 as coarse senses or concepts (yarowsky, 1992).<papid> C92-2070 </papid></citsent>
<aftsection>
<nextsent>if word is ambiguous, then it is listed in more than one category.
</nextsent>
<nextsent>since word may have different emotion associations when used in different senses, word sense level annotations were obtained by first asking an automatically generated word-choice question pertaining to the target: q1.
</nextsent>
<nextsent>which word is closest in meaning to shark (target)?
</nextsent>
<nextsent>car ? tree ? fish ? olive the near-synonym for q1 is taken from the thesaurus, and the dis tractors are randomly chosenwords.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4076">
<title id=" W11-1514.xml">from once upon a time to happily ever after tracking emotions in novels and fairy tales </title>
<section> visualizations of emotions.  </section>
<citcontext>
<prevsection>
<prevsent>emo 107 figure 4: hamlet - as you like it: relative-salience word cloud for trust words.
</prevsent>
<prevsent>figure 5: hamlet - as you like it: relative-salience word cloud for sadness words.
</prevsent>
</prevsection>
<citsent citstr=" P11-2064 ">
tions are represented by colours as per study on word colour associations (mohammad, 2011).<papid> P11-2064 </papid></citsent>
<aftsection>
<nextsent>observe how one can clearly see that hamlet has more fear, sadness, disgust, and anger, and less joy, trust, and anticipation.
</nextsent>
<nextsent>the bar graph is effective at conveying the extent to which an emotion is more prominent in one text than another, but it does not convey the source of these emotions.
</nextsent>
<nextsent>therefore, we calculate the relative salience of an emotion word across two target texts t1 and t2: relativesalience(w|t1, t2) = f1 n1 ? f2 n2 (1) where, f1 and f2 are the frequencies of in t1 and t2, respectively.
</nextsent>
<nextsent>n1 and n2 are the total number of word tokens in t1 and t2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4077">
<title id=" W10-4136.xml">chinese word segmentation model using bootstrapping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>xues approach has been become the most popular approach to chinese word segmentation for its high performance and unified way to deal with oov issues.
</prevsent>
<prevsent>most of the segmentation works since then follow this approach.
</prevsent>
</prevsection>
<citsent citstr=" I05-3027 ">
major improvements in this line of research including: 1) more sophisticated learning models were introduced instead of the maximum entropy model that xue used, like conditional random fields (crfs) model which fit the sequence tagging tasks much better than maximum entropy model (tseng et al,2005).<papid> I05-3027 </papid></citsent>
<aftsection>
<nextsent>2) more tags were introduced, as zhao et al (2006) shows 6 tags are superior to 4 tags in achieving high performance.
</nextsent>
<nextsent>3) new feature templates were added, such as templates used in representing numbers, dates, letters etc.
</nextsent>
<nextsent>(low et al, 2005) <papid> I05-3025 </papid>usually, the performance of segmentation model is evaluated on test set from the same domain as the training set.</nextsent>
<nextsent>such evaluation does not reveal its ability to deal with domain variation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4078">
<title id=" W10-4136.xml">chinese word segmentation model using bootstrapping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>2) more tags were introduced, as zhao et al (2006) shows 6 tags are superior to 4 tags in achieving high performance.
</prevsent>
<prevsent>3) new feature templates were added, such as templates used in representing numbers, dates, letters etc.
</prevsent>
</prevsection>
<citsent citstr=" I05-3025 ">
(low et al, 2005) <papid> I05-3025 </papid>usually, the performance of segmentation model is evaluated on test set from the same domain as the training set.</citsent>
<aftsection>
<nextsent>such evaluation does not reveal its ability to deal with domain variation.
</nextsent>
<nextsent>it is believed that, when test set is from other domains than the domain where training set is from, the learned model normally underperforms substantially.
</nextsent>
<nextsent>the cips-sighan-2010 bake-off task of chinese word segmentation is set to focus on the cross-domain performance of chinese word segmentation model.
</nextsent>
<nextsent>we participate in the closed test track for simplified chinese.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4079">
<title id=" W11-1106.xml">grawltcq terminology and corpora building by ranking simultaneously terms queries and documents using graph random walks </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and improve the systems overall performance.
</prevsent>
<prevsent>the manual filtering cost is therefore drastically reduced.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
graph modeling and random walks have been applied with success to many different domains of nlp, such as keyword and sentence extraction(mihalcea and tarau, 2004), <papid> W04-3252 </papid>computer-science articles ranking (nie et al, 2005), web pages ranking (haveliwala, 2002; page et al, 1999; richardson and domingos, 2002), wordnet-based word sense disambiguation (agirre and soroa, 2009) <papid> E09-1005 </papid>and lexical semantic relatedness (hughes and ramage, 2007), <papid> D07-1061 </papid>or set expansion (wang and cohen, 2007).</citsent>
<aftsection>
<nextsent>in this paper, we confirm the relevance of this approach to terminology and corpora bootstrapping.
</nextsent>
<nextsent>and documents 3.1 the grawltcq bootstrapping algorithm.
</nextsent>
<nextsent>figure 1 shows the components of the grawltcq algorithm.
</nextsent>
<nextsent>starting from user provided seed terms2,grawltcq iteratively creates queries, finds documents and extracts new terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4081">
<title id=" W11-1106.xml">grawltcq terminology and corpora building by ranking simultaneously terms queries and documents using graph random walks </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and improve the systems overall performance.
</prevsent>
<prevsent>the manual filtering cost is therefore drastically reduced.
</prevsent>
</prevsection>
<citsent citstr=" E09-1005 ">
graph modeling and random walks have been applied with success to many different domains of nlp, such as keyword and sentence extraction(mihalcea and tarau, 2004), <papid> W04-3252 </papid>computer-science articles ranking (nie et al, 2005), web pages ranking (haveliwala, 2002; page et al, 1999; richardson and domingos, 2002), wordnet-based word sense disambiguation (agirre and soroa, 2009) <papid> E09-1005 </papid>and lexical semantic relatedness (hughes and ramage, 2007), <papid> D07-1061 </papid>or set expansion (wang and cohen, 2007).</citsent>
<aftsection>
<nextsent>in this paper, we confirm the relevance of this approach to terminology and corpora bootstrapping.
</nextsent>
<nextsent>and documents 3.1 the grawltcq bootstrapping algorithm.
</nextsent>
<nextsent>figure 1 shows the components of the grawltcq algorithm.
</nextsent>
<nextsent>starting from user provided seed terms2,grawltcq iteratively creates queries, finds documents and extracts new terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4082">
<title id=" W11-1106.xml">grawltcq terminology and corpora building by ranking simultaneously terms queries and documents using graph random walks </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and improve the systems overall performance.
</prevsent>
<prevsent>the manual filtering cost is therefore drastically reduced.
</prevsent>
</prevsection>
<citsent citstr=" D07-1061 ">
graph modeling and random walks have been applied with success to many different domains of nlp, such as keyword and sentence extraction(mihalcea and tarau, 2004), <papid> W04-3252 </papid>computer-science articles ranking (nie et al, 2005), web pages ranking (haveliwala, 2002; page et al, 1999; richardson and domingos, 2002), wordnet-based word sense disambiguation (agirre and soroa, 2009) <papid> E09-1005 </papid>and lexical semantic relatedness (hughes and ramage, 2007), <papid> D07-1061 </papid>or set expansion (wang and cohen, 2007).</citsent>
<aftsection>
<nextsent>in this paper, we confirm the relevance of this approach to terminology and corpora bootstrapping.
</nextsent>
<nextsent>and documents 3.1 the grawltcq bootstrapping algorithm.
</nextsent>
<nextsent>figure 1 shows the components of the grawltcq algorithm.
</nextsent>
<nextsent>starting from user provided seed terms2,grawltcq iteratively creates queries, finds documents and extracts new terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4085">
<title id=" W10-4141.xml">an double hidden hmm and an crf for segmentation tasks with pinyins finals </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results were encourage and indicated that the proposed cmm produces better not only accuracy but also training time efficiency.
</prevsent>
<prevsent>the official results in sighan-2010 also demonstrates that our method perform very well in traditional chinese with fine-tuned features set.
</prevsent>
</prevsection>
<citsent citstr=" W06-0115 ">
since 2006 chinese word segmentation bakeoff in sighan-3 (levow, 2006), <papid> W06-0115 </papid>this is the third time to join the competition (wu et al, 2006, <papid> W06-0139 </papid>2007).</citsent>
<aftsection>
<nextsent>in this year, we join the sighan bakeoff task in both traditional and simplified chinese closed word segmentation.
</nextsent>
<nextsent>unlike most western languages, there is no explicit space between words.
</nextsent>
<nextsent>the goal of word segmentation is to identify words given the sentence.
</nextsent>
<nextsent>this technique provides important features for downstream purposes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4086">
<title id=" W10-4141.xml">an double hidden hmm and an crf for segmentation tasks with pinyins finals </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results were encourage and indicated that the proposed cmm produces better not only accuracy but also training time efficiency.
</prevsent>
<prevsent>the official results in sighan-2010 also demonstrates that our method perform very well in traditional chinese with fine-tuned features set.
</prevsent>
</prevsection>
<citsent citstr=" W06-0139 ">
since 2006 chinese word segmentation bakeoff in sighan-3 (levow, 2006), <papid> W06-0115 </papid>this is the third time to join the competition (wu et al, 2006, <papid> W06-0139 </papid>2007).</citsent>
<aftsection>
<nextsent>in this year, we join the sighan bakeoff task in both traditional and simplified chinese closed word segmentation.
</nextsent>
<nextsent>unlike most western languages, there is no explicit space between words.
</nextsent>
<nextsent>the goal of word segmentation is to identify words given the sentence.
</nextsent>
<nextsent>this technique provides important features for downstream purposes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4088">
<title id=" W10-4141.xml">an double hidden hmm and an crf for segmentation tasks with pinyins finals </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of word segmentation is to identify words given the sentence.
</prevsent>
<prevsent>this technique provides important features for downstream purposes.
</prevsent>
</prevsection>
<citsent citstr=" D07-1131 ">
examples include chinese part-of-speech (pos) tagging (wu et al, 2007), <papid> D07-1131 </papid>chinese word dependency parsing (wu et al, 2007, <papid> D07-1131 </papid>2008).</citsent>
<aftsection>
<nextsent>with the rapid growth of structural learning algorithms, such as conditional random fields (crfs) (lafferty et al, 2001) and maximum margin markov models (m3n) (taskar et al, 2003) have received great attention and be come prominent learning algorithm to many sequential labeling tasks.
</nextsent>
<nextsent>examples include part of-speech (pos) tagging (shen et al, 2007) and syntactic phrase chunking (suzuki et al, 2007).<papid> D07-1083 </papid></nextsent>
<nextsent>the chinese word segmentation can also be treated as character-based tagging task in (xue and converse, 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4090">
<title id=" W10-4141.xml">an double hidden hmm and an crf for segmentation tasks with pinyins finals </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples include chinese part-of-speech (pos) tagging (wu et al, 2007), <papid> D07-1131 </papid>chinese word dependency parsing (wu et al, 2007, <papid> D07-1131 </papid>2008).</prevsent>
<prevsent>with the rapid growth of structural learning algorithms, such as conditional random fields (crfs) (lafferty et al, 2001) and maximum margin markov models (m3n) (taskar et al, 2003) have received great attention and be come prominent learning algorithm to many sequential labeling tasks.</prevsent>
</prevsection>
<citsent citstr=" D07-1083 ">
examples include part of-speech (pos) tagging (shen et al, 2007) and syntactic phrase chunking (suzuki et al, 2007).<papid> D07-1083 </papid></citsent>
<aftsection>
<nextsent>the chinese word segmentation can also be treated as character-based tagging task in (xue and converse, 2002).
</nextsent>
<nextsent>one feature of sequential labeling is that it aims at finding non-recursive chunk fragments in given sentence.
</nextsent>
<nextsent>among these approaches, crf has been wildly used in recent sighan bakeoff tasks (jin and chen, 2008; levow, 2006).<papid> W06-0115 </papid></nextsent>
<nextsent>although these approaches do not suffer from so-called label-bias problems (lafferty et al, 2001), one limitation is that they are inefficient to train with large-scale, especially large category data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4094">
<title id=" W10-4141.xml">an double hidden hmm and an crf for segmentation tasks with pinyins finals </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unlike structural learning algorithms, our method can be simply trained without considering the entire structures and hence the training timescales linearly with the number of training examples.
</prevsent>
<prevsent>in this framework, to alleviate the ease of label-bias problems, the state transition probability is ignored.
</prevsent>
</prevsection>
<citsent citstr=" I08-4029 ">
instead, we merely utilize the property of label relationships between chunks (wu et al, 2008).<papid> I08-4029 </papid></citsent>
<aftsection>
<nextsent>to demonstrate our method, we compare to several well-known structural learning algorithms, like crf (kudo et al, 2004), <papid> W04-3230 </papid>and svm-hmm (joachims et al, 2009) on two well-known data, namely, conll-2000 syntactic chunking, sighan-3 chinese word segmentation tasks.</nextsent>
<nextsent>by following this, we apply the model to the chinese word segmentation tasks of sighan-2010 this year.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4100">
<title id=" W10-4141.xml">an double hidden hmm and an crf for segmentation tasks with pinyins finals </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this framework, to alleviate the ease of label-bias problems, the state transition probability is ignored.
</prevsent>
<prevsent>instead, we merely utilize the property of label relationships between chunks (wu et al, 2008).<papid> I08-4029 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3230 ">
to demonstrate our method, we compare to several well-known structural learning algorithms, like crf (kudo et al, 2004), <papid> W04-3230 </papid>and svm-hmm (joachims et al, 2009) on two well-known data, namely, conll-2000 syntactic chunking, sighan-3 chinese word segmentation tasks.</citsent>
<aftsection>
<nextsent>by following this, we apply the model to the chinese word segmentation tasks of sighan-2010 this year.
</nextsent>
<nextsent>the empirical results showed that our method is not only fast but also achieving more superior accuracy than structural learning methods.
</nextsent>
<nextsent>in traditional chinese, our method also achieves the state-of-the-art performance inaccuracy with fined-tune features.
</nextsent>
<nextsent>models traditional conditional markov models (cmm) is to assign the tag sequence which maximizes the observation sequence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4104">
<title id=" W10-4141.xml">an double hidden hmm and an crf for segmentation tasks with pinyins finals </title>
<section> conditional support vector markov.  </section>
<citcontext>
<prevsection>
<prevsent>for example, if the previous word is tagged as begin of noun phrase (b-np), the current word must not be end of the other phrase (e-vp, e-pp, etc).
</prevsent>
<prevsent>therefore, we only model relationships between chunk tags to generate valid phrase structure.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
wu et al (2007), <papid> D07-1131 </papid>wu et al (2008) <papid> I08-4029 </papid>presented an automatic chunk pair relation construction algorithm which can handle so-called iob1/iob2/ioe1/ioe2 (kudo and matsumoto, 2001) <papid> N01-1025 </papid>chunk representation structures with either left-to-right or right-toleft directions.</citsent>
<aftsection>
<nextsent>here, we extend this idea and generalize to fit to more chunk tags.
</nextsent>
<nextsent>that is we can model the s-tag, b2, b3 tags with dividing the leading tags into two categories.
</nextsent>
<nextsent>for details can refer the literatures.
</nextsent>
<nextsent>three large-scale and large-category dataset is used to evaluate the proposed method, namely, conll-2000 syntactic chunking (tjong kim sang and buchholz, 2000), chinese pos tagging, and three of sighan-3 word segmentation tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4106">
<title id=" W10-4141.xml">an double hidden hmm and an crf for segmentation tasks with pinyins finals </title>
<section> empirical results.  </section>
<citcontext>
<prevsection>
<prevsent>three large-scale and large-category dataset is used to evaluate the proposed method, namely, conll-2000 syntactic chunking (tjong kim sang and buchholz, 2000), chinese pos tagging, and three of sighan-3 word segmentation tasks.
</prevsent>
<prevsent>table 1 shows the statistics of those datasets.
</prevsent>
</prevsection>
<citsent citstr=" P05-1001 ">
conll-2000 chunking task is well-known and widely evaluated in many literatures (suzuki et al, 2007; <papid> D07-1083 </papid>ando and zhang, 2005; <papid> P05-1001 </papid>kudo and matsumoto, 2001; <papid> N01-1025 </papid>wu et al, 2008; <papid> I08-4029 </papid>daum?</citsent>
<aftsection>
<nextsent>iii and marcu, 2005).
</nextsent>
<nextsent>the training data was derived from treebank wsj section 15-18 while section 20 was used for testing.
</nextsent>
<nextsent>the goal is to find the non-recursive phrase structures in sentence, such as noun phrase (np), verb phrase (vp), etc. there are 11 phrase types in this dataset.
</nextsent>
<nextsent>we follow the previous best settings for svms (kudo and matsumoto, 2001; <papid> N01-1025 </papid>wu et al, 2008).<papid> I08-4029 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4119">
<title id=" W10-4141.xml">an double hidden hmm and an crf for segmentation tasks with pinyins finals </title>
<section> empirical results.  </section>
<citcontext>
<prevsection>
<prevsent>seventy-five percent out of the data is used for training while the remaining 25% is used for testing.
</prevsent>
<prevsent>however, the task of the chinese pos tagging is very different from classical english pos tagging in that there is no word boundary information in chinese text.
</prevsent>
</prevsection>
<citsent citstr=" W04-3236 ">
to achieve this, ng and low (2004) <papid> W04-3236 </papid>gave successful study on chinese pos tagging.</citsent>
<aftsection>
<nextsent>just as english phrase chunking, the iob-tags can be used to represent the chinese word and its part-of-speech tag.
</nextsent>
<nextsent>for example, the tag b-adj means the first character of chinese word which pos tag is adj (adjective).
</nextsent>
<nextsent>n this task, we simply use the iob2 to represent the chunk structure.
</nextsent>
<nextsent>in this way, the tagger needs to recognize the chunk tag by considering 118 (59*2) categories at once.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4130">
<title id=" W10-4141.xml">an double hidden hmm and an crf for segmentation tasks with pinyins finals </title>
<section> empirical results.  </section>
<citcontext>
<prevsection>
<prevsent>then the predicted labels were included as features to train the final-stage classifier.
</prevsent>
<prevsent>the final classifier is still our cmm.
</prevsent>
</prevsection>
<citsent citstr=" I05-3025 ">
third, the postprocessing method (low et al, 2005) <papid> I05-3025 </papid>is employed to enhance the unknown word segmenta tion.</citsent>
<aftsection>
<nextsent>table 4 lists the empirical results of the development set.
</nextsent>
<nextsent>by validate with development data, we found that c=1.25 and use the e-bies representation method (wu et al, 2008) <papid> I08-4029 </papid>yields better accuracy than b-bies (zhou and kit, 2007).</nextsent>
<nextsent>meanwhile, crf seems to be suitable for bies representation method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4138">
<title id=" W11-0905.xml">vignet grounding language in graphics using frame semantics </title>
<section> frame semantics and framenet.  </section>
<citcontext>
<prevsection>
<prevsent>he may also have mental prototype of the visual scenario itself 29 (e.g. standing at counter in store).
</prevsent>
<prevsent>in fs the role of syntactic theory and the lexicon is to explain how the syntactic dependents of word that realizes frame (i.e. arguments and adjuncts) are mapped to frame elements via valence patterns.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
framenet (fn; baker et al (1998), <papid> P98-1013 </papid>ruppenhofer et al (2010)) is lexical resource based on fs.</citsent>
<aftsection>
<nextsent>frames in fn (around 1000) 1 are defined in terms of their frame elements, relations to other frame sand semantic types of fes.
</nextsent>
<nextsent>beyond this, the meaning of the frame (how the fes are related to each other) is only described in natural language.
</nextsent>
<nextsent>fncontains about 11,800 lexical units, which are pairings of words and frames.
</nextsent>
<nextsent>these come with annotated example sentences (about 150,000) to illustrate their valence patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4139">
<title id=" W11-0905.xml">vignet grounding language in graphics using frame semantics </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>from world knowledge we know (via instances of the typical-location frame) that washing food typically takes place in the kitchen.
</prevsent>
<prevsent>to create ascene we compose the two vignettes together by unifying the sink in the location vignette with the sink in the action vignette.
</prevsent>
</prevsection>
<citsent citstr=" T75-2004 ">
the grounding of natural language to graphical relations has been investigated in very early text-to scene systems (boberg, 1972), (simmons, 1975), (<papid> T75-2004 </papid>kahn, 1979), (adorni et al, 1984), <papid> P84-1106 </papid>and then later in put (clay and wilhelms, 1996), and words eye (coyne and sproat, 2001).</citsent>
<aftsection>
<nextsent>other systems, such as carsim (dupuy et al, 2001), <papid> W01-1301 </papid>jack (badler et al, 1998), and confucius (ma and mckevitt, 2006) target animation and virtual environments rather than scene construction.</nextsent>
<nextsent>a graphically grounded lexical-semantic resource such as vignet would be of use to these and related domains.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4140">
<title id=" W11-0905.xml">vignet grounding language in graphics using frame semantics </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>from world knowledge we know (via instances of the typical-location frame) that washing food typically takes place in the kitchen.
</prevsent>
<prevsent>to create ascene we compose the two vignettes together by unifying the sink in the location vignette with the sink in the action vignette.
</prevsent>
</prevsection>
<citsent citstr=" P84-1106 ">
the grounding of natural language to graphical relations has been investigated in very early text-to scene systems (boberg, 1972), (simmons, 1975), (<papid> T75-2004 </papid>kahn, 1979), (adorni et al, 1984), <papid> P84-1106 </papid>and then later in put (clay and wilhelms, 1996), and words eye (coyne and sproat, 2001).</citsent>
<aftsection>
<nextsent>other systems, such as carsim (dupuy et al, 2001), <papid> W01-1301 </papid>jack (badler et al, 1998), and confucius (ma and mckevitt, 2006) target animation and virtual environments rather than scene construction.</nextsent>
<nextsent>a graphically grounded lexical-semantic resource such as vignet would be of use to these and related domains.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4141">
<title id=" W11-0905.xml">vignet grounding language in graphics using frame semantics </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to create ascene we compose the two vignettes together by unifying the sink in the location vignette with the sink in the action vignette.
</prevsent>
<prevsent>the grounding of natural language to graphical relations has been investigated in very early text-to scene systems (boberg, 1972), (simmons, 1975), (<papid> T75-2004 </papid>kahn, 1979), (adorni et al, 1984), <papid> P84-1106 </papid>and then later in put (clay and wilhelms, 1996), and words eye (coyne and sproat, 2001).</prevsent>
</prevsection>
<citsent citstr=" W01-1301 ">
other systems, such as carsim (dupuy et al, 2001), <papid> W01-1301 </papid>jack (badler et al, 1998), and confucius (ma and mckevitt, 2006) target animation and virtual environments rather than scene construction.</citsent>
<aftsection>
<nextsent>a graphically grounded lexical-semantic resource such as vignet would be of use to these and related domains.
</nextsent>
<nextsent>the concept of vignettes as graphical realizations of more general frames was introduced in (coyne et al, 2010).
</nextsent>
<nextsent>in addition to framenet, much work has been done in developing theories and resources for lexical semantics and common-sense knowledge.
</nextsent>
<nextsent>verbnet (kipper et al, 2000) focuses on verb subcat patterns grouped by levin verb classes (levin, 1993),but also grounds verb semantics into small number of causal primitives representing temporal constraints tied to causality and state changes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4142">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe this research and its implications for smt.
</prevsent>
<prevsent>some statistical machine translation (smt) systems use pattern-based rules acquired from linguistically processed bitexts.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
they acquire these rules through the alignment of parsed structure in one language with raw string in the other language (yamada and knight, 2001; <papid> P01-1067 </papid>shen et al, 2008) <papid> P08-1066 </papid>or the alignment of source/target language parse trees (zhang et al,2008; <papid> P08-1064 </papid>cowan, 2008).</citsent>
<aftsection>
<nextsent>this paper shows that machine translation (mt) can also benefit by aligning adeeper?
</nextsent>
<nextsent>level of analysis than parsed text, which includes semantic role labeling, regularization of pas sives and wh constructions, etc. we create glarf representations (meyers et al, 2009) <papid> W09-2423 </papid>for english and chinese sentences, in the form of directed acyclic graphs.</nextsent>
<nextsent>we describe two graph-based techniques for reordering english sentences to be closer to that of corresponding chinese sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4143">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe this research and its implications for smt.
</prevsent>
<prevsent>some statistical machine translation (smt) systems use pattern-based rules acquired from linguistically processed bitexts.
</prevsent>
</prevsection>
<citsent citstr=" P08-1066 ">
they acquire these rules through the alignment of parsed structure in one language with raw string in the other language (yamada and knight, 2001; <papid> P01-1067 </papid>shen et al, 2008) <papid> P08-1066 </papid>or the alignment of source/target language parse trees (zhang et al,2008; <papid> P08-1064 </papid>cowan, 2008).</citsent>
<aftsection>
<nextsent>this paper shows that machine translation (mt) can also benefit by aligning adeeper?
</nextsent>
<nextsent>level of analysis than parsed text, which includes semantic role labeling, regularization of pas sives and wh constructions, etc. we create glarf representations (meyers et al, 2009) <papid> W09-2423 </papid>for english and chinese sentences, in the form of directed acyclic graphs.</nextsent>
<nextsent>we describe two graph-based techniques for reordering english sentences to be closer to that of corresponding chinese sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4144">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe this research and its implications for smt.
</prevsent>
<prevsent>some statistical machine translation (smt) systems use pattern-based rules acquired from linguistically processed bitexts.
</prevsent>
</prevsection>
<citsent citstr=" P08-1064 ">
they acquire these rules through the alignment of parsed structure in one language with raw string in the other language (yamada and knight, 2001; <papid> P01-1067 </papid>shen et al, 2008) <papid> P08-1066 </papid>or the alignment of source/target language parse trees (zhang et al,2008; <papid> P08-1064 </papid>cowan, 2008).</citsent>
<aftsection>
<nextsent>this paper shows that machine translation (mt) can also benefit by aligning adeeper?
</nextsent>
<nextsent>level of analysis than parsed text, which includes semantic role labeling, regularization of pas sives and wh constructions, etc. we create glarf representations (meyers et al, 2009) <papid> W09-2423 </papid>for english and chinese sentences, in the form of directed acyclic graphs.</nextsent>
<nextsent>we describe two graph-based techniques for reordering english sentences to be closer to that of corresponding chinese sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4145">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they acquire these rules through the alignment of parsed structure in one language with raw string in the other language (yamada and knight, 2001; <papid> P01-1067 </papid>shen et al, 2008) <papid> P08-1066 </papid>or the alignment of source/target language parse trees (zhang et al,2008; <papid> P08-1064 </papid>cowan, 2008).</prevsent>
<prevsent>this paper shows that machine translation (mt) can also benefit by aligning adeeper?</prevsent>
</prevsection>
<citsent citstr=" W09-2423 ">
level of analysis than parsed text, which includes semantic role labeling, regularization of pas sives and wh constructions, etc. we create glarf representations (meyers et al, 2009) <papid> W09-2423 </papid>for english and chinese sentences, in the form of directed acyclic graphs.</citsent>
<aftsection>
<nextsent>we describe two graph-based techniques for reordering english sentences to be closer to that of corresponding chinese sentences.
</nextsent>
<nextsent>one technique is based on manually created rules and the other isbased on an automatic alignment of glarf representations of chinese/english sentences.
</nextsent>
<nextsent>after reordering, we align words of the reordered english with the words of the chinese, using the giza++word aligner(och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>for both techniques, the resulting alignment has higher f-score than giza++ on raw text (a 0.7% to 1.5% absolute improvement).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4146">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe two graph-based techniques for reordering english sentences to be closer to that of corresponding chinese sentences.
</prevsent>
<prevsent>one technique is based on manually created rules and the other isbased on an automatic alignment of glarf representations of chinese/english sentences.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
after reordering, we align words of the reordered english with the words of the chinese, using the giza++word aligner(och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>for both techniques, the resulting alignment has higher f-score than giza++ on raw text (a 0.7% to 1.5% absolute improvement).
</nextsent>
<nextsent>in principle, our reordered text canbe used to improve any chinese/english smt system for which giza++ (or other word aligners) are part of the processing pipeline.
</nextsent>
<nextsent>these experiments are first step in usingglarf-style analyses for mt, potentially improving systems that already perform well with aligned text lacking large gaps in surface alignment.
</nextsent>
<nextsent>we hypothesize that smt systems are most likely to benefit from deep analysis for structures where source and target language word order differs the most.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4147">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> related work in smt.  </section>
<citcontext>
<prevsection>
<prevsent>88
</prevsent>
<prevsent>four papers stand out as closely related to the present study.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
(collins et al, 2005; <papid> P05-1066 </papid>wang et al,2007) <papid> D07-1077 </papid>describe experiments which use manually created parse-tree-based rules to reorder one side of bitext: german/english in (collins et al, 2005) <papid> P05-1066 </papid>and english/chinese in (wang et al, 2007).<papid> D07-1077 </papid></citsent>
<aftsection>
<nextsent>both achieve bleu score improvements for smt: 25.2% to 26.8% for (collins et al, 2005) <papid> P05-1066 </papid>and 28.52 to 30.86 for (wang et al, 2007).<papid> D07-1077 </papid></nextsent>
<nextsent>(wang et al, 2007) <papid> D07-1077 </papid>uses rules very similar to our own as they use the same language pair, although they reorder the chinese,whereas we reorder the english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4149">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> related work in smt.  </section>
<citcontext>
<prevsection>
<prevsent>88
</prevsent>
<prevsent>four papers stand out as closely related to the present study.
</prevsent>
</prevsection>
<citsent citstr=" D07-1077 ">
(collins et al, 2005; <papid> P05-1066 </papid>wang et al,2007) <papid> D07-1077 </papid>describe experiments which use manually created parse-tree-based rules to reorder one side of bitext: german/english in (collins et al, 2005) <papid> P05-1066 </papid>and english/chinese in (wang et al, 2007).<papid> D07-1077 </papid></citsent>
<aftsection>
<nextsent>both achieve bleu score improvements for smt: 25.2% to 26.8% for (collins et al, 2005) <papid> P05-1066 </papid>and 28.52 to 30.86 for (wang et al, 2007).<papid> D07-1077 </papid></nextsent>
<nextsent>(wang et al, 2007) <papid> D07-1077 </papid>uses rules very similar to our own as they use the same language pair, although they reorder the chinese,whereas we reorder the english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4168">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> related work in smt.  </section>
<citcontext>
<prevsection>
<prevsent>(wang et al, 2007) <papid> D07-1077 </papid>uses rules very similar to our own as they use the same language pair, although they reorder the chinese,whereas we reorder the english.</prevsent>
<prevsent>the most significant differences between our research and (collins et al, 2005; <papid> P05-1066 </papid>wang et al, 2007) <papid> D07-1077 </papid>are: (1) our manual rules benefit from level of representation deeper than surface parse; and (2) in addition to the hand coded rules, we also use automatic alignment-basedrules.</prevsent>
</prevsection>
<citsent citstr=" N09-2004 ">
(wu and fung, 2009) <papid> N09-2004 </papid>uses propbank role labels (palmer et al, 2005) <papid> J05-1004 </papid>as the basis of second pass filter over an smt system to improve the bleu score from 42.99 to 43.51.</citsent>
<aftsection>
<nextsent>the main similarity tothe current study is the use of level of representation that is deeper?
</nextsent>
<nextsent>than surface parse.
</nextsent>
<nextsent>how ever, our application of linguistic structure is more like that of (wang et al, 2007) <papid> D07-1077 </papid>and our deep?</nextsent>
<nextsent>level connects all predicates and arguments in the sentence, regardless of part of speech, rather than just connecting verbs to their arguments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4169">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> related work in smt.  </section>
<citcontext>
<prevsection>
<prevsent>(wang et al, 2007) <papid> D07-1077 </papid>uses rules very similar to our own as they use the same language pair, although they reorder the chinese,whereas we reorder the english.</prevsent>
<prevsent>the most significant differences between our research and (collins et al, 2005; <papid> P05-1066 </papid>wang et al, 2007) <papid> D07-1077 </papid>are: (1) our manual rules benefit from level of representation deeper than surface parse; and (2) in addition to the hand coded rules, we also use automatic alignment-basedrules.</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
(wu and fung, 2009) <papid> N09-2004 </papid>uses propbank role labels (palmer et al, 2005) <papid> J05-1004 </papid>as the basis of second pass filter over an smt system to improve the bleu score from 42.99 to 43.51.</citsent>
<aftsection>
<nextsent>the main similarity tothe current study is the use of level of representation that is deeper?
</nextsent>
<nextsent>than surface parse.
</nextsent>
<nextsent>how ever, our application of linguistic structure is more like that of (wang et al, 2007) <papid> D07-1077 </papid>and our deep?</nextsent>
<nextsent>level connects all predicates and arguments in the sentence, regardless of part of speech, rather than just connecting verbs to their arguments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4174">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> preparing the data.  </section>
<citcontext>
<prevsection>
<prevsent>for example (crego and marino, 2006)reorders based on patterns of pos tags.
</prevsent>
<prevsent>we hypothesize that this is similar to the above approaches inthat patterns of pos tags are likely to simulate parsing or chunking.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
the two stage parsers of previous decades (hobbsand grishman, 1976) generated syntactic representation analogous to the (more accurate) output of current treebank-based parsers (charniak, 2001)<papid> P01-1017 </papid>and an additional second stage output that regularized constructions (passive, active, relative clauses) to representations similar to active clauses with no gaps, e.g., the book was read by mary was given representation similar to that of mary read the book.</citsent>
<aftsection>
<nextsent>treating the active clause as canonical provides away to reduce variation in language and thus, making it easier to acquire and apply statistical information from corpora there is more evidence for particular statistical patterns when applications learn patterns and patterns more readily match data.
</nextsent>
<nextsent>two-stage parsers were influenced by linguistic theories (harris, 1968; chomsky, 1957; bresnan and kaplan, 1982) which distinguish surface?
</nextsent>
<nextsent>and deep?
</nextsent>
<nextsent>level.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4177">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> preparing the data.  </section>
<citcontext>
<prevsection>
<prevsent>currently semantic parsing?
</prevsent>
<prevsent>refers to similar representation, e.g., (wagner et al, 2007) or our own glarf (meyers et al, 2009).<papid> W09-2423 </papid></prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
however, the term isalso used for semantic role label ers (gildea and jurafsky, 2002; <papid> J02-3001 </papid>xue, 2008), <papid> J08-2004 </papid>systems which typically label semantic relations between verbs and their arguments and rarely cover arguments of other parts of speech.</citsent>
<aftsection>
<nextsent>second stage semantic parsers like our own, connect all the tokens in the sentence.
</nextsent>
<nextsent>aligned text processed in this way can (for example) represent differences in english/chinese noun modifier order, including relative clauses.
</nextsent>
<nextsent>in contrast, fewrole label ers handle noun modifiers and none handle relative clauses.
</nextsent>
<nextsent>below, we describe the glarf framework and our system for generating glarf representations of english and chinese sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4178">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> preparing the data.  </section>
<citcontext>
<prevsection>
<prevsent>currently semantic parsing?
</prevsent>
<prevsent>refers to similar representation, e.g., (wagner et al, 2007) or our own glarf (meyers et al, 2009).<papid> W09-2423 </papid></prevsent>
</prevsection>
<citsent citstr=" J08-2004 ">
however, the term isalso used for semantic role label ers (gildea and jurafsky, 2002; <papid> J02-3001 </papid>xue, 2008), <papid> J08-2004 </papid>systems which typically label semantic relations between verbs and their arguments and rarely cover arguments of other parts of speech.</citsent>
<aftsection>
<nextsent>second stage semantic parsers like our own, connect all the tokens in the sentence.
</nextsent>
<nextsent>aligned text processed in this way can (for example) represent differences in english/chinese noun modifier order, including relative clauses.
</nextsent>
<nextsent>in contrast, fewrole label ers handle noun modifiers and none handle relative clauses.
</nextsent>
<nextsent>below, we describe the glarf framework and our system for generating glarf representations of english and chinese sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4180">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> preparing the data.  </section>
<citcontext>
<prevsection>
<prevsent>n6?
</prevsent>
<prevsent>n1?
</prevsent>
</prevsection>
<citsent citstr=" W03-1707 ">
i rules tennis n6 n2 figure 1: word-aligned logic1 dependencies represented: surface dependencies (close to the levelof the parser), logic1 dependencies (reflecting various regularizations) and logic2 dependencies (re flec ting the output of prop banker, nom banker and penn discourse treebank transducer).(palmer et al, 2005; <papid> J05-1004 </papid>xue and palmer, 2003; <papid> W03-1707 </papid>meyers et al,2004; miltsakaki et al, 2004) <papid> W04-2703 </papid>the surface dependency graph is tree; the logic1 dependency graphis an directed acyclic graph; and the logic2 dependency graph is directed graph with cycles, covering only subset of the tokens in the sentence.</citsent>
<aftsection>
<nextsent>for these experiments, we focus on the logic1 relations, but will sometimes use the surface relations as well.figure 1 is simple dependency-based logic1 representation of know the rules of tennis and its chinese translation.
</nextsent>
<nextsent>the edge labels name the relations between heads and dependents, e.g., is the sbj ofknow and the dashed lines indicate word level correspondences.
</nextsent>
<nextsent>each node is labeled with both word and unique node identifier (n1, n1?, etc.) the english system achieves f-scores for logic1 dependencies on parsed news text in the 8090% range and the chinese system achieves f-scores in the 7484% range, depending on the complexity of the text.
</nextsent>
<nextsent>the english system has been created over the course of about 9 years, and consequently is more extensive than the chinese system, which has been created over the past 3 years.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4181">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> preparing the data.  </section>
<citcontext>
<prevsection>
<prevsent>n6?
</prevsent>
<prevsent>n1?
</prevsent>
</prevsection>
<citsent citstr=" W04-2703 ">
i rules tennis n6 n2 figure 1: word-aligned logic1 dependencies represented: surface dependencies (close to the levelof the parser), logic1 dependencies (reflecting various regularizations) and logic2 dependencies (re flec ting the output of prop banker, nom banker and penn discourse treebank transducer).(palmer et al, 2005; <papid> J05-1004 </papid>xue and palmer, 2003; <papid> W03-1707 </papid>meyers et al,2004; miltsakaki et al, 2004) <papid> W04-2703 </papid>the surface dependency graph is tree; the logic1 dependency graphis an directed acyclic graph; and the logic2 dependency graph is directed graph with cycles, covering only subset of the tokens in the sentence.</citsent>
<aftsection>
<nextsent>for these experiments, we focus on the logic1 relations, but will sometimes use the surface relations as well.figure 1 is simple dependency-based logic1 representation of know the rules of tennis and its chinese translation.
</nextsent>
<nextsent>the edge labels name the relations between heads and dependents, e.g., is the sbj ofknow and the dashed lines indicate word level correspondences.
</nextsent>
<nextsent>each node is labeled with both word and unique node identifier (n1, n1?, etc.) the english system achieves f-scores for logic1 dependencies on parsed news text in the 8090% range and the chinese system achieves f-scores in the 7484% range, depending on the complexity of the text.
</nextsent>
<nextsent>the english system has been created over the course of about 9 years, and consequently is more extensive than the chinese system, which has been created over the past 3 years.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4186">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> preparing the data.  </section>
<citcontext>
<prevsection>
<prevsent>the chinese pipeline is similar, except that it includes the ldc word segmenterand prop banker (xue, 2008).<papid> J08-2004 </papid></prevsent>
<prevsent>also, the regularization routines are not as completely developed, e.g., relative clause gaps and pass ives are not handledyet.</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
the chinese system currently uses the berkeley parser (petrov and klein, 2007).<papid> N07-1051 </papid></citsent>
<aftsection>
<nextsent>each of these pipelines derives typed feature structure representations, which are then converted into the 25 tuple representation of 3 types of dependencies between pairs of tokens: surface, logic1 and logic2.to insure that the logic1 graphs are acyclic, we assume that certain edges are surface only and that the resulting directed acyclic graphs can have multiple roots.
</nextsent>
<nextsent>it turns out that the multiple rooted cases are mostly limited to few constructions, the most common being parenthetical clauses and relative clauses.
</nextsent>
<nextsent>a parenthetical clause takes the main clause as an argument.
</nextsent>
<nextsent>for example, in the word potato?, he claimed, is spelled with final e?., the verb claimed,takes the entire main clause as an argument, we assume that he claimed is dependent on the main verb (is) spelled labeled parenthetical in our surface dependency structure, but that the main verb (is) spelled is dependent of the verb claimed in our logic1 structure, labeled complement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4191">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> automatic node alignment and its.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 align-alg1.
</prevsent>
<prevsent>this work uses modified version of align alg1, graph alignment algorithm we previously used to align 1990s-style two-stage parser output formt experiments.
</prevsent>
</prevsection>
<citsent citstr=" C96-1078 ">
align-alg1 is an o(n2) algorithm, is the maximum number of nodes in the source and target graphs (meyers et al, 1996; <papid> C96-1078 </papid>meyers et al, 1998).<papid> P98-2139 </papid></citsent>
<aftsection>
<nextsent>given source tree and target tree ?, an alignment(t, ?) is partial function from nodes in to nodes ? in ?.
</nextsent>
<nextsent>an exhaustive search of possible alignments would consider all non-intersecting combinations of the t ? pairs ofsource/target nodes ? there are at most ! such pairings where  = ?.1 however, align-alg1 assumes that some of these pairings are unlikely, and1this ignores to 1 matches, which we allow, although relatively rarely.
</nextsent>
<nextsent>91 favors pairings that assume the structure of the trees correspond more closely.
</nextsent>
<nextsent>in particular, it is assumed that ancestor nodes are more likely to match if most of their descendant nodes match as well.align-alg1 finds the highest scoring alignment, where the score of an alignment is the sumof the scores of the node pairs in the partial function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4192">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> automatic node alignment and its.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 align-alg1.
</prevsent>
<prevsent>this work uses modified version of align alg1, graph alignment algorithm we previously used to align 1990s-style two-stage parser output formt experiments.
</prevsent>
</prevsection>
<citsent citstr=" P98-2139 ">
align-alg1 is an o(n2) algorithm, is the maximum number of nodes in the source and target graphs (meyers et al, 1996; <papid> C96-1078 </papid>meyers et al, 1998).<papid> P98-2139 </papid></citsent>
<aftsection>
<nextsent>given source tree and target tree ?, an alignment(t, ?) is partial function from nodes in to nodes ? in ?.
</nextsent>
<nextsent>an exhaustive search of possible alignments would consider all non-intersecting combinations of the t ? pairs ofsource/target nodes ? there are at most ! such pairings where  = ?.1 however, align-alg1 assumes that some of these pairings are unlikely, and1this ignores to 1 matches, which we allow, although relatively rarely.
</nextsent>
<nextsent>91 favors pairings that assume the structure of the trees correspond more closely.
</nextsent>
<nextsent>in particular, it is assumed that ancestor nodes are more likely to match if most of their descendant nodes match as well.align-alg1 finds the highest scoring alignment, where the score of an alignment is the sumof the scores of the node pairs in the partial function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4193">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> automatic node alignment and its.  </section>
<citcontext>
<prevsection>
<prevsent>lex1 is merger between: the ldc 2002 chinese-english dictionary and hownet.
</prevsent>
<prevsent>in addition, we manually added additional translations of units of measure from english.
</prevsent>
</prevsection>
<citsent citstr=" W07-0401 ">
we also used nedict, name translation dictionary (ji et al, 2009) and autodict, english/chinese word to word pairs with high similarity scores taken from mt phase tables created as part of the (zhang et al,2007) <papid> W07-0401 </papid>system.</citsent>
<aftsection>
<nextsent>the nedict was used both for precise matches and partial matches (since, nes can often be synonymous with sub strings of nes).
</nextsent>
<nextsent>in addition, we used some wordnet (fellbaum, 1998) synonyms of english to expand the coverage of all the dictionaries, allowing english words to match chinese word translations of their synonyms.
</nextsent>
<nextsent>we allowed additional matches of function words that served similar functions in the two languages includ ing: copulas, pronouns and determiners.
</nextsent>
<nextsent>finally, we use mutual information (mi) based approach to find further lexical information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4194">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>we compared the system output for each section against the baseline and used the sign test to calculate statistical significance.
</prevsent>
<prevsent>all system output except one5 achieve dat least   .05 and most systems achieved significance well below   .01.
</prevsent>
</prevsection>
<citsent citstr=" J07-3002 ">
informally, we observe that the rules reordering common noun modifiers produce most of the total 4we used f-scores, which (fraser and marcu, 2007) <papid> J07-3002 </papid>show to correlate well with improvements in bleu.</citsent>
<aftsection>
<nextsent>we weighted precision and recall evenly since we do not currently have bleu scores for mt that use these alignments and therefore cannot tune the weights.
</nextsent>
<nextsent>our results also showed improvements in alignment error rate (aer) (och and ney, 2000), <papid> P00-1056 </papid>which incorporate the possible?</nextsent>
<nextsent>and sure?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4195">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>informally, we observe that the rules reordering common noun modifiers produce most of the total 4we used f-scores, which (fraser and marcu, 2007) <papid> J07-3002 </papid>show to correlate well with improvements in bleu.</prevsent>
<prevsent>we weighted precision and recall evenly since we do not currently have bleu scores for mt that use these alignments and therefore cannot tune the weights.</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
our results also showed improvements in alignment error rate (aer) (och and ney, 2000), <papid> P00-1056 </papid>which incorporate the possible?</citsent>
<aftsection>
<nextsent>and sure?
</nextsent>
<nextsent>portions of the manual alignment into f-score, but do not seem to correlate well with bleu.5when run on the test corpus, the manual system outperformed the baseline system on only 13 out of 20 sections.
</nextsent>
<nextsent>improvement.
</nextsent>
<nextsent>however, space limitations prevent detailed exploration of these differences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4196">
<title id=" W11-1010.xml">improving mt word alignment using aligned multistage parses </title>
<section> concluding remarks.  </section>
<citcontext>
<prevsection>
<prevsent>we would also expect that the precision of our system to be more important than the recall, since our system would not yield an improvement if it produced too much noise.
</prevsent>
<prevsent>further experiments with current mt systems are needed to assess whether this is actually the case.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
we are considering such tests for future research, using the moses smt system (koehn et al, 2007).<papid> P07-2045 </papid>our representation had several possible advantages over pure parse-based methods.</citsent>
<aftsection>
<nextsent>we used semantic features such as temporal, locative and transparent (whether low-content words inherits its semantics) to help guide our alignment.
</nextsent>
<nextsent>the regularized structure, also, helped identify long-distancedependency relationships.
</nextsent>
<nextsent>we are also considering several improvements for our alignment-based rules: (1) using additional dictionary resources suchas catvar (habash and dorr, 2003), so that crosspart-of speech alignments can be more readily rec ognized; (2) finding more optimal orderings for unaligned source language words.
</nextsent>
<nextsent>for example, the alignment-based method reordered bright star arising from chinas policy to bright arising from china policy star, separating bright from star,even though bright star function as unit; (3) incorporating and using multi-word bilingual dictionaryentries.; (4) automatic methods for tuning parameters of our system that are currently hand-coded; (5)training mi on much larger corpus; (6) investigating possible ways to merge the manual-rules with the alignment-based approach; and (7) performing similar experiments with english/japanese bitexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4197">
<title id=" W10-4126.xml">the cipssighan clp2010 chinese word segmentation backoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we found that compared with the previous chinese word segmentation bake offs, the performance of cross-domain chinese word segmentation is not much lower, and the out-of-vocabulary recall is improved.
</prevsent>
<prevsent>chinese is written without inter-word spaces, so finding word-boundaries is an essential first step in many natural language processing tasks ranging from part of speech tagging to parsing, reference resolution and machine translation.
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
sighan, the special interest group for chinese language processing of the association for computational linguistics, successfully conducted four prior word segmentation bake offs, in 2003 (sproat and emerson, 2003), <papid> W03-1719 </papid>2005 (emerson, 2005), <papid> I05-3017 </papid>2006 (levow, 2006) <papid> W06-0115 </papid>and 2007 (jin and chen, 2007), and the bakeoff 2007 was jointly organized with the chinese information processing society of china (cips).</citsent>
<aftsection>
<nextsent>these evaluations established benchmarks for word segmentation with which researchers evaluate their segmentation system.
</nextsent>
<nextsent>after years of intensive researches, chinese word segmentation has achieved quite high precision, though the out-of-vocabulary problem is still continuing challenge.
</nextsent>
<nextsent>however, the performance of segmentation is not so satisfying for out-of-domain text.
</nextsent>
<nextsent>the cips-sighan clp 2010 chinese word segmentation bakeoff continues the ongoing series of the sighan chinese word segmentation bakeoff.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4198">
<title id=" W10-4126.xml">the cipssighan clp2010 chinese word segmentation backoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we found that compared with the previous chinese word segmentation bake offs, the performance of cross-domain chinese word segmentation is not much lower, and the out-of-vocabulary recall is improved.
</prevsent>
<prevsent>chinese is written without inter-word spaces, so finding word-boundaries is an essential first step in many natural language processing tasks ranging from part of speech tagging to parsing, reference resolution and machine translation.
</prevsent>
</prevsection>
<citsent citstr=" I05-3017 ">
sighan, the special interest group for chinese language processing of the association for computational linguistics, successfully conducted four prior word segmentation bake offs, in 2003 (sproat and emerson, 2003), <papid> W03-1719 </papid>2005 (emerson, 2005), <papid> I05-3017 </papid>2006 (levow, 2006) <papid> W06-0115 </papid>and 2007 (jin and chen, 2007), and the bakeoff 2007 was jointly organized with the chinese information processing society of china (cips).</citsent>
<aftsection>
<nextsent>these evaluations established benchmarks for word segmentation with which researchers evaluate their segmentation system.
</nextsent>
<nextsent>after years of intensive researches, chinese word segmentation has achieved quite high precision, though the out-of-vocabulary problem is still continuing challenge.
</nextsent>
<nextsent>however, the performance of segmentation is not so satisfying for out-of-domain text.
</nextsent>
<nextsent>the cips-sighan clp 2010 chinese word segmentation bakeoff continues the ongoing series of the sighan chinese word segmentation bakeoff.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4199">
<title id=" W10-4126.xml">the cipssighan clp2010 chinese word segmentation backoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we found that compared with the previous chinese word segmentation bake offs, the performance of cross-domain chinese word segmentation is not much lower, and the out-of-vocabulary recall is improved.
</prevsent>
<prevsent>chinese is written without inter-word spaces, so finding word-boundaries is an essential first step in many natural language processing tasks ranging from part of speech tagging to parsing, reference resolution and machine translation.
</prevsent>
</prevsection>
<citsent citstr=" W06-0115 ">
sighan, the special interest group for chinese language processing of the association for computational linguistics, successfully conducted four prior word segmentation bake offs, in 2003 (sproat and emerson, 2003), <papid> W03-1719 </papid>2005 (emerson, 2005), <papid> I05-3017 </papid>2006 (levow, 2006) <papid> W06-0115 </papid>and 2007 (jin and chen, 2007), and the bakeoff 2007 was jointly organized with the chinese information processing society of china (cips).</citsent>
<aftsection>
<nextsent>these evaluations established benchmarks for word segmentation with which researchers evaluate their segmentation system.
</nextsent>
<nextsent>after years of intensive researches, chinese word segmentation has achieved quite high precision, though the out-of-vocabulary problem is still continuing challenge.
</nextsent>
<nextsent>however, the performance of segmentation is not so satisfying for out-of-domain text.
</nextsent>
<nextsent>the cips-sighan clp 2010 chinese word segmentation bakeoff continues the ongoing series of the sighan chinese word segmentation bakeoff.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4201">
<title id=" W11-0702.xml">how can you say such things recognizing disagreement in informal political argument </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this work did not distinguish agreement vs. disagreement across quote/response pairs.
</prevsent>
<prevsent>rather they show that they canuse variant of lsa to improve accuracy for identifying parent post, given response post, with 70% accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W06-1639 ">
other similar work uses congressional debate transcripts or blogs or other social media to develop methods for distinguishing agreement from disagreement or to distinguish rebuttals from out-of context posts (thomas et al, 2006; <papid> W06-1639 </papid>bansal et al,2008; <papid> C08-2004 </papid>awad allah et al, 2010; walker et al, ; burfoot, 2008; mishne and glance, 2006; popescu and pennacchiotti, 2010).</citsent>
<aftsection>
<nextsent>these methods are directly applicable, but the genre of the language is so different from our informal forums that the results are not directly comparable.
</nextsent>
<nextsent>work by somasundaran &amp; wiebe (2009),  wiebe (2010) has examined debate web sites and focused on automatically determining the stance of debate participant with respect to particular issue.
</nextsent>
<nextsent>this work has treated each post as text to be classified in terms of stance, for particular topic,and shown that discourse relations such as concessions and the identification of argumentation triggers improves performance . their work, along with others, also indicates that for such tasks it is difficult to beat unigram baseline (pang and lee, 2008).
</nextsent>
<nextsent>other work has focused on the social network structure of online forums (murakami and raymond, 2010;<papid> C10-2100 </papid>agrawal et al, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4202">
<title id=" W11-0702.xml">how can you say such things recognizing disagreement in informal political argument </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this work did not distinguish agreement vs. disagreement across quote/response pairs.
</prevsent>
<prevsent>rather they show that they canuse variant of lsa to improve accuracy for identifying parent post, given response post, with 70% accuracy.
</prevsent>
</prevsection>
<citsent citstr=" C08-2004 ">
other similar work uses congressional debate transcripts or blogs or other social media to develop methods for distinguishing agreement from disagreement or to distinguish rebuttals from out-of context posts (thomas et al, 2006; <papid> W06-1639 </papid>bansal et al,2008; <papid> C08-2004 </papid>awad allah et al, 2010; walker et al, ; burfoot, 2008; mishne and glance, 2006; popescu and pennacchiotti, 2010).</citsent>
<aftsection>
<nextsent>these methods are directly applicable, but the genre of the language is so different from our informal forums that the results are not directly comparable.
</nextsent>
<nextsent>work by somasundaran &amp; wiebe (2009),  wiebe (2010) has examined debate web sites and focused on automatically determining the stance of debate participant with respect to particular issue.
</nextsent>
<nextsent>this work has treated each post as text to be classified in terms of stance, for particular topic,and shown that discourse relations such as concessions and the identification of argumentation triggers improves performance . their work, along with others, also indicates that for such tasks it is difficult to beat unigram baseline (pang and lee, 2008).
</nextsent>
<nextsent>other work has focused on the social network structure of online forums (murakami and raymond, 2010;<papid> C10-2100 </papid>agrawal et al, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4203">
<title id=" W11-0702.xml">how can you say such things recognizing disagreement in informal political argument </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>work by somasundaran &amp; wiebe (2009),  wiebe (2010) has examined debate web sites and focused on automatically determining the stance of debate participant with respect to particular issue.
</prevsent>
<prevsent>this work has treated each post as text to be classified in terms of stance, for particular topic,and shown that discourse relations such as concessions and the identification of argumentation triggers improves performance . their work, along with others, also indicates that for such tasks it is difficult to beat unigram baseline (pang and lee, 2008).
</prevsent>
</prevsection>
<citsent citstr=" C10-2100 ">
other work has focused on the social network structure of online forums (murakami and raymond, 2010;<papid> C10-2100 </papid>agrawal et al, 2003).</citsent>
<aftsection>
<nextsent>however, agarwals work assumed that adjacent posts always disagree, and did not use any of the information in the text.
</nextsent>
<nextsent>murakami &amp; raymond (2010) <papid> C10-2100 </papid>show that simple rules defined on the textual content of the post can improve over agarwals results.section 2 discusses our corpus in more detail, describes how we collected annotations using mechanical turk, and presents results of corpus analysis of the use of particular discourse cues.</nextsent>
<nextsent>section 3 describes how we set up classification experiments for distinguishing agreement from disagreement, and section 4 presents our results for agreement clas sifi cation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4205">
<title id=" W11-0702.xml">how can you say such things recognizing disagreement in informal political argument </title>
<section> data and corpus analysis.  </section>
<citcontext>
<prevsection>
<prevsent>these were biased by cue word to ensure adequate data for discourse marker analysis (see section 2.1.
</prevsent>
<prevsent>for this task we showed annotators seven q-r pairs and asked them to judge agreement/disagreement and set of other measures as shown in figure 2.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
most of our measures were scalar; we chose to dothis because previous work on estimating the relationship between mturk annotations and expert annotations suggest that taking the means of scalar annotations could be good way to reduce noise in mturk annotations (snow et al, 2008).<papid> D08-1027 </papid></citsent>
<aftsection>
<nextsent>for all of the measures annotated, the turkers were not given additional definitions of their meaning.
</nextsent>
<nextsent>for example, we let turkers to use their native intuitions about what it means for post to be sarcastic, since previous work suggests that non-specialists tend to collapse all forms of verbal irony under the term sarcastic (bryant and fox tree, 2002).
</nextsent>
<nextsent>we did not ask turkers to distinguish between sarcasm and other forms of verbal irony such as hyperbole, understatement, rhetorical questions and jocularity (gibbs, 2000).
</nextsent>
<nextsent>agreement was scalar judgment on an 11 point scale [-5,5] implemented with slider.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4206">
<title id=" W11-0702.xml">how can you say such things recognizing disagreement in informal political argument </title>
<section> data and corpus analysis.  </section>
<citcontext>
<prevsection>
<prevsent>the relatively low results for fact/emotion is perhaps due to the emotional charge many ideological arguments engender; informal examination of posts that showed the most disagreement in this category often showed cutting commentor snide remark at the end of post,which was was ignored by some annotators and evidence for others (one emotional post in figure 3 is clearly an insult, but was uniformly labeled as -5 by all annotators).
</prevsent>
<prevsent>2.1 discourse markers.
</prevsent>
</prevsection>
<citsent citstr=" P04-1085 ">
both psychological research on discourse processes (fox tree and schrock, 1999; fox tree and schrock, 2002; groen et al, 2010) and computational workon agreement (galley et al, 2004) <papid> P04-1085 </papid>indicate that discourse markers are strongly associated with particular pragmatic functions.</citsent>
<aftsection>
<nextsent>because of their salient position, we test the role of turn-initial markers in predicting upcoming content (fox tree and schrock,2002; groen et al, 2010).
</nextsent>
<nextsent>based on manual inspection of subset of the corpus, we constructed list of 20 discourse markers; 17 of these occurred at least 50 times in quote response (upper bound of 700 samples): actually, and, because, but, believe, know, see, think, just, no, oh, really, so, well, yes, you know, you mean.
</nextsent>
<nextsent>all of their occurrences became part of the 10,003 q-r pairs annotated.the top discourse markers highlighting disagreement were really (67% read response beginning with this marker as prefacing disagreement with prior post), no (66%), actually (60%), but (58%), so (58%), and you mean (57%).
</nextsent>
<nextsent>at this point, the next most disagreeable category was the unmarked category, with about 50% of respondents interpreting an unmarked post as disagreeing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4207">
<title id=" W11-0702.xml">how can you say such things recognizing disagreement in informal political argument </title>
<section> data and corpus analysis.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, the most agreeable marker was yes (73% read response beginning with this marker as prefacing an agreement) followed by know (64%), believe (62%), think (61%), and just (57%).
</prevsent>
<prevsent>the other markers were close to the unmarked category: and (50%), because (51%), oh (51%), see (52%), you know (54%), and well (55%).
</prevsent>
</prevsection>
<citsent citstr=" W10-2914 ">
the overall agreement on sarcasm was low, as in other computational work on recognizing sarcasm(davidov et al, 2010).<papid> W10-2914 </papid></citsent>
<aftsection>
<nextsent>at most, only 31% of respondents agreed that the material after discourse marker was sarcastic, with the most sarcastic markers being you mean (31%), oh (29%), really (24%),so (22%), and see (21%).
</nextsent>
<nextsent>only 15% of respondents rated the unmarked category as sarcastic (e.g., fewer than 1 out of 6 respondents).
</nextsent>
<nextsent>the cues think (10%), believe (9%), and actually (10%) were the least sarcastic markers.
</nextsent>
<nextsent>taken together, these ratings suggest that the cues really, you mean, and so can be used to indicate both 5 class very high degree neutral very low degree insult or attack well, you have proven yoruself to be man with no brain, that is for sure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4208">
<title id=" W11-0702.xml">how can you say such things recognizing disagreement in informal political argument </title>
<section> machine learning experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>liwc liwc measures and frequencies dependencies dependencies derived from the stanford parser.generalized de pendenciesdependency features generalized with respect to pos of the head word and opinion polarity of both words.
</prevsent>
<prevsent>table 2: feature sets, descriptions, and examples unigrams, bigrams, trigrams.
</prevsent>
</prevsection>
<citsent citstr=" W10-0214 ">
results of previous work suggest that unigram baseline can be difficult to beat for certain types of debates (walker et al, ; somasundaran and wiebe, 2010).<papid> W10-0214 </papid></citsent>
<aftsection>
<nextsent>thus we derived both unigrams and bigrams as features.
</nextsent>
<nextsent>we captured the final token as feature by padding with -nil- tokens when building the bigrams.
</nextsent>
<nextsent>see below for comments on initial uni/bi/tri-grams.
</nextsent>
<nextsent>meta post info.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4209">
<title id=" W11-0702.xml">how can you say such things recognizing disagreement in informal political argument </title>
<section> machine learning experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>this includes author information, time between posts, the log10 of the time between posts, the number of other quotes in the response, whether the quote responds to post by the responses author, the percent of the quoted post which is actually quoted, whether the quoted post is by the same author as the response (there were only an handful of these), whether the response mentions the quote author by name, and whether the response is longer than the quote.the forum software effectively does this annotation for us so there is no reason not to consider it as clue in our quest to understand and interpret online dialogue.
</prevsent>
<prevsent>discourse markers.
</prevsent>
</prevsection>
<citsent citstr=" J93-3003 ">
previous work on dialogue analysis has repeatedly noted the discourse functions of particular discourse markers, and our corpus analysis above also suggests their use in this particular dataset (hirschberg and litman, 1993; <papid> J93-3003 </papid>fox tree, 2010; schiff rin, 1987; di eugenio et al, 1997;<papid> P97-1011 </papid>moser and moore, 1995).<papid> P95-1018 </papid></citsent>
<aftsection>
<nextsent>however, because discourse markers can be stacked up oh, so really we decided to represent this feature as post initial unigrams, bigrams and trigrams.
</nextsent>
<nextsent>repeated punctuation.
</nextsent>
<nextsent>informal analyses of ourdata suggested that repeated sequential use of particular types of punctuation such as !!
</nextsent>
<nextsent>and ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4210">
<title id=" W11-0702.xml">how can you say such things recognizing disagreement in informal political argument </title>
<section> machine learning experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>this includes author information, time between posts, the log10 of the time between posts, the number of other quotes in the response, whether the quote responds to post by the responses author, the percent of the quoted post which is actually quoted, whether the quoted post is by the same author as the response (there were only an handful of these), whether the response mentions the quote author by name, and whether the response is longer than the quote.the forum software effectively does this annotation for us so there is no reason not to consider it as clue in our quest to understand and interpret online dialogue.
</prevsent>
<prevsent>discourse markers.
</prevsent>
</prevsection>
<citsent citstr=" P97-1011 ">
previous work on dialogue analysis has repeatedly noted the discourse functions of particular discourse markers, and our corpus analysis above also suggests their use in this particular dataset (hirschberg and litman, 1993; <papid> J93-3003 </papid>fox tree, 2010; schiff rin, 1987; di eugenio et al, 1997;<papid> P97-1011 </papid>moser and moore, 1995).<papid> P95-1018 </papid></citsent>
<aftsection>
<nextsent>however, because discourse markers can be stacked up oh, so really we decided to represent this feature as post initial unigrams, bigrams and trigrams.
</nextsent>
<nextsent>repeated punctuation.
</nextsent>
<nextsent>informal analyses of ourdata suggested that repeated sequential use of particular types of punctuation such as !!
</nextsent>
<nextsent>and ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4211">
<title id=" W11-0702.xml">how can you say such things recognizing disagreement in informal political argument </title>
<section> machine learning experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>this includes author information, time between posts, the log10 of the time between posts, the number of other quotes in the response, whether the quote responds to post by the responses author, the percent of the quoted post which is actually quoted, whether the quoted post is by the same author as the response (there were only an handful of these), whether the response mentions the quote author by name, and whether the response is longer than the quote.the forum software effectively does this annotation for us so there is no reason not to consider it as clue in our quest to understand and interpret online dialogue.
</prevsent>
<prevsent>discourse markers.
</prevsent>
</prevsection>
<citsent citstr=" P95-1018 ">
previous work on dialogue analysis has repeatedly noted the discourse functions of particular discourse markers, and our corpus analysis above also suggests their use in this particular dataset (hirschberg and litman, 1993; <papid> J93-3003 </papid>fox tree, 2010; schiff rin, 1987; di eugenio et al, 1997;<papid> P97-1011 </papid>moser and moore, 1995).<papid> P95-1018 </papid></citsent>
<aftsection>
<nextsent>however, because discourse markers can be stacked up oh, so really we decided to represent this feature as post initial unigrams, bigrams and trigrams.
</nextsent>
<nextsent>repeated punctuation.
</nextsent>
<nextsent>informal analyses of ourdata suggested that repeated sequential use of particular types of punctuation such as !!
</nextsent>
<nextsent>and ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4212">
<title id=" W11-0702.xml">how can you say such things recognizing disagreement in informal political argument </title>
<section> machine learning experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>some liwc features that we expect to be important are words per sentence (wps), pronominal forms, and positive and negative emotion words.
</prevsent>
<prevsent>dependency and generalized dependency.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
weused the stanford parser to extract dependency features for each quote and response (de marneffe etal., 2006; klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>the dependency parse forgiven sentence is set of triples, composed of grammatical relation and the pair of words for which the grammatical relation holds (reli, wj , wk), where reli is the dependency relation among words wj and wk.
</nextsent>
<nextsent>the word wj is the head of the dependency relation.following (joshi and penstein-rose?, 2009) we extracted generalized dependency features by leaving one dependency element lexicalized and generalizing the other to part of speech.
</nextsent>
<nextsent>joshi &amp; roses results suggested that this approach would work better than either fully lexicalized or fully generalized dependency features.
</nextsent>
<nextsent>opinion dependencies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4213">
<title id=" W11-0502.xml">towards multi document summarization of scientific articles making interesting comparisons with scisumm </title>
<section> literature review.  </section>
<citcontext>
<prevsection>
<prevsent>some of the insights they use in their work also apply to our problem.
</prevsent>
<prevsent>they used clustering approach over different citations for the same target article for discovery of different ways of thinking about that article.
</prevsent>
</prevsection>
<citsent citstr=" N09-1066 ">
citation text has been already shown to contain important concepts about the article that mightbe absent from other important sections of an article e.g. an abstract (mohammad et al, 2009) .<papid> N09-1066 </papid>template based generation of summaries possessing similar hierarchical topic structure as the related work section in an article has also been proposed (hoang et al, 2010).</citsent>
<aftsection>
<nextsent>in our work, we consider flat topic structure in the form of topic clusters.
</nextsent>
<nextsent>more specifically, we discover the comparable attributes of the co-cited articles using frequent term based clustering (beil et al, 2002).
</nextsent>
<nextsent>the clusters generated in this process contain set of topically related text fragments called tiles, which are extracted from the set of co-cited articles.
</nextsent>
<nextsent>each cluster is indexed with label, which is frequent term set present in the tile.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4214">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there has been intense interest in the interpretation of tense and aspect into formal understanding of the ordering and duration of events.
</prevsent>
<prevsent>this work has been in both linguistics (dowty,1979; nerbonne, 1986; vlach, 1993) and natural language understanding.
</prevsent>
</prevsection>
<citsent citstr=" P87-1002 ">
early systems investigated rule-based approaches to parsing the durations and orderings of events from the tense sand aspects of their verbs (hinrichs, 1987; <papid> P87-1002 </papid>webber, 1987; <papid> P87-1021 </papid>song and cohen, 1988; passonneau, 1988).<papid> J88-2005 </papid></citsent>
<aftsection>
<nextsent>allen (1984) and steedman (1995) focus on distinguishing between achievements (when an event culminates in result, such as john builds house) and processes (such as walking).
</nextsent>
<nextsent>more figure 1: screen shot of our story encoding interface.
</nextsent>
<nextsent>recent work has centered on markup languages for complex temporal information (mani, 2004)and corpus-based (statistical) models for predicting temporal relationships on unseen text (mani et al., 2006; <papid> P06-1095 </papid>lapata and lascarides, 2006).</nextsent>
<nextsent>our annotation interface requires fast realizerthat can be easily integrated into an interactive, on line encoding tool.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4215">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there has been intense interest in the interpretation of tense and aspect into formal understanding of the ordering and duration of events.
</prevsent>
<prevsent>this work has been in both linguistics (dowty,1979; nerbonne, 1986; vlach, 1993) and natural language understanding.
</prevsent>
</prevsection>
<citsent citstr=" P87-1021 ">
early systems investigated rule-based approaches to parsing the durations and orderings of events from the tense sand aspects of their verbs (hinrichs, 1987; <papid> P87-1002 </papid>webber, 1987; <papid> P87-1021 </papid>song and cohen, 1988; passonneau, 1988).<papid> J88-2005 </papid></citsent>
<aftsection>
<nextsent>allen (1984) and steedman (1995) focus on distinguishing between achievements (when an event culminates in result, such as john builds house) and processes (such as walking).
</nextsent>
<nextsent>more figure 1: screen shot of our story encoding interface.
</nextsent>
<nextsent>recent work has centered on markup languages for complex temporal information (mani, 2004)and corpus-based (statistical) models for predicting temporal relationships on unseen text (mani et al., 2006; <papid> P06-1095 </papid>lapata and lascarides, 2006).</nextsent>
<nextsent>our annotation interface requires fast realizerthat can be easily integrated into an interactive, on line encoding tool.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4216">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there has been intense interest in the interpretation of tense and aspect into formal understanding of the ordering and duration of events.
</prevsent>
<prevsent>this work has been in both linguistics (dowty,1979; nerbonne, 1986; vlach, 1993) and natural language understanding.
</prevsent>
</prevsection>
<citsent citstr=" J88-2005 ">
early systems investigated rule-based approaches to parsing the durations and orderings of events from the tense sand aspects of their verbs (hinrichs, 1987; <papid> P87-1002 </papid>webber, 1987; <papid> P87-1021 </papid>song and cohen, 1988; passonneau, 1988).<papid> J88-2005 </papid></citsent>
<aftsection>
<nextsent>allen (1984) and steedman (1995) focus on distinguishing between achievements (when an event culminates in result, such as john builds house) and processes (such as walking).
</nextsent>
<nextsent>more figure 1: screen shot of our story encoding interface.
</nextsent>
<nextsent>recent work has centered on markup languages for complex temporal information (mani, 2004)and corpus-based (statistical) models for predicting temporal relationships on unseen text (mani et al., 2006; <papid> P06-1095 </papid>lapata and lascarides, 2006).</nextsent>
<nextsent>our annotation interface requires fast realizerthat can be easily integrated into an interactive, on line encoding tool.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4217">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>allen (1984) and steedman (1995) focus on distinguishing between achievements (when an event culminates in result, such as john builds house) and processes (such as walking).
</prevsent>
<prevsent>more figure 1: screen shot of our story encoding interface.
</prevsent>
</prevsection>
<citsent citstr=" P06-1095 ">
recent work has centered on markup languages for complex temporal information (mani, 2004)and corpus-based (statistical) models for predicting temporal relationships on unseen text (mani et al., 2006; <papid> P06-1095 </papid>lapata and lascarides, 2006).</citsent>
<aftsection>
<nextsent>our annotation interface requires fast realizerthat can be easily integrated into an interactive, on line encoding tool.
</nextsent>
<nextsent>we found that developing acustom realizer as module to our java-based system was preferable to integrating large, general purpose system such as kpml/nigel (matthiessen and bateman, 1991) or fuf/surge (elhadadand robin, 1996).<papid> W96-0501 </papid></nextsent>
<nextsent>these realizers, along with re alpro (lavoie and rambow, 1997), <papid> A97-1039 </papid>accept tense as parameter, but do not calculate it from semantic representation of overlapping time intervals such as ours (though the nigel grammar can calculate tense from speech, event, and reference time or derings, discussed below).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4218">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recent work has centered on markup languages for complex temporal information (mani, 2004)and corpus-based (statistical) models for predicting temporal relationships on unseen text (mani et al., 2006; <papid> P06-1095 </papid>lapata and lascarides, 2006).</prevsent>
<prevsent>our annotation interface requires fast realizerthat can be easily integrated into an interactive, on line encoding tool.</prevsent>
</prevsection>
<citsent citstr=" W96-0501 ">
we found that developing acustom realizer as module to our java-based system was preferable to integrating large, general purpose system such as kpml/nigel (matthiessen and bateman, 1991) or fuf/surge (elhadadand robin, 1996).<papid> W96-0501 </papid></citsent>
<aftsection>
<nextsent>these realizers, along with re alpro (lavoie and rambow, 1997), <papid> A97-1039 </papid>accept tense as parameter, but do not calculate it from semantic representation of overlapping time intervals such as ours (though the nigel grammar can calculate tense from speech, event, and reference time or derings, discussed below).</nextsent>
<nextsent>the statistically trained fergus (chen et al, 2002) <papid> C02-1138 </papid>contrasts with our rule-based approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4219">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our annotation interface requires fast realizerthat can be easily integrated into an interactive, on line encoding tool.
</prevsent>
<prevsent>we found that developing acustom realizer as module to our java-based system was preferable to integrating large, general purpose system such as kpml/nigel (matthiessen and bateman, 1991) or fuf/surge (elhadadand robin, 1996).<papid> W96-0501 </papid></prevsent>
</prevsection>
<citsent citstr=" A97-1039 ">
these realizers, along with re alpro (lavoie and rambow, 1997), <papid> A97-1039 </papid>accept tense as parameter, but do not calculate it from semantic representation of overlapping time intervals such as ours (though the nigel grammar can calculate tense from speech, event, and reference time or derings, discussed below).</citsent>
<aftsection>
<nextsent>the statistically trained fergus (chen et al, 2002) <papid> C02-1138 </papid>contrasts with our rule-based approach.</nextsent>
<nextsent>dorr and gaasterland (1995) and grote (1998) <papid> W98-0304 </papid>focus on generating temporal connectives, such as before, based on the relative times and durations of two events; gagnon and lapalme (1996) <papid> J96-1004 </papid>focus on temporal adverbials (e.g., when to insert known time of day for an event).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4220">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we found that developing acustom realizer as module to our java-based system was preferable to integrating large, general purpose system such as kpml/nigel (matthiessen and bateman, 1991) or fuf/surge (elhadadand robin, 1996).<papid> W96-0501 </papid></prevsent>
<prevsent>these realizers, along with re alpro (lavoie and rambow, 1997), <papid> A97-1039 </papid>accept tense as parameter, but do not calculate it from semantic representation of overlapping time intervals such as ours (though the nigel grammar can calculate tense from speech, event, and reference time or derings, discussed below).</prevsent>
</prevsection>
<citsent citstr=" C02-1138 ">
the statistically trained fergus (chen et al, 2002) <papid> C02-1138 </papid>contrasts with our rule-based approach.</citsent>
<aftsection>
<nextsent>dorr and gaasterland (1995) and grote (1998) <papid> W98-0304 </papid>focus on generating temporal connectives, such as before, based on the relative times and durations of two events; gagnon and lapalme (1996) <papid> J96-1004 </papid>focus on temporal adverbials (e.g., when to insert known time of day for an event).</nextsent>
<nextsent>by comparison, we extend our approach to cover direct/indirect speech and the subjunctive/conditional forms, which theydo not report implementing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4221">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>these realizers, along with re alpro (lavoie and rambow, 1997), <papid> A97-1039 </papid>accept tense as parameter, but do not calculate it from semantic representation of overlapping time intervals such as ours (though the nigel grammar can calculate tense from speech, event, and reference time or derings, discussed below).</prevsent>
<prevsent>the statistically trained fergus (chen et al, 2002) <papid> C02-1138 </papid>contrasts with our rule-based approach.</prevsent>
</prevsection>
<citsent citstr=" W98-0304 ">
dorr and gaasterland (1995) and grote (1998) <papid> W98-0304 </papid>focus on generating temporal connectives, such as before, based on the relative times and durations of two events; gagnon and lapalme (1996) <papid> J96-1004 </papid>focus on temporal adverbials (e.g., when to insert known time of day for an event).</citsent>
<aftsection>
<nextsent>by comparison, we extend our approach to cover direct/indirect speech and the subjunctive/conditional forms, which theydo not report implementing.
</nextsent>
<nextsent>while our work focuses on english, yang and bateman (2009) <papid> P09-1071 </papid>describe recent system for generating chinese aspect expressions based on time interval representation, using kpml as their surface realizer.several other projects run tangential to our interactive narrative encoding project.</nextsent>
<nextsent>callaway and lesters storybook (2002) aims to improve fluency and discourse cohesion in realizing formally encoded narratives; ligozat and zock(1992) <papid> C92-2073 </papid>allow users to interactively construct sentences in various temporal scenarios through graphical interface.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4222">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>these realizers, along with re alpro (lavoie and rambow, 1997), <papid> A97-1039 </papid>accept tense as parameter, but do not calculate it from semantic representation of overlapping time intervals such as ours (though the nigel grammar can calculate tense from speech, event, and reference time or derings, discussed below).</prevsent>
<prevsent>the statistically trained fergus (chen et al, 2002) <papid> C02-1138 </papid>contrasts with our rule-based approach.</prevsent>
</prevsection>
<citsent citstr=" J96-1004 ">
dorr and gaasterland (1995) and grote (1998) <papid> W98-0304 </papid>focus on generating temporal connectives, such as before, based on the relative times and durations of two events; gagnon and lapalme (1996) <papid> J96-1004 </papid>focus on temporal adverbials (e.g., when to insert known time of day for an event).</citsent>
<aftsection>
<nextsent>by comparison, we extend our approach to cover direct/indirect speech and the subjunctive/conditional forms, which theydo not report implementing.
</nextsent>
<nextsent>while our work focuses on english, yang and bateman (2009) <papid> P09-1071 </papid>describe recent system for generating chinese aspect expressions based on time interval representation, using kpml as their surface realizer.several other projects run tangential to our interactive narrative encoding project.</nextsent>
<nextsent>callaway and lesters storybook (2002) aims to improve fluency and discourse cohesion in realizing formally encoded narratives; ligozat and zock(1992) <papid> C92-2073 </papid>allow users to interactively construct sentences in various temporal scenarios through graphical interface.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4223">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>dorr and gaasterland (1995) and grote (1998) <papid> W98-0304 </papid>focus on generating temporal connectives, such as before, based on the relative times and durations of two events; gagnon and lapalme (1996) <papid> J96-1004 </papid>focus on temporal adverbials (e.g., when to insert known time of day for an event).</prevsent>
<prevsent>by comparison, we extend our approach to cover direct/indirect speech and the subjunctive/conditional forms, which theydo not report implementing.</prevsent>
</prevsection>
<citsent citstr=" P09-1071 ">
while our work focuses on english, yang and bateman (2009) <papid> P09-1071 </papid>describe recent system for generating chinese aspect expressions based on time interval representation, using kpml as their surface realizer.several other projects run tangential to our interactive narrative encoding project.</citsent>
<aftsection>
<nextsent>callaway and lesters storybook (2002) aims to improve fluency and discourse cohesion in realizing formally encoded narratives; ligozat and zock(1992) <papid> C92-2073 </papid>allow users to interactively construct sentences in various temporal scenarios through graphical interface.</nextsent>
<nextsent>3.1 temporal knowledge.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4224">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>by comparison, we extend our approach to cover direct/indirect speech and the subjunctive/conditional forms, which theydo not report implementing.
</prevsent>
<prevsent>while our work focuses on english, yang and bateman (2009) <papid> P09-1071 </papid>describe recent system for generating chinese aspect expressions based on time interval representation, using kpml as their surface realizer.several other projects run tangential to our interactive narrative encoding project.</prevsent>
</prevsection>
<citsent citstr=" C92-2073 ">
callaway and lesters storybook (2002) aims to improve fluency and discourse cohesion in realizing formally encoded narratives; ligozat and zock(1992) <papid> C92-2073 </papid>allow users to interactively construct sentences in various temporal scenarios through graphical interface.</citsent>
<aftsection>
<nextsent>3.1 temporal knowledge.
</nextsent>
<nextsent>the propositions that we aim to realize take the form of predicate, one or more arguments, zeroor more attached modifiers (either negation operator or an adverbial, which is itself proposition),and an assignment in time.
</nextsent>
<nextsent>each argument is associated with semantic role (such as agent or experiencer), and may include nouns (such as char acters) or other propositions.
</nextsent>
<nextsent>in our implemented system, the set of predicates available to the annotator is adapted from the verbnet (kingsbury and palmer, 2002) and wordnet (fellbaum, 1998) linguistic databanks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4225">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> expressing single events.  </section>
<citcontext>
<prevsection>
<prevsent>actions that culminate ina state change (john built the house), and activi ties?
</prevsent>
<prevsent>that are more continuous and divisible (john read book for an hour) (dowty, 1979).
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
prior work in temporal connectives has taken advanta geof lexical information to determine the correct situation and assign aspect appropriately (moens and steedman, 1988; <papid> J88-2003 </papid>dorr and gaasterland, 1995).</citsent>
<aftsection>
<nextsent>in our case, we only distinguish between actions and stat ives, based on information from wordnet and verbnet.
</nextsent>
<nextsent>we use separate table for statives; it is similar to table 2, except the constructions replace verb conjugations with insertions of be, been, being, was, were, felt, and so on (with the latter applying to affective states).
</nextsent>
<nextsent>we do not currently distinguish between achievements and activities in selecting tense and aspect, except that the annotator is tasked with manually?
</nextsent>
<nextsent>indicating new state when an event culminates in one (e.g., the house was complete).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4226">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> alternate timelines and modalities.  </section>
<citcontext>
<prevsection>
<prevsent>this section covers more complex situations involving alternate timelines?
</prevsent>
<prevsent>the feature of our representation by which proposition in the main timeline can refer to second frame of time.
</prevsent>
</prevsection>
<citsent citstr=" W04-0208 ">
other models of time have supported similar encapsulations (crouch and pulman, 1993; mani and pustejovsky, 2004).<papid> W04-0208 </papid></citsent>
<aftsection>
<nextsent>the alternate timeline can contain references to actual events or modal events (imag ined, obligated, desired, planned, etc.) in the past the future with respect to its point of attachment on speech r?
</nextsent>
<nextsent>r hunger e?
</nextsent>
<nextsent>buy e?
</nextsent>
<nextsent>hunger reality alternate figure 2: schematic of speech act attaching to alternate timeline with hypothetical action.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4227">
<title id=" W10-4205.xml">tense and aspect assignment in narrative discourse </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>one may take these results and map them fro mother time representations with similar specifica tions.the second result is set of syntactic constructions for realizing these permutations in our story encoding interface.
</prevsent>
<prevsent>our focus here, as we have noted, is not fluency, but surface-level rendering that reflects the relationships (and, at times,the ambiguities) present in the conceptual encoding.
</prevsent>
</prevsection>
<citsent citstr=" P09-4003 ">
we consider variations in modality, such as an indicative reading as opposed to conditional or subjunctive reading, to be at the level of there alizer and not another class of tenses.we have run collection project with our encoding interface and can report success in the tools usability (elson and mckeown, 2009).<papid> P09-4003 </papid></citsent>
<aftsection>
<nextsent>two annotators each encoded 20 fables into the formal representation, with their only exposure to the semantic encodings being through the reference text generator (as in figure 1).
</nextsent>
<nextsent>both annotators became comfortable with the tool after period of training; in surveys that they completed after each task, they gave likert-scale usability scores of 4.25 and 4.30 (averaged over 20 tasks, with 5 meaning easiest to use?).
</nextsent>
<nextsent>these scores are not specific to the generation component, but they suggest that annotators could derive satisfactory tenses from their semantic structures.
</nextsent>
<nextsent>the most frequently cited deficiency in the model in terms of time was the inability to assign reference times to states and intervals (such as the next morning).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4228">
<title id=" W11-1002.xml">structured vs flat semantic role representations for machine translation evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the new results also show that, without flattening the structure of semantic frames, weighting the degree of each frames contribution gives hmeanthigher correlations than the previously best performing flattened model, as well as hter.
</prevsent>
<prevsent>in this paper we provide more concrete answer to the question: what would be better representation, structured or flat, of the roles in semantic frames to be used in semantic machine translation (mt) evaluation metric?
</prevsent>
</prevsection>
<citsent citstr=" W10-3807 ">
we compare recent studies on the meant family of semantic role labeling (srl) based mt evaluation metrics (lo andwu, 2010<papid> W10-3807 </papid>a,b, 2011a,b) by (1) contrasting their variations in semantic role representation and observing disturbing comparative results indicating that segregating the event frames in structured role representation actually damages correlation against human adequacy judgments and (2) showing how srl based mt evaluation can be improved beyond the currentstate-of-the-art compared to previous meant variants as well as hter, through the introduction of simple weighting scheme that reflects the degree of contribution of each semantic frame to the overall sentence.</citsent>
<aftsection>
<nextsent>the weighting scheme we propose usesa simple length-based heuristic that reflects the assumption that semantic frame that covers more tokens contributes more to the overall sentence translation.
</nextsent>
<nextsent>we demonstrate empirically that when the degree of each frames contribution to its sentence istaken into account, the properly structured role representation is more accurate and intuitive than the flattened role representation for srl mt evaluation metrics.
</nextsent>
<nextsent>for years, the task of measuring the performance of mt systems has been dominated by lexical ngram based machine translation evaluation metrics, such as bleu (papineni et al, 2002), <papid> P02-1040 </papid>nist (doddington, 2002), meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>per (tillmann et al, 1997), cder (leusch et al, 2006) <papid> E06-1031 </papid>and wer (nieen et al, 2000).</nextsent>
<nextsent>these metrics are excellent at ranking overall systems by averaging their scores over entire documents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4229">
<title id=" W11-1002.xml">structured vs flat semantic role representations for machine translation evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the weighting scheme we propose usesa simple length-based heuristic that reflects the assumption that semantic frame that covers more tokens contributes more to the overall sentence translation.
</prevsent>
<prevsent>we demonstrate empirically that when the degree of each frames contribution to its sentence istaken into account, the properly structured role representation is more accurate and intuitive than the flattened role representation for srl mt evaluation metrics.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
for years, the task of measuring the performance of mt systems has been dominated by lexical ngram based machine translation evaluation metrics, such as bleu (papineni et al, 2002), <papid> P02-1040 </papid>nist (doddington, 2002), meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>per (tillmann et al, 1997), cder (leusch et al, 2006) <papid> E06-1031 </papid>and wer (nieen et al, 2000).</citsent>
<aftsection>
<nextsent>these metrics are excellent at ranking overall systems by averaging their scores over entire documents.
</nextsent>
<nextsent>how ever, as mt systems improve, the shortcomings of such metrics are becoming more apparent.
</nextsent>
<nextsent>though containing roughly the correct words, mt output at the sentence remains often quite incomprehensible, and fails to preserve the meaning of the input.
</nextsent>
<nextsent>this results from the fact that n-gram based metrics arenot as reliable at ranking the adequacy of translations of individual sentences, and are particularly 10 poor at reflecting translation quality improvements involving more meaningful word sense or semantic frame decisions which human judges have no trouble distinguishing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4230">
<title id=" W11-1002.xml">structured vs flat semantic role representations for machine translation evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the weighting scheme we propose usesa simple length-based heuristic that reflects the assumption that semantic frame that covers more tokens contributes more to the overall sentence translation.
</prevsent>
<prevsent>we demonstrate empirically that when the degree of each frames contribution to its sentence istaken into account, the properly structured role representation is more accurate and intuitive than the flattened role representation for srl mt evaluation metrics.
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
for years, the task of measuring the performance of mt systems has been dominated by lexical ngram based machine translation evaluation metrics, such as bleu (papineni et al, 2002), <papid> P02-1040 </papid>nist (doddington, 2002), meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>per (tillmann et al, 1997), cder (leusch et al, 2006) <papid> E06-1031 </papid>and wer (nieen et al, 2000).</citsent>
<aftsection>
<nextsent>these metrics are excellent at ranking overall systems by averaging their scores over entire documents.
</nextsent>
<nextsent>how ever, as mt systems improve, the shortcomings of such metrics are becoming more apparent.
</nextsent>
<nextsent>though containing roughly the correct words, mt output at the sentence remains often quite incomprehensible, and fails to preserve the meaning of the input.
</nextsent>
<nextsent>this results from the fact that n-gram based metrics arenot as reliable at ranking the adequacy of translations of individual sentences, and are particularly 10 poor at reflecting translation quality improvements involving more meaningful word sense or semantic frame decisions which human judges have no trouble distinguishing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4231">
<title id=" W11-1002.xml">structured vs flat semantic role representations for machine translation evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the weighting scheme we propose usesa simple length-based heuristic that reflects the assumption that semantic frame that covers more tokens contributes more to the overall sentence translation.
</prevsent>
<prevsent>we demonstrate empirically that when the degree of each frames contribution to its sentence istaken into account, the properly structured role representation is more accurate and intuitive than the flattened role representation for srl mt evaluation metrics.
</prevsent>
</prevsection>
<citsent citstr=" E06-1031 ">
for years, the task of measuring the performance of mt systems has been dominated by lexical ngram based machine translation evaluation metrics, such as bleu (papineni et al, 2002), <papid> P02-1040 </papid>nist (doddington, 2002), meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>per (tillmann et al, 1997), cder (leusch et al, 2006) <papid> E06-1031 </papid>and wer (nieen et al, 2000).</citsent>
<aftsection>
<nextsent>these metrics are excellent at ranking overall systems by averaging their scores over entire documents.
</nextsent>
<nextsent>how ever, as mt systems improve, the shortcomings of such metrics are becoming more apparent.
</nextsent>
<nextsent>though containing roughly the correct words, mt output at the sentence remains often quite incomprehensible, and fails to preserve the meaning of the input.
</nextsent>
<nextsent>this results from the fact that n-gram based metrics arenot as reliable at ranking the adequacy of translations of individual sentences, and are particularly 10 poor at reflecting translation quality improvements involving more meaningful word sense or semantic frame decisions which human judges have no trouble distinguishing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4232">
<title id=" W11-1002.xml">structured vs flat semantic role representations for machine translation evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>though containing roughly the correct words, mt output at the sentence remains often quite incomprehensible, and fails to preserve the meaning of the input.
</prevsent>
<prevsent>this results from the fact that n-gram based metrics arenot as reliable at ranking the adequacy of translations of individual sentences, and are particularly 10 poor at reflecting translation quality improvements involving more meaningful word sense or semantic frame decisions which human judges have no trouble distinguishing.
</prevsent>
</prevsection>
<citsent citstr=" W06-3114 ">
callison-burch et al (2006) andkoehn and monz (2006), <papid> W06-3114 </papid>for example, study situations where bleu strongly disagrees with human judgment of translation quality.</citsent>
<aftsection>
<nextsent>newer avenues of research seek substitutes forn-gram based mt evaluation metrics that are better at evaluating translation adequacy, particularly at the sentence level.
</nextsent>
<nextsent>one line of research emphasizes more the structural correctness of translation.
</nextsent>
<nextsent>liu and gildea (2005) <papid> W05-0904 </papid>propose stm, metric based on syntactic structure, that addresses the failure of lexical similarity based metrics to evaluate translation grammaticality.</nextsent>
<nextsent>however, the problem remains thata grammatical translation can achieve high syntax based score yet still make significant errors arising from confusion of semantic roles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4233">
<title id=" W11-1002.xml">structured vs flat semantic role representations for machine translation evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>newer avenues of research seek substitutes forn-gram based mt evaluation metrics that are better at evaluating translation adequacy, particularly at the sentence level.
</prevsent>
<prevsent>one line of research emphasizes more the structural correctness of translation.
</prevsent>
</prevsection>
<citsent citstr=" W05-0904 ">
liu and gildea (2005) <papid> W05-0904 </papid>propose stm, metric based on syntactic structure, that addresses the failure of lexical similarity based metrics to evaluate translation grammaticality.</citsent>
<aftsection>
<nextsent>however, the problem remains thata grammatical translation can achieve high syntax based score yet still make significant errors arising from confusion of semantic roles.
</nextsent>
<nextsent>on the other hand,despite the fact that non-automatic, manually evaluated metrics, such as hter (snover et al, 2006), aremore adequacy oriented exhibit much higher correlation with human adequacy judgment, their high labor cost prohibits widespread use.
</nextsent>
<nextsent>there has also been work on explicitly evaluating mt adequacy by aggregating over very large set of linguistic features(gimenez and ma`rquez, 2007, 2008) and textual entailment (pado et al, 2009).
</nextsent>
<nextsent>a blueprint for more direct assessment of meaning preservation across translation was outlined by lo and wu (2010<papid> W10-3807 </papid>a), in which translation utility is manually evaluated with respect to the accuracy of semantic role labels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4238">
<title id=" W11-1002.xml">structured vs flat semantic role representations for machine translation evaluation </title>
<section> srl based mt evaluation metrics.  </section>
<citcontext>
<prevsection>
<prevsent>a blueprint for more direct assessment of meaning preservation across translation was outlined by lo and wu (2010<papid> W10-3807 </papid>a), in which translation utility is manually evaluated with respect to the accuracy of semantic role labels.</prevsent>
<prevsent>a good translation is one from which human readers may successfully understand at least the basic event structure who did what to whom, when, where and why?</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
(pradhan et al, 2004)<papid> N04-1030 </papid>which represents the most essential meaning of the source utterances.</citsent>
<aftsection>
<nextsent>adopting this principle,the meant family of metrics compare the semantic frames in reference translations against those thatcan be reconstructed from machine translation out put.
</nextsent>
<nextsent>preliminary results reported in (lo and wu,2010<papid> W10-3807 </papid>b) confirm that the blueprint model outperforms bleu and similar n-gram oriented evaluation metrics in correlation against human adequacy judgments, but does not fare as well as hter.</nextsent>
<nextsent>the more complete study of lo and wu (2011<papid> P11-1023 </papid>a) introduces meant and its human variants hmeant, which implement an extended version of blueprint methodology.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4240">
<title id=" W11-1002.xml">structured vs flat semantic role representations for machine translation evaluation </title>
<section> srl based mt evaluation metrics.  </section>
<citcontext>
<prevsection>
<prevsent>adopting this principle,the meant family of metrics compare the semantic frames in reference translations against those thatcan be reconstructed from machine translation out put.
</prevsent>
<prevsent>preliminary results reported in (lo and wu,2010<papid> W10-3807 </papid>b) confirm that the blueprint model outperforms bleu and similar n-gram oriented evaluation metrics in correlation against human adequacy judgments, but does not fare as well as hter.</prevsent>
</prevsection>
<citsent citstr=" P11-1023 ">
the more complete study of lo and wu (2011<papid> P11-1023 </papid>a) introduces meant and its human variants hmeant, which implement an extended version of blueprint methodology.</citsent>
<aftsection>
<nextsent>experimental results show that hmeant correlates against human adequacy judgments as well as the more expensive hter,even though hmeant can be evaluated using low cost untrained monolingual semantic role annotators while still maintaining high inter-annotator agreement (both are far superior to bleu or other surface oriented evaluation metrics).
</nextsent>
<nextsent>the study also shows that replacing the human semantic role la belers with an automatic shallow semantic parser yields an approximation that is still vastly superior to bleu while remaining about 80% as closely correlated with human adequacy judgments as hter.along with additional improvements to the accuracy of the meant family of metrics, lo and wu(2011<papid> P11-1023 </papid>b) study the impact of each individual semantic role to the metrics correlation against human adequacy judgments, as well as the time cost for humans to reconstruct the semantic frames and compare the translation accuracy of the role fillers.in general, the meant family of srl mt evaluation metrics (lo and wu, 2011<papid> P11-1023 </papid>a,b) evaluate the translation utility as follows.</nextsent>
<nextsent>first, semantic role labeling is performed (either manually or automat ically) on both the reference translation (ref) and the machine translation output (mt) to obtain the semantic frame structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4273">
<title id=" W11-1002.xml">structured vs flat semantic role representations for machine translation evaluation </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>translation accuracy, which prevents controlled consistent environment for the comparative experiments that the present work focuses on.
</prevsent>
<prevsent>the evaluation data for our experiments consists of 40 sentences randomly drawn from the darpa gale program phase 2.5 newswire evaluation corpus containing chinese input sentence, english reference translations, and the machine translation from three different state-of-the-art gale systems.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
the chinese and the english reference translation have both been annotated with gold standard propbank (palmer et al, 2005) <papid> J05-1004 </papid>semantic role labels.</citsent>
<aftsection>
<nextsent>theweightswpred, wcore, wadj, wj and wpartial can be estimated by optimizing correlation against human adequacy judgments, using any of themany standard optimization search techniques.
</nextsent>
<nextsent>in the work of lo and 13 figure 3: the flat role representation for the meant family of metrics as proposed in lo and wu (2011<papid> P11-1023 </papid>b) . wu (2011b), the correlations of all individual roles with the human adequacy judgments were found to be non-negative, therefore we found grid search to be quite adequate for estimating the weights.</nextsent>
<nextsent>we use linear weighting because we would like to keep the metrics interpretation simple and intuitive.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4292">
<title id=" W11-0709.xml">why is sxsw trending exploring multiple text sources for twitter topic summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there is not much previous work on summarizing the twitter topics.
</prevsent>
<prevsent>most previous summarization literature focused on the written text domain, as driven by the annual evaluation tracks of the duc (doc ument understanding conference) and tac (text analysis conference).
</prevsent>
</prevsection>
<citsent citstr=" J02-4003 ">
to some extent, twitter topic summarization is related to spoken document summarization, since both tasks deal with the conversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens orhigh word error rate, etc. to summarize the spoken text, (zechner, 2002) <papid> J02-4003 </papid>aimed to address problems related to disfluencies, extraction units, cross speaker coherence, etc.</citsent>
<aftsection>
<nextsent>(maskey and hirschberg, 2005; murray et al, 2006; <papid> N06-1047 </papid>galley, 2006; <papid> W06-1643 </papid>xie et al., 2008; liu and liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations.</nextsent>
<nextsent>for microblog summarization, (sharifi et al,2010<papid> N10-1100 </papid>a) proposed phrase reinforcement (pr) algorithm to summarize the twitter topic in one sen tence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4293">
<title id=" W11-0709.xml">why is sxsw trending exploring multiple text sources for twitter topic summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most previous summarization literature focused on the written text domain, as driven by the annual evaluation tracks of the duc (doc ument understanding conference) and tac (text analysis conference).
</prevsent>
<prevsent>to some extent, twitter topic summarization is related to spoken document summarization, since both tasks deal with the conversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens orhigh word error rate, etc. to summarize the spoken text, (zechner, 2002) <papid> J02-4003 </papid>aimed to address problems related to disfluencies, extraction units, cross speaker coherence, etc.</prevsent>
</prevsection>
<citsent citstr=" N06-1047 ">
(maskey and hirschberg, 2005; murray et al, 2006; <papid> N06-1047 </papid>galley, 2006; <papid> W06-1643 </papid>xie et al., 2008; liu and liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations.</citsent>
<aftsection>
<nextsent>for microblog summarization, (sharifi et al,2010<papid> N10-1100 </papid>a) proposed phrase reinforcement (pr) algorithm to summarize the twitter topic in one sen tence.</nextsent>
<nextsent>the algorithm builds word graph using the topic phrase as the root node; each word node is weighted in proportion to its distance to the root and the corresponding phrase frequency.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4294">
<title id=" W11-0709.xml">why is sxsw trending exploring multiple text sources for twitter topic summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most previous summarization literature focused on the written text domain, as driven by the annual evaluation tracks of the duc (doc ument understanding conference) and tac (text analysis conference).
</prevsent>
<prevsent>to some extent, twitter topic summarization is related to spoken document summarization, since both tasks deal with the conversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens orhigh word error rate, etc. to summarize the spoken text, (zechner, 2002) <papid> J02-4003 </papid>aimed to address problems related to disfluencies, extraction units, cross speaker coherence, etc.</prevsent>
</prevsection>
<citsent citstr=" W06-1643 ">
(maskey and hirschberg, 2005; murray et al, 2006; <papid> N06-1047 </papid>galley, 2006; <papid> W06-1643 </papid>xie et al., 2008; liu and liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations.</citsent>
<aftsection>
<nextsent>for microblog summarization, (sharifi et al,2010<papid> N10-1100 </papid>a) proposed phrase reinforcement (pr) algorithm to summarize the twitter topic in one sen tence.</nextsent>
<nextsent>the algorithm builds word graph using the topic phrase as the root node; each word node is weighted in proportion to its distance to the root and the corresponding phrase frequency.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4295">
<title id=" W11-0709.xml">why is sxsw trending exploring multiple text sources for twitter topic summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to some extent, twitter topic summarization is related to spoken document summarization, since both tasks deal with the conversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens orhigh word error rate, etc. to summarize the spoken text, (zechner, 2002) <papid> J02-4003 </papid>aimed to address problems related to disfluencies, extraction units, cross speaker coherence, etc.</prevsent>
<prevsent>(maskey and hirschberg, 2005; murray et al, 2006; <papid> N06-1047 </papid>galley, 2006; <papid> W06-1643 </papid>xie et al., 2008; liu and liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations.</prevsent>
</prevsection>
<citsent citstr=" N10-1100 ">
for microblog summarization, (sharifi et al,2010<papid> N10-1100 </papid>a) proposed phrase reinforcement (pr) algorithm to summarize the twitter topic in one sen tence.</citsent>
<aftsection>
<nextsent>the algorithm builds word graph using the topic phrase as the root node; each word node is weighted in proportion to its distance to the root and the corresponding phrase frequency.
</nextsent>
<nextsent>the summary sentence is selected as one of the highest weighted paths in the graph.
</nextsent>
<nextsent>(sharifi et al, 2010<papid> N10-1100 </papid>b; inouye,2010) introduced hybrid tf-idf approach to extract one- or multiple-sentence summary for eachtopic.</nextsent>
<nextsent>sentences were ranked according to the average tf-idf score of the consisting words; top weighted sentences were iteratively extracted, but excluding those that have high cosine similarity with the existing summary sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4302">
<title id=" W11-0709.xml">why is sxsw trending exploring multiple text sources for twitter topic summarization </title>
<section> summarization system.  </section>
<citcontext>
<prevsection>
<prevsent>for each of the topic phrases, our goal is to generate short textual summary that can best convey the main ideas of the topic contents.
</prevsent>
<prevsent>we explore and compare multiple text sources as summarization input, including the user-contributed tweets, web contents linked from the tweets, as well as combination of the two sources.
</prevsent>
</prevsection>
<citsent citstr=" N10-1132 ">
the concept-based optimization approach (gillick et al, 2009; xie et al, 2009; murray et al, 2010) <papid> N10-1132 </papid>was employed for selecting informative summary sentences and minimizing the redundancy.</citsent>
<aftsection>
<nextsent>note that our focus of this paper is not developing new summarization systems, but rather utilizing and integrating different text sources for generating more informative twitter topic summaries.
</nextsent>
<nextsent>4.1 concept-based optimization framework.
</nextsent>
<nextsent>concept-based summarization approach first extracts set of important concepts for each topic, then selects collection of sentences that can cover as many important concepts as possible, while with inthe specified length limit.
</nextsent>
<nextsent>this idea is realized using the integer linear programming-based (ilp) optimization framework, with objective function set to maximize the sum of the weighted concepts: max ? wici 68 where ci is binary variable indicating whether the concept is covered by the summary; wi is the weight assigned to ci.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4303">
<title id=" W11-0709.xml">why is sxsw trending exploring multiple text sources for twitter topic summarization </title>
<section> summarization system.  </section>
<citcontext>
<prevsection>
<prevsent>this scheme is similar to the tf-idf measure.
</prevsent>
<prevsent>this way we can select the salient urls for each topic while avoiding the spam urls.
</prevsent>
</prevsection>
<citsent citstr=" A97-1004 ">
the contents of these urls were collected and only distinct web pages were retained.we use an html parser3 to extract the textual contents, and perform sentence segmentation (reynar and ratnaparkhi, 1997) <papid> A97-1004 </papid>on the parsed web pages.</citsent>
<aftsection>
<nextsent>all the pages corresponding to the same topic were sorted by the date they were first cited in the tweets.
</nextsent>
<nextsent>these web pages were taken as another input text source for the summarization system, denoted as web?.
</nextsent>
<nextsent>4.2.4 combining tweets and web contents we expect that taking advantage of both tweets and linked web contents would benefit the topic summarization system.
</nextsent>
<nextsent>consolidating the distinct text sources may help boost the weight of key concepts and eliminate the spam information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4310">
<title id=" W11-0709.xml">why is sxsw trending exploring multiple text sources for twitter topic summarization </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for the baseline, the summary length is altered according to the sentence or word-constraint.
</prevsent>
<prevsent>the last summary tweet is cut in the middle if it exceeds the word limit.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
the rouge-1 f-scores (lin, 2004) <papid> W04-1013 </papid>are used to measure the n-gram (n=1) overlap between the system summaries and reference summaries.</citsent>
<aftsection>
<nextsent>since the rouge scores may not correlate well with the human judgments (liu and liu, 2010b), we also performed human evaluation by asking annotators toscore both the system and reference summaries regarding the linguistic quality and content responsiveness, in the hope this will benefit future research in this direction.
</nextsent>
<nextsent>5.2 automatic evaluation.
</nextsent>
<nextsent>we present the results (rouge-1 f-measure) for the general topics in table 3.
</nextsent>
<nextsent>rouge-2 and general topics r-1 f(%) refsum input source render #sent #word cov(%) orig tweets orig 29.53 30.21 94.81 norm 29.41 30.21 94.81 norm tweets norm 29.69 30.35 94.60 web 24.32 25.07 63.74 web + orig tweets 29.58 30.44 95.37 web + norm tweets 29.66 30.54 95.16 orig tweets (sharifi et al, 2010<papid> N10-1100 </papid>b) 24.37 25.68 94.81 table 3: rouge-1 f-measure and reference summary coverage scores for general topics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4335">
<title id=" W10-4137.xml">crf based experiments for cross domain chinese word segmentation at cipssighan2010 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after lot of researches, chinese word segmentation has achieved high accuracy.
</prevsent>
<prevsent>many methods have been presented, among which the crfs model has attracted more and more attention.
</prevsent>
</prevsection>
<citsent citstr=" I08-4017 ">
zhaos group used the crfs model in the task of chinese word segmentation in bakeoff-4 and they ranked at the top in all closed tests of word segmentation (zhao and kit, 2008).<papid> I08-4017 </papid></citsent>
<aftsection>
<nextsent>the crfs model has been widely used because of its excellent performance.
</nextsent>
<nextsent>however, finding better segmentation algorithm for the out-of-domain text is the focus of cip-sighan-2010 bakeoff.
</nextsent>
<nextsent>we still consider word segmentation as sequence labeling problem.
</nextsent>
<nextsent>what we concern is how to use the unlabeled corpora to enrich the supervised crfs learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4336">
<title id=" W10-4137.xml">crf based experiments for cross domain chinese word segmentation at cipssighan2010 </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>y is the label sequence, is observation sequence, zx is normalization term that makes the probability of all state sequences sum to one; fk(yt-1, yt, t) is often binary-valued feature function and ? is the weight of fk.
</prevsent>
<prevsent>in our system, we choose six types of tags according to character position in word.
</prevsent>
</prevsection>
<citsent citstr=" W06-0127 ">
according to zhaos work (zhao et al, 2006<papid> W06-0127 </papid>a), the 6 tag set enables our system to generate better crf model than the 4-tag set.</citsent>
<aftsection>
<nextsent>in our experiments, we test both the 6-tag set and the 4-tag set, and the 6-tag set truly has better result.
</nextsent>
<nextsent>the 6-tag set is defined as below: = {b, b2, b3, m, e, s} here b, b2, b3, m, represent the first, second, third, continuing and end character positions in multi-character word, and is the sin gle-character word tag.
</nextsent>
<nextsent>we adopt 6 n-gram feature templates as features.
</nextsent>
<nextsent>some researches have proved that the combination of 6-tag set and 6 n-gram feature template can achieve better performance (zhao et al, 2006<papid> W06-0127 </papid>a; zhao et al, 2006<papid> W06-0127 </papid>b; zhao and kit, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4345">
<title id=" W10-4137.xml">crf based experiments for cross domain chinese word segmentation at cipssighan2010 </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 segmentation features.
</prevsent>
<prevsent>in our system, we only take advantage of the features of the words.
</prevsent>
</prevsection>
<citsent citstr=" J04-1004 ">
we try to add other features to our experiments such as av feature (feng et al, 2004<papid> J04-1004 </papid>a; feng et al, 2004<papid> J04-1004 </papid>b; hai zhao et al, 2007) with the expectation of improving the results.</citsent>
<aftsection>
<nextsent>but the results are not satisfying.
</nextsent>
<nextsent>we believe that the feature of words frequency may be an important factor, but how to use it is worth studying.
</nextsent>
<nextsent>so finding some meaningful and effective features is the crucial point.
</nextsent>
<nextsent>4.2 oov.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4349">
<title id=" W11-0123.xml">recognizing confinement in web texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(2) a. xylitol can prevent tooth decay.
</prevsent>
<prevsent>b. xylitol is not effective at all at preventing tooth decay.
</prevsent>
</prevsection>
<citsent citstr=" W07-1401 ">
a major task in the recognizing textual entailment (rte) challenge (giampiccolo et al (2007)) <papid> W07-1401 </papid>is classifying the semantic relation between text and hypothesis into entailment, contradiction, or unknown.</citsent>
<aftsection>
<nextsent>murakami et al (2009) report on the statement map project, the goal of which is to help internet users evaluate the credibility of information sources by analyzing supporting evidence from variety of viewpoints on their topics of interest and presenting them to users together with the supporting evidence in way that makes it clear how they are related.
</nextsent>
<nextsent>a variety of techniques have been successfully employed in the rte challenge in order to recognize instances of textual entailment.
</nextsent>
<nextsent>current afflication: rakuten institute of technology 215however, as far as we know, there have been no studies on recognizing sentences which specify conditions under which query applies, despite the fact that these relations are useful information for internetusers.
</nextsent>
<nextsent>such useful sentences are plentiful on the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4350">
<title id=" W11-0123.xml">recognizing confinement in web texts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we conduct empirical evaluation of recognition of the confinement relation between queries and sentences in japanese-language web texts.
</prevsent>
<prevsent>in rte research, only three types of relations are defined: entailment, contradiction, and unknown.
</prevsent>
</prevsection>
<citsent citstr=" P07-1058 ">
rte is an important task and has been the target of much research (szpektor et al (2007); <papid> P07-1058 </papid>sammons et al (2009)).</citsent>
<aftsection>
<nextsent>however, none of the previous research has introduced relations corresponding to confinement.
</nextsent>
<nextsent>cross-document structure theory (cst, radev (2000)) <papid> W00-1009 </papid>is another approach to recognizing semantic relations between sentences.</nextsent>
<nextsent>cst is an extended rhetorical structure analysis based on rhetorical structure theory (rst).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4351">
<title id=" W11-0123.xml">recognizing confinement in web texts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>rte is an important task and has been the target of much research (szpektor et al (2007); <papid> P07-1058 </papid>sammons et al (2009)).</prevsent>
<prevsent>however, none of the previous research has introduced relations corresponding to confinement.</prevsent>
</prevsection>
<citsent citstr=" W00-1009 ">
cross-document structure theory (cst, radev (2000)) <papid> W00-1009 </papid>is another approach to recognizing semantic relations between sentences.</citsent>
<aftsection>
<nextsent>cst is an extended rhetorical structure analysis based on rhetorical structure theory (rst).
</nextsent>
<nextsent>it attempts to describe the semantic relations between two or more sentences from different source documents that are related to the same topic.
</nextsent>
<nextsent>it defines 18 kinds of semantic relations between sentences.
</nextsent>
<nextsent>etoh and okumura (2005) constructed japanese cross-document relation corpus and defined 14 kinds of semantic relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4352">
<title id=" W11-0123.xml">recognizing confinement in web texts </title>
<section> proposed system.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 i. linguistic analysis in linguistic analysis, we conduct word segmentation, pos tagging, dependency parsing, and extended modality analysis.
</prevsent>
<prevsent>this linguistic analysis acts as the basis for alignment and semantic feature extraction.
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
for syntactic analysis, we identify words and pos tags with the japanese morphological analysermecab2, and we use the japanese dependency parser cabocha (kudo and matsumoto (2002)) <papid> W02-2016 </papid>to produce dependency trees.</citsent>
<aftsection>
<nextsent>we also conduct extended modality analysis using the resources provided by matsuyoshi et al (2010).
</nextsent>
<nextsent>5.2 ii.
</nextsent>
<nextsent>structural alignment.
</nextsent>
<nextsent>to identify the consequence of s0 in s1, we use structural alignment (mizuno et al (2010)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4353">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>we present system for fusing sentences which are drawn from the same source document but have different content.
</prevsent>
<prevsent>unlike previous work, our approach is supervised, training on real-world examples of sentences fused by professional journalists in the process of editing news articles.
</prevsent>
</prevsection>
<citsent citstr=" D08-1019 ">
like filippova and strube (2008), <papid> D08-1019 </papid>our system merges dependency graphs using integer linear programming.</citsent>
<aftsection>
<nextsent>however, instead of aligning the inputs as pre process, we integrate the tasks of finding an alignment and selecting merged sentence into joint optimization problem, and learn parameters for this optimization using structured online algorithm.
</nextsent>
<nextsent>evaluation by human judges shows that our technique produces fused sentences that are both informative and readable.
</nextsent>
<nextsent>sentence fusion is the process by which content from two or more original sentences is transformed into single output sentence.
</nextsent>
<nextsent>it is usually studied in the context of multi document summarization, since fusing similar sentences can avoid repetition of material which is shared by more than one input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4355">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we discuss our experimental evaluation and give results (section 6).
</prevsent>
<prevsent>previous work on sentence fusion examines the taskin the context of multi document summarization, targeting groups of sentences with mostly redundant content.
</prevsent>
</prevsection>
<citsent citstr=" J05-3002 ">
the pioneering work on fusion is barzilay and mckeown (2005), <papid> J05-3002 </papid>which introduces the framework used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into lattice, and then extract single, connected dependency tree as the output.</citsent>
<aftsection>
<nextsent>our work most closely follows filippova and strube (2008), <papid> D08-1019 </papid>which proposes using integer linear programming (ilp) for extraction of an output dependency tree.</nextsent>
<nextsent>ilp allows specification of grammaticality constraints in terms of dependency relationships (clarke and lapata, 2008), as opposed to previous fusion methods (barzilay and mckeown,2005; <papid> J05-3002 </papid>marsi and krahmer, 2005) <papid> W05-1612 </papid>which used language modeling to extract their output.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4358">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the pioneering work on fusion is barzilay and mckeown (2005), <papid> J05-3002 </papid>which introduces the framework used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into lattice, and then extract single, connected dependency tree as the output.</prevsent>
<prevsent>our work most closely follows filippova and strube (2008), <papid> D08-1019 </papid>which proposes using integer linear programming (ilp) for extraction of an output dependency tree.</prevsent>
</prevsection>
<citsent citstr=" W05-1612 ">
ilp allows specification of grammaticality constraints in terms of dependency relationships (clarke and lapata, 2008), as opposed to previous fusion methods (barzilay and mckeown,2005; <papid> J05-3002 </papid>marsi and krahmer, 2005) <papid> W05-1612 </papid>which used language modeling to extract their output.</citsent>
<aftsection>
<nextsent>in their ilp, filippova and strube (2008) <papid> D08-1019 </papid>optimize function based on syntactic importance scores learned from corpus of general text.</nextsent>
<nextsent>while similar methods have been used for the related task of sentence compression, improvements can be obtained using supervised learning (knight and marcu, 2000; turner and charniak, 2005; <papid> P05-1036 </papid>cohn and lapata, 2009) if suitable corpus of compressed sentences can be obtained.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4361">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ilp allows specification of grammaticality constraints in terms of dependency relationships (clarke and lapata, 2008), as opposed to previous fusion methods (barzilay and mckeown,2005; <papid> J05-3002 </papid>marsi and krahmer, 2005) <papid> W05-1612 </papid>which used language modeling to extract their output.</prevsent>
<prevsent>in their ilp, filippova and strube (2008) <papid> D08-1019 </papid>optimize function based on syntactic importance scores learned from corpus of general text.</prevsent>
</prevsection>
<citsent citstr=" P05-1036 ">
while similar methods have been used for the related task of sentence compression, improvements can be obtained using supervised learning (knight and marcu, 2000; turner and charniak, 2005; <papid> P05-1036 </papid>cohn and lapata, 2009) if suitable corpus of compressed sentences can be obtained.</citsent>
<aftsection>
<nextsent>this paper is the first we know of to adopt the supervised strategy for sentence fusion.for supervised learning to be effective, it is necessary to find or produce example data.
</nextsent>
<nextsent>previous work does produce some examples written by humans, though these are used during evaluation, not for learning (a large corpus of fusions (mckeown etal., 2010) <papid> N10-1044 </papid>was recently compiled as first step toward supervised fusion system).</nextsent>
<nextsent>however, they elicit these examples by asking experimental subjects to fuse selected input sentences?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4362">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while similar methods have been used for the related task of sentence compression, improvements can be obtained using supervised learning (knight and marcu, 2000; turner and charniak, 2005; <papid> P05-1036 </papid>cohn and lapata, 2009) if suitable corpus of compressed sentences can be obtained.</prevsent>
<prevsent>this paper is the first we know of to adopt the supervised strategy for sentence fusion.for supervised learning to be effective, it is necessary to find or produce example data.</prevsent>
</prevsection>
<citsent citstr=" N10-1044 ">
previous work does produce some examples written by humans, though these are used during evaluation, not for learning (a large corpus of fusions (mckeown etal., 2010) <papid> N10-1044 </papid>was recently compiled as first step toward supervised fusion system).</citsent>
<aftsection>
<nextsent>however, they elicit these examples by asking experimental subjects to fuse selected input sentences?
</nextsent>
<nextsent>the choice of which sentences to fuse is made by the system, not the subjects.
</nextsent>
<nextsent>in contrast, our dataset consists of sentences humans actually chose to fuse as part of practical writing task.
</nextsent>
<nextsent>moreover, our sentences have disparate content, while previous work focuses on sentences whose content mostly overlaps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4368">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> data and preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>out of total of 9007 sentences in the corpus, our bigram method finds that 175 were split and 132were merged, for total of 307.
</prevsent>
<prevsent>we take 92 examples for testing and 189 for training3.following previous work (barzilay and mckeown, 2005), <papid> J05-3002 </papid>we adopt labeled dependency format for our systems input.</prevsent>
</prevsection>
<citsent citstr=" A97-1004 ">
to produce this, we segment sentences with mxterminator (reynar and ratnaparkhi, 1997) <papid> A97-1004 </papid>and parse the corpus with the self trained charniak parser (mcclosky et al, 2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>wethen convert to dependencies and apply rules to simplify and label the graph.
</nextsent>
<nextsent>an example dependency graph is shown in figure 1.
</nextsent>
<nextsent>we augment the dependency tree by adding apotential dependency labeled relative clause?
</nextsent>
<nextsent>between each subject and its verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4369">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> data and preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>out of total of 9007 sentences in the corpus, our bigram method finds that 175 were split and 132were merged, for total of 307.
</prevsent>
<prevsent>we take 92 examples for testing and 189 for training3.following previous work (barzilay and mckeown, 2005), <papid> J05-3002 </papid>we adopt labeled dependency format for our systems input.</prevsent>
</prevsection>
<citsent citstr=" N06-1020 ">
to produce this, we segment sentences with mxterminator (reynar and ratnaparkhi, 1997) <papid> A97-1004 </papid>and parse the corpus with the self trained charniak parser (mcclosky et al, 2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>wethen convert to dependencies and apply rules to simplify and label the graph.
</nextsent>
<nextsent>an example dependency graph is shown in figure 1.
</nextsent>
<nextsent>we augment the dependency tree by adding apotential dependency labeled relative clause?
</nextsent>
<nextsent>between each subject and its verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4370">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> data and preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>be 2in few cases, this creates two examples which share sentence, since the editor sometimes splits content off from one sentence and merges it into another.
</prevsent>
<prevsent>3we originally had 100 testing and 207 training examples, but found 26 of our examples were spurious, caused by faulty sentence segmentation.
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
4words with the same part of speech whose similarity is greater than 3.0 according to the information-theoretic wordnet based similarity measure of resnik (1995), using the implementation of (pedersen et al, 2004).<papid> N04-3012 </papid></citsent>
<aftsection>
<nextsent>cause input sentence (1) is extremely short, but most sentences have more.
</nextsent>
<nextsent>bodies showed signs torture said left were they side highway chilpancingo police state north hour resort acapulco root root rel sbj obj pp of rel sbj pp by pp of pp in pp about pp of pp of thean aux obj sbj rel merge?
</nextsent>
<nextsent>figure 1: the labeled dependency graph for sentences (1)and (2).
</nextsent>
<nextsent>dashed lines show correspondence arc (bod ies?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4373">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> fusion via optimization.  </section>
<citcontext>
<prevsection>
<prevsent>although this helps the system select the correct information, generating grammatical and easy-to-read fused sentence is still non-trivial (see examples in section 7).
</prevsent>
<prevsent>like filippova and strube (2008), <papid> D08-1019 </papid>we model our fusion task as constrained optimization problem, which we solve using integer linear programming (ilp).</prevsent>
</prevsection>
<citsent citstr=" P08-2049 ">
for each dependency from word to head5as pointed out by daume iii and marcu (2004) and krahmer et al (2008), <papid> P08-2049 </papid>content selection is not only difficult, but also somewhat ill-defined without discourse context information.</citsent>
<aftsection>
<nextsent>56 in the input sentences, we have binary variable xh,w, which is 1 if the dependency is retained in the output and 0 otherwise.
</nextsent>
<nextsent>however, unlike filippova and strube (2008), <papid> D08-1019 </papid>we do not know the points of correspondence between the inputs, only set of possible points.</nextsent>
<nextsent>therefore, we also introduce 0-1 integer variables ms,t for each correspondence arc, which indicate whether word in one sentence should be merged with word in another.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4376">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> fusion via optimization.  </section>
<citcontext>
<prevsection>
<prevsent>we must also introduce conjunctions between arguments of the same syntactic type; our system always inserts and?.
</prevsent>
<prevsent>finally, we choose realization for the dummy relative pronoun that using trigram language model (stolcke, 2002).
</prevsent>
</prevsection>
<citsent citstr=" N09-2057 ">
a more sophisticated approach (filippova and strube, 2009) <papid> N09-2057 </papid>might lead to better results.</citsent>
<aftsection>
<nextsent>the solution which the system finds depends on the weights which we provide for each dependency, word and merger.
</nextsent>
<nextsent>we set the weights based on dot product of features ? and parameters ?, which welearn from data using supervised structured technique (collins, 2002).<papid> W02-1001 </papid></nextsent>
<nextsent>to do so, we define loss function l(s, s?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4377">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> learning.  </section>
<citcontext>
<prevsection>
<prevsent>a more sophisticated approach (filippova and strube, 2009) <papid> N09-2057 </papid>might lead to better results.</prevsent>
<prevsent>the solution which the system finds depends on the weights which we provide for each dependency, word and merger.</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
we set the weights based on dot product of features ? and parameters ?, which welearn from data using supervised structured technique (collins, 2002).<papid> W02-1001 </papid></citsent>
<aftsection>
<nextsent>to do so, we define loss function l(s, s?)
</nextsent>
<nextsent>r which measures how poor solution is when the true solution is s?.
</nextsent>
<nextsent>for each ofour training examples, we compute the oracle solution, the best solution accessible to our system,by minimizing the loss.
</nextsent>
<nextsent>finally, we use the structured averaged perceptron update rule to push our systems parameters away from bad solutions and towards the oracle solutions for each example.our loss function is designed to measure the high level similarity between two dependency trees containing some aligned regions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4383">
<title id=" W11-1607.xml">learning to fuse disparate sentences </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>between each one.
</prevsent>
<prevsent>the readability upper 59 bound is the output of parsing and linearization on the editors original sentence (filippova and strube,2008); <papid> D08-1019 </papid>it is designed to measure the loss ingram maticality due to our preprocessing.</prevsent>
</prevsection>
<citsent citstr=" D09-1041 ">
native english speakers rated the fused sentences with respect to readability and content on scale of 1 to 5 (we give scoring rubric based on (nomoto, 2009)).<papid> D09-1041 </papid></citsent>
<aftsection>
<nextsent>12 judges participated in the study, for total of 1062 evaluations7 . each judge saw the each pair of inputs with the retained regions bold faced, plus single fusion drawn randomly from among the four systems.
</nextsent>
<nextsent>results are displayed in table 2.
</nextsent>
<nextsent>system readability content editor 4.55 4.56 readability ub 3.97 4.27 and?-splice 3.65 3.80 our system 3.12 3.83 table 2: results of human evaluation.
</nextsent>
<nextsent>readability scores indicate that the judges preferhuman-authored sentences, then the readability upper bound, then and?-splicing and finally our system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4385">
<title id=" W11-1310.xml">exemplar based word space model for compositionality detection shared task system description </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>most models represent each word as single prototype-based vector without addressing polysemy.
</prevsent>
<prevsent>we propose an exemplar-based model which is designed to handle polysemy.
</prevsent>
</prevsection>
<citsent citstr=" W11-1304 ">
this model is tested for compositionality detection and it is found to outperform existing prototype-based models.we have participated in the shared task (biemann and giesbrecht, 2011) <papid> W11-1304 </papid>and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations.</citsent>
<aftsection>
<nextsent>in the field of computational semantics, to represent the meaning of compound word, two mechanisms are commonly used.
</nextsent>
<nextsent>one is based on the distributional hypothesis (harris, 1954) and the other is on the principle of semantic compositionality (partee, 1995, p. 313).
</nextsent>
<nextsent>the distributional hypothesis (dh) states that words that occur in similar contexts tend to have similar meanings.
</nextsent>
<nextsent>using this hypothesis, distributional models like the word-space model (wsm, sahlgren, 2006) represent target words meaning as context vector (location in space).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4386">
<title id=" W11-1310.xml">exemplar based word space model for compositionality detection shared task system description </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so psc-based is composed of component dh-based vectors.both of these two mechanisms are capable of determining the meaning vector of compound word.
</prevsent>
<prevsent>forgiven compound, if dh-based vector and psc-based vector of the compound are projected into an identical space, one would expect the vectors to occupy the same location i.e. both the vectors should be nearly the same.
</prevsent>
</prevsection>
<citsent citstr=" W06-1203 ">
however the principle of semantic compositionality does not hold for non compositional compounds, which is actually whatthe existing wsms of compositionality detection exploit (giesbrecht, 2009; katz and giesbrecht, 2006; <papid> W06-1203 </papid>schone and jurafsky, 2001).<papid> W01-0513 </papid></citsent>
<aftsection>
<nextsent>the dh-based andpsc-based vectors are expected to have high similarity when compound is compositional and low similarity for non-compositional compounds.
</nextsent>
<nextsent>most methods in wsm (turney and pantel, 2010) represent word as single context vector built from merging all its corpus instances.
</nextsent>
<nextsent>such representation is called the prototype-based modelling (mur phy, 2002).
</nextsent>
<nextsent>these prototype-based vectors do not 54 distinguish the instances according to the senses ofa target word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4387">
<title id=" W11-1310.xml">exemplar based word space model for compositionality detection shared task system description </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so psc-based is composed of component dh-based vectors.both of these two mechanisms are capable of determining the meaning vector of compound word.
</prevsent>
<prevsent>forgiven compound, if dh-based vector and psc-based vector of the compound are projected into an identical space, one would expect the vectors to occupy the same location i.e. both the vectors should be nearly the same.
</prevsent>
</prevsection>
<citsent citstr=" W01-0513 ">
however the principle of semantic compositionality does not hold for non compositional compounds, which is actually whatthe existing wsms of compositionality detection exploit (giesbrecht, 2009; katz and giesbrecht, 2006; <papid> W06-1203 </papid>schone and jurafsky, 2001).<papid> W01-0513 </papid></citsent>
<aftsection>
<nextsent>the dh-based andpsc-based vectors are expected to have high similarity when compound is compositional and low similarity for non-compositional compounds.
</nextsent>
<nextsent>most methods in wsm (turney and pantel, 2010) represent word as single context vector built from merging all its corpus instances.
</nextsent>
<nextsent>such representation is called the prototype-based modelling (mur phy, 2002).
</nextsent>
<nextsent>these prototype-based vectors do not 54 distinguish the instances according to the senses ofa target word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4392">
<title id=" W11-1310.xml">exemplar based word space model for compositionality detection shared task system description </title>
<section> word space model.  </section>
<citcontext>
<prevsection>
<prevsent>ew denotes the set of exemplars of w. vw is the prototype vector of the word w, which is built by merging all the exemplars in ew 1sketch engine http://www.sketchengine.co.uk for the purposes of producing psc-based vector for compound, vector of constituent word is built using only the exemplars which do not contain the compound.
</prevsent>
<prevsent>note that the vectors are sensitive to compounds word-order since the exemplars of w1 w2 are not the same as w2 w1.
</prevsent>
</prevsection>
<citsent citstr=" P08-1028 ">
we use other wsm settings following mitchell and lapata (2008).<papid> P08-1028 </papid></citsent>
<aftsection>
<nextsent>the dimensions of the wsm are the top 2000 content words in the given corpus(along with their coarse-grained part-of-speech information).
</nextsent>
<nextsent>cosine similarity (sim) is used to measure the similarity between two vectors.
</nextsent>
<nextsent>values atthe specific positions in the vector representing context words are set to the ratio of the probability of the context word given the target word to the overall probability of the context word.
</nextsent>
<nextsent>the context window of target words exemplar is the whole sentence ofthe target word excluding the target word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4395">
<title id=" W11-1310.xml">exemplar based word space model for compositionality detection shared task system description </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it was found that no single threshold value ? held for all compounds.
</prevsent>
<prevsent>changing the threshold alters performance arbitrarily.
</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
this might be due to the polysemous nature of the constituent words which makes the composed vector vw1w2filled with noisy contexts and thus making the judgement unpredictable.in the above model, if a=0 and b=1, the resulting model is similar to that of baldwin et al (2003).<papid> W03-1812 </papid>they also observe similar behaviour of the thresh 55 old ?.</citsent>
<aftsection>
<nextsent>we try to address this problem by addressing the polysemy in wsms using exemplar-based modelling.
</nextsent>
<nextsent>the above models use simple addition based compositionality function.
</nextsent>
<nextsent>mitchell and lapata(2008) <papid> P08-1028 </papid>observed that simple multiplication function modelled compositionality better than addi tion.</nextsent>
<nextsent>contrary to that, guevara (2011) <papid> W11-0115 </papid>observed additive models worked well for building compositional vectors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4397">
<title id=" W11-1310.xml">exemplar based word space model for compositionality detection shared task system description </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the above models use simple addition based compositionality function.
</prevsent>
<prevsent>mitchell and lapata(2008) <papid> P08-1028 </papid>observed that simple multiplication function modelled compositionality better than addi tion.</prevsent>
</prevsection>
<citsent citstr=" W11-0115 ">
contrary to that, guevara (2011) <papid> W11-0115 </papid>observed additive models worked well for building compositional vectors.</citsent>
<aftsection>
<nextsent>in our work, we try using evidence from both compositionality functions, simple addition and simple multiplication.bannard et al (2003); mccarthy et al (2003) <papid> W03-1810 </papid>observed that methods based on distributional similarities between phrase and its constituent words help when determining the compositionality behaviour of phrases.</nextsent>
<nextsent>we therefore also use evidence from the similarities between each constituent word and the compound.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4398">
<title id=" W11-1310.xml">exemplar based word space model for compositionality detection shared task system description </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>mitchell and lapata(2008) <papid> P08-1028 </papid>observed that simple multiplication function modelled compositionality better than addi tion.</prevsent>
<prevsent>contrary to that, guevara (2011) <papid> W11-0115 </papid>observed additive models worked well for building compositional vectors.</prevsent>
</prevsection>
<citsent citstr=" W03-1810 ">
in our work, we try using evidence from both compositionality functions, simple addition and simple multiplication.bannard et al (2003); mccarthy et al (2003) <papid> W03-1810 </papid>observed that methods based on distributional similarities between phrase and its constituent words help when determining the compositionality behaviour of phrases.</citsent>
<aftsection>
<nextsent>we therefore also use evidence from the similarities between each constituent word and the compound.
</nextsent>
<nextsent>our approach works as follows.
</nextsent>
<nextsent>firstly, given acompound w1 w2, we build its dh-based prototype vector vw1w2 from all its exemplars ew1w2 . secondly, we remove irrelevant exemplars in ew1 and ew2 of constituent words and build the refined prototype vectors vwr1 and vwr2 of the constituent words w1 and w2 respectively.
</nextsent>
<nextsent>these refined vectors are used to compose the psc-based vectors 2 of the compound.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4399">
<title id=" W11-1310.xml">exemplar based word space model for compositionality detection shared task system description </title>
<section> our approach: exemplar-based model.  </section>
<citcontext>
<prevsection>
<prevsent>firstly, given acompound w1 w2, we build its dh-based prototype vector vw1w2 from all its exemplars ew1w2 . secondly, we remove irrelevant exemplars in ew1 and ew2 of constituent words and build the refined prototype vectors vwr1 and vwr2 of the constituent words w1 and w2 respectively.
</prevsent>
<prevsent>these refined vectors are used to compose the psc-based vectors 2 of the compound.
</prevsent>
</prevsection>
<citsent citstr=" N10-1013 ">
related work to ours is (reisinger and mooney, 2010) <papid> N10-1013 </papid>where exemplars of word are first clustered and then prototype vectors are built.</citsent>
<aftsection>
<nextsent>this work does not relate to compositionality but to measuring semantic similarity of single words.
</nextsent>
<nextsent>as such, their clusters are not influenced by other words whereas in our approach for detecting composition ality, the other constituent word plays major role.we use the compositionality functions, simple addition and simple multiplication to build vwr1+wr2 and vwr1wr2 respectively.
</nextsent>
<nextsent>based on the similarities sim(vw1w2 , vwr1), sim(vw1w2 , vwr2), sim(vw1w2 , vwr1+wr2) and sim(vw1w2 , vwr1wr2), we decide if the compound is compositional or non compositional.
</nextsent>
<nextsent>these steps are described in little more detail below.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4405">
<title id=" W10-4204.xml">hierarchical reinforcement learning for adaptive text generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, we aim to balance the degree of variation and alignment in texts and produce lexical and syntactic patterns of co-occurrence that resemble those of human texts of the same domain.
</prevsent>
<prevsent>evidence for the importance of this is provided by (halliday and hasan, 1976) who note the way that lexical cohesive ties contribute to text coherence aswell as by the theory of interactive alignment.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
according to (pickering and garrod, 2004) we would expect significant traces of lexical and syntactic self alignment in texts.approaches to nlg in the past have been either rule-based (reiter and dale, 1997) or statistical (langkilde and knight, 1998).<papid> P98-1116 </papid></citsent>
<aftsection>
<nextsent>however, the former relies on large number of hand-crafted rules, which makes it infeasible for controlling large number of interrelated variables.
</nextsent>
<nextsent>the latter typically requires training on large corpus of the do main.
</nextsent>
<nextsent>while these approaches may be better suitable for larger domains, for limited domains such as our own, we propose to overcome these drawbacks by applying reinforcement learning (rl)with hierarchical approach.
</nextsent>
<nextsent>previous work that has used rl for nlg includes (janarthanam and lemon, 2009)<papid> W09-0611 </papid>who employed it for alignment of referring expressions based on user models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4406">
<title id=" W10-4204.xml">hierarchical reinforcement learning for adaptive text generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the latter typically requires training on large corpus of the do main.
</prevsent>
<prevsent>while these approaches may be better suitable for larger domains, for limited domains such as our own, we propose to overcome these drawbacks by applying reinforcement learning (rl)with hierarchical approach.
</prevsent>
</prevsection>
<citsent citstr=" W09-0611 ">
previous work that has used rl for nlg includes (janarthanam and lemon, 2009)<papid> W09-0611 </papid>who employed it for alignment of referring expressions based on user models.</citsent>
<aftsection>
<nextsent>also, (lemon, 2008; rieser and lemon, 2009) <papid> E09-1078 </papid>used rl for optimising information presentation styles for search results.</nextsent>
<nextsent>while both approaches displayed significant effects of adaptation, they focused on single area of opti misation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4407">
<title id=" W10-4204.xml">hierarchical reinforcement learning for adaptive text generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while these approaches may be better suitable for larger domains, for limited domains such as our own, we propose to overcome these drawbacks by applying reinforcement learning (rl)with hierarchical approach.
</prevsent>
<prevsent>previous work that has used rl for nlg includes (janarthanam and lemon, 2009)<papid> W09-0611 </papid>who employed it for alignment of referring expressions based on user models.</prevsent>
</prevsection>
<citsent citstr=" E09-1078 ">
also, (lemon, 2008; rieser and lemon, 2009) <papid> E09-1078 </papid>used rl for optimising information presentation styles for search results.</citsent>
<aftsection>
<nextsent>while both approaches displayed significant effects of adaptation, they focused on single area of optimisation.
</nextsent>
<nextsent>for larger problems, however, such as theone we are aiming to solve, flat rl will not be applicable due to the large state space.
</nextsent>
<nextsent>we therefore suggest to divide the problem into number of subprob lems and apply hierarchical reinforcement learning (hrl) (barto and mahadevan, 2003) to solve it.we describe our problem in more detail in section 2, our proposed hrl architecture in sections 3 and 4 and present some results in section 5.
</nextsent>
<nextsent>we show that our learnt policies outperform baseline that does not adapt to contextual features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4408">
<title id=" W10-4204.xml">hierarchical reinforcement learning for adaptive text generation </title>
<section> generation tasks.  </section>
<citcontext>
<prevsection>
<prevsent>we focus here exclusively on hirst &amp; st-onges extra-strong?
</prevsent>
<prevsent>relations, since these can be computed from shallow properties of texts and do not require large corpus of the target domain.
</prevsent>
</prevsection>
<citsent citstr=" W00-1308 ">
in order to make fair comparison between the human texts and our own, we used part-of-speech (pos) tagger (toutanova and manning, 2000)<papid> W00-1308 </papid>1 to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs.</citsent>
<aftsection>
<nextsent>based on these categories, we compute the proportion of tokens that are members in lexical chains, the alignment score?
</nextsent>
<nextsent>(as), according to the following equation: as = lexical tokens in chains total number of tokens ? 100.
</nextsent>
<nextsent>(1) we obtained an average alignment score of 43.3% for 24 human route instructions.
</nextsent>
<nextsent>in contrast, the 1http://nlp.stanford.edu/software/tagger.shtml table 1: different text generation strategies for the same underlying route.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4409">
<title id=" W10-4204.xml">hierarchical reinforcement learning for adaptive text generation </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>both arguments are directly transferable to nlg.
</prevsent>
<prevsent>if an agent is able to act only on grounds of what it has observed in training corpus, it will not be able to react flexibly to new state representations.
</prevsent>
</prevsection>
<citsent citstr=" E06-1040 ">
moreover, it has been argued thata corpus for nlg cannot be regarded as an equivalent gold standard to the ones of other domains of nlp (belz and reiter, 2006; <papid> E06-1040 </papid>scott and moore, 2006;viethen and dale, 2006).</citsent>
<aftsection>
<nextsent>the fact that an expression for semantic concept does not appear in corpus does not mean that it is an unsuited or impossible expression.
</nextsent>
<nextsent>another alternative to pure rl isto apply semi-learnt behaviour, which can be helpful for tasks with very large state-action spaces.
</nextsent>
<nextsent>in this way, the state-action space is reduced to only sensible state-action pairs by providing the agent with prior knowledge of the domain.
</nextsent>
<nextsent>all remaining behaviour continues to be learnt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4410">
<title id=" W10-4149.xml">treebank conversion based self training strategy for parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>self-training is one of the most successful strategies.
</prevsent>
<prevsent>mcclosky et al (2006) shows that self-training effectively improves the accuracy of english parsing.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
first, they trained two stage reranking parser(charniak and johnson, 2005) <papid> P05-1022 </papid>using penn treebank (ptb)(marcus et al, 1993) and parsed 1,750k unlabeled sentences from north american news text corpus (nanc).</citsent>
<aftsection>
<nextsent>then they combined the labeled nanc sentences with ptb together as training set and retrained the first stage of the parser.
</nextsent>
<nextsent>the final result got 1.1% improvement over the previous best parser for section 23 of the penn treebank.
</nextsent>
<nextsent>huang and harper (2009) combined self-training into pcfg-la based parser both for english and chinese.
</nextsent>
<nextsent>experimental result showed that self-training contributed 0.83% absolute improvement using only 210k unlabeled sentences with single generative parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4411">
<title id=" W11-0904.xml">identifying event sentiment association using lexical equivalence and coreference approaches </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this observation prompts us to employ several strategies in conjunction with machine learning.
</prevsent>
<prevsent>these strategies are implemented based on semantic role labeling, wordnet (miller, 1990) and some handcrafted rules.
</prevsent>
</prevsection>
<citsent citstr=" S10-1077 ">
we have experimented with the tempeval-2010 evaluation challenge setup (kolya et al, 2010).<papid> S10-1077 </papid></citsent>
<aftsection>
<nextsent>evaluation results yield the precision, recall and f-measure values of approximately 93.00%, 96.00% and 94.47% respectively.
</nextsent>
<nextsent>this is approximately 12% higher f-measure in comparison to the best system (llorens et al, 2010) <papid> S10-1063 </papid>of tempeval-2010.</nextsent>
<nextsent>on the other hand, the identification of the sentiment expressions is carried out based on the sentiment word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4413">
<title id=" W11-0904.xml">identifying event sentiment association using lexical equivalence and coreference approaches </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have experimented with the tempeval-2010 evaluation challenge setup (kolya et al, 2010).<papid> S10-1077 </papid></prevsent>
<prevsent>evaluation results yield the precision, recall and f-measure values of approximately 93.00%, 96.00% and 94.47% respectively.</prevsent>
</prevsection>
<citsent citstr=" S10-1063 ">
this is approximately 12% higher f-measure in comparison to the best system (llorens et al, 2010) <papid> S10-1063 </papid>of tempeval-2010.</citsent>
<aftsection>
<nextsent>on the other hand, the identification of the sentiment expressions is carried out based on the sentiment word.
</nextsent>
<nextsent>the words are searched in three different sentiment lexicons, the subjectivity word lists (banea et al, 2008), <papid> L08-1086 </papid>sentiwordnet (baccia nella et al, 2010) and wordnet affect (strapparava and valitutti, 2004).</nextsent>
<nextsent>the coarse-grained (positive and negative) as well as ekmans (1993) six fine- grained sentiment or emotion expressions (happy, sadness, anger, disgust, fear and surprise) are tagged in the corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4415">
<title id=" W11-0904.xml">identifying event sentiment association using lexical equivalence and coreference approaches </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is approximately 12% higher f-measure in comparison to the best system (llorens et al, 2010) <papid> S10-1063 </papid>of tempeval-2010.</prevsent>
<prevsent>on the other hand, the identification of the sentiment expressions is carried out based on the sentiment word.</prevsent>
</prevsection>
<citsent citstr=" L08-1086 ">
the words are searched in three different sentiment lexicons, the subjectivity word lists (banea et al, 2008), <papid> L08-1086 </papid>sentiwordnet (baccia nella et al, 2010) and wordnet affect (strapparava and valitutti, 2004).</citsent>
<aftsection>
<nextsent>the coarse-grained (positive and negative) as well as ekmans (1993) six fine- grained sentiment or emotion expressions (happy, sadness, anger, disgust, fear and surprise) are tagged in the corpus.
</nextsent>
<nextsent>as there is no annotation in the temeval-2010 corpus for sentiment expressions, the evaluation has been carried out by the authors and it achieves the precision, recall and measure values of approximately 73.54%, 86.04% and 79.30% respectively determining the lexical equivalence of event and sentiment expressions based on the pos property at the lexical entity level is straightforward.
</nextsent>
<nextsent>if an event word also expresses the sentiment word, we have associated the corresponding sentiment type with the event word directly.
</nextsent>
<nextsent>in addition to the sentiment lexicons, the emotional verbs extracted from the verbnet (kipper-schuler, 2005) are used in this phase.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4417">
<title id=" W11-0904.xml">identifying event sentiment association using lexical equivalence and coreference approaches </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the co-reference approach for identifying the association between event and sentiment is described in section 6.
</prevsent>
<prevsent>finally section 7 concludes the paper.
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
the existing works on event extraction are based either on pattern-matching rules (mani and wilson 2000), <papid> P00-1010 </papid>or on the machine learning approach (bo guraev and ando, 2005).</citsent>
<aftsection>
<nextsent>but, still the problems persist with the high complexities involved in the proper extractions of events.
</nextsent>
<nextsent>the events expressions were annotated in the tempe val 2007 source in accordance with the timeml standard (pustejovsky et al, 2003).
</nextsent>
<nextsent>on the other hand, the task of tempeval-2010 evaluation challenge setup (verhagen et al, 2010) was aimed at identifying events from text.
</nextsent>
<nextsent>the best achieved result was obtained by (llorens et al, 2010).<papid> S10-1063 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4419">
<title id=" W11-0904.xml">identifying event sentiment association using lexical equivalence and coreference approaches </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a lexicon that provides appraisal attributes for terms was constructed and the features were used for emotion classification (whitelaw et al, 2005).
</prevsent>
<prevsent>the features along with the bag-of-words model give 90.2% accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W07-2094 ">
upar7 (chaumartin, 2007), <papid> W07-2094 </papid>rule-based system uses combination of wordnet affect and sentiwordnet.</citsent>
<aftsection>
<nextsent>the system was semi-automatically enriched with the original trial data provided during the semeval task (strappara va and mihalcea, 2007).<papid> W07-2013 </papid></nextsent>
<nextsent>swat (katz et al, 2007) <papid> W07-2067 </papid>is another supervised system that uses unigram model trained to annotate emotional content.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4420">
<title id=" W11-0904.xml">identifying event sentiment association using lexical equivalence and coreference approaches </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the features along with the bag-of-words model give 90.2% accuracy.
</prevsent>
<prevsent>upar7 (chaumartin, 2007), <papid> W07-2094 </papid>rule-based system uses combination of wordnet affect and sentiwordnet.</prevsent>
</prevsection>
<citsent citstr=" W07-2013 ">
the system was semi-automatically enriched with the original trial data provided during the semeval task (strappara va and mihalcea, 2007).<papid> W07-2013 </papid></citsent>
<aftsection>
<nextsent>swat (katz et al, 2007) <papid> W07-2067 </papid>is another supervised system that uses unigram model trained to annotate emotional content.</nextsent>
<nextsent>our motivation is that though events and sentiments are closely coupled with each other from social, psychological and commercial perspectives, very little attention has been given about their detection and analysis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4421">
<title id=" W11-0904.xml">identifying event sentiment association using lexical equivalence and coreference approaches </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>upar7 (chaumartin, 2007), <papid> W07-2094 </papid>rule-based system uses combination of wordnet affect and sentiwordnet.</prevsent>
<prevsent>the system was semi-automatically enriched with the original trial data provided during the semeval task (strappara va and mihalcea, 2007).<papid> W07-2013 </papid></prevsent>
</prevsection>
<citsent citstr=" W07-2067 ">
swat (katz et al, 2007) <papid> W07-2067 </papid>is another supervised system that uses unigram model trained to annotate emotional content.</citsent>
<aftsection>
<nextsent>our motivation is that though events and sentiments are closely coupled with each other from social, psychological and commercial perspectives, very little attention has been given about their detection and analysis.
</nextsent>
<nextsent>to the best of our knowledge, only few tasks have been attempted (fukuhara et al., 2007) (das et al, 2010).
</nextsent>
<nextsent>sometimes, the opinion topics are not necessarily spatially coherent as there may be two opinions in the same sentence on different topics, as well as opinions that are on the same topic separated by opinions that do not share that topic (stoyanov and cardie 2008).<papid> L08-1088 </papid></nextsent>
<nextsent>the authors have established their hypothesis by applying the coreference technique.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4422">
<title id=" W11-0904.xml">identifying event sentiment association using lexical equivalence and coreference approaches </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our motivation is that though events and sentiments are closely coupled with each other from social, psychological and commercial perspectives, very little attention has been given about their detection and analysis.
</prevsent>
<prevsent>to the best of our knowledge, only few tasks have been attempted (fukuhara et al., 2007) (das et al, 2010).
</prevsent>
</prevsection>
<citsent citstr=" L08-1088 ">
sometimes, the opinion topics are not necessarily spatially coherent as there may be two opinions in the same sentence on different topics, as well as opinions that are on the same topic separated by opinions that do not share that topic (stoyanov and cardie 2008).<papid> L08-1088 </papid></citsent>
<aftsection>
<nextsent>the authors have established their hypothesis by applying the coreference technique.
</nextsent>
<nextsent>similarly, we have adopted the co-reference technique based on basic rhetoric components for identifying the association between event and sentiment expressions.
</nextsent>
<nextsent>in addition to that, we have also employed the lexical equivalence approach for identifying their association.
</nextsent>
<nextsent>in this work, we propose hybrid approach for event identification from the text under the tempeval-2010 framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4428">
<title id=" W11-0904.xml">identifying event sentiment association using lexical equivalence and coreference approaches </title>
<section> event identification.  </section>
<citcontext>
<prevsection>
<prevsent>in the present work, we mainly use the various combinations of the following features: part of speech (pos) of event terms (e.g. adjective, noun and verb), tense (present, past, future, infinitive, present part, past part, or none), aspect (progressive, perfect ive and perfect ive progressive or none), class (reporting, perception, aspect ual, i_action, i_state, state, occur rence), stem (e.g., discount /s/).
</prevsent>
<prevsent>21 3.2 use of semantic roles for event identifi-.
</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
cation we use an open source semantic role labeler 1(srl) (gildea et al, 2002) (pradhan et al, 2004) <papid> N04-1030 </papid>to identify different features of the sentences.</citsent>
<aftsection>
<nextsent>for each predicate in sentence acting as event word, semantic roles extract all constituents, determining their arguments (agent, patient etc.) and adjuncts (locative, temporal etc.).
</nextsent>
<nextsent>semantic roles can be used to detect the events that are the nominalizations of verbs such as agreement for agree or construction for construct.
</nextsent>
<nextsent>nominalizations (or, deverbal nouns) are commonly defined as nouns that are morphologically derived from verbs, usually by suffix ation (quirk et al, 1985).
</nextsent>
<nextsent>event nominalizations often afford the same semantic roles as verbs and often replace them in written language (gurevich et al, 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4434">
<title id=" W11-0820.xml">the string net lexico grammatical knowledge base and its applications </title>
<section> string net content: hybrid n-grams.  </section>
<citcontext>
<prevsection>
<prevsent>the string net lexico-grammatical knowledge base has been designed to capture this heterogeneity of mwes by virtue of its unique content and structure.
</prevsent>
<prevsent>these we describe in turn below.
</prevsent>
</prevsection>
<citsent citstr=" W09-2108 ">
the content of string net consists of special breed of n-grams which we call hybrid n-grams (tsao and wible 2009; <papid> W09-2108 </papid>wible and tsao 2010).<papid> W10-0804 </papid></citsent>
<aftsection>
<nextsent>unlike traditional n-grams, there are four different categories of gram type.
</nextsent>
<nextsent>from specific to general (or abstract) these four are: specific word forms (enjoyed and enjoys would be two distinct word forms); lexemes (enjoy, including all its inflectional variations, enjoyed, enjoys, etc); rough pos categories (v, n, etc); and fine-grained pos categories (verbs are distinguished as vvn, vvd, vvt, etc.).
</nextsent>
<nextsent>a hybrid n-gram can consist of any sequence from any of these four categories with 128 our stipulation that one of the grams must be word form or lexeme (to insure that all hybrid ngrams are lexically anchored).
</nextsent>
<nextsent>a traditional bigram such as enjoyed hiking can be described by 16 distinct hybrid n-grams, such as enjoyed vvg, enjoy vvg, enjoy hike, and so on.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4435">
<title id=" W11-0820.xml">the string net lexico grammatical knowledge base and its applications </title>
<section> string net content: hybrid n-grams.  </section>
<citcontext>
<prevsection>
<prevsent>the string net lexico-grammatical knowledge base has been designed to capture this heterogeneity of mwes by virtue of its unique content and structure.
</prevsent>
<prevsent>these we describe in turn below.
</prevsent>
</prevsection>
<citsent citstr=" W10-0804 ">
the content of string net consists of special breed of n-grams which we call hybrid n-grams (tsao and wible 2009; <papid> W09-2108 </papid>wible and tsao 2010).<papid> W10-0804 </papid></citsent>
<aftsection>
<nextsent>unlike traditional n-grams, there are four different categories of gram type.
</nextsent>
<nextsent>from specific to general (or abstract) these four are: specific word forms (enjoyed and enjoys would be two distinct word forms); lexemes (enjoy, including all its inflectional variations, enjoyed, enjoys, etc); rough pos categories (v, n, etc); and fine-grained pos categories (verbs are distinguished as vvn, vvd, vvt, etc.).
</nextsent>
<nextsent>a hybrid n-gram can consist of any sequence from any of these four categories with 128 our stipulation that one of the grams must be word form or lexeme (to insure that all hybrid ngrams are lexically anchored).
</nextsent>
<nextsent>a traditional bigram such as enjoyed hiking can be described by 16 distinct hybrid n-grams, such as enjoyed vvg, enjoy vvg, enjoy hike, and so on.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4436">
<title id=" W11-1213.xml">comparable fora </title>
<section> introduction to comparable corpora.  </section>
<citcontext>
<prevsection>
<prevsent>(maia, 2003), which also became nearly standard during the recent years, emphasizes the fact that comparable monolingual corpora usually provide uswith much better linguistic quality and representativeness than translated parallel corpora.
</prevsent>
<prevsent>the other advantages over the parallel corpora, i.e. amount and availability, are obvious.nowadays, the most popular usage of comparable corpora is improving machine translation, more 1http://www.ilc.cnr.it/eagles96/corpustyp/node21.htmlprecisely, compensating the lack of parallel training data.
</prevsent>
</prevsection>
<citsent citstr=" N04-1034 ">
the articles (munteanu et al, 2004), (<papid> N04-1034 </papid>munteanu and marcu, 2005) <papid> J05-4003 </papid>and (munteanu andmarcu, 2006) <papid> P06-1011 </papid>are introducing algorithms for extracting parallel sentences and sub-sententional fragments from comparable corpora and using the automatically extracted parallel data for improving statistical machine translation algorithms performance.</citsent>
<aftsection>
<nextsent>present day most popular comparable corpora come either from the newswire resources (afp, reuters, xinhua), leading to datasets like ldc english, chinese and arabic gigaword, or fromwikipedia.
</nextsent>
<nextsent>mining wikipedia became very popular in the recent years.
</nextsent>
<nextsent>for example, (tomas et al,2008) is exploring both parallel and comparable potential of wikipedia, (filatova, 2009) <papid> W09-1605 </papid>examines multilingual aspects of selected subset of wikipedia and (gamallo and lopez, 2010) describes converting wikipedia into corpuspedia?.</nextsent>
<nextsent>just to avoid confusion: in this article, we focus only on fora or boards, i.e. standalone discussion sites on stated topic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4437">
<title id=" W11-1213.xml">comparable fora </title>
<section> introduction to comparable corpora.  </section>
<citcontext>
<prevsection>
<prevsent>(maia, 2003), which also became nearly standard during the recent years, emphasizes the fact that comparable monolingual corpora usually provide uswith much better linguistic quality and representativeness than translated parallel corpora.
</prevsent>
<prevsent>the other advantages over the parallel corpora, i.e. amount and availability, are obvious.nowadays, the most popular usage of comparable corpora is improving machine translation, more 1http://www.ilc.cnr.it/eagles96/corpustyp/node21.htmlprecisely, compensating the lack of parallel training data.
</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
the articles (munteanu et al, 2004), (<papid> N04-1034 </papid>munteanu and marcu, 2005) <papid> J05-4003 </papid>and (munteanu andmarcu, 2006) <papid> P06-1011 </papid>are introducing algorithms for extracting parallel sentences and sub-sententional fragments from comparable corpora and using the automatically extracted parallel data for improving statistical machine translation algorithms performance.</citsent>
<aftsection>
<nextsent>present day most popular comparable corpora come either from the newswire resources (afp, reuters, xinhua), leading to datasets like ldc english, chinese and arabic gigaword, or fromwikipedia.
</nextsent>
<nextsent>mining wikipedia became very popular in the recent years.
</nextsent>
<nextsent>for example, (tomas et al,2008) is exploring both parallel and comparable potential of wikipedia, (filatova, 2009) <papid> W09-1605 </papid>examines multilingual aspects of selected subset of wikipedia and (gamallo and lopez, 2010) describes converting wikipedia into corpuspedia?.</nextsent>
<nextsent>just to avoid confusion: in this article, we focus only on fora or boards, i.e. standalone discussion sites on stated topic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4438">
<title id=" W11-1213.xml">comparable fora </title>
<section> introduction to comparable corpora.  </section>
<citcontext>
<prevsection>
<prevsent>(maia, 2003), which also became nearly standard during the recent years, emphasizes the fact that comparable monolingual corpora usually provide uswith much better linguistic quality and representativeness than translated parallel corpora.
</prevsent>
<prevsent>the other advantages over the parallel corpora, i.e. amount and availability, are obvious.nowadays, the most popular usage of comparable corpora is improving machine translation, more 1http://www.ilc.cnr.it/eagles96/corpustyp/node21.htmlprecisely, compensating the lack of parallel training data.
</prevsent>
</prevsection>
<citsent citstr=" P06-1011 ">
the articles (munteanu et al, 2004), (<papid> N04-1034 </papid>munteanu and marcu, 2005) <papid> J05-4003 </papid>and (munteanu andmarcu, 2006) <papid> P06-1011 </papid>are introducing algorithms for extracting parallel sentences and sub-sententional fragments from comparable corpora and using the automatically extracted parallel data for improving statistical machine translation algorithms performance.</citsent>
<aftsection>
<nextsent>present day most popular comparable corpora come either from the newswire resources (afp, reuters, xinhua), leading to datasets like ldc english, chinese and arabic gigaword, or fromwikipedia.
</nextsent>
<nextsent>mining wikipedia became very popular in the recent years.
</nextsent>
<nextsent>for example, (tomas et al,2008) is exploring both parallel and comparable potential of wikipedia, (filatova, 2009) <papid> W09-1605 </papid>examines multilingual aspects of selected subset of wikipedia and (gamallo and lopez, 2010) describes converting wikipedia into corpuspedia?.</nextsent>
<nextsent>just to avoid confusion: in this article, we focus only on fora or boards, i.e. standalone discussion sites on stated topic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4439">
<title id=" W11-1213.xml">comparable fora </title>
<section> introduction to comparable corpora.  </section>
<citcontext>
<prevsection>
<prevsent>present day most popular comparable corpora come either from the newswire resources (afp, reuters, xinhua), leading to datasets like ldc english, chinese and arabic gigaword, or fromwikipedia.
</prevsent>
<prevsent>mining wikipedia became very popular in the recent years.
</prevsent>
</prevsection>
<citsent citstr=" W09-1605 ">
for example, (tomas et al,2008) is exploring both parallel and comparable potential of wikipedia, (filatova, 2009) <papid> W09-1605 </papid>examines multilingual aspects of selected subset of wikipedia and (gamallo and lopez, 2010) describes converting wikipedia into corpuspedia?.</citsent>
<aftsection>
<nextsent>just to avoid confusion: in this article, we focus only on fora or boards, i.e. standalone discussion sites on stated topic.
</nextsent>
<nextsent>we are not talking about comments accompanying news articles or blog posts.
</nextsent>
<nextsent>the internet discussion fora cover, in surprisingly big amounts of data and for many languages, the most unbelievable topics (real examples from the authors?
</nextsent>
<nextsent>country).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4440">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we provide new perspective to 135 documents and their keyphrases: each document and its key phrases are descriptions to the same object, but the document is written using one language, while key phrases are written using another language.
</prevsent>
<prevsent>therefore, key phrase extraction can be regarded as translation problem from the language of documents into the language of keyphrases.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
based on the idea of translation, we use word alignment models (wam) (brown et al , 1993) <papid> J93-2003 </papid>in statistical machine translation (smt) (koehn, 2010) and propose unified framework for key phrase extraction: (1) from collection of translation pairsof two languages, wam learns translation probabilities between the words in the two languages.</citsent>
<aftsection>
<nextsent>(2) according to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate key phrases, which may not necessarily frequent in their corresponding documents.
</nextsent>
<nextsent>as promising approach to solve the problem of vocabulary gap, smt has been widely exploited in many applications such as information retrieval (berger and lafferty, 1999; karimzadehgan and zhai, 2010), image and video annotation (duygulu et al , 2002), question answering (berger et al , 2000; echihabi and marcu, 2003; <papid> P03-1003 </papid>murdock and croft, 2004; soricut and brill, 2006;xue et al , 2008), query expansion and rewriting (riezler et al , 2007; <papid> P07-1059 </papid>riezler et al , 2008; riezler and liu, 2010), <papid> J10-3010 </papid>summarization (banko et al , 2000), <papid> P00-1041 </papid>collocation extraction (liu et al , 2009<papid> D09-1051 </papid>b; liu et al , 2010<papid> P10-1085 </papid>b) and paraphrasing (quirk et al , 2004; <papid> W04-3219 </papid>zhao et al , 2010).<papid> C10-1148 </papid></nextsent>
<nextsent>although smt is widely adopted solution to vocabulary gap, for various applications using smt, the crucial and non-trivial problem is to find appropriate and enough translation pairs for smt.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4442">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on the idea of translation, we use word alignment models (wam) (brown et al , 1993) <papid> J93-2003 </papid>in statistical machine translation (smt) (koehn, 2010) and propose unified framework for key phrase extraction: (1) from collection of translation pairsof two languages, wam learns translation probabilities between the words in the two languages.</prevsent>
<prevsent>(2) according to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate key phrases, which may not necessarily frequent in their corresponding documents.</prevsent>
</prevsection>
<citsent citstr=" P03-1003 ">
as promising approach to solve the problem of vocabulary gap, smt has been widely exploited in many applications such as information retrieval (berger and lafferty, 1999; karimzadehgan and zhai, 2010), image and video annotation (duygulu et al , 2002), question answering (berger et al , 2000; echihabi and marcu, 2003; <papid> P03-1003 </papid>murdock and croft, 2004; soricut and brill, 2006;xue et al , 2008), query expansion and rewriting (riezler et al , 2007; <papid> P07-1059 </papid>riezler et al , 2008; riezler and liu, 2010), <papid> J10-3010 </papid>summarization (banko et al , 2000), <papid> P00-1041 </papid>collocation extraction (liu et al , 2009<papid> D09-1051 </papid>b; liu et al , 2010<papid> P10-1085 </papid>b) and paraphrasing (quirk et al , 2004; <papid> W04-3219 </papid>zhao et al , 2010).<papid> C10-1148 </papid></citsent>
<aftsection>
<nextsent>although smt is widely adopted solution to vocabulary gap, for various applications using smt, the crucial and non-trivial problem is to find appropriate and enough translation pairs for smt.
</nextsent>
<nextsent>the most straightforward translation pairs for key phrase extraction is document-keyphrase pairs.in practice, however, it is time-consuming to annotate large collection of documents with key phrases for sufficient wam training.
</nextsent>
<nextsent>in order to solve the problem, we use titles and summaries to build translation pairs with documents.
</nextsent>
<nextsent>titles and summaries are usually accompanying with the corresponding documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4443">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on the idea of translation, we use word alignment models (wam) (brown et al , 1993) <papid> J93-2003 </papid>in statistical machine translation (smt) (koehn, 2010) and propose unified framework for key phrase extraction: (1) from collection of translation pairsof two languages, wam learns translation probabilities between the words in the two languages.</prevsent>
<prevsent>(2) according to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate key phrases, which may not necessarily frequent in their corresponding documents.</prevsent>
</prevsection>
<citsent citstr=" P07-1059 ">
as promising approach to solve the problem of vocabulary gap, smt has been widely exploited in many applications such as information retrieval (berger and lafferty, 1999; karimzadehgan and zhai, 2010), image and video annotation (duygulu et al , 2002), question answering (berger et al , 2000; echihabi and marcu, 2003; <papid> P03-1003 </papid>murdock and croft, 2004; soricut and brill, 2006;xue et al , 2008), query expansion and rewriting (riezler et al , 2007; <papid> P07-1059 </papid>riezler et al , 2008; riezler and liu, 2010), <papid> J10-3010 </papid>summarization (banko et al , 2000), <papid> P00-1041 </papid>collocation extraction (liu et al , 2009<papid> D09-1051 </papid>b; liu et al , 2010<papid> P10-1085 </papid>b) and paraphrasing (quirk et al , 2004; <papid> W04-3219 </papid>zhao et al , 2010).<papid> C10-1148 </papid></citsent>
<aftsection>
<nextsent>although smt is widely adopted solution to vocabulary gap, for various applications using smt, the crucial and non-trivial problem is to find appropriate and enough translation pairs for smt.
</nextsent>
<nextsent>the most straightforward translation pairs for key phrase extraction is document-keyphrase pairs.in practice, however, it is time-consuming to annotate large collection of documents with key phrases for sufficient wam training.
</nextsent>
<nextsent>in order to solve the problem, we use titles and summaries to build translation pairs with documents.
</nextsent>
<nextsent>titles and summaries are usually accompanying with the corresponding documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4444">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on the idea of translation, we use word alignment models (wam) (brown et al , 1993) <papid> J93-2003 </papid>in statistical machine translation (smt) (koehn, 2010) and propose unified framework for key phrase extraction: (1) from collection of translation pairsof two languages, wam learns translation probabilities between the words in the two languages.</prevsent>
<prevsent>(2) according to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate key phrases, which may not necessarily frequent in their corresponding documents.</prevsent>
</prevsection>
<citsent citstr=" J10-3010 ">
as promising approach to solve the problem of vocabulary gap, smt has been widely exploited in many applications such as information retrieval (berger and lafferty, 1999; karimzadehgan and zhai, 2010), image and video annotation (duygulu et al , 2002), question answering (berger et al , 2000; echihabi and marcu, 2003; <papid> P03-1003 </papid>murdock and croft, 2004; soricut and brill, 2006;xue et al , 2008), query expansion and rewriting (riezler et al , 2007; <papid> P07-1059 </papid>riezler et al , 2008; riezler and liu, 2010), <papid> J10-3010 </papid>summarization (banko et al , 2000), <papid> P00-1041 </papid>collocation extraction (liu et al , 2009<papid> D09-1051 </papid>b; liu et al , 2010<papid> P10-1085 </papid>b) and paraphrasing (quirk et al , 2004; <papid> W04-3219 </papid>zhao et al , 2010).<papid> C10-1148 </papid></citsent>
<aftsection>
<nextsent>although smt is widely adopted solution to vocabulary gap, for various applications using smt, the crucial and non-trivial problem is to find appropriate and enough translation pairs for smt.
</nextsent>
<nextsent>the most straightforward translation pairs for key phrase extraction is document-keyphrase pairs.in practice, however, it is time-consuming to annotate large collection of documents with key phrases for sufficient wam training.
</nextsent>
<nextsent>in order to solve the problem, we use titles and summaries to build translation pairs with documents.
</nextsent>
<nextsent>titles and summaries are usually accompanying with the corresponding documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4445">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on the idea of translation, we use word alignment models (wam) (brown et al , 1993) <papid> J93-2003 </papid>in statistical machine translation (smt) (koehn, 2010) and propose unified framework for key phrase extraction: (1) from collection of translation pairsof two languages, wam learns translation probabilities between the words in the two languages.</prevsent>
<prevsent>(2) according to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate key phrases, which may not necessarily frequent in their corresponding documents.</prevsent>
</prevsection>
<citsent citstr=" P00-1041 ">
as promising approach to solve the problem of vocabulary gap, smt has been widely exploited in many applications such as information retrieval (berger and lafferty, 1999; karimzadehgan and zhai, 2010), image and video annotation (duygulu et al , 2002), question answering (berger et al , 2000; echihabi and marcu, 2003; <papid> P03-1003 </papid>murdock and croft, 2004; soricut and brill, 2006;xue et al , 2008), query expansion and rewriting (riezler et al , 2007; <papid> P07-1059 </papid>riezler et al , 2008; riezler and liu, 2010), <papid> J10-3010 </papid>summarization (banko et al , 2000), <papid> P00-1041 </papid>collocation extraction (liu et al , 2009<papid> D09-1051 </papid>b; liu et al , 2010<papid> P10-1085 </papid>b) and paraphrasing (quirk et al , 2004; <papid> W04-3219 </papid>zhao et al , 2010).<papid> C10-1148 </papid></citsent>
<aftsection>
<nextsent>although smt is widely adopted solution to vocabulary gap, for various applications using smt, the crucial and non-trivial problem is to find appropriate and enough translation pairs for smt.
</nextsent>
<nextsent>the most straightforward translation pairs for key phrase extraction is document-keyphrase pairs.in practice, however, it is time-consuming to annotate large collection of documents with key phrases for sufficient wam training.
</nextsent>
<nextsent>in order to solve the problem, we use titles and summaries to build translation pairs with documents.
</nextsent>
<nextsent>titles and summaries are usually accompanying with the corresponding documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4446">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on the idea of translation, we use word alignment models (wam) (brown et al , 1993) <papid> J93-2003 </papid>in statistical machine translation (smt) (koehn, 2010) and propose unified framework for key phrase extraction: (1) from collection of translation pairsof two languages, wam learns translation probabilities between the words in the two languages.</prevsent>
<prevsent>(2) according to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate key phrases, which may not necessarily frequent in their corresponding documents.</prevsent>
</prevsection>
<citsent citstr=" D09-1051 ">
as promising approach to solve the problem of vocabulary gap, smt has been widely exploited in many applications such as information retrieval (berger and lafferty, 1999; karimzadehgan and zhai, 2010), image and video annotation (duygulu et al , 2002), question answering (berger et al , 2000; echihabi and marcu, 2003; <papid> P03-1003 </papid>murdock and croft, 2004; soricut and brill, 2006;xue et al , 2008), query expansion and rewriting (riezler et al , 2007; <papid> P07-1059 </papid>riezler et al , 2008; riezler and liu, 2010), <papid> J10-3010 </papid>summarization (banko et al , 2000), <papid> P00-1041 </papid>collocation extraction (liu et al , 2009<papid> D09-1051 </papid>b; liu et al , 2010<papid> P10-1085 </papid>b) and paraphrasing (quirk et al , 2004; <papid> W04-3219 </papid>zhao et al , 2010).<papid> C10-1148 </papid></citsent>
<aftsection>
<nextsent>although smt is widely adopted solution to vocabulary gap, for various applications using smt, the crucial and non-trivial problem is to find appropriate and enough translation pairs for smt.
</nextsent>
<nextsent>the most straightforward translation pairs for key phrase extraction is document-keyphrase pairs.in practice, however, it is time-consuming to annotate large collection of documents with key phrases for sufficient wam training.
</nextsent>
<nextsent>in order to solve the problem, we use titles and summaries to build translation pairs with documents.
</nextsent>
<nextsent>titles and summaries are usually accompanying with the corresponding documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4450">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on the idea of translation, we use word alignment models (wam) (brown et al , 1993) <papid> J93-2003 </papid>in statistical machine translation (smt) (koehn, 2010) and propose unified framework for key phrase extraction: (1) from collection of translation pairsof two languages, wam learns translation probabilities between the words in the two languages.</prevsent>
<prevsent>(2) according to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate key phrases, which may not necessarily frequent in their corresponding documents.</prevsent>
</prevsection>
<citsent citstr=" P10-1085 ">
as promising approach to solve the problem of vocabulary gap, smt has been widely exploited in many applications such as information retrieval (berger and lafferty, 1999; karimzadehgan and zhai, 2010), image and video annotation (duygulu et al , 2002), question answering (berger et al , 2000; echihabi and marcu, 2003; <papid> P03-1003 </papid>murdock and croft, 2004; soricut and brill, 2006;xue et al , 2008), query expansion and rewriting (riezler et al , 2007; <papid> P07-1059 </papid>riezler et al , 2008; riezler and liu, 2010), <papid> J10-3010 </papid>summarization (banko et al , 2000), <papid> P00-1041 </papid>collocation extraction (liu et al , 2009<papid> D09-1051 </papid>b; liu et al , 2010<papid> P10-1085 </papid>b) and paraphrasing (quirk et al , 2004; <papid> W04-3219 </papid>zhao et al , 2010).<papid> C10-1148 </papid></citsent>
<aftsection>
<nextsent>although smt is widely adopted solution to vocabulary gap, for various applications using smt, the crucial and non-trivial problem is to find appropriate and enough translation pairs for smt.
</nextsent>
<nextsent>the most straightforward translation pairs for key phrase extraction is document-keyphrase pairs.in practice, however, it is time-consuming to annotate large collection of documents with key phrases for sufficient wam training.
</nextsent>
<nextsent>in order to solve the problem, we use titles and summaries to build translation pairs with documents.
</nextsent>
<nextsent>titles and summaries are usually accompanying with the corresponding documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4452">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on the idea of translation, we use word alignment models (wam) (brown et al , 1993) <papid> J93-2003 </papid>in statistical machine translation (smt) (koehn, 2010) and propose unified framework for key phrase extraction: (1) from collection of translation pairsof two languages, wam learns translation probabilities between the words in the two languages.</prevsent>
<prevsent>(2) according to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate key phrases, which may not necessarily frequent in their corresponding documents.</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
as promising approach to solve the problem of vocabulary gap, smt has been widely exploited in many applications such as information retrieval (berger and lafferty, 1999; karimzadehgan and zhai, 2010), image and video annotation (duygulu et al , 2002), question answering (berger et al , 2000; echihabi and marcu, 2003; <papid> P03-1003 </papid>murdock and croft, 2004; soricut and brill, 2006;xue et al , 2008), query expansion and rewriting (riezler et al , 2007; <papid> P07-1059 </papid>riezler et al , 2008; riezler and liu, 2010), <papid> J10-3010 </papid>summarization (banko et al , 2000), <papid> P00-1041 </papid>collocation extraction (liu et al , 2009<papid> D09-1051 </papid>b; liu et al , 2010<papid> P10-1085 </papid>b) and paraphrasing (quirk et al , 2004; <papid> W04-3219 </papid>zhao et al , 2010).<papid> C10-1148 </papid></citsent>
<aftsection>
<nextsent>although smt is widely adopted solution to vocabulary gap, for various applications using smt, the crucial and non-trivial problem is to find appropriate and enough translation pairs for smt.
</nextsent>
<nextsent>the most straightforward translation pairs for key phrase extraction is document-keyphrase pairs.in practice, however, it is time-consuming to annotate large collection of documents with key phrases for sufficient wam training.
</nextsent>
<nextsent>in order to solve the problem, we use titles and summaries to build translation pairs with documents.
</nextsent>
<nextsent>titles and summaries are usually accompanying with the corresponding documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4453">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on the idea of translation, we use word alignment models (wam) (brown et al , 1993) <papid> J93-2003 </papid>in statistical machine translation (smt) (koehn, 2010) and propose unified framework for key phrase extraction: (1) from collection of translation pairsof two languages, wam learns translation probabilities between the words in the two languages.</prevsent>
<prevsent>(2) according to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate key phrases, which may not necessarily frequent in their corresponding documents.</prevsent>
</prevsection>
<citsent citstr=" C10-1148 ">
as promising approach to solve the problem of vocabulary gap, smt has been widely exploited in many applications such as information retrieval (berger and lafferty, 1999; karimzadehgan and zhai, 2010), image and video annotation (duygulu et al , 2002), question answering (berger et al , 2000; echihabi and marcu, 2003; <papid> P03-1003 </papid>murdock and croft, 2004; soricut and brill, 2006;xue et al , 2008), query expansion and rewriting (riezler et al , 2007; <papid> P07-1059 </papid>riezler et al , 2008; riezler and liu, 2010), <papid> J10-3010 </papid>summarization (banko et al , 2000), <papid> P00-1041 </papid>collocation extraction (liu et al , 2009<papid> D09-1051 </papid>b; liu et al , 2010<papid> P10-1085 </papid>b) and paraphrasing (quirk et al , 2004; <papid> W04-3219 </papid>zhao et al , 2010).<papid> C10-1148 </papid></citsent>
<aftsection>
<nextsent>although smt is widely adopted solution to vocabulary gap, for various applications using smt, the crucial and non-trivial problem is to find appropriate and enough translation pairs for smt.
</nextsent>
<nextsent>the most straightforward translation pairs for key phrase extraction is document-keyphrase pairs.in practice, however, it is time-consuming to annotate large collection of documents with key phrases for sufficient wam training.
</nextsent>
<nextsent>in order to solve the problem, we use titles and summaries to build translation pairs with documents.
</nextsent>
<nextsent>titles and summaries are usually accompanying with the corresponding documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4454">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>the most simple unsupervised method for key phrase extraction is using tfidf (salton and buckley, 1988) to rank the candidate key phrases and select the top-ranked ones as keyphrases.
</prevsent>
<prevsent>tfidf ranks candidate key phrases only according to their statistical frequencies, which thus fails to suggest key phrases with low frequencies.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
starting with text rank (mihalcea and tarau, 2004), <papid> W04-3252 </papid>graph-based ranking methods are becoming the state-of-the-art methods for key phrase extraction (liu et al , 2009<papid> D09-1051 </papid>a; liu et al , 2010<papid> P10-1085 </papid>a).</citsent>
<aftsection>
<nextsent>given document, text rank first builds word graph, in which the links between words indicate their semantic relatedness, which are estimated by the word co-occurrences in the document.
</nextsent>
<nextsent>by executing page rank (page et al , 1998) on the graph, we obtain the page rank score for each word to rank candidate keyphrases.
</nextsent>
<nextsent>in text rank, low-frequency word will benefit from its high-frequency neighbor words and thus be ranked higher as compared to using tfidf.
</nextsent>
<nextsent>this alleviates the problem of vocabulary gap to some extent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4462">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, text rank usually constructs word graph simply according to word co-occurrences as an approximation of the semantic relations between words.
</prevsent>
<prevsent>this will introduce much noise because of connecting semantically unrelated words and highly influence extraction performance.
</prevsent>
</prevsection>
<citsent citstr=" C08-1122 ">
some methods have been proposed to improvetextrank, of which expand rank (wan and xiao, 2008<papid> C08-1122 </papid>b; wan and xiao, 2008<papid> C08-1122 </papid>a) uses smal number, namely k, of neighbor documents to 136 provide more information of word relatedness for the construction of word graphs.</citsent>
<aftsection>
<nextsent>compared to text rank, expand rank performs better when facing the vocabulary gap by borrowing the information on document level.
</nextsent>
<nextsent>however, the finding of neighbor documents are usually arbitrary.
</nextsent>
<nextsent>this process may introduce much noise and result in topic drift when the document and its so-called neighbor documents are not exactly talking about the same topics.another potential approach to alleviate vocabulary gap is latent topic models (landauer et al , 1998; hofmann, 1999; blei et al , 2003), of which latent dirichlet al ocation (lda) (blei et al , 2003) is most popular.
</nextsent>
<nextsent>latent topic models learn topics from collection of documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4472">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> key phrase extraction by bridging.  </section>
<citcontext>
<prevsection>
<prevsent>given document d, we rank candidate keyphrasesby computing their likelihood pr(p|d).
</prevsent>
<prevsent>each candidate key phrase may be composed of multiple words.
</prevsent>
</prevsection>
<citsent citstr=" W03-1028 ">
as shown in (hulth, 2003), <papid> W03-1028 </papid>most key phrases are noun phrases.</citsent>
<aftsection>
<nextsent>following (mihalcea and tarau, 2004; <papid> W04-3252 </papid>wan and xiao, 2008<papid> C08-1122 </papid>b), we simply select noun phrases from the given document as candidate key phrases with the help of pos tags.</nextsent>
<nextsent>for each word t, we compute its likelihood given d, pr(t|d) = wd pr(t|w)pr(w|d), where pr(w|d) is the weight of the word in d, which is measured using normalized tfidf scores.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4478">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the average number of key phrases for each document is 2.4.
</prevsent>
<prevsent>in experiments, we use the annotated titles and summaries to construct translation pairs.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
in experiments, we select giza++ 3 (och and ney, 2003) <papid> J03-1002 </papid>to train ibm model-1 using translation pairs.</citsent>
<aftsection>
<nextsent>giza++, widely used in various applications of statistical machine translation, implements ibm models 1-5 and an hmm word alignment model.
</nextsent>
<nextsent>to evaluate methods, we use the annotated key phrases by www.163.com as the standardkeyphrases.
</nextsent>
<nextsent>if one suggested key phrase exactly matches one of the standard key phrases, it is correct keyphrase.
</nextsent>
<nextsent>we use precision =ccorrect/cmethod , recall = ccorrect/cstandard and measure = 2pr/(p + r) for evaluation, whereccorrect is the number of key phrases correctly suggested by the given method, cmethod is the number of suggested key phrases, and cstandard is the number of standard keyphrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4484">
<title id=" W11-0316.xml">automatic key phrase extraction by bridging vocabulary gap </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>in all, based on the above analysis on two parameters, we demonstrate the effectiveness and robustness of our method for key phrase extraction.
</prevsent>
<prevsent>4.1.3 when titles/summaries are unavailable suppose in some special cases, the titles or summaries are unavailable, how can we construct translation pairs?
</prevsent>
</prevsection>
<citsent citstr=" W00-0405 ">
inspired by extraction-based document summarization (goldstein et al , 2000; <papid> W00-0405 </papid>mihalcea and tarau, 2004), <papid> W04-3252 </papid>we can extract one or more important sentences from the given document to construct translation pairs.</citsent>
<aftsection>
<nextsent>unsupervised sentence extraction 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 recall precision ? = 0.01?
</nextsent>
<nextsent>= 0.05?
</nextsent>
<nextsent>= 0.10?
</nextsent>
<nextsent>= 0.30?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4487">
<title id=" W10-4155.xml">a pipeline approach to chinese personal name disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several important studies have tried tosolve the task introduced in the previous section.
</prevsent>
<prevsent>most of them treated it as an clustering problem.
</prevsent>
</prevsection>
<citsent citstr=" P98-1012 ">
bagga &amp; baldwin (1998) <papid> P98-1012 </papid>first selected tokens from local context as feature sto perform intra-document coreference resolution.</citsent>
<aftsection>
<nextsent>mann &amp; yarowsky (2003) <papid> W03-0405 </papid>extracted local biographical information as features.</nextsent>
<nextsent>niu et al (2004) <papid> P04-1076 </papid>used relation extraction results in addition to local context features and get perfect results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4488">
<title id=" W10-4155.xml">a pipeline approach to chinese personal name disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most of them treated it as an clustering problem.
</prevsent>
<prevsent>bagga &amp; baldwin (1998) <papid> P98-1012 </papid>first selected tokens from local context as feature sto perform intra-document coreference resolution.</prevsent>
</prevsection>
<citsent citstr=" W03-0405 ">
mann &amp; yarowsky (2003) <papid> W03-0405 </papid>extracted local biographical information as features.</citsent>
<aftsection>
<nextsent>niu et al (2004) <papid> P04-1076 </papid>used relation extraction results in addition to local context features and get perfect results.</nextsent>
<nextsent>al-kamha and embley (2004)clustered search results with feature set including attributes, links and page similarities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4489">
<title id=" W10-4155.xml">a pipeline approach to chinese personal name disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>bagga &amp; baldwin (1998) <papid> P98-1012 </papid>first selected tokens from local context as feature sto perform intra-document coreference resolution.</prevsent>
<prevsent>mann &amp; yarowsky (2003) <papid> W03-0405 </papid>extracted local biographical information as features.</prevsent>
</prevsection>
<citsent citstr=" P04-1076 ">
niu et al (2004) <papid> P04-1076 </papid>used relation extraction results in addition to local context features and get perfect results.</citsent>
<aftsection>
<nextsent>al-kamha and embley (2004)clustered search results with feature set including attributes, links and page similarities.
</nextsent>
<nextsent>in recent years, this problem has attracted great deal of attention from many research institutes.
</nextsent>
<nextsent>ying chen et al (2009) <papid> P09-3011 </papid>used web 1t 5-gram corpus released by google to extract additional features for clustering.masaki ikeda et al (2009) proposed two stage clustering algorithm to improve the low recall values.</nextsent>
<nextsent>in the first stage, some reliable features (like named entities) are used to connect documents about the same person.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4490">
<title id=" W10-4155.xml">a pipeline approach to chinese personal name disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>al-kamha and embley (2004)clustered search results with feature set including attributes, links and page similarities.
</prevsent>
<prevsent>in recent years, this problem has attracted great deal of attention from many research institutes.
</prevsent>
</prevsection>
<citsent citstr=" P09-3011 ">
ying chen et al (2009) <papid> P09-3011 </papid>used web 1t 5-gram corpus released by google to extract additional features for clustering.masaki ikeda et al (2009) proposed two stage clustering algorithm to improve the low recall values.</citsent>
<aftsection>
<nextsent>in the first stage, some reliable features (like named entities) are used to connect documents about the same person.
</nextsent>
<nextsent>after that, the connected documents (document cluster) are used as source from which new features (compound keyword features) are extracted.
</nextsent>
<nextsent>these new features are used in the second stage to make additional connections between documents.
</nextsent>
<nextsent>their approach is to im prove clusters step by step, where each step refines clusters conservatively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4495">
<title id=" W11-0505.xml">wikitopics what is popular on wikipedia and why </title>
<section> clustering.  </section>
<citcontext>
<prevsection>
<prevsent>we used the mallet software (mccallum, 2002) to run these topic models.
</prevsent>
<prevsent>we retrieve the latest revision of each article on the day that wikitopics selected it.
</prevsent>
</prevsection>
<citsent citstr=" W02-0109 ">
we strip unnecessary html tags and wiki templates with mwlib5 and split sentences with nltk (loper and bird, 2002).<papid> W02-0109 </papid></citsent>
<aftsection>
<nextsent>normalization, tokenization, and stop words removal were performed, but no stemming was performed.
</nextsent>
<nextsent>the uni gram (bag-of-words) model was used and the number 5http://code.pediapress.com/wiki/wiki/mwlib 35 test set # clusters b3 f-score human-1 48.6 0.70 ? 0.08 human-2 50.0 0.71 ? 0.11 human-3 53.8 0.74 ? 0.10 concomp 31.8 0.42 ? 0.18 onehop 45.2 0.58 ? 0.17 k-means tf 50 0.52 ? 0.04 k-means tf-idf 50 0.58 ? 0.09 lda 44.8 0.43 ? 0.08 table 1: clustering evaluation: f-scores are averaged across gold standard datasets.
</nextsent>
<nextsent>concomp and onehop are using the link structure.
</nextsent>
<nextsent>k-means clustering with tf-idf performs best.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4496">
<title id=" W11-0505.xml">wikitopics what is popular on wikipedia and why </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>hierarchical clustering like three-level clustering of news blaster (hatzi vassiloglou et al , 2000) could be applied to wikitopics to organize current events hierarchically.
</prevsent>
<prevsent>summarizing multiple sentences that are extracted from the articles inthe same cluster would provide comprehensive description about the current event.
</prevsent>
</prevsection>
<citsent citstr=" P10-1058 ">
integer linear programming based models (woodsend and lapata, 2010) <papid> P10-1058 </papid>may prove to be useful to generate summaries while global constraints like length, grammar, and coverage are met.</citsent>
<aftsection>
<nextsent>the problem of topic detection and tracking (tdt) is to identify and follow new events in newswire, and to detect the first story about new event (allan et al , 1998).
</nextsent>
<nextsent>allan et al  (2000) evaluated variety of vector space clustering schemes, where the best settings from those experiments were then used in our work.
</nextsent>
<nextsent>this was followed recently by petrovic?
</nextsent>
<nextsent>et al  (2010), who took an approximate approach to first story detection, as applied to twitter in an on-line streaming setting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4497">
<title id=" W11-0505.xml">wikitopics what is popular on wikipedia and why </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>et al  (2010), who took an approximate approach to first story detection, as applied to twitter in an on-line streaming setting.
</prevsent>
<prevsent>such system might provide additional information to wikitopics by helping to identify and describe current events that have yet to be explicitly described in wikipedia article.
</prevsent>
</prevsection>
<citsent citstr=" D07-1047 ">
svoreet al  (2007) <papid> D07-1047 </papid>explored enhancing single-document summariation using news query logs, which may also be applicable to wikitopics.</citsent>
<aftsection>
<nextsent>wikipedias inter-article links have been utilized to 39construct topic ontology (syed et al , 2008), word segmentation corpora (gabay et al , 2008), or to compute semantic relatedness (milne and witten, 2008).
</nextsent>
<nextsent>in our work, we found the link structure to be as useful to cluster topically related articles as well as the article text.
</nextsent>
<nextsent>in future work, the text and the link structure will be combine das chaudhuri et al  (2009) explored multi-view hierarchical clustering for wikipedia articles.
</nextsent>
<nextsent>we have described pipeline for article selection, clustering, and textual ization in order to identify and describe significant current events as according to wikipedia content, and metadata.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4498">
<title id=" W10-4173.xml">applying spectral clustering for chinese word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the disadvantage of wsd is that it derives the senses of word from existing dictionaries or other corpus and the senses cannot be extended to other domains.
</prevsent>
<prevsent>wsi can overcome this problem as it can automatically derive word senses from the given document set, or specific domain.many different approaches based on co occurence have been proposed so far.
</prevsent>
</prevsection>
<citsent citstr=" E06-1018 ">
bordag (2006) <papid> E06-1018 </papid>proposes an approach that uses triplets of co-occurences.</citsent>
<aftsection>
<nextsent>the most significant co occurences of target word are used to build triplets that consist of the target word and its two cooccurences.
</nextsent>
<nextsent>then intersection built from the co occurence list of each word in the triplet is used as feature vector.
</nextsent>
<nextsent>after merging similar triplets that have more than 80% overlapping words, clustering is performed on the triplets.
</nextsent>
<nextsent>triplets with fewer than 4 intersection words are removed in order to reduce noise.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4499">
<title id=" W10-4173.xml">applying spectral clustering for chinese word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after merging similar triplets that have more than 80% overlapping words, clustering is performed on the triplets.
</prevsent>
<prevsent>triplets with fewer than 4 intersection words are removed in order to reduce noise.
</prevsent>
</prevsection>
<citsent citstr=" E09-1013 ">
lda model has also been applied to wsi (brody and lapata, 2009).<papid> E09-1013 </papid></citsent>
<aftsection>
<nextsent>brody proposes method that treats document and topics in ldaas word context and senses respectively.
</nextsent>
<nextsent>the process of generating the context words is as follows:first generate sense from multinomial distribution given context, then generate context words given sense.
</nextsent>
<nextsent>they also derive layered model to incorporate different kind of features and use gibbs sampling method to solve the problem.
</nextsent>
<nextsent>graph-based methods become popular recently.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4500">
<title id=" W10-4173.xml">applying spectral clustering for chinese word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>graph-based methods become popular recently.
</prevsent>
<prevsent>these methods use the co-occurence graph of context words to obtain sense clusters based on sub-graph density.
</prevsent>
</prevsection>
<citsent citstr=" W07-2075 ">
markov clustering(mcl) has been used to identify dense regions of graph (agirre and soroa, 2007).<papid> W07-2075 </papid></citsent>
<aftsection>
<nextsent>spectral clustering performs well on problems in which points cluster based on shape.
</nextsent>
<nextsent>the method is that first compute the laplace matrix of the affinity matrix, then reform the data points by stacking the largest eigenvectors of the laplace matrix in columns, finally cluster the new data points using more simple clustering method like k-means (ng et al, 2001).
</nextsent>
<nextsent>our approach follows common cluster model that represents the given context as word vector and later uses spectral clustering method to group each instance in its own cluster.
</nextsent>
<nextsent>different types of polysemy may arise and the most significant distinction may be the syntactic classes of the word and the conceptually different senses (bordag, 2006).<papid> E06-1018 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4502">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>this paper makes two contributions to the area of single-word based word alignment for bilingual sentence pairs.
</prevsent>
</prevsection>
<citsent citstr=" W09-1804 ">
firstly, it integrates the ? seemingly rather different ? works of(bodrumlu et al , 2009)<papid> W09-1804 </papid>and the standard probabilistic ones into single framework.secondly, we present two algorithms to optimize the arising task.</citsent>
<aftsection>
<nextsent>the first is an iterative scheme similar to viterbi training, able to handle large tasks.
</nextsent>
<nextsent>the second is based on the in exact solution of an integer program.
</nextsent>
<nextsent>while it can handle only small corpora, it allows more insight into the quality of the model and the performance of the iterative scheme.
</nextsent>
<nextsent>finally, we present an alternative way to handle prior dictionary knowledge and discuss connections to computing ibm-3 viterbi alignments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4504">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while it can handle only small corpora, it allows more insight into the quality of the model and the performance of the iterative scheme.
</prevsent>
<prevsent>finally, we present an alternative way to handle prior dictionary knowledge and discuss connections to computing ibm-3 viterbi alignments.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the training of single word based translation models (brown et al , 1993<papid> J93-2003 </papid>b; vogel et al , 1996) <papid> C96-2141 </papid>is an essential building block for most state-of-the-art translation systems.</citsent>
<aftsection>
<nextsent>indeed, even more refined translation models (wang and waibel, 1998; <papid> P98-2221 </papid>sumita et al , 2004; deng and byrne, 2005; <papid> H05-1022 </papid>fraser and marcu, 2007<papid> J07-3002 </papid>a) are initial ized by the parameters of single word based ones.</nextsent>
<nextsent>the exception is here the joint approach of marcu and wong (2002), <papid> W02-1018 </papid>but its refinement by birch et al  (2006) <papid> W06-3123 </papid>again relies on the well known ibm models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4510">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while it can handle only small corpora, it allows more insight into the quality of the model and the performance of the iterative scheme.
</prevsent>
<prevsent>finally, we present an alternative way to handle prior dictionary knowledge and discuss connections to computing ibm-3 viterbi alignments.
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
the training of single word based translation models (brown et al , 1993<papid> J93-2003 </papid>b; vogel et al , 1996) <papid> C96-2141 </papid>is an essential building block for most state-of-the-art translation systems.</citsent>
<aftsection>
<nextsent>indeed, even more refined translation models (wang and waibel, 1998; <papid> P98-2221 </papid>sumita et al , 2004; deng and byrne, 2005; <papid> H05-1022 </papid>fraser and marcu, 2007<papid> J07-3002 </papid>a) are initial ized by the parameters of single word based ones.</nextsent>
<nextsent>the exception is here the joint approach of marcu and wong (2002), <papid> W02-1018 </papid>but its refinement by birch et al  (2006) <papid> W06-3123 </papid>again relies on the well known ibm models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4511">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, we present an alternative way to handle prior dictionary knowledge and discuss connections to computing ibm-3 viterbi alignments.
</prevsent>
<prevsent>the training of single word based translation models (brown et al , 1993<papid> J93-2003 </papid>b; vogel et al , 1996) <papid> C96-2141 </papid>is an essential building block for most state-of-the-art translation systems.</prevsent>
</prevsection>
<citsent citstr=" P98-2221 ">
indeed, even more refined translation models (wang and waibel, 1998; <papid> P98-2221 </papid>sumita et al , 2004; deng and byrne, 2005; <papid> H05-1022 </papid>fraser and marcu, 2007<papid> J07-3002 </papid>a) are initial ized by the parameters of single word based ones.</citsent>
<aftsection>
<nextsent>the exception is here the joint approach of marcu and wong (2002), <papid> W02-1018 </papid>but its refinement by birch et al  (2006) <papid> W06-3123 </papid>again relies on the well known ibm models.</nextsent>
<nextsent>traditionally (brown et al , 1993<papid> J93-2003 </papid>b; al-onaizan et al , 1999) single word based models are trained by the em-algorithm, which has the advantageous property that the collection of counts can be decomposed over the sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4512">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, we present an alternative way to handle prior dictionary knowledge and discuss connections to computing ibm-3 viterbi alignments.
</prevsent>
<prevsent>the training of single word based translation models (brown et al , 1993<papid> J93-2003 </papid>b; vogel et al , 1996) <papid> C96-2141 </papid>is an essential building block for most state-of-the-art translation systems.</prevsent>
</prevsection>
<citsent citstr=" H05-1022 ">
indeed, even more refined translation models (wang and waibel, 1998; <papid> P98-2221 </papid>sumita et al , 2004; deng and byrne, 2005; <papid> H05-1022 </papid>fraser and marcu, 2007<papid> J07-3002 </papid>a) are initial ized by the parameters of single word based ones.</citsent>
<aftsection>
<nextsent>the exception is here the joint approach of marcu and wong (2002), <papid> W02-1018 </papid>but its refinement by birch et al  (2006) <papid> W06-3123 </papid>again relies on the well known ibm models.</nextsent>
<nextsent>traditionally (brown et al , 1993<papid> J93-2003 </papid>b; al-onaizan et al , 1999) single word based models are trained by the em-algorithm, which has the advantageous property that the collection of counts can be decomposed over the sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4513">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, we present an alternative way to handle prior dictionary knowledge and discuss connections to computing ibm-3 viterbi alignments.
</prevsent>
<prevsent>the training of single word based translation models (brown et al , 1993<papid> J93-2003 </papid>b; vogel et al , 1996) <papid> C96-2141 </papid>is an essential building block for most state-of-the-art translation systems.</prevsent>
</prevsection>
<citsent citstr=" J07-3002 ">
indeed, even more refined translation models (wang and waibel, 1998; <papid> P98-2221 </papid>sumita et al , 2004; deng and byrne, 2005; <papid> H05-1022 </papid>fraser and marcu, 2007<papid> J07-3002 </papid>a) are initial ized by the parameters of single word based ones.</citsent>
<aftsection>
<nextsent>the exception is here the joint approach of marcu and wong (2002), <papid> W02-1018 </papid>but its refinement by birch et al  (2006) <papid> W06-3123 </papid>again relies on the well known ibm models.</nextsent>
<nextsent>traditionally (brown et al , 1993<papid> J93-2003 </papid>b; al-onaizan et al , 1999) single word based models are trained by the em-algorithm, which has the advantageous property that the collection of counts can be decomposed over the sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4517">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the training of single word based translation models (brown et al , 1993<papid> J93-2003 </papid>b; vogel et al , 1996) <papid> C96-2141 </papid>is an essential building block for most state-of-the-art translation systems.</prevsent>
<prevsent>indeed, even more refined translation models (wang and waibel, 1998; <papid> P98-2221 </papid>sumita et al , 2004; deng and byrne, 2005; <papid> H05-1022 </papid>fraser and marcu, 2007<papid> J07-3002 </papid>a) are initial ized by the parameters of single word based ones.</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
the exception is here the joint approach of marcu and wong (2002), <papid> W02-1018 </papid>but its refinement by birch et al  (2006) <papid> W06-3123 </papid>again relies on the well known ibm models.</citsent>
<aftsection>
<nextsent>traditionally (brown et al , 1993<papid> J93-2003 </papid>b; al-onaizan et al , 1999) single word based models are trained by the em-algorithm, which has the advantageous property that the collection of counts can be decomposed over the sentences.</nextsent>
<nextsent>refinements that also allow symmetrized models are based on bipartite graph matching (matusov et al , 2004; <papid> C04-1032 </papid>taskar et al ,2005) <papid> H05-1010 </papid>or quadratic assignment problems (lacoste julien et al , 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4518">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the training of single word based translation models (brown et al , 1993<papid> J93-2003 </papid>b; vogel et al , 1996) <papid> C96-2141 </papid>is an essential building block for most state-of-the-art translation systems.</prevsent>
<prevsent>indeed, even more refined translation models (wang and waibel, 1998; <papid> P98-2221 </papid>sumita et al , 2004; deng and byrne, 2005; <papid> H05-1022 </papid>fraser and marcu, 2007<papid> J07-3002 </papid>a) are initial ized by the parameters of single word based ones.</prevsent>
</prevsection>
<citsent citstr=" W06-3123 ">
the exception is here the joint approach of marcu and wong (2002), <papid> W02-1018 </papid>but its refinement by birch et al  (2006) <papid> W06-3123 </papid>again relies on the well known ibm models.</citsent>
<aftsection>
<nextsent>traditionally (brown et al , 1993<papid> J93-2003 </papid>b; al-onaizan et al , 1999) single word based models are trained by the em-algorithm, which has the advantageous property that the collection of counts can be decomposed over the sentences.</nextsent>
<nextsent>refinements that also allow symmetrized models are based on bipartite graph matching (matusov et al , 2004; <papid> C04-1032 </papid>taskar et al ,2005) <papid> H05-1010 </papid>or quadratic assignment problems (lacoste julien et al , 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4525">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the exception is here the joint approach of marcu and wong (2002), <papid> W02-1018 </papid>but its refinement by birch et al  (2006) <papid> W06-3123 </papid>again relies on the well known ibm models.</prevsent>
<prevsent>traditionally (brown et al , 1993<papid> J93-2003 </papid>b; al-onaizan et al , 1999) single word based models are trained by the em-algorithm, which has the advantageous property that the collection of counts can be decomposed over the sentences.</prevsent>
</prevsection>
<citsent citstr=" C04-1032 ">
refinements that also allow symmetrized models are based on bipartite graph matching (matusov et al , 2004; <papid> C04-1032 </papid>taskar et al ,2005) <papid> H05-1010 </papid>or quadratic assignment problems (lacoste julien et al , 2006).</citsent>
<aftsection>
<nextsent>recently, bodrumlu et al (2009) <papid> W09-1804 </papid>proposed the first method that treats non decomposable problem by handling all sentence pairs at once and via integer linear programming.</nextsent>
<nextsent>their (non-probabilistic) approach finds dictionaries with minimal number of entries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4526">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the exception is here the joint approach of marcu and wong (2002), <papid> W02-1018 </papid>but its refinement by birch et al  (2006) <papid> W06-3123 </papid>again relies on the well known ibm models.</prevsent>
<prevsent>traditionally (brown et al , 1993<papid> J93-2003 </papid>b; al-onaizan et al , 1999) single word based models are trained by the em-algorithm, which has the advantageous property that the collection of counts can be decomposed over the sentences.</prevsent>
</prevsection>
<citsent citstr=" H05-1010 ">
refinements that also allow symmetrized models are based on bipartite graph matching (matusov et al , 2004; <papid> C04-1032 </papid>taskar et al ,2005) <papid> H05-1010 </papid>or quadratic assignment problems (lacoste julien et al , 2006).</citsent>
<aftsection>
<nextsent>recently, bodrumlu et al (2009) <papid> W09-1804 </papid>proposed the first method that treats non decomposable problem by handling all sentence pairs at once and via integer linear programming.</nextsent>
<nextsent>their (non-probabilistic) approach finds dictionaries with minimal number of entries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4529">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for the unsupervised problem addressed in this paper they have recently been introduced in the form of posterior constraints (ganchev et al ,2010).
</prevsent>
<prevsent>in related fields of nlp lately dirichlet priors have been investigated, e.g.
</prevsent>
</prevsection>
<citsent citstr=" D07-1031 ">
(johnson, 2007).<papid> D07-1031 </papid>we present two strategies to handle the objective function addressed in this paper.</citsent>
<aftsection>
<nextsent>one of these schemes relies, like (germann et al , 2004; lacoste julien et al , 2006; denero and klein, 2008; <papid> P08-2007 </papid>bodrumlu et al , 2009), <papid> W09-1804 </papid>on integer linear programming 172 (see e.g.</nextsent>
<nextsent>(schrijver, 1986; achterberg, 2007)), but due to the large-scale nature of our problem we solve only the lp-relaxation, followed by successive strengthening.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4530">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in related fields of nlp lately dirichlet priors have been investigated, e.g.
</prevsent>
<prevsent>(johnson, 2007).<papid> D07-1031 </papid>we present two strategies to handle the objective function addressed in this paper.</prevsent>
</prevsection>
<citsent citstr=" P08-2007 ">
one of these schemes relies, like (germann et al , 2004; lacoste julien et al , 2006; denero and klein, 2008; <papid> P08-2007 </papid>bodrumlu et al , 2009), <papid> W09-1804 </papid>on integer linear programming 172 (see e.g.</citsent>
<aftsection>
<nextsent>(schrijver, 1986; achterberg, 2007)), but due to the large-scale nature of our problem we solve only the lp-relaxation, followed by successive strengthening.
</nextsent>
<nextsent>for the latter, we develop our own, exponentially large set of cuts and show that it can be handled as polynomially sized system, though in practice this is too inefficient.
</nextsent>
<nextsent>before we introduce our objective function we givea brief description of the (standard) models we consider.
</nextsent>
<nextsent>in all cases, one is given set of bilingual sentence pairs containing foreign language and english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4534">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> integer linear programming.  </section>
<citcontext>
<prevsection>
<prevsent>we limit this method to the models ibm-1 and ibm-2.
</prevsent>
<prevsent>handling an hmm would be possible, but its first order alignment model would introduce many more variables.
</prevsent>
</prevsection>
<citsent citstr=" J10-3001 ">
handling the ibm-3, based on (ravi and knight, 2010; <papid> J10-3001 </papid>schoenemann, 2010) <papid> W10-2913 </papid>seems more promising direction.</citsent>
<aftsection>
<nextsent>4.1 an ilp for the regularized ibm-1.
</nextsent>
<nextsent>the basis of our ilp is the fact that the counts nf,e and ne can only assume finite, a-priori known setof integral values, including 0.
</nextsent>
<nextsent>we introduce binary variable ncf,e ? {0, 1} for each possible value c, where we want ncf,e = 1 if nf,e(as) = c, otherwise ncf,e = 0.
</nextsent>
<nextsent>this is analogous for the variables nce and ne(as).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4536">
<title id=" W11-0320.xml">probabilistic word alignment under the l0norm </title>
<section> integer linear programming.  </section>
<citcontext>
<prevsection>
<prevsent>we limit this method to the models ibm-1 and ibm-2.
</prevsent>
<prevsent>handling an hmm would be possible, but its first order alignment model would introduce many more variables.
</prevsent>
</prevsection>
<citsent citstr=" W10-2913 ">
handling the ibm-3, based on (ravi and knight, 2010; <papid> J10-3001 </papid>schoenemann, 2010) <papid> W10-2913 </papid>seems more promising direction.</citsent>
<aftsection>
<nextsent>4.1 an ilp for the regularized ibm-1.
</nextsent>
<nextsent>the basis of our ilp is the fact that the counts nf,e and ne can only assume finite, a-priori known setof integral values, including 0.
</nextsent>
<nextsent>we introduce binary variable ncf,e ? {0, 1} for each possible value c, where we want ncf,e = 1 if nf,e(as) = c, otherwise ncf,e = 0.
</nextsent>
<nextsent>this is analogous for the variables nce and ne(as).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4564">
<title id=" W10-4135.xml">high oovrecall chinese word segmenter </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this word segmentation competition, unfortunately, only small size of unlabeled target domain data is available.
</prevsent>
<prevsent>thus we focus on handling out-of-vocabulary (oov) words.
</prevsent>
</prevsection>
<citsent citstr=" N06-2049 ">
forthis purpose, our system is based on combination of subword-based tagging method (zhang et al., 2006) <papid> N06-2049 </papid>and access or variety-based new word recognition method (feng et al, 2004).<papid> J04-1004 </papid></citsent>
<aftsection>
<nextsent>in more detail, we adopted and extended subword-basedmethod.
</nextsent>
<nextsent>subword list is augmented with new word list recognized by access or variety method.
</nextsent>
<nextsent>feature template description a) cn(2,1, 0, 1, 2) unigram of characters b) cncn+1(2,1, 0, 1) bigram of characters c) cn1cncn+1(1, 0, 1) trigram of characters d) pu(c0) whether punctuation e) (c1)t (c0)t (c+1) type of characters table 1: basic features for crf-based segmenter we participated in the close track of the word segmentation competition, on all the four test datasets, in two of which our system is ranked at the 1st position with respect to the metric of oov recall.
</nextsent>
<nextsent>2.1 subword-based tagging with crfs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4565">
<title id=" W10-4135.xml">high oovrecall chinese word segmenter </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this word segmentation competition, unfortunately, only small size of unlabeled target domain data is available.
</prevsent>
<prevsent>thus we focus on handling out-of-vocabulary (oov) words.
</prevsent>
</prevsection>
<citsent citstr=" J04-1004 ">
forthis purpose, our system is based on combination of subword-based tagging method (zhang et al., 2006) <papid> N06-2049 </papid>and access or variety-based new word recognition method (feng et al, 2004).<papid> J04-1004 </papid></citsent>
<aftsection>
<nextsent>in more detail, we adopted and extended subword-basedmethod.
</nextsent>
<nextsent>subword list is augmented with new word list recognized by access or variety method.
</nextsent>
<nextsent>feature template description a) cn(2,1, 0, 1, 2) unigram of characters b) cncn+1(2,1, 0, 1) bigram of characters c) cn1cncn+1(1, 0, 1) trigram of characters d) pu(c0) whether punctuation e) (c1)t (c0)t (c+1) type of characters table 1: basic features for crf-based segmenter we participated in the close track of the word segmentation competition, on all the four test datasets, in two of which our system is ranked at the 1st position with respect to the metric of oov recall.
</nextsent>
<nextsent>2.1 subword-based tagging with crfs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4568">
<title id=" W10-4135.xml">high oovrecall chinese word segmenter </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>feature template description a) cn(2,1, 0, 1, 2) unigram of characters b) cncn+1(2,1, 0, 1) bigram of characters c) cn1cncn+1(1, 0, 1) trigram of characters d) pu(c0) whether punctuation e) (c1)t (c0)t (c+1) type of characters table 1: basic features for crf-based segmenter we participated in the close track of the word segmentation competition, on all the four test datasets, in two of which our system is ranked at the 1st position with respect to the metric of oov recall.
</prevsent>
<prevsent>2.1 subword-based tagging with crfs.
</prevsent>
</prevsection>
<citsent citstr=" I08-4017 ">
the backbone of our system is character-based segmenter with the application of conditional random fields (crfs) (zhao and kit, 2008).<papid> I08-4017 </papid></citsent>
<aftsection>
<nextsent>in detail, we apply six-tag tagging scheme, as in(zhao et al, 2006).<papid> W06-0127 </papid></nextsent>
<nextsent>that is , each chinese character can be assigned to one of the tags in {b, b2, b3, , e, }.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4569">
<title id=" W10-4135.xml">high oovrecall chinese word segmenter </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 subword-based tagging with crfs.
</prevsent>
<prevsent>the backbone of our system is character-based segmenter with the application of conditional random fields (crfs) (zhao and kit, 2008).<papid> I08-4017 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-0127 ">
in detail, we apply six-tag tagging scheme, as in(zhao et al, 2006).<papid> W06-0127 </papid></citsent>
<aftsection>
<nextsent>that is , each chinese character can be assigned to one of the tags in {b, b2, b3, , e, }.
</nextsent>
<nextsent>refer to (zhao et al, 2006) <papid> W06-0127 </papid>for detailed meaning of the tags.</nextsent>
<nextsent>table 1 shows basic feature templates used in our system, where feature templates a, b, d, are also used in (zhu et al., 2006) <papid> W06-0141 </papid>for svm-based word segmentation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4571">
<title id=" W10-4135.xml">high oovrecall chinese word segmenter </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>that is , each chinese character can be assigned to one of the tags in {b, b2, b3, , e, }.
</prevsent>
<prevsent>refer to (zhao et al, 2006) <papid> W06-0127 </papid>for detailed meaning of the tags.</prevsent>
</prevsection>
<citsent citstr=" W06-0141 ">
table 1 shows basic feature templates used in our system, where feature templates a, b, d, are also used in (zhu et al., 2006) <papid> W06-0141 </papid>for svm-based word segmentation.</citsent>
<aftsection>
<nextsent>in order to extend basic crf-based segmenter,we first collect 2k most frequent words from training data.
</nextsent>
<nextsent>hereafter, the list of such words is referred to as subword list.
</nextsent>
<nextsent>moreover, single character words 1, if they are not contained inthe subword list, are also added.
</nextsent>
<nextsent>such proce 1by single-character word, we refer to words that consist solely of chinese character.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4581">
<title id=" W11-1608.xml">framework for abs tractive summarization using texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>according to recent study (genest et al, 2009b), there is an empirical limit intrinsic to pure extraction, ascom pared to abstraction.
</prevsent>
<prevsent>for these reasons, as well as for the technical and theoretical challenges involved, wewere motivated to come up with an abs tractive summarization model.
</prevsent>
</prevsection>
<citsent citstr=" J05-3002 ">
recent abs tractive approaches, such as sentence compression (knight and marcu, 2000) (cohn and lapata, 2009) and sentence fusion (barzilay and mckeown, 2005) <papid> J05-3002 </papid>or revision (tanaka et al, 2009)<papid> W09-2808 </papid>have focused on rewriting techniques, without consideration for complete model which would include transition to an abstract representation for content selection.</citsent>
<aftsection>
<nextsent>we believe that fully abstrac tive?
</nextsent>
<nextsent>approach requires separate process for the analysis of the text that serves as an intermediate step before the generation of sentences.
</nextsent>
<nextsent>this way,content selection can be applied to an abstract representation rather than to original sentences or generated sentences.
</nextsent>
<nextsent>we propose the concept of information items (init) to help define the abstract representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4583">
<title id=" W11-1608.xml">framework for abs tractive summarization using texttotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>according to recent study (genest et al, 2009b), there is an empirical limit intrinsic to pure extraction, ascom pared to abstraction.
</prevsent>
<prevsent>for these reasons, as well as for the technical and theoretical challenges involved, wewere motivated to come up with an abs tractive summarization model.
</prevsent>
</prevsection>
<citsent citstr=" W09-2808 ">
recent abs tractive approaches, such as sentence compression (knight and marcu, 2000) (cohn and lapata, 2009) and sentence fusion (barzilay and mckeown, 2005) <papid> J05-3002 </papid>or revision (tanaka et al, 2009)<papid> W09-2808 </papid>have focused on rewriting techniques, without consideration for complete model which would include transition to an abstract representation for content selection.</citsent>
<aftsection>
<nextsent>we believe that fully abstrac tive?
</nextsent>
<nextsent>approach requires separate process for the analysis of the text that serves as an intermediate step before the generation of sentences.
</nextsent>
<nextsent>this way,content selection can be applied to an abstract representation rather than to original sentences or generated sentences.
</nextsent>
<nextsent>we propose the concept of information items (init) to help define the abstract representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4586">
<title id=" W11-1608.xml">framework for abs tractive summarization using texttotext generation </title>
<section> abs tractive summarization at tac 2010.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 init retrieval.
</prevsent>
<prevsent>an init is defined as dated and located subject?
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
verb object triple, relying mostly on syntactical analyses from the minipar parser (lin, 1998) and linguistic annotations from the gate information extraction engine (cunningham et al, 2002).<papid> P02-1022 </papid>every verb encountered forms the basis of candidate init.</citsent>
<aftsection>
<nextsent>the verbs subject and object are extracted, if they exist, from the parse tree.
</nextsent>
<nextsent>each initis also tagged with date and location, if appropriate.
</nextsent>
<nextsent>many candidate inits are rejected, for various reasons: the difficulty of generating grammatical and meaningful sentence from them, the observed unreliability of parses that include them, or because it would lead to incorrect inits most of the time.
</nextsent>
<nextsent>the rejection rules were created manually and cover number of syntactical situations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4587">
<title id=" W11-1608.xml">framework for abs tractive summarization using texttotext generation </title>
<section> abs tractive summarization at tac 2010.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 generation.
</prevsent>
<prevsent>from each init retrieved, we directly generate new sentence, instead of first selecting inits and planning the summary.
</prevsent>
</prevsection>
<citsent citstr=" W09-0613 ">
this is accomplished using the original parse tree of the sentence from which the init is taken, and the nlg realizer simplenlg(gatt and reiter, 2009) <papid> W09-0613 </papid>to generate an actual sen tence.</citsent>
<aftsection>
<nextsent>sample generated sentences are illustrated in figure 2.
</nextsent>
<nextsent>this process ? type of text-to-text generation ? can be described as translating the parts that we want to keep from the dependency tree provided by the parser, into format that the realizer understands.
</nextsent>
<nextsent>this way we keep track of what words play what role in the generated sentence and we select directly which parts of sentence appear in generated sentence for the summary.
</nextsent>
<nextsent>all of this is driven by the previous identification of inits.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4588">
<title id=" W11-1608.xml">framework for abs tractive summarization using texttotext generation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this purpose is not the same as ours, and triplet extraction was conducted quite superficially (and thus included lot of noise), whereas we used several rules to clean up the svos that would serve as inits.
</prevsent>
<prevsent>rewriting sentences one idea at time, as we have done in this work, is also related to the field of text simplification.
</prevsent>
</prevsection>
<citsent citstr=" C96-2183 ">
text simplification has been associated with techniques that deal not only with helping readers with reading disabilities, but also to help nlp systems (chandrasekar et al, 1996).<papid> C96-2183 </papid></citsent>
<aftsection>
<nextsent>the work of (beigman klebanov et al, 2004) simplifies sentences by using minipar parses as starting point, in process similar to ours, for the purpose of helping information-seeking applications in their own task.
</nextsent>
<nextsent>(vickrey and koller, 2008) <papid> P08-1040 </papid>applies similar techniques, using sequence of rule-based simplifications of sentences, to pre process documents for semantic role labeling.</nextsent>
<nextsent>(siddharthan et al, 2004)<papid> C04-1129 </papid>uses shallow techniques for syntactical simplification of text by removing relative clauses and apposi tives, before running sentence clustering algorithm for multi-document summarization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4589">
<title id=" W11-1608.xml">framework for abs tractive summarization using texttotext generation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>text simplification has been associated with techniques that deal not only with helping readers with reading disabilities, but also to help nlp systems (chandrasekar et al, 1996).<papid> C96-2183 </papid></prevsent>
<prevsent>the work of (beigman klebanov et al, 2004) simplifies sentences by using minipar parses as starting point, in process similar to ours, for the purpose of helping information-seeking applications in their own task.</prevsent>
</prevsection>
<citsent citstr=" P08-1040 ">
(vickrey and koller, 2008) <papid> P08-1040 </papid>applies similar techniques, using sequence of rule-based simplifications of sentences, to pre process documents for semantic role labeling.</citsent>
<aftsection>
<nextsent>(siddharthan et al, 2004)<papid> C04-1129 </papid>uses shallow techniques for syntactical simplification of text by removing relative clauses and apposi tives, before running sentence clustering algorithm for multi-document summarization.</nextsent>
<nextsent>the kind of text-to-text generation involved in ourwork is related to approaches in paraphrasing (an droutsopoulos and malakasiotis, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4590">
<title id=" W11-1608.xml">framework for abs tractive summarization using texttotext generation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the work of (beigman klebanov et al, 2004) simplifies sentences by using minipar parses as starting point, in process similar to ours, for the purpose of helping information-seeking applications in their own task.
</prevsent>
<prevsent>(vickrey and koller, 2008) <papid> P08-1040 </papid>applies similar techniques, using sequence of rule-based simplifications of sentences, to pre process documents for semantic role labeling.</prevsent>
</prevsection>
<citsent citstr=" C04-1129 ">
(siddharthan et al, 2004)<papid> C04-1129 </papid>uses shallow techniques for syntactical simplification of text by removing relative clauses and apposi tives, before running sentence clustering algorithm for multi-document summarization.</citsent>
<aftsection>
<nextsent>the kind of text-to-text generation involved in ourwork is related to approaches in paraphrasing (an droutsopoulos and malakasiotis, 2010).
</nextsent>
<nextsent>paraphrase generation produces sentences with similar meanings, but paraphrase extraction from texts requiresa certain level of analysis.
</nextsent>
<nextsent>in our case, we are interested both in reformulating specific aspects of sentence, but also in identifying parts of sentences (inits) with similar meanings, for content selection.we believe that there will be more and more similarities between our work and the field of paraphrasing as we improve on our model and techniques.
</nextsent>
<nextsent>we have proposed an ambitious new way of looking at abs tractive summarization, with our proposed framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4591">
<title id=" W11-1511.xml">what we know about the voynich manuscript </title>
<section> the letter.  </section>
<citcontext>
<prevsection>
<prevsent>guy (1991) applies the vowel-consonant separation algorithm of (sukhotin, 1962) on two pagesof the biological section, and finds that four characters (o, a, c, g) separate out as vowels.
</prevsent>
<prevsent>however, the separation is not very strong, and several words do not contain these characters.
</prevsent>
</prevsection>
<citsent citstr=" P06-2065 ">
another method is to use two-state bigram hmm (knight et al, 2006; <papid> P06-2065 </papid>goldsmith and xanthos, 2009) over letters, and induce two clusters of letters with em.</citsent>
<aftsection>
<nextsent>in alphabetic languages like english, the clusters correspond almost perfectly to vowels andconsonants.
</nextsent>
<nextsent>we find that curious phenomenon occurs with the vms ? the last character of every word is generated by one of the hmm states, and all other characters by another; i.e., the word grammar is ab. there are few possible interpretations of this.
</nextsent>
<nextsent>itis possible that the vowels from every word are removed and placed at the end of the word, but this means that even long words have only one vowel, which is unlikely.
</nextsent>
<nextsent>further, the number of vowel types would be nearly half the alphabet size.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4592">
<title id=" W11-1511.xml">what we know about the voynich manuscript </title>
<section> the word.  </section>
<citcontext>
<prevsection>
<prevsent>stolfi later modified this to core-mantel-crust?
</prevsent>
<prevsent>model, where words are composed of three nested layers.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
to determine whether vms words have affixalmorphology, we run an unsupervised morphological segmentation algorithm, linguist ica (goldsmith,2001), <papid> J01-2001 </papid>on the vms text.</citsent>
<aftsection>
<nextsent>the mdl-based algorithm segments words into prefix+stem+suffix, and extracts signatures?, sets of affixes that attach to thesame set of stems.
</nextsent>
<nextsent>table 3 lists few sample signatures, showing that stems in the same signature tend to have some structural similarities.
</nextsent>
<nextsent>table 3: some morphological signatures.
</nextsent>
<nextsent>affixes stems oe+, a3 ad ae ae9 aeor aj am an ar at op+, o o2 oe oj om on or null+ saj sar scc9 scco sco2 so oe+ bsc28 bsc9 ccc8 coc8cr faeoe fak fau fc8 fc8am fcc fcc2 fcc9r fccae fccc2 fcccar9 fco9 fcs9 fczar fczc9 oear9 oesc9 of9 or8 sc29 sc89o sc8r scx9 sq9 +89, 4ofcs 4ofcz 4ofz 4opz 8aes 8aez +9, 9fs 9ps efcs fcs ps pz + c89 oefs of ofaes ofcs ofs ofz
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4593">
<title id=" W11-0329.xml">learning discriminative projections for text similarity measures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method notonly outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient.
</prevsent>
<prevsent>measures of text similarity have many applications and have been studied extensively in both the nlp and ir communities.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
for example, combination of corpus and knowledge based methods have been invented for judging word similarity (lin, 1998;<papid> P98-2127 </papid>agirre et al , 2009).<papid> N09-1003 </papid></citsent>
<aftsection>
<nextsent>similarity derived from large scale web corpus has been used for automatically extending lists of typed entities (vyas and pantel, 2009).<papid> N09-1033 </papid></nextsent>
<nextsent>judging the degree of similarity between documents is also fundamental to classical ir problems such as document retrieval (manning et al , 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4594">
<title id=" W11-0329.xml">learning discriminative projections for text similarity measures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method notonly outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient.
</prevsent>
<prevsent>measures of text similarity have many applications and have been studied extensively in both the nlp and ir communities.
</prevsent>
</prevsection>
<citsent citstr=" N09-1003 ">
for example, combination of corpus and knowledge based methods have been invented for judging word similarity (lin, 1998;<papid> P98-2127 </papid>agirre et al , 2009).<papid> N09-1003 </papid></citsent>
<aftsection>
<nextsent>similarity derived from large scale web corpus has been used for automatically extending lists of typed entities (vyas and pantel, 2009).<papid> N09-1033 </papid></nextsent>
<nextsent>judging the degree of similarity between documents is also fundamental to classical ir problems such as document retrieval (manning et al , 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4595">
<title id=" W11-0329.xml">learning discriminative projections for text similarity measures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>measures of text similarity have many applications and have been studied extensively in both the nlp and ir communities.
</prevsent>
<prevsent>for example, combination of corpus and knowledge based methods have been invented for judging word similarity (lin, 1998;<papid> P98-2127 </papid>agirre et al , 2009).<papid> N09-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" N09-1033 ">
similarity derived from large scale web corpus has been used for automatically extending lists of typed entities (vyas and pantel, 2009).<papid> N09-1033 </papid></citsent>
<aftsection>
<nextsent>judging the degree of similarity between documents is also fundamental to classical ir problems such as document retrieval (manning et al , 2008).
</nextsent>
<nextsent>in all these applications, the vector-based similarity method is the most widely used.
</nextsent>
<nextsent>term vectors are first constructed to represent the original text objects, where each term is associated with weight indicating its importance.
</nextsent>
<nextsent>a pre-selected function operating on these vectors, such as cosine,is used to output the final similarity score.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4596">
<title id=" W11-0329.xml">learning discriminative projections for text similarity measures </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>latent dirichlet allocation (lda) (blei et al , 2003) generalizes plsa to proper generative model for documents and places dirichlet priors over the parameters ? and ?.
</prevsent>
<prevsent>in the experiments in this paper, our implementation of plsa is lda with maximum posteriori (map) inference, which was shown to be comparable to the current best bayesian inference methods for lda (asuncion et al , 2009).recently, these topic models have been generalized to handle pairs or tuples of corresponding documents, which could be translations in multiple languages, or documents in the same language that are considered similar.
</prevsent>
</prevsection>
<citsent citstr=" D09-1092 ">
for instance, the poly-lingual topic model (pltm) (mimno et al , 2009) <papid> D09-1092 </papid>is an extension to lda that views documents in tu pleas having shared topic vector ?.</citsent>
<aftsection>
<nextsent>each of the documents in the tuple uses ? to select the topicsz of tokens, but could use different (language specific) word-topic-distribution multi(lz ).
</nextsent>
<nextsent>two additional models, joint plsa (jplsa) and coupled plsa (cplsa) were introduced in (platt et al ,2010).<papid> D10-1025 </papid></nextsent>
<nextsent>jplsa is close variant of pltm when documents of all languages share the same word-topicdistribution parameters, and map inference is performed instead of bayesian.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4597">
<title id=" W11-0329.xml">learning discriminative projections for text similarity measures </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, the poly-lingual topic model (pltm) (mimno et al , 2009) <papid> D09-1092 </papid>is an extension to lda that views documents in tu pleas having shared topic vector ?.</prevsent>
<prevsent>each of the documents in the tuple uses ? to select the topicsz of tokens, but could use different (language specific) word-topic-distribution multi(lz ).</prevsent>
</prevsection>
<citsent citstr=" D10-1025 ">
two additional models, joint plsa (jplsa) and coupled plsa (cplsa) were introduced in (platt et al ,2010).<papid> D10-1025 </papid></citsent>
<aftsection>
<nextsent>jplsa is close variant of pltm when documents of all languages share the same word-topicdistribution parameters, and map inference is performed instead of bayesian.
</nextsent>
<nextsent>cplsa extends jplsa by constraining paired documents to not only share the same prior topic distribution ?, but to also have similar fractions of tokens assigned to each topic.this constraint is enforced on expectation using posterior regularization (ganchev et al , 2009).
</nextsent>
<nextsent>2.2 linear projection methods.
</nextsent>
<nextsent>the earliest method for projecting term vectors intoa low-dimensional concept space is latent semantic analysis (lsa) (deerwester et al , 1990).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4599">
<title id=" W11-0329.xml">learning discriminative projections for text similarity measures </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 comparable document retrieval.
</prevsent>
<prevsent>with the growth of multiple languages on the web,there is an increasing demand of processing cross lingual documents.
</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
for instance, machine translation (mt) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the web (munteanu and marcu, 2005).<papid> J05-4003 </papid></citsent>
<aftsection>
<nextsent>word-level translation lexicons can also be learned from comparable documents (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999).<papid> P99-1067 </papid></nextsent>
<nextsent>in this cross-lingual document retrieval task, given query document in one language, the goal is to find the most similar document from the corpus in another language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4600">
<title id=" W11-0329.xml">learning discriminative projections for text similarity measures </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>with the growth of multiple languages on the web,there is an increasing demand of processing cross lingual documents.
</prevsent>
<prevsent>for instance, machine translation (mt) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the web (munteanu and marcu, 2005).<papid> J05-4003 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
word-level translation lexicons can also be learned from comparable documents (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>in this cross-lingual document retrieval task, given query document in one language, the goal is to find the most similar document from the corpus in another language.
</nextsent>
<nextsent>4.1.1 data &amp; setting we followed the comparable document retrieval setting described in (platt et al , 2010) <papid> D10-1025 </papid>and evaluated s2net on the wikipedia dataset used in that pa per.</nextsent>
<nextsent>this dataset consists of wikipedia documents 1without the ? parameter, the model still outperforms other baselines in our experiments, but with much smaller gain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4601">
<title id=" W11-0329.xml">learning discriminative projections for text similarity measures </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>with the growth of multiple languages on the web,there is an increasing demand of processing cross lingual documents.
</prevsent>
<prevsent>for instance, machine translation (mt) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the web (munteanu and marcu, 2005).<papid> J05-4003 </papid></prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
word-level translation lexicons can also be learned from comparable documents (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>in this cross-lingual document retrieval task, given query document in one language, the goal is to find the most similar document from the corpus in another language.
</nextsent>
<nextsent>4.1.1 data &amp; setting we followed the comparable document retrieval setting described in (platt et al , 2010) <papid> D10-1025 </papid>and evaluated s2net on the wikipedia dataset used in that pa per.</nextsent>
<nextsent>this dataset consists of wikipedia documents 1without the ? parameter, the model still outperforms other baselines in our experiments, but with much smaller gain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4609">
<title id=" W11-0329.xml">learning discriminative projections for text similarity measures </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, targeting the application of face verification, cho praet al  (2005) used convolutional network and designed contrastive loss function for optimizing eucliden distance metric.
</prevsent>
<prevsent>in contrast, the network of s2net is equivalent to linear projection matrix and has pairwise loss function.
</prevsent>
</prevsection>
<citsent citstr=" P10-1040 ">
in terms of the learning framework, s2net is closely related to several neural network based approaches, including autoencoders (hinton and salakhutdinov, 2006) and finding low-dimensional word representations (collobert and weston, 2008; turian et al , 2010).<papid> P10-1040 </papid></citsent>
<aftsection>
<nextsent>architecturally, s2net is also similar to ranknet (burgeset al , 2005), which can be viewed as siamese neural network that learns ranking function.the strategy that s2net takes to learn from labeled pairs of documents can be analogous to the work of distance metric learning.
</nextsent>
<nextsent>although high dimensionality is not problem to algorithms like hdlr, it suffers from different scala bility issue.as we have observed in our experiments, the algorithm can only handle small number of similarity/dissimilarity constraints (i.e., the labeled ex amples), and is not able to use large number of examples to learn better model.
</nextsent>
<nextsent>empirically, wealso found that hdlr is very sensitive to the hyperparameter settings and its performance can vary substantially from iteration to iteration.other than the applications presented in this paper, concept vectors have shown useful in traditionalir tasks.
</nextsent>
<nextsent>for instance, egozi et al  (2008) use explicit semantic analysis to improve the retrieval recall by leveraging wikipedia.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4610">
<title id=" W11-0329.xml">learning discriminative projections for text similarity measures </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in companion paper, we also demonstrated that various topic models including s2net can enhance the ranking function (gao et al , 2011).
</prevsent>
<prevsent>for text categorization, similarity between terms is often encoded as kernel functions embedded in the learning algorithms, and thus increase the classification accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W05-0601 ">
representative approaches include latent semantic kernels (cristianini et al , 2002), which learns an lsa-based kernel function from document collection, and work that computes term-similarity based on the linguistic knowledge provided by wordnet (basili et al , 2005; <papid> W05-0601 </papid>bloehdorn and moschitti, 2007).</citsent>
<aftsection>
<nextsent>in this paper, we presented s2net, discriminative approach for learning projection matrix that maps raw term-vectors to low-dimensional space.
</nextsent>
<nextsent>our learning method directly optimizes the model so that the cosine score of the projected vectors can become reliable similarity measure.
</nextsent>
<nextsent>the strength of this model design has been shown empirically in two very different tasks.
</nextsent>
<nextsent>for cross-lingual document retrieval, s2net significantly outperforms opca, which is the best prior approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4611">
<title id=" W11-1706.xml">developing robust models for favour ability analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>media analysis is discipline closely related to content analysis (krippendorff, 2004), with an emphasis on analysing content with respect to: favour ability how favourable an article is with respect to an entity.
</prevsent>
<prevsent>this will typically be on five point scale: very negative, negative, neutral, positive or very positive.key messages topics or areas that client is interested in.
</prevsent>
</prevsection>
<citsent citstr=" W10-2315 ">
this allows the client to gain feedback on the success of particular public relations campaigns, for example.media analysis has traditionally been done manually, however the explosion of content on the world wide web, in particular social media, has led to the introduction of automatic techniques for performing media analysis, e.g. tatzl and waldhauser (2010).<papid> W10-2315 </papid>in this paper, we discuss our recent findings in applying machine learning techniques to favourabilityanalysis.</citsent>
<aftsection>
<nextsent>the work is part of two-year collaboration between gorkana group, which includes one of the foremost media analysis companies, metrica, and the university of hertfordshire.
</nextsent>
<nextsent>the goal is to develop ways of automating media analysis, especially for social media.
</nextsent>
<nextsent>the data used are from traditional media (newspapers and magazines) since at the time of starting the experiment there was more manually analysed data available.
</nextsent>
<nextsent>we discuss the typical problems that arise in this kind of text mining, and the practical results we have found.the documents are supplied by dur rants, the media monitoring company within the gorkana group, and consist of text from newspaper and magazine articles in electronic form.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4612">
<title id=" W11-1706.xml">developing robust models for favour ability analysis </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>2a headline from tech crunch in inter-annotator agreement, which they report as81%, after implementing improvements to the process.
</prevsent>
<prevsent>melville et al (2009) report on an automated system for opinion mining applied to blogs, which achieves between 64% and 91% accuracy, depending on the domain, while godbole et al (2007) describe system applied to news and blogs.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
pang et al (2002) <papid> W02-1011 </papid>introduced machine learning to perform sentiment analysis.</citsent>
<aftsection>
<nextsent>they used nave bayes,support vector machines (svms) and maximum entropy on the movie review domain, and report accuracies between 77% and 83% depending on the feature set, which included unigrams, bigrams, and part-of-speech tagged unigrams.
</nextsent>
<nextsent>more recent work along these lines is described in (pang and lee, 2008; prabowo and thelwall, 2009).
</nextsent>
<nextsent>one approach to sentiment analysis is to build up lexicon of sentiment carrying words.
</nextsent>
<nextsent>turney (2002) <papid> P02-1053 </papid>described way to automatically build such lexicon based on looking at co-occurrences of words with other words whose sentiment is known.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4613">
<title id=" W11-1706.xml">developing robust models for favour ability analysis </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>more recent work along these lines is described in (pang and lee, 2008; prabowo and thelwall, 2009).
</prevsent>
<prevsent>one approach to sentiment analysis is to build up lexicon of sentiment carrying words.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
turney (2002) <papid> P02-1053 </papid>described way to automatically build such lexicon based on looking at co-occurrences of words with other words whose sentiment is known.</citsent>
<aftsection>
<nextsent>this idea was extended by gamon et al (2005) who also considered the lack of co-occurrence as useful infor mation.koppel and schler (2006) show that it is important to distinguish the two tasks of determining neutral from non-neutral sentiment, and positive versus negative sentiment, and that doing so can significantly improve the accuracy of automated systems.
</nextsent>
<nextsent>2.2 machine learning approaches.
</nextsent>
<nextsent>document classification is an ideal domain forma chine learning, because the raw data, the text, are easily manipulated, and often large amounts of text can be obtained, making the problems amenable to statistical analysis.
</nextsent>
<nextsent>a classification model is essentially mapping, from document described as set of feature values to class label.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4614">
<title id=" W11-1706.xml">developing robust models for favour ability analysis </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, we evaluate technique to reduce the number of features using attribute selection.alternative approaches to understanding the sentiment of text attempt to go beyond the simple labelling of the presence of word.
</prevsent>
<prevsent>some authors have described experiments augmenting the above feature sets with additional information.
</prevsent>
</prevsection>
<citsent citstr=" W04-3253 ">
mullen and collier (2004), <papid> W04-3253 </papid>for example, uses wordnet to add information about words found within text, and consequently reports improved classification performance in sentiment analysis task.</citsent>
<aftsection>
<nextsent>2.3 imbalanced data.
</nextsent>
<nextsent>our datasets, as is usual in many real-world applications, present varying degrees of imbalance between the two classes.
</nextsent>
<nextsent>imbalanced data must be dealt with at two parts of the process: during training, to ensure the model is capable of working with both classes, and in evaluation, to ensure model with the best performance is selected for use on novel data.
</nextsent>
<nextsent>these two elements are often treated together, but need tobe considered separately.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4617">
<title id=" W11-1706.xml">developing robust models for favour ability analysis </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>accuracy on the held out dataset is up to 10% lower than the cross-validation accuracy on the pseudo-subjectivity task, and up to 6% lower on the pseudo-sentiment task.
</prevsent>
<prevsent>this is probably due to achange in topics over time.
</prevsent>
</prevsection>
<citsent citstr=" P09-1027 ">
this degradation in performance could be reduced by techniques such asthose used to improve cross-domain sentiment analysis (li et al, 2009; wan, 2009).<papid> P09-1027 </papid></citsent>
<aftsection>
<nextsent>4.2 features.
</nextsent>
<nextsent>trigrams proved the most effective feature type in 3 out of the 5 different experiments, with unigrams and entity words proving the best in 1 case each.
</nextsent>
<nextsent>however, in many cases, there is not significant difference between the results for different datasets.
</nextsent>
<nextsent>although we only computed dependencies for one dataset, s, we found that they did not provide significant benefit on their own.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4618">
<title id=" W11-1702.xml">a verb lexicon model for deep sentiment analysis and opinion mining applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this sentence expresses positive sentiment of the killers towards the fact they murdered damilola and it expresses the negative attitude on behalf of the speaker/writer who has negative opinion of the the murderers of damilola.
</prevsent>
<prevsent>both attitudes are part of the semantic profile of the verb and should be modelled in subjectivity lexicon.
</prevsent>
</prevsection>
<citsent citstr=" D08-1083 ">
as opinion mining and sentiment analysis applications tend to utilize more and more the composition of sentences (moilanen (2007), choi and cardie (2008), <papid> D08-1083 </papid>jia et al (2009)) and to use the value and properties of the verbs expressed by its dependency trees, there is need for specialized lexicons where this information can be found.</citsent>
<aftsection>
<nextsent>for the analysis of more complex opinionated text like news, political documents, and (online) debates the identification of the attitude holder and topic are of crucial importance.
</nextsent>
<nextsent>applications that exploit the relations between the verb meaning and its arguments can better determine sentiment at sentence level and trace emotions and opninions to their holders.
</nextsent>
<nextsent>our model seeks to combine the insights from rather complex model like framenet (ruppenhofer et al (2010)) with operational models like senti wordnet where simple polarity values (positive, negative, neutral) are applied to the entire lexicon.
</nextsent>
<nextsent>subjectivity relations that exist between the different participants are labeled with information concerning both the identity of the attitude holder and the orientation (positive vs. negative) of the attitude.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4619">
<title id=" W11-1702.xml">a verb lexicon model for deep sentiment analysis and opinion mining applications </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for english, couple of smaller and larger lexicons are available.
</prevsent>
<prevsent>widely used in sentiment analysis are automatically derived or manually built polarity lexicons.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
these lexicons are lists of words (for example, hatzivassiloglou and mckeown (1997), <papid> P97-1023 </papid>kamps et al (2004), kim and hovy (2004) <papid> C04-1200 </papid>or word senses (for example, esuli and sebastiani (2006), wiebe and mihalcea (2006), <papid> P06-1134 </papid>su and markert, (2008)) <papid> W08-1207 </papid>annotated for negative or positive polarity.</citsent>
<aftsection>
<nextsent>as they attribute single polarity values (positive, negative, neutral) to words they are not able to account for more complex cases like boast (cf.
</nextsent>
<nextsent>example 1) which carry both negative and positive polarity dependening on who is the attitude holder.
</nextsent>
<nextsent>strapparava and valitutti (2004) developed wordnet-affect, an affective extension of wordnet.
</nextsent>
<nextsent>it describes direct?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4620">
<title id=" W11-1702.xml">a verb lexicon model for deep sentiment analysis and opinion mining applications </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for english, couple of smaller and larger lexicons are available.
</prevsent>
<prevsent>widely used in sentiment analysis are automatically derived or manually built polarity lexicons.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
these lexicons are lists of words (for example, hatzivassiloglou and mckeown (1997), <papid> P97-1023 </papid>kamps et al (2004), kim and hovy (2004) <papid> C04-1200 </papid>or word senses (for example, esuli and sebastiani (2006), wiebe and mihalcea (2006), <papid> P06-1134 </papid>su and markert, (2008)) <papid> W08-1207 </papid>annotated for negative or positive polarity.</citsent>
<aftsection>
<nextsent>as they attribute single polarity values (positive, negative, neutral) to words they are not able to account for more complex cases like boast (cf.
</nextsent>
<nextsent>example 1) which carry both negative and positive polarity dependening on who is the attitude holder.
</nextsent>
<nextsent>strapparava and valitutti (2004) developed wordnet-affect, an affective extension of wordnet.
</nextsent>
<nextsent>it describes direct?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4621">
<title id=" W11-1702.xml">a verb lexicon model for deep sentiment analysis and opinion mining applications </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for english, couple of smaller and larger lexicons are available.
</prevsent>
<prevsent>widely used in sentiment analysis are automatically derived or manually built polarity lexicons.
</prevsent>
</prevsection>
<citsent citstr=" P06-1134 ">
these lexicons are lists of words (for example, hatzivassiloglou and mckeown (1997), <papid> P97-1023 </papid>kamps et al (2004), kim and hovy (2004) <papid> C04-1200 </papid>or word senses (for example, esuli and sebastiani (2006), wiebe and mihalcea (2006), <papid> P06-1134 </papid>su and markert, (2008)) <papid> W08-1207 </papid>annotated for negative or positive polarity.</citsent>
<aftsection>
<nextsent>as they attribute single polarity values (positive, negative, neutral) to words they are not able to account for more complex cases like boast (cf.
</nextsent>
<nextsent>example 1) which carry both negative and positive polarity dependening on who is the attitude holder.
</nextsent>
<nextsent>strapparava and valitutti (2004) developed wordnet-affect, an affective extension of wordnet.
</nextsent>
<nextsent>it describes direct?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4622">
<title id=" W11-1702.xml">a verb lexicon model for deep sentiment analysis and opinion mining applications </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for english, couple of smaller and larger lexicons are available.
</prevsent>
<prevsent>widely used in sentiment analysis are automatically derived or manually built polarity lexicons.
</prevsent>
</prevsection>
<citsent citstr=" W08-1207 ">
these lexicons are lists of words (for example, hatzivassiloglou and mckeown (1997), <papid> P97-1023 </papid>kamps et al (2004), kim and hovy (2004) <papid> C04-1200 </papid>or word senses (for example, esuli and sebastiani (2006), wiebe and mihalcea (2006), <papid> P06-1134 </papid>su and markert, (2008)) <papid> W08-1207 </papid>annotated for negative or positive polarity.</citsent>
<aftsection>
<nextsent>as they attribute single polarity values (positive, negative, neutral) to words they are not able to account for more complex cases like boast (cf.
</nextsent>
<nextsent>example 1) which carry both negative and positive polarity dependening on who is the attitude holder.
</nextsent>
<nextsent>strapparava and valitutti (2004) developed wordnet-affect, an affective extension of wordnet.
</nextsent>
<nextsent>it describes direct?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4623">
<title id=" W11-1702.xml">a verb lexicon model for deep sentiment analysis and opinion mining applications </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, our description is not at the level of the synset but at lexical unit level which enables us to differentiate gradations of the strength of emotions within the synsets.
</prevsent>
<prevsent>this enables us to relate the attitudes directly to the syntactic-semantic patterns of the lexical unit.
</prevsent>
</prevsection>
<citsent citstr=" W06-0301 ">
also framenet (ruppenhofer et al (2010)) is used as resource in opinion mining and sentiment analysis (kim and hovy (2006)).<papid> W06-0301 </papid></citsent>
<aftsection>
<nextsent>framenet (fn) is an online lexical resource for english that contains more than 11,600 lexical units.
</nextsent>
<nextsent>the aim is to classify words into categories (frames) which give for each lexical unit the range of semantic and syntactic combinatory possibilities.
</nextsent>
<nextsent>the semantic roles range from general ones like agent, patient and theme to specific ones such as speaker, message and addressee for verbs of communication.
</nextsent>
<nextsent>fn includes frames such as communication, judgment, opinion, emotion_directed and semantic roles such as judge, experiencer, communicator which are highly relevant for opinion mining and sentiment analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4624">
<title id=" W11-0405.xml">consistency maintenance in prosodic labeling for reliable prediction of prosodic breaks </title>
<section> problem description.  </section>
<citcontext>
<prevsection>
<prevsent>the deterioration of the performance presented in section 3.2 is quite considerable, despite the fact that the same genre and level of prosodic break labeling system was selected.
</prevsent>
<prevsent>after analyzing the data, we identified three main reasons as follows.
</prevsent>
</prevsection>
<citsent citstr=" J94-1002 ">
(1) perceptual prominence of prosodic labeling systems despite the fact that three types of prosodic break have been commonly used in the speech engineering field for considerable time as shown in (os tendorf and veilleux, 1994), <papid> J94-1002 </papid>they have not been clearly defined or referenced in standard prosodic labeling conventions.</citsent>
<aftsection>
<nextsent>in particular, the notion of the minor break is rather vague, whereas those of no break and major break are intuitively clear as in (mayo et al, 1996).
</nextsent>
<nextsent>in the mbc news data labeled by postech, sentences that had all prosodic breaks tagged as no break were frequently found, even if two long clauses exist in sentence.
</nextsent>
<nextsent>most sentences had been annotated only with no break.
</nextsent>
<nextsent>the speaking rate of news announcers on air is relatively fast and no obvious audible break seems to exist in their speech.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4625">
<title id=" W11-0405.xml">consistency maintenance in prosodic labeling for reliable prediction of prosodic breaks </title>
<section> corpus building.  </section>
<citcontext>
<prevsection>
<prevsent>the most commonly used methods to assess the level of agreement among transcribers are pairwise analysis and kappa statistics.
</prevsent>
<prevsent>the reliability of inter transcriber agreement of the four experiments has been assessed with these two measurements and the result is given in table 7.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
measurement 1st 2nd 3rd 4th pairwise analysis 0.6385 0.6969 0.7375 0.7477 kappa statistics 0.5783 0.6464 0.6938 0.7057 table 7 reliability of inter transcriber agreement since the value of is greater than 0.67 in the 3rd and 4th experiment, the inter transcriber agreement for annotating prosodic breaks is considered to have reached reliable level as shown in (carletta, 1996).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>then annotation of the main corpus is performed.
</nextsent>
<nextsent>the main corpus comprising 29,686 eo-jeols is divided into five parts.
</nextsent>
<nextsent>each partition is assigned to the trained five transcribers and annotation is independently performed.
</nextsent>
<nextsent>wave surfer, which is used in the training phase, is also used in the annotation phase for the display and annotation of speech.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4626">
<title id=" W10-4217.xml">extracting parallel fragments from comparable corpora for datatotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we investigate the idea of automatically extracting parallel resources for data-to-text generation from comparable corpora obtained from the web.
</prevsent>
<prevsent>we describe our comparable corpus of data and texts relating to british hills and the techniques for extracting paired input/output fragments we have developed so far.
</prevsent>
</prevsection>
<citsent citstr=" P95-1034 ">
starting with knight, langkilde and hatzivas siloglous work on nitrogen and its successor halogen (knight and hatzivassiloglou, 1995; <papid> P95-1034 </papid>knight and langkilde, 2000), nlg has over thepast 15 years moved towards using statistical techniques, in particular in surface realisation (langk ilde, 2002; white, 2004), referring expression generation (most of the sytems submitted to the tuna and grec shared task evaluation challenges are statistical, see gatt et al (2008), <papid> W08-1131 </papid>for example), and data-to-text generation (belz, 2008).the impetus for introducing statistical techniques in nlg can be said to have originally come from machine translation (mt),1 but unlike mt, where parallel corpora of inputs (source language texts) and outputs (translated texts) occur naturally at least in some domains,2 nlg on the whole has to use manually created input/output pairs.</citsent>
<aftsection>
<nextsent>data-to-text generation (d2t) is the type of nlgthat perhaps comes closest to having naturally occuring inputs and outputs at its disposal.
</nextsent>
<nextsent>work in d2t has involved different domains including generating weather forecasts from meteorological 1nitrogen was conceived as an mt system component.
</nextsent>
<nextsent>2canadian and european parliamentary proceedings, etc.data (sripada et al, 2003), nursing reports from intensive care data (portet et al, 2009), and museum exhibit descriptions from database records (isardet al, 2003; stock et al, 2007); types of data include dynamic time-series data (e.g. medical data) and static database entries (museum exhibits).while data and texts in the three example domains cited above do occur naturally, two factors mean they cannot be used directly as example corpora or training data for building d2t systems: one, most are not freely available to researchers (e.g. by simply being available on the web), and two, more problematic ally, for the most part, there is no direct correspondence between inputs and outputs as there is, say, between source language text and its translation.
</nextsent>
<nextsent>on the whole, naturally occurring resources of data and related texts arenot strictly parallel, but are merely what has be come known as comparable in the mt literature, with only subset of data having corresponding text fragments, and other text fragments having no obvious corresponding data items.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4627">
<title id=" W10-4217.xml">extracting parallel fragments from comparable corpora for datatotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we investigate the idea of automatically extracting parallel resources for data-to-text generation from comparable corpora obtained from the web.
</prevsent>
<prevsent>we describe our comparable corpus of data and texts relating to british hills and the techniques for extracting paired input/output fragments we have developed so far.
</prevsent>
</prevsection>
<citsent citstr=" W08-1131 ">
starting with knight, langkilde and hatzivas siloglous work on nitrogen and its successor halogen (knight and hatzivassiloglou, 1995; <papid> P95-1034 </papid>knight and langkilde, 2000), nlg has over thepast 15 years moved towards using statistical techniques, in particular in surface realisation (langk ilde, 2002; white, 2004), referring expression generation (most of the sytems submitted to the tuna and grec shared task evaluation challenges are statistical, see gatt et al (2008), <papid> W08-1131 </papid>for example), and data-to-text generation (belz, 2008).the impetus for introducing statistical techniques in nlg can be said to have originally come from machine translation (mt),1 but unlike mt, where parallel corpora of inputs (source language texts) and outputs (translated texts) occur naturally at least in some domains,2 nlg on the whole has to use manually created input/output pairs.</citsent>
<aftsection>
<nextsent>data-to-text generation (d2t) is the type of nlgthat perhaps comes closest to having naturally occuring inputs and outputs at its disposal.
</nextsent>
<nextsent>work in d2t has involved different domains including generating weather forecasts from meteorological 1nitrogen was conceived as an mt system component.
</nextsent>
<nextsent>2canadian and european parliamentary proceedings, etc.data (sripada et al, 2003), nursing reports from intensive care data (portet et al, 2009), and museum exhibit descriptions from database records (isardet al, 2003; stock et al, 2007); types of data include dynamic time-series data (e.g. medical data) and static database entries (museum exhibits).while data and texts in the three example domains cited above do occur naturally, two factors mean they cannot be used directly as example corpora or training data for building d2t systems: one, most are not freely available to researchers (e.g. by simply being available on the web), and two, more problematic ally, for the most part, there is no direct correspondence between inputs and outputs as there is, say, between source language text and its translation.
</nextsent>
<nextsent>on the whole, naturally occurring resources of data and related texts arenot strictly parallel, but are merely what has be come known as comparable in the mt literature, with only subset of data having corresponding text fragments, and other text fragments having no obvious corresponding data items.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4628">
<title id=" W10-4217.xml">extracting parallel fragments from comparable corpora for datatotext generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the whole, naturally occurring resources of data and related texts arenot strictly parallel, but are merely what has be come known as comparable in the mt literature, with only subset of data having corresponding text fragments, and other text fragments having no obvious corresponding data items.
</prevsent>
<prevsent>moreover,data transformations may be necessary before corresponding text fragments can be identified.in this report, we look at the possibility of automatically extracting parallel data-text fragments from comparable corpora in the case of d2t from static database records.
</prevsent>
</prevsection>
<citsent citstr=" W09-0603 ">
such parallel data-text resource could then be used to train an existingd2t generation system, or even build new statistical generator from scratch, e.g. using techniques from statistical mt (belz and kow, 2009).<papid> W09-0603 </papid></citsent>
<aftsection>
<nextsent>the steps involved in going from comparable data andtext resources to generators that produce texts similar to those in the text resource are then as follows: (1) identify sources on the web for comparable data and texts; (2) pair up data records and texts; (3) extract parallel fragments (sets of data fields paired with word strings); (4) train d2t generator using the parallel fragments; and (5) feed data inputs to the generator which then figure 1: overview of processing steps.generates new texts describing them.
</nextsent>
<nextsent>figure 1 illustrates steps 13 which this paper focuses on.
</nextsent>
<nextsent>in section 3 we look at steps 1 and 2; in section 4 at step 3.
</nextsent>
<nextsent>first we briefly survey related work in mt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4629">
<title id=" W10-4217.xml">extracting parallel fragments from comparable corpora for datatotext generation </title>
<section> related work in mt.  </section>
<citcontext>
<prevsection>
<prevsent>first we briefly survey related work in mt.
</prevsent>
<prevsent>in statistical mt, the expense of manually creating new parallel mt corpora, and the need for very large amounts of parallel training data, has led to sizeable research effort to develop methods for automatically constructing parallel resources.this work typically starts by identifying comparable corpora.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
much of it has focused on identifying word translations incomparable corpora, e.g.rapps approach was based on the simple andel egant assumption that if words af and bf have higher than chance co-occurrence frequency in one language, then two appropriate translations ae and be in another language will also have higher than chance co-occurrence frequency (rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>at the other end of the spectrum, resnik &amp; smith (2003) <papid> J03-3002 </papid>search the web to detect web pages that are translations of each other.</nextsent>
<nextsent>other approaches aim to identify pairsof sentences (munteanu and marcu, 2005) <papid> J05-4003 </papid>or sub sentential fragments (munteanu and marcu, 2006) <papid> P06-1011 </papid>that are parallel within comparable corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4630">
<title id=" W10-4217.xml">extracting parallel fragments from comparable corpora for datatotext generation </title>
<section> related work in mt.  </section>
<citcontext>
<prevsection>
<prevsent>first we briefly survey related work in mt.
</prevsent>
<prevsent>in statistical mt, the expense of manually creating new parallel mt corpora, and the need for very large amounts of parallel training data, has led to sizeable research effort to develop methods for automatically constructing parallel resources.this work typically starts by identifying comparable corpora.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
much of it has focused on identifying word translations incomparable corpora, e.g.rapps approach was based on the simple andel egant assumption that if words af and bf have higher than chance co-occurrence frequency in one language, then two appropriate translations ae and be in another language will also have higher than chance co-occurrence frequency (rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>at the other end of the spectrum, resnik &amp; smith (2003) <papid> J03-3002 </papid>search the web to detect web pages that are translations of each other.</nextsent>
<nextsent>other approaches aim to identify pairsof sentences (munteanu and marcu, 2005) <papid> J05-4003 </papid>or sub sentential fragments (munteanu and marcu, 2006) <papid> P06-1011 </papid>that are parallel within comparable corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4631">
<title id=" W10-4217.xml">extracting parallel fragments from comparable corpora for datatotext generation </title>
<section> related work in mt.  </section>
<citcontext>
<prevsection>
<prevsent>in statistical mt, the expense of manually creating new parallel mt corpora, and the need for very large amounts of parallel training data, has led to sizeable research effort to develop methods for automatically constructing parallel resources.this work typically starts by identifying comparable corpora.
</prevsent>
<prevsent>much of it has focused on identifying word translations incomparable corpora, e.g.rapps approach was based on the simple andel egant assumption that if words af and bf have higher than chance co-occurrence frequency in one language, then two appropriate translations ae and be in another language will also have higher than chance co-occurrence frequency (rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
at the other end of the spectrum, resnik &amp; smith (2003) <papid> J03-3002 </papid>search the web to detect web pages that are translations of each other.</citsent>
<aftsection>
<nextsent>other approaches aim to identify pairsof sentences (munteanu and marcu, 2005) <papid> J05-4003 </papid>or sub sentential fragments (munteanu and marcu, 2006) <papid> P06-1011 </papid>that are parallel within comparable corpora.</nextsent>
<nextsent>the latter approach is particularly relevant toour work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4632">
<title id=" W10-4217.xml">extracting parallel fragments from comparable corpora for datatotext generation </title>
<section> related work in mt.  </section>
<citcontext>
<prevsection>
<prevsent>much of it has focused on identifying word translations incomparable corpora, e.g.rapps approach was based on the simple andel egant assumption that if words af and bf have higher than chance co-occurrence frequency in one language, then two appropriate translations ae and be in another language will also have higher than chance co-occurrence frequency (rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></prevsent>
<prevsent>at the other end of the spectrum, resnik &amp; smith (2003) <papid> J03-3002 </papid>search the web to detect web pages that are translations of each other.</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
other approaches aim to identify pairsof sentences (munteanu and marcu, 2005) <papid> J05-4003 </papid>or sub sentential fragments (munteanu and marcu, 2006) <papid> P06-1011 </papid>that are parallel within comparable corpora.</citsent>
<aftsection>
<nextsent>the latter approach is particularly relevant toour work.
</nextsent>
<nextsent>they start by translating each document in the source language (sl) word for word into the target language (tl).
</nextsent>
<nextsent>the result is given to an information retrieval (ir) system as query, and the top 20 results are retained and paired withthe given sl document.
</nextsent>
<nextsent>they then obtain all sentence pairs from each pair of sl and tl documents, and discard those sentence pairs with few words that are translations of each other.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4633">
<title id=" W10-4217.xml">extracting parallel fragments from comparable corpora for datatotext generation </title>
<section> related work in mt.  </section>
<citcontext>
<prevsection>
<prevsent>much of it has focused on identifying word translations incomparable corpora, e.g.rapps approach was based on the simple andel egant assumption that if words af and bf have higher than chance co-occurrence frequency in one language, then two appropriate translations ae and be in another language will also have higher than chance co-occurrence frequency (rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></prevsent>
<prevsent>at the other end of the spectrum, resnik &amp; smith (2003) <papid> J03-3002 </papid>search the web to detect web pages that are translations of each other.</prevsent>
</prevsection>
<citsent citstr=" P06-1011 ">
other approaches aim to identify pairsof sentences (munteanu and marcu, 2005) <papid> J05-4003 </papid>or sub sentential fragments (munteanu and marcu, 2006) <papid> P06-1011 </papid>that are parallel within comparable corpora.</citsent>
<aftsection>
<nextsent>the latter approach is particularly relevant toour work.
</nextsent>
<nextsent>they start by translating each document in the source language (sl) word for word into the target language (tl).
</nextsent>
<nextsent>the result is given to an information retrieval (ir) system as query, and the top 20 results are retained and paired withthe given sl document.
</nextsent>
<nextsent>they then obtain all sentence pairs from each pair of sl and tl documents, and discard those sentence pairs with few words that are translations of each other.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4635">
<title id=" W10-4217.xml">extracting parallel fragments from comparable corpora for datatotext generation </title>
<section> towards parallel ised corpus.  </section>
<citcontext>
<prevsection>
<prevsent>while it was fine for step 2 to produce some rough matches, in step 3, parallel fragment detection, the aim is to retain only those parts of sentence that can be said to realise some data field(s) in the set of data fields with which it has been matched.
</prevsent>
<prevsent>computing data-text associations: following some preprocessing of sentences where each occurrence of hills name and height is replaced by lexical class tokens name , height metres or height feet , the first step is to construct kind of lexicon of pairs (d,w) of data fields and words w, such that is often seen in the realisation of d. for this purpose we adapt munteanu&amp; marcus (2006) <papid> P06-1011 </papid>method for (language to lan guage) lexicon construction.</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
for this purpose we compute measure of the strength of association between data fields and words; we use the g2 log likelihood ratio which has been widely used for this sort of purpose (especially lexical association) since it was introduced to nlp (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>following moore (2004<papid> W04-3243 </papid>a) rather than munteanu &amp; marcu, our current notion of cooccurrence is that data field and word cooccur if they are present in the same pair of data fields and sentence (as identified by the method described in section 4.1 above).</nextsent>
<nextsent>we then obtain counts for the number of times each word co occurs with each data field, and the number of times it occurs without the data field being present (and conversely).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4636">
<title id=" W10-4217.xml">extracting parallel fragments from comparable corpora for datatotext generation </title>
<section> towards parallel ised corpus.  </section>
<citcontext>
<prevsection>
<prevsent>computing data-text associations: following some preprocessing of sentences where each occurrence of hills name and height is replaced by lexical class tokens name , height metres or height feet , the first step is to construct kind of lexicon of pairs (d,w) of data fields and words w, such that is often seen in the realisation of d. for this purpose we adapt munteanu&amp; marcus (2006) <papid> P06-1011 </papid>method for (language to lan guage) lexicon construction.</prevsent>
<prevsent>for this purpose we compute measure of the strength of association between data fields and words; we use the g2 log likelihood ratio which has been widely used for this sort of purpose (especially lexical association) since it was introduced to nlp (dunning, 1993).<papid> J93-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3243 ">
following moore (2004<papid> W04-3243 </papid>a) rather than munteanu &amp; marcu, our current notion of cooccurrence is that data field and word cooccur if they are present in the same pair of data fields and sentence (as identified by the method described in section 4.1 above).</citsent>
<aftsection>
<nextsent>we then obtain counts for the number of times each word co occurs with each data field, and the number of times it occurs without the data field being present (and conversely).
</nextsent>
<nextsent>this allows us to compute the g2 score, for which we use the formulation from moore (2004<papid> W04-3243 </papid>b) shown in figure 2.</nextsent>
<nextsent>if the g2 score forgiven (d,w) pair is greater than p(d)p(w), then the association is taken to be positive, i.e. is likely to be realisation of d, otherwise the association is taken to be negative, i.e. is likely not to be part of realisation of d.for each we then convert g2 scores to probabilities by dividing g2 by the appropriate normalising factor (the sum over all negative g2 scores for for obtaining the negative association probabilities, and analogously for positive associations).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4654">
<title id=" W10-4164.xml">chinese word sense induction with basic clustering algorithms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so the simplest approaches to wsi involve the use of basic word cooccurrence features and application of classical clustering algorithms, more sophisticated techniques improve performance by introducing new context features, novel clustering algorithms, or both.
</prevsent>
<prevsent>(denkowski, 2009) makes comprehensive survey of techniques for unsupervised word sense induction.
</prevsent>
</prevsection>
<citsent citstr=" W07-2002 ">
two tasks on english word sense induction were held on semeval2007 (agirre and soroa, 2007) <papid> W07-2002 </papid>and semeval2010 (manandhar and klapaftis, 2010) respectively, which greatly promote the research of english wsi.</citsent>
<aftsection>
<nextsent>however, the study on chinese word sense induction (cwsi) is inadequate (zhu, 2009), and chinese word senses have their own characteristics.
</nextsent>
<nextsent>the methods that work well in english may not work well in chinese.
</nextsent>
<nextsent>so, as an exploration, this paper proposes simple approaches utilizing basic features and basic clustering algorithms, such as parti tional method k-means and hierarchical agglomerative method.
</nextsent>
<nextsent>the rest of this paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4655">
<title id=" W11-0126.xml">extracting aspects of determiner meaning from dialogue in a virtual world environment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an example of virtual world project where language is learned through interaction is wubble world?
</prevsent>
<prevsent>(hewlett et al, 2007).
</prevsent>
</prevsection>
<citsent citstr=" W09-0628 ">
in the give challenge (byron et al, 2009) <papid> W09-0628 </papid>virtual world setup is used to evaluate natural language generation systems.</citsent>
<aftsection>
<nextsent>in previous work we have extracted words and multi-word expressions that refer to range of objects that are prominent in our virtual environment (reckman et al, 2010).
</nextsent>
<nextsent>now we investigate if aspects of determiner meaning can be learned from this dataset.
</nextsent>
<nextsent>the extracted knowledge of nouns makes the learning of determiners possible.
</nextsent>
<nextsent>we study what factors contribute to the choice of the determiner and how they relate to each other, by training decision tree classifier using these factors as features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4656">
<title id=" W11-0126.xml">extracting aspects of determiner meaning from dialogue in a virtual world environment </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>7this is typical feature for languages that have articles, and may be expressed through other means in other languages.
</prevsent>
<prevsent>249
</prevsent>
</prevsection>
<citsent citstr=" W02-0109 ">
we use the decision tree classifier from the natural language toolkit for python (loper and bird, 2002) <papid> W02-0109 </papid>and train and test it through 10-fold cross-validation on 74304 noun phrases from 5000 games, 23776 of which actually have determiners.</citsent>
<aftsection>
<nextsent>the noun phrases used all contain nouns that can refer to the selected objects, though we cannot guarantee that they were intended to do so in all cases.
</nextsent>
<nextsent>in fact, we have seen examples where this is clearly not the case, and for example filet, which normally refers to the filet object, is used in the context of salmon.
</nextsent>
<nextsent>this means that there is level of noise in our data.
</nextsent>
<nextsent>the instances where the determiner is absent are very dominant, and this part of the data is necessarily noisy, because of rare determiners that weve missed8, and possibly rather heterogeneous, as there are many reasons why people may choose to not type determiner in chat.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4657">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore,sense inventories such as wordnet contain very fine grained sense distinctions that make the sense disambiguation task very challenging (even for human annotators), whereas very detailed sense distinctions are often irrelevant for practical applications.
</prevsent>
<prevsent>in addition to this, there is growing feeling in the community that wsd should be used and evaluated in real application such as machine translation (mt) or information retrieval (ir) (agirre and edmonds, 2006).an important line of research consists in the development of dedicated wsd modules for mt. instead of assigning sense label from monolingual sense-inventory to the ambiguous words, the wsd system has to predict correct translation forthe ambiguous word in given context.
</prevsent>
</prevsection>
<citsent citstr=" H05-1097 ">
in (vick rey et al, 2005), <papid> H05-1097 </papid>the problem was defined as word translation task.</citsent>
<aftsection>
<nextsent>the translation choices of ambiguous words are gathered from parallel corpus by means of word alignment.
</nextsent>
<nextsent>the authors reported improvements on two simplified translation tasks: word translation and blank filling.
</nextsent>
<nextsent>the evaluation was done on an english-french parallel corpus butis confronted with the important limitation of having only one valid translation (the aligned translation in the parallel corpus) as gold standard translation.
</nextsent>
<nextsent>cabezas and resnik (2005) tried to improve an smt system by adding additional translations to the phrase table, but were confronted with tuning problems of this dedicated wsd feature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4658">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the evaluation was done on an english-french parallel corpus butis confronted with the important limitation of having only one valid translation (the aligned translation in the parallel corpus) as gold standard translation.
</prevsent>
<prevsent>cabezas and resnik (2005) tried to improve an smt system by adding additional translations to the phrase table, but were confronted with tuning problems of this dedicated wsd feature.
</prevsent>
</prevsection>
<citsent citstr=" P06-3010 ">
specia (2006) <papid> P06-3010 </papid>used an inductive logic programming-basedwsd system which was tested on seven ambiguous verbs in english-portuguese translation.</citsent>
<aftsection>
<nextsent>the latter systems already present promising results for theuse of wsd in mt, but really significant improvements in terms of general machine translation qual 52 ity were for the first time obtained by carpuat and wu (2007) <papid> D07-1007 </papid>and chan et al (2007).<papid> P07-1005 </papid></nextsent>
<nextsent>both papers describe the integration of dedicated wsd module in chinese-english statistical machine translation framework and report statistically significant improvements in terms of standard mt evaluation metrics.stroppa et al (2007) take completely different approach to perform some sort of implicit word sense disambiguation in mt. they introducecontext-information features that exploit source similarity, in addition to target similarity that is modeled by the language model, in an smt framework.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4659">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cabezas and resnik (2005) tried to improve an smt system by adding additional translations to the phrase table, but were confronted with tuning problems of this dedicated wsd feature.
</prevsent>
<prevsent>specia (2006) <papid> P06-3010 </papid>used an inductive logic programming-basedwsd system which was tested on seven ambiguous verbs in english-portuguese translation.</prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
the latter systems already present promising results for theuse of wsd in mt, but really significant improvements in terms of general machine translation qual 52 ity were for the first time obtained by carpuat and wu (2007) <papid> D07-1007 </papid>and chan et al (2007).<papid> P07-1005 </papid></citsent>
<aftsection>
<nextsent>both papers describe the integration of dedicated wsd module in chinese-english statistical machine translation framework and report statistically significant improvements in terms of standard mt evaluation metrics.stroppa et al (2007) take completely different approach to perform some sort of implicit word sense disambiguation in mt. they introducecontext-information features that exploit source similarity, in addition to target similarity that is modeled by the language model, in an smt framework.
</nextsent>
<nextsent>forthe estimation of these features that are very similar to the typical wsd local context features (left and right context words, part-of-speech of the focus phrase and context words), they use memory-based classification framework.
</nextsent>
<nextsent>the work we present in this paper is different from previous research in two aspects.
</nextsent>
<nextsent>firstly, we evaluate the performance of two state-of-the-art smt systems and dedicated wsd system on the translation of ambiguous words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4660">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cabezas and resnik (2005) tried to improve an smt system by adding additional translations to the phrase table, but were confronted with tuning problems of this dedicated wsd feature.
</prevsent>
<prevsent>specia (2006) <papid> P06-3010 </papid>used an inductive logic programming-basedwsd system which was tested on seven ambiguous verbs in english-portuguese translation.</prevsent>
</prevsection>
<citsent citstr=" P07-1005 ">
the latter systems already present promising results for theuse of wsd in mt, but really significant improvements in terms of general machine translation qual 52 ity were for the first time obtained by carpuat and wu (2007) <papid> D07-1007 </papid>and chan et al (2007).<papid> P07-1005 </papid></citsent>
<aftsection>
<nextsent>both papers describe the integration of dedicated wsd module in chinese-english statistical machine translation framework and report statistically significant improvements in terms of standard mt evaluation metrics.stroppa et al (2007) take completely different approach to perform some sort of implicit word sense disambiguation in mt. they introducecontext-information features that exploit source similarity, in addition to target similarity that is modeled by the language model, in an smt framework.
</nextsent>
<nextsent>forthe estimation of these features that are very similar to the typical wsd local context features (left and right context words, part-of-speech of the focus phrase and context words), they use memory-based classification framework.
</nextsent>
<nextsent>the work we present in this paper is different from previous research in two aspects.
</nextsent>
<nextsent>firstly, we evaluate the performance of two state-of-the-art smt systems and dedicated wsd system on the translation of ambiguous words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4661">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>english french and englishdutch.
</prevsent>
<prevsent>although it is crucial to measure the general translation quality after integrating dedicated wsd module in the smt system, we think it is equally interesting to conduct dedicated evaluation of the translation quality on ambiguous nouns.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
standard smt evaluation metrics such as bleu (pap ineni et al, 2002) <papid> P02-1040 </papid>or edit-distance metrics (e.g. word error rate) measure the global overlap of the translation with reference, and are thus not very sensitive to wsd errors.</citsent>
<aftsection>
<nextsent>the mis translation of an ambiguous word might be subtle change compared to the reference sentence, but it often drastically affects the global understanding of the sentence.
</nextsent>
<nextsent>secondly, we explore the potential benefits of real multilingual approach to wsd.
</nextsent>
<nextsent>the idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of polysemous word are often lexicalized across languages (resnik and yarowsky, 2000).
</nextsent>
<nextsent>many wsd studies have incorporated this cross-lingual evidence idea and have successfully applied bilingual wsd classifiers (gale and church, 1993; <papid> J93-1004 </papid>ng et al, 2003; <papid> P03-1058 </papid>diab and resnik, 2002) <papid> P02-1033 </papid>or systems that use combination of existing wordnets with multilingual evidence (tufis?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4662">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>secondly, we explore the potential benefits of real multilingual approach to wsd.
</prevsent>
<prevsent>the idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of polysemous word are often lexicalized across languages (resnik and yarowsky, 2000).
</prevsent>
</prevsection>
<citsent citstr=" J93-1004 ">
many wsd studies have incorporated this cross-lingual evidence idea and have successfully applied bilingual wsd classifiers (gale and church, 1993; <papid> J93-1004 </papid>ng et al, 2003; <papid> P03-1058 </papid>diab and resnik, 2002) <papid> P02-1033 </papid>or systems that use combination of existing wordnets with multilingual evidence (tufis?</citsent>
<aftsection>
<nextsent>et al, 2004).
</nextsent>
<nextsent>our wsd system is different in the sense that it is independent from predefined sense-inventory (it only uses the parallel corpus at hand) and that it is truly multilingual as it incorporates information from four other languages (french, dutch, spanish, italian and german depending on the target language of the classifier).
</nextsent>
<nextsent>although our classifiers are still very preliminary in terms of the feature set and parameters that are used, we obtain interesting results on our test sample of ambiguous nouns.
</nextsent>
<nextsent>we therefore believe our system can have real added value for smt, as it can easily be trained for different language pairs on exactly the same corpus which is used to train the smt system, which should make the integration lot easier.the remainder of this paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4663">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>secondly, we explore the potential benefits of real multilingual approach to wsd.
</prevsent>
<prevsent>the idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of polysemous word are often lexicalized across languages (resnik and yarowsky, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P03-1058 ">
many wsd studies have incorporated this cross-lingual evidence idea and have successfully applied bilingual wsd classifiers (gale and church, 1993; <papid> J93-1004 </papid>ng et al, 2003; <papid> P03-1058 </papid>diab and resnik, 2002) <papid> P02-1033 </papid>or systems that use combination of existing wordnets with multilingual evidence (tufis?</citsent>
<aftsection>
<nextsent>et al, 2004).
</nextsent>
<nextsent>our wsd system is different in the sense that it is independent from predefined sense-inventory (it only uses the parallel corpus at hand) and that it is truly multilingual as it incorporates information from four other languages (french, dutch, spanish, italian and german depending on the target language of the classifier).
</nextsent>
<nextsent>although our classifiers are still very preliminary in terms of the feature set and parameters that are used, we obtain interesting results on our test sample of ambiguous nouns.
</nextsent>
<nextsent>we therefore believe our system can have real added value for smt, as it can easily be trained for different language pairs on exactly the same corpus which is used to train the smt system, which should make the integration lot easier.the remainder of this paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4664">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>secondly, we explore the potential benefits of real multilingual approach to wsd.
</prevsent>
<prevsent>the idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of polysemous word are often lexicalized across languages (resnik and yarowsky, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
many wsd studies have incorporated this cross-lingual evidence idea and have successfully applied bilingual wsd classifiers (gale and church, 1993; <papid> J93-1004 </papid>ng et al, 2003; <papid> P03-1058 </papid>diab and resnik, 2002) <papid> P02-1033 </papid>or systems that use combination of existing wordnets with multilingual evidence (tufis?</citsent>
<aftsection>
<nextsent>et al, 2004).
</nextsent>
<nextsent>our wsd system is different in the sense that it is independent from predefined sense-inventory (it only uses the parallel corpus at hand) and that it is truly multilingual as it incorporates information from four other languages (french, dutch, spanish, italian and german depending on the target language of the classifier).
</nextsent>
<nextsent>although our classifiers are still very preliminary in terms of the feature set and parameters that are used, we obtain interesting results on our test sample of ambiguous nouns.
</nextsent>
<nextsent>we therefore believe our system can have real added value for smt, as it can easily be trained for different language pairs on exactly the same corpus which is used to train the smt system, which should make the integration lot easier.the remainder of this paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4665">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> statistical machine translation systems.  </section>
<citcontext>
<prevsection>
<prevsent>smt generates translations on the basis of statistical models whose parameters are derived from the analysis of sentence-aligned parallel text corpora.
</prevsent>
<prevsent>phrase-based smt is considered as the dominant paradigm in mt research today.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
it combines phrase translation model (which is based onthe noisy channel model) and phrase-based decoder in order to find the most probable translation of foreign sentence (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>usually the bayes rule is used to reformulate this translation probability: argmaxep(e|f) = argmaxep(f |e)p(e)this allows for language model p(e) that guarantees the fluency and grammatical correctness of the translation, and separate translation modelp(f |e) that focusses on the quality of the transla 53tion.
</nextsent>
<nextsent>training of both the language model (on monolingual data) as well as the translation model (on bilingual text corpora) requires large amounts of text data.research has pointed out that adding more training data, both for the translation as for the language models, results in better translation quality,(callison-burch et al, 2009).
</nextsent>
<nextsent>therefore it is important to notice that our comparison of the two smt systems is somewhat unfair, as we compared the moses research system (that was trained on the eu roparl corpus) with the google commercial system that is trained on much larger dataset.
</nextsent>
<nextsent>it remains an interesting exercise though, as we consider the commercial system as the upper bound of how far current smt can get in case it has unlimited access to text corpora and computational resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4666">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> statistical machine translation systems.  </section>
<citcontext>
<prevsection>
<prevsent>it remains an interesting exercise though, as we consider the commercial system as the upper bound of how far current smt can get in case it has unlimited access to text corpora and computational resources.
</prevsent>
<prevsent>2.1 moses.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the first statistical machine translation system we used is the off-the-shelf moses toolkit (koehn et al, 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>as the moses system is open-source, well documented, supported by very lively users forum and reaches state-of-the-art performance, it has quickly been adopted by the community and highly stimulated development in the smt field.
</nextsent>
<nextsent>it also features factored translation models, which enable the integration of linguistic and other information at the word level.
</nextsent>
<nextsent>this makes moses good candidate to experiment with for example dedicated wsd module, that requires more enhanced linguistic information (such as lemmas and part-of-speech tags).
</nextsent>
<nextsent>we trained moses for english french and english?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4667">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> statistical machine translation systems.  </section>
<citcontext>
<prevsection>
<prevsent>we can only speculate about the amount of resources that google uses to train its translation engine.
</prevsent>
<prevsent>part of the training data comes from transcripts of united nations meetings(in six official languages) and those of the european parliament (europarl corpus).
</prevsent>
</prevsection>
<citsent citstr=" D07-1090 ">
google research papers report on distributed infrastructure that isused to train on up to two trillion tokens, which result in language models containing up to 300 billion ngrams (brants et al, 2007).<papid> D07-1090 </papid></citsent>
<aftsection>
<nextsent>this section describes the para sense wsd system: multilingual classification-based approach to word sense disambiguation.
</nextsent>
<nextsent>instead of using predefined monolingual sense-inventory such as wordnet, we use language-independent framework where the word senses are derived automatically from word alignments on parallel corpus.
</nextsent>
<nextsent>we used the sentence-aligned europarl corpus (koehn, 2005) for the construction of our wsd module.
</nextsent>
<nextsent>the following six languages were selected: english (our focus language), dutch, french, german, italian and spanish.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4668">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> para sense.  </section>
<citcontext>
<prevsection>
<prevsent>(clwsd) task (lefever and hoste, 2010b), which consists in assigning 54 correct translation in five supported target languages (viz.
</prevsent>
<prevsent>french, italian, spanish, german and dutch) for an ambiguous focus word in given context.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
in order to detect all relevant translations for the twenty ambiguous focus words, we rangiza++ (och and ney, 2003) <papid> J03-1002 </papid>with its default settings on our parallel corpus.</citsent>
<aftsection>
<nextsent>the obtained word alignment output was then considered to be the classification label for the training instances forgiven classifier (e.g. the french translation resulting from the word alignment is the label that is used to train the french classifier).
</nextsent>
<nextsent>this way we obtained all class labels (or oracle translations) for all training instances for our five classifiers (english as an input language and french, german, dutch, italian and spanish as target languages).
</nextsent>
<nextsent>for the experiments described in this paper, we focused on the english?
</nextsent>
<nextsent>french and english dutch classifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4669">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> para sense.  </section>
<citcontext>
<prevsection>
<prevsent>the preprocessed english instances were used as input to build set of commonly used wsd features: ? features related to the focus word itself being the word form of the focus word, the lemma, part-of-speech and chunk information, ? local context features related to window of three words preceding and following the focus word containing for each of these words their full form, lemma, part-of-speech and chunk information these local context features are to be consider edas basic feature set.
</prevsent>
<prevsent>the senseval evaluation exercises have shown that feeding additional information sources to the classifier results in better system performance (agirre and martinez, 2004).
</prevsent>
</prevsection>
<citsent citstr=" P05-1050 ">
in future experiments we plan to integrate a.o. lemma information on the surrounding content words and semantic analysis (e.g. singular value decomposition (gliozzo et al, 2005)) <papid> P05-1050 </papid>in order to detect latent correlations between terms.</citsent>
<aftsection>
<nextsent>3.2.2 translation feature sin addition to the commonly deployed local context features, we also extracted set of binary bag of-words features from the aligned translations that are not the target language of the classifier (e.g. for the french classifier, we extract bag-of-wordsfeatures from the italian, spanish, dutch and german aligned translations).
</nextsent>
<nextsent>we preprocessed all aligned translations by means of the tree tagger tool (schmid, 1994) that outputs part-of-speech and 55 lemma information.
</nextsent>
<nextsent>per ambiguous focus word, list of all content words (nouns, adjectives, adverbs and verbs) that occurred in the aligned translations of the english sentences containing this word, wasextracted.
</nextsent>
<nextsent>this resulted in one binary feature per selected content word per language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4670">
<title id=" W11-1006.xml">an evaluation and possible improvement path for current smt behavior on ambiguous nouns </title>
<section> para sense.  </section>
<citcontext>
<prevsection>
<prevsent>this automatic translation process can be done using whatever machine translation tool, but we chose the google api because of its easy integration.
</prevsent>
<prevsent>online machine translation tools have already been used before to create artificial parallel corpora that were used for nlp tasks such as for instance named entity recognition (shah et al., 2010).
</prevsent>
</prevsection>
<citsent citstr=" P10-1023 ">
similarly, navigli and ponzetto (2010) <papid> P10-1023 </papid>used the google translate api to enrich babelnet, wide-coverage multilingual semantic network, with lexical information for all languages.once the automatic aligned translations were generated, we preprocessed them in the same way as we did for the aligned training translations.</citsent>
<aftsection>
<nextsent>in next step, we again selected all content words from these translations and constructed the binary bag-of-words features.
</nextsent>
<nextsent>to evaluate the two machine translation systems as well as the para sense system on their performance on the lexical sample of twenty ambiguous words,we used the sense inventory and test set of the semeval cross-lingual word sense disambiguationtask.
</nextsent>
<nextsent>the sense inventory was built up on the basis of the europarl corpus: all retrieved translations of polysemous word were manually grouped into clusters, which constitute different senses of that given word.
</nextsent>
<nextsent>the test instances were selected from the jrc-acquis multilingual parallel corpus2 and bnc3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4671">
<title id=" W10-4209.xml">using semantic web technology to support nlg case study  owl finds rags </title>
<section> relation to other work.  </section>
<citcontext>
<prevsection>
<prevsent>research is only just beginning to escape from time-based?
</prevsent>
<prevsent>mode of annotation, for instance by using stand-off?
</prevsent>
</prevsection>
<citsent citstr=" W02-1703 ">
annotations to indicate layout (bateman et al, 2002).<papid> W02-1703 </papid></citsent>
<aftsection>
<nextsent>in addition, most annotation schemes are partial (only describe certain aspects of the text) and non-structured (as sign simple labels to portions of text).
</nextsent>
<nextsent>for nlg,one needs way of representing all the information that is needed for generating text, and this usually has complex internal structure.
</nextsent>
<nextsent>linguistic ontologies are ontologies developed to describe linguistic concepts.
</nextsent>
<nextsent>although ontologies are used in number of nlp projects (e.g.(estival et al, 2004)), <papid> W04-0609 </papid>the ontologies used are usually ontologies of the application domain rather than the linguistic structures of natural languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4672">
<title id=" W10-4209.xml">using semantic web technology to support nlg case study  owl finds rags </title>
<section> relation to other work.  </section>
<citcontext>
<prevsection>
<prevsent>for nlg,one needs way of representing all the information that is needed for generating text, and this usually has complex internal structure.
</prevsent>
<prevsent>linguistic ontologies are ontologies developed to describe linguistic concepts.
</prevsent>
</prevsection>
<citsent citstr=" W04-0609 ">
although ontologies are used in number of nlp projects (e.g.(estival et al, 2004)), <papid> W04-0609 </papid>the ontologies used are usually ontologies of the application domain rather than the linguistic structures of natural languages.</citsent>
<aftsection>
<nextsent>the development of ontologies to describe aspects of natural languages is comparatively rare.
</nextsent>
<nextsent>the wordnet ontologies are widely used resource describing the repertoire of word senses of natural languages, but these concentrate on individual words rather than larger linguistic structures.
</nextsent>
<nextsent>more relevant to nlg is work on various versions of the generalised upper model (bateman et al, 1995), which outlines aspects of meaning relevant to making nlg decisions.
</nextsent>
<nextsent>this has been used to help translate domain knowledge in number of nlg systems (aguado et al, 1998).in summary, existing approaches to using ontologies or xml for natural language related purposes are not adequate to describe the datas truc tures needed for nlg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4673">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using paraphrases extracted from bilingual text and re-ranked on monolingual data,our system selects the set of paraphrases that minimizes the character length of sentence.
</prevsent>
<prevsent>while not currently the standard, character-basedlengths have been considered before in compression, and we believe that it is relevant for current and future applications.
</prevsent>
</prevsection>
<citsent citstr=" W06-2907 ">
character lengths have been used for document summarization (duc 2004, over and yen (2004)), summarizing for mobile devices (corston-oliver, 2001), and sub titling (glickman et al., 2006).<papid> W06-2907 </papid></citsent>
<aftsection>
<nextsent>although in the past strict word limit shave been imposed for various documents, information transmitted electronically is often limited by the number of bytes, which directly relates to the number of characters.
</nextsent>
<nextsent>mobile devices, sms messages,and microblogging sites such as twitter are increasingly important for quickly spreading information.in this context, it is important to consider character based constraints.
</nextsent>
<nextsent>we examine whether paraphrastic compression allows more information to be conveyed in the same number of characters as deletion-only compressions.
</nextsent>
<nextsent>for example, the length constraint of twitter posts or tweets is 140 characters, and many article lead sentences exceed this limit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4674">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to achieve further compression, we shortened the oracle compress ions using deletion model to yield compress ions 80% of the original sentence length and compared these to compress ions generated using just deletions.
</prevsent>
<prevsent>manual evaluation found that the oracle-then-deletion compress ions to preserve more meaning than deletion-only compress ions at uniform compression rates.
</prevsent>
</prevsection>
<citsent citstr=" N07-1023 ">
most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., galley and mckeown (2007), <papid> N07-1023 </papid>knight and marcu (2002), nomoto (2009), <papid> D09-1041 </papid>galanis and androutsopoulos (2010), <papid> N10-1131 </papid>filippova and strube (2008), <papid> W08-1105 </papid>mcdonald (2006), <papid> E06-1038 </papid>yamangil and shieber (2010), <papid> P10-1096 </papid>cohn and lapata (2008), <papid> C08-1018 </papid>cohn and lapata (2009), turner and charniak (2005)).<papid> P05-1036 </papid></citsent>
<aftsection>
<nextsent>woodsend et al (2010) <papid> D10-1050 </papid>incorporate paraphrase rules into deletionmodel.</nextsent>
<nextsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4675">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to achieve further compression, we shortened the oracle compress ions using deletion model to yield compress ions 80% of the original sentence length and compared these to compress ions generated using just deletions.
</prevsent>
<prevsent>manual evaluation found that the oracle-then-deletion compress ions to preserve more meaning than deletion-only compress ions at uniform compression rates.
</prevsent>
</prevsection>
<citsent citstr=" D09-1041 ">
most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., galley and mckeown (2007), <papid> N07-1023 </papid>knight and marcu (2002), nomoto (2009), <papid> D09-1041 </papid>galanis and androutsopoulos (2010), <papid> N10-1131 </papid>filippova and strube (2008), <papid> W08-1105 </papid>mcdonald (2006), <papid> E06-1038 </papid>yamangil and shieber (2010), <papid> P10-1096 </papid>cohn and lapata (2008), <papid> C08-1018 </papid>cohn and lapata (2009), turner and charniak (2005)).<papid> P05-1036 </papid></citsent>
<aftsection>
<nextsent>woodsend et al (2010) <papid> D10-1050 </papid>incorporate paraphrase rules into deletionmodel.</nextsent>
<nextsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4676">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to achieve further compression, we shortened the oracle compress ions using deletion model to yield compress ions 80% of the original sentence length and compared these to compress ions generated using just deletions.
</prevsent>
<prevsent>manual evaluation found that the oracle-then-deletion compress ions to preserve more meaning than deletion-only compress ions at uniform compression rates.
</prevsent>
</prevsection>
<citsent citstr=" N10-1131 ">
most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., galley and mckeown (2007), <papid> N07-1023 </papid>knight and marcu (2002), nomoto (2009), <papid> D09-1041 </papid>galanis and androutsopoulos (2010), <papid> N10-1131 </papid>filippova and strube (2008), <papid> W08-1105 </papid>mcdonald (2006), <papid> E06-1038 </papid>yamangil and shieber (2010), <papid> P10-1096 </papid>cohn and lapata (2008), <papid> C08-1018 </papid>cohn and lapata (2009), turner and charniak (2005)).<papid> P05-1036 </papid></citsent>
<aftsection>
<nextsent>woodsend et al (2010) <papid> D10-1050 </papid>incorporate paraphrase rules into deletionmodel.</nextsent>
<nextsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4677">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to achieve further compression, we shortened the oracle compress ions using deletion model to yield compress ions 80% of the original sentence length and compared these to compress ions generated using just deletions.
</prevsent>
<prevsent>manual evaluation found that the oracle-then-deletion compress ions to preserve more meaning than deletion-only compress ions at uniform compression rates.
</prevsent>
</prevsection>
<citsent citstr=" W08-1105 ">
most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., galley and mckeown (2007), <papid> N07-1023 </papid>knight and marcu (2002), nomoto (2009), <papid> D09-1041 </papid>galanis and androutsopoulos (2010), <papid> N10-1131 </papid>filippova and strube (2008), <papid> W08-1105 </papid>mcdonald (2006), <papid> E06-1038 </papid>yamangil and shieber (2010), <papid> P10-1096 </papid>cohn and lapata (2008), <papid> C08-1018 </papid>cohn and lapata (2009), turner and charniak (2005)).<papid> P05-1036 </papid></citsent>
<aftsection>
<nextsent>woodsend et al (2010) <papid> D10-1050 </papid>incorporate paraphrase rules into deletionmodel.</nextsent>
<nextsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4678">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to achieve further compression, we shortened the oracle compress ions using deletion model to yield compress ions 80% of the original sentence length and compared these to compress ions generated using just deletions.
</prevsent>
<prevsent>manual evaluation found that the oracle-then-deletion compress ions to preserve more meaning than deletion-only compress ions at uniform compression rates.
</prevsent>
</prevsection>
<citsent citstr=" E06-1038 ">
most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., galley and mckeown (2007), <papid> N07-1023 </papid>knight and marcu (2002), nomoto (2009), <papid> D09-1041 </papid>galanis and androutsopoulos (2010), <papid> N10-1131 </papid>filippova and strube (2008), <papid> W08-1105 </papid>mcdonald (2006), <papid> E06-1038 </papid>yamangil and shieber (2010), <papid> P10-1096 </papid>cohn and lapata (2008), <papid> C08-1018 </papid>cohn and lapata (2009), turner and charniak (2005)).<papid> P05-1036 </papid></citsent>
<aftsection>
<nextsent>woodsend et al (2010) <papid> D10-1050 </papid>incorporate paraphrase rules into deletionmodel.</nextsent>
<nextsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4679">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to achieve further compression, we shortened the oracle compress ions using deletion model to yield compress ions 80% of the original sentence length and compared these to compress ions generated using just deletions.
</prevsent>
<prevsent>manual evaluation found that the oracle-then-deletion compress ions to preserve more meaning than deletion-only compress ions at uniform compression rates.
</prevsent>
</prevsection>
<citsent citstr=" P10-1096 ">
most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., galley and mckeown (2007), <papid> N07-1023 </papid>knight and marcu (2002), nomoto (2009), <papid> D09-1041 </papid>galanis and androutsopoulos (2010), <papid> N10-1131 </papid>filippova and strube (2008), <papid> W08-1105 </papid>mcdonald (2006), <papid> E06-1038 </papid>yamangil and shieber (2010), <papid> P10-1096 </papid>cohn and lapata (2008), <papid> C08-1018 </papid>cohn and lapata (2009), turner and charniak (2005)).<papid> P05-1036 </papid></citsent>
<aftsection>
<nextsent>woodsend et al (2010) <papid> D10-1050 </papid>incorporate paraphrase rules into deletionmodel.</nextsent>
<nextsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4680">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to achieve further compression, we shortened the oracle compress ions using deletion model to yield compress ions 80% of the original sentence length and compared these to compress ions generated using just deletions.
</prevsent>
<prevsent>manual evaluation found that the oracle-then-deletion compress ions to preserve more meaning than deletion-only compress ions at uniform compression rates.
</prevsent>
</prevsection>
<citsent citstr=" C08-1018 ">
most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., galley and mckeown (2007), <papid> N07-1023 </papid>knight and marcu (2002), nomoto (2009), <papid> D09-1041 </papid>galanis and androutsopoulos (2010), <papid> N10-1131 </papid>filippova and strube (2008), <papid> W08-1105 </papid>mcdonald (2006), <papid> E06-1038 </papid>yamangil and shieber (2010), <papid> P10-1096 </papid>cohn and lapata (2008), <papid> C08-1018 </papid>cohn and lapata (2009), turner and charniak (2005)).<papid> P05-1036 </papid></citsent>
<aftsection>
<nextsent>woodsend et al (2010) <papid> D10-1050 </papid>incorporate paraphrase rules into deletionmodel.</nextsent>
<nextsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4681">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to achieve further compression, we shortened the oracle compress ions using deletion model to yield compress ions 80% of the original sentence length and compared these to compress ions generated using just deletions.
</prevsent>
<prevsent>manual evaluation found that the oracle-then-deletion compress ions to preserve more meaning than deletion-only compress ions at uniform compression rates.
</prevsent>
</prevsection>
<citsent citstr=" P05-1036 ">
most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., galley and mckeown (2007), <papid> N07-1023 </papid>knight and marcu (2002), nomoto (2009), <papid> D09-1041 </papid>galanis and androutsopoulos (2010), <papid> N10-1131 </papid>filippova and strube (2008), <papid> W08-1105 </papid>mcdonald (2006), <papid> E06-1038 </papid>yamangil and shieber (2010), <papid> P10-1096 </papid>cohn and lapata (2008), <papid> C08-1018 </papid>cohn and lapata (2009), turner and charniak (2005)).<papid> P05-1036 </papid></citsent>
<aftsection>
<nextsent>woodsend et al (2010) <papid> D10-1050 </papid>incorporate paraphrase rules into deletionmodel.</nextsent>
<nextsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4682">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>manual evaluation found that the oracle-then-deletion compress ions to preserve more meaning than deletion-only compress ions at uniform compression rates.
</prevsent>
<prevsent>most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., galley and mckeown (2007), <papid> N07-1023 </papid>knight and marcu (2002), nomoto (2009), <papid> D09-1041 </papid>galanis and androutsopoulos (2010), <papid> N10-1131 </papid>filippova and strube (2008), <papid> W08-1105 </papid>mcdonald (2006), <papid> E06-1038 </papid>yamangil and shieber (2010), <papid> P10-1096 </papid>cohn and lapata (2008), <papid> C08-1018 </papid>cohn and lapata (2009), turner and charniak (2005)).<papid> P05-1036 </papid></prevsent>
</prevsection>
<citsent citstr=" D10-1050 ">
woodsend et al (2010) <papid> D10-1050 </papid>incorporate paraphrase rules into deletionmodel.</citsent>
<aftsection>
<nextsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></nextsent>
<nextsent>more recent approaches in steganography have used paraphrase substitution to encode information in textbut focus on grammaticality, not meaning preservation (chang and clark, 2010).<papid> N10-1084 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4684">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>woodsend et al (2010) <papid> D10-1050 </papid>incorporate paraphrase rules into deletionmodel.</prevsent>
<prevsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></prevsent>
</prevsection>
<citsent citstr=" N10-1084 ">
more recent approaches in steganography have used paraphrase substitution to encode information in textbut focus on grammaticality, not meaning preservation (chang and clark, 2010).<papid> N10-1084 </papid></citsent>
<aftsection>
<nextsent>zhao et al (2009) <papid> P09-1094 </papid>applied an adaptable paraphrasing pipeline to sentence 2taken from the main page of http://wsj.com, april 9, 2011.compression, optimizing for f-measure over manually annotated set of gold standard paraphrases.sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (dorr et al, 2003; <papid> W03-0501 </papid>vandeghinste and pan, 2004; <papid> W04-1015 </papid>marsi et al, 2009).<papid> W09-0604 </papid></nextsent>
<nextsent>corston-oliver (2001) deleted characters from words to shorten the character length of sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4685">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></prevsent>
<prevsent>more recent approaches in steganography have used paraphrase substitution to encode information in textbut focus on grammaticality, not meaning preservation (chang and clark, 2010).<papid> N10-1084 </papid></prevsent>
</prevsection>
<citsent citstr=" P09-1094 ">
zhao et al (2009) <papid> P09-1094 </papid>applied an adaptable paraphrasing pipeline to sentence 2taken from the main page of http://wsj.com, april 9, 2011.compression, optimizing for f-measure over manually annotated set of gold standard paraphrases.sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (dorr et al, 2003; <papid> W03-0501 </papid>vandeghinste and pan, 2004; <papid> W04-1015 </papid>marsi et al, 2009).<papid> W09-0604 </papid></citsent>
<aftsection>
<nextsent>corston-oliver (2001) deleted characters from words to shorten the character length of sentences.
</nextsent>
<nextsent>to our knowledge character-based compression has not been examined before with the surging popularity and utility of twitter.
</nextsent>
<nextsent>the distinction between tightening and compression can be illustrated by considering how much space needs to be preserved.
</nextsent>
<nextsent>in the case of microblogging, often sentence has just few too many characters and needs to be tightened?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4686">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></prevsent>
<prevsent>more recent approaches in steganography have used paraphrase substitution to encode information in textbut focus on grammaticality, not meaning preservation (chang and clark, 2010).<papid> N10-1084 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-0501 ">
zhao et al (2009) <papid> P09-1094 </papid>applied an adaptable paraphrasing pipeline to sentence 2taken from the main page of http://wsj.com, april 9, 2011.compression, optimizing for f-measure over manually annotated set of gold standard paraphrases.sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (dorr et al, 2003; <papid> W03-0501 </papid>vandeghinste and pan, 2004; <papid> W04-1015 </papid>marsi et al, 2009).<papid> W09-0604 </papid></citsent>
<aftsection>
<nextsent>corston-oliver (2001) deleted characters from words to shorten the character length of sentences.
</nextsent>
<nextsent>to our knowledge character-based compression has not been examined before with the surging popularity and utility of twitter.
</nextsent>
<nextsent>the distinction between tightening and compression can be illustrated by considering how much space needs to be preserved.
</nextsent>
<nextsent>in the case of microblogging, often sentence has just few too many characters and needs to be tightened?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4687">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></prevsent>
<prevsent>more recent approaches in steganography have used paraphrase substitution to encode information in textbut focus on grammaticality, not meaning preservation (chang and clark, 2010).<papid> N10-1084 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-1015 ">
zhao et al (2009) <papid> P09-1094 </papid>applied an adaptable paraphrasing pipeline to sentence 2taken from the main page of http://wsj.com, april 9, 2011.compression, optimizing for f-measure over manually annotated set of gold standard paraphrases.sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (dorr et al, 2003; <papid> W03-0501 </papid>vandeghinste and pan, 2004; <papid> W04-1015 </papid>marsi et al, 2009).<papid> W09-0604 </papid></citsent>
<aftsection>
<nextsent>corston-oliver (2001) deleted characters from words to shorten the character length of sentences.
</nextsent>
<nextsent>to our knowledge character-based compression has not been examined before with the surging popularity and utility of twitter.
</nextsent>
<nextsent>the distinction between tightening and compression can be illustrated by considering how much space needs to be preserved.
</nextsent>
<nextsent>in the case of microblogging, often sentence has just few too many characters and needs to be tightened?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4688">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous work in sub titling has made one word substitutions to decrease character length at high compression rates (glickman et al, 2006).<papid> W06-2907 </papid></prevsent>
<prevsent>more recent approaches in steganography have used paraphrase substitution to encode information in textbut focus on grammaticality, not meaning preservation (chang and clark, 2010).<papid> N10-1084 </papid></prevsent>
</prevsection>
<citsent citstr=" W09-0604 ">
zhao et al (2009) <papid> P09-1094 </papid>applied an adaptable paraphrasing pipeline to sentence 2taken from the main page of http://wsj.com, april 9, 2011.compression, optimizing for f-measure over manually annotated set of gold standard paraphrases.sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (dorr et al, 2003; <papid> W03-0501 </papid>vandeghinste and pan, 2004; <papid> W04-1015 </papid>marsi et al, 2009).<papid> W09-0604 </papid></citsent>
<aftsection>
<nextsent>corston-oliver (2001) deleted characters from words to shorten the character length of sentences.
</nextsent>
<nextsent>to our knowledge character-based compression has not been examined before with the surging popularity and utility of twitter.
</nextsent>
<nextsent>the distinction between tightening and compression can be illustrated by considering how much space needs to be preserved.
</nextsent>
<nextsent>in the case of microblogging, often sentence has just few too many characters and needs to be tightened?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4689">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> sentence tightening.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1 illustrates the process.
</prevsent>
<prevsent>a phrase to be paraphrased, like thrown into jail, is found in german-english parallel corpus.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
the corresponding foreign phrase (festgenom men) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translation (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>other occurrences of the foreign phrase in the parallel corpus may align to another english phrase like jailed.
</nextsent>
<nextsent>following bannard and callison-burch, we treated any english phrases that share common foreign phrase as potential paraphrases of each other.
</nextsent>
<nextsent>as the original phrase occurs several times and aligns with many different foreign phrases, each ofthese may align to variety of other english paraphrases.
</nextsent>
<nextsent>thus, thrown into jail not only paraphrase sas jailed, but also as arrested, detained, imprisoned, incarcerated, locked up, taken into custody, and thrown into prison . moreover, because the method relies on noisy and potentially inaccurate word alignments, it is prone to generate many bad paraphrases, such as maltreated, thrown, cases, custody, arrest, owners, and protection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4690">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> sentence tightening.  </section>
<citcontext>
<prevsection>
<prevsent>see table 1 for an example.
</prevsent>
<prevsent>the monolingual-filtering technique we describe is by no means limited to paraphrases extracted from bilingual corpora.
</prevsent>
</prevsection>
<citsent citstr=" J10-3003 ">
it could be applied to other data driven paraphrasing techniques (see madnani and dorr (2010) <papid> J10-3003 </papid>for survey).</citsent>
<aftsection>
<nextsent>although it is particularly well suited to the bilingual extracted corpora, since the information that it adds is orthogonal to that model, it would presumably add less to paraphrasing techniques that already take advantage of monolingual distributional similarity (pereira et al, 1993; <papid> P93-1024 </papid>lin and pantel, 2001; barzilay and lee, 2003).<papid> N03-1003 </papid></nextsent>
<nextsent>in order to evaluate the paraphrase candidates and scoring techniques, we randomly selected 1,000 paraphrase sets where the source phrase was present in the corpus described in clarke and lapata (2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4691">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> sentence tightening.  </section>
<citcontext>
<prevsection>
<prevsent>the monolingual-filtering technique we describe is by no means limited to paraphrases extracted from bilingual corpora.
</prevsent>
<prevsent>it could be applied to other data driven paraphrasing techniques (see madnani and dorr (2010) <papid> J10-3003 </papid>for survey).</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
although it is particularly well suited to the bilingual extracted corpora, since the information that it adds is orthogonal to that model, it would presumably add less to paraphrasing techniques that already take advantage of monolingual distributional similarity (pereira et al, 1993; <papid> P93-1024 </papid>lin and pantel, 2001; barzilay and lee, 2003).<papid> N03-1003 </papid></citsent>
<aftsection>
<nextsent>in order to evaluate the paraphrase candidates and scoring techniques, we randomly selected 1,000 paraphrase sets where the source phrase was present in the corpus described in clarke and lapata (2008).
</nextsent>
<nextsent>for each phrase and set of candidate paraphrases, we extracted all of the contexts from the corpus in which the source phrase appeared.
</nextsent>
<nextsent>human judges were presented each sentence with the original phrase and the same sentences with each paraphrase candidate 86 ... letztewoche wurden in irland fnf landwirte festgenommen , weil sie verhindern wollten ... last week five farmers were thrown into jail in ireland because they resisted ...
</nextsent>
<nextsent>zahlreiche journalisten sind verschwunden oder wurden festgenommen , gefoltert und gettet . quite few journalists have disappeared or have been imprisoned , tortured and killed . figure 1: using bilingual parallel corpus to extract paraphrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4692">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> sentence tightening.  </section>
<citcontext>
<prevsection>
<prevsent>the monolingual-filtering technique we describe is by no means limited to paraphrases extracted from bilingual corpora.
</prevsent>
<prevsent>it could be applied to other data driven paraphrasing techniques (see madnani and dorr (2010) <papid> J10-3003 </papid>for survey).</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
although it is particularly well suited to the bilingual extracted corpora, since the information that it adds is orthogonal to that model, it would presumably add less to paraphrasing techniques that already take advantage of monolingual distributional similarity (pereira et al, 1993; <papid> P93-1024 </papid>lin and pantel, 2001; barzilay and lee, 2003).<papid> N03-1003 </papid></citsent>
<aftsection>
<nextsent>in order to evaluate the paraphrase candidates and scoring techniques, we randomly selected 1,000 paraphrase sets where the source phrase was present in the corpus described in clarke and lapata (2008).
</nextsent>
<nextsent>for each phrase and set of candidate paraphrases, we extracted all of the contexts from the corpus in which the source phrase appeared.
</nextsent>
<nextsent>human judges were presented each sentence with the original phrase and the same sentences with each paraphrase candidate 86 ... letztewoche wurden in irland fnf landwirte festgenommen , weil sie verhindern wollten ... last week five farmers were thrown into jail in ireland because they resisted ...
</nextsent>
<nextsent>zahlreiche journalisten sind verschwunden oder wurden festgenommen , gefoltert und gettet . quite few journalists have disappeared or have been imprisoned , tortured and killed . figure 1: using bilingual parallel corpus to extract paraphrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4693">
<title id=" W11-1610.xml">paraphrastic sentence compression with a character based metric tightening without deletion </title>
<section> framework for sentence tightening.  </section>
<citcontext>
<prevsection>
<prevsent>in case judges favor compress ions that have high word overlap with the original sentence, we compressed the longest sentence from each set of reference translations (huang et al, 2002) and randomly chose sentence from the set of reference translations to use as the standard for comparison.paraphrastic compress ions were generated at cosine similarity thresholds ranging from 0.60 to 0.95.
</prevsent>
<prevsent>we implemented state-of-the-art deletion model (clarke and lapata, 2008) to generate deletion-only compressions.
</prevsent>
</prevsection>
<citsent citstr=" W11-1611 ">
we fixed the compression length to 5 characters of the length of each paraphras tic compression, in order to isolate the compression quality from the effect of compression rate (napoles et al, 2011).<papid> W11-1611 </papid></citsent>
<aftsection>
<nextsent>manual evaluation used amazons mechanical turk with three-way redundancy and positive and negative controls to filter bad workers.meaning and grammar judgments were collected using two 5-point scales (5 being the highest score).
</nextsent>
<nextsent>the initial results of our substitution system show room for improvement in future work (table 2).
</nextsent>
<nextsent>we believe this is due to erroneous paraphrase substi 87 system grammar meaning compr cos.
</nextsent>
<nextsent>substitution 3.8 3.7 0.97 0.95deletion 4.1 4.0 0.97 substitution 3.4 3.2 0.89 0.85deletion 4.0 3.8 0.89 substitution 3.1 3.0 0.85 0.75deletion 3.9 3.7 0.85 substitution 2.9 2.9 0.82 0.65deletion 3.8 3.5 0.82 table 2: mean ratings of compress ions using just deletion or substitution at different paraphrase thresholds (cos.).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4694">
<title id=" W10-4128.xml">hmm revises low marginal probability by crf for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>chinese word segmentation (cws) has witnessed prominent progress in the first four sighan bakeoffs.
</prevsent>
<prevsent>since xue (2003) used character-based tagging, this method has attracted more and more attention.
</prevsent>
</prevsection>
<citsent citstr=" C04-1081 ">
some previous work (peng et al, 2004; <papid> C04-1081 </papid>tseng et al, 2005; <papid> I05-3027 </papid>low et al, 2005) <papid> I05-3025 </papid>illustrated the effectiveness of using characters as tagging units, while literatures (zhang et al, 2006; <papid> N06-2049 </papid>zhao and kit, 2007a; zhang and clark, 2007) <papid> P07-1106 </papid>focus on employing lexical words or sub words as tagging units.</citsent>
<aftsection>
<nextsent>because the word-based models can capture the word-level contextual information and iv knowledge.
</nextsent>
<nextsent>besides, many strategies are proposed to balance the iv and oov performance (wang et al, 2008).<papid> I08-4009 </papid></nextsent>
<nextsent>crf has been widely used in sequence labeling tasks and has good performance (lafferty et al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4695">
<title id=" W10-4128.xml">hmm revises low marginal probability by crf for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>chinese word segmentation (cws) has witnessed prominent progress in the first four sighan bakeoffs.
</prevsent>
<prevsent>since xue (2003) used character-based tagging, this method has attracted more and more attention.
</prevsent>
</prevsection>
<citsent citstr=" I05-3027 ">
some previous work (peng et al, 2004; <papid> C04-1081 </papid>tseng et al, 2005; <papid> I05-3027 </papid>low et al, 2005) <papid> I05-3025 </papid>illustrated the effectiveness of using characters as tagging units, while literatures (zhang et al, 2006; <papid> N06-2049 </papid>zhao and kit, 2007a; zhang and clark, 2007) <papid> P07-1106 </papid>focus on employing lexical words or sub words as tagging units.</citsent>
<aftsection>
<nextsent>because the word-based models can capture the word-level contextual information and iv knowledge.
</nextsent>
<nextsent>besides, many strategies are proposed to balance the iv and oov performance (wang et al, 2008).<papid> I08-4009 </papid></nextsent>
<nextsent>crf has been widely used in sequence labeling tasks and has good performance (lafferty et al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4696">
<title id=" W10-4128.xml">hmm revises low marginal probability by crf for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>chinese word segmentation (cws) has witnessed prominent progress in the first four sighan bakeoffs.
</prevsent>
<prevsent>since xue (2003) used character-based tagging, this method has attracted more and more attention.
</prevsent>
</prevsection>
<citsent citstr=" I05-3025 ">
some previous work (peng et al, 2004; <papid> C04-1081 </papid>tseng et al, 2005; <papid> I05-3027 </papid>low et al, 2005) <papid> I05-3025 </papid>illustrated the effectiveness of using characters as tagging units, while literatures (zhang et al, 2006; <papid> N06-2049 </papid>zhao and kit, 2007a; zhang and clark, 2007) <papid> P07-1106 </papid>focus on employing lexical words or sub words as tagging units.</citsent>
<aftsection>
<nextsent>because the word-based models can capture the word-level contextual information and iv knowledge.
</nextsent>
<nextsent>besides, many strategies are proposed to balance the iv and oov performance (wang et al, 2008).<papid> I08-4009 </papid></nextsent>
<nextsent>crf has been widely used in sequence labeling tasks and has good performance (lafferty et al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4697">
<title id=" W10-4128.xml">hmm revises low marginal probability by crf for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>chinese word segmentation (cws) has witnessed prominent progress in the first four sighan bakeoffs.
</prevsent>
<prevsent>since xue (2003) used character-based tagging, this method has attracted more and more attention.
</prevsent>
</prevsection>
<citsent citstr=" N06-2049 ">
some previous work (peng et al, 2004; <papid> C04-1081 </papid>tseng et al, 2005; <papid> I05-3027 </papid>low et al, 2005) <papid> I05-3025 </papid>illustrated the effectiveness of using characters as tagging units, while literatures (zhang et al, 2006; <papid> N06-2049 </papid>zhao and kit, 2007a; zhang and clark, 2007) <papid> P07-1106 </papid>focus on employing lexical words or sub words as tagging units.</citsent>
<aftsection>
<nextsent>because the word-based models can capture the word-level contextual information and iv knowledge.
</nextsent>
<nextsent>besides, many strategies are proposed to balance the iv and oov performance (wang et al, 2008).<papid> I08-4009 </papid></nextsent>
<nextsent>crf has been widely used in sequence labeling tasks and has good performance (lafferty et al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4698">
<title id=" W10-4128.xml">hmm revises low marginal probability by crf for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>chinese word segmentation (cws) has witnessed prominent progress in the first four sighan bakeoffs.
</prevsent>
<prevsent>since xue (2003) used character-based tagging, this method has attracted more and more attention.
</prevsent>
</prevsection>
<citsent citstr=" P07-1106 ">
some previous work (peng et al, 2004; <papid> C04-1081 </papid>tseng et al, 2005; <papid> I05-3027 </papid>low et al, 2005) <papid> I05-3025 </papid>illustrated the effectiveness of using characters as tagging units, while literatures (zhang et al, 2006; <papid> N06-2049 </papid>zhao and kit, 2007a; zhang and clark, 2007) <papid> P07-1106 </papid>focus on employing lexical words or sub words as tagging units.</citsent>
<aftsection>
<nextsent>because the word-based models can capture the word-level contextual information and iv knowledge.
</nextsent>
<nextsent>besides, many strategies are proposed to balance the iv and oov performance (wang et al, 2008).<papid> I08-4009 </papid></nextsent>
<nextsent>crf has been widely used in sequence labeling tasks and has good performance (lafferty et al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4699">
<title id=" W10-4128.xml">hmm revises low marginal probability by crf for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some previous work (peng et al, 2004; <papid> C04-1081 </papid>tseng et al, 2005; <papid> I05-3027 </papid>low et al, 2005) <papid> I05-3025 </papid>illustrated the effectiveness of using characters as tagging units, while literatures (zhang et al, 2006; <papid> N06-2049 </papid>zhao and kit, 2007a; zhang and clark, 2007) <papid> P07-1106 </papid>focus on employing lexical words or sub words as tagging units.</prevsent>
<prevsent>because the word-based models can capture the word-level contextual information and iv knowledge.</prevsent>
</prevsection>
<citsent citstr=" I08-4009 ">
besides, many strategies are proposed to balance the iv and oov performance (wang et al, 2008).<papid> I08-4009 </papid></citsent>
<aftsection>
<nextsent>crf has been widely used in sequence labeling tasks and has good performance (lafferty et al, 2001).
</nextsent>
<nextsent>zhao and kit (2007b), zhao and kit (2008) <papid> I08-4017 </papid>attempt to integrate global information with local information to further improve crf-based tagging method of cws, which provides solid foundation for strengthening crf learning with unsupervised learning outcomes.</nextsent>
<nextsent>in order to increase the accuracy of tagging using crf, we adopt the strategy, which is: if the marginal probability of characters is lower than threshold, the modified component based on hmm will be trigged; combining the confidence measure the results will be regenerated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4700">
<title id=" W10-4128.xml">hmm revises low marginal probability by crf for chinese word segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>besides, many strategies are proposed to balance the iv and oov performance (wang et al, 2008).<papid> I08-4009 </papid></prevsent>
<prevsent>crf has been widely used in sequence labeling tasks and has good performance (lafferty et al, 2001).</prevsent>
</prevsection>
<citsent citstr=" I08-4017 ">
zhao and kit (2007b), zhao and kit (2008) <papid> I08-4017 </papid>attempt to integrate global information with local information to further improve crf-based tagging method of cws, which provides solid foundation for strengthening crf learning with unsupervised learning outcomes.</citsent>
<aftsection>
<nextsent>in order to increase the accuracy of tagging using crf, we adopt the strategy, which is: if the marginal probability of characters is lower than threshold, the modified component based on hmm will be trigged; combining the confidence measure the results will be regenerated.
</nextsent>
<nextsent>in this section, we describe our system in more details.
</nextsent>
<nextsent>three modules are included in our sys tem: basic character-based crf tagger, hmm which revises the sub strings with low marginal probability and confidence measure which combines them to regenerate the result.
</nextsent>
<nextsent>in addition, we also use some rules to deal with the strings within letters and numbers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4703">
<title id=" W11-0113.xml">an abstract schema for representing semantic roles and modelling the syntax semantics interface </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>large-scale lexical semantic resources that provide relational information about words have recently received much focus in the field of natural language processing (nlp).
</prevsent>
<prevsent>in particular, data-driven models for lexical semantics require the creation of broad-coverage, hand-annotated corpora with predicate argument information, i.e. rich information about words expressing semantic relation having argument slots filled by the interpretations of their grammatical complements.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
corpora combining semantic and syntactic annotations constitute the backbone for the development of probabilistic models that automatically identify the semantic relationships, or semantic roles, conveyed by sentential constituents (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>that is, given an input sentence and target predicator the system labels constituents with general roles like agent, patient, theme, etc., or more specific roles, as in (1).
</nextsent>
<nextsent>(1) [cognizer i] admired [evaluee him] [degree greatly] [reason for his bravery and his cheerfulness].1 the task of automatic semantic role labelling (or shallow semantic parsing) is first step towards text understanding and has found use in variety of nlp applications including information extraction (sur deanu et al, 2003), <papid> P03-1002 </papid>machine translation (boas, 2002), question answering (narayanan and harabagiu, 2004), summarisation (melli et al, 2005), recognition of textual entailment relations (burchardt and frank, 2006), etc. corpora with semantic role labels additionally lend themselves to extraction of linguistic knowledge at the syntax-semantics interface.</nextsent>
<nextsent>the range of semantic and syntactic combinatorial properties (va lences) of each word in each of its senses is documented in terms of annotated corpus attestations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4704">
<title id=" W11-0113.xml">an abstract schema for representing semantic roles and modelling the syntax semantics interface </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>corpora combining semantic and syntactic annotations constitute the backbone for the development of probabilistic models that automatically identify the semantic relationships, or semantic roles, conveyed by sentential constituents (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
<prevsent>that is, given an input sentence and target predicator the system labels constituents with general roles like agent, patient, theme, etc., or more specific roles, as in (1).</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
(1) [cognizer i] admired [evaluee him] [degree greatly] [reason for his bravery and his cheerfulness].1 the task of automatic semantic role labelling (or shallow semantic parsing) is first step towards text understanding and has found use in variety of nlp applications including information extraction (sur deanu et al, 2003), <papid> P03-1002 </papid>machine translation (boas, 2002), question answering (narayanan and harabagiu, 2004), summarisation (melli et al, 2005), recognition of textual entailment relations (burchardt and frank, 2006), etc. corpora with semantic role labels additionally lend themselves to extraction of linguistic knowledge at the syntax-semantics interface.</citsent>
<aftsection>
<nextsent>the range of semantic and syntactic combinatorial properties (va lences) of each word in each of its senses is documented in terms of annotated corpus attestations.
</nextsent>
<nextsent>for instance, the valence pattern for the use of admire in (1) is shown in (2).
</nextsent>
<nextsent>(2) cognizer: noun phrase (np), subject evaluee: noun phrase (np), object degree: adverbial dependent reason: prepositional dependent 1this annotated example is from the framenet lexicon (discussed in the next section).
</nextsent>
<nextsent>in all examples throughout the paper, predicators are marked in italics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4705">
<title id=" W11-0113.xml">an abstract schema for representing semantic roles and modelling the syntax semantics interface </title>
<section> corpora with semantic roles and related work.  </section>
<citcontext>
<prevsection>
<prevsent>general syntax-semantics mappings are extracted from the annotated data and are formalised in abstract classes which readily encode generalisations concerning linking to syntactic form.
</prevsent>
<prevsent>semantically annotated corpora currently available for english implement two distinct approaches to the prickly notion of semantic role.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
the proposition bank (propbank) (kingsbury et al, 2002) is one million word corpus in which predicate-argument relations are hand-annotated for every occurrence of every verb in the wall street journal part of the penn treebank (marcus et al, 1994).<papid> H94-1020 </papid></citsent>
<aftsection>
<nextsent>verb senses are distinguished informally on the basis of semantic as well as syntactic criteria.
</nextsent>
<nextsent>the semantic arguments of verb are numbered sequentially.
</nextsent>
<nextsent>propbank uses common set of role labels (arg0 up to arg5) for all predicators, but these labels are defined on per-verb basis, i.e. they have verb-specific meanings.
</nextsent>
<nextsent>example propbank annotations: (3) a. [arg0 john] broke [arg1 the window] [arg2 with rock].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4706">
<title id=" W11-0113.xml">an abstract schema for representing semantic roles and modelling the syntax semantics interface </title>
<section> corpora with semantic roles and related work.  </section>
<citcontext>
<prevsection>
<prevsent>the latter are created manually on the basis of analysis of argument use.3 the subdivided (more coherent) propbank labels perform better for semantic role labelling (loper et al, 2007).
</prevsent>
<prevsent>a different paradigm for semantic role annotation is put forth by framenet.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the berkeley framenet project (baker et al, 1998) <papid> P98-1013 </papid>is creating an online lexical database containing semantic descriptions of words based on fill mores (1985) theory of frame semantics.</citsent>
<aftsection>
<nextsent>the basic unit of analysis is the semantic frame, i.e. schematic representation of stereotypical scene or situation.
</nextsent>
<nextsent>each frame is associated with set of predicates (including verbs, nouns, and adjectives) and set of semantic roles (called frame elements, fes) encoding the participants and props in the designated scene.
</nextsent>
<nextsent>framenet includes manually annotated example sentences from the british national corpus incorporating additional layers of phrase structure and grammatical function annotation.
</nextsent>
<nextsent>it also includes two small corpora of full-text annotation intended to facilitate statistical analysis of frame-semantic structures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4707">
<title id=" W11-0113.xml">an abstract schema for representing semantic roles and modelling the syntax semantics interface </title>
<section> corpora with semantic roles and related work.  </section>
<citcontext>
<prevsection>
<prevsent>table 1: the judgment frame 3this endeavour is part of the semlink project which aims at developing computationally explicit connections between lexical semantic resources (propbank, verbnet, framenet, wordnet).
</prevsent>
<prevsent>the idea is to combine the advantages of these resources and overcome their limitations by bridging the complementary lexical information they offer.
</prevsent>
</prevsection>
<citsent citstr=" L08-1428 ">
in related vein, the lirics (i.e. linguistic infrastructure for interoperable resources and systems) project has recently evaluated several approaches for semantic role annotation (propbank, verbnet, framenet, among others) aiming to propose iso ratified standards for semantic representation that will enable the exchange and reuse of (multilingual) language resources (petukhova and bunt, 2008).<papid> L08-1428 </papid></citsent>
<aftsection>
<nextsent>117 framenet avoids the difficulties of attempting to pin down small set of general roles.
</nextsent>
<nextsent>instead frame elements are defined locally, i.e. in terms of frames.
</nextsent>
<nextsent>frames are situated in semantic space by means of directed (asymmetric) relations.
</nextsent>
<nextsent>each frame-to-frame relation associates less dependent or more general frame (super frame) with more dependent or less general one (sub frame).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4709">
<title id=" W11-0113.xml">an abstract schema for representing semantic roles and modelling the syntax semantics interface </title>
<section> corpora with semantic roles and related work.  </section>
<citcontext>
<prevsection>
<prevsent>to corpus-induced syntax semantics mappings for abstraction of general linguistic knowledge base.
</prevsent>
<prevsent>the generalisations proposed by frank are intended to apply within frames but not across frames.
</prevsent>
</prevsection>
<citsent citstr=" W04-0817 ">
baldewein et al (2004) <papid> W04-0817 </papid>have trained semantic role classifiers re-using training instances of roles that are similar to the target role.</citsent>
<aftsection>
<nextsent>as similarity measures, they use the framenet hierarchy, peripheral roles of framenet and clusters of roles constructed automatically.
</nextsent>
<nextsent>matsubayashi et al (2009) <papid> P09-1003 </papid>also explore various machine learning features for generalising semantic roles in framenet, namely role hierarchy, human-understandable descriptors of frame elements, semantic types of filler phrases, and mappings of framenet roles to roles of verbnet.</nextsent>
<nextsent>the experimental result of the role classification using these generalisation features shows significant improvements in the system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4710">
<title id=" W11-0113.xml">an abstract schema for representing semantic roles and modelling the syntax semantics interface </title>
<section> corpora with semantic roles and related work.  </section>
<citcontext>
<prevsection>
<prevsent>baldewein et al (2004) <papid> W04-0817 </papid>have trained semantic role classifiers re-using training instances of roles that are similar to the target role.</prevsent>
<prevsent>as similarity measures, they use the framenet hierarchy, peripheral roles of framenet and clusters of roles constructed automatically.</prevsent>
</prevsection>
<citsent citstr=" P09-1003 ">
matsubayashi et al (2009) <papid> P09-1003 </papid>also explore various machine learning features for generalising semantic roles in framenet, namely role hierarchy, human-understandable descriptors of frame elements, semantic types of filler phrases, and mappings of framenet roles to roles of verbnet.</citsent>
<aftsection>
<nextsent>the experimental result of the role classification using these generalisation features shows significant improvements in the system.
</nextsent>
<nextsent>this is due to the fact that role generalisations can form remedy for the severe problem of sparse data which is inherent in lexical semantic corpus annotation.
</nextsent>
<nextsent>data sparseness, i.e. the insufficient coverage of the range of predicate senses and constructions within sensible sizes of manually annotated data, is bottleneck both for acquisition of linguistic knowledge for the semantic lexicon and for automated techniques for semantic role assignment.
</nextsent>
<nextsent>3 an abstract semantic basis for the representation of participant roles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4711">
<title id=" W11-1806.xml">event extraction as dependency parsing for bionlp 2011 </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W11-1802 ">
we describe the stanford entry to the bionlp2011 shared task on biomolecular event extraction (kim et al, 2011<papid> W11-1802 </papid>a).</citsent>
<aftsection>
<nextsent>our framework is based on the observation that event structures bear close relation to dependency graphs.
</nextsent>
<nextsent>we show that if biomolecular events are cast as these pseudo syntactic structures, standard parsing tools (maximum-spanning tree parser sand parse rerankers) can be applied to perform event extraction with minimum domain specific tuning.
</nextsent>
<nextsent>the vast majority of our domain-specific knowledge comes from the conversion to and from dependency graphs.our system performed competitively, obtaining 3rd place in the infectious diseases track (50.6% f-score), 5th place in epi genetics and post-translational modifications (31.2%), and 7th place in genia (50.0%).
</nextsent>
<nextsent>additionally, this system was part of the combined system in riedel et al (2011) to produce the highest scoring system in three out of the four event extraction tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4715">
<title id=" W11-1806.xml">event extraction as dependency parsing for bionlp 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the distinguishing aspect of our approach is that by casting event extraction as dependency parsing, wetake advantage of standard parsing tools and techniques rather than creating special purpose frameworks.
</prevsent>
<prevsent>in this paper, we show that with minimaldomain-specific tuning, we are able to achieve competitive performance across the three event extraction domains in the bionlp 2011 shared task.
</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
at the heart of our system1 is an off-the-shelf 1nlp.stanford.edu/software/eventparser.shtml dependency parser, mstparser2 (mcdonald et al, 2005; mcdonald and pereira, 2006), <papid> E06-1011 </papid>extended with event extraction-specific features and book ended by conversions to and from dependency trees.</citsent>
<aftsection>
<nextsent>while features in mst parser must be edge-factored and thus fairly local (e.g., only able to examine portion of each event at once), decoding is performed globally allowing the parser to consider trade-offs.
</nextsent>
<nextsent>furthermore, as mst parser can use n-best decoders, we are able to leverage reranker to capture global features to improve accuracy.in 2, we provide brief overview of our framework.
</nextsent>
<nextsent>we describe specific improvements for the bionlp 2011 shared task in 3.
</nextsent>
<nextsent>in 4, we present detailed results of our system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4716">
<title id=" W11-1806.xml">event extraction as dependency parsing for bionlp 2011 </title>
<section> event parsing.  </section>
<citcontext>
<prevsection>
<prevsent>finally, in 5 we give some directions for future work.
</prevsent>
<prevsent>our system includes three components: (1) anchor detection to identify and label event anchors, (2) event parsing to form candidate event structures by linking entities and event anchors, and (3) event reranking to select the best candidate event structure.
</prevsent>
</prevsection>
<citsent citstr=" P11-1163 ">
as the full details on our approach are described in mcclosky et al (2011), <papid> P11-1163 </papid>we will only provide an outline of our methods here along with additional implementation notes.</citsent>
<aftsection>
<nextsent>before running our system, we perform basic preprocessing on the corpora.
</nextsent>
<nextsent>sentences needto be segmented, tokenized, and parsed syntactically.
</nextsent>
<nextsent>we use custom versions of these (except for infectious diseases where we use those from stenetorp et al (2011)).
</nextsent>
<nextsent>to ease event parsing, our 2http://sourceforge.net/projects/mstparser/ 41 tokenizations are designed to split off suffixes which are often event anchors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4717">
<title id=" W11-1806.xml">event extraction as dependency parsing for bionlp 2011 </title>
<section> event parsing.  </section>
<citcontext>
<prevsection>
<prevsent>for example, we split the token rela-induced into the two tokens rela and in duced3 since relais protein and induced an event anchor.
</prevsent>
<prevsent>if this was single token, our event parser would be unable to link them since it cannot predict self-loops in the dependency graph.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
for syntactic parsing, we use the self-trained biomedical parsing model from mcclosky (2010) with the charniak and johnson (2005) <papid> P05-1022 </papid>reranking parser.</citsent>
<aftsection>
<nextsent>we use its actual constituency tree, the dependency graph created by applying head percolation rules, and the stanford dependencies (de marneffe and manning, 2008) extracted from the tree (collapsed and uncollapsed).
</nextsent>
<nextsent>anchor detection uses techniques inspired from named entity recognition to label each token with an event type or none.
</nextsent>
<nextsent>the features for this stage are primarily drawn from bjorne et al (2009).
</nextsent>
<nextsent>we reduce multiword event anchors to their syntactic head.4 we classify each token independently using logistic regression classifier with l2 regularization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4725">
<title id=" W11-1806.xml">event extraction as dependency parsing for bionlp 2011 </title>
<section> event parsing.  </section>
<citcontext>
<prevsection>
<prevsent>this is based on parse reranking (ratnaparkhi, 1999; collins, 2000) but uses features on event structures instead of syntactic constituency structures.
</prevsent>
<prevsent>we used mark johnsons cvlm estimator5 (charniak and johnson, 2005) <papid> P05-1022 </papid>when learning weights for the reranking model.</prevsent>
</prevsection>
<citsent citstr=" N10-1095 ">
since the reranker can incorporate the outputs from multiple decoders, we use it as an ensemble technique as in johnson and ural (2010).<papid> N10-1095 </papid></citsent>
<aftsection>
<nextsent>this section outlines the changes between ourbionlp 2011 shared task submission and the system described in mcclosky et al (2011).<papid> P11-1163 </papid></nextsent>
<nextsent>the main differences are that all dataset-specific portions ofthe model have been factored out to handle the expanded genia (ge) dataset (kim et al, 2011<papid> W11-1802 </papid>b) andthe new epi genetics and post-translational modifications (epi) and infectious diseases (id) datasets(ohta et al, 2011; <papid> W11-1803 </papid>pyysalo et al, 2011, <papid> W11-1804 </papid>respectively).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4733">
<title id=" W11-1806.xml">event extraction as dependency parsing for bionlp 2011 </title>
<section> extensions for bionlp 2011.  </section>
<citcontext>
<prevsection>
<prevsent>since the reranker can incorporate the outputs from multiple decoders, we use it as an ensemble technique as in johnson and ural (2010).<papid> N10-1095 </papid></prevsent>
<prevsent>this section outlines the changes between ourbionlp 2011 shared task submission and the system described in mcclosky et al (2011).<papid> P11-1163 </papid></prevsent>
</prevsection>
<citsent citstr=" W11-1803 ">
the main differences are that all dataset-specific portions ofthe model have been factored out to handle the expanded genia (ge) dataset (kim et al, 2011<papid> W11-1802 </papid>b) andthe new epi genetics and post-translational modifications (epi) and infectious diseases (id) datasets(ohta et al, 2011; <papid> W11-1803 </papid>pyysalo et al, 2011, <papid> W11-1804 </papid>respectively).</citsent>
<aftsection>
<nextsent>other changes are relatively minor but documented here as implementation notes.several improvements were made to anchor detection, improving its accuracy on all three domains.
</nextsent>
<nextsent>the first is the use of distributional similarity features.
</nextsent>
<nextsent>using large corpus of abstracts from pubmed (30,963,886 word tokens of 335,811word types), we cluster words by their syntactic contexts and morphological contents (clark, 2003).<papid> E03-1009 </papid></nextsent>
<nextsent>weused the ney-essen clustering model with morphology to produce 45 clusters.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4734">
<title id=" W11-1806.xml">event extraction as dependency parsing for bionlp 2011 </title>
<section> extensions for bionlp 2011.  </section>
<citcontext>
<prevsection>
<prevsent>since the reranker can incorporate the outputs from multiple decoders, we use it as an ensemble technique as in johnson and ural (2010).<papid> N10-1095 </papid></prevsent>
<prevsent>this section outlines the changes between ourbionlp 2011 shared task submission and the system described in mcclosky et al (2011).<papid> P11-1163 </papid></prevsent>
</prevsection>
<citsent citstr=" W11-1804 ">
the main differences are that all dataset-specific portions ofthe model have been factored out to handle the expanded genia (ge) dataset (kim et al, 2011<papid> W11-1802 </papid>b) andthe new epi genetics and post-translational modifications (epi) and infectious diseases (id) datasets(ohta et al, 2011; <papid> W11-1803 </papid>pyysalo et al, 2011, <papid> W11-1804 </papid>respectively).</citsent>
<aftsection>
<nextsent>other changes are relatively minor but documented here as implementation notes.several improvements were made to anchor detection, improving its accuracy on all three domains.
</nextsent>
<nextsent>the first is the use of distributional similarity features.
</nextsent>
<nextsent>using large corpus of abstracts from pubmed (30,963,886 word tokens of 335,811word types), we cluster words by their syntactic contexts and morphological contents (clark, 2003).<papid> E03-1009 </papid></nextsent>
<nextsent>weused the ney-essen clustering model with morphology to produce 45 clusters.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4735">
<title id=" W11-1806.xml">event extraction as dependency parsing for bionlp 2011 </title>
<section> extensions for bionlp 2011.  </section>
<citcontext>
<prevsection>
<prevsent>other changes are relatively minor but documented here as implementation notes.several improvements were made to anchor detection, improving its accuracy on all three domains.
</prevsent>
<prevsent>the first is the use of distributional similarity features.
</prevsent>
</prevsection>
<citsent citstr=" E03-1009 ">
using large corpus of abstracts from pubmed (30,963,886 word tokens of 335,811word types), we cluster words by their syntactic contexts and morphological contents (clark, 2003).<papid> E03-1009 </papid></citsent>
<aftsection>
<nextsent>weused the ney-essen clustering model with morphology to produce 45 clusters.
</nextsent>
<nextsent>using these clusters, we extended the feature set for anchor detection from mcclosky et al (2011) <papid> P11-1163 </papid>as follows: for each lexicalized feature we create an equivalent feature where the corresponding word is replaced by its cluster id. this yielded consistent improvements of at least 1 percentage point in both anchor detection and event 5http://github.com/bllip/bllip-parser 42 extraction in the development partition of the ge dataset.</nextsent>
<nextsent>additionally, we improved the head percolation rules for selecting the head of each multiword event anchor.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4741">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task concerns the extraction of detailed representations of 14 protein and dna modification events, the catalysis of these reactions, and the identification of instances of negated or speculatively stated event instances.
</prevsent>
<prevsent>seven teams submitted final results to the epi task in the shared task, with the highest-performing system achieving 53% f-score in the full taskand 69% f-score in the extraction of simplified set of core event arguments.
</prevsent>
</prevsection>
<citsent citstr=" W11-1802 ">
the epi genetics and post-translational modifications (epi) task is shared task on event extraction from biomedical domain scientific publications, first introduced as main task in the bionlp shared task 2011 (kim et al, 2011<papid> W11-1802 </papid>a).the epi task focuses on events relating to epi genetic change, including dna methyl ation and histone methyl ation and acetyl ation (see e.g.</citsent>
<aftsection>
<nextsent>(hol liday, 1987; jaenisch and bird, 2003)), as wellas other common protein post-translational modifications (ptms) (witze et al, 2007).
</nextsent>
<nextsent>ptms are chemical modifications of the amino acid residues of proteins, and dna methyl ation parallel modification of the nucleotides on dna.
</nextsent>
<nextsent>while these modifications are chemically simple reactions andcan thus be straightforwardly represented in full detail, they have crucial role in the regulation ofgene expression and protein function: the modifications can alter the conformation of dna or proteins and thus control their ability to associate with other molecules, making ptms key steps in protein biosynthesis for introducing the full range of protein functions.
</nextsent>
<nextsent>for instance, protein phosphorylation the attachment of phosphate ? is common mechanism for activating or inactivating enzymes by altering the conformation of protein active sites (stocket al, 1989; barford et al, 1998), and protein ubiq uitination ? the post-translational attachment of the small protein ubiquitin ? is the first step of major mechanism for the destruction (breakdown) of many proteins (glickman and ciechanover, 2002).many of the ptms targeted in the epi task involve modification of histone, core protein that forms an octameric complex that has crucial role in packaging chromosomal dna.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4747">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>them from the genome in way that is reversible through dna demethylation.
</prevsent>
<prevsent>16 figure 1: three views of protein methylation.
</prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
a) chemical formula b) event representation c) modification database entry.the bionlp09 shared task on event extraction (kim et al, 2009), <papid> W09-1401 </papid>the first task in the present shared task series, involved the extraction of nine event types including one ptm type, phosphory lation.</citsent>
<aftsection>
<nextsent>the results of the shared task showed this ptm event to be the single most reliably extracted event type in the task, with the best-performing system for the type achieving 91% precision and 76% recall (83% f-score) in its extraction (buyko et al, 2009).<papid> W09-1403 </papid></nextsent>
<nextsent>the results suggest both that the event representation is well applicable to ptm extraction and that current extraction methods are capable of reliable ptm extraction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4748">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>16 figure 1: three views of protein methylation.
</prevsent>
<prevsent>a) chemical formula b) event representation c) modification database entry.the bionlp09 shared task on event extraction (kim et al, 2009), <papid> W09-1401 </papid>the first task in the present shared task series, involved the extraction of nine event types including one ptm type, phosphory lation.</prevsent>
</prevsection>
<citsent citstr=" W09-1403 ">
the results of the shared task showed this ptm event to be the single most reliably extracted event type in the task, with the best-performing system for the type achieving 91% precision and 76% recall (83% f-score) in its extraction (buyko et al, 2009).<papid> W09-1403 </papid></citsent>
<aftsection>
<nextsent>the results suggest both that the event representation is well applicable to ptm extraction and that current extraction methods are capable of reliable ptm extraction.
</nextsent>
<nextsent>the epi task follows up on these opportunities, introducing specific, strongly biologically motivated extraction targets that are expected to be both feasible for high accuracy event extraction, relevant to the needs ofpresent-day molecular biology, and closely applicable to biomolecular database cur ation needs (see figure 1) (ohta et al, 2010<papid> W10-1903 </papid>a).</nextsent>
<nextsent>the epi task is an event extraction task in the sense popularized by number of recent domain resources and challenges (e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4749">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results of the shared task showed this ptm event to be the single most reliably extracted event type in the task, with the best-performing system for the type achieving 91% precision and 76% recall (83% f-score) in its extraction (buyko et al, 2009).<papid> W09-1403 </papid></prevsent>
<prevsent>the results suggest both that the event representation is well applicable to ptm extraction and that current extraction methods are capable of reliable ptm extraction.</prevsent>
</prevsection>
<citsent citstr=" W10-1903 ">
the epi task follows up on these opportunities, introducing specific, strongly biologically motivated extraction targets that are expected to be both feasible for high accuracy event extraction, relevant to the needs ofpresent-day molecular biology, and closely applicable to biomolecular database cur ation needs (see figure 1) (ohta et al, 2010<papid> W10-1903 </papid>a).</citsent>
<aftsection>
<nextsent>the epi task is an event extraction task in the sense popularized by number of recent domain resources and challenges (e.g.
</nextsent>
<nextsent>(pyysalo et al, 2007; kim et al,2008; thompson et al, 2009; kim et al, 2009; <papid> W09-1401 </papid>ananiadou et al, 2010)).</nextsent>
<nextsent>in broad outline, the task focuses on the extraction of information on statements regarding change in the state or properties of (physi cal) entities, modeled using an event representation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4759">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> task setting.  </section>
<citcontext>
<prevsection>
<prevsent>figure 2 illustrates these concepts.
</prevsent>
<prevsent>in its specific formulation, epi broadly follows the definition of the bionlp09 shared task on event extraction.
</prevsent>
</prevsection>
<citsent citstr=" W11-1804 ">
basic modification events are defined similarly to the phosphorylation event type targeted in the 09 and the 2011 ge and id tasks (kim et al, 2011<papid> W11-1802 </papid>b; pyysalo et al, 2011<papid> W11-1804 </papid>b), with the full task extending previously defined arguments with two additional ones, side chain and contextgene.</citsent>
<aftsection>
<nextsent>2.1 entities.
</nextsent>
<nextsent>the epi task follows the general policy of the bionlp shared task in isolating the basic task of named entity recognition from the event extraction task by providing task participants with manually annotated gene and gene product entities as starting point for extraction.
</nextsent>
<nextsent>the entity types follow the bionlp09 shared task scheme, where genes and their products are simply marked as protein.1 in addition to the given protein entities, some events involve other entities, such as the modification site.
</nextsent>
<nextsent>these entities are not given and must thus be identified by systems targeting the full task (see section 4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4770">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 annotation.
</prevsent>
<prevsent>annotation was performed manually.
</prevsent>
</prevsection>
<citsent citstr=" W09-1313 ">
the gene/protein entities automatically detected in the document selection step were provided to annotators for reference for creating protein annotations, but all entity annotations were checked and revised to conform to the specific guidelines for the task.5 for the annotation of protein entities, we adopted the genia gene/gene product (ggp) annotation guidelines (ohta et al, 2009), <papid> W09-1313 </papid>adding one specific exception: while the primary guidelines require that only specific individual gene or gene product names are annotated, we allowed also the annotation of mentions of groups of hi stones or 5this revision was substantial: only approximately 65% offinal protein annotations exactly match an automatically predicted one due to differences in annotation criteria (wang et al, 2009).</citsent>
<aftsection>
<nextsent>19 the entire histone protein family to capture histone modification events also in cases where only the group is mentioned.
</nextsent>
<nextsent>all event annotations were created from scratch without automatic support to avoid bias toward specific automatic extraction methods or approaches.the event annotation follows the genia event corpus annotation guidelines (kim et al, 2008) as they apply to protein modifications, with catalysis being annotated following the criteria for the positive regulation event type with the additional constraints that the cause of the event is gene or gene product entity and the form of regulation is catalysis of modification reaction.
</nextsent>
<nextsent>the manual annotation was performed by three experienced annotators with molecular biology background, with one chief annotator with extensive experience in domain event annotation organizing and supervising the annotator training and the over all process.
</nextsent>
<nextsent>after completion of primary annotation, we performed final check targeting simple human errors using an automatic extraction system.6 this correction process resulted in the revision of approximately 2% of the event annotations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4773">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>table 3 summarizes the participating groups and the features of their extraction systems.
</prevsent>
<prevsent>we note that, similarly to the 09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied.
</prevsent>
</prevsection>
<citsent citstr=" W11-1808 ">
in addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (riedel et al, 2011; <papid> W11-1808 </papid>mcclosky et al, 2011; <papid> W11-1806 </papid>riedel and mccallum, 2011) <papid> W11-1807 </papid>as opposed to pure pipeline systems (bjorne and salakoski, 2011; quirk et al,2011) . <papid> W11-1825 </papid>remarkably, the application of full parsing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of charniak and johnson (2005) <papid> P05-1022 </papid>with the biomedical domain model of mcclosky (2009) is applied in all but one system (liu et al, 2011) <papid> W11-1826 </papid>andthe stanford dependency representation (de marneffe et al, 2006) in all.</citsent>
<aftsection>
<nextsent>these choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (stenetorp et al., 2011).<papid> W11-1816 </papid>despite the availability of ptm and dna methyl ation resources other than those specifically introduced for the task and the phosphorylation annotations in the ge task (kim et al, 2011<papid> W11-1802 </papid>b), no participant chose to apply other corpora for training.</nextsent>
<nextsent>with the exception of externally acquired unlabeled data such as pubmed-derived word clusters applied by three groups, the task results thus reflect closed task setting in which only the given data is used for training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4774">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>table 3 summarizes the participating groups and the features of their extraction systems.
</prevsent>
<prevsent>we note that, similarly to the 09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied.
</prevsent>
</prevsection>
<citsent citstr=" W11-1806 ">
in addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (riedel et al, 2011; <papid> W11-1808 </papid>mcclosky et al, 2011; <papid> W11-1806 </papid>riedel and mccallum, 2011) <papid> W11-1807 </papid>as opposed to pure pipeline systems (bjorne and salakoski, 2011; quirk et al,2011) . <papid> W11-1825 </papid>remarkably, the application of full parsing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of charniak and johnson (2005) <papid> P05-1022 </papid>with the biomedical domain model of mcclosky (2009) is applied in all but one system (liu et al, 2011) <papid> W11-1826 </papid>andthe stanford dependency representation (de marneffe et al, 2006) in all.</citsent>
<aftsection>
<nextsent>these choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (stenetorp et al., 2011).<papid> W11-1816 </papid>despite the availability of ptm and dna methyl ation resources other than those specifically introduced for the task and the phosphorylation annotations in the ge task (kim et al, 2011<papid> W11-1802 </papid>b), no participant chose to apply other corpora for training.</nextsent>
<nextsent>with the exception of externally acquired unlabeled data such as pubmed-derived word clusters applied by three groups, the task results thus reflect closed task setting in which only the given data is used for training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4775">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>table 3 summarizes the participating groups and the features of their extraction systems.
</prevsent>
<prevsent>we note that, similarly to the 09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied.
</prevsent>
</prevsection>
<citsent citstr=" W11-1807 ">
in addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (riedel et al, 2011; <papid> W11-1808 </papid>mcclosky et al, 2011; <papid> W11-1806 </papid>riedel and mccallum, 2011) <papid> W11-1807 </papid>as opposed to pure pipeline systems (bjorne and salakoski, 2011; quirk et al,2011) . <papid> W11-1825 </papid>remarkably, the application of full parsing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of charniak and johnson (2005) <papid> P05-1022 </papid>with the biomedical domain model of mcclosky (2009) is applied in all but one system (liu et al, 2011) <papid> W11-1826 </papid>andthe stanford dependency representation (de marneffe et al, 2006) in all.</citsent>
<aftsection>
<nextsent>these choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (stenetorp et al., 2011).<papid> W11-1816 </papid>despite the availability of ptm and dna methyl ation resources other than those specifically introduced for the task and the phosphorylation annotations in the ge task (kim et al, 2011<papid> W11-1802 </papid>b), no participant chose to apply other corpora for training.</nextsent>
<nextsent>with the exception of externally acquired unlabeled data such as pubmed-derived word clusters applied by three groups, the task results thus reflect closed task setting in which only the given data is used for training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4776">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>table 3 summarizes the participating groups and the features of their extraction systems.
</prevsent>
<prevsent>we note that, similarly to the 09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied.
</prevsent>
</prevsection>
<citsent citstr=" W11-1825 ">
in addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (riedel et al, 2011; <papid> W11-1808 </papid>mcclosky et al, 2011; <papid> W11-1806 </papid>riedel and mccallum, 2011) <papid> W11-1807 </papid>as opposed to pure pipeline systems (bjorne and salakoski, 2011; quirk et al,2011) . <papid> W11-1825 </papid>remarkably, the application of full parsing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of charniak and johnson (2005) <papid> P05-1022 </papid>with the biomedical domain model of mcclosky (2009) is applied in all but one system (liu et al, 2011) <papid> W11-1826 </papid>andthe stanford dependency representation (de marneffe et al, 2006) in all.</citsent>
<aftsection>
<nextsent>these choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (stenetorp et al., 2011).<papid> W11-1816 </papid>despite the availability of ptm and dna methyl ation resources other than those specifically introduced for the task and the phosphorylation annotations in the ge task (kim et al, 2011<papid> W11-1802 </papid>b), no participant chose to apply other corpora for training.</nextsent>
<nextsent>with the exception of externally acquired unlabeled data such as pubmed-derived word clusters applied by three groups, the task results thus reflect closed task setting in which only the given data is used for training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4777">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>table 3 summarizes the participating groups and the features of their extraction systems.
</prevsent>
<prevsent>we note that, similarly to the 09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
in addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (riedel et al, 2011; <papid> W11-1808 </papid>mcclosky et al, 2011; <papid> W11-1806 </papid>riedel and mccallum, 2011) <papid> W11-1807 </papid>as opposed to pure pipeline systems (bjorne and salakoski, 2011; quirk et al,2011) . <papid> W11-1825 </papid>remarkably, the application of full parsing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of charniak and johnson (2005) <papid> P05-1022 </papid>with the biomedical domain model of mcclosky (2009) is applied in all but one system (liu et al, 2011) <papid> W11-1826 </papid>andthe stanford dependency representation (de marneffe et al, 2006) in all.</citsent>
<aftsection>
<nextsent>these choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (stenetorp et al., 2011).<papid> W11-1816 </papid>despite the availability of ptm and dna methyl ation resources other than those specifically introduced for the task and the phosphorylation annotations in the ge task (kim et al, 2011<papid> W11-1802 </papid>b), no participant chose to apply other corpora for training.</nextsent>
<nextsent>with the exception of externally acquired unlabeled data such as pubmed-derived word clusters applied by three groups, the task results thus reflect closed task setting in which only the given data is used for training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4778">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>table 3 summarizes the participating groups and the features of their extraction systems.
</prevsent>
<prevsent>we note that, similarly to the 09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied.
</prevsent>
</prevsection>
<citsent citstr=" W11-1826 ">
in addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (riedel et al, 2011; <papid> W11-1808 </papid>mcclosky et al, 2011; <papid> W11-1806 </papid>riedel and mccallum, 2011) <papid> W11-1807 </papid>as opposed to pure pipeline systems (bjorne and salakoski, 2011; quirk et al,2011) . <papid> W11-1825 </papid>remarkably, the application of full parsing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of charniak and johnson (2005) <papid> P05-1022 </papid>with the biomedical domain model of mcclosky (2009) is applied in all but one system (liu et al, 2011) <papid> W11-1826 </papid>andthe stanford dependency representation (de marneffe et al, 2006) in all.</citsent>
<aftsection>
<nextsent>these choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (stenetorp et al., 2011).<papid> W11-1816 </papid>despite the availability of ptm and dna methyl ation resources other than those specifically introduced for the task and the phosphorylation annotations in the ge task (kim et al, 2011<papid> W11-1802 </papid>b), no participant chose to apply other corpora for training.</nextsent>
<nextsent>with the exception of externally acquired unlabeled data such as pubmed-derived word clusters applied by three groups, the task results thus reflect closed task setting in which only the given data is used for training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4779">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>we note that, similarly to the 09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied.
</prevsent>
<prevsent>in addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (riedel et al, 2011; <papid> W11-1808 </papid>mcclosky et al, 2011; <papid> W11-1806 </papid>riedel and mccallum, 2011) <papid> W11-1807 </papid>as opposed to pure pipeline systems (bjorne and salakoski, 2011; quirk et al,2011) . <papid> W11-1825 </papid>remarkably, the application of full parsing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of charniak and johnson (2005) <papid> P05-1022 </papid>with the biomedical domain model of mcclosky (2009) is applied in all but one system (liu et al, 2011) <papid> W11-1826 </papid>andthe stanford dependency representation (de marneffe et al, 2006) in all.</prevsent>
</prevsection>
<citsent citstr=" W11-1816 ">
these choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (stenetorp et al., 2011).<papid> W11-1816 </papid>despite the availability of ptm and dna methyl ation resources other than those specifically introduced for the task and the phosphorylation annotations in the ge task (kim et al, 2011<papid> W11-1802 </papid>b), no participant chose to apply other corpora for training.</citsent>
<aftsection>
<nextsent>with the exception of externally acquired unlabeled data such as pubmed-derived word clusters applied by three groups, the task results thus reflect closed task setting in which only the given data is used for training.
</nextsent>
<nextsent>5.2 evaluation results.
</nextsent>
<nextsent>table 4 presents the primary results by event type, and table 5 summarizes these results.
</nextsent>
<nextsent>we note that only two teams, uturku (bjorne and salakoski, 2011) and concordu (kilicoglu and bergler, 2011),<papid> W11-1827 </papid>predicted event modifications, and only uturku predicted additional (non-core) event arguments (data not shown).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4786">
<title id=" W11-1803.xml">overview of the epi genetics and post translational modifications epi task of bionlp shared task 2011 </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>5.2 evaluation results.
</prevsent>
<prevsent>table 4 presents the primary results by event type, and table 5 summarizes these results.
</prevsent>
</prevsection>
<citsent citstr=" W11-1827 ">
we note that only two teams, uturku (bjorne and salakoski, 2011) and concordu (kilicoglu and bergler, 2011),<papid> W11-1827 </papid>predicted event modifications, and only uturku predicted additional (non-core) event arguments (data not shown).</citsent>
<aftsection>
<nextsent>the other five systems thus addressed 21msr nlpccp btmgcon corduuturku faust umass stanford size hydroxyl ation 42.25 10.26 10.20 12.80 9.45 12.84 6.32 139 dehydroxylation - - - - - - - 1 phosphorylation 67.12 51.61 50.00 49.18 40.98 47.06 44.44 130 dephosphorylation 0.00 0.00 0.00 0.00 0.00 50.00 0.00 3 ubiquitination 75.34 72.95 67.88 72.94 67.44 70.87 69.97 340 deubiquitination 54.55 40.00 0.00 31.58 0.00 42.11 14.29 17 dna methyl ation 60.21 31.21 34.54 23.82 31.02 15.65 8.22 416 dna demethylation 26.67 0.00 0.00 0.00 0.00 0.00 0.00 21 simple event total 63.05 45.17 44.97 43.01 40.96 40.62 37.84 1067 glycosylation 49.43 41.10 38.87 40.00 37.22 25.62 25.94 347 deglycosylation 40.00 35.29 0.00 38.10 30.00 35.29 26.67 27 acetyl ation 57.22 40.00 41.42 40.25 35.12 37.50 38.19 337 deacetylation 54.90 28.00 31.82 29.17 21.74 24.56 27.27 50 methyl ation 57.67 24.82 19.57 23.67 18.54 16.99 15.50 374 demethylation 35.71 0.00 0.00 0.00 0.00 0.00 0.00 13 non-simple event total 54.36 33.86 31.85 33.07 29.28 25.06 25.10 1148 catalysis 7.06 6.58 7.75 5.00 2.84 7.58 1.74 238 subtotal 55.02 36.93 36.17 35.30 32.85 30.58 28.92 2453 negation 18.60 0.00 0.00 0.00 0.00 0.00 26.51 149 speculation 37.65 0.00 0.00 0.00 0.00 0.00 6.82 103 modification total 28.07 0.00 0.00 0.00 0.00 0.00 16.37 252 total 53.33 35.03 34.27 33.52 31.22 28.97 27.88 2705 addition total 59.33 40.27 39.05 38.65 36.03 32.75 31.50 2038 removal total 44.29 22.41 15.73 22.76 14.41 23.53 17.48 132 table 4: primary evaluation f-scores by event type.
</nextsent>
<nextsent>the size?
</nextsent>
<nextsent>column gives the number of annotations of each type in the given data (training+development).
</nextsent>
<nextsent>best result for each type shown in bold.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4792">
<title id=" W11-0144.xml">incremental semantic construction in a dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe java-based implementation of the parser, used within the jindigo framework to produce an incremental dialogue system capable of handling inherently incremental phenomena such as split utterances, adjuncts, and mid-sentence clarification requests or backchannels.
</prevsent>
<prevsent>many dialogue phenomena seem to motivate an incremental view of language processing: for example, participants ability to change hearer/speaker role mid-sentence to produce or interpret back channels, or complete or continue an utterance (see e.g. yngve, 1970; lerner, 2004, amongst many others).
</prevsent>
</prevsection>
<citsent citstr=" E09-1081 ">
much recent research in dialogue systems has pursued this line, resulting in frameworks for incremental dialogue processing (schlangen and skantze, 2009) <papid> E09-1081 </papid>and systems capable of mid-utterance back channels (skantze and schlangen, 2009) <papid> E09-1085 </papid>or utterance completions (devault et al, 2009; <papid> W09-3902 </papid>bu?</citsent>
<aftsection>
<nextsent>et al, 2010).
</nextsent>
<nextsent>however, to date there has been little focus on semantics, with the systems produced either operating in domains in which semantic representation is not required (skantze and schlangen, 2009), <papid> E09-1085 </papid>or using variants of domain-specific canned lexical or phrasal matching (bu?</nextsent>
<nextsent>et al, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4793">
<title id=" W11-0144.xml">incremental semantic construction in a dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe java-based implementation of the parser, used within the jindigo framework to produce an incremental dialogue system capable of handling inherently incremental phenomena such as split utterances, adjuncts, and mid-sentence clarification requests or backchannels.
</prevsent>
<prevsent>many dialogue phenomena seem to motivate an incremental view of language processing: for example, participants ability to change hearer/speaker role mid-sentence to produce or interpret back channels, or complete or continue an utterance (see e.g. yngve, 1970; lerner, 2004, amongst many others).
</prevsent>
</prevsection>
<citsent citstr=" E09-1085 ">
much recent research in dialogue systems has pursued this line, resulting in frameworks for incremental dialogue processing (schlangen and skantze, 2009) <papid> E09-1081 </papid>and systems capable of mid-utterance back channels (skantze and schlangen, 2009) <papid> E09-1085 </papid>or utterance completions (devault et al, 2009; <papid> W09-3902 </papid>bu?</citsent>
<aftsection>
<nextsent>et al, 2010).
</nextsent>
<nextsent>however, to date there has been little focus on semantics, with the systems produced either operating in domains in which semantic representation is not required (skantze and schlangen, 2009), <papid> E09-1085 </papid>or using variants of domain-specific canned lexical or phrasal matching (bu?</nextsent>
<nextsent>et al, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4794">
<title id=" W11-0144.xml">incremental semantic construction in a dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe java-based implementation of the parser, used within the jindigo framework to produce an incremental dialogue system capable of handling inherently incremental phenomena such as split utterances, adjuncts, and mid-sentence clarification requests or backchannels.
</prevsent>
<prevsent>many dialogue phenomena seem to motivate an incremental view of language processing: for example, participants ability to change hearer/speaker role mid-sentence to produce or interpret back channels, or complete or continue an utterance (see e.g. yngve, 1970; lerner, 2004, amongst many others).
</prevsent>
</prevsection>
<citsent citstr=" W09-3902 ">
much recent research in dialogue systems has pursued this line, resulting in frameworks for incremental dialogue processing (schlangen and skantze, 2009) <papid> E09-1081 </papid>and systems capable of mid-utterance back channels (skantze and schlangen, 2009) <papid> E09-1085 </papid>or utterance completions (devault et al, 2009; <papid> W09-3902 </papid>bu?</citsent>
<aftsection>
<nextsent>et al, 2010).
</nextsent>
<nextsent>however, to date there has been little focus on semantics, with the systems produced either operating in domains in which semantic representation is not required (skantze and schlangen, 2009), <papid> E09-1085 </papid>or using variants of domain-specific canned lexical or phrasal matching (bu?</nextsent>
<nextsent>et al, 2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4796">
<title id=" W11-0144.xml">incremental semantic construction in a dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, the lack of notion of syntactic constituency means no direct equivalent for the active/passive edge distinction; detailed comparison is still to be carried out.
</prevsent>
<prevsent>367 4 dialogue system.
</prevsent>
</prevsection>
<citsent citstr=" W10-4301 ">
the dylan parser has now been integrated into working dialogue system by implementation as an interpreter module in the java-based incremental dialogue framework jindigo (skantze and hjalmarsson, 2010).<papid> W10-4301 </papid></citsent>
<aftsection>
<nextsent>jindigo follows schlangen and skantze (2009)<papid> E09-1081 </papid>s abstract architecture specification andis specifically designed to handle units smaller than fully sentential utterances; one of its specific implementations is travel agent system, and our module integrates semantic interpretation into this.as set out by schlangen and skantze (2009)<papid> E09-1081 </papid>s specification, our interpreters essential components are left buffer (lb), processor and right buffer (rb).</nextsent>
<nextsent>incremental units (ius) of various types are posted from the rb of one module to the lb of another; for our module, the lb-ius are asr word hypotheses, and after processing, domain-level concept frames are posted as rb-ius for further processing by downstream dialogue manager.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4807">
<title id=" W10-4203.xml">generating referring expressions with reference domain theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover each referring act presupposesa given state of the referential space, and the explicit representation of these pre suppositions as constraints on the suitable domain for interpretation or generation allows the implementation of reversible reference module.
</prevsent>
<prevsent>we will focus here on generation.
</prevsent>
</prevsection>
<citsent citstr=" W06-1308 ">
details about the interpretation side of rdt can be found in (salmon-alt and romary, 2001; denis et al., 2006).<papid> W06-1308 </papid></citsent>
<aftsection>
<nextsent>however most of the previous work on rdt does not address computational details.
</nextsent>
<nextsent>although(salmon-alt and romary, 2000) provides generation algorithm, the formal definition of reference domain and the explicit representation of the constraints are not provided.
</nextsent>
<nextsent>in this paper we show how rdt can be used to generate referring expressions.the context of our work is the give challenge (by ron et al, 2007; byron et al, 2009).<papid> W09-0628 </papid></nextsent>
<nextsent>this challenge aims to evaluate instruction generation systems in situated setting.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4808">
<title id=" W10-4203.xml">generating referring expressions with reference domain theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however most of the previous work on rdt does not address computational details.
</prevsent>
<prevsent>although(salmon-alt and romary, 2000) provides generation algorithm, the formal definition of reference domain and the explicit representation of the constraints are not provided.
</prevsent>
</prevsection>
<citsent citstr=" W09-0628 ">
in this paper we show how rdt can be used to generate referring expressions.the context of our work is the give challenge (by ron et al, 2007; byron et al, 2009).<papid> W09-0628 </papid></citsent>
<aftsection>
<nextsent>this challenge aims to evaluate instruction generation systems in situated setting.
</nextsent>
<nextsent>the goal is to provide instructions to player in 3d maze in order to guide him to finda hidden trophy.
</nextsent>
<nextsent>we are here interested with there ferring aspect involved in give: the player has to push buttons to open doors or disable alarms, thus the system has to generate referring expressions to these buttons.
</nextsent>
<nextsent>we first present in section 2 some definitions, then in section 3 we detail generic generation algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4810">
<title id=" W10-4203.xml">generating referring expressions with reference domain theory </title>
<section> definitions.  </section>
<citcontext>
<prevsection>
<prevsent>if this property partitions the ground, new domain is created for eachnon-singleton part of the partition, and the algorithm tries to partition it with the next property,so on recursively.
</prevsent>
<prevsent>we note sh(x, c) the set of properties of the type that are shared by all elements of x: sh(x, c) = {p|p ? val(c),x ? x; p(x) ? }.
</prevsent>
</prevsection>
<citsent citstr=" D07-1011 ">
this partitioning algorithm is slightly different from the partitioning algorithm called iapart foundin (gatt and van deemter, 2007).<papid> D07-1011 </papid></citsent>
<aftsection>
<nextsent>first, it only partitions set of objects using one unique property, whereas in iapart the same set of objects can be partitioned several times.
</nextsent>
<nextsent>and second, while iapartdestroys?
</nextsent>
<nextsent>the ground that is partitioned, our partitioning algorithm maintains both the ground and the partition attached to the domain.
</nextsent>
<nextsent>1: rs ? rs ? {d} 2: if 6= ? then 3: ? gd/rt0 4: if |p | = 1 then 5: sd ? sd ? sh(gd, t0) 6: createpartitions(d, t1..n, rs) 7: else 8: set (t0, p, ?) as ds partition structure 9: for all ? such that |x|   1 do 10: d?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4811">
<title id=" W10-4203.xml">generating referring expressions with reference domain theory </title>
<section> referring.  </section>
<citcontext>
<prevsection>
<prevsent>then this domain is used to match so called underspecified domain (salmon-alt and romary, 2001).
</prevsent>
<prevsent>third, the input rs is restructured relative to the selected reference domain.
</prevsent>
</prevsection>
<citsent citstr=" W05-1623 ">
the approach enables the implementation of type reversible reference module (klarner, 2005), <papid> W05-1623 </papid>that is module in which both directions share the expression u(n, t) matches iff ?(c, p, ) ? d; this one = {{t}} ? msd(d) this f = {{t}} ? ? ni the t ? ni ? {t} ? ? xp,x 6={t}xni =?</citsent>
<aftsection>
<nextsent>the other one 6= ? ?
</nextsent>
<nextsent>p \ = {{t}} ? msd(d) the other f 6= ? ?
</nextsent>
<nextsent>p \ = {{t}} gd ? i another one 6= ? ?
</nextsent>
<nextsent>{t} ? \ ? msd(d) another f 6= ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4812">
<title id=" W10-4203.xml">generating referring expressions with reference domain theory </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>sounds natural.
</prevsent>
<prevsent>it would be more complex to handle setting with both the linguistic and the visual focus, but we think that the rdt is well-equipped to resolve this issue.
</prevsent>
</prevsection>
<citsent citstr=" E91-1028 ">
we evaluated the rdt generation model by comparing its performances with another system also competing in the give challenge but based on classical approach on (dale and haddock, 1991) <papid> E91-1028 </papid>that is restricted to generating definite and indefinite nps.</citsent>
<aftsection>
<nextsent>we designed special evaluation world to test several reference cases, and for both approaches, we measured the average time from the moment of uttering first mention designating button to the moment of completion, that is when the button is successfully pushed.
</nextsent>
<nextsent>we also measured the average number of instructions that were provided in the meantime.
</nextsent>
<nextsent>the evaluation has been conducted with 30 subjects resulting in 20 valid games.
</nextsent>
<nextsent>the results show that the rdt performs better than the classical strategy, both for the average completion time (8.8 seconds versus 12.5 seconds) and for the number of instructions (6.4 versus 9.3).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4813">
<title id=" W10-4203.xml">generating referring expressions with reference domain theory </title>
<section> other works and extensions.  </section>
<citcontext>
<prevsection>
<prevsent>the results show that the rdt performs better than the classical strategy, both for the average completion time (8.8 seconds versus 12.5 seconds) and for the number of instructions (6.4 versus 9.3).
</prevsent>
<prevsent>we conjecture that the good results of rdt can be explained by the lower cognitive load resulting from the use of demonstrative nps and one-anaphoras.
</prevsent>
</prevsection>
<citsent citstr=" J03-1003 ">
while some regeneration models focus on the sideof generating the description itself (dale andre iter, 1995; krahmer et al, 2003), <papid> J03-1003 </papid>we tried to focus more on the side of generating the determiner.</citsent>
<aftsection>
<nextsent>while works such as (poesio et al, 1999) also generates the determiner, they relyon statistical learning of thisdeterminer.
</nextsent>
<nextsent>on the contrary we did so by representing logically the constraints carried by referring expressions on the context of its interpretation.
</nextsent>
<nextsent>how ever, the presented model has several limits.
</nextsent>
<nextsent>first, as (landragin and romary, 2003) describe, there isno one-to-one relation between the referring expressions and the referring modes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4814">
<title id=" W11-0210.xml">building a coreference annotated corpus from the domain of biochemistry </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>co references are linguistic expressions referring to the same real-world entity (jurafsky and martin,2009).
</prevsent>
<prevsent>the process of grouping all co-referring expressions in text into respective coreference chains is known as coreference resolution.
</prevsent>
</prevsection>
<citsent citstr=" M95-1001 ">
it was introduce das one of the tasks of the sixth message understanding conference (muc-6) in 1995 (grishman and sundheim, 1995) <papid> M95-1001 </papid>and is one of the information extraction tasks which have remained challenge to this day.</citsent>
<aftsection>
<nextsent>one of the reasons it is still considered an unresolved problem especially in the biomedical domain is the lack of coreference-annotated corpora which are needed for developing coreference resolution systems.
</nextsent>
<nextsent>there exist only handful of biomedical corpora which are annotated with coreference information.we have conducted review of each of them, taking into consideration their sizes, document composition, domain, types of mark able entities, types of coreference annotated, availability, and reliability in terms of inter-annotator agreement.
</nextsent>
<nextsent>of these, onlytwo corpora have been used in coreference resolution systems developed outside the research group that annotated them: meds tract (castano et al, 2002), and the medco1 corpus of abstracts which was used by the different teams who participated in the coreference supporting task of the bionlp 2011 shared task2.
</nextsent>
<nextsent>these two corpora are widely.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4815">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thiscan make evaluation difficult; for instance, system generated sentences may differ (partially or com pletely) in informational content from reference human-annotated text.
</prevsent>
<prevsent>this phenomenon has been noted and discussed in the task of pairwise sentence fusion (daume?
</prevsent>
</prevsection>
<citsent citstr=" E06-1038 ">
iii and marcu, 2004) and also in sentence compression (mcdonald, 2006).<papid> E06-1038 </papid></citsent>
<aftsection>
<nextsent>some examples are listed in table 1.in this work, we examine the task of sentence in tersection: variant of sentence fusion that does not permit semantic variation in the output.
</nextsent>
<nextsent>a strict1 intersection system is expected to produce fused sentence that contains all the information common to its input sentences and avoid information that is in just one of the inputs.
</nextsent>
<nextsent>in other words, valid intersection should only contain information that is substantiated by all input sentences.
</nextsent>
<nextsent>the set-theoretic notions of intersection (along with union) have been employed to describe variants of sentence fusion tasks in previous work (marsi and krahmer, 2005; <papid> W05-1612 </papid>krahmer et al, 2008) <papid> P08-2049 </papid>but, to our knowledge, this work is the first to explicitly tackle and evaluate the strict intersection task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4816">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a strict1 intersection system is expected to produce fused sentence that contains all the information common to its input sentences and avoid information that is in just one of the inputs.
</prevsent>
<prevsent>in other words, valid intersection should only contain information that is substantiated by all input sentences.
</prevsent>
</prevsection>
<citsent citstr=" W05-1612 ">
the set-theoretic notions of intersection (along with union) have been employed to describe variants of sentence fusion tasks in previous work (marsi and krahmer, 2005; <papid> W05-1612 </papid>krahmer et al, 2008) <papid> P08-2049 </papid>but, to our knowledge, this work is the first to explicitly tackle and evaluate the strict intersection task.</citsent>
<aftsection>
<nextsent>we focus on the case of unsupervised pairwise sentence intersection and propose strategy to yield 1we use the term strict to make explicit the distinction from traditional fusion systems, which generally aim at notions of intersection but are not formally evaluated with respect to it.
</nextsent>
<nextsent>43 (a) fusion example from daume?
</nextsent>
<nextsent>iii and marcu (2004)(i) after years of pursuing separate and conflicting paths, at&t; and digital equipment corp. agreed in june to settle their computer-to-pbx differences.
</nextsent>
<nextsent>(ii) the two will jointly develop an applications interface that can be shared by computers and pbxs of any stripe.human fusion #1 at&t; and digital equipment corp. agreed in june to settle their computer-to pbx differences and develop an applications interface that can be shared by any computer or pbx.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4817">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a strict1 intersection system is expected to produce fused sentence that contains all the information common to its input sentences and avoid information that is in just one of the inputs.
</prevsent>
<prevsent>in other words, valid intersection should only contain information that is substantiated by all input sentences.
</prevsent>
</prevsection>
<citsent citstr=" P08-2049 ">
the set-theoretic notions of intersection (along with union) have been employed to describe variants of sentence fusion tasks in previous work (marsi and krahmer, 2005; <papid> W05-1612 </papid>krahmer et al, 2008) <papid> P08-2049 </papid>but, to our knowledge, this work is the first to explicitly tackle and evaluate the strict intersection task.</citsent>
<aftsection>
<nextsent>we focus on the case of unsupervised pairwise sentence intersection and propose strategy to yield 1we use the term strict to make explicit the distinction from traditional fusion systems, which generally aim at notions of intersection but are not formally evaluated with respect to it.
</nextsent>
<nextsent>43 (a) fusion example from daume?
</nextsent>
<nextsent>iii and marcu (2004)(i) after years of pursuing separate and conflicting paths, at&t; and digital equipment corp. agreed in june to settle their computer-to-pbx differences.
</nextsent>
<nextsent>(ii) the two will jointly develop an applications interface that can be shared by computers and pbxs of any stripe.human fusion #1 at&t; and digital equipment corp. agreed in june to settle their computer-to pbx differences and develop an applications interface that can be shared by any computer or pbx.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4819">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(b) compression example from mcdonald (2006) <papid> E06-1038 </papid>tape ware , which supports dos and netware 286 , is value-added process that lets you directly connect the qa150-exat to file server and issue command from any workstation to back up the server human compression #1 tape ware supports dos and netware 286 human compression #2 tape ware lets you connect the qa150-exat to file server (hypothesized)table 1: examples of text-to-text generation problems with multiple valid human-generated outputs that differ significantly in semantic content.</prevsent>
<prevsent>italicized text is used to indicate fragments that are semantically identical.</prevsent>
</prevsection>
<citsent citstr=" J05-3002 ">
valid intersections that follows the basic framework of previous unsupervised fusion systems (barzilay and mckeown, 2005; <papid> J05-3002 </papid>filippova and strube, 2008<papid> W08-1105 </papid>b).</citsent>
<aftsection>
<nextsent>in our approach, the input sentences are first aligned using modified version of recent phrase-based alignment approach (maccartney et al, 2008).<papid> D08-1084 </papid></nextsent>
<nextsent>we assume the alignments that are produced define aspects of the input that must appear in the output fusion and consider decoding strategies to recover intersections that preserve these alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4822">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(b) compression example from mcdonald (2006) <papid> E06-1038 </papid>tape ware , which supports dos and netware 286 , is value-added process that lets you directly connect the qa150-exat to file server and issue command from any workstation to back up the server human compression #1 tape ware supports dos and netware 286 human compression #2 tape ware lets you connect the qa150-exat to file server (hypothesized)table 1: examples of text-to-text generation problems with multiple valid human-generated outputs that differ significantly in semantic content.</prevsent>
<prevsent>italicized text is used to indicate fragments that are semantically identical.</prevsent>
</prevsection>
<citsent citstr=" W08-1105 ">
valid intersections that follows the basic framework of previous unsupervised fusion systems (barzilay and mckeown, 2005; <papid> J05-3002 </papid>filippova and strube, 2008<papid> W08-1105 </papid>b).</citsent>
<aftsection>
<nextsent>in our approach, the input sentences are first aligned using modified version of recent phrase-based alignment approach (maccartney et al, 2008).<papid> D08-1084 </papid></nextsent>
<nextsent>we assume the alignments that are produced define aspects of the input that must appear in the output fusion and consider decoding strategies to recover intersections that preserve these alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4836">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>italicized text is used to indicate fragments that are semantically identical.
</prevsent>
<prevsent>valid intersections that follows the basic framework of previous unsupervised fusion systems (barzilay and mckeown, 2005; <papid> J05-3002 </papid>filippova and strube, 2008<papid> W08-1105 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" D08-1084 ">
in our approach, the input sentences are first aligned using modified version of recent phrase-based alignment approach (maccartney et al, 2008).<papid> D08-1084 </papid></citsent>
<aftsection>
<nextsent>we assume the alignments that are produced define aspects of the input that must appear in the output fusion and consider decoding strategies to recover intersections that preserve these alignments.
</nextsent>
<nextsent>in addition to search-based decoding strategy, we proposea constrained integer linear programming (ilp) formulation that attempts to decode the most fluent sentence covering all these aspects while minimizing the size and dis fluency of inter leaving text.
</nextsent>
<nextsent>this is fairly general model which can also be extended to other alignment-based tasks such as pairwise union and difference.
</nextsent>
<nextsent>as this is substantially more constrained task than generic sentence fusion, we also present novel evaluation approach that avoids out-of-contextsalience judgments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4837">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is fairly general model which can also be extended to other alignment-based tasks such as pairwise union and difference.
</prevsent>
<prevsent>as this is substantially more constrained task than generic sentence fusion, we also present novel evaluation approach that avoids out-of-contextsalience judgments.
</prevsent>
</prevsection>
<citsent citstr=" N10-1044 ">
we make use of recently released corpus of fusion candidates (mckeown etal., 2010) <papid> N10-1044 </papid>and propose crowd sourced entailment style evaluation to determine the validity of generated intersections, as well as the grammaticality of the sentences produced.</citsent>
<aftsection>
<nextsent>additionally, automated machine translation (mt) metrics are explored to quantify the amount of information missing from valid intersections.
</nextsent>
<nextsent>our decoding strategies show promise under these experiments and we discuss potential directions for improving intersection performance.
</nextsent>
<nextsent>the distinction between intersection and union oftext was introduced in the context of sentence fusion (krahmer et al, 2008; <papid> P08-2049 </papid>marsi and krahmer,2005) <papid> W05-1612 </papid>in order to distinguish between traditional fusion strategies that attempted to include only common content and fusions that attempted to include all non-redundant content from the input.</nextsent>
<nextsent>we focus here on strict sentence intersection, explicitly incorporating constraint that requires that produced fusion must not contain information that isnot present in all input sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4840">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the distinction between intersection and union oftext was introduced in the context of sentence fusion (krahmer et al, 2008; <papid> P08-2049 </papid>marsi and krahmer,2005) <papid> W05-1612 </papid>in order to distinguish between traditional fusion strategies that attempted to include only common content and fusions that attempted to include all non-redundant content from the input.</prevsent>
<prevsent>we focus here on strict sentence intersection, explicitly incorporating constraint that requires that produced fusion must not contain information that isnot present in all input sentences.</prevsent>
</prevsection>
<citsent citstr=" A00-2024 ">
this distinguishes our approach from traditional sentence fusion approaches (jing and mckeown, 2000; <papid> A00-2024 </papid>barzilay and mckeown, 2005; <papid> J05-3002 </papid>filippova and strube, 2008<papid> W08-1105 </papid>b) which generally attempt to retain common information but are typically evaluated in an abs tractive summarization context in which additional information in the fusion output does not negatively impact judgments.</citsent>
<aftsection>
<nextsent>this task is also related to the field of sentence compression which has received much attention in recent years (turner and charniak, 2005; <papid> P05-1036 </papid>mcdonald, 2006; <papid> E06-1038 </papid>clarke and lapata, 2008; filippova and strube, 2008<papid> W08-1105 </papid>a; cohn and lapata, 2009; marsi et al,2010).</nextsent>
<nextsent>intersections can be viewed as guided com 44 pres sions in which the redundancy of information content across input sentences in multi document setting is assumed to directly indicate its salience, thereby consigning it to the output.additionally, in this work, we frequently consider the sentence intersection task from the perspective of textual entailment (cf.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4856">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we focus here on strict sentence intersection, explicitly incorporating constraint that requires that produced fusion must not contain information that isnot present in all input sentences.
</prevsent>
<prevsent>this distinguishes our approach from traditional sentence fusion approaches (jing and mckeown, 2000; <papid> A00-2024 </papid>barzilay and mckeown, 2005; <papid> J05-3002 </papid>filippova and strube, 2008<papid> W08-1105 </papid>b) which generally attempt to retain common information but are typically evaluated in an abs tractive summarization context in which additional information in the fusion output does not negatively impact judgments.</prevsent>
</prevsection>
<citsent citstr=" P05-1036 ">
this task is also related to the field of sentence compression which has received much attention in recent years (turner and charniak, 2005; <papid> P05-1036 </papid>mcdonald, 2006; <papid> E06-1038 </papid>clarke and lapata, 2008; filippova and strube, 2008<papid> W08-1105 </papid>a; cohn and lapata, 2009; marsi et al,2010).</citsent>
<aftsection>
<nextsent>intersections can be viewed as guided com 44 pres sions in which the redundancy of information content across input sentences in multi document setting is assumed to directly indicate its salience, thereby consigning it to the output.additionally, in this work, we frequently consider the sentence intersection task from the perspective of textual entailment (cf.
</nextsent>
<nextsent>5.1).
</nextsent>
<nextsent>the textual entailment task involves automatically determining whether given hypothesis can be inferred from textual premise (dagan et al, 2005; bar-haim et al,2006).
</nextsent>
<nextsent>automatic construction of positive and negative entailment examples has been explored in the past (bensley and hickl, 2008) <papid> L08-1224 </papid>to provide training data for entailment systems; however the production of text that is simultaneously entailed by two (or more) sentences is far more constrained and difficult challenge.ilp has been used extensively for text-to-text generation problems in recent years (clarke and lapata, 2008; filippova and strube, 2008<papid> W08-1105 </papid>b; woodsend et al,2010), <papid> D10-1050 </papid>including techniques which incorporate syntax directly into the decoding to imporove the fluency of the resulting text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4872">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>5.1).
</prevsent>
<prevsent>the textual entailment task involves automatically determining whether given hypothesis can be inferred from textual premise (dagan et al, 2005; bar-haim et al,2006).
</prevsent>
</prevsection>
<citsent citstr=" L08-1224 ">
automatic construction of positive and negative entailment examples has been explored in the past (bensley and hickl, 2008) <papid> L08-1224 </papid>to provide training data for entailment systems; however the production of text that is simultaneously entailed by two (or more) sentences is far more constrained and difficult challenge.ilp has been used extensively for text-to-text generation problems in recent years (clarke and lapata, 2008; filippova and strube, 2008<papid> W08-1105 </papid>b; woodsend et al,2010), <papid> D10-1050 </papid>including techniques which incorporate syntax directly into the decoding to imporove the fluency of the resulting text.</citsent>
<aftsection>
<nextsent>in this paper, we focus on generating valid intersections and do not incorporate syntactic and semantic constraints into our ilp mod els; these are areas we intend to explore in the future.
</nextsent>
<nextsent>the need for strict variants of fusion is motivated by considerations of evaluation and utility in text-to text generation tasks.
</nextsent>
<nextsent>without explicit constraints on the semantic content of valid output, the operational definition of fusion can encompass the full spectrum from sentence intersection to sentence union.
</nextsent>
<nextsent>this makes the comparison of different fusion systems dependent on task-based utility2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4887">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>5.1).
</prevsent>
<prevsent>the textual entailment task involves automatically determining whether given hypothesis can be inferred from textual premise (dagan et al, 2005; bar-haim et al,2006).
</prevsent>
</prevsection>
<citsent citstr=" D10-1050 ">
automatic construction of positive and negative entailment examples has been explored in the past (bensley and hickl, 2008) <papid> L08-1224 </papid>to provide training data for entailment systems; however the production of text that is simultaneously entailed by two (or more) sentences is far more constrained and difficult challenge.ilp has been used extensively for text-to-text generation problems in recent years (clarke and lapata, 2008; filippova and strube, 2008<papid> W08-1105 </papid>b; woodsend et al,2010), <papid> D10-1050 </papid>including techniques which incorporate syntax directly into the decoding to imporove the fluency of the resulting text.</citsent>
<aftsection>
<nextsent>in this paper, we focus on generating valid intersections and do not incorporate syntactic and semantic constraints into our ilp mod els; these are areas we intend to explore in the future.
</nextsent>
<nextsent>the need for strict variants of fusion is motivated by considerations of evaluation and utility in text-to text generation tasks.
</nextsent>
<nextsent>without explicit constraints on the semantic content of valid output, the operational definition of fusion can encompass the full spectrum from sentence intersection to sentence union.
</nextsent>
<nextsent>this makes the comparison of different fusion systems dependent on task-based utility2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4888">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> the intersection task.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, intersection comprises an interesting problem in its own right.
</prevsent>
<prevsent>it necessitates the use of generalization over phrases in order to convey only the content of the input sentences when different wording is used and therefore involves more than just word deletion.
</prevsent>
</prevsection>
<citsent citstr=" C04-1057 ">
the analogy to set-theoretic intersection in this task implies an underlying consideration of each sentence as set of informational concepts, sim 2for instance, systems may trade off conciseness against grammaticality, or informational content with degree of support across the input sentences.ilar to previous work in summarization andre dundancy (filatova and hatzivassiloglou, 2004; <papid> C04-1057 </papid>thadani and mckeown, 2008).<papid> C08-1110 </papid></citsent>
<aftsection>
<nextsent>while we dont commit to any semantic representation for such elements of information, we can nevertheless attempt to identify repeated information using well-studiednatural language analysis techniques such as alignment and paraphrase recognition, and furthermore isolate this information through text-to-text generation techniques.
</nextsent>
<nextsent>consider, for example, the first sentence pair from the examples in table 2.
</nextsent>
<nextsent>a valid intersection for these sentences must not contain any information that is not substantiated by both of them, so fusion that mentions mr litvinenkos poisoning?,britain?
</nextsent>
<nextsent>or sunday?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4889">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> the intersection task.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, intersection comprises an interesting problem in its own right.
</prevsent>
<prevsent>it necessitates the use of generalization over phrases in order to convey only the content of the input sentences when different wording is used and therefore involves more than just word deletion.
</prevsent>
</prevsection>
<citsent citstr=" C08-1110 ">
the analogy to set-theoretic intersection in this task implies an underlying consideration of each sentence as set of informational concepts, sim 2for instance, systems may trade off conciseness against grammaticality, or informational content with degree of support across the input sentences.ilar to previous work in summarization andre dundancy (filatova and hatzivassiloglou, 2004; <papid> C04-1057 </papid>thadani and mckeown, 2008).<papid> C08-1110 </papid></citsent>
<aftsection>
<nextsent>while we dont commit to any semantic representation for such elements of information, we can nevertheless attempt to identify repeated information using well-studiednatural language analysis techniques such as alignment and paraphrase recognition, and furthermore isolate this information through text-to-text generation techniques.
</nextsent>
<nextsent>consider, for example, the first sentence pair from the examples in table 2.
</nextsent>
<nextsent>a valid intersection for these sentences must not contain any information that is not substantiated by both of them, so fusion that mentions mr litvinenkos poisoning?,britain?
</nextsent>
<nextsent>or sunday?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4931">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> models for intersection.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 phrase-based alignment.
</prevsent>
<prevsent>the alignment phase is major component of any intersection system as it is used to uncover the common segments in the input that must be preserved in the output.
</prevsent>
</prevsection>
<citsent citstr=" P11-2044 ">
we make use of an adaptation of the supervised manli phrase-based alignment technique originally developed for textual entailment systems (maccartney et al, 2008); <papid> D08-1084 </papid>our implementation replaces approximate search-baseddecoding with exact ilp-based alignment decoding and incorporates syntactic constraints to produce more precise alignments (thadani and mckeown, 2011).<papid> P11-2044 </papid></citsent>
<aftsection>
<nextsent>the aligner is trained on corpus of human-generated alignment annotations produced by microsoft research (brockett, 2007) for inference problems from the second recognizing textual entailment (rte2) challenge (bar-haim et al, 2006).
</nextsent>
<nextsent>entailment problems are inherently asymmetric because premise text is generally larger than hypothesis text; however, this does not apply to our intersection problems and consequently our manli implementation drops asymmetric indicator features.
</nextsent>
<nextsent>the absence of these features impacts alignment performance on rte2 data but our re implementation performs comparably to the original model under the alignment evaluation from maccartney et al (2008).<papid> D08-1084 </papid></nextsent>
<nextsent>4.2 ontology-based generalization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4933">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> models for intersection.  </section>
<citcontext>
<prevsection>
<prevsent>for our experiments, we make use of the wordnet ontology (miller, 1995) to find the hypernyms common to every aligned pair of non-identical phrases, and only attempt to detect ent ailments which are comprised of specific instances that entail general concepts.
</prevsent>
<prevsent>this approach can be augmented by theuse of entailment corpora and distributional clustering which we intend to explore in future work.
</prevsent>
</prevsection>
<citsent citstr=" N03-1013 ">
we also use the lexical resource catvar (habash anddorr, 2003) <papid> N03-1013 </papid>to try to generate morphological variants of aligned words that enable them to be interchanged without creating disfluencies.</citsent>
<aftsection>
<nextsent>4.3 pragmatic abstraction.
</nextsent>
<nextsent>our strategy assumes that aligned text must be preserved in output intersections whereas unaligned text must be minimized.
</nextsent>
<nextsent>however, unaligned text cannot simply be dropped as it may contain vital portions for generating fluent text.
</nextsent>
<nextsent>in addition, unaligned phrases can be caused by paraphrased or metaphorical text that the aligner is not capable of identifying.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4934">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> models for intersection.  </section>
<citcontext>
<prevsection>
<prevsent>we make use of language model (lm) to judge fluency and propose two techniques to decode high-scoring text from the lat tice: simple beam-search technique and an ilp 47 alignment link m m phrases from s1 shared phrases phrases from s2 alignment link phrases from s1 shared phrases phrases from s2 figure 1: the general structure of one segment of the alignment lattice, illustrating the potential inter leaving paths between aligned phrases.
</prevsent>
<prevsent>solid lines indicate paths derived from sentence 1 and dashed lines indicate paths derived from sentence 2 strategy that leverages our initial assumption that all aligned phrases must appear in the output.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
4.4.1 beam searchsearch-based decoding is often employed in phrase based mt systems (och and ney, 2003) <papid> J03-1002 </papid>and is implemented in the moses toolkit5; similar approaches have also been used in text-to-text generation tasks (barzilay and mckeown, 2005; <papid> J05-3002 </papid>soricut and marcu, 2006).<papid> P06-1139 </papid></citsent>
<aftsection>
<nextsent>this technique attempts to find the highest-scoring sentence string under the lm by unwrapping and searching through lattice.
</nextsent>
<nextsent>since the dynamic programming search could require an exponential number of search states, fixed-width beam can be used to control the number of search states being actively considered at each step.
</nextsent>
<nextsent>in order to decode an intersection problem, wefirst pick beam size and initialize the list of candidate search states with the first inter leaving paths in each sentence.
</nextsent>
<nextsent>at every iteration, we consider theb candidates with the highest normalized scores under the lm and remove them from the candidate list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4938">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> models for intersection.  </section>
<citcontext>
<prevsection>
<prevsent>we make use of language model (lm) to judge fluency and propose two techniques to decode high-scoring text from the lat tice: simple beam-search technique and an ilp 47 alignment link m m phrases from s1 shared phrases phrases from s2 alignment link phrases from s1 shared phrases phrases from s2 figure 1: the general structure of one segment of the alignment lattice, illustrating the potential inter leaving paths between aligned phrases.
</prevsent>
<prevsent>solid lines indicate paths derived from sentence 1 and dashed lines indicate paths derived from sentence 2 strategy that leverages our initial assumption that all aligned phrases must appear in the output.
</prevsent>
</prevsection>
<citsent citstr=" P06-1139 ">
4.4.1 beam searchsearch-based decoding is often employed in phrase based mt systems (och and ney, 2003) <papid> J03-1002 </papid>and is implemented in the moses toolkit5; similar approaches have also been used in text-to-text generation tasks (barzilay and mckeown, 2005; <papid> J05-3002 </papid>soricut and marcu, 2006).<papid> P06-1139 </papid></citsent>
<aftsection>
<nextsent>this technique attempts to find the highest-scoring sentence string under the lm by unwrapping and searching through lattice.
</nextsent>
<nextsent>since the dynamic programming search could require an exponential number of search states, fixed-width beam can be used to control the number of search states being actively considered at each step.
</nextsent>
<nextsent>in order to decode an intersection problem, wefirst pick beam size and initialize the list of candidate search states with the first inter leaving paths in each sentence.
</nextsent>
<nextsent>at every iteration, we consider theb candidates with the highest normalized scores under the lm and remove them from the candidate list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4940">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we now turn to the design of experiments for the strict sentence intersection task and discuss the performance of the proposed models using the corpus provided by mckeown et al (2010).<papid> N10-1044 </papid></prevsent>
<prevsent>we use beam size of 50 for the beam search decoder and 4-gramlm for all experiments.</prevsent>
</prevsection>
<citsent citstr=" N09-2047 ">
dependency parsing is accomplished with mica, tag-based parser (ban galore et al, 2009).<papid> N09-2047 </papid></citsent>
<aftsection>
<nextsent>our primary considerations for studying system-generated fusions are validity (whether the output contains only the information common to each sentence), coverage (whether the output contains all the common information in the input sentences) and the fluency of the output.
</nextsent>
<nextsent>5.1 evaluating validity and fluency.
</nextsent>
<nextsent>evaluating the validity of an intersection involves determining whether it contains only the information contained in each sentence and nothing else.
</nextsent>
<nextsent>in order to do this, we make use of the interpretation of valid intersections as being mutually entailed by the input sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4941">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>it follows that the task of judging the validity of an intersection can simply be decomposed into two tasks that judge whether the intersection is entailed by each input sentence.
</prevsent>
<prevsent>we make use of amazons mechanical turk(amt) platform to have humans evaluate the intersections produced.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
crowdsourcing annotations and judgments in this manner has been shown to be cheap and effective for natural language tasks (snowet al, 2008) <papid> D08-1027 </papid>and has recently been employed in similar entailment-detection tasks (negri and mehdad,2010; <papid> W10-0734 </papid>buzek et al, 2010).<papid> W10-0735 </papid></citsent>
<aftsection>
<nextsent>since we only seek judgments on produced intersections and avoid presenting both input sentences to users, we do not anticipate the noisiness that was noted by mckeown etal.
</nextsent>
<nextsent>(2010) when asking amt users to generate intersections.
</nextsent>
<nextsent>each entailment task is framed as multiple choice question.
</nextsent>
<nextsent>an amt user is shown just one input sentence (the premise in entailment terminology) along with potential intersection (the hypoth esis) and is required to respond to whether there is any new or different information in the latter that is not in the former.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4942">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>it follows that the task of judging the validity of an intersection can simply be decomposed into two tasks that judge whether the intersection is entailed by each input sentence.
</prevsent>
<prevsent>we make use of amazons mechanical turk(amt) platform to have humans evaluate the intersections produced.
</prevsent>
</prevsection>
<citsent citstr=" W10-0734 ">
crowdsourcing annotations and judgments in this manner has been shown to be cheap and effective for natural language tasks (snowet al, 2008) <papid> D08-1027 </papid>and has recently been employed in similar entailment-detection tasks (negri and mehdad,2010; <papid> W10-0734 </papid>buzek et al, 2010).<papid> W10-0735 </papid></citsent>
<aftsection>
<nextsent>since we only seek judgments on produced intersections and avoid presenting both input sentences to users, we do not anticipate the noisiness that was noted by mckeown etal.
</nextsent>
<nextsent>(2010) when asking amt users to generate intersections.
</nextsent>
<nextsent>each entailment task is framed as multiple choice question.
</nextsent>
<nextsent>an amt user is shown just one input sentence (the premise in entailment terminology) along with potential intersection (the hypoth esis) and is required to respond to whether there is any new or different information in the latter that is not in the former.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4943">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>it follows that the task of judging the validity of an intersection can simply be decomposed into two tasks that judge whether the intersection is entailed by each input sentence.
</prevsent>
<prevsent>we make use of amazons mechanical turk(amt) platform to have humans evaluate the intersections produced.
</prevsent>
</prevsection>
<citsent citstr=" W10-0735 ">
crowdsourcing annotations and judgments in this manner has been shown to be cheap and effective for natural language tasks (snowet al, 2008) <papid> D08-1027 </papid>and has recently been employed in similar entailment-detection tasks (negri and mehdad,2010; <papid> W10-0734 </papid>buzek et al, 2010).<papid> W10-0735 </papid></citsent>
<aftsection>
<nextsent>since we only seek judgments on produced intersections and avoid presenting both input sentences to users, we do not anticipate the noisiness that was noted by mckeown etal.
</nextsent>
<nextsent>(2010) when asking amt users to generate intersections.
</nextsent>
<nextsent>each entailment task is framed as multiple choice question.
</nextsent>
<nextsent>an amt user is shown just one input sentence (the premise in entailment terminology) along with potential intersection (the hypoth esis) and is required to respond to whether there is any new or different information in the latter that is not in the former.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4947">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>using the simple absorption law ?
</prevsent>
<prevsent>(a ? b) = a, we assume that the coverage of intersection systems can be judged by how well they can recover an input sentence from human generated unions.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the resulting outputs are compared to the original input sentences in an mt style evaluation under two commonly-used metrics:bleu (papineni et al, 2002) <papid> P02-1040 </papid>and nist (dodding ton, 2002).</citsent>
<aftsection>
<nextsent>the results of this automated evaluation are shown in table 5.
</nextsent>
<nextsent>the aligned words system here always considers words from the union sentence andcan therefore be seen as baseline system.
</nextsent>
<nextsent>we observe that the segmented decoder produces output that is judged most similar to the input sentences under bleu, which measures n-gram overlap, although results under nist (which gives additional weight to rarer n-grams) are less conclusive.
</nextsent>
<nextsent>the experimental results indicate that the two systems we describe, particularly the segmented decoder, do reasonable job of finding valid intersections with good coverage; however, producing fluent output remains challenge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4948">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we notice that the quality of alignment also factors in to this problem: systems that miss phrases which should be aligned or systems that mistakenly align faraway fragments both cause spans of unaligned text that must be then abstracted over.
</prevsent>
<prevsent>we hypothesize that these issues could be tackled with the use of joint models: system that align sas it decodes could reduce the need for abstraction over long unaligned spans, although care would have to be taken to ensure that coverage is maintained.
</prevsent>
</prevsection>
<citsent citstr=" P06-1101 ">
additionally, richer lexical resources such as wider-coverage ontologies (snow et al, 2006) <papid> P06-1101 </papid>and entailment/paraphrase dictionaries could aid in improving coverage.</citsent>
<aftsection>
<nextsent>finally, previous work in fusion (filippova and strube, 2008<papid> W08-1105 </papid>b; filippova and strube, 2009) <papid> N09-2057 </papid>has noted that models based on syntax outperform techniques that rely solely on lm scores to determine fluency, and strict intersection appears to be well-suited for further exploration in this vein.</nextsent>
<nextsent>we have examined the text-to-text generation task of strict sentence intersection, which restricts semantic variation in the output and necessarily invokes the problems of generalization and abstraction in addition to the usual challenge of producing fluent text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4963">
<title id=" W11-1606.xml">towards strict sentence intersection decoding and evaluation strategies </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>we hypothesize that these issues could be tackled with the use of joint models: system that align sas it decodes could reduce the need for abstraction over long unaligned spans, although care would have to be taken to ensure that coverage is maintained.
</prevsent>
<prevsent>additionally, richer lexical resources such as wider-coverage ontologies (snow et al, 2006) <papid> P06-1101 </papid>and entailment/paraphrase dictionaries could aid in improving coverage.</prevsent>
</prevsection>
<citsent citstr=" N09-2057 ">
finally, previous work in fusion (filippova and strube, 2008<papid> W08-1105 </papid>b; filippova and strube, 2009) <papid> N09-2057 </papid>has noted that models based on syntax outperform techniques that rely solely on lm scores to determine fluency, and strict intersection appears to be well-suited for further exploration in this vein.</citsent>
<aftsection>
<nextsent>we have examined the text-to-text generation task of strict sentence intersection, which restricts semantic variation in the output and necessarily invokes the problems of generalization and abstraction in addition to the usual challenge of producing fluent text.
</nextsent>
<nextsent>we tackle the task as lattice decoding and discuss two decoding strategies for producing valid intersections.
</nextsent>
<nextsent>in addition, we assume that strict intersection tasks are best considered as problems of mutual entailment generation and describe evaluation strategies for this task that make use of both human judgments as well as automated metrics run over related corpus.
</nextsent>
<nextsent>experimental results indicate that these systems are fairly effective at generating valid intersections and that our novel segmented decoder strategy outperforms the traditional beam search approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4964">
<title id=" W11-0402.xml">owldl formalization of the multexteast morphosyntactic specifications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another way to enhance the consistency of linguistic annotations is to make use of cross linguistic meta schemes or annotation standards, such as eagles (leech and wilson, 1996).
</prevsent>
<prevsent>the problem is that these enforce the use of the same categories across multiple languages, and thismay be inappropriate for historically and geographically unrelated languages.
</prevsent>
</prevsection>
<citsent citstr=" W03-2904 ">
for specific linguistic and historical regions, the application of standardization approaches has, however, been performed with great success, e.g., for western (leech and wilson, 1996) and eastern europe (erjavec et al, 2003) <papid> W03-2904 </papid>or the indian subcontinent (baskaran et al, 2008).</citsent>
<aftsection>
<nextsent>11 in this paper, we illustrate differences and commonalities of both approaches by creating an owl/dl terminology repository from the multext-east (mte) specifications (erjavec et al, 2003; <papid> W03-2904 </papid>erjavec, 2010), which define features for the morphosyntactic level of linguistic description, instantiate them for 16 languages and provide morphosyntactic tagsets for these lan guages.</nextsent>
<nextsent>the specifications are part of the mteresources, which also include lexicons and an annotated parallel corpus that use these morphosyntactic tagsets.the encoding of the mte specifications follows the text encoding initiative guidelines, teip5 (tei consortium, 2007), and this paper concentrates on developing semi-automatic procedure for converting them from tei xml toowl.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4966">
<title id=" W11-0402.xml">owldl formalization of the multexteast morphosyntactic specifications </title>
<section> the multext-east (mte).  </section>
<citcontext>
<prevsection>
<prevsent>while tei is more appropriate for authoring the specifications and displaying them in book-oriented format, the owl encoding has the advantages of enabling formally specifying interrelationships between the various features (con cepts, or classes) and making logical inferences based on the relationships between them, useful in mediating between different tagsets and tools (chiarcos, 2008).
</prevsent>
<prevsent>morphosyntactic specifications the mte morphosyntactic specifications define attributes and values used for word-level syntactic annotation, i.e., they provide formal grammar for the morphosyntactic properties of the languages covered.
</prevsent>
</prevsection>
<citsent citstr=" C94-1097 ">
the specifications also contain commentary, bibliography, notes, etc. following the original multext proposal (ide and veronis, 1994), <papid> C94-1097 </papid>the specifications define 14 categories (parts of speech), and for each its attributes, their values, and the languages that everyattribute-value pair is appropriate for.</citsent>
<aftsection>
<nextsent>the morphosyntactic specifications also define the mapping between the feature structures and morphosyntactic descriptions (msds).
</nextsent>
<nextsent>msds are compact strings used as tags for corpus annotation and in the morphosyntactic lexicons.
</nextsent>
<nextsent>for example, the msd ncmsn is equivalent to the feature structure consisting of the attribute-value pairs noun, type=common, gender=masculine, number=singular, case=nominative.the specifications currently cover 16 languages, in particular: bulgarian, croatian,czech, english, estonian, hungarian, macedonian, persian, polish, resian, romanian, russian, serbian, slovak, slovene, and ukrainian.for number of these languages the specifications have become de-facto standard and, for some, the mte lexicons and corpora are still the only publicly available datasets for this level of linguistic description.1 table 1 lists the defined categories and gives the number of distinct attributes, attribute-value pairs and the number of mte languages which distinguish the category.
</nextsent>
<nextsent>the feature-set is quite large, as many of the languages covered have very rich inflection, are typo logically different(inflectional, agglutinating), but also have independent traditions of linguistic description; this also leads to similar phenomena sometimes being expressed by different means (see sect.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4967">
<title id=" W11-0402.xml">owldl formalization of the multexteast morphosyntactic specifications </title>
<section> linking annotation schemes with.  </section>
<citcontext>
<prevsection>
<prevsent>5.
</prevsent>
<prevsent>13 3.2 annotation mapping.
</prevsent>
</prevsection>
<citsent citstr=" L08-1429 ">
the classic approach to link annotations with reference concepts is to specify rules that define direct mapping (zeman, 2008).<papid> L08-1429 </papid></citsent>
<aftsection>
<nextsent>it is, however, not always possible to find 1:1 mapping.one problem is conceptual overlap: common noun may occur as part of proper name, e.g., german palais baroque-style palace?
</nextsent>
<nextsent>in neues palais lit.
</nextsent>
<nextsent>new palace?, prussian royal palace in potsdam/germany.
</nextsent>
<nextsent>palais is thus both proper noun (in its function), and common noun(in its form).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4968">
<title id=" W11-0402.xml">owldl formalization of the multexteast morphosyntactic specifications </title>
<section> for every sub concept of morphosyntac-.  </section>
<citcontext>
<prevsection>
<prevsent>5 summary and discussion.
</prevsent>
<prevsent>we have described the semi-automatic creation of an onto logical model of the mte morphosyntactic specifications for 16 different languages.
</prevsent>
</prevsection>
<citsent citstr=" L08-1412 ">
such model may be fruitfully applied in various ways, e.g., within an nlp pipeline that uses onto logical specifications of annotations rather than their string representations (buykoet al, 2008; <papid> L08-1412 </papid>hellmann, 2010).</citsent>
<aftsection>
<nextsent>the ontolog ical modeling may serve also as first step towards an ontology-based documentation of the annotations within corpus query system (rehm et al, 2007; chiarcos et al, 2008), 17 or even the onto logical modeling of entire corpora (burchardt et al, 2008; <papid> I08-1051 </papid>hellmann et al., 2010) and lexicons (martin et al, 2009).as an interesting side-effect of the owl conversion of the entire body of mte resources, they could be easily integrated with existing lexical-semantic resources as linked data, e.g., owl/rdf versions of wordnet (gangemi etal., 2003), which are currently being assembled by various initiatives, e.g., in the context of the lod2 project (http://lod2.eu) and by the open linguistics working group at the open knowledge foundation (http://linguistics.okfn.org).another very important element is that the onto logical modeling of the mte annotations allows it to be interpreted in terms of existing repositories of annotation terminology such asisocat and gold.</nextsent>
<nextsent>a bridge between these terminology repositories and the mte ontology may be developed, for example, by integrating the ontology in an architecture of modular ontologies such as the ontologies of linguistic annotations (chiarcos, 2008, olia), where the linking between annotations and terminology repositories is mediated by so-called reference model?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4969">
<title id=" W11-0402.xml">owldl formalization of the multexteast morphosyntactic specifications </title>
<section> for every sub concept of morphosyntac-.  </section>
<citcontext>
<prevsection>
<prevsent>we have described the semi-automatic creation of an onto logical model of the mte morphosyntactic specifications for 16 different languages.
</prevsent>
<prevsent>such model may be fruitfully applied in various ways, e.g., within an nlp pipeline that uses onto logical specifications of annotations rather than their string representations (buykoet al, 2008; <papid> L08-1412 </papid>hellmann, 2010).</prevsent>
</prevsection>
<citsent citstr=" I08-1051 ">
the ontolog ical modeling may serve also as first step towards an ontology-based documentation of the annotations within corpus query system (rehm et al, 2007; chiarcos et al, 2008), 17 or even the onto logical modeling of entire corpora (burchardt et al, 2008; <papid> I08-1051 </papid>hellmann et al., 2010) and lexicons (martin et al, 2009).as an interesting side-effect of the owl conversion of the entire body of mte resources, they could be easily integrated with existing lexical-semantic resources as linked data, e.g., owl/rdf versions of wordnet (gangemi etal., 2003), which are currently being assembled by various initiatives, e.g., in the context of the lod2 project (http://lod2.eu) and by the open linguistics working group at the open knowledge foundation (http://linguistics.okfn.org).another very important element is that the onto logical modeling of the mte annotations allows it to be interpreted in terms of existing repositories of annotation terminology such asisocat and gold.</citsent>
<aftsection>
<nextsent>a bridge between these terminology repositories and the mte ontology may be developed, for example, by integrating the ontology in an architecture of modular ontologies such as the ontologies of linguistic annotations (chiarcos, 2008, olia), where the linking between annotations and terminology repositories is mediated by so-called reference model?
</nextsent>
<nextsent>that serves as an interface between different levels of representation.
</nextsent>
<nextsent>the mte ontology will be integrated in this model as an annotation model, i.e., its concepts will be defined as sub concepts of concepts of the olia reference model and thereby inherit the linking with gold (chiarcos et al, 2008) and isocat (chiarcos, 2010).
</nextsent>
<nextsent>the linking with these standard repositories increases the comparability of mte annotations and it serves an important documentation function.more important than merely potential applications of the mte ontology, however, is that its creation provides us with new, global perspective on the mte specifications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4970">
<title id=" W10-4007.xml">cooccurrence graph based iterative bilingual lexicon extraction from comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluation results on100 nouns shows that this method outperforms the standard context-vector based approaches.
</prevsent>
<prevsent>bilingual dictionaries play pivotal role in number of natural language processing tasks like machine translation and cross lingual information retrieval(clir).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
machine translation systems often use bilingual dictionaries in order to augment word and phrase alignment (och andney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>clir systems use bilingual dictionaries in the query translation step (grefenstette,1998).
</nextsent>
<nextsent>however, high coverage electronic bilingual dictionaries are not available for all languagepairs.
</nextsent>
<nextsent>so major research area in machine translation and clir is bilingual dictionary extraction.the most common approach for extracting bilingual dictionary is applying some statistical alignment algorithm on parallel corpus.
</nextsent>
<nextsent>however, parallel corpora are not readily available for most language pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4971">
<title id=" W10-4007.xml">cooccurrence graph based iterative bilingual lexicon extraction from comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but after careful study of news articles in english and hindi published on same days at the same city, we have observed that along with articles on similar topics, the corpora also contain lot of articles which have no topical similarity.
</prevsent>
<prevsent>thus, the corpora are quite noisy, which makes it unsuitable for lexiconextraction.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
thus another important factor incomparable corpora construction is the degree of similarity of the corpora.approaches for lexicon extraction from comparable corpora have been proposed that use the bag of-words model to find words that occur in similar lexical contexts (rapp, 1995).<papid> P95-1050 </papid></citsent>
<aftsection>
<nextsent>there have been approaches proposed which improve upon this model by using some linguistic information (yuu and tsujii, 2009).
</nextsent>
<nextsent>however, these require some linguistic tool like dependency parsers which arenot commonly obtainable for resource-poor languages.
</nextsent>
<nextsent>for example, in case of indian languages like hindi and bengali, we still do not have good enough dependency parsers.
</nextsent>
<nextsent>in this paper, we propose word co-occurrence based approach for lexicon extraction from comparable corpora using english and hindi as the source and target languages respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4973">
<title id=" W10-4007.xml">cooccurrence graph based iterative bilingual lexicon extraction from comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, in case of indian languages like hindi and bengali, we still do not have good enough dependency parsers.
</prevsent>
<prevsent>in this paper, we propose word co-occurrence based approach for lexicon extraction from comparable corpora using english and hindi as the source and target languages respectively.
</prevsent>
</prevsection>
<citsent citstr=" W95-0114 ">
we do not use any language 35 specific resource in our approach.we did experiments with 100 words in english,and show that our approach performs significantly better than the the context heterogeneity approach (fung, 1995).<papid> W95-0114 </papid></citsent>
<aftsection>
<nextsent>we show the results over corpora with varying degrees of comparability.
</nextsent>
<nextsent>the outline of the paper is as follows.
</nextsent>
<nextsent>in section 2, we analyze the different approaches for lexicon extraction from comparable corpora.
</nextsent>
<nextsent>in section 3,we present our algorithm and the experimental results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4976">
<title id=" W10-4007.xml">cooccurrence graph based iterative bilingual lexicon extraction from comparable corpora </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the context vector depended on how many distinct words occur in the particular context and also the unigram frequency of the word to be translated.
</prevsent>
<prevsent>euclidean distance between context vectors was used as similarity measure.
</prevsent>
</prevsection>
<citsent citstr=" N03-1015 ">
another approach used distributed clustering of translational equivalents for word sense acquisition from bilingual comparable corpora (kaji, 2003).<papid> N03-1015 </papid></citsent>
<aftsection>
<nextsent>however, the major drawback of this paperis the assumption that translation equivalents usually represent only one sense of the target word.
</nextsent>
<nextsent>this may not be the case for languages having similar origin, for example, hindi and bengali.approaches using context information for extracting lexical translations from comparable corpora have also been proposed (fung and yee, 1998;<papid> P98-1069 </papid>rapp, 1999).<papid> P99-1067 </papid></nextsent>
<nextsent>but they resulted in very poor cov erage.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4977">
<title id=" W10-4007.xml">cooccurrence graph based iterative bilingual lexicon extraction from comparable corpora </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>another approach used distributed clustering of translational equivalents for word sense acquisition from bilingual comparable corpora (kaji, 2003).<papid> N03-1015 </papid></prevsent>
<prevsent>however, the major drawback of this paperis the assumption that translation equivalents usually represent only one sense of the target word.</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
this may not be the case for languages having similar origin, for example, hindi and bengali.approaches using context information for extracting lexical translations from comparable corpora have also been proposed (fung and yee, 1998;<papid> P98-1069 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>but they resulted in very poor coverage.
</nextsent>
<nextsent>these approaches were improved upon by extracting phrasal alignments from comparable corpora using joint probability smt model (ku mano et al, 2007) .another proposed method uses dependency parsing and dependency heterogeneity for extracting bilingual lexicon (yuu and tsujii, 2009) . this approach was similar to that of fung, except they used dependency parser to get the tags for each word and depending on the frequency of each tag they defined vector to represent each word in question.
</nextsent>
<nextsent>here too, euclidean similarity was usedto compute the similarity between two words using their context vectors.
</nextsent>
<nextsent>however, this method is dependent on availability of dependency parser for the languages and is not feasible for languages for which resources are scarce.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4978">
<title id=" W10-4007.xml">cooccurrence graph based iterative bilingual lexicon extraction from comparable corpora </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>another approach used distributed clustering of translational equivalents for word sense acquisition from bilingual comparable corpora (kaji, 2003).<papid> N03-1015 </papid></prevsent>
<prevsent>however, the major drawback of this paperis the assumption that translation equivalents usually represent only one sense of the target word.</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
this may not be the case for languages having similar origin, for example, hindi and bengali.approaches using context information for extracting lexical translations from comparable corpora have also been proposed (fung and yee, 1998;<papid> P98-1069 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>but they resulted in very poor coverage.
</nextsent>
<nextsent>these approaches were improved upon by extracting phrasal alignments from comparable corpora using joint probability smt model (ku mano et al, 2007) .another proposed method uses dependency parsing and dependency heterogeneity for extracting bilingual lexicon (yuu and tsujii, 2009) . this approach was similar to that of fung, except they used dependency parser to get the tags for each word and depending on the frequency of each tag they defined vector to represent each word in question.
</nextsent>
<nextsent>here too, euclidean similarity was usedto compute the similarity between two words using their context vectors.
</nextsent>
<nextsent>however, this method is dependent on availability of dependency parser for the languages and is not feasible for languages for which resources are scarce.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4979">
<title id=" W11-1214.xml">unsupervised alignment of comparable data and text resources </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>we finish with discussion of our results and some conclusions (section 7).
</prevsent>
<prevsent>work in data-to-text generation has involved variety of different domains, including generating weather forecasts from meteorological data (sripada et al , 2003), nursing reports from intensive care data(portet et al , 2009), and museum exhibit descriptions from database records (isard et al , 2003; stock et al , 2007); types of data have included dynamic time-series data (such as meteorological or medicaldata) and static database entries (as in museum ex hibits).
</prevsent>
</prevsection>
<citsent citstr=" W05-1617 ">
the following is an example of an input/output pair from the m-piro project (androutsopoulos et al ., 2005), <papid> W05-1617 </papid>where the input is database record for museum artifact, and the output is description of the artifact: creation-period=archaic-period, current-location=un-museum-pennsylvania, painting-techinique-used=red-figure-technique, painted-by=eucharides, creation-time=between (500 year bc)(480 year bc) classical kylix this exhibit is kylix; it was created during the archaic period and was painted with the red figure technique by eucharides.</citsent>
<aftsection>
<nextsent>it dates from between 500 and 480 b.c. and currently it is in the university museum of pennsylvania.
</nextsent>
<nextsent>while data and texts in the three example domains cited above do occur naturally, two factors mean they cannot be used directly as target corpora or training data for building data-to-text generationsystems: one, most are not freely available to researchers (e.g. by simply being available on theweb), and two, more problematic ally, the correspondence between inputs and outputs is not as direct as it is, say, between source language text and its translation.
</nextsent>
<nextsent>in general, naturally occurring resources of data and related texts are not parallel, but are merely what has become known as comparable in the mt literature, with only subset of data having corresponding text fragments, and other text fragments having no obvious corresponding data items.
</nextsent>
<nextsent>moreover, data transformations may be necessary before corresponding text fragments can be identi fied.in this paper we look at the possibility of automatically identifying parallel data-text fragments from comparable corpora in the case of data-to-text generation from static database records.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4980">
<title id=" W11-1214.xml">unsupervised alignment of comparable data and text resources </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>in general, naturally occurring resources of data and related texts are not parallel, but are merely what has become known as comparable in the mt literature, with only subset of data having corresponding text fragments, and other text fragments having no obvious corresponding data items.
</prevsent>
<prevsent>moreover, data transformations may be necessary before corresponding text fragments can be identi fied.in this paper we look at the possibility of automatically identifying parallel data-text fragments from comparable corpora in the case of data-to-text generation from static database records.
</prevsent>
</prevsection>
<citsent citstr=" W09-0603 ">
such parallel data-text resource could then be used to train an existing data-to-text generation system, or even to build new statistical generator from scratch, e.g. using techniques from statistical mt (belz and kow, 2009).<papid> W09-0603 </papid>in statistical mt, the expense of manually creating new parallel mt corpora, and the need for very large amounts of parallel training data, has led to asizeable research effort to develop methods for automatically constructing parallel resources.</citsent>
<aftsection>
<nextsent>this work typically starts by identifying comparable corpora.much of it has focused on identifying word translations incomparable corpora, e.g. rapps approach was based on the simple and elegant assumption thatif words af and bf have higher than chance cooccurrence frequency in one language, then two appropriate translations ae and be in another language will also have higher than chance co-occurrence frequency (rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></nextsent>
<nextsent>at the other end of the spectrum, resnik and smith (2003) <papid> J03-3002 </papid>search the web to detect web pages that are translations of each other.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4981">
<title id=" W11-1214.xml">unsupervised alignment of comparable data and text resources </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, data transformations may be necessary before corresponding text fragments can be identi fied.in this paper we look at the possibility of automatically identifying parallel data-text fragments from comparable corpora in the case of data-to-text generation from static database records.
</prevsent>
<prevsent>such parallel data-text resource could then be used to train an existing data-to-text generation system, or even to build new statistical generator from scratch, e.g. using techniques from statistical mt (belz and kow, 2009).<papid> W09-0603 </papid>in statistical mt, the expense of manually creating new parallel mt corpora, and the need for very large amounts of parallel training data, has led to asizeable research effort to develop methods for automatically constructing parallel resources.</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
this work typically starts by identifying comparable corpora.much of it has focused on identifying word translations incomparable corpora, e.g. rapps approach was based on the simple and elegant assumption thatif words af and bf have higher than chance cooccurrence frequency in one language, then two appropriate translations ae and be in another language will also have higher than chance co-occurrence frequency (rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>at the other end of the spectrum, resnik and smith (2003) <papid> J03-3002 </papid>search the web to detect web pages that are translations of each other.</nextsent>
<nextsent>other approaches aim to identify pairsof sentences (munteanu and marcu, 2005) <papid> J05-4003 </papid>or sub sentential fragments (munteanu and marcu, 2006) <papid> P06-1011 </papid>that are parallel within comparable corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4982">
<title id=" W11-1214.xml">unsupervised alignment of comparable data and text resources </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, data transformations may be necessary before corresponding text fragments can be identi fied.in this paper we look at the possibility of automatically identifying parallel data-text fragments from comparable corpora in the case of data-to-text generation from static database records.
</prevsent>
<prevsent>such parallel data-text resource could then be used to train an existing data-to-text generation system, or even to build new statistical generator from scratch, e.g. using techniques from statistical mt (belz and kow, 2009).<papid> W09-0603 </papid>in statistical mt, the expense of manually creating new parallel mt corpora, and the need for very large amounts of parallel training data, has led to asizeable research effort to develop methods for automatically constructing parallel resources.</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
this work typically starts by identifying comparable corpora.much of it has focused on identifying word translations incomparable corpora, e.g. rapps approach was based on the simple and elegant assumption thatif words af and bf have higher than chance cooccurrence frequency in one language, then two appropriate translations ae and be in another language will also have higher than chance co-occurrence frequency (rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>at the other end of the spectrum, resnik and smith (2003) <papid> J03-3002 </papid>search the web to detect web pages that are translations of each other.</nextsent>
<nextsent>other approaches aim to identify pairsof sentences (munteanu and marcu, 2005) <papid> J05-4003 </papid>or sub sentential fragments (munteanu and marcu, 2006) <papid> P06-1011 </papid>that are parallel within comparable corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4983">
<title id=" W11-1214.xml">unsupervised alignment of comparable data and text resources </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>such parallel data-text resource could then be used to train an existing data-to-text generation system, or even to build new statistical generator from scratch, e.g. using techniques from statistical mt (belz and kow, 2009).<papid> W09-0603 </papid>in statistical mt, the expense of manually creating new parallel mt corpora, and the need for very large amounts of parallel training data, has led to asizeable research effort to develop methods for automatically constructing parallel resources.</prevsent>
<prevsent>this work typically starts by identifying comparable corpora.much of it has focused on identifying word translations incomparable corpora, e.g. rapps approach was based on the simple and elegant assumption thatif words af and bf have higher than chance cooccurrence frequency in one language, then two appropriate translations ae and be in another language will also have higher than chance co-occurrence frequency (rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
at the other end of the spectrum, resnik and smith (2003) <papid> J03-3002 </papid>search the web to detect web pages that are translations of each other.</citsent>
<aftsection>
<nextsent>other approaches aim to identify pairsof sentences (munteanu and marcu, 2005) <papid> J05-4003 </papid>or sub sentential fragments (munteanu and marcu, 2006) <papid> P06-1011 </papid>that are parallel within comparable corpora.</nextsent>
<nextsent>the latter approach is particularly relevant to our work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4984">
<title id=" W11-1214.xml">unsupervised alignment of comparable data and text resources </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>this work typically starts by identifying comparable corpora.much of it has focused on identifying word translations incomparable corpora, e.g. rapps approach was based on the simple and elegant assumption thatif words af and bf have higher than chance cooccurrence frequency in one language, then two appropriate translations ae and be in another language will also have higher than chance co-occurrence frequency (rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></prevsent>
<prevsent>at the other end of the spectrum, resnik and smith (2003) <papid> J03-3002 </papid>search the web to detect web pages that are translations of each other.</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
other approaches aim to identify pairsof sentences (munteanu and marcu, 2005) <papid> J05-4003 </papid>or sub sentential fragments (munteanu and marcu, 2006) <papid> P06-1011 </papid>that are parallel within comparable corpora.</citsent>
<aftsection>
<nextsent>the latter approach is particularly relevant to our work.
</nextsent>
<nextsent>munteanu and marcu start by translating each document in the source language (sl) word for word into the target language (tl).
</nextsent>
<nextsent>the result is given to an information retrieval (ir) system as query, and the top 20 results are retained and paired with the given sl document.
</nextsent>
<nextsent>they then obtain all sentence pairs from each pair of sl and tl documents, and discard those sentence pairs that have only small 103 number of words that are translations of each other.to the remaining sentences they then apply fragment detection method which tries to distinguish between source fragments that have translation on the target side, and fragments that do not.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4985">
<title id=" W11-1214.xml">unsupervised alignment of comparable data and text resources </title>
<section> background and related research.  </section>
<citcontext>
<prevsection>
<prevsent>this work typically starts by identifying comparable corpora.much of it has focused on identifying word translations incomparable corpora, e.g. rapps approach was based on the simple and elegant assumption thatif words af and bf have higher than chance cooccurrence frequency in one language, then two appropriate translations ae and be in another language will also have higher than chance co-occurrence frequency (rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></prevsent>
<prevsent>at the other end of the spectrum, resnik and smith (2003) <papid> J03-3002 </papid>search the web to detect web pages that are translations of each other.</prevsent>
</prevsection>
<citsent citstr=" P06-1011 ">
other approaches aim to identify pairsof sentences (munteanu and marcu, 2005) <papid> J05-4003 </papid>or sub sentential fragments (munteanu and marcu, 2006) <papid> P06-1011 </papid>that are parallel within comparable corpora.</citsent>
<aftsection>
<nextsent>the latter approach is particularly relevant to our work.
</nextsent>
<nextsent>munteanu and marcu start by translating each document in the source language (sl) word for word into the target language (tl).
</nextsent>
<nextsent>the result is given to an information retrieval (ir) system as query, and the top 20 results are retained and paired with the given sl document.
</nextsent>
<nextsent>they then obtain all sentence pairs from each pair of sl and tl documents, and discard those sentence pairs that have only small 103 number of words that are translations of each other.to the remaining sentences they then apply fragment detection method which tries to distinguish between source fragments that have translation on the target side, and fragments that do not.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4986">
<title id=" W11-1214.xml">unsupervised alignment of comparable data and text resources </title>
<section> extract parallel fragments (future work)..  </section>
<citcontext>
<prevsection>
<prevsent>using the example in figure 2, the aim would be to select the first two sentences only.our sentence selection method consists of (i) estimating the strength of association between data and text tokens (section 5.1); and (ii) selecting those sentences for further consideration that have sufficiently strong and/or numerous associations with data tokens (section 5.2).
</prevsent>
<prevsent>5.1 computing positive and negative.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
associations between data and text we measure the strength of association between data tokens and text tokens using log-likelihood ratios which have been widely used for this sort of purpose (especially lexical association) since they were introduced to nlp (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>they were e.g.used by munteanu &amp; marcu (2006) <papid> P06-1011 </papid>to obtain translation lexicon from word-aligned parallel texts.</nextsent>
<nextsent>we start by obtaining counts for the number of times each text token co-occurs with each data token d, the number of times occurs without being present, the number of times occurs without w,and finally, the number of times neither occurs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4988">
<title id=" W11-1214.xml">unsupervised alignment of comparable data and text resources </title>
<section> extract parallel fragments (future work)..  </section>
<citcontext>
<prevsection>
<prevsent>we start by obtaining counts for the number of times each text token co-occurs with each data token d, the number of times occurs without being present, the number of times occurs without w,and finally, the number of times neither occurs.
</prevsent>
<prevsent>cooccurrence here is at the document/data record level, i.e. data token and text token co-occur if they are present in the same document/data record pair (pairs as produced by the method described in section 3).
</prevsent>
</prevsection>
<citsent citstr=" W04-3243 ">
this allows us to compute log likelihood ratios for all data-token/text-token pairs, using one of the g2 formulations from moore (2004) <papid> W04-3243 </papid>which is shown in slightly different representation in figure 3.</citsent>
<aftsection>
<nextsent>there sulting g2 scores tell us whether the frequency with which data token and text token co-occur deviates from that expected by chance.
</nextsent>
<nextsent>if the g2 score forgiven (d,w) pair is greater than their joint probability p(d)p(w), then the asso 105 wikipedia text: black chew head is the highest point ( or county top ) of greater manchester , and forms part of the peak district , in northern england . lying within the saddleworth parish of the metropolitan borough of oldham , close to crowden , derbyshire , it stands at height of 542 metres above sea level . black chew head is an outlying part of the black hill and overlooks the chew valley , which leads to the dove stones reservoir . entry from database of british hills: name area height height ft feature classification top locality adm area1 adm area2 country black peak 542 1778 fence dewey greater glossop england derbyshire ukchew head district manchester parallel fragments: name area top adm area1 adm area2 black chew head peak district greater manchester england derbyshire black chew head is the highest point ( or county top ) of greater manchester , and forms part of the peak district , in northern england . height (m) 542 it stands at height of 542 metres above sea level . figure 2: black chew head: wikipedia article, entry in british hills database (the part of it we use), and parallel fragments that could be extracted.
</nextsent>
<nextsent>ciation is taken to be positive, i.e. is likely to be part of realisation of d, otherwise the association is taken to be negative, i.e. is likely not to be part of realisation of d. note that we use the notation g2+ below to denote g2 score which reflects positive association.
</nextsent>
<nextsent>5.2 selecting sentences on the basis of.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4991">
<title id=" W10-4110.xml">mining largescale parallel corpora from multilingual patents an english chinese example and its application to smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, we estimate the potential for mining multilingual parallel corpora involving english, chinese, japanese, korean, german, etc., which would to some extent reduce the parallel data acquisition bottleneck in multilingual information processing.
</prevsent>
<prevsent>multilingual data are critical resources for building many applications, such as machine translation (mt) and cross-lingual information retrieval.
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
many parallel corpora have been built, such as the canadian hansa rds (gale and church, 1991), <papid> P91-1023 </papid>the europarl corpus (koehn, 2005), the arabic-english and english-chinese parallel corpora used in the nist open mt evaluation.</citsent>
<aftsection>
<nextsent>however, few parallel corpora exist for many language pairs, such as chinese-japanese, japanese-korean, chinese- french or japanese-german.
</nextsent>
<nextsent>even for language pairs with several parallel corpora, such as chinese-english and arabic-english, the size of parallel corpora is still major limitation for smt systems to achieve higher performance.
</nextsent>
<nextsent>in this paper, we present way which could, to some extent, reduce the parallel data acquisition bottleneck in multilingual language processing.
</nextsent>
<nextsent>based on multilingual patents, we show how an enlarged english-chinese parallel corpus containing over 14 million high-quality sentence pairs can be mined from large number of comparable patents harvested from the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4992">
<title id=" W10-4110.xml">mining largescale parallel corpora from multilingual patents an english chinese example and its application to smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we discuss the results in section 6, and conclude in section 7.
</prevsent>
<prevsent>parallel sentences could be extracted from parallel documents or comparable corpora.
</prevsent>
</prevsection>
<citsent citstr=" P91-1022 ">
different approaches have been proposed to align sentences in parallel documents consisting of the same content in different languages based on the following information: a) the sentence length in bilingual sentences (brown et al 1991; <papid> P91-1022 </papid>gale and church, 1991); <papid> P91-1023 </papid>b) lexical information in bilingual dictionaries (ma, 2006); c) statistical translation model (chen, 1993), <papid> P93-1002 </papid>or the composite of more than one approach (simard and plamondon, 1998; moore, 2002).</citsent>
<aftsection>
<nextsent>to overcome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, which raises further challenges since the bilingual contents are not strictly parallel.
</nextsent>
<nextsent>for instance, zhao and vogel (2002) investigated the mining of parallel sentences for web bilingual news.
</nextsent>
<nextsent>munteanu and marcu (2005) <papid> J05-4003 </papid>presented method for discovering parallel sentences in large chinese, arabic, and english comparable, non-parallel corpora based on maximum entropy classifier.</nextsent>
<nextsent>cao et al, (2007) and lin et al, (2008) <papid> P08-1113 </papid>proposed two different methods utilizing the parenthesis pattern to extract term translations from bilingual web pages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4994">
<title id=" W10-4110.xml">mining largescale parallel corpora from multilingual patents an english chinese example and its application to smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we discuss the results in section 6, and conclude in section 7.
</prevsent>
<prevsent>parallel sentences could be extracted from parallel documents or comparable corpora.
</prevsent>
</prevsection>
<citsent citstr=" P93-1002 ">
different approaches have been proposed to align sentences in parallel documents consisting of the same content in different languages based on the following information: a) the sentence length in bilingual sentences (brown et al 1991; <papid> P91-1022 </papid>gale and church, 1991); <papid> P91-1023 </papid>b) lexical information in bilingual dictionaries (ma, 2006); c) statistical translation model (chen, 1993), <papid> P93-1002 </papid>or the composite of more than one approach (simard and plamondon, 1998; moore, 2002).</citsent>
<aftsection>
<nextsent>to overcome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, which raises further challenges since the bilingual contents are not strictly parallel.
</nextsent>
<nextsent>for instance, zhao and vogel (2002) investigated the mining of parallel sentences for web bilingual news.
</nextsent>
<nextsent>munteanu and marcu (2005) <papid> J05-4003 </papid>presented method for discovering parallel sentences in large chinese, arabic, and english comparable, non-parallel corpora based on maximum entropy classifier.</nextsent>
<nextsent>cao et al, (2007) and lin et al, (2008) <papid> P08-1113 </papid>proposed two different methods utilizing the parenthesis pattern to extract term translations from bilingual web pages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4995">
<title id=" W10-4110.xml">mining largescale parallel corpora from multilingual patents an english chinese example and its application to smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to overcome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, which raises further challenges since the bilingual contents are not strictly parallel.
</prevsent>
<prevsent>for instance, zhao and vogel (2002) investigated the mining of parallel sentences for web bilingual news.
</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
munteanu and marcu (2005) <papid> J05-4003 </papid>presented method for discovering parallel sentences in large chinese, arabic, and english comparable, non-parallel corpora based on maximum entropy classifier.</citsent>
<aftsection>
<nextsent>cao et al, (2007) and lin et al, (2008) <papid> P08-1113 </papid>proposed two different methods utilizing the parenthesis pattern to extract term translations from bilingual web pages.</nextsent>
<nextsent>jiang et al (2009) <papid> P09-1098 </papid>presented an adaptive pattern-based method which produced chinese-english bilingual sentences and terms with over 80% accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4996">
<title id=" W10-4110.xml">mining largescale parallel corpora from multilingual patents an english chinese example and its application to smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, zhao and vogel (2002) investigated the mining of parallel sentences for web bilingual news.
</prevsent>
<prevsent>munteanu and marcu (2005) <papid> J05-4003 </papid>presented method for discovering parallel sentences in large chinese, arabic, and english comparable, non-parallel corpora based on maximum entropy classifier.</prevsent>
</prevsection>
<citsent citstr=" P08-1113 ">
cao et al, (2007) and lin et al, (2008) <papid> P08-1113 </papid>proposed two different methods utilizing the parenthesis pattern to extract term translations from bilingual web pages.</citsent>
<aftsection>
<nextsent>jiang et al (2009) <papid> P09-1098 </papid>presented an adaptive pattern-based method which produced chinese-english bilingual sentences and terms with over 80% accuracy.</nextsent>
<nextsent>only few papers were found on the related work in the patent domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4997">
<title id=" W10-4110.xml">mining largescale parallel corpora from multilingual patents an english chinese example and its application to smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>munteanu and marcu (2005) <papid> J05-4003 </papid>presented method for discovering parallel sentences in large chinese, arabic, and english comparable, non-parallel corpora based on maximum entropy classifier.</prevsent>
<prevsent>cao et al, (2007) and lin et al, (2008) <papid> P08-1113 </papid>proposed two different methods utilizing the parenthesis pattern to extract term translations from bilingual web pages.</prevsent>
</prevsection>
<citsent citstr=" P09-1098 ">
jiang et al (2009) <papid> P09-1098 </papid>presented an adaptive pattern-based method which produced chinese-english bilingual sentences and terms with over 80% accuracy.</citsent>
<aftsection>
<nextsent>only few papers were found on the related work in the patent domain.
</nextsent>
<nextsent>higuchi et al (2001) used the titles and abstracts of 32,000 japanese-english bilingual patents to extract bilingual terms.
</nextsent>
<nextsent>utiyama and isahara (2007) mined about 2 million parallel sentences by using two parts in the description section of japanese-english comparable patents.
</nextsent>
<nextsent>lu et al (2009) derived about 160k parallel sentences from chinese-english comparable patents by aligning sentences and filtering alignments with the combination of different quality measures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4998">
<title id=" W10-4110.xml">mining largescale parallel corpora from multilingual patents an english chinese example and its application to smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another closely related work is the english-chinese parallel corpus (lu et al, 2010), which is largely extended by this work, in which both the number of patents and that of parallel sentences are augmented by about 100%, and more smt experiments are given.
</prevsent>
<prevsent>moreover, we show the potential for mining parallel corpora from multilingual patents involving other languages.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
for statistical machine translation (smt), tremendous strides have been made in two decades, including brown et al (1993), <papid> J93-2003 </papid>och and ney (2004) <papid> J04-4002 </papid>and chiang (2007).<papid> J07-2003 </papid></citsent>
<aftsection>
<nextsent>for the mt evaluation, nist (fujii et al, 2008; 2010) has been organizing open evaluations for years, and the performance of the participants has been improved rapidly.
</nextsent>
<nextsent>a patent is legal document representing an official document granting the exclusive right to make, use, and sell an invention for limited period?
</nextsent>
<nextsent>(collins english dictionary1).
</nextsent>
<nextsent>a patent application consists of different sections, and we focus on the text, i.e. only title, abstract, claims and description.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH4999">
<title id=" W10-4110.xml">mining largescale parallel corpora from multilingual patents an english chinese example and its application to smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another closely related work is the english-chinese parallel corpus (lu et al, 2010), which is largely extended by this work, in which both the number of patents and that of parallel sentences are augmented by about 100%, and more smt experiments are given.
</prevsent>
<prevsent>moreover, we show the potential for mining parallel corpora from multilingual patents involving other languages.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
for statistical machine translation (smt), tremendous strides have been made in two decades, including brown et al (1993), <papid> J93-2003 </papid>och and ney (2004) <papid> J04-4002 </papid>and chiang (2007).<papid> J07-2003 </papid></citsent>
<aftsection>
<nextsent>for the mt evaluation, nist (fujii et al, 2008; 2010) has been organizing open evaluations for years, and the performance of the participants has been improved rapidly.
</nextsent>
<nextsent>a patent is legal document representing an official document granting the exclusive right to make, use, and sell an invention for limited period?
</nextsent>
<nextsent>(collins english dictionary1).
</nextsent>
<nextsent>a patent application consists of different sections, and we focus on the text, i.e. only title, abstract, claims and description.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5000">
<title id=" W10-4110.xml">mining largescale parallel corpora from multilingual patents an english chinese example and its application to smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another closely related work is the english-chinese parallel corpus (lu et al, 2010), which is largely extended by this work, in which both the number of patents and that of parallel sentences are augmented by about 100%, and more smt experiments are given.
</prevsent>
<prevsent>moreover, we show the potential for mining parallel corpora from multilingual patents involving other languages.
</prevsent>
</prevsection>
<citsent citstr=" J07-2003 ">
for statistical machine translation (smt), tremendous strides have been made in two decades, including brown et al (1993), <papid> J93-2003 </papid>och and ney (2004) <papid> J04-4002 </papid>and chiang (2007).<papid> J07-2003 </papid></citsent>
<aftsection>
<nextsent>for the mt evaluation, nist (fujii et al, 2008; 2010) has been organizing open evaluations for years, and the performance of the participants has been improved rapidly.
</nextsent>
<nextsent>a patent is legal document representing an official document granting the exclusive right to make, use, and sell an invention for limited period?
</nextsent>
<nextsent>(collins english dictionary1).
</nextsent>
<nextsent>a patent application consists of different sections, and we focus on the text, i.e. only title, abstract, claims and description.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5001">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>if nl queries can be automatically translated into sparql queries, human users can access their desired knowledge without learning the complex query language of sparql.
</prevsent>
<prevsent>this paper presents our preliminary work fornl query processing, with focus on syntactic parsing.
</prevsent>
</prevsection>
<citsent citstr=" W07-2208 ">
we first build small treebank of natural language queries, which are from genomics track (hersh et al, 2004; hersh et al, 2005; hersh et al, 2006; hersh et al, 2007) topics (section 2 and 3).the small treebank is then used to test the performance of state-of-the-art parser, enju (ninomiya et al, 2007; <papid> W07-2208 </papid>hara et al, 2007) (<papid> W07-2202 </papid>section 4).</citsent>
<aftsection>
<nextsent>the results show that parser trained on wall-street journal (wsj) articles and medline abstracts willnot work well on query sentences.
</nextsent>
<nextsent>next, we experiment an adaptive learning technique, to seek the chance to improve the parsing performance on querysentences.
</nextsent>
<nextsent>despite the small scale of the experiments, the results enlighten directions for effective 3http://www.w3.org/tr/rdf-sparql-query/ 4http://www.w3.org/rdf/ 164 gtrec 04 05 06 07 declarative 1 0 0 0 imperative 22 60 0 0 infinitive 1 0 0 0 interrogative - wp/wrb/wdt 3 / 1 / 11 0 / 0 / 0 6 / 22 / 0 0 / 0 / 50 - non-wh 5 0 0 0 np 14 0 0 0 total 58 60 28 50 table 1: distribution of sentence constructions improvement (section 5).
</nextsent>
<nextsent>while it is reported that the state-of-art nlp technology shows reasonable performance for ir orie applications (ohta et al, 2006), nlp technology has long been developed mostly for declarative sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5002">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>if nl queries can be automatically translated into sparql queries, human users can access their desired knowledge without learning the complex query language of sparql.
</prevsent>
<prevsent>this paper presents our preliminary work fornl query processing, with focus on syntactic parsing.
</prevsent>
</prevsection>
<citsent citstr=" W07-2202 ">
we first build small treebank of natural language queries, which are from genomics track (hersh et al, 2004; hersh et al, 2005; hersh et al, 2006; hersh et al, 2007) topics (section 2 and 3).the small treebank is then used to test the performance of state-of-the-art parser, enju (ninomiya et al, 2007; <papid> W07-2208 </papid>hara et al, 2007) (<papid> W07-2202 </papid>section 4).</citsent>
<aftsection>
<nextsent>the results show that parser trained on wall-street journal (wsj) articles and medline abstracts willnot work well on query sentences.
</nextsent>
<nextsent>next, we experiment an adaptive learning technique, to seek the chance to improve the parsing performance on querysentences.
</nextsent>
<nextsent>despite the small scale of the experiments, the results enlighten directions for effective 3http://www.w3.org/tr/rdf-sparql-query/ 4http://www.w3.org/rdf/ 164 gtrec 04 05 06 07 declarative 1 0 0 0 imperative 22 60 0 0 infinitive 1 0 0 0 interrogative - wp/wrb/wdt 3 / 1 / 11 0 / 0 / 0 6 / 22 / 0 0 / 0 / 50 - non-wh 5 0 0 0 np 14 0 0 0 total 58 60 28 50 table 1: distribution of sentence constructions improvement (section 5).
</nextsent>
<nextsent>while it is reported that the state-of-art nlp technology shows reasonable performance for ir orie applications (ohta et al, 2006), nlp technology has long been developed mostly for declarative sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5003">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> syntactic features of query sentences.  </section>
<citcontext>
<prevsection>
<prevsent>(12 sentences), provide information about?(36 sentences) or provide information on?
</prevsent>
<prevsent>(12 sen tences).
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
gtrec06 consists only of wh-questions where wh-word constitutes noun phrase by itself (i.e. its vp np pp np pp np np np np vb nns in nn in nn [ ] find articles abut function of fancd2 figure 1: the tree structure for an imperative sentence part-of-speech is the wp in penn treebank (marcus et al, 1994) <papid> H94-1020 </papid>pos tag set) or is an adverb (wrb).</citsent>
<aftsection>
<nextsent>in the 2006 track, the templates for the 2005 track were reformulated into the constructions of questions andwere then utilized for deriving the questions.
</nextsent>
<nextsent>forex ample, the templates to find articles describing therole of gene involved in given disease is reformulated into the question what is the role of gene in disease??
</nextsent>
<nextsent>gtrec07 consists only of wh-questions where wh-word serves as pre-nominal modifier (wdt).
</nextsent>
<nextsent>in the 2007 track, unlike in those of last two years, questions were not categorized by the templates, but were based on biologists?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5004">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> syntactic features of query sentences.  </section>
<citcontext>
<prevsection>
<prevsent>(4 sentences), or in what + entity type?
</prevsent>
<prevsent>(1 sentence).
</prevsent>
</prevsection>
<citsent citstr=" I05-2038 ">
in contrast, the genia treebank corpus (tateisiet al, 2005)<papid> I05-2038 </papid>5 is estimated to have no imperative sentences and only seven interrogative sentences (see section 5.2.2).</citsent>
<aftsection>
<nextsent>thus, the sentence constructions in gtrec0407 are very different from those in the genia treebank.
</nextsent>
<nextsent>we built treebank (with pos) on 196 query sentences following the guidelines of the genia tree bank (tateisi and tsujii, 2006).
</nextsent>
<nextsent>the queries were first parsed using the stanford parser (klein and manning, 2003), <papid> P03-1054 </papid>and manual correction was made 5http://www-tsujii.is.s.u-tokyo.ac.jp/genia/home/ wiki.cgipage=genia+treebank 165 sbarq sq vp whnp[i168] np[i169i168] np[i169] pp wdt nns vbp vbn in nn what toxicities are [ ] associated [ ] with cytarabine figure 2: the tree structure for an interrogative sentence by the second author.</nextsent>
<nextsent>we tried to follow the guide line of the genia treebank as closely as possible, but for the constructions that are rare in genia, we used the atis corpus in penn treebank (bies et al, 1995), which is also collection of query sentences, for reference.figure 1 shows the tree for an imperative sen tence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5005">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> tree banking gtrec query sentences.  </section>
<citcontext>
<prevsection>
<prevsent>thus, the sentence constructions in gtrec0407 are very different from those in the genia treebank.
</prevsent>
<prevsent>we built treebank (with pos) on 196 query sentences following the guidelines of the genia tree bank (tateisi and tsujii, 2006).
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
the queries were first parsed using the stanford parser (klein and manning, 2003), <papid> P03-1054 </papid>and manual correction was made 5http://www-tsujii.is.s.u-tokyo.ac.jp/genia/home/ wiki.cgipage=genia+treebank 165 sbarq sq vp whnp[i168] np[i169i168] np[i169] pp wdt nns vbp vbn in nn what toxicities are [ ] associated [ ] with cytarabine figure 2: the tree structure for an interrogative sentence by the second author.</citsent>
<aftsection>
<nextsent>we tried to follow the guide line of the genia treebank as closely as possible, but for the constructions that are rare in genia, we used the atis corpus in penn treebank (bies et al, 1995), which is also collection of query sentences, for reference.figure 1 shows the tree for an imperative sentence.
</nextsent>
<nextsent>a leaf node with [ ] corresponds to nullconstituent.
</nextsent>
<nextsent>figure 2 shows the tree for an inter rogative sentence.
</nextsent>
<nextsent>co indexing is represented by assigning an id to node and reference to the id to the node which is coindexed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5009">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>iftwo question sentences were concatenated by conjunctions into one sentence, the parser would tend tofail to analyze the sentence construction for the latter sentence.
</prevsent>
<prevsent>the remaining errors in table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data.
</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
since domain adaptation has been an extensive research area in parsing research (nivre et al, 2007),<papid> D07-1096 </papid>a lot of ideas have been proposed, including un /semi-supervised approaches (roark and bacchiani, 2003; blitzer et al, 2006; <papid> W06-1615 </papid>steedman et al, 2003; <papid> E03-1008 </papid>mcclosky et al, 2006; <papid> P06-1043 </papid>clegg and shepherd, 2005; <papid> W05-1102 </papid>mcclosky et al, 2010) <papid> N10-1004 </papid>and supervised approaches (titov and henderson, 2006; <papid> W06-2902 </papid>hara et al, 2007).<papid> W07-2202 </papid></citsent>
<aftsection>
<nextsent>their main focus was on adapting parsing models trained with specific genre of text (in most casesptb-wsj) to other genres of text, such as biomedical research papers.
</nextsent>
<nextsent>a major problem tackled in such task setting is the handling of unknown words and domain-specific ways of expressions.
</nextsent>
<nextsent>however,as we explored, parsing nl queries involves significantly different problem; even when all words ina sentence are known, the sentence has very different construction from declarative sentences.although sentence constructions have gained little attention, notable exception is (judge et al,2006).<papid> P06-1063 </papid></nextsent>
<nextsent>they pointed out low accuracy of state-ofthe-art parsers on questions, and proposed supervised parser adaptation by manually creating treebank of questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5010">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>iftwo question sentences were concatenated by conjunctions into one sentence, the parser would tend tofail to analyze the sentence construction for the latter sentence.
</prevsent>
<prevsent>the remaining errors in table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data.
</prevsent>
</prevsection>
<citsent citstr=" W06-1615 ">
since domain adaptation has been an extensive research area in parsing research (nivre et al, 2007),<papid> D07-1096 </papid>a lot of ideas have been proposed, including un /semi-supervised approaches (roark and bacchiani, 2003; blitzer et al, 2006; <papid> W06-1615 </papid>steedman et al, 2003; <papid> E03-1008 </papid>mcclosky et al, 2006; <papid> P06-1043 </papid>clegg and shepherd, 2005; <papid> W05-1102 </papid>mcclosky et al, 2010) <papid> N10-1004 </papid>and supervised approaches (titov and henderson, 2006; <papid> W06-2902 </papid>hara et al, 2007).<papid> W07-2202 </papid></citsent>
<aftsection>
<nextsent>their main focus was on adapting parsing models trained with specific genre of text (in most casesptb-wsj) to other genres of text, such as biomedical research papers.
</nextsent>
<nextsent>a major problem tackled in such task setting is the handling of unknown words and domain-specific ways of expressions.
</nextsent>
<nextsent>however,as we explored, parsing nl queries involves significantly different problem; even when all words ina sentence are known, the sentence has very different construction from declarative sentences.although sentence constructions have gained little attention, notable exception is (judge et al,2006).<papid> P06-1063 </papid></nextsent>
<nextsent>they pointed out low accuracy of state-ofthe-art parsers on questions, and proposed supervised parser adaptation by manually creating treebank of questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5011">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>iftwo question sentences were concatenated by conjunctions into one sentence, the parser would tend tofail to analyze the sentence construction for the latter sentence.
</prevsent>
<prevsent>the remaining errors in table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data.
</prevsent>
</prevsection>
<citsent citstr=" E03-1008 ">
since domain adaptation has been an extensive research area in parsing research (nivre et al, 2007),<papid> D07-1096 </papid>a lot of ideas have been proposed, including un /semi-supervised approaches (roark and bacchiani, 2003; blitzer et al, 2006; <papid> W06-1615 </papid>steedman et al, 2003; <papid> E03-1008 </papid>mcclosky et al, 2006; <papid> P06-1043 </papid>clegg and shepherd, 2005; <papid> W05-1102 </papid>mcclosky et al, 2010) <papid> N10-1004 </papid>and supervised approaches (titov and henderson, 2006; <papid> W06-2902 </papid>hara et al, 2007).<papid> W07-2202 </papid></citsent>
<aftsection>
<nextsent>their main focus was on adapting parsing models trained with specific genre of text (in most casesptb-wsj) to other genres of text, such as biomedical research papers.
</nextsent>
<nextsent>a major problem tackled in such task setting is the handling of unknown words and domain-specific ways of expressions.
</nextsent>
<nextsent>however,as we explored, parsing nl queries involves significantly different problem; even when all words ina sentence are known, the sentence has very different construction from declarative sentences.although sentence constructions have gained little attention, notable exception is (judge et al,2006).<papid> P06-1063 </papid></nextsent>
<nextsent>they pointed out low accuracy of state-ofthe-art parsers on questions, and proposed supervised parser adaptation by manually creating treebank of questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5012">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>iftwo question sentences were concatenated by conjunctions into one sentence, the parser would tend tofail to analyze the sentence construction for the latter sentence.
</prevsent>
<prevsent>the remaining errors in table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data.
</prevsent>
</prevsection>
<citsent citstr=" P06-1043 ">
since domain adaptation has been an extensive research area in parsing research (nivre et al, 2007),<papid> D07-1096 </papid>a lot of ideas have been proposed, including un /semi-supervised approaches (roark and bacchiani, 2003; blitzer et al, 2006; <papid> W06-1615 </papid>steedman et al, 2003; <papid> E03-1008 </papid>mcclosky et al, 2006; <papid> P06-1043 </papid>clegg and shepherd, 2005; <papid> W05-1102 </papid>mcclosky et al, 2010) <papid> N10-1004 </papid>and supervised approaches (titov and henderson, 2006; <papid> W06-2902 </papid>hara et al, 2007).<papid> W07-2202 </papid></citsent>
<aftsection>
<nextsent>their main focus was on adapting parsing models trained with specific genre of text (in most casesptb-wsj) to other genres of text, such as biomedical research papers.
</nextsent>
<nextsent>a major problem tackled in such task setting is the handling of unknown words and domain-specific ways of expressions.
</nextsent>
<nextsent>however,as we explored, parsing nl queries involves significantly different problem; even when all words ina sentence are known, the sentence has very different construction from declarative sentences.although sentence constructions have gained little attention, notable exception is (judge et al,2006).<papid> P06-1063 </papid></nextsent>
<nextsent>they pointed out low accuracy of state-ofthe-art parsers on questions, and proposed supervised parser adaptation by manually creating treebank of questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5013">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>iftwo question sentences were concatenated by conjunctions into one sentence, the parser would tend tofail to analyze the sentence construction for the latter sentence.
</prevsent>
<prevsent>the remaining errors in table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data.
</prevsent>
</prevsection>
<citsent citstr=" W05-1102 ">
since domain adaptation has been an extensive research area in parsing research (nivre et al, 2007),<papid> D07-1096 </papid>a lot of ideas have been proposed, including un /semi-supervised approaches (roark and bacchiani, 2003; blitzer et al, 2006; <papid> W06-1615 </papid>steedman et al, 2003; <papid> E03-1008 </papid>mcclosky et al, 2006; <papid> P06-1043 </papid>clegg and shepherd, 2005; <papid> W05-1102 </papid>mcclosky et al, 2010) <papid> N10-1004 </papid>and supervised approaches (titov and henderson, 2006; <papid> W06-2902 </papid>hara et al, 2007).<papid> W07-2202 </papid></citsent>
<aftsection>
<nextsent>their main focus was on adapting parsing models trained with specific genre of text (in most casesptb-wsj) to other genres of text, such as biomedical research papers.
</nextsent>
<nextsent>a major problem tackled in such task setting is the handling of unknown words and domain-specific ways of expressions.
</nextsent>
<nextsent>however,as we explored, parsing nl queries involves significantly different problem; even when all words ina sentence are known, the sentence has very different construction from declarative sentences.although sentence constructions have gained little attention, notable exception is (judge et al,2006).<papid> P06-1063 </papid></nextsent>
<nextsent>they pointed out low accuracy of state-ofthe-art parsers on questions, and proposed supervised parser adaptation by manually creating treebank of questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5014">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>iftwo question sentences were concatenated by conjunctions into one sentence, the parser would tend tofail to analyze the sentence construction for the latter sentence.
</prevsent>
<prevsent>the remaining errors in table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data.
</prevsent>
</prevsection>
<citsent citstr=" N10-1004 ">
since domain adaptation has been an extensive research area in parsing research (nivre et al, 2007),<papid> D07-1096 </papid>a lot of ideas have been proposed, including un /semi-supervised approaches (roark and bacchiani, 2003; blitzer et al, 2006; <papid> W06-1615 </papid>steedman et al, 2003; <papid> E03-1008 </papid>mcclosky et al, 2006; <papid> P06-1043 </papid>clegg and shepherd, 2005; <papid> W05-1102 </papid>mcclosky et al, 2010) <papid> N10-1004 </papid>and supervised approaches (titov and henderson, 2006; <papid> W06-2902 </papid>hara et al, 2007).<papid> W07-2202 </papid></citsent>
<aftsection>
<nextsent>their main focus was on adapting parsing models trained with specific genre of text (in most casesptb-wsj) to other genres of text, such as biomedical research papers.
</nextsent>
<nextsent>a major problem tackled in such task setting is the handling of unknown words and domain-specific ways of expressions.
</nextsent>
<nextsent>however,as we explored, parsing nl queries involves significantly different problem; even when all words ina sentence are known, the sentence has very different construction from declarative sentences.although sentence constructions have gained little attention, notable exception is (judge et al,2006).<papid> P06-1063 </papid></nextsent>
<nextsent>they pointed out low accuracy of state-ofthe-art parsers on questions, and proposed supervised parser adaptation by manually creating treebank of questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5015">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>iftwo question sentences were concatenated by conjunctions into one sentence, the parser would tend tofail to analyze the sentence construction for the latter sentence.
</prevsent>
<prevsent>the remaining errors in table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data.
</prevsent>
</prevsection>
<citsent citstr=" W06-2902 ">
since domain adaptation has been an extensive research area in parsing research (nivre et al, 2007),<papid> D07-1096 </papid>a lot of ideas have been proposed, including un /semi-supervised approaches (roark and bacchiani, 2003; blitzer et al, 2006; <papid> W06-1615 </papid>steedman et al, 2003; <papid> E03-1008 </papid>mcclosky et al, 2006; <papid> P06-1043 </papid>clegg and shepherd, 2005; <papid> W05-1102 </papid>mcclosky et al, 2010) <papid> N10-1004 </papid>and supervised approaches (titov and henderson, 2006; <papid> W06-2902 </papid>hara et al, 2007).<papid> W07-2202 </papid></citsent>
<aftsection>
<nextsent>their main focus was on adapting parsing models trained with specific genre of text (in most casesptb-wsj) to other genres of text, such as biomedical research papers.
</nextsent>
<nextsent>a major problem tackled in such task setting is the handling of unknown words and domain-specific ways of expressions.
</nextsent>
<nextsent>however,as we explored, parsing nl queries involves significantly different problem; even when all words ina sentence are known, the sentence has very different construction from declarative sentences.although sentence constructions have gained little attention, notable exception is (judge et al,2006).<papid> P06-1063 </papid></nextsent>
<nextsent>they pointed out low accuracy of state-ofthe-art parsers on questions, and proposed supervised parser adaptation by manually creating treebank of questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5018">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their main focus was on adapting parsing models trained with specific genre of text (in most casesptb-wsj) to other genres of text, such as biomedical research papers.
</prevsent>
<prevsent>a major problem tackled in such task setting is the handling of unknown words and domain-specific ways of expressions.
</prevsent>
</prevsection>
<citsent citstr=" P06-1063 ">
however,as we explored, parsing nl queries involves significantly different problem; even when all words ina sentence are known, the sentence has very different construction from declarative sentences.although sentence constructions have gained little attention, notable exception is (judge et al,2006).<papid> P06-1063 </papid></citsent>
<aftsection>
<nextsent>they pointed out low accuracy of state-ofthe-art parsers on questions, and proposed supervised parser adaptation by manually creating treebank of questions.
</nextsent>
<nextsent>the question sentences are annotated with phrase structure trees in the ptb scheme, although function tags and empty categories are omitted.
</nextsent>
<nextsent>an lfg parser trained on the treebank then achieved significant improvement in parsing accuracy.
</nextsent>
<nextsent>(rimell and clark, 2008) <papid> D08-1050 </papid>also worked on question parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5019">
<title id=" W11-0221.xml">parsing natural language queries for life science knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the question sentences are annotated with phrase structure trees in the ptb scheme, although function tags and empty categories are omitted.
</prevsent>
<prevsent>an lfg parser trained on the treebank then achieved significant improvement in parsing accuracy.
</prevsent>
</prevsection>
<citsent citstr=" D08-1050 ">
(rimell and clark, 2008) <papid> D08-1050 </papid>also worked on question parsing.</citsent>
<aftsection>
<nextsent>they collected question sentences from trec 9-12, and annotated the sentences with poss and ccg (steedman, 2000) lexical categories.
</nextsent>
<nextsent>they reported significant improvement in ccg parsing without phrase structure annotations.on the other hand, (judge et al, 2006) <papid> P06-1063 </papid>also implied that just increasing the training data would not be enough.</nextsent>
<nextsent>we went further from their work, built small but complete treebank for nl queries, and explored what really occurred in hpsg parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5021">
<title id=" W11-1104.xml">word sense induction by community detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>awe consider graph-based approaches to wsi,which typically construct graph from word occurrences or collocations.
</prevsent>
<prevsent>the core problem is how to identify sense-specific information within the graphin order to perform sense induction.
</prevsent>
</prevsection>
<citsent citstr=" E03-1020 ">
current approaches have used clustering (dorow and widdows, 2003; <papid> E03-1020 </papid>klapaftis and manandhar, 2008) or statistical graph models (klapaftis and manandhar, 2010) <papid> D10-1073 </papid>to identify sense-specific subgraphs.we reinterpret the challenge of identifying sense specific information in co-occurrence graph as oneof community detection, where community is defined as group of connected nodes that are more connected to each other than to the rest of the graph (fortunato, 2010).</citsent>
<aftsection>
<nextsent>within the co-occurrence graph,we hypothesize that communities identify sense specific contexts for each of the terms.
</nextsent>
<nextsent>community detection identifies groups of contextual cues that constrain each of the words in community to single sense.
</nextsent>
<nextsent>to test our hypothesis, we require community detection algorithm with two key properties: (1) aword may belong to multiple, overlapping communities, which is necessary for discovering multiple senses, and (2) the community detection may be hierarchically tuned, which corresponds to sense gran ularity.
</nextsent>
<nextsent>therefore, we adapt recent, state of the art approach, link clustering (ahn et al, 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5022">
<title id=" W11-1104.xml">word sense induction by community detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>awe consider graph-based approaches to wsi,which typically construct graph from word occurrences or collocations.
</prevsent>
<prevsent>the core problem is how to identify sense-specific information within the graphin order to perform sense induction.
</prevsent>
</prevsection>
<citsent citstr=" D10-1073 ">
current approaches have used clustering (dorow and widdows, 2003; <papid> E03-1020 </papid>klapaftis and manandhar, 2008) or statistical graph models (klapaftis and manandhar, 2010) <papid> D10-1073 </papid>to identify sense-specific subgraphs.we reinterpret the challenge of identifying sense specific information in co-occurrence graph as oneof community detection, where community is defined as group of connected nodes that are more connected to each other than to the rest of the graph (fortunato, 2010).</citsent>
<aftsection>
<nextsent>within the co-occurrence graph,we hypothesize that communities identify sense specific contexts for each of the terms.
</nextsent>
<nextsent>community detection identifies groups of contextual cues that constrain each of the words in community to single sense.
</nextsent>
<nextsent>to test our hypothesis, we require community detection algorithm with two key properties: (1) aword may belong to multiple, overlapping communities, which is necessary for discovering multiple senses, and (2) the community detection may be hierarchically tuned, which corresponds to sense gran ularity.
</nextsent>
<nextsent>therefore, we adapt recent, state of the art approach, link clustering (ahn et al, 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5023">
<title id=" W11-1104.xml">word sense induction by community detection </title>
<section> word sense induction.  </section>
<citcontext>
<prevsection>
<prevsent>following previous work (veronis, 2004), we select only nouns in the context.early experiments indicated that including infrequent terms in the co-occurrence graph yielded poor performance, which we attribute to having too few connecting edges to identify meaningful community structure.
</prevsent>
<prevsent>therefore, we include only those nouns occurring in the most frequent 5000 tokens, which are likely to be representative the largest communities in which term takes part.
</prevsent>
</prevsection>
<citsent citstr=" S10-1011 ">
last, we include all the nouns and verbs used in the semeval 2010 wsi task (manandhar et al, 2010), <papid> S10-1011 </papid>which are used in our evaluation.</citsent>
<aftsection>
<nextsent>the selected context terms are then stemmed using the porter stemmer.
</nextsent>
<nextsent>building the co-occurrence graph the graph is iteratively constructed by adding edges between the terms from context.
</nextsent>
<nextsent>for each pair-wise combination of terms, an edge is added and its weight is increased by 1.
</nextsent>
<nextsent>this step effectively embeds clique if it did not exist before, connecting all ofthe contexts words within the graph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5025">
<title id=" W11-1104.xml">word sense induction by community detection </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>systems induce sense representations for target words from the training corpus and then use those representations to label the senses of the target words in unseen contexts from test corpus.we use the entire multi-sentence context for building the co-occurrence graph.
</prevsent>
<prevsent>the induced sense labeling is scored using two unsupervised and one supervised methods.
</prevsent>
</prevsection>
<citsent citstr=" D09-1056 ">
the unsupervised scores consists of two contrasting mea sures: the paired fscore (artiles et al, 2009) <papid> D09-1056 </papid>and the v-measure (rosenberg and hirschberg, 2007).<papid> D07-1043 </papid></citsent>
<aftsection>
<nextsent>briefly, the v-measure rates the homogeneity and completeness of clustering solution.
</nextsent>
<nextsent>solutions that have word clusters formed from one gold-standard sense are homogeneous; completeness measures the degree to which gold-standard senses instances are assigned to single cluster.
</nextsent>
<nextsent>the paired fscorereflects the overlap of the solution and the gold standard in cluster assignments for all pair-wise combi fscore v-meas.
</nextsent>
<nextsent>s80/20 s60/40 spd 61.1 (3) 3.6 (18) 57.64 (18) 57.64 (16) sv 56.16 (9) 8.7 (6) 57.90 (18) 57.36 (17) sf 63.4 (1) 0 (26) 56.18 (21) 56.20 (21) bestf 63.3 (1) 0 (26) 58.69 (14) 58.24 (13) bestv 26.7 (25) 16.2 (1) 58.34 (16) 57.27 (17) bests 49.8 (15) 15.7 (2) 62.44 (1) 61.96 (1) mfs 63.4 0 58.67 58.95 table 1: performance results on the semeval-2010wsi task, with rank shown in parentheses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5026">
<title id=" W11-1104.xml">word sense induction by community detection </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>systems induce sense representations for target words from the training corpus and then use those representations to label the senses of the target words in unseen contexts from test corpus.we use the entire multi-sentence context for building the co-occurrence graph.
</prevsent>
<prevsent>the induced sense labeling is scored using two unsupervised and one supervised methods.
</prevsent>
</prevsection>
<citsent citstr=" D07-1043 ">
the unsupervised scores consists of two contrasting mea sures: the paired fscore (artiles et al, 2009) <papid> D09-1056 </papid>and the v-measure (rosenberg and hirschberg, 2007).<papid> D07-1043 </papid></citsent>
<aftsection>
<nextsent>briefly, the v-measure rates the homogeneity and completeness of clustering solution.
</nextsent>
<nextsent>solutions that have word clusters formed from one gold-standard sense are homogeneous; completeness measures the degree to which gold-standard senses instances are assigned to single cluster.
</nextsent>
<nextsent>the paired fscorereflects the overlap of the solution and the gold standard in cluster assignments for all pair-wise combi fscore v-meas.
</nextsent>
<nextsent>s80/20 s60/40 spd 61.1 (3) 3.6 (18) 57.64 (18) 57.64 (16) sv 56.16 (9) 8.7 (6) 57.90 (18) 57.36 (17) sf 63.4 (1) 0 (26) 56.18 (21) 56.20 (21) bestf 63.3 (1) 0 (26) 58.69 (14) 58.24 (13) bestv 26.7 (25) 16.2 (1) 58.34 (16) 57.27 (17) bests 49.8 (15) 15.7 (2) 62.44 (1) 61.96 (1) mfs 63.4 0 58.67 58.95 table 1: performance results on the semeval-2010wsi task, with rank shown in parentheses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5030">
<title id=" W11-1104.xml">word sense induction by community detection </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>an initial analysis using the semeval-2010 wsi task demonstrates competitive performance.
</prevsent>
<prevsent>future research will address two potential avenues: (1) the impact of word frequency on community size and memberships and (2) identifying both graph properties and semantic relations within hierarchical communities that distinguish between sense granularities.
</prevsent>
</prevsection>
<citsent citstr=" P10-4006 ">
software for the wsi model and for link clustering is available as part of the s-space package (jurgens and stevens, 2010).<papid> P10-4006 </papid></citsent>
<aftsection>
<nextsent>27
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5031">
<title id=" W11-0910.xml">incorporating coercive constructions into a verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we discuss how these methods will form the basis for enhancements for verbnet supporting more accurate analysis of the relational semantics of verb across productive usages.
</prevsent>
<prevsent>automatic semantic analysis has been very successful when taking supervised learning approach on data labeled with sense tags and semantic roles (e.g., see mrquez et al, 2008).
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
underlying these recent successes are lexical resources, such as propbank (palmer et al, 2005), <papid> J05-1004 </papid>verbnet (kipper et al, 2008), and framenet (baker et al, 1998; <papid> P98-1013 </papid>fillmore et al, 2002), which encode the relational semantics of numerous lexical items, especially verbs.</citsent>
<aftsection>
<nextsent>however, because authors and speakers use verbs productively in previously unseen ways, semantic analysis systems must not be limited to direct extrapolation from previously seen usages licensed by static lexical resources (cf.
</nextsent>
<nextsent>pustejovsky &amp; jezek, 2008).
</nextsent>
<nextsent>to achieve more accurate semantic analyses, we must augment such resources with knowledge of the extensibility of verbs.
</nextsent>
<nextsent>central to verb extensibility is the process of semantic and syntactic coercion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5032">
<title id=" W11-0910.xml">incorporating coercive constructions into a verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we discuss how these methods will form the basis for enhancements for verbnet supporting more accurate analysis of the relational semantics of verb across productive usages.
</prevsent>
<prevsent>automatic semantic analysis has been very successful when taking supervised learning approach on data labeled with sense tags and semantic roles (e.g., see mrquez et al, 2008).
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
underlying these recent successes are lexical resources, such as propbank (palmer et al, 2005), <papid> J05-1004 </papid>verbnet (kipper et al, 2008), and framenet (baker et al, 1998; <papid> P98-1013 </papid>fillmore et al, 2002), which encode the relational semantics of numerous lexical items, especially verbs.</citsent>
<aftsection>
<nextsent>however, because authors and speakers use verbs productively in previously unseen ways, semantic analysis systems must not be limited to direct extrapolation from previously seen usages licensed by static lexical resources (cf.
</nextsent>
<nextsent>pustejovsky &amp; jezek, 2008).
</nextsent>
<nextsent>to achieve more accurate semantic analyses, we must augment such resources with knowledge of the extensibility of verbs.
</nextsent>
<nextsent>central to verb extensibility is the process of semantic and syntactic coercion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5034">
<title id=" W11-0910.xml">incorporating coercive constructions into a verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>importantly, constructional coercion is not an all-or nothing process ? word must be semantically and syntactically compatible in some respects with context in order for its use to be extended to that context, but the restrictions on compatibility are not hard-and-fast rules (langacker, 1987; kay &amp; fillmore, 1999; goldberg, 2006; goldberg, to appear).
</prevsent>
<prevsent>gradience of compatibility plays an important role in coercion, suggesting that probabilistic approach may be necessary for encoding knowledge of constructional coercion in verb lexicon (cf.
</prevsent>
</prevsection>
<citsent citstr=" E03-1073 ">
lapata &amp; lascarides, 2003).<papid> E03-1073 </papid></citsent>
<aftsection>
<nextsent>our hypothesis here is that, due to this gradient process of productivity, existing verb lexicons do not adequately capture the actual patterns of use of extensible constructions.
</nextsent>
<nextsent>in this paper, we focus on the caused motion (cm) construction as an initial test case.
</nextsent>
<nextsent>we first annotate the classes of an extensive verb lexicon, verbnet, as to whether the cm construction is allowed for all, some, or none of the verbs in the class, noting additionally whether it is typical or coerced usage.
</nextsent>
<nextsent>we find that many of the classes that allow the construction for at least some verbs do not include the cm frame in their definition, indicating significant shortcoming in the relational knowledge encoded in the lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5035">
<title id=" W11-0910.xml">incorporating coercive constructions into a verb lexicon </title>
<section> extensible constructions and verbnet.  </section>
<citcontext>
<prevsection>
<prevsent>verbs are also highly variable, displaying rich range of semantic and syntactic behavior.
</prevsent>
<prevsent>verb classifications help nlp systems to deal with this complexity by organizing verbs into groups that share core semantic and syntactic properties.
</prevsent>
</prevsection>
<citsent citstr=" W04-3213 ">
for example, verbnet (derived from levins [1993] work, kipper et al, 2008) is widely used for number of semantic processing tasks, including semantic role labeling (swier and stevenson, 2004), <papid> W04-3213 </papid>the creation of semantic parse trees (shi and mihalcea, 2005), and implicit argument resolution (gerber and chai, 2010).<papid> P10-1160 </papid></citsent>
<aftsection>
<nextsent>the detailed semantic predicates listed with each verbnet class also have the potential to contribute to text specific semantic representations and, thereby, to tasks requiring inferencing (zaenen et al, 2008; <papid> L08-1037 </papid>palmer et al, 2009).</nextsent>
<nextsent>verbnet identifies semantic roles and syntactic patterns characteristic of the verbs in each class makes explicit the connections between the syntactic patterns and the underlying semantic relations that can be inferred for all members of the class.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5036">
<title id=" W11-0910.xml">incorporating coercive constructions into a verb lexicon </title>
<section> extensible constructions and verbnet.  </section>
<citcontext>
<prevsection>
<prevsent>verbs are also highly variable, displaying rich range of semantic and syntactic behavior.
</prevsent>
<prevsent>verb classifications help nlp systems to deal with this complexity by organizing verbs into groups that share core semantic and syntactic properties.
</prevsent>
</prevsection>
<citsent citstr=" P10-1160 ">
for example, verbnet (derived from levins [1993] work, kipper et al, 2008) is widely used for number of semantic processing tasks, including semantic role labeling (swier and stevenson, 2004), <papid> W04-3213 </papid>the creation of semantic parse trees (shi and mihalcea, 2005), and implicit argument resolution (gerber and chai, 2010).<papid> P10-1160 </papid></citsent>
<aftsection>
<nextsent>the detailed semantic predicates listed with each verbnet class also have the potential to contribute to text specific semantic representations and, thereby, to tasks requiring inferencing (zaenen et al, 2008; <papid> L08-1037 </papid>palmer et al, 2009).</nextsent>
<nextsent>verbnet identifies semantic roles and syntactic patterns characteristic of the verbs in each class makes explicit the connections between the syntactic patterns and the underlying semantic relations that can be inferred for all members of the class.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5037">
<title id=" W11-0910.xml">incorporating coercive constructions into a verb lexicon </title>
<section> extensible constructions and verbnet.  </section>
<citcontext>
<prevsection>
<prevsent>verb classifications help nlp systems to deal with this complexity by organizing verbs into groups that share core semantic and syntactic properties.
</prevsent>
<prevsent>for example, verbnet (derived from levins [1993] work, kipper et al, 2008) is widely used for number of semantic processing tasks, including semantic role labeling (swier and stevenson, 2004), <papid> W04-3213 </papid>the creation of semantic parse trees (shi and mihalcea, 2005), and implicit argument resolution (gerber and chai, 2010).<papid> P10-1160 </papid></prevsent>
</prevsection>
<citsent citstr=" L08-1037 ">
the detailed semantic predicates listed with each verbnet class also have the potential to contribute to text specific semantic representations and, thereby, to tasks requiring inferencing (zaenen et al, 2008; <papid> L08-1037 </papid>palmer et al, 2009).</citsent>
<aftsection>
<nextsent>verbnet identifies semantic roles and syntactic patterns characteristic of the verbs in each class makes explicit the connections between the syntactic patterns and the underlying semantic relations that can be inferred for all members of the class.
</nextsent>
<nextsent>each syntactic frame in class has corresponding semantic representation that details the semantic relations between event participants across the course of the event.
</nextsent>
<nextsent>for example, one of the characteristic patterns listed for the pour class is caused-motion pattern, which accounts for sentences like she poured water from the pitcher into the bowl.
</nextsent>
<nextsent>this is represented in verbnet as follows: syntactic representation: np np pp pp agent theme source location semantic representation: motion (during(e), theme) not (prep (start(e), theme, location)) prep (start(e), theme, source) prep (end(e), theme, location) cause (agent, e) this representation details connections between the syntax and semantics using the semantic roles as links, indicating that the agent is the subject np and has caused the event, and that the theme is the object np and has new location at the end of the event.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5040">
<title id=" W11-0910.xml">incorporating coercive constructions into a verb lexicon </title>
<section> extensible constructions and verbnet.  </section>
<citcontext>
<prevsection>
<prevsent>one option might be to augment the hiccup class with the cm frame from the pour class, which would ensure that such sentences would be analyzed more accurately.
</prevsent>
<prevsent>however, given the productive nature of constructional coercion and its widespread applicability, the approach of adding any possible pattern to each class is not appropriate: this would undermine the definitional distinctions between classes and greatly lessen their usefulness.
</prevsent>
</prevsection>
<citsent citstr=" P98-1046 ">
complicating the issue is the phenomenon of regular sense extensions (dang et al, 1998), <papid> P98-1046 </papid>where what once may have been coercion has become entrenched and is now seen as different sense of the verb.</citsent>
<aftsection>
<nextsent>for example, the verbs in the push class express the general meaning of exerting force on an object, such as she pushed on the wall.
</nextsent>
<nextsent>often, the exertion of force moves the object, which can be expressed in cm construction such as she pushed the box across the room.
</nextsent>
<nextsent>verbnet accounts for this regular sense extension by including most of the push verbs in the carry class as well, which has the cm construction as one of its frames.
</nextsent>
<nextsent>deciding when to include verb in another class based on regular sense extensions, when to add frame for construction to class, or when to reject the frame as defining part of class, is made difficult by the graded nature of matches between verbs and construction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5041">
<title id=" W11-0910.xml">incorporating coercive constructions into a verb lexicon </title>
<section> our proposal: constructional profiles.  </section>
<citcontext>
<prevsection>
<prevsent>although these probabilities are intuitively very simple, estimating them from corpus data poses significant challenge.
</prevsent>
<prevsent>since construction is pairing of form with meaning, recognizing the use of particular 74 construction is not simply matter of determining the syntactic pattern of the usage; rather, certain semantic properties and relations must co-occur with the syntactic pattern.
</prevsent>
</prevsection>
<citsent citstr=" W10-0801 ">
earlier work has shown that supervised learning method was able to discriminate potential usages of the cm construction given training sentences manually labeled as either cm or not (hwang et al, 2010).<papid> W10-0801 </papid></citsent>
<aftsection>
<nextsent>here, we aim instead to identify usages of the cm construction, but without requiring an expensive manual annotation effort.
</nextsent>
<nextsent>that is, we seek an unsupervised method for estimating the probabilities in p1p3 above.
</nextsent>
<nextsent>we approach this goal in steps as follows.
</nextsent>
<nextsent>first, we examine all the classes in verbnet to see which allow the cm construction (section 4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5047">
<title id=" W11-0819.xml">fipscoview online visualisation of collocations extracted from multilingual parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that our tool is not tailored to this specific corpus.
</prevsent>
<prevsent>125 figure 1: fipscoview: system architecture.
</prevsent>
</prevsection>
<citsent citstr=" W07-1216 ">
tic parsing (wehrli, 2007) <papid> W07-1216 </papid>and syntax-based machine translation (wehrli et al, 2009), <papid> W09-0415 </papid>the on-line version is designed to offer access to the derived collocation resources to broader community.</citsent>
<aftsection>
<nextsent>figure 1 shows the architecture of fipscoview.
</nextsent>
<nextsent>the main system modules are the collocation extraction module, the search &amp; visualisation module, the con cor dancing and the sentence alignment modules.the processing flow is pipelined.
</nextsent>
<nextsent>the key module of the system, collocation extraction, relies ona syntax-based methodology that combines lexical statistics with syntactic information provided by fips, deep symbolic parser (wehrli, 2007).<papid> W07-1216 </papid></nextsent>
<nextsent>this methodology is fully described and evaluated in seretan (2011).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5048">
<title id=" W11-0819.xml">fipscoview online visualisation of collocations extracted from multilingual parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that our tool is not tailored to this specific corpus.
</prevsent>
<prevsent>125 figure 1: fipscoview: system architecture.
</prevsent>
</prevsection>
<citsent citstr=" W09-0415 ">
tic parsing (wehrli, 2007) <papid> W07-1216 </papid>and syntax-based machine translation (wehrli et al, 2009), <papid> W09-0415 </papid>the on-line version is designed to offer access to the derived collocation resources to broader community.</citsent>
<aftsection>
<nextsent>figure 1 shows the architecture of fipscoview.
</nextsent>
<nextsent>the main system modules are the collocation extraction module, the search &amp; visualisation module, the con cor dancing and the sentence alignment modules.the processing flow is pipelined.
</nextsent>
<nextsent>the key module of the system, collocation extraction, relies ona syntax-based methodology that combines lexical statistics with syntactic information provided by fips, deep symbolic parser (wehrli, 2007).<papid> W07-1216 </papid></nextsent>
<nextsent>this methodology is fully described and evaluated in seretan (2011).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5050">
<title id=" W11-0819.xml">fipscoview online visualisation of collocations extracted from multilingual parallel corpora </title>
<section> architecture and main functional ities.  </section>
<citcontext>
<prevsection>
<prevsent>this methodology is fully described and evaluated in seretan (2011).
</prevsent>
<prevsent>in principle, the extraction takes place only once, but new corpora can be processed later and results are cumulated.
</prevsent>
</prevsection>
<citsent citstr=" E03-1022 ">
the sentence alignment (nerima et al, 2003) <papid> E03-1022 </papid>is performed partially, i.e., only for the sentences actually displayed by the concordancing module.</citsent>
<aftsection>
<nextsent>it is done on the fly, thus eliminating the need of pre-aligning the corpora.
</nextsent>
<nextsent>the role of the concordancing module is to present the sentence contexts for selected collocation (cf.
</nextsent>
<nextsent>scenario described in 1).
</nextsent>
<nextsent>the words in this collocation are highlighted for readability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5051">
<title id=" W11-0819.xml">fipscoview online visualisation of collocations extracted from multilingual parallel corpora </title>
<section> architecture and main functional ities.  </section>
<citcontext>
<prevsection>
<prevsent>users can set visualisation parameters such as the minimal frequency and association score, which limit the displayed results according to the number of occurrences in the corpus and the association strength?
</prevsent>
<prevsent>between the component words, as given by the lexical association measure used to extract collocations.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
the measure we typically use is log-likelihood ratio (dunning, 1993); <papid> J93-1003 </papid>see pecina (2008) for an inventory of measures.</citsent>
<aftsection>
<nextsent>depending on these parameters, the automatically created collocation entry is more or less exhaustive (the output adapts to the specific users purpose).
</nextsent>
<nextsent>a different sub-entry is created for each part of speech of the sought word (for instance, report can either be noun or verb).
</nextsent>
<nextsent>under each sub-entry, collocations are organised by syntactic type, e.g., adjective noun (comprehensive report), noun-noun (initiative report), subject-verb (report highlights), verb-object (produce report).
</nextsent>
<nextsent>to avoid redundancy, only the col locating words are shown.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5052">
<title id=" W11-1504.xml">ere search for linguists </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>modern approaches to language description and documentation are not possible without the technology that allows the creation, retrieval and storage of diverse data types.
</prevsent>
<prevsent>a field whose main aim is to provide comprehensive record of language constructions and rules (himmelmann, 1998) is crucially dependent on software that supports the effort.
</prevsent>
</prevsection>
<citsent citstr=" J09-3007 ">
talking to the language documentation community bird (2009) <papid> J09-3007 </papid>lists as some of the immediate tasks that linguists need help with; inter linearization of text, validation issues and, what he calls, the handling of uncertain data.</citsent>
<aftsection>
<nextsent>in fact, computers always have played an important role in linguistic research.
</nextsent>
<nextsent>starting out as machines that were able to increase the efficiency of text and data management, they have become tools that allow linguists to pursue research in ways that were not previously possible.1 given an increased interest in work with naturally occurring language, new generation of search engines for on line corpora have appeared with more features that facilitate linguistic analysis (biemann et al, 2004).
</nextsent>
<nextsent>the creation of annotated corpora from private data collections, is however, still mainly seen as task that is only relevant to smaller groups of linguist sand anthropologists engaged in field work.
</nextsent>
<nextsent>shoebox/toolbox is probably the oldest software especially designed for this user group.together with the fieldwork language explorer (flex), also devel 1we would like to cite tognini-bonelli (2001) who speaks for corpus linguistics and (bird, 2009) <papid> J09-3007 </papid>who discusses natural language processing and its connection to the field of language documentation as sources describing this process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5054">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the bionlp shared task 2011, an information extraction task held over 6 months up tomarch 2011, met with community-wide participation, receiving 46 final submissions from24 teams.
</prevsent>
<prevsent>five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects.
</prevsent>
</prevsection>
<citsent citstr=" W09-1401 ">
the bionlp shared task (bionlp-st, hereafter) series represents community-wide move towardfine-grained information extraction (ie), in particular biomolecular event extraction (kim et al, 2009;<papid> W09-1401 </papid>ananiadou et al, 2010).</citsent>
<aftsection>
<nextsent>the series is complementary to bio creative (hirschman et al, 2007); while bio creative emphasizes the short-term applicability of introduced ie methods for tasks such as database cur ation, bionlp-st places more emphasis on the measurability of the state-of-the-art and traceability of challenges in extraction through an approach more closely tied to text.
</nextsent>
<nextsent>these goals were pursued in the first event, bionlp-st 2009 (kim et al, 2009), <papid> W09-1401 </papid>through high quality benchmark data provided for system development and detailed evaluation performed to identify remaining problems hindering extraction perfor mance.</nextsent>
<nextsent>also, as the complexity of the task was highand system development time limited, we encouraged focus on fine-grained ie by providing gold annotation for named entities as well as various supporting resources.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5056">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also, as the complexity of the task was highand system development time limited, we encouraged focus on fine-grained ie by providing gold annotation for named entities as well as various supporting resources.
</prevsent>
<prevsent>bionlp-st 2009 attracted wide attention, with 24 teams submitting final results.
</prevsent>
</prevsection>
<citsent citstr=" W10-1905 ">
the task setup and data since have served as the basis for numerous studies (miwa et al, 2010<papid> W10-1905 </papid>b; poon and vanderwende, 2010; <papid> N10-1123 </papid>vlachos, 2010; <papid> W10-1901 </papid>miwa et al, 2010<papid> W10-1905 </papid>a; bjorne et al, 2010).</citsent>
<aftsection>
<nextsent>as the second event of the series, bionlp-st 2011 preserves the general design and goals of the previous event, but adds new focus on variability to address limitation of bionlp-st 2009: the benchmark datasets were based on the genia corpus (kim et al, 2008), restricting the community-wide effort to resources developed by single group fora small sub domain of molecular biology.
</nextsent>
<nextsent>bionlp st 2011 is organized as joint effort of several groups preparing various tasks and resources, inwhich variability is pursued in three primary direc tions: text types, event types, and subject domains.
</nextsent>
<nextsent>consequently, generalization of fine grained bio-ie in these directions is emphasized as the main theme of the second event.
</nextsent>
<nextsent>this paper summarizes the entire bionlp-st 2011, covering the relationships between tasks and similar broad issues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5058">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also, as the complexity of the task was highand system development time limited, we encouraged focus on fine-grained ie by providing gold annotation for named entities as well as various supporting resources.
</prevsent>
<prevsent>bionlp-st 2009 attracted wide attention, with 24 teams submitting final results.
</prevsent>
</prevsection>
<citsent citstr=" N10-1123 ">
the task setup and data since have served as the basis for numerous studies (miwa et al, 2010<papid> W10-1905 </papid>b; poon and vanderwende, 2010; <papid> N10-1123 </papid>vlachos, 2010; <papid> W10-1901 </papid>miwa et al, 2010<papid> W10-1905 </papid>a; bjorne et al, 2010).</citsent>
<aftsection>
<nextsent>as the second event of the series, bionlp-st 2011 preserves the general design and goals of the previous event, but adds new focus on variability to address limitation of bionlp-st 2009: the benchmark datasets were based on the genia corpus (kim et al, 2008), restricting the community-wide effort to resources developed by single group fora small sub domain of molecular biology.
</nextsent>
<nextsent>bionlp st 2011 is organized as joint effort of several groups preparing various tasks and resources, inwhich variability is pursued in three primary direc tions: text types, event types, and subject domains.
</nextsent>
<nextsent>consequently, generalization of fine grained bio-ie in these directions is emphasized as the main theme of the second event.
</nextsent>
<nextsent>this paper summarizes the entire bionlp-st 2011, covering the relationships between tasks and similar broad issues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5059">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also, as the complexity of the task was highand system development time limited, we encouraged focus on fine-grained ie by providing gold annotation for named entities as well as various supporting resources.
</prevsent>
<prevsent>bionlp-st 2009 attracted wide attention, with 24 teams submitting final results.
</prevsent>
</prevsection>
<citsent citstr=" W10-1901 ">
the task setup and data since have served as the basis for numerous studies (miwa et al, 2010<papid> W10-1905 </papid>b; poon and vanderwende, 2010; <papid> N10-1123 </papid>vlachos, 2010; <papid> W10-1901 </papid>miwa et al, 2010<papid> W10-1905 </papid>a; bjorne et al, 2010).</citsent>
<aftsection>
<nextsent>as the second event of the series, bionlp-st 2011 preserves the general design and goals of the previous event, but adds new focus on variability to address limitation of bionlp-st 2009: the benchmark datasets were based on the genia corpus (kim et al, 2008), restricting the community-wide effort to resources developed by single group fora small sub domain of molecular biology.
</nextsent>
<nextsent>bionlp st 2011 is organized as joint effort of several groups preparing various tasks and resources, inwhich variability is pursued in three primary direc tions: text types, event types, and subject domains.
</nextsent>
<nextsent>consequently, generalization of fine grained bio-ie in these directions is emphasized as the main theme of the second event.
</nextsent>
<nextsent>this paper summarizes the entire bionlp-st 2011, covering the relationships between tasks and similar broad issues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5062">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> main tasks.  </section>
<citcontext>
<prevsection>
<prevsent>bionlp-st 2011 includes four main tracks (with five tasks) representing fine-grained bio-ie.
</prevsent>
<prevsent>2.1 genia task (ge).
</prevsent>
</prevsection>
<citsent citstr=" W11-1802 ">
the ge task (kim et al, 2011) <papid> W11-1802 </papid>preserves the task definition of bionlp-st 2009, arranged based onthe genia corpus (kim et al, 2008).</citsent>
<aftsection>
<nextsent>the data represents focused domain of molecular biology: transcription factors in human blood cells.
</nextsent>
<nextsent>the purpose of the ge task is two-fold: to measure the progress of the community since the last event, and to evaluate generalization of the technology to full papers.for the second purpose, the provided data is composed of two collections: the abstract collection, identical to the bionlp-st 2009 data, and the newfull paper collection.
</nextsent>
<nextsent>progress on the task is measured through the unchanged task definition and the abstract collection, while generalization to full papers is measured on the full paper collection.
</nextsent>
<nextsent>in this way, the ge task is intended to connect the entire event to the previous one.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5063">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> main tasks.  </section>
<citcontext>
<prevsection>
<prevsent>in this way, the ge task is intended to connect the entire event to the previous one.
</prevsent>
<prevsent>2.2 epi genetics and post-translational.
</prevsent>
</prevsection>
<citsent citstr=" W11-1803 ">
modification task (epi) the epi task (ohta et al, 2011) <papid> W11-1803 </papid>focuses on ie for protein and dna modifications, with particular emphasis on events of epi genetics interest.</citsent>
<aftsection>
<nextsent>while the basic task setup and entity definitions follow those of the ge task, epi extends on the extraction targets by defining 14 new event types relevant to task topics, including major protein modification types and their reverse reactions.
</nextsent>
<nextsent>for capturing the ways in which different entities participate in these events, the task extends the ge argument roles with two new roles specific to the domain, side chain and contextgene.
</nextsent>
<nextsent>the task design and setup are oriented toward the needs of pathway extraction and cur ation for domain databases (wu et al, 2003; ongenaert et al, 2008) and are informed by previous studies on extraction of the target events (ohta et al, 2010<papid> W10-1903 </papid>b; ohta et al, 2010<papid> W10-1903 </papid>c).</nextsent>
<nextsent>2.3 infectious diseases task (id).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5064">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> main tasks.  </section>
<citcontext>
<prevsection>
<prevsent>while the basic task setup and entity definitions follow those of the ge task, epi extends on the extraction targets by defining 14 new event types relevant to task topics, including major protein modification types and their reverse reactions.
</prevsent>
<prevsent>for capturing the ways in which different entities participate in these events, the task extends the ge argument roles with two new roles specific to the domain, side chain and contextgene.
</prevsent>
</prevsection>
<citsent citstr=" W10-1903 ">
the task design and setup are oriented toward the needs of pathway extraction and cur ation for domain databases (wu et al, 2003; ongenaert et al, 2008) and are informed by previous studies on extraction of the target events (ohta et al, 2010<papid> W10-1903 </papid>b; ohta et al, 2010<papid> W10-1903 </papid>c).</citsent>
<aftsection>
<nextsent>2.3 infectious diseases task (id).
</nextsent>
<nextsent>the id task (pyysalo et al, 2011<papid> W11-1812 </papid>a) concerns the extraction of events relevant to biomolecular mechanisms of infectious diseases from full-text publications.</nextsent>
<nextsent>the task follows the basic design of bionlp st 2009, and the identities and extraction targets are superset of the ge ones.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5070">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> main tasks.  </section>
<citcontext>
<prevsection>
<prevsent>the task design and setup are oriented toward the needs of pathway extraction and cur ation for domain databases (wu et al, 2003; ongenaert et al, 2008) and are informed by previous studies on extraction of the target events (ohta et al, 2010<papid> W10-1903 </papid>b; ohta et al, 2010<papid> W10-1903 </papid>c).</prevsent>
<prevsent>2.3 infectious diseases task (id).</prevsent>
</prevsection>
<citsent citstr=" W11-1812 ">
the id task (pyysalo et al, 2011<papid> W11-1812 </papid>a) concerns the extraction of events relevant to biomolecular mechanisms of infectious diseases from full-text publications.</citsent>
<aftsection>
<nextsent>the task follows the basic design of bionlp st 2009, and the identities and extraction targets are superset of the ge ones.
</nextsent>
<nextsent>the task extends considerably on core entities, adding to protein four new entity types, including chemical andorganism.
</nextsent>
<nextsent>the events extend on the ge definitions in allowing arguments of the new entity types as well as in introducing new event category forhigh-level biological processes.
</nextsent>
<nextsent>the task was implemented in collaboration with domain experts and informed by prior studies on domain information extraction requirements (pyysalo et al, 2010; <papid> W10-1919 </papid>ananiadou et al, 2011), including the support of systems such as patric (http://patricbrc.org).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5074">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> main tasks.  </section>
<citcontext>
<prevsection>
<prevsent>the task extends considerably on core entities, adding to protein four new entity types, including chemical andorganism.
</prevsent>
<prevsent>the events extend on the ge definitions in allowing arguments of the new entity types as well as in introducing new event category forhigh-level biological processes.
</prevsent>
</prevsection>
<citsent citstr=" W10-1919 ">
the task was implemented in collaboration with domain experts and informed by prior studies on domain information extraction requirements (pyysalo et al, 2010; <papid> W10-1919 </papid>ananiadou et al, 2011), including the support of systems such as patric (http://patricbrc.org).</citsent>
<aftsection>
<nextsent>2.4 bacteria track.
</nextsent>
<nextsent>the bacteria track consists of two tasks, bb and bi.
</nextsent>
<nextsent>2.4.1 bacteria biotope task (bb)the aim of the bb task (bossy et al, 2011) <papid> W11-1809 </papid>is to extract the habitats of bacteria mentioned in textbook level texts written for non-experts.</nextsent>
<nextsent>the texts are web pages about the state of the art knowledge about bacterial species.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5075">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> main tasks.  </section>
<citcontext>
<prevsection>
<prevsent>2.4 bacteria track.
</prevsent>
<prevsent>the bacteria track consists of two tasks, bb and bi.
</prevsent>
</prevsection>
<citsent citstr=" W11-1809 ">
2.4.1 bacteria biotope task (bb)the aim of the bb task (bossy et al, 2011) <papid> W11-1809 </papid>is to extract the habitats of bacteria mentioned in textbook level texts written for non-experts.</citsent>
<aftsection>
<nextsent>the texts are web pages about the state of the art knowledge about bacterial species.
</nextsent>
<nextsent>bb targets general relations, localization and partof , and is challenging in that texts contain more co references than usual, habitat references are not necessarily named entities, and, unlike in other bionlp-st 2011 tasks, all entities need to be recognized by participants.
</nextsent>
<nextsent>bb is the first task to target phenotypic information and, as habitats are yet to be normalized by the field community, presents an opportunity for the bionlp community to contribute to the standardization effort.
</nextsent>
<nextsent>2.4.2 bacteria interaction task (bi)the bi task (jourde et al, 2011) <papid> W11-1810 </papid>is devoted to the extraction of bacterial molecular interactions and regulations from publication abstracts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5076">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> main tasks.  </section>
<citcontext>
<prevsection>
<prevsent>bb targets general relations, localization and partof , and is challenging in that texts contain more co references than usual, habitat references are not necessarily named entities, and, unlike in other bionlp-st 2011 tasks, all entities need to be recognized by participants.
</prevsent>
<prevsent>bb is the first task to target phenotypic information and, as habitats are yet to be normalized by the field community, presents an opportunity for the bionlp community to contribute to the standardization effort.
</prevsent>
</prevsection>
<citsent citstr=" W11-1810 ">
2.4.2 bacteria interaction task (bi)the bi task (jourde et al, 2011) <papid> W11-1810 </papid>is devoted to the extraction of bacterial molecular interactions and regulations from publication abstracts.</citsent>
<aftsection>
<nextsent>mainly focused on gene transcript ional regulation in bacillus subtilis, the bi corpus is provided to participants with rich semantic annotation derived from recently proposed ontology (manine et al, 2009) defining ten entity types such as gene, protein and derivatives as well as dna sites/motifs.
</nextsent>
<nextsent>their interactions are described through ten relation types.
</nextsent>
<nextsent>the bi corpus consists of the sentences of the lll corpus (nedellec, 2005), provided with manually checked linguistic annotations.
</nextsent>
<nextsent>2 task text focus # ge abstracts, full papers domain (ht) 9 epi abstracts event types 15 id full papers domain (tcs) 10 bb web pages domain (bb) 2 bi abstracts domain (bs) 10 table 1: characteristics of bionlp-st 2011 main tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5078">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> supporting tasks.  </section>
<citcontext>
<prevsection>
<prevsent>in terms of the target event types, id targets superset of ge events and epi extends on the representation for phosphorylation events of ge.
</prevsent>
<prevsent>the two bacteria track tasks represent an independent perspective relatively far from other tasks in terms of their target information.
</prevsent>
</prevsection>
<citsent citstr=" W11-1816 ">
bionlp-st 2011 includes three supporting tasks designed to assist in primary the extraction tasks.other supporting resources made available to participants are presented in (stenetorp et al, 2011).<papid> W11-1816 </papid></citsent>
<aftsection>
<nextsent>3.1 protein coreference task (co).
</nextsent>
<nextsent>the co task (nguyen et al, 2011) concerns the recognition of co references to protein references.
</nextsent>
<nextsent>it is motivated from finding from bionlp-st 2009 result analysis: coreference structures in biomedical text hinder the extraction results of fine-grained ie systems.
</nextsent>
<nextsent>while finding connections between event triggers and protein references is major part of event extraction, it becomes much harder if one is replaced with co referencing expression.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5079">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> supporting tasks.  </section>
<citcontext>
<prevsection>
<prevsent>while finding connections between event triggers and protein references is major part of event extraction, it becomes much harder if one is replaced with co referencing expression.
</prevsent>
<prevsent>the co task seeks to address this problem.
</prevsent>
</prevsection>
<citsent citstr=" I05-2038 ">
the datasets for the task were produced based on medco annotation (su et al, 2008) and other genia resources (tateisi et al, 2005; <papid> I05-2038 </papid>kim et al, 2008).</citsent>
<aftsection>
<nextsent>event date note sample data 31 aug. 2010 support.
</nextsent>
<nextsent>tasks train.
</nextsent>
<nextsent>data 27 sep. 2010 7 weeks for development test data 15 nov. 2010 4 days for submission submission 19 nov. 2010 evaluation 22 nov. 2010 main tasks train.
</nextsent>
<nextsent>data 1 dec. 2010 3 months for development test data 1 mar. 2011 9 days for submission submission 10 mar. 2011 extended from 8 mar. evaluation 11 mar. 2011 extended from 10 mar. table 2: schedule of bionlp-st 2011 3.2 entity relations task (rel).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5084">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> supporting tasks.  </section>
<citcontext>
<prevsection>
<prevsent>the task is motivated by specific challenges: the identification of the components of proteins in text is relevant e.g. to the recognition ofsite arguments (cf.
</prevsent>
<prevsent>ge, epi and id tasks), and relations between proteins and their complexes relevant to any task involving them.
</prevsent>
</prevsection>
<citsent citstr=" S10-1006 ">
rel setup is informed by recent semantic relation tasks (hendrickxet al, 2010).<papid> S10-1006 </papid></citsent>
<aftsection>
<nextsent>the task data, consisting of new annotations forge data, extends previously introduced resource (pyysalo et al, 2009; <papid> W09-1301 </papid>ohta et al, 2010<papid> W10-1903 </papid>a).</nextsent>
<nextsent>3.3 gene renaming task (ren).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5085">
<title id=" W11-1801.xml">overview of bionlp shared task 2011 </title>
<section> supporting tasks.  </section>
<citcontext>
<prevsection>
<prevsent>ge, epi and id tasks), and relations between proteins and their complexes relevant to any task involving them.
</prevsent>
<prevsent>rel setup is informed by recent semantic relation tasks (hendrickxet al, 2010).<papid> S10-1006 </papid></prevsent>
</prevsection>
<citsent citstr=" W09-1301 ">
the task data, consisting of new annotations forge data, extends previously introduced resource (pyysalo et al, 2009; <papid> W09-1301 </papid>ohta et al, 2010<papid> W10-1903 </papid>a).</citsent>
<aftsection>
<nextsent>3.3 gene renaming task (ren).
</nextsent>
<nextsent>the ren task (jourde et al, 2011) <papid> W11-1810 </papid>objective is to extract renaming pairs of bacillus subtilis gene/proteinnames from pubmed abstracts, motivated by discrepancies between nomenclature databases that interfere with search and complicate normalization.</nextsent>
<nextsent>ren relations partially overlap several concepts:explicit renaming mentions, synonymy, and renaming deduced from biological proof.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5092">
<title id=" W11-0147.xml">collecting semantic data from mechanical turk for a lexical knowledge resource in a text to picture generating system </title>
<section> data collection from amazons mechanical turk.  </section>
<citcontext>
<prevsection>
<prevsent>the next approach for filtering the raw data was finding association measures of target-response pairs using googles 1-trillion 5-gram web corpus (ldc2006t13), by counting the frequency of each target and response word in unigram and bigram portions of the corpus and then the number of times the two words co-occur within +/- 4-word window in the 5-gram portion of the corpus.
</prevsent>
<prevsent>we also computed the sentential co-occurrences of each target-response pair (i.e. the number of sentences in which the target or the response words appear and the number of sentences in which both words occur together) on the english gigaword corpus (ldc2007t07) which is 1 billion word corpus of articles marked up from english press texts (mainly the new york times).
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
based on these counts, we used log-likelihood and log-odds ratio (dunning, 1993) <papid> J93-1003 </papid>to compute the association between the two words.</citsent>
<aftsection>
<nextsent>4.3 discussion and evaluation of automatic filtaration techniques.
</nextsent>
<nextsent>the collected responses of each amt task were ranked separately by each of the above similarity and association measures.
</nextsent>
<nextsent>we classify the ranked responses into keep?
</nextsent>
<nextsent>(higher-scoring) and reject?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5093">
<title id=" W11-1506.xml">historical event extraction from text </title>
<section> historical event extraction.  </section>
<citcontext>
<prevsection>
<prevsent>8 http://www.let.rug.nl/vannoord/alp/alpino/ 9 for word sense disambiguation the ukb system.
</prevsent>
<prevsent>(http://ixa2.si.ehu.es/ukb/) was used.
</prevsent>
</prevsection>
<citsent citstr=" E09-1005 ">
for more information the reader is referred to agirre &amp; soroa (2009).<papid> E09-1005 </papid></citsent>
<aftsection>
<nextsent>10 for more information see vossen et al (2008b)..
</nextsent>
<nextsent>11 for more information see kyoto deliverable 5.4 at.
</nextsent>
<nextsent>http://www.kyoto-project.eu/.
</nextsent>
<nextsent>12 see tools at http://www.kyoto-project.eu/..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5094">
<title id=" W11-0224.xml">meds tract  the next generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it was built on the remains of meds tract, previously created resource that included bio-relation server and an acronymdatabase.
</prevsent>
<prevsent>medstractplus uses simple and scalable natural language processing modules to structure text and is designed with re usability and extendibility in mind.
</prevsent>
</prevsection>
<citsent citstr=" W02-0312 ">
in the late 1990s, the meds tract project (pustejovskyet al, 2002) <papid> W02-0312 </papid>set out to use common natural language processing techniques and employ them to access relational information in medline abstracts.</citsent>
<aftsection>
<nextsent>meds tract used set of pipe lined python scripts where all scripts operated on in-memory objects.
</nextsent>
<nextsent>the output of this pipeline was set of relations, indexed by the pubmed identifier of the abstract inwhich they appeared.
</nextsent>
<nextsent>a perl script proposed potential acronyms using set of regular expressions on named entities in medline abstracts.
</nextsent>
<nextsent>both relations and acronyms were fed into an oracle database, where access to these data sources was enabled by set of perl cgi scripts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5095">
<title id=" W10-4150.xml">a chinese lpcfg parser with hybrid character information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>all these research adopted the same models which are also used in english parser ? the models based on the words.however, there is big difference between english and chinese: the expressing unit in english is word, while character is the smallest unit inchinese.
</prevsent>
<prevsent>due to difficulties of word segmentation, especially for different segmenting criteria, many researchers explored parsing chinese based on characters.
</prevsent>
</prevsection>
<citsent citstr=" W03-1025 ">
the parser of (luo, 2003) <papid> W03-1025 </papid>received sentence as input and conducted word segmentation and syntactic parsing at the same time, but they did not utilize the character information in generating subtree; (zhao, 2009)<papid> E09-1100 </papid>s dependency parsing tree totally abandoned the word concept,so the dependency relations are the relations between characters.we combine both word and character information to gain better performance of parsing.</citsent>
<aftsection>
<nextsent>although the criteria of segmentation are difficult to be unified, different criteria conflict only within the phrases which have little influence on the structure between phrases.
</nextsent>
<nextsent>so we still use word as our basic unit of parsing.
</nextsent>
<nextsent>although word has been proved to be effective in head-drivenparser (collins,1999), the data of word dependence is very sparse.
</nextsent>
<nextsent>while it is worthy to note that words with similar concept always share the same characters in chinese.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5096">
<title id=" W10-4150.xml">a chinese lpcfg parser with hybrid character information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>all these research adopted the same models which are also used in english parser ? the models based on the words.however, there is big difference between english and chinese: the expressing unit in english is word, while character is the smallest unit inchinese.
</prevsent>
<prevsent>due to difficulties of word segmentation, especially for different segmenting criteria, many researchers explored parsing chinese based on characters.
</prevsent>
</prevsection>
<citsent citstr=" E09-1100 ">
the parser of (luo, 2003) <papid> W03-1025 </papid>received sentence as input and conducted word segmentation and syntactic parsing at the same time, but they did not utilize the character information in generating subtree; (zhao, 2009)<papid> E09-1100 </papid>s dependency parsing tree totally abandoned the word concept,so the dependency relations are the relations between characters.we combine both word and character information to gain better performance of parsing.</citsent>
<aftsection>
<nextsent>although the criteria of segmentation are difficult to be unified, different criteria conflict only within the phrases which have little influence on the structure between phrases.
</nextsent>
<nextsent>so we still use word as our basic unit of parsing.
</nextsent>
<nextsent>although word has been proved to be effective in head-drivenparser (collins,1999), the data of word dependence is very sparse.
</nextsent>
<nextsent>while it is worthy to note that words with similar concept always share the same characters in chinese.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5097">
<title id=" W10-4150.xml">a chinese lpcfg parser with hybrid character information </title>
<section> lexical pcfg model.  </section>
<citcontext>
<prevsection>
<prevsent>these constraints will force the model to reflect characteristic of training data.
</prevsent>
<prevsent>with the feature function, maximum entropy can exploit kinds of features flexibly, some of which are very important to improve the performance of tasks at hand.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
me model has been applied successfully in many tasks, such as parser (charniak, 2000; <papid> A00-2018 </papid>luo, 2003), <papid> W03-1025 </papid>pos tagging (ratnaparkhi,1996), etc.in our experiment, we use maxent toolkit de velopped by zhang (zhang, 2004), which uses the lbfgs algorithm for parameter estimation.</citsent>
<aftsection>
<nextsent>details of the model and toolkit can be seen in (berger, 1996; zhang, 2004).our features consist of four parts: basic features, character features, context features and overlapping features of character and context.
</nextsent>
<nextsent>basic features are traditional lpcfg features, including head word, head tag and the label.
</nextsent>
<nextsent>we extract the first and last characters of word as the character features, of course for single character word the first and last character are the same.
</nextsent>
<nextsent>context features are defined as the previous and following pos tags of the current subtree, and these features utilize the information outside of the subtree very well without increasing the complexity of parsing decoder.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5099">
<title id=" W10-4150.xml">a chinese lpcfg parser with hybrid character information </title>
<section> experiment result and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>the performance of the rerank model is lightly lower than that of the single model.
</prevsent>
<prevsent>the most likely reason is that the features we count on are far from enough, and the informative features proved to be useful in (charniak and johnson, 2003) are not yet included in our discriminative ranker.
</prevsent>
</prevsection>
<citsent citstr=" P08-1067 ">
besides, the rank model we used is simple perceptron learner, more delic ated model, such as me model used in (charniak and johnson, 1more details can be found in (charniak and johnson,2003; huang, 2008).<papid> P08-1067 </papid></citsent>
<aftsection>
<nextsent>the features we used include parent rule, right branch, rule, heads, wproj described in (char niak and johnson, 2003).
</nextsent>
<nextsent>table 3: results of different features with no limit sentence length.
</nextsent>
<nextsent>feature set lr lp cb 0cb 2cb basic 80.19 79.61 79.90 1.20 56.10 83.49 +ch 81.91 81.38.
</nextsent>
<nextsent>81.65 1.10 58.34 84.95 +cont 85.53 85.34 85.44 0.83 65.62 88.86 +ch + cont 86.17 85.94 86.06 0.80 66.61 89.62 +ch + cont + ol 86.34 86.13 86.24 0.79 66.65 89.81 +ch + cont + ol + cwd 86.47 86.26 86.37 0.78 66.73 89.87 +ch + cont + ol + cwd + cm 87.03 86.77 86.90 0.75 67.06 90.36 +ch + cont + ol + cwd + cm + hpl 87.20 86.94 87.07 0.74 67.43 90.40 ch=character feature, cont=context feature ol=overlap feature, cwd=coordinate word dependence cm=corpus modifying, hpl= head position label 2003), might improve the result.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5100">
<title id=" W11-0111.xml">acquiring  entailment pairs across languages and domains a data analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>textual entailment is defined as relation between two natural language utterances (a premise and hypothesis h) that holds if human reading would infer that is most likely true?.
</prevsent>
<prevsent>see, e.g., the acl challenge paper?
</prevsent>
</prevsection>
<citsent citstr=" P10-1122 ">
by sammons et al (2010) <papid> P10-1122 </papid>for further details.</citsent>
<aftsection>
<nextsent>the successive te workshops that have taken place yearly since 2005 have produced annotation for english which amount to total of several thousand entailing premise-hypothesis sentence pairs, which we will call entailment pairs: (1) p: swedish bond yields end 21 basis points higher.
</nextsent>
<nextsent>h: swedish bond yields rose further.
</nextsent>
<nextsent>from the machine learning perspective assumed by many approaches to te, this is very small number of examples, given the complex nature of entailment.
</nextsent>
<nextsent>given the problems of manual annotation, therefore, burger and ferro (2005) <papid> W05-1209 </papid>proposed to take advantage of the structural properties of particular type of discourse ? namely newspaper articles ? to automatically harvest entailment pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5101">
<title id=" W11-0111.xml">acquiring  entailment pairs across languages and domains a data analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>h: swedish bond yields rose further.
</prevsent>
<prevsent>from the machine learning perspective assumed by many approaches to te, this is very small number of examples, given the complex nature of entailment.
</prevsent>
</prevsection>
<citsent citstr=" W05-1209 ">
given the problems of manual annotation, therefore, burger and ferro (2005) <papid> W05-1209 </papid>proposed to take advantage of the structural properties of particular type of discourse ? namely newspaper articles ? to automatically harvest entailment pairs.</citsent>
<aftsection>
<nextsent>they proposed to pair the title of each article with its first sentence, interpreting the first sentence as premise and the title as hypothesis.
</nextsent>
<nextsent>their results were mixed, with an average of 50% actual entailment pairs among all pairs constructed in this manner.
</nextsent>
<nextsent>svms which identified entailment-friendly?
</nextsent>
<nextsent>documents based on their bags of words lead to an accuracy of 77%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5102">
<title id=" W11-0111.xml">acquiring  entailment pairs across languages and domains a data analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>newsagency corpora (like reuters) are domain 102 corpus d( ? | dewac) words with highest (w)/q(w) reuters 0.98 hndler (trader), brse (exchange), prozent (per cent), erklrte (stated)stuttz 0.93 dm (german mark), prozent (per cent), millionen (millions), geschfts jahr (fiscal year), milli arden (billions) die zeit 0.64 heit (means), wei?
</prevsent>
<prevsent>(knows), lt (leaves/lets) table 8: exp. 2: domain specificity (kl distance from dewac); typical content words specific.
</prevsent>
</prevsection>
<citsent citstr=" E06-1028 ">
we quantify this intuition with an approach by ciaramita and baroni (2006), <papid> E06-1028 </papid>who propose to model the representativeness of web-crawled corpora as the kl divergence between their laplace smoothed unigram distribution and that of reference corpus, (w ? are vocabulary words): d(p,q) = ? ww (w) log (w)q(w) (4) we use the dewac german web corpus (baroni et al, 2009) as reference, making the idealizing assumption that it is representative for the german language.</citsent>
<aftsection>
<nextsent>we interpret large distance from dewac as domain specificity.
</nextsent>
<nextsent>the results in table 8 bear out our hypothesis: die zeit is less domain specific than stuttz, which in turn is less specific than reuters.
</nextsent>
<nextsent>the table also lists the content words (nouns/verbs) that are most typical for each corpus, i.e., which have the highest value of (w)/q(w).
</nextsent>
<nextsent>the lists bolster the interpretation that reuters and stuttz concentrate on the economical domain, while the typical terms of die zeit show an argumentative style, but no obvious domain bias.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5103">
<title id=" W11-0111.xml">acquiring  entailment pairs across languages and domains a data analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given these limitation of the present headline-based approach, other approaches that are more generally applicable may need to be explored.
</prevsent>
<prevsent>entailment pairs have for example been extracted from wikipedia (bos et al, 2009).
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
another direction is to build on methods to extract paraphrases from comparable corpora (barzilay and lee, 2003), <papid> N03-1003 </papid>and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction.</citsent>
<aftsection>
<nextsent>103 acknowledgments.
</nextsent>
<nextsent>the first author would like to acknowledge the support of wise scholarship granted by daad (german academic exchange service).
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5104">
<title id=" W11-0209.xml">building frame based corpus on the basis of onto logical domain knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling (srl) is process that, for each predicate in sentence, indicates what semantic relations hold among the predicate and other sentence constituents that express the participants in the event(such as who and where).
</prevsent>
<prevsent>the relations are described by using list of pre-defined possible semantic roles for that predicate (or class of predi cates).
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
recently, large corpora have been manually annotated with semantic roles in framenet (fill more et al, 2001) and propbank (palmer et al,2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>with the advent of resources, srl has be come well-defined task with substantial body of work and comparative evaluation.
</nextsent>
<nextsent>most of the work has been trained and evaluated on newswire text (see (ma`rquez et al, 2008)).
</nextsent>
<nextsent>biomedical text considerably differs from the newswire text, both in the style of the written text and the predicates involved.
</nextsent>
<nextsent>predicates in newswire text are typically verbs, biomedical text often prefersnominalizations, gerunds, and relational nouns (kilicoglu et al, 2010).<papid> W10-1906 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5105">
<title id=" W11-0209.xml">building frame based corpus on the basis of onto logical domain knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the work has been trained and evaluated on newswire text (see (ma`rquez et al, 2008)).
</prevsent>
<prevsent>biomedical text considerably differs from the newswire text, both in the style of the written text and the predicates involved.
</prevsent>
</prevsection>
<citsent citstr=" W10-1906 ">
predicates in newswire text are typically verbs, biomedical text often prefersnominalizations, gerunds, and relational nouns (kilicoglu et al, 2010).<papid> W10-1906 </papid></citsent>
<aftsection>
<nextsent>predicates like endocytosis, exocytosis and trans locate, though common in biomedical text, are absent from both the framenet and propbank data (bethard et al, 2008).
</nextsent>
<nextsent>predicates like block, generate and transform, have been used in biomedical documents with different semantic senses and require different number of semantic roles compared to framenet (tan, 2010) and propbank (wattarujeekrit et al, 2004).
</nextsent>
<nextsent>the development of srl systems for the biomedical domain is frustrated by the lack of large domain-specific corpora that are labeled with semantic roles.
</nextsent>
<nextsent>the projects, pasbio (wattarujeekrit et al., 2004), bioprop (tsai et al, 2006) <papid> W06-3308 </papid>and bio framenet (dolbey et al, 2006), have made efforts on building propbank-like and framenet like corpora for processing biomedical text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5106">
<title id=" W11-0209.xml">building frame based corpus on the basis of onto logical domain knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>predicates like block, generate and transform, have been used in biomedical documents with different semantic senses and require different number of semantic roles compared to framenet (tan, 2010) and propbank (wattarujeekrit et al, 2004).
</prevsent>
<prevsent>the development of srl systems for the biomedical domain is frustrated by the lack of large domain-specific corpora that are labeled with semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" W06-3308 ">
the projects, pasbio (wattarujeekrit et al., 2004), bioprop (tsai et al, 2006) <papid> W06-3308 </papid>and bio framenet (dolbey et al, 2006), have made efforts on building propbank-like and framenet like corpora for processing biomedical text.</citsent>
<aftsection>
<nextsent>up until recently, these corpora are relatively small.further, no general methodology exists to support domain-specific corpus construction.
</nextsent>
<nextsent>the difficulties include, how to discover and define 74 semantic frames together with associated semantic roles within the domain?
</nextsent>
<nextsent>how to collect and group domain-specific predicates to each semantic frame and how to select example sentences from publication databases, such as the pubmed/medline database containing over 20 million articles?
</nextsent>
<nextsent>in this paper, we propose that building frame-based lexicon for the domain can be strongly instructed by domain knowledge provided by ontologies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5107">
<title id=" W11-0209.xml">building frame based corpus on the basis of onto logical domain knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the collection and selection of example sen-.
</prevsent>
<prevsent>tences can be based on knowledge-based search engine for biomedical text.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
the propbank project is to add semantic layer on the penn treebank (marcus et al, 1994).<papid> H94-1020 </papid></citsent>
<aftsection>
<nextsent>for each unique verb sense, set of semantic roles is defined at its accompanying syntactic realizations.
</nextsent>
<nextsent>the verbnet project (kipper et al, 2000) systematically creates english verb entries in lexicon with syntactic and semantic information, referring to levin verb classes.
</nextsent>
<nextsent>it made efforts to classify individual verbs in propbank into verbnet classes, based on patterns of usage (kingsbury and kipper, 2003).<papid> W03-0910 </papid></nextsent>
<nextsent>the framenet project collects and analyzes the corpus (the british national corpus) attestations of target words with semantic overlapping.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5108">
<title id=" W11-0209.xml">building frame based corpus on the basis of onto logical domain knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for each unique verb sense, set of semantic roles is defined at its accompanying syntactic realizations.
</prevsent>
<prevsent>the verbnet project (kipper et al, 2000) systematically creates english verb entries in lexicon with syntactic and semantic information, referring to levin verb classes.
</prevsent>
</prevsection>
<citsent citstr=" W03-0910 ">
it made efforts to classify individual verbs in propbank into verbnet classes, based on patterns of usage (kingsbury and kipper, 2003).<papid> W03-0910 </papid></citsent>
<aftsection>
<nextsent>the framenet project collects and analyzes the corpus (the british national corpus) attestations of target words with semantic overlapping.
</nextsent>
<nextsent>the attestations are divided into semantic groups, noting especially the semantic roles of each target words, and then these small groups are combined into frames.
</nextsent>
<nextsent>ontologies have been put under the spotlight for providing the framework for semantic representation of textual information, and thus basis for text mining systems (spasic et al, 2005; ash burner et al., 2008).
</nextsent>
<nextsent>up to recently, tm systems mainly use ontologies as terminologies to recognize biomedical terms, by mapping terms occurring in text to concepts in ontologies, or use ontologies to guide and constrain analysis of nlp results, by populating ontologies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5109">
<title id=" W11-1714.xml">automatic sentiment classification of product reviews using maximal phrases based analysis </title>
<section> phrase extraction.  </section>
<citcontext>
<prevsection>
<prevsent>orthography, ? and only ? root.
</prevsent>
<prevsent>another ? interesting?
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
relatedworkisthatofturney?(2002).<papid> P02-1053 </papid>heusesan?</citsent>
<aftsection>
<nextsent>unsupervised ? learning ? algorithm ? to ? classify ? a? reviewasrecommendedornotrecommended.the?
</nextsent>
<nextsent>algorithm ? extracts ? phrases ? from ? given ? review,?
</nextsent>
<nextsent>anddeterminestheirpointwisemutualinformation?
</nextsent>
<nextsent>withthewords excellent?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5111">
<title id=" W11-1009.xml">a dependency based statistical translation model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the decoder uses the same phrase tables produced by pbt system for looking up translations of single words or of partial sub-trees.
</prevsent>
<prevsent>a mathematical model is presented and experimental results are discussed.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
several efforts are being made to incorporate syntactic analysis into phrase-base statistical translation (pbt) (och 2002; koehn et. al. 2003), <papid> N03-1017 </papid>which represents the state of the art in terms of robustness in modeling local word reordering and efficiency in decoding.</citsent>
<aftsection>
<nextsent>syntactic analysis is meant to improve some of the pitfalls of pbt: translation options selection: candidate phrases for translation are selected as consecutive ngrams.
</nextsent>
<nextsent>this may miss to consider certain syntactic phrases if their component words are far apart.
</nextsent>
<nextsent>phrase reordering: especially for languages with different word order, e.g. subject-verb object (svo) and subject-object-verb (svo) languages, long distance reordering is problem.
</nextsent>
<nextsent>this has been addressed with distance based distortion model (och 2002; koehn et al 2003), <papid> N03-1017 </papid>lexicalized phrase reordering (tillmann, 2004; koehn, et.al., 2005; al-onaizan and papineni, 2006), by hierarchical phrase reordering model (galley and manning, 2008) <papid> D08-1089 </papid>or by reordering the nodes in dependency tree (xu et al., 2009) <papid> N09-1028 </papid>movement of translations of fertile words: word with fertility higher than one can be translated into several words that do not occur con secutively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5113">
<title id=" W11-1009.xml">a dependency based statistical translation model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this may miss to consider certain syntactic phrases if their component words are far apart.
</prevsent>
<prevsent>phrase reordering: especially for languages with different word order, e.g. subject-verb object (svo) and subject-object-verb (svo) languages, long distance reordering is problem.
</prevsent>
</prevsection>
<citsent citstr=" D08-1089 ">
this has been addressed with distance based distortion model (och 2002; koehn et al 2003), <papid> N03-1017 </papid>lexicalized phrase reordering (tillmann, 2004; koehn, et.al., 2005; al-onaizan and papineni, 2006), by hierarchical phrase reordering model (galley and manning, 2008) <papid> D08-1089 </papid>or by reordering the nodes in dependency tree (xu et al., 2009) <papid> N09-1028 </papid>movement of translations of fertile words: word with fertility higher than one can be translated into several words that do not occur con secutively.</citsent>
<aftsection>
<nextsent>for example, the italian sentence lui partir?
</nextsent>
<nextsent>domani?
</nextsent>
<nextsent>translates into german as er wird morgen abreisen?.
</nextsent>
<nextsent>the italian word partir??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5114">
<title id=" W11-1009.xml">a dependency based statistical translation model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this may miss to consider certain syntactic phrases if their component words are far apart.
</prevsent>
<prevsent>phrase reordering: especially for languages with different word order, e.g. subject-verb object (svo) and subject-object-verb (svo) languages, long distance reordering is problem.
</prevsent>
</prevsection>
<citsent citstr=" N09-1028 ">
this has been addressed with distance based distortion model (och 2002; koehn et al 2003), <papid> N03-1017 </papid>lexicalized phrase reordering (tillmann, 2004; koehn, et.al., 2005; al-onaizan and papineni, 2006), by hierarchical phrase reordering model (galley and manning, 2008) <papid> D08-1089 </papid>or by reordering the nodes in dependency tree (xu et al., 2009) <papid> N09-1028 </papid>movement of translations of fertile words: word with fertility higher than one can be translated into several words that do not occur con secutively.</citsent>
<aftsection>
<nextsent>for example, the italian sentence lui partir?
</nextsent>
<nextsent>domani?
</nextsent>
<nextsent>translates into german as er wird morgen abreisen?.
</nextsent>
<nextsent>the italian word partir??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5116">
<title id=" W11-1009.xml">a dependency based statistical translation model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the dependency tree of the source language sentence allows identifying syntactically meaningful phrases as translation options, instead of grams.
</prevsent>
<prevsent>however these phrases are then still looked up in phrase translation table (pt) quite similarly to pbt.
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
thus we avoid the sparseness problem that other methods based on tree lets suffer (quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>reordering of phrases is carried out traversing the dependency tree and selecting as options phrases that are children of each head.
</nextsent>
<nextsent>hence far away but logically connected portion of phrase can be included in the reordering.
</nextsent>
<nextsent>phrase combination is performed by combining the translations of node with those of its head.
</nextsent>
<nextsent>hence only phrases that have syntactic relation are connected.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5117">
<title id=" W11-1009.xml">a dependency based statistical translation model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the language model (lm) is still consulted to ensure that the combination is proper, and the overall score of each translation is carried along.
</prevsent>
<prevsent>when all the links in the parse tree have been reduced, the root node contains candidate translations for the whole sentences alternative visit orderings of the tree may produce different translations so the final translation is the one with the highest score.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
some of the benefits of our approach include: 1) reordering is based on syntactic phrases rather than arbitrary chunks 2) computing the future cost estimation can be avoided, since the risk of choosing an easier gram is mitigated by the fact that phrases are chosen according to the dependency tree 3) since we are translating from tree to string, we can directly exploit the standard phrase tables produced by pbt tools such as giza++ (och and ney, 2000) <papid> P00-1056 </papid>and moses (koehn, 2007) 4) integration with the parser: decoding can be performed incrementally while dependency shift/reduce parser builds the parse tree (at tardi, 2006).</citsent>
<aftsection>
<nextsent>we describe in more detail the approach by presenting simple example.
</nextsent>
<nextsent>the translation of an input sentence is generated by reducing the dependency tree one link at time, i.e. merging one node with its parent and combining their translations, until single node remains.
</nextsent>
<nextsent>links must be chosen in an order that preserves the connectivity of the dependency tree.
</nextsent>
<nextsent>since there is one-to-one correspondence between links and nodes (i.e. the link between node and its head), we can use any ordering that corresponds to topological ordering of the nodes of the tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5118">
<title id=" W11-1009.xml">a dependency based statistical translation model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>objects cannot be created from editing field codes.
</prevsent>
<prevsent>p(f, e) = ppt(str(f), e) if str(f) pt str(f) is the sentence at the leaves of node ri plm is the language model probability ppt is the phrase table probability
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
yamada and knight (2001) <papid> P01-1067 </papid>introduced syntax based translation model that incorporated source language syntactic knowledge within statistical translation.</citsent>
<aftsection>
<nextsent>many similar approaches are based on constituent grammars, among which we mention (chiang, 2005) <papid> P05-1033 </papid>who introduced hierarchical translation models.</nextsent>
<nextsent>the earliest approach based on dependency grammars is the work by ashlawi et al (2000), who developed tree-to-tree translation model, based on middle-out string transduction capable of phrase reordering.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5119">
<title id=" W11-1009.xml">a dependency based statistical translation model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>p(f, e) = ppt(str(f), e) if str(f) pt str(f) is the sentence at the leaves of node ri plm is the language model probability ppt is the phrase table probability
</prevsent>
<prevsent>yamada and knight (2001) <papid> P01-1067 </papid>introduced syntax based translation model that incorporated source language syntactic knowledge within statistical translation.</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
many similar approaches are based on constituent grammars, among which we mention (chiang, 2005) <papid> P05-1033 </papid>who introduced hierarchical translation models.</citsent>
<aftsection>
<nextsent>the earliest approach based on dependency grammars is the work by ashlawi et al (2000), who developed tree-to-tree translation model, based on middle-out string transduction capable of phrase reordering.
</nextsent>
<nextsent>it translated transcribed spoken utterances from english to spanish and from english to japanese.
</nextsent>
<nextsent>improvements were reported over word-for-word baseline.
</nextsent>
<nextsent>ambati (2008) presents survey of other approaches based on dependency trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5120">
<title id=" W11-1009.xml">a dependency based statistical translation model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the number of rules used varies from 27 (for string to dependency system) to 140 (baseline system).
</prevsent>
<prevsent>the performance reached 37.25% for the system with 3 grams, 39.47% for 5-grams.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
marcu and wong (2002) <papid> W02-1018 </papid>propose joint- probability model.</citsent>
<aftsection>
<nextsent>the model establishes correspondence between source phrase and target phrase through some concept.
</nextsent>
<nextsent>the reordering is integrated into the joint probability model with the help of: 3) phrase translation probabilities error!
</nextsent>
<nextsent>objects cannot be created from editing field codes.
</nextsent>
<nextsent>denoting the probability that concept ci generates the translation error!
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5121">
<title id=" W11-1009.xml">a dependency based statistical translation model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>rules are encoded in the form of weighted synchronous grammar and express transformations on the parse trees.
</prevsent>
<prevsent>they experiment also mixing constituency and dependency trees achieving some improve 83 ments in bleu score (27.37%) over baseline system (26.16%).
</prevsent>
</prevsection>
<citsent citstr=" P08-1009 ">
cherry (2008) <papid> P08-1009 </papid>introduces cohesion feature into traditional phrase based decoder.</citsent>
<aftsection>
<nextsent>it is implemented as soft constraint which is based on the dependency syntax of the source language.
</nextsent>
<nextsent>he reports bleu score improvement on french english translation.
</nextsent>
<nextsent>the work by xu et al (2009) <papid> N09-1028 </papid>is the closest to our approach.</nextsent>
<nextsent>they perform preprocessing of the foreign sentences by parsing them with dependency parser and applying set of hand written rules to reorder the children of certain nodes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5123">
<title id=" W11-1009.xml">a dependency based statistical translation model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>translation experiments between english and five non svo asian languages show significant improvements inaccuracy in 4 out of 5 languages.
</prevsent>
<prevsent>with respect to our approach the solution by xu et al does not require any intervention on the translation tools, since the sentences are rewritten before being passed to the processing chain: on the other hand the whole collection has to undergo full parsing with higher performance costs and higher dependency on the accuracy of the parser.
</prevsent>
</prevsection>
<citsent citstr=" N10-1128 ">
dyer and resnik (2010) <papid> N10-1128 </papid>introduce translation model based on synchronous context free grammar (scfg).</citsent>
<aftsection>
<nextsent>in their model, translation examples are stored as context-free forest.
</nextsent>
<nextsent>the process of translation comprise two steps: tree based reordering and phrase transduction.
</nextsent>
<nextsent>while reordering is modeled with the context-free forest, the reordered source is transduced into the target language by finite state transducer (fst).
</nextsent>
<nextsent>the implemented model is trained on those portions of the data which it is able to generate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5125">
<title id=" W11-1009.xml">a dependency based statistical translation model </title>
<section> experimental setup and results.  </section>
<citcontext>
<prevsection>
<prevsent>this helps so that the full auxiliary can be looked up separately from the verb in the phrase table.
</prevsent>
<prevsent>4) decoder that uses the output produced by the reordering algorithm, queries the phrase table and performs beam search on the hypotheses produced according to the suggested reordering.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
moses (koehn et al, 2007) <papid> P07-2045 </papid>is used as baseline phrase-based smt system.</citsent>
<aftsection>
<nextsent>the following tools and data were used in our experiments: 1) the irstlm toolkit (marcello and cettolo, 2007) is used to train 5-gram language mod 84 el with kneser-ney smoothing on set of 4.5 million sentences from the italian wikipedia.
</nextsent>
<nextsent>2) the europarl version 6 corpus, consisting of 1,703,886 sentence pairs, is used for training.
</nextsent>
<nextsent>a tuning set of 2000 sentences from acl wmt 2007 is used to tune the parameters.
</nextsent>
<nextsent>3) the model is trained with lexical reordering.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5126">
<title id=" W11-1303.xml">two multivariate generalizations of pointwise mutual information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mutual information (shannon and weaver, 1949) isa measure of mutual dependence between two random variables.
</prevsent>
<prevsent>the measure ? and more specifically its instantiation for specific outcomes called pointwise mutual information (pmi) ? has proven to be auseful association measure in numerous natural language processing applications.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
since its introduction into the nlp community (church and hanks,1990), <papid> J90-1003 </papid>it has been used in order to tackle or improve upon several nlp problems, including collocation extraction (ibid.)</citsent>
<aftsection>
<nextsent>and word space models (pantel and lin, 2002).
</nextsent>
<nextsent>in its original form, it is restricted to the analysis of two-way co-occurrences.
</nextsent>
<nextsent>nlp problems, however, need not be restricted totwo-way co-occurrences; often, particular problem can be more naturally tackled when formulated as multi-way problem.
</nextsent>
<nextsent>notably, the framework of tensor decomposition, that has recently permeated into the nlp community (turney, 2007; ba roni and lenci, 2010; giesbrecht, 2010; <papid> N10-3005 </papid>van decruys, 2010), analyzes language issues as multi way co-occurrences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5127">
<title id=" W11-1303.xml">two multivariate generalizations of pointwise mutual information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in its original form, it is restricted to the analysis of two-way co-occurrences.
</prevsent>
<prevsent>nlp problems, however, need not be restricted totwo-way co-occurrences; often, particular problem can be more naturally tackled when formulated as multi-way problem.
</prevsent>
</prevsection>
<citsent citstr=" N10-3005 ">
notably, the framework of tensor decomposition, that has recently permeated into the nlp community (turney, 2007; ba roni and lenci, 2010; giesbrecht, 2010; <papid> N10-3005 </papid>van decruys, 2010), analyzes language issues as multi way co-occurrences.</citsent>
<aftsection>
<nextsent>up till now, little attention hasbeen devoted to the weighting of such multi-way cooccurrences (which, for the research cited above, results either in using no weighting at all, or in applying an ad-hoc weighting solution without any theoretical underpinnings).in this paper, we explore two possible generalizations of pointwise mutual information for multi-way co-occurrences from theoretical point of view.
</nextsent>
<nextsent>in section 2, we discuss some relevant related work,mainly in the field of information theory.
</nextsent>
<nextsent>in section 3 the two generalizations of pmi are laid out inmore detail, based on their global multivariate counterparts.
</nextsent>
<nextsent>section 4 then discusses some applications in the light of nlp, while section 5 concludes and hints at some directions for future research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5128">
<title id=" W11-1219.xml">language independent context aware query translation using wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we finally conclude in section 5.
</prevsent>
<prevsent>we discuss the related work of the two stages are involved in our system of language-independent context aware query translation, ? resource building/ collection (dictionaries in our case) ? query formation dictionary building can be broadly classified into two approaches, manual and automatic.
</prevsent>
</prevsection>
<citsent citstr=" W04-2209 ">
at initial stages, various projects like (breen, 2004) <papid> W04-2209 </papid>try to build dictionaries manually, taking lot of time and effort.</citsent>
<aftsection>
<nextsent>though manual approaches perform well,they lag behind when recent vocabulary is considered.
</nextsent>
<nextsent>to reduce the effort involved, automatic extraction of dictionaries has been envisioned.
</nextsent>
<nextsent>the approach followed by (kay and roscheisen, 1999)and (brown et al, 1990) <papid> J90-2002 </papid>were towards statistical machine translation, that can also be applied to dictionary building.</nextsent>
<nextsent>the major requirement for using statistical methods is the availability of bilingual parallel corpora, that again is limited for under resourced languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5129">
<title id=" W11-1219.xml">language independent context aware query translation using wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>though manual approaches perform well,they lag behind when recent vocabulary is considered.
</prevsent>
<prevsent>to reduce the effort involved, automatic extraction of dictionaries has been envisioned.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
the approach followed by (kay and roscheisen, 1999)and (brown et al, 1990) <papid> J90-2002 </papid>were towards statistical machine translation, that can also be applied to dictionary building.</citsent>
<aftsection>
<nextsent>the major requirement for using statistical methods is the availability of bilingual parallel corpora, that again is limited for under resourced languages.
</nextsent>
<nextsent>factors like sentence structure, grammatical differences, availability of language resources and the amount of parallel corpus available further hamper the recall and coverage of the dictionaries extracted.
</nextsent>
<nextsent>after parallel corpora, attempts have been made to construct bilingual dictionaries using various types of corpora like comparable corpus (sadat et al, 2003) <papid> P03-2025 </papid>and noisy parallel corpus (fung and mckeown, 1997).</nextsent>
<nextsent>though there exist various approaches, most of them make use of the language resources.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5130">
<title id=" W11-1219.xml">language independent context aware query translation using wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the major requirement for using statistical methods is the availability of bilingual parallel corpora, that again is limited for under resourced languages.
</prevsent>
<prevsent>factors like sentence structure, grammatical differences, availability of language resources and the amount of parallel corpus available further hamper the recall and coverage of the dictionaries extracted.
</prevsent>
</prevsection>
<citsent citstr=" P03-2025 ">
after parallel corpora, attempts have been made to construct bilingual dictionaries using various types of corpora like comparable corpus (sadat et al, 2003) <papid> P03-2025 </papid>and noisy parallel corpus (fung and mckeown, 1997).</citsent>
<aftsection>
<nextsent>though there exist various approaches, most of them make use of the language resources.
</nextsent>
<nextsent>wikipedia has also been used to mine dictionaries.
</nextsent>
<nextsent>(tyers and pienaar, 2008), (erdmann etal., 2008), (erdmann et al, 2009) have built bilingual dictionaries using wikipedia and language resources.
</nextsent>
<nextsent>we have mined our dictionaries similarly considering the cross lingual links present.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5131">
<title id=" W11-0803.xml">semantic clustering an attempt to identify multiword expressions in bengali </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a number of research activities regarding mwe identification have been carried out in various languages like english, german and many other european languages.
</prevsent>
<prevsent>the statistical co-occurrence measurements such as mutual information (mi) (church and hans, 1990), log-likelihood (dunning, 1993) and salience (kilgarriff and rosenzweig, 2000) have been suggested for identification of mwes.
</prevsent>
</prevsection>
<citsent citstr=" P09-2017 ">
an unsupervised graph-based algorithm to detect the compositionality of mwes has been proposed in (korkontzelos and manandhar 2009).<papid> P09-2017 </papid></citsent>
<aftsection>
<nextsent>in case of indian languages, an approach in compound noun mwe extraction (kunchukuttan and damani, 2008) and classification based approach for noun-verb collocations (venkatapathy and joshi, 2009) have been reported.
</nextsent>
<nextsent>in bengali, the works on automated extraction of mwes are limited in number.
</nextsent>
<nextsent>one method of automatic extraction of noun-verb mwe in bengali (agarwal et al, 2004) has been carried out using significance function.
</nextsent>
<nextsent>in contrast, we have proposed clustering technique to identify bengali mwes using semantic similarity measurement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5132">
<title id=" W11-0803.xml">semantic clustering an attempt to identify multiword expressions in bengali </title>
<section> system framework.  </section>
<citcontext>
<prevsection>
<prevsent>it is interesting to observe that english wordnet becomes very helpful tool to identify bengali mwes.
</prevsent>
<prevsent>wordnet detects maximum mwes correctly at the cut-off of 0.5.
</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
baldwin et al, (2003) <papid> W03-1812 </papid>suggested that wordnet::similarity measure is effective to identify empirical model of multiword expression decomposability.</citsent>
<aftsection>
<nextsent>this is also proved in this experiment as well and even for bengali language.
</nextsent>
<nextsent>there are also candidates with very low value of similarity between their constituents (for example, ganer gajat (earth of song, affectionate of song), yet they are discarded from this experiment because of their low frequency of occurrence in the corpus which could not give any judgment regarding collocation.
</nextsent>
<nextsent>whether such an unexpectedly low frequent high decomposable elements warrant an entry in the lexicon depends on the type of the lexicon being built.
</nextsent>
<nextsent>we hypothesized that sense induction by analyzing synonymous sets can assist the identification of multiword expression.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5133">
<title id=" W11-0814.xml">extracting transfer rules for multiword expressions from parallel corpora </title>
<section> procedure.  </section>
<citcontext>
<prevsection>
<prevsent>rels ? [ lbl h1 , arg0 x1 ] [ lbl h0 , arg0 e0 , arg1 ext , arg2 x1 ] [ arg0 x1 , rstr hr ] ? hcons ?[ harg hr , larg h1 ]?
</prevsent>
<prevsent>out|rels ?[ lbl h0 , arg0 e0 , arg1 ext ]?
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we are using giza++ (och and ney, 2003) <papid> J03-1002 </papid>andanymalign (lardilleux and lepage, 2009) to generate phrase tables from collection of four japanese english parallel corpora and one bilingual dictio nary.</citsent>
<aftsection>
<nextsent>the corpora are the tanaka corpus (2,930,132words: tanaka (2001)), the japanese wordnet corpus (3,355,984 words: bond et al (2010)), the japanese wikipedia corpus (7,949,605),3 and the kyoto university text corpus with nict translations (1,976,071 words: uchimoto et al (2004)).<papid> W04-2208 </papid>the dictionary is edict, japanese english dictionary (3,822,642 words: breen (2004)).</nextsent>
<nextsent>the word totals include both english and japanese words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5134">
<title id=" W11-0814.xml">extracting transfer rules for multiword expressions from parallel corpora </title>
<section> procedure.  </section>
<citcontext>
<prevsection>
<prevsent>out|rels ?[ lbl h0 , arg0 e0 , arg1 ext ]?
</prevsent>
<prevsent>we are using giza++ (och and ney, 2003) <papid> J03-1002 </papid>andanymalign (lardilleux and lepage, 2009) to generate phrase tables from collection of four japanese english parallel corpora and one bilingual dictio nary.</prevsent>
</prevsection>
<citsent citstr=" W04-2208 ">
the corpora are the tanaka corpus (2,930,132words: tanaka (2001)), the japanese wordnet corpus (3,355,984 words: bond et al (2010)), the japanese wikipedia corpus (7,949,605),3 and the kyoto university text corpus with nict translations (1,976,071 words: uchimoto et al (2004)).<papid> W04-2208 </papid>the dictionary is edict, japanese english dictionary (3,822,642 words: breen (2004)).</citsent>
<aftsection>
<nextsent>the word totals include both english and japanese words.
</nextsent>
<nextsent>we divided the corpora into development, test, and training data, and extracted the transfer rules from the training data.
</nextsent>
<nextsent>the training data of the four corpora together with the edict dictionary form aparallel corpus of 20 million words (9.6 million english words and 10.4 million japanese words).
</nextsent>
<nextsent>the japanese text is tokenized and lemmatized with the mecab morphological analyzer (kudo et al, 2004), <papid> W04-3230 </papid>and the english text is tokenized and lemmatized with the free ling analyzer (padr?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5135">
<title id=" W11-0814.xml">extracting transfer rules for multiword expressions from parallel corpora </title>
<section> procedure.  </section>
<citcontext>
<prevsection>
<prevsent>we divided the corpora into development, test, and training data, and extracted the transfer rules from the training data.
</prevsent>
<prevsent>the training data of the four corpora together with the edict dictionary form aparallel corpus of 20 million words (9.6 million english words and 10.4 million japanese words).
</prevsent>
</prevsection>
<citsent citstr=" W04-3230 ">
the japanese text is tokenized and lemmatized with the mecab morphological analyzer (kudo et al, 2004), <papid> W04-3230 </papid>and the english text is tokenized and lemmatized with the free ling analyzer (padr?</citsent>
<aftsection>
<nextsent>et al, 2010), with mwe, quantities, dates and sentence segmentation turned off.
</nextsent>
<nextsent>when applying giza++ and any malign to thelemmatized parallel corpus they produced phrase tables with 10,812,423 and 5,765,262 entries, respectively, running giza++ with the default moses settings and any malign for approximately 16 hours.3the japanese-english bilingual corpus of wikipedias kyoto articles: http://alaginrc.nict.go.jp/wikicorpus/ index_e.html 93we filtered out the entries with an absolute frequency of 1,4 and which had more than 4 words onthe japanese side or more than 3 words on the english side.
</nextsent>
<nextsent>this left us with 6,040,771 moses entries and 3,435,176 any malign entries.
</nextsent>
<nextsent>we then checked against the jacy lexicon on the japanese side and the erg lexicon on the english side to ensure that the source and the target could be parsed/generated by the mt system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5136">
<title id=" W11-0814.xml">extracting transfer rules for multiword expressions from parallel corpora </title>
<section> discussion and further work.  </section>
<citcontext>
<prevsection>
<prevsent>to improve the coverage of rule instances, we need to look at more data, such as that aligned by utiyama and takahashi (2003).neither absolute frequency nor estimated translation probability give reliable thresholds for determining whether rules are good or not.
</prevsent>
<prevsent>currently we are investigating two solutions.
</prevsent>
</prevsection>
<citsent citstr=" P03-1057 ">
one is feedback cleaning, where we investigate the impact of each new rule and discard those that degrade translation quality, following the general idea of imamura et al(2003).<papid> P03-1057 </papid></citsent>
<aftsection>
<nextsent>the second is the more traditional humanin-the loop: presenting each rule and series of relevant translation pairs to human and asking them to judge if it is good or not.
</nextsent>
<nextsent>ultimately, we would liketo extend this approach to crowd source the decisions.
</nextsent>
<nextsent>there are currently two very successful online collaborative japanese-english projects (edict and tatoeba, producing lexical entries and multilingual examples respectively) which indicates that there is large pool of interested knowledgeable people.
</nextsent>
<nextsent>finally, we are working in parallel to qualitatively improve the mwe rules in two ways.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5137">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we conclude with discussion of plans for future work, highlighting the methods potential use in enhancing automatic mt evaluation.
</prevsent>
<prevsent>in this paper, we present our initial steps towards merging the grammar-based and data-driven paraphrasing traditions, highlighting the potential of our approach to enhance the automatic evaluation of machine translation (mt).
</prevsent>
</prevsection>
<citsent citstr=" N06-1058 ">
kauchak and barzilay (2006) <papid> N06-1058 </papid>have shown that creating synthetic reference sentences by substituting synonyms from wordnet into the original reference sentences can increase the number of exact word matches withan mt systems output and yield significant improvements in correlations of bleu (papineni etal., 2002) <papid> P02-1040 </papid>scores with human judgments of translation adequacy.</citsent>
<aftsection>
<nextsent>madnani (2010) has also shown that statistical machine translation technique can be employed in monolingual setting, together with paraphrases acquired using bannard and callison burchs (2005) <papid> P05-1074 </papid>pivot method, in order to enhance the tuning phase of training an mt system by augmenting reference translation with automatic para phrases.</nextsent>
<nextsent>earlier, barzilay and lee (2003) <papid> N03-1003 </papid>and panget al  (2003) <papid> N03-1024 </papid>developed approaches to aligning multiple reference translations in order to extract paraphrases and generate new sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5138">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we conclude with discussion of plans for future work, highlighting the methods potential use in enhancing automatic mt evaluation.
</prevsent>
<prevsent>in this paper, we present our initial steps towards merging the grammar-based and data-driven paraphrasing traditions, highlighting the potential of our approach to enhance the automatic evaluation of machine translation (mt).
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
kauchak and barzilay (2006) <papid> N06-1058 </papid>have shown that creating synthetic reference sentences by substituting synonyms from wordnet into the original reference sentences can increase the number of exact word matches withan mt systems output and yield significant improvements in correlations of bleu (papineni etal., 2002) <papid> P02-1040 </papid>scores with human judgments of translation adequacy.</citsent>
<aftsection>
<nextsent>madnani (2010) has also shown that statistical machine translation technique can be employed in monolingual setting, together with paraphrases acquired using bannard and callison burchs (2005) <papid> P05-1074 </papid>pivot method, in order to enhance the tuning phase of training an mt system by augmenting reference translation with automatic para phrases.</nextsent>
<nextsent>earlier, barzilay and lee (2003) <papid> N03-1003 </papid>and panget al  (2003) <papid> N03-1024 </papid>developed approaches to aligning multiple reference translations in order to extract paraphrases and generate new sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5139">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we present our initial steps towards merging the grammar-based and data-driven paraphrasing traditions, highlighting the potential of our approach to enhance the automatic evaluation of machine translation (mt).
</prevsent>
<prevsent>kauchak and barzilay (2006) <papid> N06-1058 </papid>have shown that creating synthetic reference sentences by substituting synonyms from wordnet into the original reference sentences can increase the number of exact word matches withan mt systems output and yield significant improvements in correlations of bleu (papineni etal., 2002) <papid> P02-1040 </papid>scores with human judgments of translation adequacy.</prevsent>
</prevsection>
<citsent citstr=" P05-1074 ">
madnani (2010) has also shown that statistical machine translation technique can be employed in monolingual setting, together with paraphrases acquired using bannard and callison burchs (2005) <papid> P05-1074 </papid>pivot method, in order to enhance the tuning phase of training an mt system by augmenting reference translation with automatic para phrases.</citsent>
<aftsection>
<nextsent>earlier, barzilay and lee (2003) <papid> N03-1003 </papid>and panget al  (2003) <papid> N03-1024 </papid>developed approaches to aligning multiple reference translations in order to extract paraphrases and generate new sentences.</nextsent>
<nextsent>by starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5140">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>kauchak and barzilay (2006) <papid> N06-1058 </papid>have shown that creating synthetic reference sentences by substituting synonyms from wordnet into the original reference sentences can increase the number of exact word matches withan mt systems output and yield significant improvements in correlations of bleu (papineni etal., 2002) <papid> P02-1040 </papid>scores with human judgments of translation adequacy.</prevsent>
<prevsent>madnani (2010) has also shown that statistical machine translation technique can be employed in monolingual setting, together with paraphrases acquired using bannard and callison burchs (2005) <papid> P05-1074 </papid>pivot method, in order to enhance the tuning phase of training an mt system by augmenting reference translation with automatic para phrases.</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
earlier, barzilay and lee (2003) <papid> N03-1003 </papid>and panget al  (2003) <papid> N03-1024 </papid>developed approaches to aligning multiple reference translations in order to extract paraphrases and generate new sentences.</citsent>
<aftsection>
<nextsent>by starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives.
</nextsent>
<nextsent>however, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar.
</nextsent>
<nextsent>by contrast, grammar-basedparaphrasing methods in the natural language generation tradition (iordanskaja et al , 1991; elhadad et al , 1997; <papid> J97-2001 </papid>langkilde and knight, 1998; <papid> W98-1426 </papid>stede, 1999; langkilde-geary, 2002; velldal et al , 2004; gardent and kow, 2005; <papid> W05-1605 </papid>hogan et al , 2008) <papid> W08-1122 </papid>havethe potential to produce many such grammatical alternatives: in particular, by parsing reference sentence to representation that can be used as the in put to surface realizer, grammar-based paraphrase scan be generated if the realizer supports n-best out put.</nextsent>
<nextsent>to our knowledge though, methods of using agrammar-based surface realizer together with multiple aligned reference sentences to produce synthetic 74source liu lefei says that [in the long term] , in terms of asset al ocation, overseas investment should occupy certain proportion of [an insurance companys overall allocation] . reference liu lefei said that in terms of capital allocation , outbound investment should make up certain ratio of [overall allocations for insurance companies] [in the long run] .paraphrase liu lefei says that [in the long run], in terms of capital allocation, overseas investment should occupy the certain ratio of an [insurance companys overall allocation] table 1: zhao et al (2009) <papid> P09-1094 </papid>similarity example, with italics added to show word-level substitutions, and square brackets added to show phrase location or construction mismatches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5141">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>kauchak and barzilay (2006) <papid> N06-1058 </papid>have shown that creating synthetic reference sentences by substituting synonyms from wordnet into the original reference sentences can increase the number of exact word matches withan mt systems output and yield significant improvements in correlations of bleu (papineni etal., 2002) <papid> P02-1040 </papid>scores with human judgments of translation adequacy.</prevsent>
<prevsent>madnani (2010) has also shown that statistical machine translation technique can be employed in monolingual setting, together with paraphrases acquired using bannard and callison burchs (2005) <papid> P05-1074 </papid>pivot method, in order to enhance the tuning phase of training an mt system by augmenting reference translation with automatic para phrases.</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
earlier, barzilay and lee (2003) <papid> N03-1003 </papid>and panget al  (2003) <papid> N03-1024 </papid>developed approaches to aligning multiple reference translations in order to extract paraphrases and generate new sentences.</citsent>
<aftsection>
<nextsent>by starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives.
</nextsent>
<nextsent>however, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar.
</nextsent>
<nextsent>by contrast, grammar-basedparaphrasing methods in the natural language generation tradition (iordanskaja et al , 1991; elhadad et al , 1997; <papid> J97-2001 </papid>langkilde and knight, 1998; <papid> W98-1426 </papid>stede, 1999; langkilde-geary, 2002; velldal et al , 2004; gardent and kow, 2005; <papid> W05-1605 </papid>hogan et al , 2008) <papid> W08-1122 </papid>havethe potential to produce many such grammatical alternatives: in particular, by parsing reference sentence to representation that can be used as the in put to surface realizer, grammar-based paraphrase scan be generated if the realizer supports n-best out put.</nextsent>
<nextsent>to our knowledge though, methods of using agrammar-based surface realizer together with multiple aligned reference sentences to produce synthetic 74source liu lefei says that [in the long term] , in terms of asset al ocation, overseas investment should occupy certain proportion of [an insurance companys overall allocation] . reference liu lefei said that in terms of capital allocation , outbound investment should make up certain ratio of [overall allocations for insurance companies] [in the long run] .paraphrase liu lefei says that [in the long run], in terms of capital allocation, overseas investment should occupy the certain ratio of an [insurance companys overall allocation] table 1: zhao et al (2009) <papid> P09-1094 </papid>similarity example, with italics added to show word-level substitutions, and square brackets added to show phrase location or construction mismatches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5142">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives.
</prevsent>
<prevsent>however, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar.
</prevsent>
</prevsection>
<citsent citstr=" J97-2001 ">
by contrast, grammar-basedparaphrasing methods in the natural language generation tradition (iordanskaja et al , 1991; elhadad et al , 1997; <papid> J97-2001 </papid>langkilde and knight, 1998; <papid> W98-1426 </papid>stede, 1999; langkilde-geary, 2002; velldal et al , 2004; gardent and kow, 2005; <papid> W05-1605 </papid>hogan et al , 2008) <papid> W08-1122 </papid>havethe potential to produce many such grammatical alternatives: in particular, by parsing reference sentence to representation that can be used as the in put to surface realizer, grammar-based paraphrase scan be generated if the realizer supports n-best out put.</citsent>
<aftsection>
<nextsent>to our knowledge though, methods of using agrammar-based surface realizer together with multiple aligned reference sentences to produce synthetic 74source liu lefei says that [in the long term] , in terms of asset al ocation, overseas investment should occupy certain proportion of [an insurance companys overall allocation] . reference liu lefei said that in terms of capital allocation , outbound investment should make up certain ratio of [overall allocations for insurance companies] [in the long run] .paraphrase liu lefei says that [in the long run], in terms of capital allocation, overseas investment should occupy the certain ratio of an [insurance companys overall allocation] table 1: zhao et al (2009) <papid> P09-1094 </papid>similarity example, with italics added to show word-level substitutions, and square brackets added to show phrase location or construction mismatches.</nextsent>
<nextsent>here, the source sentence (itself reference translation) has been paraphrased to be more like the reference sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5143">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives.
</prevsent>
<prevsent>however, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar.
</prevsent>
</prevsection>
<citsent citstr=" W98-1426 ">
by contrast, grammar-basedparaphrasing methods in the natural language generation tradition (iordanskaja et al , 1991; elhadad et al , 1997; <papid> J97-2001 </papid>langkilde and knight, 1998; <papid> W98-1426 </papid>stede, 1999; langkilde-geary, 2002; velldal et al , 2004; gardent and kow, 2005; <papid> W05-1605 </papid>hogan et al , 2008) <papid> W08-1122 </papid>havethe potential to produce many such grammatical alternatives: in particular, by parsing reference sentence to representation that can be used as the in put to surface realizer, grammar-based paraphrase scan be generated if the realizer supports n-best out put.</citsent>
<aftsection>
<nextsent>to our knowledge though, methods of using agrammar-based surface realizer together with multiple aligned reference sentences to produce synthetic 74source liu lefei says that [in the long term] , in terms of asset al ocation, overseas investment should occupy certain proportion of [an insurance companys overall allocation] . reference liu lefei said that in terms of capital allocation , outbound investment should make up certain ratio of [overall allocations for insurance companies] [in the long run] .paraphrase liu lefei says that [in the long run], in terms of capital allocation, overseas investment should occupy the certain ratio of an [insurance companys overall allocation] table 1: zhao et al (2009) <papid> P09-1094 </papid>similarity example, with italics added to show word-level substitutions, and square brackets added to show phrase location or construction mismatches.</nextsent>
<nextsent>here, the source sentence (itself reference translation) has been paraphrased to be more like the reference sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5144">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives.
</prevsent>
<prevsent>however, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar.
</prevsent>
</prevsection>
<citsent citstr=" W05-1605 ">
by contrast, grammar-basedparaphrasing methods in the natural language generation tradition (iordanskaja et al , 1991; elhadad et al , 1997; <papid> J97-2001 </papid>langkilde and knight, 1998; <papid> W98-1426 </papid>stede, 1999; langkilde-geary, 2002; velldal et al , 2004; gardent and kow, 2005; <papid> W05-1605 </papid>hogan et al , 2008) <papid> W08-1122 </papid>havethe potential to produce many such grammatical alternatives: in particular, by parsing reference sentence to representation that can be used as the in put to surface realizer, grammar-based paraphrase scan be generated if the realizer supports n-best out put.</citsent>
<aftsection>
<nextsent>to our knowledge though, methods of using agrammar-based surface realizer together with multiple aligned reference sentences to produce synthetic 74source liu lefei says that [in the long term] , in terms of asset al ocation, overseas investment should occupy certain proportion of [an insurance companys overall allocation] . reference liu lefei said that in terms of capital allocation , outbound investment should make up certain ratio of [overall allocations for insurance companies] [in the long run] .paraphrase liu lefei says that [in the long run], in terms of capital allocation, overseas investment should occupy the certain ratio of an [insurance companys overall allocation] table 1: zhao et al (2009) <papid> P09-1094 </papid>similarity example, with italics added to show word-level substitutions, and square brackets added to show phrase location or construction mismatches.</nextsent>
<nextsent>here, the source sentence (itself reference translation) has been paraphrased to be more like the reference sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5145">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives.
</prevsent>
<prevsent>however, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar.
</prevsent>
</prevsection>
<citsent citstr=" W08-1122 ">
by contrast, grammar-basedparaphrasing methods in the natural language generation tradition (iordanskaja et al , 1991; elhadad et al , 1997; <papid> J97-2001 </papid>langkilde and knight, 1998; <papid> W98-1426 </papid>stede, 1999; langkilde-geary, 2002; velldal et al , 2004; gardent and kow, 2005; <papid> W05-1605 </papid>hogan et al , 2008) <papid> W08-1122 </papid>havethe potential to produce many such grammatical alternatives: in particular, by parsing reference sentence to representation that can be used as the in put to surface realizer, grammar-based paraphrase scan be generated if the realizer supports n-best out put.</citsent>
<aftsection>
<nextsent>to our knowledge though, methods of using agrammar-based surface realizer together with multiple aligned reference sentences to produce synthetic 74source liu lefei says that [in the long term] , in terms of asset al ocation, overseas investment should occupy certain proportion of [an insurance companys overall allocation] . reference liu lefei said that in terms of capital allocation , outbound investment should make up certain ratio of [overall allocations for insurance companies] [in the long run] .paraphrase liu lefei says that [in the long run], in terms of capital allocation, overseas investment should occupy the certain ratio of an [insurance companys overall allocation] table 1: zhao et al (2009) <papid> P09-1094 </papid>similarity example, with italics added to show word-level substitutions, and square brackets added to show phrase location or construction mismatches.</nextsent>
<nextsent>here, the source sentence (itself reference translation) has been paraphrased to be more like the reference sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5146">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar.
</prevsent>
<prevsent>by contrast, grammar-basedparaphrasing methods in the natural language generation tradition (iordanskaja et al , 1991; elhadad et al , 1997; <papid> J97-2001 </papid>langkilde and knight, 1998; <papid> W98-1426 </papid>stede, 1999; langkilde-geary, 2002; velldal et al , 2004; gardent and kow, 2005; <papid> W05-1605 </papid>hogan et al , 2008) <papid> W08-1122 </papid>havethe potential to produce many such grammatical alternatives: in particular, by parsing reference sentence to representation that can be used as the in put to surface realizer, grammar-based paraphrase scan be generated if the realizer supports n-best out put.</prevsent>
</prevsection>
<citsent citstr=" P09-1094 ">
to our knowledge though, methods of using agrammar-based surface realizer together with multiple aligned reference sentences to produce synthetic 74source liu lefei says that [in the long term] , in terms of asset al ocation, overseas investment should occupy certain proportion of [an insurance companys overall allocation] . reference liu lefei said that in terms of capital allocation , outbound investment should make up certain ratio of [overall allocations for insurance companies] [in the long run] .paraphrase liu lefei says that [in the long run], in terms of capital allocation, overseas investment should occupy the certain ratio of an [insurance companys overall allocation] table 1: zhao et al (2009) <papid> P09-1094 </papid>similarity example, with italics added to show word-level substitutions, and square brackets added to show phrase location or construction mismatches.</citsent>
<aftsection>
<nextsent>here, the source sentence (itself reference translation) has been paraphrased to be more like the reference sentence.
</nextsent>
<nextsent>references have not been investigated.1as an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that zhao et al  (2009) <papid> P09-1094 </papid>use to illustrate the application of their paraphrasing method to similarity detection, shown in table 1.</nextsent>
<nextsent>zhao et al  make use of large paraphrase table, similar to the phrase tables used in statistical mt, inorder to construct paraphrase candidates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5148">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, their system is not capable of generating paraphrase with in the long run at the end of the sentence, nor can it rephrase insurance companys overall allocation as overall allocations for insurance companies, which would seem to require access to more general grammatical knowledge.
</prevsent>
<prevsent>to combine grammar-based paraphrasing with lexical and phrasal alternatives gleaned from multiple reference sentences, our approach takes advan1the task is not unrelated to sentence fusion in multi doc ument summarization (barzilay and mckeown, 2005), except there the goal is to produce single, shorter sentence from multiple related input sentences.
</prevsent>
</prevsection>
<citsent citstr=" W06-1403 ">
tage of the openccg realizers ability to generate from disjunctive logical forms (dlfs), i.e. packed semantic dependency graphs (white, 2004; white,2006<papid> W06-1403 </papid>a; white, 2006<papid> W06-1403 </papid>b; nakatsu and white, 2006<papid> W06-1403 </papid>; <papid> P06-1140 </papid>espinosa et al , 2008; <papid> P08-1022 </papid>white and rajkumar, 2009).<papid> D09-1043 </papid></citsent>
<aftsection>
<nextsent>in principle, semantic dependency graphs offer better starting point for paraphrasing than the syntax trees employed by pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees.
</nextsent>
<nextsent>our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the mt (och and ney, 2003; <papid> J03-1002 </papid>haghighi et al , 2009, <papid> P09-1104 </papid>for example) or textual inference (maccartney et al , 2008, <papid> D08-1084 </papid>inter alia) literature in princi ple.</nextsent>
<nextsent>the alignments are projected onto the logical forms that result from automatically parsing thesesentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5155">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, their system is not capable of generating paraphrase with in the long run at the end of the sentence, nor can it rephrase insurance companys overall allocation as overall allocations for insurance companies, which would seem to require access to more general grammatical knowledge.
</prevsent>
<prevsent>to combine grammar-based paraphrasing with lexical and phrasal alternatives gleaned from multiple reference sentences, our approach takes advan1the task is not unrelated to sentence fusion in multi doc ument summarization (barzilay and mckeown, 2005), except there the goal is to produce single, shorter sentence from multiple related input sentences.
</prevsent>
</prevsection>
<citsent citstr=" P06-1140 ">
tage of the openccg realizers ability to generate from disjunctive logical forms (dlfs), i.e. packed semantic dependency graphs (white, 2004; white,2006<papid> W06-1403 </papid>a; white, 2006<papid> W06-1403 </papid>b; nakatsu and white, 2006<papid> W06-1403 </papid>; <papid> P06-1140 </papid>espinosa et al , 2008; <papid> P08-1022 </papid>white and rajkumar, 2009).<papid> D09-1043 </papid></citsent>
<aftsection>
<nextsent>in principle, semantic dependency graphs offer better starting point for paraphrasing than the syntax trees employed by pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees.
</nextsent>
<nextsent>our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the mt (och and ney, 2003; <papid> J03-1002 </papid>haghighi et al , 2009, <papid> P09-1104 </papid>for example) or textual inference (maccartney et al , 2008, <papid> D08-1084 </papid>inter alia) literature in princi ple.</nextsent>
<nextsent>the alignments are projected onto the logical forms that result from automatically parsing thesesentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5156">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, their system is not capable of generating paraphrase with in the long run at the end of the sentence, nor can it rephrase insurance companys overall allocation as overall allocations for insurance companies, which would seem to require access to more general grammatical knowledge.
</prevsent>
<prevsent>to combine grammar-based paraphrasing with lexical and phrasal alternatives gleaned from multiple reference sentences, our approach takes advan1the task is not unrelated to sentence fusion in multi doc ument summarization (barzilay and mckeown, 2005), except there the goal is to produce single, shorter sentence from multiple related input sentences.
</prevsent>
</prevsection>
<citsent citstr=" P08-1022 ">
tage of the openccg realizers ability to generate from disjunctive logical forms (dlfs), i.e. packed semantic dependency graphs (white, 2004; white,2006<papid> W06-1403 </papid>a; white, 2006<papid> W06-1403 </papid>b; nakatsu and white, 2006<papid> W06-1403 </papid>; <papid> P06-1140 </papid>espinosa et al , 2008; <papid> P08-1022 </papid>white and rajkumar, 2009).<papid> D09-1043 </papid></citsent>
<aftsection>
<nextsent>in principle, semantic dependency graphs offer better starting point for paraphrasing than the syntax trees employed by pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees.
</nextsent>
<nextsent>our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the mt (och and ney, 2003; <papid> J03-1002 </papid>haghighi et al , 2009, <papid> P09-1104 </papid>for example) or textual inference (maccartney et al , 2008, <papid> D08-1084 </papid>inter alia) literature in princi ple.</nextsent>
<nextsent>the alignments are projected onto the logical forms that result from automatically parsing thesesentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5157">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, their system is not capable of generating paraphrase with in the long run at the end of the sentence, nor can it rephrase insurance companys overall allocation as overall allocations for insurance companies, which would seem to require access to more general grammatical knowledge.
</prevsent>
<prevsent>to combine grammar-based paraphrasing with lexical and phrasal alternatives gleaned from multiple reference sentences, our approach takes advan1the task is not unrelated to sentence fusion in multi doc ument summarization (barzilay and mckeown, 2005), except there the goal is to produce single, shorter sentence from multiple related input sentences.
</prevsent>
</prevsection>
<citsent citstr=" D09-1043 ">
tage of the openccg realizers ability to generate from disjunctive logical forms (dlfs), i.e. packed semantic dependency graphs (white, 2004; white,2006<papid> W06-1403 </papid>a; white, 2006<papid> W06-1403 </papid>b; nakatsu and white, 2006<papid> W06-1403 </papid>; <papid> P06-1140 </papid>espinosa et al , 2008; <papid> P08-1022 </papid>white and rajkumar, 2009).<papid> D09-1043 </papid></citsent>
<aftsection>
<nextsent>in principle, semantic dependency graphs offer better starting point for paraphrasing than the syntax trees employed by pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees.
</nextsent>
<nextsent>our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the mt (och and ney, 2003; <papid> J03-1002 </papid>haghighi et al , 2009, <papid> P09-1104 </papid>for example) or textual inference (maccartney et al , 2008, <papid> D08-1084 </papid>inter alia) literature in princi ple.</nextsent>
<nextsent>the alignments are projected onto the logical forms that result from automatically parsing thesesentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5158">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tage of the openccg realizers ability to generate from disjunctive logical forms (dlfs), i.e. packed semantic dependency graphs (white, 2004; white,2006<papid> W06-1403 </papid>a; white, 2006<papid> W06-1403 </papid>b; nakatsu and white, 2006<papid> W06-1403 </papid>; <papid> P06-1140 </papid>espinosa et al , 2008; <papid> P08-1022 </papid>white and rajkumar, 2009).<papid> D09-1043 </papid></prevsent>
<prevsent>in principle, semantic dependency graphs offer better starting point for paraphrasing than the syntax trees employed by pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the mt (och and ney, 2003; <papid> J03-1002 </papid>haghighi et al , 2009, <papid> P09-1104 </papid>for example) or textual inference (maccartney et al , 2008, <papid> D08-1084 </papid>inter alia) literature in princi ple.</citsent>
<aftsection>
<nextsent>the alignments are projected onto the logical forms that result from automatically parsing thesesentences.
</nextsent>
<nextsent>the projected alignments are then converted into phrasal edits for producing dlfs in both directions, where the dis junctions represent alternative choices at the level of semantic dependencies.the resulting dlfs are fed into the openccg re alizer for n-best realization.
</nextsent>
<nextsent>in order to enhance the variety of word and phrase choices in the n-bestlists, pruning strategy is used that encourages lexical diversity.
</nextsent>
<nextsent>after merging, the approach yield san n-best list of paraphrases that contain grammatical alternatives to each original sentence, as well as paraphrases that mix and match content from the pair.the rest of the paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5159">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tage of the openccg realizers ability to generate from disjunctive logical forms (dlfs), i.e. packed semantic dependency graphs (white, 2004; white,2006<papid> W06-1403 </papid>a; white, 2006<papid> W06-1403 </papid>b; nakatsu and white, 2006<papid> W06-1403 </papid>; <papid> P06-1140 </papid>espinosa et al , 2008; <papid> P08-1022 </papid>white and rajkumar, 2009).<papid> D09-1043 </papid></prevsent>
<prevsent>in principle, semantic dependency graphs offer better starting point for paraphrasing than the syntax trees employed by pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees.</prevsent>
</prevsection>
<citsent citstr=" P09-1104 ">
our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the mt (och and ney, 2003; <papid> J03-1002 </papid>haghighi et al , 2009, <papid> P09-1104 </papid>for example) or textual inference (maccartney et al , 2008, <papid> D08-1084 </papid>inter alia) literature in princi ple.</citsent>
<aftsection>
<nextsent>the alignments are projected onto the logical forms that result from automatically parsing thesesentences.
</nextsent>
<nextsent>the projected alignments are then converted into phrasal edits for producing dlfs in both directions, where the dis junctions represent alternative choices at the level of semantic dependencies.the resulting dlfs are fed into the openccg re alizer for n-best realization.
</nextsent>
<nextsent>in order to enhance the variety of word and phrase choices in the n-bestlists, pruning strategy is used that encourages lexical diversity.
</nextsent>
<nextsent>after merging, the approach yield san n-best list of paraphrases that contain grammatical alternatives to each original sentence, as well as paraphrases that mix and match content from the pair.the rest of the paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5160">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tage of the openccg realizers ability to generate from disjunctive logical forms (dlfs), i.e. packed semantic dependency graphs (white, 2004; white,2006<papid> W06-1403 </papid>a; white, 2006<papid> W06-1403 </papid>b; nakatsu and white, 2006<papid> W06-1403 </papid>; <papid> P06-1140 </papid>espinosa et al , 2008; <papid> P08-1022 </papid>white and rajkumar, 2009).<papid> D09-1043 </papid></prevsent>
<prevsent>in principle, semantic dependency graphs offer better starting point for paraphrasing than the syntax trees employed by pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees.</prevsent>
</prevsection>
<citsent citstr=" D08-1084 ">
our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the mt (och and ney, 2003; <papid> J03-1002 </papid>haghighi et al , 2009, <papid> P09-1104 </papid>for example) or textual inference (maccartney et al , 2008, <papid> D08-1084 </papid>inter alia) literature in princi ple.</citsent>
<aftsection>
<nextsent>the alignments are projected onto the logical forms that result from automatically parsing thesesentences.
</nextsent>
<nextsent>the projected alignments are then converted into phrasal edits for producing dlfs in both directions, where the dis junctions represent alternative choices at the level of semantic dependencies.the resulting dlfs are fed into the openccg re alizer for n-best realization.
</nextsent>
<nextsent>in order to enhance the variety of word and phrase choices in the n-bestlists, pruning strategy is used that encourages lexical diversity.
</nextsent>
<nextsent>after merging, the approach yield san n-best list of paraphrases that contain grammatical alternatives to each original sentence, as well as paraphrases that mix and match content from the pair.the rest of the paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5161">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> surface realization with openccg.  </section>
<citcontext>
<prevsection>
<prevsent>finally, section 4 characterizes the recurring errors and concludes with discussion of related and future work.
</prevsent>
<prevsent>openccg is an open source java library for parsing and realization using bald ridges multimodal extensions to ccg (steedman, 2000; baldridge, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P96-1027 ">
in the chart realization tradition (kay, 1996), <papid> P96-1027 </papid>the openccg realizer takes logical forms as input and produces strings by combining signs for lexicalitems.</citsent>
<aftsection>
<nextsent>alternative realizations are scored using integrated n-gram and perceptron models (white and rajkumar, 2009), <papid> D09-1043 </papid>where the latter includes syntactic features from clark and currans (2007) <papid> J07-4004 </papid>normal form model as well as discriminative n-gram features (roark et al , 2004).<papid> P04-1007 </papid></nextsent>
<nextsent>hyper tagging (espinosaet al , 2008), <papid> P08-1022 </papid>or super tagging for surface realization, makes it practical to work with broad coveragegrammars.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5163">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> surface realization with openccg.  </section>
<citcontext>
<prevsection>
<prevsent>openccg is an open source java library for parsing and realization using bald ridges multimodal extensions to ccg (steedman, 2000; baldridge, 2002).
</prevsent>
<prevsent>in the chart realization tradition (kay, 1996), <papid> P96-1027 </papid>the openccg realizer takes logical forms as input and produces strings by combining signs for lexicalitems.</prevsent>
</prevsection>
<citsent citstr=" J07-4004 ">
alternative realizations are scored using integrated n-gram and perceptron models (white and rajkumar, 2009), <papid> D09-1043 </papid>where the latter includes syntactic features from clark and currans (2007) <papid> J07-4004 </papid>normal form model as well as discriminative n-gram features (roark et al , 2004).<papid> P04-1007 </papid></citsent>
<aftsection>
<nextsent>hyper tagging (espinosaet al , 2008), <papid> P08-1022 </papid>or super tagging for surface realization, makes it practical to work with broad coveragegrammars.</nextsent>
<nextsent>for parsing, an implementation of hockenmaier and steed mans (2002) <papid> P02-1043 </papid>generative model isused to select the best parse.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5164">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> surface realization with openccg.  </section>
<citcontext>
<prevsection>
<prevsent>openccg is an open source java library for parsing and realization using bald ridges multimodal extensions to ccg (steedman, 2000; baldridge, 2002).
</prevsent>
<prevsent>in the chart realization tradition (kay, 1996), <papid> P96-1027 </papid>the openccg realizer takes logical forms as input and produces strings by combining signs for lexicalitems.</prevsent>
</prevsection>
<citsent citstr=" P04-1007 ">
alternative realizations are scored using integrated n-gram and perceptron models (white and rajkumar, 2009), <papid> D09-1043 </papid>where the latter includes syntactic features from clark and currans (2007) <papid> J07-4004 </papid>normal form model as well as discriminative n-gram features (roark et al , 2004).<papid> P04-1007 </papid></citsent>
<aftsection>
<nextsent>hyper tagging (espinosaet al , 2008), <papid> P08-1022 </papid>or super tagging for surface realization, makes it practical to work with broad coveragegrammars.</nextsent>
<nextsent>for parsing, an implementation of hockenmaier and steed mans (2002) <papid> P02-1043 </papid>generative model isused to select the best parse.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5166">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> surface realization with openccg.  </section>
<citcontext>
<prevsection>
<prevsent>alternative realizations are scored using integrated n-gram and perceptron models (white and rajkumar, 2009), <papid> D09-1043 </papid>where the latter includes syntactic features from clark and currans (2007) <papid> J07-4004 </papid>normal form model as well as discriminative n-gram features (roark et al , 2004).<papid> P04-1007 </papid></prevsent>
<prevsent>hyper tagging (espinosaet al , 2008), <papid> P08-1022 </papid>or super tagging for surface realization, makes it practical to work with broad coveragegrammars.</prevsent>
</prevsection>
<citsent citstr=" P02-1043 ">
for parsing, an implementation of hockenmaier and steed mans (2002) <papid> P02-1043 </papid>generative model isused to select the best parse.</citsent>
<aftsection>
<nextsent>the grammar is automatically extracted from version of the ccgbank (hockenmaier and steedman, 2007) <papid> J07-3004 </papid>with propbank (palmer et al , 2005) <papid> J05-1004 </papid>roles projected onto it (boxwell and white, 2008).<papid> L08-1039 </papid></nextsent>
<nextsent>a distinctive feature of openccg is the ability to generate from disjunctive logical forms (white, 2006<papid> W06-1403 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5167">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> surface realization with openccg.  </section>
<citcontext>
<prevsection>
<prevsent>hyper tagging (espinosaet al , 2008), <papid> P08-1022 </papid>or super tagging for surface realization, makes it practical to work with broad coveragegrammars.</prevsent>
<prevsent>for parsing, an implementation of hockenmaier and steed mans (2002) <papid> P02-1043 </papid>generative model isused to select the best parse.</prevsent>
</prevsection>
<citsent citstr=" J07-3004 ">
the grammar is automatically extracted from version of the ccgbank (hockenmaier and steedman, 2007) <papid> J07-3004 </papid>with propbank (palmer et al , 2005) <papid> J05-1004 </papid>roles projected onto it (boxwell and white, 2008).<papid> L08-1039 </papid></citsent>
<aftsection>
<nextsent>a distinctive feature of openccg is the ability to generate from disjunctive logical forms (white, 2006<papid> W06-1403 </papid>a).</nextsent>
<nextsent>this capability has many benefits, such as enabling the selection of realizations according to predicted synthesis quality (nakatsu and white,2006<papid> W06-1403 </papid>and avoiding repetition in the output of dialogue system (foster and white, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5168">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> surface realization with openccg.  </section>
<citcontext>
<prevsection>
<prevsent>hyper tagging (espinosaet al , 2008), <papid> P08-1022 </papid>or super tagging for surface realization, makes it practical to work with broad coveragegrammars.</prevsent>
<prevsent>for parsing, an implementation of hockenmaier and steed mans (2002) <papid> P02-1043 </papid>generative model isused to select the best parse.</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
the grammar is automatically extracted from version of the ccgbank (hockenmaier and steedman, 2007) <papid> J07-3004 </papid>with propbank (palmer et al , 2005) <papid> J05-1004 </papid>roles projected onto it (boxwell and white, 2008).<papid> L08-1039 </papid></citsent>
<aftsection>
<nextsent>a distinctive feature of openccg is the ability to generate from disjunctive logical forms (white, 2006<papid> W06-1403 </papid>a).</nextsent>
<nextsent>this capability has many benefits, such as enabling the selection of realizations according to predicted synthesis quality (nakatsu and white,2006<papid> W06-1403 </papid>and avoiding repetition in the output of dialogue system (foster and white, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5169">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> surface realization with openccg.  </section>
<citcontext>
<prevsection>
<prevsent>hyper tagging (espinosaet al , 2008), <papid> P08-1022 </papid>or super tagging for surface realization, makes it practical to work with broad coveragegrammars.</prevsent>
<prevsent>for parsing, an implementation of hockenmaier and steed mans (2002) <papid> P02-1043 </papid>generative model isused to select the best parse.</prevsent>
</prevsection>
<citsent citstr=" L08-1039 ">
the grammar is automatically extracted from version of the ccgbank (hockenmaier and steedman, 2007) <papid> J07-3004 </papid>with propbank (palmer et al , 2005) <papid> J05-1004 </papid>roles projected onto it (boxwell and white, 2008).<papid> L08-1039 </papid></citsent>
<aftsection>
<nextsent>a distinctive feature of openccg is the ability to generate from disjunctive logical forms (white, 2006<papid> W06-1403 </papid>a).</nextsent>
<nextsent>this capability has many benefits, such as enabling the selection of realizations according to predicted synthesis quality (nakatsu and white,2006<papid> W06-1403 </papid>and avoiding repetition in the output of dialogue system (foster and white, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5175">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> surface realization with openccg.  </section>
<citcontext>
<prevsection>
<prevsent>relations lexicalized via by and the possessive, respectively connecting the head (collection or series) with the dependent (for 1http://www.hcrc.ed.ac.uk/comic/2to simplify the exposition, the features specifying infor mati structure and deictic gestures hav been omitted, as have the semantic sorts of the discourse referents.
</prevsent>
<prevsent>figure 1: two imi ar logical forms from the comic system as semantic ependency graphs, together with disjunctive logical form representing their combination as packed semantic dependency graph.
</prevsent>
</prevsection>
<citsent citstr=" P02-1041 ">
76 (baldridge and kruijff, 2002; <papid> P02-1041 </papid>white, 2006<papid> W06-1403 </papid>b), orhldsconstitute the input to the openccg re alizer.3 this graph allows free choice between the domain synonyms collection and series, as indicated by the vertical bar between their respective predications.</citsent>
<aftsection>
<nextsent>the graph also allows free choice between the creator?
</nextsent>
<nextsent>and genowner?
</nextsent>
<nextsent>relations lexicalized via by and the possessive, respectively connecting the head (collection or series) with the dependent (for villeroy and boch); this choice is indicated by an arc between the two dependency relations.
</nextsent>
<nextsent>finally, the determiner feature (detthe) on is indicated as optional, via the question mark.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5179">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> constructing dlfs from aligned.  </section>
<citcontext>
<prevsection>
<prevsent>note that as an alternative, the determiner feature could have been included in the disjunction with the creator?
</prevsent>
<prevsent>relation (though this would have been harder to show graphically); how ever, it is not necessary to do so, as constraints in the lexicalized grammar will ensure that the determiner is not generated together with the possessive.
</prevsent>
</prevsection>
<citsent citstr=" J08-4005 ">
paraphrases to develop our approach, we use the gold-standardalignments in cohn et al (2008) <papid> J08-4005 </papid>paraphrase corpus.</citsent>
<aftsection>
<nextsent>this corpus is constructed from three monolingual sentence-aligned paraphrase sub corpora from differing text genres, with word-level alignments provided by two human annotators.
</nextsent>
<nextsent>we parse each corpus sentence pair using the openccg parser to yield logical form (lf) as semantic dependency graph with the gold-standard alignments projected onto the lf pair.
</nextsent>
<nextsent>disjunctive lfs are then constructed by inspecting the graph structure of each lf in comparison with the other.
</nextsent>
<nextsent>here, an alignment is represented simply as pair n1, n2?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5184">
<title id=" W11-1609.xml">creating disjunctive logical forms from aligned sentences for grammar based paraphrase generation </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>in the case of targeted paraphrases, the generated paraphrases then approximate the process by which automatic translations are evaluated using hter (snover et al , 2006), with human in the loop, asthe closest acceptable paraphrase of reference sentence should correspond to the version of the mthypothesis with minimal changes to make it accept able.
</prevsent>
<prevsent>while in principle we might similarly acquire paraphrase rules using the pivot method, as in mad nanis approach, such rules would be quite noisy, as it is difficult problem to characterize the context sin which words or phrases can be acceptably substituted.
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
thus, our immediate focus will be on generating synthetic references with high precision, relying on grammatical alternations plus contextually acceptable alternatives present in multiple reference translations, given that metrics such as meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>and terp (snover et al , 2010) can now employ paraphrase matching as part of their scoring, complementing what can be done with our methods.</citsent>
<aftsection>
<nextsent>to the extent that we can maintain high precision in generating synthetic reference sentences, we may expect the correlations between automatic metric scores and human judgments to improve as the task of the metrics becomes simpler.
</nextsent>
<nextsent>acknowledgements this work was supported in part by nsf grant number iis-0812297.
</nextsent>
<nextsent>we are also grateful to trevor cohn for help with the paraphrase data.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5185">
<title id=" W11-0207.xml">medical entity recognition a comparaison of semantic and statistical methods </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>protein name extraction has also been studied through several approaches (e.g.
</prevsent>
<prevsent>(liang and shih, 2005; wang, 2007)).
</prevsent>
</prevsection>
<citsent citstr=" L08-1238 ">
(embarek and ferret, 2008) <papid> L08-1238 </papid>proposed an approach relying on linguistic patterns and canonical entities for the extraction of medical entities belonging to five categories: disease, treatment, drug, test, andsymptom.</citsent>
<aftsection>
<nextsent>another kind of approach uses domain specific tools such as metamap (aronson, 2001).
</nextsent>
<nextsent>metamap recognizes and categorizes medical terms by associating them to concepts and semantic types of the umls meta thesaurus and semantic network.(shadow and macdonald, 2003) presented an approach based on metamap for the extraction of medical entities of 20 medical classes from pathologist reports.
</nextsent>
<nextsent>(meystre and haug, 2005) obtained 89.9%recall and 75.5% precision for the extraction of medical problems with an approach based on metamap transfer (mmtx) and the negex negation detection algorithm.in contrast with semantic approaches which require rich domain-knowledge for rule or pattern construction, statistical approaches are more scalable.
</nextsent>
<nextsent>several approaches used classifiers such as decision trees or svms (isozaki and kazawa, 2002).<papid> C02-1054 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5186">
<title id=" W11-0207.xml">medical entity recognition a comparaison of semantic and statistical methods </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>metamap recognizes and categorizes medical terms by associating them to concepts and semantic types of the umls meta thesaurus and semantic network.(shadow and macdonald, 2003) presented an approach based on metamap for the extraction of medical entities of 20 medical classes from pathologist reports.
</prevsent>
<prevsent>(meystre and haug, 2005) obtained 89.9%recall and 75.5% precision for the extraction of medical problems with an approach based on metamap transfer (mmtx) and the negex negation detection algorithm.in contrast with semantic approaches which require rich domain-knowledge for rule or pattern construction, statistical approaches are more scalable.
</prevsent>
</prevsection>
<citsent citstr=" C02-1054 ">
several approaches used classifiers such as decision trees or svms (isozaki and kazawa, 2002).<papid> C02-1054 </papid></citsent>
<aftsection>
<nextsent>markov models-based methods are also frequently used (e.g. hidden markov models, or crfs (he and kayaalp, 2008)).
</nextsent>
<nextsent>however, the performance achieved by such supervised algorithms depends on the availability of well-annotated training corpus and on the selection of relevant feature set.hybrid approaches aim to combine the advantages of semantic and statistical approaches and to bypass some of their weaknesses (e.g. scala bility of rule-based approaches, performance of statistical methods with small training corpora).
</nextsent>
<nextsent>(proux et al, 1998) proposed hybrid approach for the extraction of gene symbols and names.
</nextsent>
<nextsent>the presented system processed unknown words with lexical rules in orderto obtain candidate categories which were then dis ambiguated with markov models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5187">
<title id=" W11-0207.xml">medical entity recognition a comparaison of semantic and statistical methods </title>
<section> bio-crf-h: hybrid method combining se-.  </section>
<citcontext>
<prevsection>
<prevsent>(rind flesch et al, 2000)), especially in genomics for more than decade, e.g. through the bio creative challenge (yeh et al, 2005).
</prevsent>
<prevsent>61section 4 presented experiments in meron english clinical texts.
</prevsent>
</prevsection>
<citsent citstr=" P04-1055 ">
to have complementary view on the performance of our methods, we performed additional experiments on the berkeley corpus (rosario and hearst, 2004) <papid> P04-1055 </papid>of scientific literature abstracts and titles extracted from medline.the original aim of this corpus was to study the extraction of semantic relationships between problems and treatments (e.g. cures, prevents, and side effect).in our context, we only use its annotation of medical entities.</citsent>
<aftsection>
<nextsent>the corpus contains two categories of medical entities: problems (1,660 entities) and treatments (1,179 entities) in 3,654 sentences (74,754 words) with mean of 20.05 words per sentence.
</nextsent>
<nextsent>we divided the corpus into 1,462 sentences for training and 2,193 for testing.
</nextsent>
<nextsent>we tested the metamap (mm), metamap+(mm+) and bio-crf methods on the berkeley corpus.
</nextsent>
<nextsent>table 6 presents the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5188">
<title id=" W11-0821.xml">the ngram statistics package textnsp  a flexible tool for identifying ngrams collocations and word associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that all of the mutual information measures are supported for trigrams, and that the log likelihood ratio is supported for 4-grams.
</prevsent>
<prevsent>the measures in the package are shown grouped by family in table 1, where the name by which the measure is known in nsp is in parentheses.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
table 1: measures of association in nsp mutual information (mi) (ll) log-likelihood ratio (dunning, 1993) <papid> J93-1003 </papid>tmi) true mi (church and hanks, 1990) (<papid> J90-1003 </papid>pmi) pointwise mi (church and hanks, 1990) (<papid> J90-1003 </papid>ps) poisson-stirling (church, 2000) <papid> C00-1027 </papid>fishers exact test (pedersen et al, 1996) (leftfisher) left tailed (rightfisher) right tailed (twotailed) two tailed chi-squared (phi) phi coefficient (church, 1991) (tscore) t-score (church et al, 1991) (x2) pearsons chi-squared (dunning, 1993) <papid> J93-1003 </papid>dice (dice) dice coefficient (smadja, 1993) (<papid> J93-1007 </papid>jaccard) jaccard measure (odds) odds ratio (blaheta and johnson, 2001) 3.1 rank.pl one natural experiment is to compare the output ofstatistic.pl for the same input using different measures of association.</citsent>
<aftsection>
<nextsent>rank.pl takes as input the out put from statistic.pl for two different measures, and computes spear mans rank correlation coefficient between them.
</nextsent>
<nextsent>in general, measures within the same family correlate more closely with each other than with measures from different family.
</nextsent>
<nextsent>as an example tmi and ll as well as dice and jaccard differ by only constant terms and therefore produce identical rankings.
</nextsent>
<nextsent>it is often worthwhile to conduct exploratory studies with multiple measures, and therank correlation can help recognize when two measures are very similar or different.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5190">
<title id=" W11-0821.xml">the ngram statistics package textnsp  a flexible tool for identifying ngrams collocations and word associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that all of the mutual information measures are supported for trigrams, and that the log likelihood ratio is supported for 4-grams.
</prevsent>
<prevsent>the measures in the package are shown grouped by family in table 1, where the name by which the measure is known in nsp is in parentheses.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
table 1: measures of association in nsp mutual information (mi) (ll) log-likelihood ratio (dunning, 1993) <papid> J93-1003 </papid>tmi) true mi (church and hanks, 1990) (<papid> J90-1003 </papid>pmi) pointwise mi (church and hanks, 1990) (<papid> J90-1003 </papid>ps) poisson-stirling (church, 2000) <papid> C00-1027 </papid>fishers exact test (pedersen et al, 1996) (leftfisher) left tailed (rightfisher) right tailed (twotailed) two tailed chi-squared (phi) phi coefficient (church, 1991) (tscore) t-score (church et al, 1991) (x2) pearsons chi-squared (dunning, 1993) <papid> J93-1003 </papid>dice (dice) dice coefficient (smadja, 1993) (<papid> J93-1007 </papid>jaccard) jaccard measure (odds) odds ratio (blaheta and johnson, 2001) 3.1 rank.pl one natural experiment is to compare the output ofstatistic.pl for the same input using different measures of association.</citsent>
<aftsection>
<nextsent>rank.pl takes as input the out put from statistic.pl for two different measures, and computes spear mans rank correlation coefficient between them.
</nextsent>
<nextsent>in general, measures within the same family correlate more closely with each other than with measures from different family.
</nextsent>
<nextsent>as an example tmi and ll as well as dice and jaccard differ by only constant terms and therefore produce identical rankings.
</nextsent>
<nextsent>it is often worthwhile to conduct exploratory studies with multiple measures, and therank correlation can help recognize when two measures are very similar or different.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5194">
<title id=" W11-0821.xml">the ngram statistics package textnsp  a flexible tool for identifying ngrams collocations and word associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that all of the mutual information measures are supported for trigrams, and that the log likelihood ratio is supported for 4-grams.
</prevsent>
<prevsent>the measures in the package are shown grouped by family in table 1, where the name by which the measure is known in nsp is in parentheses.
</prevsent>
</prevsection>
<citsent citstr=" C00-1027 ">
table 1: measures of association in nsp mutual information (mi) (ll) log-likelihood ratio (dunning, 1993) <papid> J93-1003 </papid>tmi) true mi (church and hanks, 1990) (<papid> J90-1003 </papid>pmi) pointwise mi (church and hanks, 1990) (<papid> J90-1003 </papid>ps) poisson-stirling (church, 2000) <papid> C00-1027 </papid>fishers exact test (pedersen et al, 1996) (leftfisher) left tailed (rightfisher) right tailed (twotailed) two tailed chi-squared (phi) phi coefficient (church, 1991) (tscore) t-score (church et al, 1991) (x2) pearsons chi-squared (dunning, 1993) <papid> J93-1003 </papid>dice (dice) dice coefficient (smadja, 1993) (<papid> J93-1007 </papid>jaccard) jaccard measure (odds) odds ratio (blaheta and johnson, 2001) 3.1 rank.pl one natural experiment is to compare the output ofstatistic.pl for the same input using different measures of association.</citsent>
<aftsection>
<nextsent>rank.pl takes as input the out put from statistic.pl for two different measures, and computes spear mans rank correlation coefficient between them.
</nextsent>
<nextsent>in general, measures within the same family correlate more closely with each other than with measures from different family.
</nextsent>
<nextsent>as an example tmi and ll as well as dice and jaccard differ by only constant terms and therefore produce identical rankings.
</nextsent>
<nextsent>it is often worthwhile to conduct exploratory studies with multiple measures, and therank correlation can help recognize when two measures are very similar or different.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5196">
<title id=" W11-0821.xml">the ngram statistics package textnsp  a flexible tool for identifying ngrams collocations and word associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that all of the mutual information measures are supported for trigrams, and that the log likelihood ratio is supported for 4-grams.
</prevsent>
<prevsent>the measures in the package are shown grouped by family in table 1, where the name by which the measure is known in nsp is in parentheses.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
table 1: measures of association in nsp mutual information (mi) (ll) log-likelihood ratio (dunning, 1993) <papid> J93-1003 </papid>tmi) true mi (church and hanks, 1990) (<papid> J90-1003 </papid>pmi) pointwise mi (church and hanks, 1990) (<papid> J90-1003 </papid>ps) poisson-stirling (church, 2000) <papid> C00-1027 </papid>fishers exact test (pedersen et al, 1996) (leftfisher) left tailed (rightfisher) right tailed (twotailed) two tailed chi-squared (phi) phi coefficient (church, 1991) (tscore) t-score (church et al, 1991) (x2) pearsons chi-squared (dunning, 1993) <papid> J93-1003 </papid>dice (dice) dice coefficient (smadja, 1993) (<papid> J93-1007 </papid>jaccard) jaccard measure (odds) odds ratio (blaheta and johnson, 2001) 3.1 rank.pl one natural experiment is to compare the output ofstatistic.pl for the same input using different measures of association.</citsent>
<aftsection>
<nextsent>rank.pl takes as input the out put from statistic.pl for two different measures, and computes spear mans rank correlation coefficient between them.
</nextsent>
<nextsent>in general, measures within the same family correlate more closely with each other than with measures from different family.
</nextsent>
<nextsent>as an example tmi and ll as well as dice and jaccard differ by only constant terms and therefore produce identical rankings.
</nextsent>
<nextsent>it is often worthwhile to conduct exploratory studies with multiple measures, and therank correlation can help recognize when two measures are very similar or different.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5197">
<title id=" W11-0821.xml">the ngram statistics package textnsp  a flexible tool for identifying ngrams collocations and word associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>6 development history of text::nsp.
</prevsent>
<prevsent>the ngram statistics package was originally implemented by satanjeev banerjee in 2000-2002 (banerjee and pedersen, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
amruta purandare incorporated nsp into sense clusters (purandare and pedersen, 2004) <papid> W04-2406 </papid>and added huge-count.pl, com big.pl and kocos.pl in 2002-2004.</citsent>
<aftsection>
<nextsent>bridget mcinnes added the log-likelihood ratio for longer ngrams in 2003-2004 (mcinnes, 2004).
</nextsent>
<nextsent>saiyam kohli rewrote the measures of association to use object oriented methods in 2004-2006, and also added numerous new measures for bigrams and trigams(kohli, 2006).
</nextsent>
<nextsent>mahesh joshi improved cross platform support and created an nsp wrapper for gate in 2005-2006.
</nextsent>
<nextsent>ying liu wrote find-compounds.pl and rewrote huge-count.pl in 2010-2011.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5198">
<title id=" W10-4144.xml">discriminative parse reranking for chinese with homogeneous and heterogeneous annotations </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the asymmetry in the annotation information is partially due to the difference of linguistic treatment.
</prevsent>
<prevsent>but more importantly, it shows that both treebanks have the potential of being refined with more detailed classification, on either phrasal or word categories.
</prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
one data-driven approach to derive more fine-grained annotation is the hierarchically split-merge parsing (petrov et al, 2006;<papid> P06-1055 </papid>petrov and klein, 2007), <papid> N07-1051 </papid>which induces subcategories from coarse-grained annotations through an expectation maximization procedure.</citsent>
<aftsection>
<nextsent>in combination with the coarse-to-fine parsing strategy, efficient inference can be done with cascade of grammars of different granularity.
</nextsent>
<nextsent>such parsing models have reached (close to) state-of-the-artperformance for many languages including chinese and english.
</nextsent>
<nextsent>another effective technique to improve parsing results is discriminative reranking (charniak and johnson, 2005; <papid> P05-1022 </papid>collins and koo, 2005).<papid> J05-1003 </papid></nextsent>
<nextsent>while the generative models compose candidate parse trees, discriminative reranker re orders the listof candidates in favor of those trees which maximizes the properties of being good analysis.such extra model refines the original scores assigned by the generative model by focusing its decisions on the fine details among already good?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5199">
<title id=" W10-4144.xml">discriminative parse reranking for chinese with homogeneous and heterogeneous annotations </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the asymmetry in the annotation information is partially due to the difference of linguistic treatment.
</prevsent>
<prevsent>but more importantly, it shows that both treebanks have the potential of being refined with more detailed classification, on either phrasal or word categories.
</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
one data-driven approach to derive more fine-grained annotation is the hierarchically split-merge parsing (petrov et al, 2006;<papid> P06-1055 </papid>petrov and klein, 2007), <papid> N07-1051 </papid>which induces subcategories from coarse-grained annotations through an expectation maximization procedure.</citsent>
<aftsection>
<nextsent>in combination with the coarse-to-fine parsing strategy, efficient inference can be done with cascade of grammars of different granularity.
</nextsent>
<nextsent>such parsing models have reached (close to) state-of-the-artperformance for many languages including chinese and english.
</nextsent>
<nextsent>another effective technique to improve parsing results is discriminative reranking (charniak and johnson, 2005; <papid> P05-1022 </papid>collins and koo, 2005).<papid> J05-1003 </papid></nextsent>
<nextsent>while the generative models compose candidate parse trees, discriminative reranker re orders the listof candidates in favor of those trees which maximizes the properties of being good analysis.such extra model refines the original scores assigned by the generative model by focusing its decisions on the fine details among already good?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5200">
<title id=" W10-4144.xml">discriminative parse reranking for chinese with homogeneous and heterogeneous annotations </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in combination with the coarse-to-fine parsing strategy, efficient inference can be done with cascade of grammars of different granularity.
</prevsent>
<prevsent>such parsing models have reached (close to) state-of-the-artperformance for many languages including chinese and english.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
another effective technique to improve parsing results is discriminative reranking (charniak and johnson, 2005; <papid> P05-1022 </papid>collins and koo, 2005).<papid> J05-1003 </papid></citsent>
<aftsection>
<nextsent>while the generative models compose candidate parse trees, discriminative reranker re orders the listof candidates in favor of those trees which maximizes the properties of being good analysis.such extra model refines the original scores assigned by the generative model by focusing its decisions on the fine details among already good?
</nextsent>
<nextsent>candidates.
</nextsent>
<nextsent>due to this nature, the set of feature sin the reranker focus on those global (and potentially long distance) properties which are difficult to model with the generative model.
</nextsent>
<nextsent>also, since it is not necessary for the reranker to generate the candidate trees, one can easily integrate additional external information to help adjust the ranking ofthe analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5201">
<title id=" W10-4144.xml">discriminative parse reranking for chinese with homogeneous and heterogeneous annotations </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in combination with the coarse-to-fine parsing strategy, efficient inference can be done with cascade of grammars of different granularity.
</prevsent>
<prevsent>such parsing models have reached (close to) state-of-the-artperformance for many languages including chinese and english.
</prevsent>
</prevsection>
<citsent citstr=" J05-1003 ">
another effective technique to improve parsing results is discriminative reranking (charniak and johnson, 2005; <papid> P05-1022 </papid>collins and koo, 2005).<papid> J05-1003 </papid></citsent>
<aftsection>
<nextsent>while the generative models compose candidate parse trees, discriminative reranker re orders the listof candidates in favor of those trees which maximizes the properties of being good analysis.such extra model refines the original scores assigned by the generative model by focusing its decisions on the fine details among already good?
</nextsent>
<nextsent>candidates.
</nextsent>
<nextsent>due to this nature, the set of feature sin the reranker focus on those global (and potentially long distance) properties which are difficult to model with the generative model.
</nextsent>
<nextsent>also, since it is not necessary for the reranker to generate the candidate trees, one can easily integrate additional external information to help adjust the ranking ofthe analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5203">
<title id=" W10-4144.xml">discriminative parse reranking for chinese with homogeneous and heterogeneous annotations </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>since we have already mentioned the berkeley parser in the related work, we will focus on the other two modules in the rest of this section.
</prevsent>
<prevsent>3.1 parse reranker.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
we follow collins and koo (2005)<papid> J05-1003 </papid>s discriminative reranking model to score possible parse trees of each sentence given by the berkeley parser.previous research on english shows that structured perceptron (collins, 2002) <papid> W02-1001 </papid>is one of the strongest machine learning algorithms for parse reranking (collins and duffy, 2002; <papid> P02-1034 </papid>gao et al,2007).<papid> P07-1104 </papid></citsent>
<aftsection>
<nextsent>in our system, we use the averaged perceptron algorithm to do parameter estimation.
</nextsent>
<nextsent>algorithm 1 illustrates the learning procedure.
</nextsent>
<nextsent>the parameter vector is initial ized to (0, ..., 0).
</nextsent>
<nextsent>the learner processes all the instances (t is from 1 to n) in each iteration (i).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5204">
<title id=" W10-4144.xml">discriminative parse reranking for chinese with homogeneous and heterogeneous annotations </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>since we have already mentioned the berkeley parser in the related work, we will focus on the other two modules in the rest of this section.
</prevsent>
<prevsent>3.1 parse reranker.
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
we follow collins and koo (2005)<papid> J05-1003 </papid>s discriminative reranking model to score possible parse trees of each sentence given by the berkeley parser.previous research on english shows that structured perceptron (collins, 2002) <papid> W02-1001 </papid>is one of the strongest machine learning algorithms for parse reranking (collins and duffy, 2002; <papid> P02-1034 </papid>gao et al,2007).<papid> P07-1104 </papid></citsent>
<aftsection>
<nextsent>in our system, we use the averaged perceptron algorithm to do parameter estimation.
</nextsent>
<nextsent>algorithm 1 illustrates the learning procedure.
</nextsent>
<nextsent>the parameter vector is initial ized to (0, ..., 0).
</nextsent>
<nextsent>the learner processes all the instances (t is from 1 to n) in each iteration (i).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5205">
<title id=" W10-4144.xml">discriminative parse reranking for chinese with homogeneous and heterogeneous annotations </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>since we have already mentioned the berkeley parser in the related work, we will focus on the other two modules in the rest of this section.
</prevsent>
<prevsent>3.1 parse reranker.
</prevsent>
</prevsection>
<citsent citstr=" P07-1104 ">
we follow collins and koo (2005)<papid> J05-1003 </papid>s discriminative reranking model to score possible parse trees of each sentence given by the berkeley parser.previous research on english shows that structured perceptron (collins, 2002) <papid> W02-1001 </papid>is one of the strongest machine learning algorithms for parse reranking (collins and duffy, 2002; <papid> P02-1034 </papid>gao et al,2007).<papid> P07-1104 </papid></citsent>
<aftsection>
<nextsent>in our system, we use the averaged perceptron algorithm to do parameter estimation.
</nextsent>
<nextsent>algorithm 1 illustrates the learning procedure.
</nextsent>
<nextsent>the parameter vector is initial ized to (0, ..., 0).
</nextsent>
<nextsent>the learner processes all the instances (t is from 1 to n) in each iteration (i).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5206">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>writing in simple english means that simple words are used.
</prevsent>
<prevsent>it does not mean readers want basic information.
</prevsent>
</prevsection>
<citsent citstr=" P11-2117 ">
articles do not have to be short to be simple;expand articles, add details, but use basic vo cabulary.the dataset we examine contains aligned sentence pairs of english wikipedia2 with simple english wikipedia (coster and kauchak, 2011; <papid> P11-2117 </papid>zhu et al, 2010).<papid> C10-1152 </papid></citsent>
<aftsection>
<nextsent>we view the simplification problem as an english-to-english translation problem: given aligned sentence pairs consisting of normal, un simplified sentence and simplified version of that sentence, the goal is to learn sentence simplification system to translate?
</nextsent>
<nextsent>from normal english to simplified english.
</nextsent>
<nextsent>this setup has been successfully employed in number of text-to-text applications including machine translation (och and ney, 2003),<papid> J03-1002 </papid>paraphrasing (wubben et al, 2010) <papid> W10-4223 </papid>and text compression (knight and marcu, 2002; cohn and lap ata, 2009).</nextsent>
<nextsent>table 1 shows example sentence pairs from the aligned data set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5207">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>writing in simple english means that simple words are used.
</prevsent>
<prevsent>it does not mean readers want basic information.
</prevsent>
</prevsection>
<citsent citstr=" C10-1152 ">
articles do not have to be short to be simple;expand articles, add details, but use basic vo cabulary.the dataset we examine contains aligned sentence pairs of english wikipedia2 with simple english wikipedia (coster and kauchak, 2011; <papid> P11-2117 </papid>zhu et al, 2010).<papid> C10-1152 </papid></citsent>
<aftsection>
<nextsent>we view the simplification problem as an english-to-english translation problem: given aligned sentence pairs consisting of normal, un simplified sentence and simplified version of that sentence, the goal is to learn sentence simplification system to translate?
</nextsent>
<nextsent>from normal english to simplified english.
</nextsent>
<nextsent>this setup has been successfully employed in number of text-to-text applications including machine translation (och and ney, 2003),<papid> J03-1002 </papid>paraphrasing (wubben et al, 2010) <papid> W10-4223 </papid>and text compression (knight and marcu, 2002; cohn and lap ata, 2009).</nextsent>
<nextsent>table 1 shows example sentence pairs from the aligned data set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5209">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we view the simplification problem as an english-to-english translation problem: given aligned sentence pairs consisting of normal, un simplified sentence and simplified version of that sentence, the goal is to learn sentence simplification system to translate?
</prevsent>
<prevsent>from normal english to simplified english.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
this setup has been successfully employed in number of text-to-text applications including machine translation (och and ney, 2003),<papid> J03-1002 </papid>paraphrasing (wubben et al, 2010) <papid> W10-4223 </papid>and text compression (knight and marcu, 2002; cohn and lap ata, 2009).</citsent>
<aftsection>
<nextsent>table 1 shows example sentence pairs from the aligned dataset.
</nextsent>
<nextsent>one of the challenges of text simplification is that, unlike text compression where the emphasis is often on word deletion, text simplifica 2http://en.wikipedia.org/ 1 a. normal: greene agreed that she could earn more by breaking away from 20th century fox.
</nextsent>
<nextsent>simple: greene agreed that she could earn more by leaving 20th century fox.
</nextsent>
<nextsent>b. normal: the crust and underlying relatively rigid mantle make up the lithosphere.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5210">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we view the simplification problem as an english-to-english translation problem: given aligned sentence pairs consisting of normal, un simplified sentence and simplified version of that sentence, the goal is to learn sentence simplification system to translate?
</prevsent>
<prevsent>from normal english to simplified english.
</prevsent>
</prevsection>
<citsent citstr=" W10-4223 ">
this setup has been successfully employed in number of text-to-text applications including machine translation (och and ney, 2003),<papid> J03-1002 </papid>paraphrasing (wubben et al, 2010) <papid> W10-4223 </papid>and text compression (knight and marcu, 2002; cohn and lap ata, 2009).</citsent>
<aftsection>
<nextsent>table 1 shows example sentence pairs from the aligned dataset.
</nextsent>
<nextsent>one of the challenges of text simplification is that, unlike text compression where the emphasis is often on word deletion, text simplifica 2http://en.wikipedia.org/ 1 a. normal: greene agreed that she could earn more by breaking away from 20th century fox.
</nextsent>
<nextsent>simple: greene agreed that she could earn more by leaving 20th century fox.
</nextsent>
<nextsent>b. normal: the crust and underlying relatively rigid mantle make up the lithosphere.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5211">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moves from the beginning of the sentence to the end.
</prevsent>
<prevsent>insertion: ideal engines or?
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
in d.motivated by the need to model all of these different transformations, we chose to extend statistical phrase-based translation system (koehn et al, 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>in particular, we added phrasal deletion to the probabilistic translation model.
</nextsent>
<nextsent>this addition broadens the deletion capabilities of the system since the base model only allows for deletion within phrase.
</nextsent>
<nextsent>as kauchak and coster (2011) point out, deletion isa frequently occurring phenomena in the simplification data.there are number of benefits of text simplification research.
</nextsent>
<nextsent>much of the current text data available including wikipedia, news articles and most web pages are written with an average adult reader as the target audience.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5214">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much of the current text data available including wikipedia, news articles and most web pages are written with an average adult reader as the target audience.
</prevsent>
<prevsent>text simplification can make this data available to broader range of audiences including children, language learners, the elderly, the hearing impaired and people with aphasia or cognitive disabilities (feng, 2008; carroll et al, 1998).
</prevsent>
</prevsection>
<citsent citstr=" P08-1040 ">
text simplification has also been shown to improve the performance of other natural language processing applications including semantic role labeling (vickrey and koller, 2008) <papid> P08-1040 </papid>and relation extraction (miwa et al, 2010).<papid> C10-1089 </papid></citsent>
<aftsection>
<nextsent>most previous work in the area of sentence simplification has not been from data-driven perspective.
</nextsent>
<nextsent>feng (2008) gives good historical overview of prior text simplification systems including early rule-based approaches (chandrasekar and srinivas, 1997; carroll et al, 1998; canning et al, 2000) and number of commercial approaches.
</nextsent>
<nextsent>vickrey and koller (2008) <papid> P08-1040 </papid>and miwa et al (2010) <papid> C10-1089 </papid>employ text simplification as preprocessing step, though both use manually generated rules.</nextsent>
<nextsent>our work extends recent work by zhu et al(2010) <papid> C10-1152 </papid>that also examines wikipedia/simple english wikipedia as data-driven, sentence simplification task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5215">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much of the current text data available including wikipedia, news articles and most web pages are written with an average adult reader as the target audience.
</prevsent>
<prevsent>text simplification can make this data available to broader range of audiences including children, language learners, the elderly, the hearing impaired and people with aphasia or cognitive disabilities (feng, 2008; carroll et al, 1998).
</prevsent>
</prevsection>
<citsent citstr=" C10-1089 ">
text simplification has also been shown to improve the performance of other natural language processing applications including semantic role labeling (vickrey and koller, 2008) <papid> P08-1040 </papid>and relation extraction (miwa et al, 2010).<papid> C10-1089 </papid></citsent>
<aftsection>
<nextsent>most previous work in the area of sentence simplification has not been from data-driven perspective.
</nextsent>
<nextsent>feng (2008) gives good historical overview of prior text simplification systems including early rule-based approaches (chandrasekar and srinivas, 1997; carroll et al, 1998; canning et al, 2000) and number of commercial approaches.
</nextsent>
<nextsent>vickrey and koller (2008) <papid> P08-1040 </papid>and miwa et al (2010) <papid> C10-1089 </papid>employ text simplification as preprocessing step, though both use manually generated rules.</nextsent>
<nextsent>our work extends recent work by zhu et al(2010) <papid> C10-1152 </papid>that also examines wikipedia/simple english wikipedia as data-driven, sentence simplification task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5219">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sentence simplification is closely related to the 2problem of sentence compression, another english to-english translation task.
</prevsent>
<prevsent>knight and marcu (2002) were one of the first to formalize text compression as data-driven problem and proposed probabilistic, noisy-channel model and decision tree-based model for compression.
</prevsent>
</prevsection>
<citsent citstr=" N07-1023 ">
galley and mckeown (2007) <papid> N07-1023 </papid>show improvements to thenoisy-channel approach based on rule lexicalization and rule markovization.</citsent>
<aftsection>
<nextsent>recently, number of approaches to text compression have been proposed that score transformation rules discrimina tively based on support vector machines (mcdonald,2006; <papid> E06-1038 </papid>cohn and lapata, 2009) and conditional random fields (nomoto, 2007; nomoto, 2008) <papid> P08-1035 </papid>instead of using maximum likelihood estimation.</nextsent>
<nextsent>with the exception of cohn and lapata (2009), all of these text compression approaches make the simplifying assumption that the compression process happens only via word deletion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5220">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>knight and marcu (2002) were one of the first to formalize text compression as data-driven problem and proposed probabilistic, noisy-channel model and decision tree-based model for compression.
</prevsent>
<prevsent>galley and mckeown (2007) <papid> N07-1023 </papid>show improvements to thenoisy-channel approach based on rule lexicalization and rule markovization.</prevsent>
</prevsection>
<citsent citstr=" E06-1038 ">
recently, number of approaches to text compression have been proposed that score transformation rules discrimina tively based on support vector machines (mcdonald,2006; <papid> E06-1038 </papid>cohn and lapata, 2009) and conditional random fields (nomoto, 2007; nomoto, 2008) <papid> P08-1035 </papid>instead of using maximum likelihood estimation.</citsent>
<aftsection>
<nextsent>with the exception of cohn and lapata (2009), all of these text compression approaches make the simplifying assumption that the compression process happens only via word deletion.
</nextsent>
<nextsent>we provide comparisons with some of these systems, however, for text simplification where lexical changes and reordering are frequent, most of these techniques are not appropriate.
</nextsent>
<nextsent>our proposed approach builds upon approaches employed in machine translation (mt).
</nextsent>
<nextsent>we introduce variant of phrase-based machine translation system (och and ney, 2003; <papid> J03-1002 </papid>koehn et al, 2007) <papid> P07-2045 </papid>fortext simplification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5221">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>knight and marcu (2002) were one of the first to formalize text compression as data-driven problem and proposed probabilistic, noisy-channel model and decision tree-based model for compression.
</prevsent>
<prevsent>galley and mckeown (2007) <papid> N07-1023 </papid>show improvements to thenoisy-channel approach based on rule lexicalization and rule markovization.</prevsent>
</prevsection>
<citsent citstr=" P08-1035 ">
recently, number of approaches to text compression have been proposed that score transformation rules discrimina tively based on support vector machines (mcdonald,2006; <papid> E06-1038 </papid>cohn and lapata, 2009) and conditional random fields (nomoto, 2007; nomoto, 2008) <papid> P08-1035 </papid>instead of using maximum likelihood estimation.</citsent>
<aftsection>
<nextsent>with the exception of cohn and lapata (2009), all of these text compression approaches make the simplifying assumption that the compression process happens only via word deletion.
</nextsent>
<nextsent>we provide comparisons with some of these systems, however, for text simplification where lexical changes and reordering are frequent, most of these techniques are not appropriate.
</nextsent>
<nextsent>our proposed approach builds upon approaches employed in machine translation (mt).
</nextsent>
<nextsent>we introduce variant of phrase-based machine translation system (och and ney, 2003; <papid> J03-1002 </papid>koehn et al, 2007) <papid> P07-2045 </papid>fortext simplification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5224">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>our proposed approach builds upon approaches employed in machine translation (mt).
</prevsent>
<prevsent>we introduce variant of phrase-based machine translation system (och and ney, 2003; <papid> J03-1002 </papid>koehn et al, 2007) <papid> P07-2045 </papid>fortext simplification.</prevsent>
</prevsection>
<citsent citstr=" P10-1146 ">
although mt systems that employ syntactic or hierarchical information have recently shown improvements over phrase-based approaches (chiang, 2010), <papid> P10-1146 </papid>our initial investigation with syntactically driven approaches showed poorer performance on the text simplification task and were less robust to noise in the training data.</citsent>
<aftsection>
<nextsent>both english wikipedia and simple english wikipedia have received recent analysis as possible corpus by for both sentence compression andsimplification.
</nextsent>
<nextsent>yamangil and nelken (2008) <papid> P08-2035 </papid>examine the history logs of english wikipedia to learn sentence compression rules.</nextsent>
<nextsent>yatskar et al (2010) <papid> N10-1056 </papid>learn set of candidate phrase simplification rules based on edit changes identified in both wikipedias revision histories, though they only provide list of the top phrasal rules and do not utilize them in an end-to-end simplification system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5225">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>although mt systems that employ syntactic or hierarchical information have recently shown improvements over phrase-based approaches (chiang, 2010), <papid> P10-1146 </papid>our initial investigation with syntactically driven approaches showed poorer performance on the text simplification task and were less robust to noise in the training data.</prevsent>
<prevsent>both english wikipedia and simple english wikipedia have received recent analysis as possible corpus by for both sentence compression andsimplification.</prevsent>
</prevsection>
<citsent citstr=" P08-2035 ">
yamangil and nelken (2008) <papid> P08-2035 </papid>examine the history logs of english wikipedia to learn sentence compression rules.</citsent>
<aftsection>
<nextsent>yatskar et al (2010) <papid> N10-1056 </papid>learn set of candidate phrase simplification rules based on edit changes identified in both wikipedias revision histories, though they only provide list of the top phrasal rules and do not utilize them in an end-to-end simplification system.</nextsent>
<nextsent>napoles and dredze (2010) <papid> W10-0406 </papid>provide an analysis of the differences between documents in english wikipedia and simple english wikipedia, though they do not view the dataset as parallel corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5226">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>both english wikipedia and simple english wikipedia have received recent analysis as possible corpus by for both sentence compression andsimplification.
</prevsent>
<prevsent>yamangil and nelken (2008) <papid> P08-2035 </papid>examine the history logs of english wikipedia to learn sentence compression rules.</prevsent>
</prevsection>
<citsent citstr=" N10-1056 ">
yatskar et al (2010) <papid> N10-1056 </papid>learn set of candidate phrase simplification rules based on edit changes identified in both wikipedias revision histories, though they only provide list of the top phrasal rules and do not utilize them in an end-to-end simplification system.</citsent>
<aftsection>
<nextsent>napoles and dredze (2010) <papid> W10-0406 </papid>provide an analysis of the differences between documents in english wikipedia and simple english wikipedia, though they do not view the dataset as parallel corpus.</nextsent>
<nextsent>few datasets exist for text simplification and datasets for the related task of sentence compression are small, containing no more than few thousand aligned sentence pairs (knight and marcu, 2002;cohn and lapata, 2009; nomoto, 2009).<papid> D09-1041 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5227">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>yamangil and nelken (2008) <papid> P08-2035 </papid>examine the history logs of english wikipedia to learn sentence compression rules.</prevsent>
<prevsent>yatskar et al (2010) <papid> N10-1056 </papid>learn set of candidate phrase simplification rules based on edit changes identified in both wikipedias revision histories, though they only provide list of the top phrasal rules and do not utilize them in an end-to-end simplification system.</prevsent>
</prevsection>
<citsent citstr=" W10-0406 ">
napoles and dredze (2010) <papid> W10-0406 </papid>provide an analysis of the differences between documents in english wikipedia and simple english wikipedia, though they do not view the dataset as parallel corpus.</citsent>
<aftsection>
<nextsent>few datasets exist for text simplification and datasets for the related task of sentence compression are small, containing no more than few thousand aligned sentence pairs (knight and marcu, 2002;cohn and lapata, 2009; nomoto, 2009).<papid> D09-1041 </papid></nextsent>
<nextsent>for this paper, we utilized sentence-aligned corpus generated by aligning english wikipedia with simple english wikipedia resulting in 137k aligned sentence pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5228">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> text simplification corpus.  </section>
<citcontext>
<prevsection>
<prevsent>yatskar et al (2010) <papid> N10-1056 </papid>learn set of candidate phrase simplification rules based on edit changes identified in both wikipedias revision histories, though they only provide list of the top phrasal rules and do not utilize them in an end-to-end simplification system.</prevsent>
<prevsent>napoles and dredze (2010) <papid> W10-0406 </papid>provide an analysis of the differences between documents in english wikipedia and simple english wikipedia, though they do not view the dataset as parallel corpus.</prevsent>
</prevsection>
<citsent citstr=" D09-1041 ">
few datasets exist for text simplification and datasets for the related task of sentence compression are small, containing no more than few thousand aligned sentence pairs (knight and marcu, 2002;cohn and lapata, 2009; nomoto, 2009).<papid> D09-1041 </papid></citsent>
<aftsection>
<nextsent>for this paper, we utilized sentence-aligned corpus generated by aligning english wikipedia with simple english wikipedia resulting in 137k aligned sentence pairs.
</nextsent>
<nextsent>this dataset is larger than any previously examined for sentence simplification and orders of magnitude larger than those previously examined for sentence compression.
</nextsent>
<nextsent>we give brief overview of the corpus generation process here.
</nextsent>
<nextsent>for more details and an analysis of thedata set, see (coster and kauchak, 2011).<papid> P11-2117 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5231">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> text simplification corpus.  </section>
<citcontext>
<prevsection>
<prevsent>through out this article we will refer to english wikipedia articles/sentences as normal and simple english wikipedia articles as simple.
</prevsent>
<prevsent>we aligned the normal and simple articles at the document level based on exact match of the title andthen removed all article pairs that were stubs, disambiguation pages, meta-pages or only contained asingle line.
</prevsent>
</prevsection>
<citsent citstr=" E06-1021 ">
following similar approach to previous monolingual alignment techniques (barzilay and elhadad, 2003; nelken and shieber, 2006), <papid> E06-1021 </papid>we then aligned each simple paragraph to any normal paragraph that had normalized tf-idf cosine similarity above set threshold.</citsent>
<aftsection>
<nextsent>these aligned paragraphs were then aligned at the sentence level using dynamic programming approach, picking the best sentence-level alignment from combination of the following sentence-level alignments: ? normal sentence inserted ? normal sentence deleted ? one normal sentence to one simple sentence ? two normal sentences to one simple sentence ? one normal sentence to two simple sentence following nelken and shieber (2006), <papid> E06-1021 </papid>we used tfidf cosine similarity to measure the similarity between aligned sentences and only kept aligned sentence pairs with similarity threshold above 0.5.</nextsent>
<nextsent>we 3found this thresholding approach to be more intuitive than trying to adjust skip (insertion or deletion) penalty, which has also been proposed (barzi lay and elhadad, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5236">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> simplification model.  </section>
<citcontext>
<prevsection>
<prevsent>ni are similarly defined over the normal sentence.
</prevsent>
<prevsent>p(si|ni) denotes the probability of normal phrase beingtranslated/simplified to the corresponding simplified phrase.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
these phrasal probabilities are extracted from the sentence pairs based on an em learned word alignment using giza++ (och and ney, 2000).<papid> P00-1056 </papid>phrase-based models in machine translation often require that both phrases in the phrasal probabilities contain one or more words, since phrasaldeletion/insertion is rare and can complicate the decoding process.</citsent>
<aftsection>
<nextsent>for text simplification, however,phrasal deletion commonly occurs: 47% of the sentence pairs contain deletions (coster and kauchak,2011).<papid> P11-2117 </papid></nextsent>
<nextsent>to model this deletion, we relax the restriction that the simple phrase must be non-empty and include in the translation model probabilistic phrasal deletion rules of the form p(null|ni) allowing for phrases to be deleted during simplification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5238">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> simplification model.  </section>
<citcontext>
<prevsection>
<prevsent>this second modification has two main benefits.
</prevsent>
<prevsent>frequently, if word occurs in both the normal and simple sentence and it is aligned to itself, no other words should be aligned to that word.
</prevsent>
</prevsection>
<citsent citstr=" N09-1015 ">
as other shave noted, this type of spurious alignment is particularly prevalent with function words, which tend to occur in many different contexts (chen et al, 2009).<papid> N09-1015 </papid>second, even in situations where it may be appropriate for multiple words to align to single word (for example, in compound nouns, such as president obama ? obama), removing the alignment of the extra words, allows us to delete those words in othercontexts.</citsent>
<aftsection>
<nextsent>we lose some specificity with this adaptation because some deletions can now occur independent of context, however, empirically this modification provides more benefit than hindrance for the model.
</nextsent>
<nextsent>we conjecture that the language model helps avoid these problematic cases.
</nextsent>
<nextsent>table 2 shows excerpts from an example sentence pair before the alignment alteration and after.
</nextsent>
<nextsent>in the original alignment ?, aka rodi?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5242">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>moses+del: our approach described in section 4 which is phrase-based approach with the addition of phrasal deletion.
</prevsent>
<prevsent>from the aligned dataset of 137k sentence pairs, we used 124k for training and 1,300 for testing with the remaining 12k sentences used during development.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we trained the n-gram language model used by the last four systems on the simple side of the training data.3 t3 requires parsed data which we generated using the stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>both moses and moses+del were trained using the default moses parameters and we used the last 500 sentence pairs from the training set to optimize the hyper-parameters of the log-linear model for both moses variants.
</nextsent>
<nextsent>t3 was run with the default parameters.due to runtime and memory issues, we were unable to run t3 on the full data set.4 we therefore present results for t3 trained on the largest training set that completed successfully, the first 30k sentence pairs.
</nextsent>
<nextsent>this still represents significantly larger training set than t3 has been run on previously.
</nextsent>
<nextsent>for comparison, we also provide results below for moses+del trained on the same 30k sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5243">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 evaluation.
</prevsent>
<prevsent>since there is no standard way of evaluating text simplification, we provide results for three different automatic methods, all of which compare the systems output to reference simplification.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
we used bleu (papineni et al, 2002), <papid> P02-1040 </papid>which is the weighted mean of n-gram precis ions with penalty for brevity.</citsent>
<aftsection>
<nextsent>it has been used extensively in machine translation and has been shown to correlate well with human performance judgements.
</nextsent>
<nextsent>we also adopt two automatic measures that have been used to evaluate text compression that compare the systems output to reference translation3see (turner and charniak, 2005) <papid> P05-1036 </papid>for discussion of problems that can occur for text compression when using language model trained on data from the un compressed side.</nextsent>
<nextsent>4on 30k sentences t3 took 4 days to train.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5244">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we used bleu (papineni et al, 2002), <papid> P02-1040 </papid>which is the weighted mean of n-gram precis ions with penalty for brevity.</prevsent>
<prevsent>it has been used extensively in machine translation and has been shown to correlate well with human performance judgements.</prevsent>
</prevsection>
<citsent citstr=" P05-1036 ">
we also adopt two automatic measures that have been used to evaluate text compression that compare the systems output to reference translation3see (turner and charniak, 2005) <papid> P05-1036 </papid>for discussion of problems that can occur for text compression when using language model trained on data from the un compressed side.</citsent>
<aftsection>
<nextsent>4on 30k sentences t3 took 4 days to train.
</nextsent>
<nextsent>on the full dataset, we ran t3 for week and at that point the discriminative training was using over 100gb of memory and we terminated the run.
</nextsent>
<nextsent>5 system bleu word-f1 ssa none 0.5937 0.5967 0.6179 &amp; 0.4352 0.4352 0.4871 t3* 0.2437 0.2190 0.3651 moses 0.5987 0.6076 0.6224 moses+del 0.6046 0.6149 0.6259 table 4: performance of the five approaches on the testdata.
</nextsent>
<nextsent>all differences in performance are statistically significant.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5245">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>all differences in performance are statistically significant.
</prevsent>
<prevsent>* - t3 was only trained on 30k sentence pairs for performance reasons.
</prevsent>
</prevsection>
<citsent citstr=" P06-1048 ">
(clarke and lapata, 2006): <papid> P06-1048 </papid>simple string accuracy measure (a normalized version of edit distance, abbreviated ssa) and f1 score calculated over words.</citsent>
<aftsection>
<nextsent>we calculated f1 over words instead of grammatical relations (subject, direct/indirect object, etc.) since finding the relation correspondence between the system output and the reference is non-trivial task for simplification data where reordering, insertions and lexical changes can occur.
</nextsent>
<nextsent>clarke and lapata (2006)<papid> P06-1048 </papid>showed moderate correlation with human judgement for ssa and strong correlation for the f1 measure.to measure whether the difference between system performance is statistically significant, we use bootstrap re sampling with 100 samples with the test (koehn, 2004).<papid> W04-3250 </papid></nextsent>
<nextsent>5.2 results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5247">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>(clarke and lapata, 2006): <papid> P06-1048 </papid>simple string accuracy measure (a normalized version of edit distance, abbreviated ssa) and f1 score calculated over words.</prevsent>
<prevsent>we calculated f1 over words instead of grammatical relations (subject, direct/indirect object, etc.) since finding the relation correspondence between the system output and the reference is non-trivial task for simplification data where reordering, insertions and lexical changes can occur.</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
clarke and lapata (2006)<papid> P06-1048 </papid>showed moderate correlation with human judgement for ssa and strong correlation for the f1 measure.to measure whether the difference between system performance is statistically significant, we use bootstrap re sampling with 100 samples with the test (koehn, 2004).<papid> W04-3250 </papid></citsent>
<aftsection>
<nextsent>5.2 results.
</nextsent>
<nextsent>table 4 shows the results on the test set for the different evaluation measures.
</nextsent>
<nextsent>all three of the evaluation metrics rank the five systems in the same order with moses+del performing best.
</nextsent>
<nextsent>all differences between the systems are statistically significant for all metrics at the = 0.01 level.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5250">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>bleu system original oracle moses 0.5987 0.6317 moses+del 0.6046 0.6421 table 7: bleu score for the original system versus the best possible oracle?
</prevsent>
<prevsent>translations generated by greedily selecting the best translation from an n-best list based on the reference simplification.
</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
best list to measure the potential benefit of reranking techniques, which have proved successful in many nlp applications (och et al, 2004; <papid> N04-1021 </papid>ge and mooney, 2006), <papid> P06-2034 </papid>and to understand how well the underlying model captures the phenomena exhibited in the data.for both of the phrase-based approaches, wegener ated an n-best list of size 1000 for each sentence in the test set.</citsent>
<aftsection>
<nextsent>using these n-best lists, we generate dan oracle?
</nextsent>
<nextsent>simplification of the test set by greedily selecting for each test sentence the simplification in the n-best list with the best sentence-level bleu score.
</nextsent>
<nextsent>table 7 shows the bleu scores for the original system output and the systems oracle output.
</nextsent>
<nextsent>in all cases, there is large difference between the systems current output and the oracle output, suggesting that utilizing some reranking technique could be useful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5251">
<title id=" W11-1601.xml">learning to simplify sentences using wikipedia </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>bleu system original oracle moses 0.5987 0.6317 moses+del 0.6046 0.6421 table 7: bleu score for the original system versus the best possible oracle?
</prevsent>
<prevsent>translations generated by greedily selecting the best translation from an n-best list based on the reference simplification.
</prevsent>
</prevsection>
<citsent citstr=" P06-2034 ">
best list to measure the potential benefit of reranking techniques, which have proved successful in many nlp applications (och et al, 2004; <papid> N04-1021 </papid>ge and mooney, 2006), <papid> P06-2034 </papid>and to understand how well the underlying model captures the phenomena exhibited in the data.for both of the phrase-based approaches, wegener ated an n-best list of size 1000 for each sentence in the test set.</citsent>
<aftsection>
<nextsent>using these n-best lists, we generate dan oracle?
</nextsent>
<nextsent>simplification of the test set by greedily selecting for each test sentence the simplification in the n-best list with the best sentence-level bleu score.
</nextsent>
<nextsent>table 7 shows the bleu scores for the original system output and the systems oracle output.
</nextsent>
<nextsent>in all cases, there is large difference between the systems current output and the oracle output, suggesting that utilizing some reranking technique could be useful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5254">
<title id=" W11-0813.xml">an ngram frequency database reference to handle mwe extraction in nlp applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it consists in defining an n-gram frequency database that can be used to compute ams on-thefly, allowing the extraction procedure to efficiently process all the mwes in text, even if they have not been previously observed.
</prevsent>
<prevsent>multiword expressions (mwes) are commonly defined as recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages?
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
(smadja,1993, <papid> J93-1007 </papid>143).</citsent>
<aftsection>
<nextsent>their importance in the field of natural language processing (nlp) is undeniable.
</nextsent>
<nextsent>although composed of several words, these sequences are nonetheless considered as simple units with regard to part-of-speech at the lexical as well as syntactic levels.
</nextsent>
<nextsent>their identification is therefore essential to the efficiency of applications such as parsing (nivre and nilsson, 2004), machine translation (ren et al, 2009), <papid> W09-2907 </papid>information extraction, or information retrieval (vechtomova, 2005).</nextsent>
<nextsent>in these systems, the principle of syntactic or semantic/informational unit is particularly important.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5255">
<title id=" W11-0813.xml">an ngram frequency database reference to handle mwe extraction in nlp applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>their importance in the field of natural language processing (nlp) is undeniable.
</prevsent>
<prevsent>although composed of several words, these sequences are nonetheless considered as simple units with regard to part-of-speech at the lexical as well as syntactic levels.
</prevsent>
</prevsection>
<citsent citstr=" W09-2907 ">
their identification is therefore essential to the efficiency of applications such as parsing (nivre and nilsson, 2004), machine translation (ren et al, 2009), <papid> W09-2907 </papid>information extraction, or information retrieval (vechtomova, 2005).</citsent>
<aftsection>
<nextsent>in these systems, the principle of syntactic or semantic/informational unit is particularly important.
</nextsent>
<nextsent>although the identification and extraction of mwes now deliver satisfactory results (evert and krenn, 2001; <papid> P01-1025 </papid>pearce, 2002), their integration into broader applicative context remains problematic (sag et al, 2001).</nextsent>
<nextsent>the explanations for this situation are twofold.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5256">
<title id=" W11-0813.xml">an ngram frequency database reference to handle mwe extraction in nlp applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>their identification is therefore essential to the efficiency of applications such as parsing (nivre and nilsson, 2004), machine translation (ren et al, 2009), <papid> W09-2907 </papid>information extraction, or information retrieval (vechtomova, 2005).</prevsent>
<prevsent>in these systems, the principle of syntactic or semantic/informational unit is particularly important.</prevsent>
</prevsection>
<citsent citstr=" P01-1025 ">
although the identification and extraction of mwes now deliver satisfactory results (evert and krenn, 2001; <papid> P01-1025 </papid>pearce, 2002), their integration into broader applicative context remains problematic (sag et al, 2001).</citsent>
<aftsection>
<nextsent>the explanations for this situation are twofold.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>the most effective extraction methods resort.
</nextsent>
<nextsent>to statistical association measures based on the frequency of lexical structures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5258">
<title id=" W11-0813.xml">an ngram frequency database reference to handle mwe extraction in nlp applications </title>
<section> extraction process.  </section>
<citcontext>
<prevsection>
<prevsent>association measures are conventionally used to automatically determine whether an extracted phrase is an mwe or not.
</prevsent>
<prevsent>they are mathematical functions that aim to capture the degree of cohesion or association between the constituents.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
the most frequently used measures are the log-likelihood ratio (dunning, 1993), <papid> J93-1003 </papid>the mutual information (church and hanks, 1990) <papid> J90-1003 </papid>or the 2 (church and gale, 1991), although up to 82 measures have been considered by pecina and schlesinger (2006).<papid> P06-2084 </papid></citsent>
<aftsection>
<nextsent>in this paper, we did not aim to compare ams, but simply to select some effective ones in order to evaluate the relevance of reference for mwe extraction.
</nextsent>
<nextsent>however, association measures present two main shortcomings that were troublesome for us : they are designed for bigrams, although longer mwes are quite frequent in any corpus 4, and they require the definition of threshold above which an extracted phrase is considered as an mwe.
</nextsent>
<nextsent>the first aspect isvery limiting when dealing with real data where longer units are common.
</nextsent>
<nextsent>the second may be dealt with some experimental process to obtain the optimal value forgiven dataset, but is prone to generalization problems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5259">
<title id=" W11-0813.xml">an ngram frequency database reference to handle mwe extraction in nlp applications </title>
<section> extraction process.  </section>
<citcontext>
<prevsection>
<prevsent>association measures are conventionally used to automatically determine whether an extracted phrase is an mwe or not.
</prevsent>
<prevsent>they are mathematical functions that aim to capture the degree of cohesion or association between the constituents.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
the most frequently used measures are the log-likelihood ratio (dunning, 1993), <papid> J93-1003 </papid>the mutual information (church and hanks, 1990) <papid> J90-1003 </papid>or the 2 (church and gale, 1991), although up to 82 measures have been considered by pecina and schlesinger (2006).<papid> P06-2084 </papid></citsent>
<aftsection>
<nextsent>in this paper, we did not aim to compare ams, but simply to select some effective ones in order to evaluate the relevance of reference for mwe extraction.
</nextsent>
<nextsent>however, association measures present two main shortcomings that were troublesome for us : they are designed for bigrams, although longer mwes are quite frequent in any corpus 4, and they require the definition of threshold above which an extracted phrase is considered as an mwe.
</nextsent>
<nextsent>the first aspect isvery limiting when dealing with real data where longer units are common.
</nextsent>
<nextsent>the second may be dealt with some experimental process to obtain the optimal value forgiven dataset, but is prone to generalization problems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5260">
<title id=" W11-0813.xml">an ngram frequency database reference to handle mwe extraction in nlp applications </title>
<section> extraction process.  </section>
<citcontext>
<prevsection>
<prevsent>association measures are conventionally used to automatically determine whether an extracted phrase is an mwe or not.
</prevsent>
<prevsent>they are mathematical functions that aim to capture the degree of cohesion or association between the constituents.
</prevsent>
</prevsection>
<citsent citstr=" P06-2084 ">
the most frequently used measures are the log-likelihood ratio (dunning, 1993), <papid> J93-1003 </papid>the mutual information (church and hanks, 1990) <papid> J90-1003 </papid>or the 2 (church and gale, 1991), although up to 82 measures have been considered by pecina and schlesinger (2006).<papid> P06-2084 </papid></citsent>
<aftsection>
<nextsent>in this paper, we did not aim to compare ams, but simply to select some effective ones in order to evaluate the relevance of reference for mwe extraction.
</nextsent>
<nextsent>however, association measures present two main shortcomings that were troublesome for us : they are designed for bigrams, although longer mwes are quite frequent in any corpus 4, and they require the definition of threshold above which an extracted phrase is considered as an mwe.
</nextsent>
<nextsent>the first aspect isvery limiting when dealing with real data where longer units are common.
</nextsent>
<nextsent>the second may be dealt with some experimental process to obtain the optimal value forgiven dataset, but is prone to generalization problems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5263">
<title id=" W11-0813.xml">an ngram frequency database reference to handle mwe extraction in nlp applications </title>
<section> reference building.  </section>
<citcontext>
<prevsection>
<prevsent>the second type of information can however be inferred from the frequency of the structures of order n, provided the storage and questioning system is efficient enough for real-time applications.
</prevsent>
<prevsent>the need for efficiency also applies to queries related to the me measure or the local max algorithm that partly involve the use of wildcards.this type of search tool can be efficiently implemented with patricia tree (morrison, 1968).this data structure enables the compression of grams that share common prefix and of the nodes that have only one child.
</prevsent>
</prevsection>
<citsent citstr=" C08-3010 ">
the latter compression is even more effective as most of the n-grams have aunique suffix (sekine, 2008).<papid> C08-3010 </papid></citsent>
<aftsection>
<nextsent>beyond the compression that this structure allows, it also guarantees very fast access to data insofar as query is simple tree traversal that can be done in constant time.in order to further optimize the final data structure, we store the vocabulary in table and associate an integer as unique identifier for every word.
</nextsent>
<nextsent>in this way, we avoid the word repetition (whose size in memory far exceeds that of an integer) in the tree.
</nextsent>
<nextsent>moreover, this technique also enables to speed up the query mechanism, since the keys are smaller.
</nextsent>
<nextsent>we derived two different implementations of thisstructure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5266">
<title id=" W11-0319.xml">automatically building training examples for entity extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is broadly defined as the task of extracting entities of given semantic class from texts (e.g., lists of actors, musicians, cities).
</prevsent>
<prevsent>search engines such as bing, yahoo, and google collect large sets of entities to better interpret queries (tan and peng, 2006), to improve query suggestions (cao et al , 2008) and to understand query intents (hu et al ., 2009).
</prevsent>
</prevsection>
<citsent citstr=" D09-1098 ">
in response, automated techniques for entity extraction have been proposed (pasca, 2007;wang and cohen, 2008; chaudhuri et al , 2009; pan tel et al , 2009).<papid> D09-1098 </papid></citsent>
<aftsection>
<nextsent>there is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (pasca et al , 2006; mirkin et al ,2006).<papid> P06-2075 </papid></nextsent>
<nextsent>this intuition is explored by the ensemble semantics (es) framework proposed by pennacchiotti and pantel (2009), <papid> D09-1025 </papid>which outperforms previous state-of-the-art systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5267">
<title id=" W11-0319.xml">automatically building training examples for entity extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>search engines such as bing, yahoo, and google collect large sets of entities to better interpret queries (tan and peng, 2006), to improve query suggestions (cao et al , 2008) and to understand query intents (hu et al ., 2009).
</prevsent>
<prevsent>in response, automated techniques for entity extraction have been proposed (pasca, 2007;wang and cohen, 2008; chaudhuri et al , 2009; pan tel et al , 2009).<papid> D09-1098 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-2075 ">
there is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (pasca et al , 2006; mirkin et al ,2006).<papid> P06-2075 </papid></citsent>
<aftsection>
<nextsent>this intuition is explored by the ensemble semantics (es) framework proposed by pennacchiotti and pantel (2009), <papid> D09-1025 </papid>which outperforms previous state-of-the-art systems.</nextsent>
<nextsent>a severe limitation of this type of extraction system is its reliance on editorial judgments for building large training sets for each semantic class to be extracted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5268">
<title id=" W11-0319.xml">automatically building training examples for entity extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in response, automated techniques for entity extraction have been proposed (pasca, 2007;wang and cohen, 2008; chaudhuri et al , 2009; pan tel et al , 2009).<papid> D09-1098 </papid></prevsent>
<prevsent>there is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (pasca et al , 2006; mirkin et al ,2006).<papid> P06-2075 </papid></prevsent>
</prevsection>
<citsent citstr=" D09-1025 ">
this intuition is explored by the ensemble semantics (es) framework proposed by pennacchiotti and pantel (2009), <papid> D09-1025 </papid>which outperforms previous state-of-the-art systems.</citsent>
<aftsection>
<nextsent>a severe limitation of this type of extraction system is its reliance on editorial judgments for building large training sets for each semantic class to be extracted.
</nextsent>
<nextsent>this is particularly troublesome for applications such as web search that require large numbers of semantic classes in order to have sufficient coverage of facts and objects (tan and peng, 2006).
</nextsent>
<nextsent>hand-craftingtraining sets across international markets is often infeasible.
</nextsent>
<nextsent>in an exploratory study we estimated that pool of editors would need roughly 300 working days to complete basic set of 100 english classes using the es framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5274">
<title id=" W11-0319.xml">automatically building training examples for entity extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>after the learning phase, the most confident predictions of each classifier on the unlabeled data are used to increase the labeled set of the other.
</prevsent>
<prevsent>these two phases are repeated until stop condition is met.
</prevsent>
</prevsection>
<citsent citstr=" N01-1023 ">
co-traininghas been successfully applied to various applications, such as statistical parsing (sarkar, 2001) <papid> N01-1023 </papid>andweb pages classification (yarowsky, 1998).</citsent>
<aftsection>
<nextsent>self training techniques (or bootsrapping) (yarowsky,1995) <papid> P95-1026 </papid>start with small set of labeled data, and it eratively classify unlabeled data, selecting the most confident predictions as additional training.</nextsent>
<nextsent>self training has been applied in many nlp tasks, such as word sense disambiguation (yarowsky, 1995) <papid> P95-1026 </papid>and relation extraction (hearst, 1992).<papid> C92-2082 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5275">
<title id=" W11-0319.xml">automatically building training examples for entity extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>these two phases are repeated until stop condition is met.
</prevsent>
<prevsent>co-traininghas been successfully applied to various applications, such as statistical parsing (sarkar, 2001) <papid> N01-1023 </papid>andweb pages classification (yarowsky, 1998).</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
self training techniques (or bootsrapping) (yarowsky,1995) <papid> P95-1026 </papid>start with small set of labeled data, and it eratively classify unlabeled data, selecting the most confident predictions as additional training.</citsent>
<aftsection>
<nextsent>self training has been applied in many nlp tasks, such as word sense disambiguation (yarowsky, 1995) <papid> P95-1026 </papid>and relation extraction (hearst, 1992).<papid> C92-2082 </papid></nextsent>
<nextsent>unlike typical semi-supervised approaches, our approach reduces the needed amount of labeled data not by acting on the learning algorithm itself (any algorithm can be used in our approach), but on the method to acquire the labeled training data.our work also relates to the automatic acquisition of labeled negative training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5277">
<title id=" W11-0319.xml">automatically building training examples for entity extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>co-traininghas been successfully applied to various applications, such as statistical parsing (sarkar, 2001) <papid> N01-1023 </papid>andweb pages classification (yarowsky, 1998).</prevsent>
<prevsent>self training techniques (or bootsrapping) (yarowsky,1995) <papid> P95-1026 </papid>start with small set of labeled data, and it eratively classify unlabeled data, selecting the most confident predictions as additional training.</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
self training has been applied in many nlp tasks, such as word sense disambiguation (yarowsky, 1995) <papid> P95-1026 </papid>and relation extraction (hearst, 1992).<papid> C92-2082 </papid></citsent>
<aftsection>
<nextsent>unlike typical semi-supervised approaches, our approach reduces the needed amount of labeled data not by acting on the learning algorithm itself (any algorithm can be used in our approach), but on the method to acquire the labeled training data.our work also relates to the automatic acquisition of labeled negative training data.
</nextsent>
<nextsent>yangarber etal.
</nextsent>
<nextsent>(2002) propose pattern-based bootstrapping approach for harvesting generalized names (e.g., diseases, locations), where labeled negative examples for given class are taken from positive seed examples of competing?
</nextsent>
<nextsent>classes (e.g. examples of diseases are used as negatives for locations).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5278">
<title id=" W11-0319.xml">automatically building training examples for entity extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the approach is semi-supervised, in that it requires some manually annotated seeds.
</prevsent>
<prevsent>the study shows that using competing categories improves the accuracy of the system, by avoiding sematic drift, which isa common cause of divergence in boo strapping approaches.
</prevsent>
</prevsection>
<citsent citstr=" W02-1028 ">
similar approaches are used among others in (thelen and riloff, 2002) <papid> W02-1028 </papid>for learning semantic lexicons, in (collins and singer, 1999) <papid> W99-0613 </papid>for named entity recognition, and in (fagni and sebastiani, 2007) for hierarchical text categorization.</citsent>
<aftsection>
<nextsent>some of our methods relyon the same intuition described above, i.e., using instances of other classes as negative training examples.
</nextsent>
<nextsent>yet, the es framework allows us to add further restrictions to improve the quality of the data.
</nextsent>
<nextsent>we presented simple and general techniques for automatically acquiring training data, and then tested them in the context of the ensemble semantics framework.
</nextsent>
<nextsent>experimental results show that our methods can compete with supervised systems using manually crafted training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5279">
<title id=" W11-0319.xml">automatically building training examples for entity extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the approach is semi-supervised, in that it requires some manually annotated seeds.
</prevsent>
<prevsent>the study shows that using competing categories improves the accuracy of the system, by avoiding sematic drift, which isa common cause of divergence in boo strapping approaches.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
similar approaches are used among others in (thelen and riloff, 2002) <papid> W02-1028 </papid>for learning semantic lexicons, in (collins and singer, 1999) <papid> W99-0613 </papid>for named entity recognition, and in (fagni and sebastiani, 2007) for hierarchical text categorization.</citsent>
<aftsection>
<nextsent>some of our methods relyon the same intuition described above, i.e., using instances of other classes as negative training examples.
</nextsent>
<nextsent>yet, the es framework allows us to add further restrictions to improve the quality of the data.
</nextsent>
<nextsent>we presented simple and general techniques for automatically acquiring training data, and then tested them in the context of the ensemble semantics framework.
</nextsent>
<nextsent>experimental results show that our methods can compete with supervised systems using manually crafted training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5280">
<title id=" W10-4236.xml">helping our own text massaging for computational linguistics as a new shared task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, asa discipline, computational linguistics holds privileged position: as scientists, language (of different varieties) is our object of study, and as technologists,language tasks form our agenda.
</prevsent>
<prevsent>many of the research problems we focus on could assist with writing problems.
</prevsent>
</prevsection>
<citsent citstr=" C08-1109 ">
there is already existing work that addresses specific problems in this area (see, forex ample, (tetreault and chodorow, 2008)), <papid> C08-1109 </papid>but to be genuinely useful, we require solution to the writing problem as whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored.</citsent>
<aftsection>
<nextsent>our proposal, then, is to initiate shared task that attempts to tackle the problem head-on; we want to help our own?
</nextsent>
<nextsent>by developing tools which can help non-native speakers of english (nnss) (and maybe some native ones) write academic english prose of the kind that helps paper get accepted.
</nextsent>
<nextsent>the kinds of assistance we are concerned with here go beyond that which is provided by commonly-available spelling checkers and grammar checkers such as those found in microsoft word (heidorn, 2000).
</nextsent>
<nextsent>the task can be simply expressed as text-to-text generation exercise: 1see http://acl2010.org/mentoring.htm.given text, make edits to the text to improve the quality of the english it contains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5281">
<title id=" W10-4211.xml">generating and validating abstracts of meeting conversations a user study </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>in general, users rate abstract-style summaries much more highly than extracts for these conversations.
</prevsent>
<prevsent>automatic summarizaton has been described as consisting of interpretation, transformation and generation (jones, 1999).
</prevsent>
</prevsection>
<citsent citstr=" P06-2019 ">
popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (knight and marcu, 2000; clarke and lapata,2006) <papid> P06-2019 </papid>or sentence merging (barzilay and mckeown, 2005).<papid> J05-3002 </papid></citsent>
<aftsection>
<nextsent>in contrast, in this work we clearly separate interpretation from transformation and incorporate an nlg component to generate new text to describe meeting conversations.while extraction remains the most common approach to text summarization, one application in which abs tractive summarization is widely used is data-to-text generation.
</nextsent>
<nextsent>summarization is critical for data-to-text generation because the amount of collected data may be massive.
</nextsent>
<nextsent>examples of such applications include the summarization of intensive care unit data in the medical domain (portet et al, 2009) and data from gas turbine sensors (yu et al, 2007).
</nextsent>
<nextsent>our approach is similar except that our input is text data in the form of conversations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5282">
<title id=" W10-4211.xml">generating and validating abstracts of meeting conversations a user study </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>in general, users rate abstract-style summaries much more highly than extracts for these conversations.
</prevsent>
<prevsent>automatic summarizaton has been described as consisting of interpretation, transformation and generation (jones, 1999).
</prevsent>
</prevsection>
<citsent citstr=" J05-3002 ">
popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (knight and marcu, 2000; clarke and lapata,2006) <papid> P06-2019 </papid>or sentence merging (barzilay and mckeown, 2005).<papid> J05-3002 </papid></citsent>
<aftsection>
<nextsent>in contrast, in this work we clearly separate interpretation from transformation and incorporate an nlg component to generate new text to describe meeting conversations.while extraction remains the most common approach to text summarization, one application in which abs tractive summarization is widely used is data-to-text generation.
</nextsent>
<nextsent>summarization is critical for data-to-text generation because the amount of collected data may be massive.
</nextsent>
<nextsent>examples of such applications include the summarization of intensive care unit data in the medical domain (portet et al, 2009) and data from gas turbine sensors (yu et al, 2007).
</nextsent>
<nextsent>our approach is similar except that our input is text data in the form of conversations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5283">
<title id=" W10-4211.xml">generating and validating abstracts of meeting conversations a user study </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>some previous work has compared extracts and abstracts for the task of decision audit (murray et al, 2009) , finding that human abstracts are challenging gold-standard in termsof enabling participants to work quickly and correctly identify the relevant information.
</prevsent>
<prevsent>for that task, automatic extracts and the semi-automatic abstracts of kleinbauer et al (2007) were found to be competitive with one another in terms of user satisfaction and resultant task scores.
</prevsent>
</prevsection>
<citsent citstr=" J02-4005 ">
other research on comparing extracts and abstracts has found that an automatic abs tractor outperforms agen eric extractor in the domains of technical articles (saggion and lapalme, 2002) <papid> J02-4005 </papid>and evaluative reviews (carenini and cheung, 2008), <papid> W08-1106 </papid>and that human-written abstracts were rated best overall.</citsent>
<aftsection>
<nextsent>source document interpretation in our system relies on general conversation ontology.
</nextsent>
<nextsent>the ontology is written in owl/rdf and contains upper level classes such as participant, entity, utterance, and dialogueact.
</nextsent>
<nextsent>when additional information is available about participant roles in given domain, participant subclasses such as project manager can be utilized.
</nextsent>
<nextsent>object properties connect instances of ontology classes; for example, the following entryin the ontology states that the object property has speaker has an instance of utterance as its domain and an instance of participant as its range.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5284">
<title id=" W10-4211.xml">generating and validating abstracts of meeting conversations a user study </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>some previous work has compared extracts and abstracts for the task of decision audit (murray et al, 2009) , finding that human abstracts are challenging gold-standard in termsof enabling participants to work quickly and correctly identify the relevant information.
</prevsent>
<prevsent>for that task, automatic extracts and the semi-automatic abstracts of kleinbauer et al (2007) were found to be competitive with one another in terms of user satisfaction and resultant task scores.
</prevsent>
</prevsection>
<citsent citstr=" W08-1106 ">
other research on comparing extracts and abstracts has found that an automatic abs tractor outperforms agen eric extractor in the domains of technical articles (saggion and lapalme, 2002) <papid> J02-4005 </papid>and evaluative reviews (carenini and cheung, 2008), <papid> W08-1106 </papid>and that human-written abstracts were rated best overall.</citsent>
<aftsection>
<nextsent>source document interpretation in our system relies on general conversation ontology.
</nextsent>
<nextsent>the ontology is written in owl/rdf and contains upper level classes such as participant, entity, utterance, and dialogueact.
</nextsent>
<nextsent>when additional information is available about participant roles in given domain, participant subclasses such as project manager can be utilized.
</nextsent>
<nextsent>object properties connect instances of ontology classes; for example, the following entryin the ontology states that the object property has speaker has an instance of utterance as its domain and an instance of participant as its range.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5285">
<title id=" W10-4211.xml">generating and validating abstracts of meeting conversations a user study </title>
<section> interpretation - ontology mapping.  </section>
<citcontext>
<prevsection>
<prevsent>the entities in conversation are noun phrases with mid-range document frequency.
</prevsent>
<prevsent>this is similar to the definition of concept proposed by xie et al (2009), where n-grams are weighted by tf.idf scores, except that we use noun phrases rather than any n-grams because we want to referto the entities in the generated text.
</prevsent>
</prevsection>
<citsent citstr=" W95-0110 ">
we use midrange document frequency instead of idf (churchand gale, 1995), <papid> W95-0110 </papid>where the entities occur in between 10% and 90% of the documents in the col lection.</citsent>
<aftsection>
<nextsent>we do not currently attempt coreference resolution for entities; recent work has investigated coreference resolution for multi-party dialogues (muller, 2007; gupta et al, 2007), but the challenge of resolution on such noisy data is highlighted by low accuracy (e.g. f-measure of 21.21) compared with using well-formed text.
</nextsent>
<nextsent>we map sentences to our ontology classes by building numerous supervised classifiers trained on labeled decision sentences, action sentences, etc. general extractive classifier is also trained on sentences simply labeled as important.
</nextsent>
<nextsent>we give specific example of the ontology mapping using the following excerpt from the ami corpus, with entities italicized and resulting sentence classifications shown in bold: ? a: and you two are going to work together on prototype using modelling clay.
</nextsent>
<nextsent>[action] ? a: youll get specific instructions from your personal coach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5286">
<title id=" W10-4211.xml">generating and validating abstracts of meeting conversations a user study </title>
<section> interpretation - ontology mapping.  </section>
<citcontext>
<prevsection>
<prevsent>the first set of features we use for this ontology mapping are features relating to conversational structure.
</prevsent>
<prevsent>they include sentence length, sentence position in the conversation and in the current turn, pause-style features, lexical cohesion, centro id scores, and features that measure how terms cluster between conversation participants and conversation turns.
</prevsent>
</prevsection>
<citsent citstr=" D08-1081 ">
while these features have been found to work well for generic extractive summarization (murray and carenini, 2008), <papid> D08-1081 </papid>we use additional features for capturing the more specific sentence-level phenomena of this research.</citsent>
<aftsection>
<nextsent>these include character trigrams, word bigrams, part-of-speech bigrams,word pairs, part-of-speech pairs, and varying in stantiation n-grams, described in more detail in (murray et al, 2010).<papid> N10-1132 </papid></nextsent>
<nextsent>after removing features that occur fewer than five times, we end up with 218,957 total features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5287">
<title id=" W10-4211.xml">generating and validating abstracts of meeting conversations a user study </title>
<section> interpretation - ontology mapping.  </section>
<citcontext>
<prevsection>
<prevsent>they include sentence length, sentence position in the conversation and in the current turn, pause-style features, lexical cohesion, centro id scores, and features that measure how terms cluster between conversation participants and conversation turns.
</prevsent>
<prevsent>while these features have been found to work well for generic extractive summarization (murray and carenini, 2008), <papid> D08-1081 </papid>we use additional features for capturing the more specific sentence-level phenomena of this research.</prevsent>
</prevsection>
<citsent citstr=" N10-1132 ">
these include character trigrams, word bigrams, part-of-speech bigrams,word pairs, part-of-speech pairs, and varying in stantiation n-grams, described in more detail in (murray et al, 2010).<papid> N10-1132 </papid></citsent>
<aftsection>
<nextsent>after removing features that occur fewer than five times, we end up with 218,957 total features.
</nextsent>
<nextsent>3.2 message generation.
</nextsent>
<nextsent>rather than merely classifying individual sentences as decisions, action items, and so on, we also aim to detect larger patterns ? or messages?
</nextsent>
<nextsent>within the meeting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5291">
<title id=" W10-4211.xml">generating and validating abstracts of meeting conversations a user study </title>
<section> task-based user study.  </section>
<citcontext>
<prevsection>
<prevsent>approximately 13% of the total transcript sentences are ultimately labeled as extracted sentences.
</prevsent>
<prevsent>a sentence is considered decision itemif it is linked to the decision portion of the abstract, and action and problem sentences are derived similarly.
</prevsent>
</prevsection>
<citsent citstr=" L08-1475 ">
we additionally use subjectivity and polarity annotations for the ami corpus (wil son, 2008).<papid> L08-1475 </papid></citsent>
<aftsection>
<nextsent>6.2 materials, participants and procedures.
</nextsent>
<nextsent>we selected five ami meetings for this user study, with each stage of the four-stage ami scenario represented.
</nextsent>
<nextsent>the meetings average approximately500 sentences each.
</nextsent>
<nextsent>we included the following three types of summaries for each meeting:(eh) gold-standard human extracts, (ah) gold standard human abstracts described in section 6.1, and (aa) the automatic abstracts output byour abstractor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5292">
<title id=" W11-1708.xml">a link to the past constructing historical social networks </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>cooccurrence counts of name and different interests taken from predefined set are used to determine persons expertise and to enrich their profile.
</prevsent>
<prevsent>these profiles are then used to resolve named entity coreference and to find new connections.
</prevsent>
</prevsection>
<citsent citstr=" P10-1015 ">
elson et al(2010) <papid> P10-1015 </papid>use quoted speech attribution to reconstruct the social networks of the characters in novel.</citsent>
<aftsection>
<nextsent>though this work is most related regarding the type of data used, their method can be considered complementary to ours: where they relate entities based on their conversational interaction without further analysis of the content, we try to find connections based solely on the words that occur in the text.
</nextsent>
<nextsent>efforts in more general relation extraction from text have focused on finding recurring patterns and transforming them into triples (rdf).
</nextsent>
<nextsent>relation types and labels are then deduced from the most common patterns (ravichandran and hovy, 2002; <papid> P02-1006 </papid>culotta etal, 2006).<papid> N06-1038 </papid></nextsent>
<nextsent>these approaches work well for the induction and verification of straightforwardly verbalized facto ids, but they are too restricted to capture the multitude of aspects that surround human interaction; case in point is the kind of relationship between two persons, which people can usually infer from the text, but is rarely explicitly described in single triple.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5293">
<title id=" W11-1708.xml">a link to the past constructing historical social networks </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>though this work is most related regarding the type of data used, their method can be considered complementary to ours: where they relate entities based on their conversational interaction without further analysis of the content, we try to find connections based solely on the words that occur in the text.
</prevsent>
<prevsent>efforts in more general relation extraction from text have focused on finding recurring patterns and transforming them into triples (rdf).
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
relation types and labels are then deduced from the most common patterns (ravichandran and hovy, 2002; <papid> P02-1006 </papid>culotta etal, 2006).<papid> N06-1038 </papid></citsent>
<aftsection>
<nextsent>these approaches work well for the induction and verification of straightforwardly verbalized facto ids, but they are too restricted to capture the multitude of aspects that surround human interaction; case in point is the kind of relationship between two persons, which people can usually infer from the text, but is rarely explicitly described in single triple.
</nextsent>
<nextsent>2.2 sentiment analysis.
</nextsent>
<nextsent>sentiment analysis is concerned with locating and classifying the subjective information contained in asource.
</nextsent>
<nextsent>subjectivity is inherently dependent on human interpretation and emotion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5294">
<title id=" W11-1708.xml">a link to the past constructing historical social networks </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>though this work is most related regarding the type of data used, their method can be considered complementary to ours: where they relate entities based on their conversational interaction without further analysis of the content, we try to find connections based solely on the words that occur in the text.
</prevsent>
<prevsent>efforts in more general relation extraction from text have focused on finding recurring patterns and transforming them into triples (rdf).
</prevsent>
</prevsection>
<citsent citstr=" N06-1038 ">
relation types and labels are then deduced from the most common patterns (ravichandran and hovy, 2002; <papid> P02-1006 </papid>culotta etal, 2006).<papid> N06-1038 </papid></citsent>
<aftsection>
<nextsent>these approaches work well for the induction and verification of straightforwardly verbalized facto ids, but they are too restricted to capture the multitude of aspects that surround human interaction; case in point is the kind of relationship between two persons, which people can usually infer from the text, but is rarely explicitly described in single triple.
</nextsent>
<nextsent>2.2 sentiment analysis.
</nextsent>
<nextsent>sentiment analysis is concerned with locating and classifying the subjective information contained in asource.
</nextsent>
<nextsent>subjectivity is inherently dependent on human interpretation and emotion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5295">
<title id=" W11-1708.xml">a link to the past constructing historical social networks </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>a machine can be taught to mimic these aspects, given enough examples, but the interaction of the two is what makes humans able to understand, for instance, that sarcastic comment is not meant to be taken literally.
</prevsent>
<prevsent>although the general distinction between negative and positive is intuitive for humans to make, subjectivity and sentiment are very much domain and context dependent.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
depending on the domain and context, single sentence can have opposite meanings (pang and lee, 2008).many of the approaches to automatically solv 62 ing tasks like these involve using lists of positively and negatively polarized words or phrases to calculate the overall sentiment of clause, sentence or document (pang et al 2002).<papid> W02-1011 </papid></citsent>
<aftsection>
<nextsent>as shown by kimand hovy (2006), <papid> P06-2063 </papid>the order of the words potentially influences the interpretation of text.</nextsent>
<nextsent>pang et al(2002) <papid> W02-1011 </papid>also found that the simple presence of word is more important than the number of times it appears.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5296">
<title id=" W11-1708.xml">a link to the past constructing historical social networks </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>although the general distinction between negative and positive is intuitive for humans to make, subjectivity and sentiment are very much domain and context dependent.
</prevsent>
<prevsent>depending on the domain and context, single sentence can have opposite meanings (pang and lee, 2008).many of the approaches to automatically solv 62 ing tasks like these involve using lists of positively and negatively polarized words or phrases to calculate the overall sentiment of clause, sentence or document (pang et al 2002).<papid> W02-1011 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-2063 ">
as shown by kimand hovy (2006), <papid> P06-2063 </papid>the order of the words potentially influences the interpretation of text.</citsent>
<aftsection>
<nextsent>pang et al(2002) <papid> W02-1011 </papid>also found that the simple presence of word is more important than the number of times it appears.</nextsent>
<nextsent>word sense disambiguation can be useful tool in determining polarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5298">
<title id=" W11-1708.xml">a link to the past constructing historical social networks </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>pang et al(2002) <papid> W02-1011 </papid>also found that the simple presence of word is more important than the number of times it appears.</prevsent>
<prevsent>word sense disambiguation can be useful tool in determining polarity.</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
turney (2002) <papid> P02-1053 </papid>proposed simple, but seemingly effective way to determine polarity at the word level.</citsent>
<aftsection>
<nextsent>he calculates the difference between the mutual information gain of phrase and the word excellent?
</nextsent>
<nextsent>and of the same phrase and the word poor?.
</nextsent>
<nextsent>3.1 method.
</nextsent>
<nextsent>in contrast to most previous work regarding social network extraction, we do not possess any explicit record of the network we are after.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5300">
<title id=" W11-1412.xml">grasp grammar and syntax based pattern finder in call </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in our pilot teaching experiment, we found learners have problems using articles and prepositions correctly in sentence composition (as high as 80% of the articles and 60% of the prepositions were used incorrectly), and grasp is exactly aimed at helping esl or efl learners in that area.
</prevsent>
<prevsent>until recently, collocations and usage information are compiled mostly manually (benson et al, 1986).
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
with the accessibility to large-scale corpora and powerful computers, it has become common place to compile list of collocations automatically (smadja, 1993).<papid> J93-1007 </papid></citsent>
<aftsection>
<nextsent>in addition, there are many collocation checkers developed to help non-native language learners (chang et al, 2008), or learners of english for academic purposes (durrant, 2009).
</nextsent>
<nextsent>recently, automatic generation of collocations for computational lexicography and online language learning has drawn much attention.
</nextsent>
<nextsent>sketch engine (kilgarriff et al, 2004) summarizes words grammatical and collocation behavior, while justtheword clusters the co-occurring words of single-word queries and tango (jian et al., 2004) <papid> P04-3019 </papid>accommodates cross-lingual collocation searches.</nextsent>
<nextsent>moreover, cheng et al (2006) describe how to retrieve mutually expected words using concgrams.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5301">
<title id=" W11-1412.xml">grasp grammar and syntax based pattern finder in call </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, there are many collocation checkers developed to help non-native language learners (chang et al, 2008), or learners of english for academic purposes (durrant, 2009).
</prevsent>
<prevsent>recently, automatic generation of collocations for computational lexicography and online language learning has drawn much attention.
</prevsent>
</prevsection>
<citsent citstr=" P04-3019 ">
sketch engine (kilgarriff et al, 2004) summarizes words grammatical and collocation behavior, while justtheword clusters the co-occurring words of single-word queries and tango (jian et al., 2004) <papid> P04-3019 </papid>accommodates cross-lingual collocation searches.</citsent>
<aftsection>
<nextsent>moreover, cheng et al (2006) describe how to retrieve mutually expected words using concgrams.
</nextsent>
<nextsent>in contrast, grasp, going one step further, automatically computes and displays the information that reveals the regularities of the contexts of user queries in terms of grammar patterns.
</nextsent>
<nextsent>recent work has been done on incorporating word class information into the analyses of phraseological tendencies.
</nextsent>
<nextsent>stubbs (2004) introduces phrase-frames, which are based on lexical ngrams with variable slots, while wible et al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5302">
<title id=" W11-1412.xml">grasp grammar and syntax based pattern finder in call </title>
<section> the grasp system.  </section>
<citcontext>
<prevsection>
<prevsent>the usage summary, consisting of the querys predominant attendant phraseology ranging from pattern grammar to lexical phrases, is then returned as the output of the system.
</prevsent>
<prevsent>the returned summary, or set of patterns pivoted with both content and function words, can be used for learners?
</prevsent>
</prevsection>
<citsent citstr=" W09-2108 ">
benefits directly, or passed on to an error detection and correction system (e.g., (tsao and wible, 2009) <papid> W09-2108 </papid>and some modules in (gamon et al, 2009) as rules.</citsent>
<aftsection>
<nextsent>therefore, our goal is to return reasonable-sized set of lexical and grammatical patterns characterizing the contexts of the query.
</nextsent>
<nextsent>we now formally state the problem that we are addressing.
</nextsent>
<nextsent>problem statement: we are given reference corpus from wide range of sources and learner search query q. our goal is to construct summary of word usages based on that is likely to represent the lexical or grammatical preferences on qs contexts.
</nextsent>
<nextsent>for this, we transform the words in into sets of (word position, sentence record) pairs such that the context information, whether lexically- or grammatical-oriented, of the querying words is likely to be acquired efficiently.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5304">
<title id=" W11-1412.xml">grasp grammar and syntax based pattern finder in call </title>
<section> the grasp system.  </section>
<citcontext>
<prevsection>
<prevsent>the second concerns the length of the query.
</prevsent>
<prevsent>since single words may be ambiguous in senses and contexts or grammar patterns are closely associated with words?
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
meanings (hunston and francis, 2000), we transform single-word queries into their collocations, particularly focusing on one word sense (yarowsky, 1995), <papid> P95-1026 </papid>as stepping stones to grasp patterns.</citsent>
<aftsection>
<nextsent>notice that, in implementation, users may be allowed to choose their own interested translation or collocation of the query for usage learning.
</nextsent>
<nextsent>the prototypes for first language (i.e., chinese) queries and english queries of any length are at a2 and b3 respectively.
</nextsent>
<nextsent>the goal of cross-lingual grasp is to assist efl users even when they do not know the words of their searches and to avoid incorrect queries largely because of mis collocation, misapplication, and misgeneralization.
</nextsent>
<nextsent>afterwards, we initialize grasp responses to collect usage summaries for queries (step (2)) and leverage inverted files to extract and generate each querys syntax-based contexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5305">
<title id=" W11-1412.xml">grasp grammar and syntax based pattern finder in call </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>we exploited genia tagger to obtain the lemmas and pos tags of cs sentences.
</prevsent>
<prevsent>after lemmatizing and syntactic analyses, all sentences in bnc were used to build up inverted files and used as examples for grammar pattern extraction.
</prevsent>
</prevsection>
<citsent citstr=" W10-1005 ">
5 inspired by (gamon and leacock, 2010).<papid> W10-1005 </papid></citsent>
<aftsection>
<nextsent>99 english (e) sentence with corresponding chinese (c) translation answer to 1st blank answer to 2nd blank c: ?????????????
</nextsent>
<nextsent>e: environmental protection has ___ impact ___.
</nextsent>
<nextsent>a profound on the earth c: ?????????????
</nextsent>
<nextsent>e: the real estate agent ___ record profit ___.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5306">
<title id=" W11-1412.xml">grasp grammar and syntax based pattern finder in call </title>
<section> applying grasp to error correction.  </section>
<citcontext>
<prevsection>
<prevsent>previously, number of interesting rule-based error detection/correction systems have been proposed for some specific error types such as article and preposition error (e.g., (uria et al, 2009), (lee et al, 2009), and some modules in (gamon et al, 2009)).
</prevsent>
<prevsent>statistical approaches, supervised or unsupervised, to grammar checking have become the recent trend.
</prevsent>
</prevsection>
<citsent citstr=" A00-2019 ">
for example, unsupervised systems of (chodorow and leacock, 2000) <papid> A00-2019 </papid>and (tsao and wible, 2009) <papid> W09-2108 </papid>leverage word distributions in general and/or word-specific corpus for detecting erroneous usages while (hermet et al, 2008) <papid> L08-1322 </papid>and (gamon and leacock, 2010) <papid> W10-1005 </papid>use web as corpus.</citsent>
<aftsection>
<nextsent>on the other hand, supervised models, typically treating error detection/correction as classification problem, utilize the training of well-formed texts ((de felice and pulman, 2008) and (tetreault et al, 2010)), <papid> P10-2065 </papid>learner texts, or both pair wisely (brockett et al, 2006).<papid> P06-1032 </papid></nextsent>
<nextsent>moreover, (sun et al, 2007) <papid> P07-1011 </papid>describes way to construct supervised error detection system trained on well-formed and learner texts neither pairwise nor error tagged.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5309">
<title id=" W11-1412.xml">grasp grammar and syntax based pattern finder in call </title>
<section> applying grasp to error correction.  </section>
<citcontext>
<prevsection>
<prevsent>previously, number of interesting rule-based error detection/correction systems have been proposed for some specific error types such as article and preposition error (e.g., (uria et al, 2009), (lee et al, 2009), and some modules in (gamon et al, 2009)).
</prevsent>
<prevsent>statistical approaches, supervised or unsupervised, to grammar checking have become the recent trend.
</prevsent>
</prevsection>
<citsent citstr=" L08-1322 ">
for example, unsupervised systems of (chodorow and leacock, 2000) <papid> A00-2019 </papid>and (tsao and wible, 2009) <papid> W09-2108 </papid>leverage word distributions in general and/or word-specific corpus for detecting erroneous usages while (hermet et al, 2008) <papid> L08-1322 </papid>and (gamon and leacock, 2010) <papid> W10-1005 </papid>use web as corpus.</citsent>
<aftsection>
<nextsent>on the other hand, supervised models, typically treating error detection/correction as classification problem, utilize the training of well-formed texts ((de felice and pulman, 2008) and (tetreault et al, 2010)), <papid> P10-2065 </papid>learner texts, or both pair wisely (brockett et al, 2006).<papid> P06-1032 </papid></nextsent>
<nextsent>moreover, (sun et al, 2007) <papid> P07-1011 </papid>describes way to construct supervised error detection system trained on well-formed and learner texts neither pairwise nor error tagged.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5311">
<title id=" W11-1412.xml">grasp grammar and syntax based pattern finder in call </title>
<section> applying grasp to error correction.  </section>
<citcontext>
<prevsection>
<prevsent>statistical approaches, supervised or unsupervised, to grammar checking have become the recent trend.
</prevsent>
<prevsent>for example, unsupervised systems of (chodorow and leacock, 2000) <papid> A00-2019 </papid>and (tsao and wible, 2009) <papid> W09-2108 </papid>leverage word distributions in general and/or word-specific corpus for detecting erroneous usages while (hermet et al, 2008) <papid> L08-1322 </papid>and (gamon and leacock, 2010) <papid> W10-1005 </papid>use web as corpus.</prevsent>
</prevsection>
<citsent citstr=" P10-2065 ">
on the other hand, supervised models, typically treating error detection/correction as classification problem, utilize the training of well-formed texts ((de felice and pulman, 2008) and (tetreault et al, 2010)), <papid> P10-2065 </papid>learner texts, or both pair wisely (brockett et al, 2006).<papid> P06-1032 </papid></citsent>
<aftsection>
<nextsent>moreover, (sun et al, 2007) <papid> P07-1011 </papid>describes way to construct supervised error detection system trained on well-formed and learner texts neither pairwise nor error tagged.</nextsent>
<nextsent>in contrast to the previous work in grammar checking, our pattern grammar rules are automatically inferred from general corpus (as described in section 3) and helpful for correcting errors resulting from the others (e.g., to close?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5312">
<title id=" W11-1412.xml">grasp grammar and syntax based pattern finder in call </title>
<section> applying grasp to error correction.  </section>
<citcontext>
<prevsection>
<prevsent>statistical approaches, supervised or unsupervised, to grammar checking have become the recent trend.
</prevsent>
<prevsent>for example, unsupervised systems of (chodorow and leacock, 2000) <papid> A00-2019 </papid>and (tsao and wible, 2009) <papid> W09-2108 </papid>leverage word distributions in general and/or word-specific corpus for detecting erroneous usages while (hermet et al, 2008) <papid> L08-1322 </papid>and (gamon and leacock, 2010) <papid> W10-1005 </papid>use web as corpus.</prevsent>
</prevsection>
<citsent citstr=" P06-1032 ">
on the other hand, supervised models, typically treating error detection/correction as classification problem, utilize the training of well-formed texts ((de felice and pulman, 2008) and (tetreault et al, 2010)), <papid> P10-2065 </papid>learner texts, or both pair wisely (brockett et al, 2006).<papid> P06-1032 </papid></citsent>
<aftsection>
<nextsent>moreover, (sun et al, 2007) <papid> P07-1011 </papid>describes way to construct supervised error detection system trained on well-formed and learner texts neither pairwise nor error tagged.</nextsent>
<nextsent>in contrast to the previous work in grammar checking, our pattern grammar rules are automatically inferred from general corpus (as described in section 3) and helpful for correcting errors resulting from the others (e.g., to close?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5313">
<title id=" W11-1412.xml">grasp grammar and syntax based pattern finder in call </title>
<section> applying grasp to error correction.  </section>
<citcontext>
<prevsection>
<prevsent>for example, unsupervised systems of (chodorow and leacock, 2000) <papid> A00-2019 </papid>and (tsao and wible, 2009) <papid> W09-2108 </papid>leverage word distributions in general and/or word-specific corpus for detecting erroneous usages while (hermet et al, 2008) <papid> L08-1322 </papid>and (gamon and leacock, 2010) <papid> W10-1005 </papid>use web as corpus.</prevsent>
<prevsent>on the other hand, supervised models, typically treating error detection/correction as classification problem, utilize the training of well-formed texts ((de felice and pulman, 2008) and (tetreault et al, 2010)), <papid> P10-2065 </papid>learner texts, or both pair wisely (brockett et al, 2006).<papid> P06-1032 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1011 ">
moreover, (sun et al, 2007) <papid> P07-1011 </papid>describes way to construct supervised error detection system trained on well-formed and learner texts neither pairwise nor error tagged.</citsent>
<aftsection>
<nextsent>in contrast to the previous work in grammar checking, our pattern grammar rules are automatically inferred from general corpus (as described in section 3) and helpful for correcting errors resulting from the others (e.g., to close?
</nextsent>
<nextsent>in play ~ role to close?), our pattern grammar lexicalizes on both content and function words and lexical items within may be contiguous (e.g., look forward to v-ing prp?)
</nextsent>
<nextsent>or non-contiguous (e.g., play ~ role in v-ing?), and, with word class (pos) information, error correction or grammatical suggestion is provided at sentence level.
</nextsent>
<nextsent>5.1 error correcting process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5314">
<title id=" W11-0216.xml">a study on dependency tree kernels for automatic extraction of protein protein interaction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some of these approaches are structure kernels (e.g. tree kernels), which carry out structural similarities between instances of relations, represented as phrase structures or dependency trees, in terms of common substructures.
</prevsent>
<prevsent>other kernels simply use techniques such as bag-of-words, subse quences, etc. to map the syntactic and contextual information to flat features, and later compute similarity.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
one variation of tree kernels is the dependency tree (dt) kernel (culotta and sorensen, 2004; <papid> P04-1054 </papid>nguyen et al, 2009).<papid> D09-1143 </papid></citsent>
<aftsection>
<nextsent>a dt kernel (dtk) is tree kernel that is computed on dependency tree(or subtree).
</nextsent>
<nextsent>a dependency tree encodes grammatical relations between words in sentence where the words are nodes, and dependency types (i.e. grammatical functions of children nodes with respect to their parents) are edges.
</nextsent>
<nextsent>the main advantage of dt in comparison with phrase structure tree (pst)is that the former allows for relating two words directly (and in more compact substructures than pst)even if they are far apart in the corresponding sentence according to their lexical word order.several kernel approaches exploit syntactic dependencies among words for ppi extraction from biomedical text in the form of dependency graphs or dependency paths (e.g. kim et al (2010) or airolaet al (2008)).<papid> W08-0601 </papid></nextsent>
<nextsent>however, to the best of our knowledge, there are only few works on the use of dtkernels for this task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5315">
<title id=" W11-0216.xml">a study on dependency tree kernels for automatic extraction of protein protein interaction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some of these approaches are structure kernels (e.g. tree kernels), which carry out structural similarities between instances of relations, represented as phrase structures or dependency trees, in terms of common substructures.
</prevsent>
<prevsent>other kernels simply use techniques such as bag-of-words, subse quences, etc. to map the syntactic and contextual information to flat features, and later compute similarity.
</prevsent>
</prevsection>
<citsent citstr=" D09-1143 ">
one variation of tree kernels is the dependency tree (dt) kernel (culotta and sorensen, 2004; <papid> P04-1054 </papid>nguyen et al, 2009).<papid> D09-1143 </papid></citsent>
<aftsection>
<nextsent>a dt kernel (dtk) is tree kernel that is computed on dependency tree(or subtree).
</nextsent>
<nextsent>a dependency tree encodes grammatical relations between words in sentence where the words are nodes, and dependency types (i.e. grammatical functions of children nodes with respect to their parents) are edges.
</nextsent>
<nextsent>the main advantage of dt in comparison with phrase structure tree (pst)is that the former allows for relating two words directly (and in more compact substructures than pst)even if they are far apart in the corresponding sentence according to their lexical word order.several kernel approaches exploit syntactic dependencies among words for ppi extraction from biomedical text in the form of dependency graphs or dependency paths (e.g. kim et al (2010) or airolaet al (2008)).<papid> W08-0601 </papid></nextsent>
<nextsent>however, to the best of our knowledge, there are only few works on the use of dtkernels for this task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5316">
<title id=" W11-0216.xml">a study on dependency tree kernels for automatic extraction of protein protein interaction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a dt kernel (dtk) is tree kernel that is computed on dependency tree(or subtree).
</prevsent>
<prevsent>a dependency tree encodes grammatical relations between words in sentence where the words are nodes, and dependency types (i.e. grammatical functions of children nodes with respect to their parents) are edges.
</prevsent>
</prevsection>
<citsent citstr=" W08-0601 ">
the main advantage of dt in comparison with phrase structure tree (pst)is that the former allows for relating two words directly (and in more compact substructures than pst)even if they are far apart in the corresponding sentence according to their lexical word order.several kernel approaches exploit syntactic dependencies among words for ppi extraction from biomedical text in the form of dependency graphs or dependency paths (e.g. kim et al (2010) or airolaet al (2008)).<papid> W08-0601 </papid></citsent>
<aftsection>
<nextsent>however, to the best of our knowledge, there are only few works on the use of dtkernels for this task.
</nextsent>
<nextsent>therefore, exploring the potential of dtks applied to different structures is worthwhile research direction.
</nextsent>
<nextsent>a dtk, pioneered by culotta and sorensen (2004), <papid> P04-1054 </papid>is typically applied to the minimal or smallest common subtree that includes target pair of entities.</nextsent>
<nextsent>such subtree reduces 124 figure 1: part of the dt for the sentence the binding epitopes of bmp-2 for bmpr-ia was characterized using bmp-2 mutant proteins?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5321">
<title id=" W11-0216.xml">a study on dependency tree kernels for automatic extraction of protein protein interaction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the second is that previously proposed dt structures can be further improved by introducing simplified representation of the entities as well as augmenting nodes in the dt tree structure with relevant features.
</prevsent>
<prevsent>this paper presents an evaluation of the above assumptions.
</prevsent>
</prevsection>
<citsent citstr=" E06-1015 ">
more specifically, the contributions of this paper are the following: ? we propose the use of new dt structures,which are improvement on the structures defined in nguyen et al (2009) <papid> D09-1143 </papid>with the most general (in terms of substructures) dtk, i.e. partial tree kernel (ptk) (moschitti, 2006<papid> E06-1015 </papid></citsent>
<aftsection>
<nextsent>we firstly propose the use of the un lexicalized ptk (severyn and moschitti, 2010) with our dependency structures, which significantly improves ptk.
</nextsent>
<nextsent>we compare the performance of the proposed dtks on ppi with the one of pst kernels andshow that, on biomedical text, dt kernels perform better.
</nextsent>
<nextsent>finally, we introduce novel approach (calledmildly extended dependency tree (medt) ker nel1, which achieves the best performance among various (both dt and pst) tree kernels.the remainder of the paper is organized as follows.
</nextsent>
<nextsent>in section 2, we introduce tree kernels andre lation extraction and we also review previous work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5333">
<title id=" W11-0216.xml">a study on dependency tree kernels for automatic extraction of protein protein interaction </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>an extra node is inserted as parent ofthe corresponding ne, labeled with the ne category.
</prevsent>
<prevsent>only words are considered in this tree.
</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
grammatical relation (gr) tree: gr tree is similar to dw tree except that words are replaced by their grammatical functions, e.g. prep, nsubj, etc. 2convolution kernels aim to capture structural information in term of sub-structures, providing viable alternative to flat features (moschitti, 2004).<papid> P04-1043 </papid></citsent>
<aftsection>
<nextsent>grammatical relation and words (grw) tree: grw tree is the minimal subtree that uses both words and grammatical functions, where the latter are inserted as parent nodes of the for mer.using ptk for the above dependency tree structures, the authors achieved an f-measure of 56.3 (for dw), 60.2 (for gr) and 58.5 (for grw) on the ace 2004 corpus3.moschitti (2004) <papid> P04-1043 </papid>proposed the so called path enclosed tree (pet)4 of pst for semantic role labeling.</nextsent>
<nextsent>this was later adapted by zhang et al (2005) for relation extraction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5335">
<title id=" W11-0216.xml">a study on dependency tree kernels for automatic extraction of protein protein interaction </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>grammatical relation and words (grw) tree: grw tree is the minimal subtree that uses both words and grammatical functions, where the latter are inserted as parent nodes of the for mer.using ptk for the above dependency tree structures, the authors achieved an f-measure of 56.3 (for dw), 60.2 (for gr) and 58.5 (for grw) on the ace 2004 corpus3.moschitti (2004) <papid> P04-1043 </papid>proposed the so called path enclosed tree (pet)4 of pst for semantic role labeling.</prevsent>
<prevsent>this was later adapted by zhang et al (2005) for relation extraction.</prevsent>
</prevsection>
<citsent citstr=" D07-1076 ">
a pet is the smallest common subtree of pst, which includes the two entities involved in relation.zhou et al (2007) <papid> D07-1076 </papid>proposed the so called context sensitive tree kernel approach based on pst, which expands pet to include necessary contextual in formation.</citsent>
<aftsection>
<nextsent>the expansion is carried out by some heuristics tuned on the target re task.nguyen et al (2009) <papid> D09-1143 </papid>improved the pet representation by inserting extra nodes for denoting the ne category of the entities inside the subtree.</nextsent>
<nextsent>they also used sequence kernels from tree paths, which provided higher accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5339">
<title id=" W11-0216.xml">a study on dependency tree kernels for automatic extraction of protein protein interaction </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>each of these corpora is known as converted corpus of the corresponding original corpus.
</prevsent>
<prevsent>several kernel-based re approaches have been reported to date for the ppi task.
</prevsent>
</prevsection>
<citsent citstr=" H05-1091 ">
these are based on various methods such as sub sequence kernel (lodhiet al, 2002; bunescu and mooney, 2006), dependency graph kernel (bunescu and mooney, 2005),<papid> H05-1091 </papid>etc. different work exploited dependency analyses with different kernel approaches such as bag-of 3http://projects.ldc.upenn.edu/ace/ 4also known as shortest path-enclosed tree or spt (zhou et al., 2007).<papid> D07-1076 </papid></citsent>
<aftsection>
<nextsent>126 words kernel (e.g. miwa et al (2009)), graph based kernel (e.g. kim et al (2010)), etc. however, there are only few researches that attempted the exploitation of tree kernels on dependency tree structures.
</nextsent>
<nextsent>street al (2007) used dt kernels on aimed corpus and achieved an f-score of 37.1.
</nextsent>
<nextsent>the results were far better when they combined the output of the dependency parser with that of head driven phrase structure grammar (hpsg) parser, and applied tree kernel on it.
</nextsent>
<nextsent>miwa et al (2009) also proposed hybrid kernel 5, which is composition of all-dependency-paths kernel (airola et al, 2008), <papid> W08-0601 </papid>bag-of-words kernel and sst kernel.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5342">
<title id=" W11-0216.xml">a study on dependency tree kernels for automatic extraction of protein protein interaction </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>6 of miwa et al (2009)), word (that belongs to the shortest path) has as many node representations as the number of dependency relations with other words (those belonging to the shortest path).
</prevsent>
<prevsent>perhaps, this redundancy of information might be the reason their approach achieved higher result.
</prevsent>
</prevsection>
<citsent citstr=" D07-1024 ">
in addition to work on ppi pair extraction,there has been some approaches that exploited dependency parse analyses along with kernel methods for identifying sentences that might contain ppi pairs (e.g. erkan et al (2007)).<papid> D07-1024 </papid>in this paper, we focus on finding the best representation based on single structure.</citsent>
<aftsection>
<nextsent>we speculate that this can be helpful to improve the state-of-theart using several combinations of structures and features.
</nextsent>
<nextsent>as first step, we decided to use uptk, which is more robust to over fitting as the description in the next section unveil.
</nextsent>
<nextsent>5the term hybrid kernel?
</nextsent>
<nextsent>is identical to combined kernel?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5357">
<title id=" W11-0216.xml">a study on dependency tree kernels for automatic extraction of protein protein interaction </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>another source of uncertainty is given by the tool for tree kernel computation, whichin their case is not mentioned.
</prevsent>
<prevsent>moreover, their description of pt and sst (in figure 1 of their paper) appears to be imprecise: for example, in (partial orcomplete) phrase structure trees, words can only appear as leaves but in their figure they appear as nonterminal nodes.
</prevsent>
</prevsection>
<citsent citstr=" E06-1051 ">
the comparison with other kernel approaches (i.e. not necessarily tree kernels on dt or pst) shows that there are model achieving higher results (e.g. giuliano et al (2006), <papid> E06-1051 </papid>kim et al (2010), airola et al.</citsent>
<aftsection>
<nextsent>(2008), etc).
</nextsent>
<nextsent>state-of-the-art results on most of the ppi datasets are obtained by the hybrid kernel presented in miwa et al (2009).
</nextsent>
<nextsent>as noted earlier, our work focuses on the design of an effective dtk for ppi that can be combined with others and thatcan hopefully be used to design state-of-the-art hybrid kernels.
</nextsent>
<nextsent>in this paper, we have proposed study of ppi extraction from specific biomedical data based on tree kernels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5358">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two main parts of the method are benchmark database and several well-designed evaluation metrics.
</prevsent>
<prevsent>its feasibility has been proven in the english language.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
after the release of the penn treebank (ptb) (marcus et al, 1993) <papid> J93-2004 </papid>and the parseval metrics (black et al, 1991), <papid> H91-1060 </papid>some new corpus based syntactic parsing techniques were explored in the english language.</citsent>
<aftsection>
<nextsent>based on them, many state-of-art english parser were built, including the well-known collins parser (collins, 2003), <papid> J03-4003 </papid>charniak parser (charniak and johnson, 2005) <papid> P05-1022 </papid>and berkeley parser (petrov and klein, 2007).<papid> N07-1051 </papid></nextsent>
<nextsent>by automatically transforming the constituent structure trees annotated in ptb to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (hockenmaier and steedman, 2007), <papid> J07-3004 </papid>many syntactic parser other than the cfg formalism were also developed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5359">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two main parts of the method are benchmark database and several well-designed evaluation metrics.
</prevsent>
<prevsent>its feasibility has been proven in the english language.
</prevsent>
</prevsection>
<citsent citstr=" H91-1060 ">
after the release of the penn treebank (ptb) (marcus et al, 1993) <papid> J93-2004 </papid>and the parseval metrics (black et al, 1991), <papid> H91-1060 </papid>some new corpus based syntactic parsing techniques were explored in the english language.</citsent>
<aftsection>
<nextsent>based on them, many state-of-art english parser were built, including the well-known collins parser (collins, 2003), <papid> J03-4003 </papid>charniak parser (charniak and johnson, 2005) <papid> P05-1022 </papid>and berkeley parser (petrov and klein, 2007).<papid> N07-1051 </papid></nextsent>
<nextsent>by automatically transforming the constituent structure trees annotated in ptb to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (hockenmaier and steedman, 2007), <papid> J07-3004 </papid>many syntactic parser other than the cfg formalism were also developed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5360">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>its feasibility has been proven in the english language.
</prevsent>
<prevsent>after the release of the penn treebank (ptb) (marcus et al, 1993) <papid> J93-2004 </papid>and the parseval metrics (black et al, 1991), <papid> H91-1060 </papid>some new corpus based syntactic parsing techniques were explored in the english language.</prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
based on them, many state-of-art english parser were built, including the well-known collins parser (collins, 2003), <papid> J03-4003 </papid>charniak parser (charniak and johnson, 2005) <papid> P05-1022 </papid>and berkeley parser (petrov and klein, 2007).<papid> N07-1051 </papid></citsent>
<aftsection>
<nextsent>by automatically transforming the constituent structure trees annotated in ptb to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (hockenmaier and steedman, 2007), <papid> J07-3004 </papid>many syntactic parser other than the cfg formalism were also developed.</nextsent>
<nextsent>these include malt parser (ni vre et al, 2007), mst parser (mcdonald et al, 2005), <papid> H05-1066 </papid>stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and c&c; parser (clark and curran, 2007).<papid> J07-4004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5361">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>its feasibility has been proven in the english language.
</prevsent>
<prevsent>after the release of the penn treebank (ptb) (marcus et al, 1993) <papid> J93-2004 </papid>and the parseval metrics (black et al, 1991), <papid> H91-1060 </papid>some new corpus based syntactic parsing techniques were explored in the english language.</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
based on them, many state-of-art english parser were built, including the well-known collins parser (collins, 2003), <papid> J03-4003 </papid>charniak parser (charniak and johnson, 2005) <papid> P05-1022 </papid>and berkeley parser (petrov and klein, 2007).<papid> N07-1051 </papid></citsent>
<aftsection>
<nextsent>by automatically transforming the constituent structure trees annotated in ptb to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (hockenmaier and steedman, 2007), <papid> J07-3004 </papid>many syntactic parser other than the cfg formalism were also developed.</nextsent>
<nextsent>these include malt parser (ni vre et al, 2007), mst parser (mcdonald et al, 2005), <papid> H05-1066 </papid>stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and c&c; parser (clark and curran, 2007).<papid> J07-4004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5362">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>its feasibility has been proven in the english language.
</prevsent>
<prevsent>after the release of the penn treebank (ptb) (marcus et al, 1993) <papid> J93-2004 </papid>and the parseval metrics (black et al, 1991), <papid> H91-1060 </papid>some new corpus based syntactic parsing techniques were explored in the english language.</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
based on them, many state-of-art english parser were built, including the well-known collins parser (collins, 2003), <papid> J03-4003 </papid>charniak parser (charniak and johnson, 2005) <papid> P05-1022 </papid>and berkeley parser (petrov and klein, 2007).<papid> N07-1051 </papid></citsent>
<aftsection>
<nextsent>by automatically transforming the constituent structure trees annotated in ptb to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (hockenmaier and steedman, 2007), <papid> J07-3004 </papid>many syntactic parser other than the cfg formalism were also developed.</nextsent>
<nextsent>these include malt parser (ni vre et al, 2007), mst parser (mcdonald et al, 2005), <papid> H05-1066 </papid>stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and c&c; parser (clark and curran, 2007).<papid> J07-4004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5363">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after the release of the penn treebank (ptb) (marcus et al, 1993) <papid> J93-2004 </papid>and the parseval metrics (black et al, 1991), <papid> H91-1060 </papid>some new corpus based syntactic parsing techniques were explored in the english language.</prevsent>
<prevsent>based on them, many state-of-art english parser were built, including the well-known collins parser (collins, 2003), <papid> J03-4003 </papid>charniak parser (charniak and johnson, 2005) <papid> P05-1022 </papid>and berkeley parser (petrov and klein, 2007).<papid> N07-1051 </papid></prevsent>
</prevsection>
<citsent citstr=" J07-3004 ">
by automatically transforming the constituent structure trees annotated in ptb to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (hockenmaier and steedman, 2007), <papid> J07-3004 </papid>many syntactic parser other than the cfg formalism were also developed.</citsent>
<aftsection>
<nextsent>these include malt parser (ni vre et al, 2007), mst parser (mcdonald et al, 2005), <papid> H05-1066 </papid>stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and c&c; parser (clark and curran, 2007).<papid> J07-4004 </papid></nextsent>
<nextsent>based on the penn chinese treebank (ctb) (xue et al, 2002) <papid> C02-1145 </papid>developed on the similar annotation scheme of ptb, these parsing techniques were also transferred to the chinese language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5364">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on them, many state-of-art english parser were built, including the well-known collins parser (collins, 2003), <papid> J03-4003 </papid>charniak parser (charniak and johnson, 2005) <papid> P05-1022 </papid>and berkeley parser (petrov and klein, 2007).<papid> N07-1051 </papid></prevsent>
<prevsent>by automatically transforming the constituent structure trees annotated in ptb to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (hockenmaier and steedman, 2007), <papid> J07-3004 </papid>many syntactic parser other than the cfg formalism were also developed.</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
these include malt parser (ni vre et al, 2007), mst parser (mcdonald et al, 2005), <papid> H05-1066 </papid>stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and c&c; parser (clark and curran, 2007).<papid> J07-4004 </papid></citsent>
<aftsection>
<nextsent>based on the penn chinese treebank (ctb) (xue et al, 2002) <papid> C02-1145 </papid>developed on the similar annotation scheme of ptb, these parsing techniques were also transferred to the chinese language.</nextsent>
<nextsent>(levy and manning, 2003) <papid> P03-1056 </papid>explored the feasibility of applying lexicalized pcfg in chinese.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5365">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on them, many state-of-art english parser were built, including the well-known collins parser (collins, 2003), <papid> J03-4003 </papid>charniak parser (charniak and johnson, 2005) <papid> P05-1022 </papid>and berkeley parser (petrov and klein, 2007).<papid> N07-1051 </papid></prevsent>
<prevsent>by automatically transforming the constituent structure trees annotated in ptb to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (hockenmaier and steedman, 2007), <papid> J07-3004 </papid>many syntactic parser other than the cfg formalism were also developed.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
these include malt parser (ni vre et al, 2007), mst parser (mcdonald et al, 2005), <papid> H05-1066 </papid>stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and c&c; parser (clark and curran, 2007).<papid> J07-4004 </papid></citsent>
<aftsection>
<nextsent>based on the penn chinese treebank (ctb) (xue et al, 2002) <papid> C02-1145 </papid>developed on the similar annotation scheme of ptb, these parsing techniques were also transferred to the chinese language.</nextsent>
<nextsent>(levy and manning, 2003) <papid> P03-1056 </papid>explored the feasibility of applying lexicalized pcfg in chinese.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5366">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on them, many state-of-art english parser were built, including the well-known collins parser (collins, 2003), <papid> J03-4003 </papid>charniak parser (charniak and johnson, 2005) <papid> P05-1022 </papid>and berkeley parser (petrov and klein, 2007).<papid> N07-1051 </papid></prevsent>
<prevsent>by automatically transforming the constituent structure trees annotated in ptb to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (hockenmaier and steedman, 2007), <papid> J07-3004 </papid>many syntactic parser other than the cfg formalism were also developed.</prevsent>
</prevsection>
<citsent citstr=" J07-4004 ">
these include malt parser (ni vre et al, 2007), mst parser (mcdonald et al, 2005), <papid> H05-1066 </papid>stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and c&c; parser (clark and curran, 2007).<papid> J07-4004 </papid></citsent>
<aftsection>
<nextsent>based on the penn chinese treebank (ctb) (xue et al, 2002) <papid> C02-1145 </papid>developed on the similar annotation scheme of ptb, these parsing techniques were also transferred to the chinese language.</nextsent>
<nextsent>(levy and manning, 2003) <papid> P03-1056 </papid>explored the feasibility of applying lexicalized pcfg in chinese.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5367">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by automatically transforming the constituent structure trees annotated in ptb to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (hockenmaier and steedman, 2007), <papid> J07-3004 </papid>many syntactic parser other than the cfg formalism were also developed.</prevsent>
<prevsent>these include malt parser (ni vre et al, 2007), mst parser (mcdonald et al, 2005), <papid> H05-1066 </papid>stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and c&c; parser (clark and curran, 2007).<papid> J07-4004 </papid></prevsent>
</prevsection>
<citsent citstr=" C02-1145 ">
based on the penn chinese treebank (ctb) (xue et al, 2002) <papid> C02-1145 </papid>developed on the similar annotation scheme of ptb, these parsing techniques were also transferred to the chinese language.</citsent>
<aftsection>
<nextsent>(levy and manning, 2003) <papid> P03-1056 </papid>explored the feasibility of applying lexicalized pcfg in chinese.</nextsent>
<nextsent>(li et al, 2010) <papid> P10-1113 </papid>proposed joint syntactic and semantic model for parsing chinese.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5368">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these include malt parser (ni vre et al, 2007), mst parser (mcdonald et al, 2005), <papid> H05-1066 </papid>stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>and c&c; parser (clark and curran, 2007).<papid> J07-4004 </papid></prevsent>
<prevsent>based on the penn chinese treebank (ctb) (xue et al, 2002) <papid> C02-1145 </papid>developed on the similar annotation scheme of ptb, these parsing techniques were also transferred to the chinese language.</prevsent>
</prevsection>
<citsent citstr=" P03-1056 ">
(levy and manning, 2003) <papid> P03-1056 </papid>explored the feasibility of applying lexicalized pcfg in chinese.</citsent>
<aftsection>
<nextsent>(li et al, 2010) <papid> P10-1113 </papid>proposed joint syntactic and semantic model for parsing chinese.</nextsent>
<nextsent>but till now, there is not good chinese parser whose performance can approach the state-of-art english parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5369">
<title id=" W10-4143.xml">chinese syntactic parsing evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on the penn chinese treebank (ctb) (xue et al, 2002) <papid> C02-1145 </papid>developed on the similar annotation scheme of ptb, these parsing techniques were also transferred to the chinese language.</prevsent>
<prevsent>(levy and manning, 2003) <papid> P03-1056 </papid>explored the feasibility of applying lexicalized pcfg in chinese.</prevsent>
</prevsection>
<citsent citstr=" P10-1113 ">
(li et al, 2010) <papid> P10-1113 </papid>proposed joint syntactic and semantic model for parsing chinese.</citsent>
<aftsection>
<nextsent>but till now, there is not good chinese parser whose performance can approach the state-of-art english parser.
</nextsent>
<nextsent>it is still an open challenge for parsing chinese sentences due to some special characteristics of the chinese language.
</nextsent>
<nextsent>we need to find suitable benchmark database and evaluation metrics for the chinese language.
</nextsent>
<nextsent>last year, we organized the first chinese syntactic parsing evaluation --- cips-parseval-2009 (zhou and zhu, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5370">
<title id=" W11-0223.xml">self training and co training in biomedical word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>supervised learning achieves better performance compared to other wsd approaches (jimeno-yepes et al, 2011).
</prevsent>
<prevsent>manual annotation requires large level of human effort whereas there is large quantity of unlabeled data.
</prevsent>
</prevsection>
<citsent citstr=" W04-2405 ">
our work follows (mihalcea, 2004) <papid> W04-2405 </papid>but is applied to the biomedical domain; it relies on two semi-supervised learning algorithms.we have performed experiments of semi supervised learning for word sense disambiguation in the biomedical domain.</citsent>
<aftsection>
<nextsent>in the following section, we present the evaluated algorithms.
</nextsent>
<nextsent>then, we present preliminary results for self-training and co-training, which show modest improvement with common set-up of the algorithms for the evaluated ambiguous words.
</nextsent>
<nextsent>for self-training we use the definition by (clark et al., 2003): <papid> W03-0407 </papid>tagger that is retrained on its own labeled cache on each round?.</nextsent>
<nextsent>the classifier is trained on the available training data which is then used to label the unlabeled examples from which the ones with enough prediction confidence are selected and added to the training set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5371">
<title id=" W11-0223.xml">self training and co training in biomedical word sense disambiguation </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>in the following section, we present the evaluated algorithms.
</prevsent>
<prevsent>then, we present preliminary results for self-training and co-training, which show modest improvement with common set-up of the algorithms for the evaluated ambiguous words.
</prevsent>
</prevsection>
<citsent citstr=" W03-0407 ">
for self-training we use the definition by (clark et al., 2003): <papid> W03-0407 </papid>tagger that is retrained on its own labeled cache on each round?.</citsent>
<aftsection>
<nextsent>the classifier is trained on the available training data which is then used to label the unlabeled examples from which the ones with enough prediction confidence are selected and added to the training set.
</nextsent>
<nextsent>the process is repeated for number of predefined iterations.
</nextsent>
<nextsent>co-training (blum and mitchell, 1998) uses several classifiers trained on independent views of the same instances.
</nextsent>
<nextsent>these classifiers are then used to label the unlabeled set, and from this newly annotated dataset the annotations with higher prediction probability are selected.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5372">
<title id=" W11-0504.xml">who wrote what where analyzing the content of human and automatic summaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with all their near-perfect language understanding and world knowledge, two human summarizers will still produce two different summaries of the same text, simply because they will disagree on whatsimportant.
</prevsent>
<prevsent>fortunately, usually some of this information will overlap.
</prevsent>
</prevsection>
<citsent citstr=" N04-1019 ">
this is represented by the idea behind the pyramid evaluation framework (nenkova and passonneau, 2004; <papid> N04-1019 </papid>passonneau et al, 2005), where different levels of the pyramid represent the proportion of concepts (summary content units?,or scus) mentioned by 1 to summarizers in summaries of the same text.</citsent>
<aftsection>
<nextsent>usually, there are very few scus that are mentioned by all summarizers, few more that are mentioned by some of them, and the greatest proportion are the scus that are mentioned by individual summarizers only.
</nextsent>
<nextsent>this variance in what should be gold standard makes research in automatic summarization methods particularly difficult.
</nextsent>
<nextsent>how can we reach goal so vague and under-defined?
</nextsent>
<nextsent>using term frequency to determine important concepts in text has pro vento be very successful, largely because of its simplicity and universal applicability, but statistical methods can only provide the most basic level of performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5373">
<title id=" W11-0504.xml">who wrote what where analyzing the content of human and automatic summaries </title>
<section> topic-specific summarization.  </section>
<citcontext>
<prevsection>
<prevsent>the idea that different types of stories might require different approaches is not new, although the classification varies from task to task.
</prevsent>
<prevsent>topic categories were present in document understanding confer ence2 (duc) 2001, where topics were divided into: single-event, single-subject, biographical, multiple events of same type, and opinion.
</prevsent>
</prevsection>
<citsent citstr=" P08-1094 ">
in their analysis of these results, nenkova and louis (2008) <papid> P08-1094 </papid>find that summaries of articles in what they call topic cohesive categories (single-event, single-subject, biography) are of higher quality than those in non cohesive categories (opinion, multiple event).</citsent>
<aftsection>
<nextsent>in essence, categorizing topics into types is basedon the assumption that stories of the same type follow specific template and include the same kinds of facts, and this predictability might be employed to improve the summarization process, since we at least know what kinds of information are important and what to look for.
</nextsent>
<nextsent>this was shown, among others, by bagga (1997), who analyzed source articles used in the message understanding conference (muc) and graphed the distribution of facts in articles on air vehicle launches, terrorist attacks, joint ventures, and corporate personnel changes, finding that thesame kinds of facts appeared repeatedly.
</nextsent>
<nextsent>a natural conclusion is that information extraction (ie) methods might be helpful here, and in fact, white et al (2001) <papid> H01-1054 </papid>presented an ie-based summarization system for natural disasters, where they first filled 2http://www-nlpir.nist.gov/projects/duc/ an ie template with slots related to date, location, type of disaster, damage (people, physical effects), etc. similarly, radev and mckeown (1998) <papid> J98-3005 </papid>used ie combined with natural language generation (nlg) in their summon system.</nextsent>
<nextsent>there are two ways to classify stories: according to their level of cohesiveness (to use the distinction made by nenkova and louis (2008)), <papid> P08-1094 </papid>and according to subject.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5374">
<title id=" W11-0504.xml">who wrote what where analyzing the content of human and automatic summaries </title>
<section> topic-specific summarization.  </section>
<citcontext>
<prevsection>
<prevsent>in essence, categorizing topics into types is basedon the assumption that stories of the same type follow specific template and include the same kinds of facts, and this predictability might be employed to improve the summarization process, since we at least know what kinds of information are important and what to look for.
</prevsent>
<prevsent>this was shown, among others, by bagga (1997), who analyzed source articles used in the message understanding conference (muc) and graphed the distribution of facts in articles on air vehicle launches, terrorist attacks, joint ventures, and corporate personnel changes, finding that thesame kinds of facts appeared repeatedly.
</prevsent>
</prevsection>
<citsent citstr=" H01-1054 ">
a natural conclusion is that information extraction (ie) methods might be helpful here, and in fact, white et al (2001) <papid> H01-1054 </papid>presented an ie-based summarization system for natural disasters, where they first filled 2http://www-nlpir.nist.gov/projects/duc/ an ie template with slots related to date, location, type of disaster, damage (people, physical effects), etc. similarly, radev and mckeown (1998) <papid> J98-3005 </papid>used ie combined with natural language generation (nlg) in their summon system.</citsent>
<aftsection>
<nextsent>there are two ways to classify stories: according to their level of cohesiveness (to use the distinction made by nenkova and louis (2008)), <papid> P08-1094 </papid>and according to subject.</nextsent>
<nextsent>the first classification could helpus determine which topics would be easier for automatic summarization, but the difficulty is related purely to lexical characteristics of the text; as shownin louis and nenkova (2009), <papid> E09-1062 </papid>source document similarity in terms of word overlap is one of the predictive features of multi-document summary qual ity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5375">
<title id=" W11-0504.xml">who wrote what where analyzing the content of human and automatic summaries </title>
<section> topic-specific summarization.  </section>
<citcontext>
<prevsection>
<prevsent>in essence, categorizing topics into types is basedon the assumption that stories of the same type follow specific template and include the same kinds of facts, and this predictability might be employed to improve the summarization process, since we at least know what kinds of information are important and what to look for.
</prevsent>
<prevsent>this was shown, among others, by bagga (1997), who analyzed source articles used in the message understanding conference (muc) and graphed the distribution of facts in articles on air vehicle launches, terrorist attacks, joint ventures, and corporate personnel changes, finding that thesame kinds of facts appeared repeatedly.
</prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
a natural conclusion is that information extraction (ie) methods might be helpful here, and in fact, white et al (2001) <papid> H01-1054 </papid>presented an ie-based summarization system for natural disasters, where they first filled 2http://www-nlpir.nist.gov/projects/duc/ an ie template with slots related to date, location, type of disaster, damage (people, physical effects), etc. similarly, radev and mckeown (1998) <papid> J98-3005 </papid>used ie combined with natural language generation (nlg) in their summon system.</citsent>
<aftsection>
<nextsent>there are two ways to classify stories: according to their level of cohesiveness (to use the distinction made by nenkova and louis (2008)), <papid> P08-1094 </papid>and according to subject.</nextsent>
<nextsent>the first classification could helpus determine which topics would be easier for automatic summarization, but the difficulty is related purely to lexical characteristics of the text; as shownin louis and nenkova (2009), <papid> E09-1062 </papid>source document similarity in terms of word overlap is one of the predictive features of multi-document summary qual ity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5377">
<title id=" W11-0504.xml">who wrote what where analyzing the content of human and automatic summaries </title>
<section> topic-specific summarization.  </section>
<citcontext>
<prevsection>
<prevsent>a natural conclusion is that information extraction (ie) methods might be helpful here, and in fact, white et al (2001) <papid> H01-1054 </papid>presented an ie-based summarization system for natural disasters, where they first filled 2http://www-nlpir.nist.gov/projects/duc/ an ie template with slots related to date, location, type of disaster, damage (people, physical effects), etc. similarly, radev and mckeown (1998) <papid> J98-3005 </papid>used ie combined with natural language generation (nlg) in their summon system.</prevsent>
<prevsent>there are two ways to classify stories: according to their level of cohesiveness (to use the distinction made by nenkova and louis (2008)), <papid> P08-1094 </papid>and according to subject.</prevsent>
</prevsection>
<citsent citstr=" E09-1062 ">
the first classification could helpus determine which topics would be easier for automatic summarization, but the difficulty is related purely to lexical characteristics of the text; as shownin louis and nenkova (2009), <papid> E09-1062 </papid>source document similarity in terms of word overlap is one of the predictive features of multi-document summary qual ity.</citsent>
<aftsection>
<nextsent>the second classification, according to subject matter, is what enables us to utilize more meaning oriented approaches such as ie and attempt deeper semantic analysis of the source text, and is what we describe in this paper.
</nextsent>
<nextsent>the new guided summarization task in 2010 was designed with the second classification in mind, in order to afford the participants chance to explore deeper linguistic methods of text analysis.
</nextsent>
<nextsent>there were five topic categories: (1) accidents and natural disasters, (2) attacks (crimi nal/terrorist), (3) health and safety, (4) endangered resources, and (5) trials and investigations (criminal/legal/other).3 in contrast to previous topic specific summarization tasks, the guided summarization task also provided list of required aspects, which described the type of information that should be included in the summary (if such information could be found in source documents).
</nextsent>
<nextsent>summarizers also had the option of including any other information they deemed important to the topic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5378">
<title id=" W11-1307.xml">shared task system description measuring the compositionality of bigrams using statistical methodologies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>reasonable results have been obtained in terms of average point difference and coarse precision.
</prevsent>
<prevsent>the present work examines the relative compositionality of adjective-noun (adj-nn; e.g., blue chip), verb-subject (v-subj; where noun acting as subject of verb, e.g., name imply) and verb-object (v-obj; where noun acting as an object of verb, e.g., beg question) combinations using collocation based statistical approaches.
</prevsent>
</prevsection>
<citsent citstr=" P05-1068 ">
measuring the relative compositionality is useful in applications such as machine translation where the highly non compositional collocations can be handled in special way (hwang and sasaki, 2005).<papid> P05-1068 </papid></citsent>
<aftsection>
<nextsent>multi-word expressions (mwes) are sequences of words that tend to co-occur more frequently than chance and are either idiosyncratic or decomposable into multiple simple words (baldwin, 2006).<papid> W06-1201 </papid></nextsent>
<nextsent>deciding idiomaticity of mwes is highly important for machine translation, information retrieval, question answering, lexical acquisition, parsing and language generation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5379">
<title id=" W11-1307.xml">shared task system description measuring the compositionality of bigrams using statistical methodologies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the present work examines the relative compositionality of adjective-noun (adj-nn; e.g., blue chip), verb-subject (v-subj; where noun acting as subject of verb, e.g., name imply) and verb-object (v-obj; where noun acting as an object of verb, e.g., beg question) combinations using collocation based statistical approaches.
</prevsent>
<prevsent>measuring the relative compositionality is useful in applications such as machine translation where the highly non compositional collocations can be handled in special way (hwang and sasaki, 2005).<papid> P05-1068 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-1201 ">
multi-word expressions (mwes) are sequences of words that tend to co-occur more frequently than chance and are either idiosyncratic or decomposable into multiple simple words (baldwin, 2006).<papid> W06-1201 </papid></citsent>
<aftsection>
<nextsent>deciding idiomaticity of mwes is highly important for machine translation, information retrieval, question answering, lexical acquisition, parsing and language generation.
</nextsent>
<nextsent>compositionality refers to the degree to which the meaning of mwe can be predicted by combining the meanings of its components.
</nextsent>
<nextsent>unlike syntactic compositionality (e.g. by and large), semantic compositionality is continuous (baldwin, 2006).<papid> W06-1201 </papid></nextsent>
<nextsent>several studies have been carried out for detecting compositionality of noun-noun mwes using wordnet hypothesis (baldwin et al, 2003), verb-particle constructions using statistical similarities (bannard et al, 2003; mccarthy et al, 2003) and verb-noun pairs using latent semantic analysis (katz and giesbrecht, 2006).<papid> W06-1203 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5383">
<title id=" W11-1307.xml">shared task system description measuring the compositionality of bigrams using statistical methodologies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>compositionality refers to the degree to which the meaning of mwe can be predicted by combining the meanings of its components.
</prevsent>
<prevsent>unlike syntactic compositionality (e.g. by and large), semantic compositionality is continuous (baldwin, 2006).<papid> W06-1201 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-1203 ">
several studies have been carried out for detecting compositionality of noun-noun mwes using wordnet hypothesis (baldwin et al, 2003), verb-particle constructions using statistical similarities (bannard et al, 2003; mccarthy et al, 2003) and verb-noun pairs using latent semantic analysis (katz and giesbrecht, 2006).<papid> W06-1203 </papid></citsent>
<aftsection>
<nextsent>our contributions are two-fold: firstly, we experimentally show that collocation based statistical compositionality measurement can assist in identifying the continuum of compositionality of mwes.
</nextsent>
<nextsent>secondly, we show that supervised weighted parameter tuning results inaccuracy that is comparable to the best manually selected combination of parameters.
</nextsent>
<nextsent>38
</nextsent>
<nextsent>the present task was to identify the numerical judgment of compositionality of individual phrase.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5384">
<title id=" W11-1307.xml">shared task system description measuring the compositionality of bigrams using statistical methodologies </title>
<section> proposed methodologies.  </section>
<citcontext>
<prevsection>
<prevsent>frequency: if two words occur together quite frequently, the lexical meaning of the composition may be different from the combination of their individual meanings.
</prevsent>
<prevsent>the frequency of an individual phrase is directly used in the following methods.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
point-wise information (pmi): an information-theoretic motivated measure for discovering interesting collocations is point-wise mutual information (church and hanks, 1990).<papid> J90-1003 </papid></citsent>
<aftsection>
<nextsent>it is originally defined as the mutual information between particular events and and in our case the occurrence of particular words, as follows:   = log ,. ? log ,. 1 pmi represents the amount of information provided by the occurrence of the event represented by about the occurrence of the event represented by y. t-test: t-test has been widely used for collocation discovery.
</nextsent>
<nextsent>this statistical test tells us the probability of certain constellation (nugues, 2006).
</nextsent>
<nextsent>it looks at the mean and variance of sample of measurements.
</nextsent>
<nextsent>the null hypothesis is that the sample is drawn from distribution with mean.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5385">
<title id=" W11-0610.xml">classification of atypical language in autism </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the number of develop mental syntax errors was not significantly different between these two groups.
</prevsent>
<prevsent>although there has been virtually no previous work on automated analysis of unannotated transcripts of the speech of children with asd, automatically extracted language features have shown promise in the identification of other neurological disorders such as language impairment and cognitive impairment.
</prevsent>
</prevsection>
<citsent citstr=" N09-1006 ">
gabani et al (2009) <papid> N09-1006 </papid>used part-of speech language models to derive perplexity scores for transcripts of the speech of children with and without language impairment.</citsent>
<aftsection>
<nextsent>these scores offered significant diagnostic power, achieving an f1 measure of roughly 70% when used within an support vector machine (svm) for classification.
</nextsent>
<nextsent>roark etal.
</nextsent>
<nextsent>(in press) extracted much larger set of language complexity features derived from syntactic parse trees from transcripts of narratives produced by elderly subjects for the diagnosis of mild cognitive impairment.
</nextsent>
<nextsent>selecting subset of these features for classification with an svm yielded accuracy, as measured by the area under the receiver operating characteristic curve, of 0.73.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5386">
<title id=" W11-0610.xml">classification of atypical language in autism </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>selecting subset of these features for classification with an svm yielded accuracy, as measured by the area under the receiver operating characteristic curve, of 0.73.
</prevsent>
<prevsent>language models have also been applied to thetask of error identification, but primarily in writing samples of esl learners.
</prevsent>
</prevsection>
<citsent citstr=" I08-1059 ">
gamon et al (2008) <papid> I08-1059 </papid>used word-based language models to detect and correct common esl errors, while leacock and chodorow (2003) used part-of-speech bigram language models to identify potentially ungrammatical two-word sequences in esl essays.</citsent>
<aftsection>
<nextsent>although these tasks differ in number of ways from our tasks, they demonstrate the utility of using both word and part of-speech language models for error detection.
</nextsent>
<nextsent>3.1 subjects.
</nextsent>
<nextsent>our first objective was to gather data in order reproduce the results reported in volden and lord (1991).
</nextsent>
<nextsent>as shown in table 1, the participants in our study were 50 children ages 4 to 8 with performance iq greater than 80 and diagnosis of either typical diagnosis count age (s.d.) iq (s.d.) td 17 6.24 (1.38) 125.7 (11.63) asd 20 6.38 (1.25) 108.9 (16.41) dld 13 7.01 (1.10) 100.6 (10.95) table 1: count, mean age and iq by subject group.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5389">
<title id=" W11-0610.xml">classification of atypical language in autism </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>as the size of our small corpus grows, we intend to make use of the text to assist with model building, but for this study, we used all out-of-domain data for n-gram language models and parsing models.
</prevsent>
<prevsent>using switchboard also allowed us to use the same corpus to train both n-gram and parsing models.surprisal-based features.
</prevsent>
</prevsection>
<citsent citstr=" N01-1021 ">
surprisal, or the unexpectedness of word or syntactic category in given context, is often used as psycho linguistic measure of sentence-processing difficulty (hale, 2001; <papid> N01-1021 </papid>boston et al, 2008).<papid> P08-2002 </papid></citsent>
<aftsection>
<nextsent>although surprisal is usually discussed in the context of cognitive load for language processing, we hoped that it might also capture some of the language characteristics of the semantic errors like those in table 2, which often contain common words used in surprising ways, andthe non developmental syntax errors, which often include strings of function words presented in an order that would be difficult to anticipate.
</nextsent>
<nextsent>to derive surprisal-based features, each sentence is parsed using the roark (2001) <papid> J01-2004 </papid>incremental top-down parser relying on model built again on 91 the switchboard corpus.</nextsent>
<nextsent>the incremental output of the parser shows the surprisal for each word, as well as other scores, as presented in roark et al (2009).<papid> D09-1034 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5390">
<title id=" W11-0610.xml">classification of atypical language in autism </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>as the size of our small corpus grows, we intend to make use of the text to assist with model building, but for this study, we used all out-of-domain data for n-gram language models and parsing models.
</prevsent>
<prevsent>using switchboard also allowed us to use the same corpus to train both n-gram and parsing models.surprisal-based features.
</prevsent>
</prevsection>
<citsent citstr=" P08-2002 ">
surprisal, or the unexpectedness of word or syntactic category in given context, is often used as psycho linguistic measure of sentence-processing difficulty (hale, 2001; <papid> N01-1021 </papid>boston et al, 2008).<papid> P08-2002 </papid></citsent>
<aftsection>
<nextsent>although surprisal is usually discussed in the context of cognitive load for language processing, we hoped that it might also capture some of the language characteristics of the semantic errors like those in table 2, which often contain common words used in surprising ways, andthe non developmental syntax errors, which often include strings of function words presented in an order that would be difficult to anticipate.
</nextsent>
<nextsent>to derive surprisal-based features, each sentence is parsed using the roark (2001) <papid> J01-2004 </papid>incremental top-down parser relying on model built again on 91 the switchboard corpus.</nextsent>
<nextsent>the incremental output of the parser shows the surprisal for each word, as well as other scores, as presented in roark et al (2009).<papid> D09-1034 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5391">
<title id=" W11-0610.xml">classification of atypical language in autism </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>surprisal, or the unexpectedness of word or syntactic category in given context, is often used as psycho linguistic measure of sentence-processing difficulty (hale, 2001; <papid> N01-1021 </papid>boston et al, 2008).<papid> P08-2002 </papid></prevsent>
<prevsent>although surprisal is usually discussed in the context of cognitive load for language processing, we hoped that it might also capture some of the language characteristics of the semantic errors like those in table 2, which often contain common words used in surprising ways, andthe non developmental syntax errors, which often include strings of function words presented in an order that would be difficult to anticipate.</prevsent>
</prevsection>
<citsent citstr=" J01-2004 ">
to derive surprisal-based features, each sentence is parsed using the roark (2001) <papid> J01-2004 </papid>incremental top-down parser relying on model built again on 91 the switchboard corpus.</citsent>
<aftsection>
<nextsent>the incremental output of the parser shows the surprisal for each word, as well as other scores, as presented in roark et al (2009).<papid> D09-1034 </papid></nextsent>
<nextsent>for each sentence, we collected the mean surprisal (equivalent to the cross entropy given the model); the mean syntactic surprisal; and the mean lexical surprisal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5392">
<title id=" W11-0610.xml">classification of atypical language in autism </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>although surprisal is usually discussed in the context of cognitive load for language processing, we hoped that it might also capture some of the language characteristics of the semantic errors like those in table 2, which often contain common words used in surprising ways, andthe non developmental syntax errors, which often include strings of function words presented in an order that would be difficult to anticipate.
</prevsent>
<prevsent>to derive surprisal-based features, each sentence is parsed using the roark (2001) <papid> J01-2004 </papid>incremental top-down parser relying on model built again on 91 the switchboard corpus.</prevsent>
</prevsection>
<citsent citstr=" D09-1034 ">
the incremental output of the parser shows the surprisal for each word, as well as other scores, as presented in roark et al (2009).<papid> D09-1034 </papid></citsent>
<aftsection>
<nextsent>for each sentence, we collected the mean surprisal (equivalent to the cross entropy given the model); the mean syntactic surprisal; and the mean lexical surprisal.
</nextsent>
<nextsent>the lexical and syntactic surprisal are decomposition of the total surprisal into that portion due to probability mass associated with building non-terminal structure (syntactic surprisal) and that portion due to probability mass associated with building terminal lexical items in the tree (lexical surprisal).
</nextsent>
<nextsent>we refer the reader to that paper for further details.other linguistic complexity measures the non developmental syntax errors in table 2 are characterized by their ill-formed syntactic structure.
</nextsent>
<nextsent>following roark et al (in press), in which the authors explored the relationship between linguistic structural complexity and cognitive decline, and sagae(2005), in which the authors used automatic syntactic annotation to assess syntactic development, wealso investigated the following measures of linguistic complexity: words per clause, tree nodes per word, dependency length per word, and ygnve and frazier scores per word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5393">
<title id=" W11-0610.xml">classification of atypical language in autism </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>we refer the reader to that paper for further details.other linguistic complexity measures the non developmental syntax errors in table 2 are characterized by their ill-formed syntactic structure.
</prevsent>
<prevsent>following roark et al (in press), in which the authors explored the relationship between linguistic structural complexity and cognitive decline, and sagae(2005), in which the authors used automatic syntactic annotation to assess syntactic development, wealso investigated the following measures of linguistic complexity: words per clause, tree nodes per word, dependency length per word, and ygnve and frazier scores per word.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
each of these scores can be calculated from provided syntactic parse tree, and to generate these we made use of the charniak parser (charniak, 2000), <papid> A00-2018 </papid>also trained on the switchboard treebank.</citsent>
<aftsection>
<nextsent>briefly, words per clause is the total number of words divided by the total number of clauses; and tree nodes per word is the total number of nodes in the parse tree divided by the number of words.
</nextsent>
<nextsent>the dependency length for word is the distance (in word tokens) between that word and its governor, as determined through standard head-percolation methods from the output of the charniak parser.
</nextsent>
<nextsent>we calculate the mean of this length over all words in the utterance.
</nextsent>
<nextsent>the yngve score of word is the size of the stack of shift-reduce parser after that word; and the frazier score essentially counts how many intermediate nodes exist in the tree between the word and its lowest ancestor that is either the root or has left sibling in the tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5395">
<title id=" W11-0810.xml">the web is not a person berner slee is not an organization and african americans are not locations an analysis of the performance of named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these might be peoples names, or organizations, or locations, as well as dates, times, and currencies.
</prevsent>
<prevsent>performance assessment is usually made in the context of information extraction, of which ner is generally component.
</prevsent>
</prevsection>
<citsent citstr=" E03-1038 ">
competitions have been held from the earliest days of muc (message understanding con ference), to the more recent shared tasks in conll.recent research has focused on non-english languages such as spanish, dutch, and german (meul der et al, 2002; carreras et al, 2003; <papid> E03-1038 </papid>rossler, 2004), <papid> W04-1218 </papid>and on improving the performance of unsupervised learning methods (nadeau et al, 2006; elsner et al, 2009).<papid> N09-1019 </papid>there are no well-established standards for evaluation of ner.</citsent>
<aftsection>
<nextsent>since criteria for membership in the classes can change from one competition to another,it is often not possible to compare performance directly.
</nextsent>
<nextsent>moreover, since some of the systems in the competition may use proprietary software, there sults in competition might not be replicable by others in the community; however, this applies to the state of the art for most nlp applications rather than just ner.our work is motivated by vocabulary assessment project in which we needed to identifymulti-word expressions and determine their association with other words and phrases.
</nextsent>
<nextsent>however,we found that state-of-the-art software for named entity recognition was not reliable; false positives and tagging inconsistencies significantly hindered our work.
</nextsent>
<nextsent>these results led us to examine the state of-the-art in more detail.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5396">
<title id=" W11-0810.xml">the web is not a person berner slee is not an organization and african americans are not locations an analysis of the performance of named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these might be peoples names, or organizations, or locations, as well as dates, times, and currencies.
</prevsent>
<prevsent>performance assessment is usually made in the context of information extraction, of which ner is generally component.
</prevsent>
</prevsection>
<citsent citstr=" W04-1218 ">
competitions have been held from the earliest days of muc (message understanding con ference), to the more recent shared tasks in conll.recent research has focused on non-english languages such as spanish, dutch, and german (meul der et al, 2002; carreras et al, 2003; <papid> E03-1038 </papid>rossler, 2004), <papid> W04-1218 </papid>and on improving the performance of unsupervised learning methods (nadeau et al, 2006; elsner et al, 2009).<papid> N09-1019 </papid>there are no well-established standards for evaluation of ner.</citsent>
<aftsection>
<nextsent>since criteria for membership in the classes can change from one competition to another,it is often not possible to compare performance directly.
</nextsent>
<nextsent>moreover, since some of the systems in the competition may use proprietary software, there sults in competition might not be replicable by others in the community; however, this applies to the state of the art for most nlp applications rather than just ner.our work is motivated by vocabulary assessment project in which we needed to identifymulti-word expressions and determine their association with other words and phrases.
</nextsent>
<nextsent>however,we found that state-of-the-art software for named entity recognition was not reliable; false positives and tagging inconsistencies significantly hindered our work.
</nextsent>
<nextsent>these results led us to examine the state of-the-art in more detail.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5397">
<title id=" W11-0810.xml">the web is not a person berner slee is not an organization and african americans are not locations an analysis of the performance of named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these might be peoples names, or organizations, or locations, as well as dates, times, and currencies.
</prevsent>
<prevsent>performance assessment is usually made in the context of information extraction, of which ner is generally component.
</prevsent>
</prevsection>
<citsent citstr=" N09-1019 ">
competitions have been held from the earliest days of muc (message understanding con ference), to the more recent shared tasks in conll.recent research has focused on non-english languages such as spanish, dutch, and german (meul der et al, 2002; carreras et al, 2003; <papid> E03-1038 </papid>rossler, 2004), <papid> W04-1218 </papid>and on improving the performance of unsupervised learning methods (nadeau et al, 2006; elsner et al, 2009).<papid> N09-1019 </papid>there are no well-established standards for evaluation of ner.</citsent>
<aftsection>
<nextsent>since criteria for membership in the classes can change from one competition to another,it is often not possible to compare performance directly.
</nextsent>
<nextsent>moreover, since some of the systems in the competition may use proprietary software, there sults in competition might not be replicable by others in the community; however, this applies to the state of the art for most nlp applications rather than just ner.our work is motivated by vocabulary assessment project in which we needed to identifymulti-word expressions and determine their association with other words and phrases.
</nextsent>
<nextsent>however,we found that state-of-the-art software for named entity recognition was not reliable; false positives and tagging inconsistencies significantly hindered our work.
</nextsent>
<nextsent>these results led us to examine the state of-the-art in more detail.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5398">
<title id=" W11-0810.xml">the web is not a person berner slee is not an organization and african americans are not locations an analysis of the performance of named entity recognition </title>
<section> evaluation methodology.  </section>
<citcontext>
<prevsection>
<prevsent>however, most, if not all, ner evaluations and shared tasks only focus on intrinsic ner performance and ignore any form of extrinsic evaluation.
</prevsent>
<prevsent>one of the contributions of this paper is freely available unit test based on the systematic problems we found with existing taggers.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
we compared three state-of-the-art ner taggers: one from stanford university (henceforth, stanfordtagger), one from the university of illinois (henceforth, the lbj tagger) and bbn identifinder (hence forth, identifinder).the stanford tagger is based on conditional random fields (finkel et al, 2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>it was trained on100 million words from the english giga words corpus.
</nextsent>
<nextsent>the lbj tagger is based on regularized average perceptron (ratinov and roth, 2009).<papid> W09-1119 </papid></nextsent>
<nextsent>it was trained on subset of the reuters 1996 news corpus, subset of the north american news corpus, and set of 20 web pages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5399">
<title id=" W11-0810.xml">the web is not a person berner slee is not an organization and african americans are not locations an analysis of the performance of named entity recognition </title>
<section> evaluation methodology.  </section>
<citcontext>
<prevsection>
<prevsent>we compared three state-of-the-art ner taggers: one from stanford university (henceforth, stanfordtagger), one from the university of illinois (henceforth, the lbj tagger) and bbn identifinder (hence forth, identifinder).the stanford tagger is based on conditional random fields (finkel et al, 2005).<papid> P05-1045 </papid></prevsent>
<prevsent>it was trained on100 million words from the english giga words corpus.</prevsent>
</prevsection>
<citsent citstr=" W09-1119 ">
the lbj tagger is based on regularized average perceptron (ratinov and roth, 2009).<papid> W09-1119 </papid></citsent>
<aftsection>
<nextsent>it was trained on subset of the reuters 1996 news corpus, subset of the north american news corpus, and set of 20 web pages.
</nextsent>
<nextsent>the features for both these taggers are based on local context for target word, orthographic features, label sequences, and distributional similarity.
</nextsent>
<nextsent>both taggers include non local features to ensure consistency in the tagging of identical tokens that are in close proximity.
</nextsent>
<nextsent>identi finder is state-of-the-art commercial ner tagger that uses hidden markov models (hmms) (bikel et al., 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5402">
<title id=" W11-0810.xml">the web is not a person berner slee is not an organization and african americans are not locations an analysis of the performance of named entity recognition </title>
<section> one named-entity tag per discourse.  </section>
<citcontext>
<prevsection>
<prevsent>an excerpt from this unit test is shown in table 7.
</prevsent>
<prevsent>we provide more information about the full unit test at the end of the paper.
</prevsent>
</prevsection>
<citsent citstr=" E99-1001 ">
previous papers have noted that it would be unusual for multiple occurrences of token in document to be classified as different type of entity (mikheevet al, 1999; <papid> E99-1001 </papid>curran and clark, 2003).<papid> W03-0424 </papid></citsent>
<aftsection>
<nextsent>the stanford and lbj taggers have features for non-local dependencies for this reason.
</nextsent>
<nextsent>the observation is similar to hypothesis proposed by gale, church, and yarowsky with respect to word-sense disambiguation and discourse (gale et al, 1992).<papid> H92-1045 </papid></nextsent>
<nextsent>they hypothesized that when an ambiguous word appears in document, all subsequent instances of that word inthe document will have the same sense.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5403">
<title id=" W11-0810.xml">the web is not a person berner slee is not an organization and african americans are not locations an analysis of the performance of named entity recognition </title>
<section> one named-entity tag per discourse.  </section>
<citcontext>
<prevsection>
<prevsent>an excerpt from this unit test is shown in table 7.
</prevsent>
<prevsent>we provide more information about the full unit test at the end of the paper.
</prevsent>
</prevsection>
<citsent citstr=" W03-0424 ">
previous papers have noted that it would be unusual for multiple occurrences of token in document to be classified as different type of entity (mikheevet al, 1999; <papid> E99-1001 </papid>curran and clark, 2003).<papid> W03-0424 </papid></citsent>
<aftsection>
<nextsent>the stanford and lbj taggers have features for non-local dependencies for this reason.
</nextsent>
<nextsent>the observation is similar to hypothesis proposed by gale, church, and yarowsky with respect to word-sense disambiguation and discourse (gale et al, 1992).<papid> H92-1045 </papid></nextsent>
<nextsent>they hypothesized that when an ambiguous word appears in document, all subsequent instances of that word inthe document will have the same sense.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5404">
<title id=" W11-0810.xml">the web is not a person berner slee is not an organization and african americans are not locations an analysis of the performance of named entity recognition </title>
<section> one named-entity tag per discourse.  </section>
<citcontext>
<prevsection>
<prevsent>previous papers have noted that it would be unusual for multiple occurrences of token in document to be classified as different type of entity (mikheevet al, 1999; <papid> E99-1001 </papid>curran and clark, 2003).<papid> W03-0424 </papid></prevsent>
<prevsent>the stanford and lbj taggers have features for non-local dependencies for this reason.</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
the observation is similar to hypothesis proposed by gale, church, and yarowsky with respect to word-sense disambiguation and discourse (gale et al, 1992).<papid> H92-1045 </papid></citsent>
<aftsection>
<nextsent>they hypothesized that when an ambiguous word appears in document, all subsequent instances of that word inthe document will have the same sense.
</nextsent>
<nextsent>this hypothesis is incorrect for word senses that we find ina dictionary (krovetz, 1998) but is likely to be correct for the subset of the senses that are homony mous (unrelated in meaning).
</nextsent>
<nextsent>ambiguity between named entities is similar to homonymy, and for most entities it is unlikely that they would co-occur in adocument.7 however, there are cases that are exceptions.
</nextsent>
<nextsent>for example, finkel et al (2005) <papid> P05-1045 </papid>note that in the conll dataset, the same term can be used for location and for the name of sports team.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5407">
<title id=" W11-1107.xml">simultaneous similarity learning and feature weight learning for document clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, suppose publicationsx and mention the keyword machine learn ing??
</prevsent>
<prevsent>only once.
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
although, they are mentioned only once in the text of the document, for the purposes of computing semantic similarity between the docu 42ments, it would be beneficial to give it high key word weight.a commonly used approach to estimate semantic similarity between documents is to use an external knowledge source like wordnet (pedersenet al, 2004).<papid> N04-3012 </papid></citsent>
<aftsection>
<nextsent>however, these approaches are domain dependent and language dependent.
</nextsent>
<nextsent>if document similarity can not be estimated accurately using just the text, there have been approaches incorporating multiple sources of similarity like link similarity, authorship similarity between publications(bach et al, 2004; cortes et al, 2009).
</nextsent>
<nextsent>(muthukr ishnan et al, 2010) also uses multiple sources ofsimilarity.
</nextsent>
<nextsent>in addition to improving similarity estimates between documents, it also improves similarity estimates between keywords.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5408">
<title id=" W11-1107.xml">simultaneous similarity learning and feature weight learning for document clustering </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>also, co-clusteringtakes as input the weight of all keywords to corresponding documents.
</prevsent>
<prevsent>first, we explain how similarity learning and feature weight learning can mutually benefit from each other using an example.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
for example, consider the following three publications in the field of machine translation, (brown et al, 1990; <papid> J90-2002 </papid>gale and church, 1991; <papid> P91-1023 </papid>marcu and wong, 2002)<papid> W02-1018 </papid>clearly, all the papers belong to the field of machine translation but (gale and church, 1991) <papid> P91-1023 </papid>contains the phrase machine translation??</citsent>
<aftsection>
<nextsent>only once in the entire text.
</nextsent>
<nextsent>however, we can learn to attribute some similarity between (brown et al, 1990) <papid> J90-2002 </papid>and the second publication using the text in (marcu and wong, 2002)<papid> W02-1018 </papid>.</nextsent>
<nextsent>the keywords bilingual corpora??</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5409">
<title id=" W11-1107.xml">simultaneous similarity learning and feature weight learning for document clustering </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>also, co-clusteringtakes as input the weight of all keywords to corresponding documents.
</prevsent>
<prevsent>first, we explain how similarity learning and feature weight learning can mutually benefit from each other using an example.
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
for example, consider the following three publications in the field of machine translation, (brown et al, 1990; <papid> J90-2002 </papid>gale and church, 1991; <papid> P91-1023 </papid>marcu and wong, 2002)<papid> W02-1018 </papid>clearly, all the papers belong to the field of machine translation but (gale and church, 1991) <papid> P91-1023 </papid>contains the phrase machine translation??</citsent>
<aftsection>
<nextsent>only once in the entire text.
</nextsent>
<nextsent>however, we can learn to attribute some similarity between (brown et al, 1990) <papid> J90-2002 </papid>and the second publication using the text in (marcu and wong, 2002)<papid> W02-1018 </papid>.</nextsent>
<nextsent>the keywords bilingual corpora??</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5410">
<title id=" W11-1107.xml">simultaneous similarity learning and feature weight learning for document clustering </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>also, co-clusteringtakes as input the weight of all keywords to corresponding documents.
</prevsent>
<prevsent>first, we explain how similarity learning and feature weight learning can mutually benefit from each other using an example.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
for example, consider the following three publications in the field of machine translation, (brown et al, 1990; <papid> J90-2002 </papid>gale and church, 1991; <papid> P91-1023 </papid>marcu and wong, 2002)<papid> W02-1018 </papid>clearly, all the papers belong to the field of machine translation but (gale and church, 1991) <papid> P91-1023 </papid>contains the phrase machine translation??</citsent>
<aftsection>
<nextsent>only once in the entire text.
</nextsent>
<nextsent>however, we can learn to attribute some similarity between (brown et al, 1990) <papid> J90-2002 </papid>and the second publication using the text in (marcu and wong, 2002)<papid> W02-1018 </papid>.</nextsent>
<nextsent>the keywords bilingual corpora??</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5419">
<title id=" W11-1107.xml">simultaneous similarity learning and feature weight learning for document clustering </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the three datasets are explained below.?
</prevsent>
<prevsent>aan data: the acl anthology is collection of papers from the computational linguistics journal as well as proceedings from acl conferences and workshops and includes 15, 160 papers.
</prevsent>
</prevsection>
<citsent citstr=" W09-3607 ">
to build the acl anthology network (aan), (radev et al, 2009) <papid> W09-3607 </papid>manually performed some preprocessing tasks including parsing references and building the network meta data, the citation, and the author collaboration networks.</citsent>
<aftsection>
<nextsent>the full aan includes the raw text of all the papers in addition to full citation and collaboration networks.we chose subset of papers in 3 topics (ma 47 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0 10 20 30 40 50 60 70 contentlinklinearsc-mvunifiedunified-binaryunified-tfidf (a) aan 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 10 15 20 25 30 35 40 contentlinklinearsc-mvunifiedunified-binaryunified-tfidf (b) cornell 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 10 15 20 25 30 35 40 contentlinklinearsc-mvunifiedunified-binaryunified-tfidf (c) texas 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 10 15 20 25 30 35 40 45 50 contentlinklinearsc-mvunifiedunified-binaryunified-tfidf (d) washington 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 10 15 20 25 30 35 40 45 contentlinklinearsc-mvunifiedunified-binaryunified-tfidf (e) wisconsin 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 50 100 150 200 250 300 350 400 450 500 contentlinklinearsc-mvunifiedunified-binaryunified-tfidf (f) cora figure 1: classification accuracy on the different datasets.
</nextsent>
<nextsent>the number of points labeled is plotted along the x-axis and the y-axis shows the classification accuracy on the unlabeled data.chine translation, dependency parsing, sum marization) from the acl anthology.
</nextsent>
<nextsent>these topics are three main research areas in natural language processing (nlp).
</nextsent>
<nextsent>specifically, we collected all papers which were cited by papers whose titles contain any of the following phrases, dependency parsing??, machine translation??, summarization??.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5420">
<title id=" W11-1603.xml">an unsupervised alignment algorithm for text simplification corpus construction </title>
<section> text simplification.  </section>
<citcontext>
<prevsection>
<prevsent>which is less complex in vocabulary and form, aims at reducing the efforts and costs associated with human simplification.
</prevsent>
<prevsent>in addition to transforming texts into their simplification for human consumption, text simplification has other advantages since simpler texts can be processed more efficiently by different natural language processing processors such as parsers andused in applications such as machine translation, information extraction, question answering, and text summarization.
</prevsent>
</prevsection>
<citsent citstr=" C96-2183 ">
early attempts to text simplification were based on rule-based methods where rules were designed following linguistic intuitions (chandrasekar et al, 1996).<papid> C96-2183 </papid></citsent>
<aftsection>
<nextsent>steps in the process included linguistic text analysis (including parsing) and pattern matching and transformation steps.
</nextsent>
<nextsent>other computational models of text simplification included processes of analysis, transformation, and phrase re-generation (sid dhar than, 2002) also using rule-based techniques.in the pset project (carroll et al, 1998) the proposal is for news simplification system for aphasic readers and particular attention is paid to linguistic phenomena such as passive constructions and coreference which are difficult to deal with by people with disabilities.
</nextsent>
<nextsent>the porsimples project (alusio etal., 2008) has looked into simplification of the portuguese language.
</nextsent>
<nextsent>the methodology consisted in the creation of corpus of simplification at two different levels and on the use of the corpus to train decision procedure for simplification based on linguistic features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5422">
<title id=" W11-1603.xml">an unsupervised alignment algorithm for text simplification corpus construction </title>
<section> text simplification.  </section>
<citcontext>
<prevsection>
<prevsent>the methodology consisted in the creation of corpus of simplification at two different levels and on the use of the corpus to train decision procedure for simplification based on linguistic features.
</prevsent>
<prevsent>simplification decisions about whether to simplify text or sentence have been studied following rule-based paradigms (chandrasekar et al, 1996) <papid> C96-2183 </papid>or trainable systems (petersen and ostendorf,2007) where corpus of texts and their simplifications becomes necessary.</prevsent>
</prevsection>
<citsent citstr=" E09-1027 ">
some resources are available for the english language such as parallel corpora created or studied in various projects (barzilay and elhadad, 2003; feng et al, 2009; <papid> E09-1027 </papid>petersen and ostendorf, 2007; quirk et al, 2004); <papid> W04-3219 </papid>however there is no parallel spanish corpus available for research into text simplification.</citsent>
<aftsection>
<nextsent>the algorithms to be presented here will be used to create such resource.
</nextsent>
<nextsent>the problem of sentence alignment was first tackled in the context of statistical machine translation.gale and church (1993) <papid> J93-1004 </papid>proposed dynamic programming algorithm for the sentence-level alignment of translations that exploited two facts: the length of translated sentences roughly correspond sto the length of the original sentences and the sequence of sentences in translated text largely corresponds to the original order of sentences.</nextsent>
<nextsent>with this simple approach they reached high degree of accu racy.within the field of monolingual sentence alignment large part of the work has concentrated on the alignment between text summaries and the source texts they summarize.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5423">
<title id=" W11-1603.xml">an unsupervised alignment algorithm for text simplification corpus construction </title>
<section> text simplification.  </section>
<citcontext>
<prevsection>
<prevsent>the methodology consisted in the creation of corpus of simplification at two different levels and on the use of the corpus to train decision procedure for simplification based on linguistic features.
</prevsent>
<prevsent>simplification decisions about whether to simplify text or sentence have been studied following rule-based paradigms (chandrasekar et al, 1996) <papid> C96-2183 </papid>or trainable systems (petersen and ostendorf,2007) where corpus of texts and their simplifications becomes necessary.</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
some resources are available for the english language such as parallel corpora created or studied in various projects (barzilay and elhadad, 2003; feng et al, 2009; <papid> E09-1027 </papid>petersen and ostendorf, 2007; quirk et al, 2004); <papid> W04-3219 </papid>however there is no parallel spanish corpus available for research into text simplification.</citsent>
<aftsection>
<nextsent>the algorithms to be presented here will be used to create such resource.
</nextsent>
<nextsent>the problem of sentence alignment was first tackled in the context of statistical machine translation.gale and church (1993) <papid> J93-1004 </papid>proposed dynamic programming algorithm for the sentence-level alignment of translations that exploited two facts: the length of translated sentences roughly correspond sto the length of the original sentences and the sequence of sentences in translated text largely corresponds to the original order of sentences.</nextsent>
<nextsent>with this simple approach they reached high degree of accu racy.within the field of monolingual sentence alignment large part of the work has concentrated on the alignment between text summaries and the source texts they summarize.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5424">
<title id=" W11-1603.xml">an unsupervised alignment algorithm for text simplification corpus construction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some resources are available for the english language such as parallel corpora created or studied in various projects (barzilay and elhadad, 2003; feng et al, 2009; <papid> E09-1027 </papid>petersen and ostendorf, 2007; quirk et al, 2004); <papid> W04-3219 </papid>however there is no parallel spanish corpus available for research into text simplification.</prevsent>
<prevsent>the algorithms to be presented here will be used to create such resource.</prevsent>
</prevsection>
<citsent citstr=" J93-1004 ">
the problem of sentence alignment was first tackled in the context of statistical machine translation.gale and church (1993) <papid> J93-1004 </papid>proposed dynamic programming algorithm for the sentence-level alignment of translations that exploited two facts: the length of translated sentences roughly correspond sto the length of the original sentences and the sequence of sentences in translated text largely corresponds to the original order of sentences.</citsent>
<aftsection>
<nextsent>with this simple approach they reached high degree of accu racy.within the field of monolingual sentence alignment large part of the work has concentrated on the alignment between text summaries and the source texts they summarize.
</nextsent>
<nextsent>jing (2002) <papid> J02-4006 </papid>present an algorithm which aligns strings of words to pieces of text in an original document using hidden markov model.</nextsent>
<nextsent>this approach is very specific to summary texts, concretely such summaries which have been produced by cut and paste?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5425">
<title id=" W11-1603.xml">an unsupervised alignment algorithm for text simplification corpus construction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the problem of sentence alignment was first tackled in the context of statistical machine translation.gale and church (1993) <papid> J93-1004 </papid>proposed dynamic programming algorithm for the sentence-level alignment of translations that exploited two facts: the length of translated sentences roughly correspond sto the length of the original sentences and the sequence of sentences in translated text largely corresponds to the original order of sentences.</prevsent>
<prevsent>with this simple approach they reached high degree of accu racy.within the field of monolingual sentence alignment large part of the work has concentrated on the alignment between text summaries and the source texts they summarize.</prevsent>
</prevsection>
<citsent citstr=" J02-4006 ">
jing (2002) <papid> J02-4006 </papid>present an algorithm which aligns strings of words to pieces of text in an original document using hidden markov model.</citsent>
<aftsection>
<nextsent>this approach is very specific to summary texts, concretely such summaries which have been produced by cut and paste?
</nextsent>
<nextsent>process.
</nextsent>
<nextsent>a work which is more closely related to our task is presented in barzilay and elhadad (2003).
</nextsent>
<nextsent>they carried out an experiment on two different versions of the encyclopedia britannica (the regular version and the britannica elementary) and aligned sentences in four-step procedure: they clustered paragraphs into topic?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5426">
<title id=" W11-1603.xml">an unsupervised alignment algorithm for text simplification corpus construction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally they aligned the sentences within the already aligned paragraphs.
</prevsent>
<prevsent>their similarity measure, both for paragraphs and sentences, was based on cosine distance of word overlap.
</prevsent>
</prevsection>
<citsent citstr=" E06-1021 ">
nelken and shieber (2006) <papid> E06-1021 </papid>improve over barzilay and elhadads work: they use the same dataset, but they base their similarity measure for aligning sentences on tf*idf score.</citsent>
<aftsection>
<nextsent>although this score can be obtained without any training, they apply logistic regression on these scores and train two parameters of this regression model on the training data.
</nextsent>
<nextsent>both of these approaches can be tuned by parameter settings, which results in trade-off between precision and recall.
</nextsent>
<nextsent>barzilay and elhadad report precision of 76.9% when the recall reaches 55.8%.
</nextsent>
<nextsent>nelken and shieber raise this value to 83.1% with the same recall level and show that tf*idf is much better sentence similarity measure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5432">
<title id=" W11-1512.xml">automatic verb extraction from historical swedish texts </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>this combination proved to be successful for partial parsing of medieval portuguese texts,even though there were some problems with grammar limitations, dictionary incompleteness and insufficient part-of-speech tagging.oravecz et al (2010) tried semi-automatic approach to create an annotated corpus of texts fromthe old hungarian period.
</prevsent>
<prevsent>the annotation was performed in three steps: 1) sentence segmentation and token isation, 2) standardisation/normalisation, and 3) morphological analysis and disambiguation.they concluded that normalisation is of vital importance to the performance of the morphological analyser.
</prevsent>
</prevsection>
<citsent citstr=" W07-0901 ">
for the swedish language, borin et al (2007) <papid> W07-0901 </papid>proposed named-entity recognition system adapted to swedish literature from the 19th century.</citsent>
<aftsection>
<nextsent>the system recognises person names, locations, organisations, artifacts (food/wine products, vehicles etc), work&art; (names of novels, sculptures etc), events(religious, cultural etc), measure/numerical expressions and temporal expressions.
</nextsent>
<nextsent>the named entity recognition system was evaluated on texts from the swedish literature bank without any adaptation,showing problems with spelling variation, inflectional differences, unknown names and structural issues (such as hyphens splitting single name into several entities).1 normalising the texts before applying the named entity recognition system made the f-score figures increase from 78.1% to 89.5%.
</nextsent>
<nextsent>all the results presented in this section indicate that existing natural language processing tools are not applicable to historical texts without adaptation of the tools, or the source text.
</nextsent>
<nextsent>2.2 characteristics of historical swedish texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5433">
<title id=" W11-1512.xml">automatic verb extraction from historical swedish texts </title>
<section> verb extraction.  </section>
<citcontext>
<prevsection>
<prevsent>for example, the word for will always be analysed both as verb (bring) and as preposition (for), even though in most cases the prepositional interpretation is the correct one.
</prevsent>
<prevsent>when running the maximum five steps in the verb extraction procedure, the tagger will disambiguate in cases where the morphological analyser has produced both verb interpretation and non-verb interpretation.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
the tagger used in this study is hun pos (halacsy et al, 2007), free and open source re implementation of the hmm-based tnt-tagger by brants (2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>megyesi (2008) showed that the hunpos tagger trained on the stockholm-umea?
</nextsent>
<nextsent>corpus (gustafson-capkova?
</nextsent>
<nextsent>and hartmann, 2006) is one of the best performing taggers for swedish texts.
</nextsent>
<nextsent>this section describes the experimental setup including data preparation and experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5434">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our approach achieves better overall performance for precision, recall and fmeasure metrics as compared to svm-based and llda-based models.
</prevsent>
<prevsent>entity relation detection (erd) aims at finding relations between pairs of named entities (nes) in text.availability of annotated corpora (nist, 2003; doddington et al , 2004) and introduction of shared tasks (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W10-3001 ">
(farkas et al , 2010; <papid> W10-3001 </papid>carreras and ma`rquez, 2005)) has spurred large amount of research in this field in recent times.</citsent>
<aftsection>
<nextsent>researchers have used supervised and semi-supervised approaches (hasegawa etal., 2004; <papid> P04-1053 </papid>mintz et al , 2009; <papid> P09-1113 </papid>jiang, 2009), <papid> P09-1114 </papid>and explored rich features (kambhatla, 2004), <papid> P04-3022 </papid>kernel design (culotta and sorensen, 2004; <papid> P04-1054 </papid>zhou et al , 2005; <papid> P05-1053 </papid>bunescu and mooney, 2005; <papid> H05-1091 </papid>qian et al , 2008) <papid> C08-1088 </papid>and inference algorithms (chan and roth, 2011), to detect predefined relations between nes.in this work, we explore if and how the latent semantics of the text can help in detecting entity relations.</nextsent>
<nextsent>for this, we adapt the latent dirichlet allocation (lda) approach to solve the erd task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5435">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>entity relation detection (erd) aims at finding relations between pairs of named entities (nes) in text.availability of annotated corpora (nist, 2003; doddington et al , 2004) and introduction of shared tasks (e.g.
</prevsent>
<prevsent>(farkas et al , 2010; <papid> W10-3001 </papid>carreras and ma`rquez, 2005)) has spurred large amount of research in this field in recent times.</prevsent>
</prevsection>
<citsent citstr=" P04-1053 ">
researchers have used supervised and semi-supervised approaches (hasegawa etal., 2004; <papid> P04-1053 </papid>mintz et al , 2009; <papid> P09-1113 </papid>jiang, 2009), <papid> P09-1114 </papid>and explored rich features (kambhatla, 2004), <papid> P04-3022 </papid>kernel design (culotta and sorensen, 2004; <papid> P04-1054 </papid>zhou et al , 2005; <papid> P05-1053 </papid>bunescu and mooney, 2005; <papid> H05-1091 </papid>qian et al , 2008) <papid> C08-1088 </papid>and inference algorithms (chan and roth, 2011), to detect predefined relations between nes.in this work, we explore if and how the latent semantics of the text can help in detecting entity relations.</citsent>
<aftsection>
<nextsent>for this, we adapt the latent dirichlet allocation (lda) approach to solve the erd task.
</nextsent>
<nextsent>specifically, we present erd system based on maximum entropy discriminant latent dirichlet al location (medlda).
</nextsent>
<nextsent>medlda (zhu et al , 2009), is an extension of latent dirichlet allocation (lda)that combines capability of capturing latent semantics with the discriminative capabilities of svm.
</nextsent>
<nextsent>there are number of challenges in employing the lda framework for erd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5436">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>entity relation detection (erd) aims at finding relations between pairs of named entities (nes) in text.availability of annotated corpora (nist, 2003; doddington et al , 2004) and introduction of shared tasks (e.g.
</prevsent>
<prevsent>(farkas et al , 2010; <papid> W10-3001 </papid>carreras and ma`rquez, 2005)) has spurred large amount of research in this field in recent times.</prevsent>
</prevsection>
<citsent citstr=" P09-1113 ">
researchers have used supervised and semi-supervised approaches (hasegawa etal., 2004; <papid> P04-1053 </papid>mintz et al , 2009; <papid> P09-1113 </papid>jiang, 2009), <papid> P09-1114 </papid>and explored rich features (kambhatla, 2004), <papid> P04-3022 </papid>kernel design (culotta and sorensen, 2004; <papid> P04-1054 </papid>zhou et al , 2005; <papid> P05-1053 </papid>bunescu and mooney, 2005; <papid> H05-1091 </papid>qian et al , 2008) <papid> C08-1088 </papid>and inference algorithms (chan and roth, 2011), to detect predefined relations between nes.in this work, we explore if and how the latent semantics of the text can help in detecting entity relations.</citsent>
<aftsection>
<nextsent>for this, we adapt the latent dirichlet allocation (lda) approach to solve the erd task.
</nextsent>
<nextsent>specifically, we present erd system based on maximum entropy discriminant latent dirichlet al location (medlda).
</nextsent>
<nextsent>medlda (zhu et al , 2009), is an extension of latent dirichlet allocation (lda)that combines capability of capturing latent semantics with the discriminative capabilities of svm.
</nextsent>
<nextsent>there are number of challenges in employing the lda framework for erd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5438">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>entity relation detection (erd) aims at finding relations between pairs of named entities (nes) in text.availability of annotated corpora (nist, 2003; doddington et al , 2004) and introduction of shared tasks (e.g.
</prevsent>
<prevsent>(farkas et al , 2010; <papid> W10-3001 </papid>carreras and ma`rquez, 2005)) has spurred large amount of research in this field in recent times.</prevsent>
</prevsection>
<citsent citstr=" P09-1114 ">
researchers have used supervised and semi-supervised approaches (hasegawa etal., 2004; <papid> P04-1053 </papid>mintz et al , 2009; <papid> P09-1113 </papid>jiang, 2009), <papid> P09-1114 </papid>and explored rich features (kambhatla, 2004), <papid> P04-3022 </papid>kernel design (culotta and sorensen, 2004; <papid> P04-1054 </papid>zhou et al , 2005; <papid> P05-1053 </papid>bunescu and mooney, 2005; <papid> H05-1091 </papid>qian et al , 2008) <papid> C08-1088 </papid>and inference algorithms (chan and roth, 2011), to detect predefined relations between nes.in this work, we explore if and how the latent semantics of the text can help in detecting entity relations.</citsent>
<aftsection>
<nextsent>for this, we adapt the latent dirichlet allocation (lda) approach to solve the erd task.
</nextsent>
<nextsent>specifically, we present erd system based on maximum entropy discriminant latent dirichlet al location (medlda).
</nextsent>
<nextsent>medlda (zhu et al , 2009), is an extension of latent dirichlet allocation (lda)that combines capability of capturing latent semantics with the discriminative capabilities of svm.
</nextsent>
<nextsent>there are number of challenges in employing the lda framework for erd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5439">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>entity relation detection (erd) aims at finding relations between pairs of named entities (nes) in text.availability of annotated corpora (nist, 2003; doddington et al , 2004) and introduction of shared tasks (e.g.
</prevsent>
<prevsent>(farkas et al , 2010; <papid> W10-3001 </papid>carreras and ma`rquez, 2005)) has spurred large amount of research in this field in recent times.</prevsent>
</prevsection>
<citsent citstr=" P04-3022 ">
researchers have used supervised and semi-supervised approaches (hasegawa etal., 2004; <papid> P04-1053 </papid>mintz et al , 2009; <papid> P09-1113 </papid>jiang, 2009), <papid> P09-1114 </papid>and explored rich features (kambhatla, 2004), <papid> P04-3022 </papid>kernel design (culotta and sorensen, 2004; <papid> P04-1054 </papid>zhou et al , 2005; <papid> P05-1053 </papid>bunescu and mooney, 2005; <papid> H05-1091 </papid>qian et al , 2008) <papid> C08-1088 </papid>and inference algorithms (chan and roth, 2011), to detect predefined relations between nes.in this work, we explore if and how the latent semantics of the text can help in detecting entity relations.</citsent>
<aftsection>
<nextsent>for this, we adapt the latent dirichlet allocation (lda) approach to solve the erd task.
</nextsent>
<nextsent>specifically, we present erd system based on maximum entropy discriminant latent dirichlet al location (medlda).
</nextsent>
<nextsent>medlda (zhu et al , 2009), is an extension of latent dirichlet allocation (lda)that combines capability of capturing latent semantics with the discriminative capabilities of svm.
</nextsent>
<nextsent>there are number of challenges in employing the lda framework for erd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5440">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>entity relation detection (erd) aims at finding relations between pairs of named entities (nes) in text.availability of annotated corpora (nist, 2003; doddington et al , 2004) and introduction of shared tasks (e.g.
</prevsent>
<prevsent>(farkas et al , 2010; <papid> W10-3001 </papid>carreras and ma`rquez, 2005)) has spurred large amount of research in this field in recent times.</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
researchers have used supervised and semi-supervised approaches (hasegawa etal., 2004; <papid> P04-1053 </papid>mintz et al , 2009; <papid> P09-1113 </papid>jiang, 2009), <papid> P09-1114 </papid>and explored rich features (kambhatla, 2004), <papid> P04-3022 </papid>kernel design (culotta and sorensen, 2004; <papid> P04-1054 </papid>zhou et al , 2005; <papid> P05-1053 </papid>bunescu and mooney, 2005; <papid> H05-1091 </papid>qian et al , 2008) <papid> C08-1088 </papid>and inference algorithms (chan and roth, 2011), to detect predefined relations between nes.in this work, we explore if and how the latent semantics of the text can help in detecting entity relations.</citsent>
<aftsection>
<nextsent>for this, we adapt the latent dirichlet allocation (lda) approach to solve the erd task.
</nextsent>
<nextsent>specifically, we present erd system based on maximum entropy discriminant latent dirichlet al location (medlda).
</nextsent>
<nextsent>medlda (zhu et al , 2009), is an extension of latent dirichlet allocation (lda)that combines capability of capturing latent semantics with the discriminative capabilities of svm.
</nextsent>
<nextsent>there are number of challenges in employing the lda framework for erd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5441">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>entity relation detection (erd) aims at finding relations between pairs of named entities (nes) in text.availability of annotated corpora (nist, 2003; doddington et al , 2004) and introduction of shared tasks (e.g.
</prevsent>
<prevsent>(farkas et al , 2010; <papid> W10-3001 </papid>carreras and ma`rquez, 2005)) has spurred large amount of research in this field in recent times.</prevsent>
</prevsection>
<citsent citstr=" P05-1053 ">
researchers have used supervised and semi-supervised approaches (hasegawa etal., 2004; <papid> P04-1053 </papid>mintz et al , 2009; <papid> P09-1113 </papid>jiang, 2009), <papid> P09-1114 </papid>and explored rich features (kambhatla, 2004), <papid> P04-3022 </papid>kernel design (culotta and sorensen, 2004; <papid> P04-1054 </papid>zhou et al , 2005; <papid> P05-1053 </papid>bunescu and mooney, 2005; <papid> H05-1091 </papid>qian et al , 2008) <papid> C08-1088 </papid>and inference algorithms (chan and roth, 2011), to detect predefined relations between nes.in this work, we explore if and how the latent semantics of the text can help in detecting entity relations.</citsent>
<aftsection>
<nextsent>for this, we adapt the latent dirichlet allocation (lda) approach to solve the erd task.
</nextsent>
<nextsent>specifically, we present erd system based on maximum entropy discriminant latent dirichlet al location (medlda).
</nextsent>
<nextsent>medlda (zhu et al , 2009), is an extension of latent dirichlet allocation (lda)that combines capability of capturing latent semantics with the discriminative capabilities of svm.
</nextsent>
<nextsent>there are number of challenges in employing the lda framework for erd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5443">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>entity relation detection (erd) aims at finding relations between pairs of named entities (nes) in text.availability of annotated corpora (nist, 2003; doddington et al , 2004) and introduction of shared tasks (e.g.
</prevsent>
<prevsent>(farkas et al , 2010; <papid> W10-3001 </papid>carreras and ma`rquez, 2005)) has spurred large amount of research in this field in recent times.</prevsent>
</prevsection>
<citsent citstr=" H05-1091 ">
researchers have used supervised and semi-supervised approaches (hasegawa etal., 2004; <papid> P04-1053 </papid>mintz et al , 2009; <papid> P09-1113 </papid>jiang, 2009), <papid> P09-1114 </papid>and explored rich features (kambhatla, 2004), <papid> P04-3022 </papid>kernel design (culotta and sorensen, 2004; <papid> P04-1054 </papid>zhou et al , 2005; <papid> P05-1053 </papid>bunescu and mooney, 2005; <papid> H05-1091 </papid>qian et al , 2008) <papid> C08-1088 </papid>and inference algorithms (chan and roth, 2011), to detect predefined relations between nes.in this work, we explore if and how the latent semantics of the text can help in detecting entity relations.</citsent>
<aftsection>
<nextsent>for this, we adapt the latent dirichlet allocation (lda) approach to solve the erd task.
</nextsent>
<nextsent>specifically, we present erd system based on maximum entropy discriminant latent dirichlet al location (medlda).
</nextsent>
<nextsent>medlda (zhu et al , 2009), is an extension of latent dirichlet allocation (lda)that combines capability of capturing latent semantics with the discriminative capabilities of svm.
</nextsent>
<nextsent>there are number of challenges in employing the lda framework for erd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5444">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>entity relation detection (erd) aims at finding relations between pairs of named entities (nes) in text.availability of annotated corpora (nist, 2003; doddington et al , 2004) and introduction of shared tasks (e.g.
</prevsent>
<prevsent>(farkas et al , 2010; <papid> W10-3001 </papid>carreras and ma`rquez, 2005)) has spurred large amount of research in this field in recent times.</prevsent>
</prevsection>
<citsent citstr=" C08-1088 ">
researchers have used supervised and semi-supervised approaches (hasegawa etal., 2004; <papid> P04-1053 </papid>mintz et al , 2009; <papid> P09-1113 </papid>jiang, 2009), <papid> P09-1114 </papid>and explored rich features (kambhatla, 2004), <papid> P04-3022 </papid>kernel design (culotta and sorensen, 2004; <papid> P04-1054 </papid>zhou et al , 2005; <papid> P05-1053 </papid>bunescu and mooney, 2005; <papid> H05-1091 </papid>qian et al , 2008) <papid> C08-1088 </papid>and inference algorithms (chan and roth, 2011), to detect predefined relations between nes.in this work, we explore if and how the latent semantics of the text can help in detecting entity relations.</citsent>
<aftsection>
<nextsent>for this, we adapt the latent dirichlet allocation (lda) approach to solve the erd task.
</nextsent>
<nextsent>specifically, we present erd system based on maximum entropy discriminant latent dirichlet al location (medlda).
</nextsent>
<nextsent>medlda (zhu et al , 2009), is an extension of latent dirichlet allocation (lda)that combines capability of capturing latent semantics with the discriminative capabilities of svm.
</nextsent>
<nextsent>there are number of challenges in employing the lda framework for erd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5445">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>medlda (zhu et al , 2009), is an extension of latent dirichlet allocation (lda)that combines capability of capturing latent semantics with the discriminative capabilities of svm.
</prevsent>
<prevsent>there are number of challenges in employing the lda framework for erd.
</prevsent>
</prevsection>
<citsent citstr=" D09-1026 ">
latent dirichlet al location and its supervised extensions such as labeled lda (llda) (ramage et al , 2009) <papid> D09-1026 </papid>and supervised lda (slda) (blei and mcauliffe, 2008) are powerful generative models that capture the underlying semantics of texts.</citsent>
<aftsection>
<nextsent>however, they have trouble discovering marginal classes and easily employing rich feature sets, both of which are important for erd.
</nextsent>
<nextsent>we overcome the first drawback by employing medlda framework, which integrates maximum likelihood estimation (mle) and maximum margin estimation (mme).
</nextsent>
<nextsent>specifically, it is combination of slda and support vector machines (svms).
</nextsent>
<nextsent>further, in order to employ rich and heterogeneous features we introduce separate exponential family distribution for each feature, similar to (shan et al , 2009), into our medlda model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5446">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> sample the relation type label.  </section>
<citcontext>
<prevsection>
<prevsent>(2) 3 features.
</prevsent>
<prevsent>we explore the effectiveness of incorporating features into our systems as well as the baselines.
</prevsent>
</prevsection>
<citsent citstr=" N07-1015 ">
for this, we construct feature sets similar to jiang andzhai (2007) <papid> N07-1015 </papid>and zhou (2005).</citsent>
<aftsection>
<nextsent>three kinds of features are employed: 1.
</nextsent>
<nextsent>bow the bag of words (bow) feature cap-.
</nextsent>
<nextsent>tures all the words in our mini-document.
</nextsent>
<nextsent>it comprises of the words of the two ne mentions and the words between them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5447">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>notice that medlda achieves performance close to svm for this category.
</prevsent>
<prevsent>this is because,even though both llda and medlda model hidden topics and then employ discovered hidden topic sto predict relation types, medlda does joint inference of mle and mme.
</prevsent>
</prevsection>
<citsent citstr=" N09-3012 ">
this joint inference helps to improve the detection of no-rel.finally, we also compare our systems results (using plus comp features) with the results of previous research on the same corpus (khayyamian et al ,2009).<papid> N09-3012 </papid></citsent>
<aftsection>
<nextsent>they use similar experimental settings: every pair of entities within sentence is regarded to involve negative relation instance unless it is annotated as positive in the corpus.
</nextsent>
<nextsent>a similar filter (theyuse distance filter) is used to sift out unrelated negative instances.
</nextsent>
<nextsent>their train/test ratio of data split is also the same as ours.khayyamian, mirroshandel and abolhassani (2009) employ state-of-art kernel methods developed by collins and duffy (2002) and only report fmeasures over the six ace relation types.
</nextsent>
<nextsent>for clarity, we reproduce their results in table 4 and repeat medlda fmeasures from table 3 in the last column.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5451">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous research has explored various methods and features for relationship detection and mining.
</prevsent>
<prevsent>kernel methods have been popularly used for relation detection.
</prevsent>
</prevsection>
<citsent citstr=" P05-1052 ">
some examples are are dependency tree kernels (culotta and sorensen, 2004), <papid> P04-1054 </papid>shortest dependency path kernels (bunescu and mooney, 2005), <papid> H05-1091 </papid>and more recently, convolution tree kernels (zhao and grishman, 2005; <papid> P05-1052 </papid>zhang et al , 2006) <papid> P06-1104 </papid>context-sensitive convolution tree kernels (zhou et al ., 2007) <papid> D07-1076 </papid>and dynamic syntax tree kernels (qian et al ., 2008).<papid> C08-1088 </papid></citsent>
<aftsection>
<nextsent>kernel methods for relation extraction focus on representing and capturing the structured information of the text between the entities.
</nextsent>
<nextsent>in our medlda model, instead of computing distances between subtrees, we sample topics based on their distributions.
</nextsent>
<nextsent>the sampling is not only on the (mini) document level, but also on the word level or on the syntactic or semantic level.
</nextsent>
<nextsent>our model focuses on addressing the underlying semantics more directly than typical kernel-based methods.chan and roth (2011) employ constraints using an integer linear programming (ilp) framework.using this, they apply rich linguistic and knowledge based constraints based on coreference annotations, hierarchy of relations, syntacto-semantic structure, and knowledge from wikipedia.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5452">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous research has explored various methods and features for relationship detection and mining.
</prevsent>
<prevsent>kernel methods have been popularly used for relation detection.
</prevsent>
</prevsection>
<citsent citstr=" P06-1104 ">
some examples are are dependency tree kernels (culotta and sorensen, 2004), <papid> P04-1054 </papid>shortest dependency path kernels (bunescu and mooney, 2005), <papid> H05-1091 </papid>and more recently, convolution tree kernels (zhao and grishman, 2005; <papid> P05-1052 </papid>zhang et al , 2006) <papid> P06-1104 </papid>context-sensitive convolution tree kernels (zhou et al ., 2007) <papid> D07-1076 </papid>and dynamic syntax tree kernels (qian et al ., 2008).<papid> C08-1088 </papid></citsent>
<aftsection>
<nextsent>kernel methods for relation extraction focus on representing and capturing the structured information of the text between the entities.
</nextsent>
<nextsent>in our medlda model, instead of computing distances between subtrees, we sample topics based on their distributions.
</nextsent>
<nextsent>the sampling is not only on the (mini) document level, but also on the word level or on the syntactic or semantic level.
</nextsent>
<nextsent>our model focuses on addressing the underlying semantics more directly than typical kernel-based methods.chan and roth (2011) employ constraints using an integer linear programming (ilp) framework.using this, they apply rich linguistic and knowledge based constraints based on coreference annotations, hierarchy of relations, syntacto-semantic structure, and knowledge from wikipedia.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5453">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous research has explored various methods and features for relationship detection and mining.
</prevsent>
<prevsent>kernel methods have been popularly used for relation detection.
</prevsent>
</prevsection>
<citsent citstr=" D07-1076 ">
some examples are are dependency tree kernels (culotta and sorensen, 2004), <papid> P04-1054 </papid>shortest dependency path kernels (bunescu and mooney, 2005), <papid> H05-1091 </papid>and more recently, convolution tree kernels (zhao and grishman, 2005; <papid> P05-1052 </papid>zhang et al , 2006) <papid> P06-1104 </papid>context-sensitive convolution tree kernels (zhou et al ., 2007) <papid> D07-1076 </papid>and dynamic syntax tree kernels (qian et al ., 2008).<papid> C08-1088 </papid></citsent>
<aftsection>
<nextsent>kernel methods for relation extraction focus on representing and capturing the structured information of the text between the entities.
</nextsent>
<nextsent>in our medlda model, instead of computing distances between subtrees, we sample topics based on their distributions.
</nextsent>
<nextsent>the sampling is not only on the (mini) document level, but also on the word level or on the syntactic or semantic level.
</nextsent>
<nextsent>our model focuses on addressing the underlying semantics more directly than typical kernel-based methods.chan and roth (2011) employ constraints using an integer linear programming (ilp) framework.using this, they apply rich linguistic and knowledge based constraints based on coreference annotations, hierarchy of relations, syntacto-semantic structure, and knowledge from wikipedia.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5458">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our model focuses on addressing the underlying semantics more directly than typical kernel-based methods.chan and roth (2011) employ constraints using an integer linear programming (ilp) framework.using this, they apply rich linguistic and knowledge based constraints based on coreference annotations, hierarchy of relations, syntacto-semantic structure, and knowledge from wikipedia.
</prevsent>
<prevsent>in our work, we focus on capturing the latent semantics of the text between the nes.
</prevsent>
</prevsection>
<citsent citstr=" A00-2030 ">
a variety of features have been explored for erd in previous research (zhou et al , 2005; <papid> P05-1053 </papid>zhou et al , 2008; jiang and zhai, 2007; <papid> N07-1015 </papid>miller et al , 2000).<papid> A00-2030 </papid></citsent>
<aftsection>
<nextsent>syntactic features such as pos tags and dependency path between entities; semantic features such as word-net relations, semantic parse trees and types of nes; and structural features such as which entity came first in the sentence have been found useful forerd.
</nextsent>
<nextsent>we too observe the utility of informative features for this task.
</nextsent>
<nextsent>however, exploration of the feature space is not the main focus of this work.
</nextsent>
<nextsent>rather, our focus is on whether the models are capable of incorporating rich features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5465">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our erd task is different from these as we focus on classifying the relation types into predefined relation types in the ace05 corpus.
</prevsent>
<prevsent>topic models have been applied previously for number of nlp tasks (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W06-2915 ">
(lin et al , 2006; <papid> W06-2915 </papid>titov and mcdonald, 2008).<papid> P08-1036 </papid></citsent>
<aftsection>
<nextsent>ldas have also been employed to reduce feature dimensions in relation detection systems (hachey, 2006).<papid> W06-1105 </papid></nextsent>
<nextsent>however, to the best of our knowledge, this is the first work to make use of topic models to perform relation detection.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5466">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our erd task is different from these as we focus on classifying the relation types into predefined relation types in the ace05 corpus.
</prevsent>
<prevsent>topic models have been applied previously for number of nlp tasks (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P08-1036 ">
(lin et al , 2006; <papid> W06-2915 </papid>titov and mcdonald, 2008).<papid> P08-1036 </papid></citsent>
<aftsection>
<nextsent>ldas have also been employed to reduce feature dimensions in relation detection systems (hachey, 2006).<papid> W06-1105 </papid></nextsent>
<nextsent>however, to the best of our knowledge, this is the first work to make use of topic models to perform relation detection.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5467">
<title id=" W11-1101.xml">a combination of topic models with max margin learning for relation detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>topic models have been applied previously for number of nlp tasks (e.g.
</prevsent>
<prevsent>(lin et al , 2006; <papid> W06-2915 </papid>titov and mcdonald, 2008).<papid> P08-1036 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-1105 ">
ldas have also been employed to reduce feature dimensions in relation detection systems (hachey, 2006).<papid> W06-1105 </papid></citsent>
<aftsection>
<nextsent>however, to the best of our knowledge, this is the first work to make use of topic models to perform relation detection.
</nextsent>
<nextsent>in this work, we presented system for entity relation detection based on mixed-membership medlda.
</nextsent>
<nextsent>our approach was motivated by the idea that combination of max margin and maximum likelihood can help to improve relation detection task.
</nextsent>
<nextsent>for this, we adapted the existing work on medlda and mixed membership models and formulated erdas topic detection task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5468">
<title id=" W11-0323.xml">filling the gap semi supervised learning for opinion detection across domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is good indicator of an opinion in blogs, it is less likely to occur in the same role in newspaper texts.
</prevsent>
<prevsent>while it is difficult to obtain opinion-labeled data,one can easily collect almost infinite unlabeled user generated data that contain opinions.
</prevsent>
</prevsection>
<citsent citstr=" P10-1149 ">
the use ofsemi-supervised learning (ssl), motivated by limited labeled data and plentiful unlabeled data in thereal world, has achieved promising results in various nlp studies (e.g., (furstenau and lapata, 2009; talukdar and pereira, 2010)), <papid> P10-1149 </papid>yet it has not been fully investigated for use in opinion detection.</citsent>
<aftsection>
<nextsent>although studies have shown that simple ssl methods are promising for extracting opinion features or patterns using limited opinion-labeled data (e.g., (wiebe and riloff, 2005)), few efforts have been made either to apply ssl directly to opinion detection or to examine more sophisticated ssl methods.this research is intended to fill the gap regarding application of ssl in opinion detection.
</nextsent>
<nextsent>we investigate range of ssl algorithms with focus on self training and co-training in three types of electronic documents: edited news articles, semi-structured movie reviews, and the informal and unstructured content of the blogosphere.
</nextsent>
<nextsent>we conclude that ssl is successful method for handling the shortage of opinion labeled data and the domain transfer problem.
</nextsent>
<nextsent>200
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5469">
<title id=" W11-0323.xml">filling the gap semi supervised learning for opinion detection across domains </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>we concentrate here on supervised and semi supervised approaches.
</prevsent>
<prevsent>2.1 supervised learning for opinion detection.
</prevsent>
</prevsection>
<citsent citstr=" J04-3002 ">
supervised learning algorithms that can automatically learn important opinion-bearing features froman annotated corpus have been adopted and investigated for opinion detection and yielded satisfying results (wiebe et al, 2004; <papid> J04-3002 </papid>yu and hatzivassiloglou,2003; <papid> W03-1017 </papid>zhang and yu, 2007).</citsent>
<aftsection>
<nextsent>with no classification techniques developed specifically for opinion detection, state-of-the-art topical supervised classification algorithms can achieve performance comparable to complex linguistic approaches when using binary values (i.e., presence or absence) and incorporating different types of features.
</nextsent>
<nextsent>commonly used opinion-bearing features include bag-of-words, pos tags, ngrams, low frequency words or unique words (wiebe et al, 2004; <papid> J04-3002 </papid>yang et al, 2007), semantically oriented adjectives (e.g., great?, poor?)</nextsent>
<nextsent>and more complex linguistic patterns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5471">
<title id=" W11-0323.xml">filling the gap semi supervised learning for opinion detection across domains </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>we concentrate here on supervised and semi supervised approaches.
</prevsent>
<prevsent>2.1 supervised learning for opinion detection.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
supervised learning algorithms that can automatically learn important opinion-bearing features froman annotated corpus have been adopted and investigated for opinion detection and yielded satisfying results (wiebe et al, 2004; <papid> J04-3002 </papid>yu and hatzivassiloglou,2003; <papid> W03-1017 </papid>zhang and yu, 2007).</citsent>
<aftsection>
<nextsent>with no classification techniques developed specifically for opinion detection, state-of-the-art topical supervised classification algorithms can achieve performance comparable to complex linguistic approaches when using binary values (i.e., presence or absence) and incorporating different types of features.
</nextsent>
<nextsent>commonly used opinion-bearing features include bag-of-words, pos tags, ngrams, low frequency words or unique words (wiebe et al, 2004; <papid> J04-3002 </papid>yang et al, 2007), semantically oriented adjectives (e.g., great?, poor?)</nextsent>
<nextsent>and more complex linguistic patterns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5474">
<title id=" W11-0323.xml">filling the gap semi supervised learning for opinion detection across domains </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>self-training self-training is the simplest and most commonly adopted form of ssl for opiniondetection.
</prevsent>
<prevsent>self-training was originally used to facilitate automatic identification of opinion-bearingfeatures.
</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
for example, riloff and wiebe (2003) <papid> W03-1014 </papid>proposed bootstrapping process to automatically identify subjective patterns.</citsent>
<aftsection>
<nextsent>self-training has also been applied directly for identifying subjective sentences by following standard self-training procedure: (1) train an initial supervised classifier on the labeled data; (2) apply this classifier to unlabeled data and select the most confidently labeled data, as determined by the classifier, to augment the labeled data set; and (3) re-train the classifier by restarting the whole process.
</nextsent>
<nextsent>wiebe and riloff (2005) used self trained nave bayes classifier for classifying subjective sentences and achieved better recall with modest precision over several rule-based classifiers.one shortcoming of self-training is that the resulting data may be biased: that is, the final labeled datamay consist of examples that are easiest for this particular opinion detector to identify.
</nextsent>
<nextsent>co-training the core idea of co-training is to usetwo classifiers and trade additional examples between them, assuming that the resulting union of classified examples is more balanced than examples resulting from using either classifier alone.
</nextsent>
<nextsent>when labeling new examples, final decision is made by combining the predictions of the two updated learners.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5475">
<title id=" W11-0323.xml">filling the gap semi supervised learning for opinion detection across domains </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>em-based ssl expectation-maximization (em) refers to class of iterative algorithms for maximum-likelihood estimation when dealing with 201 incomplete data.
</prevsent>
<prevsent>nigam et al (1999) combined em with nave bayes classifier to resolve the problem of topical classification, where unlabeled data were treated as incomplete data.
</prevsent>
</prevsection>
<citsent citstr=" E06-1026 ">
the em-nbssl algorithm yielded better performance than either an unsupervised lexicon-based approach or supervised approach for sentiment classification in different data domains, including blog data (aue and gamon, 2005; takamura et al, 2006).<papid> E06-1026 </papid></citsent>
<aftsection>
<nextsent>no opinion detection applications of em-based ssl have been reported in the literature.s3vms semi-supervised support vector machines (s3vms) are natural extension of svms in the semi-supervised spectrum.
</nextsent>
<nextsent>they are designed tofind the maximal margin decision boundary in vector space containing both labeled and unlabeled examples.
</nextsent>
<nextsent>although svms are the most favored supervised learning method for opinion detection, s3vmshave not been used in opinion detection.
</nextsent>
<nextsent>graph based ssl learning has been successfully applied to opinion detection (pang and lee, 2004) <papid> P04-1035 </papid>but is not appropriate for dealing with large scale data sets.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5476">
<title id=" W11-0323.xml">filling the gap semi supervised learning for opinion detection across domains </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>they are designed tofind the maximal margin decision boundary in vector space containing both labeled and unlabeled examples.
</prevsent>
<prevsent>although svms are the most favored supervised learning method for opinion detection, s3vmshave not been used in opinion detection.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
graph based ssl learning has been successfully applied to opinion detection (pang and lee, 2004) <papid> P04-1035 </papid>but is not appropriate for dealing with large scale data sets.</citsent>
<aftsection>
<nextsent>2.3 domain adaptation for opinion detection.
</nextsent>
<nextsent>when there are few opinion-labeled data in the target domain and/or when the characteristics of the target domain make it challenging to detect opinions, opinion detection systems usually borrow opinion-labeled data from other data domains.
</nextsent>
<nextsent>thisis especially common in opinion detection in the bl ogosphere (chesley et al, 2006).
</nextsent>
<nextsent>to evaluate this shallow approach, aue and gamon (2005) compared four strategies for utilizing opinion-labeleddata from one or more non-target domains and concluded that using non-targeted labeled data without an adaptation strategy is less efficient than using labeled data from the target domain, even when the majority of labels are assigned automatically by self-training algorithm.blitzer et al (2007) <papid> P07-1056 </papid>and tan et al (2009) implemented domain adaptation strategies for sentimentanalysis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5477">
<title id=" W11-0323.xml">filling the gap semi supervised learning for opinion detection across domains </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>when there are few opinion-labeled data in the target domain and/or when the characteristics of the target domain make it challenging to detect opinions, opinion detection systems usually borrow opinion-labeled data from other data domains.
</prevsent>
<prevsent>thisis especially common in opinion detection in the bl ogosphere (chesley et al, 2006).
</prevsent>
</prevsection>
<citsent citstr=" P07-1056 ">
to evaluate this shallow approach, aue and gamon (2005) compared four strategies for utilizing opinion-labeleddata from one or more non-target domains and concluded that using non-targeted labeled data without an adaptation strategy is less efficient than using labeled data from the target domain, even when the majority of labels are assigned automatically by self-training algorithm.blitzer et al (2007) <papid> P07-1056 </papid>and tan et al (2009) implemented domain adaptation strategies for sentimentanalysis.</citsent>
<aftsection>
<nextsent>although promising, their domain adaptation strategies involved sophisticated and computationally expensive methods for selecting general features to link target and non-target domains.
</nextsent>
<nextsent>while ssl is especially attractive for opinion detection because it only requires small number of labeled examples, the studies described in the previous section have concentrated on simple ssl methods.
</nextsent>
<nextsent>we intend to fill this research gap by comparing the feasibility and effectiveness of range of ssl approaches for opinion detection.
</nextsent>
<nextsent>specifically, we aim to achieve the following goals:first, to gain more comprehensive understanding of the utility of ssl in opinion detection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5478">
<title id=" W11-0323.xml">filling the gap semi supervised learning for opinion detection across domains </title>
<section> ssl experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our research treats opinion detection as binary classification problem with two categories: subjective sentences and objective sentences.
</prevsent>
<prevsent>it is evaluated in terms of classification accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W01-1626 ">
since document is normally mixture of facts and opinions (wiebe et al, 2001), <papid> W01-1626 </papid>sub-documentlevel opinion detection is more useful and meaningful than document-level opinion detection.</citsent>
<aftsection>
<nextsent>thus, we conduct all experiments on the sentence level.
</nextsent>
<nextsent>the remainder of this section explains the datasets and tools used in this study and presents the experimental design and parameter settings.
</nextsent>
<nextsent>4.1 datasets.
</nextsent>
<nextsent>three types of datasets have been explored in opinion detection studies: news articles, online reviews, and online discourse in blogs or discussion forums.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5480">
<title id=" W11-0323.xml">filling the gap semi supervised learning for opinion detection across domains </title>
<section> ssl experiments.  </section>
<citcontext>
<prevsection>
<prevsent>news article the wall street journal part of the penn treebank iii has been manually augmented with opinion related annotations.
</prevsent>
<prevsent>this set is widely used as gold-standard corpus in opinion detection research.
</prevsent>
</prevsection>
<citsent citstr=" P99-1032 ">
according to the coding manual (wiebeet al, 1999), <papid> P99-1032 </papid>subjective sentences are those expressing evaluations, opinions, emotions, and specula tions.</citsent>
<aftsection>
<nextsent>for our research, 5,174 objective sentences and 5,297 subjective sentences were selected basedon the absence or presence of manually labeled subjective expressions.
</nextsent>
<nextsent>jdpa blog post the jdpa corpus (kessler et al, 2010) is new opinion corpus released in 2010.
</nextsent>
<nextsent>it consists of blog posts that express opinions about automobile and digital cameras with named entities and sentiments expressed about them manually annotated.
</nextsent>
<nextsent>for our purpose, we extracted all sentences containing sentiment-bearing expressions as subjective sentences and manually chose objective sentences from the rest by eliminating subjective sentences that were not targeted to any labeled entities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5481">
<title id=" W11-0410.xml">a scale able automated quality assurance technique for semantic representations and proposition banks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also report on the implications ofour findings for the correctness of the semantic representations in propbank.
</prevsent>
<prevsent>it has recently been suggested that in addition tomore, bigger, and better resources, we need science of creating them (palmer et al, download date december 17 2010).
</prevsent>
</prevsection>
<citsent citstr=" W05-1306 ">
the corpus linguistics community has arguably been developing at least nascent science of annotation for years, represented by publications such as (leech, 1993; ide and brew, 2000; wynne, 2005;cohen et al, 2005<papid> W05-1306 </papid>a; cohen et al, 2005<papid> W05-1306 </papid>b) that address architectural, sampling, and procedural issues,as well as publications such as (hripcsak and rothschild, 2005; artstein and poesio, 2008) <papid> J08-4004 </papid>that address issues in inter-annotator agreement.</citsent>
<aftsection>
<nextsent>however, there is not yet significant body of work on the subject of quality assurance for corpora, or for that matter,for many other types of linguistic resources.
</nextsent>
<nextsent>(meyers et al, 2004) describe three error-checking measures used in the construction of nombank, and the use of inter-annotator agreement as quality control measure for corpus construction is discussed at some length in (marcus et al, 1993; <papid> J93-2004 </papid>palmer et al, 2005)<papid> J05-1004 </papid></nextsent>
<nextsent>however, discussion of quality control for corpora is otherwise limited or nonexistent.with the exception of the inter-annotator agreement-oriented work mentioned above, none of this work is quantitative.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5485">
<title id=" W11-0410.xml">a scale able automated quality assurance technique for semantic representations and proposition banks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also report on the implications ofour findings for the correctness of the semantic representations in propbank.
</prevsent>
<prevsent>it has recently been suggested that in addition tomore, bigger, and better resources, we need science of creating them (palmer et al, download date december 17 2010).
</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
the corpus linguistics community has arguably been developing at least nascent science of annotation for years, represented by publications such as (leech, 1993; ide and brew, 2000; wynne, 2005;cohen et al, 2005<papid> W05-1306 </papid>a; cohen et al, 2005<papid> W05-1306 </papid>b) that address architectural, sampling, and procedural issues,as well as publications such as (hripcsak and rothschild, 2005; artstein and poesio, 2008) <papid> J08-4004 </papid>that address issues in inter-annotator agreement.</citsent>
<aftsection>
<nextsent>however, there is not yet significant body of work on the subject of quality assurance for corpora, or for that matter,for many other types of linguistic resources.
</nextsent>
<nextsent>(meyers et al, 2004) describe three error-checking measures used in the construction of nombank, and the use of inter-annotator agreement as quality control measure for corpus construction is discussed at some length in (marcus et al, 1993; <papid> J93-2004 </papid>palmer et al, 2005)<papid> J05-1004 </papid></nextsent>
<nextsent>however, discussion of quality control for corpora is otherwise limited or nonexistent.with the exception of the inter-annotator agreement-oriented work mentioned above, none of this work is quantitative.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5486">
<title id=" W11-0410.xml">a scale able automated quality assurance technique for semantic representations and proposition banks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the corpus linguistics community has arguably been developing at least nascent science of annotation for years, represented by publications such as (leech, 1993; ide and brew, 2000; wynne, 2005;cohen et al, 2005<papid> W05-1306 </papid>a; cohen et al, 2005<papid> W05-1306 </papid>b) that address architectural, sampling, and procedural issues,as well as publications such as (hripcsak and rothschild, 2005; artstein and poesio, 2008) <papid> J08-4004 </papid>that address issues in inter-annotator agreement.</prevsent>
<prevsent>however, there is not yet significant body of work on the subject of quality assurance for corpora, or for that matter,for many other types of linguistic resources.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
(meyers et al, 2004) describe three error-checking measures used in the construction of nombank, and the use of inter-annotator agreement as quality control measure for corpus construction is discussed at some length in (marcus et al, 1993; <papid> J93-2004 </papid>palmer et al, 2005)<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>however, discussion of quality control for corpora is otherwise limited or nonexistent.with the exception of the inter-annotator agreement-oriented work mentioned above, none of this work is quantitative.
</nextsent>
<nextsent>this is problem if ourgoal is the development of true science of annota tion.work on quality assurance for computational lexical resources other than ontologies is especiallylacking.
</nextsent>
<nextsent>however, the body of work on quality assurance for ontologies (kohler et al, 2006; ceusters et al, 2004; cimino et al, 2003; cimino, 1998;cimino, 2001; ogren et al, 2004) is worth considering in the context of this paper.
</nextsent>
<nextsent>one common themein that work is that even manually curated lexical resources contain some percentage of errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5487">
<title id=" W11-0410.xml">a scale able automated quality assurance technique for semantic representations and proposition banks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the corpus linguistics community has arguably been developing at least nascent science of annotation for years, represented by publications such as (leech, 1993; ide and brew, 2000; wynne, 2005;cohen et al, 2005<papid> W05-1306 </papid>a; cohen et al, 2005<papid> W05-1306 </papid>b) that address architectural, sampling, and procedural issues,as well as publications such as (hripcsak and rothschild, 2005; artstein and poesio, 2008) <papid> J08-4004 </papid>that address issues in inter-annotator agreement.</prevsent>
<prevsent>however, there is not yet significant body of work on the subject of quality assurance for corpora, or for that matter,for many other types of linguistic resources.</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
(meyers et al, 2004) describe three error-checking measures used in the construction of nombank, and the use of inter-annotator agreement as quality control measure for corpus construction is discussed at some length in (marcus et al, 1993; <papid> J93-2004 </papid>palmer et al, 2005)<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>however, discussion of quality control for corpora is otherwise limited or nonexistent.with the exception of the inter-annotator agreement-oriented work mentioned above, none of this work is quantitative.
</nextsent>
<nextsent>this is problem if ourgoal is the development of true science of annota tion.work on quality assurance for computational lexical resources other than ontologies is especiallylacking.
</nextsent>
<nextsent>however, the body of work on quality assurance for ontologies (kohler et al, 2006; ceusters et al, 2004; cimino et al, 2003; cimino, 1998;cimino, 2001; ogren et al, 2004) is worth considering in the context of this paper.
</nextsent>
<nextsent>one common themein that work is that even manually curated lexical resources contain some percentage of errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5490">
<title id=" W11-0410.xml">a scale able automated quality assurance technique for semantic representations and proposition banks </title>
<section> 3.5% (163) </section>
<citcontext>
<prevsection>
<prevsent>4.1 the effect of the argument/adjunct.
</prevsent>
<prevsent>distinction the validity and usefulness of the distinction between arguments and adjuncts is an ongoing controversy in biomedical computational lexical semantics.
</prevsent>
</prevsection>
<citsent citstr=" W06-0602 ">
the bioprop project (chou et al, 2006;<papid> W06-0602 </papid>tsai et al, 2006) <papid> W06-3308 </papid>makes considerable use of adjuncts, essentially identically to propbank; however,most biomedical pas-oriented projects have relatively larger numbers of arguments and lesser use of adjuncts (wattarujeekrit et al, 2004; kogan et al, 2005; shah et al, 2005) than propbank.</citsent>
<aftsection>
<nextsent>overall, one would predict fewer non-cooccurring arguments with set of representations that made stronger distinction between arguments and adjuncts; over all arity of role sets would be smaller (see above forthe effect of arity on the number of observations required for predicate), and the arguments for such are presentation might be more core?
</nextsent>
<nextsent>to the semantics of the predicate, and might therefore be less likely to not occur overall, and therefore less likely to not cooccur.
</nextsent>
<nextsent>4.2 the effect of syntactic representation on.
</nextsent>
<nextsent>observed argument distributions the original work by cohen and hunter assumed avery simple, and very surface, syntactic representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5491">
<title id=" W11-0410.xml">a scale able automated quality assurance technique for semantic representations and proposition banks </title>
<section> 3.5% (163) </section>
<citcontext>
<prevsection>
<prevsent>4.1 the effect of the argument/adjunct.
</prevsent>
<prevsent>distinction the validity and usefulness of the distinction between arguments and adjuncts is an ongoing controversy in biomedical computational lexical semantics.
</prevsent>
</prevsection>
<citsent citstr=" W06-3308 ">
the bioprop project (chou et al, 2006;<papid> W06-0602 </papid>tsai et al, 2006) <papid> W06-3308 </papid>makes considerable use of adjuncts, essentially identically to propbank; however,most biomedical pas-oriented projects have relatively larger numbers of arguments and lesser use of adjuncts (wattarujeekrit et al, 2004; kogan et al, 2005; shah et al, 2005) than propbank.</citsent>
<aftsection>
<nextsent>overall, one would predict fewer non-cooccurring arguments with set of representations that made stronger distinction between arguments and adjuncts; over all arity of role sets would be smaller (see above forthe effect of arity on the number of observations required for predicate), and the arguments for such are presentation might be more core?
</nextsent>
<nextsent>to the semantics of the predicate, and might therefore be less likely to not occur overall, and therefore less likely to not cooccur.
</nextsent>
<nextsent>4.2 the effect of syntactic representation on.
</nextsent>
<nextsent>observed argument distributions the original work by cohen and hunter assumed avery simple, and very surface, syntactic representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5492">
<title id=" W10-4221.xml">cross linguistic attribute selection for reg comparing dutch and english </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the languages we investigate are english and dutch.many reg algorithms require training data, before they can successfully be applied to generate references in particular domain.
</prevsent>
<prevsent>the incremental algorithm (dale and reiter, 1995), for example, assumes that certain attributes are more preferred than others, and it is assumed that determining the preference order of attributes is an empirical matter that needs to be settled for each new domain.
</prevsent>
</prevsection>
<citsent citstr=" J03-1003 ">
the graph-based algorithm (krahmer et al., 2003), <papid> J03-1003 </papid>to give second example, similarly assumes that certain attributes are preferred (are cheaper?)</citsent>
<aftsection>
<nextsent>than others, and that data are required to compute the attribute-cost functions.
</nextsent>
<nextsent>traditional text corpora have been argued to be of restricted value for reg, since these typically are not semantically transparent?
</nextsent>
<nextsent>(van deemter et al, 2006).
</nextsent>
<nextsent>rather what seems to be needed isdata collected from human participants, who produce referring expressions for specific targets in settings where all properties of the target and its dis tractors are known.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5493">
<title id=" W10-4221.xml">cross linguistic attribute selection for reg comparing dutch and english </title>
<section> corpora.  </section>
<citcontext>
<prevsection>
<prevsent>the tuna corpus is split into two domains: one with descriptions of furniture and one with descriptions of people.
</prevsent>
<prevsent>the tuna corpus was used for the comparative evaluation of reg systems in the tuna challenges (2007-2009).
</prevsent>
</prevsection>
<citsent citstr=" W08-1131 ">
for our current experiments, we used the tuna 2008 challenge training and development sets (gatt et al, 2008) <papid> W08-1131 </papid>to train and evaluate the graph-based algorithm on.</citsent>
<aftsection>
<nextsent>2.2 dutch: the d-tuna corpus.
</nextsent>
<nextsent>for dutch, we used the d(utch)-tuna corpus of object descriptions (koolen and krahmer, 2010).
</nextsent>
<nextsent>the collection of this corpus was inspired by the tuna experiment described above, and was done using the same visual scenes.
</nextsent>
<nextsent>there were three conditions: text, speech and face-to-face.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5496">
<title id=" W10-4221.xml">cross linguistic attribute selection for reg comparing dutch and english </title>
<section> graph-based attribute selection.  </section>
<citcontext>
<prevsection>
<prevsent>the algorithms output isthe cheapest distinguishing subgraph, given particular cost function that assigns costs to attributes.
</prevsent>
<prevsent>by assigning zero costs to some attributes, e.g.,the type of an object, the human tendency to mention redundant attributes can be mimicked.
</prevsent>
</prevsection>
<citsent citstr=" L08-1090 ">
how ever, as shown by viethen et al (2008), <papid> L08-1090 </papid>merely assigning zero costs to an attribute is not sufficient condition for inclusion; if the graph search terminates before the free attributes are tried, they will not be included.</citsent>
<aftsection>
<nextsent>therefore, the order in which attributes are tried must be explicitly controlled.
</nextsent>
<nextsent>thus, when using the graph-based algorithm for attribute selection, two things must be specified: (1) the cost function, and (2) the order in which the attributes should be searched.
</nextsent>
<nextsent>both can be based on corpus data, as described in the next section.
</nextsent>
<nextsent>for our experiments, we used the graph-based attribute selection algorithm with two types of cost functions: stochastic costs and free-nave costs.both reflect (to different extent) the relative attribute frequencies found in training corpus: the more frequently an attribute occurs in the training data, the cheaper it is in the cost functions.stochastic costs are directly based on the attribute frequencies in the training corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5502">
<title id=" W11-0116.xml">using query patterns to learn the duration of events </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our approach uses web queries to model an events typical distribution in the real world.
</prevsent>
<prevsent>learning such rich aspect ual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g., chen and ji, 2009) in much the same way that gender has benefited nominal coreference systems.
</prevsent>
</prevsection>
<citsent citstr=" W07-2014 ">
event durations are also key to building event timelines and other deeper temporal understandings of text (verhagen et al , 2007; <papid> W07-2014 </papid>pustejovsky and verhagen, 2009).<papid> W09-2418 </papid></citsent>
<aftsection>
<nextsent>the contributions of this work are: ? demonstrating how to acquire event duration distributions by querying the web with patterns.
</nextsent>
<nextsent>showing that system that predicts event durations based only on our web count distributions can outperform supervised system that requires manually annotated training data.
</nextsent>
<nextsent>making available an event duration lexicon with duration distributions for common english events.
</nextsent>
<nextsent>we first review previous work and describe our re-implementation and augmentation of the latest supervised system for predicting event durations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5503">
<title id=" W11-0116.xml">using query patterns to learn the duration of events </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our approach uses web queries to model an events typical distribution in the real world.
</prevsent>
<prevsent>learning such rich aspect ual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g., chen and ji, 2009) in much the same way that gender has benefited nominal coreference systems.
</prevsent>
</prevsection>
<citsent citstr=" W09-2418 ">
event durations are also key to building event timelines and other deeper temporal understandings of text (verhagen et al , 2007; <papid> W07-2014 </papid>pustejovsky and verhagen, 2009).<papid> W09-2418 </papid></citsent>
<aftsection>
<nextsent>the contributions of this work are: ? demonstrating how to acquire event duration distributions by querying the web with patterns.
</nextsent>
<nextsent>showing that system that predicts event durations based only on our web count distributions can outperform supervised system that requires manually annotated training data.
</nextsent>
<nextsent>making available an event duration lexicon with duration distributions for common english events.
</nextsent>
<nextsent>we first review previous work and describe our re-implementation and augmentation of the latest supervised system for predicting event durations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5504">
<title id=" W11-0116.xml">using query patterns to learn the duration of events </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we present generated database of event durations.
</prevsent>
<prevsent>145
</prevsent>
</prevsection>
<citsent citstr=" J00-4004 ">
early work on extracting event properties focused on linguistic aspect, for example, automatically distinguishing culminated events that have an end point from non-culminated events that do not (siegel and mckeown, 2000).<papid> J00-4004 </papid></citsent>
<aftsection>
<nextsent>the more fine-grained task of predicting the duration of events was first proposed by pan et al  (2006), <papid> P06-1050 </papid>who annotated each event in small section of the time bank (pustejovsky et al , 2003) with duration lower and upper bounds.</nextsent>
<nextsent>they then trained support vector machines on their annotated corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes, hours, etc. their models used features like bags of words, heads of syntactic subjects and objects, and wordnet hypernyms of the events.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5505">
<title id=" W11-0116.xml">using query patterns to learn the duration of events </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>145
</prevsent>
<prevsent>early work on extracting event properties focused on linguistic aspect, for example, automatically distinguishing culminated events that have an end point from non-culminated events that do not (siegel and mckeown, 2000).<papid> J00-4004 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1050 ">
the more fine-grained task of predicting the duration of events was first proposed by pan et al  (2006), <papid> P06-1050 </papid>who annotated each event in small section of the time bank (pustejovsky et al , 2003) with duration lower and upper bounds.</citsent>
<aftsection>
<nextsent>they then trained support vector machines on their annotated corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes, hours, etc. their models used features like bags of words, heads of syntactic subjects and objects, and wordnet hypernyms of the events.
</nextsent>
<nextsent>this work provides valuable resource in its annotated corpus and is also good baseline.
</nextsent>
<nextsent>we replicate their work and also add new features as described below.
</nextsent>
<nextsent>our approach to the duration problem is inspired by the standard use of web patterns for the acquisition of relational lexical knowledge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5532">
<title id=" W11-0116.xml">using query patterns to learn the duration of events </title>
<section> unsupervised approach.  </section>
<citcontext>
<prevsection>
<prevsent>while supervised learning is effective for many nlp tasks, it is sensitive to the amount of available training data.
</prevsent>
<prevsent>unfortunately, the training data for event durations is very small, consisting of only 58 news articles (pan et al , 2006), <papid> P06-1050 </papid>and labeling further data is quite expensive.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
this motivates our desire to find an 1we parsed the documents into typed dependencies with the stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>147 approach that does not relyon labeled data, but instead utilizes the large amounts of text available on the web to search for duration-specific patterns.
</nextsent>
<nextsent>this section describes our web-based approach to learning event durations.
</nextsent>
<nextsent>5.1 web query patterns.
</nextsent>
<nextsent>temporal properties of events are often described explicitly in language-specific constructions which can help us infer an events duration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5533">
<title id=" W11-0116.xml">using query patterns to learn the duration of events </title>
<section> unsupervised approach.  </section>
<citcontext>
<prevsection>
<prevsent>after these queries, we have pattern-specific distribution of counts over the various buckets, coarse measure of the types of durations that might be appropriate to this event.
</prevsent>
<prevsent>figure 1(a) shows an example of such distribution.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
as can be seen in figure 1(a), this initial distribution can be skewed in various ways ? in this case, years is given far too much mass. this is because in addition to the single event interpretation of words like saying?, there are iterative or habitual interpretations (moens and steedman, 1988; <papid> J88-2003 </papid>frawley, 1992).</citsent>
<aftsection>
<nextsent>iterative events occur repeatedly over period of time, e.g., hes been saying for years that.
</nextsent>
<nextsent>the two interpretations are apparent in the raw distributions of smile and run in figure 2.
</nextsent>
<nextsent>the large peak at years for run shows that it is common to say someone was running for years.?
</nextsent>
<nextsent>conversely, it is less common to say someone was smiling for years,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5542">
<title id=" W11-0116.xml">using query patterns to learn the duration of events </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>6.2 mechanical turk dataset.
</prevsent>
<prevsent>we also collected event durations from amazons mechanical turk (mturk), an online marketplace from amazon where requesters can find workers to solve human intelligence tasks (hits) for small amounts of money.
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
prior work has shown that human judgments from mturk can often be as reliable as trained annotators (snow et al , 2008) <papid> D08-1027 </papid>or subjects in controlled lab studies (munro et al , 2010), <papid> W10-0719 </papid>particularly when judgments are aggregated over many mturk workers (turkers?).</citsent>
<aftsection>
<nextsent>our motivation for using turkers is to better analyze system errors.
</nextsent>
<nextsent>for example, if we give humans an event in isolation (no sentence context), how well can they guess the durations assigned by the pan et. al. annotators?
</nextsent>
<nextsent>this measures how big the gap is between system that looks only at the event, and system that integrates all available context.
</nextsent>
<nextsent>to collect event durations from mturk, we presented turkers with an event from the time bank (a superset of the events annotated by pan et al  (2006)) <papid> P06-1050 </papid>and asked them to decide whether the event was most likely to take seconds, minutes, hours, days, weeks, months, years or decades.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5543">
<title id=" W11-0116.xml">using query patterns to learn the duration of events </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>6.2 mechanical turk dataset.
</prevsent>
<prevsent>we also collected event durations from amazons mechanical turk (mturk), an online marketplace from amazon where requesters can find workers to solve human intelligence tasks (hits) for small amounts of money.
</prevsent>
</prevsection>
<citsent citstr=" W10-0719 ">
prior work has shown that human judgments from mturk can often be as reliable as trained annotators (snow et al , 2008) <papid> D08-1027 </papid>or subjects in controlled lab studies (munro et al , 2010), <papid> W10-0719 </papid>particularly when judgments are aggregated over many mturk workers (turkers?).</citsent>
<aftsection>
<nextsent>our motivation for using turkers is to better analyze system errors.
</nextsent>
<nextsent>for example, if we give humans an event in isolation (no sentence context), how well can they guess the durations assigned by the pan et. al. annotators?
</nextsent>
<nextsent>this measures how big the gap is between system that looks only at the event, and system that integrates all available context.
</nextsent>
<nextsent>to collect event durations from mturk, we presented turkers with an event from the time bank (a superset of the events annotated by pan et al  (2006)) <papid> P06-1050 </papid>and asked them to decide whether the event was most likely to take seconds, minutes, hours, days, weeks, months, years or decades.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZH5574">
<title id=" W11-0212.xml">in search of protein locations </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>the system was developed and evaluated using different subsets of the genomics text retrieval (trec) collection (hersh, &amp; voorhees, 2009).
</prevsent>
<prevsent>specifically 5533 articles in jbc 2002 were used for development and ~11,000 articles in jbc 2004 and 2005 were used in the evaluation.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
the syntactic paths used the dependency tree representation produced by the stanford parser (klein &amp; manning., 2003) (<papid> P03-1054 </papid>version 1.6.4).</citsent>
<aftsection>
<nextsent>101 figure 1 ? the bootstrapping approach used to generate new proteins, sub cellular locations and protein location pairs.
</nextsent>
<nextsent>inferred proteins and locations are depicted with dashed line.
</nextsent>
<nextsent>the system identified 792 new proteins in the first iteration.
</nextsent>
<nextsent>all but 3 of the most frequent 20 proteins were in uniprot.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>